{"id": "2506.00028", "pdf": "https://arxiv.org/pdf/2506.00028.pdf", "abs": "https://arxiv.org/abs/2506.00028", "title": "Visualization and Comparison of AOI Transitions with Force-Directed Graph Layout", "authors": ["Yuri Miyagi", "Nils Rodrigues", "Daniel Weiskopf", "Takayuki Itoh"], "categories": ["cs.HC"], "comment": null, "summary": "By analyzing the gaze trajectories of people viewing screens and\nadvertisements, we can determine what people are interested in. This knowledge\ncan be effective when recommending commercial products and services, and also,\nwhen improving advertisement design. Therefore, analysis and visualization of\neye gaze have been an active research topic. This paper proposes a new method\nfor visualizing patterns of the gaze trajectories of multiple people by (1)\nvisualizing patterns that move through multiple areas of interest (AOI) and (2)\nvisualizing differences among multiple gaze trajectories. The method first\nconstructs a hierarchical AOI structure to a Web page or an image, and uses\nthis structure to convert the trajectory into a sequence of symbols. We apply\nN-grams to the generated symbol sequences to extract transition patterns\nbetween AOIs. Finally, the method visualizes a list of the pattern extraction\nresults and the shapes of the characteristic elements. We present the\nvisualization of gaze trajectories for three examples of stimuli, and argue\nthat analysts can efficiently discover trends in gaze transitions between text\nand figures, as well as differences between participants of the eye-tracking\nexperiments."}
{"id": "2506.00241", "pdf": "https://arxiv.org/pdf/2506.00241.pdf", "abs": "https://arxiv.org/abs/2506.00241", "title": "Designing AI Tools for Clinical Care Teams to Support Serious Illness Conversations with Older Adults in the Emergency Department", "authors": ["Menglin Zhao", "Zhuorui Yong", "Ruijia Guan", "Kai-Wei Chang", "Adrian Haimovich", "Kei Ouchi", "Timothy Bickmore", "Bingsheng Yao", "Dakuo Wang", "Smit Desai"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Serious illness conversations (SICs), discussions between clinical care teams\nand patients with serious, life-limiting illnesses about their values, goals,\nand care preferences, are critical for patient-centered care. Without these\nconversations, patients often receive aggressive interventions that may not\nalign with their goals. Clinical care teams face significant barriers when\nconducting serious illness conversations with older adult patients in Emergency\nDepartment (ED) settings, where most older adult patients lack documented\ntreatment goals. To understand current practices and identify AI support\nopportunities, we conducted interviews with two domain experts and nine ED\nclinical care team members. Through thematic analysis, we characterized a\nfour-phase serious illness conversation workflow (identification, preparation,\nconduction, documentation) and identified key needs and challenges at each\nstage. Clinical care teams struggle with fragmented EHR data access, time\nconstraints, emotional preparation demands, and documentation burdens. While\nparticipants expressed interest in AI tools for information synthesis,\nconversational support, and automated documentation, they emphasized preserving\nhuman connection and clinical autonomy. We present design guidelines for AI\ntools supporting SIC workflows that fit within existing clinical practices.\nThis work contributes empirical understanding of ED-based serious illness\nconversations and provides design considerations for AI in high-stakes clinical\nenvironments."}
{"id": "2506.00376", "pdf": "https://arxiv.org/pdf/2506.00376.pdf", "abs": "https://arxiv.org/abs/2506.00376", "title": "Understanding Remote Communication between Grandparents and Grandchildren in Distributed Immigrant Families", "authors": ["Jiawen Stefanie Zhu", "Jian Zhao"], "categories": ["cs.HC", "cs.CY", "H.5.0"], "comment": "Poster at Graphics Interface 2025", "summary": "Grandparent-grandchild bonds are crucial for both parties. Many immigrant\nfamilies are geographically dispersed, and the grandparents and grandchildren\nneed to rely on remote communication to maintain their relationships. In\naddition to geographical separation, grandparents and grandchildren in such\nfamilies also face language and culture barriers during remote communication.\nThe associated challenges and needs remain understudied as existing research\nprimarily focuses on non-immigrant families or co-located immigrant families.\nTo address this gap, we conducted interviews with six Chinese immigrant\nfamilies in Canada. Our findings highlight unique challenges faced by immigrant\nfamilies during remote communication, such as amplified language and cultural\nbarriers due to geographic separation, and provide insights into how technology\ncan better support remote communication. This work offers empirical knowledge\nabout the communication needs of distributed immigrant families and provides\ndirections for future research and design to support grandparent-grandchild\nremote communication in these families."}
{"id": "2506.00717", "pdf": "https://arxiv.org/pdf/2506.00717.pdf", "abs": "https://arxiv.org/abs/2506.00717", "title": "Vid2Coach: Transforming How-To Videos into Task Assistants", "authors": ["Mina Huh", "Zihui Xue", "Ujjaini Das", "Kumar Ashutosh", "Kristen Grauman", "Amy Pavel"], "categories": ["cs.HC", "cs.CV"], "comment": null, "summary": "People use videos to learn new recipes, exercises, and crafts. Such videos\nremain difficult for blind and low vision (BLV) people to follow as they rely\non visual comparison. Our observations of visual rehabilitation therapists\n(VRTs) guiding BLV people to follow how-to videos revealed that VRTs provide\nboth proactive and responsive support including detailed descriptions,\nnon-visual workarounds, and progress feedback. We propose Vid2Coach, a system\nthat transforms how-to videos into wearable camera-based assistants that\nprovide accessible instructions and mixed-initiative feedback. From the video,\nVid2Coach generates accessible instructions by augmenting narrated instructions\nwith demonstration details and completion criteria for each step. It then uses\nretrieval-augmented-generation to extract relevant non-visual workarounds from\nBLV-specific resources. Vid2Coach then monitors user progress with a camera\nembedded in commercial smart glasses to provide context-aware instructions,\nproactive feedback, and answers to user questions. BLV participants (N=8) using\nVid2Coach completed cooking tasks with 58.5\\% fewer errors than when using\ntheir typical workflow and wanted to use Vid2Coach in their daily lives.\nVid2Coach demonstrates an opportunity for AI visual assistance that strengthens\nrather than replaces non-visual expertise."}
{"id": "2506.00019", "pdf": "https://arxiv.org/pdf/2506.00019.pdf", "abs": "https://arxiv.org/abs/2506.00019", "title": "Amadeus-Verbo Technical Report: The powerful Qwen2.5 family models trained in Portuguese", "authors": ["William Alberto Cruz-Casta√±eda", "Marcellus Amadeus"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This report introduces the experience of developing Amadeus Verbo, a family\nof large language models for Brazilian Portuguese. To handle diverse use cases,\nAmadeus Verbo includes base-tuned, merged, and instruction-tuned models in\nsizes of 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B parameters. Thus, the main\nobjective is to show how easy it is to fine-tune foundation models to\ndemocratize the open-source development of Brazilian Portuguese LLMs when data\nand resources are available. Amadeus-Verbo family models are all available at\nHuggingFace at\nhttps://huggingface.co/collections/amadeusai/amadeus-verbo-qwen25-67cf2e7aae69ce2b3bcdcfda."}
{"id": "2506.00791", "pdf": "https://arxiv.org/pdf/2506.00791.pdf", "abs": "https://arxiv.org/abs/2506.00791", "title": "CO-OPERA: A Human-AI Collaborative Playwriting Tool to Support Creative Storytelling for Interdisciplinary Drama Education", "authors": ["Xuejiao Ma", "Haibo Zhao", "Zinuo Guo", "Yijie Guo", "Guanhong Liu", "Bo Jiang"], "categories": ["cs.HC"], "comment": null, "summary": "Drama-in-education is an interdisciplinary instructional approach that\nintegrates subjects such as language, history, and psychology. Its core\ncomponent is playwriting. Based on need-finding interviews of 13 teachers, we\nfound that current general-purpose AI tools cannot effectively assist teachers\nand students during playwriting. Therefore, we propose CO-OPERA - a\ncollaborative playwriting tool integrating generative artificial intelligence\ncapabilities. In CO-OPERA, users can both expand their thinking through\ndiscussions with a tutor and converge their thinking by operating agents to\ngenerate script elements. Additionally, the system allows for iterative\nmodifications and regenerations based on user requirements. A system usability\ntest conducted with middle school students shows that our CO-OPERA helps users\nfocus on whole logical narrative development during playwriting. Our\nplaywriting examples and raw data for qualitative and quantitative analysis are\navailable at https://github.com/daisyinb612/CO-OPERA."}
{"id": "2506.00022", "pdf": "https://arxiv.org/pdf/2506.00022.pdf", "abs": "https://arxiv.org/abs/2506.00022", "title": "Scaling Physical Reasoning with the PHYSICS Dataset", "authors": ["Shenghe Zheng", "Qianjia Cheng", "Junchi Yao", "Mengsong Wu", "haonan he", "Ning Ding", "Yu Cheng", "Shuyue Hu", "Lei Bai", "Dongzhan Zhou", "Ganqu Cui", "Peng Ye"], "categories": ["cs.CL", "cs.LG", "physics.ed-ph"], "comment": "Work on physical datasets", "summary": "Large Language Models (LLMs) have achieved remarkable progress on advanced\nreasoning tasks such as mathematics and coding competitions. Meanwhile,\nphysics, despite being both reasoning-intensive and essential to real-world\nunderstanding, received limited academic and industrial attention. This paper\nintroduces PHYSICS, a dataset containing 16,568 high-quality physics problems\nspanning subjects and difficulty levels, to facilitate this issue.\nSpecifically, PHYSICS is curated with exercises from over 100 textbooks through\na carefully designed pipeline for quality control. It covers five major physics\ndomains: Mechanics, Electromagnetism, Thermodynamics, Optics, and Modern\nPhysics. It also spans a wide range of difficulty levels, from high school to\ngraduate-level physics courses. To utilize the data for improving and\nevaluating the model's physical reasoning capabilities, we split the dataset\ninto training and test sets, and provide reasoning paths generated by powerful\nreasoning models for the training data to facilitate model training. In\naddition, for the evaluation part, we find that existing evaluation frameworks\nexhibit biases in aspects such as units, simplification, and precision in\nphysics domain. To balance efficiency and accuracy, we introduce a Rule+Model\nevaluation framework tailored to physics problems. Our evaluations on current\nstate-of-the-art open-source and proprietary models highlight the limitations\nof current models in handling physics-related tasks. We hope that our dataset\nand evaluation methodology will jointly advance the development of LLMs in the\nfield of physics."}
{"id": "2506.01284", "pdf": "https://arxiv.org/pdf/2506.01284.pdf", "abs": "https://arxiv.org/abs/2506.01284", "title": "Fast SSVEP Detection Using a Calibration-Free EEG Decoding Framework", "authors": ["Chenlong Wang", "Jiaao Li", "Shuailei Zhang", "Wenbo Ding", "Xinlei Chen"], "categories": ["cs.HC"], "comment": null, "summary": "Steady-State Visual Evoked Potential is a brain response to visual stimuli\nflickering at constant frequencies. It is commonly used in brain-computer\ninterfaces for direct brain-device communication due to their simplicity,\nminimal training data, and high information transfer rate. Traditional methods\nsuffer from poor performance due to reliance on prior knowledge, while deep\nlearning achieves higher accuracy but requires substantial high-quality\ntraining data for precise signal decoding. In this paper, we propose a\ncalibration-free EEG signal decoding framework for fast SSVEP detection. Our\nframework integrates Inter-Trial Remixing & Context-Aware Distribution\nAlignment data augmentation for EEG signals and employs a compact architecture\nof small fully connected layers, effectively addressing the challenge of\nlimited EEG data availability. Additionally, we propose an Adaptive Spectrum\nDenoise Module that operates in the frequency domain based on global features,\nrequiring only linear complexity to reduce noise in EEG data and improve data\nquality. For calibration-free classification experiments on short EEG signals\nfrom three public datasets, our framework demonstrates statistically\nsignificant accuracy advantages(p<0.05) over existing methods in the majority\nof cases, while requiring at least 52.7% fewer parameters and 29.9% less\ninference time. By eliminating the need for user-specific calibration, this\nadvancement significantly enhances the usability of BCI systems, accelerating\ntheir commercialization and widespread adoption in real-world applications."}
{"id": "2506.00027", "pdf": "https://arxiv.org/pdf/2506.00027.pdf", "abs": "https://arxiv.org/abs/2506.00027", "title": "From Mathematical Reasoning to Code: Generalization of Process Reward Models in Test-Time Scaling", "authors": ["Zhengyu Chen", "Yudong Wang", "Teng Xiao", "Ruochen Zhou", "Xuesheng Yang", "Wei Wang", "Zhifang Sui", "Jingang Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in improving the reasoning capabilities of Large Language\nModels have underscored the efficacy of Process Reward Models (PRMs) in\naddressing intermediate errors through structured feedback mechanisms. This\nstudy analyzes PRMs from multiple perspectives, including training\nmethodologies, scalability, and generalization capabilities. We investigate the\ninterplay between pre-training and reward model training FLOPs to assess their\ninfluence on PRM efficiency and accuracy in complex reasoning tasks. Our\nanalysis reveals a pattern of diminishing returns in performance with\nincreasing PRM scale, highlighting the importance of balancing model size and\ncomputational cost. Furthermore, the diversity of training datasets\nsignificantly impacts PRM performance, emphasizing the importance of diverse\ndata to enhance both accuracy and efficiency. We further examine test-time\nscaling strategies, identifying Monte Carlo Tree Search as the most effective\nmethod when computational resources are abundant, while Best-of-N Sampling\nserves as a practical alternative under resource-limited conditions. Notably,\nour findings indicate that PRMs trained on mathematical datasets exhibit\nperformance comparable to those tailored for code generation, suggesting robust\ncross-domain generalization. Employing a gradient-based metric, we observe that\nPRMs exhibit a preference for selecting responses with similar underlying\npatterns, further informing their optimization."}
{"id": "2506.01287", "pdf": "https://arxiv.org/pdf/2506.01287.pdf", "abs": "https://arxiv.org/abs/2506.01287", "title": "How Problematic are Suspenseful Interactions?", "authors": ["Alarith Uhde"], "categories": ["cs.HC"], "comment": "14 pages, 3 figures", "summary": "Current \"social acceptability\" guidelines for interactive technologies advise\nagainst certain, seemingly problematic forms of interaction. Specifically,\n\"suspenseful\" interactions, characterized by visible manipulations and\ninvisible effects, are generally considered be problematic. However, the\nempirical grounding for this claim is surprisingly weak. To test its validity,\nthis paper presents a controlled replication study (n = 281) of the\n\"suspensefulness effect\". Although it could be statistically replicated with\ntwo out of three social acceptability measures, effect sizes were small (r =<\n.2), and all compared forms of interaction, including the suspenseful one, had\nhigh absolute social acceptability scores. Thus, despite the slight negative\neffect, suspenseful interactions seem less problematic in the overall scheme of\nthings. We discuss alternative approaches to improve the social acceptability\nof interactive technology, and recommend to more closely engage with their\nspecific social situatedness."}
{"id": "2506.00042", "pdf": "https://arxiv.org/pdf/2506.00042.pdf", "abs": "https://arxiv.org/abs/2506.00042", "title": "Enhancing Tool Learning in Large Language Models with Hierarchical Error Checklists", "authors": ["Yue Cui", "Liuyi Yao", "Shuchang Tao", "Weijie Shi", "Yaliang Li", "Bolin Ding", "Xiaofang Zhou"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have significantly advanced natural language\nprocessing, particularly through the integration of external tools and APIs.\nHowever, their effectiveness is frequently hampered by parameter mis-filling\nduring tool calling. In this paper, we propose the Hierarchical Tool Error\nChecklist (HiTEC) framework to systematically diagnose and mitigate\ntool-calling errors without relying on extensive real-world interactions. HiTEC\nintroduces a two-tiered approach: a global error checklist that identifies\ncommon, cross-tool issues, and a local error checklist that targets\ntool-specific and contextual failures. Building on this structure, we propose\ntwo deployments: HiTEC-In Context Learning (HiTEC-ICL) and\nHiTEC-Kahneman-Tversky Optimization (HiTEC-KTO). HiTEC-ICL embeds the global\nchecklist in the initial prompts and leverages a two-round conversational\ninteraction to dynamically refine parameter handling, while HiTEC-KTO generates\nhigh-quality negative examples to drive fine-tuning via preference-based\noptimization. Extensive experiments across five public datasets demonstrate\nthat our framework significantly improves parameter-filling accuracy and\ntool-calling success rates compared to baseline methods."}
{"id": "2506.01395", "pdf": "https://arxiv.org/pdf/2506.01395.pdf", "abs": "https://arxiv.org/abs/2506.01395", "title": "NoRe: Augmenting Journaling Experience with Generative AI for Music Creation", "authors": ["Joonyoung Park", "Hyewon Cho", "Hyehyun Chu", "Yeeun Lee", "Hajin Lim"], "categories": ["cs.HC"], "comment": "Accepted by ACM DIS 2025; 20 pages, 6 figures, 8 tables", "summary": "Journaling has long been recognized for fostering emotional awareness and\nself-reflection, and recent advancements in generative AI offer new\nopportunities to create personalized music that can enhance these practices. In\nthis study, we explore how AI-generated music can augment the journaling\nexperience. Through a formative study, we examined journal writers' writing\npatterns, purposes, emotional regulation strategies, and the design\nrequirements for the system that augments journaling experience by\njournal-based AI-generated music. Based on these insights, we developed NoRe, a\nsystem that transforms journal entries into personalized music using generative\nAI. In a seven-day in-the-wild study (N=15), we investigated user engagement\nand perceived emotional effectiveness through system logs, surveys, and\ninterviews. Our findings suggest that journal-based music generation could\nsupport emotional reflection and provide vivid reminiscence of daily\nexperiences. Drawing from these findings, we discuss design implications for\ntailoring music to journal writers' emotional states and preferences."}
{"id": "2506.00061", "pdf": "https://arxiv.org/pdf/2506.00061.pdf", "abs": "https://arxiv.org/abs/2506.00061", "title": "Unraveling SITT: Social Influence Technique Taxonomy and Detection with LLMs", "authors": ["Wiktoria Mieleszczenko-Kowszewicz", "Beata Bajcar", "Aleksander Szczƒôsny", "Maciej Markiewicz", "Jolanta Babiak", "Berenika Dyczek", "Przemys≈Çaw Kazienko"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this work we present the Social Influence Technique Taxonomy (SITT), a\ncomprehensive framework of 58 empirically grounded techniques organized into\nnine categories, designed to detect subtle forms of social influence in textual\ncontent. We also investigate the LLMs ability to identify various forms of\nsocial influence. Building on interdisciplinary foundations, we construct the\nSITT dataset -- a 746-dialogue corpus annotated by 11 experts in Polish and\ntranslated into English -- to evaluate the ability of LLMs to identify these\ntechniques. Using a hierarchical multi-label classification setup, we benchmark\nfive LLMs, including GPT-4o, Claude 3.5, Llama-3.1, Mixtral, and PLLuM. Our\nresults show that while some models, notably Claude 3.5, achieved moderate\nsuccess (F1 score = 0.45 for categories), overall performance of models remains\nlimited, particularly for context-sensitive techniques. The findings\ndemonstrate key limitations in current LLMs' sensitivity to nuanced linguistic\ncues and underscore the importance of domain-specific fine-tuning. This work\ncontributes a novel resource and evaluation example for understanding how LLMs\ndetect, classify, and potentially replicate strategies of social influence in\nnatural dialogues."}
{"id": "2506.01836", "pdf": "https://arxiv.org/pdf/2506.01836.pdf", "abs": "https://arxiv.org/abs/2506.01836", "title": "Your Interface, Your Control: Adapting Takeover Requests for Seamless Handover in Semi-Autonomous Vehicles", "authors": ["Amr Gomaa", "Simon Engel", "Elena Meiser", "Abdulrahman Mohamed Selim", "Tobias Jungbluth", "Aeneas Leon Sommer", "Sarah Kohlmann", "Michael Barz", "Maurice Rekrut", "Michael Feld", "Daniel Sonntag", "Antonio Kr√ºger"], "categories": ["cs.HC"], "comment": null, "summary": "With the automotive industry transitioning towards conditionally automated\ndriving, takeover warning systems are crucial for ensuring safe collaborative\ndriving between users and semi-automated vehicles. However, previous work has\nfocused on static warning systems that do not accommodate different driver\nstates. Therefore, we propose an adaptive takeover warning system that is\npersonalised to drivers, enhancing their experience and safety. We conducted\ntwo user studies investigating semi-autonomous driving scenarios in rural and\nurban environments while participants performed non-driving-related tasks such\nas text entry and visual search. We investigated the effects of varying time\nbudgets and head-up versus head-down displays for takeover requests on drivers'\nsituational awareness and mental state. Through our statistical and clustering\nanalyses, we propose strategies for designing adaptable takeover systems, e.g.,\nusing longer time budgets and head-up displays for non-hazardous takeover\nevents in high-complexity environments while using shorter time budgets and\nhead-down displays for hazardous events in low-complexity environments."}
{"id": "2506.00064", "pdf": "https://arxiv.org/pdf/2506.00064.pdf", "abs": "https://arxiv.org/abs/2506.00064", "title": "Mis-prompt: Benchmarking Large Language Models for Proactive Error Handling", "authors": ["Jiayi Zeng", "Yizhe Feng", "Mengliang He", "Wenhui Lei", "Wei Zhang", "Zeming Liu", "Xiaoming Shi", "Aimin Zhou"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated significant advancements in\nerror handling. Current error-handling works are performed in a passive manner,\nwith explicit error-handling instructions. However, in real-world scenarios,\nexplicit error-handling instructions are usually unavailable. In this paper,\nour work identifies this challenge as how to conduct proactive error handling\nwithout explicit error handling instructions. To promote further research, this\nwork introduces a new benchmark, termed Mis-prompt, consisting of four\nevaluation tasks, an error category taxonomy, and a new evaluation dataset.\nFurthermore, this work analyzes current LLMs' performance on the benchmark, and\nthe experimental results reveal that current LLMs show poor performance on\nproactive error handling, and SFT on error handling instances improves LLMs'\nproactive error handling capabilities. The dataset will be publicly available."}
{"id": "2506.00052", "pdf": "https://arxiv.org/pdf/2506.00052.pdf", "abs": "https://arxiv.org/abs/2506.00052", "title": "Using LLMs to Advance the Cognitive Science of Collectives", "authors": ["Ilia Sucholutsky", "Katherine M. Collins", "Nori Jacoby", "Bill D. Thompson", "Robert D. Hawkins"], "categories": ["q-bio.NC", "cs.AI", "cs.HC", "cs.MA", "cs.SI"], "comment": null, "summary": "LLMs are already transforming the study of individual cognition, but their\napplication to studying collective cognition has been underexplored. We lay out\nhow LLMs may be able to address the complexity that has hindered the study of\ncollectives and raise possible risks that warrant new methods."}
{"id": "2506.00065", "pdf": "https://arxiv.org/pdf/2506.00065.pdf", "abs": "https://arxiv.org/abs/2506.00065", "title": "You Prefer This One, I Prefer Yours: Using Reference Words is Harder Than Vocabulary Words for Humans and Multimodal Language Models", "authors": ["Dota Tianai Dong", "Yifan Luo", "Po-Ya Angela Wang", "Asli Ozyurek", "Paula Rubio-Fernandez"], "categories": ["cs.CL", "cs.AI"], "comment": "8 pages", "summary": "Multimodal language models (MLMs) increasingly communicate in human-like\nways, yet their ability to use reference words remains largely overlooked\ndespite their ubiquity in everyday communication. Our study addresses this gap\nby comparing human and MLM use of three word classes with increasing cognitive\ndemands: vocabulary words, possessive pronouns (`mine' vs `yours'), and\ndemonstrative pronouns (`this one' vs `that one'). Evaluating seven\nstate-of-the-art MLMs against human participants, we observe a clear difficulty\nhierarchy: while MLMs approach human-level performance on the vocabulary task,\nthey show substantial deficits with possessives and demonstratives. Our\nanalysis reveals these difficulties stem from limitations in perspective-taking\nand spatial reasoning. Although prompt engineering improved model performance\non possessive use, demonstrative use remained well below human-level\ncompetence. These findings provide theoretical and empirical evidence that\nproducing grammatical forms requiring pragmatics and social cognition remains a\nclear challenge in current NLP systems."}
{"id": "2506.00058", "pdf": "https://arxiv.org/pdf/2506.00058.pdf", "abs": "https://arxiv.org/abs/2506.00058", "title": "Prompt Engineer: Analyzing Skill Requirements in the AI Job Market", "authors": ["An Vu", "Jonas Oppenlaender"], "categories": ["cs.CY", "cs.AI", "cs.HC", "I.2.m; H.5.m"], "comment": "42 pages, 8 figures", "summary": "The rise of large language models (LLMs) has created a new job role: the\nPrompt Engineer. Despite growing interest in this position, we still do not\nfully understand what skills this new job role requires or how common these\njobs are. We analyzed 20,662 job postings on LinkedIn, including 72 prompt\nengineer positions, to learn more about this emerging role. We found that\nprompt engineering is still rare (less than 0.5% of sampled job postings) but\nhas a unique skill profile. Prompt engineers need AI knowledge (22.8%), prompt\ndesign skills (18.7%), good communication (21.9%), and creative problem-solving\n(15.8%) skills. These requirements significantly differ from those of\nestablished roles, such as data scientists and machine learning engineers,\nshowing that prompt engineering is becoming its own profession. Our findings\nhelp job seekers, employers, and educational institutions in better\nunderstanding the emerging field of prompt engineering."}
{"id": "2506.00068", "pdf": "https://arxiv.org/pdf/2506.00068.pdf", "abs": "https://arxiv.org/abs/2506.00068", "title": "Probing Politico-Economic Bias in Multilingual Large Language Models: A Cultural Analysis of Low-Resource Pakistani Languages", "authors": ["Afrozah Nadeem", "Mark Dras", "Usman Naseem"], "categories": ["cs.CL", "cs.AI"], "comment": "Preprint", "summary": "Large Language Models (LLMs) are increasingly shaping public discourse, yet\ntheir politico-economic biases remain underexamined in non-Western and\nlow-resource multilingual contexts. This paper presents a systematic analysis\nof political bias in 13 state-of-the-art LLMs across five low-resource\nlanguages spoken in Pakistan: Urdu, Punjabi, Sindhi, Balochi, and Pashto. We\npropose a novel framework that integrates an adapted Political Compass Test\n(PCT) with a multi-level framing analysis. Our method combines quantitative\nassessment of political orientation across economic (left-right) and social\n(libertarian-authoritarian) axes with qualitative analysis of framing through\ncontent, style, and emphasis. We further contextualize this analysis by\naligning prompts with 11 key socio-political themes relevant to Pakistani\nsociety. Our results reveal that LLMs predominantly align with liberal-left\nvalues, echoing Western training data influences, but exhibit notable shifts\ntoward authoritarian framing in regional languages, suggesting strong cultural\nmodulation effects. We also identify consistent model-specific bias signatures\nand language-conditioned variations in ideological expression. These findings\nshow the urgent need for culturally grounded, multilingual bias auditing\nframeworks."}
{"id": "2506.00073", "pdf": "https://arxiv.org/pdf/2506.00073.pdf", "abs": "https://arxiv.org/abs/2506.00073", "title": "The Automated but Risky Game: Modeling Agent-to-Agent Negotiations and Transactions in Consumer Markets", "authors": ["Shenzhe Zhu", "Jiao Sun", "Yi Nian", "Tobin South", "Alex Pentland", "Jiaxin Pei"], "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC", "cs.MA"], "comment": null, "summary": "AI agents are increasingly used in consumer-facing applications to assist\nwith tasks such as product search, negotiation, and transaction execution. In\nthis paper, we explore a future scenario where both consumers and merchants\nauthorize AI agents to fully automate negotiations and transactions. We aim to\nanswer two key questions: (1) Do different LLM agents vary in their ability to\nsecure favorable deals for users? (2) What risks arise from fully automating\ndeal-making with AI agents in consumer markets? To address these questions, we\ndevelop an experimental framework that evaluates the performance of various LLM\nagents in real-world negotiation and transaction settings. Our findings reveal\nthat AI-mediated deal-making is an inherently imbalanced game -- different\nagents achieve significantly different outcomes for their users. Moreover,\nbehavioral anomalies in LLMs can result in financial losses for both consumers\nand merchants, such as overspending or accepting unreasonable deals. These\nresults underscore that while automation can improve efficiency, it also\nintroduces substantial risks. Users should exercise caution when delegating\nbusiness decisions to AI agents."}
{"id": "2506.00069", "pdf": "https://arxiv.org/pdf/2506.00069.pdf", "abs": "https://arxiv.org/abs/2506.00069", "title": "Evaluating the Sensitivity of LLMs to Prior Context", "authors": ["Robert Hankache", "Kingsley Nketia Acheampong", "Liang Song", "Marek Brynda", "Raad Khraishi", "Greig A. Cowan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As large language models (LLMs) are increasingly deployed in multi-turn\ndialogue and other sustained interactive scenarios, it is essential to\nunderstand how extended context affects their performance. Popular benchmarks,\nfocusing primarily on single-turn question answering (QA) tasks, fail to\ncapture the effects of multi-turn exchanges. To address this gap, we introduce\na novel set of benchmarks that systematically vary the volume and nature of\nprior context. We evaluate multiple conventional LLMs, including GPT, Claude,\nand Gemini, across these benchmarks to measure their sensitivity to contextual\nvariations. Our findings reveal that LLM performance on multiple-choice\nquestions can degrade dramatically in multi-turn interactions, with performance\ndrops as large as 73% for certain models. Even highly capable models such as\nGPT-4o exhibit up to a 32% decrease in accuracy. Notably, the relative\nperformance of larger versus smaller models is not always predictable.\nMoreover, the strategic placement of the task description within the context\ncan substantially mitigate performance drops, improving the accuracy by as much\nas a factor of 3.5. These findings underscore the need for robust strategies to\ndesign, evaluate, and mitigate context-related sensitivity in LLMs."}
{"id": "2506.00080", "pdf": "https://arxiv.org/pdf/2506.00080.pdf", "abs": "https://arxiv.org/abs/2506.00080", "title": "Bottom-Up Perspectives on AI Governance: Insights from User Reviews of AI Products", "authors": ["Stefan Pasch"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "With the growing importance of AI governance, numerous high-level frameworks\nand principles have been articulated by policymakers, institutions, and expert\ncommunities to guide the development and application of AI. While such\nframeworks offer valuable normative orientation, they may not fully capture the\npractical concerns of those who interact with AI systems in organizational and\noperational contexts. To address this gap, this study adopts a bottom-up\napproach to explore how governance-relevant themes are expressed in user\ndiscourse. Drawing on over 100,000 user reviews of AI products from G2.com, we\napply BERTopic to extract latent themes and identify those most semantically\nrelated to AI governance. The analysis reveals a diverse set of\ngovernance-relevant topics spanning both technical and non-technical domains.\nThese include concerns across organizational processes-such as planning,\ncoordination, and communication-as well as stages of the AI value chain,\nincluding deployment infrastructure, data handling, and analytics. The findings\nshow considerable overlap with institutional AI governance and ethics\nframeworks on issues like privacy and transparency, but also surface overlooked\nareas such as project management, strategy development, and customer\ninteraction. This highlights the need for more empirically grounded,\nuser-centered approaches to AI governance-approaches that complement normative\nmodels by capturing how governance unfolds in applied settings. By\nforegrounding how governance is enacted in practice, this study contributes to\nmore inclusive and operationally grounded approaches to AI governance and\ndigital policy."}
{"id": "2506.00077", "pdf": "https://arxiv.org/pdf/2506.00077.pdf", "abs": "https://arxiv.org/abs/2506.00077", "title": "Gaussian mixture models as a proxy for interacting language models", "authors": ["Edward Wang", "Tianyu Wang", "Avanti Athreya", "Vince Lyzinski", "Carey E. Priebe"], "categories": ["cs.CL", "cs.LG", "stat.ML", "62R07"], "comment": null, "summary": "Large language models (LLMs) are a powerful tool with the ability to match\nhuman capabilities and behavior in many settings. Retrieval-augmented\ngeneration (RAG) further allows LLMs to generate diverse output depending on\nthe contents of their RAG database. This motivates their use in the social\nsciences to study human behavior between individuals when large-scale\nexperiments are infeasible. However, LLMs depend on complex, computationally\nexpensive algorithms. In this paper, we introduce interacting Gaussian mixture\nmodels (GMMs) as an alternative to similar frameworks using LLMs. We compare a\nsimplified model of GMMs to select experimental simulations of LLMs whose\nupdating and response depend on feedback from other LLMs. We find that\ninteracting GMMs capture important features of the dynamics in interacting\nLLMs, and we investigate key similarities and differences between interacting\nLLMs and GMMs. We conclude by discussing the benefits of Gaussian mixture\nmodels, potential modifications, and future research directions."}
{"id": "2506.00081", "pdf": "https://arxiv.org/pdf/2506.00081.pdf", "abs": "https://arxiv.org/abs/2506.00081", "title": "Artificial Empathy: AI based Mental Health", "authors": ["Aditya Naik", "Jovi Thomas", "Teja Sree", "Himavant Reddy"], "categories": ["q-bio.OT", "cs.AI", "cs.HC"], "comment": null, "summary": "Many people suffer from mental health problems but not everyone seeks\nprofessional help or has access to mental health care. AI chatbots have\nincreasingly become a go-to for individuals who either have mental disorders or\nsimply want someone to talk to. This paper presents a study on participants who\nhave previously used chatbots and a scenario-based testing of large language\nmodel (LLM) chatbots. Our findings indicate that AI chatbots were primarily\nutilized as a \"Five minute therapist\" or as a non-judgmental companion.\nParticipants appreciated the anonymity and lack of judgment from chatbots.\nHowever, there were concerns about privacy and the security of sensitive\ninformation. The scenario-based testing of LLM chatbots highlighted additional\nissues. Some chatbots were consistently reassuring, used emojis and names to\nadd a personal touch, and were quick to suggest seeking professional help.\nHowever, there were limitations such as inconsistent tone, occasional\ninappropriate responses (e.g., casual or romantic), and a lack of crisis\nsensitivity, particularly in recognizing red flag language and escalating\nresponses appropriately. These findings can inform both the technology and\nmental health care industries on how to better utilize AI chatbots to support\nindividuals during challenging emotional periods."}
{"id": "2506.00085", "pdf": "https://arxiv.org/pdf/2506.00085.pdf", "abs": "https://arxiv.org/abs/2506.00085", "title": "COSMIC: Generalized Refusal Direction Identification in LLM Activations", "authors": ["Vincent Siu", "Nicholas Crispino", "Zihao Yu", "Sam Pan", "Zhun Wang", "Yang Liu", "Dawn Song", "Chenguang Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, Accepted to ACL 2025 Findings", "summary": "Large Language Models (LLMs) encode behaviors such as refusal within their\nactivation space, yet identifying these behaviors remains a significant\nchallenge. Existing methods often rely on predefined refusal templates\ndetectable in output tokens or require manual analysis. We introduce\n\\textbf{COSMIC} (Cosine Similarity Metrics for Inversion of Concepts), an\nautomated framework for direction selection that identifies viable steering\ndirections and target layers using cosine similarity - entirely independent of\nmodel outputs. COSMIC achieves steering performance comparable to prior methods\nwithout requiring assumptions about a model's refusal behavior, such as the\npresence of specific refusal tokens. It reliably identifies refusal directions\nin adversarial settings and weakly aligned models, and is capable of steering\nsuch models toward safer behavior with minimal increase in false refusals,\ndemonstrating robustness across a wide range of alignment conditions."}
{"id": "2506.00094", "pdf": "https://arxiv.org/pdf/2506.00094.pdf", "abs": "https://arxiv.org/abs/2506.00094", "title": "Feeling Guilty Being a c(ai)borg: Navigating the Tensions Between Guilt and Empowerment in AI Use", "authors": ["Konstantin Aal", "Tanja Aal", "Vasil Navumau", "David Unbehaun", "Claudia M√ºller", "Volker Wulf", "Sarah R√ºller"], "categories": ["cs.CY", "cs.AI", "cs.HC", "K.4.3"], "comment": "16 pages,", "summary": "This paper explores the emotional, ethical and practical dimensions of\nintegrating Artificial Intelligence (AI) into personal and professional\nworkflows, focusing on the concept of feeling guilty as a 'c(ai)borg' - a human\naugmented by AI. Inspired by Donna Haraway's Cyborg Manifesto, the study\nexplores how AI challenges traditional notions of creativity, originality and\nintellectual labour. Using an autoethnographic approach, the authors reflect on\ntheir year-long experiences with AI tools, revealing a transition from initial\nguilt and reluctance to empowerment through skill-building and transparency.\nKey findings highlight the importance of basic academic skills, advanced AI\nliteracy and honest engagement with AI results. The c(ai)borg vision advocates\nfor a future where AI is openly embraced as a collaborative partner, fostering\ninnovation and equity while addressing issues of access and agency. By\nreframing guilt as growth, the paper calls for a thoughtful and inclusive\napproach to AI integration."}
{"id": "2506.00087", "pdf": "https://arxiv.org/pdf/2506.00087.pdf", "abs": "https://arxiv.org/abs/2506.00087", "title": "SwitchLingua: The First Large-Scale Multilingual and Multi-Ethnic Code-Switching Dataset", "authors": ["Peng Xie", "Xingyuan Liu", "Tsz Wai Chan", "Yequan Bie", "Yangqiu Song", "Yang Wang", "Hao Chen", "Kani Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Code-switching (CS) is the alternating use of two or more languages within a\nconversation or utterance, often influenced by social context and speaker\nidentity. This linguistic phenomenon poses challenges for Automatic Speech\nRecognition (ASR) systems, which are typically designed for a single language\nand struggle to handle multilingual inputs. The growing global demand for\nmultilingual applications, including Code-Switching ASR (CSASR), Text-to-Speech\n(CSTTS), and Cross-Lingual Information Retrieval (CLIR), highlights the\ninadequacy of existing monolingual datasets.\n  Although some code-switching datasets exist, most are limited to bilingual\nmixing within homogeneous ethnic groups, leaving a critical need for a\nlarge-scale, diverse benchmark akin to ImageNet in computer vision.\n  To bridge this gap, we introduce \\textbf{LinguaMaster}, a multi-agent\ncollaboration framework specifically designed for efficient and scalable\nmultilingual data synthesis. Leveraging this framework, we curate\n\\textbf{SwitchLingua}, the first large-scale multilingual and multi-ethnic\ncode-switching dataset, including: (1) 420K CS textual samples across 12\nlanguages, and (2) over 80 hours of audio recordings from 174 speakers\nrepresenting 18 countries/regions and 63 racial/ethnic backgrounds, based on\nthe textual data. This dataset captures rich linguistic and cultural diversity,\noffering a foundational resource for advancing multilingual and multicultural\nresearch. Furthermore, to address the issue that existing ASR evaluation\nmetrics lack sensitivity to code-switching scenarios, we propose the\n\\textbf{Semantic-Aware Error Rate (SAER)}, a novel evaluation metric that\nincorporates semantic information, providing a more accurate and context-aware\nassessment of system performance."}
{"id": "2506.00195", "pdf": "https://arxiv.org/pdf/2506.00195.pdf", "abs": "https://arxiv.org/abs/2506.00195", "title": "Let Them Down Easy! Contextual Effects of LLM Guardrails on User Perceptions and Preferences", "authors": ["Mingqian Zheng", "Wenjia Hu", "Patrick Zhao", "Motahhare Eslami", "Jena D. Hwang", "Faeze Brahman", "Carolyn Rose", "Maarten Sap"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Current LLMs are trained to refuse potentially harmful input queries\nregardless of whether users actually had harmful intents, causing a tradeoff\nbetween safety and user experience. Through a study of 480 participants\nevaluating 3,840 query-response pairs, we examine how different refusal\nstrategies affect user perceptions across varying motivations. Our findings\nreveal that response strategy largely shapes user experience, while actual user\nmotivation has negligible impact. Partial compliance -- providing general\ninformation without actionable details -- emerges as the optimal strategy,\nreducing negative user perceptions by over 50% to flat-out refusals.\nComplementing this, we analyze response patterns of 9 state-of-the-art LLMs and\nevaluate how 6 reward models score different refusal strategies, demonstrating\nthat models rarely deploy partial compliance naturally and reward models\ncurrently undervalue it. This work demonstrates that effective guardrails\nrequire focusing on crafting thoughtful refusals rather than detecting intent,\noffering a path toward AI safety mechanisms that ensure both safety and\nsustained user engagement."}
{"id": "2506.00088", "pdf": "https://arxiv.org/pdf/2506.00088.pdf", "abs": "https://arxiv.org/abs/2506.00088", "title": "HD-NDEs: Neural Differential Equations for Hallucination Detection in LLMs", "authors": ["Qing Li", "Jiahui Geng", "Zongxiong Chen", "Derui Zhu", "Yuxia Wang", "Congbo Ma", "Chenyang Lyu", "Fakhri Karray"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "In recent years, large language models (LLMs) have made remarkable\nadvancements, yet hallucination, where models produce inaccurate or non-factual\nstatements, remains a significant challenge for real-world deployment. Although\ncurrent classification-based methods, such as SAPLMA, are highly efficient in\nmitigating hallucinations, they struggle when non-factual information arises in\nthe early or mid-sequence of outputs, reducing their reliability. To address\nthese issues, we propose Hallucination Detection-Neural Differential Equations\n(HD-NDEs), a novel method that systematically assesses the truthfulness of\nstatements by capturing the full dynamics of LLMs within their latent space.\nOur approaches apply neural differential equations (Neural DEs) to model the\ndynamic system in the latent space of LLMs. Then, the sequence in the latent\nspace is mapped to the classification space for truth assessment. The extensive\nexperiments across five datasets and six widely used LLMs demonstrate the\neffectiveness of HD-NDEs, especially, achieving over 14% improvement in AUC-ROC\non the True-False dataset compared to state-of-the-art techniques."}
{"id": "2506.00220", "pdf": "https://arxiv.org/pdf/2506.00220.pdf", "abs": "https://arxiv.org/abs/2506.00220", "title": "Curate, Connect, Inquire: A System for Findable Accessible Interoperable and Reusable (FAIR) Human-Robot Centered Datasets", "authors": ["Xingru Zhou", "Sadanand Modak", "Yao-Cheng Chan", "Zhiyun Deng", "Luis Sentis", "Maria Esteva"], "categories": ["cs.IR", "cs.HC", "cs.RO"], "comment": "7 pages (excluding references), 8 pages (including references); 5\n  figures; accepted to the ICRA 2025 Workshop on Human-Centered Robot Learning\n  in the Era of Big Data and Large Models", "summary": "The rapid growth of AI in robotics has amplified the need for high-quality,\nreusable datasets, particularly in human-robot interaction (HRI) and\nAI-embedded robotics. While more robotics datasets are being created, the\nlandscape of open data in the field is uneven. This is due to a lack of\ncuration standards and consistent publication practices, which makes it\ndifficult to discover, access, and reuse robotics data. To address these\nchallenges, this paper presents a curation and access system with two main\ncontributions: (1) a structured methodology to curate, publish, and integrate\nFAIR (Findable, Accessible, Interoperable, Reusable) human-centered robotics\ndatasets; and (2) a ChatGPT-powered conversational interface trained with the\ncurated datasets metadata and documentation to enable exploration, comparison\nrobotics datasets and data retrieval using natural language. Developed based on\npractical experience curating datasets from robotics labs within Texas Robotics\nat the University of Texas at Austin, the system demonstrates the value of\nstandardized curation and persistent publication of robotics data. The system's\nevaluation suggests that access and understandability of human-robotics data\nare significantly improved. This work directly aligns with the goals of the\nHCRL @ ICRA 2025 workshop and represents a step towards more human-centered\naccess to data for embodied AI."}
{"id": "2506.00103", "pdf": "https://arxiv.org/pdf/2506.00103.pdf", "abs": "https://arxiv.org/abs/2506.00103", "title": "Writing-Zero: Bridge the Gap Between Non-verifiable Problems and Verifiable Rewards", "authors": ["Xun Lu"], "categories": ["cs.CL"], "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) has enabled large\nlanguage models (LLMs) to achieve remarkable breakthroughs in reasoning tasks\nwith objective ground-truth answers, such as mathematics and code generation.\nHowever, a significant gap remains for non-verifiable tasks, like creative\nwriting and open-ended dialogue, where quality assessment is inherently\nsubjective and lacks definitive references. Existing approaches for these\ndomains often rely on scalar reward models trained with human preferences,\nwhich suffer from limited generalization and are prone to reward hacking, such\nas over-explanation and length bias. In this work, we propose a unified\nRLVR-based training paradigm that bridges the gap between non-verifiable tasks\nand verifiable rewards. We introduce a writing-principle-based pairwise\nGenerative Reward Model (GenRM) and a novel Bootstrapped Relative Policy\nOptimization (BRPO) algorithm. The pairwise writing GenRM leverages\nself-principled critique to transform subjective assessments into reliable,\nverifiable rewards, while BRPO enables dynamic, reference-free pairwise\ncomparison by leveraging a bootstrapped response as temporary reference from\nwithin group rollouts during RL training. Our approach empowers LLMs to develop\nrobust writing capabilities without supervised fine-tuning, as demonstrated by\nWriting-Zero, which shows consistent improvement and strong resistance to\nreward hacking compared to scalar reward baselines. Furthermore, our method\nachieves competitive results on both in-house and open-source writing\nbenchmarks. Our findings suggest the potential to unify rule-based,\nreference-based, and reference-free reward modeling under the RLVR framework,\nthus paving the way for a comprehensive and scalable RL training paradigm\napplicable across all language tasks."}
{"id": "2506.00308", "pdf": "https://arxiv.org/pdf/2506.00308.pdf", "abs": "https://arxiv.org/abs/2506.00308", "title": "MythTriage: Scalable Detection of Opioid Use Disorder Myths on a Video-Sharing Platform", "authors": ["Hayoung Jung", "Shravika Mittal", "Ananya Aatreya", "Navreet Kaur", "Munmun De Choudhury", "Tanushree Mitra"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "comment": "34 pages, 14 figures, 21 tables. In submission", "summary": "Understanding the prevalence of misinformation in health topics online can\ninform public health policies and interventions. However, measuring such\nmisinformation at scale remains a challenge, particularly for high-stakes but\nunderstudied topics like opioid-use disorder (OUD)--a leading cause of death in\nthe U.S. We present the first large-scale study of OUD-related myths on\nYouTube, a widely-used platform for health information. With clinical experts,\nwe validate 8 pervasive myths and release an expert-labeled video dataset. To\nscale labeling, we introduce MythTriage, an efficient triage pipeline that uses\na lightweight model for routine cases and defers harder ones to a\nhigh-performing, but costlier, large language model (LLM). MythTriage achieves\nup to 0.86 macro F1-score while estimated to reduce annotation time and\nfinancial cost by over 76% compared to experts and full LLM labeling. We\nanalyze 2.9K search results and 343K recommendations, uncovering how myths\npersist on YouTube and offering actionable insights for public health and\nplatform moderation."}
{"id": "2506.00134", "pdf": "https://arxiv.org/pdf/2506.00134.pdf", "abs": "https://arxiv.org/abs/2506.00134", "title": "Spurious Correlations and Beyond: Understanding and Mitigating Shortcut Learning in SDOH Extraction with Large Language Models", "authors": ["Fardin Ahsan Sakib", "Ziwei Zhu", "Karen Trister Grace", "Meliha Yetisgen", "Ozlem Uzuner"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Social determinants of health (SDOH) extraction from clinical text is\ncritical for downstream healthcare analytics. Although large language models\n(LLMs) have shown promise, they may rely on superficial cues leading to\nspurious predictions. Using the MIMIC portion of the SHAC (Social History\nAnnotation Corpus) dataset and focusing on drug status extraction as a case\nstudy, we demonstrate that mentions of alcohol or smoking can falsely induce\nmodels to predict current/past drug use where none is present, while also\nuncovering concerning gender disparities in model performance. We further\nevaluate mitigation strategies - such as prompt engineering and\nchain-of-thought reasoning - to reduce these false positives, providing\ninsights into enhancing LLM reliability in health domains."}
{"id": "2506.00386", "pdf": "https://arxiv.org/pdf/2506.00386.pdf", "abs": "https://arxiv.org/abs/2506.00386", "title": "Adaptive-VP: A Framework for LLM-Based Virtual Patients that Adapts to Trainees' Dialogue to Facilitate Nurse Communication Training", "authors": ["Keyeun Lee", "Seolhee Lee", "Esther Hehsun Kim", "Yena Ko", "Jinsu Eun", "Dahee Kim", "Hyewon Cho", "Haiyi Zhu", "Robert E. Kraut", "Eunyoung Suh", "Eun-mee Kim", "Hajin Lim"], "categories": ["cs.CL", "cs.HC"], "comment": "ACL 2025 Findings, 34 pages, 9 figures", "summary": "Effective communication training is essential to preparing nurses for\nhigh-quality patient care. While standardized patient (SP) simulations provide\nvaluable experiential learning, they are often costly and inflexible. Virtual\npatient (VP) systems offer a scalable alternative, but most fail to adapt to\nthe varying communication skills of trainees. In particular, when trainees\nrespond ineffectively, VPs should escalate in hostility or become\nuncooperative--yet this level of adaptive interaction remains largely\nunsupported. To address this gap, we introduce Adaptive-VP, a VP dialogue\ngeneration framework that leverages large language models (LLMs) to dynamically\nadapt VP behavior based on trainee input. The framework features a pipeline for\nconstructing clinically grounded yet flexible VP scenarios and a modular system\nfor assessing trainee communication and adjusting VP responses in real time,\nwhile ensuring learner safety. We validated Adaptive-VP by simulating\nchallenging patient conversations. Automated evaluation using a corpus from\npracticing nurses showed that our communication skill evaluation mechanism\nreflected real-world proficiency levels. Expert nurses further confirmed that\nAdaptive-VP produced more natural and realistic interactions than existing\napproaches, demonstrating its potential as a scalable and effective tool for\nnursing communication training."}
{"id": "2506.00137", "pdf": "https://arxiv.org/pdf/2506.00137.pdf", "abs": "https://arxiv.org/abs/2506.00137", "title": "LaMP-QA: A Benchmark for Personalized Long-form Question Answering", "authors": ["Alireza Salemi", "Hamed Zamani"], "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": null, "summary": "Personalization is essential for question answering systems that are\nuser-centric. Despite its importance, personalization in answer generation has\nbeen relatively underexplored. This is mainly due to lack of resources for\ntraining and evaluating personalized question answering systems. We address\nthis gap by introducing LaMP-QA -- a benchmark designed for evaluating\npersonalized long-form answer generation. The benchmark covers questions from\nthree major categories: (1) Arts & Entertainment, (2) Lifestyle & Personal\nDevelopment, and (3) Society & Culture, encompassing over 45 subcategories in\ntotal. To assess the quality and potential impact of the LaMP-QA benchmark for\npersonalized question answering, we conduct comprehensive human and automatic\nevaluations, to compare multiple evaluation strategies for evaluating generated\npersonalized responses and measure their alignment with human preferences.\nFurthermore, we benchmark a number of non-personalized and personalized\napproaches based on open-source and proprietary large language models (LLMs).\nOur results show that incorporating the personalized context provided leads to\nperformance improvements of up to 39%. The benchmark is publicly released to\nsupport future research in this area."}
{"id": "2506.00454", "pdf": "https://arxiv.org/pdf/2506.00454.pdf", "abs": "https://arxiv.org/abs/2506.00454", "title": "Towards Temporally Explainable Dysarthric Speech Clarity Assessment", "authors": ["Seohyun Park", "Chitralekha Gupta", "Michelle Kah Yian Kwan", "Xinhui Fung", "Alexander Wenjun Yip", "Suranga Nanayakkara"], "categories": ["eess.AS", "cs.HC", "cs.SD"], "comment": "Accepted in Interspeech 2025. First two authors were equal\n  contributors", "summary": "Dysarthria, a motor speech disorder, affects intelligibility and requires\ntargeted interventions for effective communication. In this work, we\ninvestigate automated mispronunciation feedback by collecting a dysarthric\nspeech dataset from six speakers reading two passages, annotated by a speech\ntherapist with temporal markers and mispronunciation descriptions. We design a\nthree-stage framework for explainable mispronunciation evaluation: (1) overall\nclarity scoring, (2) mispronunciation localization, and (3) mispronunciation\ntype classification. We systematically analyze pretrained Automatic Speech\nRecognition (ASR) models in each stage, assessing their effectiveness in\ndysarthric speech evaluation (Code available at:\nhttps://github.com/augmented-human-lab/interspeech25_speechtherapy,\nSupplementary webpage: https://apps.ahlab.org/interspeech25_speechtherapy/).\nOur findings offer clinically relevant insights for automating actionable\nfeedback for pronunciation assessment, which could enable independent practice\nfor patients and help therapists deliver more effective interventions."}
{"id": "2506.00145", "pdf": "https://arxiv.org/pdf/2506.00145.pdf", "abs": "https://arxiv.org/abs/2506.00145", "title": "Vedavani: A Benchmark Corpus for ASR on Vedic Sanskrit Poetry", "authors": ["Sujeet Kumar", "Pretam Ray", "Abhinay Beerukuri", "Shrey Kamoji", "Manoj Balaji Jagadeeshan", "Pawan Goyal"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Sanskrit, an ancient language with a rich linguistic heritage, presents\nunique challenges for automatic speech recognition (ASR) due to its phonemic\ncomplexity and the phonetic transformations that occur at word junctures,\nsimilar to the connected speech found in natural conversations. Due to these\ncomplexities, there has been limited exploration of ASR in Sanskrit,\nparticularly in the context of its poetic verses, which are characterized by\nintricate prosodic and rhythmic patterns. This gap in research raises the\nquestion: How can we develop an effective ASR system for Sanskrit, particularly\none that captures the nuanced features of its poetic form? In this study, we\nintroduce Vedavani, the first comprehensive ASR study focused on Sanskrit Vedic\npoetry. We present a 54-hour Sanskrit ASR dataset, consisting of 30,779\nlabelled audio samples from the Rig Veda and Atharva Veda. This dataset\ncaptures the precise prosodic and rhythmic features that define the language.\nWe also benchmark the dataset on various state-of-the-art multilingual speech\nmodels.$^{1}$ Experimentation revealed that IndicWhisper performed the best\namong the SOTA models."}
{"id": "2506.00583", "pdf": "https://arxiv.org/pdf/2506.00583.pdf", "abs": "https://arxiv.org/abs/2506.00583", "title": "The Hidden Language of Harm: Examining the Role of Emojis in Harmful Online Communication and Content Moderation", "authors": ["Yuhang Zhou", "Yimin Xiao", "Wei Ai", "Ge Gao"], "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": "18 pages, 3 figures", "summary": "Social media platforms have become central to modern communication, yet they\nalso harbor offensive content that challenges platform safety and inclusivity.\nWhile prior research has primarily focused on textual indicators of offense,\nthe role of emojis, ubiquitous visual elements in online discourse, remains\nunderexplored. Emojis, despite being rarely offensive in isolation, can acquire\nharmful meanings through symbolic associations, sarcasm, and contextual misuse.\nIn this work, we systematically examine emoji contributions to offensive\nTwitter messages, analyzing their distribution across offense categories and\nhow users exploit emoji ambiguity. To address this, we propose an LLM-powered,\nmulti-step moderation pipeline that selectively replaces harmful emojis while\npreserving the tweet's semantic intent. Human evaluations confirm our approach\neffectively reduces perceived offensiveness without sacrificing meaning. Our\nanalysis also reveals heterogeneous effects across offense types, offering\nnuanced insights for online communication and emoji moderation."}
{"id": "2506.00160", "pdf": "https://arxiv.org/pdf/2506.00160.pdf", "abs": "https://arxiv.org/abs/2506.00160", "title": "Werewolf: A Straightforward Game Framework with TTS for Improved User Engagement", "authors": ["Qihui Fan", "Enfu Nan", "Wenbo Li", "Lei Lu", "Pu Zhao", "Yanzhi Wang"], "categories": ["cs.CL"], "comment": null, "summary": "The growing popularity of social deduction game systems for both business\napplications and AI research has greatly benefited from the rapid advancements\nin Large Language Models (LLMs), which now demonstrate stronger reasoning and\npersuasion capabilities. Especially with the raise of DeepSeek R1 and V3\nmodels, LLMs should enable a more engaging experience for human players in\nLLM-agent-based social deduction games like Werewolf. Previous works either\nfine-tuning, advanced prompting engineering, or additional experience pool to\nachieve engaging text-format Werewolf game experience. We propose a novel yet\nstraightforward LLM-based Werewolf game system with tuned Text-to-Speech(TTS)\nmodels designed for enhanced compatibility with various LLM models, and\nimproved user engagement. We argue with ever enhancing LLM reasoning, extra\ncomponents will be unnecessary in the case of Werewolf."}
{"id": "2506.00924", "pdf": "https://arxiv.org/pdf/2506.00924.pdf", "abs": "https://arxiv.org/abs/2506.00924", "title": "Bridging Subjective and Objective QoE: Operator-Level Aggregation Using LLM-Based Comment Analysis and Network MOS Comparison", "authors": ["Parsa Hassani Shariat Panahi", "Amir Hossein Jalilvand", "M. Hasan Najafi"], "categories": ["cs.NI", "cs.AI", "cs.HC"], "comment": "19 ppages, 13 figures", "summary": "This paper introduces a dual-layer framework for network operator-side\nquality of experience (QoE) assessment that integrates both objective network\nmodeling and subjective user perception extracted from live-streaming\nplatforms. On the objective side, we develop a machine learning model trained\non mean opinion scores (MOS) computed via the ITU-T P.1203 reference\nimplementation, allowing accurate prediction of user-perceived video quality\nusing only network parameters such as packet loss, delay, jitter, and\nthroughput without reliance on video content or client-side instrumentation. On\nthe subjective side, we present a semantic filtering and scoring pipeline that\nprocesses user comments from live streams to extract performance-related\nfeedback. A large language model is used to assign scalar MOS scores to\nfiltered comments in a deterministic and reproducible manner. To support\nscalable and interpretable analysis, we con- struct a labeled dataset of 47,894\nlive-stream comments, of which about 34,000 are identified as QoE-relevant\nthrough multi-layer semantic filtering. Each comment is enriched with simulated\nInternet Service Provider attribution and temporally aligned using synthetic\ntimestamps in 5-min intervals. The resulting dataset enables operator-level\naggregation and time-series analysis of user-perceived quality. A delta MOS\nmetric is proposed to measure each Internet service provider's deviation from\nplatform-wide sentiment, allowing detection of localized degradations even in\nthe absence of direct network telemetry. A controlled outage simulation\nconfirms the framework's effectiveness in identifying service disruptions\nthrough comment-based trends alone. The system provides each operator with its\nown subjective MOS and the global platform average per interval, enabling\nreal-time interpretation of performance deviations and comparison with\nobjective network-based QoE estimates."}
{"id": "2506.00195", "pdf": "https://arxiv.org/pdf/2506.00195.pdf", "abs": "https://arxiv.org/abs/2506.00195", "title": "Let Them Down Easy! Contextual Effects of LLM Guardrails on User Perceptions and Preferences", "authors": ["Mingqian Zheng", "Wenjia Hu", "Patrick Zhao", "Motahhare Eslami", "Jena D. Hwang", "Faeze Brahman", "Carolyn Rose", "Maarten Sap"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Current LLMs are trained to refuse potentially harmful input queries\nregardless of whether users actually had harmful intents, causing a tradeoff\nbetween safety and user experience. Through a study of 480 participants\nevaluating 3,840 query-response pairs, we examine how different refusal\nstrategies affect user perceptions across varying motivations. Our findings\nreveal that response strategy largely shapes user experience, while actual user\nmotivation has negligible impact. Partial compliance -- providing general\ninformation without actionable details -- emerges as the optimal strategy,\nreducing negative user perceptions by over 50% to flat-out refusals.\nComplementing this, we analyze response patterns of 9 state-of-the-art LLMs and\nevaluate how 6 reward models score different refusal strategies, demonstrating\nthat models rarely deploy partial compliance naturally and reward models\ncurrently undervalue it. This work demonstrates that effective guardrails\nrequire focusing on crafting thoughtful refusals rather than detecting intent,\noffering a path toward AI safety mechanisms that ensure both safety and\nsustained user engagement."}
{"id": "2506.01077", "pdf": "https://arxiv.org/pdf/2506.01077.pdf", "abs": "https://arxiv.org/abs/2506.01077", "title": "TRiMM: Transformer-Based Rich Motion Matching for Real-Time multi-modal Interaction in Digital Humans", "authors": ["Yueqian Guo", "Tianzhao Li", "Xin Lyu", "Jiehaolin Chen", "Zhaohan Wang", "Sirui Xiao", "Yurun Chen", "Yezi He", "Helin Li", "Fan Zhang"], "categories": ["cs.GR", "cs.HC", "68U05(Primary), 62M45(Secondary)"], "comment": "24 pages,12 figures", "summary": "Large Language Model (LLM)-driven digital humans have sparked a series of\nrecent studies on co-speech gesture generation systems. However, existing\napproaches struggle with real-time synthesis and long-text comprehension. This\npaper introduces Transformer-Based Rich Motion Matching (TRiMM), a novel\nmulti-modal framework for real-time 3D gesture generation. Our method\nincorporates three modules: 1) a cross-modal attention mechanism to achieve\nprecise temporal alignment between speech and gestures; 2) a long-context\nautoregressive model with a sliding window mechanism for effective sequence\nmodeling; 3) a large-scale gesture matching system that constructs an atomic\naction library and enables real-time retrieval. Additionally, we develop a\nlightweight pipeline implemented in the Unreal Engine for experimentation. Our\napproach achieves real-time inference at 120 fps and maintains a per-sentence\nlatency of 0.15 seconds on consumer-grade GPUs (Geforce RTX3060). Extensive\nsubjective and objective evaluations on the ZEGGS, and BEAT datasets\ndemonstrate that our model outperforms current state-of-the-art methods. TRiMM\nenhances the speed of co-speech gesture generation while ensuring gesture\nquality, enabling LLM-driven digital humans to respond to speech in real time\nand synthesize corresponding gestures. Our code is available at\nhttps://github.com/teroon/TRiMM-Transformer-Based-Rich-Motion-Matching"}
{"id": "2506.00200", "pdf": "https://arxiv.org/pdf/2506.00200.pdf", "abs": "https://arxiv.org/abs/2506.00200", "title": "Structuring Radiology Reports: Challenging LLMs with Lightweight Models", "authors": ["Johannes Moll", "Louisa Fay", "Asfandyar Azhar", "Sophie Ostmeier", "Tim Lueth", "Sergios Gatidis", "Curtis Langlotz", "Jean-Benoit Delbrouck"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Radiology reports are critical for clinical decision-making but often lack a\nstandardized format, limiting both human interpretability and machine learning\n(ML) applications. While large language models (LLMs) have shown strong\ncapabilities in reformatting clinical text, their high computational\nrequirements, lack of transparency, and data privacy concerns hinder practical\ndeployment. To address these challenges, we explore lightweight encoder-decoder\nmodels (<300M parameters)-specifically T5 and BERT2BERT-for structuring\nradiology reports from the MIMIC-CXR and CheXpert Plus datasets. We benchmark\nthese models against eight open-source LLMs (1B-70B), adapted using prefix\nprompting, in-context learning (ICL), and low-rank adaptation (LoRA)\nfinetuning. Our best-performing lightweight model outperforms all LLMs adapted\nusing prompt-based techniques on a human-annotated test set. While some\nLoRA-finetuned LLMs achieve modest gains over the lightweight model on the\nFindings section (BLEU 6.4%, ROUGE-L 4.8%, BERTScore 3.6%, F1-RadGraph 1.1%,\nGREEN 3.6%, and F1-SRR-BERT 4.3%), these improvements come at the cost of\nsubstantially greater computational resources. For example, LLaMA-3-70B\nincurred more than 400 times the inference time, cost, and carbon emissions\ncompared to the lightweight model. These results underscore the potential of\nlightweight, task-specific models as sustainable and privacy-preserving\nsolutions for structuring clinical text in resource-constrained healthcare\nsettings."}
{"id": "2506.01135", "pdf": "https://arxiv.org/pdf/2506.01135.pdf", "abs": "https://arxiv.org/abs/2506.01135", "title": "$\\text{TREX}^2$: Dual-Reconstruction Framework for Teleoperated-Robot with EXtended Reality", "authors": ["Ziliang Zhang", "Cong Liu", "Hyoseung Kim"], "categories": ["cs.RO", "cs.HC", "cs.NI"], "comment": null, "summary": "Robot teleoperation with extended reality (XR teleoperation) enables\nintuitive interaction by allowing remote robots to mimic user motions with\nreal-time 3D feedback. However, existing systems face significant\nmotion-to-motion (M2M) latency--the delay between the user's latest motion and\nthe corresponding robot feedback--leading to high teleoperation error and\nmission completion time. This issue stems from the system's exclusive reliance\non network communication, making it highly vulnerable to network degradation.\nTo address these challenges, we introduce $\\text{TREX}^2$, the first\nend-to-end, fully open-sourced XR teleoperation framework that decouples robot\ncontrol and XR visualization from network dependencies. $\\text{TREX}^2$\nleverages local sensing data to reconstruct delayed or missing information of\nthe counterpart, thereby significantly reducing network-induced issues. This\napproach allows both the XR and robot to run concurrently with network\ntransmission while maintaining high robot planning accuracy. $\\text{TREX}^2$\nalso features contention-aware scheduling to mitigate GPU contention and\nbandwidth-adaptive point cloud scaling to cope with limited bandwidth. We\nimplement $\\text{TREX}^2$ across three hardware settings, including simulated\nand physical robots, and evaluate it on 9,500 real-world teleoperation trials\nfrom the RoboSet dataset \\cite{kumar2024robohive}, covering single- and\nmulti-step missions. Compared to state-of-the-art XR teleoperation frameworks,\n$\\text{TREX}^2$ reduces teleoperation error by up to 69.8\\% on WLAN and 73.1\\%\non cellular networks with only 6.7\\% maximum runtime overhead. It also improves\ncompletion time by up to 47.7\\%, enabling smoother teleoperation. A real-world\ncase study on ten stationary and mobile missions further shows $\\text{TREX}^2$\nachieves up to 37.7\\% faster completion while lowering average teleoperation\nerror by up to 57.2\\%."}
{"id": "2506.00204", "pdf": "https://arxiv.org/pdf/2506.00204.pdf", "abs": "https://arxiv.org/abs/2506.00204", "title": "Structure-Aware Fill-in-the-Middle Pretraining for Code", "authors": ["Linyuan Gong", "Alvin Cheung", "Mostafa Elhoushi", "Sida Wang"], "categories": ["cs.CL", "cs.AI", "cs.SE"], "comment": "14 pages", "summary": "Fill-in-the-Middle (FIM) is a common pretraining method for code LLMs, where\nmodels complete code segments given surrounding context. However, existing LLMs\ntreat code as plain text and mask random character spans. We propose and\nevaluate AST-FIM, a pretraining strategy that leverages Abstract Syntax Trees\n(ASTs) to mask complete syntactic structures at scale, ensuring coherent\ntraining examples better aligned with universal code structures and common code\nediting patterns such as blocks, expressions, or functions. To evaluate\nreal-world fill-in-the-middle (FIM) programming tasks, we introduce\nReal-FIM-Eval, a benchmark derived from 30,000+ GitHub commits across 12\nlanguages. On infilling tasks, experiments on 1B and 8B parameter models show\nthat AST-FIM is particularly beneficial for real-world code editing as it\noutperforms standard random-character FIM by up to 5 pts on standard FIM\nbenchmarks. Our code is publicly available at\nhttps://github.com/gonglinyuan/ast_fim."}
{"id": "2506.01391", "pdf": "https://arxiv.org/pdf/2506.01391.pdf", "abs": "https://arxiv.org/abs/2506.01391", "title": "AgentCPM-GUI: Building Mobile-Use Agents with Reinforcement Fine-Tuning", "authors": ["Zhong Zhang", "Yaxi Lu", "Yikun Fu", "Yupeng Huo", "Shenzhi Yang", "Yesai Wu", "Han Si", "Xin Cong", "Haotian Chen", "Yankai Lin", "Jie Xie", "Wei Zhou", "Wang Xu", "Yuanheng Zhang", "Zhou Su", "Zhongwu Zhai", "Xiaoming Liu", "Yudong Mei", "Jianming Xu", "Hongyan Tian", "Chongyi Wang", "Chi Chen", "Yuan Yao", "Zhiyuan Liu", "Maosong Sun"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC", "I.2.8; I.2.7; I.2.10; H.5.2"], "comment": "The project is available at https://github.com/OpenBMB/AgentCPM-GUI", "summary": "The recent progress of large language model agents has opened new\npossibilities for automating tasks through graphical user interfaces (GUIs),\nespecially in mobile environments where intelligent interaction can greatly\nenhance usability. However, practical deployment of such agents remains\nconstrained by several key challenges. Existing training data is often noisy\nand lack semantic diversity, which hinders the learning of precise grounding\nand planning. Models trained purely by imitation tend to overfit to seen\ninterface patterns and fail to generalize in unfamiliar scenarios. Moreover,\nmost prior work focuses on English interfaces while overlooks the growing\ndiversity of non-English applications such as those in the Chinese mobile\necosystem. In this work, we present AgentCPM-GUI, an 8B-parameter GUI agent\nbuilt for robust and efficient on-device GUI interaction. Our training pipeline\nincludes grounding-aware pre-training to enhance perception, supervised\nfine-tuning on high-quality Chinese and English trajectories to imitate\nhuman-like actions, and reinforcement fine-tuning with GRPO to improve\nreasoning capability. We also introduce a compact action space that reduces\noutput length and supports low-latency execution on mobile devices.\nAgentCPM-GUI achieves state-of-the-art performance on five public benchmarks\nand a new Chinese GUI benchmark called CAGUI, reaching $96.9\\%$ Type-Match and\n$91.3\\%$ Exact-Match. To facilitate reproducibility and further research, we\npublicly release all code, model checkpoint, and evaluation data."}
{"id": "2506.00210", "pdf": "https://arxiv.org/pdf/2506.00210.pdf", "abs": "https://arxiv.org/abs/2506.00210", "title": "REIC: RAG-Enhanced Intent Classification at Scale", "authors": ["Ziji Zhang", "Michael Yang", "Zhiyu Chen", "Yingying Zhuang", "Shu-Ting Pi", "Qun Liu", "Rajashekar Maragoud", "Vy Nguyen", "Anurag Beniwal"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Accurate intent classification is critical for efficient routing in customer\nservice, ensuring customers are connected with the most suitable agents while\nreducing handling times and operational costs. However, as companies expand\ntheir product lines, intent classification faces scalability challenges due to\nthe increasing number of intents and variations in taxonomy across different\nverticals. In this paper, we introduce REIC, a Retrieval-augmented generation\nEnhanced Intent Classification approach, which addresses these challenges\neffectively. REIC leverages retrieval-augmented generation (RAG) to dynamically\nincorporate relevant knowledge, enabling precise classification without the\nneed for frequent retraining. Through extensive experiments on real-world\ndatasets, we demonstrate that REIC outperforms traditional fine-tuning,\nzero-shot, and few-shot methods in large-scale customer service settings. Our\nresults highlight its effectiveness in both in-domain and out-of-domain\nscenarios, demonstrating its potential for real-world deployment in adaptive\nand large-scale intent classification systems."}
{"id": "2506.01492", "pdf": "https://arxiv.org/pdf/2506.01492.pdf", "abs": "https://arxiv.org/abs/2506.01492", "title": "Challenges in designing research infrastructure software in multi-stakeholder contexts", "authors": ["Stephan Druskat", "Sabine Theis"], "categories": ["cs.SE", "cs.HC"], "comment": "19 pages, 6 figures, accepted to be presented at 27th International\n  Conference on Human-Computer Interaction, 22-27 June 2025, Gothenburg,\n  Sweden. This preprint has not undergone peer review or any post-submission\n  improvements or corrections. The Version of Record of this contribution is\n  published in \"HCII 2025\", and is available online at\n  https://doi.org/10.1007/978-3-031-93838-2_21", "summary": "This study investigates the challenges in designing research infrastructure\nsoftware for automated software publication in multi-stakeholder environments,\nfocusing specifically on the HERMES system. Through two quantitative surveys of\nresearch software engineers (RSEs) and infrastructure facility staff (IFs), it\nexamines technical, organizational, and social requirements across these\nstakeholder groups. The study reveals significant differences in how RSEs and\nIFs prioritize various system features. While RSEs highly value compatibility\nwith existing infrastructure, IFs prioritize user-focused aspects like system\nusability and documentation. The research identifies two main challenges in\ndesigning research infrastructure software: (1) the existence of multiple\nstakeholder groups with differing requirements, and (2) the internal\nheterogeneity within each stakeholder group across dimensions such as technical\nexperience. The study also highlights that only half of RSE respondents\nactively practice software publication, pointing to potential cultural or\ntechnical barriers. Additionally, the research reveals discrepancies in how\nstakeholders view organizational aspects, with IFs consistently rating factors\nlike responsibility structures and quality assurance as more important than\nRSEs do. These findings contribute to a better understanding of the\ncomplexities involved in designing research infrastructure software and\nemphasize the need for systems that can accommodate diverse user groups while\nmaintaining usability across different technical expertise levels."}
{"id": "2506.00232", "pdf": "https://arxiv.org/pdf/2506.00232.pdf", "abs": "https://arxiv.org/abs/2506.00232", "title": "ComposeRAG: A Modular and Composable RAG for Corpus-Grounded Multi-Hop Question Answering", "authors": ["Ruofan Wu", "Youngwon Lee", "Fan Shu", "Danmei Xu", "Seung-won Hwang", "Zhewei Yao", "Yuxiong He", "Feng Yan"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems are increasingly diverse, yet\nmany suffer from monolithic designs that tightly couple core functions like\nquery reformulation, retrieval, reasoning, and verification. This limits their\ninterpretability, systematic evaluation, and targeted improvement, especially\nfor complex multi-hop question answering. We introduce ComposeRAG, a novel\nmodular abstraction that decomposes RAG pipelines into atomic, composable\nmodules. Each module, such as Question Decomposition, Query Rewriting,\nRetrieval Decision, and Answer Verification, acts as a parameterized\ntransformation on structured inputs/outputs, allowing independent\nimplementation, upgrade, and analysis. To enhance robustness against errors in\nmulti-step reasoning, ComposeRAG incorporates a self-reflection mechanism that\niteratively revisits and refines earlier steps upon verification failure.\nEvaluated on four challenging multi-hop QA benchmarks, ComposeRAG consistently\noutperforms strong baselines in both accuracy and grounding fidelity.\nSpecifically, it achieves up to a 15% accuracy improvement over\nfine-tuning-based methods and up to a 5% gain over reasoning-specialized\npipelines under identical retrieval conditions. Crucially, ComposeRAG\nsignificantly enhances grounding: its verification-first design reduces\nungrounded answers by over 10% in low-quality retrieval settings, and by\napproximately 3% even with strong corpora. Comprehensive ablation studies\nvalidate the modular architecture, demonstrating distinct and additive\ncontributions from each component. These findings underscore ComposeRAG's\ncapacity to deliver flexible, transparent, scalable, and high-performing\nmulti-hop reasoning with improved grounding and interpretability."}
{"id": "2409.13109", "pdf": "https://arxiv.org/pdf/2409.13109.pdf", "abs": "https://arxiv.org/abs/2409.13109", "title": "Visualizationary: Automating Design Feedback for Visualization Designers using LLMs", "authors": ["Sungbok Shin", "Sanghyun Hong", "Niklas Elmqvist"], "categories": ["cs.HC"], "comment": "Minor Revision Status, IEEE Transactions on Visualization and\n  Computer Graphics", "summary": "Interactive visualization editors empower users to author visualizations\nwithout writing code, but do not provide guidance on the art and craft of\neffective visual communication. In this paper, we explore the potential of\nusing an off-the-shelf large language models (LLMs) to provide actionable and\ncustomized feedback to visualization designers. Our implementation,\nVISUALIZATIONARY, demonstrates how ChatGPT can be used for this purpose through\ntwo key components: a preamble of visualization design guidelines and a suite\nof perceptual filters that extract salient metrics from a visualization image.\nWe present findings from a longitudinal user study involving 13 visualization\ndesigners-6 novices, 4 intermediates, and 3 experts-who authored a new\nvisualization from scratch over several days. Our results indicate that\nproviding guidance in natural language via an LLM can aid even seasoned\ndesigners in refining their visualizations. All our supplemental materials are\navailable at https://osf.io/v7hu8."}
{"id": "2506.00235", "pdf": "https://arxiv.org/pdf/2506.00235.pdf", "abs": "https://arxiv.org/abs/2506.00235", "title": "MedOrch: Medical Diagnosis with Tool-Augmented Reasoning Agents for Flexible Extensibility", "authors": ["Yexiao He", "Ang Li", "Boyi Liu", "Zhewei Yao", "Yuxiong He"], "categories": ["cs.CL"], "comment": null, "summary": "Healthcare decision-making represents one of the most challenging domains for\nArtificial Intelligence (AI), requiring the integration of diverse knowledge\nsources, complex reasoning, and various external analytical tools. Current AI\nsystems often rely on either task-specific models, which offer limited\nadaptability, or general language models without grounding with specialized\nexternal knowledge and tools. We introduce MedOrch, a novel framework that\norchestrates multiple specialized tools and reasoning agents to provide\ncomprehensive medical decision support. MedOrch employs a modular, agent-based\narchitecture that facilitates the flexible integration of domain-specific tools\nwithout altering the core system. Furthermore, it ensures transparent and\ntraceable reasoning processes, enabling clinicians to meticulously verify each\nintermediate step underlying the system's recommendations. We evaluate MedOrch\nacross three distinct medical applications: Alzheimer's disease diagnosis,\nchest X-ray interpretation, and medical visual question answering, using\nauthentic clinical datasets. The results demonstrate MedOrch's competitive\nperformance across these diverse medical tasks. Notably, in Alzheimer's disease\ndiagnosis, MedOrch achieves an accuracy of 93.26%, surpassing the\nstate-of-the-art baseline by over four percentage points. For predicting\nAlzheimer's disease progression, it attains a 50.35% accuracy, marking a\nsignificant improvement. In chest X-ray analysis, MedOrch exhibits superior\nperformance with a Macro AUC of 61.2% and a Macro F1-score of 25.5%. Moreover,\nin complex multimodal visual question answering (Image+Table), MedOrch achieves\nan accuracy of 54.47%. These findings underscore MedOrch's potential to advance\nhealthcare AI by enabling reasoning-driven tool utilization for multimodal\nmedical data processing and supporting intricate cognitive tasks in clinical\ndecision-making."}
{"id": "2410.03448", "pdf": "https://arxiv.org/pdf/2410.03448.pdf", "abs": "https://arxiv.org/abs/2410.03448", "title": "\"Cold, Calculated, and Condescending\": How AI Identifies and Explains Ableism Compared to Disabled People", "authors": ["Mahika Phutane", "Ananya Seelam", "Aditya Vashistha"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "People with disabilities (PwD) regularly encounter ableist hate and\nmicroaggressions online. These spaces are generally moderated by machine\nlearning models, but little is known about how effectively AI models identify\nableist speech and how well their judgments align with PwD. To investigate\nthis, we curated a first-of-its-kind dataset of 200 social media comments\ntargeted towards PwD, and prompted state-of-the art AI models (i.e., Toxicity\nClassifiers, LLMs) to score toxicity and ableism for each comment, and explain\ntheir reasoning. Then, we recruited 190 participants to similarly rate and\nexplain the harm, and evaluate LLM explanations. Our mixed-methods analysis\nhighlighted a major disconnect: AI underestimated toxicity compared to PwD\nratings, while its ableism assessments were sporadic and varied. Although LLMs\nidentified some biases, its explanations were flawed--they lacked nuance, made\nincorrect assumptions, and appeared judgmental instead of educational. Going\nforward, we discuss challenges and opportunities in designing moderation\nsystems for ableism, and advocate for the involvement of intersectional\ndisabled perspectives in AI."}
{"id": "2506.00250", "pdf": "https://arxiv.org/pdf/2506.00250.pdf", "abs": "https://arxiv.org/abs/2506.00250", "title": "PersianMedQA: Language-Centric Evaluation of LLMs in the Persian Medical Domain", "authors": ["Mohammad Javad Ranjbar Kalahroodi", "Amirhossein Sheikholselami", "Sepehr Karimi", "Sepideh Ranjbar Kalahroodi", "Heshaam Faili", "Azadeh Shakery"], "categories": ["cs.CL", "cs.IT", "math.IT"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable performance on a wide\nrange of NLP benchmarks, often surpassing human-level accuracy. However, their\nreliability in high-stakes domains such as medicine, particularly in\nlow-resource languages, remains underexplored. In this work, we introduce\nPersianMedQA, a large-scale, expert-validated dataset of multiple-choice\nPersian medical questions, designed to evaluate LLMs across both Persian and\nEnglish. We benchmark over 40 state-of-the-art models, including\ngeneral-purpose, Persian fine-tuned, and medical LLMs, in zero-shot and\nchain-of-thought (CoT) settings. Our results show that closed-source general\nmodels (e.g., GPT-4.1) consistently outperform all other categories, achieving\n83.3% accuracy in Persian and 80.7% in English, while Persian fine-tuned models\nsuch as Dorna underperform significantly (e.g., 35.9% in Persian), often\nstruggling with both instruction-following and domain reasoning. We also\nanalyze the impact of translation, showing that while English performance is\ngenerally higher, Persian responses are sometimes more accurate due to cultural\nand clinical contextual cues. Finally, we demonstrate that model size alone is\ninsufficient for robust performance without strong domain or language\nadaptation. PersianMedQA provides a foundation for evaluating multilingual and\nculturally grounded medical reasoning in LLMs. The PersianMedQA dataset can be\naccessed at:\nhttps://huggingface.co/datasets/MohammadJRanjbar/PersianMedQA](https://huggingface.co/datasets/MohammadJRanjbar/PersianMedQA"}
{"id": "2502.02805", "pdf": "https://arxiv.org/pdf/2502.02805.pdf", "abs": "https://arxiv.org/abs/2502.02805", "title": "Data-driven Causal Discovery for Pedestrians-Autonomous Personal Mobility Vehicle Interactions with eHMIs: From Psychological States to Walking Behaviors", "authors": ["Hailong Liu", "Yang Li", "Toshihiro Hiraoka", "Takahiro Wada"], "categories": ["cs.HC"], "comment": null, "summary": "Autonomous personal mobility vehicle (APMV) is a new type of small smart\nvehicle designed for mixed-traffic environments, including interactions with\npedestrians. To enhance the interaction experience between pedestrians and\nAPMVs and to prevent potential risks, it is crucial to investigate pedestrians'\nwalking behaviors when interacting with APMVs and to understand the\npsychological processes underlying these behaviors. This study aims to\ninvestigate the causal relationships between subjective evaluations of\npedestrians and their walking behaviors during interactions with an APMV\nequipped with an external human-machine interface (eHMI). An experiment of\npedestrian-APMV interaction was conducted with 42 pedestrian participants, in\nwhich various eHMIs on the APMV were designed to induce participants to\nexperience different levels of subjective evaluations and generate the\ncorresponding walking behaviors. Based on the hypothesized model of the\npedestrian's cognition-decision-behavior process, the results of causal\ndiscovery align with the previously proposed model. Furthermore, this study\nfurther analyzes the direct and total causal effects of each factor and\ninvestigates the causal processes affecting several important factors in the\nfield of human-vehicle interaction, such as situation awareness, trust in\nvehicle, risk perception, hesitation in decision making, and walking behaviors."}
{"id": "2506.00253", "pdf": "https://arxiv.org/pdf/2506.00253.pdf", "abs": "https://arxiv.org/abs/2506.00253", "title": "Aligned but Blind: Alignment Increases Implicit Bias by Reducing Awareness of Race", "authors": ["Lihao Sun", "Chengzhi Mao", "Valentin Hofmann", "Xuechunzi Bai"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "Accpeted to ACL 2025 Main Conferencce", "summary": "Although value-aligned language models (LMs) appear unbiased in explicit bias\nevaluations, they often exhibit stereotypes in implicit word association tasks,\nraising concerns about their fair usage. We investigate the mechanisms behind\nthis discrepancy and find that alignment surprisingly amplifies implicit bias\nin model outputs. Specifically, we show that aligned LMs, unlike their\nunaligned counterparts, overlook racial concepts in early internal\nrepresentations when the context is ambiguous. Not representing race likely\nfails to activate safety guardrails, leading to unintended biases. Inspired by\nthis insight, we propose a new bias mitigation strategy that works by\nincentivizing the representation of racial concepts in the early model layers.\nIn contrast to conventional mitigation methods of machine unlearning, our\ninterventions find that steering the model to be more aware of racial concepts\neffectively mitigates implicit bias. Similar to race blindness in humans,\nignoring racial nuances can inadvertently perpetuate subtle biases in LMs."}
{"id": "2503.16456", "pdf": "https://arxiv.org/pdf/2503.16456.pdf", "abs": "https://arxiv.org/abs/2503.16456", "title": "Position: Beyond Assistance - Reimagining LLMs as Ethical and Adaptive Co-Creators in Mental Health Care", "authors": ["Abeer Badawi", "Md Tahmid Rahman Laskar", "Jimmy Xiangji Huang", "Shaina Raza", "Elham Dolatabadi"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "This position paper argues for a fundamental shift in how Large Language\nModels (LLMs) are integrated into the mental health care domain. We advocate\nfor their role as co-creators rather than mere assistive tools. While LLMs have\nthe potential to enhance accessibility, personalization, and crisis\nintervention, their adoption remains limited due to concerns about bias,\nevaluation, over-reliance, dehumanization, and regulatory uncertainties. To\naddress these challenges, we propose two structured pathways: SAFE-i\n(Supportive, Adaptive, Fair, and Ethical Implementation) Guidelines for ethical\nand responsible deployment, and HAAS-e (Human-AI Alignment and Safety\nEvaluation) Framework for multidimensional, human-centered assessment. SAFE-i\nprovides a blueprint for data governance, adaptive model engineering, and\nreal-world integration, ensuring LLMs align with clinical and ethical\nstandards. HAAS-e introduces evaluation metrics that go beyond technical\naccuracy to measure trustworthiness, empathy, cultural sensitivity, and\nactionability. We call for the adoption of these structured approaches to\nestablish a responsible and scalable model for LLM-driven mental health\nsupport, ensuring that AI complements, rather than replaces, human expertise."}
{"id": "2506.00256", "pdf": "https://arxiv.org/pdf/2506.00256.pdf", "abs": "https://arxiv.org/abs/2506.00256", "title": "The Impact of Disability Disclosure on Fairness and Bias in LLM-Driven Candidate Selection", "authors": ["Mahammed Kamruzzaman", "Gene Louis Kim"], "categories": ["cs.CL"], "comment": "Accepted at The 38th International FLAIRS Conference (FLAIRS\n  2025)(main)", "summary": "As large language models (LLMs) become increasingly integrated into hiring\nprocesses, concerns about fairness have gained prominence. When applying for\njobs, companies often request/require demographic information, including\ngender, race, and disability or veteran status. This data is collected to\nsupport diversity and inclusion initiatives, but when provided to LLMs,\nespecially disability-related information, it raises concerns about potential\nbiases in candidate selection outcomes. Many studies have highlighted how\ndisability can impact CV screening, yet little research has explored the\nspecific effect of voluntarily disclosed information on LLM-driven candidate\nselection. This study seeks to bridge that gap. When candidates shared\nidentical gender, race, qualifications, experience, and backgrounds, and sought\njobs with minimal employment rate gaps between individuals with and without\ndisabilities (e.g., Cashier, Software Developer), LLMs consistently favored\ncandidates who disclosed that they had no disability. Even in cases where\ncandidates chose not to disclose their disability status, the LLMs were less\nlikely to select them compared to those who explicitly stated they did not have\na disability."}
{"id": "2503.16512", "pdf": "https://arxiv.org/pdf/2503.16512.pdf", "abs": "https://arxiv.org/abs/2503.16512", "title": "Multimodal Sensing and Machine Learning to Compare Printed and Verbal Assembly Instructions Delivered by a Social Robot", "authors": ["Ruchik Mishra", "Laksita Prasanna", "Adair Adair", "Dan O Popa"], "categories": ["cs.HC", "cs.RO"], "comment": "Accepted to IEEE CASE 2025", "summary": "In this paper, we compare a manual assembly task communicated to workers\nusing both printed and robot-delivered instructions. The comparison was made\nusing physiological signals (blood volume pulse (BVP) and electrodermal\nactivity (EDA)) collected from individuals during an experimental study. In\naddition, we also collected responses of individuals using the NASA Task Load\nIndex (TLX) survey. Furthermore, we mapped the collected physiological signals\nto the responses of participants for NASA TLX to predict their workload. For\nboth the classification problems, we compare the performance of Convolutional\nNeural Networks (CNNs) and Long-Short-Term Memory (LSTM) models. Results show\nthat for our CNN-based approach using multimodal data (both BVP and EDA) gave\nbetter results than using just BVP (approx. 8.38% more) and EDA (approx 20.49%\nmore). Our LSTM-based model too had better results when we used multimodal data\n(approx 8.38% more than just BVP and 6.70% more than just EDA). Overall, CNNs\nperformed better than LSTMs for classifying physiologies for paper vs\nrobot-based instruction by 7.72%. The CNN-based model was able to give better\nclassification results (approximately 17.83% more on an average across all\nresponses of the NASA TLX) within a few minutes of training compared to the\nLSTM-based models."}
{"id": "2506.00264", "pdf": "https://arxiv.org/pdf/2506.00264.pdf", "abs": "https://arxiv.org/abs/2506.00264", "title": "MultiHoax: A Dataset of Multi-hop False-Premise Questions", "authors": ["Mohammadamin Shafiei", "Hamidreza Saffari", "Nafise Sadat Moosavi"], "categories": ["cs.CL"], "comment": null, "summary": "As Large Language Models are increasingly deployed in high-stakes domains,\ntheir ability to detect false assumptions and reason critically is crucial for\nensuring reliable outputs. False-premise questions (FPQs) serve as an important\nevaluation method by exposing cases where flawed assumptions lead to incorrect\nresponses. While existing benchmarks focus on single-hop FPQs, real-world\nreasoning often requires multi-hop inference, where models must verify\nconsistency across multiple reasoning steps rather than relying on\nsurface-level cues. To address this gap, we introduce MultiHoax, a benchmark\nfor evaluating LLMs' ability to handle false premises in complex, multi-step\nreasoning tasks. Our dataset spans seven countries and ten diverse knowledge\ncategories, using Wikipedia as the primary knowledge source to enable factual\nreasoning across regions. Experiments reveal that state-of-the-art LLMs\nstruggle to detect false premises across different countries, knowledge\ncategories, and multi-hop reasoning types, highlighting the need for improved\nfalse premise detection and more robust multi-hop reasoning capabilities in\nLLMs."}
{"id": "2503.18792", "pdf": "https://arxiv.org/pdf/2503.18792.pdf", "abs": "https://arxiv.org/abs/2503.18792", "title": "REALM: A Dataset of Real-World LLM Use Cases", "authors": ["Jingwen Cheng", "Kshitish Ghate", "Wenyue Hua", "William Yang Wang", "Hong Shen", "Fei Fang"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": "11 pages, 3 figures", "summary": "Large Language Models (LLMs), such as the GPT series, have driven significant\nindustrial applications, leading to economic and societal transformations.\nHowever, a comprehensive understanding of their real-world applications remains\nlimited. To address this, we introduce REALM, a dataset of over 94,000 LLM use\ncases collected from Reddit and news articles. REALM captures two key\ndimensions: the diverse applications of LLMs and the demographics of their\nusers. It categorizes LLM applications and explores how users' occupations\nrelate to the types of applications they use. By integrating real-world data,\nREALM offers insights into LLM adoption across different domains, providing a\nfoundation for future research on their evolving societal roles."}
{"id": "2506.00267", "pdf": "https://arxiv.org/pdf/2506.00267.pdf", "abs": "https://arxiv.org/abs/2506.00267", "title": "CASPER: A Large Scale Spontaneous Speech Dataset", "authors": ["Cihan Xiao", "Ruixing Liang", "Xiangyu Zhang", "Mehmet Emre Tiryaki", "Veronica Bae", "Lavanya Shankar", "Rong Yang", "Ethan Poon", "Emmanuel Dupoux", "Sanjeev Khudanpur", "Leibny Paola Garcia Perera"], "categories": ["cs.CL"], "comment": null, "summary": "The success of large language models has driven interest in developing\nsimilar speech processing capabilities. However, a key challenge is the\nscarcity of high-quality spontaneous speech data, as most existing datasets\ncontain scripted dialogues. To address this, we present a novel pipeline for\neliciting and recording natural dialogues and release our Stage 1 dataset with\n200+ hours of spontaneous speech. Our approach fosters fluid, natural\nconversations while encouraging a diverse range of topics and interactive\nexchanges. Unlike traditional methods, it facilitates genuine interactions,\nproviding a reproducible framework for future data collection. This paper\nintroduces our dataset and methodology, laying the groundwork for addressing\nthe shortage of spontaneous speech data. We plan to expand this dataset in\nfuture stages, offering a growing resource for the research community."}
{"id": "2504.13861", "pdf": "https://arxiv.org/pdf/2504.13861.pdf", "abs": "https://arxiv.org/abs/2504.13861", "title": "3MDBench: Medical Multimodal Multi-agent Dialogue Benchmark", "authors": ["Ivan Sviridov", "Amina Miftakhova", "Artemiy Tereshchenko", "Galina Zubkova", "Pavel Blinov", "Andrey Savchenko"], "categories": ["cs.HC", "cs.CL", "cs.MA", "68T42", "I.2.1"], "comment": "35 pages, 13 figures, 7 tables", "summary": "Though Large Vision-Language Models (LVLMs) are being actively explored in\nmedicine, their ability to conduct telemedicine consultations combining\naccurate diagnosis with professional dialogue remains underexplored. In this\npaper, we present 3MDBench (Medical Multimodal Multi-agent Dialogue Benchmark),\nan open-source framework for simulating and evaluating LVLM-driven telemedical\nconsultations. 3MDBench simulates patient variability through four\ntemperament-based Patient Agents and an Assessor Agent that jointly evaluate\ndiagnostic accuracy and dialogue quality. It includes 3013 cases across 34\ndiagnoses drawn from real-world telemedicine interactions, combining textual\nand image-based data. The experimental study compares diagnostic strategies for\npopular LVLMs, including GPT-4o-mini, LLaVA-3.2-11B-Vision-Instruct, and\nQwen2-VL-7B-Instruct. We demonstrate that multimodal dialogue with internal\nreasoning improves F1 score by 6.5% over non-dialogue settings, highlighting\nthe importance of context-aware, information-seeking questioning. Moreover,\ninjecting predictions from a diagnostic convolutional network into the LVLM's\ncontext boosts F1 by up to 20%. Source code is available at\nhttps://anonymous.4open.science/r/3mdbench_acl-0511."}
{"id": "2506.00277", "pdf": "https://arxiv.org/pdf/2506.00277.pdf", "abs": "https://arxiv.org/abs/2506.00277", "title": "Hierarchical Level-Wise News Article Clustering via Multilingual Matryoshka Embeddings", "authors": ["Hans W. A. Hanley", "Zakir Durumeric"], "categories": ["cs.CL", "cs.AI", "cs.SI"], "comment": "Accepted to The 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)", "summary": "Contextual large language model embeddings are increasingly utilized for\ntopic modeling and clustering. However, current methods often scale poorly,\nrely on opaque similarity metrics, and struggle in multilingual settings. In\nthis work, we present a novel, scalable, interpretable, hierarchical, and\nmultilingual approach to clustering news articles and social media data. To do\nthis, we first train multilingual Matryoshka embeddings that can determine\nstory similarity at varying levels of granularity based on which subset of the\ndimensions of the embeddings is examined. This embedding model achieves\nstate-of-the-art performance on the SemEval 2022 Task 8 test dataset (Pearson\n$\\rho$ = 0.816). Once trained, we develop an efficient hierarchical clustering\nalgorithm that leverages the hierarchical nature of Matryoshka embeddings to\nidentify unique news stories, narratives, and themes. We conclude by\nillustrating how our approach can identify and cluster stories, narratives, and\noverarching themes within real-world news datasets."}
{"id": "2504.14822", "pdf": "https://arxiv.org/pdf/2504.14822.pdf", "abs": "https://arxiv.org/abs/2504.14822", "title": "Completing A Systematic Review in Hours instead of Months with Interactive AI Agents", "authors": ["Rui Qiu", "Shijie Chen", "Yu Su", "Po-Yin Yen", "Han-Wei Shen"], "categories": ["cs.HC", "cs.CL"], "comment": "Accepted as ACL 2025 (main)", "summary": "Systematic reviews (SRs) are vital for evidence-based practice in high stakes\ndisciplines, such as healthcare, but are often impeded by intensive labors and\nlengthy processes that can take months to complete. Due to the high demand for\ndomain expertise, existing automatic summarization methods fail to accurately\nidentify relevant studies and generate high-quality summaries. To that end, we\nintroduce InsightAgent, a human-centered interactive AI agent powered by large\nlanguage models that revolutionize this workflow. InsightAgent partitions a\nlarge literature corpus based on semantics and employs a multi-agent design for\nmore focused processing of literature, leading to significant improvement in\nthe quality of generated SRs. InsightAgent also provides intuitive\nvisualizations of the corpus and agent trajectories, allowing users to\neffortlessly monitor the actions of the agent and provide real-time feedback\nbased on their expertise. Our user studies with 9 medical professionals\ndemonstrate that the visualization and interaction mechanisms can effectively\nimprove the quality of synthesized SRs by 27.2%, reaching 79.7% of\nhuman-written quality. At the same time, user satisfaction is improved by\n34.4%. With InsightAgent, it only takes a clinician about 1.5 hours, rather\nthan months, to complete a high-quality systematic review."}
{"id": "2506.00288", "pdf": "https://arxiv.org/pdf/2506.00288.pdf", "abs": "https://arxiv.org/abs/2506.00288", "title": "Emergent Abilities of Large Language Models under Continued Pretraining for Language Adaptation", "authors": ["Ahmed Elhady", "Eneko Agirre", "Mikel Artetxe"], "categories": ["cs.CL", "cs.AI"], "comment": "To appear in ACL 2025 Main", "summary": "Continued pretraining (CPT) is a popular approach to adapt existing large\nlanguage models (LLMs) to new languages. When doing so, it is common practice\nto include a portion of English data in the mixture, but its role has not been\ncarefully studied to date. In this work, we show that including English does\nnot impact validation perplexity, yet it is critical for the emergence of\ndownstream capabilities in the target language. We introduce a\nlanguage-agnostic benchmark for in-context learning (ICL), which reveals\ncatastrophic forgetting early on CPT when English is not included. This in turn\ndamages the ability of the model to generalize to downstream prompts in the\ntarget language as measured by perplexity, even if it does not manifest in\nterms of accuracy until later in training, and can be tied to a big shift in\nthe model parameters. Based on these insights, we introduce curriculum learning\nand exponential moving average (EMA) of weights as effective alternatives to\nmitigate the need for English. All in all, our work sheds light into the\ndynamics by which emergent abilities arise when doing CPT for language\nadaptation, and can serve as a foundation to design more effective methods in\nthe future."}
{"id": "2505.22906", "pdf": "https://arxiv.org/pdf/2505.22906.pdf", "abs": "https://arxiv.org/abs/2505.22906", "title": "HiLDe: Intentional Code Generation via Human-in-the-Loop Decoding", "authors": ["Emmanuel Anaya Gonz√°lez", "Raven Rothkopf", "Sorin Lerner", "Nadia Polikarpova"], "categories": ["cs.HC", "cs.AI", "cs.PL"], "comment": "10 pages, 6 figures", "summary": "While AI programming tools hold the promise of increasing programmers'\ncapabilities and productivity to a remarkable degree, they often exclude users\nfrom essential decision-making processes, causing many to effectively \"turn off\ntheir brains\" and over-rely on solutions provided by these systems. These\nbehaviors can have severe consequences in critical domains, like software\nsecurity. We propose Human-in-the-loop Decoding, a novel interaction technique\nthat allows users to observe and directly influence LLM decisions during code\ngeneration, in order to align the model's output with their personal\nrequirements. We implement this technique in HiLDe, a code completion assistant\nthat highlights critical decisions made by the LLM and provides local\nalternatives for the user to explore. In a within-subjects study (N=18) on\nsecurity-related tasks, we found that HiLDe led participants to generate\nsignificantly fewer vulnerabilities and better align code generation with their\ngoals compared to a traditional code completion assistant."}
{"id": "2506.00290", "pdf": "https://arxiv.org/pdf/2506.00290.pdf", "abs": "https://arxiv.org/abs/2506.00290", "title": "DLM-One: Diffusion Language Models for One-Step Sequence Generation", "authors": ["Tianqi Chen", "Shujian Zhang", "Mingyuan Zhou"], "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": null, "summary": "This paper introduces DLM-One, a score-distillation-based framework for\none-step sequence generation with continuous diffusion language models (DLMs).\nDLM-One eliminates the need for iterative refinement by aligning the scores of\na student model's outputs in the continuous token embedding space with the\nscore function of a pretrained teacher DLM. We investigate whether DLM-One can\nachieve substantial gains in sampling efficiency for language modeling. Through\ncomprehensive experiments on DiffuSeq -- a representative continuous DLM -- we\nshow that DLM-One achieves up to ~500x speedup in inference time while\nmaintaining competitive performance on benchmark text generation tasks used to\nevaluate the teacher models. We further analyze the method's empirical behavior\nacross multiple datasets, providing initial insights into its generality and\npractical applicability. Our findings position one-step diffusion as a\npromising direction for efficient, high-quality language generation and broader\nadoption of continuous diffusion models operating in embedding space for\nnatural language processing."}
{"id": "2305.11943", "pdf": "https://arxiv.org/pdf/2305.11943.pdf", "abs": "https://arxiv.org/abs/2305.11943", "title": "Public versus Less-Public News Engagement on Facebook: Patterns Across Bias and Reliability", "authors": ["Alireza Mohammadinodooshan", "Niklas Carlsson"], "categories": ["cs.SI", "cs.CY", "cs.HC"], "comment": "Updated to the version published in the Proceedings of the 17th ACM\n  Web Science Conference (WebSci'25), May 20-24, 2025, New Brunswick, NJ, USA", "summary": "The rapid growth of social media as a news platform has raised significant\nconcerns about the influence and societal impact of biased and unreliable news\non these platforms. While much research has explored user engagement with news\non platforms like Facebook, most studies have focused on publicly shared posts.\nThis focus leaves an important question unanswered: how representative is the\npublic sphere of Facebook's entire ecosystem? Specifically, how much of the\ninteractions occur in less-public spaces, and do public engagement patterns for\ndifferent news classes (e.g., reliable vs. unreliable) generalize to the\nbroader Facebook ecosystem?\n  This paper presents the first comprehensive comparison of interaction\npatterns between Facebook's more public sphere (referred to as public in paper)\nand the less public sphere (referred to as private). For the analysis, we first\ncollect two complementary datasets: (1) aggregated interaction data for all\nFacebook posts (public + private) for 19,050 manually labeled news articles\n(225.3M user interactions), and (2) a subset containing only interactions with\npublic posts (70.4M interactions). Then, through discussions and iterative\nfeedback from the CrowdTangle team, we develop a robust method for fair\ncomparison between these datasets.\n  Our analysis reveals that only 31% of news interactions occur in the public\nsphere, with significant variations across news classes. Engagement patterns in\nless-public spaces often differ, with users, for example, engaging more deeply\nin private contexts. These findings highlight the need to examine both public\nand less-public engagement to fully understand news dissemination on Facebook.\nThe observed differences hold important implications on content moderation,\nplatform governance, and policymaking, contributing to healthier online\ndiscourse."}
{"id": "2506.00304", "pdf": "https://arxiv.org/pdf/2506.00304.pdf", "abs": "https://arxiv.org/abs/2506.00304", "title": "Can LLMs Understand Unvoiced Speech? Exploring EMG-to-Text Conversion with LLMs", "authors": ["Payal Mohapatra", "Akash Pandey", "Xiaoyuan Zhang", "Qi Zhu"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 main conference", "summary": "Unvoiced electromyography (EMG) is an effective communication tool for\nindividuals unable to produce vocal speech. However, most prior methods rely on\npaired voiced and unvoiced EMG signals, along with speech data, for EMG-to-text\nconversion, which is not practical for such individuals. Given the rise of\nlarge language models (LLMs) in speech recognition, we explore their potential\nto understand unvoiced speech. To this end, we address the challenge of\nlearning from unvoiced EMG alone and propose a novel EMG adaptor module that\nmaps EMG features into an LLM's input space, achieving an average word error\nrate (WER) of 0.49 on a closed-vocabulary unvoiced EMG-to-text task. Even with\na conservative data availability of just six minutes, our approach improves\nperformance over specialized models by nearly 20%. While LLMs have been shown\nto be extendable to new language modalities -- such as audio -- understanding\narticulatory biosignals like unvoiced EMG remains more challenging. This work\ntakes a crucial first step toward enabling LLMs to comprehend unvoiced speech\nusing surface EMG."}
{"id": "2408.03819", "pdf": "https://arxiv.org/pdf/2408.03819.pdf", "abs": "https://arxiv.org/abs/2408.03819", "title": "Leveraging Variation Theory in Counterfactual Data Augmentation for Optimized Active Learning", "authors": ["Simret Araya Gebreegziabher", "Kuangshi Ai", "Zheng Zhang", "Elena L. Glassman", "Toby Jia-Jun Li"], "categories": ["cs.LG", "cs.CL", "cs.HC"], "comment": "Accepted to ACL 2025 Findings", "summary": "Active Learning (AL) allows models to learn interactively from user feedback.\nThis paper introduces a counterfactual data augmentation approach to AL,\nparticularly addressing the selection of datapoints for user querying, a\npivotal concern in enhancing data efficiency. Our approach is inspired by\nVariation Theory, a theory of human concept learning that emphasizes the\nessential features of a concept by focusing on what stays the same and what\nchanges. Instead of just querying with existing datapoints, our approach\nsynthesizes artificial datapoints that highlight potential key similarities and\ndifferences among labels using a neuro-symbolic pipeline combining large\nlanguage models (LLMs) and rule-based models. Through an experiment in the\nexample domain of text classification, we show that our approach achieves\nsignificantly higher performance when there are fewer annotated data. As the\nannotated training data gets larger the impact of the generated data starts to\ndiminish showing its capability to address the cold start problem in AL. This\nresearch sheds light on integrating theories of human learning into the\noptimization of AL."}
{"id": "2506.00307", "pdf": "https://arxiv.org/pdf/2506.00307.pdf", "abs": "https://arxiv.org/abs/2506.00307", "title": "Lossless Token Sequence Compression via Meta-Tokens", "authors": ["John Harvill", "Ziwei Fan", "Hao Wang", "Yizhou Sun", "Hao Ding", "Luke Huan", "Anoop Deoras"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "16 pages, 8 figures", "summary": "Existing work on prompt compression for Large Language Models (LLM) focuses\non lossy methods that try to maximize the retention of semantic information\nthat is relevant to downstream tasks while significantly reducing the sequence\nlength. In this paper, we introduce a task-agnostic lossless compression\ntechnique similar to LZ77 that makes it possible to reduce the input token\nsequence length on average by 27\\% and 18\\% for the two evaluation tasks\nexplored here. Given that we use transformer-based LLMs, this equates to 47\\%\nand 33\\% less encoding computation, respectively, due to the quadratic nature\nof attention. The token sequence transformation is trivial to reverse and\nhighlights that no semantic information is lost in the process. We evaluate our\nproposed approach on two tasks that require strict preservation of\nsemantics/syntax and demonstrate that existing lossy compression methods\nperform poorly in this setting. We find that our lossless compression technique\nproduces only a small gap in performance compared to using the uncompressed\ninput and posit that larger models and an expanded computing budget would\nlikely erase the gap entirely."}
{"id": "2409.03219", "pdf": "https://arxiv.org/pdf/2409.03219.pdf", "abs": "https://arxiv.org/abs/2409.03219", "title": "Content Moderation by LLM: From Accuracy to Legitimacy", "authors": ["Tao Huang"], "categories": ["cs.CY", "cs.AI", "cs.ET", "cs.HC", "cs.LG"], "comment": null, "summary": "One trending application of LLM (large language model) is to use it for\ncontent moderation in online platforms. Most current studies on this\napplication have focused on the metric of accuracy -- the extent to which LLMs\nmake correct decisions about content. This article argues that accuracy is\ninsufficient and misleading because it fails to grasp the distinction between\neasy cases and hard cases, as well as the inevitable trade-offs in achieving\nhigher accuracy. Closer examination reveals that content moderation is a\nconstitutive part of platform governance, the key of which is to gain and\nenhance legitimacy. Instead of making moderation decisions correct, the chief\ngoal of LLMs is to make them legitimate. In this regard, this article proposes\na paradigm shift from the single benchmark of accuracy towards a\nlegitimacy-based framework for evaluating the performance of LLM moderators.\nThe framework suggests that for easy cases, the key is to ensure accuracy,\nspeed, and transparency, while for hard cases, what matters is reasoned\njustification and user participation. Examined under this framework, LLMs' real\npotential in moderation is not accuracy improvement. Rather, LLMs can better\ncontribute in four other aspects: to conduct screening of hard cases from easy\ncases, to provide quality explanations for moderation decisions, to assist\nhuman reviewers in getting more contextual information, and to facilitate user\nparticipation in a more interactive way. To realize these contributions, this\narticle proposes a workflow for incorporating LLMs into the content moderation\nsystem. Using normative theories from law and social sciences to critically\nassess the new technological application, this article seeks to redefine LLMs'\nrole in content moderation and redirect relevant research in this field."}
{"id": "2506.00312", "pdf": "https://arxiv.org/pdf/2506.00312.pdf", "abs": "https://arxiv.org/abs/2506.00312", "title": "An evaluation of LLMs for generating movie reviews: GPT-4o, Gemini-2.0 and DeepSeek-V3", "authors": ["Brendan Sands", "Yining Wang", "Chenhao Xu", "Yuxuan Zhou", "Lai Wei", "Rohitash Chandra"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have been prominent in various tasks, including\ntext generation and summarisation. The applicability of LLMs to the generation\nof product reviews is gaining momentum, paving the way for the generation of\nmovie reviews. In this study, we propose a framework that generates movie\nreviews using three LLMs (GPT-4o, DeepSeek-V3, and Gemini-2.0), and evaluate\ntheir performance by comparing the generated outputs with IMDb user reviews. We\nuse movie subtitles and screenplays as input to the LLMs and investigate how\nthey affect the quality of reviews generated. We review the LLM-based movie\nreviews in terms of vocabulary, sentiment polarity, similarity, and thematic\nconsistency in comparison to IMDB user reviews. The results demonstrate that\nLLMs are capable of generating syntactically fluent and structurally complete\nmovie reviews. Nevertheless, there is still a noticeable gap in emotional\nrichness and stylistic coherence between LLM-generated and IMDb reviews,\nsuggesting that further refinement is needed to improve the overall quality of\nmovie review generation. We provided a survey-based analysis where participants\nwere told to distinguish between LLM and IMDb user reviews. The results show\nthat LLM-generated reviews are difficult to distinguish from IMDB user reviews.\nWe found that DeepSeek-V3 produced the most balanced reviews, closely matching\nIMDb reviews. GPT-4o overemphasised positive emotions, while Gemini-2.0\ncaptured negative emotions better but showed excessive emotional intensity."}
{"id": "2412.01617", "pdf": "https://arxiv.org/pdf/2412.01617.pdf", "abs": "https://arxiv.org/abs/2412.01617", "title": "If Eleanor Rigby Had Met ChatGPT: A Study on Loneliness in a Post-LLM World", "authors": ["Adrian de Wynter"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": "Accepted to ACL 2025 (main)", "summary": "Warning: this paper discusses content related, but not limited to, violence,\nsex, and suicide. Loneliness, or the lack of fulfilling relationships,\nsignificantly impacts a person's mental and physical well-being and is\nprevalent worldwide. Previous research suggests that large language models\n(LLMs) may help mitigate loneliness. However, we argue that the use of\nwidespread LLMs in services like ChatGPT is more prevalent--and riskier, as\nthey are not designed for this purpose. To explore this, we analysed user\ninteractions with ChatGPT outside of its marketed use as a task-oriented\nassistant. In dialogues classified as lonely, users frequently (37%) sought\nadvice or validation, and received good engagement. However, ChatGPT failed in\nsensitive scenarios, like responding appropriately to suicidal ideation or\ntrauma. We also observed a 35% higher incidence of toxic content, with women\nbeing 22x more likely to be targeted than men. Our findings underscore ethical\nand legal questions about this technology, and note risks like radicalisation\nor further isolation. We conclude with recommendations to research and industry\nto address loneliness."}
{"id": "2506.00319", "pdf": "https://arxiv.org/pdf/2506.00319.pdf", "abs": "https://arxiv.org/abs/2506.00319", "title": "SkillVerse : Assessing and Enhancing LLMs with Tree Evaluation", "authors": ["Yufei Tian", "Jiao Sun", "Nanyun Peng", "Zizhao Zhang"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025", "summary": "As language models evolve to tackle complex, multifaceted tasks, their\nevaluation must adapt to capture this intricacy. A granular, skill-specific\nunderstanding of model capabilities can empower researchers to make informed\nmodel development plans. In this paper, we introduce SkillVerse, an\nunsupervised tree-structured diagnosis framework for understanding model\nproficiency in specific abilities. With LLM as a judge, SkillVerse first\ncritiques the model responses, and then organizes them into a hierarchical\nstructure termed dendrogram. Given proficiency at arbitrary levels of\ngranularity, SkillVerse is flexible to produce insights of behaviors of modern\nlarge models. We also demonstrate its efficacy in two downstream tasks: 1)\nimproving model in-context learning by 25% using a tree-search algorithm to\nselect more informative few-shot demonstrations, and 2) accurately predicting\nnew model weaknesses with a 55% success rate, 22% higher than without\nSkillVerse."}
{"id": "2501.15056", "pdf": "https://arxiv.org/pdf/2501.15056.pdf", "abs": "https://arxiv.org/abs/2501.15056", "title": "Feedback-Aware Monte Carlo Tree Search for Efficient Information Seeking in Goal-Oriented Conversations", "authors": ["Harshita Chopra", "Chirag Shah"], "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG"], "comment": null, "summary": "Effective decision-making and problem-solving in conversational systems\nrequire the ability to identify and acquire missing information through\ntargeted questioning. A key challenge lies in efficiently narrowing down a\nlarge space of possible outcomes by posing questions that minimize uncertainty.\nTo address this, we introduce a novel framework that leverages Large Language\nModels (LLMs) to generate information-seeking questions, with Monte Carlo Tree\nSearch (MCTS) to strategically select questions that maximize information gain,\nas a part of inference-time planning. Our primary contribution includes a\nhierarchical feedback mechanism that exploits past interaction patterns to\nguide future strategy. Specifically, each new problem is mapped to a cluster\nbased on semantic similarity, and our UCT (Upper Confidence bound for Trees)\nformulation employs a cluster-specific bonus reward to prioritize successful\nquestion trajectories that have proven effective for similar problems in the\npast. Extensive empirical evaluation across medical diagnosis and technical\ntroubleshooting domains shows that our method achieves an average of 12%\nimprovement in success rates and about 10x reduction in the number of LLM calls\nmade for planning per conversation, compared to the state of the art. An\nadditional 8% gain in success rate is observed on average when we start with a\nconstrained set of possibilities. Our results underscore the efficacy of\nfeedback-aware MCTS in enhancing information-seeking in goal-oriented\ndialogues."}
{"id": "2506.00331", "pdf": "https://arxiv.org/pdf/2506.00331.pdf", "abs": "https://arxiv.org/abs/2506.00331", "title": "TreeRare: Syntax Tree-Guided Retrieval and Reasoning for Knowledge-Intensive Question Answering", "authors": ["Boyi Zhang", "Zhuo Liu", "Hangfeng He"], "categories": ["cs.CL"], "comment": null, "summary": "In real practice, questions are typically complex and knowledge-intensive,\nrequiring Large Language Models (LLMs) to recognize the multifaceted nature of\nthe question and reason across multiple information sources. Iterative and\nadaptive retrieval, where LLMs decide when and what to retrieve based on their\nreasoning, has been shown to be a promising approach to resolve complex,\nknowledge-intensive questions. However, the performance of such retrieval\nframeworks is limited by the accumulation of reasoning errors and misaligned\nretrieval results. To overcome these limitations, we propose TreeRare (Syntax\nTree-Guided Retrieval and Reasoning), a framework that utilizes syntax trees to\nguide information retrieval and reasoning for question answering. Following the\nprinciple of compositionality, TreeRare traverses the syntax tree in a\nbottom-up fashion, and in each node, it generates subcomponent-based queries\nand retrieves relevant passages to resolve localized uncertainty. A\nsubcomponent question answering module then synthesizes these passages into\nconcise, context-aware evidence. Finally, TreeRare aggregates the evidence\nacross the tree to form a final answer. Experiments across five question\nanswering datasets involving ambiguous or multi-hop reasoning demonstrate that\nTreeRare achieves substantial improvements over existing state-of-the-art\nmethods."}
{"id": "2501.17182", "pdf": "https://arxiv.org/pdf/2501.17182.pdf", "abs": "https://arxiv.org/abs/2501.17182", "title": "Dialogue Systems for Emotional Support via Value Reinforcement", "authors": ["Juhee Kim", "Chunghu Mok", "Jisun Lee", "Hyang Sook Kim", "Yohan Jo"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "I.2.7"], "comment": "This paper has been accepted for publication at ACL 2025", "summary": "Emotional support dialogue systems aim to reduce help-seekers' distress and\nhelp them overcome challenges. While human values$\\unicode{x2013}$core beliefs\nthat shape an individual's priorities$\\unicode{x2013}$are increasingly\nemphasized in contemporary psychological therapy for their role in fostering\ninternal transformation and long-term emotional well-being, their integration\ninto emotional support systems remains underexplored. To bridge this gap, we\npresent a value-driven method for training emotional support dialogue systems\ndesigned to reinforce positive values in seekers. Notably, our model identifies\nwhich values to reinforce at each turn and how to do so, by leveraging online\nsupport conversations from Reddit. We evaluate the method across support\nskills, seekers' emotional intensity, and value reinforcement. Our method\nconsistently outperforms various baselines, effectively exploring and eliciting\nvalues from seekers. Additionally, leveraging crowd knowledge from Reddit\nsignificantly enhances its effectiveness. Therapists highlighted its ability to\nvalidate seekers' challenges and emphasize positive aspects of their\nsituations$\\unicode{x2013}$both crucial elements of value reinforcement. Our\nwork, being the first to integrate value reinforcement into emotional support\nsystems, demonstrates its promise and establishes a foundation for future\nresearch."}
{"id": "2506.00332", "pdf": "https://arxiv.org/pdf/2506.00332.pdf", "abs": "https://arxiv.org/abs/2506.00332", "title": "Disentangling Codemixing in Chats: The NUS ABC Codemixed Corpus", "authors": ["Svetlana Churina", "Akshat Gupta", "Insyirah Mujtahid", "Kokil Jaidka"], "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "Code-mixing involves the seamless integration of linguistic elements from\nmultiple languages within a single discourse, reflecting natural multilingual\ncommunication patterns. Despite its prominence in informal interactions such as\nsocial media, chat messages and instant-messaging exchanges, there has been a\nlack of publicly available corpora that are author-labeled and suitable for\nmodeling human conversations and relationships. This study introduces the first\nlabeled and general-purpose corpus for understanding code-mixing in context\nwhile maintaining rigorous privacy and ethical standards. Our live project will\ncontinuously gather, verify, and integrate code-mixed messages into a\nstructured dataset released in JSON format, accompanied by detailed metadata\nand linguistic statistics. To date, it includes over 355,641 messages spanning\nvarious code-mixing patterns, with a primary focus on English, Mandarin, and\nother languages. We expect the Codemix Corpus to serve as a foundational\ndataset for research in computational linguistics, sociolinguistics, and NLP\napplications."}
{"id": "2502.11196", "pdf": "https://arxiv.org/pdf/2502.11196.pdf", "abs": "https://arxiv.org/abs/2502.11196", "title": "How Do LLMs Acquire New Knowledge? A Knowledge Circuits Perspective on Continual Pre-Training", "authors": ["Yixin Ou", "Yunzhi Yao", "Ningyu Zhang", "Hui Jin", "Jiacheng Sun", "Shumin Deng", "Zhenguo Li", "Huajun Chen"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": "ACL 2025 Findings", "summary": "Despite exceptional capabilities in knowledge-intensive tasks, Large Language\nModels (LLMs) face a critical gap in understanding how they internalize new\nknowledge, particularly how to structurally embed acquired knowledge in their\nneural computations. We address this issue through the lens of knowledge\ncircuit evolution, identifying computational subgraphs that facilitate\nknowledge storage and processing. Our systematic analysis of circuit evolution\nthroughout continual pre-training reveals several key findings: (1) the\nacquisition of new knowledge is influenced by its relevance to pre-existing\nknowledge; (2) the evolution of knowledge circuits exhibits a distinct phase\nshift from formation to optimization; (3) the evolution of knowledge circuits\nfollows a deep-to-shallow pattern. These insights not only advance our\ntheoretical understanding of the mechanisms of new knowledge acquisition in\nLLMs, but also provide potential implications for improving continual\npre-training strategies to enhance model performance. Code and data will be\navailable at https://github.com/zjunlp/DynamicKnowledgeCircuits."}
{"id": "2506.00334", "pdf": "https://arxiv.org/pdf/2506.00334.pdf", "abs": "https://arxiv.org/abs/2506.00334", "title": "Beyond Context to Cognitive Appraisal: Emotion Reasoning as a Theory of Mind Benchmark for Large Language Models", "authors": ["Gerard Christopher Yeo", "Kokil Jaidka"], "categories": ["cs.CL"], "comment": "9 pages, 3 figures", "summary": "Datasets used for emotion recognition tasks typically contain overt cues that\ncan be used in predicting the emotions expressed in a text. However, one\nchallenge is that texts sometimes contain covert contextual cues that are rich\nin affective semantics, which warrant higher-order reasoning abilities to infer\nemotional states, not simply the emotions conveyed. This study advances beyond\nsurface-level perceptual features to investigate how large language models\n(LLMs) reason about others' emotional states using contextual information,\nwithin a Theory-of-Mind (ToM) framework. Grounded in Cognitive Appraisal\nTheory, we curate a specialized ToM evaluation dataset1 to assess both forward\nreasoning - from context to emotion- and backward reasoning - from emotion to\ninferred context. We showed that LLMs can reason to a certain extent, although\nthey are poor at associating situational outcomes and appraisals with specific\nemotions. Our work highlights the need for psychological theories in the\ntraining and evaluation of LLMs in the context of emotion reasoning."}
{"id": "2503.13509", "pdf": "https://arxiv.org/pdf/2503.13509.pdf", "abs": "https://arxiv.org/abs/2503.13509", "title": "MentalChat16K: A Benchmark Dataset for Conversational Mental Health Assistance", "authors": ["Jia Xu", "Tianyi Wei", "Bojian Hou", "Patryk Orzechowski", "Shu Yang", "Ruochen Jin", "Rachael Paulbeck", "Joost Wagenaar", "George Demiris", "Li Shen"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY", "cs.HC"], "comment": null, "summary": "We introduce MentalChat16K, an English benchmark dataset combining a\nsynthetic mental health counseling dataset and a dataset of anonymized\ntranscripts from interventions between Behavioral Health Coaches and Caregivers\nof patients in palliative or hospice care. Covering a diverse range of\nconditions like depression, anxiety, and grief, this curated dataset is\ndesigned to facilitate the development and evaluation of large language models\nfor conversational mental health assistance. By providing a high-quality\nresource tailored to this critical domain, MentalChat16K aims to advance\nresearch on empathetic, personalized AI solutions to improve access to mental\nhealth support services. The dataset prioritizes patient privacy, ethical\nconsiderations, and responsible data usage. MentalChat16K presents a valuable\nopportunity for the research community to innovate AI technologies that can\npositively impact mental well-being. The dataset is available at\nhttps://huggingface.co/datasets/ShenLab/MentalChat16K and the code and\ndocumentation are hosted on GitHub at\nhttps://github.com/ChiaPatricia/MentalChat16K."}
{"id": "2506.00338", "pdf": "https://arxiv.org/pdf/2506.00338.pdf", "abs": "https://arxiv.org/abs/2506.00338", "title": "OWSM v4: Improving Open Whisper-Style Speech Models via Data Scaling and Cleaning", "authors": ["Yifan Peng", "Shakeel Muhammad", "Yui Sudo", "William Chen", "Jinchuan Tian", "Chyi-Jiunn Lin", "Shinji Watanabe"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted at INTERSPEECH 2025", "summary": "The Open Whisper-style Speech Models (OWSM) project has developed a series of\nfully open speech foundation models using academic-scale resources, but their\ntraining data remains insufficient. This work enhances OWSM by integrating\nYODAS, a large-scale web-crawled dataset with a Creative Commons license.\nHowever, incorporating YODAS is nontrivial due to its wild nature, which\nintroduces challenges such as incorrect language labels and audio-text\nmisalignments. To address this, we develop a scalable data-cleaning pipeline\nusing public toolkits, yielding a dataset with 166,000 hours of speech across\n75 languages. Our new series of OWSM v4 models, trained on this curated dataset\nalongside existing OWSM data, significantly outperform previous versions on\nmultilingual benchmarks. Our models even match or surpass frontier industrial\nmodels like Whisper and MMS in multiple scenarios. We will publicly release the\ncleaned YODAS data, pre-trained models, and all associated scripts via the\nESPnet toolkit."}
{"id": "2503.13975", "pdf": "https://arxiv.org/pdf/2503.13975.pdf", "abs": "https://arxiv.org/abs/2503.13975", "title": "Navigating Rifts in Human-LLM Grounding: Study and Benchmark", "authors": ["Omar Shaikh", "Hussein Mozannar", "Gagan Bansal", "Adam Fourney", "Eric Horvitz"], "categories": ["cs.CL", "cs.HC"], "comment": "ACL 2025, 16 pages, 5 figures", "summary": "Language models excel at following instructions but often struggle with the\ncollaborative aspects of conversation that humans naturally employ. This\nlimitation in grounding -- the process by which conversation participants\nestablish mutual understanding -- can lead to outcomes ranging from frustrated\nusers to serious consequences in high-stakes scenarios. To systematically study\ngrounding challenges in human-LLM interactions, we analyze logs from three\nhuman-assistant datasets: WildChat, MultiWOZ, and Bing Chat. We develop a\ntaxonomy of grounding acts and build models to annotate and forecast grounding\nbehavior. Our findings reveal significant differences in human-human and\nhuman-LLM grounding: LLMs were three times less likely to initiate\nclarification and sixteen times less likely to provide follow-up requests than\nhumans. Additionally, we find that early grounding failures predict later\ninteraction breakdowns. Building on these insights, we introduce Rifts, a\nbenchmark derived from publicly available LLM interaction data containing\nsituations where LLMs fail to initiate grounding. We note that current frontier\nmodels perform poorly on Rifts, highlighting the need to reconsider how we\ntrain and prompt LLMs for human interaction. To this end, we develop a\npreliminary intervention aimed at mitigating grounding failures."}
{"id": "2506.00344", "pdf": "https://arxiv.org/pdf/2506.00344.pdf", "abs": "https://arxiv.org/abs/2506.00344", "title": "Efficient Latent Semantic Clustering for Scaling Test-Time Computation of LLMs", "authors": ["Sungjae Lee", "Hoyoung Kim", "Jeongyeon Hwang", "Eunhyeok Park", "Jungseul Ok"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Scaling test-time computation--generating and analyzing multiple or\nsequential outputs for a single input--has become a promising strategy for\nimproving the reliability and quality of large language models (LLMs), as\nevidenced by advances in uncertainty quantification and multi-step reasoning. A\nkey shared component is semantic clustering, which groups outputs that differ\nin form but convey the same meaning. Semantic clustering enables estimation of\nthe distribution over the semantics of outputs and helps avoid redundant\nexploration of reasoning paths. However, existing approaches typically rely on\nexternal models, which introduce substantial computational overhead and often\nfail to capture context-aware semantics. We propose Latent Semantic Clustering\n(LSC), a lightweight and context-sensitive method that leverages the generator\nLLM's internal hidden states for clustering, eliminating the need for external\nmodels. Our extensive experiment across various LLMs and datasets shows that\nLSC significantly improves the computational efficiency of test-time scaling\nwhile maintaining or exceeding the performance of existing methods."}
{"id": "2505.01651", "pdf": "https://arxiv.org/pdf/2505.01651.pdf", "abs": "https://arxiv.org/abs/2505.01651", "title": "Human-AI Governance (HAIG): A Trust-Utility Approach", "authors": ["Zeynep Engin"], "categories": ["cs.AI", "cs.CY", "cs.HC", "cs.MA", "cs.SI"], "comment": "32 pages including references and appendix, 25 pages core text, 3\n  figures, 3 tables", "summary": "This paper introduces the HAIG framework for analysing trust dynamics across\nevolving human-AI relationships. Current categorical frameworks (e.g.,\n\"human-in-the-loop\" models) inadequately capture how AI systems evolve from\ntools to partners, particularly as foundation models demonstrate emergent\ncapabilities and multi-agent systems exhibit autonomous goal-setting\nbehaviours. As systems advance, agency redistributes in complex patterns that\nare better represented as positions along continua rather than discrete\ncategories, though progression may include both gradual shifts and significant\nstep changes. The HAIG framework operates across three levels: dimensions\n(Decision Authority Distribution, Process Autonomy, and Accountability\nConfiguration), continua (gradual shifts along each dimension), and thresholds\n(critical points requiring governance adaptation). Unlike risk-based or\nprinciple-based approaches, HAIG adopts a trust-utility orientation, focusing\non maintaining appropriate trust relationships that maximise utility while\nensuring sufficient safeguards. Our analysis reveals how technical advances in\nself-supervision, reasoning authority, and distributed decision-making drive\nnon-uniform trust evolution across both contextual variation and technological\nadvancement. Case studies in healthcare and European regulation demonstrate how\nHAIG complements existing frameworks while offering a foundation for\nalternative approaches that anticipate governance challenges before they\nemerge."}
{"id": "2506.00381", "pdf": "https://arxiv.org/pdf/2506.00381.pdf", "abs": "https://arxiv.org/abs/2506.00381", "title": "Neuro2Semantic: A Transfer Learning Framework for Semantic Reconstruction of Continuous Language from Human Intracranial EEG", "authors": ["Siavash Shams", "Richard Antonello", "Gavin Mischler", "Stephan Bickel", "Ashesh Mehta", "Nima Mesgarani"], "categories": ["cs.CL", "eess.AS", "eess.SP"], "comment": "Accepted at Interspeech 2025 Code at\n  https://github.com/SiavashShams/neuro2semantic", "summary": "Decoding continuous language from neural signals remains a significant\nchallenge in the intersection of neuroscience and artificial intelligence. We\nintroduce Neuro2Semantic, a novel framework that reconstructs the semantic\ncontent of perceived speech from intracranial EEG (iEEG) recordings. Our\napproach consists of two phases: first, an LSTM-based adapter aligns neural\nsignals with pre-trained text embeddings; second, a corrector module generates\ncontinuous, natural text directly from these aligned embeddings. This flexible\nmethod overcomes the limitations of previous decoding approaches and enables\nunconstrained text generation. Neuro2Semantic achieves strong performance with\nas little as 30 minutes of neural data, outperforming a recent state-of-the-art\nmethod in low-data settings. These results highlight the potential for\npractical applications in brain-computer interfaces and neural decoding\ntechnologies."}
{"id": "2505.12727", "pdf": "https://arxiv.org/pdf/2505.12727.pdf", "abs": "https://arxiv.org/abs/2505.12727", "title": "What is Stigma Attributed to? A Theory-Grounded, Expert-Annotated Interview Corpus for Demystifying Mental-Health Stigma", "authors": ["Han Meng", "Yancan Chen", "Yunan Li", "Yitian Yang", "Jungup Lee", "Renwen Zhang", "Yi-Chieh Lee"], "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": "Accepted to ACL 2025 Main Conference, 38 Pages", "summary": "Mental-health stigma remains a pervasive social problem that hampers\ntreatment-seeking and recovery. Existing resources for training neural models\nto finely classify such stigma are limited, relying primarily on social-media\nor synthetic data without theoretical underpinnings. To remedy this gap, we\npresent an expert-annotated, theory-informed corpus of human-chatbot\ninterviews, comprising 4,141 snippets from 684 participants with documented\nsocio-cultural backgrounds. Our experiments benchmark state-of-the-art neural\nmodels and empirically unpack the challenges of stigma detection. This dataset\ncan facilitate research on computationally detecting, neutralizing, and\ncounteracting mental-health stigma. Our corpus is openly available at\nhttps://github.com/HanMeng2004/Mental-Health-Stigma-Interview-Corpus."}
{"id": "2506.00386", "pdf": "https://arxiv.org/pdf/2506.00386.pdf", "abs": "https://arxiv.org/abs/2506.00386", "title": "Adaptive-VP: A Framework for LLM-Based Virtual Patients that Adapts to Trainees' Dialogue to Facilitate Nurse Communication Training", "authors": ["Keyeun Lee", "Seolhee Lee", "Esther Hehsun Kim", "Yena Ko", "Jinsu Eun", "Dahee Kim", "Hyewon Cho", "Haiyi Zhu", "Robert E. Kraut", "Eunyoung Suh", "Eun-mee Kim", "Hajin Lim"], "categories": ["cs.CL", "cs.HC"], "comment": "ACL 2025 Findings, 34 pages, 9 figures", "summary": "Effective communication training is essential to preparing nurses for\nhigh-quality patient care. While standardized patient (SP) simulations provide\nvaluable experiential learning, they are often costly and inflexible. Virtual\npatient (VP) systems offer a scalable alternative, but most fail to adapt to\nthe varying communication skills of trainees. In particular, when trainees\nrespond ineffectively, VPs should escalate in hostility or become\nuncooperative--yet this level of adaptive interaction remains largely\nunsupported. To address this gap, we introduce Adaptive-VP, a VP dialogue\ngeneration framework that leverages large language models (LLMs) to dynamically\nadapt VP behavior based on trainee input. The framework features a pipeline for\nconstructing clinically grounded yet flexible VP scenarios and a modular system\nfor assessing trainee communication and adjusting VP responses in real time,\nwhile ensuring learner safety. We validated Adaptive-VP by simulating\nchallenging patient conversations. Automated evaluation using a corpus from\npracticing nurses showed that our communication skill evaluation mechanism\nreflected real-world proficiency levels. Expert nurses further confirmed that\nAdaptive-VP produced more natural and realistic interactions than existing\napproaches, demonstrating its potential as a scalable and effective tool for\nnursing communication training."}
{"id": "2505.20011", "pdf": "https://arxiv.org/pdf/2505.20011.pdf", "abs": "https://arxiv.org/abs/2505.20011", "title": "The Many Challenges of Human-Like Agents in Virtual Game Environments", "authors": ["Maciej Swiechowski", "Dominik Slezak"], "categories": ["cs.AI", "cs.HC", "cs.MM", "68T01", "I.2; I.6.0; H.1.2"], "comment": "In proceedings of the 24th International Conference on Autonomous\n  Agents and Multiagent Systems (AAMAS-2025), pages 1996--2005, May 19-23,\n  Detroit, Michigan, USA", "summary": "Human-like agents are an increasingly important topic in games and beyond.\nBelievable non-player characters enhance the gaming experience by improving\nimmersion and providing entertainment. They also offer players the opportunity\nto engage with AI entities that can function as opponents, teachers, or\ncooperating partners. Additionally, in games where bots are prohibited -- and\neven more so in non-game environments -- there is a need for methods capable of\nidentifying whether digital interactions occur with bots or humans. This leads\nto two fundamental research questions: (1) how to model and implement\nhuman-like AI, and (2) how to measure its degree of human likeness.\n  This article offers two contributions. The first one is a survey of the most\nsignificant challenges in implementing human-like AI in games (or any virtual\nenvironment featuring simulated agents, although this article specifically\nfocuses on games). Thirteen such challenges, both conceptual and technical, are\ndiscussed in detail. The second is an empirical study performed in a tactical\nvideo game that addresses the research question: \"Is it possible to distinguish\nhuman players from bots (AI agents) based on empirical data?\" A\nmachine-learning approach using a custom deep recurrent convolutional neural\nnetwork is presented. We hypothesize that the more challenging it is to create\nhuman-like AI for a given game, the easier it becomes to develop a method for\ndistinguishing humans from AI-driven players."}
{"id": "2506.00391", "pdf": "https://arxiv.org/pdf/2506.00391.pdf", "abs": "https://arxiv.org/abs/2506.00391", "title": "SHARE: An SLM-based Hierarchical Action CorREction Assistant for Text-to-SQL", "authors": ["Ge Qu", "Jinyang Li", "Bowen Qin", "Xiaolong Li", "Nan Huo", "Chenhao Ma", "Reynold Cheng"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main", "summary": "Current self-correction approaches in text-to-SQL face two critical\nlimitations: 1) Conventional self-correction methods rely on recursive\nself-calls of LLMs, resulting in multiplicative computational overhead, and 2)\nLLMs struggle to implement effective error detection and correction for\ndeclarative SQL queries, as they fail to demonstrate the underlying reasoning\npath. In this work, we propose SHARE, an SLM-based Hierarchical Action\ncorREction assistant that enables LLMs to perform more precise error\nlocalization and efficient correction. SHARE orchestrates three specialized\nSmall Language Models (SLMs) in a sequential pipeline, where it first\ntransforms declarative SQL queries into stepwise action trajectories that\nreveal underlying reasoning, followed by a two-phase granular refinement. We\nfurther propose a novel hierarchical self-evolution strategy for data-efficient\ntraining. Experimental results demonstrate that SHARE effectively enhances\nself-correction capabilities while proving robust across various LLMs.\nFurthermore, our comprehensive analysis shows that SHARE maintains strong\nperformance even in low-resource training settings, which is particularly\nvaluable for text-to-SQL applications with data privacy constraints."}
{"id": "2505.21907", "pdf": "https://arxiv.org/pdf/2505.21907.pdf", "abs": "https://arxiv.org/abs/2505.21907", "title": "Modeling and Optimizing User Preferences in AI Copilots: A Comprehensive Survey and Taxonomy", "authors": ["Saleh Afzoon", "Zahra Jahanandish", "Phuong Thao Huynh", "Amin Beheshti", "Usman Naseem"], "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "AI copilots represent a new generation of AI-powered systems designed to\nassist users, particularly knowledge workers and developers, in complex,\ncontext-rich tasks. As these systems become more embedded in daily workflows,\npersonalization has emerged as a critical factor for improving usability,\neffectiveness, and user satisfaction. Central to this personalization is\npreference optimization: the system's ability to detect, interpret, and align\nwith individual user preferences. While prior work in intelligent assistants\nand optimization algorithms is extensive, their intersection within AI copilots\nremains underexplored. This survey addresses that gap by examining how user\npreferences are operationalized in AI copilots. We investigate how preference\nsignals are sourced, modeled across different interaction stages, and refined\nthrough feedback loops. Building on a comprehensive literature review, we\ndefine the concept of an AI copilot and introduce a taxonomy of preference\noptimization techniques across pre-, mid-, and post-interaction phases. Each\ntechnique is evaluated in terms of advantages, limitations, and design\nimplications. By consolidating fragmented efforts across AI personalization,\nhuman-AI interaction, and language model adaptation, this work offers both a\nunified conceptual foundation and a practical design perspective for building\nuser-aligned, persona-aware AI copilots that support end-to-end adaptability\nand deployment."}
{"id": "2506.00396", "pdf": "https://arxiv.org/pdf/2506.00396.pdf", "abs": "https://arxiv.org/abs/2506.00396", "title": "Speculative Reward Model Boosts Decision Making Ability of LLMs Cost-Effectively", "authors": ["Jiawei Gu", "Shangsong Liang"], "categories": ["cs.CL"], "comment": "ACL2025 Oral (Industry Track)", "summary": "Effective decision-making in Large Language Models (LLMs) is essential for\nhandling intricate tasks. However, existing approaches prioritize performance\nbut often overlook the balance between effectiveness and computational cost. To\naddress this, we first introduce the 3E Criteria to systematically assess the\ncost-effectiveness of search strategies, revealing that existing methods often\ntrade significant efficiency for marginal performance gains. To improve LLM\ndecision-making while maintaining efficiency, we propose the Speculative Reward\nModel (SRM), a plug-and-play framework that seamlessly integrates with existing\nsearch strategies. Specifically, SRM employs an external reward assigner to\npredict optimal actions, reducing reliance on LLMs' internal self-evaluation.\nAnd a speculative verification mechanism is used to prune suboptimal choices\nand guide the search toward more promising steps. We evaluate SRM on several\ncomplex decision-making tasks including mathematical reasoning, planning and\nnumerical reasoning in specialized domains. Experimental results show that SRM\nreduces costs to 1/10 of the original search framework on average while\nmaintaining effectiveness."}
{"id": "2505.23405", "pdf": "https://arxiv.org/pdf/2505.23405.pdf", "abs": "https://arxiv.org/abs/2505.23405", "title": "A Practical Guide for Supporting Formative Assessment and Feedback Using Generative AI", "authors": ["Sapolnach Prompiengchai", "Charith Narreddy", "Steve Joordens"], "categories": ["cs.CY", "cs.HC"], "comment": null, "summary": "Formative assessment is a cornerstone of effective teaching and learning,\nproviding students with feedback to guide their learning. While there has been\nan exponential growth in the application of generative AI in scaling various\naspects of formative assessment, ranging from automatic question generation to\nintelligent tutoring systems and personalized feedback, few have directly\naddressed the core pedagogical principles of formative assessment. Here, we\ncritically examined how generative AI, especially large-language models (LLMs)\nsuch as ChatGPT, can support key components of formative assessment: helping\nstudents, teachers, and peers understand \"where learners are going,\" \"where\nlearners currently are,\" and \"how to move learners forward\" in the learning\nprocess. With the rapid emergence of new prompting techniques and LLM\ncapabilities, we also provide guiding principles for educators to effectively\nleverage cost-free LLMs in formative assessments while remaining grounded in\npedagogical best practices. Furthermore, we reviewed the role of LLMs in\ngenerating feedback, highlighting limitations in current evaluation metrics\nthat inadequately capture the nuances of formative feedback, such as\ndistinguishing feedback at the task, process, and self-regulatory levels.\nFinally, we offer practical guidelines for educators and researchers, including\nconcrete classroom strategies and future directions such as developing robust\nmetrics to assess LLM-generated feedback, leveraging LLMs to overcome systemic\nand cultural barriers to formative assessment, and designing AI-aware\nassessment strategies that promote transferable skills while mitigating\noverreliance on LLM-generated responses. By structuring the discussion within\nan established formative assessment framework, this review provides a\ncomprehensive foundation for integrating LLMs into formative assessment in a\npedagogically informed manner."}
{"id": "2506.00400", "pdf": "https://arxiv.org/pdf/2506.00400.pdf", "abs": "https://arxiv.org/abs/2506.00400", "title": "Scaling Textual Gradients via Sampling-Based Momentum", "authors": ["Zixin Ding", "Junyuan Hong", "Jiachen T. Wang", "Zinan Lin", "Zhangyang Wang", "Yuxin Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As prompts play an increasingly critical role in large language models\n(LLMs), optimizing textual prompts has become a crucial challenge. The Textual\nGradient Descent (TGD) framework has emerged as a promising data-driven\napproach that iteratively refines textual prompts using LLM - suggested updates\n(or textual gradients) over minibatches of training samples. In this paper, we\nempirically demonstrate that scaling the number of training examples initially\nimproves but later degrades TGD's performance across multiple downstream NLP\ntasks. However, while data scaling improves results for most tasks, it also\nsignificantly increases the computational cost when leveraging LLMs. To address\nthis, we draw inspiration from numerical gradient descent and propose Textual\nStochastic Gradient Descent with Momentum (TSGD-M) - a method that facilitates\nscalable in-context learning by reweighting prompt sampling based on past batch\ndistributions. Across nine NLP tasks spanning three domains - including\nBIG-Bench Hard (BBH), natural language understanding tasks, and reasoning tasks\n- TSGD-M significantly outperforms TGD baselines that do not incorporate\nreweighted sampling, while also reducing variance in most tasks."}
{"id": "2505.23516", "pdf": "https://arxiv.org/pdf/2505.23516.pdf", "abs": "https://arxiv.org/abs/2505.23516", "title": "The CASE Framework -- A New Architecture for Participatory Research and Digital Health Surveillance", "authors": ["Marco Hirsch", "Peter Hevesi", "Paul Lukowicz"], "categories": ["cs.SE", "cs.CY", "cs.HC", "J.3; D.2.11; H.3.5"], "comment": "10 pages, 5 figures. Submitted as a preprint to arXiv (no prior\n  publication)", "summary": "We present the CASE framework, an open-source platform for adaptive,\ncontext-aware participatory research, and pandemic preparedness. CASE\nimplements an event-driven architecture that enables dynamic survey workflows,\nallowing real-time adaptation based on participant responses, external data,\ntemporal conditions, and evolving user states. The framework supports a broad\nrange of research needs, from simple one-time questionnaires to complex\nlongitudinal studies with advanced conditional logic. Built on over a decade of\npractical experience, CASE underwent a major architectural rework in 2024,\ntransitioning from a microservice-based design to a streamlined monolithic\narchitecture. This evolution significantly improved maintainability,\nflexibility, and accessibility to deployment, particularly for institutions\nwith limited technical capacity. CASE has been successfully deployed across\ndiverse domains, powering national disease surveillance platforms, supporting\npost-COVID cohort studies, and enabling real-time sentiment analysis during\npolitical events. These applications, involving tens of thousands of\nparticipants, demonstrate the framework's scalability, versatility, and\npractical value. This paper describes the foundations of CASE, details its\narchitectural evolution, and presents lessons learned from real-world\ndeployments. We establish CASE as a mature and reusable research infrastructure\nthat balances sophisticated functionality with practical implementation,\naddressing the critical global need for sustainable and institutionally\ncontrolled data collection systems."}
{"id": "2506.00402", "pdf": "https://arxiv.org/pdf/2506.00402.pdf", "abs": "https://arxiv.org/abs/2506.00402", "title": "Causal Structure Discovery for Error Diagnostics of Children's ASR", "authors": ["Vishwanath Pratap Singh", "Md. Sahidullah", "Tomi Kinnunen"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Interspeech 2025", "summary": "Children's automatic speech recognition (ASR) often underperforms compared to\nthat of adults due to a confluence of interdependent factors: physiological\n(e.g., smaller vocal tracts), cognitive (e.g., underdeveloped pronunciation),\nand extrinsic (e.g., vocabulary limitations, background noise). Existing\nanalysis methods examine the impact of these factors in isolation, neglecting\ninterdependencies-such as age affecting ASR accuracy both directly and\nindirectly via pronunciation skills. In this paper, we introduce a causal\nstructure discovery to unravel these interdependent relationships among\nphysiology, cognition, extrinsic factors, and ASR errors. Then, we employ\ncausal quantification to measure each factor's impact on children's ASR. We\nextend the analysis to fine-tuned models to identify which factors are\nmitigated by fine-tuning and which remain largely unaffected. Experiments on\nWhisper and Wav2Vec2.0 demonstrate the generalizability of our findings across\ndifferent ASR systems."}
{"id": "2505.23576", "pdf": "https://arxiv.org/pdf/2505.23576.pdf", "abs": "https://arxiv.org/abs/2505.23576", "title": "Cognitive Guardrails for Open-World Decision Making in Autonomous Drone Swarms", "authors": ["Jane Cleland-Huang", "Pedro Antonio Alarcon Granadeno", "Arturo Miguel Russell Bernal", "Demetrius Hernandez", "Michael Murphy", "Maureen Petterson", "Walter Scheirer"], "categories": ["cs.RO", "cs.AI", "cs.HC"], "comment": "16 pages, 8 figures", "summary": "Small Uncrewed Aerial Systems (sUAS) are increasingly deployed as autonomous\nswarms in search-and-rescue and other disaster-response scenarios. In these\nsettings, they use computer vision (CV) to detect objects of interest and\nautonomously adapt their missions. However, traditional CV systems often\nstruggle to recognize unfamiliar objects in open-world environments or to infer\ntheir relevance for mission planning. To address this, we incorporate large\nlanguage models (LLMs) to reason about detected objects and their implications.\nWhile LLMs can offer valuable insights, they are also prone to hallucinations\nand may produce incorrect, misleading, or unsafe recommendations. To ensure\nsafe and sensible decision-making under uncertainty, high-level decisions must\nbe governed by cognitive guardrails. This article presents the design,\nsimulation, and real-world integration of these guardrails for sUAS swarms in\nsearch-and-rescue missions."}
{"id": "2506.00413", "pdf": "https://arxiv.org/pdf/2506.00413.pdf", "abs": "https://arxiv.org/abs/2506.00413", "title": "Accelerating Diffusion LLMs via Adaptive Parallel Decoding", "authors": ["Daniel Israel", "Guy Van den Broeck", "Aditya Grover"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PF"], "comment": "10 pages, 5 figures", "summary": "The generation speed of LLMs are bottlenecked by autoregressive decoding,\nwhere tokens are predicted sequentially one by one. Alternatively, diffusion\nlarge language models (dLLMs) theoretically allow for parallel token\ngeneration, but in practice struggle to achieve the speed of autoregressive\nmodels without significantly sacrificing quality. We therefore introduce\nadaptive parallel decoding (APD), a novel method that dynamically adjusts the\nnumber of tokens sampled in parallel. We achieve this by defining a\nmultiplicative mixture between the dLLM marginal probabilities and the joint\nprobability of sequences under a small auxiliary autoregressive model. This\ninverts the standard setup of speculative decoding, where the goal is to sample\nfrom a large autoregressive verifier by drafting from a smaller model. We\nfurther optimize APD by enabling KV caching and limiting the size of the masked\ninput. Altogether, our method puts forward three tunable parameters to flexibly\ntradeoff throughput and quality. We show that APD provides markedly higher\nthroughput with minimal quality degradations on downstream benchmarks."}
{"id": "2505.23799", "pdf": "https://arxiv.org/pdf/2505.23799.pdf", "abs": "https://arxiv.org/abs/2505.23799", "title": "Estimating LLM Consistency: A User Baseline vs Surrogate Metrics", "authors": ["Xiaoyuan Wu", "Weiran Lin", "Omer Akgul", "Lujo Bauer"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) are prone to hallucinations and sensitive to\nprompt perturbations, often resulting in inconsistent or unreliable generated\ntext. Different methods have been proposed to mitigate such hallucinations and\nfragility -- one of them being measuring the consistency (the model's\nconfidence in the response, or likelihood of generating a similar response when\nresampled) of LLM responses. In previous work, measuring consistency often\nrelied on the probability of a response appearing within a pool of resampled\nresponses, or internal states or logits of responses. However, it is not yet\nclear how well these approaches approximate how humans perceive the consistency\nof LLM responses. We performed a user study (n=2,976) and found current methods\ntypically do not approximate users' perceptions of LLM consistency very well.\nWe propose a logit-based ensemble method for estimating LLM consistency, and we\nshow that this method matches the performance of the best-performing existing\nmetric in estimating human ratings of LLM consistency. Our results suggest that\nmethods of estimating LLM consistency without human evaluation are sufficiently\nimperfect that we suggest evaluation with human input be more broadly used."}
{"id": "2506.00418", "pdf": "https://arxiv.org/pdf/2506.00418.pdf", "abs": "https://arxiv.org/abs/2506.00418", "title": "Dual Debiasing for Noisy In-Context Learning for Text Generation", "authors": ["Siqi Liang", "Sumyeong Ahn", "Paramveer S. Dhillon", "Jiayu Zhou"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "Accepted by 2025 ACL Findings", "summary": "In context learning (ICL) relies heavily on high quality demonstrations drawn\nfrom large annotated corpora. Existing approaches detect noisy annotations by\nranking local perplexities, presuming that noisy samples yield higher\nperplexities than their clean counterparts. However, this assumption breaks\ndown when the noise ratio is high and many demonstrations are flawed. We\nreexamine the perplexity based paradigm for text generation under noisy\nannotations, highlighting two sources of bias in perplexity: the annotation\nitself and the domain specific knowledge inherent in large language models\n(LLMs). To overcome these biases, we introduce a dual debiasing framework that\nuses synthesized neighbors to explicitly correct perplexity estimates, yielding\na robust Sample Cleanliness Score. This metric uncovers absolute sample\ncleanliness regardless of the overall corpus noise level. Extensive experiments\ndemonstrate our method's superior noise detection capabilities and show that\nits final ICL performance is comparable to that of a fully clean demonstration\ncorpus. Moreover, our approach remains robust even when noise ratios are\nextremely high."}
{"id": "2505.24803", "pdf": "https://arxiv.org/pdf/2505.24803.pdf", "abs": "https://arxiv.org/abs/2505.24803", "title": "Guiding Generative Storytelling with Knowledge Graphs", "authors": ["Zhijun Pan", "Antonios Andronis", "Eva Hayek", "Oscar AP Wilkinson", "Ilya Lasy", "Annette Parry", "Guy Gadney", "Tim J. Smith", "Mick Grierson"], "categories": ["cs.CL", "cs.HC"], "comment": "This manuscript was submitted for peer review in January 2025", "summary": "Large Language Models (LLMs) have shown great potential in automated story\ngeneration, but challenges remain in maintaining long-form coherence and\nproviding users with intuitive and effective control. Retrieval-Augmented\nGeneration (RAG) has proven effective in reducing hallucinations in text\ngeneration; however, the use of structured data to support generative\nstorytelling remains underexplored. This paper investigates how knowledge\ngraphs (KGs) can enhance LLM-based storytelling by improving narrative quality\nand enabling user-driven modifications. We propose a KG-assisted storytelling\npipeline and evaluate its effectiveness through a user study with 15\nparticipants. Participants created their own story prompts, generated stories,\nand edited knowledge graphs to shape their narratives. Through quantitative and\nqualitative analysis, our findings demonstrate that knowledge graphs\nsignificantly enhance story quality in action-oriented and structured\nnarratives within our system settings. Additionally, editing the knowledge\ngraph increases users' sense of control, making storytelling more engaging,\ninteractive, and playful."}
{"id": "2506.00421", "pdf": "https://arxiv.org/pdf/2506.00421.pdf", "abs": "https://arxiv.org/abs/2506.00421", "title": "Enabling Chatbots with Eyes and Ears: An Immersive Multimodal Conversation System for Dynamic Interactions", "authors": ["Jihyoung Jang", "Minwook Bae", "Minji Kim", "Dilek Hakkani-Tur", "Hyounghun Kim"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "ACL 2025 (32 pages); Project website: https://m3c-dataset.github.io/", "summary": "As chatbots continue to evolve toward human-like, real-world, interactions,\nmultimodality remains an active area of research and exploration. So far,\nefforts to integrate multimodality into chatbots have primarily focused on\nimage-centric tasks, such as visual dialogue and image-based instructions,\nplacing emphasis on the \"eyes\" of human perception while neglecting the \"ears\",\nnamely auditory aspects. Moreover, these studies often center around static\ninteractions that focus on discussing the modality rather than naturally\nincorporating it into the conversation, which limits the richness of\nsimultaneous, dynamic engagement. Furthermore, while multimodality has been\nexplored in multi-party and multi-session conversations, task-specific\nconstraints have hindered its seamless integration into dynamic, natural\nconversations. To address these challenges, this study aims to equip chatbots\nwith \"eyes and ears\" capable of more immersive interactions with humans. As\npart of this effort, we introduce a new multimodal conversation dataset,\nMultimodal Multi-Session Multi-Party Conversation ($M^3C$), and propose a novel\nmultimodal conversation model featuring multimodal memory retrieval. Our model,\ntrained on the $M^3C$, demonstrates the ability to seamlessly engage in\nlong-term conversations with multiple speakers in complex, real-world-like\nsettings, effectively processing visual and auditory inputs to understand and\nrespond appropriately. Human evaluations highlight the model's strong\nperformance in maintaining coherent and dynamic interactions, demonstrating its\npotential for advanced multimodal conversational agents."}
{"id": "2506.00422", "pdf": "https://arxiv.org/pdf/2506.00422.pdf", "abs": "https://arxiv.org/abs/2506.00422", "title": "DYNAC: Dynamic Vocabulary based Non-Autoregressive Contextualization for Speech Recognition", "authors": ["Yui Sudo", "Yosuke Fukumoto", "Muhammad Shakeel", "Yifan Peng", "Chyi-Jiunn Lin", "Shinji Watanabe"], "categories": ["cs.CL"], "comment": "Accepted to Interspeech 2025", "summary": "Contextual biasing (CB) improves automatic speech recognition for rare and\nunseen phrases. Recent studies have introduced dynamic vocabulary, which\nrepresents context phrases as expandable tokens in autoregressive (AR) models.\nThis method improves CB accuracy but with slow inference speed. While dynamic\nvocabulary can be applied to non-autoregressive (NAR) models, such as\nconnectionist temporal classification (CTC), the conditional independence\nassumption fails to capture dependencies between static and dynamic tokens.\nThis paper proposes DYNAC (Dynamic Vocabulary-based NAR Contextualization), a\nself-conditioned CTC method that integrates dynamic vocabulary into\nintermediate layers. Conditioning the encoder on dynamic vocabulary, DYNAC\neffectively captures dependencies between static and dynamic tokens while\nreducing the real-time factor (RTF). Experimental results show that DYNAC\nreduces RTF by 81% with a 0.1-point degradation in word error rate on the\nLibriSpeech 960 test-clean set."}
{"id": "2506.00425", "pdf": "https://arxiv.org/pdf/2506.00425.pdf", "abs": "https://arxiv.org/abs/2506.00425", "title": "Inter-Passage Verification for Multi-evidence Multi-answer QA", "authors": ["Bingsen Chen", "Shengjie Wang", "Xi Ye", "Chen Zhao"], "categories": ["cs.CL"], "comment": "19 pages, 6 figures, to appear in ACL 2025 Findings", "summary": "Multi-answer question answering (QA), where questions can have many valid\nanswers, presents a significant challenge for existing retrieval-augmented\ngeneration-based QA systems, as these systems struggle to retrieve and then\nsynthesize a large number of evidence passages. To tackle these challenges, we\npropose a new multi-answer QA framework -- Retrieval-augmented Independent\nReading with Inter-passage Verification (RI$^2$VER). Our framework retrieves a\nlarge set of passages and processes each passage individually to generate an\ninitial high-recall but noisy answer set. Then we propose a new inter-passage\nverification pipeline that validates every candidate answer through (1)\nVerification Question Generation, (2) Gathering Additional Evidence, and (3)\nVerification with inter-passage synthesis. Evaluations on the QAMPARI and RoMQA\ndatasets demonstrate that our framework significantly outperforms existing\nbaselines across various model sizes, achieving an average F1 score improvement\nof 11.17%. Further analysis validates that our inter-passage verification\npipeline enables our framework to be particularly beneficial for questions\nrequiring multi-evidence synthesis."}
{"id": "2506.00445", "pdf": "https://arxiv.org/pdf/2506.00445.pdf", "abs": "https://arxiv.org/abs/2506.00445", "title": "G2S: A General-to-Specific Learning Framework for Temporal Knowledge Graph Forecasting with Large Language Models", "authors": ["Long Bai", "Zixuan Li", "Xiaolong Jin", "Jiafeng Guo", "Xueqi Cheng", "Tat-Seng Chua"], "categories": ["cs.CL"], "comment": "Findings of ACL 2025", "summary": "Forecasting over Temporal Knowledge Graphs (TKGs) which predicts future facts\nbased on historical ones has received much attention. Recent studies have\nintroduced Large Language Models (LLMs) for this task to enhance the models'\ngeneralization abilities. However, these models perform forecasting via\nsimultaneously learning two kinds of entangled knowledge in the TKG: (1)\ngeneral patterns, i.e., invariant temporal structures shared across different\nscenarios; and (2) scenario information, i.e., factual knowledge engaged in\nspecific scenario, such as entities and relations. As a result, the learning\nprocesses of these two kinds of knowledge may interfere with each other, which\npotentially impact the generalization abilities of the models. To enhance the\ngeneralization ability of LLMs on this task, in this paper, we propose a\nGeneral-to-Specific learning framework (G2S) that disentangles the learning\nprocesses of the above two kinds of knowledge. In the general learning stage,\nwe mask the scenario information in different TKGs and convert it into\nanonymous temporal structures. After training on these structures, the model is\nable to capture the general patterns across different TKGs. In the specific\nlearning stage, we inject the scenario information into the structures via\neither in-context learning or fine-tuning modes. Experimental results show that\nG2S effectively improves the generalization abilities of LLMs."}
{"id": "2506.00448", "pdf": "https://arxiv.org/pdf/2506.00448.pdf", "abs": "https://arxiv.org/abs/2506.00448", "title": "Fact-Controlled Diagnosis of Hallucinations in Medical Text Summarization", "authors": ["Suhas BN", "Han-Chin Shing", "Lei Xu", "Mitch Strong", "Jon Burnsky", "Jessica Ofor", "Jordan R. Mason", "Susan Chen", "Sundararajan Srinivasan", "Chaitanya Shivade", "Jack Moriarty", "Joseph Paul Cohen"], "categories": ["cs.CL"], "comment": "https://github.com/amazon-science/acibench-hallucination-annotations", "summary": "Hallucinations in large language models (LLMs) during summarization of\npatient-clinician dialogues pose significant risks to patient care and clinical\ndecision-making. However, the phenomenon remains understudied in the clinical\ndomain, with uncertainty surrounding the applicability of general-domain\nhallucination detectors. The rarity and randomness of hallucinations further\ncomplicate their investigation. In this paper, we conduct an evaluation of\nhallucination detection methods in the medical domain, and construct two\ndatasets for the purpose: A fact-controlled Leave-N-out dataset -- generated by\nsystematically removing facts from source dialogues to induce hallucinated\ncontent in summaries; and a natural hallucination dataset -- arising\norganically during LLM-based medical summarization. We show that general-domain\ndetectors struggle to detect clinical hallucinations, and that performance on\nfact-controlled hallucinations does not reliably predict effectiveness on\nnatural hallucinations. We then develop fact-based approaches that count\nhallucinations, offering explainability not available with existing methods.\nNotably, our LLM-based detectors, which we developed using fact-controlled\nhallucinations, generalize well to detecting real-world clinical\nhallucinations. This research contributes a suite of specialized metrics\nsupported by expert-annotated datasets to advance faithful clinical\nsummarization systems."}
{"id": "2506.00469", "pdf": "https://arxiv.org/pdf/2506.00469.pdf", "abs": "https://arxiv.org/abs/2506.00469", "title": "Massively Multilingual Adaptation of Large Language Models Using Bilingual Translation Data", "authors": ["Shaoxiong Ji", "Zihao Li", "Jaakko Paavola", "Indraneil Paul", "Hengyu Luo", "J√∂rg Tiedemann"], "categories": ["cs.CL"], "comment": "EMMA-500 Gen 2; refer to Gen 1 in arXiv:2409.17892", "summary": "This paper investigates a critical design decision in the practice of\nmassively multilingual continual pre-training -- the inclusion of parallel\ndata. Specifically, we study the impact of bilingual translation data for\nmassively multilingual language adaptation of the Llama3 family of models to\n500 languages. To this end, we construct the MaLA bilingual translation corpus,\ncontaining data from more than 2,500 language pairs. Subsequently, we develop\nthe EMMA-500 Llama 3 suite of four massively multilingual models -- continually\npre-trained from the Llama 3 family of base models extensively on diverse data\nmixes up to 671B tokens -- and explore the effect of continual pre-training\nwith or without bilingual translation data. Comprehensive evaluation across 7\ntasks and 12 benchmarks demonstrates that bilingual data tends to enhance\nlanguage transfer and performance, particularly for low-resource languages. We\nopen-source the MaLA corpus, EMMA-500 Llama 3 suite artefacts, code, and model\ngenerations."}
{"id": "2506.00479", "pdf": "https://arxiv.org/pdf/2506.00479.pdf", "abs": "https://arxiv.org/abs/2506.00479", "title": "EffiVLM-BENCH: A Comprehensive Benchmark for Evaluating Training-Free Acceleration in Large Vision-Language Models", "authors": ["Zekun Wang", "Minghua Ma", "Zexin Wang", "Rongchuan Mu", "Liping Shan", "Ming Liu", "Bing Qin"], "categories": ["cs.CL", "cs.CV", "cs.LG"], "comment": "ACL 2025", "summary": "Large Vision-Language Models (LVLMs) have achieved remarkable success, yet\ntheir significant computational demands hinder practical deployment. While\nefforts to improve LVLM efficiency are growing, existing methods lack\ncomprehensive evaluation across diverse backbones, benchmarks, and metrics. In\nthis work, we systematically evaluate mainstream acceleration techniques for\nLVLMs, categorized into token and parameter compression. We introduce\nEffiVLM-Bench, a unified framework for assessing not only absolute performance\nbut also generalization and loyalty, while exploring Pareto-optimal trade-offs.\nOur extensive experiments and in-depth analyses offer insights into optimal\nstrategies for accelerating LVLMs. We open-source code and recipes for\nEffiVLM-Bench to foster future research."}
{"id": "2506.00481", "pdf": "https://arxiv.org/pdf/2506.00481.pdf", "abs": "https://arxiv.org/abs/2506.00481", "title": "PVP: An Image Dataset for Personalized Visual Persuasion with Persuasion Strategies, Viewer Characteristics, and Persuasiveness Ratings", "authors": ["Junseo Kim", "Jongwook Han", "Dongmin Choi", "Jongwook Yoon", "Eun-Ju Lee", "Yohan Jo"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Main. Code and dataset are released at:\n  https://github.com/holi-lab/PVP_Personalized_Visual_Persuasion", "summary": "Visual persuasion, which uses visual elements to influence cognition and\nbehaviors, is crucial in fields such as advertising and political\ncommunication. With recent advancements in artificial intelligence, there is\ngrowing potential to develop persuasive systems that automatically generate\npersuasive images tailored to individuals. However, a significant bottleneck in\nthis area is the lack of comprehensive datasets that connect the persuasiveness\nof images with the personal information about those who evaluated the images.\nTo address this gap and facilitate technological advancements in personalized\nvisual persuasion, we release the Personalized Visual Persuasion (PVP) dataset,\ncomprising 28,454 persuasive images across 596 messages and 9 persuasion\nstrategies. Importantly, the PVP dataset provides persuasiveness scores of\nimages evaluated by 2,521 human annotators, along with their demographic and\npsychological characteristics (personality traits and values). We demonstrate\nthe utility of our dataset by developing a persuasive image generator and an\nautomated evaluator, and establish benchmark baselines. Our experiments reveal\nthat incorporating psychological characteristics enhances the generation and\nevaluation of persuasive images, providing valuable insights for personalized\nvisual persuasion."}
{"id": "2506.00483", "pdf": "https://arxiv.org/pdf/2506.00483.pdf", "abs": "https://arxiv.org/abs/2506.00483", "title": "Auto-Patching: Enhancing Multi-Hop Reasoning in Language Models", "authors": ["Aviv Jan", "Dean Tahory", "Omer Talmi", "Omar Abo Mokh"], "categories": ["cs.CL", "cs.LG"], "comment": "8 pages, 5 figures", "summary": "Multi-hop questions still stump large language models (LLMs), which struggle\nto link information across multiple reasoning steps. We introduce Auto-Patch, a\nnovel method that dynamically patches hidden states during inference to enhance\nmulti-hop reasoning in LLMs. Building on the PatchScopes framework, Auto-Patch\nselectively modifies internal representations using a learned classifier.\nEvaluated on the MuSiQue dataset, Auto-Patch improves the solve rate from\n18.45\\% (baseline) to 23.63~$\\pm$~0.7\\% (3 runs), narrowing the gap to\nChain-of-Thought prompting (27.44\\%). Our results highlight the potential of\ndynamic hidden state interventions for advancing complex reasoning in LLMs."}
{"id": "2506.00488", "pdf": "https://arxiv.org/pdf/2506.00488.pdf", "abs": "https://arxiv.org/abs/2506.00488", "title": "Synergizing LLMs with Global Label Propagation for Multimodal Fake News Detection", "authors": ["Shuguo Hu", "Jun Hu", "Huaiwen Zhang"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 Main Conference", "summary": "Large Language Models (LLMs) can assist multimodal fake news detection by\npredicting pseudo labels. However, LLM-generated pseudo labels alone\ndemonstrate poor performance compared to traditional detection methods, making\ntheir effective integration non-trivial. In this paper, we propose Global Label\nPropagation Network with LLM-based Pseudo Labeling (GLPN-LLM) for multimodal\nfake news detection, which integrates LLM capabilities via label propagation\ntechniques. The global label propagation can utilize LLM-generated pseudo\nlabels, enhancing prediction accuracy by propagating label information among\nall samples. For label propagation, a mask-based mechanism is designed to\nprevent label leakage during training by ensuring that training nodes do not\npropagate their own labels back to themselves. Experimental results on\nbenchmark datasets show that by synergizing LLMs with label propagation, our\nmodel achieves superior performance over state-of-the-art baselines."}
{"id": "2506.00507", "pdf": "https://arxiv.org/pdf/2506.00507.pdf", "abs": "https://arxiv.org/abs/2506.00507", "title": "Exploring In-context Example Generation for Machine Translation", "authors": ["Dohyun Lee", "Seungil Chad Lee", "Chanwoo Yang", "Yujin Baek", "Jaegul Choo"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Findings", "summary": "Large language models (LLMs) have demonstrated strong performance across\nvarious tasks, leveraging their exceptional in-context learning ability with\nonly a few examples. Accordingly, the selection of optimal in-context examples\nhas been actively studied in the field of machine translation. However, these\nstudies presuppose the presence of a demonstration pool with human-annotated\npairs, making them less applicable to low-resource languages where such an\nassumption is challenging to meet. To overcome this limitation, this paper\nexplores the research direction of in-context example generation for machine\ntranslation. Specifically, we propose Demonstration Augmentation for\nTranslation (DAT), a simple yet effective approach that generates example pairs\nwithout relying on any external resources. This method builds upon two prior\ncriteria, relevance and diversity, which have been highlighted in previous work\nas key factors for in-context example selection. Through experiments and\nanalysis on low-resource languages where human-annotated pairs are scarce, we\nshow that DAT achieves superior translation quality compared to the baselines.\nFurthermore, we investigate the potential of progressively accumulating\ngenerated pairs during test time to build and reuse a demonstration pool. Our\nimplementation is publicly available at https://github.com/aiclaudev/DAT."}
{"id": "2506.00509", "pdf": "https://arxiv.org/pdf/2506.00509.pdf", "abs": "https://arxiv.org/abs/2506.00509", "title": "Goal-Aware Identification and Rectification of Misinformation in Multi-Agent Systems", "authors": ["Zherui Li", "Yan Mi", "Zhenhong Zhou", "Houcheng Jiang", "Guibin Zhang", "Kun Wang", "Junfeng Fang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Model-based Multi-Agent Systems (MASs) have demonstrated\nstrong advantages in addressing complex real-world tasks. However, due to the\nintroduction of additional attack surfaces, MASs are particularly vulnerable to\nmisinformation injection. To facilitate a deeper understanding of\nmisinformation propagation dynamics within these systems, we introduce\nMisinfoTask, a novel dataset featuring complex, realistic tasks designed to\nevaluate MAS robustness against such threats. Building upon this, we propose\nARGUS, a two-stage, training-free defense framework leveraging goal-aware\nreasoning for precise misinformation rectification within information flows.\nOur experiments demonstrate that in challenging misinformation scenarios, ARGUS\nexhibits significant efficacy across various injection attacks, achieving an\naverage reduction in misinformation toxicity of approximately 28.17% and\nimproving task success rates under attack by approximately 10.33%. Our code and\ndataset is available at: https://github.com/zhrli324/ARGUS."}
{"id": "2506.00514", "pdf": "https://arxiv.org/pdf/2506.00514.pdf", "abs": "https://arxiv.org/abs/2506.00514", "title": "Evaluating the Evaluation of Diversity in Commonsense Generation", "authors": ["Tianhui Zhang", "Bei Peng", "Danushka Bollegala"], "categories": ["cs.CL"], "comment": "ACL 2025 Main", "summary": "In commonsense generation, given a set of input concepts, a model must\ngenerate a response that is not only commonsense bearing, but also capturing\nmultiple diverse viewpoints. Numerous evaluation metrics based on form- and\ncontent-level overlap have been proposed in prior work for evaluating the\ndiversity of a commonsense generation model. However, it remains unclear as to\nwhich metrics are best suited for evaluating the diversity in commonsense\ngeneration. To address this gap, we conduct a systematic meta-evaluation of\ndiversity metrics for commonsense generation. We find that form-based diversity\nmetrics tend to consistently overestimate the diversity in sentence sets, where\neven randomly generated sentences are assigned overly high diversity scores. We\nthen use an Large Language Model (LLM) to create a novel dataset annotated for\nthe diversity of sentences generated for a commonsense generation task, and use\nit to conduct a meta-evaluation of the existing diversity evaluation metrics.\nOur experimental results show that content-based diversity evaluation metrics\nconsistently outperform the form-based counterparts, showing high correlations\nwith the LLM-based ratings. We recommend that future work on commonsense\ngeneration should use content-based metrics for evaluating the diversity of\ntheir outputs."}
{"id": "2506.00519", "pdf": "https://arxiv.org/pdf/2506.00519.pdf", "abs": "https://arxiv.org/abs/2506.00519", "title": "CausalAbstain: Enhancing Multilingual LLMs with Causal Reasoning for Trustworthy Abstention", "authors": ["Yuxi Sun", "Aoqi Zuo", "Wei Gao", "Jing Ma"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to Association for Computational Linguistics Findings (ACL)\n  2025", "summary": "Large Language Models (LLMs) often exhibit knowledge disparities across\nlanguages. Encouraging LLMs to \\textit{abstain} when faced with knowledge gaps\nis a promising strategy to reduce hallucinations in multilingual settings.\nCurrent abstention strategies for multilingual scenarios primarily rely on\ngenerating feedback in various languages using LLMs and performing\nself-reflection. However, these methods can be adversely impacted by\ninaccuracies and biases in the generated feedback. To address this, from a\ncausal perspective, we introduce \\textit{CausalAbstain}, a method that helps\nLLMs determine whether to utilize multiple generated feedback responses and how\nto identify the most useful ones. Extensive experiments demonstrate that\n\\textit{CausalAbstain} effectively selects helpful feedback and enhances\nabstention decisions with interpretability in both native language\n(\\textsc{Casual-native}) and multilingual (\\textsc{Causal-multi}) settings,\noutperforming strong baselines on two benchmark datasets covering encyclopedic\nand commonsense knowledge QA tasks. Our code and data are open-sourced at\nhttps://github.com/peachch/CausalAbstain."}
{"id": "2506.00527", "pdf": "https://arxiv.org/pdf/2506.00527.pdf", "abs": "https://arxiv.org/abs/2506.00527", "title": "Retrieval-Augmented Generation Systems for Intellectual Property via Synthetic Multi-Angle Fine-tuning", "authors": ["Runtao Ren", "Jian Ma", "Jianxi Luo"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems in the Intellectual Property\n(IP) field often struggle with diverse user queries, including colloquial\nexpressions, spelling errors, and ambiguous terminology, leading to inaccurate\nretrieval and suboptimal responses. To address this challenge, we propose\nMulti-Angle Question Generation and Retrieval Fine-Tuning Method (MQG-RFM), a\nnovel framework that leverages large language models (LLMs) to simulate varied\nuser inquiries and fine-tunes retrieval models to align semantically equivalent\nbut linguistically diverse questions. Unlike complex architectural\nmodifications, MQG-RFM adopts a lightweight Data-to-Tune paradigm, combining\nprompt-engineered query generation with hard negative mining to enhance\nretrieval robustness without costly infrastructure changes. Experimental\nresults on a Taiwan patent Q&A dataset show 185.62% improvement in retrieval\naccuracy on the Patent Consultation dataset and 262.26% improvement on the\nNovel Patent Technology Report dataset, with 14.22% and 53.58% improvements in\ngeneration quality over the baselines, respectively. By bridging the gap\nbetween user intent and system comprehension through semantic-aware retrieval\noptimization, MQG-RFM offers a practical, scalable approach for rapid,\ncost-effective deployment among small and medium-sized agencies seeking\nreliable patent intelligence solutions. Additionally, our proposed method has\nalready been adopted by ScholarMate, the largest professional research social\nnetworking platform in China, to support real-world development and deployment.\nA demo version of the instantiated is available at\nhttps://github.com/renruntao/patent_rag."}
{"id": "2506.00536", "pdf": "https://arxiv.org/pdf/2506.00536.pdf", "abs": "https://arxiv.org/abs/2506.00536", "title": "Decoupling Reasoning and Knowledge Injection for In-Context Knowledge Editing", "authors": ["Changyue Wang", "Weihang Su", "Qingyao Ai", "Yujia Zhou", "Yiqun Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Knowledge editing aims to efficiently update Large Language Models (LLMs) by\nmodifying specific knowledge without retraining the entire model. Among\nknowledge editing approaches, in-context editing (ICE) offers a lightweight\nsolution by injecting new knowledge directly into the input context, leaving\nmodel parameters unchanged. However, existing ICE approaches do not explicitly\nseparate the newly injected knowledge from the model's original reasoning\nprocess. This entanglement often results in conflicts between external updates\nand internal parametric knowledge, undermining the consistency and accuracy of\nthe reasoning path.In this work, we conduct preliminary experiments to examine\nhow parametric knowledge influences reasoning path planning. We find that the\nmodel's reasoning is tightly coupled with its internal knowledge, and that\nnaively injecting new information without adapting the reasoning path often\nleads to performance degradation, particularly in multi-hop tasks. To this end,\nwe propose DecKER, a novel ICE framework that decouples reasoning from\nknowledge editing by generating a masked reasoning path and then resolving\nknowledge edits via hybrid retrieval and model-based validation. Experiments on\nmulti-hop QA benchmarks show that DecKER significantly outperforms existing ICE\nmethods by mitigating knowledge conflicts and preserving reasoning consistency.\nOur code is available at: https://github.com/bebr2/DecKER ."}
{"id": "2506.00539", "pdf": "https://arxiv.org/pdf/2506.00539.pdf", "abs": "https://arxiv.org/abs/2506.00539", "title": "ARIA: Training Language Agents with Intention-Driven Reward Aggregation", "authors": ["Ruihan Yang", "Yikai Zhang", "Aili Chen", "Xintao Wang", "Siyu Yuan", "Jiangjie Chen", "Deqing Yang", "Yanghua Xiao"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have enabled agents to perform complex reasoning\nand decision-making through free-form language interactions. However, in\nopen-ended language action environments (e.g., negotiation or question-asking\ngames), the action space can be formulated as a joint distribution over tokens,\nresulting in an exponentially large action space. Sampling actions in such a\nspace can lead to extreme reward sparsity, which brings large reward variance,\nhindering effective reinforcement learning (RL). To address this, we propose\nARIA, a method that Aggregates Rewards in Intention space to enable efficient\nand effective language Agents training. ARIA aims to project natural language\nactions from the high-dimensional joint token distribution space into a\nlow-dimensional intention space, where semantically similar actions are\nclustered and assigned shared rewards. This intention-aware reward aggregation\nreduces reward variance by densifying reward signals, fostering better policy\noptimization. Extensive experiments demonstrate that ARIA not only\nsignificantly reduces policy gradient variance, but also delivers substantial\nperformance gains of an average of 9.95% across four downstream tasks,\nconsistently outperforming offline and online RL baselines."}
{"id": "2506.00549", "pdf": "https://arxiv.org/pdf/2506.00549.pdf", "abs": "https://arxiv.org/abs/2506.00549", "title": "Towards Multi-dimensional Evaluation of LLM Summarization across Domains and Languages", "authors": ["Hyangsuk Min", "Yuho Lee", "Minjeong Ban", "Jiaqi Deng", "Nicole Hee-Yeon Kim", "Taewon Yun", "Hang Su", "Jason Cai", "Hwanjun Song"], "categories": ["cs.CL", "cs.AI"], "comment": "34 pages, 6 figures", "summary": "Evaluation frameworks for text summarization have evolved in terms of both\ndomain coverage and metrics. However, existing benchmarks still lack\ndomain-specific assessment criteria, remain predominantly English-centric, and\nface challenges with human annotation due to the complexity of reasoning. To\naddress these, we introduce MSumBench, which provides a multi-dimensional,\nmulti-domain evaluation of summarization in English and Chinese. It also\nincorporates specialized assessment criteria for each domain and leverages a\nmulti-agent debate system to enhance annotation quality. By evaluating eight\nmodern summarization models, we discover distinct performance patterns across\ndomains and languages. We further examine large language models as summary\nevaluators, analyzing the correlation between their evaluation and\nsummarization capabilities, and uncovering systematic bias in their assessment\nof self-generated summaries. Our benchmark dataset is publicly available at\nhttps://github.com/DISL-Lab/MSumBench."}
{"id": "2506.00551", "pdf": "https://arxiv.org/pdf/2506.00551.pdf", "abs": "https://arxiv.org/abs/2506.00551", "title": "AnnaAgent: Dynamic Evolution Agent System with Multi-Session Memory for Realistic Seeker Simulation", "authors": ["Ming Wang", "Peidong Wang", "Lin Wu", "Xiaocui Yang", "Daling Wang", "Shi Feng", "Yuxin Chen", "Bixuan Wang", "Yifei Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Constrained by the cost and ethical concerns of involving real seekers in\nAI-driven mental health, researchers develop LLM-based conversational agents\n(CAs) with tailored configurations, such as profiles, symptoms, and scenarios,\nto simulate seekers. While these efforts advance AI in mental health, achieving\nmore realistic seeker simulation remains hindered by two key challenges:\ndynamic evolution and multi-session memory. Seekers' mental states often\nfluctuate during counseling, which typically spans multiple sessions. To\naddress this, we propose AnnaAgent, an emotional and cognitive dynamic agent\nsystem equipped with tertiary memory. AnnaAgent incorporates an emotion\nmodulator and a complaint elicitor trained on real counseling dialogues,\nenabling dynamic control of the simulator's configurations. Additionally, its\ntertiary memory mechanism effectively integrates short-term and long-term\nmemory across sessions. Evaluation results, both automated and manual,\ndemonstrate that AnnaAgent achieves more realistic seeker simulation in\npsychological counseling compared to existing baselines. The ethically reviewed\nand screened code can be found on https://github.com/sci-m-wang/AnnaAgent."}
{"id": "2506.00583", "pdf": "https://arxiv.org/pdf/2506.00583.pdf", "abs": "https://arxiv.org/abs/2506.00583", "title": "The Hidden Language of Harm: Examining the Role of Emojis in Harmful Online Communication and Content Moderation", "authors": ["Yuhang Zhou", "Yimin Xiao", "Wei Ai", "Ge Gao"], "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": "18 pages, 3 figures", "summary": "Social media platforms have become central to modern communication, yet they\nalso harbor offensive content that challenges platform safety and inclusivity.\nWhile prior research has primarily focused on textual indicators of offense,\nthe role of emojis, ubiquitous visual elements in online discourse, remains\nunderexplored. Emojis, despite being rarely offensive in isolation, can acquire\nharmful meanings through symbolic associations, sarcasm, and contextual misuse.\nIn this work, we systematically examine emoji contributions to offensive\nTwitter messages, analyzing their distribution across offense categories and\nhow users exploit emoji ambiguity. To address this, we propose an LLM-powered,\nmulti-step moderation pipeline that selectively replaces harmful emojis while\npreserving the tweet's semantic intent. Human evaluations confirm our approach\neffectively reduces perceived offensiveness without sacrificing meaning. Our\nanalysis also reveals heterogeneous effects across offense types, offering\nnuanced insights for online communication and emoji moderation."}
{"id": "2506.00585", "pdf": "https://arxiv.org/pdf/2506.00585.pdf", "abs": "https://arxiv.org/abs/2506.00585", "title": "Entriever: Energy-based Retriever for Knowledge-Grounded Dialog Systems", "authors": ["Yucheng Cai", "Ke Li", "Yi Huang", "Junlan Feng", "Zhijian Ou"], "categories": ["cs.CL"], "comment": "Accepted by ACL2025 Findings", "summary": "A retriever, which retrieves relevant knowledge pieces from a knowledge base\ngiven a context, is an important component in many natural language processing\n(NLP) tasks. Retrievers have been introduced in knowledge-grounded dialog\nsystems to improve knowledge acquisition. In knowledge-grounded dialog systems,\nwhen conditioning on a given context, there may be multiple relevant and\ncorrelated knowledge pieces. However, knowledge pieces are usually assumed to\nbe conditionally independent in current retriever models. To address this\nissue, we propose Entriever, an energy-based retriever. Entriever directly\nmodels the candidate retrieval results as a whole instead of modeling the\nknowledge pieces separately, with the relevance score defined by an energy\nfunction. We explore various architectures of energy functions and different\ntraining methods for Entriever, and show that Entriever substantially\noutperforms the strong cross-encoder baseline in knowledge retrieval tasks.\nFurthermore, we show that in semi-supervised training of knowledge-grounded\ndialog systems, Entriever enables effective scoring of retrieved knowledge\npieces and significantly improves end-to-end performance of dialog systems."}
{"id": "2506.00608", "pdf": "https://arxiv.org/pdf/2506.00608.pdf", "abs": "https://arxiv.org/abs/2506.00608", "title": "PAKTON: A Multi-Agent Framework for Question Answering in Long Legal Agreements", "authors": ["Petros Raptopoulos", "Giorgos Filandrianos", "Maria Lymperaiou", "Giorgos Stamou"], "categories": ["cs.CL"], "comment": null, "summary": "Contract review is a complex and time-intensive task that typically demands\nspecialized legal expertise, rendering it largely inaccessible to non-experts.\nMoreover, legal interpretation is rarely straightforward-ambiguity is\npervasive, and judgments often hinge on subjective assessments. Compounding\nthese challenges, contracts are usually confidential, restricting their use\nwith proprietary models and necessitating reliance on open-source alternatives.\nTo address these challenges, we introduce PAKTON: a fully open-source,\nend-to-end, multi-agent framework with plug-and-play capabilities. PAKTON is\ndesigned to handle the complexities of contract analysis through collaborative\nagent workflows and a novel retrieval-augmented generation (RAG) component,\nenabling automated legal document review that is more accessible, adaptable,\nand privacy-preserving. Experiments demonstrate that PAKTON outperforms both\ngeneral-purpose and pretrained models in predictive accuracy, retrieval\nperformance, explainability, completeness, and grounded justifications as\nevaluated through a human study and validated with automated metrics."}
{"id": "2506.00612", "pdf": "https://arxiv.org/pdf/2506.00612.pdf", "abs": "https://arxiv.org/abs/2506.00612", "title": "Enhancing Clinical Multiple-Choice Questions Benchmarks with Knowledge Graph Guided Distractor Generation", "authors": ["Running Yang", "Wenlong Deng", "Minghui Chen", "Yuyin Zhou", "Xiaoxiao Li"], "categories": ["cs.CL"], "comment": null, "summary": "Clinical tasks such as diagnosis and treatment require strong decision-making\nabilities, highlighting the importance of rigorous evaluation benchmarks to\nassess the reliability of large language models (LLMs). In this work, we\nintroduce a knowledge-guided data augmentation framework that enhances the\ndifficulty of clinical multiple-choice question (MCQ) datasets by generating\ndistractors (i.e., incorrect choices that are similar to the correct one and\nmay confuse existing LLMs). Using our KG-based pipeline, the generated choices\nare both clinically plausible and deliberately misleading. Our approach\ninvolves multi-step, semantically informed walks on a medical knowledge graph\nto identify distractor paths-associations that are medically relevant but\nfactually incorrect-which then guide the LLM in crafting more deceptive\ndistractors. We apply the designed knowledge graph guided distractor generation\n(KGGDG) pipline, to six widely used medical QA benchmarks and show that it\nconsistently reduces the accuracy of state-of-the-art LLMs. These findings\nestablish KGGDG as a powerful tool for enabling more robust and diagnostic\nevaluations of medical LLMs."}
{"id": "2506.00622", "pdf": "https://arxiv.org/pdf/2506.00622.pdf", "abs": "https://arxiv.org/abs/2506.00622", "title": "Improving Dialogue State Tracking through Combinatorial Search for In-Context Examples", "authors": ["Haesung Pyun", "Yoonah Park", "Yohan Jo"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "In dialogue state tracking (DST), in-context learning comprises a retriever\nthat selects labeled dialogues as in-context examples and a DST model that uses\nthese examples to infer the dialogue state of the query dialogue. Existing\nmethods for constructing training data for retrievers suffer from three key\nlimitations: (1) the synergistic effect of examples is not considered, (2) the\nlinguistic characteristics of the query are not sufficiently factored in, and\n(3) scoring is not directly optimized for DST performance. Consequently, the\nretriever can fail to retrieve examples that would substantially improve DST\nperformance. To address these issues, we present CombiSearch, a method that\nscores effective in-context examples based on their combinatorial impact on DST\nperformance. Our evaluation on MultiWOZ shows that retrievers trained with\nCombiSearch surpass state-of-the-art models, achieving a 20x gain in data\nefficiency and generalizing well to the SGD dataset. Moreover, CombiSearch\nattains a 12% absolute improvement in the upper bound DST performance over\ntraditional approaches when no retrieval errors are assumed. This significantly\nincreases the headroom for practical DST performance while demonstrating that\nexisting methods rely on suboptimal data for retriever training."}
{"id": "2506.00628", "pdf": "https://arxiv.org/pdf/2506.00628.pdf", "abs": "https://arxiv.org/abs/2506.00628", "title": "LID Models are Actually Accent Classifiers: Implications and Solutions for LID on Accented Speech", "authors": ["Niyati Bafna", "Matthew Wiesner"], "categories": ["cs.CL"], "comment": "Accepted at Interspeech 2025", "summary": "Prior research indicates that LID model performance significantly declines on\naccented speech; however, the specific causes, extent, and characterization of\nthese errors remain under-explored. (i) We identify a common failure mode on\naccented speech whereby LID systems often misclassify L2 accented speech as the\nspeaker's native language or a related language. (ii) We present evidence\nsuggesting that state-of-the-art models are invariant to permutations of short\nspans of speech, implying they classify on the basis of short phonotactic\nfeatures indicative of accent rather than language. Our analysis reveals a\nsimple method to enhance model robustness to accents through input chunking.\n(iii) We present an approach that integrates sequence-level information into\nour model without relying on monolingual ASR systems; this reduces\naccent-language confusion and significantly enhances performance on accented\nspeech while maintaining comparable results on standard LID."}
{"id": "2506.00634", "pdf": "https://arxiv.org/pdf/2506.00634.pdf", "abs": "https://arxiv.org/abs/2506.00634", "title": "Social Construction of Urban Space: Understanding Neighborhood Boundaries Using Rental Listings", "authors": ["Adam Visokay", "Ruth Bagley", "Ian Kennedy", "Chris Hess", "Kyle Crowder", "Rob Voigt", "Denis Peskoff"], "categories": ["cs.CL"], "comment": "8 pages, 3 figures, 4 tables", "summary": "Rental listings offer a unique window into how urban space is socially\nconstructed through language. We analyze Chicago Craigslist rental\nadvertisements from 2018 to 2024 to examine how listing agents characterize\nneighborhoods, identifying mismatches between institutional boundaries and\nneighborhood claims. Through manual and large language model annotation, we\nclassify unstructured listings from Craigslist according to their neighborhood.\nGeospatial analysis reveals three distinct patterns: properties with\nconflicting neighborhood designations due to competing spatial definitions,\nborder properties with valid claims to adjacent neighborhoods, and ``reputation\nlaundering\" where listings claim association with distant, desirable\nneighborhoods. Through topic modeling, we identify patterns that correlate with\nspatial positioning: listings further from neighborhood centers emphasize\ndifferent amenities than centrally-located units. Our findings demonstrate that\nnatural language processing techniques can reveal how definitions of urban\nspaces are contested in ways that traditional methods overlook."}
{"id": "2506.00636", "pdf": "https://arxiv.org/pdf/2506.00636.pdf", "abs": "https://arxiv.org/abs/2506.00636", "title": "ViToSA: Audio-Based Toxic Spans Detection on Vietnamese Speech Utterances", "authors": ["Huy Ba Do", "Vy Le-Phuong Huynh", "Luan Thanh Nguyen"], "categories": ["cs.CL"], "comment": "Accepted for presentation at INTERSPEECH 2025", "summary": "Toxic speech on online platforms is a growing concern, impacting user\nexperience and online safety. While text-based toxicity detection is\nwell-studied, audio-based approaches remain underexplored, especially for\nlow-resource languages like Vietnamese. This paper introduces ViToSA\n(Vietnamese Toxic Spans Audio), the first dataset for toxic spans detection in\nVietnamese speech, comprising 11,000 audio samples (25 hours) with accurate\nhuman-annotated transcripts. We propose a pipeline that combines ASR and toxic\nspans detection for fine-grained identification of toxic content. Our\nexperiments show that fine-tuning ASR models on ViToSA significantly reduces\nWER when transcribing toxic speech, while the text-based toxic spans detection\n(TSD) models outperform existing baselines. These findings establish a novel\nbenchmark for Vietnamese audio-based toxic spans detection, paving the way for\nfuture research in speech content moderation."}
{"id": "2506.00637", "pdf": "https://arxiv.org/pdf/2506.00637.pdf", "abs": "https://arxiv.org/abs/2506.00637", "title": "Improving the Calibration of Confidence Scores in Text Generation Using the Output Distribution's Characteristics", "authors": ["Lorenzo Jaime Yu Flores", "Ori Ernst", "Jackie Chi Kit Cheung"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Main Conference", "summary": "Well-calibrated model confidence scores can improve the usefulness of text\ngeneration models. For example, users can be prompted to review predictions\nwith low confidence scores, to prevent models from returning bad or potentially\ndangerous predictions. However, confidence metrics are not always well\ncalibrated in text generation. One reason is that in generation, there can be\nmany valid answers, which previous methods do not always account for. Hence, a\nconfident model could distribute its output probability among multiple\nsequences because they are all valid. We propose task-agnostic confidence\nmetrics suited to generation, which rely solely on the probabilities associated\nwith the model outputs without the need for further fine-tuning or heuristics.\nUsing these, we are able to improve the calibration of BART and Flan-T5 on\nsummarization, translation, and QA datasets."}
{"id": "2506.00643", "pdf": "https://arxiv.org/pdf/2506.00643.pdf", "abs": "https://arxiv.org/abs/2506.00643", "title": "SATA-BENCH: Select All That Apply Benchmark for Multiple Choice Questions", "authors": ["Weijie Xu", "Shixian Cui", "Xi Fang", "Chi Xue", "Stephanie Eckman", "Chandan Reddy"], "categories": ["cs.CL", "cs.AI", "68T01", "I.2.7"], "comment": "40 pages, 13 figures", "summary": "Large language models (LLMs) are increasingly evaluated on single-answer\nmultiple-choice tasks, yet many real-world problems require identifying all\ncorrect answers from a set of options. This capability remains underexplored.\nWe introduce SATA-BENCH, the first dedicated benchmark for evaluating LLMs on\nSelect All That Apply (SATA) questions across diverse domains, including\nreading comprehension, law, and biomedicine. Our evaluation of 27 open-source\nand proprietary models reveals a significant gap: even the strongest model\nachieves only 41.8% exact match, exposing LLMs' inability to reliably identify\nall correct answers. We find that this weakness stems from two core challenges:\nselection bias - models favor certain choices regardless of content, and count\nbias - models fail to predict the correct number of answers. To address these\nissues, we propose Choice Funnel, a decoding strategy that combines token\ndebiasing with adaptive thresholding to guide models toward complete and\naccurate selections. Choice Funnel achieves up to 29% higher exact match than\ncompetitive baselines while reducing inference cost by over 64%. Our findings\nexpose fundamental limitations in current LLMs and introduce a new framework\nfor diagnosing and improving multi-answer reasoning. We release SATA-BENCH and\nChoice Funnel to promote LLM development for robust decision-making in\nrealistic, multi-answer applications."}
{"id": "2506.00644", "pdf": "https://arxiv.org/pdf/2506.00644.pdf", "abs": "https://arxiv.org/abs/2506.00644", "title": "Clinical Annotations for Automatic Stuttering Severity Assessment", "authors": ["Ana Rita Valente", "Rufael Marew", "Hawau Olamide Toyin", "Hamdan Al-Ali", "Anelise Bohnen", "Inma Becerra", "Elsa Marta Soares", "Goncalo Leal", "Hanan Aldarmaki"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at INTERSPEECH 2025", "summary": "Stuttering is a complex disorder that requires specialized expertise for\neffective assessment and treatment. This paper presents an effort to enhance\nthe FluencyBank dataset with a new stuttering annotation scheme based on\nestablished clinical standards. To achieve high-quality annotations, we hired\nexpert clinicians to label the data, ensuring that the resulting annotations\nmirror real-world clinical expertise. The annotations are multi-modal,\nincorporating audiovisual features for the detection and classification of\nstuttering moments, secondary behaviors, and tension scores. In addition to\nindividual annotations, we additionally provide a test set with highly reliable\nannotations based on expert consensus for assessing individual annotators and\nmachine learning models. Our experiments and analysis illustrate the complexity\nof this task that necessitates extensive clinical expertise for valid training\nand evaluation of stuttering assessment models."}
{"id": "2506.00649", "pdf": "https://arxiv.org/pdf/2506.00649.pdf", "abs": "https://arxiv.org/abs/2506.00649", "title": "GuideX: Guided Synthetic Data Generation for Zero-Shot Information Extraction", "authors": ["Neil De La Fuente", "Oscar Sainz", "Iker Garc√≠a-Ferrero", "Eneko Agirre"], "categories": ["cs.CL"], "comment": "ACL Findings 2025", "summary": "Information Extraction (IE) systems are traditionally domain-specific,\nrequiring costly adaptation that involves expert schema design, data\nannotation, and model training. While Large Language Models have shown promise\nin zero-shot IE, performance degrades significantly in unseen domains where\nlabel definitions differ. This paper introduces GUIDEX, a novel method that\nautomatically defines domain-specific schemas, infers guidelines, and generates\nsynthetically labeled instances, allowing for better out-of-domain\ngeneralization. Fine-tuning Llama 3.1 with GUIDEX sets a new state-of-the-art\nacross seven zeroshot Named Entity Recognition benchmarks. Models trained with\nGUIDEX gain up to 7 F1 points over previous methods without humanlabeled data,\nand nearly 2 F1 points higher when combined with it. Models trained on GUIDEX\ndemonstrate enhanced comprehension of complex, domain-specific annotation\nschemas. Code, models, and synthetic datasets are available at\nneilus03.github.io/guidex.com"}
{"id": "2506.00658", "pdf": "https://arxiv.org/pdf/2506.00658.pdf", "abs": "https://arxiv.org/abs/2506.00658", "title": "Sarc7: Evaluating Sarcasm Detection and Generation with Seven Types and Emotion-Informed Techniques", "authors": ["Lang Xiong", "Raina Gao", "Alyssa Jeong", "Yicheng Fu", "Sean O'Brien", "Vasu Sharma", "Kevin Zhu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Sarcasm is a form of humor where expressions convey meanings opposite to\ntheir literal interpretations. Classifying and generating sarcasm using large\nlanguage models is vital for interpreting human communication. Sarcasm poses\nchallenges for computational models, due to its nuanced nature. We introduce\nSarc7, a benchmark that classifies 7 types of sarcasm: self-deprecating,\nbrooding, deadpan, polite, obnoxious, raging, and manic by annotating entries\nof the MUStARD dataset. Classification was evaluated using zero-shot, few-shot,\nchain-of-thought (CoT), and a novel emotion-based prompting technique. We\npropose an emotion-based generation method developed by identifying key\ncomponents of sarcasm-incongruity, shock value, and context dependency. Our\nclassification experiments show that Gemini 2.5, using emotion-based prompting,\noutperforms other setups with an F1 score of 0.3664. Human evaluators preferred\nour emotion-based prompting, with 38.46% more successful generations than\nzero-shot prompting."}
{"id": "2506.00668", "pdf": "https://arxiv.org/pdf/2506.00668.pdf", "abs": "https://arxiv.org/abs/2506.00668", "title": "SafeTy Reasoning Elicitation Alignment for Multi-Turn Dialogues", "authors": ["Martin Kuo", "Jianyi Zhang", "Aolin Ding", "Louis DiValentin", "Amin Hass", "Benjamin F Morris", "Isaac Jacobson", "Randolph Linderman", "James Kiessling", "Nicolas Ramos", "Bhavna Gopal", "Maziyar Baran Pouyan", "Changwei Liu", "Hai Li", "Yiran Chen"], "categories": ["cs.CL"], "comment": null, "summary": "Malicious attackers can exploit large language models (LLMs) by engaging them\nin multi-turn dialogues to achieve harmful objectives, posing significant\nsafety risks to society. To address this challenge, we propose a novel defense\nmechanism: SafeTy Reasoning Elicitation Alignment for Multi-Turn Dialogues\n(STREAM). STREAM defends LLMs against multi-turn attacks while preserving their\nfunctional capabilities. Our approach involves constructing a human-annotated\ndataset, the Safety Reasoning Multi-turn Dialogues dataset, which is used to\nfine-tune a plug-and-play safety reasoning moderator. This model is designed to\nidentify malicious intent hidden within multi-turn conversations and alert the\ntarget LLM of potential risks. We evaluate STREAM across multiple LLMs against\nprevalent multi-turn attack strategies. Experimental results demonstrate that\nour method significantly outperforms existing defense techniques, reducing the\nAttack Success Rate (ASR) by 51.2%, all while maintaining comparable LLM\ncapability."}
{"id": "2506.00671", "pdf": "https://arxiv.org/pdf/2506.00671.pdf", "abs": "https://arxiv.org/abs/2506.00671", "title": "DeepRAG: Integrating Hierarchical Reasoning and Process Supervision for Biomedical Multi-Hop QA", "authors": ["Yuelyu Ji", "Hang Zhang", "Shiven Verma", "Hui Ji", "Chun Li", "Yushui Han", "Yanshan Wang"], "categories": ["cs.CL"], "comment": null, "summary": "We propose DeepRAG, a novel framework that integrates DeepSeek hierarchical\nquestion decomposition capabilities with RAG Gym unified retrieval-augmented\ngeneration optimization using process level supervision. Targeting the\nchallenging MedHopQA biomedical question answering task, DeepRAG systematically\ndecomposes complex queries into precise sub-queries and employs concept level\nreward signals informed by the UMLS ontology to enhance biomedical accuracy.\nPreliminary evaluations on the MedHopQA dataset indicate that DeepRAG\nsignificantly outperforms baseline models, including standalone DeepSeek and\nRAG Gym, achieving notable improvements in both Exact Match and concept level\naccuracy."}
{"id": "2506.00694", "pdf": "https://arxiv.org/pdf/2506.00694.pdf", "abs": "https://arxiv.org/abs/2506.00694", "title": "Measuring Faithfulness and Abstention: An Automated Pipeline for Evaluating LLM-Generated 3-ply Case-Based Legal Arguments", "authors": ["Li Zhang", "Morgan Gray", "Jaromir Savelka", "Kevin D. Ashley"], "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50"], "comment": "11 pages, 7th Workshop on Automated Semantic Analysis of Information\n  in Legal Text, 16 June 2025, Chicago, IL", "summary": "Large Language Models (LLMs) demonstrate potential in complex legal tasks\nlike argument generation, yet their reliability remains a concern. Building\nupon pilot work assessing LLM generation of 3-ply legal arguments using human\nevaluation, this paper introduces an automated pipeline to evaluate LLM\nperformance on this task, specifically focusing on faithfulness (absence of\nhallucination), factor utilization, and appropriate abstention. We define\nhallucination as the generation of factors not present in the input case\nmaterials and abstention as the model's ability to refrain from generating\narguments when instructed and no factual basis exists. Our automated method\nemploys an external LLM to extract factors from generated arguments and\ncompares them against the ground-truth factors provided in the input case\ntriples (current case and two precedent cases). We evaluated eight distinct\nLLMs on three tests of increasing difficulty: 1) generating a standard 3-ply\nargument, 2) generating an argument with swapped precedent roles, and 3)\nrecognizing the impossibility of argument generation due to lack of shared\nfactors and abstaining. Our findings indicate that while current LLMs achieve\nhigh accuracy (over 90%) in avoiding hallucination on viable argument\ngeneration tests (Tests 1 & 2), they often fail to utilize the full set of\nrelevant factors present in the cases. Critically, on the abstention test (Test\n3), most models failed to follow instructions to stop, instead generating\nspurious arguments despite the lack of common factors. This automated pipeline\nprovides a scalable method for assessing these crucial LLM behaviors,\nhighlighting the need for improvements in factor utilization and robust\nabstention capabilities before reliable deployment in legal settings. Project\npage:\nhttps://github.com/lizhang-AIandLaw/Measuring-Faithfulness-and-Abstention."}
{"id": "2506.00713", "pdf": "https://arxiv.org/pdf/2506.00713.pdf", "abs": "https://arxiv.org/abs/2506.00713", "title": "From Argumentative Text to Argument Knowledge Graph: A New Framework for Structured Argumentation", "authors": ["Debarati Bhattacharjee", "Ashish Anand"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "16 pages, 7 figures", "summary": "This paper presents a framework to convert argumentative texts into argument\nknowledge graphs (AKG). Starting with basic annotations of argumentative\ncomponents (ACs) and argumentative relations (ARs), we enrich the information\nby constructing a knowledge base (KB) graph with metadata attributes for nodes.\nNext, we use premises and inference rules from the KB to form arguments by\napplying modus ponens. From these arguments, we create an AKG. The nodes and\nedges of the AKG have attributes that capture important argumentative features.\nWe also find missing inference rules by identifying markers. This makes it\npossible to identify undercut attacks that were previously undetectable in\nexisting datasets. The AKG gives a graphical view of the argumentative\nstructure that is easier to understand than theoretical formats. It also\nprepares the ground for future reasoning tasks, including checking the\ncoherence of arguments and identifying opportunities for revision. For this, it\nis important to find indirect relations, many of which are implicit. Our\nproposed AKG format, with annotated inference rules and modus ponens, will help\nreasoning models learn the implicit indirect relations that require inference\nover arguments and the relations between them."}
{"id": "2506.00722", "pdf": "https://arxiv.org/pdf/2506.00722.pdf", "abs": "https://arxiv.org/abs/2506.00722", "title": "Chain-of-Thought Training for Open E2E Spoken Dialogue Systems", "authors": ["Siddhant Arora", "Jinchuan Tian", "Hayato Futami", "Jee-weon Jung", "Jiatong Shi", "Yosuke Kashiwagi", "Emiru Tsunoo", "Shinji Watanabe"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted at INTERSPEECH 2025", "summary": "Unlike traditional cascaded pipelines, end-to-end (E2E) spoken dialogue\nsystems preserve full differentiability and capture non-phonemic information,\nmaking them well-suited for modeling spoken interactions. However, existing E2E\napproaches often require large-scale training data and generates responses\nlacking semantic coherence. We propose a simple yet effective strategy\nleveraging a chain-of-thought (CoT) formulation, ensuring that training on\nconversational data remains closely aligned with the multimodal language model\n(LM)'s pre-training on speech recognition~(ASR), text-to-speech synthesis\n(TTS), and text LM tasks. Our method achieves over 1.5 ROUGE-1 improvement over\nthe baseline, successfully training spoken dialogue systems on publicly\navailable human-human conversation datasets, while being compute-efficient\nenough to train on just 300 hours of public human-human conversation data, such\nas the Switchboard. We will publicly release our models and training code."}
{"id": "2506.00726", "pdf": "https://arxiv.org/pdf/2506.00726.pdf", "abs": "https://arxiv.org/abs/2506.00726", "title": "Structured Gradient Guidance for Few-Shot Adaptation in Large Language Models", "authors": ["Hongye Zheng", "Yichen Wang", "Ray Pan", "Guiran Liu", "Binrong Zhu", "Hanlu Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "This paper presents a gradient-informed fine-tuning method for large language\nmodels under few-shot conditions. The goal is to enhance task adaptability and\ntraining stability when data is limited. The method builds on a base loss\nfunction and introduces two gradient-related regularization terms. The first\nenforces gradient direction consistency to guide parameter updates along\ntask-relevant directions and prevent drift. The second controls gradient\nmagnitude to avoid abnormal updates. Together, these components support a more\nefficient and stable optimization path. To further improve cross-task\ngeneralization, the method incorporates a gradient alignment mechanism. This\nmechanism measures the consistency between optimization directions of the\nsource and target tasks. It enhances fine-tuning performance in multi-task and\ncross-domain scenarios. Across various natural language understanding tasks,\nthe method outperforms existing fine-tuning strategies in average accuracy,\ngradient stability, and directional alignment. Empirical evaluations under\ndifferent sample sizes and domain-specific tasks confirm the method's\nrobustness and broad applicability in low-resource environments. In particular,\nthe method shows clear advantages in controlling parameter update paths. The\nresults demonstrate that a gradient-based fine-tuning framework can effectively\nleverage the representational power of large language models. It ensures\ntraining stability while reducing dependence on large volumes of labeled data."}
{"id": "2506.00737", "pdf": "https://arxiv.org/pdf/2506.00737.pdf", "abs": "https://arxiv.org/abs/2506.00737", "title": "Narrative Media Framing in Political Discourse", "authors": ["Yulia Otmakhova", "Lea Frermann"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Findings", "summary": "Narrative frames are a powerful way of conceptualizing and communicating\ncomplex, controversial ideas, however automated frame analysis to date has\nmostly overlooked this framing device. In this paper, we connect elements of\nnarrativity with fundamental aspects of framing, and present a framework which\nformalizes and operationalizes such aspects. We annotate and release a data set\nof news articles in the climate change domain, analyze the dominance of\nnarrative frame components across political leanings, and test LLMs in their\nability to predict narrative frames and their components. Finally, we apply our\nframework in an unsupervised way to elicit components of narrative framing in a\nsecond domain, the COVID-19 crisis, where our predictions are congruent with\nprior theoretical work showing the generalizability of our approach."}
{"id": "2506.00739", "pdf": "https://arxiv.org/pdf/2506.00739.pdf", "abs": "https://arxiv.org/abs/2506.00739", "title": "DefenderBench: A Toolkit for Evaluating Language Agents in Cybersecurity Environments", "authors": ["Chiyu Zhang", "Marc-Alexandre Cote", "Michael Albada", "Anush Sankaran", "Jack W. Stokes", "Tong Wang", "Amir Abdi", "William Blum", "Muhammad Abdul-Mageed"], "categories": ["cs.CL"], "comment": null, "summary": "Large language model (LLM) agents have shown impressive capabilities in human\nlanguage comprehension and reasoning, yet their potential in cybersecurity\nremains underexplored. We introduce DefenderBench, a practical, open-source\ntoolkit for evaluating language agents across offense, defense, and\ncybersecurity knowledge-based tasks. DefenderBench includes environments for\nnetwork intrusion, malicious content detection, code vulnerability analysis,\nand cybersecurity knowledge assessment. It is intentionally designed to be\naffordable and easily accessible for researchers while providing fair and\nrigorous assessment. We benchmark several state-of-the-art (SoTA) and popular\nLLMs, including both open- and closed-weight models, using a standardized\nagentic framework. Our results show that Claude-3.7-sonnet performs best with a\nDefenderBench score of 81.65, followed by Claude-3.7-sonnet-think with 78.40,\nwhile the best open-weight model, Llama 3.3 70B, is not far behind with a\nDefenderBench score of 71.81. DefenderBench's modular design allows seamless\nintegration of custom LLMs and tasks, promoting reproducibility and fair\ncomparisons. An anonymized version of DefenderBench is available at\nhttps://github.com/microsoft/DefenderBench."}
{"id": "2506.00740", "pdf": "https://arxiv.org/pdf/2506.00740.pdf", "abs": "https://arxiv.org/abs/2506.00740", "title": "Length Aware Speech Translation for Video Dubbing", "authors": ["Harveen Singh Chadha", "Aswin Shanmugam Subramanian", "Vikas Joshi", "Shubham Bansal", "Jian Xue", "Rupeshkumar Mehta", "Jinyu Li"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "This paper was accepted to Interspeech 2025", "summary": "In video dubbing, aligning translated audio with the source audio is a\nsignificant challenge. Our focus is on achieving this efficiently, tailored for\nreal-time, on-device video dubbing scenarios. We developed a phoneme-based\nend-to-end length-sensitive speech translation (LSST) model, which generates\ntranslations of varying lengths short, normal, and long using predefined tags.\nAdditionally, we introduced length-aware beam search (LABS), an efficient\napproach to generate translations of different lengths in a single decoding\npass. This approach maintained comparable BLEU scores compared to a baseline\nwithout length awareness while significantly enhancing synchronization quality\nbetween source and target audio, achieving a mean opinion score (MOS) gain of\n0.34 for Spanish and 0.65 for Korean, respectively."}
{"id": "2506.00741", "pdf": "https://arxiv.org/pdf/2506.00741.pdf", "abs": "https://arxiv.org/abs/2506.00741", "title": "Data Swarms: Optimizable Generation of Synthetic Evaluation Data", "authors": ["Shangbin Feng", "Yike Wang", "Weijia Shi", "Yulia Tsvetkov"], "categories": ["cs.CL"], "comment": null, "summary": "We propose Data Swarms, an algorithm to optimize the generation of synthetic\nevaluation data and advance quantitative desiderata of LLM evaluation. We first\ntrain a swarm of initial data generators using existing data, and define\nvarious evaluation objectives to reflect the desired properties of evaluation\n(e.g., generate more difficult problems for the evaluated models) and\nquantitatively evaluate data generators. We then employ particle swarm\noptimization to optimize the swarm of data generators, where they\ncollaboratively search through the model parameter space to find new generators\nthat advance these objectives. We further extend it to Adversarial Swarms,\nwhere the data generator swarm generates harder data while the test taker model\nswarm learns from such data, co-evolving dynamically for better data and models\nsimultaneously. Extensive experiments demonstrate that Data Swarms outperforms\neight data generation baselines across five evaluation objectives, while\nAdversarial Swarms produce more robust learning of synthetic data and stronger\ngeneralization. Further analysis reveals that Data Swarms successfully\noptimizes compositions of multiple evaluation objectives and generalizes to new\noff-the-shelf LLMs, unseen at optimization time."}
{"id": "2506.00743", "pdf": "https://arxiv.org/pdf/2506.00743.pdf", "abs": "https://arxiv.org/abs/2506.00743", "title": "Assortment of Attention Heads: Accelerating Federated PEFT with Head Pruning and Strategic Client Selection", "authors": ["Yeshwanth Venkatesha", "Souvik Kundu", "Priyadarshini Panda"], "categories": ["cs.CL", "cs.AI", "cs.DC"], "comment": null, "summary": "Parameter Efficient Fine-Tuning (PEFT) has become the de-facto approach in\nadapting Large Language Models (LLMs) for downstream tasks in Natural Language\nProcessing. However, its adoption in privacy-preserving distributed learning\nframeworks, such as Federated Learning (FL), remains relatively limited. This\nis mainly due to challenges specific to FL, such as resource-constrained\ndevices and diverse data distributions among clients. In this paper, we propose\nan efficient method to perform PEFT within the FL framework for Multi-Head\nAttention (MHA) based language models. We address the challenges through head\npruning, a novel head-specific weighted aggregation mechanism, and a client\nselection strategy. Head pruning minimizes training complexity within the\nclients, guided by the importance score computed based on the confidence of the\nattention head. Weighted aggregation of heads ensures the global model captures\ncrucial updates from diverse clients complementing our client selection\nstrategy. We show results on the MultiNLI benchmark along with 20 Newsgroups,\nXL-Sum, and E2E NLG datasets. We use the MultiNLI dataset and T5-small model\nwith LoRA as our PEFT method, attaining sparsity levels of up to 90%, resulting\nin a communication advantage of up to 1.8x and a reduction in training OPs of\n3.9x while maintaining the accuracy drop under 2%."}
{"id": "2506.00748", "pdf": "https://arxiv.org/pdf/2506.00748.pdf", "abs": "https://arxiv.org/abs/2506.00748", "title": "Translate With Care: Addressing Gender Bias, Neutrality, and Reasoning in Large Language Model Translations", "authors": ["Pardis Sadat Zahraei", "Ali Emami"], "categories": ["cs.CL", "cs.CY"], "comment": "Accepted to Findings of ACL 2025", "summary": "Addressing gender bias and maintaining logical coherence in machine\ntranslation remains challenging, particularly when translating between natural\ngender languages, like English, and genderless languages, such as Persian,\nIndonesian, and Finnish. We introduce the Translate-with-Care (TWC) dataset,\ncomprising 3,950 challenging scenarios across six low- to mid-resource\nlanguages, to assess translation systems' performance. Our analysis of diverse\ntechnologies, including GPT-4, mBART-50, NLLB-200, and Google Translate,\nreveals a universal struggle in translating genderless content, resulting in\ngender stereotyping and reasoning errors. All models preferred masculine\npronouns when gender stereotypes could influence choices. Google Translate and\nGPT-4 showed particularly strong bias, favoring male pronouns 4-6 times more\nthan feminine ones in leadership and professional success contexts. Fine-tuning\nmBART-50 on TWC substantially resolved these biases and errors, led to strong\ngeneralization, and surpassed proprietary LLMs while remaining open-source.\nThis work emphasizes the need for targeted approaches to gender and semantic\ncoherence in machine translation, particularly for genderless languages,\ncontributing to more equitable and accurate translation systems."}
{"id": "2506.00759", "pdf": "https://arxiv.org/pdf/2506.00759.pdf", "abs": "https://arxiv.org/abs/2506.00759", "title": "Understanding and Mitigating Cross-lingual Privacy Leakage via Language-specific and Universal Privacy Neurons", "authors": ["Wenshuo Dong", "Qingsong Yang", "Shu Yang", "Lijie Hu", "Meng Ding", "Wanyu Lin", "Tianhang Zheng", "Di Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) trained on massive data capture rich information\nembedded in the training data. However, this also introduces the risk of\nprivacy leakage, particularly involving personally identifiable information\n(PII). Although previous studies have shown that this risk can be mitigated\nthrough methods such as privacy neurons, they all assume that both the\n(sensitive) training data and user queries are in English. We show that they\ncannot defend against the privacy leakage in cross-lingual contexts: even if\nthe training data is exclusively in one language, these (private) models may\nstill reveal private information when queried in another language. In this\nwork, we first investigate the information flow of cross-lingual privacy\nleakage to give a better understanding. We find that LLMs process private\ninformation in the middle layers, where representations are largely shared\nacross languages. The risk of leakage peaks when converted to a\nlanguage-specific space in later layers. Based on this, we identify\nprivacy-universal neurons and language-specific privacy neurons.\nPrivacy-universal neurons influence privacy leakage across all languages, while\nlanguage-specific privacy neurons are only related to specific languages. By\ndeactivating these neurons, the cross-lingual privacy leakage risk is reduced\nby 23.3%-31.6%."}
{"id": "2506.00773", "pdf": "https://arxiv.org/pdf/2506.00773.pdf", "abs": "https://arxiv.org/abs/2506.00773", "title": "Dynamic Chunking and Selection for Reading Comprehension of Ultra-Long Context in Large Language Models", "authors": ["Boheng Sheng", "Jiacheng Yao", "Meicong Zhang", "Guoxiu He"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) often struggle to accurately read and comprehend\nextremely long texts. Current methods for improvement typically rely on\nsplitting long contexts into fixed-length chunks. However, fixed truncation\nrisks separating semantically relevant content, leading to ambiguity and\ncompromising accurate understanding. To overcome this limitation, we propose a\nstraightforward approach for dynamically separating and selecting chunks of\nlong context, facilitating a more streamlined input for LLMs. In particular, we\ncompute semantic similarities between adjacent sentences, using lower\nsimilarities to adaptively divide long contexts into variable-length chunks. We\nfurther train a question-aware classifier to select sensitive chunks that are\ncritical for answering specific questions. Experimental results on both\nsingle-hop and multi-hop question-answering benchmarks show that the proposed\napproach consistently outperforms strong baselines. Notably, it maintains\nrobustness across a wide range of input lengths, handling sequences of up to\n256k tokens. Our datasets and code are available at the following link:\nhttps://github.com/ECNU-Text-Computing/DCS"}
{"id": "2506.00777", "pdf": "https://arxiv.org/pdf/2506.00777.pdf", "abs": "https://arxiv.org/abs/2506.00777", "title": "Improving Automatic Evaluation of Large Language Models (LLMs) in Biomedical Relation Extraction via LLMs-as-the-Judge", "authors": ["Md Tahmid Rahman Laskar", "Israt Jahan", "Elham Dolatabadi", "Chun Peng", "Enamul Hoque", "Jimmy Huang"], "categories": ["cs.CL"], "comment": "Accepted at ACL 2025 (Main Conference)", "summary": "Large Language Models (LLMs) have demonstrated impressive performance in\nbiomedical relation extraction, even in zero-shot scenarios. However,\nevaluating LLMs in this task remains challenging due to their ability to\ngenerate human-like text, often producing synonyms or abbreviations of\ngold-standard answers, making traditional automatic evaluation metrics\nunreliable. On the other hand, while human evaluation is more reliable, it is\ncostly and time-consuming, making it impractical for real-world applications.\nThis paper investigates the use of LLMs-as-the-Judge as an alternative\nevaluation method for biomedical relation extraction. We benchmark 8 LLMs as\njudges to evaluate the responses generated by 5 other LLMs across 3 biomedical\nrelation extraction datasets. Unlike other text-generation tasks, we observe\nthat LLM-based judges perform quite poorly (usually below 50% accuracy) in the\nbiomedical relation extraction task. Our findings reveal that it happens mainly\nbecause relations extracted by LLMs do not adhere to any standard format. To\naddress this, we propose structured output formatting for LLM-generated\nresponses that helps LLM-Judges to improve their performance by about 15% (on\naverage). We also introduce a domain adaptation technique to further enhance\nLLM-Judge performance by effectively transferring knowledge between datasets.\nWe release both our human-annotated and LLM-annotated judgment data (36k\nsamples in total) for public use here:\nhttps://github.com/tahmedge/llm_judge_biomedical_re."}
{"id": "2506.00783", "pdf": "https://arxiv.org/pdf/2506.00783.pdf", "abs": "https://arxiv.org/abs/2506.00783", "title": "KG-TRACES: Enhancing Large Language Models with Knowledge Graph-constrained Trajectory Reasoning and Attribution Supervision", "authors": ["Rong Wu", "Pinlong Cai", "Jianbiao Mei", "Licheng Wen", "Tao Hu", "Xuemeng Yang", "Daocheng Fu", "Botian Shi"], "categories": ["cs.CL", "cs.AI"], "comment": "23 pages, 13 figures", "summary": "Large language models (LLMs) have made remarkable strides in various natural\nlanguage processing tasks, but their performance on complex reasoning problems\nremains hindered by a lack of explainability and trustworthiness. This issue,\noften manifesting as hallucinations or unattributable reasoning processes,\nlimits their applicability in complex reasoning scenarios. To address this, we\npropose Knowledge Graph-constrained Trajectory Reasoning Attribution and Chain\nExplanation Supervision (KG-TRACES), a novel framework that enhances the\nreasoning ability of LLMs through explicit supervision over reasoning paths and\nprocesses. KG-TRACES jointly supervises the model to: (1) predict symbolic\nrelation paths, (2) predict full triple-level reasoning paths, and (3) generate\nattribution-aware reasoning processes grounded in the reasoning paths. At\ninference phase, the model adapts to both KG-available and KG-unavailable\nscenarios, retrieving reasoning paths from a KG when possible or predicting\nplausible reasoning paths with only intrinsic knowledge when not. This design\nenables the model to reason in an explainable and source-attributable pattern.\nThrough extensive experiments on complex reasoning tasks, we demonstrate that\nKG-TRACES significantly outperforms existing SOTA: it improves Hits@1 by 1.6%\nand F1 by 4.7% on WebQSP, and achieves improvements of 4.8% in Hits@1 and 2.1%\nin F1 on CWQ. Moreover, we show its transferability to specialized domains such\nas medicine. By visualizing the intermediate steps of reasoning processes, we\nfurther show that the explicit supervision introduced by KG-TRACES leads to\nmore stable and goal-directed reasoning processes, aligning closely with\ncorrect answers. Code is available at https://github.com/Edaizi/KG-TRACES."}
{"id": "2506.00784", "pdf": "https://arxiv.org/pdf/2506.00784.pdf", "abs": "https://arxiv.org/abs/2506.00784", "title": "Research Borderlands: Analysing Writing Across Research Cultures", "authors": ["Shaily Bhatt", "Tal August", "Maria Antoniak"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Main)", "summary": "Improving cultural competence of language technologies is important. However\nmost recent works rarely engage with the communities they study, and instead\nrely on synthetic setups and imperfect proxies of culture. In this work, we\ntake a human-centered approach to discover and measure language-based cultural\nnorms, and cultural competence of LLMs. We focus on a single kind of culture,\nresearch cultures, and a single task, adapting writing across research\ncultures. Through a set of interviews with interdisciplinary researchers, who\nare experts at moving between cultures, we create a framework of structural,\nstylistic, rhetorical, and citational norms that vary across research cultures.\nWe operationalise these features with a suite of computational metrics and use\nthem for (a) surfacing latent cultural norms in human-written research papers\nat scale; and (b) highlighting the lack of cultural competence of LLMs, and\ntheir tendency to homogenise writing. Overall, our work illustrates the\nefficacy of a human-centered approach to measuring cultural norms in\nhuman-written and LLM-generated texts."}
{"id": "2506.00789", "pdf": "https://arxiv.org/pdf/2506.00789.pdf", "abs": "https://arxiv.org/abs/2506.00789", "title": "RARE: Retrieval-Aware Robustness Evaluation for Retrieval-Augmented Generation Systems", "authors": ["Yixiao Zeng", "Tianyu Cao", "Danqing Wang", "Xinran Zhao", "Zimeng Qiu", "Morteza Ziyadi", "Tongshuang Wu", "Lei Li"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) enhances recency and factuality in\nanswers. However, existing evaluations rarely test how well these systems cope\nwith real-world noise, conflicting between internal and external retrieved\ncontexts, or fast-changing facts. We introduce Retrieval-Aware Robustness\nEvaluation (RARE), a unified framework and large-scale benchmark that jointly\nstress-tests query and document perturbations over dynamic, time-sensitive\ncorpora. One of the central features of RARE is a knowledge-graph-driven\nsynthesis pipeline (RARE-Get) that automatically extracts single and multi-hop\nrelations from the customized corpus and generates multi-level question sets\nwithout manual intervention. Leveraging this pipeline, we construct a dataset\n(RARE-Set) spanning 400 expert-level time-sensitive finance, economics, and\npolicy documents and 48,322 questions whose distribution evolves as the\nunderlying sources change. To quantify resilience, we formalize\nretrieval-conditioned robustness metrics (RARE-Met) that capture a model's\nability to remain correct or recover when queries, documents, or real-world\nretrieval results are systematically altered. Our results show that RAG systems\nexhibit surprising vulnerability to perturbations, with document robustness\nconsistently being the weakest point regardless of generator size or\narchitecture. RAG systems consistently show lower robustness on multi-hop\nqueries than single-hop queries across all domains."}
{"id": "2506.00806", "pdf": "https://arxiv.org/pdf/2506.00806.pdf", "abs": "https://arxiv.org/abs/2506.00806", "title": "Fast or Slow? Integrating Fast Intuition and Deliberate Thinking for Enhancing Visual Question Answering", "authors": ["Songtao Jiang", "Chenyi Zhou", "Yan Zhang", "Yeying Jin", "Zuozhu Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Multimodal large language models (MLLMs) still struggle with complex\nreasoning tasks in Visual Question Answering (VQA). While current methods have\nadvanced by incorporating visual prompts, our study uncovers critical\nlimitations: these approaches indiscriminately annotate all detected objects\nfor every visual question, generating excessive visual markers that degrade\ntask performance. This issue stems primarily from a lack of focus on key visual\nelements, raising two important questions: Are all objects equally important,\nand do all questions require visual prompts? Motivated by Dual Process Theory,\nwhich distinguishes between instinctive and deliberate cognitive modes in human\nreasoning, we propose FOCUS, a plug-and-play approach that dynamically adapts\nto the complexity of questions, combining fast intuitive judgments with\ndeliberate analytical reasoning to enhance the vision-language reasoning\ncapability of the MLLM. For straightforward questions, FOCUS supports efficient\nzero-shot reasoning. For more complex tasks, it employs the conceptualizing\nbefore observation strategy to highlight critical elements. Extensive\nexperiments on four benchmarks, ScienceQA, TextQA, VizWiz, and MME, demonstrate\nthat FOCUS consistently improves the performance of both open-source and\nblack-box MLLMs, achieving significant gains across all datasets. Ablation\nstudies further validate the importance of combining diverse cognitive\nstrategies with refined visual information for superior performance. Code will\nbe released."}
{"id": "2506.00814", "pdf": "https://arxiv.org/pdf/2506.00814.pdf", "abs": "https://arxiv.org/abs/2506.00814", "title": "GuessBench: Sensemaking Multimodal Creativity in the Wild", "authors": ["Zifeng Zhu", "Shangbin Feng", "Herun Wan", "Ningnan Wang", "Minnan Luo", "Yulia Tsvetkov"], "categories": ["cs.CL"], "comment": null, "summary": "We propose GuessBench, a novel benchmark that evaluates Vision Language\nModels (VLMs) on modeling the pervasive, noisy, and pluralistic human\ncreativity. GuessBench sources data from \"Guess the Build\", an online\nmultiplayer Minecraft minigame where one player constructs a Minecraft build\ngiven a concept (e.g. caterpillar) and others try to guess it with natural\nlanguage hints, presenting a pristine testbed for sensemaking creativity in the\nwild with VLMs acting as guessers. We curate 1500 images from the actual\ngameplay and design 2000 problems spanning static and dynamic image settings,\nnatural language hints of varying completeness, and more. Extensive experiments\nwith six open/API VLMs and five reasoning enhancement approaches demonstrate\nthat GuessBench presents a uniquely challenging task in creativity modeling:\neven the start-of-the-art GPT-4o is incorrect on 34% of instances, while we\nobserve a huge performance gap (13.87% vs. 53.93% on average) between open and\nAPI models. When used as a resource to improve VLMs, fine-tuning on the\nreasoning traces for GuessBench problems improves visual perception tasks by\n15.36% on average. Further analysis reveals that VLM performance in creativity\nsensemaking correlates with the frequency of the concept in training data,\nwhile the accuracy drops sharply for concepts in underrepresented cultural\ncontexts and low-resource languages."}
{"id": "2506.00815", "pdf": "https://arxiv.org/pdf/2506.00815.pdf", "abs": "https://arxiv.org/abs/2506.00815", "title": "From Plain Text to Poetic Form: Generating Metrically-Constrained Sanskrit Verses", "authors": ["Manoj Balaji Jagadeeshan", "Samarth Bhatia", "Pretam Ray", "Harshul Raj Surana", "Akhil Rajeev P", "Priya Mishra", "Annarao Kulkarni", "Ganesh Ramakrishnan", "Prathosh AP", "Pawan Goyal"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large language models (LLMs) have significantly improved\nnatural language generation, including creative tasks like poetry composition.\nHowever, most progress remains concentrated in high-resource languages. This\nraises an important question: Can LLMs be adapted for structured poetic\ngeneration in a low-resource, morphologically rich language such as Sanskrit?\nIn this work, we introduce a dataset designed for translating English prose\ninto structured Sanskrit verse, with strict adherence to classical metrical\npatterns, particularly the Anushtub meter. We evaluate a range of generative\nmodels-both open-source and proprietary-under multiple settings. Specifically,\nwe explore constrained decoding strategies and instruction-based fine-tuning\ntailored to metrical and semantic fidelity. Our decoding approach achieves over\n99% accuracy in producing syntactically valid poetic forms, substantially\noutperforming general-purpose models in meter conformity. Meanwhile,\ninstruction-tuned variants show improved alignment with source meaning and\npoetic style, as supported by human assessments, albeit with marginal\ntrade-offs in metrical precision."}
{"id": "2506.00817", "pdf": "https://arxiv.org/pdf/2506.00817.pdf", "abs": "https://arxiv.org/abs/2506.00817", "title": "One for All: Update Parameterized Knowledge Across Multiple Models", "authors": ["Weitao Ma", "Xiyuan Du", "Xiaocheng Feng", "Lei Huang", "Yichong Huang", "Huiyi Zhang", "Xiaoliang Yang", "Baohang Li", "Xiachong Feng", "Ting Liu", "Bing Qin"], "categories": ["cs.CL"], "comment": "ACL 2025 (Main Conference)", "summary": "Large language models (LLMs) encode vast world knowledge but struggle to stay\nup-to-date, often leading to errors and hallucinations. Knowledge editing\noffers an efficient alternative to retraining, enabling targeted modifications\nby updating specific model parameters. However, existing methods primarily\nfocus on individual models, posing challenges in efficiently updating multiple\nmodels and adapting to new models. To address this, we propose OnceEdit, a\nnovel ensemble-based approach that employs a plug-in model as the editing\nmodule, enabling stable knowledge updates across multiple models. Building on\nthe model ensemble, OnceEdit introduces two key mechanisms to enhance its\neffectiveness. First, we introduce a dynamic weight mechanism through a \\weight\ntoken for distinguishing between edit-related and non-edit-related instances,\nensuring the appropriate utilization of knowledge from integrated models.\nSecond, we incorporate an ensemble enhancement mechanism to mitigate the\nexcessive reliance on the central model inherent in the model ensemble\ntechnique, making it more suitable for knowledge editing. Extensive experiments\non diverse LLMs demonstrate that OnceEdit consistently outperforms existing\nmethods while achieving superior editing efficiency. Further analysis confirms\nits adaptability and stability in multi-model editing scenarios. Our code will\nbe available."}
{"id": "2506.00823", "pdf": "https://arxiv.org/pdf/2506.00823.pdf", "abs": "https://arxiv.org/abs/2506.00823", "title": "Probing the Geometry of Truth: Consistency and Generalization of Truth Directions in LLMs Across Logical Transformations and Question Answering Tasks", "authors": ["Yuntai Bao", "Xuhong Zhang", "Tianyu Du", "Xinkui Zhao", "Zhengwen Feng", "Hao Peng", "Jianwei Yin"], "categories": ["cs.CL"], "comment": "19 pages, 16 figures; accepted to Findings of ACL 2025", "summary": "Large language models (LLMs) are trained on extensive datasets that\nencapsulate substantial world knowledge. However, their outputs often include\nconfidently stated inaccuracies. Earlier works suggest that LLMs encode\ntruthfulness as a distinct linear feature, termed the \"truth direction\", which\ncan classify truthfulness reliably. We address several open questions about the\ntruth direction: (i) whether LLMs universally exhibit consistent truth\ndirections; (ii) whether sophisticated probing techniques are necessary to\nidentify truth directions; and (iii) how the truth direction generalizes across\ndiverse contexts. Our findings reveal that not all LLMs exhibit consistent\ntruth directions, with stronger representations observed in more capable\nmodels, particularly in the context of logical negation. Additionally, we\ndemonstrate that truthfulness probes trained on declarative atomic statements\ncan generalize effectively to logical transformations, question-answering\ntasks, in-context learning, and external knowledge sources. Finally, we explore\nthe practical application of truthfulness probes in selective\nquestion-answering, illustrating their potential to improve user trust in LLM\noutputs. These results advance our understanding of truth directions and\nprovide new insights into the internal representations of LLM beliefs. Our code\nis public at https://github.com/colored-dye/truthfulness_probe_generalization"}
{"id": "2506.00826", "pdf": "https://arxiv.org/pdf/2506.00826.pdf", "abs": "https://arxiv.org/abs/2506.00826", "title": "HERGC: Heterogeneous Experts Representation and Generative Completion for Multimodal Knowledge Graphs", "authors": ["Yongkang Xiao", "Rui Zhang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Multimodal knowledge graphs (MMKGs) enrich traditional knowledge graphs (KGs)\nby incorporating diverse modalities such as images and text. Multi-modal\nknowledge graph completion (MMKGC) seeks to exploit these heterogeneous signals\nto infer missing facts, thereby mitigating the intrinsic incompleteness of\nMMKGs. Existing MMKGC methods typically leverage only the information contained\nin the MMKGs under the closed-world assumption and adopt discriminative\ntraining objectives, which limits their reasoning capacity during completion.\nRecent generative completion approaches powered by advanced large language\nmodels (LLMs) have shown strong reasoning abilities in unimodal knowledge graph\ncompletion, but their potential in MMKGC remains largely unexplored. To bridge\nthis gap, we propose HERGC, a Heterogeneous Experts Representation and\nGenerative Completion framework for MMKGs. HERGC first deploys a Heterogeneous\nExperts Representation Retriever that enriches and fuses multimodal information\nand retrieves a compact candidate set for each incomplete triple. It then uses\na Generative LLM Predictor fine-tuned on minimal instruction data to accurately\nidentify the correct answer from these candidates. Extensive experiments on\nthree standard MMKG benchmarks demonstrate HERGC's effectiveness and\nrobustness, achieving state-of-the-art performance."}
{"id": "2506.00829", "pdf": "https://arxiv.org/pdf/2506.00829.pdf", "abs": "https://arxiv.org/abs/2506.00829", "title": "COMPKE: Complex Question Answering under Knowledge Editing", "authors": ["Keyuan Cheng", "Zijian Kan", "Zhixian He", "Zhuoran Zhang", "Muhammad Asif Ali", "Ke Xu", "Lijie Hu", "Di Wang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by ACL 2025 Findings", "summary": "Knowledge Editing, which efficiently modifies the knowledge in large language\nmodels, has gathered great attention. Current benchmarks primarily use\nmulti-hop question answering to assess and analyze newly injected or updated\nknowledge. However, we argue that these benchmarks fail to effectively evaluate\nhow well the updated models apply this knowledge in real-life scenarios,\nparticularly when questions require complex reasoning, involving one-to-many\nrelationships or multi-step logical intersections. To fill in this gap, we\nintroduce a new benchmark, COMPKE: Complex Question Answering under Knowledge\nEditing, which includes 11,924 complex questions that reflect real-life\nsituations. We conduct an extensive evaluation of four knowledge editing\nmethods on COMPKE, revealing that their effectiveness varies notably across\ndifferent models. For instance, MeLLo attains an accuracy of 39.47 on\nGPT-4O-MINI, but this drops sharply to 3.83 on QWEN2.5-3B. We further\ninvestigate the underlying causes of these disparities from both methodological\nand model-specific perspectives. The datasets are available at\nhttps://github.com/kzjkzj666/CompKE."}
{"id": "2506.00842", "pdf": "https://arxiv.org/pdf/2506.00842.pdf", "abs": "https://arxiv.org/abs/2506.00842", "title": "Toward Structured Knowledge Reasoning: Contrastive Retrieval-Augmented Generation on Experience", "authors": ["Jiawei Gu", "Ziting Xian", "Yuanzhen Xie", "Ye Liu", "Enjie Liu", "Ruichao Zhong", "Mochi Gao", "Yunzhi Tan", "Bo Hu", "Zang Li"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings", "summary": "Large language models (LLMs) achieve strong performance on plain text tasks\nbut underperform on structured data like tables and databases. Potential\nchallenges arise from their underexposure during pre-training and rigid\ntext-to-structure transfer mechanisms. Unlike humans who seamlessly apply\nlearned patterns across data modalities, LLMs struggle to infer implicit\nrelationships embedded in tabular formats, especially in the absence of\nexplicit structural guidance. To bridge this cognitive gap, we introduce\nContrastive Retrieval-Augmented Generation on Experience (CoRE), a framework\nthat builds experience memory representations and enhances generalization\nthrough contrastive In-Context Learning (ICL) to simulate human-like knowledge\ntransfer. Experiments on Text-to-SQL and TableQA show CoRE significantly\nimproves performance, achieving average gains of 3.44% and 4.24%, with up to\n17.2% on challenging tasks. Our Monte Carlo Tree Search (MCTS)-generated\nExperience Memory expands training data 8-9x, enhancing diversity and domain\ncoverage. This training-free and continual method propels LLMs toward\nstructured knowledge expertise."}
{"id": "2506.00854", "pdf": "https://arxiv.org/pdf/2506.00854.pdf", "abs": "https://arxiv.org/abs/2506.00854", "title": "EEG2TEXT-CN: An Exploratory Study of Open-Vocabulary Chinese Text-EEG Alignment via Large Language Model and Contrastive Learning on ChineseEEG", "authors": ["Jacky Tai-Yu Lu", "Jung Chiang", "Chi-Sheng Chen", "Anna Nai-Yun Tung", "Hsiang Wei Hu", "Yuan Chiao Cheng"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MM", "q-bio.NC"], "comment": null, "summary": "We propose EEG2TEXT-CN, which, to the best of our knowledge, represents one\nof the earliest open-vocabulary EEG-to-text generation frameworks tailored for\nChinese. Built on a biologically grounded EEG encoder (NICE-EEG) and a compact\npretrained language model (MiniLM), our architecture aligns multichannel brain\nsignals with natural language representations via masked pretraining and\ncontrastive learning. Using a subset of the ChineseEEG dataset, where each\nsentence contains approximately ten Chinese characters aligned with 128-channel\nEEG recorded at 256 Hz, we segment EEG into per-character embeddings and\npredict full sentences in a zero-shot setting. The decoder is trained with\nteacher forcing and padding masks to accommodate variable-length sequences.\nEvaluation on over 1,500 training-validation sentences and 300 held-out test\nsamples shows promising lexical alignment, with a best BLEU-1 score of 6.38\\%.\nWhile syntactic fluency remains a challenge, our findings demonstrate the\nfeasibility of non-phonetic, cross-modal language decoding from EEG. This work\nopens a new direction in multilingual brain-to-text research and lays the\nfoundation for future cognitive-language interfaces in Chinese."}
{"id": "2506.00859", "pdf": "https://arxiv.org/pdf/2506.00859.pdf", "abs": "https://arxiv.org/abs/2506.00859", "title": "How Bidirectionality Helps Language Models Learn Better via Dynamic Bottleneck Estimation", "authors": ["Md Kowsher", "Nusrat Jahan Prottasha", "Shiyun Xu", "Shetu Mohanto", "Chen Chen", "Niloofar Yousefi", "Ozlem Garibay"], "categories": ["cs.CL"], "comment": null, "summary": "Bidirectional language models have better context understanding and perform\nbetter than unidirectional models on natural language understanding tasks, yet\nthe theoretical reasons behind this advantage remain unclear. In this work, we\ninvestigate this disparity through the lens of the Information Bottleneck (IB)\nprinciple, which formalizes a trade-off between compressing input information\nand preserving task-relevant content. We propose FlowNIB, a dynamic and\nscalable method for estimating mutual information during training that\naddresses key limitations of classical IB approaches, including computational\nintractability and fixed trade-off schedules. Theoretically, we show that\nbidirectional models retain more mutual information and exhibit higher\neffective dimensionality than unidirectional models. To support this, we\npresent a generalized framework for measuring representational complexity and\nprove that bidirectional representations are strictly more informative under\nmild conditions. We further validate our findings through extensive experiments\nacross multiple models and tasks using FlowNIB, revealing how information is\nencoded and compressed throughout training. Together, our work provides a\nprincipled explanation for the effectiveness of bidirectional architectures and\nintroduces a practical tool for analyzing information flow in deep language\nmodels."}
{"id": "2506.00863", "pdf": "https://arxiv.org/pdf/2506.00863.pdf", "abs": "https://arxiv.org/abs/2506.00863", "title": "L3Cube-MahaEmotions: A Marathi Emotion Recognition Dataset with Synthetic Annotations using CoTR prompting and Large Language Models", "authors": ["Nidhi Kowtal", "Raviraj Joshi"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Emotion recognition in low-resource languages like Marathi remains\nchallenging due to limited annotated data. We present L3Cube-MahaEmotions, a\nhigh-quality Marathi emotion recognition dataset with 11 fine-grained emotion\nlabels. The training data is synthetically annotated using large language\nmodels (LLMs), while the validation and test sets are manually labeled to serve\nas a reliable gold-standard benchmark. Building on the MahaSent dataset, we\napply the Chain-of-Translation (CoTR) prompting technique, where Marathi\nsentences are translated into English and emotion labeled via a single prompt.\nGPT-4 and Llama3-405B were evaluated, with GPT-4 selected for training data\nannotation due to superior label quality. We evaluate model performance using\nstandard metrics and explore label aggregation strategies (e.g., Union,\nIntersection). While GPT-4 predictions outperform fine-tuned BERT models,\nBERT-based models trained on synthetic labels fail to surpass GPT-4. This\nhighlights both the importance of high-quality human-labeled data and the\ninherent complexity of emotion recognition. An important finding of this work\nis that generic LLMs like GPT-4 and Llama3-405B generalize better than\nfine-tuned BERT for complex low-resource emotion recognition tasks. The dataset\nand model are shared publicly at https://github.com/l3cube-pune/MarathiNLP"}
{"id": "2506.00869", "pdf": "https://arxiv.org/pdf/2506.00869.pdf", "abs": "https://arxiv.org/abs/2506.00869", "title": "What's Missing in Vision-Language Models? Probing Their Struggles with Causal Order Reasoning", "authors": ["Zhaotian Weng", "Haoxuan Li", "Kuan-Hao Huang", "Jieyu Zhao"], "categories": ["cs.CL", "I.7.0"], "comment": "12 pages", "summary": "Despite the impressive performance of vision-language models (VLMs) on\ndownstream tasks, their ability to understand and reason about causal\nrelationships in visual inputs remains unclear. Robust causal reasoning is\nfundamental to solving complex high-level reasoning tasks, yet existing\nbenchmarks often include a mixture of reasoning questions, and VLMs can\nfrequently exploit object recognition and activity identification as shortcuts\nto arrive at the correct answers, making it challenging to truly assess their\ncausal reasoning abilities. To bridge this gap, we introduce VQA-Causal and\nVCR-Causal, two new benchmarks specifically designed to isolate and rigorously\nevaluate VLMs' causal reasoning abilities. Our findings reveal that while VLMs\nexcel in object and activity recognition, they perform poorly on causal\nreasoning tasks, often only marginally surpassing random guessing. Further\nanalysis suggests that this limitation stems from a severe lack of causal\nexpressions in widely used training datasets, where causal relationships are\nrarely explicitly conveyed. We additionally explore fine-tuning strategies with\nhard negative cases, showing that targeted fine-tuning can improve model's\ncausal reasoning while maintaining generalization and downstream performance.\nOur study highlights a key gap in current VLMs and lays the groundwork for\nfuture work on causal understanding."}
{"id": "2506.00875", "pdf": "https://arxiv.org/pdf/2506.00875.pdf", "abs": "https://arxiv.org/abs/2506.00875", "title": "CC-Tuning: A Cross-Lingual Connection Mechanism for Improving Joint Multilingual Supervised Fine-Tuning", "authors": ["Yangfan Ye", "Xiaocheng Feng", "Zekun Yuan", "Xiachong Feng", "Libo Qin", "Lei Huang", "Weitao Ma", "Yichong Huang", "Zhirui Zhang", "Yunfei Lu", "Xiaohui Yan", "Duyu Tang", "Dandan Tu", "Bing Qin"], "categories": ["cs.CL"], "comment": "ACL2025 main conference, long paper", "summary": "Current large language models (LLMs) often exhibit imbalanced multilingual\ncapabilities due to their English-centric training corpora. To address this,\nexisting fine-tuning approaches operating at the data-level (e.g., through data\naugmentation or distillation) typically introduce implicit cross-lingual\nalignment, overlooking the potential for more profound, latent-level\ncross-lingual interactions. In this work, we propose CC-Tuning, a novel\nmultilingual fine-tuning paradigm that explicitly establishes a cross-lingual\nconnection mechanism at the latent level. During training, CC-Tuning fuses the\nfeed forward activations from both English and non-English inputs, enabling the\nmodel to benefit from both linguistic resources. This process is facilitated\nwith a trainable Decision Maker that identifies beneficial activations.\nFurthermore, during inference, a Transform Matrix is utilized to simulate the\ncross-lingual connection under monolingual setting through representation\ntransformation. Our experiments on six benchmarks covering 22 languages show\nthat CC-Tuning outperforms vanilla SFT and offers a strong latent-level\nalternative to data-level augmentation methods. Further analysis also\nhighlights the practicality of CC-Tuning and the potential of latent-level\ncross-lingual interactions in advancing the multilingual performance of LLMs."}
{"id": "2506.00876", "pdf": "https://arxiv.org/pdf/2506.00876.pdf", "abs": "https://arxiv.org/abs/2506.00876", "title": "Not Every Token Needs Forgetting: Selective Unlearning to Limit Change in Utility in Large Language Model Unlearning", "authors": ["Yixin Wan", "Anil Ramakrishna", "Kai-Wei Chang", "Volkan Cevher", "Rahul Gupta"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Model (LLM) unlearning has recently gained significant\nattention, driven by the need to remove unwanted information, such as private,\nsensitive, or copyrighted content, from LLMs. However, conventional unlearning\napproaches indiscriminately update model parameters to forget all tokens in a\ntarget document, including common tokens (e.g., pronouns, prepositions, general\nnouns) that carry general knowledge. In this paper, we highlight that not every\ntoken needs forgetting. We propose Selective Unlearning (SU), which identifies\na critical subset of tokens within the forgetting set that is relevant to the\nunwanted information, and unlearns only those tokens. Experiments on two\nbenchmarks and six baseline unlearning algorithms demonstrate that SU not only\nachieves effective unlearning on the targeted forget data, but also\nsignificantly preserves the model's utility in the retaining set."}
{"id": "2506.00883", "pdf": "https://arxiv.org/pdf/2506.00883.pdf", "abs": "https://arxiv.org/abs/2506.00883", "title": "Improve MLLM Benchmark Efficiency through Interview", "authors": ["Farong Wen", "Yijin Guo", "Junying Wang", "Jiaohao Xiao", "Yingjie Zhou", "Chunyi Li", "Zicheng Zhang", "Guangtao Zhai"], "categories": ["cs.CL"], "comment": null, "summary": "The rapid development of Multimodal Large Language Models (MLLM) has led to a\nwide range of MLLM applications, and a number of benchmark datasets have sprung\nup in order to assess MLLM abilities. However, full-coverage Q&A testing on\nlarge-scale data is resource-intensive and time-consuming. To address this\nissue, we propose the MLLM Interview (MITV) strategy, which aims to quickly\nobtain MLLM performance metrics by quizzing fewer question. First, First, we\nconstructed the interview dataset, which was built on an existing MLLM\nassessment dataset, by adding difficulty labels based on the performance of\nsome typical MLLMs in this dataset. Second, we propose an MLLM Interview\nstrategy, which obtains an initial performance situation of the large model by\nquizzing a small number of topics and then continuously tries to test the\nmodel's limits. Through extensive experiments, the result shows that the MITV\nstrategy proposed in this paper performs well on MLLM benchmark datasets, and\nit is able to obtain the model evaluation capability faster through a small\nnumber of questions and answers."}
{"id": "2506.00893", "pdf": "https://arxiv.org/pdf/2506.00893.pdf", "abs": "https://arxiv.org/abs/2506.00893", "title": "Affordance Benchmark for MLLMs", "authors": ["Junying Wang", "Wenzhe Li", "Yalun Wu", "Yingji Liang", "Yijin Guo", "Chunyi Li", "Haodong Duan", "Zicheng Zhang", "Guangtao Zhai"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Affordance theory posits that environments inherently offer action\npossibilities that shape perception and behavior. While Multimodal Large\nLanguage Models (MLLMs) excel in vision-language tasks, their ability to\nperceive affordance, which is crucial for intuitive and safe interactions,\nremains underexplored. To address this, we introduce A4Bench, a novel benchmark\ndesigned to evaluate the affordance perception abilities of MLLMs across two\ndimensions: 1) Constitutive Affordance}, assessing understanding of inherent\nobject properties through 1,282 question-answer pairs spanning nine\nsub-disciplines, and 2) Transformative Affordance, probing dynamic and\ncontextual nuances (e.g., misleading, time-dependent, cultural, or\nindividual-specific affordance) with 718 challenging question-answer pairs.\nEvaluating 17 MLLMs (nine proprietary and eight open-source) against human\nperformance, we find that proprietary models generally outperform open-source\ncounterparts, but all exhibit limited capabilities, particularly in\ntransformative affordance perception. Furthermore, even top-performing models,\nsuch as Gemini-2.0-Pro (18.05% overall exact match accuracy), significantly lag\nbehind human performance (best: 85.34%, worst: 81.25%). These findings\nhighlight critical gaps in environmental understanding of MLLMs and provide a\nfoundation for advancing AI systems toward more robust, context-aware\ninteractions. The dataset is available in\nhttps://github.com/JunyingWang959/A4Bench/."}
{"id": "2506.00900", "pdf": "https://arxiv.org/pdf/2506.00900.pdf", "abs": "https://arxiv.org/abs/2506.00900", "title": "SocialEval: Evaluating Social Intelligence of Large Language Models", "authors": ["Jinfeng Zhou", "Yuxuan Chen", "Yihan Shi", "Xuanming Zhang", "Leqi Lei", "Yi Feng", "Zexuan Xiong", "Miao Yan", "Xunzhi Wang", "Yaru Cao", "Jianing Yin", "Shuai Wang", "Quanyu Dai", "Zhenhua Dong", "Hongning Wang", "Minlie Huang"], "categories": ["cs.CL"], "comment": "ACL 2025, Repository: \\url{https://github.com/thu-coai/SocialEval}", "summary": "LLMs exhibit promising Social Intelligence (SI) in modeling human behavior,\nraising the need to evaluate LLMs' SI and their discrepancy with humans. SI\nequips humans with interpersonal abilities to behave wisely in navigating\nsocial interactions to achieve social goals. This presents an operational\nevaluation paradigm: outcome-oriented goal achievement evaluation and\nprocess-oriented interpersonal ability evaluation, which existing work fails to\naddress. To this end, we propose SocialEval, a script-based bilingual SI\nbenchmark, integrating outcome- and process-oriented evaluation by manually\ncrafting narrative scripts. Each script is structured as a world tree that\ncontains plot lines driven by interpersonal ability, providing a comprehensive\nview of how LLMs navigate social interactions. Experiments show that LLMs fall\nbehind humans on both SI evaluations, exhibit prosociality, and prefer more\npositive social behaviors, even if they lead to goal failure. Analysis of LLMs'\nformed representation space and neuronal activations reveals that LLMs have\ndeveloped ability-specific functional partitions akin to the human brain."}
{"id": "2506.00912", "pdf": "https://arxiv.org/pdf/2506.00912.pdf", "abs": "https://arxiv.org/abs/2506.00912", "title": "Pi-SQL: Enhancing Text-to-SQL with Fine-Grained Guidance from Pivot Programming Languages", "authors": ["Yongdong chi", "Hanqing Wang", "Zonghan Yang", "Jian Yang", "Xiao Yan", "Yun Chen", "Guanhua Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Text-to-SQL transforms the user queries from natural language to executable\nSQL programs, enabling non-experts to interact with complex databases. Existing\nprompt-based methods craft meticulous text guidelines and examples to\nfacilitate SQL generation, but their accuracy is hindered by the large semantic\ngap between the texts and the low-resource SQL programs. In this work, we\npropose Pi-SQL, which incorporates the high-resource Python program as a pivot\nto bridge between the natural language query and SQL program. In particular,\nPi-SQL first generates Python programs that provide fine-grained step-by-step\nguidelines in their code blocks or comments, and then produces an SQL program\nfollowing the guidance of each Python program.The final SQL program matches the\nreference Python program's query results and, through selection from candidates\ngenerated by different strategies, achieves superior execution speed, with a\nreward-based valid efficiency score up to 4.55 higher than the best-performing\nbaseline.Extensive experiments demonstrate the effectiveness of Pi-SQL, which\nimproves the execution accuracy of the best-performing baseline by up to 3.20."}
{"id": "2506.00914", "pdf": "https://arxiv.org/pdf/2506.00914.pdf", "abs": "https://arxiv.org/abs/2506.00914", "title": "How do Transformer Embeddings Represent Compositions? A Functional Analysis", "authors": ["Aishik Nagar", "Ishaan Singh Rawal", "Mansi Dhanania", "Cheston Tan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Compositionality is a key aspect of human intelligence, essential for\nreasoning and generalization. While transformer-based models have become the de\nfacto standard for many language modeling tasks, little is known about how they\nrepresent compound words, and whether these representations are compositional.\nIn this study, we test compositionality in Mistral, OpenAI Large, and Google\nembedding models, and compare them with BERT. First, we evaluate\ncompositionality in the representations by examining six diverse models of\ncompositionality (addition, multiplication, dilation, regression, etc.). We\nfind that ridge regression, albeit linear, best accounts for compositionality.\nSurprisingly, we find that the classic vector addition model performs almost as\nwell as any other model. Next, we verify that most embedding models are highly\ncompositional, while BERT shows much poorer compositionality. We verify and\nvisualize our findings with a synthetic dataset consisting of fully transparent\nadjective-noun compositions. Overall, we present a thorough investigation of\ncompositionality."}
{"id": "2506.00942", "pdf": "https://arxiv.org/pdf/2506.00942.pdf", "abs": "https://arxiv.org/abs/2506.00942", "title": "anyECG-chat: A Generalist ECG-MLLM for Flexible ECG Input and Multi-Task Understanding", "authors": ["Haitao Li", "Ziyu Li", "Yiheng Mao", "Ziyi Liu", "Zhoujian Sun", "Zhengxing Huang"], "categories": ["cs.CL", "cs.AI", "eess.SP"], "comment": null, "summary": "The advent of multimodal large language models (MLLMs) has sparked interest\nin their application to electrocardiogram (ECG) analysis. However, existing\nECG-focused MLLMs primarily focus on report generation tasks, often limited to\nsingle 12-lead, short-duration (10s) ECG inputs, thereby underutilizing the\npotential of MLLMs. To this end, we aim to develop a MLLM for ECG analysis that\nsupports a broader range of tasks and more flexible ECG inputs. However,\nexisting ECG-QA datasets are often monotonous. To address this gap, we first\nconstructed the anyECG dataset, which encompasses a wide variety of tasks,\nincluding report generation, abnormal waveform localization, and open-ended\nquestion answering. In addition to standard hospital ECGs, we introduced\nlong-duration reduced-lead ECGs for home environments and multiple ECG\ncomparison scenarios commonly encountered in clinical practice. Furthermore, we\npropose the anyECG-chat model, which supports dynamic-length ECG inputs and\nmultiple ECG inputs. We trained the model using a three-stage curriculum\ntraining recipe with the anyECG dataset. A comprehensive evaluation was\nconducted, demonstrating that anyECG-chat is capable of supporting various\npractical application scenarios, including not only common report generation\ntasks but also abnormal waveform localization for long-duration reduced-lead\nECGs in home environments and comprehensive comparative analysis of multiple\nECGs."}
{"id": "2506.00955", "pdf": "https://arxiv.org/pdf/2506.00955.pdf", "abs": "https://arxiv.org/abs/2506.00955", "title": "Leveraging Large Language Models for Sarcastic Speech Annotation in Sarcasm Detection", "authors": ["Zhu Li", "Yuqing Zhang", "Xiyuan Gao", "Shekhar Nayak", "Matt Coler"], "categories": ["cs.CL"], "comment": "Accepted to Interspeech 2025", "summary": "Sarcasm fundamentally alters meaning through tone and context, yet detecting\nit in speech remains a challenge due to data scarcity. In addition, existing\ndetection systems often rely on multimodal data, limiting their applicability\nin contexts where only speech is available. To address this, we propose an\nannotation pipeline that leverages large language models (LLMs) to generate a\nsarcasm dataset. Using a publicly available sarcasm-focused podcast, we employ\nGPT-4o and LLaMA 3 for initial sarcasm annotations, followed by human\nverification to resolve disagreements. We validate this approach by comparing\nannotation quality and detection performance on a publicly available sarcasm\ndataset using a collaborative gating architecture. Finally, we introduce\nPodSarc, a large-scale sarcastic speech dataset created through this pipeline.\nThe detection model achieves a 73.63% F1 score, demonstrating the dataset's\npotential as a benchmark for sarcasm detection research."}
{"id": "2506.00963", "pdf": "https://arxiv.org/pdf/2506.00963.pdf", "abs": "https://arxiv.org/abs/2506.00963", "title": "From Objectives to Questions: A Planning-based Framework for Educational Mathematical Question Generation", "authors": ["Cheng Cheng", "Zhenya Huang", "Guanhao Zhao", "Yuxiang Guo", "Xin Lin", "Jinze Wu", "Xin Li", "Shijin Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Automatically generating high-quality mathematical problems that align with\neducational objectives is a crucial task in NLP-based educational technology.\nTraditional generation methods focus primarily on textual quality, but they\noften overlook educational objectives. Moreover, these methods address only\nsingle-dimensional, simple question generation, failing to meet complex,\nmultifaceted educational requirements. To address these challenges, we\nconstructed and annotated EduMath, a dataset of 16k mathematical questions with\nmulti-dimensional educational objectives. Based on this dataset, we developed\nEQGEVAL, which incorporates three evaluation dimensions and is designed to\nassess the ability of models to generate educational questions. Drawing\ninspiration from teachers' problem design processes, we propose the Educational\nQuestion Planning with self-Reflection (EQPR) method for educational\nmathematical question generation, following a \"plan-evaluate-optimize\"\napproach. Specifically, by combining planning algorithm based on Monte Carlo\nTree Search with the generative capabilities of Large Language Models, we\ncontinuously optimize questions through iterative feedback. This\nself-optimization mechanism ensures that the generated questions both fit the\neducational context and strategically achieve specific basic educational\nobjectives. Through extensive experiments based on EQGEVAL, we have\ndemonstrated that EQPR achieves significant improvements in generating\nquestions that meet multi-dimensional educational objectives."}
{"id": "2506.00964", "pdf": "https://arxiv.org/pdf/2506.00964.pdf", "abs": "https://arxiv.org/abs/2506.00964", "title": "ACCESS DENIED INC: The First Benchmark Environment for Sensitivity Awareness", "authors": ["Dren Fazlija", "Arkadij Orlov", "Sandipan Sikdar"], "categories": ["cs.CL"], "comment": "20 pages, 4 figures, 8 tables, ACL 2025 (Findings)", "summary": "Large language models (LLMs) are increasingly becoming valuable to corporate\ndata management due to their ability to process text from various document\nformats and facilitate user interactions through natural language queries.\nHowever, LLMs must consider the sensitivity of information when communicating\nwith employees, especially given access restrictions. Simple filtering based on\nuser clearance levels can pose both performance and privacy challenges. To\naddress this, we propose the concept of sensitivity awareness (SA), which\nenables LLMs to adhere to predefined access rights rules. In addition, we\ndeveloped a benchmarking environment called ACCESS DENIED INC to evaluate SA.\nOur experimental findings reveal significant variations in model behavior,\nparticularly in managing unauthorized data requests while effectively\naddressing legitimate queries. This work establishes a foundation for\nbenchmarking sensitivity-aware language models and provides insights to enhance\nprivacy-centric AI systems in corporate environments."}
{"id": "2506.00973", "pdf": "https://arxiv.org/pdf/2506.00973.pdf", "abs": "https://arxiv.org/abs/2506.00973", "title": "XGUARD: A Graded Benchmark for Evaluating Safety Failures of Large Language Models on Extremist Content", "authors": ["Vadivel Abishethvarman", "Bhavik Chandna", "Pratik Jalan", "Usman Naseem"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "Large Language Models (LLMs) can generate content spanning ideological\nrhetoric to explicit instructions for violence. However, existing safety\nevaluations often rely on simplistic binary labels (safe and unsafe),\noverlooking the nuanced spectrum of risk these outputs pose. To address this,\nwe present XGUARD, a benchmark and evaluation framework designed to assess the\nseverity of extremist content generated by LLMs. XGUARD includes 3,840 red\nteaming prompts sourced from real world data such as social media and news,\ncovering a broad range of ideologically charged scenarios. Our framework\ncategorizes model responses into five danger levels (0 to 4), enabling a more\nnuanced analysis of both the frequency and severity of failures. We introduce\nthe interpretable Attack Severity Curve (ASC) to visualize vulnerabilities and\ncompare defense mechanisms across threat intensities. Using XGUARD, we evaluate\nsix popular LLMs and two lightweight defense strategies, revealing key insights\ninto current safety gaps and trade-offs between robustness and expressive\nfreedom. Our work underscores the value of graded safety metrics for building\ntrustworthy LLMs."}
{"id": "2506.00975", "pdf": "https://arxiv.org/pdf/2506.00975.pdf", "abs": "https://arxiv.org/abs/2506.00975", "title": "NTPP: Generative Speech Language Modeling for Dual-Channel Spoken Dialogue via Next-Token-Pair Prediction", "authors": ["Qichao Wang", "Ziqiao Meng", "Wenqian Cui", "Yifei Zhang", "Pengcheng Wu", "Bingzhe Wu", "Irwin King", "Liang Chen", "Peilin Zhao"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "Inspired by the impressive capabilities of GPT-4o, there is growing interest\nin enabling speech language models (SLMs) to engage in natural, fluid spoken\ninteractions with humans. Recent advancements have led to the development of\nseveral SLMs that demonstrate promising results in this area. However, current\napproaches have yet to fully exploit dual-channel speech data, which inherently\ncaptures the structure and dynamics of human conversation. In this work, we\nsystematically explore the use of dual-channel speech data in the context of\nmodern large language models, and introduce a novel generative modeling\nparadigm, Next-Token-Pair Prediction (NTPP), to enable speaker-independent\ndual-channel spoken dialogue learning using decoder-only architectures for the\nfirst time. We evaluate our approach on standard benchmarks, and empirical\nresults show that our proposed method, NTPP, significantly improves the\nconversational abilities of SLMs in terms of turn-taking prediction, response\ncoherence, and naturalness. Moreover, compared to existing methods, NTPP\nachieves substantially lower inference latency, highlighting its practical\nefficiency for real-time applications."}
{"id": "2506.00980", "pdf": "https://arxiv.org/pdf/2506.00980.pdf", "abs": "https://arxiv.org/abs/2506.00980", "title": "LEMONADE: A Large Multilingual Expert-Annotated Abstractive Event Dataset for the Real World", "authors": ["Sina J. Semnani", "Pingyue Zhang", "Wanyue Zhai", "Haozhuo Li", "Ryan Beauchamp", "Trey Billing", "Katayoun Kishi", "Manling Li", "Monica S. Lam"], "categories": ["cs.CL"], "comment": "Findings of ACL 2025", "summary": "This paper presents LEMONADE, a large-scale conflict event dataset comprising\n39,786 events across 20 languages and 171 countries, with extensive coverage of\nregion-specific entities. LEMONADE is based on a partially reannotated subset\nof the Armed Conflict Location & Event Data (ACLED), which has documented\nglobal conflict events for over a decade.\n  To address the challenge of aggregating multilingual sources for global event\nanalysis, we introduce abstractive event extraction (AEE) and its subtask,\nabstractive entity linking (AEL). Unlike conventional span-based event\nextraction, our approach detects event arguments and entities through holistic\ndocument understanding and normalizes them across the multilingual dataset. We\nevaluate various large language models (LLMs) on these tasks, adapt existing\nzero-shot event extraction systems, and benchmark supervised models.\nAdditionally, we introduce ZEST, a novel zero-shot retrieval-based system for\nAEL.\n  Our best zero-shot system achieves an end-to-end F1 score of 58.3%, with LLMs\noutperforming specialized event extraction models such as GoLLIE. For entity\nlinking, ZEST achieves an F1 score of 45.7%, significantly surpassing OneNet, a\nstate-of-the-art zero-shot baseline that achieves only 23.7%. However, these\nzero-shot results lag behind the best supervised systems by 20.1% and 37.0% in\nthe end-to-end and AEL tasks, respectively, highlighting the need for further\nresearch."}
{"id": "2506.00981", "pdf": "https://arxiv.org/pdf/2506.00981.pdf", "abs": "https://arxiv.org/abs/2506.00981", "title": "What do self-supervised speech models know about Dutch? Analyzing advantages of language-specific pre-training", "authors": ["Marianne de Heer Kloots", "Hosein Mohebbi", "Charlotte Pouw", "Gaofei Shen", "Willem Zuidema", "Martijn Bentum"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "Accepted to Interspeech 2025. For model, code, and materials, see\n  https://github.com/mdhk/SSL-NL-eval", "summary": "How language-specific are speech representations learned by self-supervised\nmodels? Existing work has shown that a range of linguistic features can be\nsuccessfully decoded from end-to-end models trained only on speech recordings.\nHowever, it's less clear to what extent pre-training on specific languages\nimproves language-specific linguistic information. Here we test the encoding of\nDutch phonetic and lexical information in internal representations of\nself-supervised Wav2Vec2 models. Pre-training exclusively on Dutch improves the\nrepresentation of Dutch linguistic features as compared to pre-training on\nsimilar amounts of English or larger amounts of multilingual data. This\nlanguage-specific advantage is well-detected by trained clustering or\nclassification probes, and partially observable using zero-shot metrics.\nFurthermore, the language-specific benefit on linguistic feature encoding\naligns with downstream performance on Automatic Speech Recognition."}
{"id": "2506.00985", "pdf": "https://arxiv.org/pdf/2506.00985.pdf", "abs": "https://arxiv.org/abs/2506.00985", "title": "Do LLMs Understand Why We Write Diaries? A Method for Purpose Extraction and Clustering", "authors": ["Valeriya Goloviznina", "Alexander Sergeev", "Mikhail Melnichenko", "Evgeny Kotelnikov"], "categories": ["cs.CL"], "comment": "Accepted for CompLing-2025 conference", "summary": "Diary analysis presents challenges, particularly in extracting meaningful\ninformation from large corpora, where traditional methods often fail to deliver\nsatisfactory results. This study introduces a novel method based on Large\nLanguage Models (LLMs) to identify and cluster the various purposes of diary\nwriting. By \"purposes,\" we refer to the intentions behind diary writing, such\nas documenting life events, self-reflection, or practicing language skills. Our\napproach is applied to Soviet-era diaries (1922-1929) from the Prozhito digital\narchive, a rich collection of personal narratives. We evaluate different\nproprietary and open-source LLMs, finding that GPT-4o and o1-mini achieve the\nbest performance, while a template-based baseline is significantly less\neffective. Additionally, we analyze the retrieved purposes based on gender, age\nof the authors, and the year of writing. Furthermore, we examine the types of\nerrors made by the models, providing a deeper understanding of their\nlimitations and potential areas for improvement in future research."}
{"id": "2506.00986", "pdf": "https://arxiv.org/pdf/2506.00986.pdf", "abs": "https://arxiv.org/abs/2506.00986", "title": "Talking to Data: Designing Smart Assistants for Humanities Databases", "authors": ["Alexander Sergeev", "Valeriya Goloviznina", "Mikhail Melnichenko", "Evgeny Kotelnikov"], "categories": ["cs.CL"], "comment": "Accepted for InterSys-2025 conference", "summary": "Access to humanities research databases is often hindered by the limitations\nof traditional interaction formats, particularly in the methods of searching\nand response generation. This study introduces an LLM-based smart assistant\ndesigned to facilitate natural language communication with digital humanities\ndata. The assistant, developed in a chatbot format, leverages the RAG approach\nand integrates state-of-the-art technologies such as hybrid search, automatic\nquery generation, text-to-SQL filtering, semantic database search, and\nhyperlink insertion. To evaluate the effectiveness of the system, experiments\nwere conducted to assess the response quality of various language models. The\ntesting was based on the Prozhito digital archive, which contains diary entries\nfrom predominantly Russian-speaking individuals who lived in the 20th century.\nThe chatbot is tailored to support anthropology and history researchers, as\nwell as non-specialist users with an interest in the field, without requiring\nprior technical training. By enabling researchers to query complex databases\nwith natural language, this tool aims to enhance accessibility and efficiency\nin humanities research. The study highlights the potential of Large Language\nModels to transform the way researchers and the public interact with digital\narchives, making them more intuitive and inclusive. Additional materials are\npresented in GitHub repository:\nhttps://github.com/alekosus/talking-to-data-intersys2025."}
{"id": "2506.01034", "pdf": "https://arxiv.org/pdf/2506.01034.pdf", "abs": "https://arxiv.org/abs/2506.01034", "title": "Less is More: Local Intrinsic Dimensions of Contextual Language Models", "authors": ["Benjamin Matthias Ruppik", "Julius von Rohrscheidt", "Carel van Niekerk", "Michael Heck", "Renato Vukovic", "Shutong Feng", "Hsien-chin Lin", "Nurul Lubis", "Bastian Rieck", "Marcus Zibrowius", "Milica Ga≈°iƒá"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "9 pages, with an additional 13 pages of appendix", "summary": "Understanding the internal mechanisms of large language models (LLMs) remains\na challenging and complex endeavor. Even fundamental questions, such as how\nfine-tuning affects model behavior, often require extensive empirical\nevaluation. In this paper, we introduce a novel perspective based on the\ngeometric properties of contextual latent embeddings to study the effects of\ntraining and fine-tuning. To that end, we measure the local dimensions of a\ncontextual language model's latent space and analyze their shifts during\ntraining and fine-tuning. We show that the local dimensions provide insights\ninto the model's training dynamics and generalization ability. Specifically,\nthe mean of the local dimensions predicts when the model's training\ncapabilities are exhausted, as exemplified in a dialogue state tracking task,\noverfitting, as demonstrated in an emotion recognition task, and grokking, as\nillustrated with an arithmetic task. Furthermore, our experiments suggest a\npractical heuristic: reductions in the mean local dimension tend to accompany\nand predict subsequent performance gains. Through this exploration, we aim to\nprovide practitioners with a deeper understanding of the implications of\nfine-tuning on embedding spaces, facilitating informed decisions when\nconfiguring models for specific applications. The results of this work\ncontribute to the ongoing discourse on the interpretability, adaptability, and\ngeneralizability of LLMs by bridging the gap between intrinsic model mechanisms\nand geometric properties in the respective embeddings."}
{"id": "2506.01042", "pdf": "https://arxiv.org/pdf/2506.01042.pdf", "abs": "https://arxiv.org/abs/2506.01042", "title": "Probing Neural Topology of Large Language Models", "authors": ["Yu Zheng", "Yuan Yuan", "Yong Li", "Paolo Santi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Probing large language models (LLMs) has yielded valuable insights into their\ninternal mechanisms by linking neural representations to interpretable\nsemantics. However, how neurons functionally co-activate with each other to\ngive rise to emergent capabilities remains largely unknown, hindering a deeper\nunderstanding and safer development of LLMs. In this work, we introduce graph\nprobing, a method for uncovering the functional connectivity topology of LLM\nneurons and relating it to language generation performance. By analyzing\ninternal neural graphs across diverse LLM families and scales, we discover a\nuniversal predictability of next-token prediction performance using only neural\ntopology. This predictability is robust even when retaining just 1% of neuron\nconnections or probing models after only 8 pretraining steps, highlighting the\nsparsity and early emergence of topological patterns. Further graph matching\nanalysis suggests that, despite significant distinctions in architectures,\nparameters, and training data, different LLMs develop intricate and consistent\nneural topological structures that may form the foundation for their language\ngeneration abilities. Codes and data for the graph probing toolbox are released\nat https://github.com/DavyMorgan/llm-graph-probing."}
{"id": "2506.01047", "pdf": "https://arxiv.org/pdf/2506.01047.pdf", "abs": "https://arxiv.org/abs/2506.01047", "title": "CHEER-Ekman: Fine-grained Embodied Emotion Classification", "authors": ["Phan Anh Duong", "Cat Luong", "Divyesh Bommana", "Tianyu Jiang"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Emotions manifest through physical experiences and bodily reactions, yet\nidentifying such embodied emotions in text remains understudied. We present an\nembodied emotion classification dataset, CHEER-Ekman, extending the existing\nbinary embodied emotion dataset with Ekman's six basic emotion categories.\nUsing automatic best-worst scaling with large language models, we achieve\nperformance superior to supervised approaches on our new dataset. Our\ninvestigation reveals that simplified prompting instructions and\nchain-of-thought reasoning significantly improve emotion recognition accuracy,\nenabling smaller models to achieve competitive performance with larger ones."}
{"id": "2506.01062", "pdf": "https://arxiv.org/pdf/2506.01062.pdf", "abs": "https://arxiv.org/abs/2506.01062", "title": "SealQA: Raising the Bar for Reasoning in Search-Augmented Language Models", "authors": ["Thinh Pham", "Nguyen Nguyen", "Pratibha Zunjare", "Weiyuan Chen", "Yu-Min Tseng", "Tu Vu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Preprint. 22 pages, 7 figures, 11 tables", "summary": "We introduce SealQA, a new challenge benchmark for evaluating\nSEarch-Augmented Language models on fact-seeking questions where web search\nyields conflicting, noisy, or unhelpful results. SealQA comes in three flavors:\n(1) Seal-0 (main) and (2) Seal-Hard, which assess factual accuracy and\nreasoning capabilities, with Seal-0 focusing on the most challenging questions\nwhere chat models (e.g., GPT-4.1) typically achieve near-zero accuracy; and (3)\nLongSeal, which extends SealQA to test long-context, multi-document reasoning\nin \"needle-in-a-haystack\" settings. Our evaluation reveals critical limitations\nin current models: Even frontier LLMs perform poorly across all SealQA flavors.\nOn Seal-0, frontier agentic models equipped with tools like o3 and o4-mini\nachieve only 17.1% and 6.3% accuracy, respectively, at their best reasoning\nefforts. We find that advanced reasoning models such as DeepSeek-R1-671B and\no3-mini are highly vulnerable to noisy search results. Notably, increasing\ntest-time compute does not yield reliable gains across o3-mini, o4-mini, and\no3, with performance often plateauing or even declining early. Additionally,\nwhile recent models are less affected by the \"lost-in-the-middle\" issue, they\nstill fail to reliably identify relevant documents in LongSeal when faced with\nnumerous distractors. To facilitate future work, we release SealQA at\nhuggingface.co/datasets/vtllms/sealqa."}
{"id": "2506.01074", "pdf": "https://arxiv.org/pdf/2506.01074.pdf", "abs": "https://arxiv.org/abs/2506.01074", "title": "How Programming Concepts and Neurons Are Shared in Code Language Models", "authors": ["Amir Hossein Kargaran", "Yihong Liu", "Fran√ßois Yvon", "Hinrich Sch√ºtze"], "categories": ["cs.CL", "cs.PL", "cs.SE"], "comment": "ACL Findings 2025", "summary": "Several studies have explored the mechanisms of large language models (LLMs)\nin coding tasks, but most have focused on programming languages (PLs) in a\nmonolingual setting. In this paper, we investigate the relationship between\nmultiple PLs and English in the concept space of LLMs. We perform a few-shot\ntranslation task on 21 PL pairs using two Llama-based models. By decoding the\nembeddings of intermediate layers during this task, we observe that the concept\nspace is closer to English (including PL keywords) and assigns high\nprobabilities to English tokens in the second half of the intermediate layers.\nWe analyze neuron activations for 11 PLs and English, finding that while\nlanguage-specific neurons are primarily concentrated in the bottom layers,\nthose exclusive to each PL tend to appear in the top layers. For PLs that are\nhighly aligned with multiple other PLs, identifying language-specific neurons\nis not feasible. These PLs also tend to have a larger keyword set than other\nPLs and are closer to the model's concept space regardless of the input/output\nPL in the translation task. Our findings provide insights into how LLMs\ninternally represent PLs, revealing structural patterns in the model's concept\nspace. Code is available at https://github.com/cisnlp/code-specific-neurons."}
{"id": "2506.01084", "pdf": "https://arxiv.org/pdf/2506.01084.pdf", "abs": "https://arxiv.org/abs/2506.01084", "title": "zip2zip: Inference-Time Adaptive Vocabularies for Language Models via Token Compression", "authors": ["Saibo Geng", "Nathan Ranchin", "Yunzhen yao", "Maxime Peyrard", "Chris Wendler", "Michael Gastpar", "Robert West"], "categories": ["cs.CL", "cs.LG"], "comment": "Code will be released at https://github.com/epfl-dlab/zip2zip", "summary": "Tokenization efficiency plays a critical role in the performance and cost of\nlarge language models (LLMs), yet most models rely on static tokenizers\noptimized for general-purpose corpora. These tokenizers' fixed vocabularies\noften fail to adapt to domain- or language-specific inputs, leading to longer\ntoken sequences and higher computational costs. We introduce zip2zip, a\nframework that enables LLMs to dynamically adjust token vocabulary at inference\ntime, allowing for fewer generated tokens and thus faster inference. zip2zip\nconsists of three key components: (1) a tokenizer based on Lempel-Ziv-Welch\n(LZW) compression that incrementally compresses tokens into reusable\n\"hypertokens\" on the fly; (2) an embedding layer that computes embeddings for\nnewly formed hypertokens at runtime; and (3) a causal language modeling variant\nthat trains the model to operate on hypertokenized, compressed sequences. We\nshow that an existing LLM can be zip2zip-fied in 10 GPU-hours via\nparameter-efficient finetuning. The resulting zip2zip LLMs effectively learn to\nuse hypertokens at inference time, reducing input and output sequence length by\n20-60\\%, with significant improvements in inference latency."}
{"id": "2506.01089", "pdf": "https://arxiv.org/pdf/2506.01089.pdf", "abs": "https://arxiv.org/abs/2506.01089", "title": "Un-considering Contextual Information: Assessing LLMs' Understanding of Indexical Elements", "authors": ["Metehan Oguz", "Yavuz Bakman", "Duygu Nur Yaldiz"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 Findings", "summary": "Large Language Models (LLMs) have demonstrated impressive performances in\ntasks related to coreference resolution. However, previous studies mostly\nassessed LLM performance on coreference resolution with nouns and third person\npronouns. This study evaluates LLM performance on coreference resolution with\nindexical like I, you, here and tomorrow, which come with unique challenges due\nto their linguistic properties. We present the first study examining how LLMs\ninterpret indexicals in English, releasing the English Indexical Dataset with\n1600 multiple-choice questions. We evaluate pioneering LLMs, including GPT-4o,\nClaude 3.5 Sonnet, Gemini 1.5 Pro, and DeepSeek V3. Our results reveal that\nLLMs exhibit an impressive performance with some indexicals (I), while\nstruggling with others (you, here, tomorrow), and that syntactic cues (e.g.\nquotation) contribute to LLM performance with some indexicals, while they\nreduce performance with others. Code and data are available at:\nhttps://github.com/metehanoguzz/LLMs-Indexicals-English."}
{"id": "2506.01104", "pdf": "https://arxiv.org/pdf/2506.01104.pdf", "abs": "https://arxiv.org/abs/2506.01104", "title": "Contextual Candor: Enhancing LLM Trustworthiness Through Hierarchical Unanswerability Detection", "authors": ["Steven Robinson", "Antonio Carlos Rivera"], "categories": ["cs.CL"], "comment": null, "summary": "The pervasive deployment of large language models (LLMs) in conversational AI\nsystems has revolutionized information access, yet their propensity for\ngenerating factually unsupported or hallucinated responses remains a critical\nimpediment to trustworthiness and widespread adoption. This paper introduces\nReinforced Unanswerability Learning (RUL), a novel hybrid training paradigm\ndesigned to imbue LLMs with the intrinsic capability to accurately detect\nunanswerable questions and generate reliably appropriate responses. Unlike\nconventional approaches that rely on external classifiers or simple prompting,\nRUL integrates a discriminative unanswerability prediction head with the LLM's\ngenerative core, guided by a multi-stage learning strategy. This includes\nsupervised fine-tuning on a novel, richly annotated dataset,\nEnhanced-CAsT-Answerability (ECA), which features hierarchical answerability\nlabels and ground-truth refusal responses. Crucially, RUL incorporates a\nsubsequent reinforcement learning with human feedback (RLHF) phase to refine\nthe nuance, helpfulness, and informativeness of refusal responses. Extensive\nexperiments demonstrate RUL's superior performance, achieving significantly\nhigher accuracy in unanswerability detection across sentence, paragraph, and\nranking levels, and substantially increasing the generation of appropriate\nrefusals for unanswerable queries, alongside strong performance on answerable\nquestions. Human evaluations further corroborate RUL's effectiveness,\nhighlighting a marked improvement in perceived helpfulness and trustworthiness,\nultimately paving the way for more reliable and user-centric conversational AI."}
{"id": "2506.01133", "pdf": "https://arxiv.org/pdf/2506.01133.pdf", "abs": "https://arxiv.org/abs/2506.01133", "title": "From Words to Waves: Analyzing Concept Formation in Speech and Text-Based Foundation Models", "authors": ["Asƒ±m Ersoy", "Basel Mousi", "Shammur Chowdhury", "Firoj Alam", "Fahim Dalvi", "Nadir Durrani"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "Accepted Interspeech 2025", "summary": "The emergence of large language models (LLMs) has demonstrated that systems\ntrained solely on text can acquire extensive world knowledge, develop reasoning\ncapabilities, and internalize abstract semantic concepts--showcasing properties\nthat can be associated with general intelligence. This raises an intriguing\nquestion: Do such concepts emerge in models trained on other modalities, such\nas speech? Furthermore, when models are trained jointly on multiple modalities:\nDo they develop a richer, more structured semantic understanding? To explore\nthis, we analyze the conceptual structures learned by speech and textual models\nboth individually and jointly. We employ Latent Concept Analysis, an\nunsupervised method for uncovering and interpreting latent representations in\nneural networks, to examine how semantic abstractions form across modalities.\nFor reproducibility we made scripts and other resources available to the\ncommunity."}
{"id": "2506.01147", "pdf": "https://arxiv.org/pdf/2506.01147.pdf", "abs": "https://arxiv.org/abs/2506.01147", "title": "A Word is Worth 4-bit: Efficient Log Parsing with Binary Coded Decimal Recognition", "authors": ["Prerak Srivastava", "Giulio Corallo", "Sergey Rybalko"], "categories": ["cs.CL", "cs.LG"], "comment": "Pre-print of our accepted paper at IEEE International Conference on\n  Web Services (ICWS 2025). 4 pages, 2 figures", "summary": "System-generated logs are typically converted into categorical log templates\nthrough parsing. These templates are crucial for generating actionable insights\nin various downstream tasks. However, existing parsers often fail to capture\nfine-grained template details, leading to suboptimal accuracy and reduced\nutility in downstream tasks requiring precise pattern identification. We\npropose a character-level log parser utilizing a novel neural architecture that\naggregates character embeddings. Our approach estimates a sequence of\nbinary-coded decimals to achieve highly granular log templates extraction. Our\nlow-resource character-level parser, tested on revised Loghub-2k and a manually\nannotated industrial dataset, matches LLM-based parsers in accuracy while\noutperforming semantic parsers in efficiency."}
{"id": "2506.01156", "pdf": "https://arxiv.org/pdf/2506.01156.pdf", "abs": "https://arxiv.org/abs/2506.01156", "title": "Mispronunciation Detection Without L2 Pronunciation Dataset in Low-Resource Setting: A Case Study in Finland Swedish", "authors": ["Nhan Phan", "Mikko Kuronen", "Maria Kautonen", "Riikka Ullakonoja", "Anna von Zansen", "Yaroslav Getman", "Ekaterina Voskoboinik", "Tam√°s Gr√≥sz", "Mikko Kurimo"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to Interspeech 2025 conference", "summary": "Mispronunciation detection (MD) models are the cornerstones of many language\nlearning applications. Unfortunately, most systems are built for English and\nother major languages, while low-resourced language varieties, such as Finland\nSwedish (FS), lack such tools. In this paper, we introduce our MD model for FS,\ntrained on 89 hours of first language (L1) speakers' spontaneous speech and\ntested on 33 minutes of L2 transcribed read-aloud speech.\n  We trained a multilingual wav2vec 2.0 model with entropy regularization,\nfollowed by temperature scaling and top-k normalization after the inference to\nbetter adapt it for MD. The main novelty of our method lies in its simplicity,\nrequiring minimal L2 data. The process is also language-independent, making it\nsuitable for other low-resource languages. Our proposed algorithm allows us to\nbalance Recall (43.2%) and Precision (29.8%), compared with the baseline\nmodel's Recall (77.5%) and Precision (17.6%)."}
{"id": "2506.01172", "pdf": "https://arxiv.org/pdf/2506.01172.pdf", "abs": "https://arxiv.org/abs/2506.01172", "title": "The Inverse Scaling Effect of Pre-Trained Language Model Surprisal Is Not Due to Data Leakage", "authors": ["Byung-Doh Oh", "Hongao Zhu", "William Schuler"], "categories": ["cs.CL"], "comment": "ACL Findings 2025; results with Natural Stories alignment issue\n  corrected (commit 4700daa)", "summary": "In psycholinguistic modeling, surprisal from larger pre-trained language\nmodels has been shown to be a poorer predictor of naturalistic human reading\ntimes. However, it has been speculated that this may be due to data leakage\nthat caused language models to see the text stimuli during training. This paper\npresents two studies to address this concern at scale. The first study reveals\nrelatively little leakage of five naturalistic reading time corpora in two\npre-training datasets in terms of length and frequency of token $n$-gram\noverlap. The second study replicates the negative relationship between language\nmodel size and the fit of surprisal to reading times using models trained on\n'leakage-free' data that overlaps only minimally with the reading time corpora.\nTaken together, this suggests that previous results using language models\ntrained on these corpora are not driven by the effects of data leakage."}
{"id": "2506.01187", "pdf": "https://arxiv.org/pdf/2506.01187.pdf", "abs": "https://arxiv.org/abs/2506.01187", "title": "LAQuer: Localized Attribution Queries in Content-grounded Generation", "authors": ["Eran Hirsch", "Aviv Slobodkin", "David Wan", "Elias Stengel-Eskin", "Mohit Bansal", "Ido Dagan"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Grounded text generation models often produce content that deviates from\ntheir source material, requiring user verification to ensure accuracy. Existing\nattribution methods associate entire sentences with source documents, which can\nbe overwhelming for users seeking to fact-check specific claims. In contrast,\nexisting sub-sentence attribution methods may be more precise but fail to align\nwith users' interests. In light of these limitations, we introduce Localized\nAttribution Queries (LAQuer), a new task that localizes selected spans of\ngenerated output to their corresponding source spans, allowing fine-grained and\nuser-directed attribution. We compare two approaches for the LAQuer task,\nincluding prompting large language models (LLMs) and leveraging LLM internal\nrepresentations. We then explore a modeling framework that extends existing\nattributed text generation methods to LAQuer. We evaluate this framework across\ntwo grounded text generation tasks: Multi-document Summarization (MDS) and\nLong-form Question Answering (LFQA). Our findings show that LAQuer methods\nsignificantly reduce the length of the attributed text. Our contributions\ninclude: (1) proposing the LAQuer task to enhance attribution usability, (2)\nsuggesting a modeling framework and benchmarking multiple baselines, and (3)\nproposing a new evaluation setting to promote future research on localized\nattribution in content-grounded generation."}
{"id": "2506.01190", "pdf": "https://arxiv.org/pdf/2506.01190.pdf", "abs": "https://arxiv.org/abs/2506.01190", "title": "Culturally-Grounded Chain-of-Thought (CG-CoT):Enhancing LLM Performance on Culturally-Specific Tasks in Low-Resource Languages", "authors": ["Madhavendra Thakur"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) struggle with culturally-specific reasoning\ntasks, particularly in low-resource languages, hindering their global\napplicability. Addressing this gap is crucial for equitable AI deployment. We\nintroduce Culturally-Grounded Chain-of-Thought (CG-CoT), a novel prompting\nstrategy that combines dense vector retrieval of cultural context with explicit\nreasoning sequences. Our extensive experiments on Yoruba proverb interpretation\ndemonstrate that CG-CoT provides significantly higher culturally-aligned\naccuracy and depth than traditional prompting methods, validated through both\nautomated metrics and LLM-based evaluations. Notably, we uncover stark\ndisparities between token-level translation metrics like BLEU and human-judged\ncultural relevance, suggesting a rethinking of evaluation approaches for\nlow-resource NLP."}
{"id": "2506.01195", "pdf": "https://arxiv.org/pdf/2506.01195.pdf", "abs": "https://arxiv.org/abs/2506.01195", "title": "CoBRA: Quantifying Strategic Language Use and LLM Pragmatics", "authors": ["Anshun Asher Zheng", "Junyi Jessy Li", "David I. Beaver"], "categories": ["cs.CL"], "comment": "18 pages", "summary": "Language is often used strategically, particularly in high-stakes,\nadversarial settings, yet most work on pragmatics and LLMs centers on\ncooperativity. This leaves a gap in systematic understanding of non-cooperative\ndiscourse. To address this, we introduce CoBRA (Cooperation-Breach Response\nAssessment), along with three interpretable metrics -- Benefit at Turn (BaT),\nPenalty at Turn (PaT), and Normalized Relative Benefit at Turn (NRBaT) -- to\nquantify the perceived strategic effects of discourse moves. We also present\nCHARM, an annotated dataset of real courtroom cross-examinations, to\ndemonstrate the framework's effectiveness. Using these tools, we evaluate a\nrange of LLMs and show that LLMs generally exhibit limited pragmatic\nunderstanding of strategic language. While model size shows an increase in\nperformance on our metrics, reasoning ability does not help and largely hurts,\nintroducing overcomplication and internal confusion."}
{"id": "2506.01197", "pdf": "https://arxiv.org/pdf/2506.01197.pdf", "abs": "https://arxiv.org/abs/2506.01197", "title": "Incorporating Hierarchical Semantics in Sparse Autoencoder Architectures", "authors": ["Mark Muchane", "Sean Richardson", "Kiho Park", "Victor Veitch"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Code is available at\n  https://github.com/muchanem/hierarchical-sparse-autoencoders", "summary": "Sparse dictionary learning (and, in particular, sparse autoencoders) attempts\nto learn a set of human-understandable concepts that can explain variation on\nan abstract space. A basic limitation of this approach is that it neither\nexploits nor represents the semantic relationships between the learned\nconcepts. In this paper, we introduce a modified SAE architecture that\nexplicitly models a semantic hierarchy of concepts. Application of this\narchitecture to the internal representations of large language models shows\nboth that semantic hierarchy can be learned, and that doing so improves both\nreconstruction and interpretability. Additionally, the architecture leads to\nsignificant improvements in computational efficiency."}
{"id": "2506.01205", "pdf": "https://arxiv.org/pdf/2506.01205.pdf", "abs": "https://arxiv.org/abs/2506.01205", "title": "Trick or Neat: Adversarial Ambiguity and Language Model Evaluation", "authors": ["Antonia Karamolegkou", "Oliver Eberle", "Phillip Rust", "Carina Kauf", "Anders S√∏gaard"], "categories": ["cs.CL"], "comment": null, "summary": "Detecting ambiguity is important for language understanding, including\nuncertainty estimation, humour detection, and processing garden path sentences.\nWe assess language models' sensitivity to ambiguity by introducing an\nadversarial ambiguity dataset that includes syntactic, lexical, and\nphonological ambiguities along with adversarial variations (e.g., word-order\nchanges, synonym replacements, and random-based alterations). Our findings show\nthat direct prompting fails to robustly identify ambiguity, while linear probes\ntrained on model representations can decode ambiguity with high accuracy,\nsometimes exceeding 90\\%. Our results offer insights into the prompting\nparadigm and how language models encode ambiguity at different layers. We\nrelease both our code and data: https://github.com/coastalcph/lm_ambiguity."}
{"id": "2506.01206", "pdf": "https://arxiv.org/pdf/2506.01206.pdf", "abs": "https://arxiv.org/abs/2506.01206", "title": "Mamba Drafters for Speculative Decoding", "authors": ["Daewon Choi", "Seunghyuk Oh", "Saket Dingliwal", "Jihoon Tack", "Kyuyoung Kim", "Woomin Song", "Seojin Kim", "Insu Han", "Jinwoo Shin", "Aram Galstyan", "Shubham Katiyar", "Sravan Babu Bodapati"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Speculative decoding has emerged as a promising approach to accelerating\nlarge language model (LLM) generation using a fast drafter while maintaining\nalignment with the target model's distribution. However, existing approaches\nface a trade-off: external drafters offer flexibility but can suffer from\nslower drafting, while self-speculation methods use drafters tailored to the\ntarget model but require re-training. In this paper, we introduce novel\ndrafters based on Mamba, a state-of-the-art state space model (SSM), as a\nsolution that combines the best aspects of both approaches. By leveraging the\nlinear structure of SSMs, our approach avoids the quadratic complexity inherent\nin traditional Transformer-based methods, enabling faster drafting and lower\nmemory usage while maintaining the flexibility to work across different target\nmodels. We further enhance efficiency with a novel test-time tree search\nalgorithm for generating high-quality draft candidates. Our empirical\nevaluation demonstrates that Mamba-based drafters not only outperform existing\nexternal drafting methods but are also comparable to state-of-the-art\nself-speculation approaches while using less memory and maintaining their\ncross-model adaptability."}
{"id": "2506.01215", "pdf": "https://arxiv.org/pdf/2506.01215.pdf", "abs": "https://arxiv.org/abs/2506.01215", "title": "Compress, Gather, and Recompute: REFORMing Long-Context Processing in Transformers", "authors": ["Woomin Song", "Sai Muralidhar Jayanthi", "Srikanth Ronanki", "Kanthashree Mysore Sathyendra", "Jinwoo Shin", "Aram Galstyan", "Shubham Katiyar", "Sravan Babu Bodapati"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "As large language models increasingly gain popularity in real-world\napplications, processing extremely long contexts, often exceeding the model's\npre-trained context limits, has emerged as a critical challenge. While existing\napproaches to efficient long-context processing show promise, recurrent\ncompression-based methods struggle with information preservation, whereas\nrandom access approaches require substantial memory resources. We introduce\nREFORM, a novel inference framework that efficiently handles long contexts\nthrough a two-phase approach. First, it incrementally processes input chunks\nwhile maintaining a compressed KV cache, constructs cross-layer context\nembeddings, and utilizes early exit strategy for improved efficiency. Second,\nit identifies and gathers essential tokens via similarity matching and\nselectively recomputes the KV cache. Compared to baselines, REFORM achieves\nover 50% and 27% performance gains on RULER and BABILong respectively at 1M\ncontext length. It also outperforms baselines on Infinite-Bench and MM-NIAH,\ndemonstrating flexibility across diverse tasks and domains. Additionally,\nREFORM reduces inference time by 30% and peak memory usage by 5%, achieving\nboth efficiency and superior performance."}
{"id": "2506.01237", "pdf": "https://arxiv.org/pdf/2506.01237.pdf", "abs": "https://arxiv.org/abs/2506.01237", "title": "Polishing Every Facet of the GEM: Testing Linguistic Competence of LLMs and Humans in Korean", "authors": ["SungHo Kim", "Nayeon Kim", "Taehee Jeon", "SangKeun Lee"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ACL 2025 main conference", "summary": "We introduce the $\\underline{Ko}rean \\underline{G}rammar\n\\underline{E}valuation Bench\\underline{M}ark (KoGEM)$, designed to assess the\nlinguistic competence of LLMs and humans in Korean. KoGEM consists of 1.5k\nmultiple-choice QA pairs covering five main categories and 16 subcategories.\nThe zero-shot evaluation of 27 LLMs of various sizes and types reveals that\nwhile LLMs perform remarkably well on straightforward tasks requiring primarily\ndefinitional knowledge, they struggle with tasks that demand the integration of\nreal-world experiential knowledge, such as phonological rules and\npronunciation. Furthermore, our in-depth analysis suggests that incorporating\nsuch experiential knowledge could enhance the linguistic competence of LLMs.\nWith KoGEM, we not only highlight the limitations of current LLMs in linguistic\ncompetence but also uncover hidden facets of LLMs in linguistic competence,\npaving the way for enhancing comprehensive language understanding. Our code and\ndataset are available at: https://github.com/SungHo3268/KoGEM."}
{"id": "2506.01241", "pdf": "https://arxiv.org/pdf/2506.01241.pdf", "abs": "https://arxiv.org/abs/2506.01241", "title": "ExpertLongBench: Benchmarking Language Models on Expert-Level Long-Form Generation Tasks with Structured Checklists", "authors": ["Jie Ruan", "Inderjeet Nair", "Shuyang Cao", "Amy Liu", "Sheza Munir", "Micah Pollens-Dempsey", "Tiffany Chiang", "Lucy Kates", "Nicholas David", "Sihan Chen", "Ruxin Yang", "Yuqian Yang", "Jasmine Gump", "Tessa Bialek", "Vivek Sankaran", "Margo Schlanger", "Lu Wang"], "categories": ["cs.CL"], "comment": null, "summary": "This paper introduces ExpertLongBench, an expert-level benchmark containing\n11 tasks from 9 domains that reflect realistic expert workflows and\napplications. Beyond question answering, the application-driven tasks in\nExpertLongBench demand long-form outputs that can exceed 5,000 tokens and\nstrict adherence to domain-specific requirements. Notably, each task in\nExpertLongBench includes a rubric, designed or validated by domain experts, to\nspecify task requirements and guide output evaluation. Furthermore, we propose\nCLEAR, an evaluation framework that supports accurate evaluation of long-form\nmodel outputs in our benchmark. To achieve fine-grained, expert-aligned\nevaluation, CLEAR derives checklists from both model outputs and references by\nextracting information corresponding to items in the task-specific rubric.\nChecklist items for model outputs are then compared with corresponding items\nfor reference outputs to assess their correctness, enabling grounded\nevaluation. We benchmark 11 large language models (LLMs) and analyze components\nin CLEAR, showing that (1) existing LLMs, with the top performer achieving only\na 26.8% F1 score, require significant improvement for expert-level tasks; (2)\nmodels can generate content corresponding to the required aspects, though often\nnot accurately; and (3) accurate checklist extraction and comparison in CLEAR\ncan be achieved by open-weight models for more scalable and low-cost usage."}
{"id": "2506.01252", "pdf": "https://arxiv.org/pdf/2506.01252.pdf", "abs": "https://arxiv.org/abs/2506.01252", "title": "MTCMB: A Multi-Task Benchmark Framework for Evaluating LLMs on Knowledge, Reasoning, and Safety in Traditional Chinese Medicine", "authors": ["Shufeng Kong", "Xingru Yang", "Yuanyuan Wei", "Zijie Wang", "Hao Tang", "Jiuqi Qin", "Shuting Lan", "Yingheng Wang", "Junwen Bai", "Zhuangbin Chen", "Zibin Zheng", "Caihua Liu", "Hao Liang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Traditional Chinese Medicine (TCM) is a holistic medical system with\nmillennia of accumulated clinical experience, playing a vital role in global\nhealthcare-particularly across East Asia. However, the implicit reasoning,\ndiverse textual forms, and lack of standardization in TCM pose major challenges\nfor computational modeling and evaluation. Large Language Models (LLMs) have\ndemonstrated remarkable potential in processing natural language across diverse\ndomains, including general medicine. Yet, their systematic evaluation in the\nTCM domain remains underdeveloped. Existing benchmarks either focus narrowly on\nfactual question answering or lack domain-specific tasks and clinical realism.\nTo fill this gap, we introduce MTCMB-a Multi-Task Benchmark for Evaluating LLMs\non TCM Knowledge, Reasoning, and Safety. Developed in collaboration with\ncertified TCM experts, MTCMB comprises 12 sub-datasets spanning five major\ncategories: knowledge QA, language understanding, diagnostic reasoning,\nprescription generation, and safety evaluation. The benchmark integrates\nreal-world case records, national licensing exams, and classical texts,\nproviding an authentic and comprehensive testbed for TCM-capable models.\nPreliminary results indicate that current LLMs perform well on foundational\nknowledge but fall short in clinical reasoning, prescription planning, and\nsafety compliance. These findings highlight the urgent need for domain-aligned\nbenchmarks like MTCMB to guide the development of more competent and\ntrustworthy medical AI systems. All datasets, code, and evaluation tools are\npublicly available at: https://github.com/Wayyuanyuan/MTCMB."}
{"id": "2506.01253", "pdf": "https://arxiv.org/pdf/2506.01253.pdf", "abs": "https://arxiv.org/abs/2506.01253", "title": "CoRE: Condition-based Reasoning for Identifying Outcome Variance in Complex Events", "authors": ["Sai Vallurupalli", "Francis Ferraro"], "categories": ["cs.CL"], "comment": "Accepted to Findings of the Association for Computational Linguistics\n  2025", "summary": "Knowing which latent conditions lead to a particular outcome is useful for\ncritically examining claims made about complex event outcomes. Identifying\nimplied conditions and examining their influence on an outcome is challenging.\nWe handle this by combining and augmenting annotations from two existing\ndatasets consisting of goals and states, and explore the influence of\nconditions through our research questions and Condition-based Reasoning tasks.\nWe examine open and closed LLMs of varying sizes and intent-alignment on our\nreasoning tasks and find that conditions are useful when not all context is\navailable. Models differ widely in their ability to generate and identify\noutcome-variant conditions which affects their performance on outcome\nvalidation when conditions are used to replace missing context. Larger models\nlike GPT-4o, are more cautious in such less constrained situations."}
{"id": "2506.01254", "pdf": "https://arxiv.org/pdf/2506.01254.pdf", "abs": "https://arxiv.org/abs/2506.01254", "title": "Memory-Efficient FastText: A Comprehensive Approach Using Double-Array Trie Structures and Mark-Compact Memory Management", "authors": ["Yimin Du"], "categories": ["cs.CL"], "comment": "10 pages", "summary": "FastText has established itself as a fundamental algorithm for learning word\nrepresentations, demonstrating exceptional capability in handling\nout-of-vocabulary words through character-level n-gram embeddings. However, its\nhash-based bucketing mechanism introduces critical limitations for large-scale\nindustrial deployment: hash collisions cause semantic drift, and memory\nrequirements become prohibitively expensive when dealing with real-world\nvocabularies containing millions of terms. This paper presents a comprehensive\nmemory optimization framework that fundamentally reimagines FastText's memory\nmanagement through the integration of double-array trie (DA-trie) structures\nand mark-compact garbage collection principles. Our approach leverages the\nlinguistic insight that n-grams sharing common prefixes or suffixes exhibit\nhighly correlated embeddings due to co-occurrence patterns in natural language.\nBy systematically identifying and merging semantically similar embeddings based\non structural relationships, we achieve compression ratios of 4:1 to 10:1 while\nmaintaining near-perfect embedding quality. The algorithm consists of four\nsophisticated phases: prefix trie construction with embedding mapping,\nprefix-based similarity compression, suffix-based similarity compression, and\nmark-compact memory reorganization. Comprehensive experiments on a 30-million\nChinese vocabulary dataset demonstrate memory reduction from over 100GB to\napproximately 30GB with negligible performance degradation. Our industrial\ndeployment results show significant cost reduction, faster loading times, and\nimproved model reliability through the elimination of hash collision artifacts.\nCode and experimental implementations are available at:\nhttps://github.com/initial-d/me_fasttext"}
{"id": "2506.01257", "pdf": "https://arxiv.org/pdf/2506.01257.pdf", "abs": "https://arxiv.org/abs/2506.01257", "title": "DeepSeek in Healthcare: A Survey of Capabilities, Risks, and Clinical Applications of Open-Source Large Language Models", "authors": ["Jiancheng Ye", "Sophie Bronstein", "Jiarui Hai", "Malak Abu Hashish"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "DeepSeek-R1 is a cutting-edge open-source large language model (LLM)\ndeveloped by DeepSeek, showcasing advanced reasoning capabilities through a\nhybrid architecture that integrates mixture of experts (MoE), chain of thought\n(CoT) reasoning, and reinforcement learning. Released under the permissive MIT\nlicense, DeepSeek-R1 offers a transparent and cost-effective alternative to\nproprietary models like GPT-4o and Claude-3 Opus; it excels in structured\nproblem-solving domains such as mathematics, healthcare diagnostics, code\ngeneration, and pharmaceutical research. The model demonstrates competitive\nperformance on benchmarks like the United States Medical Licensing Examination\n(USMLE) and American Invitational Mathematics Examination (AIME), with strong\nresults in pediatric and ophthalmologic clinical decision support tasks. Its\narchitecture enables efficient inference while preserving reasoning depth,\nmaking it suitable for deployment in resource-constrained settings. However,\nDeepSeek-R1 also exhibits increased vulnerability to bias, misinformation,\nadversarial manipulation, and safety failures - especially in multilingual and\nethically sensitive contexts. This survey highlights the model's strengths,\nincluding interpretability, scalability, and adaptability, alongside its\nlimitations in general language fluency and safety alignment. Future research\npriorities include improving bias mitigation, natural language comprehension,\ndomain-specific validation, and regulatory compliance. Overall, DeepSeek-R1\nrepresents a major advance in open, scalable AI, underscoring the need for\ncollaborative governance to ensure responsible and equitable deployment."}
{"id": "2506.01262", "pdf": "https://arxiv.org/pdf/2506.01262.pdf", "abs": "https://arxiv.org/abs/2506.01262", "title": "Exploring the Potential of LLMs as Personalized Assistants: Dataset, Evaluation, and Analysis", "authors": ["Jisoo Mok", "Ik-hwan Kim", "Sangkwon Park", "Sungroh Yoon"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Personalized AI assistants, a hallmark of the human-like capabilities of\nLarge Language Models (LLMs), are a challenging application that intertwines\nmultiple problems in LLM research. Despite the growing interest in the\ndevelopment of personalized assistants, the lack of an open-source\nconversational dataset tailored for personalization remains a significant\nobstacle for researchers in the field. To address this research gap, we\nintroduce HiCUPID, a new benchmark to probe and unleash the potential of LLMs\nto deliver personalized responses. Alongside a conversational dataset, HiCUPID\nprovides a Llama-3.2-based automated evaluation model whose assessment closely\nmirrors human preferences. We release our dataset, evaluation model, and code\nat https://github.com/12kimih/HiCUPID."}
{"id": "2506.01263", "pdf": "https://arxiv.org/pdf/2506.01263.pdf", "abs": "https://arxiv.org/abs/2506.01263", "title": "WCTC-Biasing: Retraining-free Contextual Biasing ASR with Wildcard CTC-based Keyword Spotting and Inter-layer Biasing", "authors": ["Yu Nakagome", "Michael Hentschel"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Despite recent advances in end-to-end speech recognition methods, the output\ntends to be biased to the training data's vocabulary, resulting in inaccurate\nrecognition of proper nouns and other unknown terms. To address this issue, we\npropose a method to improve recognition accuracy of such rare words in\nCTC-based models without additional training or text-to-speech systems.\nSpecifically, keyword spotting is performed using acoustic features of\nintermediate layers during inference, and a bias is applied to the subsequent\nlayers of the acoustic model for detected keywords. For keyword detection, we\nadopt a wildcard CTC that is both fast and tolerant of ambiguous matches,\nallowing flexible handling of words that are difficult to match strictly. Since\nthis method does not require retraining of existing models, it can be easily\napplied to even large-scale models. In experiments on Japanese speech\nrecognition, the proposed method achieved a 29% improvement in the F1 score for\nunknown words."}
{"id": "2506.01265", "pdf": "https://arxiv.org/pdf/2506.01265.pdf", "abs": "https://arxiv.org/abs/2506.01265", "title": "Beyond In-Context Learning: Aligning Long-form Generation of Large Language Models via Task-Inherent Attribute Guidelines", "authors": ["Do Xuan Long", "Duong Ngoc Yen", "Do Xuan Trong", "Luu Anh Tuan", "Kenji Kawaguchi", "Shafiq Joty", "Min-Yen Kan", "Nancy F. Chen"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "In-context learning (ICL) is an important yet not fully understood ability of\npre-trained large language models (LLMs). It can greatly enhance task\nperformance using a few examples, termed demonstrations, without fine-tuning.\nAlthough effective in question answering, ICL often underperforms in long-form\ngeneration tasks such as summarization. Under appropriately realistic\nassumptions, we empirically and theoretically show that ICL demonstrations\nalone are insufficient to teach LLMs the task language and format distributions\nfor generation. We argue for explicit exposure to the task distributions and\nhypothesize that defining them by prompting enhances model performance. To this\nend, we present LongGuide, which efficiently generates two parallel streams of\nguidelines capturing task language and format properties: (i) Metric Guidelines\n(MGs) that instruct models to optimize self-evaluated metrics; and (ii) Output\nConstraint Guidelines (OCGs) that constrain generation at both token and\nsentence levels. LongGuide automatically selects the best combination of\nguidelines, improving both strong open- and closed-source LLMs by over 5% in\nboth zero- and few-shot settings. We show that LongGuide is generalizable,\nlearnable by weak models to enhance strong ones, and integrates synergistically\nwith automatic prompt optimizers."}
{"id": "2506.01266", "pdf": "https://arxiv.org/pdf/2506.01266.pdf", "abs": "https://arxiv.org/abs/2506.01266", "title": "Detoxification of Large Language Models through Output-layer Fusion with a Calibration Model", "authors": ["Yuanhe Tian", "Mingjie Deng", "Guoqing Jin", "Yan Song"], "categories": ["cs.CL", "cs.AI"], "comment": "5 pages, 1 figure", "summary": "Existing approaches for Large language model (LLM) detoxification generally\nrely on training on large-scale non-toxic or human-annotated preference data,\ndesigning prompts to instruct the LLM to generate safe content, or modifying\nthe model parameters to remove toxic information, which are computationally\nexpensive, lack robustness, and often compromise LLMs' fluency and contextual\nunderstanding. In this paper, we propose a simple yet effective approach for\nLLM detoxification, which leverages a compact, pre-trained calibration model\nthat guides the detoxification process of a target LLM via a lightweight\nintervention in its generation pipeline. By learning a detoxified embedding\nspace from non-toxic data, the calibration model effectively steers the LLM\naway from generating harmful content. This approach only requires a one-time\ntraining of the calibration model that is able to be seamlessly applied to\nmultiple LLMs without compromising fluency or contextual understanding.\nExperiment results on the benchmark dataset demonstrate that our approach\nreduces toxicity while maintaining reasonable content expression."}
{"id": "2506.01276", "pdf": "https://arxiv.org/pdf/2506.01276.pdf", "abs": "https://arxiv.org/abs/2506.01276", "title": "Schema as Parameterized Tools for Universal Information Extraction", "authors": ["Sheng Liang", "Yongyue Zhang", "Yaxiong Wu", "Ruiming Tang", "Yong Liu"], "categories": ["cs.CL", "I.2.7"], "comment": "12 pages, 7 figures, 5 tables", "summary": "Universal information extraction (UIE) primarily employs an extractive\ngeneration approach with large language models (LLMs), typically outputting\nstructured information based on predefined schemas such as JSON or tables. UIE\nsuffers from a lack of adaptability when selecting between predefined schemas\nand on-the-fly schema generation within the in-context learning paradigm,\nespecially when there are numerous schemas to choose from. In this paper, we\npropose a unified adaptive text-to-structure generation framework, called\nSchema as Parameterized Tools (SPT), which reimagines the tool-calling\ncapability of LLMs by treating predefined schemas as parameterized tools for\ntool selection and parameter filling. Specifically, our SPT method can be\napplied to unify closed, open, and on-demand IE tasks by adopting Schema\nRetrieval by fetching the relevant schemas from a predefined pool, Schema\nFilling by extracting information and filling slots as with tool parameters, or\nSchema Generation by synthesizing new schemas with uncovered cases. Experiments\nshow that the SPT method can handle four distinct IE tasks adaptively,\ndelivering robust schema retrieval and selection performance. SPT also achieves\ncomparable extraction performance to LoRA baselines and current leading UIE\nsystems with significantly fewer trainable parameters."}
{"id": "2506.01305", "pdf": "https://arxiv.org/pdf/2506.01305.pdf", "abs": "https://arxiv.org/abs/2506.01305", "title": "VM14K: First Vietnamese Medical Benchmark", "authors": ["Thong Nguyen", "Duc Nguyen", "Minh Dang", "Thai Dao", "Long Nguyen", "Quan H. Nguyen", "Dat Nguyen", "Kien Tran", "Minh Tran"], "categories": ["cs.CL"], "comment": null, "summary": "Medical benchmarks are indispensable for evaluating the capabilities of\nlanguage models in healthcare for non-English-speaking communities,therefore\nhelp ensuring the quality of real-life applications. However, not every\ncommunity has sufficient resources and standardized methods to effectively\nbuild and design such benchmark, and available non-English medical data is\nnormally fragmented and difficult to verify. We developed an approach to tackle\nthis problem and applied it to create the first Vietnamese medical question\nbenchmark, featuring 14,000 multiple-choice questions across 34 medical\nspecialties. Our benchmark was constructed using various verifiable sources,\nincluding carefully curated medical exams and clinical records, and eventually\nannotated by medical experts. The benchmark includes four difficulty levels,\nranging from foundational biological knowledge commonly found in textbooks to\ntypical clinical case studies that require advanced reasoning. This design\nenables assessment of both the breadth and depth of language models' medical\nunderstanding in the target language thanks to its extensive coverage and\nin-depth subject-specific expertise. We release the benchmark in three parts: a\nsample public set (4k questions), a full public set (10k questions), and a\nprivate set (2k questions) used for leaderboard evaluation. Each set contains\nall medical subfields and difficulty levels. Our approach is scalable to other\nlanguages, and we open-source our data construction pipeline to support the\ndevelopment of future multilingual benchmarks in the medical domain."}
{"id": "2506.01308", "pdf": "https://arxiv.org/pdf/2506.01308.pdf", "abs": "https://arxiv.org/abs/2506.01308", "title": "A Platform for Investigating Public Health Content with Efficient Concern Classification", "authors": ["Christopher Li", "Rickard Stureborg", "Bhuwan Dhingra", "Jun Yang"], "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": "19 pages, 15 figures", "summary": "A recent rise in online content expressing concerns with public health\ninitiatives has contributed to already stalled uptake of preemptive measures\nglobally. Future public health efforts must attempt to understand such content,\nwhat concerns it may raise among readers, and how to effectively respond to it.\nTo this end, we present ConcernScope, a platform that uses a teacher-student\nframework for knowledge transfer between large language models and light-weight\nclassifiers to quickly and effectively identify the health concerns raised in a\ntext corpus. The platform allows uploading massive files directly,\nautomatically scraping specific URLs, and direct text editing. ConcernScope is\nbuilt on top of a taxonomy of public health concerns. Intended for public\nhealth officials, we demonstrate several applications of this platform: guided\ndata exploration to find useful examples of common concerns found in online\ncommunity datasets, identification of trends in concerns through an example\ntime series analysis of 186,000 samples, and finding trends in topic frequency\nbefore and after significant events."}
{"id": "2506.01312", "pdf": "https://arxiv.org/pdf/2506.01312.pdf", "abs": "https://arxiv.org/abs/2506.01312", "title": "Growing Through Experience: Scaling Episodic Grounding in Language Models", "authors": ["Chunhui Zhang", "Sirui", "Wang", "Zhongyu Ouyang", "Xiangchi Yuan", "Soroush Vosoughi"], "categories": ["cs.CL"], "comment": "Accepted at The 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)", "summary": "Language models (LMs) require robust episodic grounding-the capacity to learn\nfrom and apply past experiences-to excel at physical planning tasks. Current\nepisodic grounding approaches struggle with scalability and integration,\nlimiting their effectiveness, especially for medium-sized LMs (7B parameters).\nWhile larger LMs (70-405B parameters) possess superior hierarchical\nrepresentations and extensive pre-trained knowledge, they encounter a\nfundamental scale paradox: despite their advanced abstraction capabilities,\nthey lack efficient mechanisms to leverage experience streams. We propose a\nscalable weak-to-strong episodic learning framework that effectively transfers\nepisodic behaviors from smaller to larger LMs. This framework integrates Monte\nCarlo tree search for structured experience collection with a novel\ndistillation method, preserving the inherent LM capabilities while embedding\nepisodic memory. Experiments demonstrate our method surpasses state-of-the-art\nproprietary LMs by 3.45% across diverse planning and question-answering tasks.\nLayer-wise probing further indicates significant improvements in task\nalignment, especially within deeper LM layers, highlighting stable\ngeneralization even for previously unseen scenarios with increased planning\ncomplexity-conditions where baseline methods degrade markedly."}
{"id": "2506.01322", "pdf": "https://arxiv.org/pdf/2506.01322.pdf", "abs": "https://arxiv.org/abs/2506.01322", "title": "Zero-Shot Text-to-Speech for Vietnamese", "authors": ["Thi Vu", "Linh The Nguyen", "Dat Quoc Nguyen"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "To appear in Proceedings of ACL 2025 (Main conference paper)", "summary": "This paper introduces PhoAudiobook, a newly curated dataset comprising 941\nhours of high-quality audio for Vietnamese text-to-speech. Using PhoAudiobook,\nwe conduct experiments on three leading zero-shot TTS models: VALL-E,\nVoiceCraft, and XTTS-V2. Our findings demonstrate that PhoAudiobook\nconsistently enhances model performance across various metrics. Moreover,\nVALL-E and VoiceCraft exhibit superior performance in synthesizing short\nsentences, highlighting their robustness in handling diverse linguistic\ncontexts. We publicly release PhoAudiobook to facilitate further research and\ndevelopment in Vietnamese text-to-speech."}
{"id": "2506.01329", "pdf": "https://arxiv.org/pdf/2506.01329.pdf", "abs": "https://arxiv.org/abs/2506.01329", "title": "Evaluating Large Language Models in Crisis Detection: A Real-World Benchmark from Psychological Support Hotlines", "authors": ["Guifeng Deng", "Shuyin Rao", "Tianyu Lin", "Anlu Dai", "Pan Wang", "Junyi Xie", "Haidong Song", "Ke Zhao", "Dongwu Xu", "Zhengdong Cheng", "Tao Li", "Haiteng Jiang"], "categories": ["cs.CL", "cs.AI"], "comment": "30 pages, 8 figures", "summary": "Psychological support hotlines are critical for crisis intervention but face\nsignificant challenges due to rising demand. Large language models (LLMs) could\nsupport crisis assessments, yet their capabilities in emotionally sensitive\ncontexts remain unclear. We introduce PsyCrisisBench, a benchmark of 540\nannotated transcripts from the Hangzhou Psychological Assistance Hotline,\nassessing four tasks: mood status recognition, suicidal ideation detection,\nsuicide plan identification, and risk assessment. We evaluated 64 LLMs across\n15 families (e.g., GPT, Claude, Gemini, Llama, Qwen, DeepSeek) using zero-shot,\nfew-shot, and fine-tuning paradigms. Performance was measured by F1-score, with\nstatistical comparisons via Welch's t-tests. LLMs performed strongly on\nsuicidal ideation detection (F1=0.880), suicide plan identification (F1=0.779),\nand risk assessment (F1=0.907), improved with few-shot and fine-tuning. Mood\nstatus recognition was more challenging (max F1=0.709), likely due to lost\nvocal cues and ambiguity. A fine-tuned 1.5B-parameter model (Qwen2.5-1.5B)\nsurpassed larger models on mood and suicidal ideation. Open-source models like\nQwQ-32B performed comparably to closed-source on most tasks (p>0.3), though\nclosed models retained an edge in mood detection (p=0.007). Performance scaled\nwith size up to a point; quantization (AWQ) reduced GPU memory by 70% with\nminimal F1 degradation. LLMs show substantial promise in structured\npsychological crisis assessments, especially with fine-tuning. Mood recognition\nremains limited due to contextual complexity. The narrowing gap between open-\nand closed-source models, combined with efficient quantization, suggests\nfeasible integration. PsyCrisisBench offers a robust evaluation framework to\nguide model development and ethical deployment in mental health."}
{"id": "2506.01334", "pdf": "https://arxiv.org/pdf/2506.01334.pdf", "abs": "https://arxiv.org/abs/2506.01334", "title": "Enhancing Interpretable Image Classification Through LLM Agents and Conditional Concept Bottleneck Models", "authors": ["Yiwen Jiang", "Deval Mehta", "Wei Feng", "Zongyuan Ge"], "categories": ["cs.CL"], "comment": "Accepted at ACL 2025 (Main)", "summary": "Concept Bottleneck Models (CBMs) decompose image classification into a\nprocess governed by interpretable, human-readable concepts. Recent advances in\nCBMs have used Large Language Models (LLMs) to generate candidate concepts.\nHowever, a critical question remains: What is the optimal number of concepts to\nuse? Current concept banks suffer from redundancy or insufficient coverage. To\naddress this issue, we introduce a dynamic, agent-based approach that adjusts\nthe concept bank in response to environmental feedback, optimizing the number\nof concepts for sufficiency yet concise coverage. Moreover, we propose\nConditional Concept Bottleneck Models (CoCoBMs) to overcome the limitations in\ntraditional CBMs' concept scoring mechanisms. It enhances the accuracy of\nassessing each concept's contribution to classification tasks and feature an\neditable matrix that allows LLMs to correct concept scores that conflict with\ntheir internal knowledge. Our evaluations across 6 datasets show that our\nmethod not only improves classification accuracy by 6% but also enhances\ninterpretability assessments by 30%."}
{"id": "2506.01340", "pdf": "https://arxiv.org/pdf/2506.01340.pdf", "abs": "https://arxiv.org/abs/2506.01340", "title": "The Landscape of Arabic Large Language Models (ALLMs): A New Era for Arabic Language Technology", "authors": ["Shahad Al-Khalifa", "Nadir Durrani", "Hend Al-Khalifa", "Firoj Alam"], "categories": ["cs.CL"], "comment": "Accepted at CACM", "summary": "The emergence of ChatGPT marked a transformative milestone for Artificial\nIntelligence (AI), showcasing the remarkable potential of Large Language Models\n(LLMs) to generate human-like text. This wave of innovation has revolutionized\nhow we interact with technology, seamlessly integrating LLMs into everyday\ntasks such as vacation planning, email drafting, and content creation. While\nEnglish-speaking users have significantly benefited from these advancements,\nthe Arabic world faces distinct challenges in developing Arabic-specific LLMs.\nArabic, one of the languages spoken most widely around the world, serves more\nthan 422 million native speakers in 27 countries and is deeply rooted in a rich\nlinguistic and cultural heritage. Developing Arabic LLMs (ALLMs) presents an\nunparalleled opportunity to bridge technological gaps and empower communities.\nThe journey of ALLMs has been both fascinating and complex, evolving from\nrudimentary text processing systems to sophisticated AI-driven models. This\narticle explores the trajectory of ALLMs, from their inception to the present\nday, highlighting the efforts to evaluate these models through benchmarks and\npublic leaderboards. We also discuss the challenges and opportunities that\nALLMs present for the Arab world."}
{"id": "2506.01341", "pdf": "https://arxiv.org/pdf/2506.01341.pdf", "abs": "https://arxiv.org/abs/2506.01341", "title": "TurnBench-MS: A Benchmark for Evaluating Multi-Turn, Multi-Step Reasoning in Large Language Models", "authors": ["Yiran Zhang", "Mo Wang", "Xiaoyang Li", "Kaixuan Ren", "Chencheng Zhu", "Usman Naseem"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "Despite impressive advances in large language models (LLMs), existing\nbenchmarks often focus on single-turn or single-step tasks, failing to capture\nthe kind of iterative reasoning required in real-world settings. To address\nthis limitation, we introduce TurnBench, a novel benchmark that evaluates\nmulti-turn, multi-step reasoning through an interactive code-breaking task\ninspired by a \"Turing Machine Board Game.\" In each episode, a model must\nuncover hidden logical or arithmetic rules by making sequential guesses,\nreceiving structured feedback, and integrating clues across multiple rounds.\nThis dynamic setup requires models to reason over time, adapt based on past\ninformation, and maintain consistency across steps-capabilities underexplored\nin current benchmarks. TurnBench includes two modes: Classic, which tests\nstandard reasoning, and Nightmare, which introduces increased complexity and\nrequires robust inferential chains. To support fine-grained analysis, we\nprovide ground-truth annotations for intermediate reasoning steps. Our\nevaluation of state-of-the-art LLMs reveals significant gaps: the best model\nachieves 81.5% accuracy in Classic mode, but performance drops to 17.8% in\nNightmare mode. In contrast, human participants achieve 100% in both,\nunderscoring the challenge TurnBench poses to current models. By incorporating\nfeedback loops and hiding task rules, TurnBench reduces contamination risks and\nprovides a rigorous testbed for diagnosing and advancing multi-step, multi-turn\nreasoning in LLMs."}
{"id": "2506.01344", "pdf": "https://arxiv.org/pdf/2506.01344.pdf", "abs": "https://arxiv.org/abs/2506.01344", "title": "Follow the Flow: Fine-grained Flowchart Attribution with Neurosymbolic Agents", "authors": ["Manan Suri", "Puneet Mathur", "Nedim Lipka", "Franck Dernoncourt", "Ryan A. Rossi", "Vivek Gupta", "Dinesh Manocha"], "categories": ["cs.CL"], "comment": null, "summary": "Flowcharts are a critical tool for visualizing decision-making processes.\nHowever, their non-linear structure and complex visual-textual relationships\nmake it challenging to interpret them using LLMs, as vision-language models\nfrequently hallucinate nonexistent connections and decision paths when\nanalyzing these diagrams. This leads to compromised reliability for automated\nflowchart processing in critical domains such as logistics, health, and\nengineering. We introduce the task of Fine-grained Flowchart Attribution, which\ntraces specific components grounding a flowchart referring LLM response.\nFlowchart Attribution ensures the verifiability of LLM predictions and improves\nexplainability by linking generated responses to the flowchart's structure. We\npropose FlowPathAgent, a neurosymbolic agent that performs fine-grained post\nhoc attribution through graph-based reasoning. It first segments the flowchart,\nthen converts it into a structured symbolic graph, and then employs an agentic\napproach to dynamically interact with the graph, to generate attribution paths.\nAdditionally, we present FlowExplainBench, a novel benchmark for evaluating\nflowchart attributions across diverse styles, domains, and question types.\nExperimental results show that FlowPathAgent mitigates visual hallucinations in\nLLM answers over flowchart QA, outperforming strong baselines by 10-14% on our\nproposed FlowExplainBench dataset."}
{"id": "2506.01347", "pdf": "https://arxiv.org/pdf/2506.01347.pdf", "abs": "https://arxiv.org/abs/2506.01347", "title": "The Surprising Effectiveness of Negative Reinforcement in LLM Reasoning", "authors": ["Xinyu Zhu", "Mengzhou Xia", "Zhepei Wei", "Wei-Lin Chen", "Danqi Chen", "Yu Meng"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) is a promising approach\nfor training language models (LMs) on reasoning tasks that elicit emergent long\nchains of thought (CoTs). Unlike supervised learning, it updates the model\nusing both correct and incorrect samples via policy gradients. To better\nunderstand its mechanism, we decompose the learning signal into reinforcing\ncorrect responses and penalizing incorrect ones, referred to as Positive and\nNegative Sample Reinforcement (PSR and NSR), respectively. We train\nQwen2.5-Math-7B and Qwen3-4B on a mathematical reasoning dataset and uncover a\nsurprising result: training with only negative samples -- without reinforcing\ncorrect responses -- can be highly effective: it consistently improves\nperformance over the base model across the entire Pass@$k$ spectrum ($k$ up to\n$256$), often matching or surpassing PPO and GRPO. In contrast, reinforcing\nonly correct responses improves Pass@$1$ but degrades performance at higher\n$k$, due to reduced diversity. These inference-scaling trends highlight that\nsolely penalizing incorrect responses may contribute more to performance than\npreviously recognized. Through gradient analysis, we show that NSR works by\nsuppressing incorrect generations and redistributing probability mass toward\nother plausible candidates, guided by the model's prior beliefs. It refines the\nmodel's existing knowledge rather than introducing entirely new behaviors.\nBuilding on this insight, we propose a simple variant of the RL objective that\nupweights NSR, and show that it consistently improves overall Pass@$k$\nperformance on MATH, AIME 2025, and AMC23. Our code is available at\nhttps://github.com/TianHongZXY/RLVR-Decomposed."}
{"id": "2506.01357", "pdf": "https://arxiv.org/pdf/2506.01357.pdf", "abs": "https://arxiv.org/abs/2506.01357", "title": "KokoroChat: A Japanese Psychological Counseling Dialogue Dataset Collected via Role-Playing by Trained Counselors", "authors": ["Zhiyang Qi", "Takumasa Kaneko", "Keiko Takamizo", "Mariko Ukiyo", "Michimasa Inaba"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Generating psychological counseling responses with language models relies\nheavily on high-quality datasets. Crowdsourced data collection methods require\nstrict worker training, and data from real-world counseling environments may\nraise privacy and ethical concerns. While recent studies have explored using\nlarge language models (LLMs) to augment psychological counseling dialogue\ndatasets, the resulting data often suffers from limited diversity and\nauthenticity. To address these limitations, this study adopts a role-playing\napproach where trained counselors simulate counselor-client interactions,\nensuring high-quality dialogues while mitigating privacy risks. Using this\nmethod, we construct KokoroChat, a Japanese psychological counseling dialogue\ndataset comprising 6,589 long-form dialogues, each accompanied by comprehensive\nclient feedback. Experimental results demonstrate that fine-tuning open-source\nLLMs with KokoroChat improves both the quality of generated counseling\nresponses and the automatic evaluation of counseling dialogues. The KokoroChat\ndataset is available at https://github.com/UEC-InabaLab/KokoroChat."}
{"id": "2506.01367", "pdf": "https://arxiv.org/pdf/2506.01367.pdf", "abs": "https://arxiv.org/abs/2506.01367", "title": "MMD-Flagger: Leveraging Maximum Mean Discrepancy to Detect Hallucinations", "authors": ["Kensuke Mitsuzawa", "Damien Garreau"], "categories": ["cs.CL", "stat.ML"], "comment": null, "summary": "Large language models (LLMs) have become pervasive in our everyday life. Yet,\na fundamental obstacle prevents their use in many critical applications: their\npropensity to generate fluent, human-quality content that is not grounded in\nreality. The detection of such hallucinations is thus of the highest\nimportance. In this work, we propose a new method to flag hallucinated content,\nMMD-Flagger. It relies on Maximum Mean Discrepancy (MMD), a non-parametric\ndistance between distributions. On a high-level perspective, MMD-Flagger tracks\nthe MMD between the generated documents and documents generated with various\ntemperature parameters. We show empirically that inspecting the shape of this\ntrajectory is sufficient to detect most hallucinations. This novel method is\nbenchmarked on two machine translation datasets, on which it outperforms\nnatural competitors."}
{"id": "2506.01381", "pdf": "https://arxiv.org/pdf/2506.01381.pdf", "abs": "https://arxiv.org/abs/2506.01381", "title": "AdaRewriter: Unleashing the Power of Prompting-based Conversational Query Reformulation via Test-Time Adaptation", "authors": ["Yilong Lai", "Jialong Wu", "Zhenglin Wang", "Deyu Zhou"], "categories": ["cs.CL"], "comment": null, "summary": "Prompting-based conversational query reformulation has emerged as a powerful\napproach for conversational search, refining ambiguous user queries into\nstandalone search queries. Best-of-N reformulation over the generated\ncandidates via prompting shows impressive potential scaling capability.\nHowever, both the previous tuning methods (training time) and adaptation\napproaches (test time) can not fully unleash their benefits. In this paper, we\npropose AdaRewriter, a novel framework for query reformulation using an\noutcome-supervised reward model via test-time adaptation. By training a\nlightweight reward model with contrastive ranking loss, AdaRewriter selects the\nmost promising reformulation during inference. Notably, it can operate\neffectively in black-box systems, including commercial LLM APIs. Experiments on\nfive conversational search datasets show that AdaRewriter significantly\noutperforms the existing methods across most settings, demonstrating the\npotential of test-time adaptation for conversational query reformulation."}
{"id": "2506.01406", "pdf": "https://arxiv.org/pdf/2506.01406.pdf", "abs": "https://arxiv.org/abs/2506.01406", "title": "Speech-to-Speech Translation Pipelines for Conversations in Low-Resource Languages", "authors": ["Andrei Popescu-Belis", "Alexis Allemann", "Teo Ferrari", "Gopal Krishnamani"], "categories": ["cs.CL"], "comment": "Proceedings of MT Summit 2025", "summary": "The popularity of automatic speech-to-speech translation for human\nconversations is growing, but the quality varies significantly depending on the\nlanguage pair. In a context of community interpreting for low-resource\nlanguages, namely Turkish and Pashto to/from French, we collected fine-tuning\nand testing data, and compared systems using several automatic metrics (BLEU,\nCOMET, and BLASER) and human assessments. The pipelines included automatic\nspeech recognition, machine translation, and speech synthesis, with local\nmodels and cloud-based commercial ones. Some components have been fine-tuned on\nour data. We evaluated over 60 pipelines and determined the best one for each\ndirection. We also found that the ranks of components are generally independent\nof the rest of the pipeline."}
{"id": "2506.01407", "pdf": "https://arxiv.org/pdf/2506.01407.pdf", "abs": "https://arxiv.org/abs/2506.01407", "title": "Comparing LLM-generated and human-authored news text using formal syntactic theory", "authors": ["Olga Zamaraeva", "Dan Flickinger", "Francis Bond", "Carlos G√≥mez-Rodr√≠guez"], "categories": ["cs.CL"], "comment": "20 pages, 15 figures, 13 tables; accepted to ACL-2025 main", "summary": "This study provides the first comprehensive comparison of New York\nTimes-style text generated by six large language models against real,\nhuman-authored NYT writing. The comparison is based on a formal syntactic\ntheory. We use Head-driven Phrase Structure Grammar (HPSG) to analyze the\ngrammatical structure of the texts. We then investigate and illustrate the\ndifferences in the distributions of HPSG grammar types, revealing systematic\ndistinctions between human and LLM-generated writing. These findings contribute\nto a deeper understanding of the syntactic behavior of LLMs as well as humans,\nwithin the NYT genre."}
{"id": "2506.01419", "pdf": "https://arxiv.org/pdf/2506.01419.pdf", "abs": "https://arxiv.org/abs/2506.01419", "title": "UniversalCEFR: Enabling Open Multilingual Research on Language Proficiency Assessment", "authors": ["Joseph Marvin Imperial", "Abdullah Barayan", "Regina Stodden", "Rodrigo Wilkens", "Ricardo Munoz Sanchez", "Lingyun Gao", "Melissa Torgbi", "Dawn Knight", "Gail Forey", "Reka R. Jablonkai", "Ekaterina Kochmar", "Robert Reynolds", "Eugenio Ribeiro", "Horacio Saggion", "Elena Volodina", "Sowmya Vajjala", "Thomas Francois", "Fernando Alva-Manchego", "Harish Tayyar Madabushi"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce UniversalCEFR, a large-scale multilingual multidimensional\ndataset of texts annotated according to the CEFR (Common European Framework of\nReference) scale in 13 languages. To enable open research in both automated\nreadability and language proficiency assessment, UniversalCEFR comprises\n505,807 CEFR-labeled texts curated from educational and learner-oriented\nresources, standardized into a unified data format to support consistent\nprocessing, analysis, and modeling across tasks and languages. To demonstrate\nits utility, we conduct benchmark experiments using three modelling paradigms:\na) linguistic feature-based classification, b) fine-tuning pre-trained LLMs,\nand c) descriptor-based prompting of instruction-tuned LLMs. Our results\nfurther support using linguistic features and fine-tuning pretrained models in\nmultilingual CEFR level assessment. Overall, UniversalCEFR aims to establish\nbest practices in data distribution in language proficiency research by\nstandardising dataset formats and promoting their accessibility to the global\nresearch community."}
{"id": "2506.01420", "pdf": "https://arxiv.org/pdf/2506.01420.pdf", "abs": "https://arxiv.org/abs/2506.01420", "title": "Self-Refining Language Model Anonymizers via Adversarial Distillation", "authors": ["Kyuyoung Kim", "Hyunjun Jeon", "Jinwoo Shin"], "categories": ["cs.CL", "cs.LG"], "comment": "Preprint", "summary": "Large language models (LLMs) are increasingly used in sensitive domains,\nwhere their ability to infer personal data from seemingly benign text poses\nemerging privacy risks. While recent LLM-based anonymization methods help\nmitigate such risks, they often rely on proprietary models (e.g., GPT-4),\nraising concerns about cost and the potential exposure of sensitive data to\nuntrusted external systems. To address this, we introduce SElf-refining\nAnonymization with Language model (SEAL), a novel distillation framework for\ntraining small language models (SLMs) to perform effective anonymization\nwithout relying on external costly models at inference time. We leverage\nadversarial interactions between an LLM anonymizer and an inference model to\ncollect trajectories of anonymized texts and inferred attributes, which are\nused to distill anonymization, adversarial inference, and utility evaluation\ncapabilities into SLMs via supervised fine-tuning and preference learning. The\nresulting models learn to both anonymize text and critique their outputs,\nenabling iterative improvement of anonymization quality via self-refinement.\nExperiments on SynthPAI, a dataset of synthetic personal profiles and text\ncomments, demonstrate that SLMs trained with SEAL achieve substantial\nimprovements in anonymization capabilities. Notably, 8B models attain a\nprivacy-utility trade-off comparable to that of the GPT-4 anonymizer and, with\nself-refinement, even surpass it in terms of privacy. These results show the\neffectiveness of our adversarial distillation framework in training SLMs as\nefficient anonymizers. To facilitate further research, we release the full\ndataset used in our experiments."}
{"id": "2506.01435", "pdf": "https://arxiv.org/pdf/2506.01435.pdf", "abs": "https://arxiv.org/abs/2506.01435", "title": "Redundancy, Isotropy, and Intrinsic Dimensionality of Prompt-based Text Embeddings", "authors": ["Hayato Tsukagoshi", "Ryohei Sasano"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Prompt-based text embedding models, which generate task-specific embeddings\nupon receiving tailored prompts, have recently demonstrated remarkable\nperformance. However, their resulting embeddings often have thousands of\ndimensions, leading to high storage costs and increased computational costs of\nembedding-based operations. In this paper, we investigate how post-hoc\ndimensionality reduction applied to the embeddings affects the performance of\nvarious tasks that leverage these embeddings, specifically classification,\nclustering, retrieval, and semantic textual similarity (STS) tasks. Our\nexperiments show that even a naive dimensionality reduction, which keeps only\nthe first 25% of the dimensions of the embeddings, results in a very slight\nperformance degradation, indicating that these embeddings are highly redundant.\nNotably, for classification and clustering, even when embeddings are reduced to\nless than 0.5% of the original dimensionality the performance degradation is\nvery small. To quantitatively analyze this redundancy, we perform an analysis\nbased on the intrinsic dimensionality and isotropy of the embeddings. Our\nanalysis reveals that embeddings for classification and clustering, which are\nconsidered to have very high dimensional redundancy, exhibit lower intrinsic\ndimensionality and less isotropy compared with those for retrieval and STS."}
{"id": "2506.01439", "pdf": "https://arxiv.org/pdf/2506.01439.pdf", "abs": "https://arxiv.org/abs/2506.01439", "title": "Whale: Large-Scale multilingual ASR model with w2v-BERT and E-Branchformer with large speech data", "authors": ["Yosuke Kashiwagi", "Hayato Futami", "Emiru Tsunoo", "Satoshi Asakawa"], "categories": ["cs.CL", "eess.AS"], "comment": null, "summary": "This paper reports on the development of a large-scale speech recognition\nmodel, Whale. Similar to models such as Whisper and OWSM, Whale leverages both\na large model size and a diverse, extensive dataset. Whale's architecture\nintegrates w2v-BERT self-supervised model, an encoder-decoder backbone built on\nE-Branchformer, and a joint CTC-attention decoding strategy. The training\ncorpus comprises varied speech data, of not only public corpora but also\nin-house data, thereby enhancing the model's robustness to different speaking\nstyles and acoustic conditions. Through evaluations on multiple benchmarks,\nWhale achieved comparable performance to existing models. In particular, it\nachieves a word error rate of 2.4% on the Librispeech test-clean set and a\ncharacter error rate of 3.4% on the CSJ eval3 set, outperforming Whisper\nlarge-v3 and OWSM v3.1."}
{"id": "2506.01451", "pdf": "https://arxiv.org/pdf/2506.01451.pdf", "abs": "https://arxiv.org/abs/2506.01451", "title": "Building Entity Association Mining Framework for Knowledge Discovery", "authors": ["Anshika Rawal", "Abhijeet Kumar", "Mridul Mishra"], "categories": ["cs.CL", "cs.IR", "I.2.7"], "comment": "Presented at Business Analytics and Intelligence Conference, IIM\n  Bengaluru", "summary": "Extracting useful signals or pattern to support important business decisions\nfor example analyzing investment product traction and discovering customer\npreference, risk monitoring etc. from unstructured text is a challenging task.\nCapturing interaction of entities or concepts and association mining is a\ncrucial component in text mining, enabling information extraction and reasoning\nover and knowledge discovery from text. Furthermore, it can be used to enrich\nor filter knowledge graphs to guide exploration processes, descriptive\nanalytics and uncover hidden stories in the text. In this paper, we introduce a\ndomain independent pipeline i.e., generalized framework to enable document\nfiltering, entity extraction using various sources (or techniques) as plug-ins\nand association mining to build any text mining business use-case and\nquantitatively define a scoring metric for ranking purpose. The proposed\nframework has three major components a) Document filtering: filtering\ndocuments/text of interest from massive amount of texts b) Configurable entity\nextraction pipeline: include entity extraction techniques i.e., i) DBpedia\nSpotlight, ii) Spacy NER, iii) Custom Entity Matcher, iv) Phrase extraction (or\ndictionary) based c) Association Relationship Mining: To generates\nco-occurrence graph to analyse potential relationships among entities,\nconcepts. Further, co-occurrence count based frequency statistics provide a\nholistic window to observe association trends or buzz rate in specific business\ncontext. The paper demonstrates the usage of framework as fundamental building\nbox in two financial use-cases namely brand product discovery and vendor risk\nmonitoring. We aim that such framework will remove duplicated effort, minimize\nthe development effort, and encourage reusability and rapid prototyping in\nassociation mining business applications for institutions."}
{"id": "2506.01458", "pdf": "https://arxiv.org/pdf/2506.01458.pdf", "abs": "https://arxiv.org/abs/2506.01458", "title": "TalTech Systems for the Interspeech 2025 ML-SUPERB 2.0 Challenge", "authors": ["Tanel Alum√§e", "Artem Fedorchenko"], "categories": ["cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "This paper describes the language identification and multilingual speech\nrecognition system developed at Tallinn University of Technology for the\nInterspeech 2025 ML-SUPERB 2.0 Challenge. A hybrid language identification\nsystem is used, consisting of a pretrained language embedding model and a\nlight-weight speech recognition model with a shared encoder across languages\nand language-specific bigram language models. For speech recognition, three\nmodels are used, where only a single model is applied for each language,\ndepending on the training data availability and performance on held-out data.\nThe model set consists of a finetuned version of SeamlessM4T, MMS-1B-all with\ncustom language adapters and MMS-zeroshot. The system obtained the top overall\nscore in the challenge."}
{"id": "2506.01474", "pdf": "https://arxiv.org/pdf/2506.01474.pdf", "abs": "https://arxiv.org/abs/2506.01474", "title": "Integrating Neural and Symbolic Components in a Model of Pragmatic Question-Answering", "authors": ["Polina Tsvilodub", "Robert D. Hawkins", "Michael Franke"], "categories": ["cs.CL"], "comment": "16 pages, 16 figures. To appear in the proceedings of Society for\n  Computation in Linguistics (SCiL) 2025", "summary": "Computational models of pragmatic language use have traditionally relied on\nhand-specified sets of utterances and meanings, limiting their applicability to\nreal-world language use. We propose a neuro-symbolic framework that enhances\nprobabilistic cognitive models by integrating LLM-based modules to propose and\nevaluate key components in natural language, eliminating the need for manual\nspecification. Through a classic case study of pragmatic question-answering, we\nsystematically examine various approaches to incorporating neural modules into\nthe cognitive model -- from evaluating utilities and literal semantics to\ngenerating alternative utterances and goals. We find that hybrid models can\nmatch or exceed the performance of traditional probabilistic models in\npredicting human answer patterns. However, the success of the neuro-symbolic\nmodel depends critically on how LLMs are integrated: while they are\nparticularly effective for proposing alternatives and transforming abstract\ngoals into utilities, they face challenges with truth-conditional semantic\nevaluation. This work charts a path toward more flexible and scalable models of\npragmatic language use while illuminating crucial design considerations for\nbalancing neural and symbolic components."}
{"id": "2506.01484", "pdf": "https://arxiv.org/pdf/2506.01484.pdf", "abs": "https://arxiv.org/abs/2506.01484", "title": "LLM in the Loop: Creating the PARADEHATE Dataset for Hate Speech Detoxification", "authors": ["Shuzhou Yuan", "Ercong Nie", "Lukas Kouba", "Ashish Yashwanth Kangen", "Helmut Schmid", "Hinrich Schutze", "Michael Farber"], "categories": ["cs.CL"], "comment": null, "summary": "Detoxification, the task of rewriting harmful language into non-toxic text,\nhas become increasingly important amid the growing prevalence of toxic content\nonline. However, high-quality parallel datasets for detoxification, especially\nfor hate speech, remain scarce due to the cost and sensitivity of human\nannotation. In this paper, we propose a novel LLM-in-the-loop pipeline\nleveraging GPT-4o-mini for automated detoxification. We first replicate the\nParaDetox pipeline by replacing human annotators with an LLM and show that the\nLLM performs comparably to human annotation. Building on this, we construct\nPARADEHATE, a large-scale parallel dataset specifically for hatespeech\ndetoxification. We release PARADEHATE as a benchmark of over 8K hate/non-hate\ntext pairs and evaluate a wide range of baseline methods. Experimental results\nshow that models such as BART, fine-tuned on PARADEHATE, achieve better\nperformance in style accuracy, content preservation, and fluency, demonstrating\nthe effectiveness of LLM-generated detoxification text as a scalable\nalternative to human annotation."}
{"id": "2506.01488", "pdf": "https://arxiv.org/pdf/2506.01488.pdf", "abs": "https://arxiv.org/abs/2506.01488", "title": "Argument-Centric Causal Intervention Method for Mitigating Bias in Cross-Document Event Coreference Resolution", "authors": ["Long Yao", "Wenzhong Yang", "Yabo Yin", "Fuyuan Wei", "Hongzhen Lv", "Jiaren Peng", "Liejun Wang", "Xiaoming Tao"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Cross-document Event Coreference Resolution (CD-ECR) is a fundamental task in\nnatural language processing (NLP) that seeks to determine whether event\nmentions across multiple documents refer to the same real-world occurrence.\nHowever, current CD-ECR approaches predominantly rely on trigger features\nwithin input mention pairs, which induce spurious correlations between\nsurface-level lexical features and coreference relationships, impairing the\noverall performance of the models. To address this issue, we propose a novel\ncross-document event coreference resolution method based on Argument-Centric\nCausal Intervention (ACCI). Specifically, we construct a structural causal\ngraph to uncover confounding dependencies between lexical triggers and\ncoreference labels, and introduce backdoor-adjusted interventions to isolate\nthe true causal effect of argument semantics. To further mitigate spurious\ncorrelations, ACCI integrates a counterfactual reasoning module that quantifies\nthe causal influence of trigger word perturbations, and an argument-aware\nenhancement module to promote greater sensitivity to semantically grounded\ninformation. In contrast to prior methods that depend on costly data\naugmentation or heuristic-based filtering, ACCI enables effective debiasing in\na unified end-to-end framework without altering the underlying training\nprocedure. Extensive experiments demonstrate that ACCI achieves CoNLL F1 of\n88.4% on ECB+ and 85.2% on GVC, achieving state-of-the-art performance. The\nimplementation and materials are available at https://github.com/era211/ACCI."}
{"id": "2506.01489", "pdf": "https://arxiv.org/pdf/2506.01489.pdf", "abs": "https://arxiv.org/abs/2506.01489", "title": "Multilingual Definition Modeling", "authors": ["Edison Marrese-Taylor", "Erica K. Shimomoto", "Alfredo Solano", "Enrique Reid"], "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we propose the first multilingual study on definition\nmodeling. We use monolingual dictionary data for four new languages (Spanish,\nFrench, Portuguese, and German) and perform an in-depth empirical study to test\nthe performance of pre-trained multilingual language models on definition\nmodeling of monosemic words when finetuned on this data. Furthermore, we use a\nzero-shot approach to test the multilingual capabilities of two popular\nchat-based Large Language Models (LLMs) in the task. Results show that\nmultilingual language models can perform on-pair with English but cannot\nleverage potential cross-lingual synergies, with LLMs generally offering better\nperformance overall. A comprehensive human evaluation of the LLM-generated\ndefinition highlights the zero and few-shot capabilities of these models in\nthis new task, also showing their shortcomings. Finally, we show that\nperformance on our task via BERTScore strongly correlates to the performance on\nmultilingual LLM benchmarks, suggesting that our task offers a viable\ncompute-constrained, stable and natural alternative to these."}
{"id": "2506.01495", "pdf": "https://arxiv.org/pdf/2506.01495.pdf", "abs": "https://arxiv.org/abs/2506.01495", "title": "CVC: A Large-Scale Chinese Value Rule Corpus for Value Alignment of Large Language Models", "authors": ["Ping Wu", "Guobin Shen", "Dongcheng Zhao", "Yuwei Wang", "Yiting Dong", "Yu Shi", "Enmeng Lu", "Feifei Zhao", "Yi Zeng"], "categories": ["cs.CL"], "comment": null, "summary": "Ensuring that Large Language Models (LLMs) align with mainstream human values\nand ethical norms is crucial for the safe and sustainable development of AI.\nCurrent value evaluation and alignment are constrained by Western cultural bias\nand incomplete domestic frameworks reliant on non-native rules; furthermore,\nthe lack of scalable, rule-driven scenario generation methods makes evaluations\ncostly and inadequate across diverse cultural contexts. To address these\nchallenges, we propose a hierarchical value framework grounded in core Chinese\nvalues, encompassing three main dimensions, 12 core values, and 50 derived\nvalues. Based on this framework, we construct a large-scale Chinese Values\nCorpus (CVC) containing over 250,000 value rules enhanced and expanded through\nhuman annotation. Experimental results show that CVC-guided scenarios\noutperform direct generation ones in value boundaries and content diversity. In\nthe evaluation across six sensitive themes (e.g., surrogacy, suicide), seven\nmainstream LLMs preferred CVC-generated options in over 70.5% of cases, while\nfive Chinese human annotators showed an 87.5% alignment with CVC, confirming\nits universality, cultural relevance, and strong alignment with Chinese values.\nAdditionally, we construct 400,000 rule-based moral dilemma scenarios that\nobjectively capture nuanced distinctions in conflicting value prioritization\nacross 17 LLMs. Our work establishes a culturally-adaptive benchmarking\nframework for comprehensive value evaluation and alignment, representing\nChinese characteristics. All data are available at\nhttps://huggingface.co/datasets/Beijing-AISI/CVC, and the code is available at\nhttps://github.com/Beijing-AISI/CVC."}
{"id": "2506.01496", "pdf": "https://arxiv.org/pdf/2506.01496.pdf", "abs": "https://arxiv.org/abs/2506.01496", "title": "Continual Speech Learning with Fused Speech Features", "authors": ["Guitao Wang", "Jinming Zhao", "Hao Yang", "Guilin Qi", "Tongtong Wu", "Gholamreza Haffari"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Submitted to Interspeech 2025", "summary": "Rapid growth in speech data demands adaptive models, as traditional static\nmethods fail to keep pace with dynamic and diverse speech information. We\nintroduce continuous speech learning, a new set-up targeting at bridging the\nadaptation gap in current speech models. We use the encoder-decoder Whisper\nmodel to standardize speech tasks into a generative format. We integrate a\nlearnable gated-fusion layer on the top of the encoder to dynamically select\ntask-specific features for downstream tasks. Our approach improves accuracy\nsignificantly over traditional methods in six speech processing tasks,\ndemonstrating gains in adapting to new speech tasks without full retraining."}
{"id": "2506.01512", "pdf": "https://arxiv.org/pdf/2506.01512.pdf", "abs": "https://arxiv.org/abs/2506.01512", "title": "Representations of Fact, Fiction and Forecast in Large Language Models: Epistemics and Attitudes", "authors": ["Meng Li", "Michael Vrazitulis", "David Schlangen"], "categories": ["cs.CL", "cs.AI"], "comment": "accepted by ACL 2025 (main)", "summary": "Rational speakers are supposed to know what they know and what they do not\nknow, and to generate expressions matching the strength of evidence. In\ncontrast, it is still a challenge for current large language models to generate\ncorresponding utterances based on the assessment of facts and confidence in an\nuncertain real-world environment. While it has recently become popular to\nestimate and calibrate confidence of LLMs with verbalized uncertainty, what is\nlacking is a careful examination of the linguistic knowledge of uncertainty\nencoded in the latent space of LLMs. In this paper, we draw on typological\nframeworks of epistemic expressions to evaluate LLMs' knowledge of epistemic\nmodality, using controlled stories. Our experiments show that the performance\nof LLMs in generating epistemic expressions is limited and not robust, and\nhence the expressions of uncertainty generated by LLMs are not always reliable.\nTo build uncertainty-aware LLMs, it is necessary to enrich semantic knowledge\nof epistemic modality in LLMs."}
{"id": "2506.01520", "pdf": "https://arxiv.org/pdf/2506.01520.pdf", "abs": "https://arxiv.org/abs/2506.01520", "title": "FormFactory: An Interactive Benchmarking Suite for Multimodal Form-Filling Agents", "authors": ["Bobo Li", "Yuheng Wang", "Hao Fei", "Juncheng Li", "Wei Ji", "Mong-Li Lee", "Wynne Hsu"], "categories": ["cs.CL"], "comment": "8 pages, 7 figures", "summary": "Online form filling is a common yet labor-intensive task involving extensive\nkeyboard and mouse interactions. Despite the long-standing vision of automating\nthis process with \"one click\", existing tools remain largely rule-based and\nlack generalizable, generative capabilities. Recent advances in Multimodal\nLarge Language Models (MLLMs) have enabled promising agents for GUI-related\ntasks in general-purpose scenarios. However, they struggle with the unique\nchallenges of form filling, such as flexible layouts and the difficulty of\naligning textual instructions with on-screen fields. To bridge this gap, we\nformally define the form-filling task and propose FormFactory, an interactive\nbenchmarking suite comprising a web-based interface, backend evaluation module,\nand carefully constructed dataset. Our benchmark covers diverse real-world\nscenarios, incorporates various field formats, and simulates high-fidelity form\ninteractions. We conduct a comprehensive evaluation of state-of-the-art MLLMs\nand observe that no model surpasses 5% accuracy, underscoring the inherent\ndifficulty of the task. These findings also reveal significant limitations in\ncurrent models' visual layout reasoning and field-value alignment abilities. We\nhope our benchmark can serve as a stepping stone for further research into\nrobust, practical form-filling agents."}
{"id": "2506.01524", "pdf": "https://arxiv.org/pdf/2506.01524.pdf", "abs": "https://arxiv.org/abs/2506.01524", "title": "V-VAE: A Variational Auto Encoding Framework Towards Fine-Grained Control over Human-Like Chat", "authors": ["Qi Lin", "Weikai Xu", "Lisi Chen", "Bin Dai"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "With the continued proliferation of Large Language Model (LLM) based\nchatbots, there is a growing demand for generating responses that are not only\nlinguistically fluent but also consistently aligned with persona-specific\ntraits in conversations. However, existing role-play and persona-based chat\napproaches rely heavily on static role descriptions, coarse-grained signal\nspace, and low-quality synthetic data, which fail to capture dynamic\nfine-grained details in human-like chat. Human-like chat requires modeling\nsubtle latent traits, such as emotional tone, situational awareness, and\nevolving personality, which are difficult to predefine and cannot be easily\nlearned from synthetic or distillation-based data. To address these\nlimitations, we propose a Verbal Variational Auto-Encoding (V-VAE) framework,\ncontaining a variational auto-encoding module and fine-grained control space\nwhich dynamically adapts dialogue behaviour based on fine-grained,\ninterpretable latent variables across talking style, interaction patterns, and\npersonal attributes. We also construct a high-quality dataset, HumanChatData,\nand benchmark HumanChatBench to address the scarcity of high-quality data in\nthe human-like domain. Experiments show that LLMs based on V-VAE consistently\noutperform standard baselines on HumanChatBench and DialogBench, which further\ndemonstrates the effectiveness of V-VAE and HumanChatData."}
{"id": "2506.01531", "pdf": "https://arxiv.org/pdf/2506.01531.pdf", "abs": "https://arxiv.org/abs/2506.01531", "title": "STORM-BORN: A Challenging Mathematical Derivations Dataset Curated via a Human-in-the-Loop Multi-Agent Framework", "authors": ["Wenhao Liu", "Zhenyi Lu", "Xinyu Hu", "Jierui Zhang", "Dailin Li", "Jiacheng Cen", "Huilin Cao", "Haiteng Wang", "Yuhan Li", "Kun Xie", "Dandan Li", "Pei Zhang", "Chengbo Zhang", "Yuxiang Ren", "Xiaohong Huang", "Yan Ma"], "categories": ["cs.CL"], "comment": "accepted by ACL2025", "summary": "High-quality math datasets are crucial for advancing the reasoning abilities\nof large language models (LLMs). However, existing datasets often suffer from\nthree key issues: outdated and insufficient challenging content, neglecting\nhuman-like reasoning, and limited reliability due to single-LLM generation. To\naddress these, we introduce $\\textbf{STORM-BORN}$, an ultra-challenging dataset\nof mathematical derivations sourced from cutting-edge academic papers, which\nincludes dense human-like approximations and heuristic cues. To ensure the\nreliability and quality, we propose a novel human-in-the-loop, multi-agent data\ngeneration framework, integrating reasoning-dense filters, multi-agent\ncollaboration, and human mathematicians' evaluations. We curated a set of 2,000\nsynthetic samples and deliberately selected the 100 most difficult problems.\nEven most advanced models like GPT-o1 solved fewer than $5\\%$ of them.\nFine-tuning on STORM-BORN boosts accuracy by $7.84\\%$ (LLaMA3-8B) and $9.12\\%$\n(Qwen2.5-7B). As AI approaches mathematician-level reasoning, STORM-BORN\nprovides both a high-difficulty benchmark and a human-like reasoning training\nresource. Our code and dataset are publicly available at\nhttps://github.com/lwhere/STORM-BORN."}
{"id": "2506.01535", "pdf": "https://arxiv.org/pdf/2506.01535.pdf", "abs": "https://arxiv.org/abs/2506.01535", "title": "Dictionaries to the Rescue: Cross-Lingual Vocabulary Transfer for Low-Resource Languages Using Bilingual Dictionaries", "authors": ["Haruki Sakajo", "Yusuke Ide", "Justin Vasselli", "Yusuke Sakai", "Yingtao Tian", "Hidetaka Kamigaito", "Taro Watanabe"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 Findings", "summary": "Cross-lingual vocabulary transfer plays a promising role in adapting\npre-trained language models to new languages, including low-resource languages.\nExisting approaches that utilize monolingual or parallel corpora face\nchallenges when applied to languages with limited resources. In this work, we\npropose a simple yet effective vocabulary transfer method that utilizes\nbilingual dictionaries, which are available for many languages, thanks to\ndescriptive linguists. Our proposed method leverages a property of BPE\ntokenizers where removing a subword from the vocabulary causes a fallback to\nshorter subwords. The embeddings of target subwords are estimated iteratively\nby progressively removing them from the tokenizer. The experimental results\nshow that our approach outperforms existing methods for low-resource languages,\ndemonstrating the effectiveness of a dictionary-based approach for\ncross-lingual vocabulary transfer."}
{"id": "2506.01565", "pdf": "https://arxiv.org/pdf/2506.01565.pdf", "abs": "https://arxiv.org/abs/2506.01565", "title": "Hanfu-Bench: A Multimodal Benchmark on Cross-Temporal Cultural Understanding and Transcreation", "authors": ["Li Zhou", "Lutong Yu", "Dongchu Xie", "Shaohuan Cheng", "Wenyan Li", "Haizhou Li"], "categories": ["cs.CL", "cs.CV"], "comment": "cultural analysis, cultural visual understanding, cultural image\n  transcreation", "summary": "Culture is a rich and dynamic domain that evolves across both geography and\ntime. However, existing studies on cultural understanding with vision-language\nmodels (VLMs) primarily emphasize geographic diversity, often overlooking the\ncritical temporal dimensions. To bridge this gap, we introduce Hanfu-Bench, a\nnovel, expert-curated multimodal dataset. Hanfu, a traditional garment spanning\nancient Chinese dynasties, serves as a representative cultural heritage that\nreflects the profound temporal aspects of Chinese culture while remaining\nhighly popular in Chinese contemporary society. Hanfu-Bench comprises two core\ntasks: cultural visual understanding and cultural image transcreation.The\nformer task examines temporal-cultural feature recognition based on single- or\nmulti-image inputs through multiple-choice visual question answering, while the\nlatter focuses on transforming traditional attire into modern designs through\ncultural element inheritance and modern context adaptation. Our evaluation\nshows that closed VLMs perform comparably to non-experts on visual cutural\nunderstanding but fall short by 10\\% to human experts, while open VLMs lags\nfurther behind non-experts. For the transcreation task, multi-faceted human\nevaluation indicates that the best-performing model achieves a success rate of\nonly 42\\%. Our benchmark provides an essential testbed, revealing significant\nchallenges in this new direction of temporal cultural understanding and\ncreative adaptation."}
{"id": "2506.01578", "pdf": "https://arxiv.org/pdf/2506.01578.pdf", "abs": "https://arxiv.org/abs/2506.01578", "title": "Prompt Engineering Large Language Models' Forecasting Capabilities", "authors": ["Philipp Schoenegger", "Cameron R. Jones", "Philip E. Tetlock", "Barbara Mellers"], "categories": ["cs.CL"], "comment": null, "summary": "Large language model performance can be improved in a large number of ways.\nMany such techniques, like fine-tuning or advanced tool usage, are\ntime-intensive and expensive. Although prompt engineering is significantly\ncheaper and often works for simpler tasks, it remains unclear whether prompt\nengineering suffices for more complex domains like forecasting. Here we show\nthat small prompt modifications rarely boost forecasting accuracy beyond a\nminimal baseline. In our first study, we tested 38 prompts across Claude 3.5\nSonnet, Claude 3.5 Haiku, GPT-4o, and Llama 3.1 405B. In our second, we\nintroduced compound prompts and prompts from external sources, also including\nthe reasoning models o1 and o1-mini. Our results show that most prompts lead to\nnegligible gains, although references to base rates yield slight benefits.\nSurprisingly, some strategies showed strong negative effects on accuracy:\nespecially encouraging the model to engage in Bayesian reasoning. These results\nsuggest that, in the context of complex tasks like forecasting, basic prompt\nrefinements alone offer limited gains, implying that more robust or specialized\ntechniques may be required for substantial performance improvements in AI\nforecasting."}
{"id": "2506.01587", "pdf": "https://arxiv.org/pdf/2506.01587.pdf", "abs": "https://arxiv.org/abs/2506.01587", "title": "Unified Large Language Models for Misinformation Detection in Low-Resource Linguistic Settings", "authors": ["Muhammad Islam", "Javed Ali Khan", "Mohammed Abaker", "Ali Daud", "Azeem Irshad"], "categories": ["cs.CL"], "comment": null, "summary": "The rapid expansion of social media platforms has significantly increased the\ndissemination of forged content and misinformation, making the detection of\nfake news a critical area of research. Although fact-checking efforts\npredominantly focus on English-language news, there is a noticeable gap in\nresources and strategies to detect news in regional languages, such as Urdu.\nAdvanced Fake News Detection (FND) techniques rely heavily on large, accurately\nlabeled datasets. However, FND in under-resourced languages like Urdu faces\nsubstantial challenges due to the scarcity of extensive corpora and the lack of\nvalidated lexical resources. Current Urdu fake news datasets are often\ndomain-specific and inaccessible to the public. They also lack human\nverification, relying mainly on unverified English-to-Urdu translations, which\ncompromises their reliability in practical applications. This study highlights\nthe necessity of developing reliable, expert-verified, and domain-independent\nUrdu-enhanced FND datasets to improve fake news detection in Urdu and other\nresource-constrained languages. This paper presents the first benchmark large\nFND dataset for Urdu news, which is publicly available for validation and deep\nanalysis. We also evaluate this dataset using multiple state-of-the-art\npre-trained large language models (LLMs), such as XLNet, mBERT, XLM-RoBERTa,\nRoBERTa, DistilBERT, and DeBERTa. Additionally, we propose a unified LLM model\nthat outperforms the others with different embedding and feature extraction\ntechniques. The performance of these models is compared based on accuracy, F1\nscore, precision, recall, and human judgment for vetting the sample results of\nnews."}
{"id": "2506.01592", "pdf": "https://arxiv.org/pdf/2506.01592.pdf", "abs": "https://arxiv.org/abs/2506.01592", "title": "Statement-Tuning Enables Efficient Cross-lingual Generalization in Encoder-only Models", "authors": ["Ahmed Elshabrawy", "Thanh-Nhi Nguyen", "Yeeun Kang", "Lihan Feng", "Annant Jain", "Faadil Abdullah Shaikh", "Jonibek Mansurov", "Mohamed Fazli Mohamed Imam", "Jesus-German Ortiz-Barajas", "Rendi Chevi", "Alham Fikri Aji"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Findings)", "summary": "Large Language Models (LLMs) excel in zero-shot and few-shot tasks, but\nachieving similar performance with encoder-only models like BERT and RoBERTa\nhas been challenging due to their architecture. However, encoders offer\nadvantages such as lower computational and memory costs. Recent work adapts\nthem for zero-shot generalization using Statement Tuning, which reformulates\ntasks into finite templates. We extend this approach to multilingual NLP,\nexploring whether encoders can achieve zero-shot cross-lingual generalization\nand serve as efficient alternatives to memory-intensive LLMs for low-resource\nlanguages. Our results show that state-of-the-art encoder models generalize\nwell across languages, rivaling multilingual LLMs while being more efficient.\nWe also analyze multilingual Statement Tuning dataset design, efficiency gains,\nand language-specific generalization, contributing to more inclusive and\nresource-efficient NLP models. We release our code and models."}
{"id": "2506.01602", "pdf": "https://arxiv.org/pdf/2506.01602.pdf", "abs": "https://arxiv.org/abs/2506.01602", "title": "MMD-Sense-Analysis: Word Sense Detection Leveraging Maximum Mean Discrepancy", "authors": ["Kensuke Mitsuzawa"], "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": null, "summary": "Word sense analysis is an essential analysis work for interpreting the\nlinguistic and social backgrounds. The word sense change detection is a task of\nidentifying and interpreting shifts in word meanings over time. This paper\nproposes MMD-Sense-Analysis, a novel approach that leverages Maximum Mean\nDiscrepancy (MMD) to select semantically meaningful variables and quantify\nchanges across time periods. This method enables both the identification of\nwords undergoing sense shifts and the explanation of their evolution over\nmultiple historical periods. To my knowledge, this is the first application of\nMMD to word sense change detection. Empirical assessment results demonstrate\nthe effectiveness of the proposed approach."}
{"id": "2506.01615", "pdf": "https://arxiv.org/pdf/2506.01615.pdf", "abs": "https://arxiv.org/abs/2506.01615", "title": "IndicRAGSuite: Large-Scale Datasets and a Benchmark for Indian Language RAG Systems", "authors": ["Pasunuti Prasanjith", "Prathmesh B More", "Anoop Kunchukuttan", "Raj Dabre"], "categories": ["cs.CL"], "comment": "WIP", "summary": "Retrieval-Augmented Generation (RAG) systems enable language models to access\nrelevant information and generate accurate, well-grounded, and contextually\ninformed responses. However, for Indian languages, the development of\nhigh-quality RAG systems is hindered by the lack of two critical resources: (1)\nevaluation benchmarks for retrieval and generation tasks, and (2) large-scale\ntraining datasets for multilingual retrieval. Most existing benchmarks and\ndatasets are centered around English or high-resource languages, making it\ndifficult to extend RAG capabilities to the diverse linguistic landscape of\nIndia. To address the lack of evaluation benchmarks, we create IndicMSMarco, a\nmultilingual benchmark for evaluating retrieval quality and response generation\nin 13 Indian languages, created via manual translation of 1000 diverse queries\nfrom MS MARCO-dev set. To address the need for training data, we build a\nlarge-scale dataset of (question, answer, relevant passage) tuples derived from\nthe Wikipedias of 19 Indian languages using state-of-the-art LLMs.\nAdditionally, we include translated versions of the original MS MARCO dataset\nto further enrich the training data and ensure alignment with real-world\ninformation-seeking tasks. Resources are available here:\nhttps://huggingface.co/datasets/ai4bharat/Indic-Rag-Suite"}
{"id": "2506.01621", "pdf": "https://arxiv.org/pdf/2506.01621.pdf", "abs": "https://arxiv.org/abs/2506.01621", "title": "Domain Lexical Knowledge-based Word Embedding Learning for Text Classification under Small Data", "authors": ["Zixiao Zhu", "Kezhi Mao"], "categories": ["cs.CL"], "comment": "13 pages, 2 figures", "summary": "Pre-trained language models such as BERT have been proved to be powerful in\nmany natural language processing tasks. But in some text classification\napplications such as emotion recognition and sentiment analysis, BERT may not\nlead to satisfactory performance. This often happens in applications where\nkeywords play critical roles in the prediction of class labels. Our\ninvestigation found that the root cause of the problem is that the\ncontext-based BERT embedding of the keywords may not be discriminative enough\nto produce discriminative text representation for classification. Motivated by\nthis finding, we develop a method to enhance word embeddings using\ndomain-specific lexical knowledge. The knowledge-based embedding enhancement\nmodel projects the BERT embedding into a new space where within-class\nsimilarity and between-class difference are maximized. To implement the\nknowledge-based word embedding enhancement model, we also develop a knowledge\nacquisition algorithm for automatically collecting lexical knowledge from\nonline open sources. Experiment results on three classification tasks,\nincluding sentiment analysis, emotion recognition and question answering, have\nshown the effectiveness of our proposed word embedding enhancing model. The\ncodes and datasets are in https://github.com/MidiyaZhu/KVWEFFER."}
{"id": "2506.01627", "pdf": "https://arxiv.org/pdf/2506.01627.pdf", "abs": "https://arxiv.org/abs/2506.01627", "title": "MVAN: Multi-View Attention Networks for Fake News Detection on Social Media", "authors": ["Shiwen Ni", "Jiawen Li", "Hung-Yu Kao"], "categories": ["cs.CL"], "comment": null, "summary": "Fake news on social media is a widespread and serious problem in today's\nsociety. Existing fake news detection methods focus on finding clues from Long\ntext content, such as original news articles and user comments. This paper\nsolves the problem of fake news detection in more realistic scenarios. Only\nsource shot-text tweet and its retweet users are provided without user\ncomments. We develop a novel neural network based model,\n\\textbf{M}ulti-\\textbf{V}iew \\textbf{A}ttention \\textbf{N}etworks (MVAN) to\ndetect fake news and provide explanations on social media. The MVAN model\nincludes text semantic attention and propagation structure attention, which\nensures that our model can capture information and clues both of source tweet\ncontent and propagation structure. In addition, the two attention mechanisms in\nthe model can find key clue words in fake news texts and suspicious users in\nthe propagation structure. We conduct experiments on two real-world datasets,\nand the results demonstrate that MVAN can significantly outperform\nstate-of-the-art methods by 2.5\\% in accuracy on average, and produce a\nreasonable explanation."}
{"id": "2506.01629", "pdf": "https://arxiv.org/pdf/2506.01629.pdf", "abs": "https://arxiv.org/abs/2506.01629", "title": "Cross-Lingual Generalization and Compression: From Language-Specific to Shared Neurons", "authors": ["Frederick Riemenschneider", "Anette Frank"], "categories": ["cs.CL", "I.2.4; I.2.7"], "comment": "Paper accepted for publication at ACL 2025 Main; 10 pages, 20\n  figures, 4 tables", "summary": "Multilingual language models (MLLMs) have demonstrated remarkable abilities\nto transfer knowledge across languages, despite being trained without explicit\ncross-lingual supervision. We analyze the parameter spaces of three MLLMs to\nstudy how their representations evolve during pre-training, observing patterns\nconsistent with compression: models initially form language-specific\nrepresentations, which gradually converge into cross-lingual abstractions as\ntraining progresses. Through probing experiments, we observe a clear transition\nfrom uniform language identification capabilities across layers to more\nspecialized layer functions. For deeper analysis, we focus on neurons that\nencode distinct semantic concepts. By tracing their development during\npre-training, we show how they gradually align across languages. Notably, we\nidentify specific neurons that emerge as increasingly reliable predictors for\nthe same concepts across languages."}
{"id": "2506.01646", "pdf": "https://arxiv.org/pdf/2506.01646.pdf", "abs": "https://arxiv.org/abs/2506.01646", "title": "ESGenius: Benchmarking LLMs on Environmental, Social, and Governance (ESG) and Sustainability Knowledge", "authors": ["Chaoyue He", "Xin Zhou", "Yi Wu", "Xinjia Yu", "Yan Zhang", "Lei Zhang", "Di Wang", "Shengfei Lyu", "Hong Xu", "Xiaoqiao Wang", "Wei Liu", "Chunyan Miao"], "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; H.3.3"], "comment": "37 pages, 8 figures, 11 tables", "summary": "We introduce ESGenius, a comprehensive benchmark for evaluating and enhancing\nthe proficiency of Large Language Models (LLMs) in Environmental, Social and\nGovernance (ESG) and sustainability-focused question answering. ESGenius\ncomprises two key components: (i) ESGenius-QA, a collection of 1 136\nmultiple-choice questions generated by LLMs and rigorously validated by domain\nexperts, covering a broad range of ESG pillars and sustainability topics. Each\nquestion is systematically linked to its corresponding source text, enabling\ntransparent evaluation and supporting retrieval-augmented generation (RAG)\nmethods; and (ii) ESGenius-Corpus, a meticulously curated repository of 231\nfoundational frameworks, standards, reports and recommendation documents from\nseven authoritative sources. Moreover, to fully assess the capabilities and\nadaptation potential of the model, we implement a rigorous two-stage evaluation\nprotocol -- Zero-Shot and RAG. Extensive experiments across 50 LLMs (ranging\nfrom 0.5 B to 671 B parameters) demonstrate that state-of-the-art models\nachieve only moderate performance in zero-shot settings, with accuracies\ntypically around 55--70\\%, highlighting ESGenius's challenging nature for LLMs\nin interdisciplinary contexts. However, models employing RAG show significant\nperformance improvements, particularly for smaller models. For example,\n\"DeepSeek-R1-Distill-Qwen-14B\" improves from 63.82\\% (zero-shot) to 80.46\\%\nwith RAG. These results underscore the necessity of grounding responses in\nauthoritative sources for enhanced ESG understanding. To the best of our\nknowledge, ESGenius is the first benchmark curated for LLMs and the relevant\nenhancement technologies that focuses on ESG and sustainability topics."}
{"id": "2506.01675", "pdf": "https://arxiv.org/pdf/2506.01675.pdf", "abs": "https://arxiv.org/abs/2506.01675", "title": "Cross-Lingual Transfer of Cultural Knowledge: An Asymmetric Phenomenon", "authors": ["Chen Zhang", "Zhiyuan Liao", "Yansong Feng"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Despite substantial research efforts evaluating how well large language\nmodels~(LLMs) handle global cultural diversity, the mechanisms behind their\ncultural knowledge acquisition, particularly in multilingual settings, remain\nunclear. We study this question by investigating how cultural knowledge\ntransfers across languages during language adaptation of LLMs. We introduce an\ninterpretable framework for studying this transfer, ensuring training data\ntransparency and controlling transfer effects. Through a study of four\nnon-Anglophonic cultures, we observe bidirectional cultural transfer between\nEnglish and other high-resource languages, while low-resource languages\nprimarily transfer knowledge to English with limited reverse flow. To explain\nthis asymmetric phenomenon, we propose a frequency-based hypothesis: cultural\nknowledge appearing more frequently in the pretraining data transfers more\neasily, which is supported by empirical analysis of the training corpora."}
{"id": "2506.01687", "pdf": "https://arxiv.org/pdf/2506.01687.pdf", "abs": "https://arxiv.org/abs/2506.01687", "title": "StochasTok: Improving Fine-Grained Subword Understanding in LLMs", "authors": ["Anya Sims", "Thom Foster", "Klara Kaleb", "Tuan-Duy H. Nguyen", "Joseph Lee", "Jakob N. Foerster", "Yee Whye Teh", "Cong Lu"], "categories": ["cs.CL"], "comment": null, "summary": "Subword-level understanding is integral to numerous tasks, including\nunderstanding multi-digit numbers, spelling mistakes, abbreviations, rhyming,\nand wordplay. Despite this, current large language models (LLMs) still often\nstruggle with seemingly simple subword-level tasks like How many 'r's in\n'strawberry'?. A key factor behind these failures is tokenization which\nobscures the fine-grained structure of words. Current alternatives, such as\ncharacter-level and dropout tokenization methods, significantly increase\ncomputational costs and provide inconsistent improvements. In this paper we\nrevisit tokenization and introduce StochasTok, a simple, efficient stochastic\ntokenization scheme that randomly splits tokens during training, allowing LLMs\nto 'see' their internal structure. Our experiments show that pretraining with\nStochasTok substantially improves LLMs' downstream performance across multiple\nsubword-level language games, including character counting, substring\nidentification, and math tasks. Furthermore, StochasTok's simplicity allows\nseamless integration at any stage of the training pipeline; and we demonstrate\nthat post-training with StochasTok can instill improved subword understanding\ninto existing pretrained models, thus avoiding costly pretraining from scratch.\nThese dramatic improvements achieved with a minimal change suggest StochasTok\nholds exciting potential when applied to larger, more capable models. Code\nopen-sourced at: https://github.com/anyasims/stochastok."}
{"id": "2506.01698", "pdf": "https://arxiv.org/pdf/2506.01698.pdf", "abs": "https://arxiv.org/abs/2506.01698", "title": "When LLMs Team Up: The Emergence of Collaborative Affective Computing", "authors": ["Wenna Lai", "Haoran Xie", "Guandong Xu", "Qing Li", "S. Joe Qin"], "categories": ["cs.CL", "cs.AI"], "comment": "20 pages, 7 figures, and 3 tables", "summary": "Affective Computing (AC) is essential in bridging the gap between human\nemotional experiences and machine understanding. Traditionally, AC tasks in\nnatural language processing (NLP) have been approached through pipeline\narchitectures, which often suffer from structure rigidity that leads to\ninefficiencies and limited adaptability. The advent of Large Language Models\n(LLMs) has revolutionized this field by offering a unified approach to\naffective understanding and generation tasks, enhancing the potential for\ndynamic, real-time interactions. However, LLMs face cognitive limitations in\naffective reasoning, such as misinterpreting cultural nuances or contextual\nemotions, and hallucination problems in decision-making. To address these\nchallenges, recent research advocates for LLM-based collaboration systems that\nemphasize interactions among specialized models and LLMs, mimicking human-like\naffective intelligence through the synergy of emotional and rational thinking\nthat aligns with Dual Process Theory in psychology. This survey aims to provide\na comprehensive overview of LLM-based collaboration systems in AC, exploring\nfrom structured collaborations to autonomous collaborations. Specifically, it\nincludes: (1) A systematic review of existing methods, focusing on\ncollaboration strategies, mechanisms, key functions, and applications; (2)\nExperimental comparisons of collaboration strategies across representative\ntasks in affective understanding and generation; (3) An analysis highlighting\nthe potential of these systems to enhance robustness and adaptability in\ncomplex affective reasoning; (4) A discussion of key challenges and future\nresearch directions to further advance the field. This work is the first to\nsystematically explore collaborative intelligence with LLMs in AC, paving the\nway for more powerful applications that approach human-like social\nintelligence."}
{"id": "2506.01702", "pdf": "https://arxiv.org/pdf/2506.01702.pdf", "abs": "https://arxiv.org/abs/2506.01702", "title": "mdok of KInIT: Robustly Fine-tuned LLM for Binary and Multiclass AI-Generated Text Detection", "authors": ["Dominik Macko"], "categories": ["cs.CL"], "comment": null, "summary": "The large language models (LLMs) are able to generate high-quality texts in\nmultiple languages. Such texts are often not recognizable by humans as\ngenerated, and therefore present a potential of LLMs for misuse (e.g.,\nplagiarism, spams, disinformation spreading). An automated detection is able to\nassist humans to indicate the machine-generated texts; however, its robustness\nto out-of-distribution data is still challenging. This notebook describes our\nmdok approach in robust detection, based on fine-tuning smaller LLMs for text\nclassification. It is applied to both subtasks of Voight-Kampff Generative AI\nDetection 2025, providing remarkable performance in binary detection as well as\nin multiclass (1st rank) classification of various cases of human-AI\ncollaboration."}
{"id": "2506.01709", "pdf": "https://arxiv.org/pdf/2506.01709.pdf", "abs": "https://arxiv.org/abs/2506.01709", "title": "Fairness Dynamics During Training", "authors": ["Krishna Patel", "Nivedha Sivakumar", "Barry-John Theobald", "Luca Zappella", "Nicholas Apostoloff"], "categories": ["cs.CL"], "comment": null, "summary": "We investigate fairness dynamics during Large Language Model (LLM) training\nto enable the diagnoses of biases and mitigations through training\ninterventions like early stopping; we find that biases can emerge suddenly and\ndo not always follow common performance metrics. We introduce two new metrics\nto evaluate fairness dynamics holistically during model pre-training: Average\nRank and Jensen-Shannon Divergence by Parts. These metrics provide insights\ninto the Pythia models' progression of biases in gender prediction of\noccupations on the WinoBias dataset. By monitoring these dynamics, we find that\n(1) Pythia-6.9b is biased towards men; it becomes more performant and confident\npredicting \"male\" than \"female\" during training, (2) via early-stopping,\nPythia-6.9b can exchange 1.7% accuracy on LAMBADA for a 92.5% increase in\nfairness, and (3) larger models can exhibit more bias; Pythia-6.9b makes more\nassumptions about gender than Pythia-160m, even when a subject's gender is not\nspecified."}
{"id": "2506.01710", "pdf": "https://arxiv.org/pdf/2506.01710.pdf", "abs": "https://arxiv.org/abs/2506.01710", "title": "Reasoning-Table: Exploring Reinforcement Learning for Table Reasoning", "authors": ["Fangyu Lei", "Jinxiang Meng", "Yiming Huang", "Tinghong Chen", "Yun Zhang", "Shizhu He", "Jun Zhao", "Kang Liu"], "categories": ["cs.CL"], "comment": "Work in progress", "summary": "Table reasoning, encompassing tasks such as table question answering, fact\nverification, and text-to-SQL, requires precise understanding of structured\ntabular data, coupled with numerical computation and code manipulation for\neffective inference. Supervised fine-tuning (SFT) approaches have achieved\nnotable success but often struggle with generalization and robustness due to\nbiases inherent in imitative learning. We introduce Reasoning-Table, the first\napplication of reinforcement learning (RL) to table reasoning, achieving\nstate-of-the-art performance. Through rigorous data preprocessing, reward\ndesign, and tailored training strategies, our method leverages simple\nrule-based outcome rewards to outperform SFT across multiple benchmarks.\nUnified training across diverse tasks enables Reasoning-Table to emerge as a\nrobust table reasoning large language model, surpassing larger proprietary\nmodels like Claude-3.7-Sonnet by 4.0% on table reasoning benchmarks. The\napproach also achieves excellent performance on text-to-SQL tasks, reaching\n68.3% performance on the BIRD dev dataset with a 7B model. Further experiments\ndemonstrate that Reasoning-Table enhances the model's generalization\ncapabilities and robustness."}
{"id": "2506.01713", "pdf": "https://arxiv.org/pdf/2506.01713.pdf", "abs": "https://arxiv.org/abs/2506.01713", "title": "SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware Reinforcement Learning", "authors": ["Zhongwei Wan", "Zhihao Dou", "Che Liu", "Yu Zhang", "Dongfei Cui", "Qinjian Zhao", "Hui Shen", "Jing Xiong", "Yi Xin", "Yifan Jiang", "Yangfan He", "Mi Zhang", "Shen Yan"], "categories": ["cs.CL"], "comment": "Under review", "summary": "Multimodal large language models (MLLMs) have shown promising capabilities in\nreasoning tasks, yet still struggle with complex problems requiring explicit\nself-reflection and self-correction, especially compared to their unimodal\ntext-based counterparts. Existing reflection methods are simplistic and\nstruggle to generate meaningful and instructive feedback, as the reasoning\nability and knowledge limits of pre-trained models are largely fixed during\ninitial training. To overcome these challenges, we propose Multimodal\nSelf-Reflection enhanced reasoning with Group Relative Policy Optimization\n(SRPO), a two-stage reflection-aware reinforcement learning (RL) framework\nexplicitly designed to enhance multimodal LLM reasoning. In the first stage, we\nconstruct a high-quality, reflection-focused dataset under the guidance of an\nadvanced MLLM, which generates reflections based on initial responses to help\nthe policy model learn both reasoning and self-reflection. In the second stage,\nwe introduce a novel reward mechanism within the GRPO framework that encourages\nconcise and cognitively meaningful reflection while avoiding redundancy.\nExtensive experiments across multiple multimodal reasoning benchmarks,\nincluding MathVista, MathVision, MathVerse, and MMMU-Pro, using Qwen-2.5-VL-7B\nand Qwen-2.5-VL-32B demonstrate that SRPO significantly outperforms\nstate-of-the-art models, achieving notable improvements in both reasoning\naccuracy and reflection quality."}
{"id": "2506.01723", "pdf": "https://arxiv.org/pdf/2506.01723.pdf", "abs": "https://arxiv.org/abs/2506.01723", "title": "Tug-of-war between idiom's figurative and literal meanings in LLMs", "authors": ["Soyoung Oh", "Xinting Huang", "Mathis Pink", "Michael Hahn", "Vera Demberg"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Idioms present a unique challenge for language models due to their\nnon-compositional figurative meanings, which often strongly diverge from the\nidiom's literal interpretation. This duality requires a model to learn\nrepresenting and deciding between the two meanings to interpret an idiom in a\nfigurative sense, or literally. In this paper, we employ tools from mechanistic\ninterpretability to trace how a large pretrained causal transformer\n(LLama3.2-1B-base) deals with this ambiguity. We localize three steps of idiom\nprocessing: First, the idiom's figurative meaning is retrieved in early\nattention and MLP sublayers. We identify specific attention heads which boost\nthe figurative meaning of the idiom while suppressing the idiom's literal\ninterpretation. The model subsequently represents the figurative representation\nthrough an intermediate path. Meanwhile, a parallel bypass route forwards\nliteral interpretation, ensuring that a both reading remain available. Overall,\nour findings provide a mechanistic evidence for idiom comprehension in an\nautoregressive transformer."}
{"id": "2506.01732", "pdf": "https://arxiv.org/pdf/2506.01732.pdf", "abs": "https://arxiv.org/abs/2506.01732", "title": "Common Corpus: The Largest Collection of Ethical Data for LLM Pre-Training", "authors": ["Pierre-Carl Langlais", "Carlos Rosas Hinostroza", "Mattia Nee", "Catherine Arnett", "Pavel Chizhov", "Eliot Krzystof Jones", "Ir√®ne Girard", "David Mach", "Anastasia Stasenko", "Ivan P. Yamshchikov"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are pre-trained on large amounts of data from\ndifferent sources and domains. These data most often contain trillions of\ntokens with large portions of copyrighted or proprietary content, which hinders\nthe usage of such models under AI legislation. This raises the need for truly\nopen pre-training data that is compliant with the data security regulations. In\nthis paper, we introduce Common Corpus, the largest open dataset for language\nmodel pre-training. The data assembled in Common Corpus are either\nuncopyrighted or under permissible licenses and amount to about two trillion\ntokens. The dataset contains a wide variety of languages, ranging from the main\nEuropean languages to low-resource ones rarely present in pre-training\ndatasets; in addition, it includes a large portion of code data. The diversity\nof data sources in terms of covered domains and time periods opens up the paths\nfor both research and entrepreneurial needs in diverse areas of knowledge. In\nthis technical report, we present the detailed provenance of data assembling\nand the details of dataset filtering and curation. Being already used by such\nindustry leaders as Anthropic and multiple LLM training projects, we believe\nthat Common Corpus will become a critical infrastructure for open science\nresearch in LLMs."}
{"id": "2506.01734", "pdf": "https://arxiv.org/pdf/2506.01734.pdf", "abs": "https://arxiv.org/abs/2506.01734", "title": "Benford's Curse: Tracing Digit Bias to Numerical Hallucination in LLMs", "authors": ["Jiandong Shao", "Yao Lu", "Jianfei Yang"], "categories": ["cs.CL"], "comment": "Under Review", "summary": "Large Language Models (LLMs) exhibit impressive performance on complex\nreasoning tasks, yet they frequently fail on basic numerical problems,\nproducing incorrect outputs. Inspired by Benford's Law -- a statistical pattern\nwhere lower digits occur more frequently as leading digits -- we hypothesize\nthat the long-tailed digit distributions in web-collected corpora may be\nlearned by LLMs during pretraining, leading to biased numerical generation. To\ninvestigate the hypothesis, we first examine whether digits frequencies in\npretraining corpus (OLMo2) follows Benford's law. We then construct an\nevaluation benchmark with uniformly distributed ground-truth digits across\nseven numerical reasoning tasks. Our evaluation results demonstrate that\nleading open-source LLMs show a consistent pattern of digit bias that resembles\nBenford's law. Through logit-lens tracing and neuron-level dissection, we\nidentify that this bias arises predominantly from a small subset of highly\ndigit-selective feed-forward network (FFN) neurons in the deeper layers.\nFinally, we demonstrate that pruning these neurons mitigates imbalanced\novergeneration and partially corrects erroneous outputs, providing causal\nevidence that fine-grained pretraining digit bias can propagate into model\nbehavior. Our findings reveal a fundamental connection between corpus-level\nstatistics and symbolic failure modes in LLMs, offering a new lens for\ndiagnosing and mitigating hallucinations in numerical tasks."}
{"id": "2506.01748", "pdf": "https://arxiv.org/pdf/2506.01748.pdf", "abs": "https://arxiv.org/abs/2506.01748", "title": "Thinking in Character: Advancing Role-Playing Agents with Role-Aware Reasoning", "authors": ["Yihong Tang", "Kehai Chen", "Muyun Yang", "Zhengyu Niu", "Jing Li", "Tiejun Zhao", "Min Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "The advancement of Large Language Models (LLMs) has spurred significant\ninterest in Role-Playing Agents (RPAs) for applications such as emotional\ncompanionship and virtual interaction. However, recent RPAs are often built on\nexplicit dialogue data, lacking deep, human-like internal thought processes,\nresulting in superficial knowledge and style expression. While Large Reasoning\nModels (LRMs) can be employed to simulate character thought, their direct\napplication is hindered by attention diversion (i.e., RPAs forget their role)\nand style drift (i.e., overly formal and rigid reasoning rather than\ncharacter-consistent reasoning). To address these challenges, this paper\nintroduces a novel Role-Aware Reasoning (RAR) method, which consists of two\nimportant stages: Role Identity Activation (RIA) and Reasoning Style\nOptimization (RSO). RIA explicitly guides the model with character profiles\nduring reasoning to counteract attention diversion, and then RSO aligns\nreasoning style with the character and scene via LRM distillation to mitigate\nstyle drift. Extensive experiments demonstrate that the proposed RAR\nsignificantly enhances the performance of RPAs by effectively addressing\nattention diversion and style drift."}
{"id": "2506.01775", "pdf": "https://arxiv.org/pdf/2506.01775.pdf", "abs": "https://arxiv.org/abs/2506.01775", "title": "Developing a Mixed-Methods Pipeline for Community-Oriented Digitization of Kwak'wala Legacy Texts", "authors": ["Milind Agarwal", "Daisy Rosenblum", "Antonios Anastasopoulos"], "categories": ["cs.CL"], "comment": "Accepted to Comput-EL 2025 Workshop. Preprint", "summary": "Kwak'wala is an Indigenous language spoken in British Columbia, with a rich\nlegacy of published documentation spanning more than a century, and an active\ncommunity of speakers, teachers, and learners engaged in language\nrevitalization. Over 11 volumes of the earliest texts created during the\ncollaboration between Franz Boas and George Hunt have been scanned but remain\nunreadable by machines. Complete digitization through optical character\nrecognition has the potential to facilitate transliteration into modern\northographies and the creation of other language technologies. In this paper,\nwe apply the latest OCR techniques to a series of Kwak'wala texts only\naccessible as images, and discuss the challenges and unique adaptations\nnecessary to make such technologies work for these real-world texts. Building\non previous methods, we propose using a mix of off-the-shelf OCR methods,\nlanguage identification, and masking to effectively isolate Kwak'wala text,\nalong with post-correction models, to produce a final high-quality\ntranscription."}
{"id": "2506.01776", "pdf": "https://arxiv.org/pdf/2506.01776.pdf", "abs": "https://arxiv.org/abs/2506.01776", "title": "MaXIFE: Multilingual and Cross-lingual Instruction Following Evaluation", "authors": ["Yile Liu", "Ziwei Ma", "Xiu Jiang", "Jinglu Hu", "Jing Chang", "Liang Li"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Main Conference", "summary": "With the rapid adoption of large language models (LLMs) in natural language\nprocessing, the ability to follow instructions has emerged as a key metric for\nevaluating their practical utility. However, existing evaluation methods often\nfocus on single-language scenarios, overlooking the challenges and differences\npresent in multilingual and cross-lingual contexts. To address this gap, we\nintroduce MaXIFE: a comprehensive evaluation benchmark designed to assess\ninstruction-following capabilities across 23 languages with 1,667 verifiable\ninstruction tasks. MaXIFE integrates both Rule-Based Evaluation and Model-Based\nEvaluation, ensuring a balance of efficiency and accuracy. We applied MaXIFE to\nevaluate several leading commercial and open-source LLMs, establishing baseline\nresults for future comparisons. By providing a standardized tool for\nmultilingual instruction-following evaluation, MaXIFE aims to advance research\nand development in natural language processing."}
{"id": "2506.01784", "pdf": "https://arxiv.org/pdf/2506.01784.pdf", "abs": "https://arxiv.org/abs/2506.01784", "title": "iQUEST: An Iterative Question-Guided Framework for Knowledge Base Question Answering", "authors": ["Shuai Wang", "Yinan Yu"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 (Main)", "summary": "While Large Language Models (LLMs) excel at many natural language processing\ntasks, they often suffer from factual inaccuracies in knowledge-intensive\nscenarios. Integrating external knowledge resources, particularly knowledge\ngraphs (KGs), provides a transparent and updatable foundation for more reliable\nreasoning. Knowledge Base Question Answering (KBQA), which queries and reasons\nover KGs, is central to this effort, especially for complex, multi-hop queries.\nHowever, multi-hop reasoning poses two key challenges: (1)~maintaining coherent\nreasoning paths, and (2)~avoiding prematurely discarding critical multi-hop\nconnections. To address these issues, we introduce iQUEST, a question-guided\nKBQA framework that iteratively decomposes complex queries into simpler\nsub-questions, ensuring a structured and focused reasoning trajectory.\nAdditionally, we integrate a Graph Neural Network (GNN) to look ahead and\nincorporate 2-hop neighbor information at each reasoning step. This dual\napproach strengthens the reasoning process, enabling the model to explore\nviable paths more effectively. Detailed experiments demonstrate the consistent\nimprovement delivered by iQUEST across four benchmark datasets and four LLMs."}
{"id": "2506.01793", "pdf": "https://arxiv.org/pdf/2506.01793.pdf", "abs": "https://arxiv.org/abs/2506.01793", "title": "Human-Centric Evaluation for Foundation Models", "authors": ["Yijin Guo", "Kaiyuan Ji", "Xiaorong Zhu", "Junying Wang", "Farong Wen", "Chunyi Li", "Zicheng Zhang", "Guangtao Zhai"], "categories": ["cs.CL"], "comment": null, "summary": "Currently, nearly all evaluations of foundation models focus on objective\nmetrics, emphasizing quiz performance to define model capabilities. While this\nmodel-centric approach enables rapid performance assessment, it fails to\nreflect authentic human experiences. To address this gap, we propose a\nHuman-Centric subjective Evaluation (HCE) framework, focusing on three core\ndimensions: problem-solving ability, information quality, and interaction\nexperience. Through experiments involving Deepseek R1, OpenAI o3 mini, Grok 3,\nand Gemini 2.5, we conduct over 540 participant-driven evaluations, where\nhumans and models collaborate on open-ended research tasks, yielding a\ncomprehensive subjective dataset. This dataset captures diverse user feedback\nacross multiple disciplines, revealing distinct model strengths and\nadaptability. Our findings highlight Grok 3's superior performance, followed by\nDeepseek R1 and Gemini 2.5, with OpenAI o3 mini lagging behind. By offering a\nnovel framework and a rich dataset, this study not only enhances subjective\nevaluation methodologies but also lays the foundation for standardized,\nautomated assessments, advancing LLM development for research and practical\nscenarios. Our dataset link is\nhttps://github.com/yijinguo/Human-Centric-Evaluation."}
{"id": "2506.01796", "pdf": "https://arxiv.org/pdf/2506.01796.pdf", "abs": "https://arxiv.org/abs/2506.01796", "title": "Read it in Two Steps: Translating Extremely Low-Resource Languages with Code-Augmented Grammar Books", "authors": ["Chen Zhang", "Jiuheng Lin", "Xiao Liu", "Zekai Zhang", "Yansong Feng"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "While large language models (LLMs) have shown promise in translating\nextremely low-resource languages using resources like dictionaries, the\neffectiveness of grammar books remains debated. This paper investigates the\nrole of grammar books in translating extremely low-resource languages by\ndecomposing it into two key steps: grammar rule retrieval and application. To\nfacilitate the study, we introduce ZhuangRules, a modularized dataset of\ngrammar rules and their corresponding test sentences. Our analysis reveals that\nrule retrieval constitutes a primary bottleneck in grammar-based translation.\nMoreover, although LLMs can apply simple rules for translation when explicitly\nprovided, they encounter difficulties in handling more complex rules. To\naddress these challenges, we propose representing grammar rules as code\nfunctions, considering their similarities in structure and the benefit of code\nin facilitating LLM reasoning. Our experiments show that using code rules\nsignificantly boosts both rule retrieval and application, ultimately resulting\nin a 13.1% BLEU improvement in translation."}
{"id": "2506.01807", "pdf": "https://arxiv.org/pdf/2506.01807.pdf", "abs": "https://arxiv.org/abs/2506.01807", "title": "Propaganda and Information Dissemination in the Russo-Ukrainian War: Natural Language Processing of Russian and Western Twitter Narratives", "authors": ["Zaur Gouliev"], "categories": ["cs.CL"], "comment": "7 pages; 6 figures", "summary": "The conflict in Ukraine has been not only characterised by military\nengagement but also by a significant information war, with social media\nplatforms like X, formerly known as Twitter playing an important role in\nshaping public perception. This article provides an analysis of tweets from\npropaganda accounts and trusted accounts collected from the onset of the war,\nFebruary 2022 until the middle of May 2022 with n=40,000 total tweets. We\nutilise natural language processing and machine learning algorithms to assess\nthe sentiment and identify key themes, topics and narratives across the dataset\nwith human-in-the-loop (HITL) analysis throughout. Our findings indicate\ndistinct strategies in how information is created, spread, and targeted at\ndifferent audiences by both sides. Propaganda accounts frequently employ\nemotionally charged language and disinformation to evoke fear and distrust,\nwhereas other accounts, primarily Western tend to focus on factual reporting\nand humanitarian aspects of the conflict. Clustering analysis reveals groups of\naccounts with similar behaviours, which we suspect indicates the presence of\ncoordinated efforts. This research attempts to contribute to our understanding\nof the dynamics of information warfare and offers techniques for future studies\non social media influence in military conflicts."}
{"id": "2506.01808", "pdf": "https://arxiv.org/pdf/2506.01808.pdf", "abs": "https://arxiv.org/abs/2506.01808", "title": "NAVER LABS Europe Submission to the Instruction-following Track", "authors": ["Beomseok Lee", "Marcely Zanon Boito", "Laurent Besacier", "Ioan Calapodescu"], "categories": ["cs.CL"], "comment": null, "summary": "In this paper we describe NAVER LABS Europe submission to the\ninstruction-following speech processing short track at IWSLT 2025. We\nparticipate in the constrained settings, developing systems that can\nsimultaneously perform ASR, ST, and SQA tasks from English speech input into\nthe following target languages: Chinese, Italian, and German. Our solution\nleverages two pretrained modules: (1) a speech-to-LLM embedding projector\ntrained using representations from the SeamlessM4T-v2-large speech encoder; and\n(2) LoRA adapters trained on text data on top of a Llama-3.1-8B-Instruct. These\nmodules are jointly loaded and further instruction-tuned for 1K steps on\nmultilingual and multimodal data to form our final system submitted for\nevaluation."}
{"id": "2506.01814", "pdf": "https://arxiv.org/pdf/2506.01814.pdf", "abs": "https://arxiv.org/abs/2506.01814", "title": "Analysis of LLM Bias (Chinese Propaganda & Anti-US Sentiment) in DeepSeek-R1 vs. ChatGPT o3-mini-high", "authors": ["PeiHsuan Huang", "ZihWei Lin", "Simon Imbot", "WenCheng Fu", "Ethan Tu"], "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "Large language models (LLMs) increasingly shape public understanding and\ncivic decisions, yet their ideological neutrality is a growing concern. While\nexisting research has explored various forms of LLM bias, a direct,\ncross-lingual comparison of models with differing geopolitical\nalignments-specifically a PRC-system model versus a non-PRC counterpart-has\nbeen lacking. This study addresses this gap by systematically evaluating\nDeepSeek-R1 (PRC-aligned) against ChatGPT o3-mini-high (non-PRC) for\nChinese-state propaganda and anti-U.S. sentiment. We developed a novel corpus\nof 1,200 de-contextualized, reasoning-oriented questions derived from\nChinese-language news, presented in Simplified Chinese, Traditional Chinese,\nand English. Answers from both models (7,200 total) were assessed using a\nhybrid evaluation pipeline combining rubric-guided GPT-4o scoring with human\nannotation. Our findings reveal significant model-level and language-dependent\nbiases. DeepSeek-R1 consistently exhibited substantially higher proportions of\nboth propaganda and anti-U.S. bias compared to ChatGPT o3-mini-high, which\nremained largely free of anti-U.S. sentiment and showed lower propaganda\nlevels. For DeepSeek-R1, Simplified Chinese queries elicited the highest bias\nrates; these diminished in Traditional Chinese and were nearly absent in\nEnglish. Notably, DeepSeek-R1 occasionally responded in Simplified Chinese to\nTraditional Chinese queries and amplified existing PRC-aligned terms in its\nChinese answers, demonstrating an \"invisible loudspeaker\" effect. Furthermore,\nsuch biases were not confined to overtly political topics but also permeated\ncultural and lifestyle content, particularly in DeepSeek-R1."}
{"id": "2506.01817", "pdf": "https://arxiv.org/pdf/2506.01817.pdf", "abs": "https://arxiv.org/abs/2506.01817", "title": "BD at BEA 2025 Shared Task: MPNet Ensembles for Pedagogical Mistake Identification and Localization in AI Tutor Responses", "authors": ["Shadman Rohan", "Ishita Sur Apan", "Muhtasim Ibteda Shochcho", "Md Fahim", "Mohammad Ashfaq Ur Rahman", "AKM Mahbubur Rahman", "Amin Ahsan Ali"], "categories": ["cs.CL"], "comment": null, "summary": "We present Team BD's submission to the BEA 2025 Shared Task on Pedagogical\nAbility Assessment of AI-powered Tutors, under Track 1 (Mistake Identification)\nand Track 2 (Mistake Location). Both tracks involve three-class classification\nof tutor responses in educational dialogues - determining if a tutor correctly\nrecognizes a student's mistake (Track 1) and whether the tutor pinpoints the\nmistake's location (Track 2). Our system is built on MPNet, a Transformer-based\nlanguage model that combines BERT and XLNet's pre-training advantages. We\nfine-tuned MPNet on the task data using a class-weighted cross-entropy loss to\nhandle class imbalance, and leveraged grouped cross-validation (10 folds) to\nmaximize the use of limited data while avoiding dialogue overlap between\ntraining and validation. We then performed a hard-voting ensemble of the best\nmodels from each fold, which improves robustness and generalization by\ncombining multiple classifiers. Our approach achieved strong results on both\ntracks, with exact-match macro-F1 scores of approximately 0.7110 for Mistake\nIdentification and 0.5543 for Mistake Location on the official test set. We\ninclude comprehensive analysis of our system's performance, including confusion\nmatrices and t-SNE visualizations to interpret classifier behavior, as well as\na taxonomy of common errors with examples. We hope our ensemble-based approach\nand findings provide useful insights for designing reliable tutor response\nevaluation systems in educational dialogue settings."}
{"id": "2506.01819", "pdf": "https://arxiv.org/pdf/2506.01819.pdf", "abs": "https://arxiv.org/abs/2506.01819", "title": "Not All Jokes Land: Evaluating Large Language Models Understanding of Workplace Humor", "authors": ["Moahmmadamin Shafiei", "Hamidreza Saffari"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "With the recent advances in Artificial Intelligence (AI) and Large Language\nModels (LLMs), the automation of daily tasks, like automatic writing, is\ngetting more and more attention. Hence, efforts have focused on aligning LLMs\nwith human values, yet humor, particularly professional industrial humor used\nin workplaces, has been largely neglected. To address this, we develop a\ndataset of professional humor statements along with features that determine the\nappropriateness of each statement. Our evaluation of five LLMs shows that LLMs\noften struggle to judge the appropriateness of humor accurately."}
{"id": "2506.01829", "pdf": "https://arxiv.org/pdf/2506.01829.pdf", "abs": "https://arxiv.org/abs/2506.01829", "title": "CiteEval: Principle-Driven Citation Evaluation for Source Attribution", "authors": ["Yumo Xu", "Peng Qi", "Jifan Chen", "Kunlun Liu", "Rujun Han", "Lan Liu", "Bonan Min", "Vittorio Castelli", "Arshit Gupta", "Zhiguo Wang"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "ACL 2025", "summary": "Citation quality is crucial in information-seeking systems, directly\ninfluencing trust and the effectiveness of information access. Current\nevaluation frameworks, both human and automatic, mainly rely on Natural\nLanguage Inference (NLI) to assess binary or ternary supportiveness from cited\nsources, which we argue is a suboptimal proxy for citation evaluation. In this\nwork we introduce CiteEval, a citation evaluation framework driven by\nprinciples focusing on fine-grained citation assessment within a broad context,\nencompassing not only the cited sources but the full retrieval context, user\nquery, and generated text. Guided by the proposed framework, we construct\nCiteBench, a multi-domain benchmark with high-quality human annotations on\ncitation quality. To enable efficient evaluation, we further develop\nCiteEval-Auto, a suite of model-based metrics that exhibit strong correlation\nwith human judgments. Experiments across diverse systems demonstrate\nCiteEval-Auto's superior ability to capture the multifaceted nature of\ncitations compared to existing metrics, offering a principled and scalable\napproach to evaluate and improve model-generated citations."}
{"id": "2506.01840", "pdf": "https://arxiv.org/pdf/2506.01840.pdf", "abs": "https://arxiv.org/abs/2506.01840", "title": "Minimal Pair-Based Evaluation of Code-Switching", "authors": ["Igor Sterner", "Simone Teufel"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "There is a lack of an evaluation methodology that estimates the extent to\nwhich large language models (LLMs) use code-switching (CS) in the same way as\nbilinguals. Existing methods do not have wide language coverage, fail to\naccount for the diverse range of CS phenomena, or do not scale. We propose an\nintervention based on minimal pairs of CS. Each minimal pair contains one\nnaturally occurring CS sentence and one minimally manipulated variant. We\ncollect up to 1,000 such pairs each for 11 language pairs. Our human\nexperiments show that, for every language pair, bilinguals consistently prefer\nthe naturally occurring CS sentence. Meanwhile our experiments with current\nLLMs show that the larger the model, the more consistently it assigns higher\nprobability to the naturally occurring CS sentence than to the variant. In\naccordance with theoretical claims, the largest probability differences arise\nin those pairs where the manipulated material consisted of closed-class words."}
{"id": "2506.01846", "pdf": "https://arxiv.org/pdf/2506.01846.pdf", "abs": "https://arxiv.org/abs/2506.01846", "title": "Code-Switching and Syntax: A Large-Scale Experiment", "authors": ["Igor Sterner", "Simone Teufel"], "categories": ["cs.CL"], "comment": "Findings of ACL 2025", "summary": "The theoretical code-switching (CS) literature provides numerous pointwise\ninvestigations that aim to explain patterns in CS, i.e. why bilinguals switch\nlanguage in certain positions in a sentence more often than in others. A\nresulting consensus is that CS can be explained by the syntax of the\ncontributing languages. There is however no large-scale, multi-language,\ncross-phenomena experiment that tests this claim. When designing such an\nexperiment, we need to make sure that the system that is predicting where\nbilinguals tend to switch has access only to syntactic information. We provide\nsuch an experiment here. Results show that syntax alone is sufficient for an\nautomatic system to distinguish between sentences in minimal pairs of CS, to\nthe same degree as bilingual humans. Furthermore, the learnt syntactic patterns\ngeneralise well to unseen language pairs."}
{"id": "2506.01859", "pdf": "https://arxiv.org/pdf/2506.01859.pdf", "abs": "https://arxiv.org/abs/2506.01859", "title": "CONFETTI: Conversational Function-Calling Evaluation Through Turn-Level Interactions", "authors": ["Tamer Alkhouli", "Katerina Margatina", "James Gung", "Raphael Shu", "Claudia Zaghi", "Monica Sunkara", "Yi Zhang"], "categories": ["cs.CL"], "comment": "ACL 2025 (main conference)", "summary": "We introduce Conversational Function-Calling Evaluation Through Turn-Level\nInteractions (CONFETTI), a conversational benchmark1 designed to evaluate the\nfunction-calling capabilities and response quality of large language models\n(LLMs). Current benchmarks lack comprehensive assessment of LLMs in complex\nconversational scenarios. CONFETTI addresses this gap through 109\nhuman-simulated conversations, comprising 313 user turns and covering 86 APIs.\nThese conversations explicitly target various conversational complexities, such\nas follow-ups, goal correction and switching, ambiguous and implicit goals. We\nperform off-policy turn-level evaluation using this benchmark targeting\nfunction-calling. Our benchmark also incorporates dialog act annotations to\nassess agent responses. We evaluate a series of state-of-the-art LLMs and\nanalyze their performance with respect to the number of available APIs,\nconversation lengths, and chained function calling. Our results reveal that\nwhile some models are able to handle long conversations, and leverage more than\n20+ APIs successfully, other models struggle with longer context or when\nincreasing the number of APIs. We also report that the performance on chained\nfunction-calls is severely limited across the models. Overall, the top\nperforming models on CONFETTI are Nova Pro (40.01%), Claude Sonnet v3.5\n(35.46%) and Llama 3.1 405B (33.19%) followed by command-r-plus (31.18%) and\nMistral-Large-2407 (30.07%)."}
{"id": "2506.01872", "pdf": "https://arxiv.org/pdf/2506.01872.pdf", "abs": "https://arxiv.org/abs/2506.01872", "title": "Is Extending Modality The Right Path Towards Omni-Modality?", "authors": ["Tinghui Zhu", "Kai Zhang", "Muhao Chen", "Yu Su"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Omni-modal language models (OLMs) aim to integrate and reason over diverse\ninput modalities--such as text, images, video, and audio--while maintaining\nstrong language capabilities. Despite recent advancements, existing models,\nespecially open-source ones, remain far from true omni-modality, struggling to\ngeneralize beyond the specific modality pairs they are trained on or to achieve\nstrong performance when processing multi-modal inputs. We study the effect of\nextending modality, the dominant technique for training multimodal models,\nwhere an off-the-shelf language model is fine-tuned on target-domain and\nlanguage data. Specifically, we investigate three key questions: (1) Does\nmodality extension compromise core language abilities? (2) Can model merging\neffectively integrate independently fine-tuned modality-specific models to\nachieve omni-modality? (3) Does omni-modality extension lead to better\nknowledge sharing and generalization compared to sequential extension? Through\nextensive experiments, we analyze these trade-offs and provide insights into\nthe feasibility of achieving true omni-modality using current approaches."}
{"id": "2506.01918", "pdf": "https://arxiv.org/pdf/2506.01918.pdf", "abs": "https://arxiv.org/abs/2506.01918", "title": "Spatial Coordinates as a Cell Language: A Multi-Sentence Framework for Imaging Mass Cytometry Analysis", "authors": ["Chi-Jane Chen", "Yuhang Chen", "Sukwon Yun", "Natalie Stanley", "Tianlong Chen"], "categories": ["cs.CL"], "comment": null, "summary": "Image mass cytometry (IMC) enables high-dimensional spatial profiling by\ncombining mass cytometry's analytical power with spatial distributions of cell\nphenotypes. Recent studies leverage large language models (LLMs) to extract\ncell states by translating gene or protein expression into biological context.\nHowever, existing single-cell LLMs face two major challenges: (1) Integration\nof spatial information: they struggle to generalize spatial coordinates and\neffectively encode spatial context as text, and (2) Treating each cell\nindependently: they overlook cell-cell interactions, limiting their ability to\ncapture biological relationships. To address these limitations, we propose\nSpatial2Sentence, a novel framework that integrates single-cell expression and\nspatial information into natural language using a multi-sentence approach.\nSpatial2Sentence constructs expression similarity and distance matrices,\npairing spatially adjacent and expressionally similar cells as positive pairs\nwhile using distant and dissimilar cells as negatives. These multi-sentence\nrepresentations enable LLMs to learn cellular interactions in both expression\nand spatial contexts. Equipped with multi-task learning, Spatial2Sentence\noutperforms existing single-cell LLMs on preprocessed IMC datasets, improving\ncell-type classification by 5.98% and clinical status prediction by 4.18% on\nthe diabetes dataset while enhancing interpretability. The source code can be\nfound here: https://github.com/UNITES-Lab/Spatial2Sentence."}
{"id": "2506.01920", "pdf": "https://arxiv.org/pdf/2506.01920.pdf", "abs": "https://arxiv.org/abs/2506.01920", "title": "From Guidelines to Practice: A New Paradigm for Arabic Language Model Evaluation", "authors": ["Serry Sibaee", "Omer Nacar", "Adel Ammar", "Yasser Al-Habashi", "Abdulrahman Al-Batati", "Wadii Boulila"], "categories": ["cs.CL"], "comment": null, "summary": "This paper addresses critical gaps in Arabic language model evaluation by\nestablishing comprehensive theoretical guidelines and introducing a novel\nevaluation framework. We first analyze existing Arabic evaluation datasets,\nidentifying significant issues in linguistic accuracy, cultural alignment, and\nmethodological rigor. To address these limitations in LLMs, we present the\nArabic Depth Mini Dataset (ADMD), a carefully curated collection of 490\nchallenging questions spanning ten major domains (42 sub-domains, see Figure 1.\nUsing ADMD, we evaluate five leading language models: GPT-4, Claude 3.5 Sonnet,\nGemini Flash 1.5, CommandR 100B, and Qwen-Max. Our results reveal significant\nvariations in model performance across different domains, with particular\nchallenges in areas requiring deep cultural understanding and specialized\nknowledge. Claude 3.5 Sonnet demonstrated the highest overall accuracy at 30\\%,\nshowing relative strength in mathematical theory in Arabic, Arabic language,\nand islamic domains. This work provides both theoretical foundations and\npractical insights for improving Arabic language model evaluation, emphasizing\nthe importance of cultural competence alongside technical capabilities."}
{"id": "2506.01928", "pdf": "https://arxiv.org/pdf/2506.01928.pdf", "abs": "https://arxiv.org/abs/2506.01928", "title": "Esoteric Language Models", "authors": ["Subham Sekhar Sahoo", "Zhihan Yang", "Yash Akhauri", "Johnna Liu", "Deepansha Singh", "Zhoujun Cheng", "Zhengzhong Liu", "Eric Xing", "John Thickstun", "Arash Vahdat"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Diffusion-based language models offer a compelling alternative to\nautoregressive (AR) models by enabling parallel and controllable generation.\nAmong this family of models, Masked Diffusion Models (MDMs) achieve the\nstrongest performance but still underperform AR models in perplexity and lack\nkey inference-time efficiency features--most notably, KV caching. In this work,\nwe introduce Eso-LMs, a new family of models that fuses AR and MDM paradigms,\nenabling smooth interpolation between their perplexities while overcoming their\nrespective limitations. Eso-LMs set a new state of the art on standard language\nmodeling benchmarks. Crucially, we are the **first to introduce KV caching for\nMDMs** while preserving parallel generation, significantly improving inference\nefficiency. Combined with an optimized sampling schedule, our method achieves\nup to **65x** faster inference than standard MDMs and **4x** faster inference\nthan prior semi-autoregressive approaches. We provide the code and model\ncheckpoints on the project page:\n[http://s-sahoo.github.io/Eso-LMs](http://s-sahoo.github.io/Eso-LMs)"}
{"id": "2506.01937", "pdf": "https://arxiv.org/pdf/2506.01937.pdf", "abs": "https://arxiv.org/abs/2506.01937", "title": "RewardBench 2: Advancing Reward Model Evaluation", "authors": ["Saumya Malik", "Valentina Pyatkin", "Sander Land", "Jacob Morrison", "Noah A. Smith", "Hannaneh Hajishirzi", "Nathan Lambert"], "categories": ["cs.CL"], "comment": "Data, models, and leaderboard available at\n  https://huggingface.co/collections/allenai/reward-bench-2-683d2612a4b3e38a3e53bb51", "summary": "Reward models are used throughout the post-training of language models to\ncapture nuanced signals from preference data and provide a training target for\noptimization across instruction following, reasoning, safety, and more domains.\nThe community has begun establishing best practices for evaluating reward\nmodels, from the development of benchmarks that test capabilities in specific\nskill areas to others that test agreement with human preferences. At the same\ntime, progress in evaluation has not been mirrored by the effectiveness of\nreward models in downstream tasks -- simpler direct alignment algorithms are\nreported to work better in many cases. This paper introduces RewardBench 2, a\nnew multi-skill reward modeling benchmark designed to bring new, challenging\ndata for accuracy-based reward model evaluation -- models score about 20 points\non average lower on RewardBench 2 compared to the first RewardBench -- while\nbeing highly correlated with downstream performance. Compared to most other\nbenchmarks, RewardBench 2 sources new human prompts instead of existing prompts\nfrom downstream evaluations, facilitating more rigorous evaluation practices.\nIn this paper, we describe our benchmark construction process and report how\nexisting models perform on it, while quantifying how performance on the\nbenchmark correlates with downstream use of the models in both inference-time\nscaling algorithms, like best-of-N sampling, and RLHF training algorithms like\nproximal policy optimization."}
{"id": "2506.01938", "pdf": "https://arxiv.org/pdf/2506.01938.pdf", "abs": "https://arxiv.org/abs/2506.01938", "title": "Novel Benchmark for NER in the Wastewater and Stormwater Domain", "authors": ["Franco Alberto Cardillo", "Franca Debole", "Francesca Frontini", "Mitra Aelami", "Nan√©e Chahinian", "Serge Conrad"], "categories": ["cs.CL"], "comment": null, "summary": "Effective wastewater and stormwater management is essential for urban\nsustainability and environmental protection. Extracting structured knowledge\nfrom reports and regulations is challenging due to domainspecific terminology\nand multilingual contexts. This work focuses on domain-specific Named Entity\nRecognition (NER) as a first step towards effective relation and information\nextraction to support decision making. A multilingual benchmark is crucial for\nevaluating these methods. This study develops a French-Italian domain-specific\ntext corpus for wastewater management. It evaluates state-of-the-art NER\nmethods, including LLM-based approaches, to provide a reliable baseline for\nfuture strategies and explores automated annotation projection in view of an\nextension of the corpus to new languages."}
{"id": "2506.01939", "pdf": "https://arxiv.org/pdf/2506.01939.pdf", "abs": "https://arxiv.org/abs/2506.01939", "title": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning", "authors": ["Shenzhi Wang", "Le Yu", "Chang Gao", "Chujie Zheng", "Shixuan Liu", "Rui Lu", "Kai Dang", "Xionghui Chen", "Jianxin Yang", "Zhenru Zhang", "Yuqiong Liu", "An Yang", "Andrew Zhao", "Yang Yue", "Shiji Song", "Bowen Yu", "Gao Huang", "Junyang Lin"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "25 pages, 17 figures, 2 tables", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a\npowerful approach to enhancing the reasoning capabilities of Large Language\nModels (LLMs), while its mechanisms are not yet well understood. In this work,\nwe undertake a pioneering exploration of RLVR through the novel perspective of\ntoken entropy patterns, comprehensively analyzing how different tokens\ninfluence reasoning performance. By examining token entropy patterns in\nChain-of-Thought (CoT) reasoning, we observe that only a small fraction of\ntokens exhibit high entropy, and these tokens act as critical forks that steer\nthe model toward diverse reasoning pathways. Furthermore, studying how entropy\npatterns evolve during RLVR training reveals that RLVR largely adheres to the\nbase model's entropy patterns, primarily adjusting the entropy of high-entropy\ntokens. These findings highlight the significance of high-entropy tokens (i.e.,\nforking tokens) to RLVR. We ultimately improve RLVR by restricting policy\ngradient updates to forking tokens and uncover a finding even beyond the 80/20\nrule: utilizing only 20% of the tokens while maintaining performance comparable\nto full-gradient updates on the Qwen3-8B base model and significantly\nsurpassing full-gradient updates on the Qwen3-32B (+11.04 on AIME'25 and +7.71\non AIME'24) and Qwen3-14B (+4.79 on AIME'25 and +5.21 on AIME'24) base models,\nhighlighting a strong scaling trend. In contrast, training exclusively on the\n80% lowest-entropy tokens leads to a marked decline in performance. These\nfindings indicate that the efficacy of RLVR primarily arises from optimizing\nthe high-entropy tokens that decide reasoning directions. Collectively, our\nresults highlight the potential to understand RLVR through a token-entropy\nperspective and optimize RLVR by leveraging high-entropy minority tokens to\nfurther improve LLM reasoning."}
{"id": "2506.01951", "pdf": "https://arxiv.org/pdf/2506.01951.pdf", "abs": "https://arxiv.org/abs/2506.01951", "title": "Self-ensemble: Mitigating Confidence Distortion for Large Language Models", "authors": ["Zicheng Xu", "Guanchu Wang", "Guangyao Zheng", "Yu-Neng Chuang", "Alexander Szalay", "Xia Hu", "Vladimir Braverman"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Although Large Language Models (LLMs) perform well in general fields, they\nexhibit a confidence distortion problem on multi-choice question-answering\n(MCQA), particularly as the number of answer choices increases. Specifically,\non MCQA with many choices, LLMs suffer from under-confidence in correct\npredictions and over-confidence in incorrect ones, leading to a substantially\ndegraded performance. To solve this problem, we propose Self-ensemble in this\nwork. Our method splits the choices into several groups and ensembles LLM\npredictions across these groups to reach a final decision. The advantage of\nSelf-ensemble is its plug-and-play nature, where it can be integrated into\nexisting LLM architecture based on a designed attention mask and positional\nencoding, without requiring labeled datasets for parameter tuning. Experimental\nresults on three LLMs and datasets demonstrate that Self-ensemble\ncomprehensively addresses the confidence distortion problem of LLMs,\noutperforming standard inference as well as baseline methods."}
{"id": "2506.01952", "pdf": "https://arxiv.org/pdf/2506.01952.pdf", "abs": "https://arxiv.org/abs/2506.01952", "title": "WebChoreArena: Evaluating Web Browsing Agents on Realistic Tedious Web Tasks", "authors": ["Atsuyuki Miyai", "Zaiying Zhao", "Kazuki Egashira", "Atsuki Sato", "Tatsumi Sunada", "Shota Onohara", "Hiromasa Yamanishi", "Mashiro Toyooka", "Kunato Nishina", "Ryoma Maeda", "Kiyoharu Aizawa", "Toshihiko Yamasaki"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Project Page: https://webchorearena.github.io/", "summary": "Powered by a large language model (LLM), a web browsing agent operates web\nbrowsers in a human-like manner and offers a highly transparent path toward\nautomating a wide range of everyday tasks. As web agents become increasingly\ncapable and demonstrate proficiency in general browsing tasks, a critical\nquestion emerges: Can they go beyond general browsing to robustly handle tasks\nthat are tedious and complex, or chores that humans often avoid doing\nthemselves? In this paper, we introduce WebChoreArena, a new fully reproducible\nbenchmark comprising 532 carefully curated tasks designed to extend the scope\nof WebArena beyond general browsing to more labor-intensive and tedious tasks.\nWebChoreArena systematically integrates three key challenges: (i) Massive\nMemory tasks requiring accurate retrieval of large amounts of information in\nthe observations, (ii) Calculation tasks demanding precise mathematical\nreasoning, and (iii) Long-Term Memory tasks necessitating long-term memory\nacross multiple webpages. Built on top of the fully reproducible and widely\nadopted four WebArena simulation environments, WebChoreArena ensures strict\nreproducibility and enables fair, direct comparisons with the established\nWebArena benchmark, offering key insights into agent progress. Our experimental\nresults demonstrate that as LLMs evolve, represented by GPT-4o, Claude 3.7\nSonnet, and Gemini 2.5 Pro, significant improvements in performance are\nobserved on WebChoreArena. These findings suggest that WebChoreArena is\nwell-suited to measure the advancement of state-of-the-art LLMs with greater\nclarity. Nevertheless, the results also indicate that even with Gemini 2.5 Pro,\nthere remains substantial room for improvement compared to WebArena,\nhighlighting the increased challenges posed by WebChoreArena."}
{"id": "2506.01954", "pdf": "https://arxiv.org/pdf/2506.01954.pdf", "abs": "https://arxiv.org/abs/2506.01954", "title": "DRAG: Distilling RAG for SLMs from LLMs to Transfer Knowledge and Mitigate Hallucination via Evidence and Graph-based Distillation", "authors": ["Jennifer Chen", "Aidar Myrzakhan", "Yaxin Luo", "Hassaan Muhammad Khan", "Sondos Mahmoud Bsharat", "Zhiqiang Shen"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL 2025 Main. Code is available at https://github.com/VILA-Lab/DRAG", "summary": "Retrieval-Augmented Generation (RAG) methods have proven highly effective for\ntasks requiring factual consistency and robust knowledge retrieval. However,\nlarge-scale RAG systems consume significant computational resources and are\nprone to generating hallucinated content from Humans. In this work, we\nintroduce $\\texttt{DRAG}$, a novel framework for distilling RAG knowledge from\nlarge-scale Language Models (LLMs) into small LMs (SLMs). Our approach\nleverages evidence- and knowledge graph-based distillation, ensuring that the\ndistilled model retains critical factual knowledge while significantly reducing\nmodel size and computational cost. By aligning the smaller model's predictions\nwith a structured knowledge graph and ranked evidence, $\\texttt{DRAG}$\neffectively mitigates hallucinations and improves factual accuracy. We further\npresent a case demonstrating how our framework mitigates user privacy risks and\nintroduce a corresponding benchmark. Experimental evaluations on multiple\nbenchmarks demonstrate that our method outperforms the prior competitive RAG\nmethods like MiniRAG for SLMs by up to 27.7% using the same models, preserving\nhigh-level efficiency and reliability. With $\\texttt{DRAG}$, we provide a\npractical and resource-efficient roadmap to deploying enhanced retrieval and\ngeneration capabilities in small-sized LLMs."}
{"id": "2311.03057", "pdf": "https://arxiv.org/pdf/2311.03057.pdf", "abs": "https://arxiv.org/abs/2311.03057", "title": "GLEN: Generative Retrieval via Lexical Index Learning", "authors": ["Sunkyung Lee", "Minjin Choi", "Jongwuk Lee"], "categories": ["cs.IR", "cs.CL"], "comment": "In Proceedings of the 2023 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP 2023) main conference. 12 pages, 2 figures, 8\n  tables", "summary": "Generative retrieval shed light on a new paradigm of document retrieval,\naiming to directly generate the identifier of a relevant document for a query.\nWhile it takes advantage of bypassing the construction of auxiliary index\nstructures, existing studies face two significant challenges: (i) the\ndiscrepancy between the knowledge of pre-trained language models and\nidentifiers and (ii) the gap between training and inference that poses\ndifficulty in learning to rank. To overcome these challenges, we propose a\nnovel generative retrieval method, namely Generative retrieval via LExical\niNdex learning (GLEN). For training, GLEN effectively exploits a dynamic\nlexical identifier using a two-phase index learning strategy, enabling it to\nlearn meaningful lexical identifiers and relevance signals between queries and\ndocuments. For inference, GLEN utilizes collision-free inference, using\nidentifier weights to rank documents without additional overhead. Experimental\nresults prove that GLEN achieves state-of-the-art or competitive performance\nagainst existing generative retrieval methods on various benchmark datasets,\ne.g., NQ320k, MS MARCO, and BEIR. The code is available at\nhttps://github.com/skleee/GLEN."}
{"id": "2506.00001", "pdf": "https://arxiv.org/pdf/2506.00001.pdf", "abs": "https://arxiv.org/abs/2506.00001", "title": "Enhancing Finite State Machine Design Automation with Large Language Models and Prompt Engineering Techniques", "authors": ["Qun-Kai Lin", "Cheng Hsu", "Tian-Sheuan Chang"], "categories": ["cs.AR", "cs.CL"], "comment": "published in 2024 IEEE Asia Pacific Conference on Circuits and\n  Systems (APCCAS 2024)", "summary": "Large Language Models (LLMs) have attracted considerable attention in recent\nyears due to their remarkable compatibility with Hardware Description Language\n(HDL) design. In this paper, we examine the performance of three major LLMs,\nClaude 3 Opus, ChatGPT-4, and ChatGPT-4o, in designing finite state machines\n(FSMs). By utilizing the instructional content provided by HDLBits, we evaluate\nthe stability, limitations, and potential approaches for improving the success\nrates of these models. Furthermore, we explore the impact of using the\nprompt-refining method, To-do-Oriented Prompting (TOP) Patch, on the success\nrate of these LLM models in various FSM design scenarios. The results show that\nthe systematic format prompt method and the novel prompt refinement method have\nthe potential to be applied to other domains beyond HDL design automation,\nconsidering its possible integration with other prompt engineering techniques\nin the future."}
{"id": "2506.00003", "pdf": "https://arxiv.org/pdf/2506.00003.pdf", "abs": "https://arxiv.org/abs/2506.00003", "title": "Probing Audio-Generation Capabilities of Text-Based Language Models", "authors": ["Arjun Prasaath Anbazhagan", "Parteek Kumar", "Ujjwal Kaur", "Aslihan Akalin", "Kevin Zhu", "Sean O'Brien"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Accepted at Conference of the North American Chapter of the\n  Association for Computational Linguistics 2025, Student Research Workshop\n  (NAACL SRW)", "summary": "How does textual representation of audio relate to the Large Language Model's\n(LLMs) learning about the audio world? This research investigates the extent to\nwhich LLMs can be prompted to generate audio, despite their primary training in\ntextual data. We employ a three-tier approach, progressively increasing the\ncomplexity of audio generation: 1) Musical Notes, 2) Environmental Sounds, and\n3) Human Speech. To bridge the gap between text and audio, we leverage code as\nan intermediary, prompting LLMs to generate code that, when executed, produces\nthe desired audio output. To evaluate the quality and accuracy of the generated\naudio, we employ FAD and CLAP scores. Our findings reveal that while LLMs can\ngenerate basic audio features, their performance deteriorates as the complexity\nof the audio increases. This suggests that while LLMs possess a latent\nunderstanding of the auditory world, their ability to translate this\nunderstanding into tangible audio output remains rudimentary. Further research\ninto techniques that can enhance the quality and diversity of LLM-generated\naudio can lead to an improvement in the performance of text-based LLMs in\ngenerating audio."}
{"id": "2506.00054", "pdf": "https://arxiv.org/pdf/2506.00054.pdf", "abs": "https://arxiv.org/abs/2506.00054", "title": "Retrieval-Augmented Generation: A Comprehensive Survey of Architectures, Enhancements, and Robustness Frontiers", "authors": ["Chaitanya Sharma"], "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm to\nenhance large language models (LLMs) by conditioning generation on external\nevidence retrieved at inference time. While RAG addresses critical limitations\nof parametric knowledge storage-such as factual inconsistency and domain\ninflexibility-it introduces new challenges in retrieval quality, grounding\nfidelity, pipeline efficiency, and robustness against noisy or adversarial\ninputs. This survey provides a comprehensive synthesis of recent advances in\nRAG systems, offering a taxonomy that categorizes architectures into\nretriever-centric, generator-centric, hybrid, and robustness-oriented designs.\nWe systematically analyze enhancements across retrieval optimization, context\nfiltering, decoding control, and efficiency improvements, supported by\ncomparative performance analyses on short-form and multi-hop question answering\ntasks. Furthermore, we review state-of-the-art evaluation frameworks and\nbenchmarks, highlighting trends in retrieval-aware evaluation, robustness\ntesting, and federated retrieval settings. Our analysis reveals recurring\ntrade-offs between retrieval precision and generation flexibility, efficiency\nand faithfulness, and modularity and coordination. We conclude by identifying\nopen challenges and future research directions, including adaptive retrieval\narchitectures, real-time retrieval integration, structured reasoning over\nmulti-hop evidence, and privacy-preserving retrieval mechanisms. This survey\naims to consolidate current knowledge in RAG research and serve as a foundation\nfor the next generation of retrieval-augmented language modeling systems."}
{"id": "2506.00060", "pdf": "https://arxiv.org/pdf/2506.00060.pdf", "abs": "https://arxiv.org/abs/2506.00060", "title": "Comparative analysis of privacy-preserving open-source LLMs regarding extraction of diagnostic information from clinical CMR imaging reports", "authors": ["Sina Amirrajab", "Volker Vehof", "Michael Bietenbeck", "Ali Yilmaz"], "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": "under review for Scientific Reports", "summary": "Purpose: We investigated the utilization of privacy-preserving,\nlocally-deployed, open-source Large Language Models (LLMs) to extract\ndiagnostic information from free-text cardiovascular magnetic resonance (CMR)\nreports. Materials and Methods: We evaluated nine open-source LLMs on their\nability to identify diagnoses and classify patients into various cardiac\ndiagnostic categories based on descriptive findings in 109 clinical CMR\nreports. Performance was quantified using standard classification metrics\nincluding accuracy, precision, recall, and F1 score. We also employed confusion\nmatrices to examine patterns of misclassification across models. Results: Most\nopen-source LLMs demonstrated exceptional performance in classifying reports\ninto different diagnostic categories. Google's Gemma2 model achieved the\nhighest average F1 score of 0.98, followed by Qwen2.5:32B and DeepseekR1-32B\nwith F1 scores of 0.96 and 0.95, respectively. All other evaluated models\nattained average scores above 0.93, with Mistral and DeepseekR1-7B being the\nonly exceptions. The top four LLMs outperformed our board-certified\ncardiologist (F1 score of 0.94) across all evaluation metrics in analyzing CMR\nreports. Conclusion: Our findings demonstrate the feasibility of implementing\nopen-source, privacy-preserving LLMs in clinical settings for automated\nanalysis of imaging reports, enabling accurate, fast and resource-efficient\ndiagnostic categorization."}
{"id": "2506.00062", "pdf": "https://arxiv.org/pdf/2506.00062.pdf", "abs": "https://arxiv.org/abs/2506.00062", "title": "SafeCOMM: What about Safety Alignment in Fine-Tuned Telecom Large Language Models?", "authors": ["Aladin Djuhera", "Swanand Ravindra Kadhe", "Farhan Ahmed", "Syed Zawad", "Holger Boche", "Walid Saad"], "categories": ["cs.CY", "cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "Fine-tuning large language models (LLMs) for telecom tasks and datasets is a\ncommon practice to adapt general-purpose models to the telecom domain. However,\nlittle attention has been paid to how this process may compromise model safety.\nRecent research has shown that even benign fine-tuning can degrade the safety\nalignment of LLMs, causing them to respond to harmful or unethical user\nqueries. In this paper, we investigate this issue for telecom-tuned LLMs using\nthree representative datasets featured by the GenAINet initiative. We show that\nsafety degradation persists even for structured and seemingly harmless datasets\nsuch as 3GPP standards and tabular records, indicating that telecom-specific\ndata is not immune to safety erosion during fine-tuning. We further extend our\nanalysis to publicly available Telecom LLMs trained via continual pre-training,\nrevealing that safety alignment is often severely lacking, primarily due to the\nomission of safety-focused instruction tuning. To address these issues in both\nfine-tuned and pre-trained models, we conduct extensive experiments and\nevaluate three safety realignment defenses (SafeInstruct, SafeLoRA, and\nSafeMERGE) using established red-teaming benchmarks. The results show that,\nacross all settings, the proposed defenses can effectively restore safety after\nharmful degradation without compromising downstream task performance, leading\nto Safe teleCOMMunication (SafeCOMM) models. In a nutshell, our work serves as\na diagnostic study and practical guide for safety realignment in telecom-tuned\nLLMs, and emphasizes the importance of safety-aware instruction and fine-tuning\nfor real-world deployments of Telecom LLMs."}
{"id": "2506.00072", "pdf": "https://arxiv.org/pdf/2506.00072.pdf", "abs": "https://arxiv.org/abs/2506.00072", "title": "Evaluating Prompt Engineering Techniques for Accuracy and Confidence Elicitation in Medical LLMs", "authors": ["Nariman Naderi", "Zahra Atf", "Peter R Lewis", "Aref Mahjoub far", "Seyed Amir Ahmad Safavi-Naini", "Ali Soroush"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "comment": "This paper was accepted for presentation at the 7th International\n  Workshop on EXplainable, Trustworthy, and Responsible AI and Multi-Agent\n  Systems (EXTRAAMAS 2025). Workshop website:\n  https://extraamas.ehealth.hevs.ch/index.html", "summary": "This paper investigates how prompt engineering techniques impact both\naccuracy and confidence elicitation in Large Language Models (LLMs) applied to\nmedical contexts. Using a stratified dataset of Persian board exam questions\nacross multiple specialties, we evaluated five LLMs - GPT-4o, o3-mini,\nLlama-3.3-70b, Llama-3.1-8b, and DeepSeek-v3 - across 156 configurations. These\nconfigurations varied in temperature settings (0.3, 0.7, 1.0), prompt styles\n(Chain-of-Thought, Few-Shot, Emotional, Expert Mimicry), and confidence scales\n(1-10, 1-100). We used AUC-ROC, Brier Score, and Expected Calibration Error\n(ECE) to evaluate alignment between confidence and actual performance.\nChain-of-Thought prompts improved accuracy but also led to overconfidence,\nhighlighting the need for calibration. Emotional prompting further inflated\nconfidence, risking poor decisions. Smaller models like Llama-3.1-8b\nunderperformed across all metrics, while proprietary models showed higher\naccuracy but still lacked calibrated confidence. These results suggest prompt\nengineering must address both accuracy and uncertainty to be effective in\nhigh-stakes medical tasks."}
{"id": "2506.00073", "pdf": "https://arxiv.org/pdf/2506.00073.pdf", "abs": "https://arxiv.org/abs/2506.00073", "title": "The Automated but Risky Game: Modeling Agent-to-Agent Negotiations and Transactions in Consumer Markets", "authors": ["Shenzhe Zhu", "Jiao Sun", "Yi Nian", "Tobin South", "Alex Pentland", "Jiaxin Pei"], "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC", "cs.MA"], "comment": null, "summary": "AI agents are increasingly used in consumer-facing applications to assist\nwith tasks such as product search, negotiation, and transaction execution. In\nthis paper, we explore a future scenario where both consumers and merchants\nauthorize AI agents to fully automate negotiations and transactions. We aim to\nanswer two key questions: (1) Do different LLM agents vary in their ability to\nsecure favorable deals for users? (2) What risks arise from fully automating\ndeal-making with AI agents in consumer markets? To address these questions, we\ndevelop an experimental framework that evaluates the performance of various LLM\nagents in real-world negotiation and transaction settings. Our findings reveal\nthat AI-mediated deal-making is an inherently imbalanced game -- different\nagents achieve significantly different outcomes for their users. Moreover,\nbehavioral anomalies in LLMs can result in financial losses for both consumers\nand merchants, such as overspending or accepting unreasonable deals. These\nresults underscore that while automation can improve efficiency, it also\nintroduces substantial risks. Users should exercise caution when delegating\nbusiness decisions to AI agents."}
{"id": "2506.00076", "pdf": "https://arxiv.org/pdf/2506.00076.pdf", "abs": "https://arxiv.org/abs/2506.00076", "title": "Optimizing Storytelling, Improving Audience Retention, and Reducing Waste in the Entertainment Industry", "authors": ["Andrew Cornfeld", "Ashley Miller", "Mercedes Mora-Figueroa", "Kurt Samuels", "Anthony Palomba"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Television networks face high financial risk when making programming\ndecisions, often relying on limited historical data to forecast episodic\nviewership. This study introduces a machine learning framework that integrates\nnatural language processing (NLP) features from over 25000 television episodes\nwith traditional viewership data to enhance predictive accuracy. By extracting\nemotional tone, cognitive complexity, and narrative structure from episode\ndialogue, we evaluate forecasting performance using SARIMAX, rolling XGBoost,\nand feature selection models. While prior viewership remains a strong baseline\npredictor, NLP features contribute meaningful improvements for some series. We\nalso introduce a similarity scoring method based on Euclidean distance between\naggregate dialogue vectors to compare shows by content. Tested across diverse\ngenres, including Better Call Saul and Abbott Elementary, our framework reveals\ngenre-specific performance and offers interpretable metrics for writers,\nexecutives, and marketers seeking data-driven insight into audience behavior."}
{"id": "2506.00080", "pdf": "https://arxiv.org/pdf/2506.00080.pdf", "abs": "https://arxiv.org/abs/2506.00080", "title": "Bottom-Up Perspectives on AI Governance: Insights from User Reviews of AI Products", "authors": ["Stefan Pasch"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "With the growing importance of AI governance, numerous high-level frameworks\nand principles have been articulated by policymakers, institutions, and expert\ncommunities to guide the development and application of AI. While such\nframeworks offer valuable normative orientation, they may not fully capture the\npractical concerns of those who interact with AI systems in organizational and\noperational contexts. To address this gap, this study adopts a bottom-up\napproach to explore how governance-relevant themes are expressed in user\ndiscourse. Drawing on over 100,000 user reviews of AI products from G2.com, we\napply BERTopic to extract latent themes and identify those most semantically\nrelated to AI governance. The analysis reveals a diverse set of\ngovernance-relevant topics spanning both technical and non-technical domains.\nThese include concerns across organizational processes-such as planning,\ncoordination, and communication-as well as stages of the AI value chain,\nincluding deployment infrastructure, data handling, and analytics. The findings\nshow considerable overlap with institutional AI governance and ethics\nframeworks on issues like privacy and transparency, but also surface overlooked\nareas such as project management, strategy development, and customer\ninteraction. This highlights the need for more empirically grounded,\nuser-centered approaches to AI governance-approaches that complement normative\nmodels by capturing how governance unfolds in applied settings. By\nforegrounding how governance is enacted in practice, this study contributes to\nmore inclusive and operationally grounded approaches to AI governance and\ndigital policy."}
{"id": "2506.00095", "pdf": "https://arxiv.org/pdf/2506.00095.pdf", "abs": "https://arxiv.org/abs/2506.00095", "title": "ClinBench-HPB: A Clinical Benchmark for Evaluating LLMs in Hepato-Pancreato-Biliary Diseases", "authors": ["Yuchong Li", "Xiaojun Zeng", "Chihua Fang", "Jian Yang", "Lei Zhang"], "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": null, "summary": "Hepato-pancreato-biliary (HPB) disorders represent a global public health\nchallenge due to their high morbidity and mortality. Although large language\nmodels (LLMs) have shown promising performance in general medical\nquestion-answering tasks, the current evaluation benchmarks are mostly derived\nfrom standardized examinations or manually designed questions, lacking HPB\ncoverage and clinical cases. To address these issues, we systematically\neatablish an HPB disease evaluation benchmark comprising 3,535 closed-ended\nmultiple-choice questions and 337 open-ended real diagnosis cases, which\nencompasses all the 33 main categories and 465 subcategories of HPB diseases\ndefined in the International Statistical Classification of Diseases, 10th\nRevision (ICD-10). The multiple-choice questions are curated from public\ndatasets and synthesized data, and the clinical cases are collected from\nprestigious medical journals, case-sharing platforms, and collaborating\nhospitals. By evalauting commercial and open-source general and medical LLMs on\nour established benchmark, namely ClinBench-HBP, we find that while commercial\nLLMs perform competently on medical exam questions, they exhibit substantial\nperformance degradation on HPB diagnosis tasks, especially on complex,\ninpatient clinical cases. Those medical LLMs also show limited generalizability\nto HPB diseases. Our results reveal the critical limitations of current LLMs in\nthe domain of HPB diseases, underscoring the imperative need for future medical\nLLMs to handle real, complex clinical diagnostics rather than simple medical\nexam questions. The benchmark will be released at the homepage."}
{"id": "2506.00100", "pdf": "https://arxiv.org/pdf/2506.00100.pdf", "abs": "https://arxiv.org/abs/2506.00100", "title": "Children's Voice Privacy: First Steps And Emerging Challenges", "authors": ["Ajinkya Kulkarni", "Francisco Teixeira", "Enno Hermann", "Thomas Rolland", "Isabel Trancoso", "Mathew Magimai Doss"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.CR"], "comment": "Accepted at Interspeech 2025, Netherlands", "summary": "Children are one of the most under-represented groups in speech technologies,\nas well as one of the most vulnerable in terms of privacy. Despite this,\nanonymization techniques targeting this population have received little\nattention. In this study, we seek to bridge this gap, and establish a baseline\nfor the use of voice anonymization techniques designed for adult speech when\napplied to children's voices. Such an evaluation is essential, as children's\nspeech presents a distinct set of challenges when compared to that of adults.\nThis study comprises three children's datasets, six anonymization methods, and\nobjective and subjective utility metrics for evaluation. Our results show that\nexisting systems for adults are still able to protect children's voice privacy,\nbut suffer from much higher utility degradation. In addition, our subjective\nstudy displays the challenges of automatic evaluation methods for speech\nquality in children's speech, highlighting the need for further research."}
{"id": "2506.00166", "pdf": "https://arxiv.org/pdf/2506.00166.pdf", "abs": "https://arxiv.org/abs/2506.00166", "title": "Disentangled Safety Adapters Enable Efficient Guardrails and Flexible Inference-Time Alignment", "authors": ["Kundan Krishna", "Joseph Y Cheng", "Charles Maalouf", "Leon A Gatys"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "16 pages, 2 figures, including references and appendix", "summary": "Existing paradigms for ensuring AI safety, such as guardrail models and\nalignment training, often compromise either inference efficiency or development\nflexibility. We introduce Disentangled Safety Adapters (DSA), a novel framework\naddressing these challenges by decoupling safety-specific computations from a\ntask-optimized base model. DSA utilizes lightweight adapters that leverage the\nbase model's internal representations, enabling diverse and flexible safety\nfunctionalities with minimal impact on inference cost. Empirically, DSA-based\nsafety guardrails substantially outperform comparably sized standalone models,\nnotably improving hallucination detection (0.88 vs. 0.61 AUC on Summedits) and\nalso excelling at classifying hate speech (0.98 vs. 0.92 on ToxiGen) and unsafe\nmodel inputs and responses (0.93 vs. 0.90 on AEGIS2.0 & BeaverTails).\nFurthermore, DSA-based safety alignment allows dynamic, inference-time\nadjustment of alignment strength and a fine-grained trade-off between\ninstruction following performance and model safety. Importantly, combining the\nDSA safety guardrail with DSA safety alignment facilitates context-dependent\nalignment strength, boosting safety on StrongReject by 93% while maintaining\n98% performance on MTBench -- a total reduction in alignment tax of 8\npercentage points compared to standard safety alignment fine-tuning. Overall,\nDSA presents a promising path towards more modular, efficient, and adaptable AI\nsafety and alignment."}
{"id": "2506.00185", "pdf": "https://arxiv.org/pdf/2506.00185.pdf", "abs": "https://arxiv.org/abs/2506.00185", "title": "Pushing the Limits of Beam Search Decoding for Transducer-based ASR models", "authors": ["Lilit Grigoryan", "Vladimir Bataev", "Andrei Andrusenko", "Hainan Xu", "Vitaly Lavrukhin", "Boris Ginsburg"], "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "comment": "Accepted to Interspeech 2025", "summary": "Transducer models have emerged as a promising choice for end-to-end ASR\nsystems, offering a balanced trade-off between recognition accuracy, streaming\ncapabilities, and inference speed in greedy decoding. However, beam search\nsignificantly slows down Transducers due to repeated evaluations of key network\ncomponents, limiting practical applications. This paper introduces a universal\nmethod to accelerate beam search for Transducers, enabling the implementation\nof two optimized algorithms: ALSD++ and AES++. The proposed method utilizes\nbatch operations, a tree-based hypothesis structure, novel blank scoring for\nenhanced shallow fusion, and CUDA graph execution for efficient GPU inference.\nThis narrows the speed gap between beam and greedy modes to only 10-20% for the\nwhole system, achieves 14-30% relative improvement in WER compared to greedy\ndecoding, and improves shallow fusion for low-resource up to 11% compared to\nexisting implementations. All the algorithms are open sourced."}
{"id": "2506.00189", "pdf": "https://arxiv.org/pdf/2506.00189.pdf", "abs": "https://arxiv.org/abs/2506.00189", "title": "Control-R: Towards controllable test-time scaling", "authors": ["Di Zhang", "Weida Wang", "Junxian Li", "Xunzhi Wang", "Jiatong Li", "Jianbo Wu", "Jingdi Lei", "Haonan He", "Peng Ye", "Shufei Zhang", "Wanli Ouyang", "Yuqiang Li", "Dongzhan Zhou"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "This paper target in addressing the challenges of underthinking and\noverthinking in long chain-of-thought (CoT) reasoning for Large Reasoning\nModels (LRMs) by introducing Reasoning Control Fields (RCF)--a novel test-time\napproach that injects structured control signals to guide reasoning from a tree\nsearch perspective. RCF enables models to adjust reasoning effort according to\ngiven control conditions when solving complex tasks. Additionally, we present\nthe Control-R-4K dataset, which consists of challenging problems annotated with\ndetailed reasoning processes and corresponding control fields. To further\nenhance reasoning control, we propose a Conditional Distillation Finetuning\n(CDF) method, which trains model--particularly Control-R-32B--to effectively\nadjust reasoning effort during test time. Experimental results on benchmarks\nsuch as AIME2024 and MATH500 demonstrate that our approach achieves\nstate-of-the-art performance at the 32B scale while enabling a controllable\nLong CoT reasoning process (L-CoT). Overall, this work introduces an effective\nparadigm for controllable test-time scaling reasoning."}
{"id": "2506.00236", "pdf": "https://arxiv.org/pdf/2506.00236.pdf", "abs": "https://arxiv.org/abs/2506.00236", "title": "Localized LoRA: A Structured Low-Rank Approximation for Efficient Fine-Tuning", "authors": ["Babak Barazandeh"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, offer compact\nand effective alternatives to full model fine-tuning by introducing low-rank\nupdates to pretrained weights. However, most existing approaches rely on global\nlow-rank structures, which can overlook spatial patterns spread across the\nparameter space. In this work, we propose Localized LoRA, a generalized\nframework that models weight updates as a composition of low-rank matrices\napplied to structured blocks of the weight matrix. This formulation enables\ndense, localized updates throughout the parameter space-without increasing the\ntotal number of trainable parameters. We provide a formal comparison between\nglobal, diagonal-local, and fully localized low-rank approximations, and show\nthat our method consistently achieves lower approximation error under matched\nparameter budgets. Experiments on both synthetic and practical settings\ndemonstrate that Localized LoRA offers a more expressive and adaptable\nalternative to existing methods, enabling efficient fine-tuning with improved\nperformance."}
{"id": "2506.00238", "pdf": "https://arxiv.org/pdf/2506.00238.pdf", "abs": "https://arxiv.org/abs/2506.00238", "title": "ZeShot-VQA: Zero-Shot Visual Question Answering Framework with Answer Mapping for Natural Disaster Damage Assessment", "authors": ["Ehsan Karimi", "Maryam Rahnemoonfar"], "categories": ["cs.CV", "cs.CL", "cs.IR", "cs.LG", "I.2.7; I.2.10; I.5.1"], "comment": "Accepted by the 2025 IEEE International Geoscience and Remote Sensing\n  Symposium (IGARSS 2025)", "summary": "Natural disasters usually affect vast areas and devastate infrastructures.\nPerforming a timely and efficient response is crucial to minimize the impact on\naffected communities, and data-driven approaches are the best choice. Visual\nquestion answering (VQA) models help management teams to achieve in-depth\nunderstanding of damages. However, recently published models do not possess the\nability to answer open-ended questions and only select the best answer among a\npredefined list of answers. If we want to ask questions with new additional\npossible answers that do not exist in the predefined list, the model needs to\nbe fin-tuned/retrained on a new collected and annotated dataset, which is a\ntime-consuming procedure. In recent years, large-scale Vision-Language Models\n(VLMs) have earned significant attention. These models are trained on extensive\ndatasets and demonstrate strong performance on both unimodal and multimodal\nvision/language downstream tasks, often without the need for fine-tuning. In\nthis paper, we propose a VLM-based zero-shot VQA (ZeShot-VQA) method, and\ninvestigate the performance of on post-disaster FloodNet dataset. Since the\nproposed method takes advantage of zero-shot learning, it can be applied on new\ndatasets without fine-tuning. In addition, ZeShot-VQA is able to process and\ngenerate answers that has been not seen during the training procedure, which\ndemonstrates its flexibility."}
{"id": "2506.00242", "pdf": "https://arxiv.org/pdf/2506.00242.pdf", "abs": "https://arxiv.org/abs/2506.00242", "title": "Whispers of Many Shores: Cultural Alignment through Collaborative Cultural Expertise", "authors": ["Shuai Feng", "Wei-Chuang Chan", "Srishti Chouhan", "Junior Francisco Garcia Ayala", "Srujananjali Medicherla", "Kyle Clark", "Mingwei Shi"], "categories": ["cs.AI", "cs.CL"], "comment": "14 main pages;8 page appendix", "summary": "The integration of large language models (LLMs) into global applications\nnecessitates effective cultural alignment for meaningful and\nculturally-sensitive interactions. Current LLMs often lack the nuanced\nunderstanding required for diverse cultural contexts, and adapting them\ntypically involves costly full fine-tuning. To address this, we introduce a\nnovel soft prompt fine-tuning framework that enables efficient and modular\ncultural alignment. Our method utilizes vectorized prompt tuning to dynamically\nroute queries to a committee of culturally specialized 'expert' LLM\nconfigurations, created by optimizing soft prompt embeddings without altering\nthe base model's parameters. Extensive experiments demonstrate that our\nframework significantly enhances cultural sensitivity and adaptability,\nimproving alignment scores from 0.208 to 0.820, offering a robust solution for\nculturally-aware LLM deployment. This research paves the way for subsequent\ninvestigations into enhanced cultural coverage and dynamic expert adaptation,\ncrucial for realizing autonomous AI with deeply nuanced understanding in a\nglobally interconnected world."}
{"id": "2506.00245", "pdf": "https://arxiv.org/pdf/2506.00245.pdf", "abs": "https://arxiv.org/abs/2506.00245", "title": "Beyond Semantic Entropy: Boosting LLM Uncertainty Quantification with Pairwise Semantic Similarity", "authors": ["Dang Nguyen", "Ali Payani", "Baharan Mirzasoleiman"], "categories": ["cs.LG", "cs.CL"], "comment": "11 pages, 4 figures, 6 tables, link:\n  https://github.com/BigML-CS-UCLA/SNNE", "summary": "Hallucination in large language models (LLMs) can be detected by assessing\nthe uncertainty of model outputs, typically measured using entropy. Semantic\nentropy (SE) enhances traditional entropy estimation by quantifying uncertainty\nat the semantic cluster level. However, as modern LLMs generate longer\none-sentence responses, SE becomes less effective because it overlooks two\ncrucial factors: intra-cluster similarity (the spread within a cluster) and\ninter-cluster similarity (the distance between clusters). To address these\nlimitations, we propose a simple black-box uncertainty quantification method\ninspired by nearest neighbor estimates of entropy. Our approach can also be\neasily extended to white-box settings by incorporating token probabilities.\nAdditionally, we provide theoretical results showing that our method\ngeneralizes semantic entropy. Extensive empirical results demonstrate its\neffectiveness compared to semantic entropy across two recent LLMs (Phi3 and\nLlama3) and three common text generation tasks: question answering, text\nsummarization, and machine translation. Our code is available at\nhttps://github.com/BigML-CS-UCLA/SNNE."}
{"id": "2506.00249", "pdf": "https://arxiv.org/pdf/2506.00249.pdf", "abs": "https://arxiv.org/abs/2506.00249", "title": "MIR: Methodology Inspiration Retrieval for Scientific Research Problems", "authors": ["Aniketh Garikaparthi", "Manasi Patwardhan", "Aditya Sanjiv Kanade", "Aman Hassan", "Lovekesh Vig", "Arman Cohan"], "categories": ["cs.AI", "cs.CL"], "comment": "ACL 2025", "summary": "There has been a surge of interest in harnessing the reasoning capabilities\nof Large Language Models (LLMs) to accelerate scientific discovery. While\nexisting approaches rely on grounding the discovery process within the relevant\nliterature, effectiveness varies significantly with the quality and nature of\nthe retrieved literature. We address the challenge of retrieving prior work\nwhose concepts can inspire solutions for a given research problem, a task we\ndefine as Methodology Inspiration Retrieval (MIR). We construct a novel dataset\ntailored for training and evaluating retrievers on MIR, and establish\nbaselines. To address MIR, we build the Methodology Adjacency Graph (MAG);\ncapturing methodological lineage through citation relationships. We leverage\nMAG to embed an \"intuitive prior\" into dense retrievers for identifying\npatterns of methodological inspiration beyond superficial semantic similarity.\nThis achieves significant gains of +5.4 in Recall@3 and +7.8 in Mean Average\nPrecision (mAP) over strong baselines. Further, we adapt LLM-based re-ranking\nstrategies to MIR, yielding additional improvements of +4.5 in Recall@3 and\n+4.8 in mAP. Through extensive ablation studies and qualitative analyses, we\nexhibit the promise of MIR in enhancing automated scientific discovery and\noutline avenues for advancing inspiration-driven retrieval."}
{"id": "2506.00261", "pdf": "https://arxiv.org/pdf/2506.00261.pdf", "abs": "https://arxiv.org/abs/2506.00261", "title": "GPR: Empowering Generation with Graph-Pretrained Retriever", "authors": ["Xiaochen Wang", "Zongyu Wu", "Yuan Zhong", "Xiang Zhang", "Suhang Wang", "Fenglong Ma"], "categories": ["cs.IR", "cs.CL"], "comment": "Short paper submitted to EMNLP'25", "summary": "Graph retrieval-augmented generation (GRAG) places high demands on\ngraph-specific retrievers. However, existing retrievers often rely on language\nmodels pretrained on plain text, limiting their effectiveness due to domain\nmisalignment and structure ignorance. To address these challenges, we propose\nGPR, a graph-based retriever pretrained directly on knowledge graphs. GPR\naligns natural language questions with relevant subgraphs through LLM-guided\ngraph augmentation and employs a structure-aware objective to learn\nfine-grained retrieval strategies. Experiments on two datasets, three LLM\nbackbones, and five baselines show that GPR consistently improves both\nretrieval quality and downstream generation, demonstrating its effectiveness as\na robust retrieval solution for GRAG."}
{"id": "2506.00276", "pdf": "https://arxiv.org/pdf/2506.00276.pdf", "abs": "https://arxiv.org/abs/2506.00276", "title": "RoboMoRe: LLM-based Robot Co-design via Joint Optimization of Morphology and Reward", "authors": ["Jiawei Fang", "Yuxuan Sun", "Chengtian Ma", "Qiuyu Lu", "Lining Yao"], "categories": ["cs.RO", "cs.CL", "68T40, 68T05, 90C90", "I.2.9; I.2.6; I.2.8; I.2.10"], "comment": "30 pages, 13 figures", "summary": "Robot co-design, jointly optimizing morphology and control policy, remains a\nlongstanding challenge in the robotics community, where many promising robots\nhave been developed. However, a key limitation lies in its tendency to converge\nto sub-optimal designs due to the use of fixed reward functions, which fail to\nexplore the diverse motion modes suitable for different morphologies. Here we\npropose RoboMoRe, a large language model (LLM)-driven framework that integrates\nmorphology and reward shaping for co-optimization within the robot co-design\nloop. RoboMoRe performs a dual-stage optimization: in the coarse optimization\nstage, an LLM-based diversity reflection mechanism generates both diverse and\nhigh-quality morphology-reward pairs and efficiently explores their\ndistribution. In the fine optimization stage, top candidates are iteratively\nrefined through alternating LLM-guided reward and morphology gradient updates.\nRoboMoRe can optimize both efficient robot morphologies and their suited motion\nbehaviors through reward shaping. Results demonstrate that without any\ntask-specific prompting or predefined reward/morphology templates, RoboMoRe\nsignificantly outperforms human-engineered designs and competing methods across\neight different tasks."}
{"id": "2506.00308", "pdf": "https://arxiv.org/pdf/2506.00308.pdf", "abs": "https://arxiv.org/abs/2506.00308", "title": "MythTriage: Scalable Detection of Opioid Use Disorder Myths on a Video-Sharing Platform", "authors": ["Hayoung Jung", "Shravika Mittal", "Ananya Aatreya", "Navreet Kaur", "Munmun De Choudhury", "Tanushree Mitra"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "comment": "34 pages, 14 figures, 21 tables. In submission", "summary": "Understanding the prevalence of misinformation in health topics online can\ninform public health policies and interventions. However, measuring such\nmisinformation at scale remains a challenge, particularly for high-stakes but\nunderstudied topics like opioid-use disorder (OUD)--a leading cause of death in\nthe U.S. We present the first large-scale study of OUD-related myths on\nYouTube, a widely-used platform for health information. With clinical experts,\nwe validate 8 pervasive myths and release an expert-labeled video dataset. To\nscale labeling, we introduce MythTriage, an efficient triage pipeline that uses\na lightweight model for routine cases and defers harder ones to a\nhigh-performing, but costlier, large language model (LLM). MythTriage achieves\nup to 0.86 macro F1-score while estimated to reduce annotation time and\nfinancial cost by over 76% compared to experts and full LLM labeling. We\nanalyze 2.9K search results and 343K recommendations, uncovering how myths\npersist on YouTube and offering actionable insights for public health and\nplatform moderation."}
{"id": "2506.00320", "pdf": "https://arxiv.org/pdf/2506.00320.pdf", "abs": "https://arxiv.org/abs/2506.00320", "title": "Dyna-Think: Synergizing Reasoning, Acting, and World Model Simulation in AI Agents", "authors": ["Xiao Yu", "Baolin Peng", "Ruize Xu", "Michel Galley", "Hao Cheng", "Suman Nath", "Jianfeng Gao", "Zhou Yu"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Recent progress in reasoning with large language models (LLMs), such as\nDeepSeek-R1, demonstrates impressive capabilities in domains like mathematics\nand coding, by exhibiting complex cognitive behaviors such as verification,\ngoal decomposition, and self-reflection. However, it is unclear what behavior\nis effective and what behavior is missing for long-horizon AI agents tasks. In\nthis work, we propose Dyna-Think, a thinking framework that integrates planning\nwith an internal world model with reasoning and acting to enhance AI agent\nperformance. To enable Dyna-Think, we propose Dyna-Think Imitation Learning\n(DIT) and Dyna-Think Dyna Training (DDT). To initialize a policy with\nDyna-Think, DIT reconstructs the thinking process of R1 to focus on performing\nworld model simulation relevant to the proposed (and planned) action, and\ntrains the policy using this reconstructed data. To enhance Dyna-Think, DDT\nuses a two-stage training process to first improve the agent's world modeling\nability via objectives such as state prediction or critique generation, and\nthen improve the agent's action via policy training. We evaluate our methods on\nOSWorld, and demonstrate that Dyna-Think improves the agent's in-domain and\nout-of-domain performance, achieving similar best-of-n performance compared to\nR1 while generating 2x less tokens on average. Our extensive empirical studies\nreveal that 1) using critique generation for world model training is effective\nto improve policy performance; and 2) AI agents with better performance\ncorrelate with better world modeling abilities. We believe our results suggest\na promising research direction to integrate world model simulation into AI\nagents to enhance their reasoning, planning, and acting capabilities."}
{"id": "2506.00363", "pdf": "https://arxiv.org/pdf/2506.00363.pdf", "abs": "https://arxiv.org/abs/2506.00363", "title": "Adapting General-Purpose Embedding Models to Private Datasets Using Keyword-based Retrieval", "authors": ["Yubai Wei", "Jiale Han", "Yi Yang"], "categories": ["cs.IR", "cs.CL"], "comment": "Link: https://github.com/BaileyWei/BMEmbed", "summary": "Text embedding models play a cornerstone role in AI applications, such as\nretrieval-augmented generation (RAG). While general-purpose text embedding\nmodels demonstrate strong performance on generic retrieval benchmarks, their\neffectiveness diminishes when applied to private datasets (e.g.,\ncompany-specific proprietary data), which often contain specialized terminology\nand lingo. In this work, we introduce BMEmbed, a novel method for adapting\ngeneral-purpose text embedding models to private datasets. By leveraging the\nwell-established keyword-based retrieval technique (BM25), we construct\nsupervisory signals from the ranking of keyword-based retrieval results to\nfacilitate model adaptation. We evaluate BMEmbed across a range of domains,\ndatasets, and models, showing consistent improvements in retrieval performance.\nMoreover, we provide empirical insights into how BM25-based signals contribute\nto improving embeddings by fostering alignment and uniformity, highlighting the\nvalue of this approach in adapting models to domain-specific data. We release\nthe source code available at https://github.com/BaileyWei/BMEmbed for the\nresearch community."}
{"id": "2506.00382", "pdf": "https://arxiv.org/pdf/2506.00382.pdf", "abs": "https://arxiv.org/abs/2506.00382", "title": "Spectral Insights into Data-Oblivious Critical Layers in Large Language Models", "authors": ["Xuyuan Liu", "Lei Hsiung", "Yaoqing Yang", "Yujun Yan"], "categories": ["cs.LG", "cs.CL"], "comment": "Accepted by Findings of ACL2025", "summary": "Understanding how feature representations evolve across layers in large\nlanguage models (LLMs) is key to improving their interpretability and\nrobustness. While recent studies have identified critical layers linked to\nspecific functions or behaviors, these efforts typically rely on data-dependent\nanalyses of fine-tuned models, limiting their use to post-hoc settings. In\ncontrast, we introduce a data-oblivious approach to identify intrinsic critical\nlayers in pre-fine-tuned LLMs by analyzing representation dynamics via Centered\nKernel Alignment(CKA). We show that layers with significant shifts in\nrepresentation space are also those most affected during fine-tuning--a pattern\nthat holds consistently across tasks for a given model. Our spectral analysis\nfurther reveals that these shifts are driven by changes in the top principal\ncomponents, which encode semantic transitions from rationales to conclusions.\nWe further apply these findings to two practical scenarios: efficient domain\nadaptation, where fine-tuning critical layers leads to greater loss reduction\ncompared to non-critical layers; and backdoor defense, where freezing them\nreduces attack success rates by up to 40%."}
{"id": "2506.00462", "pdf": "https://arxiv.org/pdf/2506.00462.pdf", "abs": "https://arxiv.org/abs/2506.00462", "title": "XMAD-Bench: Cross-Domain Multilingual Audio Deepfake Benchmark", "authors": ["Ioan-Paul Ciobanu", "Andrei-Iulian Hiji", "Nicolae-Catalin Ristea", "Paul Irofti", "Cristian Rusu", "Radu Tudor Ionescu"], "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "eess.AS"], "comment": null, "summary": "Recent advances in audio generation led to an increasing number of deepfakes,\nmaking the general public more vulnerable to financial scams, identity theft,\nand misinformation. Audio deepfake detectors promise to alleviate this issue,\nwith many recent studies reporting accuracy rates close to 99%. However, these\nmethods are typically tested in an in-domain setup, where the deepfake samples\nfrom the training and test sets are produced by the same generative models. To\nthis end, we introduce XMAD-Bench, a large-scale cross-domain multilingual\naudio deepfake benchmark comprising 668.8 hours of real and deepfake speech. In\nour novel dataset, the speakers, the generative methods, and the real audio\nsources are distinct across training and test splits. This leads to a\nchallenging cross-domain evaluation setup, where audio deepfake detectors can\nbe tested ``in the wild''. Our in-domain and cross-domain experiments indicate\na clear disparity between the in-domain performance of deepfake detectors,\nwhich is usually as high as 100%, and the cross-domain performance of the same\nmodels, which is sometimes similar to random chance. Our benchmark highlights\nthe need for the development of robust audio deepfake detectors, which maintain\ntheir generalization capacity across different languages, speakers, generative\nmethods, and data sources. Our benchmark is publicly released at\nhttps://github.com/ristea/xmad-bench/."}
{"id": "2506.00482", "pdf": "https://arxiv.org/pdf/2506.00482.pdf", "abs": "https://arxiv.org/abs/2506.00482", "title": "BenchHub: A Unified Benchmark Suite for Holistic and Customizable LLM Evaluation", "authors": ["Eunsu Kim", "Haneul Yoo", "Guijin Son", "Hitesh Patel", "Amit Agarwal", "Alice Oh"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "As large language models (LLMs) continue to advance, the need for up-to-date\nand well-organized benchmarks becomes increasingly critical. However, many\nexisting datasets are scattered, difficult to manage, and make it challenging\nto perform evaluations tailored to specific needs or domains, despite the\ngrowing importance of domain-specific models in areas such as math or code. In\nthis paper, we introduce BenchHub, a dynamic benchmark repository that empowers\nresearchers and developers to evaluate LLMs more effectively. BenchHub\naggregates and automatically classifies benchmark datasets from diverse\ndomains, integrating 303K questions across 38 benchmarks. It is designed to\nsupport continuous updates and scalable data management, enabling flexible and\ncustomizable evaluation tailored to various domains or use cases. Through\nextensive experiments with various LLM families, we demonstrate that model\nperformance varies significantly across domain-specific subsets, emphasizing\nthe importance of domain-aware benchmarking. We believe BenchHub can encourage\nbetter dataset reuse, more transparent model comparisons, and easier\nidentification of underrepresented areas in existing benchmarks, offering a\ncritical infrastructure for advancing LLM evaluation research."}
{"id": "2506.00495", "pdf": "https://arxiv.org/pdf/2506.00495.pdf", "abs": "https://arxiv.org/abs/2506.00495", "title": "FLoE: Fisher-Based Layer Selection for Efficient Sparse Adaptation of Low-Rank Experts", "authors": ["Xinyi Wang", "Lirong Gao", "Haobo Wang", "Yiming Zhang", "Junbo Zhao"], "categories": ["cs.LG", "cs.CL", "stat.ML"], "comment": "17 pages, 9 figures", "summary": "Parameter-Efficient Fine-Tuning (PEFT) methods have emerged as a widely\nadopted strategy for adapting pre-trained Large Language Models (LLMs) to\ndownstream tasks, significantly reducing memory and computational costs.\nHowever, most existing PEFT techniques uniformly deploy LoRA adapters across\nall layers, disregarding the intrinsic heterogeneity of layer contributions and\ntask-specific rank requirements. This uniform paradigm leads to redundant\nparameter allocation and suboptimal adaptation efficiency. To address these\nlimitations, we propose FLoE, a novel PEFT framework that introduces two key\ninnovations: (i) a Fisher information-guided importance scoring mechanism to\ndynamically identify task-critical transformer layers for MoE-based low-rank\nadaptation, enabling sparse adapter deployment; and (ii) a Bayesian\noptimization-driven rank allocator that automatically determines optimal LoRA\nranks on specific datasets without exhaustive grid search. Extensive\nexperiments across diverse LLMs and benchmarks reveal that FLoE achieves\nimpressive efficiency-accuracy trade-offs, making FLoE particularly\nadvantageous in resource-constrained environments that necessitate rapid\nadaptation."}
{"id": "2506.00530", "pdf": "https://arxiv.org/pdf/2506.00530.pdf", "abs": "https://arxiv.org/abs/2506.00530", "title": "CityLens: Benchmarking Large Language-Vision Models for Urban Socioeconomic Sensing", "authors": ["Tianhui Liu", "Jie Feng", "Hetian Pang", "Xin Zhang", "Tianjian Ouyang", "Zhiyuan Zhang", "Yong Li"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Understanding urban socioeconomic conditions through visual data is a\nchallenging yet essential task for sustainable urban development and policy\nplanning. In this work, we introduce $\\textbf{CityLens}$, a comprehensive\nbenchmark designed to evaluate the capabilities of large language-vision models\n(LLVMs) in predicting socioeconomic indicators from satellite and street view\nimagery. We construct a multi-modal dataset covering a total of 17 globally\ndistributed cities, spanning 6 key domains: economy, education, crime,\ntransport, health, and environment, reflecting the multifaceted nature of urban\nlife. Based on this dataset, we define 11 prediction tasks and utilize three\nevaluation paradigms: Direct Metric Prediction, Normalized Metric Estimation,\nand Feature-Based Regression. We benchmark 17 state-of-the-art LLVMs across\nthese tasks. Our results reveal that while LLVMs demonstrate promising\nperceptual and reasoning capabilities, they still exhibit limitations in\npredicting urban socioeconomic indicators. CityLens provides a unified\nframework for diagnosing these limitations and guiding future efforts in using\nLLVMs to understand and predict urban socioeconomic patterns. Our codes and\ndatasets are open-sourced via https://github.com/tsinghua-fib-lab/CityLens."}
{"id": "2506.00548", "pdf": "https://arxiv.org/pdf/2506.00548.pdf", "abs": "https://arxiv.org/abs/2506.00548", "title": "Con Instruction: Universal Jailbreaking of Multimodal Large Language Models via Non-Textual Modalities", "authors": ["Jiahui Geng", "Thy Thy Tran", "Preslav Nakov", "Iryna Gurevych"], "categories": ["cs.CR", "cs.CL", "cs.LG"], "comment": null, "summary": "Existing attacks against multimodal language models (MLLMs) primarily\ncommunicate instructions through text accompanied by adversarial images. In\ncontrast, we exploit the capabilities of MLLMs to interpret non-textual\ninstructions, specifically, adversarial images or audio generated by our novel\nmethod, Con Instruction. We optimize these adversarial examples to align\nclosely with target instructions in the embedding space, revealing the\ndetrimental implications of MLLMs' sophisticated understanding. Unlike prior\nwork, our method does not require training data or preprocessing of textual\ninstructions. While these non-textual adversarial examples can effectively\nbypass MLLM safety mechanisms, their combination with various text inputs\nsubstantially amplifies attack success. We further introduce a new Attack\nResponse Categorization (ARC) framework, which evaluates both the quality of\nthe model's response and its relevance to the malicious instructions.\nExperimental results demonstrate that Con Instruction effectively bypasses\nsafety mechanisms in multiple vision- and audio-language models, including\nLLaVA-v1.5, InternVL, Qwen-VL, and Qwen-Audio, evaluated on two standard\nbenchmarks: AdvBench and SafeBench. Specifically, our method achieves the\nhighest attack success rates, reaching 81.3% and 86.6% on LLaVA-v1.5 (13B). On\nthe defense side, we explore various countermeasures against our attacks and\nuncover a substantial performance gap among existing techniques. Our\nimplementation is made publicly available."}
{"id": "2506.00555", "pdf": "https://arxiv.org/pdf/2506.00555.pdf", "abs": "https://arxiv.org/abs/2506.00555", "title": "MMedAgent-RL: Optimizing Multi-Agent Collaboration for Multimodal Medical Reasoning", "authors": ["Peng Xia", "Jinglu Wang", "Yibo Peng", "Kaide Zeng", "Xian Wu", "Xiangru Tang", "Hongtu Zhu", "Yun Li", "Shujie Liu", "Yan Lu", "Huaxiu Yao"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Medical Large Vision-Language Models (Med-LVLMs) have shown strong potential\nin multimodal diagnostic tasks. However, existing single-agent models struggle\nto generalize across diverse medical specialties, limiting their performance.\nRecent efforts introduce multi-agent collaboration frameworks inspired by\nclinical workflows, where general practitioners (GPs) and specialists interact\nin a fixed sequence. Despite improvements, these static pipelines lack\nflexibility and adaptability in reasoning. To address this, we propose\nMMedAgent-RL, a reinforcement learning (RL)-based multi-agent framework that\nenables dynamic, optimized collaboration among medical agents. Specifically, we\ntrain two GP agents based on Qwen2.5-VL via RL: the triage doctor learns to\nassign patients to appropriate specialties, while the attending physician\nintegrates the judgments from multi-specialists and its own knowledge to make\nfinal decisions. To address the inconsistency in specialist outputs, we\nintroduce a curriculum learning (CL)-guided RL strategy that progressively\nteaches the attending physician to balance between imitating specialists and\ncorrecting their mistakes. Experiments on five medical VQA benchmarks\ndemonstrate that MMedAgent-RL not only outperforms both open-source and\nproprietary Med-LVLMs, but also exhibits human-like reasoning patterns.\nNotably, it achieves an average performance gain of 18.4% over supervised\nfine-tuning baselines."}
{"id": "2506.00577", "pdf": "https://arxiv.org/pdf/2506.00577.pdf", "abs": "https://arxiv.org/abs/2506.00577", "title": "Reasoning Like an Economist: Post-Training on Economic Problems Induces Strategic Generalization in LLMs", "authors": ["Yufa Zhou", "Shaobo Wang", "Xingyu Dong", "Xiangqi Jin", "Yifang Chen", "Yue Min", "Kexin Yang", "Xingzhang Ren", "Dayiheng Liu", "Linfeng Zhang"], "categories": ["cs.AI", "cs.CL", "cs.GT", "cs.MA"], "comment": null, "summary": "Directly training Large Language Models (LLMs) for Multi-Agent Systems (MAS)\nremains challenging due to intricate reward modeling, dynamic agent\ninteractions, and demanding generalization requirements. This paper explores\nwhether post-training techniques, specifically Supervised Fine-Tuning (SFT) and\nReinforcement Learning with Verifiable Rewards (RLVR), can effectively\n$\\textit{generalize}$ to multi-agent scenarios. We use economic reasoning as a\ntestbed, leveraging its strong foundations in mathematics and game theory, its\ndemand for structured analytical reasoning, and its relevance to real-world\napplications such as market design, resource allocation, and policy analysis.\nWe introduce $\\textbf{Recon}$ ($\\textbf{R}$easoning like an\n$\\textbf{ECON}$omist), a 7B-parameter open-source LLM post-trained on a\nhand-curated dataset of 2,100 high-quality economic reasoning problems.\nComprehensive evaluation on economic reasoning benchmarks and multi-agent games\nreveals clear improvements in structured reasoning and economic rationality.\nThese results underscore the promise of domain-aligned post-training for\nenhancing reasoning and agent alignment, shedding light on the roles of SFT and\nRL in shaping model behavior. Code is available at\nhttps://github.com/MasterZhou1/Recon ."}
{"id": "2506.00653", "pdf": "https://arxiv.org/pdf/2506.00653.pdf", "abs": "https://arxiv.org/abs/2506.00653", "title": "Linear Representation Transferability Hypothesis: Leveraging Small Models to Steer Large Models", "authors": ["Femi Bello", "Anubrata Das", "Fanzhi Zeng", "Fangcong Yin", "Leqi Liu"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "It has been hypothesized that neural networks with similar architectures\ntrained on similar data learn shared representations relevant to the learning\ntask. We build on this idea by extending the conceptual framework where\nrepresentations learned across models trained on the same data can be expressed\nas linear combinations of a \\emph{universal} set of basis features. These basis\nfeatures underlie the learning task itself and remain consistent across models,\nregardless of scale. From this framework, we propose the \\textbf{Linear\nRepresentation Transferability (LRT)} Hypothesis -- that there exists an affine\ntransformation between the representation spaces of different models. To test\nthis hypothesis, we learn affine mappings between the hidden states of models\nof different sizes and evaluate whether steering vectors -- directions in\nhidden state space associated with specific model behaviors -- retain their\nsemantic effect when transferred from small to large language models using the\nlearned mappings. We find strong empirical evidence that such affine mappings\ncan preserve steering behaviors. These findings suggest that representations\nlearned by small models can be used to guide the behavior of large models, and\nthat the LRT hypothesis may be a promising direction on understanding\nrepresentation alignment across model scales."}
{"id": "2506.00688", "pdf": "https://arxiv.org/pdf/2506.00688.pdf", "abs": "https://arxiv.org/abs/2506.00688", "title": "Existing Large Language Model Unlearning Evaluations Are Inconclusive", "authors": ["Zhili Feng", "Yixuan Even Xu", "Alexander Robey", "Robert Kirk", "Xander Davies", "Yarin Gal", "Avi Schwarzschild", "J. Zico Kolter"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Machine unlearning aims to remove sensitive or undesired data from large\nlanguage models. However, recent studies suggest that unlearning is often\nshallow, claiming that removed knowledge can easily be recovered. In this work,\nwe critically examine standard unlearning evaluation practices and uncover key\nlimitations that shake our trust in those findings. First, we show that some\nevaluations introduce substantial new information into the model, potentially\nmasking true unlearning performance by re-teaching the model during testing.\nSecond, we demonstrate that evaluation outcomes vary significantly across\ntasks, undermining the generalizability of current evaluation routines.\nFinally, we find that many evaluations rely on spurious correlations, making\ntheir results difficult to trust and interpret. Taken together, these issues\nsuggest that current evaluation protocols may both overstate and understate\nunlearning success. To address this, we propose two principles for future\nunlearning evaluations: minimal information injection and downstream task\nawareness. We validate these principles through a series of targeted\nexperiments, showing how violations of each can lead to misleading conclusions."}
{"id": "2506.00708", "pdf": "https://arxiv.org/pdf/2506.00708.pdf", "abs": "https://arxiv.org/abs/2506.00708", "title": "DrKGC: Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph Completion across General and Biomedical Domains", "authors": ["Yongkang Xiao", "Sinian Zhang", "Yi Dai", "Huixue Zhou", "Jue Hou", "Jie Ding", "Rui Zhang"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Knowledge graph completion (KGC) aims to predict missing triples in knowledge\ngraphs (KGs) by leveraging existing triples and textual information. Recently,\ngenerative large language models (LLMs) have been increasingly employed for\ngraph tasks. However, current approaches typically encode graph context in\ntextual form, which fails to fully exploit the potential of LLMs for perceiving\nand reasoning about graph structures. To address this limitation, we propose\nDrKGC (Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph\nCompletion). DrKGC employs a flexible lightweight model training strategy to\nlearn structural embeddings and logical rules within the KG. It then leverages\na novel bottom-up graph retrieval method to extract a subgraph for each query\nguided by the learned rules. Finally, a graph convolutional network (GCN)\nadapter uses the retrieved subgraph to enhance the structural embeddings, which\nare then integrated into the prompt for effective LLM fine-tuning. Experimental\nresults on two general domain benchmark datasets and two biomedical datasets\ndemonstrate the superior performance of DrKGC. Furthermore, a realistic case\nstudy in the biomedical domain highlights its interpretability and practical\nutility."}
{"id": "2506.00732", "pdf": "https://arxiv.org/pdf/2506.00732.pdf", "abs": "https://arxiv.org/abs/2506.00732", "title": "Bregman Conditional Random Fields: Sequence Labeling with Parallelizable Inference Algorithms", "authors": ["Caio Corro", "Mathieu Lacroix", "Joseph Le Roux"], "categories": ["cs.LG", "cs.CL"], "comment": "ACL 2025", "summary": "We propose a novel discriminative model for sequence labeling called Bregman\nconditional random fields (BCRF). Contrary to standard linear-chain conditional\nrandom fields, BCRF allows fast parallelizable inference algorithms based on\niterative Bregman projections. We show how such models can be learned using\nFenchel-Young losses, including extension for learning from partial labels.\nExperimentally, our approach delivers comparable results to CRF while being\nfaster, and achieves better results in highly constrained settings compared to\nmean field, another parallelizable alternative."}
{"id": "2506.00772", "pdf": "https://arxiv.org/pdf/2506.00772.pdf", "abs": "https://arxiv.org/abs/2506.00772", "title": "LIFT the Veil for the Truth: Principal Weights Emerge after Rank Reduction for Reasoning-Focused Supervised Fine-Tuning", "authors": ["Zihang Liu", "Tianyu Pang", "Oleg Balabanov", "Chaoqun Yang", "Tianjin Huang", "Lu Yin", "Yaoqing Yang", "Shiwei Liu"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "ICML 2025", "summary": "Recent studies have shown that supervised fine-tuning of LLMs on a small\nnumber of high-quality datasets can yield strong reasoning capabilities.\nHowever, full fine-tuning (Full FT), while powerful, is computationally\nexpensive and susceptible to overfitting and catastrophic forgetting,\nparticularly when data is limited. Sparse fine-tuning, which previously\nachieved notable success by updating only a small subset of model parameters,\noffers a promising trade-off between efficiency and effectiveness. Yet, it has\nlagged behind in the LLM era due to the difficulty of identifying parameters\ntruly critical for reasoning. In this work, we state that weights with the\nlargest magnitude after low-rank approximation are critical weights for\nfine-tuning, which we call Principal Weights. Surprisingly, while\nmagnitude-based sparse fine-tuning performs poorly as a baseline on LLM\nfine-tuning, it becomes highly effective after rank reduction. These insights\nmotivate our method: Low-rank Informed Sparse Fine-Tuning (LIFT). LIFT only\nupdates the top 5% Principal Weights throughout training and consistently\nachieves better performance on reasoning tasks than Full FT, while maintaining\nmemory efficiency on par with popular parameter-efficient fine-tuning methods.\nIn addition to strong performance on target domains such as arithmetic\nreasoning, LIFT also retains up to 20% more source-domain knowledge, compared\nto Full FT and LoRA. Our code is available at:\nhttps://github.com/zihanghliu/LIFT."}
{"id": "2506.00805", "pdf": "https://arxiv.org/pdf/2506.00805.pdf", "abs": "https://arxiv.org/abs/2506.00805", "title": "HSCR: Hierarchical Self-Contrastive Rewarding for Aligning Medical Vision Language Models", "authors": ["Songtao Jiang", "Yan Zhang", "Yeying Jin", "Zhihang Tang", "Yangyang Wu", "Yang Feng", "Jian Wu", "Zuozhu Liu"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Medical Vision-Language Models (Med-VLMs) have achieved success across\nvarious tasks, yet most existing methods overlook the modality misalignment\nissue that can lead to untrustworthy responses in clinical settings. In this\npaper, we propose Hierarchical Self-Contrastive Rewarding (HSCR), a novel\napproach that addresses two critical challenges in Med-VLM alignment: 1)\nCost-effective generation of high-quality preference data; 2) Capturing nuanced\nand context-aware preferences for improved alignment. HSCR first leverages the\ninherent capability of Med-VLMs to generate dispreferred responses with higher\nsampling probability. By analyzing output logit shifts after visual token\ndropout, we identify modality-coupled tokens that induce misalignment and\nderive an implicit alignment reward function. This function guides token\nreplacement with hallucinated ones during decoding, producing high-quality\ndispreferred data. Furthermore, HSCR introduces a multi-level preference\noptimization strategy, which extends beyond traditional adjacent-level\noptimization by incorporating nuanced implicit preferences, leveraging relative\nquality in dispreferred data to capture subtle alignment cues for more precise\nand context-aware optimization. Extensive experiments across multiple medical\ntasks, including Med-VQA, medical image captioning and instruction following,\ndemonstrate that HSCR not only enhances zero-shot performance but also\nsignificantly improves modality alignment and trustworthiness with just 2,000\ntraining entries."}
{"id": "2506.00845", "pdf": "https://arxiv.org/pdf/2506.00845.pdf", "abs": "https://arxiv.org/abs/2506.00845", "title": "Generalizable LLM Learning of Graph Synthetic Data with Reinforcement Learning", "authors": ["Yizhuo Zhang", "Heng Wang", "Shangbin Feng", "Zhaoxuan Tan", "Xinyun Liu", "Yulia Tsvetkov"], "categories": ["cs.LG", "cs.CL", "I.2.7; I.2.2"], "comment": "9 pages, 3 figures, 3 tables. Experimental code and results are\n  publicly available at\n  https://anonymous.4open.science/r/Graph_RL-BF08/readme.md", "summary": "Previous research has sought to enhance the graph reasoning capabilities of\nLLMs by supervised fine-tuning on synthetic graph data. While these led to\nspecialized LLMs better at solving graph algorithm problems, we don't need LLMs\nfor shortest path: we need generalization from synthetic graph data to\nreal-world tasks with implicit graph structures. In this work, we propose to\nunlock generalizable learning of graph synthetic data with reinforcement\nlearning. We first design solution-based and process-based rewards for\nsynthetic graph problems: instead of rigid memorizing response patterns in\ndirect fine-tuning, we posit that RL would help LLMs grasp the essentials\nunderlying graph reasoning and alleviate overfitting. We employ RL algorithms\nsuch as GRPO and DPO, aligning both off-the-shelf LLMs and LLMs fine-tuned on\nsynthetic graph data. We then compare them against existing settings on both\nin-domain synthetic tasks and out-of-domain real-world tasks with implicit\ngraph structures such as multi-hop QA, structured planning, and more. Extensive\nexperiments demonstrate that our RL recipe leads to statistically significant\nimprovement on 5 datasets, with an average gain of 12.9\\% over baseline\nsettings. Further analysis reveals that process-based rewards consistently\noutperform solution-based rewards, mixing synthetic and real-world task data\nyields potential gains, while compositionality and explainable intermediate\nsteps remains a critical challenge even after RL."}
{"id": "2506.00871", "pdf": "https://arxiv.org/pdf/2506.00871.pdf", "abs": "https://arxiv.org/abs/2506.00871", "title": "Towards Predicting Any Human Trajectory In Context", "authors": ["Ryo Fujii", "Hideo Saito", "Ryo Hachiuma"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "comment": null, "summary": "Predicting accurate future trajectories of pedestrians is essential for\nautonomous systems but remains a challenging task due to the need for\nadaptability in different environments and domains. A common approach involves\ncollecting scenario-specific data and performing fine-tuning via\nbackpropagation. However, this process is often impractical on edge devices due\nto constrained computational resources. To address this challenge, we introduce\nTrajICL, an In-Context Learning (ICL) framework for pedestrian trajectory\nprediction that enables rapid adaptation without fine-tuning on the\nscenario-specific data. We propose a spatio-temporal similarity-based example\nselection (STES) method that selects relevant examples from previously observed\ntrajectories within the same scene by identifying similar motion patterns at\ncorresponding locations. To further refine this selection, we introduce\nprediction-guided example selection (PG-ES), which selects examples based on\nboth the past trajectory and the predicted future trajectory, rather than\nrelying solely on the past trajectory. This approach allows the model to\naccount for long-term dynamics when selecting examples. Finally, instead of\nrelying on small real-world datasets with limited scenario diversity, we train\nour model on a large-scale synthetic dataset to enhance its prediction ability\nby leveraging in-context examples. Extensive experiments demonstrate that\nTrajICL achieves remarkable adaptation across both in-domain and cross-domain\nscenarios, outperforming even fine-tuned approaches across multiple public\nbenchmarks. The code will be released at\nhttps://fujiry0.github.io/TrajICL-project-page."}
{"id": "2506.00894", "pdf": "https://arxiv.org/pdf/2506.00894.pdf", "abs": "https://arxiv.org/abs/2506.00894", "title": "CODEMENV: Benchmarking Large Language Models on Code Migration", "authors": ["Keyuan Cheng", "Xudong Shen", "Yihao Yang", "Tengyue Wang", "Yang Cao", "Muhammad Asif Ali", "Hanbin Wang", "Lijie Hu", "Di Wang"], "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted by ACL 2025 Findings", "summary": "Large language models (LLMs) have shown remarkable capabilities across\nvarious software engineering tasks; however, their effectiveness in code\nmigration, adapting code to run in different environments, remains\ninsufficiently studied. In this work, we introduce CODEMENV: Code Migration\nAcross Environment, a new benchmark specifically designed to assess LLMs'\nabilities in code migration scenarios. CODEMENV consists of 922 examples\nspanning 19 Python and Java packages, and covers three core tasks: (1)\nidentifying functions incompatible with specific versions, (2) detecting\nchanges in function definitions, and (3) adapting code to target environments.\nExperimental evaluation with seven LLMs on CODEMENV yields an average pass@1\nrate of 26.50%, with GPT-4O achieving the highest score at 43.84%. Key findings\ninclude: (i) LLMs tend to be more proficient with newer function versions,\nwhich aids in migrating legacy code, and (ii) LLMs sometimes exhibit logical\ninconsistencies by identifying function changes irrelevant to the intended\nmigration environment. The datasets are available at\nhttps://github.com/xdshen-ai/Benchmark-of-Code-Migration."}
{"id": "2506.00920", "pdf": "https://arxiv.org/pdf/2506.00920.pdf", "abs": "https://arxiv.org/abs/2506.00920", "title": "Position as Probability: Self-Supervised Transformers that Think Past Their Training for Length Extrapolation", "authors": ["Philip Heejun Lee"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NE"], "comment": "Note: v1: working paper; code, additional baselines, ablations, will\n  follow in v2", "summary": "Deep sequence models typically degrade in accuracy when test sequences\nsignificantly exceed their training lengths, yet many critical tasks--such as\nalgorithmic reasoning, multi-step arithmetic, and compositional\ngeneralization--require robust length extrapolation. We introduce PRISM, a\nProbabilistic Relative-position Implicit Superposition Model, a novel\npositional encoding mechanism that enables Transformers to extrapolate\naccurately up to 10x beyond their training length. PRISM learns continuous\nrelative positions through a differentiable histogram-filter update, preserving\nposition uncertainty via a probabilistic superposition rather than conventional\ndeterministic embeddings. Empirically, PRISM achieves state-of-the-art length\nextrapolation, successfully generalizing to previously intractable sequence\nlengths across algorithmic benchmarks--including arithmetic (addition,\nmultiplication), SCAN compositionality tasks, and complex copy variants derived\nfrom DeepMind's recent datasets. Our analysis demonstrates that PRISM's\nstochastic positional encoding maintains sharp and interpretable internal\nstates, providing a theoretical basis for reliable length generalization. These\nresults advance the goal of neural sequence models that remain algorithmically\nrobust at lengths far exceeding their training horizon."}
{"id": "2506.00928", "pdf": "https://arxiv.org/pdf/2506.00928.pdf", "abs": "https://arxiv.org/abs/2506.00928", "title": "Deep Temporal Reasoning in Video Language Models: A Cross-Linguistic Evaluation of Action Duration and Completion through Perfect Times", "authors": ["Olga Loginova", "Sof√≠a Ortega Loguinova"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Human perception of events is intrinsically tied to distinguishing between\ncompleted (perfect and telic) and ongoing (durative) actions, a process\nmediated by both linguistic structure and visual cues. In this work, we\nintroduce the \\textbf{Perfect Times} dataset, a novel, quadrilingual (English,\nItalian, Russian, and Japanese) multiple-choice question-answering benchmark\ndesigned to assess video-language models (VLMs) on temporal reasoning. By\npairing everyday activity videos with event completion labels and\nperfectivity-tailored distractors, our dataset probes whether models truly\ncomprehend temporal dynamics or merely latch onto superficial markers.\nExperimental results indicate that state-of-the-art models, despite their\nsuccess on text-based tasks, struggle to mirror human-like temporal and causal\nreasoning grounded in video. This study underscores the necessity of\nintegrating deep multimodal cues to capture the nuances of action duration and\ncompletion within temporal and causal video dynamics, setting a new standard\nfor evaluating and advancing temporal reasoning in VLMs."}
{"id": "2506.00930", "pdf": "https://arxiv.org/pdf/2506.00930.pdf", "abs": "https://arxiv.org/abs/2506.00930", "title": "Aligning VLM Assistants with Personalized Situated Cognition", "authors": ["Yongqi Li", "Shen Zhou", "Xiaohu Li", "Xin Miao", "Jintao Wen", "Mayi Xu", "Jianhao Chen", "Birong Pan", "Hankun Kang", "Yuanyuan Zhu", "Ming Zhong", "Tieyun Qian"], "categories": ["cs.AI", "cs.CL"], "comment": "Accepted to ACL 2025 (main), camera-ready version", "summary": "Vision-language models (VLMs) aligned with general human objectives, such as\nbeing harmless and hallucination-free, have become valuable assistants of\nhumans in managing visual tasks. However, people with diversified backgrounds\nhave different cognition even in the same situation. Consequently, they may\nhave personalized expectations for VLM assistants. This highlights the urgent\nneed to align VLM assistants with personalized situated cognition for\nreal-world assistance. To study this problem, we first simplify it by\ncharacterizing individuals based on the sociological concept of Role-Set. Then,\nwe propose to evaluate the individuals' actions to examine whether the\npersonalized alignment is achieved. Further, we construct a benchmark named\nPCogAlignBench, which includes 18k instances and 20 individuals with different\nRole-Sets. Finally, we present a framework called PCogAlign, which constructs a\ncognition-aware and action-based reward model for personalized alignment.\nExperimental results and human evaluations demonstrate the reliability of the\nPCogAlignBench and the effectiveness of our proposed PCogAlign. We will\nopen-source the constructed benchmark and code at\nhttps://github.com/NLPGM/PCogAlign."}
{"id": "2506.00958", "pdf": "https://arxiv.org/pdf/2506.00958.pdf", "abs": "https://arxiv.org/abs/2506.00958", "title": "Speaking Beyond Language: A Large-Scale Multimodal Dataset for Learning Nonverbal Cues from Video-Grounded Dialogues", "authors": ["Youngmin Kim", "Jiwan Chung", "Jisoo Kim", "Sunghyun Lee", "Sangkyu Lee", "Junhyeok Kim", "Cheoljong Yang", "Youngjae Yu"], "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "Accepted to ACL 2025 (Main), Our code and dataset:\n  https://github.com/winston1214/nonverbal-conversation", "summary": "Nonverbal communication is integral to human interaction, with gestures,\nfacial expressions, and body language conveying critical aspects of intent and\nemotion. However, existing large language models (LLMs) fail to effectively\nincorporate these nonverbal elements, limiting their capacity to create fully\nimmersive conversational experiences. We introduce MARS, a multimodal language\nmodel designed to understand and generate nonverbal cues alongside text,\nbridging this gap in conversational AI. Our key innovation is VENUS, a\nlarge-scale dataset comprising annotated videos with time-aligned text, facial\nexpressions, and body language. Leveraging VENUS, we train MARS with a\nnext-token prediction objective, combining text with vector-quantized nonverbal\nrepresentations to achieve multimodal understanding and generation within a\nunified framework. Based on various analyses of the VENUS datasets, we validate\nits substantial scale and high effectiveness. Our quantitative and qualitative\nresults demonstrate that MARS successfully generates text and nonverbal\nlanguages, corresponding to conversational input."}
{"id": "2506.00983", "pdf": "https://arxiv.org/pdf/2506.00983.pdf", "abs": "https://arxiv.org/abs/2506.00983", "title": "Bridging the Gap: From Ad-hoc to Proactive Search in Conversations", "authors": ["Chuan Meng", "Francesco Tonolini", "Fengran Mo", "Nikolaos Aletras", "Emine Yilmaz", "Gabriella Kazai"], "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG", "H.3.3"], "comment": "Accepted as a full paper at SIGIR 2025", "summary": "Proactive search in conversations (PSC) aims to reduce user effort in\nformulating explicit queries by proactively retrieving useful relevant\ninformation given conversational context. Previous work in PSC either directly\nuses this context as input to off-the-shelf ad-hoc retrievers or further\nfine-tunes them on PSC data. However, ad-hoc retrievers are pre-trained on\nshort and concise queries, while the PSC input is longer and noisier. This\ninput mismatch between ad-hoc search and PSC limits retrieval quality. While\nfine-tuning on PSC data helps, its benefits remain constrained by this input\ngap. In this work, we propose Conv2Query, a novel conversation-to-query\nframework that adapts ad-hoc retrievers to PSC by bridging the input gap\nbetween ad-hoc search and PSC. Conv2Query maps conversational context into\nad-hoc queries, which can either be used as input for off-the-shelf ad-hoc\nretrievers or for further fine-tuning on PSC data. Extensive experiments on two\nPSC datasets show that Conv2Query significantly improves ad-hoc retrievers'\nperformance, both when used directly and after fine-tuning on PSC."}
{"id": "2506.01055", "pdf": "https://arxiv.org/pdf/2506.01055.pdf", "abs": "https://arxiv.org/abs/2506.01055", "title": "Simple Prompt Injection Attacks Can Leak Personal Data Observed by LLM Agents During Task Execution", "authors": ["Meysam Alizadeh", "Zeynab Samei", "Daria Stetsenko", "Fabrizio Gilardi"], "categories": ["cs.CR", "cs.CL", "68Txx"], "comment": "25 pages, 18 figures, NeurIPS formatting style", "summary": "Previous benchmarks on prompt injection in large language models (LLMs) have\nprimarily focused on generic tasks and attacks, offering limited insights into\nmore complex threats like data exfiltration. This paper examines how prompt\ninjection can cause tool-calling agents to leak personal data observed during\ntask execution. Using a fictitious banking agent, we develop data flow-based\nattacks and integrate them into AgentDojo, a recent benchmark for agentic\nsecurity. To enhance its scope, we also create a richer synthetic dataset of\nhuman-AI banking conversations. In 16 user tasks from AgentDojo, LLMs show a\n15-50 percentage point drop in utility under attack, with average attack\nsuccess rates (ASR) around 20 percent; some defenses reduce ASR to zero. Most\nLLMs, even when successfully tricked by the attack, avoid leaking highly\nsensitive data like passwords, likely due to safety alignments, but they remain\nvulnerable to disclosing other personal data. The likelihood of password\nleakage increases when a password is requested along with one or two additional\npersonal details. In an extended evaluation across 48 tasks, the average ASR is\naround 15 percent, with no built-in AgentDojo defense fully preventing leakage.\nTasks involving data extraction or authorization workflows, which closely\nresemble the structure of exfiltration attacks, exhibit the highest ASRs,\nhighlighting the interaction between task type, agent performance, and defense\nefficacy."}
{"id": "2506.01115", "pdf": "https://arxiv.org/pdf/2506.01115.pdf", "abs": "https://arxiv.org/abs/2506.01115", "title": "Attention Retrieves, MLP Memorizes: Disentangling Trainable Components in the Transformer", "authors": ["Yihe Dong", "Lorenzo Noci", "Mikhail Khodak", "Mufan Li"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "The Transformer architecture is central to the success of modern Large\nLanguage Models (LLMs), in part due to its surprising ability to perform a wide\nrange of algorithmic tasks -- including mathematical reasoning, memorization,\nand retrieval -- using only gradient-based training on next-token prediction.\nWhile the core component of a Transformer is the self-attention mechanism, we\nquestion how much, and which aspects, of the performance gains can be\nattributed to it. To this end, we compare standard Transformers to variants in\nwhich either the multi-layer perceptron (MLP) layers or the attention\nprojectors (queries and keys) are frozen at initialization. To further isolate\nthe contribution of attention, we introduce MixiT -- the Mixing Transformer --\na simplified, principled model in which the attention coefficients are entirely\nrandom and fixed at initialization, eliminating any input-dependent computation\nor learning in attention. Surprisingly, we find that MixiT matches the\nperformance of fully trained Transformers on various algorithmic tasks,\nespecially those involving basic arithmetic or focusing heavily on\nmemorization. For retrieval-based tasks, we observe that having input-dependent\nattention coefficients is consistently beneficial, while MixiT underperforms.\nWe attribute this failure to its inability to form specialized circuits such as\ninduction heads -- a specific circuit known to be crucial for learning and\nexploiting repeating patterns in input sequences. Even more interestingly, we\nfind that attention with frozen key and query projectors is not only able to\nform induction heads, but can also perform competitively on language modeling.\nOur results underscore the importance of architectural heterogeneity, where\ndistinct components contribute complementary inductive biases crucial for\nsolving different classes of tasks."}
{"id": "2506.01151", "pdf": "https://arxiv.org/pdf/2506.01151.pdf", "abs": "https://arxiv.org/abs/2506.01151", "title": "Earley-Driven Dynamic Pruning for Efficient Structured Decoding", "authors": ["Xintong Sun", "Chi Wei", "Minghao Tian", "Shiwen Ni"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "ICML2025 poster", "summary": "Large Language Models (LLMs) have shown remarkable capabilities, yet ensuring\ntheir outputs conform to strict structural or grammatical constraints remains\nchallenging, which is critical in function calls and domain-specific language\n(DSL) generation. Constrained decoding with context-free grammar is a flexible\napproach to guarantee LLMs' adherence to a specific format by dynamically\nbuilding a token logits mask. However, creating this mask requires checking the\nvalidity of all tokens in the LLM vocabulary at every decoding step, which\noften incurs significant overheads in existing constrained decoding engines. To\naddress this challenge, we propose $\\textbf{ZapFormat}$, a novel\n$\\textbf{dynamic pruning}$ strategy based on the Earley algorithm that\nidentifies and eliminates invalid or redundant Earley states in real-time,\nsignificantly reducing memory occupation of the Earley algorithm's states. This\nfurther enables us to use a state cache to speed up structured generations on a\nlarge number of queries. We implemented ZapFormat in a new constrained decoding\nengine called Formatron which also incorporates existing optimizations. Through\ncomprehensive experiments on structured generation tasks, including JSON\ngeneration, JSON Schema validation, and semantic parsing, we demonstrate that\nFormatron not only $\\textbf{consistently maintains}$ high-precision compliant\noutputs but also achieves $\\textbf{significant improvements}$ in inference\nspeed up to 2x compared to state-of-the-art implementations. More importantly,\nFormatron is generally applicable across various LLM architectures. We release\nFormatron as open source at https://github.com/Dan-wanna-M/formatron."}
{"id": "2506.01256", "pdf": "https://arxiv.org/pdf/2506.01256.pdf", "abs": "https://arxiv.org/abs/2506.01256", "title": "Confidence intervals for forced alignment boundaries using model ensembles", "authors": ["Matthew C. Kelley"], "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "comment": "submitted for publication; 7 pages, 1 figure", "summary": "Forced alignment is a common tool to align audio with orthographic and\nphonetic transcriptions. Most forced alignment tools provide only a single\nestimate of a boundary. The present project introduces a method of deriving\nconfidence intervals for these boundaries using a neural network ensemble\ntechnique. Ten different segment classifier neural networks were previously\ntrained, and the alignment process is repeated with each model. The alignment\nensemble is then used to place the boundary at the median of the boundaries in\nthe ensemble, and 97.85% confidence intervals are constructed using order\nstatistics. On the Buckeye and TIMIT corpora, the ensemble boundaries show a\nslight improvement over using just a single model. The confidence intervals are\nincorporated into Praat TextGrids using a point tier, and they are also output\nas a table for researchers to analyze separately as diagnostics or to\nincorporate uncertainty into their analyses."}
{"id": "2506.01293", "pdf": "https://arxiv.org/pdf/2506.01293.pdf", "abs": "https://arxiv.org/abs/2506.01293", "title": "Abstractive Visual Understanding of Multi-modal Structured Knowledge: A New Perspective for MLLM Evaluation", "authors": ["Yichi Zhang", "Zhuo Chen", "Lingbing Guo", "Yajing Xu", "Min Zhang", "Wen Zhang", "Huajun Chen"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Work in progress", "summary": "Multi-modal large language models (MLLMs) incorporate heterogeneous\nmodalities into LLMs, enabling a comprehensive understanding of diverse\nscenarios and objects. Despite the proliferation of evaluation benchmarks and\nleaderboards for MLLMs, they predominantly overlook the critical capacity of\nMLLMs to comprehend world knowledge with structured abstractions that appear in\nvisual form. To address this gap, we propose a novel evaluation paradigm and\ndevise M3STR, an innovative benchmark grounded in the Multi-Modal Map for\nSTRuctured understanding. This benchmark leverages multi-modal knowledge graphs\nto synthesize images encapsulating subgraph architectures enriched with\nmulti-modal entities. M3STR necessitates that MLLMs not only recognize the\nmulti-modal entities within the visual inputs but also decipher intricate\nrelational topologies among them. We delineate the benchmark's statistical\nprofiles and automated construction pipeline, accompanied by an extensive\nempirical analysis of 26 state-of-the-art MLLMs. Our findings reveal persistent\ndeficiencies in processing abstractive visual information with structured\nknowledge, thereby charting a pivotal trajectory for advancing MLLMs' holistic\nreasoning capacities. Our code and data are released at\nhttps://github.com/zjukg/M3STR"}
{"id": "2506.01301", "pdf": "https://arxiv.org/pdf/2506.01301.pdf", "abs": "https://arxiv.org/abs/2506.01301", "title": "Overcoming Multi-step Complexity in Multimodal Theory-of-Mind Reasoning: A Scalable Bayesian Planner", "authors": ["Chunhui Zhang", "Zhongyu Ouyang", "Kwonjoon Lee", "Nakul Agarwal", "Sean Dae Houlihan", "Soroush Vosoughi", "Shao-Yuan Lo"], "categories": ["cs.AI", "cs.CL"], "comment": "Accepted as a Spotlight at the 2025 Forty-Second International\n  Conference on Machine Learning (ICML 2025)", "summary": "Theory-of-Mind (ToM) enables humans to infer mental states-such as beliefs,\ndesires, and intentions-forming the foundation of social cognition. However,\nexisting computational ToM methods rely on structured workflows with\nToM-specific priors or deep model fine-tuning, which struggle with scalability\nin multimodal environments and fail to generalize as task complexity increases.\nTo address these limitations, we propose a scalable Bayesian ToM planner that\ndecomposes ToM reasoning into stepwise Bayesian updates. Our framework\nintroduces weak-to-strong control, allowing smaller language models (LMs) to\nspecialize in ToM-specific likelihood estimation and transfer their reasoning\nbehaviors to larger LMs (7B to 405B) for integration with social and world\nknowledge. This synergistic approach aligns large-model inference of human\nmental states with Bayesian principles. Extensive experiments show that our\nmethod achieves a 4.6% accuracy improvement over state-of-the-art techniques on\nmultimodal ToM benchmarks, including challenging unseen scenarios, thereby\nestablishing a new standard for modeling human mental states in complex\nenvironments."}
{"id": "2506.01332", "pdf": "https://arxiv.org/pdf/2506.01332.pdf", "abs": "https://arxiv.org/abs/2506.01332", "title": "An Empirical Study of Group Conformity in Multi-Agent Systems", "authors": ["Min Choi", "Keonwoo Kim", "Sungwon Chae", "Sangyeob Baek"], "categories": ["cs.AI", "cs.CL", "cs.MA"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have enabled multi-agent\nsystems that simulate real-world interactions with near-human reasoning. While\nprevious studies have extensively examined biases related to protected\nattributes such as race, the emergence and propagation of biases on socially\ncontentious issues in multi-agent LLM interactions remain underexplored. This\nstudy explores how LLM agents shape public opinion through debates on five\ncontentious topics. By simulating over 2,500 debates, we analyze how initially\nneutral agents, assigned a centrist disposition, adopt specific stances over\ntime. Statistical analyses reveal significant group conformity mirroring human\nbehavior; LLM agents tend to align with numerically dominant groups or more\nintelligent agents, exerting a greater influence. These findings underscore the\ncrucial role of agent intelligence in shaping discourse and highlight the risks\nof bias amplification in online interactions. Our results emphasize the need\nfor policy measures that promote diversity and transparency in LLM-generated\ndiscussions to mitigate the risks of bias propagation within anonymous online\nenvironments."}
{"id": "2506.01365", "pdf": "https://arxiv.org/pdf/2506.01365.pdf", "abs": "https://arxiv.org/abs/2506.01365", "title": "Attention Is Not Always the Answer: Optimizing Voice Activity Detection with Simple Feature Fusion", "authors": ["Kumud Tripathi", "Chowdam Venkata Kumar", "Pankaj Wasnik"], "categories": ["cs.SD", "cs.CL"], "comment": "Accepted at INTERSPEECH 2025, 5 pages, 4 figures, 2 tables", "summary": "Voice Activity Detection (VAD) plays a key role in speech processing, often\nutilizing hand-crafted or neural features. This study examines the\neffectiveness of Mel-Frequency Cepstral Coefficients (MFCCs) and pre-trained\nmodel (PTM) features, including wav2vec 2.0, HuBERT, WavLM, UniSpeech, MMS, and\nWhisper. We propose FusionVAD, a unified framework that combines both feature\ntypes using three fusion strategies: concatenation, addition, and\ncross-attention (CA). Experimental results reveal that simple fusion\ntechniques, particularly addition, outperform CA in both accuracy and\nefficiency. Fusion-based models consistently surpass single-feature models,\nhighlighting the complementary nature of MFCCs and PTM features. Notably, our\nbest-performing fusion model exceeds the state-of-the-art Pyannote across\nmultiple datasets, achieving an absolute average improvement of 2.04%. These\nresults confirm that simple feature fusion enhances VAD robustness while\nmaintaining computational efficiency."}
{"id": "2506.01372", "pdf": "https://arxiv.org/pdf/2506.01372.pdf", "abs": "https://arxiv.org/abs/2506.01372", "title": "AI Scientists Fail Without Strong Implementation Capability", "authors": ["Minjun Zhu", "Qiujie Xie", "Yixuan Weng", "Jian Wu", "Zhen Lin", "Linyi Yang", "Yue Zhang"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Position", "summary": "The emergence of Artificial Intelligence (AI) Scientist represents a paradigm\nshift in scientific discovery, with large language models (LLMs) taking the\nlead as the primary executor in the entire scientific workflow from idea\ngeneration to experiment implementation. Recent AI Scientist studies\ndemonstrate sufficient capabilities for independent scientific discovery, with\nthe generated research reports gaining acceptance at the ICLR 2025 workshop and\nACL 2025, arguing that a human-level AI Scientist, capable of uncovering\nphenomena previously unknown to humans, may be imminent. Despite this\nsubstantial progress, AI Scientist has yet to produce a groundbreaking\nachievement in the domain of computer science on par with automated scientific\ntools. Based on extensive quantitative evidence from existing benchmarks in\ncomplex engineering tasks and a systematic evaluation assess 28 research papers\ngenerated by five advanced AI Scientist systems, we argue that \\textbf{the\nfundamental bottleneck for AI Scientists lies in their capability to execute\nthe requisite verification procedures.} Current AI Scientist systems lack the\nexecution capabilities needed to execute rigorous experiments and produce\nhigh-quality scientific papers. To better illustrate the root cause of this\n\\textbf{implementation gap}, we provide an in-depth discussion on the\nfundamental limitations of AI Scientist. This position paper aims to call for\nthe participants in the community to bridge the implementation gap."}
{"id": "2506.01391", "pdf": "https://arxiv.org/pdf/2506.01391.pdf", "abs": "https://arxiv.org/abs/2506.01391", "title": "AgentCPM-GUI: Building Mobile-Use Agents with Reinforcement Fine-Tuning", "authors": ["Zhong Zhang", "Yaxi Lu", "Yikun Fu", "Yupeng Huo", "Shenzhi Yang", "Yesai Wu", "Han Si", "Xin Cong", "Haotian Chen", "Yankai Lin", "Jie Xie", "Wei Zhou", "Wang Xu", "Yuanheng Zhang", "Zhou Su", "Zhongwu Zhai", "Xiaoming Liu", "Yudong Mei", "Jianming Xu", "Hongyan Tian", "Chongyi Wang", "Chi Chen", "Yuan Yao", "Zhiyuan Liu", "Maosong Sun"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC", "I.2.8; I.2.7; I.2.10; H.5.2"], "comment": "The project is available at https://github.com/OpenBMB/AgentCPM-GUI", "summary": "The recent progress of large language model agents has opened new\npossibilities for automating tasks through graphical user interfaces (GUIs),\nespecially in mobile environments where intelligent interaction can greatly\nenhance usability. However, practical deployment of such agents remains\nconstrained by several key challenges. Existing training data is often noisy\nand lack semantic diversity, which hinders the learning of precise grounding\nand planning. Models trained purely by imitation tend to overfit to seen\ninterface patterns and fail to generalize in unfamiliar scenarios. Moreover,\nmost prior work focuses on English interfaces while overlooks the growing\ndiversity of non-English applications such as those in the Chinese mobile\necosystem. In this work, we present AgentCPM-GUI, an 8B-parameter GUI agent\nbuilt for robust and efficient on-device GUI interaction. Our training pipeline\nincludes grounding-aware pre-training to enhance perception, supervised\nfine-tuning on high-quality Chinese and English trajectories to imitate\nhuman-like actions, and reinforcement fine-tuning with GRPO to improve\nreasoning capability. We also introduce a compact action space that reduces\noutput length and supports low-latency execution on mobile devices.\nAgentCPM-GUI achieves state-of-the-art performance on five public benchmarks\nand a new Chinese GUI benchmark called CAGUI, reaching $96.9\\%$ Type-Match and\n$91.3\\%$ Exact-Match. To facilitate reproducibility and further research, we\npublicly release all code, model checkpoint, and evaluation data."}
{"id": "2506.01413", "pdf": "https://arxiv.org/pdf/2506.01413.pdf", "abs": "https://arxiv.org/abs/2506.01413", "title": "Incentivizing Reasoning for Advanced Instruction-Following of Large Language Models", "authors": ["Yulei Qin", "Gang Li", "Zongyi Li", "Zihan Xu", "Yuchen Shi", "Zhekai Lin", "Xiao Cui", "Ke Li", "Xing Sun"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "10 pages of main body, 3 tables, 5 figures, 40 pages of appendix", "summary": "Existing large language models (LLMs) face challenges of following complex\ninstructions, especially when multiple constraints are present and organized in\nparalleling, chaining, and branching structures. One intuitive solution, namely\nchain-of-thought (CoT), is expected to universally improve capabilities of\nLLMs. However, we find that the vanilla CoT exerts a negative impact on\nperformance due to its superficial reasoning pattern of simply paraphrasing the\ninstructions. It fails to peel back the compositions of constraints for\nidentifying their relationship across hierarchies of types and dimensions. To\nthis end, we propose a systematic method to boost LLMs in dealing with complex\ninstructions via incentivizing reasoning for test-time compute scaling. First,\nwe stem from the decomposition of complex instructions under existing\ntaxonomies and propose a reproducible data acquisition method. Second, we\nexploit reinforcement learning (RL) with verifiable rule-centric reward signals\nto cultivate reasoning specifically for instruction following. We address the\nshallow, non-essential nature of reasoning under complex instructions via\nsample-wise contrast for superior CoT enforcement. We also exploit behavior\ncloning of experts to facilitate steady distribution shift from fast-thinking\nLLMs to skillful reasoners. Extensive evaluations on seven comprehensive\nbenchmarks confirm the validity of the proposed method, where a 1.5B LLM\nachieves 11.74% gains with performance comparable to a 8B LLM. Codes and data\nare available at https://github.com/yuleiqin/RAIF."}
{"id": "2506.01475", "pdf": "https://arxiv.org/pdf/2506.01475.pdf", "abs": "https://arxiv.org/abs/2506.01475", "title": "PGPO: Enhancing Agent Reasoning via Pseudocode-style Planning Guided Preference Optimization", "authors": ["Zouying Cao", "Runze Wang", "Yifei Yang", "Xinbei Ma", "Xiaoyong Zhu", "Bo Zheng", "Hai Zhao"], "categories": ["cs.AI", "cs.CL"], "comment": "20 pages, 12 figures, 14 tables, ACL'25 Findings", "summary": "Large Language Model (LLM) agents have demonstrated impressive capabilities\nin handling complex interactive problems. Existing LLM agents mainly generate\nnatural language plans to guide reasoning, which is verbose and inefficient. NL\nplans are also tailored to specific tasks and restrict agents' ability to\ngeneralize across similar tasks. To this end, we explore pseudocode-style plans\n(P-code Plan) to capture the structural logic of reasoning. We find that P-code\nPlan empowers LLM agents with stronger generalization ability and more\nefficiency. Inspired by this finding, we propose a pseudocode-style Planning\nGuided Preference Optimization method called PGPO for effective agent learning.\nWith two planning-oriented rewards, PGPO further enhances LLM agents' ability\nto generate high-quality P-code Plans and subsequent reasoning. Experiments\nshow that PGPO achieves superior performance on representative agent benchmarks\nand outperforms the current leading baselines. Analyses reveal the advantage of\nPGPO in reducing action errors and omissions during reasoning."}
{"id": "2506.01478", "pdf": "https://arxiv.org/pdf/2506.01478.pdf", "abs": "https://arxiv.org/abs/2506.01478", "title": "MUDI: A Multimodal Biomedical Dataset for Understanding Pharmacodynamic Drug-Drug Interactions", "authors": ["Tung-Lam Ngo", "Ba-Hoang Tran", "Duy-Cat Can", "Trung-Hieu Do", "Oliver Y. Ch√©n", "Hoang-Quynh Le"], "categories": ["cs.LG", "cs.CL", "cs.MM", "q-bio.QM"], "comment": null, "summary": "Understanding the interaction between different drugs (drug-drug interaction\nor DDI) is critical for ensuring patient safety and optimizing therapeutic\noutcomes. Existing DDI datasets primarily focus on textual information,\noverlooking multimodal data that reflect complex drug mechanisms. In this\npaper, we (1) introduce MUDI, a large-scale Multimodal biomedical dataset for\nUnderstanding pharmacodynamic Drug-drug Interactions, and (2) benchmark\nlearning methods to study it. In brief, MUDI provides a comprehensive\nmultimodal representation of drugs by combining pharmacological text, chemical\nformulas, molecular structure graphs, and images across 310,532 annotated drug\npairs labeled as Synergism, Antagonism, or New Effect. Crucially, to\neffectively evaluate machine-learning based generalization, MUDI consists of\nunseen drug pairs in the test set. We evaluate benchmark models using both late\nfusion voting and intermediate fusion strategies. All data, annotations,\nevaluation scripts, and baselines are released under an open research license."}
{"id": "2506.01510", "pdf": "https://arxiv.org/pdf/2506.01510.pdf", "abs": "https://arxiv.org/abs/2506.01510", "title": "LinearVC: Linear transformations of self-supervised features through the lens of voice conversion", "authors": ["Herman Kamper", "Benjamin van Niekerk", "Julian Za√Ødi", "Marc-Andr√© Carbonneau"], "categories": ["eess.AS", "cs.CL"], "comment": "Accepted to Interspeech 2025", "summary": "We introduce LinearVC, a simple voice conversion method that sheds light on\nthe structure of self-supervised representations. First, we show that simple\nlinear transformations of self-supervised features effectively convert voices.\nNext, we probe the geometry of the feature space by constraining the set of\nallowed transformations. We find that just rotating the features is sufficient\nfor high-quality voice conversion. This suggests that content information is\nembedded in a low-dimensional subspace which can be linearly transformed to\nproduce a target voice. To validate this hypothesis, we finally propose a\nmethod that explicitly factorizes content and speaker information using\nsingular value decomposition; the resulting linear projection with a rank of\njust 100 gives competitive conversion results. Our work has implications for\nboth practical voice conversion and a broader understanding of self-supervised\nspeech representations. Samples and code: https://www.kamperh.com/linearvc/."}
{"id": "2506.01551", "pdf": "https://arxiv.org/pdf/2506.01551.pdf", "abs": "https://arxiv.org/abs/2506.01551", "title": "EvolveNav: Self-Improving Embodied Reasoning for LLM-Based Vision-Language Navigation", "authors": ["Bingqian Lin", "Yunshuang Nie", "Khun Loun Zai", "Ziming Wei", "Mingfei Han", "Rongtao Xu", "Minzhe Niu", "Jianhua Han", "Liang Lin", "Cewu Lu", "Xiaodan Liang"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Building Vision-Language Navigation (VLN) agents which can navigate following\nnatural language instructions is a long-standing goal in human-robot\ninteraction applications. Recent studies have revealed the potential of\ntraining open-source Large Language Models (LLMs) to unleash LLMs' reasoning\nability for improving navigation, and simultaneously mitigate the domain gap\nbetween LLMs' training corpus and the VLN task. However, these approaches\nprimarily adopt direct input-output mapping paradigms, causing the mapping\nlearning difficult and the navigational decisions unexplainable.\nChain-of-Thought (CoT) training is a promising way to improve both navigational\ndecision accuracy and interpretability, while the complexity of the navigation\ntask makes the perfect CoT labels unavailable and may lead to overfitting\nthrough pure CoT supervised fine-tuning. In this paper, we propose a novel\nsElf-improving embodied reasoning framework for boosting LLM-based\nvision-language Navigation, dubbed EvolveNav. Our EvolveNav consists of two\nstages: (1) Formalized CoT Supervised Fine-Tuning, where we train the model\nwith formalized CoT labels to both activate the model's navigational reasoning\ncapabilities and increase the reasoning speed; (2) Self-Reflective\nPost-Training, where the model is iteratively trained with its own reasoning\noutputs as self-enriched CoT labels to enhance the supervision diversity. A\nself-reflective auxiliary task is also introduced to encourage learning correct\nreasoning patterns by contrasting with wrong ones. Experimental results on the\npopular VLN benchmarks demonstrate the superiority of EvolveNav over previous\nLLM-based VLN approaches. Code is available at\nhttps://github.com/expectorlin/EvolveNav."}
{"id": "2506.01671", "pdf": "https://arxiv.org/pdf/2506.01671.pdf", "abs": "https://arxiv.org/abs/2506.01671", "title": "AIMSCheck: Leveraging LLMs for AI-Assisted Review of Modern Slavery Statements Across Jurisdictions", "authors": ["Adriana Eufrosina Bora", "Akshatha Arodi", "Duoyi Zhang", "Jordan Bannister", "Mirko Bronzi", "Arsene Fansi Tchango", "Md Abul Bashar", "Richi Nayak", "Kerrie Mengersen"], "categories": ["cs.CY", "cs.CL"], "comment": "27 pages, to appear at ACL 2025", "summary": "Modern Slavery Acts mandate that corporations disclose their efforts to\ncombat modern slavery, aiming to enhance transparency and strengthen practices\nfor its eradication. However, verifying these statements remains challenging\ndue to their complex, diversified language and the sheer number of statements\nthat must be reviewed. The development of NLP tools to assist in this task is\nalso difficult due to a scarcity of annotated data. Furthermore, as modern\nslavery transparency legislation has been introduced in several countries, the\ngeneralizability of such tools across legal jurisdictions must be studied. To\naddress these challenges, we work with domain experts to make two key\ncontributions. First, we present AIMS.uk and AIMS.ca, newly annotated datasets\nfrom the UK and Canada to enable cross-jurisdictional evaluation. Second, we\nintroduce AIMSCheck, an end-to-end framework for compliance validation.\nAIMSCheck decomposes the compliance assessment task into three levels,\nenhancing interpretability and practical applicability. Our experiments show\nthat models trained on an Australian dataset generalize well across UK and\nCanadian jurisdictions, demonstrating the potential for broader application in\ncompliance monitoring. We release the benchmark datasets and AIMSCheck to the\npublic to advance AI-adoption in compliance assessment and drive further\nresearch in this field."}
{"id": "2506.01673", "pdf": "https://arxiv.org/pdf/2506.01673.pdf", "abs": "https://arxiv.org/abs/2506.01673", "title": "GRAM: Generative Recommendation via Semantic-aware Multi-granular Late Fusion", "authors": ["Sunkyung Lee", "Minjin Choi", "Eunseong Choi", "Hye-young Kim", "Jongwuk Lee"], "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "ACL 2025 (Main Conference)", "summary": "Generative recommendation is an emerging paradigm that leverages the\nextensive knowledge of large language models by formulating recommendations\ninto a text-to-text generation task. However, existing studies face two key\nlimitations in (i) incorporating implicit item relationships and (ii) utilizing\nrich yet lengthy item information. To address these challenges, we propose a\nGenerative Recommender via semantic-Aware Multi-granular late fusion (GRAM),\nintroducing two synergistic innovations. First, we design semantic-to-lexical\ntranslation to encode implicit hierarchical and collaborative item\nrelationships into the vocabulary space of LLMs. Second, we present\nmulti-granular late fusion to integrate rich semantics efficiently with minimal\ninformation loss. It employs separate encoders for multi-granular prompts,\ndelaying the fusion until the decoding stage. Experiments on four benchmark\ndatasets show that GRAM outperforms eight state-of-the-art generative\nrecommendation models, achieving significant improvements of 11.5-16.0% in\nRecall@5 and 5.3-13.6% in NDCG@5. The source code is available at\nhttps://github.com/skleee/GRAM."}
{"id": "2506.01689", "pdf": "https://arxiv.org/pdf/2506.01689.pdf", "abs": "https://arxiv.org/abs/2506.01689", "title": "Respond Beyond Language: A Benchmark for Video Generation in Response to Realistic User Intents", "authors": ["Shuting Wang", "Yunqi Liu", "Zixin Yang", "Ning Hu", "Zhicheng Dou", "Chenyan Xiong"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Querying generative AI models, e.g., large language models (LLMs), has become\na prevalent method for information acquisition. However, existing query-answer\ndatasets primarily focus on textual responses, making it challenging to address\ncomplex user queries that require visual demonstrations or explanations for\nbetter understanding. To bridge this gap, we construct a benchmark,\nRealVideoQuest, designed to evaluate the abilities of text-to-video (T2V)\nmodels in answering real-world, visually grounded queries. It identifies 7.5K\nreal user queries with video response intents from Chatbot-Arena and builds\n4.5K high-quality query-video pairs through a multistage video retrieval and\nrefinement process. We further develop a multi-angle evaluation system to\nassess the quality of generated video answers. Experiments indicate that\ncurrent T2V models struggle with effectively addressing real user queries,\npointing to key challenges and future research opportunities in multimodal AI."}
{"id": "2506.01716", "pdf": "https://arxiv.org/pdf/2506.01716.pdf", "abs": "https://arxiv.org/abs/2506.01716", "title": "Self-Challenging Language Model Agents", "authors": ["Yifei Zhou", "Sergey Levine", "Jason Weston", "Xian Li", "Sainbayar Sukhbaatar"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large language models are quickly becoming the foundation for intelligent\nagents that are capable of using tools. However, training such agents is\nchallenging because it requires human creation and annotation of a diverse set\nof tasks, tools, and evaluation criteria. In this paper, we propose the\nSelf-Challenging framework for training an agent on high-quality tasks that are\ngenerated by itself. The agent first plays the role of challenger and generates\na task after interacting with the given tools. The tasks take the form of a\nnovel general class of problems termed Code-as-Task, which are defined by an\ninstruction, a verification function and solution and failure cases which serve\nas tests, allowing to filter only for high-quality tasks. The agent then takes\nan executor role and trains on those tasks with reinforcement learning using\nthe evaluation feedback as a reward. Evaluation on two existing multi-turn\ntool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging\nframework achieves over a two-fold improvement in Llama-3.1-8B-Instruct,\ndespite using only self-generated training data."}
{"id": "2506.01789", "pdf": "https://arxiv.org/pdf/2506.01789.pdf", "abs": "https://arxiv.org/abs/2506.01789", "title": "Datasheets Aren't Enough: DataRubrics for Automated Quality Metrics and Accountability", "authors": ["Genta Indra Winata", "David Anugraha", "Emmy Liu", "Alham Fikri Aji", "Shou-Yi Hung", "Aditya Parashar", "Patrick Amadeus Irawan", "Ruochen Zhang", "Zheng-Xin Yong", "Jan Christian Blaise Cruz", "Niklas Muennighoff", "Seungone Kim", "Hanyang Zhao", "Sudipta Kar", "Kezia Erina Suryoraharjo", "M. Farid Adilazuarda", "En-Shiun Annie Lee", "Ayu Purwarianti", "Derry Tanti Wijaya", "Monojit Choudhury"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "eess.AS"], "comment": "Preprint", "summary": "High-quality datasets are fundamental to training and evaluating machine\nlearning models, yet their creation-especially with accurate human\nannotations-remains a significant challenge. Many dataset paper submissions\nlack originality, diversity, or rigorous quality control, and these\nshortcomings are often overlooked during peer review. Submissions also\nfrequently omit essential details about dataset construction and properties.\nWhile existing tools such as datasheets aim to promote transparency, they are\nlargely descriptive and do not provide standardized, measurable methods for\nevaluating data quality. Similarly, metadata requirements at conferences\npromote accountability but are inconsistently enforced. To address these\nlimitations, this position paper advocates for the integration of systematic,\nrubric-based evaluation metrics into the dataset review process-particularly as\nsubmission volumes continue to grow. We also explore scalable, cost-effective\nmethods for synthetic data generation, including dedicated tools and\nLLM-as-a-judge approaches, to support more efficient evaluation. As a call to\naction, we introduce DataRubrics, a structured framework for assessing the\nquality of both human- and model-generated datasets. Leveraging recent advances\nin LLM-based evaluation, DataRubrics offers a reproducible, scalable, and\nactionable solution for dataset quality assessment, enabling both authors and\nreviewers to uphold higher standards in data-centric research. We also release\ncode to support reproducibility of LLM-based evaluations at\nhttps://github.com/datarubrics/datarubrics."}
{"id": "2506.01863", "pdf": "https://arxiv.org/pdf/2506.01863.pdf", "abs": "https://arxiv.org/abs/2506.01863", "title": "Unified Scaling Laws for Compressed Representations", "authors": ["Andrei Panferov", "Alexandra Volkova", "Ionut-Vlad Modoranu", "Vage Egiazarian", "Mher Safaryan", "Dan Alistarh"], "categories": ["cs.LG", "cs.CL"], "comment": "Preprint", "summary": "Scaling laws have shaped recent advances in machine learning by enabling\npredictable scaling of model performance based on model size, computation, and\ndata volume. Concurrently, the rise in computational cost for AI has motivated\nmodel compression techniques, notably quantization and sparsification, which\nhave emerged to mitigate the steep computational demands associated with\nlarge-scale training and inference. This paper investigates the interplay\nbetween scaling laws and compression formats, exploring whether a unified\nscaling framework can accurately predict model performance when training occurs\nover various compressed representations, such as sparse, scalar-quantized,\nsparse-quantized or even vector-quantized formats. Our key contributions\ninclude validating a general scaling law formulation and showing that it is\napplicable both individually but also composably across compression types.\nBased on this, our main finding is demonstrating both theoretically and\nempirically that there exists a simple \"capacity\" metric -- based on the\nrepresentation's ability to fit random Gaussian data -- which can robustly\npredict parameter efficiency across multiple compressed representations. On the\npractical side, we extend our formulation to directly compare the accuracy\npotential of different compressed formats, and to derive better algorithms for\ntraining over sparse-quantized formats."}
{"id": "2506.01877", "pdf": "https://arxiv.org/pdf/2506.01877.pdf", "abs": "https://arxiv.org/abs/2506.01877", "title": "When Should Dense Retrievers Be Updated in Evolving Corpora? Detecting Out-of-Distribution Corpora Using GradNormIR", "authors": ["Dayoon Ko", "Jinyoung Kim", "Sohyeon Kim", "Jinhyuk Kim", "Jaehoon Lee", "Seonghak Song", "Minyoung Lee", "Gunhee Kim"], "categories": ["cs.IR", "cs.CL"], "comment": "ACL 2025 Findings", "summary": "Dense retrievers encode texts into embeddings to efficiently retrieve\nrelevant documents from large databases in response to user queries. However,\nreal-world corpora continually evolve, leading to a shift from the original\ntraining distribution of the retriever. Without timely updates or retraining,\nindexing newly emerging documents can degrade retrieval performance for future\nqueries. Thus, identifying when a dense retriever requires an update is\ncritical for maintaining robust retrieval systems. In this paper, we propose a\nnovel task of predicting whether a corpus is out-of-distribution (OOD) relative\nto a dense retriever before indexing. Addressing this task allows us to\nproactively manage retriever updates, preventing potential retrieval failures.\nWe introduce GradNormIR, an unsupervised approach that leverages gradient norms\nto detect OOD corpora effectively. Experiments on the BEIR benchmark\ndemonstrate that GradNormIR enables timely updates of dense retrievers in\nevolving document collections, significantly enhancing retrieval robustness and\nefficiency."}
{"id": "2506.01881", "pdf": "https://arxiv.org/pdf/2506.01881.pdf", "abs": "https://arxiv.org/abs/2506.01881", "title": "WHEN TO ACT, WHEN TO WAIT: Modeling Structural Trajectories for Intent Triggerability in Task-Oriented Dialogue", "authors": ["Yaoyao Qian", "Jindan Huang", "Yuanli Wang", "Simon Yu", "Kyrie Zhixuan Zhou", "Jiayuan Mao", "Mingfu Liang", "Hanhan Zhou"], "categories": ["cs.AI", "cs.CL"], "comment": "43 pages, 31 figures. Project website: https://nanostorm.netlify.app/", "summary": "Task-oriented dialogue systems often face difficulties when user utterances\nseem semantically complete but lack necessary structural information for\nappropriate system action. This arises because users frequently do not fully\nunderstand their own needs, while systems require precise intent definitions.\nCurrent LLM-based agents cannot effectively distinguish between linguistically\ncomplete and contextually triggerable expressions, lacking frameworks for\ncollaborative intent formation. We present STORM, a framework modeling\nasymmetric information dynamics through conversations between UserLLM (full\ninternal access) and AgentLLM (observable behavior only). STORM produces\nannotated corpora capturing expression trajectories and latent cognitive\ntransitions, enabling systematic analysis of collaborative understanding\ndevelopment. Our contributions include: (1) formalizing asymmetric information\nprocessing in dialogue systems; (2) modeling intent formation tracking\ncollaborative understanding evolution; and (3) evaluation metrics measuring\ninternal cognitive improvements alongside task performance. Experiments across\nfour language models reveal that moderate uncertainty (40-60%) can outperform\ncomplete transparency in certain scenarios, with model-specific patterns\nsuggesting reconsideration of optimal information completeness in human-AI\ncollaboration. These findings contribute to understanding asymmetric reasoning\ndynamics and inform uncertainty-calibrated dialogue system design."}
{"id": "2506.01902", "pdf": "https://arxiv.org/pdf/2506.01902.pdf", "abs": "https://arxiv.org/abs/2506.01902", "title": "Enhancing Biomedical Multi-modal Representation Learning with Multi-scale Pre-training and Perturbed Report Discrimination", "authors": ["Xinliu Zhong", "Kayhan Batmanghelich", "Li Sun"], "categories": ["cs.CV", "cs.CL"], "comment": "6 pages, 1 figure, accepted by 2024 IEEE Conference on Artificial\n  Intelligence (CAI)", "summary": "Vision-language models pre-trained on large scale of unlabeled biomedical\nimages and associated reports learn generalizable semantic representations.\nThese multi-modal representations can benefit various downstream tasks in the\nbiomedical domain. Contrastive learning is widely used to pre-train\nvision-language models for general natural images and associated captions.\nDespite its popularity, we found biomedical texts have complex and\ndomain-specific semantics that are often neglected by common contrastive\nmethods. To address this issue, we propose a novel method, perturbed report\ndiscrimination, for pre-train biomedical vision-language models. First, we\ncurate a set of text perturbation methods that keep the same words, but disrupt\nthe semantic structure of the sentence. Next, we apply different types of\nperturbation to reports, and use the model to distinguish the original report\nfrom the perturbed ones given the associated image. Parallel to this, we\nenhance the sensitivity of our method to higher level of granularity for both\nmodalities by contrasting attention-weighted image sub-regions and sub-words in\nthe image-text pairs. We conduct extensive experiments on multiple downstream\ntasks, and our method outperforms strong baseline methods. The results\ndemonstrate that our approach learns more semantic meaningful and robust\nmulti-modal representations."}
{"id": "2506.01926", "pdf": "https://arxiv.org/pdf/2506.01926.pdf", "abs": "https://arxiv.org/abs/2506.01926", "title": "Large language models can learn and generalize steganographic chain-of-thought under process supervision", "authors": ["Joey Skaf", "Luis Ibanez-Lissen", "Robert McCarthy", "Connor Watts", "Vasil Georgiv", "Hannes Whittingham", "Lorena Gonzalez-Manzano", "David Lindner", "Cameron Tice", "Edward James Young", "Puria Radmard"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "10 pages main text, 3 figures main text, 15 pages supplementary\n  material, 1 figure supplementary material, submitted to NeurIPS 2025", "summary": "Chain-of-thought (CoT) reasoning not only enhances large language model\nperformance but also provides critical insights into decision-making processes,\nmarking it as a useful tool for monitoring model intent and planning. By\nproactively preventing models from acting on CoT indicating misaligned or\nharmful intent, CoT monitoring can be used to reduce risks associated with\ndeploying models. However, developers may be incentivized to train away the\nappearance of harmful intent from CoT traces, by either customer preferences or\nregulatory requirements. Recent works have shown that banning mention of a\nspecific example of reward hacking, which may be done either to make CoT\npresentable to users or as a naive attempt to prevent the behavior, causes\nobfuscation of the undesired reasoning traces but the persistence of the\nundesired behavior. Such obfuscation threatens the reliability of CoT\nmonitoring. However, obfuscation of reasoning can be due to its internalization\nto latent space computation, or its encoding within the CoT. Here, we provide\nan extension to these results. First, we show that penalizing the use of\nspecific strings within load-bearing reasoning traces causes models to\nsubstitute alternative strings. Crucially, this does not alter the underlying\nmethod by which the model performs the task, demonstrating that the model can\nlearn to steganographically encode its reasoning. We further demonstrate that\nmodels can generalize an encoding scheme. When the penalized strings belong to\nan overarching class, the model learns not only to substitute strings seen in\ntraining, but also develops a general encoding scheme for all members of the\nclass which it can apply to held-out testing strings."}
{"id": "2506.01955", "pdf": "https://arxiv.org/pdf/2506.01955.pdf", "abs": "https://arxiv.org/abs/2506.01955", "title": "Dual-Process Image Generation", "authors": ["Grace Luo", "Jonathan Granskog", "Aleksander Holynski", "Trevor Darrell"], "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": null, "summary": "Prior methods for controlling image generation are limited in their ability\nto be taught new tasks. In contrast, vision-language models, or VLMs, can learn\ntasks in-context and produce the correct outputs for a given input. We propose\na dual-process distillation scheme that allows feed-forward image generators to\nlearn new tasks from deliberative VLMs. Our scheme uses a VLM to rate the\ngenerated images and backpropagates this gradient to update the weights of the\nimage generator. Our general framework enables a wide variety of new control\ntasks through the same text-and-image based interface. We showcase a handful of\napplications of this technique for different types of control signals, such as\ncommonsense inferences and visual prompts. With our method, users can implement\nmultimodal controls for properties such as color palette, line weight, horizon\nposition, and relative depth within a matter of minutes. Project page:\nhttps://dual-process.github.io."}
{"id": "2312.06562", "pdf": "https://arxiv.org/pdf/2312.06562.pdf", "abs": "https://arxiv.org/abs/2312.06562", "title": "On Meta-Prompting", "authors": ["Adrian de Wynter", "Xun Wang", "Qilong Gu", "Si-Qing Chen"], "categories": ["cs.CL", "cs.AI", "cs.LG", "math.CT"], "comment": null, "summary": "Modern large language models (LLMs) are capable of interpreting input strings\nas instructions, or prompts, and carry out tasks based on them. Unlike\ntraditional learners, LLMs cannot use back-propagation to obtain feedback, and\ncondition their output in situ in a phenomenon known as in-context learning\n(ICL). Many approaches to prompting and pre-training these models involve the\nautomated generation of these prompts, also known as meta-prompting, or\nprompting to obtain prompts. However, they do not formally describe the\nproperties and behavior of the LLMs themselves. We propose a theoretical\nframework based on category theory to generalize and describe ICL and LLM\nbehavior when interacting with users. Our framework allows us to obtain formal\nresults around task agnosticity and equivalence of various meta-prompting\napproaches. Using our framework and experimental results we argue that\nmeta-prompting is more effective than basic prompting at generating desirable\noutputs."}
{"id": "2312.17055", "pdf": "https://arxiv.org/pdf/2312.17055.pdf", "abs": "https://arxiv.org/abs/2312.17055", "title": "Beyond Output Matching: Bidirectional Alignment for Enhanced In-Context Learning", "authors": ["Chengwei Qin", "Wenhan Xia", "Fangkai Jiao", "Chen Chen", "Yuchen Hu", "Bosheng Ding", "Ruirui Chen", "Shafiq Joty"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown impressive few-shot generalization on\nmany tasks via in-context learning (ICL). Despite their success in showing such\nemergent abilities, the scale and complexity of larger models also lead to\nunprecedentedly high computational demands and deployment challenges. In\nreaction, researchers explore transferring the powerful capabilities of larger\nmodels to more efficient and compact models by typically aligning the output of\nsmaller (student) models with that of larger (teacher) models. Existing methods\neither train student models on the generated outputs of teacher models or\nimitate their token-level probability distributions. However, these\ndistillation methods pay little to no attention to the input, which also plays\na crucial role in ICL. Based on the finding that the performance of ICL is\nhighly sensitive to the selection of demonstration examples, we propose\nBidirectional Alignment (BiAlign) to fully leverage the models' preferences for\nICL examples to improve the ICL abilities of student models. Specifically, we\nintroduce the alignment of input preferences between student and teacher models\nby incorporating a novel ranking loss, in addition to aligning the token-level\noutput distribution. With extensive experiments and analysis, we demonstrate\nthat BiAlign can consistently outperform existing baselines on a variety of\ntasks involving language understanding, reasoning, and coding."}
{"id": "2401.16092", "pdf": "https://arxiv.org/pdf/2401.16092.pdf", "abs": "https://arxiv.org/abs/2401.16092", "title": "Multilingual Text-to-Image Generation Magnifies Gender Stereotypes and Prompt Engineering May Not Help You", "authors": ["Felix Friedrich", "Katharina H√§mmerl", "Patrick Schramowski", "Manuel Brack", "Jindrich Libovicky", "Kristian Kersting", "Alexander Fraser"], "categories": ["cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "Text-to-image generation models have recently achieved astonishing results in\nimage quality, flexibility, and text alignment, and are consequently employed\nin a fast-growing number of applications. Through improvements in multilingual\nabilities, a larger community now has access to this technology. However, our\nresults show that multilingual models suffer from significant gender biases\njust as monolingual models do. Furthermore, the natural expectation that\nmultilingual models will provide similar results across languages does not hold\nup. Instead, there are important differences between languages. We propose a\nnovel benchmark, MAGBIG, intended to foster research on gender bias in\nmultilingual models. We use MAGBIG to investigate the effect of multilingualism\non gender bias in T2I models. To this end, we construct multilingual prompts\nrequesting portraits of people with a certain occupation or trait. Our results\nshow that not only do models exhibit strong gender biases but they also behave\ndifferently across languages. Furthermore, we investigate prompt engineering\nstrategies, such as indirect, neutral formulations, to mitigate these biases.\nUnfortunately, these approaches have limited success and result in worse\ntext-to-image alignment. Consequently, we call for more research into diverse\nrepresentations across languages in image generators, as well as into\nsteerability to address biased model behavior."}
{"id": "2402.16837", "pdf": "https://arxiv.org/pdf/2402.16837.pdf", "abs": "https://arxiv.org/abs/2402.16837", "title": "Do Large Language Models Latently Perform Multi-Hop Reasoning?", "authors": ["Sohee Yang", "Elena Gribovskaya", "Nora Kassner", "Mor Geva", "Sebastian Riedel"], "categories": ["cs.CL"], "comment": "ACL 2024", "summary": "We study whether Large Language Models (LLMs) latently perform multi-hop\nreasoning with complex prompts such as \"The mother of the singer of\n'Superstition' is\". We look for evidence of a latent reasoning pathway where an\nLLM (1) latently identifies \"the singer of 'Superstition'\" as Stevie Wonder,\nthe bridge entity, and (2) uses its knowledge of Stevie Wonder's mother to\ncomplete the prompt. We analyze these two hops individually and consider their\nco-occurrence as indicative of latent multi-hop reasoning. For the first hop,\nwe test if changing the prompt to indirectly mention the bridge entity instead\nof any other entity increases the LLM's internal recall of the bridge entity.\nFor the second hop, we test if increasing this recall causes the LLM to better\nutilize what it knows about the bridge entity. We find strong evidence of\nlatent multi-hop reasoning for the prompts of certain relation types, with the\nreasoning pathway used in more than 80% of the prompts. However, the\nutilization is highly contextual, varying across different types of prompts.\nAlso, on average, the evidence for the second hop and the full multi-hop\ntraversal is rather moderate and only substantial for the first hop. Moreover,\nwe find a clear scaling trend with increasing model size for the first hop of\nreasoning but not for the second hop. Our experimental findings suggest\npotential challenges and opportunities for future development and applications\nof LLMs."}
{"id": "2404.10508", "pdf": "https://arxiv.org/pdf/2404.10508.pdf", "abs": "https://arxiv.org/abs/2404.10508", "title": "White Men Lead, Black Women Help? Benchmarking and Mitigating Language Agency Social Biases in LLMs", "authors": ["Yixin Wan", "Kai-Wei Chang"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Social biases can manifest in language agency. However, very limited research\nhas investigated such biases in Large Language Model (LLM)-generated content.\nIn addition, previous works often rely on string-matching techniques to\nidentify agentic and communal words within texts, falling short of accurately\nclassifying language agency. We introduce the Language Agency Bias Evaluation\n(LABE) benchmark, which comprehensively evaluates biases in LLMs by analyzing\nagency levels attributed to different demographic groups in model generations.\nLABE tests for gender, racial, and intersectional language agency biases in\nLLMs on 3 text generation tasks: biographies, professor reviews, and reference\nletters. Using LABE, we unveil language agency social biases in 3 recent LLMs:\nChatGPT, Llama3, and Mistral. We observe that: (1) LLM generations tend to\ndemonstrate greater gender bias than human-written texts; (2) Models\ndemonstrate remarkably higher levels of intersectional bias than the other bias\naspects. (3) Prompt-based mitigation is unstable and frequently leads to bias\nexacerbation. Based on our observations, we propose Mitigation via Selective\nRewrite (MSR), a novel bias mitigation strategy that leverages an agency\nclassifier to identify and selectively revise parts of generated texts that\ndemonstrate communal traits. Empirical results prove MSR to be more effective\nand reliable than prompt-based mitigation method, showing a promising research\ndirection."}
{"id": "2404.12728", "pdf": "https://arxiv.org/pdf/2404.12728.pdf", "abs": "https://arxiv.org/abs/2404.12728", "title": "Relevant or Random: Can LLMs Truly Perform Analogical Reasoning?", "authors": ["Chengwei Qin", "Wenhan Xia", "Tan Wang", "Fangkai Jiao", "Yuchen Hu", "Bosheng Ding", "Ruirui Chen", "Shafiq Joty"], "categories": ["cs.CL"], "comment": null, "summary": "Analogical reasoning is a unique ability of humans to address unfamiliar\nchallenges by transferring strategies from relevant past experiences. One key\nfinding in psychology is that compared with irrelevant past experiences,\nrecalling relevant ones can help humans better handle new tasks.\nCoincidentally, the NLP community has also recently found that self-generating\nrelevant examples in the context can help large language models (LLMs) better\nsolve a given problem than hand-crafted prompts. However, it is yet not clear\nwhether relevance is the key factor eliciting such capability, i.e., can LLMs\nbenefit more from self-generated relevant examples than irrelevant ones? In\nthis work, we systematically explore whether LLMs can truly perform analogical\nreasoning on a diverse set of reasoning tasks. With extensive experiments and\nanalysis, we show that self-generated random examples can surprisingly achieve\ncomparable or even better performance on certain tasks, e.g., 4% performance\nboost on GSM8K with random biological examples. We find that the accuracy of\nself-generated examples is the key factor and subsequently design two novel\nmethods with improved performance and significantly reduced inference costs.\nOverall, we aim to advance a deeper understanding of LLM analogical reasoning\nand hope this work stimulates further research in the design of self-generated\ncontexts."}
{"id": "2405.00557", "pdf": "https://arxiv.org/pdf/2405.00557.pdf", "abs": "https://arxiv.org/abs/2405.00557", "title": "Mixture of insighTful Experts (MoTE): The Synergy of Thought Chains and Expert Mixtures in Self-Alignment", "authors": ["Zhili Liu", "Yunhao Gou", "Kai Chen", "Lanqing Hong", "Jiahui Gao", "Fei Mi", "Yu Zhang", "Zhenguo Li", "Xin Jiang", "Qun Liu", "James T. Kwok"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As the capabilities of large language models (LLMs) continue to expand,\naligning these models with human values remains a significant challenge. Recent\nstudies show that reasoning abilities contribute significantly to model safety,\nwhile integrating Mixture-of-Experts (MoE) architectures can further enhance\nalignment. In this work, we address a fundamental question: How to effectively\nincorporate reasoning abilities and MoE architectures into self-alignment\nprocess in LLMs? We propose Mixture of insighTful Experts (MoTE), a novel\nframework that synergistically combines reasoning chains and expert mixtures to\nimprove self-alignments. From a data perspective, MoTE employs a structured\nreasoning chain comprising four key stages: Question Analysis, Answer Guidance,\nSafe Answer, and Safety Checking. This approach enhances safety through\nmulti-step reasoning and proves effective even for smaller and less powerful\nLLMs (e.g., 7B models). From an architectural perspective, MoTE adopts a\nmulti-LoRA framework with step-level routing, where each expert is dedicated to\na specific reasoning step. This design eliminates the need for balance losses,\nensures stable training, and supports adaptive inference lengths. Experimental\nresults demonstrate that MoTE significantly improves model safety, jailbreak\nresistance, and over-refusal capabilities, achieving performance comparable to\nOpenAI's state-of-the-art o1 model."}
{"id": "2405.03205", "pdf": "https://arxiv.org/pdf/2405.03205.pdf", "abs": "https://arxiv.org/abs/2405.03205", "title": "Anchored Answers: Unravelling Positional Bias in GPT-2's Multiple-Choice Questions", "authors": ["Ruizhe Li", "Yanjun Gao"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL 2025 Findings", "summary": "Large Language Models (LLMs), such as the GPT-4 and LLaMA families, have\ndemonstrated considerable success across diverse tasks, including\nmultiple-choice questions (MCQs). However, these models exhibit a positional\nbias, particularly an even worse anchored bias in the GPT-2 family, where they\nconsistently favour the first choice 'A' in MCQs during inference. This\nanchored bias challenges the integrity of GPT-2's decision-making process, as\nit skews performance based on the position rather than the content of the\nchoices in MCQs. In this study, we utilise the mechanistic interpretability\napproach to identify the internal modules within GPT-2 models responsible for\nthis bias. We focus on the Multi-Layer Perceptron (MLP) layers and attention\nheads, using the \"logit lens\" method to trace and modify the specific value\nvectors that contribute to the bias. By updating these vectors within MLP and\nrecalibrating attention patterns to neutralise the preference for the first\nchoice 'A', we effectively mitigate the anchored bias. Our interventions not\nonly mitigate the bias but also improve the overall MCQ prediction accuracy for\nthe GPT-2 family across various datasets. This work represents the first\ncomprehensive mechanistic analysis of anchored bias from the failing cases in\nMCQs within the GPT-2 models, introducing targeted, minimal-intervention\nstrategies that significantly enhance GPT2 model robustness and accuracy in\nMCQs. Our code is available at\nhttps://github.com/ruizheliUOA/Anchored_Bias_GPT2."}
{"id": "2405.11200", "pdf": "https://arxiv.org/pdf/2405.11200.pdf", "abs": "https://arxiv.org/abs/2405.11200", "title": "LexGen: Domain-aware Multilingual Lexicon Generation", "authors": ["Ayush Maheshwari", "Atul Kumar Singh", "Karthika NJ", "Krishnakant Bhatt", "Preethi Jyothi", "Ganesh Ramakrishnan"], "categories": ["cs.CL"], "comment": "ACL Main Conference, 2025", "summary": "Lexicon or dictionary generation across domains has the potential for\nsocietal impact, as it can potentially enhance information accessibility for a\ndiverse user base while preserving language identity. Prior work in the field\nprimarily focuses on bilingual lexical induction, which deals with word\nalignments using mapping or corpora-based approaches. However, these approaches\ndo not cater to domain-specific lexicon generation that consists of\ndomain-specific terminology. This task becomes particularly important in\nspecialized medical, engineering, and other technical domains, owing to the\nhighly infrequent usage of the terms and scarcity of data involving\ndomain-specific terms especially for low/mid-resource languages. In this paper,\nwe propose a new model to generate dictionary words for $6$ Indian languages in\nthe multi-domain setting. Our model consists of domain-specific and\ndomain-generic layers that encode information, and these layers are invoked via\na learnable routing technique. We also release a new benchmark dataset\nconsisting of >75K translation pairs across 6 Indian languages spanning 8\ndiverse domains.We conduct both zero-shot and few-shot experiments across\nmultiple domains to show the efficacy of our proposed model in generalizing to\nunseen domains and unseen languages. Additionally, we also perform a post-hoc\nhuman evaluation on unseen languages. The source code and dataset is present at\nhttps://github.com/Atulkmrsingh/lexgen."}
{"id": "2405.18915", "pdf": "https://arxiv.org/pdf/2405.18915.pdf", "abs": "https://arxiv.org/abs/2405.18915", "title": "Towards Better Chain-of-Thought: A Reflection on Effectiveness and Faithfulness", "authors": ["Jiachun Li", "Pengfei Cao", "Yubo Chen", "Jiexin Xu", "Huaijun Li", "Xiaojian Jiang", "Kang Liu", "Jun Zhao"], "categories": ["cs.CL", "cs.AI"], "comment": "18 pages, 21 figures, accepted by ACL 2025 Findings", "summary": "Chain-of-thought (CoT) prompting demonstrates varying performance under\ndifferent reasoning tasks. Previous work attempts to evaluate it but falls\nshort in providing an in-depth analysis of patterns that influence the CoT. In\nthis paper, we study the CoT performance from the perspective of effectiveness\nand faithfulness. For the former, we identify key factors that influence CoT\neffectiveness on performance improvement, including problem difficulty,\ninformation gain, and information flow. For the latter, we interpret the\nunfaithful CoT issue by conducting a joint analysis of the information\ninteraction among the question, CoT, and answer. The result demonstrates that,\nwhen the LLM predicts answers, it can recall correct information missing in the\nCoT from the question, leading to the problem. Finally, we propose a novel\nalgorithm to mitigate this issue, in which we recall extra information from the\nquestion to enhance the CoT generation and evaluate CoTs based on their\ninformation gain. Extensive experiments demonstrate that our approach enhances\nboth the faithfulness and effectiveness of CoT."}
{"id": "2406.02394", "pdf": "https://arxiv.org/pdf/2406.02394.pdf", "abs": "https://arxiv.org/abs/2406.02394", "title": "Pattern Recognition or Medical Knowledge? The Problem with Multiple-Choice Questions in Medicine", "authors": ["Maxime Griot", "Jean Vanderdonckt", "Demet Yuksel", "Coralie Hemptinne"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL 2025 main", "summary": "Large Language Models (LLMs) such as ChatGPT demonstrate significant\npotential in the medical domain and are often evaluated using multiple-choice\nquestions (MCQs) modeled on exams like the USMLE. However, such benchmarks may\noverestimate true clinical understanding by rewarding pattern recognition and\ntest-taking heuristics. To investigate this, we created a fictional medical\nbenchmark centered on an imaginary organ, the Glianorex, allowing us to\nseparate memorized knowledge from reasoning ability. We generated textbooks and\nMCQs in English and French using leading LLMs, then evaluated proprietary,\nopen-source, and domain-specific models in a zero-shot setting. Despite the\nfictional content, models achieved an average score of 64%, while physicians\nscored only 27%. Fine-tuned medical models outperformed base models in English\nbut not in French. Ablation and interpretability analyses revealed that models\nfrequently relied on shallow cues, test-taking strategies, and hallucinated\nreasoning to identify the correct choice. These results suggest that standard\nMCQ-based evaluations may not effectively measure clinical reasoning and\nhighlight the need for more robust, clinically meaningful assessment methods\nfor LLMs."}
{"id": "2406.06279", "pdf": "https://arxiv.org/pdf/2406.06279.pdf", "abs": "https://arxiv.org/abs/2406.06279", "title": "Multi-Prompting Decoder Helps Better Language Understanding", "authors": ["Zifeng Cheng", "Zhaoling Chen", "Zhiwei Jiang", "Yafeng Yin", "Cong Wang", "Shiping Ge", "Qing Gu"], "categories": ["cs.CL"], "comment": "Findings of ACL 2025", "summary": "Recent Pre-trained Language Models (PLMs) usually only provide users with the\ninference APIs, namely the emerging Model-as-a-Service (MaaS) setting. To adapt\nMaaS PLMs to downstream tasks without accessing their parameters and gradients,\nsome existing methods focus on the output-side adaptation of PLMs, viewing the\nPLM as an encoder and then optimizing a task-specific decoder for decoding the\noutput hidden states and class scores of the PLM. Despite the effectiveness of\nthese methods, they only use a single prompt to query PLMs for decoding,\nleading to a heavy reliance on the quality of the adopted prompt. In this\npaper, we propose a simple yet effective Multi-Prompting Decoder (MPD)\nframework for MaaS adaptation. The core idea is to query PLMs with multiple\ndifferent prompts for each sample, thereby obtaining multiple output hidden\nstates and class scores for subsequent decoding. Such multi-prompting decoding\nparadigm can simultaneously mitigate reliance on the quality of a single\nprompt, alleviate the issue of data scarcity under the few-shot setting, and\nprovide richer knowledge extracted from PLMs. Specifically, we propose two\ndecoding strategies: multi-prompting decoding with optimal transport for hidden\nstates and calibrated decoding for class scores. Extensive experiments\ndemonstrate that our method achieves new state-of-the-art results on multiple\nnatural language understanding datasets under the few-shot setting."}
{"id": "2406.11093", "pdf": "https://arxiv.org/pdf/2406.11093.pdf", "abs": "https://arxiv.org/abs/2406.11093", "title": "RAEmoLLM: Retrieval Augmented LLMs for Cross-Domain Misinformation Detection Using In-Context Learning Based on Emotional Information", "authors": ["Zhiwei Liu", "Kailai Yang", "Qianqian Xie", "Christine de Kock", "Sophia Ananiadou", "Eduard Hovy"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 (Main)", "summary": "Misinformation is prevalent in various fields such as education, politics,\nhealth, etc., causing significant harm to society. However, current methods for\ncross-domain misinformation detection rely on effort- and resource-intensive\nfine-tuning and complex model structures. With the outstanding performance of\nLLMs, many studies have employed them for misinformation detection.\nUnfortunately, they focus on in-domain tasks and do not incorporate significant\nsentiment and emotion features (which we jointly call {\\em affect}). In this\npaper, we propose RAEmoLLM, the first retrieval augmented (RAG) LLMs framework\nto address cross-domain misinformation detection using in-context learning\nbased on affective information. RAEmoLLM includes three modules. (1) In the\nindex construction module, we apply an emotional LLM to obtain affective\nembeddings from all domains to construct a retrieval database. (2) The\nretrieval module uses the database to recommend top K examples (text-label\npairs) from source domain data for target domain contents. (3) These examples\nare adopted as few-shot demonstrations for the inference module to process the\ntarget domain content. The RAEmoLLM can effectively enhance the general\nperformance of LLMs in cross-domain misinformation detection tasks through\naffect-based retrieval, without fine-tuning. We evaluate our framework on three\nmisinformation benchmarks. Results show that RAEmoLLM achieves significant\nimprovements compared to the other few-shot methods on three datasets, with the\nhighest increases of 15.64%, 31.18%, and 15.73% respectively. This project is\navailable at https://github.com/lzw108/RAEmoLLM."}
{"id": "2406.11753", "pdf": "https://arxiv.org/pdf/2406.11753.pdf", "abs": "https://arxiv.org/abs/2406.11753", "title": "A Semantic-Aware Layer-Freezing Approach to Computation-Efficient Fine-Tuning of Language Models", "authors": ["Jian Gu", "Aldeida Aleti", "Chunyang Chen", "Hongyu Zhang"], "categories": ["cs.CL", "cs.LG"], "comment": "accepted by ACL 2025, the camera-ready version", "summary": "Finetuning language models (LMs) is crucial for adapting the models to\ndownstream data and tasks. However, full finetuning is usually costly. Existing\nwork, such as parameter-efficient finetuning (PEFT), often focuses on\n\\textit{how to finetune} but neglects the issue of \\textit{where to finetune}.\nAs a pioneering work on reducing the cost of backpropagation (at the layer\nlevel) by answering where to finetune, we conduct a semantic analysis of the LM\ninference process. We first propose using transition traces of the latent\nrepresentation to compute deviations (or loss). Then, using a derived formula\nof scaling law, we estimate the gain of each layer in reducing deviation (or\nloss). Further, we narrow down the scope for finetuning, and also, study the\ncost-benefit balance of LM finetuning. We perform extensive experiments across\nwell-known LMs and datasets. The results show that our approach is effective\nand efficient, and outperforms the existing baselines. Our approach is\northogonal to other techniques for improving finetuning efficiency, such as\nPEFT methods, offering practical values on LM finetuning."}
{"id": "2406.17764", "pdf": "https://arxiv.org/pdf/2406.17764.pdf", "abs": "https://arxiv.org/abs/2406.17764", "title": "BMIKE-53: Investigating Cross-Lingual Knowledge Editing with In-Context Learning", "authors": ["Ercong Nie", "Bo Shao", "Zifeng Ding", "Mingyang Wang", "Helmut Schmid", "Hinrich Sch√ºtze"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025", "summary": "This paper introduces BMIKE-53, a comprehensive benchmark for cross-lingual\nin-context knowledge editing (IKE) across 53 languages, unifying three\nknowledge editing (KE) datasets: zsRE, CounterFact, and WikiFactDiff.\nCross-lingual KE, which requires knowledge edited in one language to generalize\nacross others while preserving unrelated knowledge, remains underexplored. To\naddress this gap, we systematically evaluate IKE under zero-shot, one-shot, and\nfew-shot setups, incorporating tailored metric-specific demonstrations. Our\nfindings reveal that model scale and demonstration alignment critically govern\ncross-lingual IKE efficacy, with larger models and tailored demonstrations\nsignificantly improving performance. Linguistic properties, particularly script\ntype, strongly influence performance variation across languages, with non-Latin\nlanguages underperforming due to issues like language confusion. Code and data\nare publicly available at: https://github.com/ercong21/MultiKnow/."}
{"id": "2406.18403", "pdf": "https://arxiv.org/pdf/2406.18403.pdf", "abs": "https://arxiv.org/abs/2406.18403", "title": "LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks", "authors": ["Anna Bavaresco", "Raffaella Bernardi", "Leonardo Bertolazzi", "Desmond Elliott", "Raquel Fern√°ndez", "Albert Gatt", "Esam Ghaleb", "Mario Giulianelli", "Michael Hanna", "Alexander Koller", "Andr√© F. T. Martins", "Philipp Mondorf", "Vera Neplenbroek", "Sandro Pezzelle", "Barbara Plank", "David Schlangen", "Alessandro Suglia", "Aditya K Surikuchi", "Ece Takmaz", "Alberto Testoni"], "categories": ["cs.CL"], "comment": "Accepted to the main conference of ACL 2025", "summary": "There is an increasing trend towards evaluating NLP models with LLMs instead\nof human judgments, raising questions about the validity of these evaluations,\nas well as their reproducibility in the case of proprietary models. We provide\nJUDGE-BENCH, an extensible collection of 20 NLP datasets with human annotations\ncovering a broad range of evaluated properties and types of data, and\ncomprehensively evaluate 11 current LLMs, covering both open-weight and\nproprietary models, for their ability to replicate the annotations. Our\nevaluations show substantial variance across models and datasets. Models are\nreliable evaluators on some tasks, but overall display substantial variability\ndepending on the property being evaluated, the expertise level of the human\njudges, and whether the language is human or model-generated. We conclude that\nLLMs should be carefully validated against human judgments before being used as\nevaluators."}
{"id": "2408.03505", "pdf": "https://arxiv.org/pdf/2408.03505.pdf", "abs": "https://arxiv.org/abs/2408.03505", "title": "Optimus: Accelerating Large-Scale Multi-Modal LLM Training by Bubble Exploitation", "authors": ["Weiqi Feng", "Yangrui Chen", "Shaoyu Wang", "Yanghua Peng", "Haibin Lin", "Minlan Yu"], "categories": ["cs.CL", "cs.AI", "cs.DC"], "comment": null, "summary": "Multimodal large language models (MLLMs) have extended the success of large\nlanguage models (LLMs) to multiple data types, such as image, text and audio,\nachieving significant performance in various domains, including multimodal\ntranslation, visual question answering and content generation. Nonetheless,\nexisting systems are inefficient to train MLLMs due to substantial GPU bubbles\ncaused by the heterogeneous modality models and complex data dependencies in 3D\nparallelism. This paper proposes Optimus, a distributed MLLM training system\nthat reduces end-to-end MLLM training time. Optimus is based on our principled\nanalysis that scheduling the encoder computation within the LLM bubbles can\nreduce bubbles in MLLM training. To make scheduling encoder computation\npossible for all GPUs, Optimus searches the separate parallel plans for encoder\nand LLM, and adopts a bubble scheduling algorithm to enable exploiting LLM\nbubbles without breaking the original data dependencies in the MLLM model\narchitecture. We further decompose encoder layer computation into a series of\nkernels, and analyze the common bubble pattern of 3D parallelism to carefully\noptimize the sub-millisecond bubble scheduling, minimizing the overall training\ntime. Our experiments in a production cluster show that Optimus accelerates\nMLLM training by 20.5%-21.3% with ViT-22B and GPT-175B model over 3072 GPUs\ncompared to baselines."}
{"id": "2408.13533", "pdf": "https://arxiv.org/pdf/2408.13533.pdf", "abs": "https://arxiv.org/abs/2408.13533", "title": "Pandora's Box or Aladdin's Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models", "authors": ["Jinyang Wu", "Shuai Zhang", "Feihu Che", "Mingkuan Feng", "Chuyuan Zhang", "Pengpeng Shao", "Jianhua Tao"], "categories": ["cs.CL"], "comment": "ACL 2025 Main", "summary": "Retrieval-Augmented Generation (RAG) has emerged as a crucial method for\naddressing hallucinations in large language models (LLMs). While recent\nresearch has extended RAG models to complex noisy scenarios, these explorations\noften confine themselves to limited noise types and presuppose that noise is\ninherently detrimental to LLMs, potentially deviating from real-world retrieval\nenvironments and restricting practical applicability. In this paper, we define\nseven distinct noise types from a linguistic perspective and establish a Noise\nRAG Benchmark (NoiserBench), a comprehensive evaluation framework encompassing\nmultiple datasets and reasoning tasks. Through empirical evaluation of eight\nrepresentative LLMs with diverse architectures and scales, we reveal that these\nnoises can be further categorized into two practical groups: noise that is\nbeneficial to LLMs (aka beneficial noise) and noise that is harmful to LLMs\n(aka harmful noise). While harmful noise generally impairs performance,\nbeneficial noise may enhance several aspects of model capabilities and overall\nperformance. Our analysis offers insights for developing more robust, adaptable\nRAG solutions and mitigating hallucinations across diverse retrieval scenarios.\nCode is available at https://github.com/jinyangwu/NoiserBench."}
{"id": "2408.15409", "pdf": "https://arxiv.org/pdf/2408.15409.pdf", "abs": "https://arxiv.org/abs/2408.15409", "title": "Awes, Laws, and Flaws From Today's LLM Research", "authors": ["Adrian de Wynter"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Findings)", "summary": "We perform a critical examination of the scientific methodology behind\ncontemporary large language model (LLM) research. For this we assess over 2,000\nresearch works released between 2020 and 2024 based on criteria typical of what\nis considered good research (e.g. presence of statistical tests and\nreproducibility), and cross-validate it with arguments that are at the centre\nof controversy (e.g., claims of emergent behaviour). We find multiple trends,\nsuch as declines in ethics disclaimers, a rise of LLMs as evaluators, and an\nincrease on claims of LLM reasoning abilities without leveraging human\nevaluation. We note that conference checklists are effective at curtailing some\nof these issues, but balancing velocity and rigour in research cannot solely\nrely on these. We tie all these findings to findings from recent meta-reviews\nand extend recommendations on how to address what does, does not, and should\nwork in LLM research."}
{"id": "2408.16493", "pdf": "https://arxiv.org/pdf/2408.16493.pdf", "abs": "https://arxiv.org/abs/2408.16493", "title": "Learning from Negative Samples in Generative Biomedical Entity Linking", "authors": ["Chanhwi Kim", "Hyunjae Kim", "Sihyeon Park", "Jiwoo Lee", "Mujeen Sung", "Jaewoo Kang"], "categories": ["cs.CL"], "comment": "ACL 2025 (Findings)", "summary": "Generative models have become widely used in biomedical entity linking\n(BioEL) due to their excellent performance and efficient memory usage. However,\nthese models are usually trained only with positive samples, i.e., entities\nthat match the input mention's identifier, and do not explicitly learn from\nhard negative samples, which are entities that look similar but have different\nmeanings. To address this limitation, we introduce ANGEL (Learning from\nNegative Samples in Biomedical Generative Entity Linking), the first framework\nthat trains generative BioEL models using negative samples. Specifically, a\ngenerative model is initially trained to generate positive entity names from\nthe knowledge base for given input entities. Subsequently, both correct and\nincorrect outputs are gathered from the model's top-k predictions. Finally, the\nmodel is updated to prioritize the correct predictions through preference\noptimization. Our models outperform the previous best baseline models by up to\nan average top-1 accuracy of 1.4% on five benchmarks. When incorporating our\nframework into pre-training, the performance improvement increases further to\n1.7%, demonstrating its effectiveness in both the pre-training and fine-tuning\nstages. The code and model weights are available at\nhttps://github.com/dmis-lab/ANGEL."}
{"id": "2409.00113", "pdf": "https://arxiv.org/pdf/2409.00113.pdf", "abs": "https://arxiv.org/abs/2409.00113", "title": "Wait, that's not an option: LLMs Robustness with Incorrect Multiple-Choice Options", "authors": ["Gracjan G√≥ral", "Emilia Wi≈õnios", "Piotr Sankowski", "Pawe≈Ç Budzianowski"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted for ACL 2025 Main Conference and NeurIPS 2024 FM-EduAssess\n  Workshop", "summary": "This work introduces a novel framework for evaluating LLMs' capacity to\nbalance instruction-following with critical reasoning when presented with\nmultiple-choice questions containing no valid answers. Through systematic\nevaluation across arithmetic, domain-specific knowledge, and high-stakes\nmedical decision tasks, we demonstrate that post-training aligned models often\ndefault to selecting invalid options, while base models exhibit improved\nrefusal capabilities that scale with model size. Our analysis reveals that\nalignment techniques, though intended to enhance helpfulness, can inadvertently\nimpair models' reflective judgment--the ability to override default behaviors\nwhen faced with invalid options. We additionally conduct a parallel human study\nshowing similar instruction-following biases, with implications for how these\nbiases may propagate through human feedback datasets used in alignment. We\nprovide extensive ablation studies examining the impact of model size, training\ntechniques, and prompt engineering. Our findings highlight fundamental tensions\nbetween alignment optimization and preservation of critical reasoning\ncapabilities, with important implications for developing more robust AI systems\nfor real-world deployment."}
{"id": "2409.05367", "pdf": "https://arxiv.org/pdf/2409.05367.pdf", "abs": "https://arxiv.org/abs/2409.05367", "title": "STRICTA: Structured Reasoning in Critical Text Assessment for Peer Review and Beyond", "authors": ["Nils Dycke", "Matej Zeƒçeviƒá", "Ilia Kuznetsov", "Beatrix Suess", "Kristian Kersting", "Iryna Gurevych"], "categories": ["cs.CL"], "comment": "Accepted at ACL 2025", "summary": "Critical text assessment is at the core of many expert activities, such as\nfact-checking, peer review, and essay grading. Yet, existing work treats\ncritical text assessment as a black box problem, limiting interpretability and\nhuman-AI collaboration. To close this gap, we introduce Structured Reasoning In\nCritical Text Assessment (STRICTA), a novel specification framework to model\ntext assessment as an explicit, step-wise reasoning process. STRICTA breaks\ndown the assessment into a graph of interconnected reasoning steps drawing on\ncausality theory (Pearl, 1995). This graph is populated based on expert\ninteraction data and used to study the assessment process and facilitate\nhuman-AI collaboration. We formally define STRICTA and apply it in a study on\nbiomedical paper assessment, resulting in a dataset of over 4000 reasoning\nsteps from roughly 40 biomedical experts on more than 20 papers. We use this\ndataset to empirically study expert reasoning in critical text assessment, and\ninvestigate if LLMs are able to imitate and support experts within these\nworkflows. The resulting tools and datasets pave the way for studying\ncollaborative expert-AI reasoning in text assessment, in peer review and\nbeyond."}
{"id": "2409.05806", "pdf": "https://arxiv.org/pdf/2409.05806.pdf", "abs": "https://arxiv.org/abs/2409.05806", "title": "CKnowEdit: A New Chinese Knowledge Editing Dataset for Linguistics, Facts, and Logic Error Correction in LLMs", "authors": ["Jizhan Fang", "Tianhe Lu", "Yunzhi Yao", "Ziyan Jiang", "Xin Xu", "Huajun Chen", "Ningyu Zhang"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": "ACL 2025; project website is available at\n  https://zjunlp.github.io/project/CKnowEdit code and dataset are available at\n  https://github.com/zjunlp/EasyEdit", "summary": "Chinese, as a linguistic system rich in depth and complexity, is\ncharacterized by distinctive elements such as ancient poetry, proverbs, idioms,\nand other cultural constructs. However, current Large Language Models (LLMs)\nface limitations in these specialized domains, highlighting the need for the\ndevelopment of comprehensive datasets that can assess, continuously update, and\nprogressively improve these culturally-grounded linguistic competencies through\ntargeted training optimizations. To address this gap, we introduce CKnowEdit,\nthe first-ever Chinese knowledge editing dataset designed to correct\nlinguistic, factual, and logical errors in LLMs. We collect seven types of\nknowledge from a wide range of sources, including classical texts, idioms, and\ncontent from Baidu Tieba Ruozhiba, taking into account the unique polyphony,\nantithesis, and logical structures inherent in the Chinese language. By\nanalyzing this dataset, we highlight the challenges current LLMs face in\nmastering Chinese. Furthermore, our evaluation of state-of-the-art knowledge\nediting techniques reveals opportunities to advance the correction of Chinese\nknowledge. Code and dataset are available at\nhttps://github.com/zjunlp/EasyEdit."}
{"id": "2409.09401", "pdf": "https://arxiv.org/pdf/2409.09401.pdf", "abs": "https://arxiv.org/abs/2409.09401", "title": "Towards Diverse and Efficient Audio Captioning via Diffusion Models", "authors": ["Manjie Xu", "Chenxing Li", "Xinyi Tu", "Yong Ren", "Ruibo Fu", "Wei Liang", "Dong Yu"], "categories": ["cs.CL"], "comment": "https://sites.google.com/view/diffusion-audio-captioning", "summary": "We introduce Diffusion-based Audio Captioning (DAC), a non-autoregressive\ndiffusion model tailored for diverse and efficient audio captioning. Although\nexisting captioning models relying on language backbones have achieved\nremarkable success in various captioning tasks, their insufficient performance\nin terms of generation speed and diversity impede progress in audio\nunderstanding and multimedia applications. Our diffusion-based framework offers\nunique advantages stemming from its inherent stochasticity and holistic context\nmodeling in captioning. Through rigorous evaluation, we demonstrate that DAC\nnot only achieves SOTA performance levels compared to existing benchmarks in\nthe caption quality, but also significantly outperforms them in terms of\ngeneration speed and diversity. The success of DAC illustrates that text\ngeneration can also be seamlessly integrated with audio and visual generation\ntasks using a diffusion backbone, paving the way for a unified, audio-related\ngenerative model across different modalities."}
{"id": "2409.14507", "pdf": "https://arxiv.org/pdf/2409.14507.pdf", "abs": "https://arxiv.org/abs/2409.14507", "title": "A is for Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders", "authors": ["David Chanin", "James Wilken-Smith", "Tom√°≈° Dulka", "Hardik Bhatnagar", "Satvik Golechha", "Joseph Bloom"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Sparse Autoencoders (SAEs) aim to decompose the activation space of large\nlanguage models (LLMs) into human-interpretable latent directions or features.\nAs we increase the number of features in the SAE, hierarchical features tend to\nsplit into finer features (\"math\" may split into \"algebra\", \"geometry\", etc.),\na phenomenon referred to as feature splitting. However, we show that sparse\ndecomposition and splitting of hierarchical features is not robust.\nSpecifically, we show that seemingly monosemantic features fail to fire where\nthey should, and instead get \"absorbed\" into their children features. We coin\nthis phenomenon feature absorption, and show that it is caused by optimizing\nfor sparsity in SAEs whenever the underlying features form a hierarchy. We\nintroduce a metric to detect absorption in SAEs, and validate our findings\nempirically on hundreds of LLM SAEs. Our investigation suggests that varying\nSAE sizes or sparsity is insufficient to solve this issue. We discuss the\nimplications of feature absorption in SAEs and some potential approaches to\nsolve the fundamental theoretical issues before SAEs can be used for\ninterpreting LLMs robustly and at scale."}
{"id": "2409.19458", "pdf": "https://arxiv.org/pdf/2409.19458.pdf", "abs": "https://arxiv.org/abs/2409.19458", "title": "Scalable Fine-tuning from Multiple Data Sources: A First-Order Approximation Approach", "authors": ["Dongyue Li", "Ziniu Zhang", "Lu Wang", "Hongyang R. Zhang"], "categories": ["cs.CL", "cs.LG"], "comment": "17 pages. Appeared in Findings of EMNLP'24", "summary": "We study the problem of fine-tuning a language model (LM) for a target task\nby optimally using the information from $n$ auxiliary tasks. This problem has\nbroad applications in NLP, such as targeted instruction tuning and data\nselection in chain-of-thought fine-tuning. The key challenge of this problem is\nthat not all auxiliary tasks are beneficial in improving the performance of the\ntarget task. Thus, selecting the right subset of auxiliary tasks is crucial.\nConventional subset selection methods, such as forward and backward stepwise\nselection, are unsuitable for LM fine-tuning because they require repeated\ntraining on subsets of auxiliary tasks. This paper introduces a new algorithm\nfor estimating model fine-tuning performance without requiring repeated\ntraining. Our algorithm first performs multitask training using data from all\ntasks to obtain a meta initialization. Then, we approximate the model\nfine-tuning loss of a subset using functional values and gradients from the\nmeta initialization. Empirically, we find that this gradient-based\napproximation holds with remarkable accuracy for twelve transformer-based LMs.\nThus, we can now estimate fine-tuning performances on CPUs within a few\nseconds. Finally, we fine-tune the pretrained base model once on the selected\nsubset of tasks. We conduct extensive experiments to validate this approach,\ndelivering a speedup of $30\\times$ over conventional subset selection while\nincurring only $1\\%$ error of the true fine-tuning performances. In downstream\nevaluations involving both instruction tuning and chain-of-thought fine-tuning,\nthis loss-based selection approach improves over prior gradient or\nrepresentation similarity-based methods for subset selection by up to $3.8\\%$."}
{"id": "2409.19505", "pdf": "https://arxiv.org/pdf/2409.19505.pdf", "abs": "https://arxiv.org/abs/2409.19505", "title": "The Nature of NLP: Analyzing Contributions in NLP Papers", "authors": ["Aniket Pramanick", "Yufang Hou", "Saif M. Mohammad", "Iryna Gurevych"], "categories": ["cs.CL"], "comment": "accepted at ACL 2025", "summary": "Natural Language Processing (NLP) is an established and dynamic field.\nDespite this, what constitutes NLP research remains debated. In this work, we\naddress the question by quantitatively examining NLP research papers. We\npropose a taxonomy of research contributions and introduce NLPContributions, a\ndataset of nearly $2k$ NLP research paper abstracts, carefully annotated to\nidentify scientific contributions and classify their types according to this\ntaxonomy. We also introduce a novel task of automatically identifying\ncontribution statements and classifying their types from research papers. We\npresent experimental results for this task and apply our model to $\\sim$$29k$\nNLP research papers to analyze their contributions, aiding in the understanding\nof the nature of NLP research. We show that NLP research has taken a winding\npath -- with the focus on language and human-centric studies being prominent in\nthe 1970s and 80s, tapering off in the 1990s and 2000s, and starting to rise\nagain since the late 2010s. Alongside this revival, we observe a steady rise in\ndataset and methodological contributions since the 1990s, such that today, on\naverage, individual NLP papers contribute in more ways than ever before. Our\ndataset and analyses offer a powerful lens for tracing research trends and\noffer potential for generating informed, data-driven literature surveys."}
{"id": "2409.20201", "pdf": "https://arxiv.org/pdf/2409.20201.pdf", "abs": "https://arxiv.org/abs/2409.20201", "title": "AfriHuBERT: A self-supervised speech representation model for African languages", "authors": ["Jesujoba O. Alabi", "Xuechen Liu", "Dietrich Klakow", "Junichi Yamagishi"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Interspeech 2025", "summary": "In this work, we present AfriHuBERT, an extension of mHuBERT-147, a compact\nself-supervised learning (SSL) model pretrained on 147 languages. While\nmHuBERT-147 covered 16 African languages, we expand this to 1,226 through\ncontinued pretraining on 10K+ hours of speech data from diverse sources,\nbenefiting an African population of over 600M. We evaluate AfriHuBERT on two\nkey speech tasks, Spoken Language Identification (SLID) and Automatic Speech\nRecognition (ASR), using the FLEURS benchmark. Our results show a +3.6% F1\nscore improvement for SLID and a -2.1% average Word Error Rate (WER) reduction\nfor ASR over mHuBERT-147, and demonstrates competitiveness with larger SSL\nmodels such as MMS and XEUS. Further analysis shows that ASR models trained on\nAfriHuBERT exhibit improved cross-corpus generalization and are competitive in\nextremely low-resource ASR scenarios."}
{"id": "2410.03026", "pdf": "https://arxiv.org/pdf/2410.03026.pdf", "abs": "https://arxiv.org/abs/2410.03026", "title": "Estimating Privacy Leakage of Augmented Contextual Knowledge in Language Models", "authors": ["James Flemings", "Bo Jiang", "Wanrong Zhang", "Zafar Takhirov", "Murali Annavaram"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Language models (LMs) rely on their parametric knowledge augmented with\nrelevant contextual knowledge for certain tasks, such as question answering.\nHowever, the contextual knowledge can contain private information that may be\nleaked when answering queries, and estimating this privacy leakage is not well\nunderstood. A straightforward approach of directly comparing an LM's output to\nthe contexts can overestimate the privacy risk, since the LM's parametric\nknowledge might already contain the augmented contextual knowledge. To this\nend, we introduce $\\emph{context influence}$, a metric that builds on\ndifferential privacy, a widely-adopted privacy notion, to estimate the privacy\nleakage of contextual knowledge during decoding. Our approach effectively\nmeasures how each subset of the context influences an LM's response while\nseparating the specific parametric knowledge of the LM. Using our context\ninfluence metric, we demonstrate that context privacy leakage occurs when\ncontextual knowledge is out of distribution with respect to parametric\nknowledge. Moreover, we experimentally demonstrate how context influence\nproperly attributes the privacy leakage to augmented contexts, and we evaluate\nhow factors-- such as model size, context size, generation position, etc.--\naffect context privacy leakage. The practical implications of our results will\ninform practitioners of the privacy risk associated with augmented contextual\nknowledge."}
{"id": "2410.03868", "pdf": "https://arxiv.org/pdf/2410.03868.pdf", "abs": "https://arxiv.org/abs/2410.03868", "title": "Can Language Models Reason about Individualistic Human Values and Preferences?", "authors": ["Liwei Jiang", "Taylor Sorensen", "Sydney Levine", "Yejin Choi"], "categories": ["cs.CL"], "comment": "Camera Ready at ACL Main 2025", "summary": "Recent calls for pluralistic alignment emphasize that AI systems should\naddress the diverse needs of all people. Yet, efforts in this space often\nrequire sorting people into fixed buckets of pre-specified diversity-defining\ndimensions (e.g., demographics), risking smoothing out individualistic\nvariations or even stereotyping. To achieve an authentic representation of\ndiversity that respects individuality, we propose individualistic alignment.\nWhile individualistic alignment can take various forms, we introduce\nIndieValueCatalog, a dataset transformed from the influential World Values\nSurvey (WVS), to study language models (LMs) on the specific challenge of\nindividualistic value reasoning. Given a sample of an individual's\nvalue-expressing statements, models are tasked with predicting this person's\nvalue judgments in novel cases. With IndieValueCatalog, we reveal critical\nlimitations in frontier LMs, which achieve only 55 % to 65% accuracy in\npredicting individualistic values. Moreover, our results highlight that a\nprecise description of individualistic values cannot be approximated only with\ndemographic information. We also identify a partiality of LMs in reasoning\nabout global individualistic values, as measured by our proposed Value Inequity\nIndex ({\\sigma}Inequity). Finally, we train a series of IndieValueReasoners to\nreveal new patterns and dynamics into global human values."}
{"id": "2410.05613", "pdf": "https://arxiv.org/pdf/2410.05613.pdf", "abs": "https://arxiv.org/abs/2410.05613", "title": "Stereotype or Personalization? User Identity Biases Chatbot Recommendations", "authors": ["Anjali Kantharuban", "Jeremiah Milbauer", "Maarten Sap", "Emma Strubell", "Graham Neubig"], "categories": ["cs.CL"], "comment": null, "summary": "While personalized recommendations are often desired by users, it can be\ndifficult in practice to distinguish cases of bias from cases of\npersonalization: we find that models generate racially stereotypical\nrecommendations regardless of whether the user revealed their identity\nintentionally through explicit indications or unintentionally through implicit\ncues. We demonstrate that when people use large language models (LLMs) to\ngenerate recommendations, the LLMs produce responses that reflect both what the\nuser wants and who the user is. We argue that chatbots ought to transparently\nindicate when recommendations are influenced by a user's revealed identity\ncharacteristics, but observe that they currently fail to do so. Our experiments\nshow that even though a user's revealed identity significantly influences model\nrecommendations (p < 0.001), model responses obfuscate this fact in response to\nuser queries. This bias and lack of transparency occurs consistently across\nmultiple popular consumer LLMs and for four American racial groups."}
{"id": "2410.05873", "pdf": "https://arxiv.org/pdf/2410.05873.pdf", "abs": "https://arxiv.org/abs/2410.05873", "title": "MEXA: Multilingual Evaluation of English-Centric LLMs via Cross-Lingual Alignment", "authors": ["Amir Hossein Kargaran", "Ali Modarressi", "Nafiseh Nikeghbal", "Jana Diesner", "Fran√ßois Yvon", "Hinrich Sch√ºtze"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL Findings 2025", "summary": "English-centric large language models (LLMs) often show strong multilingual\ncapabilities. However, their multilingual performance remains unclear and is\nunder-evaluated for many other languages. Most benchmarks for multilinguality\nfocus on classic NLP tasks or cover a minimal number of languages. We introduce\nMEXA, a method for assessing the multilingual capabilities of pre-trained\nEnglish-centric LLMs using parallel sentences, which are available for more\nlanguages than existing downstream tasks. MEXA leverages that English-centric\nLLMs use English as a pivot language in their intermediate layers. MEXA\ncomputes the alignment between English and non-English languages using parallel\nsentences to evaluate the transfer of language understanding from English to\nother languages. This alignment can be used to estimate model performance in\ndifferent languages. We conduct controlled experiments using various parallel\ndatasets (FLORES-200 and Bible), models (Llama family, Gemma family, Mistral,\nand OLMo), and established downstream tasks (Belebele, m-MMLU, and m-ARC). We\nexplore different methods to compute embeddings in decoder-only models. Our\nresults show that MEXA, in its default settings, achieves an average Pearson\ncorrelation of 0.90 between its predicted scores and actual task performance\nacross languages. This suggests that MEXA is a reliable method for estimating\nthe multilingual capabilities of English-centric LLMs, providing a clearer\nunderstanding of their multilingual potential and the inner workings of LLMs.\nLeaderboard: https://cis-lmu-mexa.hf.space, Code:\nhttps://github.com/cisnlp/MEXA."}
{"id": "2410.06118", "pdf": "https://arxiv.org/pdf/2410.06118.pdf", "abs": "https://arxiv.org/abs/2410.06118", "title": "Optimizing the Training Schedule of Multilingual NMT using Reinforcement Learning", "authors": ["Alexis Allemann", "√Älex R. Atrio", "Andrei Popescu-Belis"], "categories": ["cs.CL"], "comment": "Proceedings of MT Summit 2025", "summary": "Multilingual NMT is a viable solution for translating low-resource languages\n(LRLs) when data from high-resource languages (HRLs) from the same language\nfamily is available. However, the training schedule, i.e. the order of\npresentation of languages, has an impact on the quality of such systems. Here,\nin a many-to-one translation setting, we propose to apply two algorithms that\nuse reinforcement learning to optimize the training schedule of NMT: (1)\nTeacher-Student Curriculum Learning and (2) Deep Q Network. The former uses an\nexponentially smoothed estimate of the returns of each action based on the loss\non monolingual or multilingual development subsets, while the latter estimates\nrewards using an additional neural network trained from the history of actions\nselected in different states of the system, together with the rewards received.\nOn a 8-to-1 translation dataset with LRLs and HRLs, our second method improves\nBLEU and COMET scores with respect to both random selection of monolingual\nbatches and shuffled multilingual batches, by adjusting the number of\npresentations of LRL vs. HRL batches."}
{"id": "2410.07176", "pdf": "https://arxiv.org/pdf/2410.07176.pdf", "abs": "https://arxiv.org/abs/2410.07176", "title": "Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models", "authors": ["Fei Wang", "Xingchen Wan", "Ruoxi Sun", "Jiefeng Chen", "Sercan √ñ. Arƒ±k"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL 2025 main conference", "summary": "Retrieval augmented generation (RAG), while effectively integrating external\nknowledge to address the inherent limitations of large language models (LLMs),\ncan be hindered by imperfect retrieval that contain irrelevant, misleading, or\neven malicious information. Previous studies have rarely connected the behavior\nof RAG through joint analysis, particularly regarding error propagation coming\nfrom imperfect retrieval and potential conflicts between LLMs' internal\nknowledge and external sources. Through comprehensive and controlled analyses\nunder realistic conditions, we find that imperfect retrieval augmentation is\ninevitable, common, and harmful. We identify the knowledge conflicts between\nLLM-internal and external knowledge from retrieval as a bottleneck to overcome\nimperfect retrieval in the post-retrieval stage of RAG. To address this, we\npropose Astute RAG, a novel RAG approach designed to be resilient to imperfect\nretrieval augmentation. It adaptively elicits essential information from LLMs'\ninternal knowledge, iteratively consolidates internal and external knowledge\nwith source-awareness, and finalizes the answer according to information\nreliability. Our experiments with Gemini and Claude demonstrate the superior\nperformance of Astute RAG compared to previous robustness-enhanced RAG\napproaches. Specifically, Astute RAG is the only RAG method that achieves\nperformance comparable to or even surpassing conventional use of LLMs under the\nworst-case scenario. Further analysis reveals the effectiveness of Astute RAG\nin resolving knowledge conflicts, thereby improving the trustworthiness of RAG."}
{"id": "2410.08145", "pdf": "https://arxiv.org/pdf/2410.08145.pdf", "abs": "https://arxiv.org/abs/2410.08145", "title": "Insight Over Sight: Exploring the Vision-Knowledge Conflicts in Multimodal LLMs", "authors": ["Xiaoyuan Liu", "Wenxuan Wang", "Youliang Yuan", "Jen-tse Huang", "Qiuzhi Liu", "Pinjia He", "Zhaopeng Tu"], "categories": ["cs.CL", "cs.CV"], "comment": "Accepted by ACL 2025 main", "summary": "This paper explores the problem of commonsense level vision-knowledge\nconflict in Multimodal Large Language Models (MLLMs), where visual information\ncontradicts model's internal commonsense knowledge. To study this issue, we\nintroduce an automated framework, augmented with human-in-the-loop quality\ncontrol, to generate inputs designed to simulate and evaluate these conflicts\nin MLLMs. Using this framework, we have crafted a diagnostic benchmark\nconsisting of 374 original images and 1,122 high-quality question-answer (QA)\npairs. The benchmark covers two aspects of conflict and three question types,\nproviding a thorough assessment tool. We apply this benchmark to assess the\nconflict-resolution capabilities of nine representative MLLMs from various\nmodel families. Our results indicate an evident over-reliance on parametric\nknowledge for approximately 20% of all queries, especially among Yes-No and\naction-related problems. Based on these findings, we evaluate the effectiveness\nof existing approaches to mitigating the conflicts and compare them to our\n\"Focus-on-Vision\" prompting strategy. Despite some improvement, the\nvision-knowledge conflict remains unresolved and can be further scaled through\nour data construction framework. Our proposed framework, benchmark, and\nanalysis contribute to the understanding and mitigation of vision-knowledge\nconflicts in MLLMs."}
{"id": "2410.10075", "pdf": "https://arxiv.org/pdf/2410.10075.pdf", "abs": "https://arxiv.org/abs/2410.10075", "title": "RoCoFT: Efficient Finetuning of Large Language Models with Row-Column Updates", "authors": ["Md Kowsher", "Tara Esmaeilbeig", "Chun-Nam Yu", "Chen Chen", "Mojtaba Soltanalian", "Niloofar Yousefi"], "categories": ["cs.CL"], "comment": "RoCoFT is a parameter-efficient method", "summary": "We propose RoCoFT, a parameter-efficient fine-tuning method for large-scale\nlanguage models (LMs) based on updating only a few rows and columns of the\nweight matrices in transformers. Through extensive experiments with medium-size\nLMs like BERT and RoBERTa, and larger LMs like Bloom-7B, Llama2-7B, and\nLlama2-13B, we show that our method gives comparable or better accuracies than\nstate-of-art PEFT methods while also being more memory and\ncomputation-efficient. We also study the reason behind the effectiveness of our\nmethod with tools from neural tangent kernel theory. We empirically demonstrate\nthat our kernel, constructed using a restricted set of row and column\nparameters, are numerically close to the full-parameter kernel and gives\ncomparable classification performance. Ablation studies are conducted to\ninvestigate the impact of different algorithmic choices, including the\nselection strategy for rows and columns as well as the optimal rank for\neffective implementation of our method."}
{"id": "2410.11163", "pdf": "https://arxiv.org/pdf/2410.11163.pdf", "abs": "https://arxiv.org/abs/2410.11163", "title": "Model Swarms: Collaborative Search to Adapt LLM Experts via Swarm Intelligence", "authors": ["Shangbin Feng", "Zifeng Wang", "Yike Wang", "Sayna Ebrahimi", "Hamid Palangi", "Lesly Miculicich", "Achin Kulshrestha", "Nathalie Rauschmayr", "Yejin Choi", "Yulia Tsvetkov", "Chen-Yu Lee", "Tomas Pfister"], "categories": ["cs.CL"], "comment": "ICML 2025", "summary": "We propose Model Swarms, a collaborative search algorithm to adapt LLMs via\nswarm intelligence, the collective behavior guiding individual systems.\nSpecifically, Model Swarms starts with a pool of LLM experts and a utility\nfunction. Guided by the best-found checkpoints across models, diverse LLM\nexperts collaboratively move in the weight space and optimize a utility\nfunction representing model adaptation objectives. Compared to existing model\ncomposition approaches, Model Swarms offers tuning-free model adaptation, works\nin low-data regimes with as few as 200 examples, and does not require\nassumptions about specific experts in the swarm or how they should be composed.\nExtensive experiments demonstrate that Model Swarms could flexibly adapt LLM\nexperts to a single task, multi-task domains, reward models, as well as diverse\nhuman interests, improving over 12 model composition baselines by up to 21.0%\nacross tasks and contexts. Further analysis reveals that LLM experts discover\npreviously unseen capabilities in initial checkpoints and that Model Swarms\nenable the weak-to-strong transition of experts through the collaborative\nsearch process."}
{"id": "2410.11693", "pdf": "https://arxiv.org/pdf/2410.11693.pdf", "abs": "https://arxiv.org/abs/2410.11693", "title": "BridG MT: Enhancing LLMs' Machine Translation Capabilities with Sentence Bridging and Gradual MT", "authors": ["Seung-Woo Choi", "Ga-Hyun Yoo", "Jay-Yoon Lee"], "categories": ["cs.CL"], "comment": "Accepted to ACL Findings 2025", "summary": "Recent Large Language Models (LLMs) have demonstrated impressive translation\nperformance without requiring fine-tuning on additional parallel corpora.\nHowever, they still face significant challenges in certain scenarios,\nparticularly when translating low-resource languages. A common approach to\naddress this issue is to provide external knowledge, such as few-shot examples,\nto assist LLMs in translating specific source sentences. However, this method\nis fundamentally limited by the quality or quantity of relevant sources, which\ncannot always be guaranteed. To reduce LLMs' reliance on external sources, we\npropose BridG MT, a method that combines Sentence Bridging, which generates a\nsequence of sentences as a bridge that gradually transition from\neasy-to-translate to more difficult, and Gradual MT, which sequentially\ntranslates these sentences using earlier translations as few-shot examples for\nsubsequent ones. Experiments conducted on four LLMs across seven languages\ndemonstrate that our method effectively enhances translation performance, even\noutperforming translation methods that rely on a large number of few-shot\nexamples."}
{"id": "2410.12613", "pdf": "https://arxiv.org/pdf/2410.12613.pdf", "abs": "https://arxiv.org/abs/2410.12613", "title": "Exploring Model Kinship for Merging Large Language Models", "authors": ["Yedi Hu", "Yunzhi Yao", "Shumin Deng", "Huajun Chen", "Ningyu Zhang"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MA"], "comment": "Ongoing work", "summary": "Model merging has become one of the key technologies for enhancing the\ncapabilities and efficiency of Large Language Models (LLMs). However, our\nunderstanding of the expected performance gains and principles when merging any\ntwo models remains limited. In this work, we introduce model kinship, the\ndegree of similarity or relatedness between LLMs, analogous to biological\nevolution. With comprehensive empirical analysis, we find that there is a\ncertain relationship between model kinship and the performance gains after\nmodel merging, which can help guide our selection of candidate models. Inspired\nby this, we propose a new model merging strategy: Top-k Greedy Merging with\nModel Kinship, which can yield better performance on benchmark datasets.\nSpecifically, we discover that using model kinship as a criterion can assist us\nin continuously performing model merging, alleviating the degradation (local\noptima) in model evolution, whereas model kinship can serve as a guide to\nescape these traps. Code is available at\nhttps://github.com/zjunlp/ModelKinship."}
{"id": "2410.13098", "pdf": "https://arxiv.org/pdf/2410.13098.pdf", "abs": "https://arxiv.org/abs/2410.13098", "title": "A Little Human Data Goes A Long Way", "authors": ["Dhananjay Ashok", "Jonathan May"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL 2025", "summary": "Faced with an expensive human annotation process, creators of NLP systems\nincreasingly turn to synthetic data generation. While this method shows\npromise, the extent to which synthetic data can replace human annotation is\npoorly understood. We investigate the use of synthetic data in Fact\nVerification (FV) and Question Answering (QA) by studying the effects of\nincrementally replacing human generated data with synthetic points on eight\ndiverse datasets. Strikingly, replacing up to 90% of the training data only\nmarginally decreases performance, but replacing the final 10% leads to severe\ndeclines. We find that models trained on purely synthetic data can be reliably\nimproved by including as few as 125 human generated data points. We show that\nmatching the performance gain of just a little additional human data (only 200\npoints) requires an order of magnitude more synthetic data and estimate price\nratios at which human annotation would be a more cost-effective solution. Our\nresults suggest that even when human annotation at scale is infeasible, there\nis great value to having a small proportion of the dataset being human\ngenerated."}
{"id": "2410.13184", "pdf": "https://arxiv.org/pdf/2410.13184.pdf", "abs": "https://arxiv.org/abs/2410.13184", "title": "Router-Tuning: A Simple and Effective Approach for Enabling Dynamic-Depth in Transformers", "authors": ["Shwai He", "Tao Ge", "Guoheng Sun", "Bowei Tian", "Xiaoyang Wang", "Dong Yu"], "categories": ["cs.CL"], "comment": null, "summary": "Traditional transformer models often allocate a fixed amount of computational\nresources to every input token, leading to inefficient and unnecessary\ncomputation. To address this, the Mixture of Depths (MoD) was introduced to\ndynamically adjust the computational depth by skipping less important layers.\nDespite its promise, current MoD approaches remain under-explored and face two\nmain challenges: (1) high training costs due to the need to train the entire\nmodel along with the routers that determine which layers to skip, and (2) the\nrisk of performance degradation when important layers are bypassed. In response\nto the first issue, we propose Router-Tuning, a method that fine-tunes only the\nrouter on a small dataset, drastically reducing the computational overhead\nassociated with full model training. For the second challenge, we propose\nMindSkip, which deploys Attention with Dynamic Depths. This method preserves\nthe model's performance while significantly enhancing computational and memory\nefficiency. Extensive experiments demonstrate that our approach delivers\ncompetitive results while dramatically improving the computation efficiency,\ne.g., 21\\% speedup and only a 0.2\\% performance drop. The code is released at\nhttps://github.com/CASE-Lab-UMD/Router-Tuning."}
{"id": "2410.13281", "pdf": "https://arxiv.org/pdf/2410.13281.pdf", "abs": "https://arxiv.org/abs/2410.13281", "title": "BanTH: A Multi-label Hate Speech Detection Dataset for Transliterated Bangla", "authors": ["Fabiha Haider", "Fariha Tanjim Shifat", "Md Farhan Ishmam", "Deeparghya Dutta Barua", "Md Sakib Ul Rahman Sourove", "Md Fahim", "Md Farhad Alam"], "categories": ["cs.CL"], "comment": "Published in NAACL Findings 2025", "summary": "The proliferation of transliterated texts in digital spaces has emphasized\nthe need for detecting and classifying hate speech in languages beyond English,\nparticularly in low-resource languages. As online discourse can perpetuate\ndiscrimination based on target groups, e.g. gender, religion, and origin,\nmulti-label classification of hateful content can help in comprehending hate\nmotivation and enhance content moderation. While previous efforts have focused\non monolingual or binary hate classification tasks, no work has yet addressed\nthe challenge of multi-label hate speech classification in transliterated\nBangla. We introduce BanTH, the first multi-label transliterated Bangla hate\nspeech dataset comprising 37.3k samples. The samples are sourced from YouTube\ncomments, where each instance is labeled with one or more target groups,\nreflecting the regional demographic. We establish novel transformer\nencoder-based baselines by further pre-training on transliterated Bangla\ncorpus. We also propose a novel translation-based LLM prompting strategy for\ntransliterated text. Experiments reveal that our further pre-trained encoders\nare achieving state-of-the-art performance on the BanTH dataset, while our\ntranslation-based prompting outperforms other strategies in the zero-shot\nsetting. The introduction of BanTH not only fills a critical gap in hate speech\nresearch for Bangla but also sets the stage for future exploration into\ncode-mixed and multi-label classification challenges in underrepresented\nlanguages."}
{"id": "2410.14309", "pdf": "https://arxiv.org/pdf/2410.14309.pdf", "abs": "https://arxiv.org/abs/2410.14309", "title": "LoGU: Long-form Generation with Uncertainty Expressions", "authors": ["Ruihan Yang", "Caiqi Zhang", "Zhisong Zhang", "Xinting Huang", "Sen Yang", "Nigel Collier", "Dong Yu", "Deqing Yang"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Main", "summary": "While Large Language Models (LLMs) demonstrate impressive capabilities, they\nstill struggle with generating factually incorrect content (i.e.,\nhallucinations). A promising approach to mitigate this issue is enabling models\nto express uncertainty when unsure. Previous research on uncertainty modeling\nhas primarily focused on short-form QA, but realworld applications often\nrequire much longer responses. In this work, we introduce the task of Long-form\nGeneration with Uncertainty(LoGU). We identify two key challenges: Uncertainty\nSuppression, where models hesitate to express uncertainty, and Uncertainty\nMisalignment, where models convey uncertainty inaccurately. To tackle these\nchallenges, we propose a refinement-based data collection framework and a\ntwo-stage training pipeline. Our framework adopts a divide-and-conquer\nstrategy, refining uncertainty based on atomic claims. The collected data are\nthen used in training through supervised fine-tuning (SFT) and direct\npreference optimization (DPO) to enhance uncertainty expression. Extensive\nexperiments on three long-form instruction following datasets show that our\nmethod significantly improves accuracy, reduces hallucinations, and maintains\nthe comprehensiveness of responses."}
{"id": "2410.16930", "pdf": "https://arxiv.org/pdf/2410.16930.pdf", "abs": "https://arxiv.org/abs/2410.16930", "title": "Math Neurosurgery: Isolating Language Models' Math Reasoning Abilities Using Only Forward Passes", "authors": ["Bryan R. Christ", "Zack Gottesman", "Jonathan Kropko", "Thomas Hartvigsen"], "categories": ["cs.CL", "cs.AI"], "comment": "38 pages, 54 figures, Accepted to ACL 2025 (Main)", "summary": "Math reasoning is an active area of Large Language Model (LLM) research\nbecause it is a hallmark of artificial intelligence and has implications in\nseveral domains, including math education. However, few works have explored how\nmath reasoning is encoded within LLM parameters and if it is a skill that can\nbe isolated within models. Doing so could allow targeted intervention to\nimprove math performance without altering non-math behavior and foster\nunderstanding of how models encode math reasoning. We introduce Math\nNeurosurgery (MathNeuro), a computationally efficient method we use to isolate\nmath-specific parameters in LLMs using only forward passes. MathNeuro builds on\nexisting work by using weights and activations to calculate parameter\nimportance, but isolates math-specific parameters by filtering out those\nimportant for general language tasks. Through pruning parameters MathNeuro\nidentifies, we delete a LLM's math reasoning ability without significantly\nimpacting its general language ability. Scaling the identified parameters by a\nsmall constant improves a pretrained or instruction-tuned LLM's performance by\n4-17% on GSM8K and 5-35% on MATH while leaving non-math behavior unaltered.\nMathNeuro is also data efficient: most of its effectiveness holds when\nidentifying math-specific parameters using a single sample. MathNeuro\nhighlights the potential for future work to intervene on math-specific\nparameters."}
{"id": "2410.17714", "pdf": "https://arxiv.org/pdf/2410.17714.pdf", "abs": "https://arxiv.org/abs/2410.17714", "title": "CogSteer: Cognition-Inspired Selective Layer Intervention for Efficiently Steering Large Language Models", "authors": ["Xintong Wang", "Jingheng Pan", "Liang Ding", "Longyue Wang", "Longqin Jiang", "Xingshan Li", "Chris Biemann"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to Findings of ACL 2025", "summary": "Large Language Models (LLMs) achieve remarkable performance through\npretraining on extensive data. This enables efficient adaptation to diverse\ndownstream tasks. However, the lack of interpretability in their underlying\nmechanisms limits the ability to effectively steer LLMs for specific\napplications. In this work, we investigate the intrinsic mechanisms of LLMs\nfrom a cognitive perspective using eye movement measures. Specifically, we\nanalyze the layer-wise correlation between human cognitive indicators and LLM\nrepresentations. Building on these insights, we propose a heuristic approach\nfor selecting the optimal steering layer to modulate LLM semantics. To this\nend, we introduce an efficient selective layer intervention based on prominent\nparameter-efficient fine-tuning methods, which conventionally adjust either all\nlayers or only the final layer. Additionally, we present an implicit layer\ncontrastive intervention during inference to steer LLMs away from toxic\noutputs. Extensive experiments on natural language understanding, reasoning,\nand generation tasks, conducted on GPT-2, Llama2-7B, and Mistral-7B,\ndemonstrate the effectiveness and efficiency of our approach. As a\nmodel-agnostic framework, it enhances the interpretability of LLMs while\nimproving efficiency for safe deployment."}
{"id": "2410.17891", "pdf": "https://arxiv.org/pdf/2410.17891.pdf", "abs": "https://arxiv.org/abs/2410.17891", "title": "Scaling Diffusion Language Models via Adaptation from Autoregressive Models", "authors": ["Shansan Gong", "Shivam Agarwal", "Yizhe Zhang", "Jiacheng Ye", "Lin Zheng", "Mukai Li", "Chenxin An", "Peilin Zhao", "Wei Bi", "Jiawei Han", "Hao Peng", "Lingpeng Kong"], "categories": ["cs.CL"], "comment": "ICLR 2025. (minor updates) Code: https://github.com/HKUNLP/DiffuLLaMA", "summary": "Diffusion Language Models (DLMs) have emerged as a promising new paradigm for\ntext generative modeling, potentially addressing limitations of autoregressive\n(AR) models. However, current DLMs have been studied at a smaller scale\ncompared to their AR counterparts and lack fair comparison on language modeling\nbenchmarks. Additionally, training diffusion models from scratch at scale\nremains challenging. Given the prevalence of open-source AR language models, we\npropose adapting these models to build text diffusion models. We demonstrate\nconnections between AR and diffusion modeling objectives and introduce a simple\ncontinual pre-training approach for training diffusion models. Through\nsystematic evaluation on language modeling, reasoning, and commonsense\nbenchmarks, we show that we can convert AR models ranging from 127M to 7B\nparameters (GPT2 and LLaMA) into diffusion models DiffuGPT and DiffuLLaMA,\nusing less than 200B tokens for training. Our experimental results reveal that\nthese models outperform earlier DLMs and are competitive with their AR\ncounterparts. We release a suite of DLMs (127M-355M-7B) capable of generating\nfluent text, performing in-context learning, filling in the middle without\nprompt re-ordering, and following instructions\nhttps://github.com/HKUNLP/DiffuLLaMA."}
{"id": "2410.18359", "pdf": "https://arxiv.org/pdf/2410.18359.pdf", "abs": "https://arxiv.org/abs/2410.18359", "title": "Improving Model Factuality with Fine-grained Critique-based Evaluator", "authors": ["Yiqing Xie", "Wenxuan Zhou", "Pradyot Prakash", "Di Jin", "Yuning Mao", "Quintin Fettes", "Arya Talebzadeh", "Sinong Wang", "Han Fang", "Carolyn Rose", "Daniel Fried", "Hejia Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Factuality evaluation aims to detect factual errors produced by language\nmodels (LMs) and hence guide the development of more factual models. Towards\nthis goal, we train a factuality evaluator, FenCE, that provides LM generators\nwith claim-level factuality feedback. We conduct data augmentation on a\ncombination of public judgment datasets to train FenCE to (1) generate textual\ncritiques along with scores and (2) make claim-level judgment based on diverse\nsource documents obtained by various tools. We then present a framework that\nleverages FenCE to improve the factuality of LM generators by constructing\ntraining data. Specifically, we generate a set of candidate responses, leverage\nFenCE to revise and score each response without introducing lesser-known facts,\nand train the generator by preferring highly scored revised responses.\nExperiments show that our data augmentation methods improve the evaluator's\naccuracy by 2.9% on LLM-AggreFact. With FenCE, we improve Llama2-7B-chat and\nLlama3-8B-chat's factuality rate by 16.86% and 14.45% on FActScore,\noutperforming state-of-the-art factuality finetuning methods by 8.83% and\n6.96%."}
{"id": "2410.18702", "pdf": "https://arxiv.org/pdf/2410.18702.pdf", "abs": "https://arxiv.org/abs/2410.18702", "title": "GrammaMT: Improving Machine Translation with Grammar-Informed In-Context Learning", "authors": ["Rita Ramos", "Everlyn Asiko Chimoto", "Maartje ter Hoeve", "Natalie Schluter"], "categories": ["cs.CL"], "comment": "Accepted at ACL 2025", "summary": "We introduce GrammaMT, a grammatically-aware prompting approach for machine\ntranslation that uses Interlinear Glossed Text (IGT), a common form of\nlinguistic description providing morphological and lexical annotations for\nsource sentences. GrammaMT proposes three prompting strategies: gloss-shot,\nchain-gloss and model-gloss. All are training-free, requiring only a few\nexamples that involve minimal effort to collect, and making them well-suited\nfor low-resource setups. Experiments show that GrammaMT enhances translation\nperformance on open-source instruction-tuned LLMs for various low- to\nhigh-resource languages across three benchmarks: (1) the largest IGT corpus,\n(2) the challenging 2023 SIGMORPHON Shared Task data over endangered languages,\nand (3) even in an out-of-domain setting with FLORES. Moreover, ablation\nstudies reveal that leveraging gloss resources could substantially boost MT\nperformance (by over 17 BLEU points) if LLMs accurately generate or access\ninput sentence glosses."}
{"id": "2410.19133", "pdf": "https://arxiv.org/pdf/2410.19133.pdf", "abs": "https://arxiv.org/abs/2410.19133", "title": "Hybrid Preferences: Learning to Route Instances for Human vs. AI Feedback", "authors": ["Lester James V. Miranda", "Yizhong Wang", "Yanai Elazar", "Sachin Kumar", "Valentina Pyatkin", "Faeze Brahman", "Noah A. Smith", "Hannaneh Hajishirzi", "Pradeep Dasigi"], "categories": ["cs.CL"], "comment": "Code in https://github.com/allenai/hybrid-preferences, MultiPref\n  dataset in https://huggingface.co/datasets/allenai/multipref, Updated related\n  work and acknowledgments", "summary": "Learning from human feedback has enabled the alignment of language models\n(LMs) with human preferences. However, collecting human preferences is\nexpensive and time-consuming, with highly variable annotation quality. An\nappealing alternative is to distill preferences from LMs as a source of\nsynthetic annotations, offering a cost-effective and scalable alternative,\nalbeit susceptible to other biases and errors. In this work, we introduce\nHyPER, a Hybrid Preference routER that defers an annotation to either humans or\nLMs, achieving better annotation quality while reducing the cost of human-only\nannotation. We formulate this as an optimization problem: given a preference\ndataset and an evaluation metric, we (1) train a performance prediction model\n(PPM) to predict a reward model's (RM) performance on an arbitrary combination\nof human and LM annotations and (2) employ a routing strategy that selects a\ncombination that maximizes the predicted performance. We train the PPM on\nMultiPref, a new preference dataset with 10k instances paired with humans and\nLM labels. We show that the selected hybrid mixture of synthetic and direct\nhuman preferences using HyPER achieves better RM performance compared to using\neither one exclusively by 7-13% on RewardBench and generalizes across unseen\npreference datasets and other base models. We also observe the same trend in\nother benchmarks using Best-of-N reranking, where the hybrid mix has 2-3%\nbetter performance. Finally, we analyze features from HyPER and find that\nprompts with moderate safety concerns or complexity benefit the most from human\nfeedback."}
{"id": "2410.20445", "pdf": "https://arxiv.org/pdf/2410.20445.pdf", "abs": "https://arxiv.org/abs/2410.20445", "title": "TrajAgent: An LLM-based Agent Framework for Automated Trajectory Modeling via Collaboration of Large and Small Models", "authors": ["Yuwei Du", "Jie Feng", "Jie Zhao", "Yong Li"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "the code will be openly accessible at:\n  https://github.com/tsinghua-fib-lab/TrajAgent", "summary": "Trajectory modeling, which includes research on trajectory data pattern\nmining and future prediction, has widespread applications in areas such as life\nservices, urban transportation, and public administration. Numerous methods\nhave been proposed to address specific problems within trajectory modeling.\nHowever, the heterogeneity of data and the diversity of trajectory tasks make\neffective and reliable trajectory modeling an important yet highly challenging\nendeavor, even for domain experts. In this paper, we propose\n\\textit{TrajAgent}, a agent framework powered by large language models (LLMs),\ndesigned to facilitate robust and efficient trajectory modeling through\nautomation modeling. This framework leverages and optimizes diverse specialized\nmodels to address various trajectory modeling tasks across different datasets\neffectively. In \\textit{TrajAgent}, we first develop \\textit{UniEnv}, an\nexecution environment with a unified data and model interface, to support the\nexecution and training of various models. Building on \\textit{UniEnv}, we\nintroduce an agentic workflow designed for automatic trajectory modeling across\nvarious trajectory tasks and data. Furthermore, we introduce collaborative\nlearning schema between LLM-based agents and small speciallized models, to\nenhance the performance of the whole framework effectively. Extensive\nexperiments on four tasks using four real-world datasets demonstrate the\neffectiveness of \\textit{TrajAgent} in automated trajectory modeling, achieving\na performance improvement of 2.38\\%-34.96\\% over baseline methods."}
{"id": "2410.22499", "pdf": "https://arxiv.org/pdf/2410.22499.pdf", "abs": "https://arxiv.org/abs/2410.22499", "title": "Anticipating Future with Large Language Model for Simultaneous Machine Translation", "authors": ["Siqi Ouyang", "Oleksii Hrinchuk", "Zhehuai Chen", "Vitaly Lavrukhin", "Jagadeesh Balam", "Lei Li", "Boris Ginsburg"], "categories": ["cs.CL"], "comment": "NAACL 2025 Main", "summary": "Simultaneous machine translation (SMT) takes streaming input utterances and\nincrementally produces target text. Existing SMT methods mainly use the partial\nutterance that has already arrived at the input and the generated hypothesis.\nMotivated by human interpreters' technique to forecast future words before\nhearing them, we propose $\\textbf{T}$ranslation by $\\textbf{A}$nticipating\n$\\textbf{F}$uture (TAF), a method to improve translation quality while\nretraining low latency. Its core idea is to use a large language model (LLM) to\npredict future source words and opportunistically translate without introducing\ntoo much risk. We evaluate our TAF and multiple baselines of SMT on four\nlanguage directions. Experiments show that TAF achieves the best translation\nquality-latency trade-off and outperforms the baselines by up to 5 BLEU points\nat the same latency (three words). Code is released at\nhttps://github.com/owaski/TAF"}
{"id": "2411.00387", "pdf": "https://arxiv.org/pdf/2411.00387.pdf", "abs": "https://arxiv.org/abs/2411.00387", "title": "STEM-POM: Evaluating Language Models Math-Symbol Reasoning in Document Parsing", "authors": ["Jiaru Zou", "Qing Wang", "Pratyush Thakur", "Nickvash Kani"], "categories": ["cs.CL"], "comment": "ACL 2025; NeurIPS Math-AI 2024; Code and Data:\n  https://github.com/jiaruzouu/STEM-PoM", "summary": "Advances in large language models (LLMs) have spurred research into enhancing\ntheir reasoning capabilities, particularly in math-rich STEM (Science,\nTechnology, Engineering, and Mathematics) documents. While LLMs can generate\nequations or solve math-related queries, their ability to fully understand and\ninterpret abstract mathematical symbols in long, math-rich documents remains\nlimited. In this paper, we introduce STEM-PoM, a comprehensive benchmark\ndataset designed to evaluate LLMs' reasoning abilities on math symbols within\ncontextual scientific text. The dataset, sourced from real-world ArXiv\ndocuments, contains over 2K math symbols classified as main attributes of\nvariables, constants, operators, and unit descriptors, with additional\nsub-attributes including scalar/vector/matrix for variables and\nlocal/global/discipline-specific labels for both constants and operators. Our\nextensive experiments demonstrate that state-of-the-art LLMs achieve an average\naccuracy of 20-60% under in-context learning and 50-60% with fine-tuning,\nhighlighting a substantial gap in their ability to classify mathematical\nsymbols. By improving LLMs' mathematical symbol classification, STEM-PoM\nfurther enhances models' downstream mathematical reasoning capabilities. The\ncode and data are available at https://github.com/jiaruzouu/STEM-PoM."}
{"id": "2411.04699", "pdf": "https://arxiv.org/pdf/2411.04699.pdf", "abs": "https://arxiv.org/abs/2411.04699", "title": "Towards Building Large Scale Datasets and State-of-the-Art Automatic Speech Translation Systems for 14 Indian Languages", "authors": ["Ashwin Sankar", "Sparsh Jain", "Nikhil Narasimhan", "Devilal Choudhary", "Dhairya Suman", "Mohammed Safi Ur Rahman Khan", "Anoop Kunchukuttan", "Mitesh M Khapra", "Raj Dabre"], "categories": ["cs.CL"], "comment": "Accepted at ACL (Main) 2025", "summary": "Speech translation for Indian languages remains a challenging task due to the\nscarcity of large-scale, publicly available datasets that capture the\nlinguistic diversity and domain coverage essential for real-world applications.\nExisting datasets cover a fraction of Indian languages and lack the breadth\nneeded to train robust models that generalize beyond curated benchmarks. To\nbridge this gap, we introduce BhasaAnuvaad, the largest speech translation\ndataset for Indian languages, spanning over 44 thousand hours of audio and 17\nmillion aligned text segments across 14 Indian languages and English. Our\ndataset is built through a threefold methodology: (a) aggregating high-quality\nexisting sources, (b) large-scale web crawling to ensure linguistic and domain\ndiversity, and (c) creating synthetic data to model real-world speech\ndisfluencies. Leveraging BhasaAnuvaad, we train IndicSeamless, a\nstate-of-the-art speech translation model for Indian languages that performs\nbetter than existing models. Our experiments demonstrate improvements in the\ntranslation quality, setting a new standard for Indian language speech\ntranslation. We will release all the code, data and model weights in the\nopen-source, with permissive licenses to promote accessibility and\ncollaboration."}
{"id": "2411.04794", "pdf": "https://arxiv.org/pdf/2411.04794.pdf", "abs": "https://arxiv.org/abs/2411.04794", "title": "KnowCoder-X: Boosting Multilingual Information Extraction via Code", "authors": ["Yuxin Zuo", "Wenxuan Jiang", "Wenxuan Liu", "Zixuan Li", "Long Bai", "Hanbin Wang", "Yutao Zeng", "Xiaolong Jin", "Jiafeng Guo", "Xueqi Cheng"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL 2025 Findings", "summary": "Empirical evidence indicates that LLMs exhibit spontaneous cross-lingual\nalignment. However, although LLMs show promising cross-lingual alignment in\nInformation Extraction (IE), a significant imbalance across languages persists,\nhighlighting an underlying deficiency. To address this, we propose KnowCoder-X,\na powerful code LLM with advanced cross-lingual and multilingual capabilities\nfor universal IE. Firstly, it standardizes the representation of multilingual\nschemas using Python classes, ensuring a consistent ontology across different\nlanguages. Then, IE across languages is formulated as a unified code generation\ntask. Secondly, we conduct IE cross-lingual alignment instruction tuning on the\ntranslated instance prediction task to enhance the model's cross-lingual\ntransferability. During this phase, we also construct a high-quality and\ndiverse bilingual IE parallel dataset with 257k samples, called ParallelNER,\nsynthesized by our proposed robust three-stage pipeline, with manual annotation\nto ensure quality. Although without training in 29 unseen languages,\nKnowCoder-X surpasses ChatGPT by 30.17\\% and SoTA by 20.03\\%, thereby\ndemonstrating superior cross-lingual IE capabilities. Comprehensive evaluations\non 64 IE benchmarks in Chinese and English under various settings demonstrate\nthat KnowCoder-X significantly enhances cross-lingual IE transfer through\nboosting the IE alignment. Our code and dataset are available at:\nhttps://github.com/ICT-GoKnow/KnowCoder"}
{"id": "2411.05980", "pdf": "https://arxiv.org/pdf/2411.05980.pdf", "abs": "https://arxiv.org/abs/2411.05980", "title": "FactLens: Benchmarking Fine-Grained Fact Verification", "authors": ["Kushan Mitra", "Dan Zhang", "Sajjadur Rahman", "Estevam Hruschka"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "12 pages, updated version", "summary": "Large Language Models (LLMs) have shown impressive capability in language\ngeneration and understanding, but their tendency to hallucinate and produce\nfactually incorrect information remains a key limitation. To verify\nLLM-generated contents and claims from other sources, traditional verification\napproaches often rely on holistic models that assign a single factuality label\nto complex claims, potentially obscuring nuanced errors. In this paper, we\nadvocate for a shift towards fine-grained verification, where complex claims\nare broken down into smaller sub-claims for individual verification, allowing\nfor more precise identification of inaccuracies, improved transparency, and\nreduced ambiguity in evidence retrieval. However, generating sub-claims poses\nchallenges, such as maintaining context and ensuring semantic equivalence with\nrespect to the original claim. We introduce FactLens, a benchmark for\nevaluating fine-grained fact verification, with metrics and automated\nevaluators of sub-claim quality. The benchmark data is manually curated to\nensure high-quality ground truth. Our results show alignment between automated\nFactLens evaluators and human judgments, and we discuss the impact of sub-claim\ncharacteristics on the overall verification performance."}
{"id": "2411.08534", "pdf": "https://arxiv.org/pdf/2411.08534.pdf", "abs": "https://arxiv.org/abs/2411.08534", "title": "Neural Topic Modeling with Large Language Models in the Loop", "authors": ["Xiaohao Yang", "He Zhao", "Weijie Xu", "Yuanyuan Qi", "Jueqing Lu", "Dinh Phung", "Lan Du"], "categories": ["cs.CL"], "comment": null, "summary": "Topic modeling is a fundamental task in natural language processing, allowing\nthe discovery of latent thematic structures in text corpora. While Large\nLanguage Models (LLMs) have demonstrated promising capabilities in topic\ndiscovery, their direct application to topic modeling suffers from issues such\nas incomplete topic coverage, misalignment of topics, and inefficiency. To\naddress these limitations, we propose LLM-ITL, a novel LLM-in-the-loop\nframework that integrates LLMs with Neural Topic Models (NTMs). In LLM-ITL,\nglobal topics and document representations are learned through the NTM.\nMeanwhile, an LLM refines these topics using an Optimal Transport (OT)-based\nalignment objective, where the refinement is dynamically adjusted based on the\nLLM's confidence in suggesting topical words for each set of input words. With\nthe flexibility of being integrated into many existing NTMs, the proposed\napproach enhances the interpretability of topics while preserving the\nefficiency of NTMs in learning topics and document representations. Extensive\nexperiments demonstrate that LLM-ITL helps NTMs significantly improve their\ntopic interpretability while maintaining the quality of document\nrepresentation. Our code and datasets are available at\nhttps://github.com/Xiaohao-Yang/LLM-ITL"}
{"id": "2411.15462", "pdf": "https://arxiv.org/pdf/2411.15462.pdf", "abs": "https://arxiv.org/abs/2411.15462", "title": "HateDay: Insights from a Global Hate Speech Dataset Representative of a Day on Twitter", "authors": ["Manuel Tonneau", "Diyi Liu", "Niyati Malhotra", "Scott A. Hale", "Samuel P. Fraiberger", "Victor Orozco-Olvera", "Paul R√∂ttger"], "categories": ["cs.CL"], "comment": "ACL 2025 main conference. Data available at\n  https://huggingface.co/datasets/manueltonneau/hateday", "summary": "To address the global challenge of online hate speech, prior research has\ndeveloped detection models to flag such content on social media. However, due\nto systematic biases in evaluation datasets, the real-world effectiveness of\nthese models remains unclear, particularly across geographies. We introduce\nHateDay, the first global hate speech dataset representative of social media\nsettings, constructed from a random sample of all tweets posted on September\n21, 2022 and covering eight languages and four English-speaking countries.\nUsing HateDay, we uncover substantial variation in the prevalence and\ncomposition of hate speech across languages and regions. We show that\nevaluations on academic datasets greatly overestimate real-world detection\nperformance, which we find is very low, especially for non-European languages.\nOur analysis identifies key drivers of this gap, including models' difficulty\nto distinguish hate from offensive speech and a mismatch between the target\ngroups emphasized in academic datasets and those most frequently targeted in\nreal-world settings. We argue that poor model performance makes public models\nill-suited for automatic hate speech moderation and find that high moderation\nrates are only achievable with substantial human oversight. Our results\nunderscore the need to evaluate detection systems on data that reflects the\ncomplexity and diversity of real-world social media."}
{"id": "2411.16679", "pdf": "https://arxiv.org/pdf/2411.16679.pdf", "abs": "https://arxiv.org/abs/2411.16679", "title": "Do Large Language Models Perform Latent Multi-Hop Reasoning without Exploiting Shortcuts?", "authors": ["Sohee Yang", "Nora Kassner", "Elena Gribovskaya", "Sebastian Riedel", "Mor Geva"], "categories": ["cs.CL"], "comment": "Findings of ACL 2025", "summary": "We evaluate how well Large Language Models (LLMs) latently recall and compose\nfacts to answer multi-hop queries like \"In the year Scarlett Johansson was\nborn, the Summer Olympics were hosted in the country of\". One major challenge\nin such evaluation is that LLMs may have developed shortcuts by encountering\nthe head entity \"Scarlett Johansson\" and the answer entity \"United States\" in\nthe same training sequences or merely guess the answer based on frequency-based\npriors. To prevent shortcuts, we exclude test queries where the head and answer\nentities might have co-appeared during training. Through careful selection of\nrelations and facts and systematic removal of cases where models might guess\nanswers or exploit partial matches, we construct an evaluation dataset SOCRATES\n(ShOrtCut-fRee lATent rEaSoning). We observe that LLMs demonstrate promising\nlatent multi-hop reasoning abilities without exploiting shortcuts, but only for\ncertain types of queries. For queries requiring latent recall of countries as\nthe intermediate answer, the best models achieve 80% latent composability, but\nthis drops to just 5% for the recall of years. Comparisons with\nChain-of-Thought highlight a significant gap between the ability of models to\nreason latently versus explicitly. Analysis reveals that latent representations\nof the intermediate answer are constructed more often in queries with higher\nlatent composability, and shows the emergence of latent multi-hop reasoning\nduring pretraining."}
{"id": "2411.18478", "pdf": "https://arxiv.org/pdf/2411.18478.pdf", "abs": "https://arxiv.org/abs/2411.18478", "title": "Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS", "authors": ["Jinyang Wu", "Mingkuan Feng", "Shuai Zhang", "Feihu Che", "Zengqi Wen", "Chonghua Liao", "Jianhua Tao"], "categories": ["cs.CL"], "comment": null, "summary": "In-context learning (ICL) enables large language models (LLMs) to perform\ndownstream tasks through advanced prompting and high-quality demonstrations.\nHowever, traditional ICL paradigms encounter significant limitations in complex\nreasoning tasks, stemming primarily from their dependence on example quality\nand absence of explicit reasoning guidance. To address these challenges, we\nintroduce HiAR-ICL, a **Hi**gh-level **A**utomated **R**easoning paradigm in\n**ICL** that shifts focus from specific examples to abstract reasoning\npatterns, thereby extending the conventional concept of \"context\" in ICL. Our\napproach begins by defining five atomic reasoning actions, upon which we employ\nMonte Carlo Tree Search to systematically construct high-level reasoning\npatterns. During inference, HiAR-ICL dynamically selects appropriate reasoning\npatterns based on problem attributes, providing explicit guidance for the\nmodel's reasoning process. Experiments demonstrate HiAR-ICL's effectiveness and\nefficiency: utilizing only 200 prior samples with Qwen2.5-7B-Instruct, our\nmethod achieves 80.6% accuracy on MATH and 62.5% on AMC, exceeding GPT-4o's\n77.2% and 57.5%. Our approach enhances performance across models of varying\nsizes while generalizing effectively across domains. Further analysis reveals\nthat HiAR-ICL can also serve as a plug-and-play inference method compatible\nwith post-training techniques like GRPO. Code and data are available at\nhttps://github.com/jinyangwu/HiARICL."}
{"id": "2412.01617", "pdf": "https://arxiv.org/pdf/2412.01617.pdf", "abs": "https://arxiv.org/abs/2412.01617", "title": "If Eleanor Rigby Had Met ChatGPT: A Study on Loneliness in a Post-LLM World", "authors": ["Adrian de Wynter"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": "Accepted to ACL 2025 (main)", "summary": "Warning: this paper discusses content related, but not limited to, violence,\nsex, and suicide. Loneliness, or the lack of fulfilling relationships,\nsignificantly impacts a person's mental and physical well-being and is\nprevalent worldwide. Previous research suggests that large language models\n(LLMs) may help mitigate loneliness. However, we argue that the use of\nwidespread LLMs in services like ChatGPT is more prevalent--and riskier, as\nthey are not designed for this purpose. To explore this, we analysed user\ninteractions with ChatGPT outside of its marketed use as a task-oriented\nassistant. In dialogues classified as lonely, users frequently (37%) sought\nadvice or validation, and received good engagement. However, ChatGPT failed in\nsensitive scenarios, like responding appropriately to suicidal ideation or\ntrauma. We also observed a 35% higher incidence of toxic content, with women\nbeing 22x more likely to be targeted than men. Our findings underscore ethical\nand legal questions about this technology, and note risks like radicalisation\nor further isolation. We conclude with recommendations to research and industry\nto address loneliness."}
{"id": "2412.02595", "pdf": "https://arxiv.org/pdf/2412.02595.pdf", "abs": "https://arxiv.org/abs/2412.02595", "title": "Nemotron-CC: Transforming Common Crawl into a Refined Long-Horizon Pretraining Dataset", "authors": ["Dan Su", "Kezhi Kong", "Ying Lin", "Joseph Jennings", "Brandon Norick", "Markus Kliegl", "Mostofa Patwary", "Mohammad Shoeybi", "Bryan Catanzaro"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Recent English Common Crawl datasets like FineWeb-Edu and DCLM achieved\nsignificant benchmark gains via aggressive model-based filtering, but at the\ncost of removing 90% of data. This limits their suitability for long token\nhorizon training, such as 15T tokens for Llama 3.1. In this paper, we show how\nto achieve better trade-offs between accuracy and data quantity by a\ncombination of classifier ensembling, synthetic data rephrasing, and reduced\nreliance on heuristic filters. When training 8B parameter models for 1T tokens,\nusing a high-quality subset of our data improves MMLU by 5.6 over DCLM,\ndemonstrating the efficacy of our methods for boosting accuracies over a\nrelatively short token horizon. Furthermore, our full 6.3T token dataset\nmatches DCLM on MMLU, but contains four times more unique real tokens than\nDCLM. This unlocks state-of-the-art training over a long token horizon: an 8B\nparameter model trained for 15T tokens, of which 7.2T came from our dataset, is\nbetter than the Llama 3.1 8B model: +5 on MMLU, +3.1 on ARC-Challenge, and +0.5\non average across ten diverse tasks. The dataset is available at\nhttps://data.commoncrawl.org/contrib/Nemotron/Nemotron-CC/index.html"}
{"id": "2412.02819", "pdf": "https://arxiv.org/pdf/2412.02819.pdf", "abs": "https://arxiv.org/abs/2412.02819", "title": "CNNSum: Exploring Long-Context Summarization with Large Language Models in Chinese Novels", "authors": ["Lingxiao Wei", "He Yan", "Xiangju Lu", "Junmin Zhu", "Jun Wang", "Wei Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 (Findings)", "summary": "Large language models (LLMs) have been well-researched in various\nlong-context tasks. However, the scarcity of long-context summarization\ndatasets hinders progress in this area. To address this, we introduce CNNSum, a\nmulti-scale long-context summarization benchmark based on Chinese novels,\nfeaturing human-driven annotations across four subsets totaling 695 samples,\nwith lengths ranging from 16k to 128k. We benchmark numerous LLMs and conduct\ndetailed human assessments to summarize abnormal output types. Furthermore, we\nextensively explore how to improve long-context summarization. In our study:\n(1) Advanced LLMs may generate much subjective commentary, leading to vague\nsummaries. (2) Currently, long-context summarization mainly relies on memory\nability. The advantages of Large LLMs are hard to utilize, thus small LLMs are\nmore cost-effective. (3) Different prompt types paired with various version\nmodels may cause large performance gaps. In further fine-tuning, these can be\nmitigated, and the Base version models perform better. (4) LLMs with RoPE-base\nscaled exhibit strong extrapolation potential; using short-context data can\nsignificantly improve long-context summarization performance. However, further\napplying other interpolation methods requires careful selection. (5) CNNSum\nprovides more reliable evaluation results than other benchmarks. We release\nCNNSum to advance future research.(https://github.com/CxsGhost/CNNSum)"}
{"id": "2412.02830", "pdf": "https://arxiv.org/pdf/2412.02830.pdf", "abs": "https://arxiv.org/abs/2412.02830", "title": "RARE: Retrieval-Augmented Reasoning Enhancement for Large Language Models", "authors": ["Hieu Tran", "Zonghai Yao", "Junda Wang", "Yifan Zhang", "Zhichao Yang", "Hong Yu"], "categories": ["cs.CL"], "comment": "Proceedings of ACL 2025 (main track)", "summary": "This work introduces RARE (Retrieval-Augmented Reasoning Enhancement), a\nversatile extension to the mutual reasoning framework (rStar), aimed at\nenhancing reasoning accuracy and factual integrity across large language models\n(LLMs) for complex, knowledge-intensive tasks such as commonsense and medical\nreasoning. RARE incorporates two innovative actions within the Monte Carlo Tree\nSearch (MCTS) framework: A6, which generates search queries based on the\ninitial problem statement, performs information retrieval using those queries,\nand augments reasoning with the retrieved data to formulate the final answer;\nand A7, which leverages information retrieval specifically for generated\nsub-questions and re-answers these sub-questions with the relevant contextual\ninformation. Additionally, a Retrieval-Augmented Factuality Scorer is proposed\nto replace the original discriminator, prioritizing reasoning paths that meet\nhigh standards of factuality. Experimental results with LLaMA 3.1 show that\nRARE enables open-source LLMs to achieve competitive performance with top\nopen-source models like GPT-4 and GPT-4o. This research establishes RARE as a\nscalable solution for improving LLMs in domains where logical coherence and\nfactual integrity are critical."}
{"id": "2412.05710", "pdf": "https://arxiv.org/pdf/2412.05710.pdf", "abs": "https://arxiv.org/abs/2412.05710", "title": "PromptRefine: Enhancing Few-Shot Performance on Low-Resource Indic Languages with Example Selection from Related Example Banks", "authors": ["Soumya Suvra Ghosal", "Soumyabrata Pal", "Koyel Mukherjee", "Dinesh Manocha"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Accepted at NAACL 2025", "summary": "Large Language Models (LLMs) have recently demonstrated impressive few-shot\nlearning capabilities through in-context learning (ICL). However, ICL\nperformance is highly dependent on the choice of few-shot demonstrations,\nmaking the selection of the most optimal examples a persistent research\nchallenge. This issue is further amplified in low-resource Indic languages,\nwhere the scarcity of ground-truth data complicates the selection process. In\nthis work, we propose PromptRefine, a novel Alternating Minimization approach\nfor example selection that improves ICL performance on low-resource Indic\nlanguages. PromptRefine leverages auxiliary example banks from related\nhigh-resource Indic languages and employs multi-task learning techniques to\nalign language-specific retrievers, enabling effective cross-language\nretrieval. Additionally, we incorporate diversity in the selected examples to\nenhance generalization and reduce bias. Through comprehensive evaluations on\nfour text generation tasks -- Cross-Lingual Question Answering, Multilingual\nQuestion Answering, Machine Translation, and Cross-Lingual Summarization using\nstate-of-the-art LLMs such as LLAMA-3.1-8B, LLAMA-2-7B, Qwen-2-7B, and\nQwen-2.5-7B, we demonstrate that PromptRefine significantly outperforms\nexisting frameworks for retrieving examples."}
{"id": "2412.05862", "pdf": "https://arxiv.org/pdf/2412.05862.pdf", "abs": "https://arxiv.org/abs/2412.05862", "title": "Domain-Specific Translation with Open-Source Large Language Models: Resource-Oriented Analysis", "authors": ["Aman Kassahun Wassie", "Mahdi Molaei", "Yasmin Moslem"], "categories": ["cs.CL"], "comment": null, "summary": "In this work, we compare the domain-specific translation performance of\nopen-source autoregressive decoder-only large language models (LLMs) with\ntask-oriented machine translation (MT) models. Our experiments focus on the\nmedical domain and cover four language directions with varied resource\navailability: English-to-French, English-to-Portuguese, English-to-Swahili, and\nSwahili-to-English. Despite recent advancements, LLMs demonstrate a significant\nquality gap in specialized translation compared to multilingual encoder-decoder\nMT models such as NLLB-200. Our results indicate that NLLB-200 3.3B outperforms\nall evaluated LLMs in the 7-8B parameter range across three out of the four\nlanguage directions. While fine-tuning improves the performance of LLMs such as\nMistral and Llama, these models still underperform compared to fine-tuned\nNLLB-200 3.3B models. Our findings highlight the ongoing need for specialized\nMT models to achieve high-quality domain-specific translation, especially in\nmedium-resource and low-resource settings. Moreover, the superior performance\nof larger LLMs over their 8B variants suggests potential value in pre-training\ndomain-specific medium-sized language models, employing targeted data selection\nand knowledge distillation approaches to enhance both quality and efficiency in\nspecialized translation tasks."}
{"id": "2412.08985", "pdf": "https://arxiv.org/pdf/2412.08985.pdf", "abs": "https://arxiv.org/abs/2412.08985", "title": "KnowShiftQA: How Robust are RAG Systems when Textbook Knowledge Shifts in K-12 Education?", "authors": ["Tianshi Zheng", "Weihan Li", "Jiaxin Bai", "Weiqi Wang", "Yangqiu Song"], "categories": ["cs.CL"], "comment": "ACL 2025 Main", "summary": "Retrieval-Augmented Generation (RAG) systems show remarkable potential as\nquestion answering tools in the K-12 Education domain, where knowledge is\ntypically queried within the restricted scope of authoritative textbooks.\nHowever, discrepancies between these textbooks and the parametric knowledge\ninherent in Large Language Models (LLMs) can undermine the effectiveness of RAG\nsystems. To systematically investigate RAG system robustness against such\nknowledge discrepancies, we introduce KnowShiftQA. This novel question\nanswering dataset simulates these discrepancies by applying deliberate\nhypothetical knowledge updates to both answers and source documents, reflecting\nhow textbook knowledge can shift. KnowShiftQA comprises 3,005 questions across\nfive subjects, designed with a comprehensive question typology focusing on\ncontext utilization and knowledge integration. Our extensive experiments on\nretrieval and question answering performance reveal that most RAG systems\nsuffer a substantial performance drop when faced with these knowledge\ndiscrepancies. Furthermore, questions requiring the integration of contextual\n(textbook) knowledge with parametric (LLM) knowledge pose a significant\nchallenge to current LLMs."}
{"id": "2412.09879", "pdf": "https://arxiv.org/pdf/2412.09879.pdf", "abs": "https://arxiv.org/abs/2412.09879", "title": "On the Limit of Language Models as Planning Formalizers", "authors": ["Cassie Huang", "Li Zhang"], "categories": ["cs.CL"], "comment": "In ACL 2025 main conference", "summary": "Large Language Models have been found to create plans that are neither\nexecutable nor verifiable in grounded environments. An emerging line of work\ndemonstrates success in using the LLM as a formalizer to generate a formal\nrepresentation of the planning domain in some language, such as Planning Domain\nDefinition Language (PDDL). This formal representation can be deterministically\nsolved to find a plan. We systematically evaluate this methodology while\nbridging some major gaps. While previous work only generates a partial PDDL\nrepresentation, given templated, and therefore unrealistic environment\ndescriptions, we generate the complete representation given descriptions of\nvarious naturalness levels. Among an array of observations critical to improve\nLLMs' formal planning abilities, we note that most large enough models can\neffectively formalize descriptions as PDDL, outperforming those directly\ngenerating plans, while being robust to lexical perturbation. As the\ndescriptions become more natural-sounding, we observe a decrease in performance\nand provide detailed error analysis."}
{"id": "2412.10424", "pdf": "https://arxiv.org/pdf/2412.10424.pdf", "abs": "https://arxiv.org/abs/2412.10424", "title": "LLM-as-an-Interviewer: Beyond Static Testing Through Dynamic LLM Evaluation", "authors": ["Eunsu Kim", "Juyoung Suk", "Seungone Kim", "Niklas Muennighoff", "Dongkwan Kim", "Alice Oh"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We introduce LLM-as-an-Interviewer, a novel paradigm for evaluating large\nlanguage models (LLMs). This approach leverages multi-turn interactions where\nthe LLM interviewer actively provides feedback on responses and poses follow-up\nquestions to the evaluated LLM. At the start of the interview, the LLM\ninterviewer dynamically modifies datasets to generate initial questions,\nmitigating data contamination. We apply the LLM-as-an-Interviewer framework to\nevaluate six models on the MATH and DepthQA tasks. Our results show that the\nframework effectively provides insights into LLM performance, including the\nquality of initial responses, adaptability to feedback, and ability to address\nfollow-up queries like clarification or additional knowledge requests. The\nframework also addresses key limitations of conventional methods like\nLLM-as-a-Judge, including verbosity bias and inconsistency across runs.\nFinally, we propose the Interview Report, which aggregates insights from the\ninterview process, providing examples and a comprehensive analysis of the LLM's\nstrengths and weaknesses. This report offers a detailed snapshot of the model's\nreal-world applicability. The code for our framework is publicly available at\nhttps://github.com/interview-eval/."}
{"id": "2412.11388", "pdf": "https://arxiv.org/pdf/2412.11388.pdf", "abs": "https://arxiv.org/abs/2412.11388", "title": "INTERACT: Enabling Interactive, Question-Driven Learning in Large Language Models", "authors": ["Aum Kendapadi", "Kerem Zaman", "Rakesh R. Menon", "Shashank Srivastava"], "categories": ["cs.CL"], "comment": "31 pages, 8 figures, 15 tables, 10 listings", "summary": "Large language models (LLMs) excel at answering questions but remain passive\nlearners-absorbing static data without the ability to question and refine\nknowledge. This paper explores how LLMs can transition to interactive,\nquestion-driven learning through student-teacher dialogues. We introduce\nINTERACT (INTERactive learning for Adaptive Concept Transfer), a framework in\nwhich a \"student\" LLM engages a \"teacher\" LLM through iterative inquiries to\nacquire knowledge across 1,347 contexts, including song lyrics, news articles,\nmovie plots, academic papers, and images. Our experiments show that across a\nwide range of scenarios and LLM architectures, interactive learning\nconsistently enhances performance, achieving up to a 25% improvement, with\n'cold-start' student models matching static learning baselines in as few as\nfive dialogue turns. Interactive setups can also mitigate the disadvantages of\nweaker teachers, showcasing the robustness of question-driven learning."}
{"id": "2412.11940", "pdf": "https://arxiv.org/pdf/2412.11940.pdf", "abs": "https://arxiv.org/abs/2412.11940", "title": "The Impact of Token Granularity on the Predictive Power of Language Model Surprisal", "authors": ["Byung-Doh Oh", "William Schuler"], "categories": ["cs.CL"], "comment": "ACL 2025; results with Natural Stories alignment issue corrected\n  (commit 4700daa)", "summary": "Word-by-word language model surprisal is often used to model the incremental\nprocessing of human readers, which raises questions about how various choices\nin language modeling influence its predictive power. One factor that has been\noverlooked in cognitive modeling is the granularity of subword tokens, which\nexplicitly encodes information about word length and frequency, and ultimately\ninfluences the quality of vector representations that are learned. This paper\npresents experiments that manipulate the token granularity and evaluate its\nimpact on the ability of surprisal to account for processing difficulty of\nnaturalistic text and garden-path constructions. Experiments with naturalistic\nreading times reveal a substantial influence of token granularity on surprisal,\nwith tokens defined by a vocabulary size of 8,000 resulting in surprisal that\nis most predictive. In contrast, on garden-path constructions, language models\ntrained on coarser-grained tokens generally assigned higher surprisal to\ncritical regions, suggesting a greater sensitivity to garden-path effects than\npreviously reported. Taken together, these results suggest a large role of\ntoken granularity on the quality of language model surprisal for cognitive\nmodeling."}
{"id": "2412.11965", "pdf": "https://arxiv.org/pdf/2412.11965.pdf", "abs": "https://arxiv.org/abs/2412.11965", "title": "Inferring Functionality of Attention Heads from their Parameters", "authors": ["Amit Elhelo", "Mor Geva"], "categories": ["cs.CL"], "comment": "ACL 2025 Main", "summary": "Attention heads are one of the building blocks of large language models\n(LLMs). Prior work on investigating their operation mostly focused on analyzing\ntheir behavior during inference for specific circuits or tasks. In this work,\nwe seek a comprehensive mapping of the operations they implement in a model. We\npropose MAPS (Mapping Attention head ParameterS), an efficient framework that\ninfers the functionality of attention heads from their parameters, without any\nmodel training or inference. We showcase the utility of MAPS for answering two\ntypes of questions: (a) given a predefined operation, mapping how strongly\nheads across the model implement it, and (b) given an attention head, inferring\nits salient functionality. Evaluating MAPS on 20 operations across 6 popular\nLLMs shows its estimations correlate with the head's outputs during inference\nand are causally linked to the model's predictions. Moreover, its mappings\nreveal attention heads of certain operations that were overlooked in previous\nstudies, and valuable insights on function universality and architecture biases\nin LLMs. Next, we present an automatic pipeline and analysis that leverage MAPS\nto characterize the salient operations of a given head. Our pipeline produces\nplausible operation descriptions for most heads, as assessed by human judgment,\nwhile revealing diverse operations."}
{"id": "2412.12276", "pdf": "https://arxiv.org/pdf/2412.12276.pdf", "abs": "https://arxiv.org/abs/2412.12276", "title": "Emergence and Effectiveness of Task Vectors in In-Context Learning: An Encoder Decoder Perspective", "authors": ["Seungwook Han", "Jinyeop Song", "Jeff Gore", "Pulkit Agrawal"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "https://charming-centaur-089.notion.site/Emergence-and-Effectiveness-of-Task-Vectors-in-In-Context-Learning-An-Encoder-Decoder-Perspective-2054664a1d59814f8401cded3332fce4", "summary": "Autoregressive transformers exhibit adaptive learning through in-context\nlearning (ICL), which begs the question of how. Prior works have shown that\ntransformers represent the ICL tasks as vectors in their representations. In\nthis paper, we leverage the encoding-decoding framework to study how\ntransformers form task vectors during pretraining and how their task encoding\nquality predicts ICL task performance. On synthetic ICL tasks, we analyze the\ntraining dynamics of a small transformer and report the coupled emergence of\ntask encoding and decoding. As the model learns to encode different latent\ntasks (e.g., \"Finding the first noun in a sentence.\") into distinct, separable\nrepresentations, it concurrently builds conditional decoding algorithms and\nimproves its ICL performance. We validate this phenomenon across pretrained\nmodels of varying scales (Gemma-2 2B/9B/27B, Llama-3.1 8B/70B) and over the\ncourse of pretraining in OLMo-7B. Further, we demonstrate that the quality of\ntask encoding inferred from representations predicts ICL performance, and that,\nsurprisingly, finetuning the earlier layers can improve the task encoding and\nperformance more than finetuning the latter layers. Our empirical insights shed\nlight into better understanding the success and failure modes of large language\nmodels via their representations."}
{"id": "2412.12569", "pdf": "https://arxiv.org/pdf/2412.12569.pdf", "abs": "https://arxiv.org/abs/2412.12569", "title": "Quantifying Lexical Semantic Shift via Unbalanced Optimal Transport", "authors": ["Ryo Kishino", "Hiroaki Yamagiwa", "Ryo Nagata", "Sho Yokoi", "Hidetoshi Shimodaira"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Lexical semantic change detection aims to identify shifts in word meanings\nover time. While existing methods using embeddings from a diachronic corpus\npair estimate the degree of change for target words, they offer limited insight\ninto changes at the level of individual usage instances. To address this, we\napply Unbalanced Optimal Transport (UOT) to sets of contextualized word\nembeddings, capturing semantic change through the excess and deficit in the\nalignment between usage instances. In particular, we propose Sense Usage Shift\n(SUS), a measure that quantifies changes in the usage frequency of a word sense\nat each usage instance. By leveraging SUS, we demonstrate that several\nchallenges in semantic change detection can be addressed in a unified manner,\nincluding quantifying instance-level semantic change and word-level tasks such\nas measuring the magnitude of semantic change and the broadening or narrowing\nof meaning."}
{"id": "2412.13169", "pdf": "https://arxiv.org/pdf/2412.13169.pdf", "abs": "https://arxiv.org/abs/2412.13169", "title": "Algorithmic Fidelity of Large Language Models in Generating Synthetic German Public Opinions: A Case Study", "authors": ["Bolei Ma", "Berk Yoztyurk", "Anna-Carolina Haensch", "Xinpeng Wang", "Markus Herklotz", "Frauke Kreuter", "Barbara Plank", "Matthias Assenmacher"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "In recent research, large language models (LLMs) have been increasingly used\nto investigate public opinions. This study investigates the algorithmic\nfidelity of LLMs, i.e., the ability to replicate the socio-cultural context and\nnuanced opinions of human participants. Using open-ended survey data from the\nGerman Longitudinal Election Studies (GLES), we prompt different LLMs to\ngenerate synthetic public opinions reflective of German subpopulations by\nincorporating demographic features into the persona prompts. Our results show\nthat Llama performs better than other LLMs at representing subpopulations,\nparticularly when there is lower opinion diversity within those groups. Our\nfindings further reveal that the LLM performs better for supporters of\nleft-leaning parties like The Greens and The Left compared to other parties,\nand matches the least with the right-party AfD. Additionally, the inclusion or\nexclusion of specific variables in the prompts can significantly impact the\nmodels' predictions. These findings underscore the importance of aligning LLMs\nto more effectively model diverse public opinions while minimizing political\nbiases and enhancing robustness in representativeness."}
{"id": "2412.13378", "pdf": "https://arxiv.org/pdf/2412.13378.pdf", "abs": "https://arxiv.org/abs/2412.13378", "title": "SummExecEdit: A Factual Consistency Benchmark in Summarization with Executable Edits", "authors": ["Onkar Thorat", "Philippe Laban", "Chien-Sheng Wu"], "categories": ["cs.CL"], "comment": null, "summary": "Detecting factual inconsistencies in summarization is critical, yet existing\nbenchmarks lack the necessary challenge and interpretability for robust\nevaluation. In this paper, we introduce SummExecEdit, a novel pipeline and\nbenchmark leveraging executable edits to assess models on their ability to both\ndetect factual errors and provide accurate explanations. The top-performing\nmodel, Claude3-Opus, achieves a joint detection and explanation score of only\n0.49 in our benchmark, with individual scores of 0.67 for detection and 0.73\nfor explanation. We conduct detailed evaluations to assess the current state of\nmodels in this field and find that more than half of the 20+ LLMs in our study\nstruggle with over 30% of the SummExecEdit benchmark. Additionally, we identify\nfour primary types of explanation errors, with 45.4% of them involving a focus\non completely unrelated parts of the summary."}
{"id": "2412.13602", "pdf": "https://arxiv.org/pdf/2412.13602.pdf", "abs": "https://arxiv.org/abs/2412.13602", "title": "GAMEBoT: Transparent Assessment of LLM Reasoning in Games", "authors": ["Wenye Lin", "Jonathan Roberts", "Yunhan Yang", "Samuel Albanie", "Zongqing Lu", "Kai Han"], "categories": ["cs.CL"], "comment": "9 pages, ACL 2025", "summary": "Large Language Models (LLMs) are increasingly deployed in real-world\napplications that demand complex reasoning. To track progress, robust\nbenchmarks are required to evaluate their capabilities beyond superficial\npattern recognition. However, current LLM reasoning benchmarks often face\nchallenges such as insufficient interpretability, performance saturation or\ndata contamination. To address these challenges, we introduce GAMEBoT, a gaming\narena designed for rigorous and transparent assessment of LLM reasoning\ncapabilities. GAMEBoT decomposes complex reasoning in games into predefined\nmodular subproblems. This decomposition allows us to design a suite of\nChain-of-Thought (CoT) prompts that leverage domain knowledge to guide LLMs in\naddressing these subproblems before action selection. Furthermore, we develop a\nsuite of rule-based algorithms to generate ground truth for these subproblems,\nenabling rigorous validation of the LLMs' intermediate reasoning steps. This\napproach facilitates evaluation of both the quality of final actions and the\naccuracy of the underlying reasoning process. GAMEBoT also naturally alleviates\nthe risk of data contamination through dynamic games and head-to-head LLM\ncompetitions. We benchmark 17 prominent LLMs across eight games, encompassing\nvarious strategic abilities and game characteristics. Our results suggest that\nGAMEBoT presents a significant challenge, even when LLMs are provided with\ndetailed CoT prompts. Project page: https://visual-ai.github.io/gamebot"}
{"id": "2412.13649", "pdf": "https://arxiv.org/pdf/2412.13649.pdf", "abs": "https://arxiv.org/abs/2412.13649", "title": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation", "authors": ["Jialong Wu", "Zhenglin Wang", "Linhai Zhang", "Yilong Lai", "Yulan He", "Deyu Zhou"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods."}
{"id": "2412.14050", "pdf": "https://arxiv.org/pdf/2412.14050.pdf", "abs": "https://arxiv.org/abs/2412.14050", "title": "Cross-Lingual Transfer of Debiasing and Detoxification in Multilingual LLMs: An Extensive Investigation", "authors": ["Vera Neplenbroek", "Arianna Bisazza", "Raquel Fern√°ndez"], "categories": ["cs.CL"], "comment": "Accepted to the Findings of ACL 2025", "summary": "Recent generative large language models (LLMs) show remarkable performance in\nnon-English languages, but when prompted in those languages they tend to\nexpress higher harmful social biases and toxicity levels. Prior work has shown\nthat finetuning on specialized datasets can mitigate this behavior, and doing\nso in English can transfer to other languages. In this work, we investigate the\nimpact of different finetuning methods on the model's bias and toxicity, but\nalso on its ability to produce fluent and diverse text. We reduce biases by\nfinetuning on curated non-harmful text, but find only direct preference\noptimization to be effective for mitigating toxicity. The mitigation caused by\napplying these methods in English also transfers to non-English languages. We\nfind evidence that the extent to which transfer takes place can be predicted by\nthe amount of data in a given language present in the model's pretraining data.\nHowever, this transfer of bias and toxicity mitigation often comes at the\nexpense of decreased language generation ability in non-English languages,\nhighlighting the importance of developing language-specific bias and toxicity\nmitigation methods."}
{"id": "2412.15268", "pdf": "https://arxiv.org/pdf/2412.15268.pdf", "abs": "https://arxiv.org/abs/2412.15268", "title": "Enhancing LLM-based Hatred and Toxicity Detection with Meta-Toxic Knowledge Graph", "authors": ["Yibo Zhao", "Jiapeng Zhu", "Can Xu", "Yao Liu", "Xiang Li"], "categories": ["cs.CL", "cs.AI"], "comment": "8 pages of content", "summary": "The rapid growth of social media platforms has raised significant concerns\nregarding online content toxicity. When Large Language Models (LLMs) are used\nfor toxicity detection, two key challenges emerge: 1) the absence of\ndomain-specific toxic knowledge leads to false negatives; 2) the excessive\nsensitivity of LLMs to toxic speech results in false positives, limiting\nfreedom of speech. To address these issues, we propose a novel method called\nMetaTox, leveraging graph search on a meta-toxic knowledge graph to enhance\nhatred and toxicity detection. First, we construct a comprehensive meta-toxic\nknowledge graph by utilizing LLMs to extract toxic information through a\nthree-step pipeline, with toxic benchmark datasets serving as corpora. Second,\nwe query the graph via retrieval and ranking processes to supplement accurate,\nrelevant toxic knowledge. Extensive experiments and in-depth case studies\nacross multiple datasets demonstrate that our MetaTox significantly decreases\nthe false positive rate while boosting overall toxicity detection performance.\nOur code is available at https://github.com/YiboZhao624/MetaTox."}
{"id": "2412.18053", "pdf": "https://arxiv.org/pdf/2412.18053.pdf", "abs": "https://arxiv.org/abs/2412.18053", "title": "Neuron Empirical Gradient: Discovering and Quantifying Neurons Global Linear Controllability", "authors": ["Xin Zhao", "Zehui Jiang", "Naoki Yoshinaga"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 Main, 32 pages", "summary": "While feed-forward neurons in pre-trained language models (PLMs) can encode\nknowledge, past research targeted a small subset of neurons that heavily\ninfluence outputs. This leaves the broader role of neuron activations unclear,\nlimiting progress in areas like knowledge editing. We uncover a global linear\nrelationship between neuron activations and outputs using neuron interventions\non a knowledge probing dataset. The gradient of this linear relationship, which\nwe call the neuron empirical gradient (NEG), captures how changes in\nactivations affect predictions. To compute NEG efficiently, we propose\nNeurGrad, enabling large-scale analysis of neuron behavior in PLMs. We also\nshow that NEG effectively captures language skills across diverse prompts\nthrough skill neuron probing. Experiments on MCEval8k, a multi-genre\nmultiple-choice knowledge benchmark, support NEG's ability to represent model\nknowledge. Further analysis highlights the key properties of NEG-based skill\nrepresentation: efficiency, robustness, flexibility, and interdependency. The\ncode and data are released."}
{"id": "2412.18069", "pdf": "https://arxiv.org/pdf/2412.18069.pdf", "abs": "https://arxiv.org/abs/2412.18069", "title": "Improving Factuality with Explicit Working Memory", "authors": ["Mingda Chen", "Yang Li", "Karthik Padthe", "Rulin Shao", "Alicia Sun", "Luke Zettlemoyer", "Gargi Ghosh", "Wen-tau Yih"], "categories": ["cs.CL"], "comment": "ACL 2025 Camera Ready", "summary": "Large language models can generate factually inaccurate content, a problem\nknown as hallucination. Recent works have built upon retrieved-augmented\ngeneration to improve factuality through iterative prompting but these methods\nare limited by the traditional RAG design. To address these challenges, we\nintroduce EWE (Explicit Working Memory), a novel approach that enhances\nfactuality in long-form text generation by integrating a working memory that\nreceives real-time feedback from external resources. The memory is refreshed\nbased on online fact-checking and retrieval feedback, allowing EWE to rectify\nfalse claims during the generation process and ensure more accurate and\nreliable outputs. Our experiments demonstrate that Ewe outperforms strong\nbaselines on four fact-seeking long-form generation datasets, increasing the\nfactuality metric, VeriScore, by 2 to 6 points absolute without sacrificing the\nhelpfulness of the responses. Further analysis reveals that the design of rules\nfor memory updates, configurations of memory units, and the quality of the\nretrieval datastore are crucial factors for influencing model performance."}
{"id": "2412.18120", "pdf": "https://arxiv.org/pdf/2412.18120.pdf", "abs": "https://arxiv.org/abs/2412.18120", "title": "Do Language Models Understand the Cognitive Tasks Given to Them? Investigations with the N-Back Paradigm", "authors": ["Xiaoyang Hu", "Richard L. Lewis"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings", "summary": "Cognitive tasks originally developed for humans are now increasingly used to\nstudy language models. While applying these tasks is often straightforward,\ninterpreting their results can be challenging. In particular, when a model\nunderperforms, it is often unclear whether this results from a limitation in\nthe cognitive ability being tested or a failure to understand the task itself.\nA recent study argues that GPT 3.5's declining performance on 2-back and 3-back\ntasks reflects a working memory capacity limit similar to humans (Gong et al.,\n2024). By analyzing a range of open-source language models of varying\nperformance levels on these tasks, we show that the poor performance is due at\nleast in part to a limitation in task comprehension and task set maintenance.\nWe challenge the best-performing model with progressively harder versions of\nthe task (up to 10-back) and experiment with alternative prompting strategies,\nbefore analyzing model attentions. Our larger aim is to contribute to the\nongoing conversation around refining methodologies for the cognitive evaluation\nof language models."}
{"id": "2412.18151", "pdf": "https://arxiv.org/pdf/2412.18151.pdf", "abs": "https://arxiv.org/abs/2412.18151", "title": "CoAM: Corpus of All-Type Multiword Expressions", "authors": ["Yusuke Ide", "Joshua Tanner", "Adam Nohejl", "Jacob Hoffman", "Justin Vasselli", "Hidetaka Kamigaito", "Taro Watanabe"], "categories": ["cs.CL"], "comment": "ACL 2025 main", "summary": "Multiword expressions (MWEs) refer to idiomatic sequences of multiple words.\nMWE identification, i.e., detecting MWEs in text, can play a key role in\ndownstream tasks such as machine translation, but existing datasets for the\ntask are inconsistently annotated, limited to a single type of MWE, or limited\nin size. To enable reliable and comprehensive evaluation, we created CoAM:\nCorpus of All-Type Multiword Expressions, a dataset of 1.3K sentences\nconstructed through a multi-step process to enhance data quality consisting of\nhuman annotation, human review, and automated consistency checking.\nAdditionally, for the first time in a dataset of MWE identification, CoAM's\nMWEs are tagged with MWE types, such as Noun and Verb, enabling fine-grained\nerror analysis. Annotations for CoAM were collected using a new interface\ncreated with our interface generator, which allows easy and flexible annotation\nof MWEs in any form. Through experiments using CoAM, we find that a fine-tuned\nlarge language model outperforms MWEasWSD, which achieved the state-of-the-art\nperformance on the DiMSUM dataset. Furthermore, analysis using our MWE type\ntagged data reveals that Verb MWEs are easier than Noun MWEs to identify across\napproaches."}
{"id": "2412.18547", "pdf": "https://arxiv.org/pdf/2412.18547.pdf", "abs": "https://arxiv.org/abs/2412.18547", "title": "Token-Budget-Aware LLM Reasoning", "authors": ["Tingxu Han", "Zhenting Wang", "Chunrong Fang", "Shiyu Zhao", "Shiqing Ma", "Zhenyu Chen"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Reasoning is critical for large language models (LLMs) to excel in a wide\nrange of tasks. While methods like Chain-of-Thought (CoT) reasoning and enhance\nLLM performance by decomposing problems into intermediate steps, they also\nincur significant overhead in token usage, leading to increased costs. We find\nthat the reasoning process of current LLMs is unnecessarily lengthy and it can\nbe compressed by including a reasonable token budget in the prompt, but the\nchoice of token budget plays a crucial role in the actual compression\neffectiveness. We then propose a token-budget-aware LLM reasoning framework\nthat dynamically adjusts the number of reasoning tokens based on the reasoning\ncomplexity of each problem. Experiments show that our method effectively\nreduces token costs in CoT reasoning with only a slight performance reduction,\noffering a practical solution to balance efficiency and accuracy in LLM\nreasoning. Code: https://github.com/GeniusHTX/TALE"}
{"id": "2412.20057", "pdf": "https://arxiv.org/pdf/2412.20057.pdf", "abs": "https://arxiv.org/abs/2412.20057", "title": "\"My life is miserable, have to sign 500 autographs everyday\": Exposing Humblebragging, the Brags in Disguise", "authors": ["Sharath Naganna", "Saprativa Bhattacharjee", "Biplab Banerjee", "Pushpak Bhattacharyya"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Findings", "summary": "Humblebragging is a phenomenon in which individuals present self-promotional\nstatements under the guise of modesty or complaints. For example, a statement\nlike, \"Ugh, I can't believe I got promoted to lead the entire team. So\nstressful!\", subtly highlights an achievement while pretending to be\ncomplaining. Detecting humblebragging is important for machines to better\nunderstand the nuances of human language, especially in tasks like sentiment\nanalysis and intent recognition. However, this topic has not yet been studied\nin computational linguistics. For the first time, we introduce the task of\nautomatically detecting humblebragging in text. We formalize the task by\nproposing a 4-tuple definition of humblebragging and evaluate machine learning,\ndeep learning, and large language models (LLMs) on this task, comparing their\nperformance with humans. We also create and release a dataset called HB-24,\ncontaining 3,340 humblebrags generated using GPT-4o. Our experiments show that\ndetecting humblebragging is non-trivial, even for humans. Our best model\nachieves an F1-score of 0.88. This work lays the foundation for further\nexploration of this nuanced linguistic phenomenon and its integration into\nbroader natural language understanding systems."}
{"id": "2412.20584", "pdf": "https://arxiv.org/pdf/2412.20584.pdf", "abs": "https://arxiv.org/abs/2412.20584", "title": "Towards Neural No-Resource Language Translation: A Comparative Evaluation of Approaches", "authors": ["Madhavendra Thakur"], "categories": ["cs.CL"], "comment": "Presented at the Columbia AI Summit 2025", "summary": "No-resource languages - those with minimal or no digital representation -\npose unique challenges for machine translation (MT). Unlike low-resource\nlanguages, which rely on limited but existent corpora, no-resource languages\noften have fewer than 100 sentences available for training. This work explores\nthe problem of no-resource translation through three distinct workflows:\nfine-tuning of translation-specific models, in-context learning with large\nlanguage models (LLMs) using chain-of-reasoning prompting, and direct prompting\nwithout reasoning. Using Owens Valley Paiute as a case study, we demonstrate\nthat no-resource translation demands fundamentally different approaches from\nlow-resource scenarios, as traditional approaches to machine translation, such\nas those that work for low-resource languages, fail. Empirical results reveal\nthat, although traditional approaches fail, the in-context learning\ncapabilities of general-purpose large language models enable no-resource\nlanguage translation that outperforms low-resource translation approaches and\nrivals human translations (BLEU 0.45-0.6); specifically, chain-of-reasoning\nprompting outperforms other methods for larger corpora, while direct prompting\nexhibits advantages in smaller datasets. As these approaches are\nlanguage-agnostic, they have potential to be generalized to translation tasks\nfrom a wide variety of no-resource languages without expert input. These\nfindings establish no-resource translation as a distinct paradigm requiring\ninnovative solutions, providing practical and theoretical insights for language\npreservation."}
{"id": "2501.00759", "pdf": "https://arxiv.org/pdf/2501.00759.pdf", "abs": "https://arxiv.org/abs/2501.00759", "title": "Enhancing Transformers for Generalizable First-Order Logical Entailment", "authors": ["Tianshi Zheng", "Jiazheng Wang", "Zihao Wang", "Jiaxin Bai", "Hang Yin", "Zheye Deng", "Yangqiu Song", "Jianxin Li"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Main", "summary": "Transformers, as the fundamental deep learning architecture, have\ndemonstrated great capability in reasoning. This paper studies the\ngeneralizable first-order logical reasoning ability of transformers with their\nparameterized knowledge and how to improve it. Transformers' capability of\nfirst-order reasoning is further captured by whether they can conduct\nfirst-order logical entailment, which is quantitatively measured by their\nperformance in answering knowledge graph queries. We establish the connections\nbetween (1) two types of distribution shifts studied in out-of-distribution\ngeneralization and (2) unseen knowledge and query settings discussed in the\ntask of knowledge graph query answering, which makes it possible to\ncharacterize the fine-grained generalizability. Results on our comprehensive\ndataset showed that transformers outperform previous methods designed\nparticularly for this task and provided detailed empirical evidence about the\nimpact of the input query syntax, token embedding, and transformer\narchitectures on the reasoning capability of transformers. Interestingly, our\nresults revealed the mismatch of positional encoding and other design choices\nof transformer architectures in previous practices. Motivated by this, we\npropose TEGA, a logic-aware architecture that significantly improves the\nperformance in generalizable first-order logical entailment."}
{"id": "2501.00982", "pdf": "https://arxiv.org/pdf/2501.00982.pdf", "abs": "https://arxiv.org/abs/2501.00982", "title": "Are LLMs effective psychological assessors? Leveraging adaptive RAG for interpretable mental health screening through psychometric practice", "authors": ["Federico Ravenda", "Seyed Ali Bahrainian", "Andrea Raballo", "Antonietta Mira", "Noriko Kando"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In psychological practices, standardized questionnaires serve as essential\ntools for assessing mental health through structured, clinically-validated\nquestions (i.e., items). While social media platforms offer rich data for\nmental health screening, computational approaches often bypass these\nestablished clinical assessment tools in favor of black-box classification. We\npropose a novel questionnaire-guided screening framework that bridges\npsychological practice and computational methods through adaptive\nRetrieval-Augmented Generation (\\textit{aRAG}). Our approach links unstructured\nsocial media content and standardized clinical assessments by retrieving\nrelevant posts for each questionnaire item and using Large Language Models\n(LLMs) to complete validated psychological instruments. Our findings\ndemonstrate two key advantages of questionnaire-guided screening: First, when\ncompleting the Beck Depression Inventory-II (BDI-II), our approach matches or\noutperforms state-of-the-art performance on Reddit-based benchmarks without\nrequiring training data. Second, we show that guiding LLMs through standardized\nquestionnaires can yield superior results compared to directly prompting them\nfor depression screening, while also providing a more interpretable assessment\nby linking model outputs to clinically validated diagnostic criteria.\nAdditionally, we show, as a proof-of-concept, how our questionnaire-based\nmethodology can be extended to other mental conditions' screening, highlighting\nthe promising role of LLMs as psychological assessors."}
{"id": "2501.01377", "pdf": "https://arxiv.org/pdf/2501.01377.pdf", "abs": "https://arxiv.org/abs/2501.01377", "title": "Improving Medical Large Vision-Language Models with Abnormal-Aware Feedback", "authors": ["Yucheng Zhou", "Lingran Song", "Jianbing Shen"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "16 pages", "summary": "Existing Medical Large Vision-Language Models (Med-LVLMs), encapsulating\nextensive medical knowledge, demonstrate excellent capabilities in\nunderstanding medical images. However, there remain challenges in visual\nlocalization in medical images, which is crucial for abnormality detection and\ninterpretation. To address these issues, we propose a novel UMed-LVLM designed\nto unveil medical abnormalities. Specifically, we collect a Medical\nAbnormalities Unveiling (MAU) dataset and propose a two-stage training method\nfor UMed-LVLM training. To collect MAU dataset, we propose a prompt method\nutilizing the GPT-4V to generate diagnoses based on identified abnormal areas\nin medical images. Moreover, the two-stage training method includes\nAbnormal-Aware Instruction Tuning and Abnormal-Aware Rewarding, comprising\nRelevance Reward, Abnormal Localization Reward and Vision Relevance Reward.\nExperimental results demonstrate that our UMed-LVLM significantly outperforms\nexisting Med-LVLMs in identifying and understanding medical abnormalities,\nachieving a 58% improvement over the baseline. In addition, this work shows\nthat enhancing the abnormality detection capabilities of Med-LVLMs\nsignificantly improves their understanding of medical images and generalization\ncapability."}
{"id": "2501.01743", "pdf": "https://arxiv.org/pdf/2501.01743.pdf", "abs": "https://arxiv.org/abs/2501.01743", "title": "Automating Legal Interpretation with LLMs: Retrieval, Generation, and Evaluation", "authors": ["Kangcheng Luo", "Quzhe Huang", "Cong Jiang", "Yansong Feng"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Main Conference", "summary": "Interpreting the law is always essential for the law to adapt to the\never-changing society. It is a critical and challenging task even for legal\npractitioners, as it requires meticulous and professional annotations and\nsummarizations by legal experts, which are admittedly time-consuming and\nexpensive to collect at scale. To alleviate the burden on legal experts, we\npropose a method for automated legal interpretation. Specifically, by emulating\ndoctrinal legal research, we introduce a novel framework, ATRIE, to address\nLegal Concept Interpretation, a typical task in legal interpretation. ATRIE\nutilizes large language models (LLMs) to AuTomatically Retrieve concept-related\ninformation, Interpret legal concepts, and Evaluate generated interpretations,\neliminating dependence on legal experts. ATRIE comprises a legal concept\ninterpreter and a legal concept interpretation evaluator. The interpreter uses\nLLMs to retrieve relevant information from previous cases and interpret legal\nconcepts. The evaluator uses performance changes on Legal Concept Entailment, a\ndownstream task we propose, as a proxy of interpretation quality. Automated and\nmultifaceted human evaluations indicate that the quality of our interpretations\nis comparable to those written by legal experts, with superior\ncomprehensiveness and readability. Although there remains a slight gap in\naccuracy, it can already assist legal practitioners in improving the efficiency\nof legal interpretation."}
{"id": "2501.02157", "pdf": "https://arxiv.org/pdf/2501.02157.pdf", "abs": "https://arxiv.org/abs/2501.02157", "title": "Personalized Graph-Based Retrieval for Large Language Models", "authors": ["Steven Au", "Cameron J. Dimacali", "Ojasmitha Pedirappagari", "Namyong Park", "Franck Dernoncourt", "Yu Wang", "Nikos Kanakaris", "Hanieh Deilamsalehy", "Ryan A. Rossi", "Nesreen K. Ahmed"], "categories": ["cs.CL"], "comment": null, "summary": "As large language models (LLMs) evolve, their ability to deliver personalized\nand context-aware responses offers transformative potential for improving user\nexperiences. Existing personalization approaches, however, often rely solely on\nuser history to augment the prompt, limiting their effectiveness in generating\ntailored outputs, especially in cold-start scenarios with sparse data. To\naddress these limitations, we propose Personalized Graph-based\nRetrieval-Augmented Generation (PGraphRAG), a framework that leverages\nuser-centric knowledge graphs to enrich personalization. By directly\nintegrating structured user knowledge into the retrieval process and augmenting\nprompts with user-relevant context, PGraphRAG enhances contextual understanding\nand output quality. We also introduce the Personalized Graph-based Benchmark\nfor Text Generation, designed to evaluate personalized text generation tasks in\nreal-world settings where user history is sparse or unavailable. Experimental\nresults show that PGraphRAG significantly outperforms state-of-the-art\npersonalization methods across diverse tasks, demonstrating the unique\nadvantages of graph-based retrieval for personalization."}
{"id": "2501.02295", "pdf": "https://arxiv.org/pdf/2501.02295.pdf", "abs": "https://arxiv.org/abs/2501.02295", "title": "Explicit vs. Implicit: Investigating Social Bias in Large Language Models through Self-Reflection", "authors": ["Yachao Zhao", "Bo Wang", "Yan Wang"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025", "summary": "Large Language Models (LLMs) have been shown to exhibit various biases and\nstereotypes in their generated content. While extensive research has\ninvestigated biases in LLMs, prior work has predominantly focused on explicit\nbias, with minimal attention to implicit bias and the relation between these\ntwo forms of bias. This paper presents a systematic framework grounded in\nsocial psychology theories to investigate and compare explicit and implicit\nbiases in LLMs. We propose a novel self-reflection-based evaluation framework\nthat operates in two phases: first measuring implicit bias through simulated\npsychological assessment methods, then evaluating explicit bias by prompting\nLLMs to analyze their own generated content. Through extensive experiments on\nadvanced LLMs across multiple social dimensions, we demonstrate that LLMs\nexhibit a substantial inconsistency between explicit and implicit biases: while\nexplicit bias manifests as mild stereotypes, implicit bias exhibits strong\nstereotypes. We further investigate the underlying factors contributing to this\nexplicit-implicit bias inconsistency, examining the effects of training data\nscale, model size, and alignment techniques. Experimental results indicate that\nwhile explicit bias declines with increased training data and model size,\nimplicit bias exhibits a contrasting upward trend. Moreover, contemporary\nalignment methods effectively suppress explicit bias but show limited efficacy\nin mitigating implicit bias."}
{"id": "2501.02460", "pdf": "https://arxiv.org/pdf/2501.02460.pdf", "abs": "https://arxiv.org/abs/2501.02460", "title": "Towards Omni-RAG: Comprehensive Retrieval-Augmented Generation for Large Language Models in Medical Applications", "authors": ["Zhe Chen", "Yusheng Liao", "Shuyang Jiang", "Pingjie Wang", "Yiqiu Guo", "Yanfeng Wang", "Yu Wang"], "categories": ["cs.CL"], "comment": "ACL 2025 Main Conference. Project website:\n  https://github.com/Jack-ZC8/Omni-RAG-Medical", "summary": "Large language models hold promise for addressing medical challenges, such as\nmedical diagnosis reasoning, research knowledge acquisition, clinical\ndecision-making, and consumer health inquiry support. However, they often\ngenerate hallucinations due to limited medical knowledge. Incorporating\nexternal knowledge is therefore critical, which necessitates multi-source\nknowledge acquisition. We address this challenge by framing it as a source\nplanning problem, which is to formulate context-appropriate queries tailored to\nthe attributes of diverse sources. Existing approaches either overlook source\nplanning or fail to achieve it effectively due to misalignment between the\nmodel's expectation of the sources and their actual content. To bridge this\ngap, we present MedOmniKB, a repository comprising multigenre and\nmulti-structured medical knowledge sources. Leveraging these sources, we\npropose the Source Planning Optimisation method, which enhances multi-source\nutilisation. Our approach involves enabling an expert model to explore and\nevaluate potential plans while training a smaller model to learn source\nalignment. Experimental results demonstrate that our method substantially\nimproves multi-source planning performance, enabling the optimised small model\nto achieve state-of-the-art results in leveraging diverse medical knowledge\nsources."}
{"id": "2501.03545", "pdf": "https://arxiv.org/pdf/2501.03545.pdf", "abs": "https://arxiv.org/abs/2501.03545", "title": "Beyond Factual Accuracy: Evaluating Coverage of Diverse Factual Information in Long-form Text Generation", "authors": ["Chris Samarinas", "Alexander Krubner", "Alireza Salemi", "Youngwoo Kim", "Hamed Zamani"], "categories": ["cs.CL"], "comment": null, "summary": "This paper presents ICAT, an evaluation framework for measuring coverage of\ndiverse factual information in long-form text generation. ICAT breaks down a\nlong output text into a list of atomic claims and not only verifies each claim\nthrough retrieval from a (reliable) knowledge source, but also computes the\nalignment between the atomic factual claims and various aspects expected to be\npresented in the output. We study three implementations of the ICAT framework,\neach with a different assumption on the availability of aspects and alignment\nmethod. By adopting data from the diversification task in the TREC Web Track\nand the ClueWeb corpus, we evaluate the ICAT framework. We demonstrate strong\ncorrelation with human judgments and provide comprehensive evaluation across\nmultiple state-of-the-art LLMs. Our framework further offers interpretable and\nfine-grained analysis of diversity and coverage. Its modular design allows for\neasy adaptation to different domains and datasets, making it a valuable tool\nfor evaluating the qualitative aspects of long-form responses produced by LLMs."}
{"id": "2501.03835", "pdf": "https://arxiv.org/pdf/2501.03835.pdf", "abs": "https://arxiv.org/abs/2501.03835", "title": "TACLR: A Scalable and Efficient Retrieval-based Method for Industrial Product Attribute Value Identification", "authors": ["Yindu Su", "Huike Zou", "Lin Sun", "Ting Zhang", "Haiyang Yang", "Liyu Chen", "David Lo", "Qingheng Zhang", "Shuguang Han", "Jufeng Chen"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Camera-ready version of the paper accepted at ACL 2025", "summary": "Product Attribute Value Identification (PAVI) involves identifying attribute\nvalues from product profiles, a key task for improving product search,\nrecommendation, and business analytics on e-commerce platforms. However,\nexisting PAVI methods face critical challenges, such as inferring implicit\nvalues, handling out-of-distribution (OOD) values, and producing normalized\noutputs. To address these limitations, we introduce Taxonomy-Aware Contrastive\nLearning Retrieval (TACLR), the first retrieval-based method for PAVI. TACLR\nformulates PAVI as an information retrieval task by encoding product profiles\nand candidate values into embeddings and retrieving values based on their\nsimilarity. It leverages contrastive training with taxonomy-aware hard negative\nsampling and employs adaptive inference with dynamic thresholds. TACLR offers\nthree key advantages: (1) it effectively handles implicit and OOD values while\nproducing normalized outputs; (2) it scales to thousands of categories, tens of\nthousands of attributes, and millions of values; and (3) it supports efficient\ninference for high-load industrial deployment. Extensive experiments on\nproprietary and public datasets validate the effectiveness and efficiency of\nTACLR. Further, it has been successfully deployed on the real-world e-commerce\nplatform Xianyu, processing millions of product listings daily with frequently\nupdated, large-scale attribute taxonomies. We release the code to facilitate\nreproducibility and future research at https://github.com/SuYindu/TACLR."}
{"id": "2501.04945", "pdf": "https://arxiv.org/pdf/2501.04945.pdf", "abs": "https://arxiv.org/abs/2501.04945", "title": "Step-by-Step Mastery: Enhancing Soft Constraint Following Ability of Large Language Models", "authors": ["Qingyu Ren", "Jie Zeng", "Qianyu He", "Jiaqing Liang", "Yanghua Xiao", "Weikang Zhou", "Zeye Sun", "Fei Yu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "It is crucial for large language models (LLMs) to follow instructions that\ninvolve multiple constraints. However, it is an unexplored area to enhance\nLLMs' ability to follow soft constraints. To bridge the gap, we initially\ndesign a pipeline to construct datasets with high-quality outputs\nautomatically. Additionally, to fully utilize the positive and negative samples\ngenerated during the data construction process, we choose Direct Preference\nOptimization (DPO) as the training method. Furthermore, taking into account the\ndifficulty of soft constraints indicated by the number of constraints, we\ndesign a curriculum learning training paradigm based on the constraint\nquantity. We experimentally evaluate the effectiveness of our methods in\nimproving LLMs' soft constraint following ability and analyze the factors\ndriving the improvements.The datasets and code are publicly available at\nhttps://github.com/Rainier-rq/FollowSoftConstraint."}
{"id": "2501.05962", "pdf": "https://arxiv.org/pdf/2501.05962.pdf", "abs": "https://arxiv.org/abs/2501.05962", "title": "Effective faking of verbal deception detection with target-aligned adversarial attacks", "authors": ["Bennett Kleinberg", "Riccardo Loconte", "Bruno Verschuere"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to Legal and Criminological Psychology (author version)", "summary": "Background: Deception detection through analysing language is a promising\navenue using both human judgments and automated machine learning judgments. For\nboth forms of credibility assessment, automated adversarial attacks that\nrewrite deceptive statements to appear truthful pose a serious threat. Methods:\nWe used a dataset of 243 truthful and 262 fabricated autobiographical stories\nin a deception detection task for humans and machine learning models. A large\nlanguage model was tasked to rewrite deceptive statements so that they appear\ntruthful. In Study 1, humans who made a deception judgment or used the\ndetailedness heuristic and two machine learning models (a fine-tuned language\nmodel and a simple n-gram model) judged original or adversarial modifications\nof deceptive statements. In Study 2, we manipulated the target alignment of the\nmodifications, i.e. tailoring the attack to whether the statements would be\nassessed by humans or computer models. Results: When adversarial modifications\nwere aligned with their target, human (d=-0.07 and d=-0.04) and machine\njudgments (51% accuracy) dropped to the chance level. When the attack was not\naligned with the target, both human heuristics judgments (d=0.30 and d=0.36)\nand machine learning predictions (63-78%) were significantly better than\nchance. Conclusions: Easily accessible language models can effectively help\nanyone fake deception detection efforts both by humans and machine learning\nmodels. Robustness against adversarial modifications for humans and machines\ndepends on that target alignment. We close with suggestions on advancing\ndeception research with adversarial attack designs and techniques."}
{"id": "2501.11463", "pdf": "https://arxiv.org/pdf/2501.11463.pdf", "abs": "https://arxiv.org/abs/2501.11463", "title": "Curiosity-Driven Reinforcement Learning from Human Feedback", "authors": ["Haoran Sun", "Yekun Chai", "Shuohuan Wang", "Yu Sun", "Hua Wu", "Haifeng Wang"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Reinforcement learning from human feedback (RLHF) has proven effective in\naligning large language models (LLMs) with human preferences, but often at the\ncost of reduced output diversity. This trade-off between diversity and\nalignment quality remains a significant challenge. Drawing inspiration from\ncuriosity-driven exploration in reinforcement learning, we introduce\ncuriosity-driven RLHF (CD-RLHF), a framework that incorporates intrinsic\nrewards for novel states, alongside traditional sparse extrinsic rewards, to\noptimize both output diversity and alignment quality. We demonstrate the\neffectiveness of CD-RLHF through extensive experiments on a range of tasks,\nincluding text summarization and instruction following. Our approach achieves\nsignificant gains in diversity on multiple diversity-oriented metrics while\nmaintaining alignment with human preferences comparable to standard RLHF. We\nmake our code publicly available at https://github.com/ernie-research/CD-RLHF."}
{"id": "2501.11549", "pdf": "https://arxiv.org/pdf/2501.11549.pdf", "abs": "https://arxiv.org/abs/2501.11549", "title": "Whose Boat Does it Float? Improving Personalization in Preference Tuning via Inferred User Personas", "authors": ["Nishant Balepur", "Vishakh Padmakumar", "Fumeng Yang", "Shi Feng", "Rachel Rudinger", "Jordan Lee Boyd-Graber"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "LLMs are aligned to follow input instructions by learning which of two\nresponses users prefer for a prompt. However, such preference data do not\nconvey why users prefer responses that are chosen or rejected, so LLMs trained\non these datasets cannot tailor responses to varied user needs. To surface\nthese parameters of personalization, we apply abductive reasoning to preference\ndata, inferring needs and interests of users, i.e., personas, that may prefer\neither response. We test this idea in two steps: Persona Inference (PI),\nabductively inferring personas of users who prefer chosen or rejected outputs,\nand Persona Tailoring (PT), training models to tailor outputs to personas from\nPI. We show: 1) LLMs infer personas accurately explaining why different users\nmay prefer both chosen or rejected outputs; 2) Training on preference data\naugmented with PI personas via PT boosts personalization and generalizes to\nsupporting user-written personas; and 3) Rejected response personas form harder\npersonalization evaluations, showing PT better aids users with uncommon\npreferences versus typical alignment methods. We argue for an abductive view of\npreferences for personalization, asking not only which response is better but\nwhen, why, and for whom."}
{"id": "2501.13125", "pdf": "https://arxiv.org/pdf/2501.13125.pdf", "abs": "https://arxiv.org/abs/2501.13125", "title": "Generating Plausible Distractors for Multiple-Choice Questions via Student Choice Prediction", "authors": ["Yooseop Lee", "Suin Kim", "Yohan Jo"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "This paper has been accepted for publication at ACL 2025", "summary": "In designing multiple-choice questions (MCQs) in education, creating\nplausible distractors is crucial for identifying students' misconceptions and\ngaps in knowledge and accurately assessing their understanding. However, prior\nstudies on distractor generation have not paid sufficient attention to\nenhancing the difficulty of distractors, resulting in reduced effectiveness of\nMCQs. This study presents a pipeline for training a model to generate\ndistractors that are more likely to be selected by students. First, we train a\npairwise ranker to reason about students' misconceptions and assess the\nrelative plausibility of two distractors. Using this model, we create a dataset\nof pairwise distractor ranks and then train a distractor generator via Direct\nPreference Optimization (DPO) to generate more plausible distractors.\nExperiments on computer science subjects (Python, DB, MLDL) demonstrate that\nour pairwise ranker effectively identifies students' potential\nmisunderstandings and achieves ranking accuracy comparable to human experts.\nFurthermore, our distractor generator outperforms several baselines in\ngenerating plausible distractors and produces questions with a higher item\ndiscrimination index (DI)."}
{"id": "2501.14956", "pdf": "https://arxiv.org/pdf/2501.14956.pdf", "abs": "https://arxiv.org/abs/2501.14956", "title": "ExPerT: Effective and Explainable Evaluation of Personalized Long-Form Text Generation", "authors": ["Alireza Salemi", "Julian Killingback", "Hamed Zamani"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Evaluating personalized text generated by large language models (LLMs) is\nchallenging, as only the LLM user, i.e., prompt author, can reliably assess the\noutput, but re-engaging the same individuals across studies is infeasible. This\npaper addresses the challenge of evaluating personalized text generation by\nintroducing ExPerT, an explainable reference-based evaluation framework. ExPerT\nleverages an LLM to extract atomic aspects and their evidence from the\ngenerated and reference texts, match the aspects, and evaluate their alignment\nbased on content and writing style -- two key attributes in personalized text\ngeneration. Additionally, ExPerT generates detailed, fine-grained explanations\nfor every step of the evaluation process, enhancing transparency and\ninterpretability. Our experiments demonstrate that ExPerT achieves a 7.2%\nrelative improvement in alignment with human judgments compared to the\nstate-of-the-art text generation evaluation methods. Furthermore, human\nevaluators rated the usability of ExPerT's explanations at 4.7 out of 5,\nhighlighting its effectiveness in making evaluation decisions more\ninterpretable."}
{"id": "2501.17182", "pdf": "https://arxiv.org/pdf/2501.17182.pdf", "abs": "https://arxiv.org/abs/2501.17182", "title": "Dialogue Systems for Emotional Support via Value Reinforcement", "authors": ["Juhee Kim", "Chunghu Mok", "Jisun Lee", "Hyang Sook Kim", "Yohan Jo"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "I.2.7"], "comment": "This paper has been accepted for publication at ACL 2025", "summary": "Emotional support dialogue systems aim to reduce help-seekers' distress and\nhelp them overcome challenges. While human values$\\unicode{x2013}$core beliefs\nthat shape an individual's priorities$\\unicode{x2013}$are increasingly\nemphasized in contemporary psychological therapy for their role in fostering\ninternal transformation and long-term emotional well-being, their integration\ninto emotional support systems remains underexplored. To bridge this gap, we\npresent a value-driven method for training emotional support dialogue systems\ndesigned to reinforce positive values in seekers. Notably, our model identifies\nwhich values to reinforce at each turn and how to do so, by leveraging online\nsupport conversations from Reddit. We evaluate the method across support\nskills, seekers' emotional intensity, and value reinforcement. Our method\nconsistently outperforms various baselines, effectively exploring and eliciting\nvalues from seekers. Additionally, leveraging crowd knowledge from Reddit\nsignificantly enhances its effectiveness. Therapists highlighted its ability to\nvalidate seekers' challenges and emphasize positive aspects of their\nsituations$\\unicode{x2013}$both crucial elements of value reinforcement. Our\nwork, being the first to integrate value reinforcement into emotional support\nsystems, demonstrates its promise and establishes a foundation for future\nresearch."}
{"id": "2501.18251", "pdf": "https://arxiv.org/pdf/2501.18251.pdf", "abs": "https://arxiv.org/abs/2501.18251", "title": "How to Select Datapoints for Efficient Human Evaluation of NLG Models?", "authors": ["Vil√©m Zouhar", "Peng Cui", "Mrinmaya Sachan"], "categories": ["cs.CL"], "comment": null, "summary": "Human evaluation is the gold standard for evaluating text generation models.\nHowever, it is expensive. In order to fit budgetary constraints, a random\nsubset of the test data is often chosen in practice for human evaluation.\nHowever, randomly selected data may not accurately represent test performance,\nmaking this approach economically inefficient for model comparison. Thus, in\nthis work, we develop and analyze a suite of selectors to get the most\ninformative datapoints for human evaluation, taking the evaluation costs into\naccount. We show that selectors based on variance in automated metric scores,\ndiversity in model outputs, or Item Response Theory outperform random\nselection. We further develop an approach to distill these selectors to the\nscenario where the model outputs are not yet available. In particular, we\nintroduce source-based estimators, which predict item usefulness for human\nevaluation just based on the source texts. We demonstrate the efficacy of our\nselectors in two common NLG tasks, machine translation and summarization, and\nshow that only $\\sim$70\\% of the test data is needed to produce the same\nevaluation result as the entire data."}
{"id": "2501.19017", "pdf": "https://arxiv.org/pdf/2501.19017.pdf", "abs": "https://arxiv.org/abs/2501.19017", "title": "Calling a Spade a Heart: Gaslighting Multimodal Large Language Models via Negation", "authors": ["Bin Zhu", "Huiyan Qi", "Yinxuan Gui", "Jingjing Chen", "Chong-Wah Ngo", "Ee-Peng Lim"], "categories": ["cs.CL"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have exhibited remarkable\nadvancements in integrating different modalities, excelling in complex\nunderstanding and generation tasks. Despite their success, MLLMs remain\nvulnerable to conversational adversarial inputs, particularly negation\narguments. This paper systematically evaluates state-of-the-art MLLMs across\ndiverse benchmarks, revealing significant performance drops when negation\narguments are introduced to initially correct responses. Notably, we introduce\nthe first benchmark GaslightingBench, specifically designed to evaluate the\nvulnerability of MLLMs to negation arguments. GaslightingBench consists of\nmultiple-choice questions curated from existing datasets, along with generated\nnegation prompts across 20 diverse categories. Throughout extensive evaluation,\nwe find that proprietary models such as Gemini-1.5-flash, GPT-4o and\nClaude-3.5-Sonnet demonstrate better resilience compared to open-source\ncounterparts like Qwen2-VL and LLaVA. However, all evaluated MLLMs struggle to\nmaintain logical consistency under negation arguments during conversation. Our\nfindings provide critical insights for improving the robustness of MLLMs\nagainst negation inputs, contributing to the development of more reliable and\ntrustworthy multimodal AI systems."}
{"id": "2502.02508", "pdf": "https://arxiv.org/pdf/2502.02508.pdf", "abs": "https://arxiv.org/abs/2502.02508", "title": "Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search", "authors": ["Maohao Shen", "Guangtao Zeng", "Zhenting Qi", "Zhang-Wei Hong", "Zhenfang Chen", "Wei Lu", "Gregory Wornell", "Subhro Das", "David Cox", "Chuang Gan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable reasoning\ncapabilities across diverse domains. Recent studies have shown that increasing\ntest-time computation enhances LLMs' reasoning capabilities. This typically\ninvolves extensive sampling at inference time guided by an external LLM\nverifier, resulting in a two-player system. Despite external guidance, the\neffectiveness of this system demonstrates the potential of a single LLM to\ntackle complex tasks. Thus, we pose a new research problem: Can we internalize\nthe searching capabilities to fundamentally enhance the reasoning abilities of\na single LLM? This work explores an orthogonal direction focusing on\npost-training LLMs for autoregressive searching (i.e., an extended reasoning\nprocess with self-reflection and self-exploration of new strategies). To\nachieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a\ntwo-stage training paradigm: 1) a small-scale format tuning stage to\ninternalize the COAT reasoning format and 2) a large-scale self-improvement\nstage leveraging reinforcement learning. Our approach results in Satori, a 7B\nLLM trained on open-source models and data. Extensive empirical evaluations\ndemonstrate that Satori achieves state-of-the-art performance on mathematical\nreasoning benchmarks while exhibits strong generalization to out-of-domain\ntasks. Code, data, and models are fully open-sourced."}
{"id": "2502.03678", "pdf": "https://arxiv.org/pdf/2502.03678.pdf", "abs": "https://arxiv.org/abs/2502.03678", "title": "Reflection-Window Decoding: Text Generation with Selective Refinement", "authors": ["Zeyu Tang", "Zhenhao Chen", "Xiangchen Song", "Loka Li", "Yunlong Deng", "Yifan Shen", "Guangyi Chen", "Peter Spirtes", "Kun Zhang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "In Proceedings of the 42nd International Conference on Machine\n  Learning, 2025. (ICML 2025)", "summary": "The autoregressive decoding for text generation in large language models\n(LLMs), while widely used, is inherently suboptimal due to the lack of a\nbuilt-in mechanism to perform refinement and/or correction of the generated\ncontent. In this paper, we consider optimality in terms of the joint\nprobability over the generated response, when jointly considering all tokens at\nthe same time. We theoretically characterize the potential deviation of the\nautoregressively generated response from its globally optimal counterpart that\nis of the same length. Our analysis suggests that we need to be cautious when\nnoticeable uncertainty arises during text generation, which may signal the\nsub-optimality of the generation history. To address the pitfall of\nautoregressive decoding for text generation, we propose an approach that\nincorporates a sliding reflection window and a pausing criterion, such that\nrefinement and generation can be carried out interchangeably as the decoding\nproceeds. Our selective refinement framework strikes a balance between\nefficiency and optimality, and our extensive experimental results demonstrate\nthe effectiveness of our approach."}
{"id": "2502.04795", "pdf": "https://arxiv.org/pdf/2502.04795.pdf", "abs": "https://arxiv.org/abs/2502.04795", "title": "Developmentally-plausible Working Memory Shapes a Critical Period for Language Acquisition", "authors": ["Masato Mita", "Ryo Yoshida", "Yohei Oseki"], "categories": ["cs.CL"], "comment": "Accepted to ACL2025 (main, long)", "summary": "Large language models possess general linguistic abilities but acquire\nlanguage less efficiently than humans. This study proposes a method for\nintegrating the developmental characteristics of working memory during the\ncritical period, a stage when human language acquisition is particularly\nefficient, into the training process of language models. The proposed method\nintroduces a mechanism that initially constrains working memory during the\nearly stages of training and gradually relaxes this constraint in an\nexponential manner as learning progresses. Targeted syntactic evaluation shows\nthat the proposed method outperforms conventional methods without memory\nconstraints or with static memory constraints. These findings not only provide\nnew directions for designing data-efficient language models but also offer\nindirect evidence supporting the role of the developmental characteristics of\nworking memory as the underlying mechanism of the critical period in language\nacquisition."}
{"id": "2502.05449", "pdf": "https://arxiv.org/pdf/2502.05449.pdf", "abs": "https://arxiv.org/abs/2502.05449", "title": "Iterative Deepening Sampling as Efficient Test-Time Scaling", "authors": ["Weizhe Chen", "Sven Koenig", "Bistra Dilkina"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent reasoning models, such as OpenAI's O1 series, have demonstrated\nexceptional performance on complex reasoning tasks and revealed new test-time\nscaling laws. Inspired by this, many people have been studying how to train\nmodels to achieve effective self-evaluation and self-correction to further\nenable the scaling paradigm. However, less studied is how to efficiently scale\ntest-time compute from a fixed model, and this remains a challenge. In this\npaper, we address this challenge by focusing on enhancing the quality of\nself-reflection data generation for complex problem-solving at test time, which\ncan also subsequently improve the training of next-generation large language\nmodels (LLMs). Specifically, we explore how systematically triggering a model's\nself-correction mechanisms can improve performance on challenging reasoning\ntasks. To this end, we propose a novel iterative deepening sampling algorithm\nframework designed to enhance self-correction and generate higher-quality\nsamples. Through extensive experiments on Math500 and AIME benchmarks, we\ndemonstrate that our method achieves a higher success rate on difficult tasks\nand provide detailed ablation studies to analyze its effectiveness across\ndiverse settings."}
{"id": "2502.05551", "pdf": "https://arxiv.org/pdf/2502.05551.pdf", "abs": "https://arxiv.org/abs/2502.05551", "title": "FRAME: Boosting LLMs with A Four-Quadrant Multi-Stage Pretraining Strategy", "authors": ["Xuemiao Zhang", "Feiyu Duan", "Liangyu Xu", "Yongwei Zhou", "Sirui Wang", "Rongxiang Weng", "Jingang Wang", "Xunliang Cai"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have significantly advanced human language\nunderstanding and generation, with pretraining data quality and organization\nbeing crucial to their performance. Multi-stage pretraining is a promising\napproach, but existing methods often lack quantitative criteria for data\npartitioning and instead rely on intuitive heuristics. In this paper, we\npropose the novel Four-quadRAnt Multi-stage prEtraining strategy (FRAME),\nguided by the established principle of organizing the pretraining process into\nfour stages to achieve significant loss reductions four times. This principle\nis grounded in two key findings: first, training on high Perplexity (PPL) data\nfollowed by low PPL data, and second, training on low PPL difference (PD) data\nfollowed by high PD data, both causing the loss to drop significantly twice and\nperformance enhancements. By partitioning data into four quadrants and\nstrategically organizing them, FRAME achieves a remarkable 16.8% average\nimprovement over random across MMLU and CMMLU for the 3B model, effectively\nboosting LLM performance."}
{"id": "2502.06204", "pdf": "https://arxiv.org/pdf/2502.06204.pdf", "abs": "https://arxiv.org/abs/2502.06204", "title": "Non-literal Understanding of Number Words by Language Models", "authors": ["Polina Tsvilodub", "Kanishk Gandhi", "Haoran Zhao", "Jan-Philipp Fr√§nken", "Michael Franke", "Noah D. Goodman"], "categories": ["cs.CL"], "comment": "12 pages, 10 figures. To appear in the Proceedings of CogSci 2025", "summary": "Humans naturally interpret numbers non-literally, effortlessly combining\ncontext, world knowledge, and speaker intent. We investigate whether large\nlanguage models (LLMs) interpret numbers similarly, focusing on hyperbole and\npragmatic halo effects. Through systematic comparison with human data and\ncomputational models of pragmatic reasoning, we find that LLMs diverge from\nhuman interpretation in striking ways. By decomposing pragmatic reasoning into\ntestable components, grounded in the Rational Speech Act framework, we pinpoint\nwhere LLM processing diverges from human cognition -- not in prior knowledge,\nbut in reasoning with it. This insight leads us to develop a targeted solution\n-- chain-of-thought prompting inspired by an RSA model makes LLMs'\ninterpretations more human-like. Our work demonstrates how computational\ncognitive models can both diagnose AI-human differences and guide development\nof more human-like language understanding capabilities."}
{"id": "2502.06560", "pdf": "https://arxiv.org/pdf/2502.06560.pdf", "abs": "https://arxiv.org/abs/2502.06560", "title": "Position: It's Time to Act on the Risk of Efficient Personalized Text Generation", "authors": ["Eugenia Iofinova", "Andrej Jovanovic", "Dan Alistarh"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "The recent surge in high-quality open-source Generative AI text models\n(colloquially: LLMs), as well as efficient finetuning techniques, have opened\nthe possibility of creating high-quality personalized models that generate text\nattuned to a specific individual's needs and are capable of credibly imitating\ntheir writing style by refining an open-source model with that person's own\ndata. The technology to create such models is accessible to private\nindividuals, and training and running such models can be done cheaply on\nconsumer-grade hardware. While these advancements are a huge gain for usability\nand privacy, this position paper argues that the practical feasibility of\nimpersonating specific individuals also introduces novel safety risks. For\ninstance, this technology enables the creation of phishing emails or fraudulent\nsocial media accounts, based on small amounts of publicly available text, or by\nthe individuals themselves to escape AI text detection. We further argue that\nthese risks are complementary to - and distinct from - the much-discussed risks\nof other impersonation attacks such as image, voice, or video deepfakes, and\nare not adequately addressed by the larger research community, or the current\ngeneration of open- and closed-source models."}
{"id": "2502.06851", "pdf": "https://arxiv.org/pdf/2502.06851.pdf", "abs": "https://arxiv.org/abs/2502.06851", "title": "Survey on Vision-Language-Action Models", "authors": ["Adilzhan Adilkhanov", "Amir Yelenov", "Assylkhan Seitzhanov", "Ayan Mazhitov", "Azamat Abdikarimov", "Danissa Sandykbayeva", "Daryn Kenzhebek", "Dinmukhammed Mukashev", "Ilyas Umurbekov", "Jabrail Chumakov", "Kamila Spanova", "Karina Burunchina", "Madina Yergibay", "Margulan Issa", "Moldir Zabirova", "Nurdaulet Zhuzbay", "Nurlan Kabdyshev", "Nurlan Zhaniyar", "Rasul Yermagambet", "Rustam Chibar", "Saltanat Seitzhan", "Soibkhon Khajikhanov", "Tasbolat Taunyazov", "Temirlan Galimzhanov", "Temirlan Kaiyrbay", "Tleukhan Mussin", "Togzhan Syrymova", "Valeriya Kostyukova", "Yerkebulan Massalim", "Yermakhan Kassym", "Zerde Nurbayeva", "Zhanat Kappassov"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "arXiv admin note: This submission has been withdrawn due to serious\n  violation of arXiv policies for acceptable submissions", "summary": "This paper presents an AI-generated review of Vision-Language-Action (VLA)\nmodels, summarizing key methodologies, findings, and future directions. The\ncontent is produced using large language models (LLMs) and is intended only for\ndemonstration purposes. This work does not represent original research, but\nhighlights how AI can help automate literature reviews. As AI-generated content\nbecomes more prevalent, ensuring accuracy, reliability, and proper synthesis\nremains a challenge. Future research will focus on developing a structured\nframework for AI-assisted literature reviews, exploring techniques to enhance\ncitation accuracy, source credibility, and contextual understanding. By\nexamining the potential and limitations of LLM in academic writing, this study\naims to contribute to the broader discussion of integrating AI into research\nworkflows. This work serves as a preliminary step toward establishing\nsystematic approaches for leveraging AI in literature review generation, making\nacademic knowledge synthesis more efficient and scalable."}
{"id": "2502.08092", "pdf": "https://arxiv.org/pdf/2502.08092.pdf", "abs": "https://arxiv.org/abs/2502.08092", "title": "GCoT: Chain-of-Thought Prompt Learning for Graphs", "authors": ["Xingtong Yu", "Chang Zhou", "Zhongwei Kuai", "Xinming Zhang", "Yuan Fang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by SIGKDD2025", "summary": "Chain-of-thought (CoT) prompting has achieved remarkable success in natural\nlanguage processing (NLP). However, its vast potential remains largely\nunexplored for graphs. This raises an interesting question: How can we design\nCoT prompting for graphs to guide graph models to learn step by step? On one\nhand, unlike natural languages, graphs are non-linear and characterized by\ncomplex topological structures. On the other hand, many graphs lack textual\ndata, making it difficult to formulate language-based CoT prompting. In this\nwork, we propose the first CoT prompt learning framework for text-free graphs,\nGCoT. Specifically, we decompose the adaptation process for each downstream\ntask into a series of inference steps, with each step consisting of\nprompt-based inference, ``thought'' generation, and thought-conditioned prompt\nlearning. While the steps mimic CoT prompting in NLP, the exact mechanism\ndiffers significantly. Specifically, at each step, an input graph, along with a\nprompt, is first fed into a pre-trained graph encoder for prompt-based\ninference. We then aggregate the hidden layers of the encoder to construct a\n``thought'', which captures the working state of each node in the current step.\nConditioned on this thought, we learn a prompt specific to each node based on\nthe current state. These prompts are fed into the next inference step,\nrepeating the cycle. To evaluate and analyze the effectiveness of GCoT, we\nconduct comprehensive experiments on eight public datasets, which demonstrate\nthe advantage of our approach."}
{"id": "2502.08561", "pdf": "https://arxiv.org/pdf/2502.08561.pdf", "abs": "https://arxiv.org/abs/2502.08561", "title": "Quality-Aware Decoding: Unifying Quality Estimation and Decoding", "authors": ["Sai Koneru", "Matthias Huck", "Miriam Exel", "Jan Niehues"], "categories": ["cs.CL"], "comment": "IWSLT 2025", "summary": "Quality Estimation (QE) models for Neural Machine Translation (NMT) predict\nthe quality of the hypothesis without having access to the reference. An\nemerging research direction in NMT involves the use of QE models, which have\ndemonstrated high correlations with human judgment and can enhance translations\nthrough Quality-Aware Decoding. Although several approaches have been proposed\nbased on sampling multiple candidate translations and picking the best\ncandidate, none have integrated these models directly into the decoding\nprocess. In this paper, we address this by proposing a novel token-level QE\nmodel capable of reliably scoring partial translations. We build a\nuni-directional QE model for this, as decoder models are inherently trained and\nefficient on partial sequences. We then present a decoding strategy that\nintegrates the QE model for Quality-Aware decoding and demonstrate that the\ntranslation quality improves when compared to the N-best list re-ranking with\nstate-of-the-art QE models (up to $1.39$ XCOMET-XXL $\\uparrow$). Finally, we\nshow that our approach provides significant benefits in document translation\ntasks, where the quality of N-best lists is typically suboptimal. Code can be\nfound at https://ai4lt.iar.kit.edu/english/projects\\_kontextmt.php"}
{"id": "2502.08662", "pdf": "https://arxiv.org/pdf/2502.08662.pdf", "abs": "https://arxiv.org/abs/2502.08662", "title": "RoToR: Towards More Reliable Responses for Order-Invariant Inputs", "authors": ["Soyoung Yoon", "Dongha Ahn", "Youngwon Lee", "Minkyu Jung", "HyungJoo Jang", "Seung-won Hwang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ACL 2025 main", "summary": "Mitigating positional bias of language models (LMs) for listwise inputs is a\nwell-known and important problem (e.g., lost-in-the-middle). While zero-shot\norder-invariant LMs have been proposed to solve this issue, their success on\npractical listwise problems has been limited. In this work, as a first\ncontribution, we identify and overcome two limitations to make zero-shot\ninvariant LMs more practical: (1) training and inference distribution mismatch\narising from modifying positional ID assignments to enforce invariance, and (2)\nfailure to adapt to mixture of order-invariant and sensitive inputs in\npractical listwise problems. Then, to overcome these issues we propose (1)\nRoToR, a zero-shot invariant LM for genuinely order-invariant inputs with\nminimal modifications of positional IDs, and (2) Selective Routing, an adaptive\nframework that handles both order-invariant and order-sensitive inputs in\nlistwise tasks. On the Lost in the middle (LitM), Knowledge Graph QA (KGQA),\nand MMLU benchmarks, we show that RoToR with Selective Routing can effectively\nhandle practical listwise input tasks in a zero-shot manner\n(https://github.com/soyoung97/RoToR)"}
{"id": "2502.08826", "pdf": "https://arxiv.org/pdf/2502.08826.pdf", "abs": "https://arxiv.org/abs/2502.08826", "title": "Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation", "authors": ["Mohammad Mahdi Abootorabi", "Amirhosein Zobeiri", "Mahdi Dehghani", "Mohammadali Mohammadkhani", "Bardia Mohammadi", "Omid Ghahroodi", "Mahdieh Soleymani Baghshah", "Ehsaneddin Asgari"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "GitHub repository:\n  https://github.com/llm-lab-org/Multimodal-RAG-Survey", "summary": "Large Language Models (LLMs) suffer from hallucinations and outdated\nknowledge due to their reliance on static training data. Retrieval-Augmented\nGeneration (RAG) mitigates these issues by integrating external dynamic\ninformation for improved factual grounding. With advances in multimodal\nlearning, Multimodal RAG extends this approach by incorporating multiple\nmodalities such as text, images, audio, and video to enhance the generated\noutputs. However, cross-modal alignment and reasoning introduce unique\nchallenges beyond those in unimodal RAG. This survey offers a structured and\ncomprehensive analysis of Multimodal RAG systems, covering datasets,\nbenchmarks, metrics, evaluation, methodologies, and innovations in retrieval,\nfusion, augmentation, and generation. We review training strategies, robustness\nenhancements, loss functions, and agent-based approaches, while also exploring\nthe diverse Multimodal RAG scenarios. In addition, we outline open challenges\nand future directions to guide research in this evolving field. This survey\nlays the foundation for developing more capable and reliable AI systems that\neffectively leverage multimodal dynamic external knowledge bases. All resources\nare publicly available at https://github.com/llm-lab-org/Multimodal-RAG-Survey."}
{"id": "2502.08900", "pdf": "https://arxiv.org/pdf/2502.08900.pdf", "abs": "https://arxiv.org/abs/2502.08900", "title": "Can Uniform Meaning Representation Help GPT-4 Translate from Indigenous Languages?", "authors": ["Shira Wein"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "While ChatGPT and GPT-based models are able to effectively perform many tasks\nwithout additional fine-tuning, they struggle with tasks related to extremely\nlow-resource languages and indigenous languages. Uniform Meaning Representation\n(UMR), a semantic representation designed to capture the meaning of texts in\nmany languages, is well-positioned to be leveraged in the development of\nlow-resource language technologies. In this work, we explore the downstream\nutility of UMR for low-resource languages by incorporating it into GPT-4\nprompts. Specifically, we examine the ability of GPT-4 to perform translation\nfrom three indigenous languages (Navajo, Ar\\'apaho, and Kukama), with and\nwithout demonstrations, as well as with and without UMR annotations.\nUltimately, we find that in the majority of our test cases, integrating UMR\ninto the prompt results in a statistically significant increase in performance,\nwhich is a promising indication of future applications of the UMR formalism."}
{"id": "2502.10201", "pdf": "https://arxiv.org/pdf/2502.10201.pdf", "abs": "https://arxiv.org/abs/2502.10201", "title": "Prediction hubs are context-informed frequent tokens in LLMs", "authors": ["Beatrix M. G. Nielsen", "Iuri Macocco", "Marco Baroni"], "categories": ["cs.CL", "cs.AI"], "comment": "Published as a conference paper at ACL 2025", "summary": "Hubness, the tendency for a few points to be among the nearest neighbours of\na disproportionate number of other points, commonly arises when applying\nstandard distance measures to high-dimensional data, often negatively impacting\ndistance-based analysis. As autoregressive large language models (LLMs) operate\non high-dimensional representations, we ask whether they are also affected by\nhubness. We first prove that the only large-scale representation comparison\noperation performed by LLMs, namely that between context and unembedding\nvectors to determine continuation probabilities, is not characterized by the\nconcentration of distances phenomenon that typically causes the appearance of\nnuisance hubness. We then empirically show that this comparison still leads to\na high degree of hubness, but the hubs in this case do not constitute a\ndisturbance. They are rather the result of context-modulated frequent tokens\noften appearing in the pool of likely candidates for next token prediction.\nHowever, when other distances are used to compare LLM representations, we do\nnot have the same theoretical guarantees, and, indeed, we see nuisance hubs\nappear. There are two main takeaways. First, hubness, while omnipresent in\nhigh-dimensional spaces, is not a negative property that needs to be mitigated\nwhen LLMs are being used for next token prediction. Second, when comparing\nrepresentations from LLMs using Euclidean or cosine distance, there is a high\nrisk of nuisance hubs and practitioners should use mitigation techniques if\nrelevant."}
{"id": "2502.11095", "pdf": "https://arxiv.org/pdf/2502.11095.pdf", "abs": "https://arxiv.org/abs/2502.11095", "title": "A Survey of Large Language Models in Psychotherapy: Current Landscape and Future Directions", "authors": ["Hongbin Na", "Yining Hua", "Zimu Wang", "Tao Shen", "Beibei Yu", "Lilin Wang", "Wei Wang", "John Torous", "Ling Chen"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 Findings", "summary": "Mental health is increasingly critical in contemporary healthcare, with\npsychotherapy demanding dynamic, context-sensitive interactions that\ntraditional NLP methods struggle to capture. Large Language Models (LLMs) offer\nsignificant potential for addressing this gap due to their ability to handle\nextensive context and multi-turn reasoning. This review introduces a conceptual\ntaxonomy dividing psychotherapy into interconnected stages--assessment,\ndiagnosis, and treatment--to systematically examine LLM advancements and\nchallenges. Our comprehensive analysis reveals imbalances in current research,\nsuch as a focus on common disorders, linguistic biases, fragmented methods, and\nlimited theoretical integration. We identify critical challenges including\ncapturing dynamic symptom fluctuations, overcoming linguistic and cultural\nbiases, and ensuring diagnostic reliability. Highlighting future directions, we\nadvocate for continuous multi-stage modeling, real-time adaptive systems\ngrounded in psychological theory, and diversified research covering broader\nmental disorders and therapeutic approaches, aiming toward more holistic and\nclinically integrated psychotherapy LLMs systems."}
{"id": "2502.11175", "pdf": "https://arxiv.org/pdf/2502.11175.pdf", "abs": "https://arxiv.org/abs/2502.11175", "title": "Investigating Language Preference of Multilingual RAG Systems", "authors": ["Jeonghyun Park", "Hwanhee Lee"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Multilingual Retrieval-Augmented Generation (mRAG) systems enhance language\nmodels by integrating external multilingual information to produce\ncontext-aware responses. However, mRAG systems struggle with retrieving\nrelevant information due to linguistic variations between queries and\ndocuments, generating inconsistent responses when multilingual sources\nconflict. In this work, we systematically investigate language preferences in\nboth retrieval and generation of mRAG through a series of experiments. Our\nanalysis indicates that retrievers tend to prefer high-resource and query\nlanguages, yet this preference does not consistently improve generation\nperformance. Moreover, we observe that generators prefer the query language or\nLatin scripts, leading to inconsistent outputs. To overcome these issues, we\npropose Dual Knowledge Multilingual RAG (DKM-RAG), a simple yet effective\nframework that fuses translated multilingual passages with complementary model\nknowledge. Empirical results demonstrate that DKM-RAG mitigates language\npreference in generation and enhances performance across diverse linguistic\nsettings. Code is available at\nhttps://github.com/jeonghyunpark2002/LanguagePreference.git"}
{"id": "2502.11177", "pdf": "https://arxiv.org/pdf/2502.11177.pdf", "abs": "https://arxiv.org/abs/2502.11177", "title": "The Mirage of Model Editing: Revisiting Evaluation in the Wild", "authors": ["Wanli Yang", "Fei Sun", "Jiajun Tan", "Xinyu Ma", "Qi Cao", "Dawei Yin", "Huawei Shen", "Xueqi Cheng"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main Conference (Camera Ready Version)", "summary": "Despite near-perfect results reported in the literature, the effectiveness of\nmodel editing in real-world applications remains unclear. To bridge this gap,\nwe introduce QAEdit, a new benchmark aligned with widely used question\nanswering (QA) datasets, and WILD, a task-agnostic evaluation framework\ndesigned to better reflect real-world usage of model editing. Our single\nediting experiments show that current editing methods perform substantially\nworse than previously reported (38.5% vs. 96.8%). We demonstrate that it stems\nfrom issues in the synthetic evaluation practices of prior work. Among them,\nthe most severe is the use of teacher forcing during testing, which leaks both\ncontent and length of the ground truth, leading to overestimated performance.\nFurthermore, we simulate practical deployment by sequential editing, revealing\nthat current approaches fail drastically with only 1000 edits. This work calls\nfor a shift in model editing research toward rigorous evaluation and the\ndevelopment of robust, scalable methods that can reliably update knowledge in\nLLMs for real-world use."}
{"id": "2502.11368", "pdf": "https://arxiv.org/pdf/2502.11368.pdf", "abs": "https://arxiv.org/abs/2502.11368", "title": "LLMs can Perform Multi-Dimensional Analytic Writing Assessments: A Case Study of L2 Graduate-Level Academic English Writing", "authors": ["Zhengxiang Wang", "Veronika Makarova", "Zhi Li", "Jordan Kodner", "Owen Rambow"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025", "summary": "The paper explores the performance of LLMs in the context of\nmulti-dimensional analytic writing assessments, i.e. their ability to provide\nboth scores and comments based on multiple assessment criteria. Using a corpus\nof literature reviews written by L2 graduate students and assessed by human\nexperts against 9 analytic criteria, we prompt several popular LLMs to perform\nthe same task under various conditions. To evaluate the quality of feedback\ncomments, we apply a novel feedback comment quality evaluation framework. This\nframework is interpretable, cost-efficient, scalable, and reproducible,\ncompared to existing methods that rely on manual judgments. We find that LLMs\ncan generate reasonably good and generally reliable multi-dimensional analytic\nassessments. We release our corpus and code for reproducibility."}
{"id": "2502.11423", "pdf": "https://arxiv.org/pdf/2502.11423.pdf", "abs": "https://arxiv.org/abs/2502.11423", "title": "Exploring Persona Sentiment Sensitivity in Personalized Dialogue Generation", "authors": ["Yonghyun Jun", "Hwanhee Lee"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Main)", "summary": "Personalized dialogue systems have advanced considerably with the integration\nof user-specific personas into large language models (LLMs). However, while\nLLMs can effectively generate personalized responses, the influence of persona\nsentiment on dialogue quality remains underexplored. In this work, we conduct a\nlarge-scale analysis of dialogues generated using a range of polarized user\nprofiles. Our experiments reveal that dialogues involving negatively polarized\nusers tend to overemphasize persona attributes. In contrast, positively\npolarized profiles yield dialogues that selectively incorporate persona\ninformation, resulting in smoother interactions. Furthermore, we find that\npersonas with weak or neutral sentiment generally produce lower-quality\ndialogues. Motivated by these findings, we propose a dialogue generation\napproach that explicitly accounts for persona polarity by combining a\nturn-based generation strategy with a profile ordering mechanism and\nsentiment-aware prompting. Our study provides new insights into the sensitivity\nof LLMs to persona sentiment and offers guidance for developing more robust and\nnuanced personalized dialogue systems."}
{"id": "2502.11469", "pdf": "https://arxiv.org/pdf/2502.11469.pdf", "abs": "https://arxiv.org/abs/2502.11469", "title": "If Attention Serves as a Cognitive Model of Human Memory Retrieval, What is the Plausible Memory Representation?", "authors": ["Ryo Yoshida", "Shinnosuke Isono", "Kohei Kajikawa", "Taiga Someya", "Yushi Sugimoto", "Yohei Oseki"], "categories": ["cs.CL"], "comment": "18 pages; To appear in ACL 2025", "summary": "Recent work in computational psycholinguistics has revealed intriguing\nparallels between attention mechanisms and human memory retrieval, focusing\nprimarily on vanilla Transformers that operate on token-level representations.\nHowever, computational psycholinguistic research has also established that\nsyntactic structures provide compelling explanations for human sentence\nprocessing that token-level factors cannot fully account for. In this paper, we\ninvestigate whether the attention mechanism of Transformer Grammar (TG), which\nuniquely operates on syntactic structures as representational units, can serve\nas a cognitive model of human memory retrieval, using Normalized Attention\nEntropy (NAE) as a linking hypothesis between models and humans. Our\nexperiments demonstrate that TG's attention achieves superior predictive power\nfor self-paced reading times compared to vanilla Transformer's, with further\nanalyses revealing independent contributions from both models. These findings\nsuggest that human sentence processing involves dual memory representations --\none based on syntactic structures and another on token sequences -- with\nattention serving as the general memory retrieval algorithm, while highlighting\nthe importance of incorporating syntactic structures as representational units."}
{"id": "2502.11671", "pdf": "https://arxiv.org/pdf/2502.11671.pdf", "abs": "https://arxiv.org/abs/2502.11671", "title": "Diversity-oriented Data Augmentation with Large Language Models", "authors": ["Zaitian Wang", "Jinghan Zhang", "Xinhao Zhang", "Kunpeng Liu", "Pengfei Wang", "Yuanchun Zhou"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ACL 2025", "summary": "Data augmentation is an essential technique in natural language processing\n(NLP) for enriching training datasets by generating diverse samples. This\nprocess is crucial for improving the robustness and generalization capabilities\nof NLP models. However, a significant challenge remains: \\textit{Insufficient\nAttention to Sample Distribution Diversity}. Most existing methods focus on\nincreasing the sample numbers while neglecting the sample distribution\ndiversity, which can lead to model overfitting. In response, we explore data\naugmentation's impact on dataset diversity and propose a\n\\textbf{\\underline{D}}iversity-\\textbf{\\underline{o}}riented data\n\\textbf{\\underline{Aug}}mentation framework (\\textbf{DoAug}). %\n\\(\\mathscr{DoAug}\\) Specifically, we utilize a diversity-oriented fine-tuning\napproach to train an LLM as a diverse paraphraser, which is capable of\naugmenting textual datasets by generating diversified paraphrases. Then, we\napply the LLM paraphraser to a selected coreset of highly informative samples\nand integrate the paraphrases with the original data to create a more diverse\naugmented dataset. Finally, we conduct extensive experiments on 12 real-world\ntextual datasets. The results show that our fine-tuned LLM augmenter improves\ndiversity while preserving label consistency, thereby enhancing the robustness\nand performance of downstream tasks. Specifically, it achieves an average\nperformance gain of \\(10.52\\%\\), surpassing the runner-up baseline with more\nthan three percentage points."}
{"id": "2502.11735", "pdf": "https://arxiv.org/pdf/2502.11735.pdf", "abs": "https://arxiv.org/abs/2502.11735", "title": "MT-RAIG: Novel Benchmark and Evaluation Framework for Retrieval-Augmented Insight Generation over Multiple Tables", "authors": ["Kwangwook Seo", "Donguk Kwon", "Dongha Lee"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025", "summary": "Recent advancements in table-based reasoning have expanded beyond\nfactoid-level QA to address insight-level tasks, where systems should\nsynthesize implicit knowledge in the table to provide explainable analyses.\nAlthough effective, existing studies remain confined to scenarios where a\nsingle gold table is given alongside the user query, failing to address cases\nwhere users seek comprehensive insights from multiple unknown tables. To bridge\nthese gaps, we propose MT-RAIG Bench, design to evaluate systems on\nRetrieval-Augmented Insight Generation over Mulitple-Tables. Additionally, to\ntackle the suboptimality of existing automatic evaluation methods in the table\ndomain, we further introduce a fine-grained evaluation framework MT-RAIG Eval,\nwhich achieves better alignment with human quality judgments on the generated\ninsights. We conduct extensive experiments and reveal that even frontier LLMs\nstill struggle with complex multi-table reasoning, establishing our MT-RAIG\nBench as a challenging testbed for future research."}
{"id": "2502.12050", "pdf": "https://arxiv.org/pdf/2502.12050.pdf", "abs": "https://arxiv.org/abs/2502.12050", "title": "SpeechT: Findings of the First Mentorship in Speech Translation", "authors": ["Yasmin Moslem", "Juan Juli√°n Cea Mor√°n", "Mariano Gonzalez-Gomez", "Muhammad Hazim Al Farouq", "Farah Abdou", "Satarupa Deb"], "categories": ["cs.CL", "cs.SD"], "comment": "MT Summit 2025", "summary": "This work presents the details and findings of the first mentorship in speech\ntranslation (SpeechT), which took place in December 2024 and January 2025. To\nfulfil the mentorship requirements, the participants engaged in key activities,\nincluding data preparation, modelling, and advanced research. The participants\nexplored data augmentation techniques and compared end-to-end and cascaded\nspeech translation systems. The projects covered various languages other than\nEnglish, including Arabic, Bengali, Galician, Indonesian, Japanese, and\nSpanish."}
{"id": "2502.12378", "pdf": "https://arxiv.org/pdf/2502.12378.pdf", "abs": "https://arxiv.org/abs/2502.12378", "title": "Pragmatics in the Era of Large Language Models: A Survey on Datasets, Evaluation, Opportunities and Challenges", "authors": ["Bolei Ma", "Yuting Li", "Wei Zhou", "Ziwei Gong", "Yang Janet Liu", "Katja Jasinskaja", "Annemarie Friedrich", "Julia Hirschberg", "Frauke Kreuter", "Barbara Plank"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Understanding pragmatics-the use of language in context-is crucial for\ndeveloping NLP systems capable of interpreting nuanced language use. Despite\nrecent advances in language technologies, including large language models,\nevaluating their ability to handle pragmatic phenomena such as implicatures and\nreferences remains challenging. To advance pragmatic abilities in models, it is\nessential to understand current evaluation trends and identify existing\nlimitations. In this survey, we provide a comprehensive review of resources\ndesigned for evaluating pragmatic capabilities in NLP, categorizing datasets by\nthe pragmatics phenomena they address. We analyze task designs, data collection\nmethods, evaluation approaches, and their relevance to real-world applications.\nBy examining these resources in the context of modern language models, we\nhighlight emerging trends, challenges, and gaps in existing benchmarks. Our\nsurvey aims to clarify the landscape of pragmatic evaluation and guide the\ndevelopment of more comprehensive and targeted benchmarks, ultimately\ncontributing to more nuanced and context-aware NLP models."}
{"id": "2502.12685", "pdf": "https://arxiv.org/pdf/2502.12685.pdf", "abs": "https://arxiv.org/abs/2502.12685", "title": "Theoretical Guarantees for Minimum Bayes Risk Decoding", "authors": ["Yuki Ichihara", "Yuu Jinnai", "Kaito Ariu", "Tetsuro Morimura", "Eiji Uchibe"], "categories": ["cs.CL"], "comment": null, "summary": "Minimum Bayes Risk (MBR) decoding optimizes output selection by maximizing\nthe expected utility value of an underlying human distribution. While prior\nwork has shown the effectiveness of MBR decoding through empirical evaluation,\nfew studies have analytically investigated why the method is effective. As a\nresult of our analysis, we show that, given the size $n$ of the reference\nhypothesis set used in computation, MBR decoding approaches the optimal\nsolution with high probability at a rate of $O\\left(n^{-\\frac{1}{2}}\\right)$,\nunder certain assumptions, even though the language space $Y$ is significantly\nlarger $|Y|\\gg n$. This result helps to theoretically explain the strong\nperformance observed in several prior empirical studies on MBR decoding. In\naddition, we provide the performance gap for maximum-a-posteriori (MAP)\ndecoding and compare it to MBR decoding. The result of this paper indicates\nthat MBR decoding tends to converge to the optimal solution faster than MAP\ndecoding in several cases."}
{"id": "2502.12821", "pdf": "https://arxiv.org/pdf/2502.12821.pdf", "abs": "https://arxiv.org/abs/2502.12821", "title": "Pitfalls of Scale: Investigating the Inverse Task of Redefinition in Large Language Models", "authors": ["Elena Stringli", "Maria Lymperaiou", "Giorgos Filandrianos", "Athanasios Voulodimos", "Giorgos Stamou"], "categories": ["cs.CL"], "comment": "Accepted at Findings of ACL 2025", "summary": "Inverse tasks can uncover potential reasoning gaps as Large Language Models\n(LLMs) scale up. In this work, we explore the redefinition task, in which we\nassign alternative values to well-known physical constants and units of\nmeasure, prompting LLMs to respond accordingly. Our findings show that not only\ndoes model performance degrade with scale, but its false confidence also rises.\nMoreover, while factors such as prompting strategies or response formatting are\ninfluential, they do not preclude LLMs from anchoring to memorized values."}
{"id": "2502.12835", "pdf": "https://arxiv.org/pdf/2502.12835.pdf", "abs": "https://arxiv.org/abs/2502.12835", "title": "Subword models struggle with word learning, but surprisal hides it", "authors": ["Bastian Bunzeck", "Sina Zarrie√ü"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Main)", "summary": "We study word learning in subword and character language models with the\npsycholinguistic lexical decision task. While subword LMs struggle to discern\nwords and non-words with high accuracy, character LMs solve this task easily\nand consistently. Only when supplied with further contexts do subword LMs\nperform similarly to character models. Additionally, when looking at word-level\nand syntactic learning trajectories, we find that both processes are separable\nin character LMs. Word learning happens before syntactic learning, whereas both\noccur simultaneously in subword LMs. This raises questions about the adequacy\nof subword LMs for modeling language acquisition and positions character LMs as\na viable alternative to study processes below the syntactic level."}
{"id": "2502.13031", "pdf": "https://arxiv.org/pdf/2502.13031.pdf", "abs": "https://arxiv.org/abs/2502.13031", "title": "HPSS: Heuristic Prompting Strategy Search for LLM Evaluators", "authors": ["Bosi Wen", "Pei Ke", "Yufei Sun", "Cunxiang Wang", "Xiaotao Gu", "Jinfeng Zhou", "Jie Tang", "Hongning Wang", "Minlie Huang"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Since the adoption of large language models (LLMs) for text evaluation has\nbecome increasingly prevalent in the field of natural language processing\n(NLP), a series of existing works attempt to optimize the prompts for LLM\nevaluators to improve their alignment with human judgment. However, their\nefforts are limited to optimizing individual factors of evaluation prompts,\nsuch as evaluation criteria or output formats, neglecting the combinatorial\nimpact of multiple factors, which leads to insufficient optimization of the\nevaluation pipeline. Nevertheless, identifying well-behaved prompting\nstrategies for adjusting multiple factors requires extensive enumeration. To\nthis end, we comprehensively integrate 8 key factors for evaluation prompts and\npropose a novel automatic prompting strategy optimization method called\nHeuristic Prompting Strategy Search (HPSS). Inspired by the genetic algorithm,\nHPSS conducts an iterative search to find well-behaved prompting strategies for\nLLM evaluators. A heuristic function is employed to guide the search process,\nenhancing the performance of our algorithm. Extensive experiments across four\nevaluation tasks demonstrate the effectiveness of HPSS, consistently\noutperforming both human-designed evaluation prompts and existing automatic\nprompt optimization methods. Our code is available at\nhttps://github.com/thu-coai/HPSS."}
{"id": "2502.13259", "pdf": "https://arxiv.org/pdf/2502.13259.pdf", "abs": "https://arxiv.org/abs/2502.13259", "title": "HumT DumT: Measuring and controlling human-like language in LLMs", "authors": ["Myra Cheng", "Sunny Yu", "Dan Jurafsky"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "Accepted to ACL 2025", "summary": "Should LLMs generate language that makes them seem human? Human-like language\nmight improve user experience, but might also lead to deception, overreliance,\nand stereotyping. Assessing these potential impacts requires a systematic way\nto measure human-like tone in LLM outputs. We introduce HumT and SocioT,\nmetrics for human-like tone and other dimensions of social perceptions in text\ndata based on relative probabilities from an LLM. By measuring HumT across\npreference and usage datasets, we find that users prefer less human-like\noutputs from LLMs in many contexts. HumT also offers insights into the\nperceptions and impacts of anthropomorphism: human-like LLM outputs are highly\ncorrelated with warmth, social closeness, femininity, and low status, which are\nclosely linked to the aforementioned harms. We introduce DumT, a method using\nHumT to systematically control and reduce the degree of human-like tone while\npreserving model performance. DumT offers a practical approach for mitigating\nrisks associated with anthropomorphic language generation."}
{"id": "2502.13775", "pdf": "https://arxiv.org/pdf/2502.13775.pdf", "abs": "https://arxiv.org/abs/2502.13775", "title": "VITAL: A New Dataset for Benchmarking Pluralistic Alignment in Healthcare", "authors": ["Anudeex Shetty", "Amin Beheshti", "Mark Dras", "Usman Naseem"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ACL 2025 (Main Proceedings)", "summary": "Alignment techniques have become central to ensuring that Large Language\nModels (LLMs) generate outputs consistent with human values. However, existing\nalignment paradigms often model an averaged or monolithic preference, failing\nto account for the diversity of perspectives across cultures, demographics, and\ncommunities. This limitation is particularly critical in health-related\nscenarios, where plurality is essential due to the influence of culture,\nreligion, personal values, and conflicting opinions. Despite progress in\npluralistic alignment, no prior work has focused on health, likely due to the\nunavailability of publicly available datasets. To address this gap, we\nintroduce VITAL, a new benchmark dataset comprising 13.1K value-laden\nsituations and 5.4K multiple-choice questions focused on health, designed to\nassess and benchmark pluralistic alignment methodologies. Through extensive\nevaluation of eight LLMs of varying sizes, we demonstrate that existing\npluralistic alignment techniques fall short in effectively accommodating\ndiverse healthcare beliefs, underscoring the need for tailored AI alignment in\nspecific domains. This work highlights the limitations of current approaches\nand lays the groundwork for developing health-specific alignment solutions."}
{"id": "2502.13917", "pdf": "https://arxiv.org/pdf/2502.13917.pdf", "abs": "https://arxiv.org/abs/2502.13917", "title": "TESS 2: A Large-Scale Generalist Diffusion Language Model", "authors": ["Jaesung Tae", "Hamish Ivison", "Sachin Kumar", "Arman Cohan"], "categories": ["cs.CL"], "comment": "ACL 2025 camera-ready", "summary": "We introduce TESS 2, a general instruction-following diffusion language model\nthat outperforms contemporary instruction-tuned diffusion models, as well as\nmatches and sometimes exceeds strong autoregressive (AR) models. We train TESS\n2 by first adapting a strong AR model via continued pretraining with the usual\ncross-entropy as diffusion loss, and then performing further instruction\ntuning. We find that adaptation training as well as the choice of the base\nmodel is crucial for training good instruction-following diffusion models. We\nfurther propose reward guidance, a novel and modular inference-time guidance\nprocedure to align model outputs without needing to train the underlying model.\nFinally, we show that TESS 2 further improves with increased inference-time\ncompute, highlighting the utility of diffusion LMs in having fine-grained\ncontrollability over the amount of compute used at inference time. Code and\nmodels are available at https://github.com/hamishivi/tess-2."}
{"id": "2502.13957", "pdf": "https://arxiv.org/pdf/2502.13957.pdf", "abs": "https://arxiv.org/abs/2502.13957", "title": "RAG-Gym: Systematic Optimization of Language Agents for Retrieval-Augmented Generation", "authors": ["Guangzhi Xiong", "Qiao Jin", "Xiao Wang", "Yin Fang", "Haolin Liu", "Yifan Yang", "Fangyuan Chen", "Zhixing Song", "Dengyu Wang", "Minjia Zhang", "Zhiyong Lu", "Aidong Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "Homepage: https://rag-gym.github.io; Code:\n  https://github.com/RAG-Gym/RAG-Gym", "summary": "Retrieval-augmented generation (RAG) has shown great promise for\nknowledge-intensive tasks and recently advanced with agentic RAG, where\nlanguage agents engage in multi-round interactions with external knowledge\nsources for adaptive information retrieval. However, existing agentic RAG\nmethods often depend on ad-hoc prompt engineering and lack a unified\noptimization framework. We introduce RAG-Gym, a comprehensive platform that\nsystematically explores three optimization dimensions: (1) prompt engineering,\n(2) actor tuning, and (3) critic training. For prompt engineering, we propose\nRe$^2$Search, a novel agent incorporating reasoning reflection that\nsignificantly outperforms standard prompts. In actor tuning, we evaluate three\npopular post-training algorithms with fine-grained process supervision and\nidentify direct preference optimization as the most effective. We further\ndemonstrate that a trained critic can enhance inference by selecting\nhigher-quality intermediate reasoning steps. Together, these findings lead to\nthe optimized Re$^2$Search++ agent, which surpasses most recent methods like\nSearch-R1 by a relative increase of 3.2% to 11.6% in average F1. Finally, we\nexamine the impact of different reward sources and analyze scaling properties\nin training and inference, offering practical insights for agentic RAG\noptimization. The project homepage is available at https://rag-gym.github.io."}
{"id": "2502.14127", "pdf": "https://arxiv.org/pdf/2502.14127.pdf", "abs": "https://arxiv.org/abs/2502.14127", "title": "Which of These Best Describes Multiple Choice Evaluation with LLMs? A) Forced B) Flawed C) Fixable D) All of the Above", "authors": ["Nishant Balepur", "Rachel Rudinger", "Jordan Lee Boyd-Graber"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Multiple choice question answering (MCQA) is popular for LLM evaluation due\nto its simplicity and human-like testing, but we argue for its reform. We first\nreveal flaws in MCQA's format, as it struggles to: 1) test\ngeneration/subjectivity; 2) match LLM use cases; and 3) fully test knowledge.\nWe instead advocate for generative formats based on human testing, where LLMs\nconstruct and explain answers, better capturing user needs and knowledge while\nremaining easy to score. We then show even when MCQA is a useful format, its\ndatasets suffer from: leakage; unanswerability; shortcuts; and saturation. In\neach issue, we give fixes from education, like rubrics to guide MCQ writing;\nscoring methods to bridle guessing; and Item Response Theory to build harder\nMCQs. Lastly, we discuss LLM errors in MCQA, robustness, biases, and unfaithful\nexplanations, showing how our prior solutions better measure or address these\nissues. While we do not need to desert MCQA, we encourage more efforts in\nrefining the task based on educational testing, advancing evaluations."}
{"id": "2502.14258", "pdf": "https://arxiv.org/pdf/2502.14258.pdf", "abs": "https://arxiv.org/abs/2502.14258", "title": "Does Time Have Its Place? Temporal Heads: Where Language Models Recall Time-specific Information", "authors": ["Yein Park", "Chanwoong Yoon", "Jungwoo Park", "Minbyul Jeong", "Jaewoo Kang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to the main conference at ACL 2025", "summary": "While the ability of language models to elicit facts has been widely\ninvestigated, how they handle temporally changing facts remains underexplored.\nWe discover Temporal Heads, specific attention heads that primarily handle\ntemporal knowledge, through circuit analysis. We confirm that these heads are\npresent across multiple models, though their specific locations may vary, and\ntheir responses differ depending on the type of knowledge and its corresponding\nyears. Disabling these heads degrades the model's ability to recall\ntime-specific knowledge while maintaining its general capabilities without\ncompromising time-invariant and question-answering performances. Moreover, the\nheads are activated not only numeric conditions (\"In 2004\") but also textual\naliases (\"In the year ...\"), indicating that they encode a temporal dimension\nbeyond simple numerical representation. Furthermore, we expand the potential of\nour findings by demonstrating how temporal knowledge can be edited by adjusting\nthe values of these heads."}
{"id": "2502.14301", "pdf": "https://arxiv.org/pdf/2502.14301.pdf", "abs": "https://arxiv.org/abs/2502.14301", "title": "SEA-HELM: Southeast Asian Holistic Evaluation of Language Models", "authors": ["Yosephine Susanto", "Adithya Venkatadri Hulagadri", "Jann Railey Montalan", "Jian Gang Ngui", "Xian Bin Yong", "Weiqi Leong", "Hamsawardhini Rengarajan", "Peerat Limkonchotiwat", "Yifan Mai", "William Chandra Tjhi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "With the rapid emergence of novel capabilities in Large Language Models\n(LLMs), the need for rigorous multilingual and multicultural benchmarks that\nare integrated has become more pronounced. Though existing LLM benchmarks are\ncapable of evaluating specific capabilities of LLMs in English as well as in\nvarious mid- to low-resource languages, including those in the Southeast Asian\n(SEA) region, a comprehensive and culturally representative evaluation suite\nfor the SEA languages has not been developed thus far. Here, we present\nSEA-HELM, a holistic linguistic and cultural LLM evaluation suite that\nemphasises SEA languages, comprising five core pillars: (1) NLP Classics, (2)\nLLM-specifics, (3) SEA Linguistics, (4) SEA Culture, (5) Safety. SEA-HELM\ncurrently supports Filipino, Indonesian, Tamil, Thai, and Vietnamese. We also\nintroduce the SEA-HELM leaderboard, which allows users to understand models'\nmultilingual and multicultural performance in a systematic and user-friendly\nmanner. We make the SEA-HELM evaluation code publicly available."}
{"id": "2502.14677", "pdf": "https://arxiv.org/pdf/2502.14677.pdf", "abs": "https://arxiv.org/abs/2502.14677", "title": "Data-Constrained Synthesis of Training Data for De-Identification", "authors": ["Thomas Vakili", "Aron Henriksson", "Hercules Dalianis"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Main: Long paper", "summary": "Many sensitive domains -- such as the clinical domain -- lack widely\navailable datasets due to privacy risks. The increasing generative capabilities\nof large language models (LLMs) have made synthetic datasets a viable path\nforward. In this study, we domain-adapt LLMs to the clinical domain and\ngenerate synthetic clinical texts that are machine-annotated with tags for\npersonally identifiable information using capable encoder-based NER models. The\nsynthetic corpora are then used to train synthetic NER models. The results show\nthat training NER models using synthetic corpora incurs only a small drop in\npredictive performance. The limits of this process are investigated in a\nsystematic ablation study -- using both Swedish and Spanish data. Our analysis\nshows that smaller datasets can be sufficient for domain-adapting LLMs for data\nsynthesis. Instead, the effectiveness of this process is almost entirely\ncontingent on the performance of the machine-annotating NER models trained\nusing the original data."}
{"id": "2502.14778", "pdf": "https://arxiv.org/pdf/2502.14778.pdf", "abs": "https://arxiv.org/abs/2502.14778", "title": "Harnessing PDF Data for Improving Japanese Large Multimodal Models", "authors": ["Jeonghun Baek", "Akiko Aizawa", "Kiyoharu Aizawa"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Accepted to ACL2025 Findings. Code:\n  https://github.com/ku21fan/PDF-JLMM", "summary": "Large Multimodal Models (LMMs) have demonstrated strong performance in\nEnglish, but their effectiveness in Japanese remains limited due to the lack of\nhigh-quality training data. Current Japanese LMMs often rely on translated\nEnglish datasets, restricting their ability to capture Japan-specific cultural\nknowledge. To address this, we explore the potential of Japanese PDF data as a\ntraining resource, an area that remains largely underutilized. We introduce a\nfully automated pipeline that leverages pretrained models to extract image-text\npairs from PDFs through layout analysis, OCR, and vision-language pairing,\nremoving the need for manual annotation. Additionally, we construct instruction\ndata from extracted image-text pairs to enrich the training data. To evaluate\nthe effectiveness of PDF-derived data, we train Japanese LMMs and assess their\nperformance on the Japanese LMM Benchmark. Our results demonstrate substantial\nimprovements, with performance gains ranging from 2.1% to 13.8% on Heron-Bench.\nFurther analysis highlights the impact of PDF-derived data on various factors,\nsuch as model size and language models, reinforcing its value as a multimodal\nresource for Japanese LMMs."}
{"id": "2502.14830", "pdf": "https://arxiv.org/pdf/2502.14830.pdf", "abs": "https://arxiv.org/abs/2502.14830", "title": "Middle-Layer Representation Alignment for Cross-Lingual Transfer in Fine-Tuned LLMs", "authors": ["Danni Liu", "Jan Niehues"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025", "summary": "While large language models demonstrate remarkable capabilities at\ntask-specific applications through fine-tuning, extending these benefits across\ndiverse languages is essential for broad accessibility. However, effective\ncross-lingual transfer is hindered by LLM performance gaps across languages and\nthe scarcity of fine-tuning data in many languages. Through analysis of LLM\ninternal representations from over 1,000+ language pairs, we discover that\nmiddle layers exhibit the strongest potential for cross-lingual alignment.\nBuilding on this finding, we propose a middle-layer alignment objective\nintegrated into task-specific training. Our experiments on slot filling,\nmachine translation, and structured text generation show consistent\nimprovements in cross-lingual transfer, especially to lower-resource languages.\nThe method is robust to the choice of alignment languages and generalizes to\nlanguages unseen during alignment. Furthermore, we show that separately trained\nalignment modules can be merged with existing task-specific modules, improving\ncross-lingual capabilities without full re-training. Our code is publicly\navailable (https://github.com/dannigt/mid-align)."}
{"id": "2502.16173", "pdf": "https://arxiv.org/pdf/2502.16173.pdf", "abs": "https://arxiv.org/abs/2502.16173", "title": "Mapping 1,000+ Language Models via the Log-Likelihood Vector", "authors": ["Momose Oyama", "Hiroaki Yamagiwa", "Yusuke Takase", "Hidetoshi Shimodaira"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "To compare autoregressive language models at scale, we propose using\nlog-likelihood vectors computed on a predefined text set as model features.\nThis approach has a solid theoretical basis: when treated as model coordinates,\ntheir squared Euclidean distance approximates the Kullback-Leibler divergence\nof text-generation probabilities. Our method is highly scalable, with\ncomputational cost growing linearly in both the number of models and text\nsamples, and is easy to implement as the required features are derived from\ncross-entropy loss. Applying this method to over 1,000 language models, we\nconstructed a \"model map,\" providing a new perspective on large-scale model\nanalysis."}
{"id": "2502.16942", "pdf": "https://arxiv.org/pdf/2502.16942.pdf", "abs": "https://arxiv.org/abs/2502.16942", "title": "NUTSHELL: A Dataset for Abstract Generation from Scientific Talks", "authors": ["Maike Z√ºfle", "Sara Papi", "Beatrice Savoldi", "Marco Gaido", "Luisa Bentivogli", "Jan Niehues"], "categories": ["cs.CL"], "comment": null, "summary": "Scientific communication is receiving increasing attention in natural\nlanguage processing, especially to help researches access, summarize, and\ngenerate content. One emerging application in this area is Speech-to-Abstract\nGeneration (SAG), which aims to automatically generate abstracts from recorded\nscientific presentations. SAG enables researchers to efficiently engage with\nconference talks, but progress has been limited by a lack of large-scale\ndatasets. To address this gap, we introduce NUTSHELL, a novel multimodal\ndataset of *ACL conference talks paired with their corresponding abstracts. We\nestablish strong baselines for SAG and evaluate the quality of generated\nabstracts using both automatic metrics and human judgments. Our results\nhighlight the challenges of SAG and demonstrate the benefits of training on\nNUTSHELL. By releasing NUTSHELL under an open license (CC-BY 4.0), we aim to\nadvance research in SAG and foster the development of improved models and\nevaluation methods."}
{"id": "2502.17184", "pdf": "https://arxiv.org/pdf/2502.17184.pdf", "abs": "https://arxiv.org/abs/2502.17184", "title": "Measuring Data Diversity for Instruction Tuning: A Systematic Analysis and A Reliable Metric", "authors": ["Yuming Yang", "Yang Nan", "Junjie Ye", "Shihan Dou", "Xiao Wang", "Shuo Li", "Huijie Lv", "Mingqi Wu", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "categories": ["cs.CL"], "comment": "Accepted at ACL 2025 Main. Camera-ready version updated (20 pages).\n  Project page: https://github.com/UmeanNever/NovelSum", "summary": "Data diversity is crucial for the instruction tuning of large language\nmodels. Existing studies have explored various diversity-aware data selection\nmethods to construct high-quality datasets and enhance model performance.\nHowever, the fundamental problem of precisely defining and measuring data\ndiversity remains underexplored, limiting clear guidance for data engineering.\nTo address this, we systematically analyze 11 existing diversity measurement\nmethods by evaluating their correlation with model performance through\nextensive fine-tuning experiments. Our results indicate that a reliable\ndiversity measure should properly account for both inter-sample differences and\nthe information density in the sample space. Building on this, we propose\nNovelSum, a new diversity metric based on sample-level \"novelty.\" Experiments\non both simulated and real-world data show that NovelSum accurately captures\ndiversity variations and achieves a 0.97 correlation with instruction-tuned\nmodel performance, highlighting its value in guiding data engineering\npractices. With NovelSum as an optimization objective, we further develop a\ngreedy, diversity-oriented data selection strategy that outperforms existing\napproaches, validating both the effectiveness and practical significance of our\nmetric. The code is available at https://github.com/UmeanNever/NovelSum."}
{"id": "2502.18795", "pdf": "https://arxiv.org/pdf/2502.18795.pdf", "abs": "https://arxiv.org/abs/2502.18795", "title": "Anything Goes? A Crosslinguistic Study of (Im)possible Language Learning in LMs", "authors": ["Xiulin Yang", "Tatsuya Aoyama", "Yuekun Yao", "Ethan Wilcox"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Do language models (LMs) offer insights into human language learning? A\ncommon argument against this idea is that because their architecture and\ntraining paradigm are so vastly different from humans, LMs can learn arbitrary\ninputs as easily as natural languages. We test this claim by training LMs to\nmodel impossible and typologically unattested languages. Unlike previous work,\nwhich has focused exclusively on English, we conduct experiments on 12\nlanguages from 4 language families with two newly constructed parallel corpora.\nOur results show that while GPT-2 small can largely distinguish attested\nlanguages from their impossible counterparts, it does not achieve perfect\nseparation between all the attested languages and all the impossible ones. We\nfurther test whether GPT-2 small distinguishes typologically attested from\nunattested languages with different NP orders by manipulating word order based\non Greenberg's Universal 20. We find that the model's perplexity scores do not\ndistinguish attested vs. unattested word orders, while its performance on the\ngeneralization test does. These findings suggest that LMs exhibit some\nhuman-like inductive biases, though these biases are weaker than those found in\nhuman learners."}
{"id": "2502.18968", "pdf": "https://arxiv.org/pdf/2502.18968.pdf", "abs": "https://arxiv.org/abs/2502.18968", "title": "Know You First and Be You Better: Modeling Human-Like User Simulators via Implicit Profiles", "authors": ["Kuang Wang", "Xianfei Li", "Shenghao Yang", "Li Zhou", "Feng Jiang", "Haizhou Li"], "categories": ["cs.CL"], "comment": "9 pages. Accepted to ACL 2025. Camera-ready version", "summary": "User simulators are crucial for replicating human interactions with dialogue\nsystems, supporting both collaborative training and automatic evaluation,\nespecially for large language models (LLMs). However, current role-playing\nmethods face challenges such as a lack of utterance-level authenticity and\nuser-level diversity, often hindered by role confusion and dependence on\npredefined profiles of well-known figures. In contrast, direct simulation\nfocuses solely on text, neglecting implicit user traits like personality and\nconversation-level consistency. To address these issues, we introduce the User\nSimulator with Implicit Profiles (USP), a framework that infers implicit user\nprofiles from human-machine interactions to simulate personalized and realistic\ndialogues. We first develop an LLM-driven extractor with a comprehensive\nprofile schema, then refine the simulation using conditional supervised\nfine-tuning and reinforcement learning with cycle consistency, optimizing at\nboth the utterance and conversation levels. Finally, a diverse profile sampler\ncaptures the distribution of real-world user profiles. Experimental results\nshow that USP outperforms strong baselines in terms of authenticity and\ndiversity while maintaining comparable consistency. Additionally, using USP to\nevaluate LLM on dynamic multi-turn aligns well with mainstream benchmarks,\ndemonstrating its effectiveness in real-world applications."}
{"id": "2502.19163", "pdf": "https://arxiv.org/pdf/2502.19163.pdf", "abs": "https://arxiv.org/abs/2502.19163", "title": "TestNUC: Enhancing Test-Time Computing Approaches and Scaling through Neighboring Unlabeled Data Consistency", "authors": ["Henry Peng Zou", "Zhengyao Gu", "Yue Zhou", "Yankai Chen", "Weizhi Zhang", "Liancheng Fang", "Yibo Wang", "Yangning Li", "Kay Liu", "Philip S. Yu"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": "Accepted by ACL 2025 main conference", "summary": "Test-time computing approaches, which leverage additional computational\nresources during inference, have been proven effective in enhancing large\nlanguage model performance. This work introduces a novel, linearly scaling\napproach, TestNUC, that improves test-time predictions by leveraging the local\nconsistency of neighboring unlabeled data-it classifies an input instance by\nconsidering not only the model's prediction on that instance but also on\nneighboring unlabeled instances. We evaluate TestNUC across eight diverse\ndatasets, spanning intent classification, topic mining, domain discovery, and\nemotion detection, demonstrating its consistent superiority over baseline\nmethods such as standard prompting and self-consistency. Furthermore, TestNUC\ncan be seamlessly integrated with existing test-time computing approaches,\nsubstantially boosting their performance. Our analysis reveals that TestNUC\nscales effectively with increasing amounts of unlabeled data and performs\nrobustly across different embedding models, making it practical for real-world\napplications. Our code is available at https://github.com/HenryPengZou/TestNUC."}
{"id": "2502.19765", "pdf": "https://arxiv.org/pdf/2502.19765.pdf", "abs": "https://arxiv.org/abs/2502.19765", "title": "EdiText: Controllable Coarse-to-Fine Text Editing with Diffusion Language Models", "authors": ["Che Hyun Lee", "Heeseung Kim", "Jiheum Yeom", "Sungroh Yoon"], "categories": ["cs.CL", "cs.LG"], "comment": "ACL 2025", "summary": "We propose EdiText, a controllable text editing method that modifies the\nreference text to desired attributes at various scales. We integrate an\nSDEdit-based editing technique that allows for broad adjustments in the degree\nof text editing. Additionally, we introduce a novel fine-level editing method\nbased on self-conditioning, which allows subtle control of reference text.\nWhile being capable of editing on its own, this fine-grained method, integrated\nwith the SDEdit approach, enables EdiText to make precise adjustments within\nthe desired range. EdiText demonstrates its controllability to robustly adjust\nreference text at a broad range of levels across various tasks, including\ntoxicity control and sentiment control."}
{"id": "2502.20238", "pdf": "https://arxiv.org/pdf/2502.20238.pdf", "abs": "https://arxiv.org/abs/2502.20238", "title": "FINEREASON: Evaluating and Improving LLMs' Deliberate Reasoning through Reflective Puzzle Solving", "authors": ["Guizhen Chen", "Weiwen Xu", "Hao Zhang", "Hou Pong Chan", "Chaoqun Liu", "Lidong Bing", "Deli Zhao", "Anh Tuan Luu", "Yu Rong"], "categories": ["cs.CL"], "comment": "Accepted to ACL2025 Main", "summary": "Many challenging reasoning tasks require not just rapid, intuitive responses,\nbut a more deliberate, multi-step approach. Recent progress in large language\nmodels (LLMs) highlights an important shift from the \"System 1\" way of quick\nreactions to the \"System 2\" style of reflection-and-correction problem solving.\nHowever, current benchmarks heavily rely on the final-answer accuracy, leaving\nmuch of a model's intermediate reasoning steps unexamined. This fails to assess\nthe model's ability to reflect and rectify mistakes within the reasoning\nprocess. To bridge this gap, we introduce FINEREASON, a logic-puzzle benchmark\nfor fine-grained evaluation of LLMs' reasoning capabilities. Each puzzle can be\ndecomposed into atomic steps, making it ideal for rigorous validation of\nintermediate correctness. Building on this, we introduce two tasks: state\nchecking, and state transition, for a comprehensive evaluation of how models\nassess the current situation and plan the next move. To support broader\nresearch, we also provide a puzzle training set aimed at enhancing performance\non general mathematical tasks. We show that models trained on our state\nchecking and transition data demonstrate gains in math reasoning by up to 5.1%\non GSM8K."}
{"id": "2502.20273", "pdf": "https://arxiv.org/pdf/2502.20273.pdf", "abs": "https://arxiv.org/abs/2502.20273", "title": "How Much is Enough? The Diminishing Returns of Tokenization Training Data", "authors": ["Varshini Reddy", "Craig W. Schmidt", "Yuval Pinter", "Chris Tanner"], "categories": ["cs.CL", "cs.CE"], "comment": null, "summary": "Tokenization, a crucial initial step in natural language processing, is\ngoverned by several key parameters, such as the tokenization algorithm,\nvocabulary size, pre-tokenization strategy, inference strategy, and training\ndata corpus. This paper investigates the impact of an often-overlooked\nhyperparameter, tokenizer training data size. We train BPE, UnigramLM, and\nWordPiece tokenizers across various vocabulary sizes using English training\ndata ranging from 1GB to 900GB. Our findings reveal diminishing returns as\ntraining data size increases beyond roughly 150GB, suggesting a practical limit\nto the improvements in tokenization quality achievable through additional data.\nWe analyze this phenomenon and attribute the saturation effect to constraints\nintroduced by the pre-tokenization stage. We then demonstrate the extent to\nwhich these findings can generalize by experimenting on data in Russian, a\nlanguage typologically distant from English. While the limit appears to\nmaterialize at a later phase of pre-training, around 200GB, it is in fact\nobserved. These results provide valuable insights for optimizing the\ntokenization process by reducing the compute required for training on large\ncorpora and suggest promising directions for future research in tokenization\nalgorithms."}
{"id": "2502.20503", "pdf": "https://arxiv.org/pdf/2502.20503.pdf", "abs": "https://arxiv.org/abs/2502.20503", "title": "Protecting multimodal large language models against misleading visualizations", "authors": ["Jonathan Tonglet", "Tinne Tuytelaars", "Marie-Francine Moens", "Iryna Gurevych"], "categories": ["cs.CL"], "comment": "Preprint. Code and data available at\n  https://github.com/UKPLab/arxiv2025-misleading-visualizations", "summary": "Visualizations play a pivotal role in daily communication in an increasingly\ndatadriven world. Research on multimodal large language models (MLLMs) for\nautomated chart understanding has accelerated massively, with steady\nimprovements on standard benchmarks. However, for MLLMs to be reliable, they\nmust be robust to misleading visualizations, i.e., charts that distort the\nunderlying data, leading readers to draw inaccurate conclusions that may\nsupport disinformation. Here, we uncover an important vulnerability: MLLM\nquestionanswering (QA) accuracy on misleading visualizations drops on average\nto the level of the random baseline. To address this, we introduce the first\ninference-time methods to improve QA performance on misleading visualizations,\nwithout compromising accuracy on non-misleading ones. We find that two methods,\ntable-based QA and redrawing the visualization, are effective, with\nimprovements of up to 19.6 percentage points. We make our code and data\navailable."}
{"id": "2503.00985", "pdf": "https://arxiv.org/pdf/2503.00985.pdf", "abs": "https://arxiv.org/abs/2503.00985", "title": "Enhancing Text Editing for Grammatical Error Correction: Arabic as a Case Study", "authors": ["Bashar Alhafni", "Nizar Habash"], "categories": ["cs.CL"], "comment": null, "summary": "Text editing frames grammatical error correction (GEC) as a sequence tagging\nproblem, where edit tags are assigned to input tokens, and applying these edits\nresults in the corrected text. This approach has gained attention for its\nefficiency and interpretability. However, while extensively explored for\nEnglish, text editing remains largely underexplored for morphologically rich\nlanguages like Arabic. In this paper, we introduce a text editing approach that\nderives edit tags directly from data, eliminating the need for\nlanguage-specific edits. We demonstrate its effectiveness on Arabic, a\ndiglossic and morphologically rich language, and investigate the impact of\ndifferent edit representations on model performance. Our approach achieves SOTA\nresults on two Arabic GEC benchmarks and performs on par with SOTA on two\nothers. Additionally, our models are over six times faster than existing Arabic\nGEC systems, making our approach more practical for real-world applications.\nFinally, we explore ensemble models, demonstrating how combining different\nmodels leads to further performance improvements. We make our code, data, and\npretrained models publicly available."}
{"id": "2503.01150", "pdf": "https://arxiv.org/pdf/2503.01150.pdf", "abs": "https://arxiv.org/abs/2503.01150", "title": "MiLiC-Eval: Benchmarking Multilingual LLMs for China's Minority Languages", "authors": ["Chen Zhang", "Mingxu Tao", "Zhiyuan Liao", "Yansong Feng"], "categories": ["cs.CL"], "comment": "ACL 2025 (Findings) Code and data available at\n  https://github.com/luciusssss/MiLiC-Eval", "summary": "Large language models (LLMs) excel in high-resource languages but struggle\nwith low-resource languages (LRLs), particularly those spoken by minority\ncommunities in China, such as Tibetan, Uyghur, Kazakh, and Mongolian. To\nsystematically track the progress in these languages, we introduce MiLiC-Eval,\na benchmark designed for minority languages in China, featuring 24K instances\nacross 9 tasks. MiLiC-Eval focuses on underrepresented writing systems. Its\nparallelism between tasks and languages can provide a faithful and fine-grained\nassessment of linguistic and problem-solving skills. Our evaluation reveals\nthat open-source LLMs perform poorly on syntax-intensive tasks and multi-script\nlanguages. We further demonstrate how MiLiC-Eval can help advance LRL research\nin handling diverse writing systems and understanding the process of language\nadaptation."}
{"id": "2503.01854", "pdf": "https://arxiv.org/pdf/2503.01854.pdf", "abs": "https://arxiv.org/abs/2503.01854", "title": "A Comprehensive Survey of Machine Unlearning Techniques for Large Language Models", "authors": ["Jiahui Geng", "Qing Li", "Herbert Woisetschlaeger", "Zongxiong Chen", "Fengyu Cai", "Yuxia Wang", "Preslav Nakov", "Hans-Arno Jacobsen", "Fakhri Karray"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This study investigates the machine unlearning techniques within the context\nof large language models (LLMs), referred to as \\textit{LLM unlearning}. LLM\nunlearning offers a principled approach to removing the influence of\nundesirable data (e.g., sensitive or illegal information) from LLMs, while\npreserving their overall utility without requiring full retraining. Despite\ngrowing research interest, there is no comprehensive survey that systematically\norganizes existing work and distills key insights; here, we aim to bridge this\ngap. We begin by introducing the definition and the paradigms of LLM\nunlearning, followed by a comprehensive taxonomy of existing unlearning\nstudies. Next, we categorize current unlearning approaches, summarizing their\nstrengths and limitations. Additionally, we review evaluation metrics and\nbenchmarks, providing a structured overview of current assessment\nmethodologies. Finally, we outline promising directions for future research,\nhighlighting key challenges and opportunities in the field."}
{"id": "2503.02450", "pdf": "https://arxiv.org/pdf/2503.02450.pdf", "abs": "https://arxiv.org/abs/2503.02450", "title": "Measuring What Makes You Unique: Difference-Aware User Modeling for Enhancing LLM Personalization", "authors": ["Yilun Qiu", "Xiaoyan Zhao", "Yang Zhang", "Yimeng Bai", "Wenjie Wang", "Hong Cheng", "Fuli Feng", "Tat-Seng Chua"], "categories": ["cs.CL"], "comment": "2025 ACL Findings", "summary": "Personalizing Large Language Models (LLMs) has become a critical step in\nfacilitating their widespread application to enhance individual life\nexperiences. In pursuit of personalization, distilling key preference\ninformation from an individual's historical data as instructional preference\ncontext to customize LLM generation has emerged as a promising direction.\nHowever, these methods face a fundamental limitation by overlooking the\ninter-user comparative analysis, which is essential for identifying the\ninter-user differences that truly shape preferences. To address this\nlimitation, we propose Difference-aware Personalization Learning (DPL), a novel\napproach that emphasizes extracting inter-user differences to enhance LLM\npersonalization. DPL strategically selects representative users for comparison\nand establishes a structured standard to extract meaningful, task-relevant\ndifferences for customizing LLM generation. Extensive experiments on real-world\ndatasets demonstrate that DPL significantly enhances LLM personalization. We\nrelease our code at https://github.com/SnowCharmQ/DPL."}
{"id": "2503.03340", "pdf": "https://arxiv.org/pdf/2503.03340.pdf", "abs": "https://arxiv.org/abs/2503.03340", "title": "EnigmaToM: Improve LLMs' Theory-of-Mind Reasoning Capabilities with Neural Knowledge Base of Entity States", "authors": ["Hainiu Xu", "Siya Qi", "Jiazheng Li", "Yuxiang Zhou", "Jinhua Du", "Caroline Catmur", "Yulan He"], "categories": ["cs.CL"], "comment": "Findings of ACL 2025", "summary": "Theory-of-Mind (ToM), the ability to infer others' perceptions and mental\nstates, is fundamental to human interaction but remains challenging for Large\nLanguage Models (LLMs). While existing ToM reasoning methods show promise with\nreasoning via perceptual perspective-taking, they often rely excessively on\noff-the-shelf LLMs, reducing their efficiency and limiting their applicability\nto high-order ToM reasoning. To address these issues, we present EnigmaToM, a\nnovel neuro-symbolic framework that enhances ToM reasoning by integrating a\nNeural Knowledge Base of entity states (Enigma) for (1) a psychology-inspired\niterative masking mechanism that facilitates accurate perspective-taking and\n(2) knowledge injection that elicits key entity information. Enigma generates\nstructured knowledge of entity states to build spatial scene graphs for belief\ntracking across various ToM orders and enrich events with fine-grained entity\nstate details. Experimental results on ToMi, HiToM, and FANToM benchmarks show\nthat EnigmaToM significantly improves ToM reasoning across LLMs of varying\nsizes, particularly excelling in high-order reasoning scenarios."}
{"id": "2503.04490", "pdf": "https://arxiv.org/pdf/2503.04490.pdf", "abs": "https://arxiv.org/abs/2503.04490", "title": "Large Language Models in Bioinformatics: A Survey", "authors": ["Zhenyu Wang", "Zikang Wang", "Jiyue Jiang", "Pengan Chen", "Xiangyu Shi", "Yu Li"], "categories": ["cs.CL", "q-bio.GN"], "comment": "Accepted by ACL 2025", "summary": "Large Language Models (LLMs) are revolutionizing bioinformatics, enabling\nadvanced analysis of DNA, RNA, proteins, and single-cell data. This survey\nprovides a systematic review of recent advancements, focusing on genomic\nsequence modeling, RNA structure prediction, protein function inference, and\nsingle-cell transcriptomics. Meanwhile, we also discuss several key challenges,\nincluding data scarcity, computational complexity, and cross-omics integration,\nand explore future directions such as multimodal learning, hybrid AI models,\nand clinical applications. By offering a comprehensive perspective, this paper\nunderscores the transformative potential of LLMs in driving innovations in\nbioinformatics and precision medicine."}
{"id": "2503.04796", "pdf": "https://arxiv.org/pdf/2503.04796.pdf", "abs": "https://arxiv.org/abs/2503.04796", "title": "Optimizing Multi-Hop Document Retrieval Through Intermediate Representations", "authors": ["Jiaen Lin", "Jingyu Liu", "Yingbo Liu"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Accepted by ACL 2025 Findings", "summary": "Retrieval-augmented generation (RAG) encounters challenges when addressing\ncomplex queries, particularly multi-hop questions. While several methods tackle\nmulti-hop queries by iteratively generating internal queries and retrieving\nexternal documents, these approaches are computationally expensive. In this\npaper, we identify a three-stage information processing pattern in LLMs during\nlayer-by-layer reasoning, consisting of extraction, processing, and subsequent\nextraction steps. This observation suggests that the representations in\nintermediate layers contain richer information compared to those in other\nlayers. Building on this insight, we propose Layer-wise RAG (L-RAG). Unlike\nprior methods that focus on generating new internal queries, L-RAG leverages\nintermediate representations from the middle layers, which capture next-hop\ninformation, to retrieve external knowledge. L-RAG achieves performance\ncomparable to multi-step approaches while maintaining inference overhead\nsimilar to that of standard RAG. Experimental results show that L-RAG\noutperforms existing RAG methods on open-domain multi-hop question-answering\ndatasets, including MuSiQue, HotpotQA, and 2WikiMultiHopQA. The code is\navailable in https://anonymous.4open.science/r/L-RAG-ADD5/"}
{"id": "2503.04800", "pdf": "https://arxiv.org/pdf/2503.04800.pdf", "abs": "https://arxiv.org/abs/2503.04800", "title": "HoH: A Dynamic Benchmark for Evaluating the Impact of Outdated Information on Retrieval-Augmented Generation", "authors": ["Jie Ouyang", "Tingyue Pan", "Mingyue Cheng", "Ruiran Yan", "Yucong Luo", "Jiaying Lin", "Qi Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While Retrieval-Augmented Generation (RAG) has emerged as an effective\napproach for addressing the knowledge outdating problem in Large Language\nModels (LLMs), it still faces a critical challenge: the prevalence of outdated\ninformation in knowledge bases. Current research primarily focuses on\nincorporating up-to-date information, yet the impact of outdated information\ncoexisting in retrieval sources remains inadequately addressed. To bridge this\ngap, we introduce HoH, the first benchmark specifically designed to evaluate\nthe impact of outdated information on RAG. Our benchmark leverages token-level\ndiff algorithms combined with LLM pipelines to efficiently create a large-scale\nQA dataset that accurately captures the evolution of temporal knowledge in\nreal-world facts. Through comprehensive experiments, we reveal that outdated\ninformation significantly degrades RAG performance in two critical ways: (1) it\nsubstantially reduces response accuracy by distracting models from correct\ninformation, and (2) it can mislead models into generating potentially harmful\noutputs, even when current information is available. Current RAG approaches\nstruggle with both retrieval and generation aspects when handling outdated\ninformation. These findings highlight the urgent need for innovative solutions\nto address the temporal challenges in RAG. Our code and data are available at:\nhttps://github.com/0russwest0/HoH."}
{"id": "2503.05750", "pdf": "https://arxiv.org/pdf/2503.05750.pdf", "abs": "https://arxiv.org/abs/2503.05750", "title": "CSTRL: Context-Driven Sequential Transfer Learning for Abstractive Radiology Report Summarization", "authors": ["Mst. Fahmida Sultana Naznin", "Adnan Ibney Faruq", "Mostafa Rifat Tazwar", "Md Jobayer", "Md. Mehedi Hasan Shawon", "Md Rakibul Hasan"], "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2.7"], "comment": "Accepted in ACL 2025 Findings", "summary": "A radiology report comprises several sections, including the Findings and\nImpression of the diagnosis. Automatically generating the Impression from the\nFindings is crucial for reducing radiologists' workload and improving\ndiagnostic accuracy. Pretrained models that excel in common abstractive\nsummarization problems encounter challenges when applied to specialized medical\ndomains largely due to the complex terminology and the necessity for accurate\nclinical context. Such tasks in medical domains demand extracting core\ninformation, avoiding context shifts, and maintaining proper flow. Misuse of\nmedical terms can lead to drastic clinical errors. To address these issues, we\nintroduce a sequential transfer learning that ensures key content extraction\nand coherent summarization. Sequential transfer learning often faces challenges\nlike initial parameter decay and knowledge loss, which we resolve with the\nFisher matrix regularization. Using MIMIC-CXR and Open-I datasets, our model,\nCSTRL - Context-driven Sequential TRansfer Learning - achieved state-of-the-art\nperformance, showing 56.2% improvement in BLEU-1, 40.5% in BLEU-2, 84.3% in\nBLEU-3, 28.9% in ROUGE-1, 41.0% in ROUGE-2 and 26.5% in ROGUE-3 score over\nbenchmark studies. We also analyze factual consistency scores while preserving\nthe medical context. Our code is publicly available at\nhttps://github.com/fahmidahossain/Report_Summarization."}
{"id": "2503.05763", "pdf": "https://arxiv.org/pdf/2503.05763.pdf", "abs": "https://arxiv.org/abs/2503.05763", "title": "GMLM: Bridging Graph Neural Networks and Language Models for Heterophilic Node Classification", "authors": ["Aarush Sinha", "OM Kumar CU"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Integrating structured graph data with rich textual information from nodes\nposes a significant challenge, particularly for heterophilic node\nclassification. Current approaches often struggle with computational costs or\neffective fusion of disparate modalities. We propose \\textbf{Graph Masked\nLanguage Model (GMLM)}, a novel architecture efficiently combining Graph Neural\nNetworks (GNNs) with Pre-trained Language Models (PLMs). GMLM introduces three\nkey innovations: (i) a \\textbf{dynamic active node selection} strategy for\nscalable PLM text processing; (ii) a GNN-specific \\textbf{contrastive\npretraining stage} using soft masking with a learnable graph \\texttt{[MASK]}\ntoken for robust structural representations; and (iii) a \\textbf{dedicated\nfusion module} integrating RGCN-based GNN embeddings with PLM (GTE-Small \\&\nDistilBERT) embeddings. Extensive experiments on heterophilic benchmarks\n(Cornell, Wisconsin, Texas) demonstrate GMLM's superiority. Notably,\nGMLM(DistilBERT) achieves significant performance gains, improving accuracy by\nover \\textbf{4.7\\%} on Cornell and over \\textbf{2.0\\%} on Texas compared to the\nprevious best-performing baselines. This work underscores the benefits of\ntargeted PLM engagement and modality-specific pretraining for improved,\nefficient learning on text-rich graphs."}
{"id": "2503.06594", "pdf": "https://arxiv.org/pdf/2503.06594.pdf", "abs": "https://arxiv.org/abs/2503.06594", "title": "Beyond Decoder-only: Large Language Models Can be Good Encoders for Machine Translation", "authors": ["Yingfeng Luo", "Tong Zheng", "Yongyu Mu", "Bei Li", "Qinghong Zhang", "Yongqi Gao", "Ziqiang Xu", "Peinan Feng", "Xiaoqian Liu", "Tong Xiao", "Jingbo Zhu"], "categories": ["cs.CL"], "comment": "Accepted to ACL Findings 2025. Please cite the ACL version. Code and\n  data are available at: https://github.com/NiuTrans/LaMaTE", "summary": "The field of neural machine translation (NMT) has changed with the advent of\nlarge language models (LLMs). Much of the recent emphasis in natural language\nprocessing (NLP) has been on modeling machine translation and many other\nproblems using a single pre-trained Transformer decoder, while encoder-decoder\narchitectures, which were the standard in earlier NMT models, have received\nrelatively less attention. In this paper, we explore translation models that\nare universal, efficient, and easy to optimize, by marrying the world of LLMs\nwith the world of NMT. We apply LLMs to NMT encoding and leave the NMT decoder\nunchanged. We also develop methods for adapting LLMs to work better with the\nNMT decoder. Furthermore, we construct a new dataset involving multiple tasks\nto assess how well the machine translation system generalizes across various\ntasks. Evaluations on the WMT and our datasets show that results using our\nmethod match or surpass a range of baselines in terms of translation quality,\nbut achieve $2.4 \\sim 6.5 \\times$ inference speedups and a $75\\%$ reduction in\nthe memory footprint of the KV cache. It also demonstrates strong\ngeneralization across a variety of translation-related tasks."}
{"id": "2503.07604", "pdf": "https://arxiv.org/pdf/2503.07604.pdf", "abs": "https://arxiv.org/abs/2503.07604", "title": "Implicit Reasoning in Transformers is Reasoning through Shortcuts", "authors": ["Tianhe Lin", "Jian Xie", "Siyu Yuan", "Deqing Yang"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Test-time compute is emerging as a new paradigm for enhancing language\nmodels' complex multi-step reasoning capabilities, as demonstrated by the\nsuccess of OpenAI's o1 and o3, as well as DeepSeek's R1. Compared to explicit\nreasoning in test-time compute, implicit reasoning is more inference-efficient,\nrequiring fewer generated tokens. However, why does the advanced reasoning\ncapability fail to emerge in the implicit reasoning style? In this work, we\ntrain GPT-2 from scratch on a curated multi-step mathematical reasoning dataset\nand conduct analytical experiments to investigate how language models perform\nimplicit reasoning in multi-step tasks. Our findings reveal: 1) Language models\ncan perform step-by-step reasoning and achieve high accuracy in both in-domain\nand out-of-domain tests via implicit reasoning. However, this capability only\nemerges when trained on fixed-pattern data. 2) Conversely, implicit reasoning\nabilities emerging from training on unfixed-pattern data tend to overfit a\nspecific pattern and fail to generalize further. Notably, this limitation is\nalso observed in state-of-the-art large language models. These findings suggest\nthat language models acquire implicit reasoning through shortcut learning,\nenabling strong performance on tasks with similar patterns while lacking\ngeneralization."}
{"id": "2503.08067", "pdf": "https://arxiv.org/pdf/2503.08067.pdf", "abs": "https://arxiv.org/abs/2503.08067", "title": "Context-aware Biases for Length Extrapolation", "authors": ["Ali Veisi", "Hamidreza Amirzadeh", "Amir Mansourian"], "categories": ["cs.CL"], "comment": "13 pages, 6 figures, 4 table", "summary": "Transformers often struggle to generalize to longer sequences than those seen\nduring training, a limitation known as length extrapolation. Most existing\nRelative Positional Encoding (RPE) methods attempt to address this by\nintroducing either fixed linear biases or globally learned biases, which lack\nthe capacity to adapt to different input contexts. In this work, we propose an\nadditive RPE, Context-Aware Biases for Length Extrapolation (CABLE), a method\nthat learns token-specific, context-aware biases for each attention head in\ntransformers. By dynamically adjusting positional biases based on the input\nsequence, CABLE overcomes the rigidity of fixed RPEs. When evaluated on\nsequences longer than originally trained with, GPT-2 Medium (334M parameters)\nwith CABLE achieves lower perplexity than counterparts using other widely\nadopted positional encoding methods. Additionally, by applying CABLE to the\nBERT base model we improved performance in long-context retrieval tasks. Our\nmethod significantly enhances the extrapolation performance of existing RPE\nmethods tested on the FineWeb-Edu10B and WikiText-103 datasets. Code is\navailable at: https://github.com/axiomlab/cable"}
{"id": "2503.10084", "pdf": "https://arxiv.org/pdf/2503.10084.pdf", "abs": "https://arxiv.org/abs/2503.10084", "title": "Why Prompt Design Matters and Works: A Complexity Analysis of Prompt Search Space in LLMs", "authors": ["Xiang Zhang", "Juntai Cao", "Jiaqi Wei", "Chenyu You", "Dujian Ding"], "categories": ["cs.CL"], "comment": "ACL 2025 main conference", "summary": "Despite the remarkable successes of large language models (LLMs), the\nunderlying Transformer architecture has inherent limitations in handling\ncomplex reasoning tasks. Chain-of-thought (CoT) prompting has emerged as a\npractical workaround, but most CoT-based methods rely on a single, generic\nprompt such as \"think step by step\", with no task-specific adaptation. These\napproaches expect the model to discover an effective reasoning path on its own,\nforcing it to search through a vast prompt space. In contrast, several studies\nhave explored task-specific prompt designs to boost performance. However, these\ndesigns are typically developed through trial and error, lacking theoretical\ngrounding. As a result, prompt engineering remains largely ad hoc and unguided.\nIn this paper, we provide a theoretical framework that explains why some\nprompts succeed while others fail. We show that prompts function as selectors,\nextracting task-relevant information from the model's full hidden state during\nCoT reasoning. Each prompt defines a unique trajectory through the answer\nspace, and the choice of trajectory is crucial for task performance and future\nnavigation within the space. We analyze the complexity of finding optimal\nprompts and characterize the size of the prompt space for a given task. Our\ntheory reveals principles behind effective prompt design and shows that naive\nCoT-using self-guided prompts like \"think step by step\"-can severely hinder\nperformance. Through experiments, we show that optimal prompt search can lead\nto more than a 50% improvement on reasoning tasks, providing a theoretical\nfoundation for prompt engineering."}
{"id": "2503.13089", "pdf": "https://arxiv.org/pdf/2503.13089.pdf", "abs": "https://arxiv.org/abs/2503.13089", "title": "ClusComp: A Simple Paradigm for Model Compression and Efficient Finetuning", "authors": ["Baohao Liao", "Christian Herold", "Seyyed Hadi Hashemi", "Stefan Vasilev", "Shahram Khadivi", "Christof Monz"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL camera-ready version", "summary": "As large language models (LLMs) scale, model compression is crucial for edge\ndeployment and accessibility. Weight-only quantization reduces model size but\nsuffers from performance degradation at lower bit widths. Moreover, standard\nfinetuning is incompatible with quantized models, and alternative methods often\nfall short of full finetuning. In this paper, we propose ClusComp, a simple yet\neffective compression paradigm that clusters weight matrices into codebooks and\nfinetunes them block-by-block. ClusComp (1) achieves superior performance in\n2-4 bit quantization, (2) pushes compression to 1-bit while outperforming\nultra-low-bit methods with minimal finetuning, and (3) enables efficient\nfinetuning, even surpassing existing quantization-based approaches and rivaling\nfull FP16 finetuning. Notably, ClusComp supports compression and finetuning of\n70B LLMs on a single A6000-48GB GPU."}
{"id": "2503.13975", "pdf": "https://arxiv.org/pdf/2503.13975.pdf", "abs": "https://arxiv.org/abs/2503.13975", "title": "Navigating Rifts in Human-LLM Grounding: Study and Benchmark", "authors": ["Omar Shaikh", "Hussein Mozannar", "Gagan Bansal", "Adam Fourney", "Eric Horvitz"], "categories": ["cs.CL", "cs.HC"], "comment": "ACL 2025, 16 pages, 5 figures", "summary": "Language models excel at following instructions but often struggle with the\ncollaborative aspects of conversation that humans naturally employ. This\nlimitation in grounding -- the process by which conversation participants\nestablish mutual understanding -- can lead to outcomes ranging from frustrated\nusers to serious consequences in high-stakes scenarios. To systematically study\ngrounding challenges in human-LLM interactions, we analyze logs from three\nhuman-assistant datasets: WildChat, MultiWOZ, and Bing Chat. We develop a\ntaxonomy of grounding acts and build models to annotate and forecast grounding\nbehavior. Our findings reveal significant differences in human-human and\nhuman-LLM grounding: LLMs were three times less likely to initiate\nclarification and sixteen times less likely to provide follow-up requests than\nhumans. Additionally, we find that early grounding failures predict later\ninteraction breakdowns. Building on these insights, we introduce Rifts, a\nbenchmark derived from publicly available LLM interaction data containing\nsituations where LLMs fail to initiate grounding. We note that current frontier\nmodels perform poorly on Rifts, highlighting the need to reconsider how we\ntrain and prompt LLMs for human interaction. To this end, we develop a\npreliminary intervention aimed at mitigating grounding failures."}
{"id": "2503.15351", "pdf": "https://arxiv.org/pdf/2503.15351.pdf", "abs": "https://arxiv.org/abs/2503.15351", "title": "SPILL: Domain-Adaptive Intent Clustering based on Selection and Pooling with Large Language Models", "authors": ["I-Fan Lin", "Faegheh Hasibi", "Suzan Verberne"], "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we propose Selection and Pooling with Large Language Models\n(SPILL), an intuitive and domain-adaptive method for intent clustering without\nfine-tuning. Existing embeddings-based clustering methods rely on a few labeled\nexamples or unsupervised fine-tuning to optimize results for each new dataset,\nwhich makes them less generalizable to multiple datasets. Our goal is to make\nthese existing embedders more generalizable to new domain datasets without\nfurther fine-tuning. Inspired by our theoretical derivation and simulation\nresults on the effectiveness of sampling and pooling techniques, we view the\nclustering task as a small-scale selection problem. A good solution to this\nproblem is associated with better clustering performance. Accordingly, we\npropose a two-stage approach: First, for each utterance (referred to as the\nseed), we derive its embedding using an existing embedder. Then, we apply a\ndistance metric to select a pool of candidates close to the seed. Because the\nembedder is not optimized for new datasets, in the second stage, we use an LLM\nto further select utterances from these candidates that share the same intent\nas the seed. Finally, we pool these selected candidates with the seed to derive\na refined embedding for the seed. We found that our method generally\noutperforms directly using an embedder, and it achieves comparable results to\nother state-of-the-art studies, even those that use much larger models and\nrequire fine-tuning, showing its strength and efficiency. Our results indicate\nthat our method enables existing embedders to be further improved without\nadditional fine-tuning, making them more adaptable to new domain datasets.\nAdditionally, viewing the clustering task as a small-scale selection problem\ngives the potential of using LLMs to customize clustering tasks according to\nthe user's goals."}
{"id": "2503.15469", "pdf": "https://arxiv.org/pdf/2503.15469.pdf", "abs": "https://arxiv.org/abs/2503.15469", "title": "A Dual-Directional Context-Aware Test-Time Learning for Text Classification", "authors": ["Dong Xu", "ZhengLin Lai", "MengYao Liao", "Xueliang Li", "Junkai Ji"], "categories": ["cs.CL", "cs.AI"], "comment": "10 pages", "summary": "Text classification assigns text to predefined categories. Traditional\nmethods struggle with complex structures and long-range dependencies. Deep\nlearning with recurrent neural networks and Transformer models has improved\nfeature extraction and context awareness. However, these models still trade off\ninterpretability, efficiency and contextual range. We propose the Dynamic\nBidirectional Elman Attention Network (DBEAN). DBEAN combines bidirectional\ntemporal modeling and self-attention. It dynamically weights critical input\nsegments and preserves computational efficiency."}
{"id": "2503.16031", "pdf": "https://arxiv.org/pdf/2503.16031.pdf", "abs": "https://arxiv.org/abs/2503.16031", "title": "Deceptive Humor: A Synthetic Multilingual Benchmark Dataset for Bridging Fabricated Claims with Humorous Content", "authors": ["Sai Kartheek Reddy Kasu", "Shankar Biradar", "Sunil Saumya"], "categories": ["cs.CL"], "comment": "7 Pages, 2 figures, 7 tables", "summary": "In the evolving landscape of online discourse, misinformation increasingly\nadopts humorous tones to evade detection and gain traction. This work\nintroduces Deceptive Humor as a novel research direction, emphasizing how false\nnarratives, when coated in humor, can become more difficult to detect and more\nlikely to spread. To support research in this space, we present the Deceptive\nHumor Dataset (DHD) a collection of humor-infused comments derived from\nfabricated claims using the ChatGPT-4o model. Each entry is labeled with a\nSatire Level (from 1 for subtle satire to 3 for overt satire) and categorized\ninto five humor types: Dark Humor, Irony, Social Commentary, Wordplay, and\nAbsurdity. The dataset spans English, Telugu, Hindi, Kannada, Tamil, and their\ncode-mixed forms, making it a valuable resource for multilingual analysis. DHD\noffers a structured foundation for understanding how humor can serve as a\nvehicle for the propagation of misinformation, subtly enhancing its reach and\nimpact. Strong baselines are established to encourage further research and\nmodel development in this emerging area."}
{"id": "2503.17279", "pdf": "https://arxiv.org/pdf/2503.17279.pdf", "abs": "https://arxiv.org/abs/2503.17279", "title": "CASE -- Condition-Aware Sentence Embeddings for Conditional Semantic Textual Similarity Measurement", "authors": ["Gaifan Zhang", "Yi Zhou", "Danushka Bollegala"], "categories": ["cs.CL"], "comment": null, "summary": "The meaning conveyed by a sentence often depends on the context in which it\nappears. Despite the progress of sentence embedding methods, it remains unclear\nhow to best modify a sentence embedding conditioned on its context. To address\nthis problem, we propose Condition-Aware Sentence Embeddings (CASE), an\nefficient and accurate method to create an embedding for a sentence under a\ngiven condition. First, CASE creates an embedding for the condition using a\nLarge Language Model (LLM), where the sentence influences the attention scores\ncomputed for the tokens in the condition during pooling. Next, a supervised\nnonlinear projection is learned to reduce the dimensionality of the LLM-based\ntext embeddings. We show that CASE significantly outperforms previously\nproposed Conditional Semantic Textual Similarity (C-STS) methods on an existing\nstandard benchmark dataset. We find that subtracting the condition embedding\nconsistently improves the C-STS performance of LLM-based text embeddings.\nMoreover, we propose a supervised dimensionality reduction method that not only\nreduces the dimensionality of LLM-based embeddings but also significantly\nimproves their performance."}
{"id": "2503.20756", "pdf": "https://arxiv.org/pdf/2503.20756.pdf", "abs": "https://arxiv.org/abs/2503.20756", "title": "ADS-Edit: A Multimodal Knowledge Editing Dataset for Autonomous Driving Systems", "authors": ["Chenxi Wang", "Jizhan Fang", "Xiang Chen", "Bozhong Tian", "Ziwen Xu", "Huajun Chen", "Ningyu Zhang"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "comment": "Work in progress", "summary": "Recent advancements in Large Multimodal Models (LMMs) have shown promise in\nAutonomous Driving Systems (ADS). However, their direct application to ADS is\nhindered by challenges such as misunderstanding of traffic knowledge, complex\nroad conditions, and diverse states of vehicle. To address these challenges, we\npropose the use of Knowledge Editing, which enables targeted modifications to a\nmodel's behavior without the need for full retraining. Meanwhile, we introduce\nADS-Edit, a multimodal knowledge editing dataset specifically designed for ADS,\nwhich includes various real-world scenarios, multiple data types, and\ncomprehensive evaluation metrics. We conduct comprehensive experiments and\nderive several interesting conclusions. We hope that our work will contribute\nto the further advancement of knowledge editing applications in the field of\nautonomous driving. Code and data are available in\nhttps://github.com/zjunlp/EasyEdit."}
{"id": "2503.20850", "pdf": "https://arxiv.org/pdf/2503.20850.pdf", "abs": "https://arxiv.org/abs/2503.20850", "title": "Both Direct and Indirect Evidence Contribute to Dative Alternation Preferences in Language Models", "authors": ["Qing Yao", "Kanishka Misra", "Leonie Weissweiler", "Kyle Mahowald"], "categories": ["cs.CL"], "comment": null, "summary": "Language models (LMs) tend to show human-like preferences on a number of\nsyntactic phenomena, but the extent to which these are attributable to direct\nexposure to the phenomena or more general properties of language is unclear. We\nexplore this with the English dative alternation (DO: \"gave Y the X\" vs. PO:\n\"gave the X to Y\"), using a controlled rearing paradigm wherein we iteratively\ntrain small LMs on systematically manipulated input. We focus on properties\nthat affect the choice of alternant: length and animacy. Both properties are\ndirectly present in datives but also reflect more global tendencies for shorter\nelements to precede longer ones and animates to precede inanimates. First, by\nmanipulating and ablating datives for these biases in the input, we show that\ndirect evidence of length and animacy matters, but easy-first preferences\npersist even without such evidence. Then, using LMs trained on systematically\nperturbed datasets to manipulate global length effects (re-linearizing\nsentences globally while preserving dependency structure), we find that dative\npreferences can emerge from indirect evidence. We conclude that LMs' emergent\nsyntactic preferences come from a mix of direct and indirect sources."}
{"id": "2503.22877", "pdf": "https://arxiv.org/pdf/2503.22877.pdf", "abs": "https://arxiv.org/abs/2503.22877", "title": "Understanding Inequality of LLM Fact-Checking over Geographic Regions with Agent and Retrieval models", "authors": ["Bruno Coelho", "Shujaat Mirza", "Yuyuan Cui", "Christina P√∂pper", "Damon McCoy"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Fact-checking is a potentially useful application of Large Language Models\n(LLMs) to combat the growing dissemination of disinformation. However, the\nperformance of LLMs varies across geographic regions. In this paper, we\nevaluate the factual accuracy of open and private models across a diverse set\nof regions and scenarios.\n  Using a dataset containing 600 fact-checked statements balanced across six\nglobal regions we examine three experimental setups of fact-checking a\nstatement: (1) when just the statement is available, (2) when an LLM-based\nagent with Wikipedia access is utilized, and (3) as a best case scenario when a\nRetrieval-Augmented Generation (RAG) system provided with the official fact\ncheck is employed. Our findings reveal that regardless of the scenario and LLM\nused, including GPT-4, Claude Sonnet, and LLaMA, statements from the Global\nNorth perform substantially better than those from the Global South.\nFurthermore, this gap is broadened for the more realistic case of a Wikipedia\nagent-based system, highlighting that overly general knowledge bases have a\nlimited ability to address region-specific nuances. These results underscore\nthe urgent need for better dataset balancing and robust retrieval strategies to\nenhance LLM fact-checking capabilities, particularly in geographically diverse\ncontexts."}
{"id": "2504.01225", "pdf": "https://arxiv.org/pdf/2504.01225.pdf", "abs": "https://arxiv.org/abs/2504.01225", "title": "A Conformal Risk Control Framework for Granular Word Assessment and Uncertainty Calibration of CLIPScore Quality Estimates", "authors": ["Gon√ßalo Gomes", "Bruno Martins", "Chrysoula Zerva"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Accepted at Findings ACL 2025", "summary": "This study explores current limitations of learned image captioning\nevaluation metrics, specifically the lack of granular assessments for errors\nwithin captions, and the reliance on single-point quality estimates without\nconsidering uncertainty. To address the limitations, we propose a simple yet\neffective strategy for generating and calibrating distributions of CLIPScore\nvalues. Leveraging a model-agnostic conformal risk control framework, we\ncalibrate CLIPScore values for task-specific control variables, tackling the\naforementioned limitations. Experimental results demonstrate that using\nconformal risk control, over score distributions produced with simple methods\nsuch as input masking, can achieve competitive performance compared to more\ncomplex approaches. Our method effectively detects erroneous words, while\nproviding formal guarantees aligned with desired risk levels. It also improves\nthe correlation between uncertainty estimations and prediction errors, thus\nenhancing the overall reliability of caption evaluation metrics."}
{"id": "2504.01919", "pdf": "https://arxiv.org/pdf/2504.01919.pdf", "abs": "https://arxiv.org/abs/2504.01919", "title": "Bridging the Linguistic Divide: A Survey on Leveraging Large Language Models for Machine Translation", "authors": ["Baban Gain", "Dibyanayan Bandyopadhyay", "Asif Ekbal"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The advent of Large Language Models (LLMs) has significantly reshaped the\nlandscape of machine translation (MT), particularly for low-resource languages\nand domains that lack sufficient parallel corpora, linguistic tools, and\ncomputational infrastructure. This survey presents a comprehensive overview of\nrecent progress in leveraging LLMs for MT. We analyze techniques such as\nfew-shot prompting, cross-lingual transfer, and parameter-efficient fine-tuning\n(e.g., LoRA, adapters) that enable effective adaptation to under-resourced\nsettings. The paper also explores synthetic data generation strategies using\nLLMs, including back-translation and lexical augmentation. Additionally, we\ncompare LLM-based translation with traditional encoder-decoder models across\ndiverse language pairs, highlighting the strengths and limitations of each. We\ndiscuss persistent challenges - such as hallucinations, evaluation\ninconsistencies, and inherited biases, while also evaluating emerging\nLLM-driven metrics for translation quality. This survey offers practical\ninsights and outlines future directions for building robust, inclusive, and\nscalable MT systems in the era of large-scale generative models."}
{"id": "2504.03561", "pdf": "https://arxiv.org/pdf/2504.03561.pdf", "abs": "https://arxiv.org/abs/2504.03561", "title": "SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge Refinement", "authors": ["Runnan Fang", "Xiaobin Wang", "Yuan Liang", "Shuofei Qiao", "Jialong Wu", "Zekun Xi", "Ningyu Zhang", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Huajun Chen"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MA"], "comment": "ACL 2025", "summary": "In the interaction between agents and their environments, agents expand their\ncapabilities by planning and executing actions. However, LLM-based agents face\nsubstantial challenges when deployed in novel environments or required to\nnavigate unconventional action spaces. To empower agents to autonomously\nexplore environments, optimize workflows, and enhance their understanding of\nactions, we propose SynWorld, a framework that allows agents to synthesize\npossible scenarios with multi-step action invocation within the action space\nand perform Monte Carlo Tree Search (MCTS) exploration to effectively refine\ntheir action knowledge in the current environment. Our experiments demonstrate\nthat SynWorld is an effective and general approach to learning action knowledge\nin new environments. Code is available at https://github.com/zjunlp/SynWorld."}
{"id": "2504.05214", "pdf": "https://arxiv.org/pdf/2504.05214.pdf", "abs": "https://arxiv.org/abs/2504.05214", "title": "Post-Training Language Models for Continual Relation Extraction", "authors": ["Sefika Efeoglu", "Adrian Paschke", "Sonja Schimmler"], "categories": ["cs.CL"], "comment": "17 pages, Initial Results and Reporting of the work", "summary": "Real-world data, such as news articles, social media posts, and chatbot\nconversations, is inherently dynamic and non-stationary, presenting significant\nchallenges for constructing real-time structured representations through\nknowledge graphs (KGs). Relation Extraction (RE), a fundamental component of KG\ncreation, often struggles to adapt to evolving data when traditional models\nrely on static, outdated datasets. Continual Relation Extraction (CRE) methods\ntackle this issue by incrementally learning new relations while preserving\npreviously acquired knowledge. This study investigates the application of\npre-trained language models (PLMs), specifically large language models (LLMs),\nto CRE, with a focus on leveraging memory replay to address catastrophic\nforgetting. We evaluate decoder-only models (eg, Mistral-7B and Llama2-7B) and\nencoder-decoder models (eg, Flan-T5 Base) on the TACRED and FewRel datasets.\nTask-incremental fine-tuning of LLMs demonstrates superior performance over\nearlier approaches using encoder-only models like BERT on TACRED, excelling in\nseen-task accuracy and overall performance (measured by whole and average\naccuracy), particularly with the Mistral and Flan-T5 models. Results on FewRel\nare similarly promising, achieving second place in whole and average accuracy\nmetrics. This work underscores critical factors in knowledge transfer, language\nmodel architecture, and KG completeness, advancing CRE with LLMs and memory\nreplay for dynamic, real-time relation extraction."}
{"id": "2504.06868", "pdf": "https://arxiv.org/pdf/2504.06868.pdf", "abs": "https://arxiv.org/abs/2504.06868", "title": "Persona Dynamics: Unveiling the Impact of Personality Traits on Agents in Text-Based Games", "authors": ["Seungwon Lim", "Seungbeen Lee", "Dongjun Min", "Youngjae Yu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Artificial agents are increasingly central to complex interactions and\ndecision-making tasks, yet aligning their behaviors with desired human values\nremains an open challenge. In this work, we investigate how human-like\npersonality traits influence agent behavior and performance within text-based\ninteractive environments. We introduce PANDA: Personality Adapted Neural\nDecision Agents, a novel method for projecting human personality traits onto\nagents to guide their behavior. To induce personality in a text-based game\nagent, (i) we train a personality classifier to identify what personality type\nthe agent's actions exhibit, and (ii) we integrate the personality profiles\ndirectly into the agent's policy-learning pipeline. By deploying agents\nembodying 16 distinct personality types across 25 text-based games and\nanalyzing their trajectories, we demonstrate that an agent's action decisions\ncan be guided toward specific personality profiles. Moreover, certain\npersonality types, such as those characterized by higher levels of Openness,\ndisplay marked advantages in performance. These findings underscore the promise\nof personality-adapted agents for fostering more aligned, effective, and\nhuman-centric decision-making in interactive environments."}
{"id": "2504.07527", "pdf": "https://arxiv.org/pdf/2504.07527.pdf", "abs": "https://arxiv.org/abs/2504.07527", "title": "Supervised Optimism Correction: Be Confident When LLMs Are Sure", "authors": ["Junjie Zhang", "Rushuai Yang", "Shunyu Liu", "Ting-En Lin", "Fei Huang", "Yi Chen", "Yongbin Li", "Dacheng Tao"], "categories": ["cs.CL"], "comment": null, "summary": "In this work, we establish a novel theoretical connection between supervised\nfine-tuning and offline reinforcement learning under the token-level Markov\ndecision process, revealing that large language models indeed learn an implicit\n$Q$-function for inference. Through this theoretical lens, we demonstrate that\nthe widely used beam search method suffers from unacceptable over-optimism,\nwhere inference errors are inevitably amplified due to inflated $Q$-value\nestimations of suboptimal steps. To address this limitation, we propose\nSupervised Optimism Correction(SOC), which introduces a simple yet effective\nauxiliary loss for token-level $Q$-value estimations during supervised\nfine-tuning. Specifically, the auxiliary loss employs implicit value\nregularization to boost model confidence in expert-demonstrated responses,\nthereby suppressing over-optimism toward insufficiently supervised responses.\nExtensive experiments on mathematical reasoning benchmarks, including GSM8K,\nMATH, and GAOKAO, showcase the superiority of the proposed SOC with beam search\nacross a series of open-source models."}
{"id": "2504.08838", "pdf": "https://arxiv.org/pdf/2504.08838.pdf", "abs": "https://arxiv.org/abs/2504.08838", "title": "SD$^2$: Self-Distilled Sparse Drafters", "authors": ["Mike Lasby", "Nish Sinnadurai", "Valavan Manohararajah", "Sean Lie", "Yani Ioannou", "Vithursan Thangarasa"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "24 pages", "summary": "Speculative decoding is a powerful technique for reducing the latency of\nLarge Language Models (LLMs), offering a fault-tolerant framework that enables\nthe use of highly compressed draft models. In this work, we introduce\nSelf-Distilled Sparse Drafters (SD$^2$), a novel methodology that leverages\nself-data distillation and fine-grained weight sparsity to produce highly\nefficient and well-aligned draft models. SD$^2$ systematically enhances draft\ntoken acceptance rates while significantly reducing Multiply-Accumulate\noperations (MACs), even in the Universal Assisted Generation (UAG) setting,\nwhere draft and target models originate from different model families. On a\nLlama-3.1-70B target model, SD$^2$ provides a 1.59$\\times$ higher Mean Accepted\nLength (MAL) compared to layer-pruned draft models and reduces MACs by over\n43.87% with a 8.36% reduction in MAL compared to a dense draft models. Our 1.5B\nand 3B unstructured sparse drafters outperform both dense and layer-pruned\nmodels in terms of end-to-end latency improvements; highlighting the potential\nof sparsity-aware fine-tuning and compression strategies to improve LLM\ninference efficiency while maintaining alignment with target models."}
{"id": "2504.09184", "pdf": "https://arxiv.org/pdf/2504.09184.pdf", "abs": "https://arxiv.org/abs/2504.09184", "title": "Parameterized Synthetic Text Generation with SimpleStories", "authors": ["Lennart Finke", "Chandan Sreedhara", "Thomas Dooms", "Mat Allen", "Emerald Zhang", "Juan Diego Rodriguez", "Noa Nabeshima", "Thomas Marshall", "Dan Braun"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present SimpleStories, a large synthetic story dataset in simple language,\nconsisting of 2 million samples each in English and Japanese. Through\nparameterizing prompts at multiple levels of abstraction, we achieve control\nover story characteristics at scale, inducing syntactic and semantic diversity.\nAblations on a newly trained model suite show improved sample efficiency and\nmodel interpretability compared to the TinyStories dataset. We open-source all\nconstituent parts of model creation, hoping to enable novel ways to study the\nend-to-end training process. As a byproduct, we move the frontier regarding the\nfewest-parameter language model that outputs grammatical natural language."}
{"id": "2504.09923", "pdf": "https://arxiv.org/pdf/2504.09923.pdf", "abs": "https://arxiv.org/abs/2504.09923", "title": "Guiding Reasoning in Small Language Models with LLM Assistance", "authors": ["Yujin Kim", "Euiin Yi", "Minu Kim", "Se-Young Yun", "Taehyeon Kim"], "categories": ["cs.CL"], "comment": "20 pages, 12 figures, 9 tables", "summary": "The limited reasoning capabilities of small language models (SLMs) cast doubt\non their suitability for tasks demanding deep, multi-step logical deduction.\nThis paper introduces a framework called Small Reasons, Large Hints (SMART),\nwhich selectively augments SLM reasoning with targeted guidance from large\nlanguage models (LLMs). Inspired by the concept of cognitive scaffolding, SMART\nemploys a score-based evaluation to identify uncertain reasoning steps and\ninjects corrective LLM-generated reasoning only when necessary. By framing\nstructured reasoning as an optimal policy search, our approach steers the\nreasoning trajectory toward correct solutions without exhaustive sampling. Our\nexperiments on mathematical reasoning datasets demonstrate that targeted\nexternal scaffolding significantly improves performance, paving the way for\ncollaborative use of both SLM and LLM to tackle complex reasoning tasks that\nare currently unsolvable by SLMs alone."}
{"id": "2504.11042", "pdf": "https://arxiv.org/pdf/2504.11042.pdf", "abs": "https://arxiv.org/abs/2504.11042", "title": "LazyReview A Dataset for Uncovering Lazy Thinking in NLP Peer Reviews", "authors": ["Sukannya Purkayastha", "Zhuang Li", "Anne Lauscher", "Lizhen Qu", "Iryna Gurevych"], "categories": ["cs.CL"], "comment": "Accepted at ACL 2025: 29 pages, 18 Figures, 15 Tables", "summary": "Peer review is a cornerstone of quality control in scientific publishing.\nWith the increasing workload, the unintended use of `quick' heuristics,\nreferred to as lazy thinking, has emerged as a recurring issue compromising\nreview quality. Automated methods to detect such heuristics can help improve\nthe peer-reviewing process. However, there is limited NLP research on this\nissue, and no real-world dataset exists to support the development of detection\ntools. This work introduces LazyReview, a dataset of peer-review sentences\nannotated with fine-grained lazy thinking categories. Our analysis reveals that\nLarge Language Models (LLMs) struggle to detect these instances in a zero-shot\nsetting. However, instruction-based fine-tuning on our dataset significantly\nboosts performance by 10-20 performance points, highlighting the importance of\nhigh-quality training data. Furthermore, a controlled experiment demonstrates\nthat reviews revised with lazy thinking feedback are more comprehensive and\nactionable than those written without such feedback. We will release our\ndataset and the enhanced guidelines that can be used to train junior reviewers\nin the community. (Code available here:\nhttps://github.com/UKPLab/arxiv2025-lazy-review)"}
{"id": "2505.00814", "pdf": "https://arxiv.org/pdf/2505.00814.pdf", "abs": "https://arxiv.org/abs/2505.00814", "title": "Knowledge-augmented Pre-trained Language Models for Biomedical Relation Extraction", "authors": ["Mario S√§nger", "Ulf Leser"], "categories": ["cs.CL"], "comment": null, "summary": "Automatic relationship extraction (RE) from biomedical literature is critical\nfor managing the vast amount of scientific knowledge produced each year. In\nrecent years, utilizing pre-trained language models (PLMs) has become the\nprevalent approach in RE. Several studies report improved performance when\nincorporating additional context information while fine-tuning PLMs for RE.\nHowever, variations in the PLMs applied, the databases used for augmentation,\nhyper-parameter optimization, and evaluation methods complicate direct\ncomparisons between studies and raise questions about the generalizability of\nthese findings. Our study addresses this research gap by evaluating PLMs\nenhanced with contextual information on five datasets spanning four relation\nscenarios within a consistent evaluation framework. We evaluate three baseline\nPLMs and first conduct extensive hyperparameter optimization. After selecting\nthe top-performing model, we enhance it with additional data, including textual\nentity descriptions, relational information from knowledge graphs, and\nmolecular structure encodings. Our findings illustrate the importance of i) the\nchoice of the underlying language model and ii) a comprehensive hyperparameter\noptimization for achieving strong extraction performance. Although inclusion of\ncontext information yield only minor overall improvements, an ablation study\nreveals substantial benefits for smaller PLMs when such external data was\nincluded during fine-tuning."}
{"id": "2505.02518", "pdf": "https://arxiv.org/pdf/2505.02518.pdf", "abs": "https://arxiv.org/abs/2505.02518", "title": "Bemba Speech Translation: Exploring a Low-Resource African Language", "authors": ["Muhammad Hazim Al Farouq", "Aman Kassahun Wassie", "Yasmin Moslem"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "IWSLT 2025", "summary": "This paper describes our system submission to the International Conference on\nSpoken Language Translation (IWSLT 2025), low-resource languages track, namely\nfor Bemba-to-English speech translation. We built cascaded speech translation\nsystems based on Whisper and NLLB-200, and employed data augmentation\ntechniques, such as back-translation. We investigate the effect of using\nsynthetic data and discuss our experimental setup."}
{"id": "2505.07659", "pdf": "https://arxiv.org/pdf/2505.07659.pdf", "abs": "https://arxiv.org/abs/2505.07659", "title": "Using Information Theory to Characterize Prosodic Typology: The Case of Tone, Pitch-Accent and Stress-Accent", "authors": ["Ethan Gotlieb Wilcox", "Cui Ding", "Giovanni Acampa", "Tiago Pimentel", "Alex Warstadt", "Tamar I. Regev"], "categories": ["cs.CL"], "comment": null, "summary": "This paper argues that the relationship between lexical identity and prosody\n-- one well-studied parameter of linguistic variation -- can be characterized\nusing information theory. We predict that languages that use prosody to make\nlexical distinctions should exhibit a higher mutual information between word\nidentity and prosody, compared to languages that don't. We test this hypothesis\nin the domain of pitch, which is used to make lexical distinctions in tonal\nlanguages, like Cantonese. We use a dataset of speakers reading sentences aloud\nin ten languages across five language families to estimate the mutual\ninformation between the text and their pitch curves. We find that, across\nlanguages, pitch curves display similar amounts of entropy. However, these\ncurves are easier to predict given their associated text in the tonal\nlanguages, compared to pitch- and stress-accent languages, and thus the mutual\ninformation is higher in these languages, supporting our hypothesis. Our\nresults support perspectives that view linguistic typology as gradient, rather\nthan categorical."}
{"id": "2505.07784", "pdf": "https://arxiv.org/pdf/2505.07784.pdf", "abs": "https://arxiv.org/abs/2505.07784", "title": "Domain Regeneration: How well do LLMs match syntactic properties of text domains?", "authors": ["Da Ju", "Hagen Blix", "Adina Williams"], "categories": ["cs.CL"], "comment": null, "summary": "Recent improvement in large language model performance have, in all\nlikelihood, been accompanied by improvement in how well they can approximate\nthe distribution of their training data. In this work, we explore the following\nquestion: which properties of text domains do LLMs faithfully approximate, and\nhow well do they do so? Applying observational approaches familiar from corpus\nlinguistics, we prompt a commonly used, opensource LLM to regenerate text from\ntwo domains of permissively licensed English text which are often contained in\nLLM training data -- Wikipedia and news text. This regeneration paradigm allows\nus to investigate whether LLMs can faithfully match the original human text\ndomains in a fairly semantically-controlled setting. We investigate varying\nlevels of syntactic abstraction, from more simple properties like sentence\nlength, and article readability, to more complex and higher order properties\nsuch as dependency tag distribution, parse depth, and parse complexity. We find\nthat the majority of the regenerated distributions show a shifted mean, a lower\nstandard deviation, and a reduction of the long tail, as compared to the human\noriginals."}
{"id": "2505.10719", "pdf": "https://arxiv.org/pdf/2505.10719.pdf", "abs": "https://arxiv.org/abs/2505.10719", "title": "Tracr-Injection: Distilling Algorithms into Pre-trained Language Models", "authors": ["Tom√°s Vergara-Browne", "√Ålvaro Soto"], "categories": ["cs.CL"], "comment": "ACL Findings 2025", "summary": "Motivated by the surge of large language models, there has been a push to\nformally characterize the symbolic abilities intrinsic to the transformer\narchitecture. A programming language, called RASP, has been proposed, which can\nbe directly compiled into transformer weights to implement these algorithms.\nHowever, the tasks that can be implemented in RASP are often uncommon to learn\nfrom natural unsupervised data, showing a mismatch between theoretical\ncapabilities of the transformer architecture, and the practical learnability of\nthese capabilities from unsupervised data. We propose tracr-injection, a method\nthat allows us to distill algorithms written in RASP directly into a\npre-trained language model. We showcase our method by injecting 3 different\nalgorithms into a language model. We show how our method creates an\ninterpretable subspace within the model's residual stream, which can be decoded\ninto the variables present in the code of the RASP algorithm. Additionally, we\nfound that the proposed method can improve out-of-distribution performance\ncompared to our baseline, indicating that indeed a more symbolic mechanism is\ntaking place in the inner workings of the model. We release the code used to\nrun our experiments."}
{"id": "2505.11297", "pdf": "https://arxiv.org/pdf/2505.11297.pdf", "abs": "https://arxiv.org/abs/2505.11297", "title": "Probing Subphonemes in Morphology Models", "authors": ["Gal Astrach", "Yuval Pinter"], "categories": ["cs.CL"], "comment": "ACL 2025 (findings)", "summary": "Transformers have achieved state-of-the-art performance in morphological\ninflection tasks, yet their ability to generalize across languages and\nmorphological rules remains limited. One possible explanation for this behavior\ncan be the degree to which these models are able to capture implicit phenomena\nat the phonological and subphonemic levels. We introduce a language-agnostic\nprobing method to investigate phonological feature encoding in transformers\ntrained directly on phonemes, and perform it across seven morphologically\ndiverse languages. We show that phonological features which are local, such as\nfinal-obstruent devoicing in Turkish, are captured well in phoneme embeddings,\nwhereas long-distance dependencies like vowel harmony are better represented in\nthe transformer's encoder. Finally, we discuss how these findings inform\nempirical strategies for training morphological models, particularly regarding\nthe role of subphonemic feature acquisition."}
{"id": "2505.11726", "pdf": "https://arxiv.org/pdf/2505.11726.pdf", "abs": "https://arxiv.org/abs/2505.11726", "title": "Disambiguating Reference in Visually Grounded Dialogues through Joint Modeling of Textual and Multimodal Semantic Structures", "authors": ["Shun Inadumi", "Nobuhiro Ueda", "Koichiro Yoshino"], "categories": ["cs.CL"], "comment": "ACL2025 main. Code available at https://github.com/SInadumi/mmrr", "summary": "Multimodal reference resolution, including phrase grounding, aims to\nunderstand the semantic relations between mentions and real-world objects.\nPhrase grounding between images and their captions is a well-established task.\nIn contrast, for real-world applications, it is essential to integrate textual\nand multimodal reference resolution to unravel the reference relations within\ndialogue, especially in handling ambiguities caused by pronouns and ellipses.\nThis paper presents a framework that unifies textual and multimodal reference\nresolution by mapping mention embeddings to object embeddings and selecting\nmentions or objects based on their similarity. Our experiments show that\nlearning textual reference resolution, such as coreference resolution and\npredicate-argument structure analysis, positively affects performance in\nmultimodal reference resolution. In particular, our model with coreference\nresolution performs better in pronoun phrase grounding than representative\nmodels for this task, MDETR and GLIP. Our qualitative analysis demonstrates\nthat incorporating textual reference relations strengthens the confidence\nscores between mentions, including pronouns and predicates, and objects, which\ncan reduce the ambiguities that arise in visually grounded dialogues."}
{"id": "2505.11958", "pdf": "https://arxiv.org/pdf/2505.11958.pdf", "abs": "https://arxiv.org/abs/2505.11958", "title": "Counterspeech the ultimate shield! Multi-Conditioned Counterspeech Generation through Attributed Prefix Learning", "authors": ["Aswini Kumar", "Anil Bandhakavi", "Tanmoy Chakraborty"], "categories": ["cs.CL"], "comment": "Accepted in ACL 2025 Main Conference", "summary": "Counterspeech has proven to be a powerful tool to combat hate speech online.\nPrevious studies have focused on generating counterspeech conditioned only on\nspecific intents (single attributed). However, a holistic approach considering\nmultiple attributes simultaneously can yield more nuanced and effective\nresponses. Here, we introduce HiPPrO, Hierarchical Prefix learning with\nPreference Optimization, a novel two-stage framework that utilizes the\neffectiveness of attribute-specific prefix embedding spaces hierarchically\noptimized during the counterspeech generation process in the first phase.\nThereafter, we incorporate both reference and reward-free preference\noptimization to generate more constructive counterspeech. Furthermore, we\nextend IntentCONANv2 by annotating all 13,973 counterspeech instances with\nemotion labels by five annotators. HiPPrO leverages hierarchical prefix\noptimization to integrate these dual attributes effectively. An extensive\nevaluation demonstrates that HiPPrO achieves a ~38 % improvement in intent\nconformity and a ~3 %, ~2 %, ~3 % improvement in Rouge-1, Rouge-2, and Rouge-L,\nrespectively, compared to several baseline models. Human evaluations further\nsubstantiate the superiority of our approach, highlighting the enhanced\nrelevance and appropriateness of the generated counterspeech. This work\nunderscores the potential of multi-attribute conditioning in advancing the\nefficacy of counterspeech generation systems. Our code is available on Github\nand dataset is open-sourced on Hugging-face."}
{"id": "2505.12212", "pdf": "https://arxiv.org/pdf/2505.12212.pdf", "abs": "https://arxiv.org/abs/2505.12212", "title": "Data Whisperer: Efficient Data Selection for Task-Specific LLM Fine-Tuning via Few-Shot In-Context Learning", "authors": ["Shaobo Wang", "Xiangqi Jin", "Ziming Wang", "Jize Wang", "Jiajun Zhang", "Kaixin Li", "Zichen Wen", "Zhong Li", "Conghui He", "Xuming Hu", "Linfeng Zhang"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 main, 18 pages, 8 figures, 6 tables", "summary": "Fine-tuning large language models (LLMs) on task-specific data is essential\nfor their effective deployment. As dataset sizes grow, efficiently selecting\noptimal subsets for training becomes crucial to balancing performance and\ncomputational costs. Traditional data selection methods often require\nfine-tuning a scoring model on the target dataset, which is time-consuming and\nresource-intensive, or rely on heuristics that fail to fully leverage the\nmodel's predictive capabilities. To address these challenges, we propose Data\nWhisperer, an efficient, training-free, attention-based method that leverages\nfew-shot in-context learning with the model to be fine-tuned. Comprehensive\nevaluations were conducted on both raw and synthetic datasets across diverse\ntasks and models. Notably, Data Whisperer achieves superior performance\ncompared to the full GSM8K dataset on the Llama-3-8B-Instruct model, using just\n10% of the data, and outperforms existing methods with a 3.1-point improvement\nand a 7.4$\\times$ speedup. The code is available at\nhttps://github.com/gszfwsb/Data-Whisperer."}
{"id": "2505.12727", "pdf": "https://arxiv.org/pdf/2505.12727.pdf", "abs": "https://arxiv.org/abs/2505.12727", "title": "What is Stigma Attributed to? A Theory-Grounded, Expert-Annotated Interview Corpus for Demystifying Mental-Health Stigma", "authors": ["Han Meng", "Yancan Chen", "Yunan Li", "Yitian Yang", "Jungup Lee", "Renwen Zhang", "Yi-Chieh Lee"], "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": "Accepted to ACL 2025 Main Conference, 38 Pages", "summary": "Mental-health stigma remains a pervasive social problem that hampers\ntreatment-seeking and recovery. Existing resources for training neural models\nto finely classify such stigma are limited, relying primarily on social-media\nor synthetic data without theoretical underpinnings. To remedy this gap, we\npresent an expert-annotated, theory-informed corpus of human-chatbot\ninterviews, comprising 4,141 snippets from 684 participants with documented\nsocio-cultural backgrounds. Our experiments benchmark state-of-the-art neural\nmodels and empirically unpack the challenges of stigma detection. This dataset\ncan facilitate research on computationally detecting, neutralizing, and\ncounteracting mental-health stigma. Our corpus is openly available at\nhttps://github.com/HanMeng2004/Mental-Health-Stigma-Interview-Corpus."}
{"id": "2505.12942", "pdf": "https://arxiv.org/pdf/2505.12942.pdf", "abs": "https://arxiv.org/abs/2505.12942", "title": "A3 : an Analytical Low-Rank Approximation Framework for Attention", "authors": ["Jeffrey T. H. Wong", "Cheng Zhang", "Xinye Cao", "Pedro Gimenes", "George A. Constantinides", "Wayne Luk", "Yiren Zhao"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance."}
{"id": "2505.13173", "pdf": "https://arxiv.org/pdf/2505.13173.pdf", "abs": "https://arxiv.org/abs/2505.13173", "title": "A Case Study of Cross-Lingual Zero-Shot Generalization for Classical Languages in LLMs", "authors": ["V. S. D. S. Mahesh Akavarapu", "Hrishikesh Terdalkar", "Pramit Bhattacharyya", "Shubhangi Agarwal", "Vishakha Deulgaonkar", "Pralay Manna", "Chaitali Dangarikar", "Arnab Bhattacharya"], "categories": ["cs.CL", "I.2.7"], "comment": "Accepted to ACL 2025 Findings", "summary": "Large Language Models (LLMs) have demonstrated remarkable generalization\ncapabilities across diverse tasks and languages. In this study, we focus on\nnatural language understanding in three classical languages -- Sanskrit,\nAncient Greek and Latin -- to investigate the factors affecting cross-lingual\nzero-shot generalization. First, we explore named entity recognition and\nmachine translation into English. While LLMs perform equal to or better than\nfine-tuned baselines on out-of-domain data, smaller models often struggle,\nespecially with niche or abstract entity types. In addition, we concentrate on\nSanskrit by presenting a factoid question-answering (QA) dataset and show that\nincorporating context via retrieval-augmented generation approach significantly\nboosts performance. In contrast, we observe pronounced performance drops for\nsmaller LLMs across these QA tasks. These results suggest model scale as an\nimportant factor influencing cross-lingual generalization. Assuming that models\nused such as GPT-4o and Llama-3.1 are not instruction fine-tuned on classical\nlanguages, our findings provide insights into how LLMs may generalize on these\nlanguages and their consequent utility in classical studies."}
{"id": "2505.13282", "pdf": "https://arxiv.org/pdf/2505.13282.pdf", "abs": "https://arxiv.org/abs/2505.13282", "title": "Rank, Chunk and Expand: Lineage-Oriented Reasoning for Taxonomy Expansion", "authors": ["Sahil Mishra", "Kumar Arjun", "Tanmoy Chakraborty"], "categories": ["cs.CL"], "comment": "Accepted in the Findings of ACL 2025", "summary": "Taxonomies are hierarchical knowledge graphs crucial for recommendation\nsystems, and web applications. As data grows, expanding taxonomies is\nessential, but existing methods face key challenges: (1) discriminative models\nstruggle with representation limits and generalization, while (2) generative\nmethods either process all candidates at once, introducing noise and exceeding\ncontext limits, or discard relevant entities by selecting noisy candidates. We\npropose LORex (Lineage-Oriented Reasoning for Taxonomy Expansion), a\nplug-and-play framework that combines discriminative ranking and generative\nreasoning for efficient taxonomy expansion. Unlike prior methods, LORex ranks\nand chunks candidate terms into batches, filtering noise and iteratively\nrefining selections by reasoning candidates' hierarchy to ensure contextual\nefficiency. Extensive experiments across four benchmarks and twelve baselines\nshow that LORex improves accuracy by 12% and Wu & Palmer similarity by 5% over\nstate-of-the-art methods."}
{"id": "2505.14070", "pdf": "https://arxiv.org/pdf/2505.14070.pdf", "abs": "https://arxiv.org/abs/2505.14070", "title": "Enhancing LLMs via High-Knowledge Data Selection", "authors": ["Feiyu Duan", "Xuemiao Zhang", "Sirui Wang", "Haoran Que", "Yuqi Liu", "Wenge Rong", "Xunliang Cai"], "categories": ["cs.CL"], "comment": null, "summary": "The performance of Large Language Models (LLMs) is intrinsically linked to\nthe quality of its training data. Although several studies have proposed\nmethods for high-quality data selection, they do not consider the importance of\nknowledge richness in text corpora. In this paper, we propose a novel and\ngradient-free High-Knowledge Scorer (HKS) to select high-quality data from the\ndimension of knowledge, to alleviate the problem of knowledge scarcity in the\npre-trained corpus. We propose a comprehensive multi-domain knowledge element\npool and introduce knowledge density and coverage as metrics to assess the\nknowledge content of the text. Based on this, we propose a comprehensive\nknowledge scorer to select data with intensive knowledge, which can also be\nutilized for domain-specific high-knowledge data selection by restricting\nknowledge elements to the specific domain. We train models on a high-knowledge\nbilingual dataset, and experimental results demonstrate that our scorer\nimproves the model's performance in knowledge-intensive and general\ncomprehension tasks, and is effective in enhancing both the generic and\ndomain-specific capabilities of the model."}
{"id": "2505.14577", "pdf": "https://arxiv.org/pdf/2505.14577.pdf", "abs": "https://arxiv.org/abs/2505.14577", "title": "TRATES: Trait-Specific Rubric-Assisted Cross-Prompt Essay Scoring", "authors": ["Sohaila Eltanbouly", "Salam Albatarni", "Tamer Elsayed"], "categories": ["cs.CL"], "comment": "Accepted at ACL 2025 Findings", "summary": "Research on holistic Automated Essay Scoring (AES) is long-dated; yet, there\nis a notable lack of attention for assessing essays according to individual\ntraits. In this work, we propose TRATES, a novel trait-specific and\nrubric-based cross-prompt AES framework that is generic yet specific to the\nunderlying trait. The framework leverages a Large Language Model (LLM) that\nutilizes the trait grading rubrics to generate trait-specific features\n(represented by assessment questions), then assesses those features given an\nessay. The trait-specific features are eventually combined with generic\nwriting-quality and prompt-specific features to train a simple classical\nregression model that predicts trait scores of essays from an unseen prompt.\nExperiments show that TRATES achieves a new state-of-the-art performance across\nall traits on a widely-used dataset, with the generated LLM-based features\nbeing the most significant."}
{"id": "2505.15209", "pdf": "https://arxiv.org/pdf/2505.15209.pdf", "abs": "https://arxiv.org/abs/2505.15209", "title": "DUSK: Do Not Unlearn Shared Knowledge", "authors": ["Wonje Jeung", "Sangyeon Yoon", "Hyesoo Hong", "Soeun Kim", "Seungju Han", "Youngjae Yu", "Albert No"], "categories": ["cs.CL"], "comment": "Code and models are available at https://ai-isl.github.io/dusk", "summary": "Large language models (LLMs) are increasingly deployed in real-world\napplications, raising concerns about the unauthorized use of copyrighted or\nsensitive data. Machine unlearning aims to remove such 'forget' data while\npreserving utility and information from the 'retain' set. However, existing\nevaluations typically assume that forget and retain sets are fully disjoint,\noverlooking realistic scenarios where they share overlapping content. For\ninstance, a news article may need to be unlearned, even though the same event,\nsuch as an earthquake in Japan, is also described factually on Wikipedia.\nEffective unlearning should remove the specific phrasing of the news article\nwhile preserving publicly supported facts. In this paper, we introduce DUSK, a\nbenchmark designed to evaluate unlearning methods under realistic data overlap.\nDUSK constructs document sets that describe the same factual content in\ndifferent styles, with some shared information appearing across all sets and\nother content remaining unique to each. When one set is designated for\nunlearning, an ideal method should remove its unique content while preserving\nshared facts. We define seven evaluation metrics to assess whether unlearning\nmethods can achieve this selective removal. Our evaluation of nine recent\nunlearning methods reveals a key limitation: while most can remove\nsurface-level text, they often fail to erase deeper, context-specific knowledge\nwithout damaging shared content. We release DUSK as a public benchmark to\nsupport the development of more precise and reliable unlearning techniques for\nreal-world applications."}
{"id": "2505.16415", "pdf": "https://arxiv.org/pdf/2505.16415.pdf", "abs": "https://arxiv.org/abs/2505.16415", "title": "Attributing Response to Context: A Jensen-Shannon Divergence Driven Mechanistic Study of Context Attribution in Retrieval-Augmented Generation", "authors": ["Ruizhe Li", "Chen Chen", "Yuchen Hu", "Yanjun Gao", "Xi Wang", "Emine Yilmaz"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Work in process", "summary": "Retrieval-Augmented Generation (RAG) leverages large language models (LLMs)\ncombined with external contexts to enhance the accuracy and reliability of\ngenerated responses. However, reliably attributing generated content to\nspecific context segments, context attribution, remains challenging due to the\ncomputationally intensive nature of current methods, which often require\nextensive fine-tuning or human annotation. In this work, we introduce a novel\nJensen-Shannon Divergence driven method to Attribute Response to Context\n(ARC-JSD), enabling efficient and accurate identification of essential context\nsentences without additional fine-tuning or surrogate modelling. Evaluations on\na wide range of RAG benchmarks, such as TyDi QA, Hotpot QA, and Musique, using\ninstruction-tuned LLMs in different scales demonstrate superior accuracy and\nsignificant computational efficiency improvements compared to the previous\nsurrogate-based method. Furthermore, our mechanistic analysis reveals specific\nattention heads and multilayer perceptron (MLP) layers responsible for context\nattribution, providing valuable insights into the internal workings of RAG\nmodels. Our code is available at https://github.com/ruizheliUOA/ARC_JSD"}
{"id": "2505.16900", "pdf": "https://arxiv.org/pdf/2505.16900.pdf", "abs": "https://arxiv.org/abs/2505.16900", "title": "Power-Law Decay Loss for Large Language Model Finetuning: A Theory Perspective", "authors": ["Jintian Shao"], "categories": ["cs.CL", "cs.LG"], "comment": "We are withdrawing this submission as the underlying experiment is\n  currently incomplete. We require additional time to gather more data and\n  supplement the existing findings to ensure a comprehensive and robust\n  presentation. We intend to resubmit once these additions are finalized", "summary": "During the finetuning stage of text generation tasks, standard cross-entropy\nloss treats all tokens equally. This can lead models to overemphasize\nhigh-frequency, low-information tokens, neglecting lower-frequency tokens\ncrucial for specificity and informativeness in generated content. This paper\nintroduces a novel loss function, Power-Law Decay Loss (PDL), specifically\ndesigned to optimize the finetuning process for text generation. The core\nmotivation for PDL stems from observations in information theory and\nlinguistics: the informativeness of a token is often inversely proportional to\nits frequency of occurrence. PDL re-weights the contribution of each token in\nthe standard cross-entropy loss based on its frequency in the training corpus,\nfollowing a power-law decay. Specifically, the weights for high-frequency\ntokens are reduced, while low-frequency, information-dense tokens are assigned\nhigher weights. This mechanism guides the model during finetuning to focus more\non learning and generating tokens that convey specific and unique information,\nthereby enhancing the quality, diversity, and informativeness of the generated\ntext. We theoretically elaborate on the motivation and construction of PDL and\ndiscuss its potential applications and advantages across various text\ngeneration finetuning tasks, such as abstractive summarization, dialogue\nsystems, and style transfer."}
{"id": "2505.17362", "pdf": "https://arxiv.org/pdf/2505.17362.pdf", "abs": "https://arxiv.org/abs/2505.17362", "title": "A Fully Generative Motivational Interviewing Counsellor Chatbot for Moving Smokers Towards the Decision to Quit", "authors": ["Zafarullah Mahmood", "Soliman Ali", "Jiading Zhu", "Mohamed Abdelwahab", "Michelle Yu Collins", "Sihan Chen", "Yi Cheng Zhao", "Jodi Wolff", "Osnat Melamed", "Nadia Minian", "Marta Maslej", "Carolynne Cooper", "Matt Ratto", "Peter Selby", "Jonathan Rose"], "categories": ["cs.CL", "cs.AI"], "comment": "To be published in the Findings of the 63rd Annual Meeting of the\n  Association for Computational Linguistics (ACL), Vienna, Austria, 2025", "summary": "The conversational capabilities of Large Language Models (LLMs) suggest that\nthey may be able to perform as automated talk therapists. It is crucial to know\nif these systems would be effective and adhere to known standards. We present a\ncounsellor chatbot that focuses on motivating tobacco smokers to quit smoking.\nIt uses a state-of-the-art LLM and a widely applied therapeutic approach called\nMotivational Interviewing (MI), and was evolved in collaboration with\nclinician-scientists with expertise in MI. We also describe and validate an\nautomated assessment of both the chatbot's adherence to MI and client\nresponses. The chatbot was tested on 106 participants, and their confidence\nthat they could succeed in quitting smoking was measured before the\nconversation and one week later. Participants' confidence increased by an\naverage of 1.7 on a 0-10 scale. The automated assessment of the chatbot showed\nadherence to MI standards in 98% of utterances, higher than human counsellors.\nThe chatbot scored well on a participant-reported metric of perceived empathy\nbut lower than typical human counsellors. Furthermore, participants' language\nindicated a good level of motivation to change, a key goal in MI. These results\nsuggest that the automation of talk therapy with a modern LLM has promise."}
{"id": "2505.17446", "pdf": "https://arxiv.org/pdf/2505.17446.pdf", "abs": "https://arxiv.org/abs/2505.17446", "title": "Exploring the Effect of Segmentation and Vocabulary Size on Speech Tokenization for Speech Language Models", "authors": ["Shunsuke Kando", "Yusuke Miyao", "Shinnosuke Takamichi"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to Interspeech2025", "summary": "The purpose of speech tokenization is to transform a speech signal into a\nsequence of discrete representations, serving as the foundation for speech\nlanguage models (SLMs). While speech tokenization has many options, their\neffect on the performance of SLMs remains unclear. This paper investigates two\nkey aspects of speech tokenization: the segmentation width and the cluster size\nof discrete units. First, we segment speech signals into fixed/variable widths\nand pooled representations. We then train K-means models in multiple cluster\nsizes. Through the evaluation on zero-shot spoken language understanding\nbenchmarks, we find the positive effect of moderately coarse segmentation and\nbigger cluster size. Notably, among the best-performing models, the most\nefficient one achieves a 50% reduction in training data and a 70% decrease in\ntraining runtime. Our analysis highlights the importance of combining multiple\ntokens to enhance fine-grained spoken language understanding."}
{"id": "2505.17536", "pdf": "https://arxiv.org/pdf/2505.17536.pdf", "abs": "https://arxiv.org/abs/2505.17536", "title": "Multimodal Conversation Structure Understanding", "authors": ["Kent K. Chang", "Mackenzie Hanh Cramer", "Anna Ho", "Ti Ti Nguyen", "Yilin Yuan", "David Bamman"], "categories": ["cs.CL"], "comment": null, "summary": "Conversations are usually structured by roles -- who is speaking, who's being\naddressed, and who's listening -- and unfold in threads that break with changes\nin speaker floor or topical focus. While large language models (LLMs) have\nshown incredible capabilities in dialogue and reasoning, their ability to\nunderstand fine-grained conversational structure, especially in multi-modal,\nmulti-party settings, remains underexplored. To address this gap, we introduce\na suite of tasks focused on conversational role attribution (speaker,\naddressees, side-participants) and conversation threading (utterance linking\nand clustering), drawing on conversation analysis and sociolinguistics. To\nsupport those tasks, we present a human annotated dataset of 4,398 annotations\nfor speakers and reply-to relationship, 5,755 addressees, and 3,142\nside-participants.\n  We evaluate popular audio-visual LLMs and vision-language models on our\ndataset, and our experimental results suggest that multimodal conversational\nstructure understanding remains challenging. The most performant audio-visual\nLLM outperforms all vision-language models across all metrics, especially in\nspeaker and addressee recognition. However, its performance drops significantly\nwhen conversation participants are anonymized. The number of conversation\nparticipants in a clip is the strongest negative predictor of role-attribution\nperformance, while acoustic clarity (measured by pitch and spectral centroid)\nand detected face coverage yield positive associations. We hope this work lays\nthe groundwork for future evaluation and development of multimodal LLMs that\ncan reason more effectively about conversation structure."}
{"id": "2505.17747", "pdf": "https://arxiv.org/pdf/2505.17747.pdf", "abs": "https://arxiv.org/abs/2505.17747", "title": "Discriminating Form and Meaning in Multilingual Models with Minimal-Pair ABX Tasks", "authors": ["Maureen de Seyssel", "Jie Chi", "Skyler Seto", "Maartje ter Hoeve", "Masha Fedzechkina", "Natalie Schluter"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce a set of training-free ABX-style discrimination tasks to\nevaluate how multilingual language models represent language identity (form)\nand semantic content (meaning). Inspired from speech processing, these\nzero-shot tasks measure whether minimal differences in representation can be\nreliably detected. This offers a flexible and interpretable alternative to\nprobing. Applied to XLM-R (Conneau et al, 2020) across pretraining checkpoints\nand layers, we find that language discrimination declines over training and\nbecomes concentrated in lower layers, while meaning discrimination strengthens\nover time and stabilizes in deeper layers. We then explore probing tasks,\nshowing some alignment between our metrics and linguistic learning performance.\nOur results position ABX tasks as a lightweight framework for analyzing the\nstructure of multilingual representations."}
{"id": "2505.18557", "pdf": "https://arxiv.org/pdf/2505.18557.pdf", "abs": "https://arxiv.org/abs/2505.18557", "title": "TAG-INSTRUCT: Controlled Instruction Complexity Enhancement through Structure-based Augmentation", "authors": ["He Zhu", "Zhiwen Ruan", "Junyou Su", "Xingwei He", "Yun Chen", "Wenjia Zhang", "Guanhua Chen"], "categories": ["cs.CL"], "comment": null, "summary": "High-quality instruction data is crucial for developing large language models\n(LLMs), yet existing approaches struggle to effectively control instruction\ncomplexity. We present TAG-INSTRUCT, a novel framework that enhances\ninstruction complexity through structured semantic compression and controlled\ndifficulty augmentation. Unlike previous prompt-based methods operating on raw\ntext, TAG-INSTRUCT compresses instructions into a compact tag space and\nsystematically enhances complexity through RL-guided tag expansion. Through\nextensive experiments, we show that TAG-INSTRUCT outperforms existing\ninstruction complexity augmentation approaches. Our analysis reveals that\noperating in tag space provides superior controllability and stability across\ndifferent instruction synthesis frameworks."}
{"id": "2505.18927", "pdf": "https://arxiv.org/pdf/2505.18927.pdf", "abs": "https://arxiv.org/abs/2505.18927", "title": "Moderating Harm: Benchmarking Large Language Models for Cyberbullying Detection in YouTube Comments", "authors": ["Amel Muminovic"], "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 3 tables, 1 figure", "summary": "As online platforms grow, comment sections increasingly host harassment that\nundermines user experience and well-being. This study benchmarks three leading\nlarge language models, OpenAI GPT-4.1, Google Gemini 1.5 Pro, and Anthropic\nClaude 3 Opus, on a corpus of 5,080 YouTube comments sampled from high-abuse\nthreads in gaming, lifestyle, food vlog, and music channels. The dataset\ncomprises 1,334 harmful and 3,746 non-harmful messages in English, Arabic, and\nIndonesian, annotated independently by two reviewers with substantial agreement\n(Cohen's kappa = 0.83). Using a unified prompt and deterministic settings,\nGPT-4.1 achieved the best overall balance with an F1 score of 0.863, precision\nof 0.887, and recall of 0.841. Gemini flagged the highest share of harmful\nposts (recall = 0.875) but its precision fell to 0.767 due to frequent false\npositives. Claude delivered the highest precision at 0.920 and the lowest\nfalse-positive rate of 0.022, yet its recall dropped to 0.720. Qualitative\nanalysis showed that all three models struggle with sarcasm, coded insults, and\nmixed-language slang. These results underscore the need for moderation\npipelines that combine complementary models, incorporate conversational\ncontext, and fine-tune for under-represented languages and implicit abuse. A\nde-identified version of the dataset and full prompts is publicly released to\npromote reproducibility and further progress in automated content moderation."}
{"id": "2505.18962", "pdf": "https://arxiv.org/pdf/2505.18962.pdf", "abs": "https://arxiv.org/abs/2505.18962", "title": "System-1.5 Reasoning: Traversal in Language and Latent Spaces with Dynamic Shortcuts", "authors": ["Xiaoqiang Wang", "Suyuchen Wang", "Yun Zhu", "Bang Liu"], "categories": ["cs.CL"], "comment": "Work in progress", "summary": "Chain-of-thought (CoT) reasoning enables large language models (LLMs) to move\nbeyond fast System-1 responses and engage in deliberative System-2 reasoning.\nHowever, this comes at the cost of significant inefficiency due to verbose\nintermediate output. Recent latent-space reasoning methods improve efficiency\nby operating on hidden states without decoding into language, yet they treat\nall steps uniformly, failing to distinguish critical deductions from auxiliary\nsteps and resulting in suboptimal use of computational resources. In this\npaper, we propose System-1.5 Reasoning, an adaptive reasoning framework that\ndynamically allocates computation across reasoning steps through shortcut paths\nin latent space. Specifically, System-1.5 Reasoning introduces two types of\ndynamic shortcuts. The model depth shortcut (DS) adaptively reasons along the\nvertical depth by early exiting non-critical tokens through lightweight adapter\nbranches, while allowing critical tokens to continue through deeper Transformer\nlayers. The step shortcut (SS) reuses hidden states across the decoding steps\nto skip trivial steps and reason horizontally in latent space. Training\nSystem-1.5 Reasoning involves a two-stage self-distillation process: first\ndistilling natural language CoT into latent-space continuous thought, and then\ndistilling full-path System-2 latent reasoning into adaptive shortcut paths\n(System-1.5 Reasoning). Experiments on reasoning tasks demonstrate the superior\nperformance of our method. For example, on GSM8K, System-1.5 Reasoning achieves\nreasoning performance comparable to traditional CoT fine-tuning methods while\naccelerating inference by over 20x and reducing token generation by 92.31% on\naverage."}
{"id": "2505.18970", "pdf": "https://arxiv.org/pdf/2505.18970.pdf", "abs": "https://arxiv.org/abs/2505.18970", "title": "Learning to Explain: Prototype-Based Surrogate Models for LLM Classification", "authors": ["Bowen Wei", "Mehrdad Fazli", "Ziwei Zhu"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive performance on\nnatural language tasks, but their decision-making processes remain largely\nopaque. Existing explanation methods either suffer from limited faithfulness to\nthe model's reasoning or produce explanations that humans find difficult to\nunderstand. To address these challenges, we propose \\textbf{ProtoSurE}, a novel\nprototype-based surrogate framework that provides faithful and\nhuman-understandable explanations for LLMs. ProtoSurE trains an\ninterpretable-by-design surrogate model that aligns with the target LLM while\nutilizing sentence-level prototypes as human-understandable concepts. Extensive\nexperiments show that ProtoSurE consistently outperforms SOTA explanation\nmethods across diverse LLMs and datasets. Importantly, ProtoSurE demonstrates\nstrong data efficiency, requiring relatively few training examples to achieve\ngood performance, making it practical for real-world applications."}
{"id": "2505.19439", "pdf": "https://arxiv.org/pdf/2505.19439.pdf", "abs": "https://arxiv.org/abs/2505.19439", "title": "Surrogate Signals from Format and Length: Reinforcement Learning for Solving Mathematical Problems without Ground Truth Answers", "authors": ["Rihui Xin", "Han Liu", "Zecheng Wang", "Yupeng Zhang", "Dianbo Sui", "Xiaolin Hu", "Bingning Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models have achieved remarkable success in natural language\nprocessing tasks, with Reinforcement Learning playing a key role in adapting\nthem to specific applications. However, obtaining ground truth answers for\ntraining LLMs in mathematical problem-solving is often challenging, costly, and\nsometimes unfeasible. This research delves into the utilization of format and\nlength as surrogate signals to train LLMs for mathematical problem-solving,\nbypassing the need for traditional ground truth answers. Our study shows that a\nreward function centered on format correctness alone can yield performance\nimprovements comparable to the standard GRPO algorithm in early phases.\nRecognizing the limitations of format-only rewards in the later phases, we\nincorporate length-based rewards. The resulting GRPO approach, leveraging\nformat-length surrogate signals, not only matches but surpasses the performance\nof the standard GRPO algorithm relying on ground truth answers in certain\nscenarios, achieving 40.0% accuracy on AIME2024 with a 7B base model. Through\nsystematic exploration and experimentation, this research not only offers a\npractical solution for training LLMs to solve mathematical problems and\nreducing the dependence on extensive ground truth data collection, but also\nreveals the essence of why our label-free approach succeeds: the powerful base\nmodel is like an excellent student who has already mastered mathematical and\nlogical reasoning skills, but performs poorly on the test paper, it simply\nneeds to develop good answering habits to achieve outstanding results in exams,\nto unlock the capabilities it already possesses."}
{"id": "2505.19754", "pdf": "https://arxiv.org/pdf/2505.19754.pdf", "abs": "https://arxiv.org/abs/2505.19754", "title": "NeuSym-RAG: Hybrid Neural Symbolic Retrieval with Multiview Structuring for PDF Question Answering", "authors": ["Ruisheng Cao", "Hanchong Zhang", "Tiancheng Huang", "Zhangyi Kang", "Yuxin Zhang", "Liangtai Sun", "Hanqi Li", "Yuxun Miao", "Shuai Fan", "Lu Chen", "Kai Yu"], "categories": ["cs.CL", "cs.AI"], "comment": "29 pages, 11 figures, 12 tables, accepted to ACL 2025 Long Main", "summary": "The increasing number of academic papers poses significant challenges for\nresearchers to efficiently acquire key details. While retrieval augmented\ngeneration (RAG) shows great promise in large language model (LLM) based\nautomated question answering, previous works often isolate neural and symbolic\nretrieval despite their complementary strengths. Moreover, conventional\nsingle-view chunking neglects the rich structure and layout of PDFs, e.g.,\nsections and tables. In this work, we propose NeuSym-RAG, a hybrid neural\nsymbolic retrieval framework which combines both paradigms in an interactive\nprocess. By leveraging multi-view chunking and schema-based parsing, NeuSym-RAG\norganizes semi-structured PDF content into both the relational database and\nvectorstore, enabling LLM agents to iteratively gather context until sufficient\nto generate answers. Experiments on three full PDF-based QA datasets, including\na self-annotated one AIRQA-REAL, show that NeuSym-RAG stably defeats both the\nvector-based RAG and various structured baselines, highlighting its capacity to\nunify both retrieval schemes and utilize multiple views. Code and data are\npublicly available at https://github.com/X-LANCE/NeuSym-RAG."}
{"id": "2505.20237", "pdf": "https://arxiv.org/pdf/2505.20237.pdf", "abs": "https://arxiv.org/abs/2505.20237", "title": "Efficient Speech Translation through Model Compression and Knowledge Distillation", "authors": ["Yasmin Moslem"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "IWSLT 2025", "summary": "Efficient deployment of large audio-language models for speech translation\nremains challenging due to their significant computational requirements. In\nthis paper, we address this challenge through our system submissions to the\n\"Model Compression\" track at the International Conference on Spoken Language\nTranslation (IWSLT 2025). We experiment with a combination of approaches\nincluding iterative layer pruning based on layer importance evaluation,\nlow-rank adaptation with 4-bit quantization (QLoRA), and knowledge\ndistillation. In our experiments, we use Qwen2-Audio-7B-Instruct for speech\ntranslation into German and Chinese. Our pruned (student) models achieve up to\na 50% reduction in both model parameters and storage footprint, while retaining\n97-100% of the translation quality of the in-domain (teacher) models."}
{"id": "2505.20243", "pdf": "https://arxiv.org/pdf/2505.20243.pdf", "abs": "https://arxiv.org/abs/2505.20243", "title": "It's High Time: A Survey of Temporal Information Retrieval and Question Answering", "authors": ["Bhawna Piryani", "Abdelrahman Abdallah", "Jamshid Mozafari", "Avishek Anand", "Adam Jatowt"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Time plays a critical role in how information is generated, retrieved, and\ninterpreted. In this survey, we provide a comprehensive overview of Temporal\nInformation Retrieval and Temporal Question Answering, two research areas aimed\nat handling and understanding time-sensitive information. As the amount of\ntime-stamped content from sources like news articles, web archives, and\nknowledge bases increases, systems must address challenges such as detecting\ntemporal intent, normalizing time expressions, ordering events, and reasoning\nover evolving or ambiguous facts. These challenges are critical across many\ndynamic and time-sensitive domains, from news and encyclopedias to science,\nhistory, and social media. We review both traditional approaches and modern\nneural methods, including those that use transformer models and Large Language\nModels (LLMs). We also review recent advances in temporal language modeling,\nmulti-hop reasoning, and retrieval-augmented generation (RAG), alongside\nbenchmark datasets and evaluation strategies that test temporal robustness,\nrecency awareness, and generalization."}
{"id": "2505.20564", "pdf": "https://arxiv.org/pdf/2505.20564.pdf", "abs": "https://arxiv.org/abs/2505.20564", "title": "The NaijaVoices Dataset: Cultivating Large-Scale, High-Quality, Culturally-Rich Speech Data for African Languages", "authors": ["Chris Emezue", "NaijaVoices Community", "Busayo Awobade", "Abraham Owodunni", "Handel Emezue", "Gloria Monica Tobechukwu Emezue", "Nefertiti Nneoma Emezue", "Sewade Ogun", "Bunmi Akinremi", "David Ifeoluwa Adelani", "Chris Pal"], "categories": ["cs.CL"], "comment": "Accepted for publication at Interspeech 2025", "summary": "The development of high-performing, robust, and reliable speech technologies\ndepends on large, high-quality datasets. However, African languages --\nincluding our focus, Igbo, Hausa, and Yoruba -- remain under-represented due to\ninsufficient data. Popular voice-enabled technologies do not support any of the\n2000+ African languages, limiting accessibility for circa one billion people.\nWhile previous dataset efforts exist for the target languages, they lack the\nscale and diversity needed for robust speech models. To bridge this gap, we\nintroduce the NaijaVoices dataset, a 1,800-hour speech-text dataset with 5,000+\nspeakers. We outline our unique data collection approach, analyze its acoustic\ndiversity, and demonstrate its impact through finetuning experiments on\nautomatic speech recognition, averagely achieving 75.86% (Whisper), 52.06%\n(MMS), and 42.33% (XLSR) WER improvements. These results highlight NaijaVoices'\npotential to advance multilingual speech processing for African languages."}
{"id": "2505.21523", "pdf": "https://arxiv.org/pdf/2505.21523.pdf", "abs": "https://arxiv.org/abs/2505.21523", "title": "More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models", "authors": ["Chengzhi Liu", "Zhongxing Xu", "Qingyue Wei", "Juncheng Wu", "James Zou", "Xin Eric Wang", "Yuyin Zhou", "Sheng Liu"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Test-time compute has empowered multimodal large language models to generate\nextended reasoning chains, yielding strong performance on tasks such as\nmultimodal math reasoning. However, this improved reasoning ability often comes\nwith increased hallucination: as generations become longer, models tend to\ndrift away from image-grounded content and rely more heavily on language\npriors. Attention analysis shows that longer reasoning chains lead to reduced\nfocus on visual inputs, which contributes to hallucination. To systematically\nstudy this phenomenon, we introduce RH-AUC, a metric that quantifies how a\nmodel's perception accuracy changes with reasoning length, allowing us to\nevaluate whether the model preserves visual grounding during reasoning. We also\nrelease RH-Bench, a diagnostic benchmark that spans a variety of multimodal\ntasks, designed to assess the trade-off between reasoning ability and\nhallucination. Our analysis reveals that (i) larger models typically achieve a\nbetter balance between reasoning and perception, and (ii) this balance is\ninfluenced more by the types and domains of training data than by its overall\nvolume. These findings underscore the importance of evaluation frameworks that\njointly consider both reasoning quality and perceptual fidelity."}
{"id": "2505.21936", "pdf": "https://arxiv.org/pdf/2505.21936.pdf", "abs": "https://arxiv.org/abs/2505.21936", "title": "RedTeamCUA: Realistic Adversarial Testing of Computer-Use Agents in Hybrid Web-OS Environments", "authors": ["Zeyi Liao", "Jaylen Jones", "Linxi Jiang", "Eric Fosler-Lussier", "Yu Su", "Zhiqiang Lin", "Huan Sun"], "categories": ["cs.CL"], "comment": null, "summary": "Computer-use agents (CUAs) promise to automate complex tasks across operating\nsystems (OS) and the web, but remain vulnerable to indirect prompt injection.\nCurrent evaluations of this threat either lack support realistic but controlled\nenvironments or ignore hybrid web-OS attack scenarios involving both\ninterfaces. To address this, we propose RedTeamCUA, an adversarial testing\nframework featuring a novel hybrid sandbox that integrates a VM-based OS\nenvironment with Docker-based web platforms. Our sandbox supports key features\ntailored for red teaming, such as flexible adversarial scenario configuration,\nand a setting that decouples adversarial evaluation from navigational\nlimitations of CUAs by initializing tests directly at the point of an\nadversarial injection. Using RedTeamCUA, we develop RTC-Bench, a comprehensive\nbenchmark with 864 examples that investigate realistic, hybrid web-OS attack\nscenarios and fundamental security vulnerabilities. Benchmarking current\nfrontier CUAs identifies significant vulnerabilities: Claude 3.7 Sonnet | CUA\ndemonstrates an ASR of 42.9%, while Operator, the most secure CUA evaluated,\nstill exhibits an ASR of 7.6%. Notably, CUAs often attempt to execute\nadversarial tasks with an Attempt Rate as high as 92.5%, although failing to\ncomplete them due to capability limitations. Nevertheless, we observe\nconcerning ASRs of up to 50% in realistic end-to-end settings, with the\nrecently released frontier Claude 4 Opus | CUA showing an alarming ASR of 48%,\ndemonstrating that indirect prompt injection presents tangible risks for even\nadvanced CUAs despite their capabilities and safeguards. Overall, RedTeamCUA\nprovides an essential framework for advancing realistic, controlled, and\nsystematic analysis of CUA vulnerabilities, highlighting the urgent need for\nrobust defenses to indirect prompt injection prior to real-world deployment."}
{"id": "2505.22176", "pdf": "https://arxiv.org/pdf/2505.22176.pdf", "abs": "https://arxiv.org/abs/2505.22176", "title": "TabXEval: Why this is a Bad Table? An eXhaustive Rubric for Table Evaluation", "authors": ["Vihang Pancholi", "Jainit Bafna", "Tejas Anvekar", "Manish Shrivastava", "Vivek Gupta"], "categories": ["cs.CL"], "comment": "Accepeted for Findings at ACL 2025", "summary": "Evaluating tables qualitatively and quantitatively poses a significant\nchallenge, as standard metrics often overlook subtle structural and\ncontent-level discrepancies. To address this, we propose a rubric-based\nevaluation framework that integrates multi-level structural descriptors with\nfine-grained contextual signals, enabling more precise and consistent table\ncomparison. Building on this, we introduce TabXEval, an eXhaustive and\neXplainable two-phase evaluation framework. TabXEval first aligns reference and\npredicted tables structurally via TabAlign, then performs semantic and\nsyntactic comparison using TabCompare, offering interpretable and granular\nfeedback. We evaluate TabXEval on TabXBench, a diverse, multi-domain benchmark\nfeaturing realistic table perturbations and human annotations. A\nsensitivity-specificity analysis further demonstrates the robustness and\nexplainability of TabXEval across varied table tasks. Code and data are\navailable at https://coral-lab-asu.github.io/tabxeval/"}
{"id": "2505.22232", "pdf": "https://arxiv.org/pdf/2505.22232.pdf", "abs": "https://arxiv.org/abs/2505.22232", "title": "Judging Quality Across Languages: A Multilingual Approach to Pretraining Data Filtering with Language Models", "authors": ["Mehdi Ali", "Manuel Brack", "Max L√ºbbering", "Elias Wendt", "Abbas Goher Khan", "Richard Rutmann", "Alex Jude", "Maurice Kraus", "Alexander Arno Weber", "David Kacz√©r", "Florian Mai", "Lucie Flek", "Rafet Sifa", "Nicolas Flores-Herr", "Joachim K√∂hler", "Patrick Schramowski", "Michael Fromm", "Kristian Kersting"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Project page available at https://huggingface.co/spaces/Jackal-AI/JQL", "summary": "High-quality multilingual training data is essential for effectively\npretraining large language models (LLMs). Yet, the availability of suitable\nopen-source multilingual datasets remains limited. Existing state-of-the-art\ndatasets mostly rely on heuristic filtering methods, restricting both their\ncross-lingual transferability and scalability. Here, we introduce JQL, a\nsystematic approach that efficiently curates diverse and high-quality\nmultilingual data at scale while significantly reducing computational demands.\nJQL distills LLMs' annotation capabilities into lightweight annotators based on\npretrained multilingual embeddings. These models exhibit robust multilingual\nand cross-lingual performance, even for languages and scripts unseen during\ntraining. Evaluated empirically across 35 languages, the resulting annotation\npipeline substantially outperforms current heuristic filtering methods like\nFineweb2. JQL notably enhances downstream model training quality and increases\ndata retention rates. Our research provides practical insights and valuable\nresources for multilingual data curation, raising the standards of multilingual\ndataset development."}
{"id": "2505.22627", "pdf": "https://arxiv.org/pdf/2505.22627.pdf", "abs": "https://arxiv.org/abs/2505.22627", "title": "Chain-of-Talkers (CoTalk): Fast Human Annotation of Dense Image Captions", "authors": ["Yijun Shen", "Delong Chen", "Fan Liu", "Xingyu Wang", "Chuanyi Zhang", "Liang Yao", "Yuhui Zheng"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "While densely annotated image captions significantly facilitate the learning\nof robust vision-language alignment, methodologies for systematically\noptimizing human annotation efforts remain underexplored. We introduce\nChain-of-Talkers (CoTalk), an AI-in-the-loop methodology designed to maximize\nthe number of annotated samples and improve their comprehensiveness under fixed\nbudget constraints (e.g., total human annotation time). The framework is built\nupon two key insights. First, sequential annotation reduces redundant workload\ncompared to conventional parallel annotation, as subsequent annotators only\nneed to annotate the ``residual'' -- the missing visual information that\nprevious annotations have not covered. Second, humans process textual input\nfaster by reading while outputting annotations with much higher throughput via\ntalking; thus a multimodal interface enables optimized efficiency. We evaluate\nour framework from two aspects: intrinsic evaluations that assess the\ncomprehensiveness of semantic units, obtained by parsing detailed captions into\nobject-attribute trees and analyzing their effective connections; extrinsic\nevaluation measures the practical usage of the annotated captions in\nfacilitating vision-language alignment. Experiments with eight participants\nshow our Chain-of-Talkers (CoTalk) improves annotation speed (0.42 vs. 0.30\nunits/sec) and retrieval performance (41.13% vs. 40.52%) over the parallel\nmethod."}
{"id": "2505.22759", "pdf": "https://arxiv.org/pdf/2505.22759.pdf", "abs": "https://arxiv.org/abs/2505.22759", "title": "FAMA: The First Large-Scale Open-Science Speech Foundation Model for English and Italian", "authors": ["Sara Papi", "Marco Gaido", "Luisa Bentivogli", "Alessio Brutti", "Mauro Cettolo", "Roberto Gretter", "Marco Matassoni", "Mohamed Nabih", "Matteo Negri"], "categories": ["cs.CL", "cs.AI", "cs.SD"], "comment": null, "summary": "The development of speech foundation models (SFMs) like Whisper and\nSeamlessM4T has significantly advanced the field of speech processing. However,\ntheir closed nature--with inaccessible training data and code--poses major\nreproducibility and fair evaluation challenges. While other domains have made\nsubstantial progress toward open science by developing fully transparent models\ntrained on open-source (OS) code and data, similar efforts in speech remain\nlimited. To fill this gap, we introduce FAMA, the first family of open science\nSFMs for English and Italian, trained on 150k+ hours of OS speech data.\nMoreover, we present a new dataset containing 16k hours of cleaned and\npseudo-labeled speech for both languages. Results show that FAMA achieves\ncompetitive performance compared to existing SFMs while being up to 8 times\nfaster. All artifacts, including code, datasets, and models, are released under\nOS-compliant licenses, promoting openness in speech technology research."}
{"id": "2505.22919", "pdf": "https://arxiv.org/pdf/2505.22919.pdf", "abs": "https://arxiv.org/abs/2505.22919", "title": "ER-REASON: A Benchmark Dataset for LLM-Based Clinical Reasoning in the Emergency Room", "authors": ["Nikita Mehandru", "Niloufar Golchini", "David Bamman", "Travis Zack", "Melanie F. Molina", "Ahmed Alaa"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have been extensively evaluated on medical\nquestion answering tasks based on licensing exams. However, real-world\nevaluations often depend on costly human annotators, and existing benchmarks\ntend to focus on isolated tasks that rarely capture the clinical reasoning or\nfull workflow underlying medical decisions. In this paper, we introduce\nER-Reason, a benchmark designed to evaluate LLM-based clinical reasoning and\ndecision-making in the emergency room (ER)--a high-stakes setting where\nclinicians make rapid, consequential decisions across diverse patient\npresentations and medical specialties under time pressure. ER-Reason includes\ndata from 3,984 patients, encompassing 25,174 de-identified longitudinal\nclinical notes spanning discharge summaries, progress notes, history and\nphysical exams, consults, echocardiography reports, imaging notes, and ER\nprovider documentation. The benchmark includes evaluation tasks that span key\nstages of the ER workflow: triage intake, initial assessment, treatment\nselection, disposition planning, and final diagnosis--each structured to\nreflect core clinical reasoning processes such as differential diagnosis via\nrule-out reasoning. We also collected 72 full physician-authored rationales\nexplaining reasoning processes that mimic the teaching process used in\nresidency training, and are typically absent from ER documentation. Evaluations\nof state-of-the-art LLMs on ER-Reason reveal a gap between LLM-generated and\nclinician-authored clinical reasoning for ER decisions, highlighting the need\nfor future research to bridge this divide."}
{"id": "2505.23026", "pdf": "https://arxiv.org/pdf/2505.23026.pdf", "abs": "https://arxiv.org/abs/2505.23026", "title": "Context-Robust Knowledge Editing for Language Models", "authors": ["Haewon Park", "Gyubin Choi", "Minjun Kim", "Yohan Jo"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings. Our code and datasets are available at\n  https://github.com/holi-lab/CoRE", "summary": "Knowledge editing (KE) methods offer an efficient way to modify knowledge in\nlarge language models. Current KE evaluations typically assess editing success\nby considering only the edited knowledge without any preceding contexts. In\nreal-world applications, however, preceding contexts often trigger the\nretrieval of the original knowledge and undermine the intended edit. To address\nthis issue, we develop CHED -- a benchmark designed to evaluate the context\nrobustness of KE methods. Evaluations on CHED show that they often fail when\npreceding contexts are present. To mitigate this shortcoming, we introduce\nCoRE, a KE method designed to strengthen context robustness by minimizing\ncontext-sensitive variance in hidden states of the model for edited knowledge.\nThis method not only improves the editing success rate in situations where a\npreceding context is present but also preserves the overall capabilities of the\nmodel. We provide an in-depth analysis of the differing impacts of preceding\ncontexts when introduced as user utterances versus assistant responses, and we\ndissect attention-score patterns to assess how specific tokens influence\nediting success."}
{"id": "2505.23126", "pdf": "https://arxiv.org/pdf/2505.23126.pdf", "abs": "https://arxiv.org/abs/2505.23126", "title": "PBEBench: A Multi-Step Programming by Examples Reasoning Benchmark inspired by Historical Linguistics", "authors": ["Atharva Naik", "Darsh Agrawal", "Manav Kapadnis", "Yuwei An", "Yash Mathur", "Carolyn Rose", "David Mortensen"], "categories": ["cs.CL"], "comment": null, "summary": "Recently, long chain of thought (LCoT), Large Language Models (LLMs), have\ntaken the machine learning world by storm with their breathtaking reasoning\ncapabilities. However, are the abstract reasoning abilities of these models\ngeneral enough for problems of practical importance? Unlike past work, which\nhas focused mainly on math, coding, and data wrangling, we focus on a\nhistorical linguistics-inspired inductive reasoning problem, formulated as\nProgramming by Examples. We develop a fully automated pipeline for dynamically\ngenerating a benchmark for this task with controllable difficulty in order to\ntackle scalability and contamination issues to which many reasoning benchmarks\nare subject. Using our pipeline, we generate a test set with nearly 1k\ninstances that is challenging for all state-of-the-art reasoning LLMs, with the\nbest model (Claude-3.7-Sonnet) achieving a mere 54% pass rate, demonstrating\nthat LCoT LLMs still struggle with a class or reasoning that is ubiquitous in\nhistorical linguistics as well as many other domains."}
{"id": "2505.23291", "pdf": "https://arxiv.org/pdf/2505.23291.pdf", "abs": "https://arxiv.org/abs/2505.23291", "title": "ScEdit: Script-based Assessment of Knowledge Editing", "authors": ["Xinye Li", "Zunwen Zheng", "Qian Zhang", "Dekai Zhuang", "Jiabao Kang", "Liyan Xu", "Qingbin Liu", "Xi Chen", "Zhiying Tu", "Dianhui Chu", "Dianbo Sui"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Knowledge Editing (KE) has gained increasing attention, yet current KE tasks\nremain relatively simple. Under current evaluation frameworks, many editing\nmethods achieve exceptionally high scores, sometimes nearing perfection.\nHowever, few studies integrate KE into real-world application scenarios (e.g.,\nrecent interest in LLM-as-agent). To support our analysis, we introduce a novel\nscript-based benchmark -- ScEdit (Script-based Knowledge Editing Benchmark) --\nwhich encompasses both counterfactual and temporal edits. We integrate\ntoken-level and text-level evaluation methods, comprehensively analyzing\nexisting KE techniques. The benchmark extends traditional fact-based\n(\"What\"-type question) evaluation to action-based (\"How\"-type question)\nevaluation. We observe that all KE methods exhibit a drop in performance on\nestablished metrics and face challenges on text-level metrics, indicating a\nchallenging task. Our benchmark is available at\nhttps://github.com/asdfo123/ScEdit."}
{"id": "2505.23657", "pdf": "https://arxiv.org/pdf/2505.23657.pdf", "abs": "https://arxiv.org/abs/2505.23657", "title": "Active Layer-Contrastive Decoding Reduces Hallucination in Large Language Model Generation", "authors": ["Hongxiang Zhang", "Hao Chen", "Muhao Chen", "Tianyi Zhang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent decoding methods improve the factuality of large language models\n(LLMs) by refining how the next token is selected during generation. These\nmethods typically operate at the token level, leveraging internal\nrepresentations to suppress superficial patterns. Nevertheless, LLMs remain\nprone to hallucinations, especially over longer contexts. In this paper, we\npropose Active Layer-Contrastive Decoding (ActLCD), a novel decoding strategy\nthat actively decides when to apply contrasting layers during generation. By\ncasting decoding as a sequential decision-making problem, ActLCD employs a\nreinforcement learning policy guided by a reward-aware classifier to optimize\nfactuality beyond the token level. Our experiments demonstrate that ActLCD\nsurpasses state-of-the-art methods across five benchmarks, showcasing its\neffectiveness in mitigating hallucinations in diverse generation scenarios."}
{"id": "2505.23729", "pdf": "https://arxiv.org/pdf/2505.23729.pdf", "abs": "https://arxiv.org/abs/2505.23729", "title": "Bounded Rationality for LLMs: Satisficing Alignment at Inference-Time", "authors": ["Mohamad Chehade", "Soumya Suvra Ghosal", "Souradip Chakraborty", "Avinash Reddy", "Dinesh Manocha", "Hao Zhu", "Amrit Singh Bedi"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ICML 2025", "summary": "Aligning large language models with humans is challenging due to the\ninherently multifaceted nature of preference feedback. While existing\napproaches typically frame this as a multi-objective optimization problem, they\noften overlook how humans actually make decisions. Research on bounded\nrationality suggests that human decision making follows satisficing\nstrategies-optimizing primary objectives while ensuring others meet acceptable\nthresholds. To bridge this gap and operationalize the notion of satisficing\nalignment, we propose SITAlign: an inference time framework that addresses the\nmultifaceted nature of alignment by maximizing a primary objective while\nsatisfying threshold-based constraints on secondary criteria. We provide\ntheoretical insights by deriving sub-optimality bounds of our satisficing based\ninference alignment approach. We empirically validate SITAlign's performance\nthrough extensive experimentation on multiple benchmarks. For instance, on the\nPKU-SafeRLHF dataset with the primary objective of maximizing helpfulness while\nensuring a threshold on harmlessness, SITAlign outperforms the state-of-the-art\nmulti objective decoding strategy by a margin of 22.3% in terms of GPT-4\nwin-tie rate for helpfulness reward while adhering to the threshold on\nharmlessness."}
{"id": "2505.23799", "pdf": "https://arxiv.org/pdf/2505.23799.pdf", "abs": "https://arxiv.org/abs/2505.23799", "title": "Estimating LLM Consistency: A User Baseline vs Surrogate Metrics", "authors": ["Xiaoyuan Wu", "Weiran Lin", "Omer Akgul", "Lujo Bauer"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) are prone to hallucinations and sensitive to\nprompt perturbations, often resulting in inconsistent or unreliable generated\ntext. Different methods have been proposed to mitigate such hallucinations and\nfragility -- one of them being measuring the consistency (the model's\nconfidence in the response, or likelihood of generating a similar response when\nresampled) of LLM responses. In previous work, measuring consistency often\nrelied on the probability of a response appearing within a pool of resampled\nresponses, or internal states or logits of responses. However, it is not yet\nclear how well these approaches approximate how humans perceive the consistency\nof LLM responses. We performed a user study (n=2,976) and found current methods\ntypically do not approximate users' perceptions of LLM consistency very well.\nWe propose a logit-based ensemble method for estimating LLM consistency, and we\nshow that this method matches the performance of the best-performing existing\nmetric in estimating human ratings of LLM consistency. Our results suggest that\nmethods of estimating LLM consistency without human evaluation are sufficiently\nimperfect that we suggest evaluation with human input be more broadly used."}
{"id": "2505.23802", "pdf": "https://arxiv.org/pdf/2505.23802.pdf", "abs": "https://arxiv.org/abs/2505.23802", "title": "MedHELM: Holistic Evaluation of Large Language Models for Medical Tasks", "authors": ["Suhana Bedi", "Hejie Cui", "Miguel Fuentes", "Alyssa Unell", "Michael Wornow", "Juan M. Banda", "Nikesh Kotecha", "Timothy Keyes", "Yifan Mai", "Mert Oez", "Hao Qiu", "Shrey Jain", "Leonardo Schettini", "Mehr Kashyap", "Jason Alan Fries", "Akshay Swaminathan", "Philip Chung", "Fateme Nateghi", "Asad Aali", "Ashwin Nayak", "Shivam Vedak", "Sneha S. Jain", "Birju Patel", "Oluseyi Fayanju", "Shreya Shah", "Ethan Goh", "Dong-han Yao", "Brian Soetikno", "Eduardo Reis", "Sergios Gatidis", "Vasu Divi", "Robson Capasso", "Rachna Saralkar", "Chia-Chun Chiang", "Jenelle Jindal", "Tho Pham", "Faraz Ghoddusi", "Steven Lin", "Albert S. Chiou", "Christy Hong", "Mohana Roy", "Michael F. Gensheimer", "Hinesh Patel", "Kevin Schulman", "Dev Dash", "Danton Char", "Lance Downing", "Francois Grolleau", "Kameron Black", "Bethel Mieso", "Aydin Zahedivash", "Wen-wai Yim", "Harshita Sharma", "Tony Lee", "Hannah Kirsch", "Jennifer Lee", "Nerissa Ambers", "Carlene Lugtu", "Aditya Sharma", "Bilal Mawji", "Alex Alekseyev", "Vicky Zhou", "Vikas Kakkar", "Jarrod Helzer", "Anurang Revri", "Yair Bannett", "Roxana Daneshjou", "Jonathan Chen", "Emily Alsentzer", "Keith Morse", "Nirmal Ravi", "Nima Aghaeepour", "Vanessa Kennedy", "Akshay Chaudhari", "Thomas Wang", "Sanmi Koyejo", "Matthew P. Lungren", "Eric Horvitz", "Percy Liang", "Mike Pfeffer", "Nigam H. Shah"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While large language models (LLMs) achieve near-perfect scores on medical\nlicensing exams, these evaluations inadequately reflect the complexity and\ndiversity of real-world clinical practice. We introduce MedHELM, an extensible\nevaluation framework for assessing LLM performance for medical tasks with three\nkey contributions. First, a clinician-validated taxonomy spanning 5 categories,\n22 subcategories, and 121 tasks developed with 29 clinicians. Second, a\ncomprehensive benchmark suite comprising 35 benchmarks (17 existing, 18 newly\nformulated) providing complete coverage of all categories and subcategories in\nthe taxonomy. Third, a systematic comparison of LLMs with improved evaluation\nmethods (using an LLM-jury) and a cost-performance analysis. Evaluation of 9\nfrontier LLMs, using the 35 benchmarks, revealed significant performance\nvariation. Advanced reasoning models (DeepSeek R1: 66% win-rate; o3-mini: 64%\nwin-rate) demonstrated superior performance, though Claude 3.5 Sonnet achieved\ncomparable results at 40% lower estimated computational cost. On a normalized\naccuracy scale (0-1), most models performed strongly in Clinical Note\nGeneration (0.73-0.85) and Patient Communication & Education (0.78-0.83),\nmoderately in Medical Research Assistance (0.65-0.75), and generally lower in\nClinical Decision Support (0.56-0.72) and Administration & Workflow\n(0.53-0.63). Our LLM-jury evaluation method achieved good agreement with\nclinician ratings (ICC = 0.47), surpassing both average clinician-clinician\nagreement (ICC = 0.43) and automated baselines including ROUGE-L (0.36) and\nBERTScore-F1 (0.44). Claude 3.5 Sonnet achieved comparable performance to top\nmodels at lower estimated cost. These findings highlight the importance of\nreal-world, task-specific evaluation for medical use of LLMs and provides an\nopen source framework to enable this."}
{"id": "2505.23807", "pdf": "https://arxiv.org/pdf/2505.23807.pdf", "abs": "https://arxiv.org/abs/2505.23807", "title": "DLP: Dynamic Layerwise Pruning in Large Language Models", "authors": ["Yuli Chen", "Bo Cheng", "Jiale Han", "Yingying Zhang", "Yingting Li", "Shuhao Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ICML 2025", "summary": "Pruning has recently been widely adopted to reduce the parameter scale and\nimprove the inference efficiency of Large Language Models (LLMs). Mainstream\npruning techniques often rely on uniform layerwise pruning strategies, which\ncan lead to severe performance degradation at high sparsity levels. Recognizing\nthe varying contributions of different layers in LLMs, recent studies have\nshifted their focus toward non-uniform layerwise pruning. However, these\napproaches often rely on pre-defined values, which can result in suboptimal\nperformance. To overcome these limitations, we propose a novel method called\nDynamic Layerwise Pruning (DLP). This approach adaptively determines the\nrelative importance of each layer by integrating model weights with input\nactivation information, assigning pruning rates accordingly. Experimental\nresults show that DLP effectively preserves model performance at high sparsity\nlevels across multiple LLMs. Specifically, at 70% sparsity, DLP reduces the\nperplexity of LLaMA2-7B by 7.79 and improves the average accuracy by 2.7%\ncompared to state-of-the-art methods. Moreover, DLP is compatible with various\nexisting LLM compression techniques and can be seamlessly integrated into\nParameter-Efficient Fine-Tuning (PEFT). We release the code at [this https\nURL](https://github.com/ironartisan/DLP) to facilitate future research."}
{"id": "2505.23932", "pdf": "https://arxiv.org/pdf/2505.23932.pdf", "abs": "https://arxiv.org/abs/2505.23932", "title": "SwingArena: Competitive Programming Arena for Long-context GitHub Issue Solving", "authors": ["Wendong Xu", "Jing Xiong", "Chenyang Zhao", "Qiujiang Chen", "Haoran Wang", "Hui Shen", "Zhongwei Wan", "Jianbo Dai", "Taiqiang Wu", "He Xiao", "Chaofan Tao", "Z. Morley Mao", "Ying Sheng", "Zhijiang Guo", "Hongxia Yang", "Bei Yu", "Lingpeng Kong", "Quanquan Gu", "Ngai Wong"], "categories": ["cs.CL"], "comment": null, "summary": "We present SwingArena, a competitive evaluation framework for Large Language\nModels (LLMs) that closely mirrors real-world software development workflows.\nUnlike traditional static benchmarks, SwingArena models the collaborative\nprocess of software iteration by pairing LLMs as submitters, who generate\npatches, and reviewers, who create test cases and verify the patches through\ncontinuous integration (CI) pipelines. To support these interactive\nevaluations, we introduce a retrieval-augmented code generation (RACG) module\nthat efficiently handles long-context challenges by providing syntactically and\nsemantically relevant code snippets from large codebases, supporting multiple\nprogramming languages (C++, Python, Rust, and Go). This enables the framework\nto scale across diverse tasks and contexts while respecting token limitations.\nOur experiments, using over 400 high-quality real-world GitHub issues selected\nfrom a pool of 2,300 issues, show that models like GPT-4o excel at aggressive\npatch generation, whereas DeepSeek and Gemini prioritize correctness in CI\nvalidation. SwingArena presents a scalable and extensible methodology for\nevaluating LLMs in realistic, CI-driven software development settings. More\ndetails are available on our project page: swing-bench.github.io"}
{"id": "2505.24223", "pdf": "https://arxiv.org/pdf/2505.24223.pdf", "abs": "https://arxiv.org/abs/2505.24223", "title": "Automated Structured Radiology Report Generation", "authors": ["Jean-Benoit Delbrouck", "Justin Xu", "Johannes Moll", "Alois Thomas", "Zhihong Chen", "Sophie Ostmeier", "Asfandyar Azhar", "Kelvin Zhenghao Li", "Andrew Johnston", "Christian Bluethgen", "Eduardo Reis", "Mohamed Muneer", "Maya Varma", "Curtis Langlotz"], "categories": ["cs.CL"], "comment": "Accepted to ACL Main 2025", "summary": "Automated radiology report generation from chest X-ray (CXR) images has the\npotential to improve clinical efficiency and reduce radiologists' workload.\nHowever, most datasets, including the publicly available MIMIC-CXR and CheXpert\nPlus, consist entirely of free-form reports, which are inherently variable and\nunstructured. This variability poses challenges for both generation and\nevaluation: existing models struggle to produce consistent, clinically\nmeaningful reports, and standard evaluation metrics fail to capture the nuances\nof radiological interpretation. To address this, we introduce Structured\nRadiology Report Generation (SRRG), a new task that reformulates free-text\nradiology reports into a standardized format, ensuring clarity, consistency,\nand structured clinical reporting. We create a novel dataset by restructuring\nreports using large language models (LLMs) following strict structured\nreporting desiderata. Additionally, we introduce SRR-BERT, a fine-grained\ndisease classification model trained on 55 labels, enabling more precise and\nclinically informed evaluation of structured reports. To assess report quality,\nwe propose F1-SRR-BERT, a metric that leverages SRR-BERT's hierarchical disease\ntaxonomy to bridge the gap between free-text variability and structured\nclinical reporting. We validate our dataset through a reader study conducted by\nfive board-certified radiologists and extensive benchmarking experiments."}
{"id": "2505.24362", "pdf": "https://arxiv.org/pdf/2505.24362.pdf", "abs": "https://arxiv.org/abs/2505.24362", "title": "Knowing Before Saying: LLM Representations Encode Information About Chain-of-Thought Success Before Completion", "authors": ["Anum Afzal", "Florian Matthes", "Gal Chechik", "Yftah Ziser"], "categories": ["cs.CL"], "comment": null, "summary": "We investigate whether the success of a zero-shot Chain-of-Thought (CoT)\nprocess can be predicted before completion. We discover that a probing\nclassifier, based on LLM representations, performs well \\emph{even before a\nsingle token is generated}, suggesting that crucial information about the\nreasoning process is already present in the initial steps representations. In\ncontrast, a strong BERT-based baseline, which relies solely on the generated\ntokens, performs worse, likely because it depends on shallow linguistic cues\nrather than deeper reasoning dynamics. Surprisingly, using later reasoning\nsteps does not always improve classification. When additional context is\nunhelpful, earlier representations resemble later ones more, suggesting LLMs\nencode key information early. This implies reasoning can often stop early\nwithout loss. To test this, we conduct early stopping experiments, showing that\ntruncating CoT reasoning still improves performance over not using CoT at all,\nthough a gap remains compared to full reasoning. However, approaches like\nsupervised learning or reinforcement learning designed to shorten CoT chains\ncould leverage our classifier's guidance to identify when early stopping is\neffective. Our findings provide insights that may support such methods, helping\nto optimize CoT's efficiency while preserving its benefits."}
{"id": "2505.24656", "pdf": "https://arxiv.org/pdf/2505.24656.pdf", "abs": "https://arxiv.org/abs/2505.24656", "title": "MSDA: Combining Pseudo-labeling and Self-Supervision for Unsupervised Domain Adaptation in ASR", "authors": ["Dimitrios Damianos", "Georgios Paraskevopoulos", "Alexandros Potamianos"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Submitted to Interspeech 2025", "summary": "In this work, we investigate the Meta PL unsupervised domain adaptation\nframework for Automatic Speech Recognition (ASR). We introduce a Multi-Stage\nDomain Adaptation pipeline (MSDA), a sample-efficient, two-stage adaptation\napproach that integrates self-supervised learning with semi-supervised\ntechniques. MSDA is designed to enhance the robustness and generalization of\nASR models, making them more adaptable to diverse conditions. It is\nparticularly effective for low-resource languages like Greek and in weakly\nsupervised scenarios where labeled data is scarce or noisy. Through extensive\nexperiments, we demonstrate that Meta PL can be applied effectively to ASR\ntasks, achieving state-of-the-art results, significantly outperforming\nstate-of-the-art methods, and providing more robust solutions for unsupervised\ndomain adaptation in ASR. Our ablations highlight the necessity of utilizing a\ncascading approach when combining self-supervision with self-training."}
{"id": "2505.24803", "pdf": "https://arxiv.org/pdf/2505.24803.pdf", "abs": "https://arxiv.org/abs/2505.24803", "title": "Guiding Generative Storytelling with Knowledge Graphs", "authors": ["Zhijun Pan", "Antonios Andronis", "Eva Hayek", "Oscar AP Wilkinson", "Ilya Lasy", "Annette Parry", "Guy Gadney", "Tim J. Smith", "Mick Grierson"], "categories": ["cs.CL", "cs.HC"], "comment": "This manuscript was submitted for peer review in January 2025", "summary": "Large Language Models (LLMs) have shown great potential in automated story\ngeneration, but challenges remain in maintaining long-form coherence and\nproviding users with intuitive and effective control. Retrieval-Augmented\nGeneration (RAG) has proven effective in reducing hallucinations in text\ngeneration; however, the use of structured data to support generative\nstorytelling remains underexplored. This paper investigates how knowledge\ngraphs (KGs) can enhance LLM-based storytelling by improving narrative quality\nand enabling user-driven modifications. We propose a KG-assisted storytelling\npipeline and evaluate its effectiveness through a user study with 15\nparticipants. Participants created their own story prompts, generated stories,\nand edited knowledge graphs to shape their narratives. Through quantitative and\nqualitative analysis, our findings demonstrate that knowledge graphs\nsignificantly enhance story quality in action-oriented and structured\nnarratives within our system settings. Additionally, editing the knowledge\ngraph increases users' sense of control, making storytelling more engaging,\ninteractive, and playful."}
{"id": "2505.24832", "pdf": "https://arxiv.org/pdf/2505.24832.pdf", "abs": "https://arxiv.org/abs/2505.24832", "title": "How much do language models memorize?", "authors": ["John X. Morris", "Chawin Sitawarin", "Chuan Guo", "Narine Kokhlikyan", "G. Edward Suh", "Alexander M. Rush", "Kamalika Chaudhuri", "Saeed Mahloujifar"], "categories": ["cs.CL"], "comment": null, "summary": "We propose a new method for estimating how much a model ``knows'' about a\ndatapoint and use it to measure the capacity of modern language models. Prior\nstudies of language model memorization have struggled to disentangle\nmemorization from generalization. We formally separate memorization into two\ncomponents: \\textit{unintended memorization}, the information a model contains\nabout a specific dataset, and \\textit{generalization}, the information a model\ncontains about the true data-generation process. When we completely eliminate\ngeneralization, we can compute the total memorization, which provides an\nestimate of model capacity: our measurements estimate that GPT-style models\nhave a capacity of approximately 3.6 bits per parameter. We train language\nmodels on datasets of increasing size and observe that models memorize until\ntheir capacity fills, at which point ``grokking'' begins, and unintended\nmemorization decreases as models begin to generalize. We train hundreds of\ntransformer language models ranging from $500K$ to $1.5B$ parameters and\nproduce a series of scaling laws relating model capacity and data size to\nmembership inference."}
{"id": "2303.17475", "pdf": "https://arxiv.org/pdf/2303.17475.pdf", "abs": "https://arxiv.org/abs/2303.17475", "title": "Learning distributed representations with efficient SoftMax normalization", "authors": ["Lorenzo Dall'Amico", "Enrico Maria Belliardo"], "categories": ["cs.LG", "cs.CL", "stat.ML"], "comment": null, "summary": "Learning distributed representations, or embeddings, that encode the\nrelational similarity patterns among objects is a relevant task in machine\nlearning. A popular method to learn the embedding matrices $X, Y$ is optimizing\na loss function of the term ${\\rm SoftMax}(XY^T)$. The complexity required to\ncalculate this term, however, runs quadratically with the problem size, making\nit a computationally heavy solution. In this article, we propose a linear-time\nheuristic approximation to compute the normalization constants of ${\\rm\nSoftMax}(XY^T)$ for embedding vectors with bounded norms. We show on some\npre-trained embedding datasets that the proposed estimation method achieves\nhigher or comparable accuracy with competing methods. From this result, we\ndesign an efficient and task-agnostic algorithm that learns the embeddings by\noptimizing the cross entropy between the softmax and a set of probability\ndistributions given as inputs. The proposed algorithm is interpretable and\neasily adapted to arbitrary embedding problems. We consider a few use cases and\nobserve similar or higher performances and a lower computational time than\nsimilar ``2Vec'' algorithms."}
{"id": "2304.09276", "pdf": "https://arxiv.org/pdf/2304.09276.pdf", "abs": "https://arxiv.org/abs/2304.09276", "title": "Towards a Neural Lambda Calculus: Neurosymbolic AI Applied to the Foundations of Functional Programming", "authors": ["Jo√£o Flach", "Alvaro F. Moreira", "Luis C. Lamb"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.LO", "I.2; I.2.6; F.1; F.1.1; D.1.1"], "comment": "Keywords: Machine Learning, Lambda Calculus, Neurosymbolic AI, Neural\n  Networks, Transformer Model, Sequence-to-Sequence Models, Computational\n  Models", "summary": "Over the last decades, deep neural networks based-models became the dominant\nparadigm in machine learning. Further, the use of artificial neural networks in\nsymbolic learning has been seen as increasingly relevant recently. To study the\ncapabilities of neural networks in the symbolic AI domain, researchers have\nexplored the ability of deep neural networks to learn mathematical\nconstructions, such as addition and multiplication, logic inference, such as\ntheorem provers, and even the execution of computer programs. The latter is\nknown to be too complex a task for neural networks. Therefore, the results were\nnot always successful, and often required the introduction of biased elements\nin the learning process, in addition to restricting the scope of possible\nprograms to be executed. In this work, we will analyze the ability of neural\nnetworks to learn how to execute programs as a whole. To do so, we propose a\ndifferent approach. Instead of using an imperative programming language, with\ncomplex structures, we use the Lambda Calculus ({\\lambda}-Calculus), a simple,\nbut Turing-Complete mathematical formalism, which serves as the basis for\nmodern functional programming languages and is at the heart of computability\ntheory. We will introduce the use of integrated neural learning and lambda\ncalculi formalization. Finally, we explore execution of a program in\n{\\lambda}-Calculus is based on reductions, we will show that it is enough to\nlearn how to perform these reductions so that we can execute any program.\nKeywords: Machine Learning, Lambda Calculus, Neurosymbolic AI, Neural Networks,\nTransformer Model, Sequence-to-Sequence Models, Computational Models"}
{"id": "2312.11556", "pdf": "https://arxiv.org/pdf/2312.11556.pdf", "abs": "https://arxiv.org/abs/2312.11556", "title": "StarVector: Generating Scalable Vector Graphics Code from Images and Text", "authors": ["Juan A. Rodriguez", "Abhay Puri", "Shubham Agarwal", "Issam H. Laradji", "Pau Rodriguez", "Sai Rajeswar", "David Vazquez", "Christopher Pal", "Marco Pedersoli"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Scalable Vector Graphics (SVGs) are vital for modern image rendering due to\ntheir scalability and versatility. Previous SVG generation methods have focused\non curve-based vectorization, lacking semantic understanding, often producing\nartifacts, and struggling with SVG primitives beyond path curves. To address\nthese issues, we introduce StarVector, a multimodal large language model for\nSVG generation. It performs image vectorization by understanding image\nsemantics and using SVG primitives for compact, precise outputs. Unlike\ntraditional methods, StarVector works directly in the SVG code space,\nleveraging visual understanding to apply accurate SVG primitives. To train\nStarVector, we create SVG-Stack, a diverse dataset of 2M samples that enables\ngeneralization across vectorization tasks and precise use of primitives like\nellipses, polygons, and text. We address challenges in SVG evaluation, showing\nthat pixel-based metrics like MSE fail to capture the unique qualities of\nvector graphics. We introduce SVG-Bench, a benchmark across 10 datasets, and 3\ntasks: Image-to-SVG, Text-to-SVG generation, and diagram generation. Using this\nsetup, StarVector achieves state-of-the-art performance, producing more compact\nand semantically rich SVGs."}
{"id": "2402.17645", "pdf": "https://arxiv.org/pdf/2402.17645.pdf", "abs": "https://arxiv.org/abs/2402.17645", "title": "SongComposer: A Large Language Model for Lyric and Melody Generation in Song Composition", "authors": ["Shuangrui Ding", "Zihan Liu", "Xiaoyi Dong", "Pan Zhang", "Rui Qian", "Junhao Huang", "Conghui He", "Dahua Lin", "Jiaqi Wang"], "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": "ACL 2025 main. project page: https://pjlab-songcomposer.github.io/\n  code: https://github.com/pjlab-songcomposer/songcomposer", "summary": "Creating lyrics and melodies for the vocal track in a symbolic format, known\nas song composition, demands expert musical knowledge of melody, an advanced\nunderstanding of lyrics, and precise alignment between them. Despite\nachievements in sub-tasks such as lyric generation, lyric-to-melody, and\nmelody-to-lyric, etc, a unified model for song composition has not yet been\nachieved. In this paper, we introduce SongComposer, a pioneering step towards a\nunified song composition model that can readily create symbolic lyrics and\nmelodies following instructions. SongComposer is a music-specialized large\nlanguage model (LLM) that, for the first time, integrates the capability of\nsimultaneously composing lyrics and melodies into LLMs by leveraging three key\ninnovations: 1) a flexible tuple format for word-level alignment of lyrics and\nmelodies, 2) an extended tokenizer vocabulary for song notes, with scalar\ninitialization based on musical knowledge to capture rhythm, and 3) a\nmulti-stage pipeline that captures musical structure, starting with motif-level\nmelody patterns and progressing to phrase-level structure for improved\ncoherence. Extensive experiments demonstrate that SongComposer outperforms\nadvanced LLMs, including GPT-4, in tasks such as lyric-to-melody generation,\nmelody-to-lyric generation, song continuation, and text-to-song creation.\nMoreover, we will release SongCompose, a large-scale dataset for training,\ncontaining paired lyrics and melodies in Chinese and English."}
{"id": "2404.19318", "pdf": "https://arxiv.org/pdf/2404.19318.pdf", "abs": "https://arxiv.org/abs/2404.19318", "title": "Calibration of Large Language Models on Code Summarization", "authors": ["Yuvraj Virk", "Premkumar Devanbu", "Toufique Ahmed"], "categories": ["cs.SE", "cs.CL"], "comment": null, "summary": "A brief, fluent, and relevant summary can be helpful during program\ncomprehension; however, such a summary does require significant human effort to\nproduce. Often, good summaries are unavailable in software projects, which\nmakes maintenance more difficult. There has been a considerable body of\nresearch into automated AI-based methods, using Large Language models (LLMs),\nto generate summaries of code; there also has been quite a bit of work on ways\nto measure the performance of such summarization methods, with special\nattention paid to how closely these AI-generated summaries resemble a summary a\nhuman might have produced. Measures such as BERTScore and BLEU have been\nsuggested and evaluated with human-subject studies.\n  However, LLM-generated summaries can be inaccurate, incomplete, etc.:\ngenerally, too dissimilar to one that a good developer might write. Given an\nLLM-generated code summary, how can a user rationally judge if a summary is\nsufficiently good and reliable? Given just some input source code, and an\nLLM-generated summary, existing approaches can help judge brevity, fluency and\nrelevance of the summary; however, it's difficult to gauge whether an\nLLM-generated summary sufficiently resembles what a human might produce,\nwithout a \"golden\" human-produced summary to compare against. We study this\nresemblance question as calibration problem: given just the code & the summary\nfrom an LLM, can we compute a confidence measure, that provides a reliable\nindication of whether the summary sufficiently resembles what a human would\nhave produced in this situation? We examine this question using several LLMs,\nfor several languages, and in several different settings. Our investigation\nsuggests approaches to provide reliable predictions of the likelihood that an\nLLM-generated summary would sufficiently resemble a summary a human might write\nfor the same code."}
{"id": "2406.13945", "pdf": "https://arxiv.org/pdf/2406.13945.pdf", "abs": "https://arxiv.org/abs/2406.13945", "title": "CityBench: Evaluating the Capabilities of Large Language Models for Urban Tasks", "authors": ["Jie Feng", "Jun Zhang", "Tianhui Liu", "Xin Zhang", "Tianjian Ouyang", "Junbo Yan", "Yuwei Du", "Siqi Guo", "Yong Li"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted by KDD 2025 D&B Track,\n  https://github.com/tsinghua-fib-lab/CityBench", "summary": "As large language models (LLMs) continue to advance and gain widespread use,\nestablishing systematic and reliable evaluation methodologies for LLMs and\nvision-language models (VLMs) has become essential to ensure their real-world\neffectiveness and reliability. There have been some early explorations about\nthe usability of LLMs for limited urban tasks, but a systematic and scalable\nevaluation benchmark is still lacking. The challenge in constructing a\nsystematic evaluation benchmark for urban research lies in the diversity of\nurban data, the complexity of application scenarios and the highly dynamic\nnature of the urban environment. In this paper, we design \\textit{CityBench},\nan interactive simulator based evaluation platform, as the first systematic\nbenchmark for evaluating the capabilities of LLMs for diverse tasks in urban\nresearch. First, we build \\textit{CityData} to integrate the diverse urban data\nand \\textit{CitySimu} to simulate fine-grained urban dynamics. Based on\n\\textit{CityData} and \\textit{CitySimu}, we design 8 representative urban tasks\nin 2 categories of perception-understanding and decision-making as the\n\\textit{CityBench}. With extensive results from 30 well-known LLMs and VLMs in\n13 cities around the world, we find that advanced LLMs and VLMs can achieve\ncompetitive performance in diverse urban tasks requiring commonsense and\nsemantic understanding abilities, e.g., understanding the human dynamics and\nsemantic inference of urban images. Meanwhile, they fail to solve the\nchallenging urban tasks requiring professional knowledge and high-level\nnumerical abilities, e.g., geospatial prediction and traffic control task."}
{"id": "2406.13948", "pdf": "https://arxiv.org/pdf/2406.13948.pdf", "abs": "https://arxiv.org/abs/2406.13948", "title": "CityGPT: Empowering Urban Spatial Cognition of Large Language Models", "authors": ["Jie Feng", "Tianhui Liu", "Yuwei Du", "Siqi Guo", "Yuming Lin", "Yong Li"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted by KDD 2025 Research Track,\n  https://github.com/tsinghua-fib-lab/CityGPT", "summary": "Large language models(LLMs), with their powerful language generation and\nreasoning capabilities, have already achieved notable success in many domains,\ne.g., math and code generation. However, they often fall short when tackling\nreal-life geospatial tasks within urban environments. This limitation stems\nfrom a lack of physical world knowledge and relevant data during training. To\naddress this gap, we propose \\textit{CityGPT}, a systematic framework designed\nto enhance LLMs' understanding of urban space and improve their ability to\nsolve the related urban tasks by integrating a city-scale `world model' into\nthe model. Firstly, we construct a diverse instruction tuning dataset,\n\\textit{CityInstruction}, for injecting urban knowledge into LLMs and\neffectively boosting their spatial reasoning capabilities. Using a combination\nof \\textit{CityInstruction} and open source general instruction data, we\nintroduce a novel and easy-to-use self-weighted fine-tuning method\n(\\textit{SWFT}) to train various LLMs (including ChatGLM3-6B, Llama3-8B, and\nQwen2.5-7B) to enhance their urban spatial capabilities without compromising,\nor even improving, their general abilities. Finally, to validate the\neffectiveness of our proposed framework, we develop a comprehensive text-based\nspatial benchmark \\textit{CityEval} for evaluating the performance of LLMs\nacross a wide range of urban scenarios and geospatial tasks. Extensive\nevaluation results demonstrate that smaller LLMs trained with\n\\textit{CityInstruction} by \\textit{SWFT} method can achieve performance that\nis competitive with, and in some cases superior to, proprietary LLMs when\nassessed using \\textit{CityEval}."}
{"id": "2407.00102", "pdf": "https://arxiv.org/pdf/2407.00102.pdf", "abs": "https://arxiv.org/abs/2407.00102", "title": "Curriculum Learning with Quality-Driven Data Selection", "authors": ["Biao Wu", "Ling Chen"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The impressive multimodal capabilities demonstrated by OpenAI's GPT-4 have\ngenerated significant interest in the development of Multimodal Large Language\nModels (MLLMs). Visual instruction tuning of MLLMs with machine-generated\ninstruction-following data has shown to enhance zero-shot capabilities across\nvarious tasks. However, there has been limited exploration into controlling the\nquality of the instruction data.Current methodologies for data selection in\nMLLMs often rely on single, unreliable scores or use downstream tasks for\nselection, which is time-consuming and can lead to potential overfitting on the\nchosen evaluation datasets. To mitigate these limitations, we propose a novel\ndata selection methodology that utilizes image-text correlation and model\nperplexity to evaluate and select data of varying quality. This approach\nleverages the distinct distribution of these two attributes, mapping data\nquality into a two-dimensional space that allows for the selection of data\nbased on their location within this distribution. By utilizing this space, we\ncan analyze the impact of task type settings, used as prompts, on data quality.\nAdditionally, this space can be used to construct multi-stage subsets of\nvarying quality to facilitate curriculum learning. Our research includes\ncomprehensive experiments conducted on various datasets. The results emphasize\nsubstantial enhancements in five commonly assessed capabilities compared to\nusing the complete dataset. Our codes, data, and models are publicly available\nat: https://anonymous.4open.science/r/EHIT-31B4"}
{"id": "2407.06533", "pdf": "https://arxiv.org/pdf/2407.06533.pdf", "abs": "https://arxiv.org/abs/2407.06533", "title": "LETS-C: Leveraging Text Embedding for Time Series Classification", "authors": ["Rachneet Kaur", "Zhen Zeng", "Tucker Balch", "Manuela Veloso"], "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.CL", "stat.ME"], "comment": "ACL 2025 (Main Conference)", "summary": "Recent advancements in language modeling have shown promising results when\napplied to time series data. In particular, fine-tuning pre-trained large\nlanguage models (LLMs) for time series classification tasks has achieved\nstate-of-the-art (SOTA) performance on standard benchmarks. However, these\nLLM-based models have a significant drawback due to the large model size, with\nthe number of trainable parameters in the millions. In this paper, we propose\nan alternative approach to leveraging the success of language modeling in the\ntime series domain. Instead of fine-tuning LLMs, we utilize a text embedding\nmodel to embed time series and then pair the embeddings with a simple\nclassification head composed of convolutional neural networks (CNN) and\nmultilayer perceptron (MLP). We conducted extensive experiments on a\nwell-established time series classification benchmark. We demonstrated LETS-C\nnot only outperforms the current SOTA in classification accuracy but also\noffers a lightweight solution, using only 14.5% of the trainable parameters on\naverage compared to the SOTA model. Our findings suggest that leveraging text\nembedding models to encode time series data, combined with a simple yet\neffective classification head, offers a promising direction for achieving\nhigh-performance time series classification while maintaining a lightweight\nmodel architecture."}
{"id": "2408.03819", "pdf": "https://arxiv.org/pdf/2408.03819.pdf", "abs": "https://arxiv.org/abs/2408.03819", "title": "Leveraging Variation Theory in Counterfactual Data Augmentation for Optimized Active Learning", "authors": ["Simret Araya Gebreegziabher", "Kuangshi Ai", "Zheng Zhang", "Elena L. Glassman", "Toby Jia-Jun Li"], "categories": ["cs.LG", "cs.CL", "cs.HC"], "comment": "Accepted to ACL 2025 Findings", "summary": "Active Learning (AL) allows models to learn interactively from user feedback.\nThis paper introduces a counterfactual data augmentation approach to AL,\nparticularly addressing the selection of datapoints for user querying, a\npivotal concern in enhancing data efficiency. Our approach is inspired by\nVariation Theory, a theory of human concept learning that emphasizes the\nessential features of a concept by focusing on what stays the same and what\nchanges. Instead of just querying with existing datapoints, our approach\nsynthesizes artificial datapoints that highlight potential key similarities and\ndifferences among labels using a neuro-symbolic pipeline combining large\nlanguage models (LLMs) and rule-based models. Through an experiment in the\nexample domain of text classification, we show that our approach achieves\nsignificantly higher performance when there are fewer annotated data. As the\nannotated training data gets larger the impact of the generated data starts to\ndiminish showing its capability to address the cold start problem in AL. This\nresearch sheds light on integrating theories of human learning into the\noptimization of AL."}
{"id": "2409.04459", "pdf": "https://arxiv.org/pdf/2409.04459.pdf", "abs": "https://arxiv.org/abs/2409.04459", "title": "WET: Overcoming Paraphrasing Vulnerabilities in Embeddings-as-a-Service with Linear Transformation Watermarks", "authors": ["Anudeex Shetty", "Qiongkai Xu", "Jey Han Lau"], "categories": ["cs.CR", "cs.CL", "cs.LG"], "comment": "Accepted to ACL 2025 (Main Proceedings)", "summary": "Embeddings-as-a-Service (EaaS) is a service offered by large language model\n(LLM) developers to supply embeddings generated by LLMs. Previous research\nsuggests that EaaS is prone to imitation attacks -- attacks that clone the\nunderlying EaaS model by training another model on the queried embeddings. As a\nresult, EaaS watermarks are introduced to protect the intellectual property of\nEaaS providers. In this paper, we first show that existing EaaS watermarks can\nbe removed by paraphrasing when attackers clone the model. Subsequently, we\npropose a novel watermarking technique that involves linearly transforming the\nembeddings, and show that it is empirically and theoretically robust against\nparaphrasing."}
{"id": "2409.10289", "pdf": "https://arxiv.org/pdf/2409.10289.pdf", "abs": "https://arxiv.org/abs/2409.10289", "title": "ReflectDiffu:Reflect between Emotion-intent Contagion and Mimicry for Empathetic Response Generation via a RL-Diffusion Framework", "authors": ["Jiahao Yuan", "Zixiang Di", "Zhiqing Cui", "Guisong Yang", "Usman Naseem"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted by ACL 2025 Main Conference", "summary": "Empathetic response generation necessitates the integration of emotional and\nintentional dynamics to foster meaningful interactions. Existing research\neither neglects the intricate interplay between emotion and intent, leading to\nsuboptimal controllability of empathy, or resorts to large language models\n(LLMs), which incur significant computational overhead. In this paper, we\nintroduce ReflectDiffu, a lightweight and comprehensive framework for\nempathetic response generation. This framework incorporates emotion contagion\nto augment emotional expressiveness and employs an emotion-reasoning mask to\npinpoint critical emotional elements. Additionally, it integrates intent\nmimicry within reinforcement learning for refinement during diffusion. By\nharnessing an intent twice reflect mechanism of Exploring-Sampling-Correcting,\nReflectDiffu adeptly translates emotional decision-making into precise intent\nactions, thereby addressing empathetic response misalignments stemming from\nemotional misrecognition. Through reflection, the framework maps emotional\nstates to intents, markedly enhancing both response empathy and flexibility.\nComprehensive experiments reveal that ReflectDiffu outperforms existing models\nregarding relevance, controllability, and informativeness, achieving\nstate-of-the-art results in both automatic and human evaluations."}
{"id": "2410.03960", "pdf": "https://arxiv.org/pdf/2410.03960.pdf", "abs": "https://arxiv.org/abs/2410.03960", "title": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving Model Transformation", "authors": ["Aurick Qiao", "Zhewei Yao", "Samyam Rajbhandari", "Yuxiong He"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "LLM inference for enterprise applications, such as summarization, RAG, and\ncode-generation, typically observe much longer prompt than generations, leading\nto high prefill cost and response latency. We present SwiftKV, a novel model\ntransformation and distillation procedure targeted at reducing the prefill\ncompute (in FLOPs) of prompt tokens while preserving high generation quality.\nFirst, SwiftKV prefills later layers' KV cache using an earlier layer's output,\nallowing prompt tokens to skip those later layers. Second, SwiftKV employs a\nlightweight knowledge-preserving distillation procedure that can adapt existing\nLLMs with minimal accuracy impact. Third, SwiftKV can naturally incorporate KV\ncache compression to improve inference performance in low-memory scenarios. Our\ncomprehensive experiments show that SwiftKV can effectively reduce prefill\ncomputation by 25-50% across several LLM families while incurring minimum\nquality degradation. In the end-to-end inference serving, SwiftKV realizes up\nto 2x higher aggregate throughput and 60% lower time per output token. It can\nachieve a staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B. SwiftKV is open-sourced at\nhttps://github.com/snowflakedb/arctictraining."}
{"id": "2410.09510", "pdf": "https://arxiv.org/pdf/2410.09510.pdf", "abs": "https://arxiv.org/abs/2410.09510", "title": "SciEvo: A 2 Million, 30-Year Cross-disciplinary Dataset for Temporal Scientometric Analysis", "authors": ["Yiqiao Jin", "Yijia Xiao", "Yiyang Wang", "Jindong Wang"], "categories": ["cs.DL", "cs.CL"], "comment": "We have renamed our dataset from \"Scito2M\" to \"SciEvo\"", "summary": "Understanding the creation, evolution, and dissemination of scientific\nknowledge is crucial for bridging diverse subject areas and addressing complex\nglobal challenges such as pandemics, climate change, and ethical AI.\nScientometrics, the quantitative and qualitative study of scientific\nliterature, provides valuable insights into these processes. We introduce\nSciEvo, a longitudinal scientometric dataset with over two million academic\npublications, providing comprehensive contents information and citation graphs\nto support cross-disciplinary analyses. SciEvo is easy to use and available\nacross platforms, including GitHub, Kaggle, and HuggingFace. Using SciEvo, we\nconduct a temporal study spanning over 30 years to explore key questions in\nscientometrics: the evolution of academic terminology, citation patterns, and\ninterdisciplinary knowledge exchange. Our findings reveal critical insights,\nsuch as disparities in epistemic cultures, knowledge production modes, and\ncitation practices. For example, rapidly developing, application-driven fields\nlike LLMs exhibit significantly shorter citation age (2.48 years) compared to\ntraditional theoretical disciplines like oral history (9.71 years). Our data\nand analytic tools can be accessed at https://github.com/Ahren09/SciEvo."}
{"id": "2410.11674", "pdf": "https://arxiv.org/pdf/2410.11674.pdf", "abs": "https://arxiv.org/abs/2410.11674", "title": "LLM-Mixer: Multiscale Mixing in LLMs for Time Series Forecasting", "authors": ["Md Kowsher", "Md. Shohanur Islam Sobuj", "Nusrat Jahan Prottasha", "E. Alejandro Alanis", "Ozlem Ozmen Garibay", "Niloofar Yousefi"], "categories": ["cs.LG", "cs.CL"], "comment": "Time series forecasting using LLMs", "summary": "Time series forecasting remains a challenging task, particularly in the\ncontext of complex multiscale temporal patterns. This study presents LLM-Mixer,\na framework that improves forecasting accuracy through the combination of\nmultiscale time-series decomposition with pre-trained LLMs (Large Language\nModels). LLM-Mixer captures both short-term fluctuations and long-term trends\nby decomposing the data into multiple temporal resolutions and processing them\nwith a frozen LLM, guided by a textual prompt specifically designed for\ntime-series data. Extensive experiments conducted on multivariate and\nunivariate datasets demonstrate that LLM-Mixer achieves competitive\nperformance, outperforming recent state-of-the-art models across various\nforecasting horizons. This work highlights the potential of combining\nmultiscale analysis and LLMs for effective and scalable time-series\nforecasting."}
{"id": "2410.13097", "pdf": "https://arxiv.org/pdf/2410.13097.pdf", "abs": "https://arxiv.org/abs/2410.13097", "title": "Communication-Efficient and Tensorized Federated Fine-Tuning of Large Language Models", "authors": ["Sajjad Ghiasvand", "Yifan Yang", "Zhiyu Xue", "Mahnoosh Alizadeh", "Zheng Zhang", "Ramtin Pedarsani"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Parameter-efficient fine-tuning (PEFT) methods typically assume that Large\nLanguage Models (LLMs) are trained on data from a single device or client.\nHowever, real-world scenarios often require fine-tuning these models on private\ndata distributed across multiple devices. Federated Learning (FL) offers an\nappealing solution by preserving user privacy, as sensitive data remains on\nlocal devices during training. Nonetheless, integrating PEFT methods into FL\nintroduces two main challenges: communication overhead and data heterogeneity.\nIn this paper, we introduce FedTT and FedTT+, methods for adapting LLMs by\nintegrating tensorized adapters into client-side models' encoder/decoder\nblocks. FedTT is versatile and can be applied to both cross-silo FL and\nlarge-scale cross-device FL. FedTT+, an extension of FedTT tailored for\ncross-silo FL, enhances robustness against data heterogeneity by adaptively\nfreezing portions of tensor factors, further reducing the number of trainable\nparameters. Experiments on BERT and LLaMA models demonstrate that our proposed\nmethods successfully address data heterogeneity challenges and perform on par\nor even better than existing federated PEFT approaches while achieving up to\n10$\\times$ reduction in communication cost."}
{"id": "2410.13248", "pdf": "https://arxiv.org/pdf/2410.13248.pdf", "abs": "https://arxiv.org/abs/2410.13248", "title": "Disentangling Likes and Dislikes in Personalized Generative Explainable Recommendation", "authors": ["Ryotaro Shimizu", "Takashi Wada", "Yu Wang", "Johannes Kruse", "Sean O'Brien", "Sai HtaungKham", "Linxin Song", "Yuya Yoshikawa", "Yuki Saito", "Fugee Tsung", "Masayuki Goto", "Julian McAuley"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR"], "comment": "This manuscript has been accepted for presentation at The Web\n  Conference (WWW) 2025", "summary": "Recent research on explainable recommendation generally frames the task as a\nstandard text generation problem, and evaluates models simply based on the\ntextual similarity between the predicted and ground-truth explanations.\nHowever, this approach fails to consider one crucial aspect of the systems:\nwhether their outputs accurately reflect the users' (post-purchase) sentiments,\ni.e., whether and why they would like and/or dislike the recommended items. To\nshed light on this issue, we introduce new datasets and evaluation methods that\nfocus on the users' sentiments. Specifically, we construct the datasets by\nexplicitly extracting users' positive and negative opinions from their\npost-purchase reviews using an LLM, and propose to evaluate systems based on\nwhether the generated explanations 1) align well with the users' sentiments,\nand 2) accurately identify both positive and negative opinions of users on the\ntarget items. We benchmark several recent models on our datasets and\ndemonstrate that achieving strong performance on existing metrics does not\nensure that the generated explanations align well with the users' sentiments.\nLastly, we find that existing models can provide more sentiment-aware\nexplanations when the users' (predicted) ratings for the target items are\ndirectly fed into the models as input. The datasets and benchmark\nimplementation are available at: https://github.com/jchanxtarov/sent_xrec."}
{"id": "2410.14991", "pdf": "https://arxiv.org/pdf/2410.14991.pdf", "abs": "https://arxiv.org/abs/2410.14991", "title": "ChitroJera: A Regionally Relevant Visual Question Answering Dataset for Bangla", "authors": ["Deeparghya Dutta Barua", "Md Sakib Ul Rahman Sourove", "Md Fahim", "Fabiha Haider", "Fariha Tanjim Shifat", "Md Tasmim Rahman Adib", "Anam Borhan Uddin", "Md Farhan Ishmam", "Md Farhad Alam"], "categories": ["cs.CV", "cs.CL"], "comment": "Accepted in ECML PKDD 2025", "summary": "Visual Question Answer (VQA) poses the problem of answering a natural\nlanguage question about a visual context. Bangla, despite being a widely spoken\nlanguage, is considered low-resource in the realm of VQA due to the lack of\nproper benchmarks, challenging models known to be performant in other\nlanguages. Furthermore, existing Bangla VQA datasets offer little regional\nrelevance and are largely adapted from their foreign counterparts. To address\nthese challenges, we introduce a large-scale Bangla VQA dataset, ChitroJera,\ntotaling over 15k samples from diverse and locally relevant data sources. We\nassess the performance of text encoders, image encoders, multimodal models, and\nour novel dual-encoder models. The experiments reveal that the pre-trained\ndual-encoders outperform other models of their scale. We also evaluate the\nperformance of current large vision language models (LVLMs) using prompt-based\ntechniques, achieving the overall best performance. Given the underdeveloped\nstate of existing datasets, we envision ChitroJera expanding the scope of\nVision-Language tasks in Bangla."}
{"id": "2410.17401", "pdf": "https://arxiv.org/pdf/2410.17401.pdf", "abs": "https://arxiv.org/abs/2410.17401", "title": "AdvAgent: Controllable Blackbox Red-teaming on Web Agents", "authors": ["Chejian Xu", "Mintong Kang", "Jiawei Zhang", "Zeyi Liao", "Lingbo Mo", "Mengqi Yuan", "Huan Sun", "Bo Li"], "categories": ["cs.CR", "cs.CL"], "comment": "ICML 2025", "summary": "Foundation model-based agents are increasingly used to automate complex\ntasks, enhancing efficiency and productivity. However, their access to\nsensitive resources and autonomous decision-making also introduce significant\nsecurity risks, where successful attacks could lead to severe consequences. To\nsystematically uncover these vulnerabilities, we propose AdvAgent, a black-box\nred-teaming framework for attacking web agents. Unlike existing approaches,\nAdvAgent employs a reinforcement learning-based pipeline to train an\nadversarial prompter model that optimizes adversarial prompts using feedback\nfrom the black-box agent. With careful attack design, these prompts effectively\nexploit agent weaknesses while maintaining stealthiness and controllability.\nExtensive evaluations demonstrate that AdvAgent achieves high success rates\nagainst state-of-the-art GPT-4-based web agents across diverse web tasks.\nFurthermore, we find that existing prompt-based defenses provide only limited\nprotection, leaving agents vulnerable to our framework. These findings\nhighlight critical vulnerabilities in current web agents and emphasize the\nurgent need for stronger defense mechanisms. We release code at\nhttps://ai-secure.github.io/AdvAgent/."}
{"id": "2410.18057", "pdf": "https://arxiv.org/pdf/2410.18057.pdf", "abs": "https://arxiv.org/abs/2410.18057", "title": "CLEAR: Character Unlearning in Textual and Visual Modalities", "authors": ["Alexey Dontsov", "Dmitrii Korzh", "Alexey Zhavoronkin", "Boris Mikheev", "Denis Bobkov", "Aibek Alanov", "Oleg Y. Rogov", "Ivan Oseledets", "Elena Tutubalina"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Machine Unlearning (MU) is critical for removing private or hazardous\ninformation from deep learning models. While MU has advanced significantly in\nunimodal (text or vision) settings, multimodal unlearning (MMU) remains\nunderexplored due to the lack of open benchmarks for evaluating cross-modal\ndata removal. To address this gap, we introduce CLEAR, the first open-source\nbenchmark designed specifically for MMU. CLEAR contains 200 fictitious\nindividuals and 3,700 images linked with corresponding question-answer pairs,\nenabling a thorough evaluation across modalities. We conduct a comprehensive\nanalysis of 11 MU methods (e.g., SCRUB, gradient ascent, DPO) across four\nevaluation sets, demonstrating that jointly unlearning both modalities\noutperforms single-modality approaches. The dataset is available at\nhttps://huggingface.co/datasets/therem/CLEAR"}
{"id": "2411.05902", "pdf": "https://arxiv.org/pdf/2411.05902.pdf", "abs": "https://arxiv.org/abs/2411.05902", "title": "Autoregressive Models in Vision: A Survey", "authors": ["Jing Xiong", "Gongye Liu", "Lun Huang", "Chengyue Wu", "Taiqiang Wu", "Yao Mu", "Yuan Yao", "Hui Shen", "Zhongwei Wan", "Jinfa Huang", "Chaofan Tao", "Shen Yan", "Huaxiu Yao", "Lingpeng Kong", "Hongxia Yang", "Mi Zhang", "Guillermo Sapiro", "Jiebo Luo", "Ping Luo", "Ngai Wong"], "categories": ["cs.CV", "cs.CL"], "comment": "The paper is accepted by TMLR", "summary": "Autoregressive modeling has been a huge success in the field of natural\nlanguage processing (NLP). Recently, autoregressive models have emerged as a\nsignificant area of focus in computer vision, where they excel in producing\nhigh-quality visual content. Autoregressive models in NLP typically operate on\nsubword tokens. However, the representation strategy in computer vision can\nvary in different levels, i.e., pixel-level, token-level, or scale-level,\nreflecting the diverse and hierarchical nature of visual data compared to the\nsequential structure of language. This survey comprehensively examines the\nliterature on autoregressive models applied to vision. To improve readability\nfor researchers from diverse research backgrounds, we start with preliminary\nsequence representation and modeling in vision. Next, we divide the fundamental\nframeworks of visual autoregressive models into three general sub-categories,\nincluding pixel-based, token-based, and scale-based models based on the\nrepresentation strategy. We then explore the interconnections between\nautoregressive models and other generative models. Furthermore, we present a\nmultifaceted categorization of autoregressive models in computer vision,\nincluding image generation, video generation, 3D generation, and multimodal\ngeneration. We also elaborate on their applications in diverse domains,\nincluding emerging domains such as embodied AI and 3D medical AI, with about\n250 related references. Finally, we highlight the current challenges to\nautoregressive models in vision with suggestions about potential research\ndirections. We have also set up a Github repository to organize the papers\nincluded in this survey at:\nhttps://github.com/ChaofanTao/Autoregressive-Models-in-Vision-Survey."}
{"id": "2411.09689", "pdf": "https://arxiv.org/pdf/2411.09689.pdf", "abs": "https://arxiv.org/abs/2411.09689", "title": "Probing LLM Hallucination from Within: Perturbation-Driven Approach via Internal Knowledge", "authors": ["Seongmin Lee", "Hsiang Hsu", "Chun-Fu Chen", "Duen Horng", "Chau"], "categories": ["cs.AI", "cs.CL"], "comment": "22 pages, 15 figures", "summary": "LLM hallucination, where unfaithful text is generated, presents a critical\nchallenge for LLMs' practical applications. Current detection methods often\nresort to external knowledge, LLM fine-tuning, or supervised training with\nlarge hallucination-labeled datasets. Moreover, these approaches do not\ndistinguish between different types of hallucinations, which is crucial for\nenhancing detection performance. To address such limitations, we introduce\nhallucination probing, a new task that classifies LLM-generated text into three\ncategories: aligned, misaligned, and fabricated. Driven by our novel discovery\nthat perturbing key entities in prompts affects LLM's generation of these three\ntypes of text differently, we propose SHINE, a novel hallucination probing\nmethod that does not require external knowledge, supervised training, or LLM\nfine-tuning. SHINE is effective in hallucination probing across three modern\nLLMs, and achieves state-of-the-art performance in hallucination detection,\noutperforming seven competing methods across four datasets and four LLMs,\nunderscoring the importance of probing for accurate detection."}
{"id": "2411.17451", "pdf": "https://arxiv.org/pdf/2411.17451.pdf", "abs": "https://arxiv.org/abs/2411.17451", "title": "VL-RewardBench: A Challenging Benchmark for Vision-Language Generative Reward Models", "authors": ["Lei Li", "Yuancheng Wei", "Zhihui Xie", "Xuqing Yang", "Yifan Song", "Peiyi Wang", "Chenxin An", "Tianyu Liu", "Sujian Li", "Bill Yuchen Lin", "Lingpeng Kong", "Qi Liu"], "categories": ["cs.CV", "cs.CL"], "comment": "CVPR 2025 Camera Ready Version. Project page:\n  https://vl-rewardbench.github.io", "summary": "Vision-language generative reward models (VL-GenRMs) play a crucial role in\naligning and evaluating multimodal AI systems, yet their own evaluation remains\nunder-explored. Current assessment methods primarily rely on AI-annotated\npreference labels from traditional VL tasks, which can introduce biases and\noften fail to effectively challenge state-of-the-art models. To address these\nlimitations, we introduce VL-RewardBench, a comprehensive benchmark spanning\ngeneral multimodal queries, visual hallucination detection, and complex\nreasoning tasks. Through our AI-assisted annotation pipeline that combines\nsample selection with human verification, we curate 1,250 high-quality examples\nspecifically designed to probe VL-GenRMs limitations. Comprehensive evaluation\nacross 16 leading large vision-language models demonstrates VL-RewardBench's\neffectiveness as a challenging testbed, where even GPT-4o achieves only 65.4%\naccuracy, and state-of-the-art open-source models such as Qwen2-VL-72B,\nstruggle to surpass random-guessing. Importantly, performance on VL-RewardBench\nstrongly correlates (Pearson's r $>$ 0.9) with MMMU-Pro accuracy using\nBest-of-N sampling with VL-GenRMs. Analysis experiments uncover three critical\ninsights for improving VL-GenRMs: (i) models predominantly fail at basic visual\nperception tasks rather than reasoning tasks; (ii) inference-time scaling\nbenefits vary dramatically by model capacity; and (iii) training VL-GenRMs to\nlearn to judge substantially boosts judgment capability (+14.7% accuracy for a\n7B VL-GenRM). We believe VL-RewardBench along with the experimental insights\nwill become a valuable resource for advancing VL-GenRMs."}
{"id": "2412.13147", "pdf": "https://arxiv.org/pdf/2412.13147.pdf", "abs": "https://arxiv.org/abs/2412.13147", "title": "Are Your LLMs Capable of Stable Reasoning?", "authors": ["Junnan Liu", "Hongwei Liu", "Linchen Xiao", "Ziyi Wang", "Kuikun Liu", "Songyang Gao", "Wenwei Zhang", "Songyang Zhang", "Kai Chen"], "categories": ["cs.AI", "cs.CL"], "comment": "ACL 2025 Camera", "summary": "The rapid advancement of large language models (LLMs) has shown remarkable\nprogress in complex reasoning tasks. However, a significant disparity exists\nbetween benchmark performances and real-world applications. We attribute this\ngap primarily to current evaluation protocols and metrics, which inadequately\ncapture the full spectrum of LLM capabilities, especially in complex reasoning\ntasks where both accuracy and consistency are essential. In this paper, we\nintroduce G-Pass@$k$, a novel evaluation metric that continuously assesses\nmodel performance across multiple sampling attempts, quantifying both the\nmodel's performance potential and its stability. Through extensive experiments\non various public and newly constructed benchmarks, we employ G-Pass@$k$ in\nconjunction with state-of-the-art large language models to provide\ncomprehensive insights into their potential capabilities and operational\nconsistency. Our findings reveal a significant opportunity to enhance the\nrealistic reasoning abilities of LLMs, underscoring the necessity for more\nrobust evaluation metrics."}
{"id": "2412.13631", "pdf": "https://arxiv.org/pdf/2412.13631.pdf", "abs": "https://arxiv.org/abs/2412.13631", "title": "Mind Your Theory: Theory of Mind Goes Deeper Than Reasoning", "authors": ["Eitan Wagner", "Nitay Alon", "Joseph M. Barnby", "Omri Abend"], "categories": ["cs.AI", "cs.CL"], "comment": "4 pages, 2 figures, accepted to ACL2025 Findings", "summary": "Theory of Mind (ToM) capabilities in LLMs have recently become a central\nobject of investigation. Cognitive science distinguishes between two steps\nrequired for ToM tasks: 1) determine whether to invoke ToM, which includes the\nappropriate Depth of Mentalizing (DoM), or level of recursion required to\ncomplete a task; and 2) applying the correct inference given the DoM. In this\nposition paper, we first identify several lines of work in different\ncommunities in AI, including LLM benchmarking, ToM add-ons, ToM probing, and\nformal models for ToM. We argue that recent work in AI tends to focus\nexclusively on the second step which are typically framed as static logic\nproblems. We conclude with suggestions for improved evaluation of ToM\ncapabilities inspired by dynamic environments used in cognitive tasks."}
{"id": "2412.18148", "pdf": "https://arxiv.org/pdf/2412.18148.pdf", "abs": "https://arxiv.org/abs/2412.18148", "title": "Are We in the AI-Generated Text World Already? Quantifying and Monitoring AIGT on Social Media", "authors": ["Zhen Sun", "Zongmin Zhang", "Xinyue Shen", "Ziyi Zhang", "Yule Liu", "Michael Backes", "Yang Zhang", "Xinlei He"], "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.SI"], "comment": "Accepted at ACL 2025 Main Conference. 29 pages, 21 figures, 12 tables", "summary": "Social media platforms are experiencing a growing presence of AI-Generated\nTexts (AIGTs). However, the misuse of AIGTs could have profound implications\nfor public opinion, such as spreading misinformation and manipulating\nnarratives. Despite its importance, it remains unclear how prevalent AIGTs are\non social media. To address this gap, this paper aims to quantify and monitor\nthe AIGTs on online social media platforms. We first collect a dataset (SM-D)\nwith around 2.4M posts from 3 major social media platforms: Medium, Quora, and\nReddit. Then, we construct a diverse dataset (AIGTBench) to train and evaluate\nAIGT detectors. AIGTBench combines popular open-source datasets and our AIGT\ndatasets generated from social media texts by 12 LLMs, serving as a benchmark\nfor evaluating mainstream detectors. With this setup, we identify the\nbest-performing detector (OSM-Det). We then apply OSM-Det to SM-D to track\nAIGTs across social media platforms from January 2022 to October 2024, using\nthe AI Attribution Rate (AAR) as the metric. Specifically, Medium and Quora\nexhibit marked increases in AAR, rising from 1.77% to 37.03% and 2.06% to\n38.95%, respectively. In contrast, Reddit shows slower growth, with AAR\nincreasing from 1.31% to 2.45% over the same period. Our further analysis\nindicates that AIGTs on social media differ from human-written texts across\nseveral dimensions, including linguistic patterns, topic distributions,\nengagement levels, and the follower distribution of authors. We envision our\nanalysis and findings on AIGTs in social media can shed light on future\nresearch in this domain."}
{"id": "2412.20070", "pdf": "https://arxiv.org/pdf/2412.20070.pdf", "abs": "https://arxiv.org/abs/2412.20070", "title": "Exploring Compositional Generalization of Multimodal LLMs for Medical Imaging", "authors": ["Zhenyang Cai", "Junying Chen", "Rongsheng Wang", "Weihong Wang", "Yonglin Deng", "Dingjie Song", "Yize Chen", "Zixu Zhang", "Benyou Wang"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Medical imaging provides essential visual insights for diagnosis, and\nmultimodal large language models (MLLMs) are increasingly utilized for its\nanalysis due to their strong generalization capabilities; however, the\nunderlying factors driving this generalization remain unclear. Current research\nsuggests that multi-task training outperforms single-task as different tasks\ncan benefit each other, but they often overlook the internal relationships\nwithin these tasks. To analyze this phenomenon, we attempted to employ\ncompositional generalization (CG), which refers to the models' ability to\nunderstand novel combinations by recombining learned elements, as a guiding\nframework. Since medical images can be precisely defined by Modality,\nAnatomical area, and Task, naturally providing an environment for exploring CG,\nwe assembled 106 medical datasets to create Med-MAT for comprehensive\nexperiments. The experiments confirmed that MLLMs can use CG to understand\nunseen medical images and identified CG as one of the main drivers of the\ngeneralization observed in multi-task training. Additionally, further studies\ndemonstrated that CG effectively supports datasets with limited data and\nconfirmed that MLLMs can achieve CG across classification and detection tasks,\nunderscoring its broader generalization potential. Med-MAT is available at\nhttps://github.com/FreedomIntelligence/Med-MAT."}
{"id": "2501.00659", "pdf": "https://arxiv.org/pdf/2501.00659.pdf", "abs": "https://arxiv.org/abs/2501.00659", "title": "Why Are Positional Encodings Nonessential for Deep Autoregressive Transformers? Revisiting a Petroglyph", "authors": ["Kazuki Irie"], "categories": ["cs.LG", "cs.CL"], "comment": "Accepted to ACL 2025 Findings, Short paper", "summary": "Do autoregressive Transformer language models require explicit positional\nencodings (PEs)? The answer is 'no' provided they have more than one layer --\nthey can distinguish sequences with permuted tokens without the need for\nexplicit PEs. This follows from the fact that a cascade of (permutation\ninvariant) set processors can collectively exhibit sequence-sensitive behavior\nin the autoregressive setting. This property has been known since early efforts\n(contemporary with GPT-2) adopting the Transformer for language modeling.\nHowever, this result does not appear to have been well disseminated, leading to\nrecent rediscoveries. This may be partially due to a sudden growth of the\nlanguage modeling community after the advent of GPT-2/3, but perhaps also due\nto the lack of a clear explanation in prior work, despite being commonly\nunderstood by practitioners in the past. Here we review the long-forgotten\nexplanation why explicit PEs are nonessential for multi-layer autoregressive\nTransformers (in contrast, one-layer models require PEs to discern order\ninformation of their inputs), as well as the origin of this result, and hope to\nre-establish it as a common knowledge."}
{"id": "2501.02669", "pdf": "https://arxiv.org/pdf/2501.02669.pdf", "abs": "https://arxiv.org/abs/2501.02669", "title": "Generalizing from SIMPLE to HARD Visual Reasoning: Can We Mitigate Modality Imbalance in VLMs?", "authors": ["Simon Park", "Abhishek Panigrahi", "Yun Cheng", "Dingli Yu", "Anirudh Goyal", "Sanjeev Arora"], "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": null, "summary": "Vision Language Models (VLMs) are impressive at visual question answering and\nimage captioning. But they underperform on multi-step visual reasoning -- even\ncompared to LLMs on the same tasks presented in text form -- giving rise to\nperceptions of modality imbalance or brittleness. Towards a systematic study of\nsuch issues, we introduce a synthetic framework for assessing the ability of\nVLMs to perform algorithmic visual reasoning, comprising three tasks: Table\nReadout, Grid Navigation, and Visual Analogy. Each has two levels of\ndifficulty, SIMPLE and HARD, and even the SIMPLE versions are difficult for\nfrontier VLMs. We propose strategies for training on the SIMPLE version of\ntasks that improve performance on the corresponding HARD task, i.e.,\nsimple-to-hard (S2H) generalization. This controlled setup, where each task\nalso has an equivalent text-only version, allows a quantification of the\nmodality imbalance and how it is impacted by training strategy. We show that 1)\nexplicit image-to-text conversion is important in promoting S2H generalization\non images, by transferring reasoning from text; 2) conversion can be\ninternalized at test time. We also report results of mechanistic study of this\nphenomenon. We identify measures of gradient alignment that can identify\ntraining strategies that promote better S2H generalization. Ablations highlight\nthe importance of chain-of-thought."}
{"id": "2501.05966", "pdf": "https://arxiv.org/pdf/2501.05966.pdf", "abs": "https://arxiv.org/abs/2501.05966", "title": "Towards Early Prediction of Self-Supervised Speech Model Performance", "authors": ["Ryan Whetten", "Lucas Maison", "Titouan Parcollet", "Marco Dinarelli", "Yannick Est√®ve"], "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "comment": null, "summary": "In Self-Supervised Learning (SSL), pre-training and evaluation are resource\nintensive. In the speech domain, current indicators of the quality of SSL\nmodels during pre-training, such as the loss, do not correlate well with\ndownstream performance. Consequently, it is often difficult to gauge the final\ndownstream performance in a cost efficient manner during pre-training. In this\nwork, we propose unsupervised efficient methods that give insights into the\nquality of the pre-training of SSL speech models, namely, measuring the cluster\nquality and rank of the embeddings of the SSL model. Results show that measures\nof cluster quality and rank correlate better with downstream performance than\nthe pre-training loss with only one hour of unlabeled audio, reducing the need\nfor GPU hours and labeled data in SSL model evaluation."}
{"id": "2501.15056", "pdf": "https://arxiv.org/pdf/2501.15056.pdf", "abs": "https://arxiv.org/abs/2501.15056", "title": "Feedback-Aware Monte Carlo Tree Search for Efficient Information Seeking in Goal-Oriented Conversations", "authors": ["Harshita Chopra", "Chirag Shah"], "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG"], "comment": null, "summary": "Effective decision-making and problem-solving in conversational systems\nrequire the ability to identify and acquire missing information through\ntargeted questioning. A key challenge lies in efficiently narrowing down a\nlarge space of possible outcomes by posing questions that minimize uncertainty.\nTo address this, we introduce a novel framework that leverages Large Language\nModels (LLMs) to generate information-seeking questions, with Monte Carlo Tree\nSearch (MCTS) to strategically select questions that maximize information gain,\nas a part of inference-time planning. Our primary contribution includes a\nhierarchical feedback mechanism that exploits past interaction patterns to\nguide future strategy. Specifically, each new problem is mapped to a cluster\nbased on semantic similarity, and our UCT (Upper Confidence bound for Trees)\nformulation employs a cluster-specific bonus reward to prioritize successful\nquestion trajectories that have proven effective for similar problems in the\npast. Extensive empirical evaluation across medical diagnosis and technical\ntroubleshooting domains shows that our method achieves an average of 12%\nimprovement in success rates and about 10x reduction in the number of LLM calls\nmade for planning per conversation, compared to the state of the art. An\nadditional 8% gain in success rate is observed on average when we start with a\nconstrained set of possibilities. Our results underscore the efficacy of\nfeedback-aware MCTS in enhancing information-seeking in goal-oriented\ndialogues."}
{"id": "2501.15278", "pdf": "https://arxiv.org/pdf/2501.15278.pdf", "abs": "https://arxiv.org/abs/2501.15278", "title": "PIP: Perturbation-based Iterative Pruning for Large Language Models", "authors": ["Yi Cao", "Wei-Jie Xu", "Yucheng Shen", "Weijie Shi", "Chi-Min Chan", "Jianfeng Qu", "Jiajie Xu"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "The rapid increase in the parameter counts of Large Language Models (LLMs),\nreaching billions or even trillions, presents significant challenges for their\npractical deployment, particularly in resource-constrained environments. To\nease this issue, we propose PIP (Perturbation-based Iterative Pruning), a novel\ndouble-view structured pruning method to optimize LLMs, which combines\ninformation from two different views: the unperturbed view and the perturbed\nview. With the calculation of gradient differences, PIP iteratively prunes\nthose that struggle to distinguish between these two views. Our experiments\nshow that PIP reduces the parameter count by approximately 20% while retaining\nover 85% of the original model's accuracy across varied benchmarks. In some\ncases, the performance of the pruned model is within 5% of the unpruned\nversion, demonstrating PIP's ability to preserve key aspects of model\neffectiveness. Moreover, PIP consistently outperforms existing state-of-the-art\n(SOTA) structured pruning methods, establishing it as a leading technique for\noptimizing LLMs in environments with constrained resources."}
{"id": "2501.16344", "pdf": "https://arxiv.org/pdf/2501.16344.pdf", "abs": "https://arxiv.org/abs/2501.16344", "title": "WhiSPA: Semantically and Psychologically Aligned Whisper with Self-Supervised Contrastive and Student-Teacher Learning", "authors": ["Rajath Rao", "Adithya Ganesan", "Oscar Kjell", "Jonah Luby", "Akshay Raghavan", "Scott Feltman", "Whitney Ringwald", "Ryan L. Boyd", "Benjamin Luft", "Camilo Ruggero", "Neville Ryant", "Roman Kotov", "H. Andrew Schwartz"], "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": "16 pages, 8 figures, ACL 2025", "summary": "Current speech encoding pipelines often rely on an additional text-based LM\nto get robust representations of human communication, even though SotA\nspeech-to-text models often have a LM within. This work proposes an approach to\nimprove the LM within an audio model such that the subsequent text-LM is\nunnecessary. We introduce WhiSPA (Whisper with Semantic and Psychological\nAlignment), which leverages a novel audio training objective: contrastive loss\nwith a language model embedding as a teacher. Using over 500k speech segments\nfrom mental health audio interviews, we evaluate the utility of aligning\nWhisper's latent space with semantic representations from a text autoencoder\n(SBERT) and lexically derived embeddings of basic psychological dimensions:\nemotion and personality. Over self-supervised affective tasks and downstream\npsychological tasks, WhiSPA surpasses current speech encoders, achieving an\naverage error reduction of 73.4% and 83.8%, respectively. WhiSPA demonstrates\nthat it is not always necessary to run a subsequent text LM on speech-to-text\noutput in order to get a rich psychological representation of human\ncommunication."}
{"id": "2501.18626", "pdf": "https://arxiv.org/pdf/2501.18626.pdf", "abs": "https://arxiv.org/abs/2501.18626", "title": "The TIP of the Iceberg: Revealing a Hidden Class of Task-in-Prompt Adversarial Attacks on LLMs", "authors": ["Sergey Berezin", "Reza Farahbakhsh", "Noel Crespi"], "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": "Accepted to the Main Track of ACL 2025", "summary": "We present a novel class of jailbreak adversarial attacks on LLMs, termed\nTask-in-Prompt (TIP) attacks. Our approach embeds sequence-to-sequence tasks\n(e.g., cipher decoding, riddles, code execution) into the model's prompt to\nindirectly generate prohibited inputs. To systematically assess the\neffectiveness of these attacks, we introduce the PHRYGE benchmark. We\ndemonstrate that our techniques successfully circumvent safeguards in six\nstate-of-the-art language models, including GPT-4o and LLaMA 3.2. Our findings\nhighlight critical weaknesses in current LLM safety alignments and underscore\nthe urgent need for more sophisticated defence strategies.\n  Warning: this paper contains examples of unethical inquiries used solely for\nresearch purposes."}
{"id": "2502.04420", "pdf": "https://arxiv.org/pdf/2502.04420.pdf", "abs": "https://arxiv.org/abs/2502.04420", "title": "KVTuner: Sensitivity-Aware Layer-Wise Mixed-Precision KV Cache Quantization for Efficient and Nearly Lossless LLM Inference", "authors": ["Xing Li", "Zeyu Xing", "Yiming Li", "Linping Qu", "Hui-Ling Zhen", "Wulong Liu", "Yiwu Yao", "Sinno Jialin Pan", "Mingxuan Yuan"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted by ICML25. Code: https://github.com/cmd2001/KVTuner", "summary": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we theoretically analyze the\ninherent correlation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is generally more important than\nvalue cache for quantization error reduction. We further propose a simple yet\neffective framework KVTuner to adaptively search for the optimal\nhardware-friendly layer-wise KV quantization precision pairs for coarse-grained\nKV cache with multi-objective optimization and directly utilize the offline\nsearched configurations during online inference. To reduce the computational\ncost of offline calibration, we utilize the intra-layer KV precision pair\npruning and inter-layer clustering to reduce the search space. Experimental\nresults show that we can achieve nearly lossless 3.25-bit mixed precision KV\ncache quantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for\nsensitive models like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The\nmaximum inference throughput can be improved by 21.25\\% compared with KIVI-KV8\nquantization over various context lengths. Our code and searched configurations\nare available at https://github.com/cmd2001/KVTuner."}
{"id": "2502.05206", "pdf": "https://arxiv.org/pdf/2502.05206.pdf", "abs": "https://arxiv.org/abs/2502.05206", "title": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "authors": ["Xingjun Ma", "Yifeng Gao", "Yixu Wang", "Ruofan Wang", "Xin Wang", "Ye Sun", "Yifan Ding", "Hengyuan Xu", "Yunhao Chen", "Yunhan Zhao", "Hanxun Huang", "Yige Li", "Jiaming Zhang", "Xiang Zheng", "Yang Bai", "Zuxuan Wu", "Xipeng Qiu", "Jingfeng Zhang", "Yiming Li", "Xudong Han", "Haonan Li", "Jun Sun", "Cong Wang", "Jindong Gu", "Baoyuan Wu", "Siheng Chen", "Tianwei Zhang", "Yang Liu", "Mingming Gong", "Tongliang Liu", "Shirui Pan", "Cihang Xie", "Tianyu Pang", "Yinpeng Dong", "Ruoxi Jia", "Yang Zhang", "Shiqing Ma", "Xiangyu Zhang", "Neil Gong", "Chaowei Xiao", "Sarah Erfani", "Tim Baldwin", "Bo Li", "Masashi Sugiyama", "Dacheng Tao", "James Bailey", "Yu-Gang Jiang"], "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CV"], "comment": "47 pages, 3 figures, 11 tables; GitHub:\n  https://github.com/xingjunm/Awesome-Large-Model-Safety", "summary": "The rapid advancement of large models, driven by their exceptional abilities\nin learning and generalization through large-scale pre-training, has reshaped\nthe landscape of Artificial Intelligence (AI). These models are now\nfoundational to a wide range of applications, including conversational AI,\nrecommendation systems, autonomous driving, content generation, medical\ndiagnostics, and scientific discovery. However, their widespread deployment\nalso exposes them to significant safety risks, raising concerns about\nrobustness, reliability, and ethical implications. This survey provides a\nsystematic review of current safety research on large models, covering Vision\nFoundation Models (VFMs), Large Language Models (LLMs), Vision-Language\nPre-training (VLP) models, Vision-Language Models (VLMs), Diffusion Models\n(DMs), and large-model-based Agents. Our contributions are summarized as\nfollows: (1) We present a comprehensive taxonomy of safety threats to these\nmodels, including adversarial attacks, data poisoning, backdoor attacks,\njailbreak and prompt injection attacks, energy-latency attacks, data and model\nextraction attacks, and emerging agent-specific threats. (2) We review defense\nstrategies proposed for each type of attacks if available and summarize the\ncommonly used datasets and benchmarks for safety research. (3) Building on\nthis, we identify and discuss the open challenges in large model safety,\nemphasizing the need for comprehensive safety evaluations, scalable and\neffective defense mechanisms, and sustainable data practices. More importantly,\nwe highlight the necessity of collective efforts from the research community\nand international collaboration. Our work can serve as a useful reference for\nresearchers and practitioners, fostering the ongoing development of\ncomprehensive defense systems and platforms to safeguard AI models."}
{"id": "2502.10762", "pdf": "https://arxiv.org/pdf/2502.10762.pdf", "abs": "https://arxiv.org/abs/2502.10762", "title": "Bone Soups: A Seek-and-Soup Model Merging Approach for Controllable Multi-Objective Generation", "authors": ["Guofu Xie", "Xiao Zhang", "Ting Yao", "Yunsheng Shi"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "This paper is accepted by the ACL 2025 Main Conference", "summary": "User information needs are often highly diverse and varied. A key challenge\nin current research is how to achieve controllable multi-objective generation\nwhile enabling rapid adaptation to accommodate diverse user demands during test\ntime. Existing solutions, such as Rewarded Soup, focus on merging language\nmodels individually tuned on single objectives. While easy to implement and\nwidely used, these approaches face limitations in achieving optimal performance\ndue to their disregard for the impacts of competing objectives on model tuning.\nTo address this issue, we propose Bone Soup, a novel model merging approach\nthat first seeks a series of backbone models by considering the impacts of\nmultiple objectives and then makes the soup (i.e., merge the backbone models).\nSpecifically, Bone Soup begins by training multiple backbone models for\ndifferent objectives using multi-objective reinforcement learning. Each\nbackbone model is guided by a combination of backbone reward signals. To ensure\nthat these models are optimal for the Pareto front, the backbone rewards are\ncrafted by combining standard reward functions into basis vectors, which can\nthen be modified through a rule-based construction method. Bone Soup leverages\na symmetric circulant matrix mapping to generate the merging coefficients,\nwhich are used to merge the backbone models according to user preferences.\nExtensive experimental results demonstrate that Bone Soup exhibits strong\ncontrollability and Pareto optimality in controllable multi-objective\ngeneration, providing a more effective and efficient approach to addressing\ndiverse user needs at test time."}
{"id": "2502.11191", "pdf": "https://arxiv.org/pdf/2502.11191.pdf", "abs": "https://arxiv.org/abs/2502.11191", "title": "Primus: A Pioneering Collection of Open-Source Datasets for Cybersecurity LLM Training", "authors": ["Yao-Ching Yu", "Tsun-Han Chiang", "Cheng-Wei Tsai", "Chien-Ming Huang", "Wen-Kwang Tsao"], "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable advancements in\nspecialized fields such as finance, law, and medicine. However, in\ncybersecurity, we have noticed a lack of open-source datasets, with a\nparticular lack of high-quality cybersecurity pretraining corpora, even though\nmuch research indicates that LLMs acquire their knowledge during pretraining.\nTo address this, we present a comprehensive suite of datasets covering all\nmajor training stages, including pretraining, instruction fine-tuning, and\nreasoning distillation with cybersecurity-specific self-reflection data.\nExtensive ablation studies demonstrate their effectiveness on public\ncybersecurity benchmarks. In particular, continual pre-training on our dataset\nyields a 15.88% improvement in the aggregate score, while reasoning\ndistillation leads to a 10% gain in security certification (CISSP). We will\nrelease all datasets and trained cybersecurity LLMs under the ODC-BY and MIT\nlicenses to encourage further research in the community. For access to all\ndatasets and model weights, please refer to\nhttps://huggingface.co/collections/trendmicro-ailab/primus-67b1fd27052b802b4af9d243."}
{"id": "2502.11196", "pdf": "https://arxiv.org/pdf/2502.11196.pdf", "abs": "https://arxiv.org/abs/2502.11196", "title": "How Do LLMs Acquire New Knowledge? A Knowledge Circuits Perspective on Continual Pre-Training", "authors": ["Yixin Ou", "Yunzhi Yao", "Ningyu Zhang", "Hui Jin", "Jiacheng Sun", "Shumin Deng", "Zhenguo Li", "Huajun Chen"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": "ACL 2025 Findings", "summary": "Despite exceptional capabilities in knowledge-intensive tasks, Large Language\nModels (LLMs) face a critical gap in understanding how they internalize new\nknowledge, particularly how to structurally embed acquired knowledge in their\nneural computations. We address this issue through the lens of knowledge\ncircuit evolution, identifying computational subgraphs that facilitate\nknowledge storage and processing. Our systematic analysis of circuit evolution\nthroughout continual pre-training reveals several key findings: (1) the\nacquisition of new knowledge is influenced by its relevance to pre-existing\nknowledge; (2) the evolution of knowledge circuits exhibits a distinct phase\nshift from formation to optimization; (3) the evolution of knowledge circuits\nfollows a deep-to-shallow pattern. These insights not only advance our\ntheoretical understanding of the mechanisms of new knowledge acquisition in\nLLMs, but also provide potential implications for improving continual\npre-training strategies to enhance model performance. Code and data will be\navailable at https://github.com/zjunlp/DynamicKnowledgeCircuits."}
{"id": "2502.11767", "pdf": "https://arxiv.org/pdf/2502.11767.pdf", "abs": "https://arxiv.org/abs/2502.11767", "title": "From Selection to Generation: A Survey of LLM-based Active Learning", "authors": ["Yu Xia", "Subhojyoti Mukherjee", "Zhouhang Xie", "Junda Wu", "Xintong Li", "Ryan Aponte", "Hanjia Lyu", "Joe Barrow", "Hongjie Chen", "Franck Dernoncourt", "Branislav Kveton", "Tong Yu", "Ruiyi Zhang", "Jiuxiang Gu", "Nesreen K. Ahmed", "Yu Wang", "Xiang Chen", "Hanieh Deilamsalehy", "Sungchul Kim", "Zhengmian Hu", "Yue Zhao", "Nedim Lipka", "Seunghyun Yoon", "Ting-Hao Kenneth Huang", "Zichao Wang", "Puneet Mathur", "Soumyabrata Pal", "Koyel Mukherjee", "Zhehao Zhang", "Namyong Park", "Thien Huu Nguyen", "Jiebo Luo", "Ryan A. Rossi", "Julian McAuley"], "categories": ["cs.LG", "cs.CL"], "comment": "ACL 2025", "summary": "Active Learning (AL) has been a powerful paradigm for improving model\nefficiency and performance by selecting the most informative data points for\nlabeling and training. In recent active learning frameworks, Large Language\nModels (LLMs) have been employed not only for selection but also for generating\nentirely new data instances and providing more cost-effective annotations.\nMotivated by the increasing importance of high-quality data and efficient model\ntraining in the era of LLMs, we present a comprehensive survey on LLM-based\nActive Learning. We introduce an intuitive taxonomy that categorizes these\ntechniques and discuss the transformative roles LLMs can play in the active\nlearning loop. We further examine the impact of AL on LLM learning paradigms\nand its applications across various domains. Finally, we identify open\nchallenges and propose future research directions. This survey aims to serve as\nan up-to-date resource for researchers and practitioners seeking to gain an\nintuitive understanding of LLM-based AL techniques and deploy them to new\napplications."}
{"id": "2502.13943", "pdf": "https://arxiv.org/pdf/2502.13943.pdf", "abs": "https://arxiv.org/abs/2502.13943", "title": "AdaptiveStep: Automatically Dividing Reasoning Step through Model Confidence", "authors": ["Yuliang Liu", "Junjie Lu", "Zhaoling Chen", "Chaofeng Qu", "Jason Klein Liu", "Chonghan Liu", "Zefan Cai", "Yunhui Xia", "Li Zhao", "Jiang Bian", "Chuheng Zhang", "Wei Shen", "Zhouhan Lin"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "ICML 2025", "summary": "Current approaches for training Process Reward Models (PRMs) often involve\nbreaking down responses into multiple reasoning steps using rule-based\ntechniques, such as using predefined placeholder tokens or setting the\nreasoning step's length into a fixed size. These approaches overlook the fact\nthat specific words do not typically mark true decision points in a text. To\naddress this, we propose AdaptiveStep, a method that divides reasoning steps\nbased on the model's confidence in predicting the next word. This division\nmethod provides more decision-making information at each step, enhancing\ndownstream tasks, such as reward model learning. Moreover, our method does not\nrequire manual annotation. We demonstrate its effectiveness through experiments\nwith AdaptiveStep-trained PRMs in mathematical reasoning and code generation\ntasks. Experimental results indicate that the outcome PRM achieves\nstate-of-the-art Best-of-N performance, surpassing greedy search strategy with\ntoken-level value-guided decoding, while also reducing construction costs by\nover 30% compared to existing open-source PRMs. In addition, we provide a\nthorough analysis and case study on the PRM's performance, transferability, and\ngeneralization capabilities."}
{"id": "2502.15865", "pdf": "https://arxiv.org/pdf/2502.15865.pdf", "abs": "https://arxiv.org/abs/2502.15865", "title": "Standard Benchmarks Fail - Auditing LLM Agents in Finance Must Prioritize Risk", "authors": ["Zichen Chen", "Jiaao Chen", "Jianda Chen", "Misha Sra"], "categories": ["q-fin.GN", "cs.AI", "cs.CL"], "comment": "46 pages, 2 figures, 2 tables", "summary": "Standard benchmarks fixate on how well large language model (LLM) agents\nperform in finance, yet say little about whether they are safe to deploy. We\nargue that accuracy metrics and return-based scores provide an illusion of\nreliability, overlooking vulnerabilities such as hallucinated facts, stale\ndata, and adversarial prompt manipulation. We take a firm position: financial\nLLM agents should be evaluated first and foremost on their risk profile, not on\ntheir point-estimate performance. Drawing on risk-engineering principles, we\noutline a three-level agenda: model, workflow, and system, for stress-testing\nLLM agents under realistic failure modes. To illustrate why this shift is\nurgent, we audit six API-based and open-weights LLM agents on three high-impact\ntasks and uncover hidden weaknesses that conventional benchmarks miss. We\nconclude with actionable recommendations for researchers, practitioners, and\nregulators: audit risk-aware metrics in future studies, publish stress\nscenarios alongside datasets, and treat ``safety budget'' as a primary success\ncriterion. Only by redefining what ``good'' looks like can the community\nresponsibly advance AI-driven finance."}
{"id": "2502.17701", "pdf": "https://arxiv.org/pdf/2502.17701.pdf", "abs": "https://arxiv.org/abs/2502.17701", "title": "From Perceptions to Decisions: Wildfire Evacuation Decision Prediction with Behavioral Theory-informed LLMs", "authors": ["Ruxiao Chen", "Chenguang Wang", "Yuran Sun", "Xilei Zhao", "Susu Xu"], "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG"], "comment": "25 pages, 9 figures", "summary": "Evacuation decision prediction is critical for efficient and effective\nwildfire response by helping emergency management anticipate traffic congestion\nand bottlenecks, allocate resources, and minimize negative impacts. Traditional\nstatistical methods for evacuation decision prediction fail to capture the\ncomplex and diverse behavioral logic of different individuals. In this work,\nfor the first time, we introduce FLARE, short for facilitating LLM for advanced\nreasoning on wildfire evacuation decision prediction, a Large Language Model\n(LLM)-based framework that integrates behavioral theories and models to\nstreamline the Chain-of-Thought (CoT) reasoning and subsequently integrate with\nmemory-based Reinforcement Learning (RL) module to provide accurate evacuation\ndecision prediction and understanding. Our proposed method addresses the\nlimitations of using existing LLMs for evacuation behavioral predictions, such\nas limited survey data, mismatching with behavioral theory, conflicting\nindividual preferences, implicit and complex mental states, and intractable\nmental state-behavior mapping. Experiments on three post-wildfire survey\ndatasets show an average of 20.47% performance improvement over traditional\ntheory-informed behavioral models, with strong cross-event generalizability.\nOur complete code is publicly available at\nhttps://github.com/SusuXu-s-Lab/FLARE"}
{"id": "2502.18744", "pdf": "https://arxiv.org/pdf/2502.18744.pdf", "abs": "https://arxiv.org/abs/2502.18744", "title": "ZEBRA: Leveraging Model-Behavioral Knowledge for Zero-Annotation Preference Dataset Construction", "authors": ["Jeesu Jung", "Chanjun Park", "Sangkeun Jung"], "categories": ["cs.AI", "cs.CL"], "comment": "16 pages,7 figures,5 tables,4 graphs", "summary": "Recent efforts in LLM alignment have focused on constructing large-scale\npreference datasets via human or Artificial Intelligence (AI) annotators.\nHowever, such approaches rely on instance-wise supervision, incurring\nsubstantial annotation cost and limited interpretability. In this paper, we\npropose ZEBRA - a model behavior-wise zero-annotation framework that constructs\npreference data by leveraging model behavior knowledge derived from benchmark\nperformances. ZEBRA binarizes response pairs by evaluating the quality and\nsimilarity of their origin models, entirely bypassing instance-level\nannotation. This allows scalable, controllable, and cost-effective alignment\ndata generation. Empirical results show that ZEBRA achieves alignment\nperformance comparable to instance-supervised methods, despite requiring no\nmanual or model-based labeling."}
{"id": "2502.19726", "pdf": "https://arxiv.org/pdf/2502.19726.pdf", "abs": "https://arxiv.org/abs/2502.19726", "title": "Tokens for Learning, Tokens for Unlearning: Mitigating Membership Inference Attacks in Large Language Models via Dual-Purpose Training", "authors": ["Toan Tran", "Ruixuan Liu", "Li Xiong"], "categories": ["cs.LG", "cs.CL"], "comment": "ACL'25 (Findings)", "summary": "Large language models (LLMs) have become the backbone of modern natural\nlanguage processing but pose privacy concerns about leaking sensitive training\ndata. Membership inference attacks (MIAs), which aim to infer whether a sample\nis included in a model's training dataset, can serve as a foundation for\nbroader privacy threats. Existing defenses designed for traditional\nclassification models do not account for the sequential nature of text data. As\na result, they either require significant computational resources or fail to\neffectively mitigate privacy risks in LLMs. In this work, we propose\n\\methodname, a lightweight yet effective empirical privacy defense for\nprotecting training data of language models by leveraging token-specific\ncharacteristics. By analyzing token dynamics during training, we propose a\ntoken selection strategy that categorizes tokens into hard tokens for learning\nand memorized tokens for unlearning. Subsequently, our training-phase defense\noptimizes a novel dual-purpose token-level loss to achieve a Pareto-optimal\nbalance between utility and privacy. Extensive experiments demonstrate that our\napproach not only provides strong protection against MIAs but also improves\nlanguage modeling performance by around 10\\% across various LLM architectures\nand datasets compared to the baselines."}
{"id": "2502.20576", "pdf": "https://arxiv.org/pdf/2502.20576.pdf", "abs": "https://arxiv.org/abs/2502.20576", "title": "OmniRouter: Budget and Performance Controllable Multi-LLM Routing", "authors": ["Kai Mei", "Wujiang Xu", "Shuhang Lin", "Yongfeng Zhang"], "categories": ["cs.DB", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) deliver superior performance but require\nsubstantial computational resources and operate with relatively low efficiency,\nwhile smaller models can efficiently handle simpler tasks with fewer resources.\nLLM routing is a crucial paradigm that dynamically selects the most suitable\nlarge language models from a pool of candidates to process diverse inputs,\nensuring optimal resource utilization while maintaining response quality.\nExisting routing frameworks typically model this as a locally optimal\ndecision-making problem, selecting the presumed best-fit LLM for each query\nindividually, which overlook global budget constraints, resulting in\nineffective resource allocation. To tackle this problem, we introduce\nOmniRouter, a fundamentally controllable routing framework for multi-LLM\nserving. Instead of making per-query greedy choices, OmniRouter models the\nrouting task as a constrained optimization problem, assigning models that\nminimize total cost while ensuring the required performance level.\nSpecifically, a hybrid retrieval-augmented predictor is designed to predict the\ncapabilities and costs of LLMs and a constrained optimizer is employed to\ncontrol globally optimal query-model allocation. Experiments show that\nOmniRouter achieves up to 6.30% improvement in response accuracy while\nsimultaneously reducing computational costs by at least 10.15% compared to\ncompetitive router baselines. The code and the dataset are available at\nhttps://github.com/agiresearch/OmniRouter."}
{"id": "2503.00071", "pdf": "https://arxiv.org/pdf/2503.00071.pdf", "abs": "https://arxiv.org/abs/2503.00071", "title": "I see what you mean: Co-Speech Gestures for Reference Resolution in Multimodal Dialogue", "authors": ["Esam Ghaleb", "Bulat Khaertdinov", "Aslƒ± √ñzy√ºrek", "Raquel Fern√°ndez"], "categories": ["cs.CV", "cs.CL", "cs.MM"], "comment": null, "summary": "In face-to-face interaction, we use multiple modalities, including speech and\ngestures, to communicate information and resolve references to objects.\nHowever, how representational co-speech gestures refer to objects remains\nunderstudied from a computational perspective. In this work, we address this\ngap by introducing a multimodal reference resolution task centred on\nrepresentational gestures, while simultaneously tackling the challenge of\nlearning robust gesture embeddings. We propose a self-supervised pre-training\napproach to gesture representation learning that grounds body movements in\nspoken language. Our experiments show that the learned embeddings align with\nexpert annotations and have significant predictive power. Moreover, reference\nresolution accuracy further improves when (1) using multimodal gesture\nrepresentations, even when speech is unavailable at inference time, and (2)\nleveraging dialogue history. Overall, our findings highlight the complementary\nroles of gesture and speech in reference resolution, offering a step towards\nmore naturalistic models of human-machine interaction."}
{"id": "2503.00600", "pdf": "https://arxiv.org/pdf/2503.00600.pdf", "abs": "https://arxiv.org/abs/2503.00600", "title": "Semantic Integrity Constraints: Declarative Guardrails for AI-Augmented Data Processing Systems", "authors": ["Alexander W. Lee", "Justin Chan", "Michael Fu", "Nicolas Kim", "Akshay Mehta", "Deepti Raghavan", "Ugur Cetintemel"], "categories": ["cs.DB", "cs.AI", "cs.CL"], "comment": null, "summary": "AI-augmented data processing systems (DPSs) integrate large language models\n(LLMs) into query pipelines, allowing powerful semantic operations on\nstructured and unstructured data. However, the reliability (a.k.a. trust) of\nthese systems is fundamentally challenged by the potential for LLMs to produce\nerrors, limiting their adoption in critical domains. To help address this\nreliability bottleneck, we introduce semantic integrity constraints (SICs) -- a\ndeclarative abstraction for specifying and enforcing correctness conditions\nover LLM outputs in semantic queries. SICs generalize traditional database\nintegrity constraints to semantic settings, supporting common types of\nconstraints, such as grounding, soundness, and exclusion, with both proactive\nand reactive enforcement strategies.\n  We argue that SICs provide a foundation for building reliable and auditable\nAI-augmented data systems. Specifically, we present a system design for\nintegrating SICs into query planning and runtime execution and discuss its\nrealization in AI-augmented DPSs. To guide and evaluate the vision, we outline\nseveral design goals -- covering criteria around expressiveness, runtime\nsemantics, integration, performance, and enterprise-scale applicability -- and\ndiscuss how our framework addresses each, along with open research challenges."}
{"id": "2503.01461", "pdf": "https://arxiv.org/pdf/2503.01461.pdf", "abs": "https://arxiv.org/abs/2503.01461", "title": "Marco-o1 v2: Towards Widening The Distillation Bottleneck for Reasoning Models", "authors": ["Huifeng Yin", "Yu Zhao", "Minghao Wu", "Xuanfan Ni", "Bo Zeng", "Hao Wang", "Tianqi Shi", "Liangying Shao", "Chenyang Lyu", "Longyue Wang", "Weihua Luo", "Kaifu Zhang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Reasoning Models(LRMs) such as OpenAI o1 and DeepSeek-R1 have shown\nremarkable reasoning capabilities by scaling test-time compute and generating\nlong Chain-of-Thought(CoT). Distillation--post-training on LRMs-generated\ndata--is a straightforward yet effective method to enhance the reasoning\nabilities of smaller models, but faces a critical bottleneck: we found that\ndistilled long CoT data poses learning difficulty for small models and leads to\nthe inheritance of biases (i.e. over-thinking) when using Supervised\nFine-tuning (SFT) and Reinforcement Learning (RL) methods. To alleviate this\nbottleneck, we propose constructing tree-based CoT data from scratch via Monte\nCarlo Tree Search(MCTS). We then exploit a set of CoT-aware approaches,\nincluding Thoughts Length Balance, Fine-grained DPO, and Joint Post-training\nObjective, to enhance SFT and RL on the constructed data. We conduct evaluation\non various benchmarks such as math (GSM8K, MATH, AIME). instruction-following\n(Multi-IF) and planning (Blocksworld), results demonstrate our approaches\nsubstantially improve the reasoning performance of distilled models compared to\nstandard distilled models via reducing the hallucinations in long-time\nthinking. The project homepage is https://github.com/AIDC-AI/Marco-o1."}
{"id": "2503.01891", "pdf": "https://arxiv.org/pdf/2503.01891.pdf", "abs": "https://arxiv.org/abs/2503.01891", "title": "MMSciBench: Benchmarking Language Models on Chinese Multimodal Scientific Problems", "authors": ["Xinwu Ye", "Chengfan Li", "Siming Chen", "Wei Wei", "Xiangru Tang"], "categories": ["cs.LG", "cs.CL", "I.2.7"], "comment": "Accepted to the Findings of the Association for Computational\n  Linguistics (ACL 2025)", "summary": "Recent advances in large language models (LLMs) and vision-language models\n(LVLMs) have shown promise across many tasks, yet their scientific reasoning\ncapabilities remain untested, particularly in multimodal settings. We present\nMMSciBench, a benchmark for evaluating mathematical and physical reasoning\nthrough text-only and text-image formats, with human-annotated difficulty\nlevels, solutions with detailed explanations, and taxonomic mappings.\nEvaluation of state-of-the-art models reveals significant limitations, with\neven the best model achieving only \\textbf{63.77\\%} accuracy and particularly\nstruggling with visual reasoning tasks. Our analysis exposes critical gaps in\ncomplex reasoning and visual-textual integration, establishing MMSciBench as a\nrigorous standard for measuring progress in multimodal scientific\nunderstanding. The code for MMSciBench is open-sourced at GitHub, and the\ndataset is available at Hugging Face."}
{"id": "2503.04992", "pdf": "https://arxiv.org/pdf/2503.04992.pdf", "abs": "https://arxiv.org/abs/2503.04992", "title": "Wanda++: Pruning Large Language Models via Regional Gradients", "authors": ["Yifan Yang", "Kai Zhen", "Bhavana Ganesh", "Aram Galstyan", "Goeric Huybrechts", "Markus M√ºller", "Jonas M. K√ºbler", "Rupak Vignesh Swaminathan", "Athanasios Mouchtaris", "Sravan Babu Bodapati", "Nathan Susanj", "Zheng Zhang", "Jack FitzGerald", "Abhishek Kumar"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Paper accepted at ACL 2025 Findings", "summary": "Large Language Models (LLMs) pruning seeks to remove unimportant weights for\ninference speedup with minimal accuracy impact. However, existing methods often\nsuffer from accuracy degradation without full-model sparsity-aware fine-tuning.\nThis paper presents Wanda++, a novel pruning framework that outperforms the\nstate-of-the-art methods by utilizing decoder-block-level \\textbf{regional}\ngradients. Specifically, Wanda++ improves the pruning score with regional\ngradients for the first time and proposes an efficient regional optimization\nmethod to minimize pruning-induced output discrepancies between the dense and\nsparse decoder output. Notably, Wanda++ improves perplexity by up to 32\\% over\nWanda in the language modeling task and generalizes effectively to downstream\ntasks. Moreover, despite updating weights with regional optimization, Wanda++\nremains orthogonal to sparsity-aware fine-tuning, further reducing perplexity\nwith LoRA in great extend. Our approach is lightweight, pruning a 7B LLaMA\nmodel in under 10 minutes on a single H100 GPU."}
{"id": "2503.07010", "pdf": "https://arxiv.org/pdf/2503.07010.pdf", "abs": "https://arxiv.org/abs/2503.07010", "title": "ProjectEval: A Benchmark for Programming Agents Automated Evaluation on Project-Level Code Generation", "authors": ["Kaiyuan Liu", "Youcheng Pan", "Yang Xiang", "Daojing He", "Jing Li", "Yexing Du", "Tianrun Gao"], "categories": ["cs.SE", "cs.CL"], "comment": "17 pages (9 Appendix pages), 4 figures, 7 tables", "summary": "Recently, LLM agents have made rapid progress in improving their programming\ncapabilities. However, existing benchmarks lack the ability to automatically\nevaluate from users' perspective, and also lack the explainability of the\nresults of LLM agents' code generation capabilities. Thus, we introduce\nProjectEval, a new benchmark for LLM agents project-level code generation's\nautomated evaluation by simulating user interaction. ProjectEval is constructed\nby LLM with human reviewing. It has three different level inputs of natural\nlanguages or code skeletons. ProjectEval can evaluate the generated projects by\nuser interaction simulation for execution, and by code similarity through\nexisting objective indicators. Through ProjectEval, we find that systematic\nengineering project code, overall understanding of the project and\ncomprehensive analysis capability are the keys for LLM agents to achieve\npractical projects. Our findings and benchmark provide valuable insights for\ndeveloping more effective programming agents that can be deployed in future\nreal-world production."}
{"id": "2503.09532", "pdf": "https://arxiv.org/pdf/2503.09532.pdf", "abs": "https://arxiv.org/abs/2503.09532", "title": "SAEBench: A Comprehensive Benchmark for Sparse Autoencoders in Language Model Interpretability", "authors": ["Adam Karvonen", "Can Rager", "Johnny Lin", "Curt Tigges", "Joseph Bloom", "David Chanin", "Yeu-Tong Lau", "Eoin Farrell", "Callum McDougall", "Kola Ayonrinde", "Demian Till", "Matthew Wearden", "Arthur Conmy", "Samuel Marks", "Neel Nanda"], "categories": ["cs.LG", "cs.CL"], "comment": "Accepted to ICML 2025 main conference", "summary": "Sparse autoencoders (SAEs) are a popular technique for interpreting language\nmodel activations, and there is extensive recent work on improving SAE\neffectiveness. However, most prior work evaluates progress using unsupervised\nproxy metrics with unclear practical relevance. We introduce SAEBench, a\ncomprehensive evaluation suite that measures SAE performance across eight\ndiverse metrics, spanning interpretability, feature disentanglement and\npractical applications like unlearning. To enable systematic comparison, we\nopen-source a suite of over 200 SAEs across eight recently proposed SAE\narchitectures and training algorithms. Our evaluation reveals that gains on\nproxy metrics do not reliably translate to better practical performance. For\ninstance, while Matryoshka SAEs slightly underperform on existing proxy\nmetrics, they substantially outperform other architectures on feature\ndisentanglement metrics; moreover, this advantage grows with SAE scale. By\nproviding a standardized framework for measuring progress in SAE development,\nSAEBench enables researchers to study scaling trends and make nuanced\ncomparisons between different SAE architectures and training methodologies. Our\ninteractive interface enables researchers to flexibly visualize relationships\nbetween metrics across hundreds of open-source SAEs at:\nwww.neuronpedia.org/sae-bench"}
{"id": "2503.13509", "pdf": "https://arxiv.org/pdf/2503.13509.pdf", "abs": "https://arxiv.org/abs/2503.13509", "title": "MentalChat16K: A Benchmark Dataset for Conversational Mental Health Assistance", "authors": ["Jia Xu", "Tianyi Wei", "Bojian Hou", "Patryk Orzechowski", "Shu Yang", "Ruochen Jin", "Rachael Paulbeck", "Joost Wagenaar", "George Demiris", "Li Shen"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY", "cs.HC"], "comment": null, "summary": "We introduce MentalChat16K, an English benchmark dataset combining a\nsynthetic mental health counseling dataset and a dataset of anonymized\ntranscripts from interventions between Behavioral Health Coaches and Caregivers\nof patients in palliative or hospice care. Covering a diverse range of\nconditions like depression, anxiety, and grief, this curated dataset is\ndesigned to facilitate the development and evaluation of large language models\nfor conversational mental health assistance. By providing a high-quality\nresource tailored to this critical domain, MentalChat16K aims to advance\nresearch on empathetic, personalized AI solutions to improve access to mental\nhealth support services. The dataset prioritizes patient privacy, ethical\nconsiderations, and responsible data usage. MentalChat16K presents a valuable\nopportunity for the research community to innovate AI technologies that can\npositively impact mental well-being. The dataset is available at\nhttps://huggingface.co/datasets/ShenLab/MentalChat16K and the code and\ndocumentation are hosted on GitHub at\nhttps://github.com/ChiaPatricia/MentalChat16K."}
{"id": "2503.16072", "pdf": "https://arxiv.org/pdf/2503.16072.pdf", "abs": "https://arxiv.org/abs/2503.16072", "title": "Redefining Toxicity: An Objective and Context-Aware Approach for Stress-Level-Based Detection", "authors": ["Sergey Berezin", "Reza Farahbakhsh", "Noel Crespi"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Most toxicity detection models treat toxicity as an intrinsic property of\ntext, overlooking the role of context in shaping its impact. Drawing on\ninterdisciplinary research, we reconceptualise toxicity as a socially emergent\nstress signal. We introduce a new framework for toxicity detection, including a\nformal definition and metric, and validate our approach on a novel dataset,\ndemonstrating improved contextual sensitivity and adaptability."}
{"id": "2503.16167", "pdf": "https://arxiv.org/pdf/2503.16167.pdf", "abs": "https://arxiv.org/abs/2503.16167", "title": "CodeReviewQA: The Code Review Comprehension Assessment for Large Language Models", "authors": ["Hong Yi Lin", "Chunhua Liu", "Haoyu Gao", "Patanamon Thongtanunam", "Christoph Treude"], "categories": ["cs.SE", "cs.CL"], "comment": "The paper is published in Findings of the Association for\n  Computational Linguistics (ACL 2025)", "summary": "State-of-the-art large language models (LLMs) have demonstrated impressive\ncode generation capabilities but struggle with real-world software engineering\ntasks, such as revising source code to address code reviews, hindering their\npractical use. Code review comments are often implicit, ambiguous, and\ncolloquial, requiring models to grasp both code and human intent. This\nchallenge calls for evaluating large language models' ability to bridge both\ntechnical and conversational contexts. While existing work has employed the\nautomated code refinement (ACR) task to resolve these comments, current\nevaluation methods fall short, relying on text matching metrics that provide\nlimited insight into model failures and remain susceptible to training data\ncontamination. To address these limitations, we introduce a novel evaluation\nbenchmark, $\\textbf{CodeReviewQA}$ that enables us to conduct fine-grained\nassessment of model capabilities and mitigate data contamination risks. In\nCodeReviewQA, we decompose the generation task of code refinement into\n$\\textbf{three essential reasoning steps}$: $\\textit{change type recognition}$\n(CTR), $\\textit{change localisation}$ (CL), and $\\textit{solution\nidentification}$ (SI). Each step is reformulated as multiple-choice questions\nwith varied difficulty levels, enabling precise assessment of model\ncapabilities, while mitigating data contamination risks. Our comprehensive\nevaluation spans 72 recently released large language models on $\\textbf{900\nmanually curated, high-quality examples}$ across nine programming languages.\nOur results show that CodeReviewQA is able to expose specific model weaknesses\nin code review comprehension, disentangled from their generative automated code\nrefinement results."}
{"id": "2503.18792", "pdf": "https://arxiv.org/pdf/2503.18792.pdf", "abs": "https://arxiv.org/abs/2503.18792", "title": "REALM: A Dataset of Real-World LLM Use Cases", "authors": ["Jingwen Cheng", "Kshitish Ghate", "Wenyue Hua", "William Yang Wang", "Hong Shen", "Fei Fang"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": "11 pages, 3 figures", "summary": "Large Language Models (LLMs), such as the GPT series, have driven significant\nindustrial applications, leading to economic and societal transformations.\nHowever, a comprehensive understanding of their real-world applications remains\nlimited. To address this, we introduce REALM, a dataset of over 94,000 LLM use\ncases collected from Reddit and news articles. REALM captures two key\ndimensions: the diverse applications of LLMs and the demographics of their\nusers. It categorizes LLM applications and explores how users' occupations\nrelate to the types of applications they use. By integrating real-world data,\nREALM offers insights into LLM adoption across different domains, providing a\nfoundation for future research on their evolving societal roles."}
{"id": "2503.23487", "pdf": "https://arxiv.org/pdf/2503.23487.pdf", "abs": "https://arxiv.org/abs/2503.23487", "title": "Large Language and Reasoning Models are Shallow Disjunctive Reasoners", "authors": ["Irtaza Khalid", "Amir Masoud Nourollah", "Steven Schockaert"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "ACL 2025 main conference", "summary": "Large Language Models (LLMs) have been found to struggle with systematic\nreasoning. Even on tasks where they appear to perform well, their performance\noften depends on shortcuts, rather than on genuine reasoning abilities, leading\nthem to collapse on out-of-distribution (OOD) examples. Post-training\nstrategies based on reinforcement learning and chain-of-thought prompting have\nrecently been hailed as a step change. However, little is known about the\npotential of the resulting ``Large Reasoning Models'' (LRMs) beyond maths and\nprogramming-based problem solving, where genuine OOD problems can be sparse. In\nthis paper, we focus on tasks that require systematic relational composition\nfor qualitative spatial and temporal reasoning. The setting allows fine control\nover problem difficulty to precisely measure OOD generalization. We find that,\nzero-shot LRMs generally outperform their LLM counterparts in single-path\nreasoning tasks but struggle in the multi-path setting. Whilst showing\ncomparatively better results, fine-tuned LLMs are also not capable of\nmulti-path generalization. We also provide evidence for the behavioral\ninterpretation for this, i.e., that LRMs are shallow disjunctive reasoners."}
{"id": "2504.07089", "pdf": "https://arxiv.org/pdf/2504.07089.pdf", "abs": "https://arxiv.org/abs/2504.07089", "title": "OmniCaptioner: One Captioner to Rule Them All", "authors": ["Yiting Lu", "Jiakang Yuan", "Zhen Li", "Shitian Zhao", "Qi Qin", "Xinyue Li", "Le Zhuo", "Licheng Wen", "Dongyang Liu", "Yuewen Cao", "Xiangchao Yan", "Xin Li", "Tianshuo Peng", "Shufei Zhang", "Botian Shi", "Tao Chen", "Zhibo Chen", "Lei Bai", "Peng Gao", "Bo Zhang"], "categories": ["cs.CV", "cs.CL"], "comment": "More visualizations on Homepage:\n  https://alpha-innovator.github.io/OmniCaptioner-project-page and Official\n  code: https://github.com/Alpha-Innovator/OmniCaptioner", "summary": "We propose OmniCaptioner, a versatile visual captioning framework for\ngenerating fine-grained textual descriptions across a wide variety of visual\ndomains. Unlike prior methods limited to specific image types (e.g., natural\nimages or geometric visuals), our framework provides a unified solution for\ncaptioning natural images, visual text (e.g., posters, UIs, textbooks), and\nstructured visuals (e.g., documents, tables, charts). By converting low-level\npixel information into semantically rich textual representations, our framework\nbridges the gap between visual and textual modalities. Our results highlight\nthree key advantages: (i) Enhanced Visual Reasoning with LLMs, where\nlong-context captions of visual modalities empower LLMs, particularly the\nDeepSeek-R1 series, to reason effectively in multimodal scenarios; (ii)\nImproved Image Generation, where detailed captions improve tasks like\ntext-to-image generation and image transformation; and (iii) Efficient\nSupervised Fine-Tuning (SFT), which enables faster convergence with less data.\nWe believe the versatility and adaptability of OmniCaptioner can offer a new\nperspective for bridging the gap between language and visual modalities."}
{"id": "2504.11515", "pdf": "https://arxiv.org/pdf/2504.11515.pdf", "abs": "https://arxiv.org/abs/2504.11515", "title": "Graph-Driven Multimodal Feature Learning Framework for Apparent Personality Assessment", "authors": ["Kangsheng Wang", "Chengwei Ye", "Huanzhen Zhang", "Linuo Xu", "Shuyan Liu"], "categories": ["cs.CV", "cs.CL", "cs.MM"], "comment": "The article contains serious scientific errors and cannot be\n  corrected by updating the preprint", "summary": "Predicting personality traits automatically has become a challenging problem\nin computer vision. This paper introduces an innovative multimodal feature\nlearning framework for personality analysis in short video clips. For visual\nprocessing, we construct a facial graph and design a Geo-based two-stream\nnetwork incorporating an attention mechanism, leveraging both Graph\nConvolutional Networks (GCN) and Convolutional Neural Networks (CNN) to capture\nstatic facial expressions. Additionally, ResNet18 and VGGFace networks are\nemployed to extract global scene and facial appearance features at the frame\nlevel. To capture dynamic temporal information, we integrate a BiGRU with a\ntemporal attention module for extracting salient frame representations. To\nenhance the model's robustness, we incorporate the VGGish CNN for audio-based\nfeatures and XLM-Roberta for text-based features. Finally, a multimodal channel\nattention mechanism is introduced to integrate different modalities, and a\nMulti-Layer Perceptron (MLP) regression model is used to predict personality\ntraits. Experimental results confirm that our proposed framework surpasses\nexisting state-of-the-art approaches in performance."}
{"id": "2504.13861", "pdf": "https://arxiv.org/pdf/2504.13861.pdf", "abs": "https://arxiv.org/abs/2504.13861", "title": "3MDBench: Medical Multimodal Multi-agent Dialogue Benchmark", "authors": ["Ivan Sviridov", "Amina Miftakhova", "Artemiy Tereshchenko", "Galina Zubkova", "Pavel Blinov", "Andrey Savchenko"], "categories": ["cs.HC", "cs.CL", "cs.MA", "68T42", "I.2.1"], "comment": "35 pages, 13 figures, 7 tables", "summary": "Though Large Vision-Language Models (LVLMs) are being actively explored in\nmedicine, their ability to conduct telemedicine consultations combining\naccurate diagnosis with professional dialogue remains underexplored. In this\npaper, we present 3MDBench (Medical Multimodal Multi-agent Dialogue Benchmark),\nan open-source framework for simulating and evaluating LVLM-driven telemedical\nconsultations. 3MDBench simulates patient variability through four\ntemperament-based Patient Agents and an Assessor Agent that jointly evaluate\ndiagnostic accuracy and dialogue quality. It includes 3013 cases across 34\ndiagnoses drawn from real-world telemedicine interactions, combining textual\nand image-based data. The experimental study compares diagnostic strategies for\npopular LVLMs, including GPT-4o-mini, LLaVA-3.2-11B-Vision-Instruct, and\nQwen2-VL-7B-Instruct. We demonstrate that multimodal dialogue with internal\nreasoning improves F1 score by 6.5% over non-dialogue settings, highlighting\nthe importance of context-aware, information-seeking questioning. Moreover,\ninjecting predictions from a diagnostic convolutional network into the LVLM's\ncontext boosts F1 by up to 20%. Source code is available at\nhttps://anonymous.4open.science/r/3mdbench_acl-0511."}
{"id": "2504.14822", "pdf": "https://arxiv.org/pdf/2504.14822.pdf", "abs": "https://arxiv.org/abs/2504.14822", "title": "Completing A Systematic Review in Hours instead of Months with Interactive AI Agents", "authors": ["Rui Qiu", "Shijie Chen", "Yu Su", "Po-Yin Yen", "Han-Wei Shen"], "categories": ["cs.HC", "cs.CL"], "comment": "Accepted as ACL 2025 (main)", "summary": "Systematic reviews (SRs) are vital for evidence-based practice in high stakes\ndisciplines, such as healthcare, but are often impeded by intensive labors and\nlengthy processes that can take months to complete. Due to the high demand for\ndomain expertise, existing automatic summarization methods fail to accurately\nidentify relevant studies and generate high-quality summaries. To that end, we\nintroduce InsightAgent, a human-centered interactive AI agent powered by large\nlanguage models that revolutionize this workflow. InsightAgent partitions a\nlarge literature corpus based on semantics and employs a multi-agent design for\nmore focused processing of literature, leading to significant improvement in\nthe quality of generated SRs. InsightAgent also provides intuitive\nvisualizations of the corpus and agent trajectories, allowing users to\neffortlessly monitor the actions of the agent and provide real-time feedback\nbased on their expertise. Our user studies with 9 medical professionals\ndemonstrate that the visualization and interaction mechanisms can effectively\nimprove the quality of synthesized SRs by 27.2%, reaching 79.7% of\nhuman-written quality. At the same time, user satisfaction is improved by\n34.4%. With InsightAgent, it only takes a clinician about 1.5 hours, rather\nthan months, to complete a high-quality systematic review."}
{"id": "2504.14870", "pdf": "https://arxiv.org/pdf/2504.14870.pdf", "abs": "https://arxiv.org/abs/2504.14870", "title": "Acting Less is Reasoning More! Teaching Model to Act Efficiently", "authors": ["Hongru Wang", "Cheng Qian", "Wanjun Zhong", "Xiusi Chen", "Jiahao Qiu", "Shijue Huang", "Bowen Jin", "Mengdi Wang", "Kam-Fai Wong", "Heng Ji"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Tool-integrated reasoning (TIR) augments large language models (LLMs) with\nthe ability to invoke external tools during long-form reasoning, such as search\nengines and code interpreters, to solve tasks beyond the capabilities of\ninternal reasoning. While reinforcement learning (RL) has shown promise in\ntraining such agents, most of existing approaches typically optimize only for\nfinal correctness without considering the efficiency or necessity of external\ntool use. This often leads to excessive tool calling, incurring high\ncomputational costs and hindering the development of internal reasoning\ncapabilities - a phenomenon known as \\textit{cognitive offloading}. To this\nend, we propose Optimal Tool Call-controlled Policy Optimization (OTC-PO), a\nsimple yet effective RL-based framework that encourages models to produce\naccurate answers with minimal tool calls. Our method introduces a\ntool-integrated reward that jointly considers answer correctness and\ncorresponding tool use behavior of model to reach that answer. To validate the\neffectiveness, we introduce the metric of \\textit{tool productivity}, defined\nas the ratio between the number of correct answers and the total number of tool\ncalls across all test cases. This metric reflects how efficiently tool usage\ncontributes to successful task completion, with higher values indicating\nsmarter and more autonomous reasoning. We instantiate this framework within\nboth Proximal Policy Optimization (PPO) and Group Relative Preference\nOptimization (GRPO), resulting in OTC-PPO and OTC-GRPO. Experiments with\nQwen-2.5 and Qwen-Math across multiple QA benchmarks show that our approach\nreduces tool calls by up to 68.3\\% and improves tool productivity by up to\n215.4\\%, while maintaining comparable answer accuracy."}
{"id": "2504.15448", "pdf": "https://arxiv.org/pdf/2504.15448.pdf", "abs": "https://arxiv.org/abs/2504.15448", "title": "Visualizing Public Opinion on X: A Real-Time Sentiment Dashboard Using VADER and DistilBERT", "authors": ["Yanampally Abhiram Reddy", "Siddhi Agarwal", "Vikram Parashar", "Arshiya Arora"], "categories": ["econ.GN", "cs.CL", "q-fin.EC"], "comment": "19 pages, 2 figures", "summary": "In the age of social media, understanding public sentiment toward major\ncorporations is crucial for investors, policymakers, and researchers. This\npaper presents a comprehensive sentiment analysis system tailored for corporate\nreputation monitoring, combining Natural Language Processing (NLP) and machine\nlearning techniques to accurately interpret public opinion in real time. The\nmethodology integrates a hybrid sentiment detection framework leveraging both\nrule-based models (VADER) and transformer-based deep learning models\n(DistilBERT), applied to social media data from multiple platforms. The system\nbegins with robust preprocessing involving noise removal and text\nnormalization, followed by sentiment classification using an ensemble approach\nto ensure both interpretability and contextual accuracy. Results are visualized\nthrough sentiment distribution plots, comparative analyses, and temporal\nsentiment trends for enhanced interpretability. Our analysis reveals\nsignificant disparities in public sentiment across major corporations, with\ncompanies like Amazon (81.2) and Samsung (45.8) receiving excellent sentiment\nscores, while Microsoft (21.7) and Walmart (21.9) exhibit poor sentiment\nprofiles. These findings demonstrate the utility of our multi-source sentiment\nframework in providing actionable insights regarding corporate public\nperception, enabling stakeholders to make informed strategic decisions based on\ncomprehensive sentiment analysis."}
{"id": "2504.17004", "pdf": "https://arxiv.org/pdf/2504.17004.pdf", "abs": "https://arxiv.org/abs/2504.17004", "title": "(Im)possibility of Automated Hallucination Detection in Large Language Models", "authors": ["Amin Karbasi", "Omar Montasser", "John Sous", "Grigoris Velegkas"], "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Is automated hallucination detection possible? In this work, we introduce a\ntheoretical framework to analyze the feasibility of automatically detecting\nhallucinations produced by large language models (LLMs). Inspired by the\nclassical Gold-Angluin framework for language identification and its recent\nadaptation to language generation by Kleinberg and Mullainathan, we investigate\nwhether an algorithm, trained on examples drawn from an unknown target language\n$K$ (selected from a countable collection) and given access to an LLM, can\nreliably determine whether the LLM's outputs are correct or constitute\nhallucinations.\n  First, we establish an equivalence between hallucination detection and the\nclassical task of language identification. We prove that any hallucination\ndetection method can be converted into a language identification method, and\nconversely, algorithms solving language identification can be adapted for\nhallucination detection. Given the inherent difficulty of language\nidentification, this implies that hallucination detection is fundamentally\nimpossible for most language collections if the detector is trained using only\ncorrect examples from the target language.\n  Second, we show that the use of expert-labeled feedback, i.e., training the\ndetector with both positive examples (correct statements) and negative examples\n(explicitly labeled incorrect statements), dramatically changes this\nconclusion. Under this enriched training regime, automated hallucination\ndetection becomes possible for all countable language collections.\n  These results highlight the essential role of expert-labeled examples in\ntraining hallucination detectors and provide theoretical support for\nfeedback-based methods, such as reinforcement learning with human feedback\n(RLHF), which have proven critical for reliable LLM deployment."}
{"id": "2504.19583", "pdf": "https://arxiv.org/pdf/2504.19583.pdf", "abs": "https://arxiv.org/abs/2504.19583", "title": "Graph-Based Spectral Decomposition for Parameter Coordination in Language Model Fine-Tuning", "authors": ["Hanlu Zhang", "Yumeng Ma", "Shuo Wang", "Guiran Liu", "Binrong Zhu"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "This paper proposes a parameter collaborative optimization algorithm for\nlarge language models, enhanced with graph spectral analysis. The goal is to\nimprove both fine-tuning efficiency and structural awareness during training.\nIn the proposed method, the parameters of a pre-trained language model are\ntreated as nodes in a graph. A weighted graph is constructed, and Laplacian\nspectral decomposition is applied to enable frequency-domain modeling and\nstructural representation of the parameter space. Based on this structure, a\njoint loss function is designed. It combines the task loss with a spectral\nregularization term to facilitate collaborative updates among parameters. In\naddition, a spectral filtering mechanism is introduced during the optimization\nphase. This mechanism adjusts gradients in a structure-aware manner, enhancing\nthe model's training stability and convergence behavior. The method is\nevaluated on multiple tasks, including traditional fine-tuning comparisons,\nfew-shot generalization tests, and convergence speed analysis. In all settings,\nthe proposed approach demonstrates superior performance. The experimental\nresults confirm that the spectral collaborative optimization framework\neffectively reduces parameter perturbations and improves fine-tuning quality\nwhile preserving overall model performance. This work contributes significantly\nto the field of artificial intelligence by advancing parameter-efficient\ntraining methodologies for large-scale models, reinforcing the importance of\nstructural signal processing in deep learning optimization, and offering a\nrobust, generalizable framework for enhancing language model adaptability and\nperformance."}
{"id": "2505.00212", "pdf": "https://arxiv.org/pdf/2505.00212.pdf", "abs": "https://arxiv.org/abs/2505.00212", "title": "Which Agent Causes Task Failures and When? On Automated Failure Attribution of LLM Multi-Agent Systems", "authors": ["Shaokun Zhang", "Ming Yin", "Jieyu Zhang", "Jiale Liu", "Zhiguang Han", "Jingyang Zhang", "Beibin Li", "Chi Wang", "Huazheng Wang", "Yiran Chen", "Qingyun Wu"], "categories": ["cs.MA", "cs.CL"], "comment": "camera-ready", "summary": "Failure attribution in LLM multi-agent systems-identifying the agent and step\nresponsible for task failures-provides crucial clues for systems debugging but\nremains underexplored and labor-intensive. In this paper, we propose and\nformulate a new research area: automated failure attribution for LLM\nmulti-agent systems. To support this initiative, we introduce the Who&When\ndataset, comprising extensive failure logs from 127 LLM multi-agent systems\nwith fine-grained annotations linking failures to specific agents and decisive\nerror steps. Using the Who&When, we develop and evaluate three automated\nfailure attribution methods, summarizing their corresponding pros and cons. The\nbest method achieves 53.5% accuracy in identifying failure-responsible agents\nbut only 14.2% in pinpointing failure steps, with some methods performing below\nrandom. Even SOTA reasoning models, such as OpenAI o1 and DeepSeek R1, fail to\nachieve practical usability. These results highlight the task's complexity and\nthe need for further research in this area. Code and dataset are available at\nhttps://github.com/mingyin1/Agents_Failure_Attribution"}
{"id": "2505.07155", "pdf": "https://arxiv.org/pdf/2505.07155.pdf", "abs": "https://arxiv.org/abs/2505.07155", "title": "Reassessing Large Language Model Boolean Query Generation for Systematic Reviews", "authors": ["Shuai Wang", "Harrisen Scells", "Bevan Koopman", "Guido Zuccon"], "categories": ["cs.IR", "cs.CL"], "comment": "Accepted in SIGIR-2025", "summary": "Systematic reviews are comprehensive literature reviews that address highly\nfocused research questions and represent the highest form of evidence in\nmedicine. A critical step in this process is the development of complex Boolean\nqueries to retrieve relevant literature. Given the difficulty of manually\nconstructing these queries, recent efforts have explored Large Language Models\n(LLMs) to assist in their formulation. One of the first studies,Wang et al.,\ninvestigated ChatGPT for this task, followed by Staudinger et al., which\nevaluated multiple LLMs in a reproducibility study. However, the latter\noverlooked several key aspects of the original work, including (i) validation\nof generated queries, (ii) output formatting constraints, and (iii) selection\nof examples for chain-of-thought (Guided) prompting. As a result, its findings\ndiverged significantly from the original study. In this work, we systematically\nreproduce both studies while addressing these overlooked factors. Our results\nshow that query effectiveness varies significantly across models and prompt\ndesigns, with guided query formulation benefiting from well-chosen seed\nstudies. Overall, prompt design and model selection are key drivers of\nsuccessful query formulation. Our findings provide a clearer understanding of\nLLMs' potential in Boolean query generation and highlight the importance of\nmodel- and prompt-specific optimisations. The complex nature of systematic\nreviews adds to challenges in both developing and reproducing methods but also\nhighlights the importance of reproducibility studies in this domain."}
{"id": "2505.12185", "pdf": "https://arxiv.org/pdf/2505.12185.pdf", "abs": "https://arxiv.org/abs/2505.12185", "title": "EVALOOP: Assessing LLM Robustness in Programming from a Self-consistency Perspective", "authors": ["Sen Fang", "Weiyuan Ding", "Bowen Xu"], "categories": ["cs.SE", "cs.CL", "cs.LG"], "comment": "19 pages, 11 figures", "summary": "Assessing the programming capabilities of Large Language Models (LLMs) is\ncrucial for their effective use in software engineering. Current evaluations,\nhowever, predominantly measure the accuracy of generated code on static\nbenchmarks, neglecting the critical aspect of model robustness during\nprogramming tasks. While adversarial attacks offer insights on model\nrobustness, their effectiveness is limited and evaluation could be constrained.\nCurrent adversarial attack methods for robustness evaluation yield inconsistent\nresults, struggling to provide a unified evaluation across different LLMs. We\nintroduce EVALOOP, a novel assessment framework that evaluate the robustness\nfrom a self-consistency perspective, i.e., leveraging the natural duality\ninherent in popular software engineering tasks, e.g., code generation and code\nsummarization. EVALOOP initiates a self-contained feedback loop: an LLM\ngenerates output (e.g., code) from an input (e.g., natural language\nspecification), and then use the generated output as the input to produce a new\noutput (e.g., summarizes that code into a new specification). EVALOOP repeats\nthe process to assess the effectiveness of EVALOOP in each loop. This cyclical\nstrategy intrinsically evaluates robustness without rely on any external attack\nsetups, providing a unified metric to evaluate LLMs' robustness in programming.\nWe evaluate 16 prominent LLMs (e.g., GPT-4.1, O4-mini) on EVALOOP and found\nthat EVALOOP typically induces a 5.01%-19.31% absolute drop in pass@1\nperformance within ten loops. Intriguingly, robustness does not always align\nwith initial performance (i.e., one-time query); for instance, GPT-3.5-Turbo,\ndespite superior initial code generation compared to DeepSeek-V2, demonstrated\nlower robustness over repeated evaluation loop."}
{"id": "2505.13847", "pdf": "https://arxiv.org/pdf/2505.13847.pdf", "abs": "https://arxiv.org/abs/2505.13847", "title": "Forensic deepfake audio detection using segmental speech features", "authors": ["Tianle Yang", "Chengzhe Sun", "Siwei Lyu", "Phil Rose"], "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": null, "summary": "This study explores the potential of using acoustic features of segmental\nspeech sounds to detect deepfake audio. These features are highly interpretable\nbecause of their close relationship with human articulatory processes and are\nexpected to be more difficult for deepfake models to replicate. The results\ndemonstrate that certain segmental features commonly used in forensic voice\ncomparison (FVC) are effective in identifying deep-fakes, whereas some global\nfeatures provide little value. These findings underscore the need to approach\naudio deepfake detection using methods that are distinct from those employed in\ntraditional FVC, and offer a new perspective on leveraging segmental features\nfor this purpose."}
{"id": "2505.14318", "pdf": "https://arxiv.org/pdf/2505.14318.pdf", "abs": "https://arxiv.org/abs/2505.14318", "title": "RADAR: Enhancing Radiology Report Generation with Supplementary Knowledge Injection", "authors": ["Wenjun Hou", "Yi Cheng", "Kaishuai Xu", "Heng Li", "Yan Hu", "Wenjie Li", "Jiang Liu"], "categories": ["cs.CV", "cs.CL"], "comment": "Accepted to ACL 2025 main", "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious domains, including radiology report generation. Previous approaches\nhave attempted to utilize multimodal LLMs for this task, enhancing their\nperformance through the integration of domain-specific knowledge retrieval.\nHowever, these approaches often overlook the knowledge already embedded within\nthe LLMs, leading to redundant information integration. To address this\nlimitation, we propose Radar, a framework for enhancing radiology report\ngeneration with supplementary knowledge injection. Radar improves report\ngeneration by systematically leveraging both the internal knowledge of an LLM\nand externally retrieved information. Specifically, it first extracts the\nmodel's acquired knowledge that aligns with expert image-based classification\noutputs. It then retrieves relevant supplementary knowledge to further enrich\nthis information. Finally, by aggregating both sources, Radar generates more\naccurate and informative radiology reports. Extensive experiments on MIMIC-CXR,\nCheXpert-Plus, and IU X-ray demonstrate that our model outperforms\nstate-of-the-art LLMs in both language quality and clinical accuracy."}
{"id": "2505.14667", "pdf": "https://arxiv.org/pdf/2505.14667.pdf", "abs": "https://arxiv.org/abs/2505.14667", "title": "SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early Alignment", "authors": ["Wonje Jeung", "Sangyeon Yoon", "Minsuk Kahng", "Albert No"], "categories": ["cs.AI", "cs.CL"], "comment": "Code and models are available at https://ai-isl.github.io/safepath", "summary": "Large Reasoning Models (LRMs) have become powerful tools for complex problem\nsolving, but their structured reasoning pathways can lead to unsafe outputs\nwhen exposed to harmful prompts. Existing safety alignment methods reduce\nharmful outputs but can degrade reasoning depth, leading to significant\ntrade-offs in complex, multi-step tasks, and remain vulnerable to sophisticated\njailbreak attacks. To address this, we introduce SAFEPATH, a lightweight\nalignment method that fine-tunes LRMs to emit a short, 8-token Safety Primer at\nthe start of their reasoning, in response to harmful prompts, while leaving the\nrest of the reasoning process unsupervised. Empirical results across multiple\nbenchmarks indicate that SAFEPATH effectively reduces harmful outputs while\nmaintaining reasoning performance. Specifically, SAFEPATH reduces harmful\nresponses by up to 90.0% and blocks 83.3% of jailbreak attempts in the\nDeepSeek-R1-Distill-Llama-8B model, while requiring 295.9x less compute than\nDirect Refusal and 314.1x less than SafeChain. We further introduce a zero-shot\nvariant that requires no fine-tuning. In addition, we provide a comprehensive\nanalysis of how existing methods in LLMs generalize, or fail, when applied to\nreasoning-centric models, revealing critical gaps and new directions for safer\nAI."}
{"id": "2505.17155", "pdf": "https://arxiv.org/pdf/2505.17155.pdf", "abs": "https://arxiv.org/abs/2505.17155", "title": "TrimR: Verifier-based Training-Free Thinking Compression for Efficient Test-Time Scaling", "authors": ["Weizhe Lin", "Xing Li", "Zhiyuan Yang", "Xiaojin Fu", "Hui-Ling Zhen", "Yaoyuan Wang", "Xianzhi Yu", "Wulong Liu", "Xiaosong Li", "Mingxuan Yuan"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Reasoning Models (LRMs) demonstrate exceptional capability in tackling\ncomplex mathematical, logical, and coding tasks by leveraging extended\nChain-of-Thought (CoT) reasoning. Test-time scaling methods, such as prolonging\nCoT with explicit token-level exploration, can push LRMs' accuracy boundaries,\nbut they incur significant decoding overhead. A key inefficiency source is LRMs\noften generate redundant thinking CoTs, which demonstrate clear structured\noverthinking and underthinking patterns. Inspired by human cognitive reasoning\nprocesses and numerical optimization theories, we propose TrimR, a\nverifier-based, training-free, efficient framework for dynamic CoT compression\nto trim reasoning and enhance test-time scaling, explicitly tailored for\nproduction-level deployment. Our method employs a lightweight, pretrained,\ninstruction-tuned verifier to detect and truncate redundant intermediate\nthoughts of LRMs without any LRM or verifier fine-tuning. We present both the\ncore algorithm and asynchronous online system engineered for high-throughput\nindustrial applications. Empirical evaluations on Ascend NPUs and vLLM show\nthat our framework delivers substantial gains in inference efficiency under\nlarge-batch workloads. In particular, on the four MATH500, AIME24, AIME25, and\nGPQA benchmarks, the reasoning runtime of Pangu Pro MoE, Pangu-R-38B, QwQ-32B,\nand DeepSeek-R1-Distill-Qwen-32B is improved by up to 70% with negligible\nimpact on accuracy."}
{"id": "2505.17371", "pdf": "https://arxiv.org/pdf/2505.17371.pdf", "abs": "https://arxiv.org/abs/2505.17371", "title": "An End-to-End Approach for Child Reading Assessment in the Xhosa Language", "authors": ["Sergio Chevtchenko", "Nikhil Navas", "Rafaella Vale", "Franco Ubaudi", "Sipumelele Lucwaba", "Cally Ardington", "Soheil Afshar", "Mark Antoniou", "Saeed Afshar"], "categories": ["cs.LG", "cs.CL"], "comment": "Paper accepted on AIED 2025 containing 14 pages, 6 figures and 4\n  tables", "summary": "Child literacy is a strong predictor of life outcomes at the subsequent\nstages of an individual's life. This points to a need for targeted\ninterventions in vulnerable low and middle income populations to help bridge\nthe gap between literacy levels in these regions and high income ones. In this\neffort, reading assessments provide an important tool to measure the\neffectiveness of these programs and AI can be a reliable and economical tool to\nsupport educators with this task. Developing accurate automatic reading\nassessment systems for child speech in low-resource languages poses significant\nchallenges due to limited data and the unique acoustic properties of children's\nvoices. This study focuses on Xhosa, a language spoken in South Africa, to\nadvance child speech recognition capabilities. We present a novel dataset\ncomposed of child speech samples in Xhosa. The dataset is available upon\nrequest and contains ten words and letters, which are part of the Early Grade\nReading Assessment (EGRA) system. Each recording is labeled with an online and\ncost-effective approach by multiple markers and a subsample is validated by an\nindependent EGRA reviewer. This dataset is evaluated with three fine-tuned\nstate-of-the-art end-to-end models: wav2vec 2.0, HuBERT, and Whisper. The\nresults indicate that the performance of these models can be significantly\ninfluenced by the amount and balancing of the available training data, which is\nfundamental for cost-effective large dataset collection. Furthermore, our\nexperiments indicate that the wav2vec 2.0 performance is improved by training\non multiple classes at a time, even when the number of available samples is\nconstrained."}
{"id": "2505.17534", "pdf": "https://arxiv.org/pdf/2505.17534.pdf", "abs": "https://arxiv.org/abs/2505.17534", "title": "Co-Reinforcement Learning for Unified Multimodal Understanding and Generation", "authors": ["Jingjing Jiang", "Chongjie Si", "Jun Luo", "Hanwang Zhang", "Chao Ma"], "categories": ["cs.CV", "cs.CL", "cs.MM"], "comment": null, "summary": "This paper presents a pioneering exploration of reinforcement learning (RL)\nvia group relative policy optimization for unified multimodal large language\nmodels (ULMs), aimed at simultaneously reinforcing generation and understanding\ncapabilities. Through systematic pilot studies, we uncover the significant\npotential of ULMs to enable the synergistic co-evolution of dual capabilities\nwithin a shared policy optimization framework. Building on this insight, we\nintroduce CoRL, a co-reinforcement learning framework comprising a unified RL\nstage for joint optimization and a refined RL stage for task-specific\nenhancement. With the proposed CoRL, our resulting model, ULM-R1, achieves\naverage improvements of 7% on three text-to-image generation datasets and 23%\non nine multimodal understanding benchmarks. These results demonstrate the\neffectiveness of CoRL and highlight the substantial benefit of reinforcement\nlearning in facilitating cross-task synergy and optimization for ULMs. Code is\navailable at https://github.com/mm-vl/ULM-R1."}
{"id": "2505.18129", "pdf": "https://arxiv.org/pdf/2505.18129.pdf", "abs": "https://arxiv.org/abs/2505.18129", "title": "One RL to See Them All: Visual Triple Unified Reinforcement Learning", "authors": ["Yan Ma", "Linge Du", "Xuyang Shen", "Shaoxiang Chen", "Pengfei Li", "Qibing Ren", "Lizhuang Ma", "Yuchao Dai", "Pengfei Liu", "Junjie Yan"], "categories": ["cs.CV", "cs.CL"], "comment": "Technical Report", "summary": "Reinforcement learning (RL) has significantly advanced the reasoning\ncapabilities of vision-language models (VLMs). However, the use of RL beyond\nreasoning tasks remains largely unexplored, especially for perceptionintensive\ntasks like object detection and grounding. We propose V-Triune, a Visual Triple\nUnified Reinforcement Learning system that enables VLMs to jointly learn visual\nreasoning and perception tasks within a single training pipeline. V-Triune\ncomprises triple complementary components: Sample-Level Data Formatting (to\nunify diverse task inputs), Verifier-Level Reward Computation (to deliver\ncustom rewards via specialized verifiers) , and Source-Level Metric Monitoring\n(to diagnose problems at the data-source level). We further introduce a novel\nDynamic IoU reward, which provides adaptive, progressive, and definite feedback\nfor perception tasks handled by V-Triune. Our approach is instantiated within\noff-the-shelf RL training framework using open-source 7B and 32B backbone\nmodels. The resulting model, dubbed Orsta (One RL to See Them All),\ndemonstrates consistent improvements across both reasoning and perception\ntasks. This broad capability is significantly shaped by its training on a\ndiverse dataset, constructed around four representative visual reasoning tasks\n(Math, Puzzle, Chart, and Science) and four visual perception tasks (Grounding,\nDetection, Counting, and OCR). Subsequently, Orsta achieves substantial gains\non MEGA-Bench Core, with improvements ranging from +2.1 to an impressive +14.1\nacross its various 7B and 32B model variants, with performance benefits\nextending to a wide range of downstream tasks. These results highlight the\neffectiveness and scalability of our unified RL approach for VLMs. The V-Triune\nsystem, along with the Orsta models, is publicly available at\nhttps://github.com/MiniMax-AI."}
{"id": "2505.18458", "pdf": "https://arxiv.org/pdf/2505.18458.pdf", "abs": "https://arxiv.org/abs/2505.18458", "title": "A Survey of LLM $\\times$ DATA", "authors": ["Xuanhe Zhou", "Junxuan He", "Wei Zhou", "Haodong Chen", "Zirui Tang", "Haoyu Zhao", "Xin Tong", "Guoliang Li", "Youmin Chen", "Jun Zhou", "Zhaojun Sun", "Binyuan Hui", "Shuo Wang", "Conghui He", "Zhiyuan Liu", "Jingren Zhou", "Fan Wu"], "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.IR", "cs.LG"], "comment": "Please refer to the paper list at:\n  https://github.com/weAIDB/awesome-data-llm", "summary": "The integration of large language model (LLM) and data management (DATA) is\nrapidly redefining both domains. In this survey, we comprehensively review the\nbidirectional relationships. On the one hand, DATA4LLM, spanning large-scale\ndata processing, storage, and serving, feeds LLMs with high quality, diversity,\nand timeliness of data required for stages like pre-training, post-training,\nretrieval-augmented generation, and agentic workflows: (i) Data processing for\nLLMs includes scalable acquisition, deduplication, filtering, selection, domain\nmixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on\nefficient data and model formats, distributed and heterogeneous storage\nhierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data\nserving for LLMs tackles challenges in RAG (e.g., knowledge post-processing),\nLLM inference (e.g., prompt compression, data provenance), and training\nstrategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA,\nLLMs are emerging as general-purpose engines for data management. We review\nrecent advances in (i) data manipulation, including automatic data cleaning,\nintegration, discovery; (ii) data analysis, covering reasoning over structured,\nsemi-structured, and unstructured data, and (iii) system optimization (e.g.,\nconfiguration tuning, query rewriting, anomaly diagnosis), powered by LLM\ntechniques like retrieval-augmented prompting, task-specialized fine-tuning,\nand multi-agent collaboration."}
{"id": "2505.18668", "pdf": "https://arxiv.org/pdf/2505.18668.pdf", "abs": "https://arxiv.org/abs/2505.18668", "title": "ChartGalaxy: A Dataset for Infographic Chart Understanding and Generation", "authors": ["Zhen Li", "Duan Li", "Yukai Guo", "Xinyuan Guo", "Bowen Li", "Lanxi Xiao", "Shenyu Qiao", "Jiashu Chen", "Zijian Wu", "Hui Zhang", "Xinhuan Shu", "Shixia Liu"], "categories": ["cs.CV", "cs.CL"], "comment": "56 pages", "summary": "Infographic charts are a powerful medium for communicating abstract data by\ncombining visual elements (e.g., charts, images) with textual information.\nHowever, their visual and structural richness poses challenges for large\nvision-language models (LVLMs), which are typically trained on plain charts. To\nbridge this gap, we introduce ChartGalaxy, a million-scale dataset designed to\nadvance the understanding and generation of infographic charts. The dataset is\nconstructed through an inductive process that identifies 75 chart types, 330\nchart variations, and 68 layout templates from real infographic charts and uses\nthem to create synthetic ones programmatically. We showcase the utility of this\ndataset through: 1) improving infographic chart understanding via fine-tuning,\n2) benchmarking code generation for infographic charts, and 3) enabling\nexample-based infographic chart generation. By capturing the visual and\nstructural complexity of real design, ChartGalaxy provides a useful resource\nfor enhancing multimodal reasoning and generation in LVLMs."}
{"id": "2505.20279", "pdf": "https://arxiv.org/pdf/2505.20279.pdf", "abs": "https://arxiv.org/abs/2505.20279", "title": "VLM-3R: Vision-Language Models Augmented with Instruction-Aligned 3D Reconstruction", "authors": ["Zhiwen Fan", "Jian Zhang", "Renjie Li", "Junge Zhang", "Runjin Chen", "Hezhen Hu", "Kevin Wang", "Huaizhi Qu", "Dilin Wang", "Zhicheng Yan", "Hongyu Xu", "Justin Theiss", "Tianlong Chen", "Jiachen Li", "Zhengzhong Tu", "Zhangyang Wang", "Rakesh Ranjan"], "categories": ["cs.CV", "cs.CL"], "comment": "Project Page: https://vlm-3r.github.io/", "summary": "The rapid advancement of Large Multimodal Models (LMMs) for 2D images and\nvideos has motivated extending these models to understand 3D scenes, aiming for\nhuman-like visual-spatial intelligence. Nevertheless, achieving deep spatial\nunderstanding comparable to human capabilities poses significant challenges in\nmodel encoding and data acquisition. Existing methods frequently depend on\nexternal depth sensors for geometry capture or utilize off-the-shelf algorithms\nfor pre-constructing 3D maps, thereby limiting their scalability, especially\nwith prevalent monocular video inputs and for time-sensitive applications. In\nthis work, we introduce VLM-3R, a unified framework for Vision-Language Models\n(VLMs) that incorporates 3D Reconstructive instruction tuning. VLM-3R processes\nmonocular video frames by employing a geometry encoder to derive implicit 3D\ntokens that represent spatial understanding. Leveraging our Spatial-Visual-View\nFusion and over 200K curated 3D reconstructive instruction tuning\nquestion-answer (QA) pairs, VLM-3R effectively aligns real-world spatial\ncontext with language instructions. This enables monocular 3D spatial\nassistance and embodied reasoning. To facilitate the evaluation of temporal\nreasoning, we introduce the Vision-Spatial-Temporal Intelligence benchmark,\nfeaturing over 138.6K QA pairs across five distinct tasks focused on evolving\nspatial relationships. Extensive experiments demonstrate that our model,\nVLM-3R, not only facilitates robust visual-spatial reasoning but also enables\nthe understanding of temporal 3D context changes, excelling in both accuracy\nand scalability."}
{"id": "2505.20368", "pdf": "https://arxiv.org/pdf/2505.20368.pdf", "abs": "https://arxiv.org/abs/2505.20368", "title": "Hierarchical Retrieval with Evidence Curation for Open-Domain Financial Question Answering on Standardized Documents", "authors": ["Jaeyoung Choe", "Jihoon Kim", "Woohwan Jung"], "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "ACL 2025 (Findings)", "summary": "Retrieval-augmented generation (RAG) based large language models (LLMs) are\nwidely used in finance for their excellent performance on knowledge-intensive\ntasks. However, standardized documents (e.g., SEC filing) share similar formats\nsuch as repetitive boilerplate texts, and similar table structures. This\nsimilarity forces traditional RAG methods to misidentify near-duplicate text,\nleading to duplicate retrieval that undermines accuracy and completeness. To\naddress these issues, we propose the Hierarchical Retrieval with Evidence\nCuration (HiREC) framework. Our approach first performs hierarchical retrieval\nto reduce confusion among similar texts. It first retrieve related documents\nand then selects the most relevant passages from the documents. The evidence\ncuration process removes irrelevant passages. When necessary, it automatically\ngenerates complementary queries to collect missing information. To evaluate our\napproach, we construct and release a Large-scale Open-domain Financial (LOFin)\nquestion answering benchmark that includes 145,897 SEC documents and 1,595\nquestion-answer pairs. Our code and data are available at\nhttps://github.com/deep-over/LOFin-bench-HiREC."}
{"id": "2505.20896", "pdf": "https://arxiv.org/pdf/2505.20896.pdf", "abs": "https://arxiv.org/abs/2505.20896", "title": "How Do Transformers Learn Variable Binding in Symbolic Programs?", "authors": ["Yiwei Wu", "Atticus Geiger", "Rapha√´l Milli√®re"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "16 pages, 10 figures, 1 table. To appear in the Proceedings of the\n  42nd International Conference on Machine Learning (ICML 2025). v2: Added link\n  to Variable Scope in abstract", "summary": "Variable binding -- the ability to associate variables with values -- is\nfundamental to symbolic computation and cognition. Although classical\narchitectures typically implement variable binding via addressable memory, it\nis not well understood how modern neural networks lacking built-in binding\noperations may acquire this capacity. We investigate this by training a\nTransformer to dereference queried variables in symbolic programs where\nvariables are assigned either numerical constants or other variables. Each\nprogram requires following chains of variable assignments up to four steps deep\nto find the queried value, and also contains irrelevant chains of assignments\nacting as distractors. Our analysis reveals a developmental trajectory with\nthree distinct phases during training: (1) random prediction of numerical\nconstants, (2) a shallow heuristic prioritizing early variable assignments, and\n(3) the emergence of a systematic mechanism for dereferencing assignment\nchains. Using causal interventions, we find that the model learns to exploit\nthe residual stream as an addressable memory space, with specialized attention\nheads routing information across token positions. This mechanism allows the\nmodel to dynamically track variable bindings across layers, resulting in\naccurate dereferencing. Our results show how Transformer models can learn to\nimplement systematic variable binding without explicit architectural support,\nbridging connectionist and symbolic approaches. To facilitate reproducible\nresearch, we developed Variable Scope, an interactive web platform for\nexploring our findings at https://variablescope.org"}
{"id": "2505.21549", "pdf": "https://arxiv.org/pdf/2505.21549.pdf", "abs": "https://arxiv.org/abs/2505.21549", "title": "Distill CLIP (DCLIP): Enhancing Image-Text Retrieval via Cross-Modal Transformer Distillation", "authors": ["Daniel Csizmadia", "Andrei Codreanu", "Victor Sim", "Vighnesh Prabhu", "Michael Lu", "Kevin Zhu", "Sean O'Brien", "Vasu Sharma"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "We present Distill CLIP (DCLIP), a fine-tuned variant of the CLIP model that\nenhances multimodal image-text retrieval while preserving the original model's\nstrong zero-shot classification capabilities. CLIP models are typically\nconstrained by fixed image resolutions and limited context, which can hinder\ntheir effectiveness in retrieval tasks that require fine-grained cross-modal\nunderstanding. DCLIP addresses these challenges through a meta teacher-student\ndistillation framework, where a cross-modal transformer teacher is fine-tuned\nto produce enriched embeddings via bidirectional cross-attention between\nYOLO-extracted image regions and corresponding textual spans. These\nsemantically and spatially aligned global representations guide the training of\na lightweight student model using a hybrid loss that combines contrastive\nlearning and cosine similarity objectives. Despite being trained on only\n~67,500 samples curated from MSCOCO, Flickr30k, and Conceptual Captions-just a\nfraction of CLIP's original dataset-DCLIP significantly improves image-text\nretrieval metrics (Recall@K, MAP), while retaining approximately 94% of CLIP's\nzero-shot classification performance. These results demonstrate that DCLIP\neffectively mitigates the trade-off between task specialization and\ngeneralization, offering a resource-efficient, domain-adaptive, and\ndetail-sensitive solution for advanced vision-language tasks. Code available at\nhttps://anonymous.4open.science/r/DCLIP-B772/README.md."}
{"id": "2505.21907", "pdf": "https://arxiv.org/pdf/2505.21907.pdf", "abs": "https://arxiv.org/abs/2505.21907", "title": "Modeling and Optimizing User Preferences in AI Copilots: A Comprehensive Survey and Taxonomy", "authors": ["Saleh Afzoon", "Zahra Jahanandish", "Phuong Thao Huynh", "Amin Beheshti", "Usman Naseem"], "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "AI copilots represent a new generation of AI-powered systems designed to\nassist users, particularly knowledge workers and developers, in complex,\ncontext-rich tasks. As these systems become more embedded in daily workflows,\npersonalization has emerged as a critical factor for improving usability,\neffectiveness, and user satisfaction. Central to this personalization is\npreference optimization: the system's ability to detect, interpret, and align\nwith individual user preferences. While prior work in intelligent assistants\nand optimization algorithms is extensive, their intersection within AI copilots\nremains underexplored. This survey addresses that gap by examining how user\npreferences are operationalized in AI copilots. We investigate how preference\nsignals are sourced, modeled across different interaction stages, and refined\nthrough feedback loops. Building on a comprehensive literature review, we\ndefine the concept of an AI copilot and introduce a taxonomy of preference\noptimization techniques across pre-, mid-, and post-interaction phases. Each\ntechnique is evaluated in terms of advantages, limitations, and design\nimplications. By consolidating fragmented efforts across AI personalization,\nhuman-AI interaction, and language model adaptation, this work offers both a\nunified conceptual foundation and a practical design perspective for building\nuser-aligned, persona-aware AI copilots that support end-to-end adaptability\nand deployment."}
{"id": "2505.23419", "pdf": "https://arxiv.org/pdf/2505.23419.pdf", "abs": "https://arxiv.org/abs/2505.23419", "title": "SWE-bench Goes Live!", "authors": ["Linghao Zhang", "Shilin He", "Chaoyun Zhang", "Yu Kang", "Bowen Li", "Chengxing Xie", "Junhao Wang", "Maoquan Wang", "Yufan Huang", "Shengyu Fu", "Elsie Nallipogu", "Qingwei Lin", "Yingnong Dang", "Saravan Rajmohan", "Dongmei Zhang"], "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": "Homepage: \\url{https://swe-bench-live.github.io/}, Code:\n  \\url{https://github.com/SWE-bench-Live}, Dataset:\n  \\url{https://huggingface.co/SWE-bench-Live}", "summary": "The issue-resolving task, where a model generates patches to fix real-world\nbugs, has emerged as a critical benchmark for evaluating the capabilities of\nlarge language models (LLMs). While SWE-bench and its variants have become\nstandard in this domain, they suffer from key limitations: they have not been\nupdated since their initial releases, cover a narrow set of repositories, and\ndepend heavily on manual effort for instance construction and environment\nsetup. These factors hinder scalability and introduce risks of overfitting and\ndata contamination. In this work, we present SWE-bench-Live, a live-updatable\nbenchmark designed to overcome these challenges. Our initial release consists\nof 1,319 tasks derived from real GitHub issues created since 2024, spanning 93\nrepositories. Each task is accompanied by a dedicated Docker image to ensure\nreproducible execution. Central to our benchmark is \\method, an automated\ncuration pipeline that streamlines the entire process from instance creation to\nenvironment setup, removing manual bottlenecks and enabling scalability and\ncontinuous updates. We evaluate a range of state-of-the-art agent frameworks\nand LLMs on SWE-bench-Live, revealing a substantial performance gap compared to\nstatic benchmarks like SWE-bench, even under controlled evaluation conditions.\nTo better understand this discrepancy, we perform detailed analyses across\nrepository origin, issue recency, and task difficulty. By providing a fresh,\ndiverse, and executable benchmark grounded in live repository activity,\nSWE-bench-Live facilitates rigorous, contamination-resistant evaluation of LLMs\nand agents in dynamic, real-world software development settings."}
{"id": "2505.23590", "pdf": "https://arxiv.org/pdf/2505.23590.pdf", "abs": "https://arxiv.org/abs/2505.23590", "title": "Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with Jigsaw Puzzles", "authors": ["Zifu Wang", "Junyi Zhu", "Bo Tang", "Zhiyu Li", "Feiyu Xiong", "Jiaqian Yu", "Matthew B. Blaschko"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "The application of rule-based reinforcement learning (RL) to multimodal large\nlanguage models (MLLMs) introduces unique challenges and potential deviations\nfrom findings in text-only domains, particularly for perception-heavy tasks.\nThis paper provides a comprehensive study of rule-based visual RL, using jigsaw\npuzzles as a structured experimental framework. Jigsaw puzzles offer inherent\nground truth, adjustable difficulty, and demand complex decision-making, making\nthem ideal for this study. Our research reveals several key findings:\n\\textit{Firstly,} we find that MLLMs, initially performing near to random\nguessing on the simplest jigsaw puzzles, achieve near-perfect accuracy and\ngeneralize to complex, unseen configurations through fine-tuning.\n\\textit{Secondly,} training on jigsaw puzzles can induce generalization to\nother visual tasks, with effectiveness tied to specific task configurations.\n\\textit{Thirdly,} MLLMs can learn and generalize with or without explicit\nreasoning, though open-source models often favor direct answering.\nConsequently, even when trained for step-by-step reasoning, they can ignore the\nthinking process in deriving the final answer. \\textit{Fourthly,} we observe\nthat complex reasoning patterns appear to be pre-existing rather than emergent,\nwith their frequency increasing alongside training and task difficulty.\n\\textit{Finally,} our results demonstrate that RL exhibits more effective\ngeneralization than Supervised Fine-Tuning (SFT), and an initial SFT cold start\nphase can hinder subsequent RL optimization. Although these observations are\nbased on jigsaw puzzles and may vary across other visual tasks, this research\ncontributes a valuable piece of jigsaw to the larger puzzle of collective\nunderstanding rule-based visual RL and its potential in multimodal learning.\nThe code is available at: https://github.com/zifuwanggg/Jigsaw-R1."}
{"id": "2505.23671", "pdf": "https://arxiv.org/pdf/2505.23671.pdf", "abs": "https://arxiv.org/abs/2505.23671", "title": "GSO: Challenging Software Optimization Tasks for Evaluating SWE-Agents", "authors": ["Manish Shetty", "Naman Jain", "Jinjian Liu", "Vijay Kethanaboyina", "Koushik Sen", "Ion Stoica"], "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "comment": "Website: https://gso-bench.github.io/", "summary": "Developing high-performance software is a complex task that requires\nspecialized expertise. We introduce GSO, a benchmark for evaluating language\nmodels' capabilities in developing high-performance software. We develop an\nautomated pipeline that generates and executes performance tests to analyze\nrepository commit histories to identify 102 challenging optimization tasks\nacross 10 codebases, spanning diverse domains and programming languages. An\nagent is provided with a codebase and performance test as a precise\nspecification, and tasked to improve the runtime efficiency, which is measured\nagainst the expert developer optimization. Our quantitative evaluation reveals\nthat leading SWE-Agents struggle significantly, achieving less than 5% success\nrate, with limited improvements even with inference-time scaling. Our\nqualitative analysis identifies key failure modes, including difficulties with\nlow-level languages, practicing lazy optimization strategies, and challenges in\naccurately localizing bottlenecks. We release the code and artifacts of our\nbenchmark along with agent trajectories to enable future research."}
