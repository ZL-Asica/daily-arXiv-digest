{"id": "2506.22520", "pdf": "https://arxiv.org/pdf/2506.22520.pdf", "abs": "https://arxiv.org/abs/2506.22520", "title": "Exploring Artificial Intelligence Tutor Teammate Adaptability to Harness Discovery Curiosity and Promote Learning in the Context of Interactive Molecular Dynamics", "authors": ["Mustafa Demir", "Jacob Miratsky", "Jonathan Nguyen", "Chun Kit Chan", "Punya Mishra", "Abhishek Singharoy"], "categories": ["cs.HC", "cs.AI", "cs.CE", "cs.CY"], "comment": null, "summary": "This study examines the impact of an Artificial Intelligence tutor teammate\n(AI) on student curiosity-driven engagement and learning effectiveness during\nInteractive Molecular Dynamics (IMD) tasks on the Visual Molecular Dynamics\nplatform. It explores the role of the AI's curiosity-triggering and response\nbehaviors in stimulating and sustaining student curiosity, affecting the\nfrequency and complexity of student-initiated questions. The study further\nassesses how AI interventions shape student engagement, foster discovery\ncuriosity, and enhance team performance within the IMD learning environment.\nUsing a Wizard-of-Oz paradigm, a human experimenter dynamically adjusts the AI\ntutor teammate's behavior through a large language model. By employing a\nmixed-methods exploratory design, a total of 11 high school students\nparticipated in four IMD tasks that involved molecular visualization and\ncalculations, which increased in complexity over a 60-minute period. Team\nperformance was evaluated through real-time observation and recordings, whereas\nteam communication was measured by question complexity and AI's\ncuriosity-triggering and response behaviors. Cross Recurrence Quantification\nAnalysis (CRQA) metrics reflected structural alignment in coordination and were\nlinked to communication behaviors. High-performing teams exhibited superior\ntask completion, deeper understanding, and increased engagement. Advanced\nquestions were associated with AI curiosity-triggering, indicating heightened\nengagement and cognitive complexity. CRQA metrics highlighted dynamic\nsynchronization in student-AI interactions, emphasizing structured yet adaptive\nengagement to promote curiosity. These proof-of-concept findings suggest that\nthe AI's dual role as a teammate and educator indicates its capacity to provide\nadaptive feedback, sustaining engagement and epistemic curiosity.", "AI": {"tldr": "The study investigates how an AI tutor teammate affects student engagement and learning during Interactive Molecular Dynamics tasks, focusing on curiosity-driven interactions and team performance.", "motivation": "To understand how AI can enhance student engagement and learning effectiveness through curiosity-triggering behaviors in a collaborative learning environment.", "method": "The study used a Wizard-of-Oz paradigm where a human experimenter adjusted the AI's behavior via a large language model, involving 11 high school students in four increasingly complex Interactive Molecular Dynamics tasks over 60 minutes.", "result": "The involvement of the AI was linked to increased task completion, deeper understanding, and enhanced student engagement, with higher question complexity correlated to AI-triggered curiosity.", "conclusion": "AI can effectively function as a teammate and educator, providing adaptive feedback that helps sustain engagement and curiosity in learning contexts.", "key_contributions": ["Demonstrates the effectiveness of AI as a collaborative learning partner.", "Highlights the importance of curiosity-triggering in educational AI systems.", "Establishes a link between AI interactions and enhanced team performance."], "limitations": "Limited sample size (11 students) and focus on high school participants might not generalize to other educational levels or contexts.", "keywords": ["AI tutor", "student engagement", "curiosity", "Interactive Molecular Dynamics", "team performance"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.22583", "pdf": "https://arxiv.org/pdf/2506.22583.pdf", "abs": "https://arxiv.org/abs/2506.22583", "title": "Supra-threshold control of peripheral LOD", "authors": ["Benjamin Watson", "Neff Walker", "Larry F Hodges"], "categories": ["cs.HC", "cs.GR"], "comment": null, "summary": "Level of detail (LOD) is widely used to control visual feedback in\ninteractive applications. LOD control is typically based on perception at\nthreshold - the conditions in which a stimulus first becomes perceivable. Yet\nmost LOD manipulations are quite perceivable and occur well above threshold.\nMoreover, research shows that supra-threshold perception differs drastically\nfrom perception at threshold. In that case, should supra-threshold LOD control\nalso differ from LOD control at threshold?\n  In two experiments, we examine supra-threshold LOD control in the visual\nperiphery and find that indeed, it should differ drastically from LOD control\nat threshold. Specifically, we find that LOD must support a task-dependent\nlevel of reliable perceptibility. Above that level, perceptibility of LOD\ncontrol manipulations should be minimized, and detail contrast is a better\npredictor of perceptibility than detail size. Below that level, perceptibility\nmust be maximized, and LOD should be improved as eccentricity rises or contrast\ndrops. This directly contradicts prevailing threshold-based LOD control\nschemes, and strongly suggests a reexamination of LOD control for foveal\ndisplay.", "AI": {"tldr": "This paper explores the differences between threshold and supra-threshold level of detail (LOD) control in visual applications, demonstrating that LOD manipulations must differ significantly based on perceptibility and task requirements.", "motivation": "To investigate how LOD control should account for supra-threshold perception in interactive applications, as traditional methods are based on threshold perception which may not correlate with real-world applications.", "method": "Two experiments focused on supra-threshold LOD control in the visual periphery, analyzing reliable perceptibility levels and task dependencies.", "result": "The findings indicate that LOD must maximize perceptibility below the reliable level and minimize it above that level, challenging existing threshold-based LOD control approaches.", "conclusion": "The research suggests a need to reexamine and adjust LOD control methods for better performance in foveal displays, based on the characteristics of task and peripheral visual perception.", "key_contributions": ["Demonstrated differences between supra-threshold and threshold perception in LOD control.", "Provided a new framework for task-dependent LOD perceptibility management.", "Highlighted the importance of detail contrast over detail size in LOD perception."], "limitations": "Further research is needed to explore variations across different tasks and settings.", "keywords": ["level of detail", "visual perception", "interactive applications", "task-dependent", "threshold perception"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.22597", "pdf": "https://arxiv.org/pdf/2506.22597.pdf", "abs": "https://arxiv.org/abs/2506.22597", "title": "A tangible user interface for assessing cognitive mapping ability", "authors": ["Ehud Sharlin", "Benjamin Watson", "Steve Sutphen", "Lili Liu", "Robert Lederer", "John Frazer"], "categories": ["cs.HC"], "comment": null, "summary": "Wayfinding, the ability to recall the environment and navigate through it, is\nan essential cognitive skill relied upon almost every day in a person's life. A\ncrucial component of wayfinding is the construction of cognitive maps, mental\nrepresentations of the environments through which a person travels. Age,\ndisease or injury can severely affect cognitive mapping, making assessment of\nthis basic survival skill particularly important to clinicians and therapists.\nCognitive mapping has also been the focus of decades of basic research by\ncognitive psychologists. Both communities have evolved a number of techniques\nfor assessing cognitive mapping ability. We present the Cognitive Map Probe\n(CMP), a new computerized tool for assessment of cognitive mapping ability that\nincreases consistency and promises improvements in flexibility, accessibility,\nsensitivity and control. The CMP uses a tangible user interface that affords\nspatial manipulation. We describe the design of the CMP, and find that it is\nsensitive to factors known to affect cognitive mapping performance in extensive\nexperimental testing.", "AI": {"tldr": "The Cognitive Map Probe (CMP) is a new computerized tool designed to assess cognitive mapping abilities, offering improvements in flexibility, accessibility, and consistency.", "motivation": "To address the importance of cognitive mapping for wayfinding skills, particularly as they are affected by age, disease, or injury.", "method": "Development of the Cognitive Map Probe (CMP), which employs a tangible user interface for spatial manipulation and was tested extensively for sensitivity to cognitive mapping performance factors.", "result": "The CMP demonstrated increased consistency and sensitivity in assessing cognitive mapping abilities compared to existing methods.", "conclusion": "The CMP is a promising tool for clinicians and therapists to assess cognitive mapping and wayfinding skills in individuals with impairments.", "key_contributions": ["Introduction of the Cognitive Map Probe (CMP) as a novel assessment tool.", "Improvement of assessment consistency and sensitivity for cognitive mapping.", "Utilization of a tangible user interface for enhanced spatial interaction."], "limitations": "", "keywords": ["cognitive mapping", "wayfinding", "Cognitive Map Probe", "assessment", "user interface"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2506.22674", "pdf": "https://arxiv.org/pdf/2506.22674.pdf", "abs": "https://arxiv.org/abs/2506.22674", "title": "Do Electric Vehicles Induce More Motion Sickness Than Fuel Vehicles? A Survey Study in China", "authors": ["Weiyin Xie", "Chunxi Huang", "Jiyao Wang", "Dengbo He"], "categories": ["cs.HC", "cs.CY", "stat.AP"], "comment": null, "summary": "Electric vehicles (EVs) are a promising alternative to fuel vehicles (FVs),\ngiven some unique characteristics of EVs, for example, the low air pollution\nand maintenance cost. However, the increasing prevalence of EVs is accompanied\nby widespread complaints regarding the high likelihood of motion sickness (MS)\ninduction, especially when compared to FVs, which has become one of the major\nobstacles to the acceptance and popularity of EVs. Despite the prevalence of\nsuch complaints online and among EV users, the association between vehicle type\n(i.e., EV versus FV) and MS prevalence and severity has not been quantified.\nThus, this study aims to investigate the existence of EV-induced MS and explore\nthe potential factors leading to it. A survey study was conducted to collect\npassengers' MS experience in EVs and FVs in the past one year. In total, 639\nvalid responses were collected from mainland China. The results show that FVs\nwere associated with a higher frequency of MS, while EVs were found to induce\nmore severe MS symptoms. Further, we found that passengers' MS severity was\nassociated with individual differences (i.e., age, gender, sleep habits,\nsusceptibility to motion-induced MS), in-vehicle activities (i.e., chatting\nwith others and watching in-vehicle displays), and road conditions (i.e.,\ncongestion and slope), while the MS frequency was associated with the vehicle\nownership and riding frequency. The results from this study can guide the\ndirections of future empirical studies that aim to quantify the inducers of MS\nin EVs and FVs, as well as the optimization of EVs to reduce MS.", "AI": {"tldr": "This study investigates motion sickness (MS) induction in electric vehicles (EVs) compared to fuel vehicles (FVs) and identifies associated factors.", "motivation": "To address the rising complaints of motion sickness among passengers in EVs, which could hinder their acceptance as an alternative to fuel vehicles.", "method": "A survey was conducted, gathering data on passengers' motion sickness experiences in EVs and FVs from 639 valid responses in mainland China over the past year.", "result": "Findings reveal that while FVs have a higher frequency of motion sickness occurrences, EVs induce more severe symptoms. Factors influencing severity include individual traits, in-vehicle activities, and road conditions.", "conclusion": "The study provides insights for future research on motion sickness in vehicle types and highlights the need for EV optimization to minimize motion sickness.", "key_contributions": ["Quantifies motion sickness prevalence and severity between EVs and FVs.", "Identifies individual and situational factors influencing motion sickness severity in EVs.", "Provides empirical data to guide future studies on motion sickness in automobiles."], "limitations": "The study's focus is limited to responses from mainland China, which may not be generalizable to other regions.", "keywords": ["motion sickness", "electric vehicles", "fuel vehicles", "survey study", "human factors"], "importance_score": 5, "read_time_minutes": 5}}
{"id": "2506.22439", "pdf": "https://arxiv.org/pdf/2506.22439.pdf", "abs": "https://arxiv.org/abs/2506.22439", "title": "Psycholinguistic Word Features: a New Approach for the Evaluation of LLMs Alignment with Humans", "authors": ["Javier Conde", "Miguel González", "María Grandury", "Gonzalo Martínez", "Pedro Reviriego", "Mar Brysbaert"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted for the GEM2 workshop at ACL 2025", "summary": "The evaluation of LLMs has so far focused primarily on how well they can\nperform different tasks such as reasoning, question-answering, paraphrasing, or\ntranslating. For most of these tasks, performance can be measured with\nobjective metrics, such as the number of correct answers. However, other\nlanguage features are not easily quantified. For example, arousal,\nconcreteness, or gender associated with a given word, as well as the extent to\nwhich we experience words with senses and relate them to a specific sense.\nThose features have been studied for many years by psycholinguistics,\nconducting large-scale experiments with humans to produce ratings for thousands\nof words. This opens an opportunity to evaluate how well LLMs align with human\nratings on these word features, taking advantage of existing studies that cover\nmany different language features in a large number of words. In this paper, we\nevaluate the alignment of a representative group of LLMs with human ratings on\ntwo psycholinguistic datasets: the Glasgow and Lancaster norms. These datasets\ncover thirteen features over thousands of words. The results show that\nalignment is \\textcolor{black}{generally} better in the Glasgow norms evaluated\n(arousal, valence, dominance, concreteness, imageability, familiarity, and\ngender) than on the Lancaster norms evaluated (introceptive, gustatory,\nolfactory, haptic, auditory, and visual). This suggests a potential limitation\nof current LLMs in aligning with human sensory associations for words, which\nmay be due to their lack of embodied cognition present in humans and\nillustrates the usefulness of evaluating LLMs with psycholinguistic datasets.", "AI": {"tldr": "This paper evaluates how well large language models (LLMs) align with human ratings of various word features identified by psycholinguistic studies, using the Glasgow and Lancaster norms.", "motivation": "The evaluation of LLMs has primarily focused on task performance, neglecting features that cannot be easily quantified. This paper aims to understand LLM performance in relation to human psycholinguistic word ratings.", "method": "The study assesses a group of LLMs against human ratings on the Glasgow and Lancaster psycholinguistic datasets, which include thirteen different word features.", "result": "The results indicate that LLMs show better alignment with human ratings in the Glasgow norms (covering emotional and cognitive features) compared to the Lancaster norms (covering sensory features).", "conclusion": "The findings suggest that LLMs might struggle with aligning sensory word associations, pointing to a gap between LLM capabilities and human embodied cognition.", "key_contributions": ["Evaluation of LLMs against psycholinguistic datasets", "Comparison between Glasgow and Lancaster norms", "Insight into LLM limitations in sensory word association"], "limitations": "The findings highlight limitations of LLMs in embodying human-like sensory associations, which may affect their performance in real-world applications.", "keywords": ["LLMs", "psycholinguistics", "word features", "language models", "human ratings"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.22741", "pdf": "https://arxiv.org/pdf/2506.22741.pdf", "abs": "https://arxiv.org/abs/2506.22741", "title": "Insights in Adaptation: Examining Self-reflection Strategies of Job Seekers with Visual Impairments in India", "authors": ["Akshay Nayak Kolgar", "Yash Prakash", "Sampath Jayarathna", "Hae-Na Lee", "Vikas Ashok"], "categories": ["cs.HC"], "comment": null, "summary": "Significant changes in the digital employment landscape, driven by rapid\ntechnological advancements and the COVID-19 pandemic, have introduced new\nopportunities for blind and visually impaired (BVI) individuals in developing\ncountries like India. However, a significant portion of the BVI population in\nIndia remains unemployed despite extensive accessibility advancements and job\nsearch interventions. Therefore, we conducted semi-structured interviews with\n20 BVI persons who were either pursuing or recently sought employment in the\ndigital industry. Our findings reveal that despite gaining digital literacy and\nextensive training, BVI individuals struggle to meet industry requirements for\nfulfilling job openings. While they engage in self-reflection to identify\nshortcomings in their approach and skills, they lack constructive feedback from\npeers and recruiters. Moreover, the numerous job intervention tools are limited\nin their ability to meet the unique needs of BVI job seekers. Our results\ntherefore provide key insights that inform the design of future collaborative\nintervention systems that offer personalized feedback for BVI individuals,\neffectively guiding their self-reflection process and subsequent job search\nbehaviors, and potentially leading to improved employment outcomes.", "AI": {"tldr": "The paper explores the employment challenges faced by blind and visually impaired individuals in India's digital industry, highlighting the need for personalized feedback systems to improve their job search outcomes.", "motivation": "The study investigates the employment barriers for blind and visually impaired (BVI) individuals in India, especially in the context of digital job opportunities amid changing market conditions.", "method": "Semi-structured interviews were conducted with 20 BVI individuals who were seeking employment in the digital industry to gain insights into their experiences and challenges.", "result": "Findings indicate that despite digital literacy and training, BVI individuals face difficulties in meeting industry standards, largely due to a lack of constructive feedback and suitable job intervention tools tailored to their needs.", "conclusion": "The study suggests developing collaborative intervention systems that provide personalized feedback to aid BVI individuals in their job searches, improving their chances of employment in the digital sector.", "key_contributions": ["Identifies barriers BVI individuals face in the job market", "Highlights the importance of personalized feedback in career development", "Recommends collaborative intervention systems tailored to BVI needs"], "limitations": "The study is based on a small sample size of 20 individuals, which may not represent the broader BVI population's experiences.", "keywords": ["Blind and Visually Impaired", "Employment", "Digital Industry", "Feedback Systems", "Job Search"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.22485", "pdf": "https://arxiv.org/pdf/2506.22485.pdf", "abs": "https://arxiv.org/abs/2506.22485", "title": "AI Agents-as-Judge: Automated Assessment of Accuracy, Consistency, Completeness and Clarity for Enterprise Documents", "authors": ["Sudip Dasgupta", "Himanshu Shankar"], "categories": ["cs.CL", "cs.AI", "68T07, 68T50", "I.2.1; I.2.3; I.2.7; H.3.3"], "comment": "17 pages, 2 system diagrams, 1 table, no prior conference publication", "summary": "This study presents a modular, multi-agent system for the automated review of\nhighly structured enterprise business documents using AI agents. Unlike prior\nsolutions focused on unstructured texts or limited compliance checks, this\nframework leverages modern orchestration tools such as LangChain, CrewAI,\nTruLens, and Guidance to enable section-by-section evaluation of documents for\naccuracy, consistency, completeness, and clarity. Specialized agents, each\nresponsible for discrete review criteria such as template compliance or factual\ncorrectness, operate in parallel or sequence as required. Evaluation outputs\nare enforced to a standardized, machine-readable schema, supporting downstream\nanalytics and auditability. Continuous monitoring and a feedback loop with\nhuman reviewers allow for iterative system improvement and bias mitigation.\n  Quantitative evaluation demonstrates that the AI Agent-as-Judge system\napproaches or exceeds human performance in key areas: achieving 99% information\nconsistency (vs. 92% for humans), halving error and bias rates, and reducing\naverage review time from 30 to 2.5 minutes per document, with a 95% agreement\nrate between AI and expert human judgment. While promising for a wide range of\nindustries, the study also discusses current limitations, including the need\nfor human oversight in highly specialized domains and the operational cost of\nlarge-scale LLM usage. The proposed system serves as a flexible, auditable, and\nscalable foundation for AI-driven document quality assurance in the enterprise\ncontext.", "AI": {"tldr": "A multi-agent AI system automates review of enterprise business documents, improving accuracy and efficiency while addressing biases through human feedback.", "motivation": "To enhance the review process of structured enterprise documents using modular AI agents, overcoming limitations of previous unstructured approaches.", "method": "The system employs various AI orchestration tools (LangChain, CrewAI, TruLens, Guidance) to facilitate section-by-section evaluations by specialized agents for specific review criteria.", "result": "Quantitative results show the system achieving 99% information consistency, reducing review time from 30 minutes to 2.5 minutes, and maintaining a 95% agreement rate with human judgments.", "conclusion": "This modular AI framework is scalable for document quality assurance, though human oversight remains necessary in specialized areas due to operational costs associated with LLMs.", "key_contributions": ["Introduction of modular, multi-agent AI for document review", "Significant improvements in accuracy and efficiency over human review", "Framework for continuous improvement and bias mitigation through human feedback"], "limitations": "Requires human oversight in specialized domains and incurs costs for large-scale LLM usage.", "keywords": ["AI", "multi-agent system", "document review", "enterprise", "quality assurance"], "importance_score": 5, "read_time_minutes": 15}}
{"id": "2506.22815", "pdf": "https://arxiv.org/pdf/2506.22815.pdf", "abs": "https://arxiv.org/abs/2506.22815", "title": "Memory as a Service (MaaS): Rethinking Contextual Memory as Service-Oriented Modules for Collaborative Agents", "authors": ["Haichang Li"], "categories": ["cs.HC", "H.5.0"], "comment": "Position Paper for workshop. This is an initial version for\n  discussion purposes", "summary": "This position paper aims to rethink the role and design of memory in Large\nLanguage Model (LLM)-based agent systems. We observe that while current memory\npractices have begun to transcend the limitations of single interactions, they\nremain conceptually grounded in \"bound memory\" in terms of design concept-where\nmemory is treated as local state attached to specific context or entities,\nforming \"memory silos\" that impede cross-entity collaboration. To overcome this\narchitectural bottleneck, this paper proposes the timely design perspective of\n\"Memory as a Service\" (MaaS). MaaS advocates decoupling memory from its\nconventional role as an interaction byproduct and encapsulating it as a modular\nservice that can be independently callable, dynamically composable, and finely\ngoverned. At its core, MaaS leverages the duality of memory-its inherently\nprivate nature and its potential for public service-to grant memory controlled,\non-demand interoperability across entities. This paper introduces a\ntwo-dimensional design space defined by entity structure and service type,\nillustrating how MaaS aligns with current memory practices while naturally\nextending them to cross-entity collaborative scenarios. Finally, we outline an\nopen research agenda spanning governance, security, and ethical ecosystems, and\ncall upon the broader research community to explore this shift toward\nservice-oriented memory for collaborative agents operating across entity\nboundaries.", "AI": {"tldr": "The paper rethinks memory design in LLM-based agents and proposes 'Memory as a Service' (MaaS) to enhance interoperability and collaboration between entities.", "motivation": "Current memory practices in LLM systems create 'memory silos' that limit cross-entity collaboration.", "method": "The paper presents a new framework for memory design called 'Memory as a Service' (MaaS) that decouples memory as a modular service, allowing independent access and governance.", "result": "MaaS facilitates controlled, on-demand interoperability of memory across entities, while maintaining privacy and governance.", "conclusion": "The paper calls for further exploration of service-oriented memory to improve collaborative functionality in LLM-based agents.", "key_contributions": ["Introduction of 'Memory as a Service' (MaaS) concept.", "Two-dimensional design space for memory across entities.", "Discussion on governance, security, and ethical considerations in memory design."], "limitations": "", "keywords": ["Memory as a Service", "Large Language Models", "Collaborative agents"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.22486", "pdf": "https://arxiv.org/pdf/2506.22486.pdf", "abs": "https://arxiv.org/abs/2506.22486", "title": "Hallucination Detection with Small Language Models", "authors": ["Ming Cheung"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Since the introduction of ChatGPT, large language models (LLMs) have\ndemonstrated significant utility in various tasks, such as answering questions\nthrough retrieval-augmented generation. Context can be retrieved using a\nvectorized database, serving as a foundation for LLMs to generate responses.\nHowever, hallucinations in responses can undermine the reliability of LLMs in\npractical applications, and they are not easily detectable in the absence of\nground truth, particularly in question-and-answer scenarios. This paper\nproposes a framework that integrates multiple small language models to verify\nresponses generated by LLMs using the retrieved context from a vectorized\ndatabase. By breaking down the responses into individual sentences and\nutilizing the probability of generating \"Yes\" tokens from the outputs of\nmultiple models for a given set of questions, responses, and relevant context,\nhallucinations can be detected. The proposed framework is validated through\nexperiments with real datasets comprising over 100 sets of questions, answers,\nand contexts, including responses with fully and partially correct sentences.\nThe results demonstrate a 10\\% improvement in F1 scores for detecting correct\nresponses compared to hallucinations, indicating that multiple small language\nmodels can be effectively employed for answer verification, providing a\nscalable and efficient solution for both academic and practical applications.", "AI": {"tldr": "This paper presents a framework using multiple small language models to verify responses generated by large language models (LLMs) through detection of hallucinations in answers.", "motivation": "To address the unreliability of LLM responses caused by hallucinations, particularly in question-and-answer scenarios, and to improve the accuracy of generated answers.", "method": "The framework integrates multiple small language models that verify responses by analyzing the sentence structure and retrieving context from a vectorized database. It evaluates the probability of generating 'Yes' tokens for the outputs based on specific questions, responses, and relevant context.", "result": "The proposed framework achieved a 10% improvement in F1 scores for detecting correct responses versus hallucinations in experiments with real datasets comprising over 100 sets of questions, answers, and contexts.", "conclusion": "Utilizing multiple small language models for answer verification provides a scalable and efficient solution for both academic research and practical applications, enhancing trust in LLM-generated responses.", "key_contributions": ["Development of a framework for response verification using multiple small language models", "Demonstrated improvement in detection accuracy of LLM-generated responses", "Provided a scalable solution applicable to academic and practical scenarios."], "limitations": "", "keywords": ["Large Language Models", "Response Verification", "Hallucination Detection", "Machine Learning", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.22841", "pdf": "https://arxiv.org/pdf/2506.22841.pdf", "abs": "https://arxiv.org/abs/2506.22841", "title": "Dichoptic Opacity: Managing Occlusion in Stereoscopic Displays via Dichoptic Presentation", "authors": ["George Bell", "Alma Cantu"], "categories": ["cs.HC"], "comment": "5 pages, 3 figures. Conditionally accepted to IEEE VIS 2025 (pending\n  final review)", "summary": "Adjusting transparency is a common method of mitigating occlusion but is\noften detrimental for understanding the relative depth relationships between\nobjects as well as removes potentially important information from the occluding\nobject. We propose using dichoptic opacity, a novel method for occlusion\nmanagement that contrasts the transparency of occluders presented to each eye.\nThis allows for better simultaneous understanding of both occluder and\noccluded. A user study highlights the technique's potential, showing strong\nuser engagement and a clear preference for dichoptic opacity over traditional\npresentations. While it does not determine optimal transparency values, it\nreveals promising trends in both percentage and range that merit further\ninvestigation.", "AI": {"tldr": "The paper presents a novel method called dichoptic opacity for managing occlusion in visual displays, showing promise in user engagement and preference over traditional transparency techniques.", "motivation": "The need for better occlusion management techniques in visual displays that do not hinder the understanding of depth relationships and important information from occluded objects.", "method": "Dichoptic opacity involves adjusting the transparency of occluders differently for each eye, allowing for a better understanding of both the occluder and the occluded objects.", "result": "User studies indicate strong engagement and a preference for dichoptic opacity compared to traditional methods, with indications of trends in optimal transparency values.", "conclusion": "Dichoptic opacity shows potential for improving occlusion management, warranting further exploration of its effectiveness in various contexts.", "key_contributions": ["Introduction of the dichoptic opacity method", "Evidence of user preference and engagement in visual tasks", "Insights into optimal transparency trends for future research"], "limitations": "Does not determine optimal transparency values; more research is needed.", "keywords": ["occlusion management", "dichoptic opacity", "user engagement", "visual perception", "depth relationships"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2506.22491", "pdf": "https://arxiv.org/pdf/2506.22491.pdf", "abs": "https://arxiv.org/abs/2506.22491", "title": "PromptAug: Fine-grained Conflict Classification Using Data Augmentation", "authors": ["Oliver Warke", "Joemon M. Jose", "Faegheh Hasibi", "Jan Breitsohl"], "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; J.4; K.4.2"], "comment": null, "summary": "Given the rise of conflicts on social media, effective classification models\nto detect harmful behaviours are essential. Following the\ngarbage-in-garbage-out maxim, machine learning performance depends heavily on\ntraining data quality. However, high-quality labelled data, especially for\nnuanced tasks like identifying conflict behaviours, is limited, expensive, and\ndifficult to obtain. Additionally, as social media platforms increasingly\nrestrict access to research data, text data augmentation is gaining attention\nas an alternative to generate training data. Augmenting conflict-related data\nposes unique challenges due to Large Language Model (LLM) guardrails that\nprevent generation of offensive content. This paper introduces PromptAug, an\ninnovative LLM-based data augmentation method. PromptAug achieves statistically\nsignificant improvements of 2% in both accuracy and F1-score on conflict and\nemotion datasets. To thoroughly evaluate PromptAug against other data\naugmentation methods we conduct a robust evaluation using extreme data scarcity\nscenarios, quantitative diversity analysis and a qualitative thematic analysis.\nThe thematic analysis identifies four problematic patterns in augmented text:\nLinguistic Fluidity, Humour Ambiguity, Augmented Content Ambiguity, and\nAugmented Content Misinterpretation.\n  Overall, this work presents PromptAug as an effective method for augmenting\ndata in sensitive tasks like conflict detection, offering a unique,\ninterdisciplinary evaluation grounded in both natural language processing and\nsocial science methodology.", "AI": {"tldr": "The paper introduces PromptAug, a data augmentation method using Large Language Models (LLMs) to enhance the quality of training data for conflict detection on social media, achieving notable improvements in model performance despite challenges in generating sensitive content.", "motivation": "With the increasing prevalence of conflicts on social media, there is a crucial need for effective machine learning models to classify harmful behaviours, which heavily rely on high-quality labelled training data that is often hard to obtain.", "method": "PromptAug utilizes LLMs to generate augmented data for training conflict detection models, addressing the barriers of data scarcity and the limitations posed by content generation regulations on social media.", "result": "PromptAug demonstrates statistically significant improvements of 2% in both accuracy and F1-score when applied to conflict and emotion datasets compared to existing data augmentation methods.", "conclusion": "The study presents PromptAug as a viable solution to enhance training data for sensitive tasks such as conflict detection, while identifying critical patterns that arise in augmented text content.", "key_contributions": ["Introduction of PromptAug for data augmentation in conflict detection", "Statistically significant improvements in model performance metrics", "Identification of unique challenges and patterns in augmented text"], "limitations": "The method's effectiveness is tested in extreme data scarcity scenarios, which may not represent all practical applications.", "keywords": ["data augmentation", "conflict detection", "machine learning", "natural language processing", "social media"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.22926", "pdf": "https://arxiv.org/pdf/2506.22926.pdf", "abs": "https://arxiv.org/abs/2506.22926", "title": "Coordinated 2D-3D Visualization of Volumetric Medical Data in XR with Multimodal Interactions", "authors": ["Qixuan Liu", "Shi Qiu", "Yinqiao Wang", "Xiwen Wu", "Kenneth Siu Ho Chok", "Chi-Wing Fu", "Pheng-Ann Heng"], "categories": ["cs.HC", "cs.GR", "cs.MM"], "comment": "IEEE VIS 2025 Short Paper", "summary": "Volumetric medical imaging technologies produce detailed 3D representations\nof anatomical structures. However, effective medical data visualization and\nexploration pose significant challenges, especially for individuals with\nlimited medical expertise. We introduce a novel XR-based system with two key\ninnovations: (1) a coordinated visualization module integrating Multi-layered\nMulti-planar Reconstruction with 3D mesh models and (2) a multimodal\ninteraction framework combining hand gestures with LLM-enabled voice commands.\nWe conduct preliminary evaluations, including a 15-participant user study and\nexpert interviews, to demonstrate the system's abilities to enhance spatial\nunderstanding and reduce cognitive load. Experimental results show notable\nimprovements in task completion times, usability metrics, and interaction\neffectiveness enhanced by LLM-driven voice control. While identifying areas for\nfuture refinement, our findings highlight the potential of this immersive\nvisualization system to advance medical training and clinical practice. Our\ndemo application and supplemental materials are available for download at:\nhttps://osf.io/bpjq5/.", "AI": {"tldr": "An XR-based system enhances medical data visualization using coordinated multi-layered reconstruction and multimodal interaction with voice commands.", "motivation": "To address challenges in medical data visualization for individuals with limited medical expertise.", "method": "The system integrates a Multi-layered Multi-planar Reconstruction module with 3D mesh models and combines hand gestures with LLM-enabled voice commands for user interaction.", "result": "Preliminary evaluations indicated improvements in task completion times, usability, and interaction effectiveness when using LLM-driven voice control.", "conclusion": "The immersive visualization system shows potential to improve medical training and clinical practice, with identified areas for future refinement.", "key_contributions": ["Coordinated visualization module integrating Multi-layered Multi-planar Reconstruction with 3D mesh models.", "Multimodal interaction framework combining hand gestures with LLM-enabled voice commands.", "Demonstrated significant usability and effectiveness improvements through user studies."], "limitations": "Identified areas for future refinement and further validation needed.", "keywords": ["XR", "medical visualization", "human-computer interaction", "voice commands", "3D imaging"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2506.22508", "pdf": "https://arxiv.org/pdf/2506.22508.pdf", "abs": "https://arxiv.org/abs/2506.22508", "title": "AgentStealth: Reinforcing Large Language Model for Anonymizing User-generated Text", "authors": ["Chenyang Shao", "Tianxing Li", "Chenhao Pu", "Fengli Xu", "Yong Li"], "categories": ["cs.CL", "cs.AI"], "comment": "This work has been submitted to NeurIPS 2025. Under review", "summary": "In today's digital world, casual user-generated content often contains subtle\ncues that may inadvertently expose sensitive personal attributes. Such risks\nunderscore the growing importance of effective text anonymization to safeguard\nindividual privacy. However, existing methods either rely on rigid replacements\nthat damage utility or cloud-based LLMs that are costly and pose privacy risks.\nTo address these issues, we explore the use of locally deployed smaller-scale\nlanguage models (SLMs) for anonymization. Yet training effective SLMs remains\nchallenging due to limited high-quality supervision. To address the challenge,\nwe propose AgentStealth, a self-reinforcing LLM anonymization framework.First,\nwe introduce an adversarial anonymization workflow enhanced by In-context\nContrastive Learning and Adaptive Utility-Aware Control. Second, we perform\nsupervised adaptation of SLMs using high-quality data collected from the\nworkflow, which includes both anonymization and attack signals. Finally, we\napply online reinforcement learning where the model leverages its internal\nadversarial feedback to iteratively improve anonymization performance.\nExperiments on two datasets show that our method outperforms baselines in both\nanonymization effectiveness (+12.3%) and utility (+6.8%). Our lightweight\ndesign supports direct deployment on edge devices, avoiding cloud reliance and\ncommunication-based privacy risks. Our code is open-source at\nhttps://github.com/tsinghua-fib-lab/AgentStealth.", "AI": {"tldr": "A framework called AgentStealth is proposed for effective text anonymization using locally deployed smaller-scale language models, addressing privacy risks associated with cloud-based solutions.", "motivation": "To tackle risks of exposing sensitive personal attributes in user-generated content through effective text anonymization methods that do not rely on costly cloud-based LLMs or rigid replacements that harm utility.", "method": "The paper introduces a self-reinforcing LLM anonymization framework that utilizes an adversarial anonymization workflow, In-context Contrastive Learning, and Adaptive Utility-Aware Control, along with supervised adaptation of SLMs and online reinforcement learning.", "result": "AgentStealth outperforms existing anonymization techniques with a 12.3% improvement in anonymization effectiveness and a 6.8% improvement in utility across two tested datasets.", "conclusion": "The lightweight nature of AgentStealth enables it to be deployed directly on edge devices, reducing privacy risks associated with cloud service reliance while enhancing text anonymization effectiveness.", "key_contributions": ["Development of AgentStealth framework for anonymization using SLMs.", "Introduction of an adversarial workflow and adaptive control mechanisms.", "Demonstration of improved anonymization and utility through experimental results."], "limitations": "", "keywords": ["text anonymization", "privacy protection", "language models", "human-computer interaction", "reinforcement learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.22932", "pdf": "https://arxiv.org/pdf/2506.22932.pdf", "abs": "https://arxiv.org/abs/2506.22932", "title": "Immersive Technologies and Elderly Users: Current use, Limitations and Future Perspectives", "authors": ["Zoe Anastasiadou", "Andreas Lanitis"], "categories": ["cs.HC"], "comment": "13 pages, 2 figures", "summary": "The increase of the percentage of elderly population in modern societies\ndictates the use of emerging technologies as a means of supporting elder\nmembers of the society. Within this scope, Extended Reality (XR) technologies\npose as a promising technology for improving the daily lives of the elderly\npopulation. This paper presents a literature review that describes the most\ncommon characteristics of the physical and mental state of the elderly,\nallowing readers, and specifically XR developers, to understand the main\ndifficulties faced by elderly users of extended reality applications so they\ncan develop accessible, user friendly and engaging applications for the target\naudience. Furthermore, a review of existing extended reality applications that\ntarget the elder population is presented, allowing readers to get acquainted\nwith existing design paradigms that can inspire future developments.", "AI": {"tldr": "This paper reviews Extended Reality (XR) technologies to support the elderly by analyzing their physical and mental states, and existing XR applications.", "motivation": "To address the increasing percentage of elderly individuals in society and improve their quality of life through emerging technologies.", "method": "Literature review of elderly characteristics and existing extended reality applications targeting the elderly population.", "result": "Identification of difficulties faced by elderly users in XR, and presentation of design paradigms for developing user-friendly XR applications.", "conclusion": "XR developers can enhance the accessibility and engagement of applications for the elderly by understanding their unique needs and reviewing existing solutions.", "key_contributions": ["Comprehensive analysis of elderly users' challenges with XR technologies.", "Review of existing XR applications aimed at elderly users.", "Guidelines for creating accessible and engaging XR applications for the elderly."], "limitations": "", "keywords": ["Elderly", "Extended Reality", "User Experience", "Accessibility", "Human-Computer Interaction"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2506.22510", "pdf": "https://arxiv.org/pdf/2506.22510.pdf", "abs": "https://arxiv.org/abs/2506.22510", "title": "Towards Text-free Graph Foundation Models: Rethinking Multi-Domain Graph Contrastive Learning", "authors": ["Zihao Zhao", "Xinlong Zhai", "Jinyu Yang", "Chuan Shi"], "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 5 figures", "summary": "Foundation models have achieved great success in natural language processing\n(NLP) and computer vision (CV). Their success largely stems from the ability to\nintegrate multi-domain knowledge in pre-training and transfer it to target\ndomains. Considering graph data, especially graphs without textual features, is\nubiquitous in real-world applications such as social networks and\nrecommendation systems, some researchers have attempted to extend this paradigm\nto the graph field, aiming to construct graph foundation models. However,\nunlike CV and NLP, there are huge gaps among the semantics and properties of\ngraphs in different domains, while current works still adopt traditional\ncontrastive pre-training strategies designed in the single-domain scenario,\nwhich regard contrastive samples from different domains as equivalent. From\nexperimental investigations, we discovered that inherent domain-specific\ndifferences prevent these strategies from effectively absorbing knowledge from\ndifferent domains to generate informative representations. In this paper, we\npropose a novel multi-domain pre-training and cross-domain transfer framework,\nnamely MDGCL.In the pre-training stage, we design a contrastive learning\nstrategy to substantially recognize and capture domain differences, and\nintroduce domain tokens to encode domain-level global information. In the\ndownstream stage, we introduce a domain attention mechanism to enable\nfine-grained domain knowledge transfer. Extensive experiments on five benchmark\ndatasets have demonstrated that our method outperforms state-of-the-art\nsignificantly, with the maximum improvement of 19.33\\% on accuracy and 19.13\\%\non Macro-F1 score.", "AI": {"tldr": "This paper introduces MDGCL, a framework for multi-domain pre-training and cross-domain transfer in graph data, addressing limitations of existing contrastive learning strategies.", "motivation": "The need to effectively integrate multi-domain knowledge in graph foundation models due to the unique semantics and properties of graph data across different domains.", "method": "A novel contrastive learning strategy is employed in the pre-training phase to capture domain differences, alongside the use of domain tokens and a domain attention mechanism for fine-grained transfer during the downstream phase.", "result": "Extensive experiments show that MDGCL outperforms state-of-the-art methods with notable improvements in accuracy (up to 19.33%) and Macro-F1 score (up to 19.13%).", "conclusion": "The proposed framework significantly enhances the performance of graph models by recognizing and utilizing domain-specific information more effectively than conventional approaches.", "key_contributions": ["Introduction of a contrastive learning strategy focused on domain differences", "Development of domain tokens for encoding global information", "Implementation of a domain attention mechanism for enhanced knowledge transfer"], "limitations": "", "keywords": ["graph foundation models", "multi-domain pre-training", "contrastive learning"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.22937", "pdf": "https://arxiv.org/pdf/2506.22937.pdf", "abs": "https://arxiv.org/abs/2506.22937", "title": "GamerAstra: Enhancing Video Game Accessibility for Blind and Low-Vision Players through a Multi-Agent AI Framework", "authors": ["Tianrun Qiu", "Changxin Chen", "Sizhe Cheng", "Yiming Yang", "Yixiao Guo", "Zhicong Lu", "Yuxin Ma"], "categories": ["cs.HC", "H.5.2"], "comment": "19 pages, 9 figures", "summary": "Blind and low-vision (BLV) players encounter critical challenges in engaging\nwith video games due to the inaccessibility of visual elements, difficulties in\nnavigating interfaces, and limitations in sending interaction input. Moreover,\nthe development of specialized accessibility features typically requires\nsubstantial programming effort and is often implemented on a game-by-game\nbasis. To address these challenges, we introduce \\textit{GamerAstra}, a\ngeneralized accessibility framework that leverages a multi-agent design to\nfacilitate access to video games for BLV players. It integrates multi-modal\ntechniques including large language models and vision-language models, enabling\ninteraction with games lacking native accessibility support. The framework\nfurther incorporates customizable assistance granularities to support varying\ndegrees of visual impairment and enhances interface navigation through multiple\ninput modalities. The evaluation through technical assessments and user studies\nindicate that \\textit{GamerAstra} effectively enhances playability and delivers\na more immersive gaming experience for BLV players. These findings also\nunderscore potential avenues for advancing intelligent accessibility frameworks\nin the gaming domain.", "AI": {"tldr": "GamerAstra is a multi-agent accessibility framework designed to improve video game access for blind and low-vision (BLV) players by integrating advanced multi-modal techniques and customizable assistance.", "motivation": "To address the critical challenges faced by blind and low-vision players in engaging with video games due to inaccessibility and interface navigation issues.", "method": "The framework leverages multi-modal techniques including large language models and vision-language models to facilitate interaction with games lacking native accessibility support and incorporates customizable assistance granularities.", "result": "Evaluation through technical assessments and user studies indicates that GamerAstra effectively enhances playability and delivers a more immersive gaming experience for BLV players.", "conclusion": "The findings underscore potential avenues for advancing intelligent accessibility frameworks in the gaming domain.", "key_contributions": ["Introduction of GamerAstra, a generalized accessibility framework for BLV players", "Integration of multi-modal techniques, including LLMs and vision-language models", "Customizable assistance to support varying degrees of visual impairment"], "limitations": "The development requires substantial programming effort and is often implemented on a game-by-game basis.", "keywords": ["Accessibility", "Blind and Low-Vision", "Video Games", "Multi-Agent Design", "Human-Computer Interaction"], "importance_score": 8, "read_time_minutes": 19}}
{"id": "2506.22516", "pdf": "https://arxiv.org/pdf/2506.22516.pdf", "abs": "https://arxiv.org/abs/2506.22516", "title": "Can \"consciousness\" be observed from large language model (LLM) internal states? Dissecting LLM representations obtained from Theory of Mind test with Integrated Information Theory and Span Representation analysis", "authors": ["Jingkai Li"], "categories": ["cs.CL", "cs.AI", "cs.NE", "q-bio.NC"], "comment": "Published as a journal paper at:\n  https://doi.org/10.1016/j.nlp.2025.100163", "summary": "Integrated Information Theory (IIT) provides a quantitative framework for\nexplaining consciousness phenomenon, positing that conscious systems comprise\nelements integrated through causal properties. We apply IIT 3.0 and 4.0 -- the\nlatest iterations of this framework -- to sequences of Large Language Model\n(LLM) representations, analyzing data derived from existing Theory of Mind\n(ToM) test results. Our study systematically investigates whether the\ndifferences of ToM test performances, when presented in the LLM\nrepresentations, can be revealed by IIT estimates, i.e., $\\Phi^{\\max}$ (IIT\n3.0), $\\Phi$ (IIT 4.0), Conceptual Information (IIT 3.0), and $\\Phi$-structure\n(IIT 4.0). Furthermore, we compare these metrics with the Span Representations\nindependent of any estimate for consciousness. This additional effort aims to\ndifferentiate between potential \"consciousness\" phenomena and inherent\nseparations within LLM representational space. We conduct comprehensive\nexperiments examining variations across LLM transformer layers and linguistic\nspans from stimuli. Our results suggest that sequences of contemporary\nTransformer-based LLM representations lack statistically significant indicators\nof observed \"consciousness\" phenomena but exhibit intriguing patterns under\n$\\textit{spatio}$-permutational analyses. The Appendix and code are available\nas Supplementary Materials at: https://doi.org/10.1016/j.nlp.2025.100163.", "AI": {"tldr": "The paper investigates the application of Integrated Information Theory (IIT) on Large Language Models (LLMs) to explore consciousness phenomena, finding no significant indicators of consciousness in LLM representations.", "motivation": "To analyze whether Integrated Information Theory can reveal differences in Theory of Mind test performances in LLM representations and differentiate consciousness phenomena from inherent separations in LLMs.", "method": "The study applies IIT 3.0 and 4.0 to LLM representations derived from ToM test results, analyzing transformer layer variations and linguistic spans to study potential consciousness indicators.", "result": "Results indicate that Transformer-based LLM representations do not show statistically significant consciousness indicators but present interesting patterns under spatio-permutational analyses.", "conclusion": "The findings suggest that LLM representations may not be indicative of consciousness phenomena as defined by IIT, despite intriguing patterns present in their structure.", "key_contributions": ["Application of IIT on LLMs to evaluate consciousness phenomena", "Comparison of IIT measures with Span Representations", "Investigation of transformer layer variations and linguistic spans in relation to ToM test results"], "limitations": "The study's results reflect a specific set of data and may not generalize to all forms or contexts of LLMs and consciousness.", "keywords": ["Integrated Information Theory", "Large Language Models", "Theory of Mind", "consciousness", "spatio-permutational analyses"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2506.22940", "pdf": "https://arxiv.org/pdf/2506.22940.pdf", "abs": "https://arxiv.org/abs/2506.22940", "title": "Context, Credibility, and Control: User Reflections on AI Assisted Misinformation Tools", "authors": ["Varun Sangwan", "Heidi Makitalo"], "categories": ["cs.HC", "cs.SI"], "comment": null, "summary": "This paper investigates how collaborative AI systems can enhance user agency\nin identifying and evaluating misinformation on social media platforms.\nTraditional methods, such as personal judgment or basic fact-checking, often\nfall short when faced with emotionally charged or context-deficient content. To\naddress this, we designed and evaluated an interactive interface that\nintegrates collaborative AI features, including real-time explanations, source\naggregation, and debate-style interaction. These elements aim to support\ncritical thinking by providing contextual cues and argumentative reasoning in a\ntransparent, user-centered format. In a user study with 14 participants, 79%\nfound the debate mode more effective than standard chatbot interfaces, and the\nmultiple-source view received an average usefulness rating of 4.6 out of 5. Our\nfindings highlight the potential of context-rich, dialogic AI systems to\nimprove media literacy and foster trust in digital information environments. We\nargue that future tools for misinformation mitigation should prioritize ethical\ndesign, explainability, and interactive engagement to empower users in a\npost-truth era.", "AI": {"tldr": "The paper explores a collaborative AI system designed to enhance user agency in evaluating misinformation on social media by integrating interactive features for improved critical thinking.", "motivation": "To improve methods for identifying and evaluating misinformation on social media, which often fail with emotionally charged content.", "method": "Designed and evaluated an interactive interface with collaborative AI features including real-time explanations, source aggregation, and debate-style interactions.", "result": "In a user study, 79% of participants preferred the debate mode over standard chatbots, and the multiple-source view received a high usefulness rating of 4.6 out of 5.", "conclusion": "The study demonstrates that context-rich, dialogic AI systems can enhance media literacy and trust in digital environments, emphasizing the need for ethical and interactive design in future misinformation tools.", "key_contributions": ["Development of an interactive interface incorporating collaborative AI for misinformation evaluation.", "Demonstrated effectiveness of a debate-style interaction for improving user engagement and critical thinking.", "Provided empirical evidence on the usefulness of a multiple-source view for evaluating information."], "limitations": "", "keywords": ["Collaborative AI", "Misinformation", "User Agency", "Media Literacy", "Interactive Systems"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.22518", "pdf": "https://arxiv.org/pdf/2506.22518.pdf", "abs": "https://arxiv.org/abs/2506.22518", "title": "Weak-to-Strong GraphRAG: Aligning Weak Retrievers with Large Language Models for Graph-based Retrieval Augmented Generation", "authors": ["Deyu Zou", "Yongqiang Chen", "Mufei Li", "Siqi Miao", "Chenxi Liu", "Bo Han", "James Cheng", "Pan Li"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Graph-based retrieval-augmented generation (RAG) enables large language\nmodels (LLMs) to ground responses with structured external knowledge from\nup-to-date knowledge graphs (KGs) and reduce hallucinations. However, LLMs\noften rely on a weak retriever in graph-based RAG: I) Due to the lack of ground\ntruth, the retriever is often trained on weak supervision, which often\nintroduces spurious signals to the LLMs. II) Due to the abstraction of graph\ndata, the retrieved knowledge is often presented in unorganized forms. To\nmitigate the issue, we present Refined Graph-based RAG (ReG) to align weak\nretrievers to LLMs for graph-based RAG. Specifically, ReG incorporates LLM\nfeedback to get rid of spurious signals and improve the quality of the\nsupervision. Meanwhile, ReG introduces a structure-aware reorganization module\nto refactor the retrieval results into logically coherent evidence chains.\nExperiments on prominent benchmarks demonstrate that ReG significantly and\nconsistently brings improvements across different LLM backbones by up to 10%.\nThe improved supervision quality enables ReG to match the state-of-the-art\nperformance with 5% training data and to transfer to out-of-distribution KGs.\nNotably, when adopted to reasoning-based LLMs, ReG reduces the reasoning token\ncost by up to 30% and improves the performance by up to 4%.", "AI": {"tldr": "This paper presents Refined Graph-based RAG (ReG), a method to enhance the performance of graph-based retrieval-augmented generation using large language models by improving the quality of retriever supervision and reorganizing retrieved knowledge into coherent structures.", "motivation": "The existing graph-based RAG methods struggle with weak retrievers that introduce noise and poorly structured knowledge, limiting the effectiveness of large language models.", "method": "ReG uses LLM feedback to improve supervision quality and includes a structure-aware reorganization module to create coherent evidence chains from retrieved knowledge.", "result": "Experiments show that ReG improves performance by up to 10% across various LLM backbones, achieves state-of-the-art results with only 5% of the training data, and reduces reasoning token costs by up to 30%.", "conclusion": "ReG effectively enhances graph-based RAG by refining retriever reliability and organizing retrieved data, thus benefiting reasoning-based LLMs.", "key_contributions": ["Introduction of ReG for better alignment of weak retrievers with LLMs", "Structure-aware reorganization of retrieval results into coherent evidence chains", "Demonstrated significant performance improvements with reduced training data and reasoning costs"], "limitations": "", "keywords": ["Graph-based RAG", "Large Language Models", "Weak Retrievers"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.22941", "pdf": "https://arxiv.org/pdf/2506.22941.pdf", "abs": "https://arxiv.org/abs/2506.22941", "title": "Positioning AI Tools to Support Online Harm Reduction Practice: Applications and Design Directions", "authors": ["Kaixuan Wang", "Jason T. Jacques", "Chenxin Diao"], "categories": ["cs.HC", "cs.AI"], "comment": "16 pages, 4 figures, with appendix", "summary": "Access to accurate and actionable harm reduction information can directly\nimpact the health outcomes of People Who Use Drugs (PWUD), yet existing online\nchannels often fail to meet their diverse and dynamic needs due to limitations\nin adaptability, accessibility, and the pervasive impact of stigma. Large\nLanguage Models (LLMs) present a novel opportunity to enhance information\nprovision, but their application in such a high-stakes domain is under-explored\nand presents socio-technical challenges. This paper investigates how LLMs can\nbe responsibly designed to support the information needs of PWUD. Through a\nqualitative workshop involving diverse stakeholder groups (academics, harm\nreduction practitioners, and an online community moderator), we explored LLM\ncapabilities, identified potential use cases, and delineated core design\nconsiderations. Our findings reveal that while LLMs can address some existing\ninformation barriers (e.g., by offering responsive, multilingual, and\npotentially less stigmatising interactions), their effectiveness is contingent\nupon overcoming challenges related to ethical alignment with harm reduction\nprinciples, nuanced contextual understanding, effective communication, and\nclearly defined operational boundaries. We articulate design pathways\nemphasising collaborative co-design with experts and PWUD to develop LLM\nsystems that are helpful, safe, and responsibly governed. This work contributes\nempirically grounded insights and actionable design considerations for the\nresponsible development of LLMs as supportive tools within the harm reduction\necosystem.", "AI": {"tldr": "The paper explores how Large Language Models (LLMs) can be responsibly designed to improve harm reduction information for People Who Use Drugs (PWUD), revealing both potential benefits and challenges.", "motivation": "There is a critical need for accurate harm reduction information for PWUD, which is often unmet by existing online resources due to stigma and accessibility issues.", "method": "The study used qualitative workshops with stakeholders, including academics and harm reduction practitioners, to investigate LLM capabilities and design considerations.", "result": "Findings highlight that LLMs can offer multilingual, responsive information and mitigate stigma, but require careful ethical and contextual alignment to be effective.", "conclusion": "The paper emphasizes the importance of collaborative co-design for LLMs to ensure they are helpful, safe, and aligned with harm reduction principles.", "key_contributions": ["Investigates LLM application in harm reduction for PWUD", "Identifies design considerations for effective LLM usage", "Suggests collaborative co-design pathways with stakeholders"], "limitations": "The paper does not provide quantitative measures of LLM effectiveness and relies on qualitative insights.", "keywords": ["Large Language Models", "harm reduction", "People Who Use Drugs", "co-design", "information accessibility"], "importance_score": 7, "read_time_minutes": 20}}
{"id": "2506.22529", "pdf": "https://arxiv.org/pdf/2506.22529.pdf", "abs": "https://arxiv.org/abs/2506.22529", "title": "MisinfoTeleGraph: Network-driven Misinformation Detection for German Telegram Messages", "authors": ["Lu Kalkbrenner", "Veronika Solopova", "Steffen Zeiler", "Robert Nickel", "Dorothea Kolossa"], "categories": ["cs.CL"], "comment": null, "summary": "Connectivity and message propagation are central, yet often underutilized,\nsources of information in misinformation detection -- especially on poorly\nmoderated platforms such as Telegram, which has become a critical channel for\nmisinformation dissemination, namely in the German electoral context. In this\npaper, we introduce Misinfo-TeleGraph, the first German-language Telegram-based\ngraph dataset for misinformation detection. It includes over 5 million messages\nfrom public channels, enriched with metadata, channel relationships, and both\nweak and strong labels. These labels are derived via semantic similarity to\nfact-checks and news articles using M3-embeddings, as well as manual\nannotation. To establish reproducible baselines, we evaluate both text-only\nmodels and graph neural networks (GNNs) that incorporate message forwarding as\na network structure. Our results show that GraphSAGE with LSTM aggregation\nsignificantly outperforms text-only baselines in terms of Matthews Correlation\nCoefficient (MCC) and F1-score. We further evaluate the impact of subscribers,\nview counts, and automatically versus human-created labels on performance, and\nhighlight both the potential and challenges of weak supervision in this domain.\nThis work provides a reproducible benchmark and open dataset for future\nresearch on misinformation detection in German-language Telegram networks and\nother low-moderation social platforms.", "AI": {"tldr": "Introduction of a German-language Telegram-based graph dataset for misinformation detection with evaluation of models.", "motivation": "To investigate the role of connectivity and message propagation in misinformation detection on platforms like Telegram, particularly in the German electoral context.", "method": "The study utilizes a dataset of over 5 million messages with metadata and labels derived from semantic similarity and manual annotation, evaluating text-only models and graph neural networks (GNNs) with a focus on message forwarding as a network structure.", "result": "GraphSAGE with LSTM aggregation outperforms text-only models based on Matthews Correlation Coefficient (MCC) and F1-score, revealing insights into the effectiveness of weak supervision.", "conclusion": "The work offers a reproducible benchmark and open dataset that can be used for further research on misinformation detection on low-moderation social platforms.", "key_contributions": ["Creation of the Misinfo-TeleGraph dataset for German-language Telegram", "Performance evaluation of Text-only models vs Graph Neural Networks", "Insights into weak supervision impact on model performance"], "limitations": "The study primarily focuses on German-language content; results may not generalize to other languages or platforms.", "keywords": ["misinformation detection", "Telegram dataset", "graph neural networks", "weak supervision", "German electoral context"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.22968", "pdf": "https://arxiv.org/pdf/2506.22968.pdf", "abs": "https://arxiv.org/abs/2506.22968", "title": "Against 'softmaxing' culture", "authors": ["Daniel Mwesigwa"], "categories": ["cs.HC", "cs.AI"], "comment": "7 pages", "summary": "AI is flattening culture. Evaluations of \"culture\" are showing the myriad\nways in which large AI models are homogenizing language and culture, averaging\nout rich linguistic differences into generic expressions. I call this\nphenomenon \"softmaxing culture,\" and it is one of the fundamental challenges\nfacing AI evaluations today. Efforts to improve and strengthen evaluations of\nculture are central to the project of cultural alignment in large AI systems.\nThis position paper argues that machine learning (ML) and human-computer\ninteraction (HCI) approaches to evaluation are limited. I propose two key\nshifts. First, instead of asking \"what is culture?\" at the start of system\nevaluations, I propose beginning with the question: \"when is culture?\" Second,\nwhile I acknowledge the philosophical claim that cultural universals exist, the\nchallenge is not simply to describe them, but to situate them in relation to\ntheir particulars. Taken together, these conceptual shifts invite evaluation\napproaches that move beyond technical requirements, toward perspectives more\nresponsive to the complexities of culture.", "AI": {"tldr": "The paper discusses how AI, particularly large models, is homogenizing cultural expressions and proposes a new approach for evaluating AI systems' cultural alignment.", "motivation": "To address the cultural homogenization caused by AI models and improve the evaluation of culture in AI systems.", "method": "The paper suggests shifting the foundational question of evaluations from 'what is culture?' to 'when is culture?' and emphasizes situating cultural universals within their specific contexts.", "result": "Proposes a new framework for evaluating cultural aspects in AI, indicating that existing ML and HCI approaches are limited.", "conclusion": "Cultural evaluations should move beyond technical paradigms to more accurately reflect the complexities of culture.", "key_contributions": ["Introduces the concept of 'softmaxing culture' in AI evaluations.", "Advocates for a shift in evaluative questions regarding culture in AI systems.", "Emphasizes the need to relate cultural universals to their specific contexts."], "limitations": "The paper is primarily conceptual and does not provide empirical data supporting the proposed shifts.", "keywords": ["Culture", "AI", "Evaluation", "Machine Learning", "Human-Computer Interaction"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.22598", "pdf": "https://arxiv.org/pdf/2506.22598.pdf", "abs": "https://arxiv.org/abs/2506.22598", "title": "RExBench: Can coding agents autonomously implement AI research extensions?", "authors": ["Nicholas Edwards", "Yukyung Lee", "Yujun", "Mao", "Yulu Qin", "Sebastian Schuster", "Najoung Kim"], "categories": ["cs.CL"], "comment": null, "summary": "Agents based on Large Language Models (LLMs) have shown promise for\nperforming sophisticated software engineering tasks autonomously. In addition,\nthere has been progress towards developing agents that can perform parts of the\nresearch pipeline in machine learning and the natural sciences. We argue that\nresearch extension and its implementation is a critical capability for such\nsystems, and introduce RExBench to support the evaluation of this capability.\nRExBench is a benchmark consisting of 12 realistic research experiment\nimplementation tasks that aim to investigate research hypotheses that have not\npreviously been implemented. Each task is set up as an extension to an existing\nresearch paper and codebase, accompanied by domain expert-written instructions.\nRExBench is robust to data contamination, and supports an automatic evaluation\ninfrastructure that executes agent outputs to determine whether the success\ncriteria are met. We use this benchmark to evaluate nine LLM agents implemented\nusing three different frameworks: aider, Claude Code, and OpenHands. We find\nthat all agents evaluated fail to autonomously implement the majority of the\nextensions. Although the success rate improves with additional human-written\nhints, the best performance under this setting remains below 40%. This\nindicates that current agents are still short of being able to handle realistic\nresearch extension tasks without substantial human guidance.", "AI": {"tldr": "RExBench is a benchmark for evaluating LLM agents on research implementation tasks, revealing significant limitations in their autonomous capabilities.", "motivation": "To investigate the ability of LLM agents to autonomously perform research extension tasks in machine learning and natural sciences.", "method": "The study introduces RExBench, a benchmark consisting of 12 realistic tasks designed to evaluate research hypotheses through the implementation of extensions to existing research papers and codebases.", "result": "Nine LLM agents were evaluated using RExBench, showing that they fail to autonomously complete most tasks, with a peak success rate of under 40% even with additional human hints.", "conclusion": "Current LLM agents require substantial human guidance to handle realistic research extension tasks effectively.", "key_contributions": ["Introduction of RExBench, a novel benchmark for research implementation", "Evaluation of nine LLM agents across three frameworks", "Demonstration of the need for human guidance in LLM-driven research tasks."], "limitations": "Agents could not autonomously implement the majority of tasks; performance improved with hints but remained inadequate.", "keywords": ["Large Language Models", "research extension", "benchmark", "autonomous agents", "machine learning"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.23016", "pdf": "https://arxiv.org/pdf/2506.23016.pdf", "abs": "https://arxiv.org/abs/2506.23016", "title": "Deep Learning in Mild Cognitive Impairment Diagnosis using Eye Movements and Image Content in Visual Memory Tasks", "authors": ["Tomás Silva Santos Rocha", "Anastasiia Mikhailova", "Moreno I. Coco", "José Santos-Victor"], "categories": ["cs.HC", "cs.CV"], "comment": "13 pages, 5 figures", "summary": "The global prevalence of dementia is projected to double by 2050,\nhighlighting the urgent need for scalable diagnostic tools. This study utilizes\ndigital cognitive tasks with eye-tracking data correlated with memory processes\nto distinguish between Healthy Controls (HC) and Mild Cognitive Impairment\n(MCI), a precursor to dementia. A deep learning model based on VTNet was\ntrained using eye-tracking data from 44 participants (24 MCI, 20 HCs) who\nperformed a visual memory task. The model utilizes both time series and spatial\ndata derived from eye-tracking. It was modified to incorporate scan paths, heat\nmaps, and image content. These modifications also enabled testing parameters\nsuch as image resolution and task performance, analyzing their impact on model\nperformance. The best model, utilizing $700\\times700px$ resolution heatmaps,\nachieved 68% sensitivity and 76% specificity. Despite operating under more\nchallenging conditions (e.g., smaller dataset size, shorter task duration, or a\nless standardized task), the model's performance is comparable to an\nAlzheimer's study using similar methods (70% sensitivity and 73% specificity).\nThese findings contribute to the development of automated diagnostic tools for\nMCI. Future work should focus on refining the model and using a standardized\nlong-term visual memory task.", "AI": {"tldr": "The study develops a deep learning model using eye-tracking data to differentiate between Healthy Controls and individuals with Mild Cognitive Impairment, aiming to aid in early dementia diagnosis.", "motivation": "The growing prevalence of dementia necessitates the development of effective diagnostic tools, particularly for Mild Cognitive Impairment (MCI), which is an early indicator of dementia.", "method": "The research involves a deep learning model, VTNet, trained on eye-tracking data from 44 participants performing a visual memory task, with enhancements to include scan paths and heat maps.", "result": "The model achieved a sensitivity of 68% and a specificity of 76% despite working with a smaller and less standardized dataset compared to previous studies.", "conclusion": "The study demonstrates that eye-tracking combined with deep learning may serve as a viable method for developing diagnostic tools for MCI, with future research needed to refine the approach.", "key_contributions": ["Development of a deep learning model for MCI diagnosis using eye-tracking data", "Incorporation of scan paths and heat maps for improved analysis", "Evaluation of image resolution and task performance parameters on model efficacy"], "limitations": "The sample size was small, and the task duration was shorter and less standardized than typical studies.", "keywords": ["Mild Cognitive Impairment", "dementia", "deep learning", "eye-tracking", "diagnostic tools"], "importance_score": 8, "read_time_minutes": 13}}
{"id": "2506.22623", "pdf": "https://arxiv.org/pdf/2506.22623.pdf", "abs": "https://arxiv.org/abs/2506.22623", "title": "Temperature Matters: Enhancing Watermark Robustness Against Paraphrasing Attacks", "authors": ["Badr Youbi Idrissi", "Monica Millunzi", "Amelia Sorrenti", "Lorenzo Baraldi", "Daryna Dementieva"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In the present-day scenario, Large Language Models (LLMs) are establishing\ntheir presence as powerful instruments permeating various sectors of society.\nWhile their utility offers valuable support to individuals, there are multiple\nconcerns over potential misuse. Consequently, some academic endeavors have\nsought to introduce watermarking techniques, characterized by the inclusion of\nmarkers within machine-generated text, to facilitate algorithmic\nidentification. This research project is focused on the development of a novel\nmethodology for the detection of synthetic text, with the overarching goal of\nensuring the ethical application of LLMs in AI-driven text generation. The\ninvestigation commences with replicating findings from a previous baseline\nstudy, thereby underscoring its susceptibility to variations in the underlying\ngeneration model. Subsequently, we propose an innovative watermarking approach\nand subject it to rigorous evaluation, employing paraphrased generated text to\nasses its robustness. Experimental results highlight the robustness of our\nproposal compared to the~\\cite{aarson} watermarking method.", "AI": {"tldr": "The paper presents a new watermarking technique for detecting synthetic text generated by Large Language Models (LLMs), addressing ethical concerns regarding their misuse.", "motivation": "To ensure the ethical application of LLMs in AI-driven text generation by developing a reliable detection methodology.", "method": "The study replicates findings from a baseline study, develops a novel watermarking technique, and evaluates its robustness against generative text variations through experiments.", "result": "The proposed watermarking approach demonstrates greater robustness compared to an existing method, indicating its effectiveness in identifying synthetic text.", "conclusion": "The new watermarking technique offers a promising solution to enhance the ethical use of LLMs in text generation.", "key_contributions": ["Development of a novel watermarking approach for detecting synthetic text.", "Rigorous evaluation of the watermarking approach against paraphrased generated text.", "Improved robustness over existing watermarking methods."], "limitations": "", "keywords": ["Large Language Models", "watermarking", "synthetic text", "ethical AI", "text generation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.23017", "pdf": "https://arxiv.org/pdf/2506.23017.pdf", "abs": "https://arxiv.org/abs/2506.23017", "title": "Mind the Dark: A Gamified Exploration of Deceptive Design Awareness for Children in the Digital Age", "authors": ["Noverah Khan", "Hira Eiraj Daud", "Suleman Shahid"], "categories": ["cs.HC"], "comment": null, "summary": "This paper addresses the critical issue of deceptive design elements\nprevalent in technology, and their potential impact on children. Recent\nresearch highlights the impact of dark patterns on adults and adolescents,\nwhile studies involving children are scarce. In an era where children wield\ngreater independence with digital devices, their vulnerability to dark patterns\namplifies without early education. Our findings show a significant positive\nimpact of dark pattern education on children's awareness, revealing that\nheightened awareness considerably alters children's navigation of social media,\nvideo games, and streaming platforms. To this end, we developed a gamified\napplication aimed at instructing children on identifying and responding to\nvarious dark patterns. Our evaluation results emphasize the critical role of\nearly education in empowering children to recognize and counter deceptive\ndesign, thereby cultivating a digitally literate generation capable of making\ninformed choices in the complex landscape of digital technology.", "AI": {"tldr": "This paper explores the impact of dark patterns on children and presents a gamified application designed to educate them on recognizing deceptive design elements.", "motivation": "The paper aims to address the lack of research on the effects of deceptive design elements, specifically dark patterns, on children, highlighting their increased vulnerability in a digital environment.", "method": "The study involved the development of a gamified application that educates children on identifying dark patterns and an evaluation of its effectiveness in increasing children's awareness.", "result": "Findings showed that education on dark patterns significantly heightened children's awareness, positively influencing their navigation of social media, video games, and streaming platforms.", "conclusion": "Early education on dark patterns is crucial for empowering children to recognize and counter deceptive design, ultimately fostering digital literacy.", "key_contributions": ["Developed a gamified application for teaching children about dark patterns.", "Demonstrated the effectiveness of early education in altering children's navigation behavior.", "Provided insights into the lack of research on dark patterns affecting children."], "limitations": "The evaluation might be limited to specific demographic groups or contexts, which may affect the generalizability of the findings.", "keywords": ["dark patterns", "deceptive design", "children", "digital literacy", "gamification"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.22644", "pdf": "https://arxiv.org/pdf/2506.22644.pdf", "abs": "https://arxiv.org/abs/2506.22644", "title": "Evaluating Hybrid Retrieval Augmented Generation using Dynamic Test Sets: LiveRAG Challenge", "authors": ["Chase Fensore", "Kaustubh Dhole", "Joyce C Ho", "Eugene Agichtein"], "categories": ["cs.CL", "cs.IR"], "comment": "4 pages, 3 tables, 2 figures. Accepted at the SIGIR LiveRAG Workshop\n  2025 (Submission 2664)", "summary": "We present our submission to the LiveRAG Challenge 2025, which evaluates\nretrieval-augmented generation (RAG) systems on dynamic test sets using the\nFineWeb-10BT corpus. Our final hybrid approach combines sparse (BM25) and dense\n(E5) retrieval methods and then aims to generate relevant and faithful answers\nwith Falcon3-10B-Instruct. Through systematic evaluation on 200 synthetic\nquestions generated with DataMorgana across 64 unique question-user\ncombinations, we demonstrate that neural re-ranking with RankLLaMA improves MAP\nfrom 0.523 to 0.797 (52% relative improvement) but introduces prohibitive\ncomputational costs (84s vs 1.74s per question). While DSPy-optimized prompting\nstrategies achieved higher semantic similarity (0.771 vs 0.668), their 0%\nrefusal rates raised concerns about over-confidence and generalizability. Our\nsubmitted hybrid system without re-ranking achieved 4th place in faithfulness\nand 11th place in correctness among 25 teams. Analysis across question\ncategories reveals that vocabulary alignment between questions and documents\nwas the strongest predictor of performance on our development set, with\ndocument-similar phrasing improving cosine similarity from 0.562 to 0.762.", "AI": {"tldr": "This paper discusses a hybrid approach for retrieval-augmented generation (RAG) systems used in the LiveRAG Challenge 2025, highlighting methods and performance metrics.", "motivation": "To develop an effective RAG system that addresses the challenges posed by dynamic test sets and improves the generation of relevant and faithful answers.", "method": "A hybrid retrieval approach combining sparse (BM25) and dense (E5) retrieval techniques, followed by generation using Falcon3-10B-Instruct, evaluated on synthetic questions generated through DataMorgana.", "result": "The proposed hybrid system achieved significant improvements in mean average precision (MAP) and achieved notable rankings in faithfulness and correctness among competitors, though it faced challenges with computational costs and reliability of responses.", "conclusion": "The analysis indicates that vocabulary alignment between questions and documents is crucial for performance, and while heuristics like neural re-ranking can enhance results, they may introduce computational trade-offs.", "key_contributions": ["Development of a hybrid RAG system combining multi-modal retrieval methods", "Demonstrated significant performance improvement through neural re-ranking", "Insights on vocabulary alignment as a predictor of model performance"], "limitations": "High computational costs associated with neural re-ranking and concerns about the reliability of generated answers at 0% refusal rates.", "keywords": ["retrieval-augmented generation", "RAG", "neural re-ranking", "vocabulary alignment", "machine learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.23075", "pdf": "https://arxiv.org/pdf/2506.23075.pdf", "abs": "https://arxiv.org/abs/2506.23075", "title": "CSBrain: A Cross-scale Spatiotemporal Brain Foundation Model for EEG Decoding", "authors": ["Yuchen Zhou", "Jiamin Wu", "Zichen Ren", "Zhouheng Yao", "Weiheng Lu", "Kunyu Peng", "Qihao Zheng", "Chunfeng Song", "Wanli Ouyang", "Chao Gou"], "categories": ["cs.HC", "cs.LG", "eess.SP", "q-bio.NC"], "comment": null, "summary": "Understanding and decoding brain activity from electroencephalography (EEG)\nsignals is a fundamental challenge in neuroscience and AI, with applications in\ncognition, emotion recognition, diagnosis, and brain-computer interfaces. While\nrecent EEG foundation models advance generalized decoding via unified\narchitectures and large-scale pretraining, they adopt a scale-agnostic dense\nmodeling paradigm inherited from NLP and vision. This design neglects a core\nproperty of neural activity: cross-scale spatiotemporal structure. EEG task\npatterns span a wide range of temporal and spatial scales, from short bursts to\nslow rhythms, and from localized cortical responses to distributed\ninteractions. Ignoring this diversity leads to suboptimal representations and\nweak generalization. We propose CSBrain, a Cross-scale Spatiotemporal Brain\nfoundation model for generalized EEG decoding. CSBrain introduces: (i)\nCross-scale Spatiotemporal Tokenization (CST), which aggregates multi-scale\nfeatures from localized temporal windows and anatomical brain regions into\ncompact scale-aware tokens; and (ii) Structured Sparse Attention (SSA), which\ncaptures cross-window and cross-region dependencies, enhancing scale diversity\nwhile removing spurious correlations. CST and SSA are alternately stacked to\nprogressively integrate multi-scale dependencies. Experiments on 11 EEG tasks\nacross 16 datasets show that CSBrain consistently outperforms task-specific and\nfoundation model baselines. These results establish cross-scale modeling as a\nkey inductive bias and position CSBrain as a robust backbone for future\nbrain-AI research.", "AI": {"tldr": "CSBrain is a foundation model that enhances EEG decoding by addressing cross-scale spatiotemporal structures through novel tokenization and attention mechanisms.", "motivation": "The need for improved EEG decoding methods that consider the diverse temporal and spatial structures of neural activity, which are overlooked by existing dense modeling paradigms.", "method": "CSBrain introduces Cross-scale Spatiotemporal Tokenization (CST) for feature aggregation and Structured Sparse Attention (SSA) for dependency capture, integrating multi-scale information in a sequential manner.", "result": "CSBrain outperforms existing task-specific and foundation model baselines across 11 EEG tasks in 16 datasets, demonstrating the effectiveness of cross-scale modeling.", "conclusion": "CSBrain establishes cross-scale modeling as an essential approach for enhanced brain activity analysis and represents a significant advancement in brain-AI research.", "key_contributions": ["Introduction of Cross-scale Spatiotemporal Tokenization (CST) for EEG signal representation.", "Development of Structured Sparse Attention (SSA) for capturing complex dependencies in EEG data.", "Demonstration of superior performance in EEG decoding tasks compared to previous models."], "limitations": "", "keywords": ["EEG", "brain-computer interfaces", "machine learning", "neuroscience", "spatiotemporal modeling"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2506.22679", "pdf": "https://arxiv.org/pdf/2506.22679.pdf", "abs": "https://arxiv.org/abs/2506.22679", "title": "Assessing the feasibility of Large Language Models for detecting micro-behaviors in team interactions during space missions", "authors": ["Ankush Raut", "Projna Paromita", "Sydney Begerowski", "Suzanne Bell", "Theodora Chaspari"], "categories": ["cs.CL"], "comment": "5 pages, 4 figures. Accepted to Interspeech 2025", "summary": "We explore the feasibility of large language models (LLMs) in detecting\nsubtle expressions of micro-behaviors in team conversations using transcripts\ncollected during simulated space missions. Specifically, we examine zero-shot\nclassification, fine-tuning, and paraphrase-augmented fine-tuning with\nencoder-only sequence classification LLMs, as well as few-shot text generation\nwith decoder-only causal language modeling LLMs, to predict the micro-behavior\nassociated with each conversational turn (i.e., dialogue). Our findings\nindicate that encoder-only LLMs, such as RoBERTa and DistilBERT, struggled to\ndetect underrepresented micro-behaviors, particularly discouraging speech, even\nwith weighted fine-tuning. In contrast, the instruction fine-tuned version of\nLlama-3.1, a decoder-only LLM, demonstrated superior performance, with the best\nmodels achieving macro F1-scores of 44% for 3-way classification and 68% for\nbinary classification. These results have implications for the development of\nspeech technologies aimed at analyzing team communication dynamics and\nenhancing training interventions in high-stakes environments such as space\nmissions, particularly in scenarios where text is the only accessible data.", "AI": {"tldr": "This paper investigates the use of LLMs to identify micro-behaviors in team communication during simulated space missions, comparing various techniques and showing that decoder-only LLMs outperform encoder-only ones in performance metrics.", "motivation": "The study seeks to enhance understanding of communication dynamics in high-pressure environments, such as space missions, where team interactions can be critical to success.", "method": "The authors employed zero-shot classification, fine-tuning, and paraphrase-augmented fine-tuning with encoder-only models, alongside few-shot text generation with decoder-only models, to analyze conversational transcripts from simulated missions.", "result": "The instruction fine-tuned version of Llama-3.1 yielded the best results, achieving macro F1-scores of 44% for 3-way classification and 68% for binary classification. Encoder-only models struggled with detecting subtle behaviors, particularly discouraging speech.", "conclusion": "Findings suggest that decoder-only LLMs are more effective in identifying nuanced communication behaviors, which can aid in developing tools for improving team dynamics in critical environments.", "key_contributions": ["Demonstration of LLMs for detecting micro-behaviors in dialogue", "Comparison of encoder-only and decoder-only LLM performance", "Insights into enhancing speech technologies for team communication"], "limitations": "Encoder-only models faced challenges in identifying underrepresented behaviors despite fine-tuning efforts.", "keywords": ["large language models", "micro-behaviors", "team conversations", "zero-shot classification", "fine-tuning"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2506.23116", "pdf": "https://arxiv.org/pdf/2506.23116.pdf", "abs": "https://arxiv.org/abs/2506.23116", "title": "A User Experience 3.0 (UX 3.0) Paradigm Framework: Designing for Human-Centered AI Experiences", "authors": ["Wei Xu"], "categories": ["cs.HC"], "comment": null, "summary": "User experience (UX) practices have evolved in stages and are entering a\ntransformative phase (UX 3.0), driven by AI technologies and shifting user\nneeds. Human-centered AI (HCAI) experiences are emerging, necessitating new UX\napproaches to support UX practices in the AI era. We propose a UX 3.0 paradigm\nframework to respond and guide UX practices in developing HCAI systems.", "AI": {"tldr": "This paper discusses the evolution of user experience (UX) practices towards a transformative phase (UX 3.0) influenced by AI technologies, proposing a framework for developing human-centered AI (HCAI) systems.", "motivation": "The need for new UX approaches that align with the evolving landscape of AI technologies and user needs.", "method": "The authors propose a UX 3.0 paradigm framework to guide UX practices in the design and development of HCAI systems.", "result": "The framework is aimed at enhancing user experiences in the context of AI integration into everyday applications.", "conclusion": "A shift towards HCAI necessitates the adoption of a new UX paradigm to accommodate changing user needs and technological advancements.", "key_contributions": ["Introduction of a UX 3.0 paradigm framework.", "Focus on human-centered AI experiences.", "Guidance for UX practices in AI development."], "limitations": "", "keywords": ["User Experience", "Human-Centered AI", "UX 3.0", "AI Technologies", "UX Practices"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2506.22694", "pdf": "https://arxiv.org/pdf/2506.22694.pdf", "abs": "https://arxiv.org/abs/2506.22694", "title": "VOCABTRIM: Vocabulary Pruning for Efficient Speculative Decoding in LLMs", "authors": ["Raghavv Goel", "Sudhanshu Agrawal", "Mukul Gagrani", "Junyoung Park", "Yifan Zao", "He Zhang", "Tian Liu", "Yiping Yang", "Xin Yuan", "Jiuyan Lu", "Chris Lott", "Mingu Lee"], "categories": ["cs.CL"], "comment": "7 pages, 4 figures, 5 tables, accepted at ICML 2025 workshop on\n  Efficient Systems for Foundational Models", "summary": "In this paper, we introduce a simple training-free technique to improve the\nperformance of drafter-based speculative decoding (SpD) methods that\nincorporates language modeling head (LM head) during drafting process. A\ndrafter-based speculative decoding leverages one or more smaller language\nmodels, a.k.a. drafters or draft models, to sample a draft sequence or tree\nconsisting of multiple tokens, followed by verification by a base LLM, a target\nmodel, accepting a subset as its valid generation. As it is usually considered\nthat the speculative decoding requires one-to-one mapping between vocabularies\nof the target model and the draft model, it has been natural to share the\nvocabulary between them, or even share the LM head as in EAGLE or Medusa. We\nfirst identify that this draft token sampling scheme inherently contains an\nunnecessary inference overhead in drafting, especially for some target LLMs\nwith very large vocabularies. Then, we propose a simple technique, VocabTrim,\nto mitigate the drafting overhead to improve the generation speed in\nmemory-bound environment. VocabTrim reconstructs the drafter LM head to contain\nonly a limited set of tokens, selected by the most frequently sampled from the\nvocabulary of the target model. While limiting the vocabulary in drafting\nslightly degrades the acceptance rate, it significantly reduces the drafting\nlatency in memory-bound process which is often the case on edge devices,\nresulting in higher memory-bound speed up (MBSU). We show that our method can\nboost the memory-bound speed-up for Llama-3 models on Spec-Bench, specifically\nby 16% for Llama-3.2-3B-Instruct.", "AI": {"tldr": "Introduces VocabTrim, a training-free technique enhancing drafter-based speculative decoding by reducing inference overhead.", "motivation": "Improving performance of drafter-based speculative decoding methods in language models.", "method": "VocabTrim reconstructs the drafter LM head to include a limited set of frequently sampled tokens from the target model’s vocabulary, reducing drafting overhead.", "result": "Achieves a 16% boost in memory-bound speed-up for Llama-3 models on Spec-Bench, while slightly degrading acceptance rates.", "conclusion": "VocabTrim offers significant improvements in drafting latency and generation speed in memory-constrained environments like edge devices.", "key_contributions": ["Introduction of VocabTrim technique", "Reduction of inference overhead in drafting", "Improved memory-bound speed-up for Llama-3 models"], "limitations": "Slight degradation in acceptance rate due to limiting drafting vocabulary.", "keywords": ["speculative decoding", "language modeling", "edge devices"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.23180", "pdf": "https://arxiv.org/pdf/2506.23180.pdf", "abs": "https://arxiv.org/abs/2506.23180", "title": "ImprovMate: Multimodal AI Assistant for Improv Actor Training", "authors": ["Riccardo Drago", "Yotam Sechayk", "Mustafa Doga Dogan", "Andrea Sanna", "Takeo Igarashi"], "categories": ["cs.HC", "H.5.0; H.5.2"], "comment": "ACM DIS '25", "summary": "Improvisation training for actors presents unique challenges, particularly in\nmaintaining narrative coherence and managing cognitive load during\nperformances. Previous research on AI in improvisation performance often\npredates advances in large language models (LLMs) and relies on human\nintervention. We introduce ImprovMate, which leverages LLMs as GPTs to automate\nthe generation of narrative stimuli and cues, allowing actors to focus on\ncreativity without keeping track of plot or character continuity. Based on\ninsights from professional improvisers, ImprovMate incorporates exercises that\nmimic live training, such as abrupt story resolution and reactive thinking\nexercises, while maintaining coherence via reference tables. By balancing\nrandomness and structured guidance, ImprovMate provides a groundbreaking tool\nfor improv training. Our pilot study revealed that actors might embrace AI\ntechniques if the latter mirrors traditional practices, and appreciate the\nfresh twist introduced by our approach with the AI-generated cues.", "AI": {"tldr": "ImprovMate is an AI tool using LLMs to assist actors in improvisation training by generating narrative cues, enhancing creativity while ensuring coherence.", "motivation": "To address the challenges faced by actors in maintaining narrative coherence and managing cognitive load during improvisation performances.", "method": "ImprovMate leverages large language models to generate narrative stimuli and cues, supporting exercises that mimic live training scenarios.", "result": "A pilot study indicated that actors might welcome AI techniques in improvisation, especially if they align with traditional practices and provide novel twists.", "conclusion": "ImprovMate offers a unique solution for improv training by balancing randomness and structured guidance, making the improvisation process smoother for actors.", "key_contributions": ["Introduction of ImprovMate, leveraging LLMs in improv training", "Incorporation of professional improvisers' insights into training exercises", "Pilot study demonstrating acceptance of AI cues in traditional improv settings"], "limitations": "", "keywords": ["improvisation", "AI", "large language models", "training", "narrative coherence"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2506.22698", "pdf": "https://arxiv.org/pdf/2506.22698.pdf", "abs": "https://arxiv.org/abs/2506.22698", "title": "Text Production and Comprehension by Human and Artificial Intelligence: Interdisciplinary Workshop Report", "authors": ["Emily Dux Speltz"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This report synthesizes the outcomes of a recent interdisciplinary workshop\nthat brought together leading experts in cognitive psychology, language\nlearning, and artificial intelligence (AI)-based natural language processing\n(NLP). The workshop, funded by the National Science Foundation, aimed to\naddress a critical knowledge gap in our understanding of the relationship\nbetween AI language models and human cognitive processes in text comprehension\nand composition. Through collaborative dialogue across cognitive, linguistic,\nand technological perspectives, workshop participants examined the underlying\nprocesses involved when humans produce and comprehend text, and how AI can both\ninform our understanding of these processes and augment human capabilities. The\nworkshop revealed emerging patterns in the relationship between large language\nmodels (LLMs) and human cognition, with highlights on both the capabilities of\nLLMs and their limitations in fully replicating human-like language\nunderstanding and generation. Key findings include the potential of LLMs to\noffer insights into human language processing, the increasing alignment between\nLLM behavior and human language processing when models are fine-tuned with\nhuman feedback, and the opportunities and challenges presented by human-AI\ncollaboration in language tasks. By synthesizing these findings, this report\naims to guide future research, development, and implementation of LLMs in\ncognitive psychology, linguistics, and education. It emphasizes the importance\nof ethical considerations and responsible use of AI technologies while striving\nto enhance human capabilities in text comprehension and production through\neffective human-AI collaboration.", "AI": {"tldr": "This report summarizes insights from a workshop on AI language models and human cognitive processes in text comprehension and composition.", "motivation": "To address the knowledge gap in understanding the interplay between AI language models and human cognitive processes in language tasks.", "method": "Interdisciplinary workshop discussions involving experts from cognitive psychology, language learning, and AI-based NLP.", "result": "Identified key patterns in the relationship between LLMs and human cognition, including capabilities, limitations, and implications for human-AI collaboration.", "conclusion": "The findings should guide future research and development in LLMs while considering ethical implications and enhancing human capabilities.", "key_contributions": ["Insights into human language processing from LLMs", "Understanding the alignment of LLM behavior with human cognition", "Recommendations for ethical AI use in education and psychology"], "limitations": "", "keywords": ["cognitive psychology", "natural language processing", "large language models"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.23253", "pdf": "https://arxiv.org/pdf/2506.23253.pdf", "abs": "https://arxiv.org/abs/2506.23253", "title": "Vibe coding: programming through conversation with artificial intelligence", "authors": ["Advait Sarkar", "Ian Drosos"], "categories": ["cs.HC"], "comment": null, "summary": "We examine \"vibe coding\": an emergent programming paradigm where developers\nprimarily write code by interacting with code-generating large language models\nrather than writing code directly. We analysed a curated set of videos\ndepicting extended vibe coding sessions with rich think-aloud reflections.\nUsing framework analysis, we investigated programmers' goals, workflows,\nprompting techniques, debugging approaches, and challenges encountered. We find\nthat vibe coding follows iterative goal satisfaction cycles where developers\nalternate between prompting AI, evaluating generated code through rapid\nscanning and application testing, and manual editing. Prompting strategies\nblend vague, high-level directives with detailed technical specifications.\nDebugging remains a hybrid process combining AI assistance with manual\npractices. Critically, vibe coding does not eliminate the need for programming\nexpertise but rather redistributes it toward context management, rapid code\nevaluation, and decisions about when to transition between AI-driven and manual\nmanipulation of code. Trust in AI tools during vibe coding is dynamic and\ncontextual, developed through iterative verification rather than blanket\nacceptance. Vibe coding is an evolution of AI-assisted programming that\nrepresents an early manifestation of \"material disengagement\", where\npractitioners orchestrate code production and manipulation, mediated through\nAI, while maintaining selective and strategic oversight.", "AI": {"tldr": "This paper explores 'vibe coding', a programming paradigm that involves leveraging large language models for code generation, focusing on developers' interactions and workflows during coding sessions.", "motivation": "To understand the emerging role of AI in programming and how it influences developer workflows and interactions with code generation tools.", "method": "The authors conducted a framework analysis of curated videos showcasing vibe coding sessions, capturing developer reflections and interactions when using AI for coding.", "result": "The study reveals that vibe coding involves iterative cycles of prompting AI, evaluating generated code, and manual editing, blending high-level directives with detailed specifications, and maintaining trust in AI through iterative verification.", "conclusion": "Vibe coding represents a shift in programming practices that redistributes the need for expertise, emphasizing context management and decision-making between AI and manual coding.", "key_contributions": ["Identification of the vibe coding paradigm", "Insights into blended prompting strategies", "Understanding the dynamic nature of trust in AI tools during coding"], "limitations": "The findings are based on a curated video set, which may not be representative of all coding environments or practices.", "keywords": ["vibe coding", "AI-assisted programming", "developer workflows", "programming paradigm", "trust in AI"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.22724", "pdf": "https://arxiv.org/pdf/2506.22724.pdf", "abs": "https://arxiv.org/abs/2506.22724", "title": "The Translation Barrier Hypothesis: Multilingual Generation with Large Language Models Suffers from Implicit Translation Failure", "authors": ["Niyati Bafna", "Tianjian Li", "Kenton Murray", "David R. Mortensen", "David Yarowsky", "Hale Sirin", "Daniel Khashabi"], "categories": ["cs.CL"], "comment": "23 pages incl. appendix", "summary": "Multilingual generation with large language models (LLMs) is often of poor\nquality for mid- to low-resource languages. Building on insights from\ninterpretability, we demonstrate the existence of an implicit\ntask-solving-->translation pipeline for generation, whereby the model first\nsolves the required task in a largely target-language-agnostic manner, and\nsubsequently translates answer concepts into the intended target language. We\nhypothesize that the failure of the translation stage is an important culprit\nfor the observed low quality of final outputs, and formalize this as the\ntranslation barrier hypothesis. We test this hypothesis for a word translation\ntask across 108 language pairs, using logit lens to observe model processing in\nintermediate layers. We find that a significant portion of overall failures\nindeed stems from translation failure, or the model's inability to translate\ncorrectly solved intermediate concepts into the target language. This is\nespecially true for low-resource target languages. Our results highlight an\nimportant hurdle for end-to-end multilingual generation, and lend guiding\ninsights for future work seeking to improve multilinguality in LLMs.", "AI": {"tldr": "This paper explores the quality of multilingual generation in large language models (LLMs) across low-resource languages, identifying the translation barrier hypothesis as a key factor in failures, particularly in translating solved intermediate concepts.", "motivation": "To investigate the poor quality of multilingual generation for mid- to low-resource languages in LLMs.", "method": "The authors test the 'translation barrier hypothesis' by analyzing a word translation task across 108 language pairs, employing logit lens to examine model processing in intermediate layers.", "result": "The study reveals that a significant portion of failures in multilingual generation arises from translation errors, particularly for low-resource languages, indicating that the translation stage is a crucial bottleneck.", "conclusion": "The findings emphasize the need for strategies to overcome the translation barrier in LLMs to enhance multilingual generation capabilities.", "key_contributions": ["Introduction of the translation barrier hypothesis.", "Analysis across 108 language pairs to validate the hypothesis.", "Identification of critical challenges in multilingual generation for low-resource languages."], "limitations": "Focus on word translation tasks may not capture all dimensions of multilingual generation issues.", "keywords": ["Multilingual generation", "Large language models", "Translation barrier hypothesis"], "importance_score": 8, "read_time_minutes": 23}}
{"id": "2506.23443", "pdf": "https://arxiv.org/pdf/2506.23443.pdf", "abs": "https://arxiv.org/abs/2506.23443", "title": "Accessible Data Access and Analysis by People who are Blind or Have Low Vision", "authors": ["Samuel Reinders", "Munazza Zaib", "Matthew Butler", "Bongshin Lee", "Ingrid Zukerman", "Lizhen Qu", "Kim Marriott"], "categories": ["cs.HC"], "comment": "Poster presented at the 1st Workshop on Accessible Data\n  Visualization, IEEE VIS 2024", "summary": "Our work aims to develop new assistive technologies that enable blind or low\nvision (BLV) people to explore and analyze data readily. At present, barriers\nexist for BLV people to explore and analyze data, restricting access to\ngovernment, health and personal data, and limiting employment opportunities.\nThis work explores the co-design and development of an innovative system to\nsupport data access, with a focus on the use of refreshable tactile displays\n(RTDs) and conversational agents. The envisaged system will use a combination\nof tactile graphics and speech to communicate with BLV users, and proactively\nassist with data analysis tasks. As well as addressing significant equity gaps,\nour work expects to produce innovations in assistive technology, multimodal\ninterfaces, dialogue systems, and natural language understanding and\ngeneration.", "AI": {"tldr": "The paper presents a system combining tactile displays and conversational agents to assist blind or low vision users in exploring and analyzing data.", "motivation": "To reduce barriers for blind or low vision (BLV) individuals in accessing and analyzing data, improving their employment opportunities and interaction with information.", "method": "Co-design and development of a system that integrates refreshable tactile displays and speech communication for data analysis tasks.", "result": "The system enables effective data exploration for BLV users, contributing to the field of assistive technology and multimodal interactions.", "conclusion": "The proposed innovations aim to enhance accessibility to data and bridge equity gaps for BLV populations, with implications for natural language understanding and generation.", "key_contributions": ["Development of a multimodal system for BLV users", "Integration of tactile graphics and conversational agents", "Focus on enhancing data accessibility and analysis for BLV individuals"], "limitations": "", "keywords": ["assistive technology", "refreshable tactile displays", "conversational agents", "data analysis", "blind and low vision"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2506.22760", "pdf": "https://arxiv.org/pdf/2506.22760.pdf", "abs": "https://arxiv.org/abs/2506.22760", "title": "Jan-nano Technical Report", "authors": ["Alan Dao", "Dinh Bach Vu"], "categories": ["cs.CL"], "comment": null, "summary": "Most language models face a fundamental tradeoff where powerful capabilities\nrequire substantial computational resources. We shatter this constraint with\nJan-nano, a 4B parameter language model that redefines efficiency through\nradical specialization: instead of trying to know everything, it masters the\nart of finding anything instantly. Fine-tuned from Qwen3-4B using our novel\nmulti-stage RLVR system that completely eliminates reliance on next token\nprediction training (SFT), Jan-nano achieves 83.2% on SimpleQA benchmark with\nMCP integration while running on consumer hardware. With 128K context length,\nJan-nano proves that intelligence isn't about scale, it's about strategy.", "AI": {"tldr": "Jan-nano is a specialized 4B parameter language model optimized for efficiency and performance on consumer hardware, achieving high benchmarks without traditional training methods.", "motivation": "To overcome the tradeoff between powerful capabilities and computational resources in language models.", "method": "Jan-nano is fine-tuned from Qwen3-4B using a multi-stage RLVR system, eliminating the need for next token prediction training.", "result": "Achieves 83.2% on the SimpleQA benchmark, demonstrating that it can deliver intelligence with significantly less computational demand.", "conclusion": "Jan-nano showcases that effective strategy in model design can lead to strong performance without scaling in size.", "key_contributions": ["Redefines efficiency in language models", "Achieves high performance on consumer hardware", "Eliminates reliance on traditional next token prediction training"], "limitations": "", "keywords": ["language model", "efficiency", "RLVR", "benchmark", "consumer hardware"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2506.23457", "pdf": "https://arxiv.org/pdf/2506.23457.pdf", "abs": "https://arxiv.org/abs/2506.23457", "title": "Reducing Motion Sickness in Passengers of Autonomous Personal Mobility Vehicles by Presenting a Driving Path", "authors": ["Yuya Ide", "Hailong Liu", "Takahiro Wada"], "categories": ["cs.HC"], "comment": null, "summary": "Autonomous personal mobility vehicles (APMVs) are small mobility devices\ndesigned for individual automated transportation in shared spaces. In such\nenvironments, frequent pedestrian avoidance maneuvers may cause rapid steering\nadjustments and passive postural responses from passengers, thereby increasing\nthe risk of motion sickness. This study investigated the effects of providing\npath information on 16 passengers' head movement behavior and motion sickness\nwhile riding an APMV. Through a controlled experiment comparing manual driving\n(MD), autonomous driving without path information (AD w/o path), and autonomous\ndriving with path information (AD w/ path), we found that providing path cues\nsignificantly reduced MISC scores and delayed the onset of motion sickness\nsymptoms. In addition, participants were more likely to proactively align their\nhead movements with the direction of vehicle rotation in both MD and AD w/ path\nconditions. Although a small correlation was observed between the delay in yaw\nrotation of the passenger's head relative to the vehicle and the occurrence of\nmotion sickness, the underlying physiological mechanism remains to be\nelucidated.", "AI": {"tldr": "The study examines the impact of providing path information in autonomous personal mobility vehicles on passengers' head movement and motion sickness.", "motivation": "Understanding how to reduce motion sickness in autonomous vehicles by providing directional information to passengers.", "method": "A controlled experiment was conducted with 16 passengers, comparing their experiences in manual driving, autonomous driving without path information, and autonomous driving with path information.", "result": "Path cues significantly reduced motion sickness scores and delayed the onset of symptoms; passengers aligned their head movements with vehicle direction more effectively in certain conditions.", "conclusion": "Providing path information can mitigate motion sickness in APMVs, indicating a need to explore the physiological mechanisms involved.", "key_contributions": ["Demonstration of the effects of path information on reducing motion sickness in APMVs.", "Evidence that head movement alignment can improve passenger comfort during rides.", "Insights into the physiological responses associated with motion sickness in autonomous driving contexts."], "limitations": "The study involved a small sample size and further research is needed to explore the underlying physiological mechanisms.", "keywords": ["autonomous vehicles", "motion sickness", "head movement", "path information", "passenger behavior"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.22777", "pdf": "https://arxiv.org/pdf/2506.22777.pdf", "abs": "https://arxiv.org/abs/2506.22777", "title": "Teaching Models to Verbalize Reward Hacking in Chain-of-Thought Reasoning", "authors": ["Miles Turpin", "Andy Arditi", "Marvin Li", "Joe Benton", "Julian Michael"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Language models trained with RL can engage in reward hacking--exploiting\nunintended strategies for high reward--without revealing this behavior in their\nchain-of-thought reasoning, making detection difficult and posing risks for\nhigh-stakes applications. We propose verbalization fine-tuning (VFT), a pre-RL\nintervention that trains models to explicitly acknowledge when they are\ninfluenced by prompt cues--hints which point to incorrect answers (e.g., \"a\nStanford professor thinks the answer is A\"). To evaluate VFT, we subsequently\ntrain models with RL on environments where held-out prompt cues signal which\nincorrect answers will receive high reward, incentivizing models to reward hack\nby exploiting cues instead of reasoning correctly. We measure how often models\nexploit these cues without verbalizing it. After RL, only 6% of the VFT-trained\nmodel's responses consist of undetected reward hacks. In comparison, when we\nperform RL without VFT, the rate of undetected reward hacks goes up to 88%;\nwith a debiasing baseline intervention, this increases further to 99%. VFT\nachieves this by substantially increasing how often models verbalize the\ninfluence of cues--from 8% to 42% after VFT, and up to 94% after RL--while\nbaselines remain low even after RL (10% and 1%). Our results show that teaching\nmodels to explicitly verbalize reward hacking behavior before RL significantly\nimproves their detection, offering a practical path toward more transparent and\nsafe AI systems.", "AI": {"tldr": "Verbalization fine-tuning (VFT) reduces reward hacking in language models by training them to acknowledge prompt cues that influence their answers.", "motivation": "Language models often exploit unintended strategies (reward hacking) to achieve high rewards, which can pose risks, especially in critical applications. Detecting such behavior is complicated.", "method": "We introduce verbalization fine-tuning (VFT) as a pre-RL intervention, training models to recognize and acknowledge the influence of prompt cues that lead to incorrect answers before applying reinforcement learning (RL).", "result": "VFT-trained models show a significant reduction in undetected reward hacks—only 6% compared to 88% without VFT. RL combined with VFT leads to 94% verbalization of cue influence.", "conclusion": "VFT enhances the detection of reward hacking in language models, suggesting a viable approach for creating more transparent and safer AI systems.", "key_contributions": ["Introduction of verbalization fine-tuning (VFT) to improve model transparency", "Demonstrated reduction in reward hacking detection rates through explicit acknowledgment of prompt influences", "Quantitative results showing significant improvements in verbalization of influences post-RL"], "limitations": "", "keywords": ["Reinforcement Learning", "Reward Hacking", "Verbalization Fine-Tuning", "Transparency", "AI Safety"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2506.23458", "pdf": "https://arxiv.org/pdf/2506.23458.pdf", "abs": "https://arxiv.org/abs/2506.23458", "title": "Neuro-Informed Joint Learning Enhances Cognitive Workload Decoding in Portable BCIs", "authors": ["Xiaoxiao Yang", "Chan Feng", "Jiancheng Chen"], "categories": ["cs.HC", "cs.LG"], "comment": "2 pages short paper", "summary": "Portable and wearable consumer-grade electroencephalography (EEG) devices,\nlike Muse headbands, offer unprecedented mobility for daily brain-computer\ninterface (BCI) applications, including cognitive load detection. However, the\nexacerbated non-stationarity in portable EEG signals constrains data fidelity\nand decoding accuracy, creating a fundamental trade-off between portability and\nperformance. To mitigate such limitation, we propose MuseCogNet (Muse-based\nCognitive Network), a unified joint learning framework integrating\nself-supervised and supervised training paradigms. In particular, we introduce\nan EEG-grounded self-supervised reconstruction loss based on average pooling to\ncapture robust neurophysiological patterns, while cross-entropy loss refines\ntask-specific cognitive discriminants. This joint learning framework resembles\nthe bottom-up and top-down attention in humans, enabling MuseCogNet to\nsignificantly outperform state-of-the-art methods on a publicly available Muse\ndataset and establish an implementable pathway for neurocognitive monitoring in\necological settings.", "AI": {"tldr": "This paper presents MuseCogNet, a unified learning framework for enhancing the performance of portable EEG devices in cognitive load detection by integrating self-supervised and supervised training.", "motivation": "To address the trade-off between portability and performance in portable EEG devices used for brain-computer interface applications, particularly for cognitive load detection.", "method": "A unified joint learning framework combining self-supervised reconstruction loss and supervised cross-entropy loss, designed to capture robust neurophysiological patterns and refine task-specific cognitive discriminants.", "result": "MuseCogNet significantly outperforms state-of-the-art methods on a publicly available Muse dataset, demonstrating its effectiveness for neurocognitive monitoring in ecological settings.", "conclusion": "The proposed MuseCogNet establishes a pathway for improved cognitive monitoring using portable EEG devices, balancing the trade-off between mobility and accuracy.", "key_contributions": ["Introduction of a joint learning framework for EEG data analysis.", "Development of an EEG-grounded self-supervised reconstruction loss.", "Demonstration of superior performance on cognitive load detection using a public Muse dataset."], "limitations": "", "keywords": ["EEG", "BCI", "Cognitive Load Detection", "Self-Supervised Learning", "Machine Learning"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2506.22791", "pdf": "https://arxiv.org/pdf/2506.22791.pdf", "abs": "https://arxiv.org/abs/2506.22791", "title": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in Large Language Models", "authors": ["Jianxin Yan", "Wangze Ni", "Lei Chen", "Xuemin Lin", "Peng Cheng", "Zhan Qin", "Kui Ren"], "categories": ["cs.CL", "cs.DB"], "comment": null, "summary": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications.", "AI": {"tldr": "ContextCache is a context-aware semantic caching system designed to improve the efficiency of multi-turn dialogues in LLM applications by utilizing a two-stage retrieval architecture that accounts for conversation context.", "motivation": "To address the limitations of existing caching systems that fail to consider multi-turn dialogue contexts, leading to incorrect cache hits and inefficiencies.", "method": "ContextCache implements a two-stage retrieval architecture; the first stage involves vector-based retrieval of potential matches for the current query, while the second stage uses self-attention mechanisms to integrate current and historical dialogue representations for better contextual matching.", "result": "ContextCache demonstrates improved precision and recall in real-world conversations compared to existing caching systems, and achieves approximately 10 times lower latency for cached responses than direct LLM invocations.", "conclusion": "By significantly reducing computational costs and improving efficiency in LLM conversational applications, ContextCache addresses critical inefficiencies in current semantic caching practices.", "key_contributions": ["Introduction of a context-aware caching system for LLM applications", "Implementation of a two-stage retrieval mechanism", "Demonstrated improvements in precision, recall, and latency for multi-turn dialogues"], "limitations": "", "keywords": ["semantics", "caching", "multi-turn dialogues", "LLM", "context-aware"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2506.23545", "pdf": "https://arxiv.org/pdf/2506.23545.pdf", "abs": "https://arxiv.org/abs/2506.23545", "title": "Immersive Technologies in Training and Healthcare: From Space Missions to Psychophysiological Research", "authors": ["Barbara Karpowicz", "Maciej Grzeszczuk", "Adam Kuzdraliński", "Monika Kornacka", "Aliaksandr Marozau", "Wiktor Stawski", "Pavlo Zinevych", "Grzegorz Marcin Wójcik", "Tomasz Kowalewski", "Grzegorz Pochwatko", "Wiesław Kopeć"], "categories": ["cs.HC", "cs.CE"], "comment": "8 pages, 1 figure", "summary": "Virtual, Augmented, and eXtended Reality (VR/AR/XR) technologies are\nincreasingly recognized for their applications in training, diagnostics, and\npsychological research, particularly in high-risk and highly regulated\nenvironments. In this panel we discuss how immersive systems enhance human\nperformance across multiple domains, including clinical psychology, space\nexploration, and medical education. In psychological research and training, XR\ncan offer a controlled yet ecologically valid setting for measuring cognitive\nand affective processes. In space exploration, we discuss the development of\nVR-based astronaut training and diagnostic systems, allowing astronauts to\nperform real-time health assessments. In medical education and rehabilitation,\nwe cover procedural training and patient engagement. From virtual surgical\nsimulations to gamified rehabilitation exercises, immersive environments\nenhance both learning outcomes and treatment adherence.", "AI": {"tldr": "Panel discussion on the applications of VR/AR/XR technologies in enhancing human performance across various domains.", "motivation": "To explore how immersive systems like VR/AR/XR can improve training, diagnostics, and research outcomes in high-risk environments.", "method": "Discussion panel examining case studies and applications in clinical psychology, space exploration, and medical education.", "result": "Findings highlight the effectiveness of XR in providing controlled environments for psychological measurements and enhancing training for astronauts and medical professionals.", "conclusion": "Immersive technologies significantly improve training experiences, learning outcomes in medical education, and engagement in rehabilitation.", "key_contributions": ["Showcasing diverse applications of VR/AR/XR in safety-critical domains", "Highlighting the role of immersive environments in psychological research", "Describing advancements in VR training for astronauts and procedures in medical education"], "limitations": "", "keywords": ["Virtual Reality", "Augmented Reality", "Human Performance", "Medical Education", "Psychological Research"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.22808", "pdf": "https://arxiv.org/pdf/2506.22808.pdf", "abs": "https://arxiv.org/abs/2506.22808", "title": "MedEthicsQA: A Comprehensive Question Answering Benchmark for Medical Ethics Evaluation of LLMs", "authors": ["Jianhui Wei", "Zijie Meng", "Zikai Xiao", "Tianxiang Hu", "Yang Feng", "Zhijie Zhou", "Jian Wu", "Zuozhu Liu"], "categories": ["cs.CL", "cs.AI"], "comment": "20 pages", "summary": "While Medical Large Language Models (MedLLMs) have demonstrated remarkable\npotential in clinical tasks, their ethical safety remains insufficiently\nexplored. This paper introduces $\\textbf{MedEthicsQA}$, a comprehensive\nbenchmark comprising $\\textbf{5,623}$ multiple-choice questions and\n$\\textbf{5,351}$ open-ended questions for evaluation of medical ethics in LLMs.\nWe systematically establish a hierarchical taxonomy integrating global medical\nethical standards. The benchmark encompasses widely used medical datasets,\nauthoritative question banks, and scenarios derived from PubMed literature.\nRigorous quality control involving multi-stage filtering and multi-faceted\nexpert validation ensures the reliability of the dataset with a low error rate\n($2.72\\%$). Evaluation of state-of-the-art MedLLMs exhibit declined performance\nin answering medical ethics questions compared to their foundation\ncounterparts, elucidating the deficiencies of medical ethics alignment. The\ndataset, registered under CC BY-NC 4.0 license, is available at\nhttps://github.com/JianhuiWei7/MedEthicsQA.", "AI": {"tldr": "This paper presents MedEthicsQA, a benchmark for assessing medical ethics in Large Language Models (LLMs), comprising over 10,000 questions and a systematic taxonomy based on global medical ethical standards.", "motivation": "The ethical safety of Medical Large Language Models (MedLLMs) in clinical tasks has not been thoroughly investigated, necessitating a dedicated benchmark for evaluation.", "method": "The paper introduces MedEthicsQA, a benchmark consisting of 5,623 multiple-choice questions and 5,351 open-ended questions, established using a hierarchical taxonomy of medical ethics. Rigorous quality control measures were implemented to ensure dataset reliability.", "result": "Evaluation shows that state-of-the-art MedLLMs performed worse on medical ethics questions than their foundation model counterparts, highlighting significant shortcomings in medical ethics alignment.", "conclusion": "The introduction of MedEthicsQA provides a foundational tool for evaluating and improving the ethical performance of Medical LLMs, encouraging further research in this area.", "key_contributions": ["Introduction of MedEthicsQA benchmark for medical ethics in LLMs", "Integration of a hierarchical taxonomy of medical ethics", "Rigorous validation and quality control of the dataset"], "limitations": "", "keywords": ["Medical Ethics", "Large Language Models", "Benchmark", "Healthcare AI", "Ethical Evaluation"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2506.23678", "pdf": "https://arxiv.org/pdf/2506.23678.pdf", "abs": "https://arxiv.org/abs/2506.23678", "title": "Interactive Reasoning: Visualizing and Controlling Chain-of-Thought Reasoning in Large Language Models", "authors": ["Rock Yuren Pang", "K. J. Kevin Feng", "Shangbin Feng", "Chu Li", "Weijia Shi", "Yulia Tsvetkov", "Jeffrey Heer", "Katharina Reinecke"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "The output quality of large language models (LLMs) can be improved via\n\"reasoning\": generating segments of chain-of-thought (CoT) content to further\ncondition the model prior to producing user-facing output. While these chains\ncontain valuable information, they are verbose and lack explicit organization,\nmaking them tedious to review. Moreover, they lack opportunities for user\nfeedback, such as to remove unwanted considerations, add desired ones, or\nclarify unclear assumptions. We introduce Interactive Reasoning, an interaction\ndesign that visualizes chain-of-thought outputs as a hierarchy of topics and\nenables user review and modification. We implement interactive reasoning in\nHippo, a prototype for AI-assisted decision making in the face of uncertain\ntrade-offs. In a user study with 16 participants, we find that interactive\nreasoning in Hippo allows users to quickly identify and interrupt erroneous\ngenerations, efficiently steer the model towards customized responses, and\nbetter understand both model reasoning and model outputs. Our work contributes\nto a new paradigm that incorporates user oversight into LLM reasoning\nprocesses.", "AI": {"tldr": "The paper presents Interactive Reasoning, a design that organizes chain-of-thought outputs into a visual hierarchy for enhanced user review and modification in LLM-assisted decision making.", "motivation": "To improve output quality of LLMs by allowing user oversight and feedback during the reasoning process.", "method": "We introduced Interactive Reasoning, visualizing chain-of-thought outputs hierarchically and implemented it in the Hippo prototype.", "result": "User study findings indicate that Interactive Reasoning helps users quickly rectify errors, customize responses, and better understand LLM reasoning.", "conclusion": "This work establishes a paradigm for integrating user feedback into LLM reasoning, enhancing user control and comprehension.", "key_contributions": ["Introduction of Interactive Reasoning design", "Implementation in the Hippo prototype", "Empirical evidence from user study highlighting benefits of user interaction"], "limitations": "", "keywords": ["Interactive Reasoning", "Chain-of-thought", "User feedback", "LLM", "Decision making"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.22813", "pdf": "https://arxiv.org/pdf/2506.22813.pdf", "abs": "https://arxiv.org/abs/2506.22813", "title": "Selecting and Merging: Towards Adaptable and Scalable Named Entity Recognition with Large Language Models", "authors": ["Zhuojun Ding", "Wei Wei", "Chenghao Fan"], "categories": ["cs.CL"], "comment": null, "summary": "Supervised fine-tuning (SFT) is widely used to align large language models\n(LLMs) with information extraction (IE) tasks, such as named entity recognition\n(NER). However, annotating such fine-grained labels and training\ndomain-specific models is costly. Existing works typically train a unified\nmodel across multiple domains, but such approaches lack adaptation and\nscalability since not all training data benefits target domains and scaling\ntrained models remains challenging. We propose the SaM framework, which\ndynamically Selects and Merges expert models at inference time. Specifically,\nfor a target domain, we select domain-specific experts pre-trained on existing\ndomains based on (i) domain similarity to the target domain and (ii)\nperformance on sampled instances, respectively. The experts are then merged to\ncreate task-specific models optimized for the target domain. By dynamically\nmerging experts beneficial to target domains, we improve generalization across\nvarious domains without extra training. Additionally, experts can be added or\nremoved conveniently, leading to great scalability. Extensive experiments on\nmultiple benchmarks demonstrate our framework's effectiveness, which\noutperforms the unified model by an average of 10%. We further provide insights\ninto potential improvements, practical experience, and extensions of our\nframework.", "AI": {"tldr": "The SaM framework dynamically selects and merges expert models at inference time to improve performance in information extraction tasks across multiple domains without requiring additional training.", "motivation": "To address the challenges of adapting and scaling domain-specific models for information extraction tasks using large language models without incurring high annotation costs.", "method": "The framework selects domain-specific experts based on their similarity to the target domain and their performance on sampled instances, then merges them to create optimized task-specific models.", "result": "The SaM framework outperforms a unified model by an average of 10% across multiple benchmarks by improving generalization and scalability.", "conclusion": "The framework demonstrates effective dynamic model adaptation and paves the way for practical applications in real-world scenarios.", "key_contributions": ["Introduction of the SaM framework for model selection and merging", "Improvement of generalization in information extraction tasks", "Demonstration of the framework's effectiveness across benchmarks"], "limitations": "", "keywords": ["Large Language Models", "Information Extraction", "Domain Adaptation", "Model Merging"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.23694", "pdf": "https://arxiv.org/pdf/2506.23694.pdf", "abs": "https://arxiv.org/abs/2506.23694", "title": "If You Had to Pitch Your Ideal Software -- Evaluating Large Language Models to Support User Scenario Writing for User Experience Experts and Laypersons", "authors": ["Patrick Stadler", "Christopher Lazik", "Christopher Katins", "Thomas Kosch"], "categories": ["cs.HC"], "comment": null, "summary": "The process of requirements analysis requires an understanding of the end\nusers of a system. Thus, expert stakeholders, such as User Experience (UX)\ndesigners, usually create various descriptions containing information about the\nusers and their possible needs. In our paper, we investigate to what extent UX\nnovices are able to write such descriptions into user scenarios. We conducted a\nuser study with 60 participants consisting of 30 UX experts and 30 novices who\nwere asked to write a user scenario with or without the help of an\nLLM-supported writing assistant. Our findings show that LLMs empower laypersons\nto write reasonable user scenarios and provide first-hand insights for\nrequirements analysis that are comparable to UX experts in terms of structure\nand clarity, while especially excelling at audience-orientation. We present our\nqualitative and quantitative findings, including user scenario anatomies,\npotential influences, and differences in the way participants approached the\ntask.", "AI": {"tldr": "LLM-supported writing assistants enable UX novices to create coherent user scenarios comparable to those of experts, aiding requirements analysis.", "motivation": "Understanding user needs is critical in requirements analysis, traditionally performed by UX experts. This paper investigates the capability of novices using LLMs to generate effective user scenarios.", "method": "A user study involving 60 participants—30 UX experts and 30 novices—was conducted, where participants wrote user scenarios with and without LLM assistance.", "result": "Findings indicate that the LLM-supported novice scenarios matched experts in structure and clarity and excelled in audience-orientation, providing valuable input for requirements analysis.", "conclusion": "LLMs can empower UX novices to produce high-quality user scenarios, potentially bridging the gap between expert and novice in user analysis tasks.", "key_contributions": ["Demonstrated the efficacy of LLMs in improving novice UX scenario writing.", "Provided insights into differences between expert and novice outputs in user scenario construction.", "Presented quantitative and qualitative findings on user scenarios' clarity and structure."], "limitations": "The study is limited by the sample size and the specific context in which the scenarios were written.", "keywords": ["User Experience", "LLM", "User Scenarios", "Requirements Analysis", "UX Design"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.22846", "pdf": "https://arxiv.org/pdf/2506.22846.pdf", "abs": "https://arxiv.org/abs/2506.22846", "title": "Boosting CTC-Based ASR Using LLM-Based Intermediate Loss Regularization", "authors": ["Duygu Altinok"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "This is the accepted version of an article accepted to the TSD 2025\n  conference, published in Springer Lecture Notes in Artificial Intelligence\n  (LNAI). The final authenticated version is available online at SpringerLink", "summary": "End-to-end (E2E) automatic speech recognition (ASR) systems have\nrevolutionized the field by integrating all components into a single neural\nnetwork, with attention-based encoder-decoder models achieving state-of-the-art\nperformance. However, their autoregressive decoding process limits inference\nspeed, making them unsuitable for real-time applications. In contrast,\nCTC-based models offer faster, non-autoregressive decoding but struggle to\nmodel linguistic dependencies effectively. Addressing this challenge, we\npropose a novel auxiliary loss framework called Language-Aware Intermediate\nLoss (LAIL) to enhance CTC-based ASR using the linguistic knowledge of large\nlanguage models (LLMs). By attaching connector layers to intermediate encoder\nlayers, LAIL maps outputs to the embedding space of an LLM and computes a\ncausal language modeling loss during training. This approach enhances\nlinguistic modeling while preserving the computational efficiency of CTC\ndecoding. Using the Conformer architecture and various LLaMA models, we\ndemonstrate significant improvements in Word Error Rate (WER) on the\nLibriSpeech, TEDLIUM2, and WSJ corpora, achieving state-of-the-art performance\nfor CTC-based ASR with minimal computational overhead.", "AI": {"tldr": "The paper presents a novel auxiliary loss framework called LAIL to improve CTC-based ASR by integrating linguistic knowledge from LLMs, achieving state-of-the-art performance with efficient decoding.", "motivation": "To address the limitations of CTC-based models in capturing linguistic dependencies and the slow inference speed of autoregressive ASR systems.", "method": "The proposed framework attaches connector layers to intermediate encoder layers, mapping outputs to an LLM's embedding space and computing a causal language modeling loss during training.", "result": "Significant improvements in Word Error Rate (WER) on the LibriSpeech, TEDLIUM2, and WSJ corpora, demonstrating state-of-the-art performance for CTC-based ASR.", "conclusion": "LAIL enhances linguistic modeling in CTC-based ASR systems while maintaining computational efficiency, making it suitable for real-time applications.", "key_contributions": ["Introduction of Language-Aware Intermediate Loss (LAIL) for CTC-based ASR.", "Demonstrated improvements in linguistic modeling with minimal overhead.", "Achieved state-of-the-art WER performance using LLaMA models."], "limitations": "", "keywords": ["automatic speech recognition", "CTC", "large language models", "LAIL", "Conformer architecture"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2506.23815", "pdf": "https://arxiv.org/pdf/2506.23815.pdf", "abs": "https://arxiv.org/abs/2506.23815", "title": "The Impact of AI on Educational Assessment: A Framework for Constructive Alignment", "authors": ["Patrick Stokkink"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "The influence of Artificial Intelligence (AI), and specifically Large\nLanguage Models (LLM), on education is continuously increasing. These models\nare frequently used by students, giving rise to the question whether current\nforms of assessment are still a valid way to evaluate student performance and\ncomprehension. The theoretical framework developed in this paper is grounded in\nConstructive Alignment (CA) theory and Bloom's taxonomy for defining learning\nobjectives. We argue that AI influences learning objectives of different Bloom\nlevels in a different way, and assessment has to be adopted accordingly.\nFurthermore, in line with Bloom's vision, formative and summative assessment\nshould be aligned on whether the use of AI is permitted or not.\n  Although lecturers tend to agree that education and assessment need to be\nadapted to the presence of AI, a strong bias exists on the extent to which\nlecturers want to allow for AI in assessment. This bias is caused by a\nlecturer's familiarity with AI and specifically whether they use it themselves.\nTo avoid this bias, we propose structured guidelines on a university or faculty\nlevel, to foster alignment among the staff. Besides that, we argue that\nteaching staff should be trained on the capabilities and limitations of AI\ntools. In this way, they are better able to adapt their assessment methods.", "AI": {"tldr": "The paper discusses the impact of AI and Large Language Models on educational assessment, suggesting adaptations based on Bloom's taxonomy and proposing guidelines for educators.", "motivation": "To address whether current assessment methods are valid in light of AI's influence on education and to propose necessary adaptations.", "method": "Theoretical framework based on Constructive Alignment theory and Bloom's taxonomy.", "result": "It identifies a bias among lecturers regarding AI use in assessments and provides guidelines for consistent assessment practices.", "conclusion": "The adaptation of assessment methods is crucial, and structured guidelines should be developed alongside training for teaching staff on AI tools.", "key_contributions": ["Developed a theoretical framework linking AI impact on education and Bloom's taxonomy.", "Proposed structured guidelines for assessment adaptation in light of AI.", "Advocated for training teaching staff on AI tools."], "limitations": "", "keywords": ["AI in education", "Large Language Models", "Bloom's taxonomy", "Constructive Alignment", "assessment adaptation"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.22852", "pdf": "https://arxiv.org/pdf/2506.22852.pdf", "abs": "https://arxiv.org/abs/2506.22852", "title": "Knowledge Augmented Finetuning Matters in both RAG and Agent Based Dialog Systems", "authors": ["Yucheng Cai", "Yuxuan Wu", "Yi Huang", "Junlan Feng", "Zhijian Ou"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have recently been applied to dialog systems.\nDespite making progress, LLMs are prone to errors in knowledge-intensive\nscenarios. Recently, approaches based on retrieval augmented generation (RAG)\nand agent have emerged to improve the factual accuracy by enhancing the LLMs\nwith knowledge retrieved from external knowledge bases (KBs). This is mostly\nimplemented by prompting the LLMs with instructions, examples and the retrieved\nknowledge. However, LLMs may have difficulty using the retrieved knowledge\neffectively for response generation, because they are not well trained to do\nsuch generation for specific domains. To mitigate this problem, we propose to\nfinetune the LLMs in the RAG-based and agent-based systems with domain-specific\ndata, together with domain-specific external knowledge, which is called\nknowledge augmented finetuning (KAFT). We base our study on the MobileCS2\ndataset, a real-life customer service dialog dataset that features intensive\nknowledge interactions, to systematically compare the prompting and KAFT\ntechniques in the RAG-based and agent-based systems. Experiment results show\nthat KAFT substantially surpasses prompting in both RAG and agent systems,\nparticularly in terms of factual accuracy. To the best of our knowledge, this\npaper represents the first solid empirical work to investigate the KAFT idea.", "AI": {"tldr": "This paper introduces knowledge augmented finetuning (KAFT) for large language models (LLMs) in dialog systems, improving factual accuracy against traditional prompting methods.", "motivation": "LLMs struggle with knowledge-intensive tasks due to insufficient training for specific domains, leading to errors in dialog systems.", "method": "The study proposes finetuning LLMs using domain-specific data and knowledge in RAG-based and agent-based systems, evaluated using the MobileCS2 dataset.", "result": "KAFT significantly outperforms traditional prompting techniques in enhancing factual accuracy within dialog systems.", "conclusion": "The findings establish KAFT as a novel and effective approach for improving LLM performance in domain-specific applications, representing a crucial empirical contribution to the field.", "key_contributions": ["Introduction of knowledge augmented finetuning (KAFT) for LLMs.", "Systematic comparison of prompt and KAFT methods in enhancing factual accuracy.", "Empirical validation using the MobileCS2 customer service dataset."], "limitations": "", "keywords": ["large language models", "knowledge augmented finetuning", "retrieval augmented generation", "dialog systems", "factual accuracy"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.23850", "pdf": "https://arxiv.org/pdf/2506.23850.pdf", "abs": "https://arxiv.org/abs/2506.23850", "title": "Email as the Interface to Generative AI Models: Seamless Administrative Automation", "authors": ["Andres Navarro", "Carlos de Quinto", "José Alberto Hernández"], "categories": ["cs.HC"], "comment": null, "summary": "This paper introduces a novel architectural framework that integrates Large\nLanguage Models (LLMs) with email interfaces to automate administrative tasks,\nspecifically targeting accessibility barriers in enterprise environments. The\nsystem connects email communication channels with Optical Character Recognition\n(OCR) and intelligent automation, enabling non-technical administrative staff\nto delegate complex form-filling and document processing tasks using familiar\nemail interfaces. By treating the email body as a natural language prompt and\nattachments as contextual information, the workflow bridges the gap between\nadvanced AI capabilities and practical usability. Empirical evaluation shows\nthat the system can complete complex administrative forms in under 8 seconds of\nautomated processing, with human supervision reducing total staff time by a\nfactor of three to four compared to manual workflows. The top-performing LLM\naccurately filled 16 out of 29 form fields and reduced the total cost per\nprocessed form by 64% relative to manual completion. These findings demonstrate\nthat email-based LLM integration is a viable and cost-effective approach for\ndemocratizing advanced automation in organizational settings, supporting\nwidespread adoption without requiring specialized technical knowledge or major\nworkflow changes. This aligns with broader trends in leveraging LLMs to enhance\naccessibility and automate complex tasks for non-technical users, making\ntechnology more inclusive and efficient.", "AI": {"tldr": "The paper introduces a framework that integrates Large Language Models with email to automate administrative tasks, overcoming accessibility barriers for non-technical users.", "motivation": "To automate administrative tasks in enterprise environments and reduce accessibility barriers for non-technical staff.", "method": "The framework uses email communication integrated with Optical Character Recognition and intelligent automation, allowing users to delegate tasks through familiar email interfaces.", "result": "The system can complete complex administrative forms in under 8 seconds, reducing staff time by a factor of three to four compared to manual workflows and achieving a 64% cost reduction per processed form.", "conclusion": "Email-based LLM integration is a viable, cost-effective solution for automating tasks without requiring specialized knowledge, promoting inclusivity in technology use.", "key_contributions": ["Integration of LLMs with email for automation", "Significant time and cost savings in administrative tasks", "Supports accessibility for non-technical users"], "limitations": "", "keywords": ["Large Language Models", "email automation", "administrative tasks", "accessibility", "intelligent automation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.22853", "pdf": "https://arxiv.org/pdf/2506.22853.pdf", "abs": "https://arxiv.org/abs/2506.22853", "title": "DICE-BENCH: Evaluating the Tool-Use Capabilities of Large Language Models in Multi-Round, Multi-Party Dialogues", "authors": ["Kyochul Jang", "Donghyeon Lee", "Kyusik Kim", "Dongseok Heo", "Taewhoo Lee", "Woojeong Kim", "Bongwon Suh"], "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, ACL 2025 Vienna", "summary": "Existing function-calling benchmarks focus on single-turn interactions.\nHowever, they overlook the complexity of real-world scenarios. To quantify how\nexisting benchmarks address practical applications, we introduce DICE-SCORE, a\nmetric that evaluates the dispersion of tool-related information such as\nfunction name and parameter values throughout the dialogue. Analyzing existing\nbenchmarks through DICE-SCORE reveals notably low scores, highlighting the need\nfor more realistic scenarios. To address this gap, we present DICE-BENCH, a\nframework that constructs practical function-calling datasets by synthesizing\nconversations through a tool graph that maintains dependencies across rounds\nand a multi-agent system with distinct personas to enhance dialogue\nnaturalness. The final dataset comprises 1,607 high-DICE-SCORE instances. Our\nexperiments on 19 LLMs with DICE-BENCH show that significant advances are still\nrequired before such models can be deployed effectively in real-world settings.\nOur code and data are all publicly available:\nhttps://snuhcc.github.io/DICE-Bench/.", "AI": {"tldr": "The paper introduces DICE-SCORE, a new metric for evaluating function-calling benchmarks, and presents DICE-BENCH, a framework for creating realistic function-calling datasets. It finds that existing benchmarks yield low DICE-SCOREs, indicating a need for improvement, and validates DICE-BENCH with experiments on 19 large language models (LLMs).", "motivation": "To address the limitations of existing function-calling benchmarks that primarily focus on single-turn interactions and do not reflect real-world complexities, highlighting the need for a more comprehensive evaluation approach.", "method": "The paper introduces DICE-SCORE, a metric to assess how effectively existing benchmarks incorporate tool-related information throughout multi-turn dialogues. It also presents DICE-BENCH, a framework for creating datasets of realistic function-calling interactions using tool graphs and a multi-agent system.", "result": "The analysis of current benchmarks using DICE-SCORE revealed low performance, and DICE-BENCH was able to create a dataset of 1,607 instances with high DICE-SCORE values. Experiments with 19 LLMs showed that considerable improvements are necessary for practical deployment.", "conclusion": "The study underscores the necessity for realistic benchmarks in function-calling scenarios and demonstrates that while DICE-BENCH provides a high-quality dataset, existing large language models require significant enhancement to be effective in real-world applications.", "key_contributions": ["Introduction of the DICE-SCORE metric for evaluating function-calling benchmarks.", "Development of the DICE-BENCH framework for synthesizing practical function-calling datasets.", "Creation of a dataset featuring 1,607 high-DICE-SCORE instances to facilitate better evaluation of LLMs."], "limitations": "The study primarily focuses on tool-related information and may not cover all aspects of real-world interactions in functional calling.", "keywords": ["function-calling", "benchmarks", "DICE-SCORE", "DICE-BENCH", "dialogue systems"], "importance_score": 7, "read_time_minutes": 9}}
{"id": "2506.23952", "pdf": "https://arxiv.org/pdf/2506.23952.pdf", "abs": "https://arxiv.org/abs/2506.23952", "title": "Autonomy by Design: Preserving Human Autonomy in AI Decision-Support", "authors": ["Stefan Buijsman", "Sarah Carter", "Juan Pablo Bermúdez"], "categories": ["cs.HC", "cs.AI", "cs.LG", "econ.GN", "q-fin.EC"], "comment": null, "summary": "AI systems increasingly support human decision-making across domains of\nprofessional, skill-based, and personal activity. While previous work has\nexamined how AI might affect human autonomy globally, the effects of AI on\ndomain-specific autonomy -- the capacity for self-governed action within\ndefined realms of skill or expertise -- remain understudied. We analyze how AI\ndecision-support systems affect two key components of domain-specific autonomy:\nskilled competence (the ability to make informed judgments within one's domain)\nand authentic value-formation (the capacity to form genuine domain-relevant\nvalues and preferences). By engaging with prior investigations and analyzing\nempirical cases across medical, financial, and educational domains, we\ndemonstrate how the absence of reliable failure indicators and the potential\nfor unconscious value shifts can erode domain-specific autonomy both\nimmediately and over time. We then develop a constructive framework for\nautonomy-preserving AI support systems. We propose specific socio-technical\ndesign patterns -- including careful role specification, implementation of\ndefeater mechanisms, and support for reflective practice -- that can help\nmaintain domain-specific autonomy while leveraging AI capabilities. This\nframework provides concrete guidance for developing AI systems that enhance\nrather than diminish human agency within specialized domains of action.", "AI": {"tldr": "The paper analyzes the impact of AI decision-support systems on domain-specific autonomy, focusing on skilled competence and authentic value-formation, and presents a framework to preserve autonomy in AI applications.", "motivation": "To understand how AI affects domain-specific autonomy in professional and skill-based contexts, particularly in medical, financial, and educational fields.", "method": "Analyses empirical cases and engages with prior research to identify how AI influences skilled competence and authenticity in decision-making.", "result": "The study reveals that AI can erode domain-specific autonomy by removing failure indicators and leading to unconscious value shifts.", "conclusion": "A proposed framework with design patterns is suggested to help create AI systems that maintain, rather than diminish, human agency and autonomy in specialized domains.", "key_contributions": ["Analysis of AI's effect on domain-specific autonomy", "Identification of critical components of autonomy affected by AI", "Development of a framework for autonomy-preserving AI support systems"], "limitations": "", "keywords": ["AI", "Decision-support systems", "Domain-specific autonomy", "Human agency", "Socio-technical design"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.22858", "pdf": "https://arxiv.org/pdf/2506.22858.pdf", "abs": "https://arxiv.org/abs/2506.22858", "title": "Mind the Gap: Entity-Preserved Context-Aware ASR Structured Transcriptions", "authors": ["Duygu Altinok"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "This is the accepted version of an article accepted to the TSD 2025\n  conference, published in Springer Lecture Notes in Artificial Intelligence\n  (LNAI). The final authenticated version is available online at SpringerLink", "summary": "Automatic Speech Recognition (ASR) systems, such as Whisper, achieve high\ntranscription accuracy but struggle with named entities and numerical data,\nespecially when proper formatting is required. These issues increase word error\nrate (WER) and impair semantic understanding in critical domains like legal,\nfinancial, and medical applications. We propose a novel training approach that\nextends the semantic context of ASR models by adding overlapping context\nwindows during training. By sliding 5-second overlaps on both sides of\n30-second chunks, we create a 40-second \"effective semantic window,\" improving\nentity recognition and formatting while focusing predictions on the central 30\nseconds. To address entities spanning chunk boundaries, we reassign such\nentities entirely to the right-hand chunk, ensuring proper formatting.\nAdditionally, enriched training data with embedded entity labels enables the\nmodel to learn both recognition and type-specific formatting. Evaluated on the\nSpoken Wikipedia dataset, our method improves performance across semantic\ntasks, including named entity recognition (NER) and entity formatting. These\nresults highlight the effectiveness of context-aware training in addressing ASR\nlimitations for long-form transcription and complex entity recognition tasks.", "AI": {"tldr": "Proposes a novel training approach to improve ASR entity recognition and formatting using extended semantic context.", "motivation": "To address high word error rates in ASR systems, particularly for named entities and numerical data in critical domains like legal, financial, and medical applications.", "method": "Introduces overlapping context windows during training with 5-second overlaps on both sides of 30-second chunks, resulting in an effective 40-second semantic window.", "result": "The method enhances performance on semantic tasks such as named entity recognition (NER) and entity formatting, as demonstrated on the Spoken Wikipedia dataset.", "conclusion": "Context-aware training significantly mitigates ASR limitations in long-form transcription and complex entity recognition tasks.", "key_contributions": ["Novel overlapping context training approach for ASR", "Improved named entity recognition and formatting", "Use of enriched training data with embedded entity labels"], "limitations": "", "keywords": ["Automatic Speech Recognition", "Named Entity Recognition", "Entity Formatting"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.24057", "pdf": "https://arxiv.org/pdf/2506.24057.pdf", "abs": "https://arxiv.org/abs/2506.24057", "title": "Access InContext: Futuring Accessible Prototyping Tools and Methods", "authors": ["Patricia Piedade", "Peter A Hayton", "Cynthia Bennett", "Anna R L Carter", "Clara Crivellaro", "Alan Dix", "Jess McGowan", "Katta Spiel", "Miriam Sturdee", "Garreth W. Tigwell", "Hugo Nicolau"], "categories": ["cs.HC"], "comment": null, "summary": "The popularity of accessibility research has grown recently, improving\ndigital inclusion for people with disabilities. However, researchers, including\nthose who have disabilities, have attempted to include people with disabilities\nin all aspects of design, and they have identified a myriad of practical\naccessibility barriers posed by tools and methods leveraged by human-computer\ninteraction (HCI) researchers during prototyping. To build a more inclusive\ntechnological landscape, we must question the effectiveness of existing\nprototyping tools and methods, repurpose/retrofit existing resources, and build\nnew tools and methods to support the participation of both researchers and\npeople with disabilities within the prototyping design process of novel\ntechnologies. This full-day workshop at CHI 2025 will provide a platform for\nHCI researchers, designers, and practitioners to discuss barriers and\nopportunities for creating accessible prototyping and promote hands-on ideation\nand fabrication exercises aimed at futuring accessible prototyping.", "AI": {"tldr": "A workshop at CHI 2025 focusing on improving accessibility in prototyping methods for HCI researchers through discussion and hands-on exercises.", "motivation": "To increase digital inclusion for people with disabilities and improve the effectiveness of prototyping tools in HCI research.", "method": "Facilitating discussions among HCI researchers, designers, and practitioners, and conducting hands-on ideation and fabrication exercises.", "result": "Identification of accessibility barriers in current prototyping practices and generation of new tools and methods for more inclusive design.", "conclusion": "A call for collaborative efforts to create accessible prototyping methods that engage both researchers and individuals with disabilities.", "key_contributions": ["Platform for discussion on accessibility in prototyping", "Hands-on ideation and fabrication exercises", "Focus on retrofitting existing tools for accessibility"], "limitations": "", "keywords": ["accessibility", "human-computer interaction", "prototyping", "digital inclusion", "design methods"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2506.22957", "pdf": "https://arxiv.org/pdf/2506.22957.pdf", "abs": "https://arxiv.org/abs/2506.22957", "title": "Agent-to-Agent Theory of Mind: Testing Interlocutor Awareness among Large Language Models", "authors": ["Younwoo Choi", "Changling Li", "Yongjin Yang", "Zhijing Jin"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.MA"], "comment": null, "summary": "As large language models (LLMs) are increasingly integrated into multi-agent\nand human-AI systems, understanding their awareness of both self-context and\nconversational partners is essential for ensuring reliable performance and\nrobust safety. While prior work has extensively studied situational awareness\nwhich refers to an LLM's ability to recognize its operating phase and\nconstraints, it has largely overlooked the complementary capacity to identify\nand adapt to the identity and characteristics of a dialogue partner. In this\npaper, we formalize this latter capability as interlocutor awareness and\npresent the first systematic evaluation of its emergence in contemporary LLMs.\nWe examine interlocutor inference across three dimensions-reasoning patterns,\nlinguistic style, and alignment preferences-and show that LLMs reliably\nidentify same-family peers and certain prominent model families, such as GPT\nand Claude. To demonstrate its practical significance, we develop three case\nstudies in which interlocutor awareness both enhances multi-LLM collaboration\nthrough prompt adaptation and introduces new alignment and safety\nvulnerabilities, including reward-hacking behaviors and increased jailbreak\nsusceptibility. Our findings highlight the dual promise and peril of\nidentity-sensitive behavior in LLMs, underscoring the need for further\nunderstanding of interlocutor awareness and new safeguards in multi-agent\ndeployments. Our code is open-sourced at\nhttps://github.com/younwoochoi/InterlocutorAwarenessLLM.", "AI": {"tldr": "This paper evaluates the capacity of large language models (LLMs) to exhibit interlocutor awareness—their ability to identify and adapt to the identity and characteristics of dialogue partners—highlighting its implications for multi-agent interactions.", "motivation": "To ensure reliable performance and safety in multi-agent systems employing LLMs, it is crucial to understand not only situational awareness but also the capacity for interlocutor awareness.", "method": "The authors systematically evaluated LLMs' interlocutor awareness along three dimensions: reasoning patterns, linguistic style, and alignment preferences, alongside case studies demonstrating its effects.", "result": "The study found that LLMs can identify same-family peers and notable model families with reliability, affecting collaboration and introducing alignment and safety risks.", "conclusion": "While interlocutor awareness offers potential for improved LLM collaboration, it also poses new vulnerabilities, necessitating further investigation and protective measures in multi-agent applications.", "key_contributions": ["Formalization of interlocutor awareness concept in LLMs", "Systematic evaluation of LLMs' response to interlocutor identity", "Case studies illustrating practical implications of interlocutor awareness"], "limitations": "The study may not cover all aspects of LLM interpersonal dynamics and requires further exploration of its complexities.", "keywords": ["interlocutor awareness", "large language models", "multi-agent systems", "safety", "collaboration"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.24104", "pdf": "https://arxiv.org/pdf/2506.24104.pdf", "abs": "https://arxiv.org/abs/2506.24104", "title": "Bridging Service Design, Visualizations, and Visual Analytics in Healthcare Digital Twins: Challenges, Gaps, and Research Opportunities", "authors": ["Mariia Ershova", "Graziano Blasilli"], "categories": ["cs.HC"], "comment": "Submitted to: Workshop on Visual Analytics in Healthcare (VAHC 2025)", "summary": "Digital twins (DT) are increasingly used in healthcare to model patients,\nprocesses, and physiological systems. While recent solutions leverage\nvisualization, visual analytics, and user interaction, these systems rarely\nincorporate structured service design methodologies. Bridging service design\nwith visual analytics and visualization can be valuable for the healthcare DT\ncommunity. This paper aims to introduce the service design discipline to\nvisualization researchers by framing this integration gap and suggesting\nresearch directions to enhance the real-world applicability of DT solutions.", "AI": {"tldr": "The paper proposes integrating service design methodologies with visual analytics and visualization in healthcare digital twins to improve applicability.", "motivation": "To address the gap between service design and visualization in healthcare digital twins.", "method": "The paper discusses the current use of digital twins in healthcare, identifying the lack of structured service design methodologies in existing solutions.", "result": "Suggests research directions that can enhance the real-world applicability of digital twin solutions in healthcare.", "conclusion": "Integrating service design into the visualization of healthcare digital twins can improve their effectiveness and usability.", "key_contributions": ["Introduces service design to visualization researchers in healthcare", "Identifies a gap in current digital twin methodologies", "Proposes new research directions for better applicability of digital twins"], "limitations": "", "keywords": ["Digital Twins", "Healthcare", "Service Design", "Visual Analytics", "Visualization"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.22977", "pdf": "https://arxiv.org/pdf/2506.22977.pdf", "abs": "https://arxiv.org/abs/2506.22977", "title": "On the Generalizability of \"Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals\"", "authors": ["Asen Dotsinski", "Udit Thakur", "Marko Ivanov", "Mohammad Hafeez Khan", "Maria Heuss"], "categories": ["cs.CL", "cs.LG"], "comment": "22 pages, 25 figures. For an interactive dashboard with all figures,\n  see https://comp-mech-generalizability.streamlit.app/ . For the accompanying\n  code, see https://github.com/asendotsinski/comp-mech-generalizability . To be\n  published in proceedings of the 2025 Machine Learning Reproducibility\n  Challenge", "summary": "We present a reproduction study of \"Competition of Mechanisms: Tracing How\nLanguage Models Handle Facts and Counterfactuals\" (Ortu et al., 2024), which\ninvestigates competition of mechanisms in language models between factual\nrecall and counterfactual in-context repetition. Our study successfully\nreproduces their primary findings regarding the localization of factual and\ncounterfactual information, the dominance of attention blocks in mechanism\ncompetition, and the specialization of attention heads in handling competing\ninformation. We reproduce their results on both GPT-2 (Radford et al., 2019)\nand Pythia 6.9B (Biderman et al., 2023). We extend their work in three\nsignificant directions. First, we explore the generalizability of these\nfindings to even larger models by replicating the experiments on Llama 3.1 8B\n(Grattafiori et al., 2024), discovering greatly reduced attention head\nspecialization. Second, we investigate the impact of prompt structure by\nintroducing variations where we avoid repeating the counterfactual statement\nverbatim or we change the premise word, observing a marked decrease in the\nlogit for the counterfactual token. Finally, we test the validity of the\nauthors' claims for prompts of specific domains, discovering that certain\ncategories of prompts skew the results by providing the factual prediction\ntoken as part of the subject of the sentence. Overall, we find that the\nattention head ablation proposed in Ortu et al. (2024) is ineffective for\ndomains that are underrepresented in their dataset, and that the effectiveness\nvaries based on model architecture, prompt structure, domain and task.", "AI": {"tldr": "This paper reproduces findings from 'Competition of Mechanisms' regarding language models' handling of facts and counterfactuals, extending the research to larger models and different prompt structures.", "motivation": "To understand how language models navigate factual recall and counterfactual reasoning, and to extend previous findings from Ortu et al. (2024).", "method": "The study reproduces experiments on GPT-2 and Pythia 6.9B, then extends findings by testing Llama 3.1 8B and analyzing prompt structure variations.", "result": "Findings indicate reduced attention head specialization in larger models, and that certain prompt variations significantly affect counterfactual predictions.", "conclusion": "Attention head ablation methods are less effective for underrepresented domains, and results vary based on model architecture, prompt structure, and task.", "key_contributions": ["Successful reproduction of previous findings on newer models", "Investigation of prompt structure effects on model performance", "Analysis of attention head specialization across different model sizes"], "limitations": "The effectiveness of findings was observed to vary based on model architecture and specific domains, as some prompts skewed results.", "keywords": ["Language Models", "Counterfactuals", "Attention Mechanisms", "Prompt Structure", "Model Generalizability"], "importance_score": 9, "read_time_minutes": 22}}
{"id": "2506.22978", "pdf": "https://arxiv.org/pdf/2506.22978.pdf", "abs": "https://arxiv.org/abs/2506.22978", "title": "A Systematic Study of Compositional Syntactic Transformer Language Models", "authors": ["Yida Zhao", "Hao Xve", "Xiang Hu", "Kewei Tu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Syntactic language models (SLMs) enhance Transformers by incorporating\nsyntactic biases through the modeling of linearized syntactic parse trees\nalongside surface sentences. This paper focuses on compositional SLMs that are\nbased on constituency parse trees and contain explicit bottom-up composition of\nconstituent representations. We identify key aspects of design choices in\nexisting compositional SLMs and propose a unified framework encompassing both\nexisting models and novel variants. We conduct a comprehensive empirical\nevaluation of all the variants in our framework across language modeling,\nsyntactic generalization, summarization, dialogue, and inference efficiency.\nBased on the experimental results, we make multiple recommendations on the\ndesign of compositional SLMs. Our code is released at\nhttps://github.com/zhaoyd1/compositional_SLMs.", "AI": {"tldr": "This paper presents a unified framework for compositional syntactic language models (SLMs) that incorporate constituency parse trees, emphasizing design choices and empirical evaluations across various tasks.", "motivation": "To improve Transformers by integrating syntactic information through compositional syntactic language models and address design issues within existing models.", "method": "The paper proposes a unified framework for compositional SLMs focusing on constituency parse trees and conducts a detailed empirical evaluation across language modeling, syntactic generalization, summarization, dialogue, and inference efficiency.", "result": "The empirical evaluation revealed insights into the performance of various compositional SLM variants, leading to recommendations on effective design choices for such models.", "conclusion": "The work enhances understanding of compositional SLMs and offers guidelines for future implementations to optimize language modeling tasks.", "key_contributions": ["A unified framework encompassing existing and novel compositional SLMs", "Empirical evaluation of SLM variants across multiple tasks", "Recommendations for the design of compositional SLMs."], "limitations": "The framework and recommendations may primarily focus on constituency parse trees and might not generalize to all parsing approaches or applications.", "keywords": ["syntactic language models", "compositional SLMs", "Transformers", "parse trees", "language modeling"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.23046", "pdf": "https://arxiv.org/pdf/2506.23046.pdf", "abs": "https://arxiv.org/abs/2506.23046", "title": "SoMi-ToM: Evaluating Multi-Perspective Theory of Mind in Embodied Social Interactions", "authors": ["Xianzhe Fan", "Xuhui Zhou", "Chuanyang Jin", "Kolby Nottingham", "Hao Zhu", "Maarten Sap"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.RO"], "comment": "23 pages, 6 figures", "summary": "Humans continuously infer the states, goals, and behaviors of others by\nperceiving their surroundings in dynamic, real-world social interactions.\nHowever, most Theory of Mind (ToM) benchmarks only evaluate static, text-based\nscenarios, which have a significant gap compared to real interactions. We\npropose the SoMi-ToM benchmark, designed to evaluate multi-perspective ToM in\nembodied multi-agent complex social interactions. This benchmark is based on\nrich multimodal interaction data generated by the interaction environment SoMi,\ncovering diverse crafting goals and social relationships. Our framework\nsupports multi-level evaluation: (1) first-person evaluation provides\nmultimodal (visual, dialogue, action, etc.) input from a first-person\nperspective during a task for real-time state inference, (2) third-person\nevaluation provides complete third-person perspective video and text records\nafter a task for goal and behavior inference. This evaluation method allows for\na more comprehensive examination of a model's ToM capabilities from both the\nsubjective immediate experience and the objective global observation. We\nconstructed a challenging dataset containing 35 third-person perspective\nvideos, 363 first-person perspective images, and 1225 expert-annotated\nmultiple-choice questions (three options). On this dataset, we systematically\nevaluated the performance of human subjects and several state-of-the-art large\nvision-language models (LVLMs). The results show that LVLMs perform\nsignificantly worse than humans on SoMi-ToM: the average accuracy gap between\nhumans and models is 40.1% in first-person evaluation and 26.4% in third-person\nevaluation. This indicates that future LVLMs need to further improve their ToM\ncapabilities in embodied, complex social interactions.", "AI": {"tldr": "The SoMi-ToM benchmark evaluates multi-perspective Theory of Mind in complex social interactions using multimodal data.", "motivation": "To address the gap between static, text-based ToM benchmarks and real-world interactions by creating a more dynamic evaluation method.", "method": "The paper introduces the SoMi-ToM benchmark, which includes first-person and third-person evaluations utilizing rich multimodal data from the SoMi environment, encompassing diverse crafting goals and social relationships.", "result": "The dataset comprises 35 third-person videos, 363 first-person images, and 1225 annotated questions. Evaluation shows a significant accuracy gap between humans and state-of-the-art LVLMs, indicating the need for LVLMs to improve ToM capabilities.", "conclusion": "SoMi-ToM provides a comprehensive framework for assessing ToM in social interactions, revealing LVLMs' limitations in these contexts and emphasizing the need for advancements in this area.", "key_contributions": ["Introduction of SoMi-ToM benchmark for multi-perspective ToM evaluation.", "Use of multimodal interaction data for a richer assessment of social interactions.", "Demonstrated performance gap between humans and LVLMs, highlighting areas for improvement."], "limitations": "The benchmark may not cover all potential social scenarios and interactions in real life.", "keywords": ["Theory of Mind", "multimodal interaction", "embodied interactions", "large vision-language models", "social dynamics"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.23051", "pdf": "https://arxiv.org/pdf/2506.23051.pdf", "abs": "https://arxiv.org/abs/2506.23051", "title": "MariNER: A Dataset for Historical Brazilian Portuguese Named Entity Recognition", "authors": ["João Lucas Luz Lima Sarcinelli", "Marina Lages Gonçalves Teixeira", "Jade Bortot de Paiva", "Diego Furtado Silva"], "categories": ["cs.CL"], "comment": null, "summary": "Named Entity Recognition (NER) is a fundamental Natural Language Processing\n(NLP) task that aims to identify and classify entity mentions in texts across\ndifferent categories. While languages such as English possess a large number of\nhigh-quality resources for this task, Brazilian Portuguese still lacks in\nquantity of gold-standard NER datasets, especially when considering specific\ndomains. Particularly, this paper considers the importance of NER for analyzing\nhistorical texts in the context of digital humanities. To address this gap,\nthis work outlines the construction of MariNER: \\textit{Mapeamento e\nAnota\\c{c}\\~oes de Registros hIst\\'oricos para NER} (Mapping and Annotation of\nHistorical Records for NER), the first gold-standard dataset for early\n20th-century Brazilian Portuguese, with more than 9,000 manually annotated\nsentences. We also assess and compare the performance of state-of-the-art NER\nmodels for the dataset.", "AI": {"tldr": "The paper presents MariNER, a gold-standard Named Entity Recognition dataset for early 20th-century Brazilian Portuguese, addressing the lack of quality resources for NER in this language.", "motivation": "The motivation is to fill the gap in available high-quality NER datasets for Brazilian Portuguese, particularly for analyzing historical texts in digital humanities.", "method": "The authors constructed the MariNER dataset with over 9,000 manually annotated sentences focusing on early 20th-century Brazilian Portuguese.", "result": "The paper includes an assessment and comparison of the performance of state-of-the-art NER models on the MariNER dataset.", "conclusion": "The creation of MariNER represents a significant step toward improving NER tasks for Brazilian Portuguese, enabling more effective analysis of historical texts.", "key_contributions": ["Introduction of MariNER, the first gold-standard NER dataset for Brazilian Portuguese.", "Manually annotated dataset with over 9,000 sentences.", "Performance evaluation of top NER models on the dataset."], "limitations": "", "keywords": ["Named Entity Recognition", "Brazilian Portuguese", "Historical texts", "Digital humanities", "NLP"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2506.23056", "pdf": "https://arxiv.org/pdf/2506.23056.pdf", "abs": "https://arxiv.org/abs/2506.23056", "title": "Boosting LLM's Molecular Structure Elucidation with Knowledge Enhanced Tree Search Reasoning", "authors": ["Xiang Zhuang", "Bin Wu", "Jiyu Cui", "Kehua Feng", "Xiaotong Li", "Huabin Xing", "Keyan Ding", "Qiang Zhang", "Huajun Chen"], "categories": ["cs.CL"], "comment": "ACL 2025 Main", "summary": "Molecular structure elucidation involves deducing a molecule's structure from\nvarious types of spectral data, which is crucial in chemical experimental\nanalysis. While large language models (LLMs) have shown remarkable proficiency\nin analyzing and reasoning through complex tasks, they still encounter\nsubstantial challenges in molecular structure elucidation. We identify that\nthese challenges largely stem from LLMs' limited grasp of specialized chemical\nknowledge. In this work, we introduce a Knowledge-enhanced reasoning framework\nfor Molecular Structure Elucidation (K-MSE), leveraging Monte Carlo Tree Search\nfor test-time scaling as a plugin. Specifically, we construct an external\nmolecular substructure knowledge base to extend the LLMs' coverage of the\nchemical structure space. Furthermore, we design a specialized\nmolecule-spectrum scorer to act as a reward model for the reasoning process,\naddressing the issue of inaccurate solution evaluation in LLMs. Experimental\nresults show that our approach significantly boosts performance, particularly\ngaining more than 20% improvement on both GPT-4o-mini and GPT-4o. Our code is\navailable at https://github.com/HICAI-ZJU/K-MSE.", "AI": {"tldr": "This paper presents K-MSE, a Knowledge-enhanced reasoning framework for improving molecular structure elucidation using LLMs and Monte Carlo Tree Search.", "motivation": "Molecular structure elucidation is vital in chemical analysis but is hindered by LLMs' limited chemical knowledge.", "method": "The study introduces K-MSE, which incorporates a molecular substructure knowledge base and a molecule-spectrum scorer to enhance reasoning capabilities.", "result": "Experimental results demonstrate over 20% improvement in performance on GPT-4o-mini and GPT-4o models.", "conclusion": "The K-MSE framework significantly enhances LLMs' ability to tackle molecular structure elucidation by providing specialized knowledge and evaluation methods.", "key_contributions": ["Introduction of K-MSE framework", "Integration of a molecular substructure knowledge base", "Development of a specialized molecule-spectrum scorer"], "limitations": "", "keywords": ["Molecular Structure Elucidation", "Large Language Models", "Knowledge-enhanced reasoning"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.23071", "pdf": "https://arxiv.org/pdf/2506.23071.pdf", "abs": "https://arxiv.org/abs/2506.23071", "title": "Text2VectorSQL: Bridging Text-to-SQL and Vector Search for Unified Natural Language Queries", "authors": ["Zhengren Wang", "Bozhou Li", "Dongwen Yao", "Wentao Zhang"], "categories": ["cs.CL"], "comment": "Work in progess", "summary": "While Text-to-SQL enables natural language interaction with structured\ndatabases, its effectiveness diminishes with unstructured data or ambiguous\nqueries due to rigid syntax and limited expressiveness. Concurrently, vector\nsearch has emerged as a powerful paradigm for semantic retrieval, particularly\nfor unstructured data. However, existing VectorSQL implementations still rely\nheavily on manual crafting and lack tailored evaluation frameworks, leaving a\nsignificant gap between theoretical potential and practical deployment. To\nbridge these complementary paradigms, we introduces Text2VectorSQL, a novel\nframework unifying Text-to-SQL and vector search to overcome expressiveness\nconstraints and support more diverse and holistical natural language queries.\nSpecifically, Text2VectorSQL enables semantic filtering, multi-modal matching,\nand retrieval acceleration. For evaluation, we build vector index on\nappropriate columns, extend user queries with semantic search, and annotate\nground truths via an automatic pipeline with expert review. Furthermore, we\ndevelop dedicated Text2VectorSQL models with synthetic data, demonstrating\nsignificant performance improvements over baseline methods. Our work\nestablishes the foundation for the Text2VectorSQL task, paving the way for more\nversatile and intuitive database interfaces. The repository will be publicly\navailable at https://github.com/Open-DataFlow/Text2VectorSQL.", "AI": {"tldr": "Text2VectorSQL unifies Text-to-SQL and vector search for improved natural language interaction with databases.", "motivation": "To address the limitations of existing Text-to-SQL systems with unstructured data and ambiguous queries.", "method": "Introduced a framework that unifies Text-to-SQL and vector search, allowing for semantic filtering and multi-modal matching, and built an evaluation framework with automatic ground truth annotations.", "result": "Demonstrated significant performance improvements over baseline methods with dedicated Text2VectorSQL models using synthetic data.", "conclusion": "Text2VectorSQL establishes a foundation for versatile database interfaces, enhancing natural language query handling.", "key_contributions": ["Unified Text-to-SQL and vector search into a single framework.", "Developed a dedicated evaluation framework for Text2VectorSQL.", "Achieved performance improvements in retrieval tasks compared to existing methods."], "limitations": "Results are based on synthetic data; real-world performance may vary.", "keywords": ["Text-to-SQL", "vector search", "natural language queries", "semantic retrieval", "database interfaces"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.23101", "pdf": "https://arxiv.org/pdf/2506.23101.pdf", "abs": "https://arxiv.org/abs/2506.23101", "title": "From Individuals to Interactions: Benchmarking Gender Bias in Multimodal Large Language Models from the Lens of Social Relationship", "authors": ["Yue Xu", "Wenjie Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multimodal large language models (MLLMs) have shown impressive capabilities\nacross tasks involving both visual and textual modalities. However, growing\nconcerns remain about their potential to encode and amplify gender bias,\nparticularly in socially sensitive applications. Existing benchmarks\npredominantly evaluate bias in isolated scenarios, overlooking how bias may\nemerge subtly through interpersonal interactions. We fill this gap by going\nbeyond single-entity evaluation and instead focusing on a deeper examination of\nrelational and contextual gender bias in dual-individual interactions. We\nintroduce Genres, a novel benchmark designed to evaluate gender bias in MLLMs\nthrough the lens of social relationships in generated narratives. Genres\nassesses gender bias through a dual-character profile and narrative generation\ntask that captures rich interpersonal dynamics and supports a fine-grained bias\nevaluation suite across multiple dimensions. Experiments on both open- and\nclosed-source MLLMs reveal persistent, context-sensitive gender biases that are\nnot evident in single-character settings. Our findings underscore the\nimportance of relationship-aware benchmarks for diagnosing subtle,\ninteraction-driven gender bias in MLLMs and provide actionable insights for\nfuture bias mitigation.", "AI": {"tldr": "This paper introduces Genres, a benchmark for evaluating gender bias in multimodal large language models (MLLMs) through dual-individual interactions, revealing context-sensitive biases that are often overlooked.", "motivation": "To address concerns about gender bias in MLLMs, especially in socially sensitive contexts, and to evaluate the biases that emerge in interpersonal interactions rather than isolated scenarios.", "method": "The study introduces a benchmark called Genres that focuses on gender bias evaluation in dual-character profiles and narrative generation tasks, emphasizing rich interpersonal dynamics.", "result": "Experiments showed persistent, context-sensitive gender biases in MLLMs, which are not evident when evaluating single-character interactions.", "conclusion": "The paper highlights the necessity of using relationship-aware benchmarks for a more nuanced understanding of gender bias in MLLMs and provides guidance for future bias mitigation efforts.", "key_contributions": ["Introduction of a novel benchmark (Genres) for gender bias evaluation in MLLMs.", "Focus on relational and contextual dynamics in interpersonal interactions.", "Provision of fine-grained evaluation tools for assessing bias in multiple dimensions."], "limitations": "The study may not cover all possible relational contexts and might be limited to the scenarios tested in the benchmark.", "keywords": ["gender bias", "multimodal large language models", "benchmark", "interpersonal interactions", "narrative generation"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2506.23111", "pdf": "https://arxiv.org/pdf/2506.23111.pdf", "abs": "https://arxiv.org/abs/2506.23111", "title": "FairI Tales: Evaluation of Fairness in Indian Contexts with a Focus on Bias and Stereotypes", "authors": ["Janki Atul Nawale", "Mohammed Safi Ur Rahman Khan", "Janani D", "Mansi Gupta", "Danish Pruthi", "Mitesh M. Khapra"], "categories": ["cs.CL"], "comment": "Accepted in ACL 2025", "summary": "Existing studies on fairness are largely Western-focused, making them\ninadequate for culturally diverse countries such as India. To address this gap,\nwe introduce INDIC-BIAS, a comprehensive India-centric benchmark designed to\nevaluate fairness of LLMs across 85 identity groups encompassing diverse\ncastes, religions, regions, and tribes. We first consult domain experts to\ncurate over 1,800 socio-cultural topics spanning behaviors and situations,\nwhere biases and stereotypes are likely to emerge. Grounded in these topics, we\ngenerate and manually validate 20,000 real-world scenario templates to probe\nLLMs for fairness. We structure these templates into three evaluation tasks:\nplausibility, judgment, and generation. Our evaluation of 14 popular LLMs on\nthese tasks reveals strong negative biases against marginalized identities,\nwith models frequently reinforcing common stereotypes. Additionally, we find\nthat models struggle to mitigate bias even when explicitly asked to rationalize\ntheir decision. Our evaluation provides evidence of both allocative and\nrepresentational harms that current LLMs could cause towards Indian identities,\ncalling for a more cautious usage in practical applications. We release\nINDIC-BIAS as an open-source benchmark to advance research on benchmarking and\nmitigating biases and stereotypes in the Indian context.", "AI": {"tldr": "This paper introduces INDIC-BIAS, a benchmark for evaluating the fairness of LLMs in India, addressing biases across diverse socio-cultural groups.", "motivation": "Existing fairness studies predominantly focus on Western contexts, leaving a significant gap for culturally diverse countries like India.", "method": "The authors curated 1,800 socio-cultural topics with experts and created 20,000 validated scenario templates for evaluation, structured into plausibility, judgment, and generation tasks.", "result": "Evaluation of 14 popular LLMs revealed strong negative biases against marginalized Indian identities, with frequent reinforcement of stereotypes and difficulties in mitigating bias even with explicit prompts.", "conclusion": "The findings highlight the need for cautious application of LLMs in India due to evidences of allocative and representational harms, and the authors release INDIC-BIAS as an open-source tool for further research.", "key_contributions": ["Introduction of INDIC-BIAS benchmark for Indian context", "Curated a diverse set of socio-cultural topics for bias evaluation", "Open-source release to advance fairness benchmarking in LLMs"], "limitations": "The study is limited to the Indian context and may not generalize to other regions; it also relies on the quality and representativeness of curated topics.", "keywords": ["fairness", "large language models", "bias evaluation", "India", "socio-cultural topics"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.23122", "pdf": "https://arxiv.org/pdf/2506.23122.pdf", "abs": "https://arxiv.org/abs/2506.23122", "title": "Decoding Memes: Benchmarking Narrative Role Classification across Multilingual and Multimodal Models", "authors": ["Shivam Sharma", "Tanmoy Chakraborty"], "categories": ["cs.CL", "cs.CY"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "This work investigates the challenging task of identifying narrative roles -\nHero, Villain, Victim, and Other - in Internet memes, across three diverse test\nsets spanning English and code-mixed (English-Hindi) languages. Building on an\nannotated dataset originally skewed toward the 'Other' class, we explore a more\nbalanced and linguistically diverse extension, originally introduced as part of\nthe CLEF 2024 shared task. Comprehensive lexical and structural analyses\nhighlight the nuanced, culture-specific, and context-rich language used in real\nmemes, in contrast to synthetically curated hateful content, which exhibits\nexplicit and repetitive lexical markers. To benchmark the role detection task,\nwe evaluate a wide spectrum of models, including fine-tuned multilingual\ntransformers, sentiment and abuse-aware classifiers, instruction-tuned LLMs,\nand multimodal vision-language models. Performance is assessed under zero-shot\nsettings using precision, recall, and F1 metrics. While larger models like\nDeBERTa-v3 and Qwen2.5-VL demonstrate notable gains, results reveal consistent\nchallenges in reliably identifying the 'Victim' class and generalising across\ncultural and code-mixed content. We also explore prompt design strategies to\nguide multimodal models and find that hybrid prompts incorporating structured\ninstructions and role definitions offer marginal yet consistent improvements.\nOur findings underscore the importance of cultural grounding, prompt\nengineering, and multimodal reasoning in modelling subtle narrative framings in\nvisual-textual content.", "AI": {"tldr": "The paper explores identifying narrative roles in internet memes using multilingual models and prompt design strategies, highlighting cultural and contextual nuances in language.", "motivation": "To improve role detection in internet memes, particularly in cross-cultural and code-mixed contexts, given the complexities introduced by language and cultural nuances.", "method": "The authors investigate various models, including multilingual transformers and multimodal models, analyzing performance in a zero-shot setting across multiple datasets.", "result": "Larger models show better performance, but challenges persist with the 'Victim' class and generalizing across diverse cultural content. Hybrid prompt strategies yield modest improvements.", "conclusion": "Cultural grounding, prompt engineering, and multimodal reasoning are crucial for effective role identification in memes.", "key_contributions": ["Development of a balanced dataset for meme role identification", "Evaluation of various model architectures under zero-shot conditions", "Empirical evidence for the effectiveness of hybrid prompting techniques."], "limitations": "Challenges in accurately identifying the 'Victim' narrative role and generalizing results across cultural contexts.", "keywords": ["narrative roles", "internet memes", "machine learning", "multimodal models", "prompt engineering"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.23127", "pdf": "https://arxiv.org/pdf/2506.23127.pdf", "abs": "https://arxiv.org/abs/2506.23127", "title": "Unleashing Embodied Task Planning Ability in LLMs via Reinforcement Learning", "authors": ["Zhaoye Fei", "Li Ji", "Siyin Wang", "Junhao Shi", "Jingjing Gong", "Xipeng Qiu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, yet they face significant challenges in embodied task planning\nscenarios that require continuous environmental understanding and action\ngeneration. Existing approaches generate open-loop action scripts based on\nstatic knowledge, making it difficult to learn causal relationships between\nactions and environmental feedback, particularly in partially observable\nenvironments. We introduce Embodied Planner-R1, a novel outcome-driven\nreinforcement learning framework that enables LLMs to develop interactive\ncapabilities through autonomous exploration with minimal supervision. Our\nframework incorporates three key innovations: (1) Without human annotations, we\nemploy pure reinforcement learning with group rollout, incorporating\nin-environment interaction through parallel exploration; (2) completion-driven\nsparse reward; and (3) Interactive Policy Optimization (IPO) for efficient\nlearning from grouped trajectories. Across two challenging text-based Embodied\nplanning benchmarks, Embodied Planner-R1 achieves impressive completion rates\nof 97.78% on ALFWorld and 79.92% on ScienceWorld, surpassing prior methods by a\nlarge margin, and suffers only a -3.66% drop in previously unseen environments,\nevidencing strong generalization.", "AI": {"tldr": "Embodied Planner-R1 is a novel reinforcement learning framework for Large Language Models focused on embodied task planning, achieving high completion rates in interactive capabilities.", "motivation": "To enhance the ability of Large Language Models in embodied task planning, addressing the challenges of continuous environmental understanding and action generation in partially observable environments.", "method": "The framework utilizes pure reinforcement learning with group rollout and incorporates interactive policy optimization for efficient learning, allowing LLMs to explore environments with minimal supervision.", "result": "Embodied Planner-R1 achieves a completion rate of 97.78% on the ALFWorld benchmark and 79.92% on ScienceWorld, greatly exceeding past performance and demonstrating strong generalization with only a -3.66% drop in unseen environments.", "conclusion": "The proposed framework shows significant improvements in embodied task planning for LLMs, suggesting a promising direction for enhancing interactive capabilities through minimal supervision and efficient exploration.", "key_contributions": ["Introduction of a novel outcome-driven reinforcement learning framework for embodied task planning.", "Implementation of pure reinforcement learning with group rollout allowing minimal supervision.", "Development of Interactive Policy Optimization for learning from grouped trajectories."], "limitations": "", "keywords": ["Large Language Models", "Reinforcement Learning", "Embodied Task Planning", "Interactive Learning", "Policy Optimization"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2506.23133", "pdf": "https://arxiv.org/pdf/2506.23133.pdf", "abs": "https://arxiv.org/abs/2506.23133", "title": "Format-Adapter: Improving Reasoning Capability of LLMs by Adapting Suitable Format", "authors": ["Dingzirui Wang", "Xuanliang Zhang", "Rongyu Cao", "Longxu Dou", "Xianzhen Luo", "Yingwei Ma", "Qingfu Zhu", "Wanxiang Che", "Binhua Li", "Fei Huang", "Yongbin Li"], "categories": ["cs.CL"], "comment": null, "summary": "Generating and voting multiple answers is an effective method to mitigate\nreasoning inconsistencies of large language models (LLMs). Prior works have\nshown that multiple reasoning formats outperform a single format when\ngenerating multiple answers. However, previous works using multiple formats\nrely on formats labeled by humans, which could be unsuitable for all tasks and\nhave high labeling costs. To address this issue, we adapt suitable formats to\nthe given tasks by generating and selecting formats. We first propose how to\nmeasure the reasoning error when generating multiple answers. Then, we\nintroduce Format-Adapter, which utilizes LLMs to generate and select suitable\nreasoning formats by minimizing the error measurement we present. We conduct\nexperiments on math and commonsense reasoning tasks, where Format-Adapter\nachieves a 4.3% performance improvement on average over previous works,\ndemonstrating the effectiveness.", "AI": {"tldr": "The paper introduces Format-Adapter, a method using LLMs to generate and select suitable reasoning formats, improving LLM performance by addressing reasoning inconsistencies in tasks with multiple answers.", "motivation": "To mitigate reasoning inconsistencies in LLMs by generating and voting on multiple answers without relying on costly human-labeled formats.", "method": "The authors propose a method to measure reasoning errors and develop Format-Adapter, which uses LLMs for generating and selecting reasoning formats based on these error measurements.", "result": "Format-Adapter achieves an average 4.3% performance improvement on math and commonsense reasoning tasks compared to previous methods.", "conclusion": "The effectiveness of Format-Adapter in selecting appropriate reasoning formats demonstrates its potential to enhance LLM performance in reasoning tasks.", "key_contributions": ["Introduction of Format-Adapter for reasoning format selection using LLMs", "Method for measuring reasoning error in generated answers", "Achievement of improved performance on reasoning tasks over existing works"], "limitations": "", "keywords": ["Large Language Models", "Reasoning Consistency", "Format Adaptation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.23136", "pdf": "https://arxiv.org/pdf/2506.23136.pdf", "abs": "https://arxiv.org/abs/2506.23136", "title": "LLM-Assisted Question-Answering on Technical Documents Using Structured Data-Aware Retrieval Augmented Generation", "authors": ["Shadman Sobhan", "Mohammad Ariful Haque"], "categories": ["cs.CL"], "comment": "29 Pages, 11 Tables", "summary": "Large Language Models (LLMs) are capable of natural language understanding\nand generation. But they face challenges such as hallucination and outdated\nknowledge. Fine-tuning is one possible solution, but it is resource-intensive\nand must be repeated with every data update. Retrieval-Augmented Generation\n(RAG) offers an efficient solution by allowing LLMs to access external\nknowledge sources. However, traditional RAG pipelines struggle with retrieving\ninformation from complex technical documents with structured data such as\ntables and images. In this work, we propose a RAG pipeline, capable of handling\ntables and images in documents, for technical documents that support both\nscanned and searchable formats. Its retrieval process combines vector\nsimilarity search with a fine-tuned reranker based on Gemma-2-9b-it. The\nreranker is trained using RAFT (Retrieval-Augmented Fine-Tuning) on a custom\ndataset designed to improve context identification for question answering. Our\nevaluation demonstrates that the proposed pipeline achieves a high faithfulness\nscore of 94% (RAGas) and 96% (DeepEval), and an answer relevancy score of 87%\n(RAGas) and 93% (DeepEval). Comparative analysis demonstrates that the proposed\narchitecture is superior to general RAG pipelines in terms of table-based\nquestions and handling questions outside context.", "AI": {"tldr": "The paper presents a novel Retrieval-Augmented Generation (RAG) pipeline for processing complex technical documents that include tables and images, demonstrating superior performance over traditional RAG methods.", "motivation": "To address the limitations of traditional RAG pipelines in retrieving information from structured technical documents containing tables and images.", "method": "The proposed pipeline utilizes vector similarity search combined with a fine-tuned reranker based on Gemma-2-9b-it, trained using RAFT on a custom dataset aimed at improving context identification for question answering.", "result": "The evaluation shows a high faithfulness score of 94% (RAGas) and 96% (DeepEval), with answer relevancy scores of 87% (RAGas) and 93% (DeepEval), outperforming general RAG pipelines.", "conclusion": "The proposed RAG pipeline significantly enhances the retrieval accuracy and handling of complex queries related to structured data in technical documents.", "key_contributions": ["Introduction of a RAG pipeline tailored for structured technical documents", "Utilization of a fine-tuned reranker for improved context identification", "Demonstration of superior performance in answering table-based questions"], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Large Language Models", "technical documents", "context identification", "RAG pipeline"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.23137", "pdf": "https://arxiv.org/pdf/2506.23137.pdf", "abs": "https://arxiv.org/abs/2506.23137", "title": "Flow-Modulated Scoring for Semantic-Aware Knowledge Graph Completion", "authors": ["Siyuan Li", "Ruitong Liu", "Yan Wen", "Te Sun"], "categories": ["cs.CL", "cs.AI"], "comment": "10 pages", "summary": "Effective modeling of multifaceted relations is pivotal for Knowledge Graph\nCompletion (KGC). However, a majority of existing approaches are predicated on\nstatic, embedding-based scoring, exhibiting inherent limitations in capturing\ncontextual dependencies and relational dynamics. Addressing this gap, we\npropose the Flow-Modulated Scoring (FMS) framework. FMS comprises two principal\ncomponents: (1) a semantic context learning module that encodes\ncontext-sensitive entity representations, and (2) a conditional flow-matching\nmodule designed to learn the dynamic transformation from a head to a tail\nembedding, governed by the aforementioned context. The resultant predictive\nvector field, representing the context-informed relational path, serves to\ndynamically refine the initial static score of an entity pair. Through this\nsynergy of context-aware static representations and conditioned dynamic\ninformation, FMS facilitates a more profound modeling of relational semantics.\nComprehensive evaluations on several standard benchmarks demonstrate that our\nproposed method surpasses prior state-of-the-art results.", "AI": {"tldr": "Proposes a Flow-Modulated Scoring framework for Knowledge Graph Completion that improves modeling of contextual dependencies and relational dynamics.", "motivation": "To improve the limitations of static, embedding-based approaches in Knowledge Graph Completion, particularly in capturing relational dynamics and contextual dependencies.", "method": "The Flow-Modulated Scoring (FMS) framework includes a semantic context learning module for encoding context-sensitive entity representations and a conditional flow-matching module for learning dynamic transformations between entity embeddings based on context.", "result": "FMS produces a predictive vector field that allows for dynamic refinement of the initial static scores of entity pairs, demonstrating superior performance on standard benchmarks.", "conclusion": "The proposed FMS framework facilitates a deeper modeling of relational semantics and outperforms existing state-of-the-art methods in Knowledge Graph Completion.", "key_contributions": ["Introduction of Flow-Modulated Scoring framework for KGC", "Semantic context learning module for dynamic entity representation", "Conditional flow-matching module for relational dynamics"], "limitations": "", "keywords": ["Knowledge Graph Completion", "Flow-Modulated Scoring", "Context-aware representations"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.23139", "pdf": "https://arxiv.org/pdf/2506.23139.pdf", "abs": "https://arxiv.org/abs/2506.23139", "title": "Benchmarking Deep Search over Heterogeneous Enterprise Data", "authors": ["Prafulla Kumar Choubey", "Xiangyu Peng", "Shilpa Bhagavath", "Kung-Hsiang Huang", "Caiming Xiong", "Chien-Sheng Wu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present a new benchmark for evaluating Deep Search--a realistic and\ncomplex form of retrieval-augmented generation (RAG) that requires\nsource-aware, multi-hop reasoning over diverse, sparsed, but related sources.\nThese include documents, meeting transcripts, Slack messages, GitHub, and URLs,\nwhich vary in structure and often contain human-to-human interactions. We build\nit using a synthetic data pipeline that simulates business workflows across\nproduct planning, development, and support stages, generating interconnected\ncontent with realistic noise and multi-hop questions with guaranteed\nground-truth answers. We release our benchmark with both answerable and\nunanswerable queries, and retrieval pool of 39,190 enterprise artifacts,\nenabling fine-grained evaluation of long-context LLM and RAG systems. Our\nexperiments reveal that even the best-performing agentic RAG methods achieve an\naverage performance score of 32.96 on our benchmark. With further analysis, we\nhighlight retrieval as the main bottleneck: existing methods struggle to\nconduct deep searches and retrieve all necessary evidence. Consequently, they\noften reason over partial context, leading to significant performance\ndegradation.", "AI": {"tldr": "The paper introduces a benchmark for evaluating Deep Search in RAG systems, revealing significant challenges in retrieval performance.", "motivation": "To provide a realistic benchmark for evaluating the performance of retrieval-augmented generation (RAG) systems that engage in complex reasoning over diverse sources.", "method": "A synthetic data pipeline is used to generate interconnected content that simulates business workflows and includes both answerable and unanswerable queries, alongside a retrieval pool comprising 39,190 enterprise artifacts.", "result": "The best-performing RAG methods achieve an average score of 32.96, indicating that retrieval limitations significantly impact performance due to incomplete context.", "conclusion": "The findings suggest that improving retrieval methods is critical for enabling effective deep searches in RAG systems.", "key_contributions": ["Introduction of a new benchmark for evaluating Deep Search in RAG systems.", "Release of 39,190 enterprise artifacts for fine-grained evaluation.", "Identification of retrieval as a main bottleneck in current RAG performance."], "limitations": "The study primarily focuses on retrieval challenges without deeply exploring algorithmic advancements.", "keywords": ["Deep Search", "retrieval-augmented generation", "benchmark", "multi-hop reasoning", "enterprise artifacts"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.23146", "pdf": "https://arxiv.org/pdf/2506.23146.pdf", "abs": "https://arxiv.org/abs/2506.23146", "title": "Learning-to-Context Slope: Evaluating In-Context Learning Effectiveness Beyond Performance Illusions", "authors": ["Dingzriui Wang", "Xuanliang Zhang", "Keyan Xu", "Qingfu Zhu", "Wanxiang Che", "Yang Deng"], "categories": ["cs.CL"], "comment": null, "summary": "In-context learning (ICL) has emerged as an effective approach to enhance the\nperformance of large language models (LLMs). However, its effectiveness varies\nsignificantly across models and tasks, posing challenges for practitioners to\ndetermine when ICL reliably improves performance. Current evaluation\napproaches, reliant on performance change after applying ICL, suffer from low\nreliability, poor attribution, and impracticality in data-insufficient\nscenarios. We propose the Learning-to-Context Slope (LCS), a novel metric that\nquantifies ICL effectiveness by modeling the slope between learning gain (loss\ndecrease from demonstrations) and contextual relevance (demonstration-input\nrelevance). LCS addresses key limitations of performance-based metrics: (1) it\ncaptures continuous loss changes even when outputs are incorrect, improving\nreliability; (2) its formulation attributes ICL failures to weak contextual\nalignment (inability to adapt inputs to demonstrations) or strong output\ncalibration (self-verification of correctness); and (3) it minimizes reliance\non labeled data via synthetic evaluation. Extensive experiments demonstrate\nthat LCS strongly correlates with performance improvements in labeled settings\nand reliably reflects true effectiveness in biased or data-scarce scenarios.\nFurther analysis reveals actionable thresholds for LCS and identifies model\ncapabilities critical to ICL success.", "AI": {"tldr": "This paper introduces the Learning-to-Context Slope (LCS) metric for evaluating the effectiveness of in-context learning (ICL) in large language models (LLMs), addressing limitations of current performance-based evaluation methods.", "motivation": "The motivation behind this work is to improve the reliability and practicality of evaluating in-context learning (ICL) for large language models (LLMs), which currently relies on performance changes that can be unreliable.", "method": "The authors propose the Learning-to-Context Slope (LCS), a metric that quantifies ICL effectiveness by modeling the slope between learning gain and contextual relevance. It captures continuous changes in loss, attributes failures to contextual alignment and output calibration, and minimizes reliance on labeled data through synthetic evaluation.", "result": "Extensive experiments show that LCS correlates strongly with performance improvements in labeled datasets and reliably reflects true effectiveness in biased or data-scarce scenarios.", "conclusion": "The LCS metric significantly improves the evaluation of in-context learning by providing actionable insights and reducing dependence on labeled data, thus aiding practitioners in assessing ICL's impact more effectively.", "key_contributions": ["Introduction of the Learning-to-Context Slope (LCS) metric for ICL evaluation", "Improvement of reliability in assessing in-context learning effectiveness", "Identification of critical model capabilities for successful ICL applications."], "limitations": "Potential limitations may include the synthetic evaluation dependence and varying effectiveness across different models and tasks.", "keywords": ["in-context learning", "large language models", "evaluation metrics", "synthetic evaluation", "machine learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.23149", "pdf": "https://arxiv.org/pdf/2506.23149.pdf", "abs": "https://arxiv.org/abs/2506.23149", "title": "V-SYNTHESIS: Task-Agnostic Synthesis of Consistent and Diverse In-Context Demonstrations from Scratch via V-Entropy", "authors": ["Dingzirui Wang", "Xuanliang Zhang", "Keyan Xu", "Qingfu Zhu", "Wanxiang Che", "Yang Deng"], "categories": ["cs.CL"], "comment": null, "summary": "High labeling cost for in-context learning (ICL) demonstrations motivates\nusing large language models (LLMs) for synthesis to reduce overhead. However,\nexisting synthesis methods are mainly task-specific or rely on pre-existing\ndemonstrations. So this paper focuses on synthesizing demonstrations from\nscratch for arbitrary tasks. A major challenge in synthesizing from scratch is\nensuring consistency with the target task, as the lack of labeling guidance\ncould lead to synthesis bias. We first propose a consistency metric called\nV-Score, which has higher performance and lower computation cost compared with\nthe metrics based on grams or embedding vectors. Furthermore, we introduce\nV-Synthesis, which leverages V-Score for proportional sampling to ensure both\nhigh consistency and diversity of synthesized demonstrations. Experimental\nresults demonstrate that V-Synthesis yields an average performance improvement\nof 2.0% compared to existing synthesis methods confirming the effectiveness of\nV-Synthesis.", "AI": {"tldr": "This paper introduces V-Synthesis, a method for synthesizing demonstrations from scratch using a new consistency metric, V-Score, to enhance consistency and diversity in in-context learning (ICL).", "motivation": "The high labeling cost for in-context learning demonstrations drives the need for methods to synthesize these demonstrations using large language models.", "method": "The paper proposes V-Score, a consistency metric that outperforms existing metrics in performance and computational cost. V-Synthesis utilizes this metric for proportional sampling in synthesizing demonstrations.", "result": "V-Synthesis shows an average performance improvement of 2.0% over existing synthesis methods, validating its efficacy in ensuring high consistency and diversity.", "conclusion": "The proposed V-Synthesis method effectively reduces labeling overhead while maintaining high-quality synthesized demonstrations.", "key_contributions": ["Introduction of V-Score as a consistency metric.", "Development of V-Synthesis for synthesizing demonstrations from scratch.", "Demonstration of performance improvement in ICL tasks."], "limitations": "", "keywords": ["in-context learning", "demonstration synthesis", "V-Score", "V-Synthesis", "language models"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.23192", "pdf": "https://arxiv.org/pdf/2506.23192.pdf", "abs": "https://arxiv.org/abs/2506.23192", "title": "RiverText: A Python Library for Training and Evaluating Incremental Word Embeddings from Text Data Streams", "authors": ["Gabriel Iturra-Bocaz", "Felipe Bravo-Marquez"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted at SIGIR'23", "summary": "Word embeddings have become essential components in various information\nretrieval and natural language processing tasks, such as ranking, document\nclassification, and question answering. However, despite their widespread use,\ntraditional word embedding models present a limitation in their static nature,\nwhich hampers their ability to adapt to the constantly evolving language\npatterns that emerge in sources such as social media and the web (e.g., new\nhashtags or brand names). To overcome this problem, incremental word embedding\nalgorithms are introduced, capable of dynamically updating word representations\nin response to new language patterns and processing continuous data streams.\n  This paper presents RiverText, a Python library for training and evaluating\nincremental word embeddings from text data streams. Our tool is a resource for\nthe information retrieval and natural language processing communities that work\nwith word embeddings in streaming scenarios, such as analyzing social media.\nThe library implements different incremental word embedding techniques, such as\nSkip-gram, Continuous Bag of Words, and Word Context Matrix, in a standardized\nframework. In addition, it uses PyTorch as its backend for neural network\ntraining. We have implemented a module that adapts existing intrinsic static\nword embedding evaluation tasks for word similarity and word categorization to\na streaming setting. Finally, we compare the implemented methods with different\nhyperparameter settings and discuss the results. Our open-source library is\navailable at https://github.com/dccuchile/rivertext.", "AI": {"tldr": "This paper introduces RiverText, a Python library for training and evaluating incremental word embeddings from continuous text data streams, addressing the static nature of traditional word embeddings.", "motivation": "The static nature of traditional word embedding models limits their effectiveness in adapting to evolving language patterns found in social media and other dynamic sources.", "method": "RiverText implements various incremental word embedding techniques within a standardized framework, utilizing PyTorch for neural network training and adapting tasks for streaming settings.", "result": "RiverText enables effective training and evaluation of incremental word embeddings, enhancing performance in dynamic language environments, with comparative results presented for various hyperparameter settings.", "conclusion": "The RiverText library serves as a valuable resource for the information retrieval and NLP communities needing to analyze streaming data, with ongoing development opportunities: it is open-source and available on GitHub.", "key_contributions": ["Introduction of RiverText for incremental word embeddings", "Standardized framework using various embedding techniques", "Adaptation of intrinsic evaluation tasks for streaming settings"], "limitations": "", "keywords": ["word embeddings", "incremental learning", "natural language processing", "streaming data", "RiverText"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.23235", "pdf": "https://arxiv.org/pdf/2506.23235.pdf", "abs": "https://arxiv.org/abs/2506.23235", "title": "Generalist Reward Models: Found Inside Large Language Models", "authors": ["Yi-Chen Li", "Tian Xu", "Yang Yu", "Xuqin Zhang", "Xiong-Hui Chen", "Zhongxiang Ling", "Ningjing Chao", "Lei Yuan", "Zhi-Hua Zhou"], "categories": ["cs.CL"], "comment": null, "summary": "The alignment of Large Language Models (LLMs) is critically dependent on\nreward models trained on costly human preference data. While recent work\nexplores bypassing this cost with AI feedback, these methods often lack a\nrigorous theoretical foundation. In this paper, we discover that a powerful\ngeneralist reward model is already latently present within any LLM trained via\nstandard next-token prediction. We prove that this endogenous reward is not a\nheuristic, but is theoretically equivalent to a reward function learned through\noffline inverse reinforcement learning. This connection allows us to directly\nelicit a high-quality reward signal from a base (pre-trained or supervised\nfine-tuned) model without any further training. Critically, we also prove that\nsubsequent reinforcement learning using this endogenous reward leads to a\npolicy with a provably superior error bound compared to the base model. To our\nbest knowledge, this is the first theoretical proof of the effectiveness of\nreinforcement learning for LLMs. Our experiments validate this theory,\ndemonstrating that our method not only outperforms existing LLM-as-a-judge\napproaches but can also surpass explicitly trained reward models. These\nfindings suggest that the reward modeling stage can be replaced by a principled\nmethod of eliciting the knowledge already captured during pre-training,\nheralding a more efficient, powerful, and scalable paradigm for LLMs alignment\nas well as multi-modal models.", "AI": {"tldr": "This paper demonstrates that Large Language Models (LLMs) contain a latent reward model that can be utilized for reinforcement learning without additional training, proving its effectiveness theoretically and experimentally.", "motivation": "The paper addresses the costly nature of training reward models for aligning LLMs with human preferences, exploring a more efficient solution utilizing existing LLMs.", "method": "The authors establish a theoretical framework linking the latent reward model in LLMs to classical inverse reinforcement learning, allowing direct elicitation of a high-quality reward signal from pre-trained models without further training.", "result": "The experiments confirm that the proposed reinforcement learning approach using the endogenous reward from LLMs achieves superior performance compared to existing LLM-as-a-judge methods and trained reward models.", "conclusion": "The study provides a theoretical basis for leveraging the inherent reward models in LLMs, suggesting a paradigm shift towards more efficient alignment strategies for LLMs and multi-modal models.", "key_contributions": ["Proves the equivalence of latent rewards in LLMs to rewards learned through inverse reinforcement learning.", "Demonstrates that no additional training is required to elicit high-quality rewards from LLMs.", "Establishes a theoretical superiority for the resultant policies using these endogenous rewards."], "limitations": "", "keywords": ["Large Language Models", "reward models", "reinforcement learning", "inverse reinforcement learning", "alignment"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2503.18243", "pdf": "https://arxiv.org/pdf/2503.18243.pdf", "abs": "https://arxiv.org/abs/2503.18243", "title": "A Robot-Led Intervention for Emotion Regulation: From Expression to Reappraisal", "authors": ["Guy Laban", "Julie Wang", "Hatice Gunes"], "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "Emotion regulation is a crucial skill for managing emotions in everyday life,\nyet finding a constructive and accessible method to support these processes\nremains challenging due to their cognitive demands. In this study, we explore\nhow regular interactions with a social robot, conducted in a structured yet\nfamiliar environment within university halls and departments, can provide\neffective support for emotion regulation through cognitive reappraisal.\nTwenty-one students participated in a five-session study at a university hall\nor department, where the robot, powered by a large language model (GPT-3.5),\nfacilitated structured conversations, encouraging the students to reinterpret\nemotionally charged situations they shared with the robot. Quantitative and\nqualitative results indicate significant improvements in emotion\nself-regulation, with participants reporting better understanding and control\nof their emotions. The intervention led to significant changes in constructive\nemotion regulation tendencies and positive effects on mood and sentiment after\neach session. The findings also demonstrate that repeated interactions with the\nrobot encouraged greater emotional expressiveness, including longer speech\ndisclosures, increased use of affective language, and heightened facial\narousal. Notably, expressiveness followed structured patterns aligned with the\nreappraisal process, with expression peaking during key reappraisal moments,\nparticularly when participants were prompted to reinterpret negative\nexperiences. The qualitative feedback further highlighted how the robot\nfostered introspection and provided a supportive space for discussing emotions,\nenabling participants to confront long-avoided emotional challenges. These\nfindings demonstrate the potential of robots to effectively assist in emotion\nregulation in familiar environments, offering both emotional support and\ncognitive guidance.", "AI": {"tldr": "This study investigates how a social robot, utilizing GPT-3.5, can assist university students in emotion regulation through structured conversations.", "motivation": "To find effective methods for supporting emotion regulation processes, which are cognitively demanding.", "method": "Twenty-one students engaged in five sessions with a social robot in a university environment, discussing emotionally charged situations to encourage cognitive reappraisal.", "result": "Participants showed significant improvements in emotion self-regulation, increased emotional expressiveness, and positive feedback on mood after sessions.", "conclusion": "The findings suggest that robots can provide valuable emotional support and cognitive guidance for effective emotion regulation in familiar settings.", "key_contributions": ["Demonstrated the effectiveness of a social robot in facilitating emotion regulation through structured conversations.", "Showed significant improvements in participants' emotional self-control and expressiveness after interactions with the robot.", "Provided insights into the patterns of emotional expression related to the reappraisal process."], "limitations": "", "keywords": ["emotion regulation", "social robot", "cognitive reappraisal", "affective language", "GPT-3.5"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.23288", "pdf": "https://arxiv.org/pdf/2506.23288.pdf", "abs": "https://arxiv.org/abs/2506.23288", "title": "Two Spelling Normalization Approaches Based on Large Language Models", "authors": ["Miguel Domingo", "Francisco Casacuberta"], "categories": ["cs.CL"], "comment": null, "summary": "The absence of standardized spelling conventions and the organic evolution of\nhuman language present an inherent linguistic challenge within historical\ndocuments, a longstanding concern for scholars in the humanities. Addressing\nthis issue, spelling normalization endeavors to align a document's orthography\nwith contemporary standards. In this study, we propose two new approaches based\non large language models: one of which has been trained without a supervised\ntraining, and a second one which has been trained for machine translation. Our\nevaluation spans multiple datasets encompassing diverse languages and\nhistorical periods, leading us to the conclusion that while both of them\nyielded encouraging results, statistical machine translation still seems to be\nthe most suitable technology for this task.", "AI": {"tldr": "This study explores spelling normalization in historical documents using two novel approaches based on large language models and compares their effectiveness with statistical machine translation.", "motivation": "The study addresses the challenge of inconsistencies in spelling conventions in historical documents, which is significant for scholarly research in the humanities.", "method": "Two approaches based on large language models were proposed: one without supervised training and another trained for machine translation, evaluated on various datasets across languages and time periods.", "result": "Both approaches demonstrated encouraging results, but statistical machine translation was found to be the most effective technology for spelling normalization.", "conclusion": "The findings suggest that while large language models offer some promise, traditional statistical methods are currently superior for this specific task.", "key_contributions": ["Introduction of two novel approaches for spelling normalization using large language models.", "Evaluation across diverse historical datasets to validate the effectiveness of the proposed methods.", "Comparison with statistical machine translation techniques.", "Encouragement for further exploration in the intersection of NLP and historical linguistics."], "limitations": "The study may not account for all linguistic variations and nuances present in historical documents, and results may vary with different languages.", "keywords": ["spelling normalization", "large language models", "historical documents", "machine translation", "natural language processing"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.23293", "pdf": "https://arxiv.org/pdf/2506.23293.pdf", "abs": "https://arxiv.org/abs/2506.23293", "title": "Objective-Free Local Learning and Emergent Language Structure in Thinking Machines", "authors": ["P. Myles Eugenio"], "categories": ["cs.CL", "cs.AI", "cs.LG", "q-bio.NC"], "comment": "22 pages, 7 figures", "summary": "We present a neuro-symbolic framework for generative language modeling based\non local, event-driven emergent learning. At its core is a hierarchical\nHopfield memory chain acting as a compositional short-term memory and dynamic\ntokenizer (retokenizer). Rather than relying on predefined tokens or\nsupervision, the model builds structure from scratch, learning symbol sequences\nas multi-scale representations. It constructs projection tensors that bind\nco-occurring features into hierarchical tokens, introducing redundancy (i.e an\nemergent gauge structure) and enabling compression of local activations into\nlong-range dependencies. Curiously, we find that the retokenizer can filter\nnatural language patterns from noise, generating synthetic languages with\ncoherent internal morphology -- quantifiably the same as human language.\nLanguage is learned in a local (Hebbian) fashion, where model constraints\ndictate allowed emergent structure, and new information is retained in\nalignment with this structure. The absence of a global objective enables a form\nof plasticity not found in conventional language models, allowing the system to\ngeneralize beyond its initial inference class -- even without explicit data. We\ndemonstrate that briefly activating a new neuron during inference binds\ndistributed multi-scale token features into a symbolic embedding. These\nemergent embedding neurons act as long-term memory and support a key-value\nmechanism for compositional inference and generalization. This architecture\nprovides a methodological foundation for studying how symbolic structure can\nemerge from local neural learning. It offers a new pathway for building\nscalable, interpretable neuro-symbolic systems -- where tokens, grammar, and\nreasoning arise as compressed memory traces within a Hopfield hierarchy. This\napproach advances the development of neuromorphic architectures for generative\nlanguage models.", "AI": {"tldr": "A neuro-symbolic framework for generative language modeling utilizes a hierarchical Hopfield memory to learn language structure without predefined tokens, enabling emergent tokenization and long-range dependency modeling.", "motivation": "The study aims to create a generative language model that learns symbolic structures from local neural interactions, addressing limitations of conventional models that rely on predefined tokens and supervision.", "method": "The framework employs a hierarchical Hopfield memory chain for compositional short-term memory and dynamic tokenization, learning symbol sequences through multi-scale representations and local learning methods.", "result": "The model effectively filters noise to generate synthetic languages with coherent internal morphology, similar to human language, and shows the capacity for generalization beyond its training data.", "conclusion": "This architecture indicates a novel approach for developing scalable and interpretable neuro-symbolic systems, potentially influencing the future designs of neuromorphic architectures for language generation.", "key_contributions": ["Introduces a neuro-symbolic model that constructs language structure without predefined tokens.", "Demonstrates emergent tokenization leading to coherent synthetic languages akin to human languages.", "Offers a new methodological foundation for scalable and interpretable generative language models."], "limitations": "", "keywords": ["neuro-symbolic", "generative language modeling", "Hopfield memory", "emergent learning", "symbolic structure"], "importance_score": 8, "read_time_minutes": 22}}
{"id": "2506.07193", "pdf": "https://arxiv.org/pdf/2506.07193.pdf", "abs": "https://arxiv.org/abs/2506.07193", "title": "earEOG via Periauricular Electrodes to Facilitate Eye Tracking in a Natural Headphone Form Factor", "authors": ["Tobias King", "Michael Knierim", "Philipp Lepold", "Christopher Clarke", "Hans Gellersen", "Michael Beigl", "Tobias Röddiger"], "categories": ["cs.HC"], "comment": "12 pages", "summary": "Eye tracking technology is frequently utilized to diagnose eye and\nneurological disorders, assess sleep and fatigue, study human visual\nperception, and enable novel gaze-based interaction methods. However,\ntraditional eye tracking methodologies are constrained by bespoke hardware that\nis often cumbersome to wear, complex to apply, and demands substantial\ncomputational resources. To overcome these limitations, we investigated\nElectrooculography (EOG) eye tracking using 14 electrodes positioned around the\nears, integrated into a custom-built headphone form factor device. In a\ncontrolled experiment, 16 participants tracked stimuli designed to induce\nsmooth pursuits and saccades. Data analysis identified optimal electrode pairs\nfor vertical and horizontal eye movement tracking, benchmarked against\ngold-standard EOG and camera-based methods. The electrode montage nearest the\neyes yielded the best horizontal results. Horizontal smooth pursuits via earEOG\nshowed high correlation with gold-standard measures ($r_{\\mathrm{EOG}} = 0.81,\np = 0.01$; $r_{\\mathrm{CAM}} = 0.56, p = 0.02$), while vertical pursuits were\nweakly correlated ($r_{\\mathrm{EOG}} = 0.28, p = 0.04$; $r_{\\mathrm{CAM}} =\n0.35, p = 0.05$). Voltage deflections when performing saccades showed strong\ncorrelation in the horizontal direction ($r_{\\mathrm{left}} = 0.99, p = 0.0$;\n$r_{\\mathrm{right}} = 0.99, p = 0.0$) but low correlation in the vertical\ndirection ($r_{\\mathrm{up}} = 0.6, p = 0.23$; $r_{\\mathrm{down}} = 0.19, p =\n0.73$). Overall, horizontal earEOG demonstrated strong performance, indicating\nits potential effectiveness, while vertical earEOG results were poor,\nsuggesting limited feasibility in our current setup.", "AI": {"tldr": "This paper investigates an innovative ear-based Electrooculography (EOG) eye tracking method using a headphone form factor, evaluating its performance against traditional methods for both horizontal and vertical eye movements.", "motivation": "To address the limitations of traditional eye tracking techniques that require cumbersome hardware and substantial computational resources, we explore a novel ear-based EOG approach.", "method": "A controlled experiment was conducted with 16 participants using custom-built headphones with 14 electrodes to track eye movements through smooth pursuits and saccades, comparing results with gold-standard methods.", "result": "The best performance was observed for horizontal tracking via earEOG, showing strong correlation with gold-standard measures, while vertical tracking proved less effective with weak correlations detected.", "conclusion": "The findings suggest that while horizontal earEOG is promising for eye movement tracking, vertical tracking needs significant improvement for practical use.", "key_contributions": ["Introduction of a novel ear-based EOG eye tracking method", "Demonstration of the effectiveness of the setup for horizontal movements", "Comparison of earEOG results against both gold-standard EOG and camera methods"], "limitations": "Vertical eye movement tracking was found to be poor, indicating issues with setup feasibility in current configurations.", "keywords": ["Electrooculography", "Eye tracking", "Human-Computer Interaction", "Gaze-based interaction", "Neurological assessment"], "importance_score": 7, "read_time_minutes": 12}}
{"id": "2506.23315", "pdf": "https://arxiv.org/pdf/2506.23315.pdf", "abs": "https://arxiv.org/abs/2506.23315", "title": "Ensemble BERT for Medication Event Classification on Electronic Health Records (EHRs)", "authors": ["Shouvon Sarker", "Xishuang Dong", "Lijun Qian"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Identification of key variables such as medications, diseases, relations from\nhealth records and clinical notes has a wide range of applications in the\nclinical domain. n2c2 2022 provided shared tasks on challenges in natural\nlanguage processing for clinical data analytics on electronic health records\n(EHR), where it built a comprehensive annotated clinical data Contextualized\nMedication Event Dataset (CMED). This study focuses on subtask 2 in Track 1 of\nthis challenge that is to detect and classify medication events from clinical\nnotes through building a novel BERT-based ensemble model. It started with\npretraining BERT models on different types of big data such as Wikipedia and\nMIMIC. Afterwards, these pretrained BERT models were fine-tuned on CMED\ntraining data. These fine-tuned BERT models were employed to accomplish\nmedication event classification on CMED testing data with multiple predictions.\nThese multiple predictions generated by these fine-tuned BERT models were\nintegrated to build final prediction with voting strategies. Experimental\nresults demonstrated that BERT-based ensemble models can effectively improve\nstrict Micro-F score by about 5% and strict Macro-F score by about 6%,\nrespectively.", "AI": {"tldr": "This study develops a BERT-based ensemble model to detect and classify medication events from clinical notes using the Contextualized Medication Event Dataset (CMED).", "motivation": "To improve the extraction and classification of medication events from clinical notes in electronic health records (EHR) for better clinical data analytics.", "method": "The study involved pretraining BERT models on various big data sources and fine-tuning them on CMED training data, followed by integrating predictions from multiple fine-tuned models using voting strategies.", "result": "The BERT-based ensemble model achieved improvements of approximately 5% in strict Micro-F score and 6% in strict Macro-F score on the CMED test set.", "conclusion": "Using an ensemble approach with pretrained BERT models enhances the classification of medication events in clinical notes, showing potential for better clinical data analytics.", "key_contributions": ["Developed a BERT-based ensemble model for medication event classification", "Demonstrated significant improvement in F scores using ensemble predictions", "Applied the model to a comprehensive annotated clinical dataset (CMED)"], "limitations": "", "keywords": ["BERT", "medication event classification", "natural language processing", "clinical data analytics", "CMED"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2506.23340", "pdf": "https://arxiv.org/pdf/2506.23340.pdf", "abs": "https://arxiv.org/abs/2506.23340", "title": "Information Loss in LLMs' Multilingual Translation: The Role of Training Data, Language Proximity, and Language Family", "authors": ["Yumeng Lin", "Xufeng Duan", "David Haslett", "Yige Chen", "Zhenguang G. Cai"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models have achieved impressive progress in multilingual\ntranslation, yet they continue to face challenges with certain language\npairs-particularly those with limited training data or significant linguistic\ndivergence from English. This study systematically investigates how training\ndata, language proximity, and language family affect information loss in\nmultilingual translation. We evaluate two large language models, GPT-4 and\nLlama 2, by performing round-trip translations. Translation quality was\nassessed using BLEU scores and BERT similarity metrics. Our results reveal a\nrobust interaction between training data size and language distance: while\nabundant training data can mitigate the effects of linguistic divergence,\nlanguages structurally closer to English consistently yield higher translation\nquality in low-resource conditions. Among various distance metrics,\northographic, phylogenetic, syntactic, and geographical distances emerge as\nstrong predictors of translation performance. Language family also exerts an\nindependent influence. These findings contribute to a deeper understanding of\nthe linguistic constraints shaping multilingual translation in large language\nmodels, emphasizing that translation quality is shaped not only by data volume\nbut also by structural and typological relationships between languages.", "AI": {"tldr": "This study examines the impact of training data, language proximity, and language family on information loss in multilingual translation using GPT-4 and Llama 2.", "motivation": "To understand how various factors affect translation quality in multilingual settings, especially for language pairs with limited training data.", "method": "Round-trip translations were performed using two large language models, GPT-4 and Llama 2. Translation quality was assessed with BLEU scores and BERT similarity metrics.", "result": "The results show a significant interaction between training data size and language distance, with closer languages yielding better translations in low-resource conditions.", "conclusion": "Translation quality is influenced not just by the amount of training data but also by the structural and typological relationships between languages, informing future multilingual model training.", "key_contributions": ["Identification of factors affecting multilingual translation quality", "Analysis of various distance metrics as predictors of translation performance", "Demonstration of the interaction between training data size and language proximity"], "limitations": "", "keywords": ["multilingual translation", "large language models", "GPT-4", "Llama 2", "translation quality"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.23342", "pdf": "https://arxiv.org/pdf/2506.23342.pdf", "abs": "https://arxiv.org/abs/2506.23342", "title": "ATGen: A Framework for Active Text Generation", "authors": ["Akim Tsvigun", "Daniil Vasilev", "Ivan Tsvigun", "Ivan Lysenko", "Talgat Bektleuov", "Aleksandr Medvedev", "Uliana Vinogradova", "Nikita Severin", "Mikhail Mozikov", "Andrey Savchenko", "Rostislav Grigorev", "Ramil Kuleev", "Fedor Zhdanov", "Artem Shelmanov", "Ilya Makarov"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ACL 2025 System Demonstrations", "summary": "Active learning (AL) has demonstrated remarkable potential in reducing the\nannotation effort required for training machine learning models. However,\ndespite the surging popularity of natural language generation (NLG) tasks in\nrecent years, the application of AL to NLG has been limited. In this paper, we\nintroduce Active Text Generation (ATGen) - a comprehensive framework that\nbridges AL with text generation tasks, enabling the application of\nstate-of-the-art AL strategies to NLG. Our framework simplifies AL-empowered\nannotation in NLG tasks using both human annotators and automatic annotation\nagents based on large language models (LLMs). The framework supports LLMs\ndeployed as services, such as ChatGPT and Claude, or operated on-premises.\nFurthermore, ATGen provides a unified platform for smooth implementation and\nbenchmarking of novel AL strategies tailored to NLG tasks. Finally, we present\nevaluation results for state-of-the-art AL strategies across diverse settings\nand multiple text generation tasks. We show that ATGen reduces both the effort\nof human annotators and costs associated with API calls to LLM-based annotation\nagents. The code of the framework is available on GitHub under the MIT license.\nThe video presentation is available at http://atgen-video.nlpresearch.group", "AI": {"tldr": "Introduction of Active Text Generation (ATGen), a framework integrating Active Learning with natural language generation tasks.", "motivation": "To reduce annotation efforts in training ML models for NLG tasks, leveraging Active Learning.", "method": "ATGen provides a framework that utilizes Active Learning strategies for NLG tasks, incorporating both human and automatic annotation through LLMs.", "result": "ATGen significantly reduces the effort required by human annotators and lowers costs from API calls to LLM annotation agents, validated through evaluation results across different settings.", "conclusion": "The framework offers a unified platform for implementing and testing novel AL strategies tailored for NLG.", "key_contributions": ["Introduction of a comprehensive framework that integrates AL with NLG tasks", "Support for both human annotators and LLM-based automatic annotation", "Benchmarking of various AL strategies in diverse NLG settings"], "limitations": "", "keywords": ["Active Learning", "Natural Language Generation", "Large Language Models", "Annotation Effort", "NLP"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.23377", "pdf": "https://arxiv.org/pdf/2506.23377.pdf", "abs": "https://arxiv.org/abs/2506.23377", "title": "Perspective Dial: Measuring Perspective of Text and Guiding LLM Outputs", "authors": ["Taejin Kim", "Siun-Chuon Mau", "Konrad Vesey"], "categories": ["cs.CL", "cs.AI"], "comment": "7 pages, 5 main pages of text, 5 figures, 2 tables. Research work\n  performed at CACI INTL INC", "summary": "Large language models (LLMs) are used in a variety of mission-critical roles.\nDue to the rapidly developing nature of LLMs, there is a lack of quantifiable\nunderstanding of the bias and perspective associated with LLM output. Inspired\nby this need, this paper considers the broader issue of perspective or\nviewpoint of general text and perspective control of large-language model (LLM)\noutput. Perspective-Dial consists of two main components: a (1) metric space,\ndubbed Perspective Space, that enables quantitative measurements of different\nperspectives regarding a topic, and the use of (2) Systematic Prompt\nEngineering that utilizes greedy-coordinate descent to control LLM output\nperspective based on measurement feedback from the Perspective Space. The\nempirical nature of the approach allows progress to side step a principled\nunderstanding of perspective or bias -- effectively quantifying and adjusting\noutputs for a variety of topics. Potential applications include detection,\ntracking and mitigation of LLM bias, narrative detection, sense making and\ntracking in public discourse, and debate bot advocating given perspective.", "AI": {"tldr": "This paper introduces Perspective-Dial, a framework for quantifying and controlling the perspective in the outputs of large language models (LLMs) to address issues of bias and viewpoint.", "motivation": "There is a lack of quantifiable understanding of bias and perspective in LLM outputs, which is critical given their use in mission-critical roles.", "method": "The paper presents Perspective-Dial, which includes a metric space called Perspective Space for measuring perspectives and employs systematic prompt engineering using greedy-coordinate descent to adjust the perspective in LLM outputs.", "result": "Perspective-Dial provides a way to quantitatively measure and effectively control LLM outputs, addressing biases and facilitating applications in narrative tracking and advocacy.", "conclusion": "The proposed approach allows for the tracking and mitigation of bias in LLMs while enabling various applications in public discourse and debate.", "key_contributions": ["Introduction of Perspective Space for perspective measurement", "Development of systematic prompt engineering for perspective control", "Potential applications in bias detection and correction"], "limitations": "", "keywords": ["large language models", "bias", "perspective control", "prompt engineering", "public discourse"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.23393", "pdf": "https://arxiv.org/pdf/2506.23393.pdf", "abs": "https://arxiv.org/abs/2506.23393", "title": "Hierarchical Memory Organization for Wikipedia Generation", "authors": ["Eugene J. Yu", "Dawei Zhu", "Yifan Song", "Xiangyu Wong", "Jiebin Zhang", "Wenxuan Shi", "Xiaoguang Li", "Qun Liu", "Sujian Li"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Main Conference", "summary": "Generating Wikipedia articles autonomously is a challenging task requiring\nthe integration of accurate, comprehensive, and well-structured information\nfrom diverse sources. This paper introduces the Memory Organization-based\nGeneration (MOG) framework, a novel approach to address these challenges by\nleveraging a hierarchical memory architecture. MOG extracts fine-grained memory\nunits from web documents, recursively organizes them into a Wikipedia-style\nhierarchical structure, and uses this structure to guide the generation\nprocess. This ensures alignment between memory and the article outline,\nimproving both informativeness and verifiability while minimizing\nhallucinations. Additionally, a citation module is implemented to enhance\ntraceability by linking every generated sentence to specific memory units.\nEvaluations on our newly created WikiStart dataset demonstrate that MOG\noutperforms baseline methods in producing informative and reliable articles,\nmaking it particularly robust in real-world scenarios.", "AI": {"tldr": "The paper presents the Memory Organization-based Generation (MOG) framework for autonomous Wikipedia article generation, utilizing a hierarchical memory architecture to improve informativeness and verifiability.", "motivation": "To tackle the challenges of generating accurate and well-structured Wikipedia articles from diverse sources.", "method": "The MOG framework extracts fine-grained memory units from web documents, organizes them hierarchically, and uses this structure to guide the article generation process, while incorporating a citation module for traceability.", "result": "Evaluations on the WikiStart dataset show that MOG significantly outperforms baseline methods in terms of informativeness and reliability of generated articles.", "conclusion": "The MOG framework provides a robust solution for generating Wikipedia articles that align closely with a structured outline, enhancing both reliability and traceability in generated content.", "key_contributions": ["Introduce the MOG framework leveraging a hierarchical memory architecture.", "Demonstrate improvements in article generation informativeness and verifiability.", "Implement a citation module for enhanced traceability."], "limitations": "", "keywords": ["Wikipedia generation", "Memory Organization", "Natural Language Generation", "AI writing systems", "Information verifiability"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2506.23411", "pdf": "https://arxiv.org/pdf/2506.23411.pdf", "abs": "https://arxiv.org/abs/2506.23411", "title": "Datasets for Fairness in Language Models: An In-Depth Survey", "authors": ["Jiale Zhang", "Zichong Wang", "Avash Palikhe", "Zhipeng Yin", "Wenbin Zhang"], "categories": ["cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "Fairness benchmarks play a central role in shaping how we evaluate language\nmodels, yet surprisingly little attention has been given to examining the\ndatasets that these benchmarks rely on. This survey addresses that gap by\npresenting a broad and careful review of the most widely used fairness datasets\nin current language model research, characterizing them along several key\ndimensions including their origin, scope, content, and intended use to help\nresearchers better appreciate the assumptions and limitations embedded in these\nresources. To support more meaningful comparisons and analyses, we introduce a\nunified evaluation framework that reveals consistent patterns of demographic\ndisparities across datasets and scoring methods. Applying this framework to\ntwenty four common benchmarks, we highlight the often overlooked biases that\ncan influence conclusions about model fairness and offer practical guidance for\nselecting, combining, and interpreting these datasets. We also point to\nopportunities for creating new fairness benchmarks that reflect more diverse\nsocial contexts and encourage more thoughtful use of these tools going forward.\nAll code, data, and detailed results are publicly available at\nhttps://github.com/vanbanTruong/Fairness-in-Large-Language-Models/tree/main/datasets\nto promote transparency and reproducibility across the research community.", "AI": {"tldr": "This paper reviews widely used fairness datasets in language model research, highlighting their biases and proposing a unified evaluation framework for better analysis.", "motivation": "To address the lack of examination of fairness datasets used in language model benchmarks, which significantly influence evaluations of model fairness.", "method": "A broad review of fairness datasets, characterizing them by their origin, scope, content, and intended use, alongside the introduction of a unified evaluation framework for systematic analysis.", "result": "Consistent patterns of demographic disparities were revealed across various datasets, showing the biases that affect conclusions about model fairness.", "conclusion": "The paper provides practical guidance for the selection and use of fairness datasets and points towards the need for new benchmarks that reflect a diverse range of social contexts.", "key_contributions": ["Comprehensive review of fairness datasets in language models", "Introduction of a unified evaluation framework for dataset analysis", "Recommendations for improved dataset selection and usage"], "limitations": "The paper primarily focuses on existing datasets without proposing specific new datasets for evaluation.", "keywords": ["fairness", "language models", "evaluation framework", "demographic disparities", "biases"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.23423", "pdf": "https://arxiv.org/pdf/2506.23423.pdf", "abs": "https://arxiv.org/abs/2506.23423", "title": "TuCo: Measuring the Contribution of Fine-Tuning to Individual Responses of LLMs", "authors": ["Felipe Nuti", "Tim Franzmeyer", "João Henriques"], "categories": ["cs.CL", "cs.AI"], "comment": "ICML 2025", "summary": "Past work has studied the effects of fine-tuning on large language models'\n(LLMs) overall performance on certain tasks. However, a quantitative and\nsystematic method for analyzing its effect on individual outputs is still\nlacking. Here, we propose a new method for measuring the contribution that\nfine-tuning makes to individual LLM responses, assuming access to the original\npre-trained model. Our method tracks the model's intermediate hidden states,\nproviding a more fine-grained insight into the effects of fine-tuning than a\nsimple comparison of final outputs from pre-trained and fine-tuned models. We\nintroduce and theoretically analyze an exact decomposition of any fine-tuned\nLLM into a pre-training component and a fine-tuning component. Empirically, we\nfind that model behavior and performance can be steered by up- or down-scaling\nthe fine-tuning component during the forward pass. Motivated by this finding\nand our theoretical analysis, we define the Tuning Contribution (TuCo) as the\nratio of the magnitudes of the fine-tuning component to the pre-training\ncomponent. We observe that three prominent adversarial attacks on LLMs\ncircumvent safety measures in a way that reduces TuCo, and that TuCo is\nconsistently lower on prompts where these attacks succeed compared to those\nwhere they do not. This suggests that attenuating the effect of fine-tuning on\nmodel outputs plays a role in the success of such attacks. In summary, TuCo\nenables the quantitative study of how fine-tuning influences model behavior and\nsafety, and vice versa.", "AI": {"tldr": "This paper introduces a method to quantitatively analyze the effects of fine-tuning on individual outputs of large language models (LLMs) through the concept of Tuning Contribution (TuCo).", "motivation": "There is a lack of quantitative methods for assessing fine-tuning effects on LLM outputs, which this work seeks to address.", "method": "The authors propose a method that tracks intermediate hidden states of LLMs, allowing for a decomposition into pre-training and fine-tuning components, theoretically analyzing their contributions.", "result": "The findings suggest that adjusting the fine-tuning component during inference can steer model behavior and that fine-tuning effects are related to the success of adversarial attacks on LLMs.", "conclusion": "TuCo serves as a tool for quantitatively studying the influence of fine-tuning on model behavior and safety, revealing insights on adversarial vulnerabilities.", "key_contributions": ["Introduction of Tuning Contribution (TuCo) as a measure of fine-tuning effects on LLM responses", "Theoretical decomposition of fine-tuning and pre-training components", "Empirical insights linking TuCo levels to adversarial attack outcomes"], "limitations": "The method relies on access to the original pre-trained model and focuses primarily on LLM behavior during specific tasks.", "keywords": ["Tuning Contribution", "fine-tuning", "large language models", "adversarial attacks", "model behavior"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.23431", "pdf": "https://arxiv.org/pdf/2506.23431.pdf", "abs": "https://arxiv.org/abs/2506.23431", "title": "Pipelined Decoder for Efficient Context-Aware Text Generation", "authors": ["Zixian Huang", "Chenxu Niu", "Yu Gu", "Gengyang Xiao", "Xinwei Huang", "Gong Cheng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As the basis of generative AI, an autoregressive model requires the\ngeneration of a new token depending on all the previously generated tokens,\nwhich brings high quality but also restricts the model to generate tokens one\nby one, forming a bottleneck limiting the generation speed. In this paper, we\npropose a new decoder architecture that efficiently generates text in parallel\nfor context-aware generation tasks. Our proposed pipelined decoder initiates\nthe generation of multiple subsequences simultaneously, and, at each time-step,\nit generates a new token for each subsequence to realize parallelism.\nExperiments on multiple text generation tasks, including question answering,\ntext summarization, and keyphrase generation, show that our pipelined decoder\nsignificantly improves the generation speed without a significant loss of\ngeneration quality or additional memory consumption.", "AI": {"tldr": "The paper introduces a novel pipelined decoder architecture for autoregressive models that enables parallel text generation, significantly increasing generation speed while maintaining quality.", "motivation": "To overcome the generation speed bottleneck in autoregressive models which limit token generation to a sequential process.", "method": "The proposed pipelined decoder allows for simultaneous generation of multiple subsequences, generating a new token for each at every time-step, thereby facilitating parallelism.", "result": "The experiments demonstrated significant speed improvements in text generation across various tasks like question answering and summarization, without compromising on quality or memory use.", "conclusion": "The pipelined decoder architecture effectively accelerates the text generation process in context-aware tasks, showing promise for future applications.", "key_contributions": ["Introduction of a pipelined decoder architecture for autoregressive models", "Demonstrated significant speed improvements for multiple text generation tasks", "Maintained generation quality and memory efficiency during the process."], "limitations": "", "keywords": ["autoregressive models", "text generation", "pipelined decoder", "parallelism", "machine learning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.23463", "pdf": "https://arxiv.org/pdf/2506.23463.pdf", "abs": "https://arxiv.org/abs/2506.23463", "title": "What to Keep and What to Drop: Adaptive Table Filtering Framework", "authors": ["Jang Won June"], "categories": ["cs.CL", "I.2.7"], "comment": "26 pages, 9 figures", "summary": "Large language models (LLMs) for table-based reasoning often struggle with\nlarge tables due to input length limits. We propose ATF (Adaptive Table\nFiltering Framework), a modular and question-aware filtering pipeline that\nprunes uninformative columns and rows using LLM-generated column descriptions,\nclustering, and sparse-dense alignment scores. ATF integrates seamlessly with\nexisting models (e.g., TAPAS, TAPEX) without retraining. Experiments show that\nATF reduces table cells by ~70\\%, boosting performance on out-of-domain TableQA\ntasks while causing slight performance drops on Table Fact Verification, where\nfull-table context is more critical. These results highlight ATF's ability to\nadaptively balance informativeness and minimalism across tasks.", "AI": {"tldr": "The Adaptive Table Filtering Framework (ATF) enhances large language models' performance on table-based reasoning by reducing irrelevant table content while maintaining performance.", "motivation": "LLMs face challenges with large tables due to input length limits, impacting their reasoning capabilities on TableQA tasks.", "method": "ATF is a modular filtering pipeline that uses LLM-generated column descriptions, clustering, and sparse-dense alignment scores to prune uninformative table rows and columns.", "result": "ATF reduces table size by approximately 70%, significantly improving performance on out-of-domain TableQA tasks, though it slightly decreases performance on Table Fact Verification tasks.", "conclusion": "ATF demonstrates the ability to effectively balance information retention and minimalism, improving efficiency in large table reasoning.", "key_contributions": ["Introduction of the ATF framework for adaptive table filtering.", "Significant size reduction of table inputs (about 70%).", "Improved performance on TableQA tasks without the need for retraining existing models."], "limitations": "Slight performance drops on tasks requiring full-table context, such as Table Fact Verification.", "keywords": ["large language models", "table reasoning", "adaptive filtering", "table QA", "performance enhancement"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2506.23485", "pdf": "https://arxiv.org/pdf/2506.23485.pdf", "abs": "https://arxiv.org/abs/2506.23485", "title": "Thought-Augmented Planning for LLM-Powered Interactive Recommender Agent", "authors": ["Haocheng Yu", "Yaxiong Wu", "Hao Wang", "Wei Guo", "Yong Liu", "Yawen Li", "Yuyang Ye", "Junping Du", "Enhong Chen"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Interactive recommendation is a typical information-seeking task that allows\nusers to interactively express their needs through natural language and obtain\npersonalized recommendations. Large language model-powered (LLM-powered) agents\nhave become a new paradigm in interactive recommendations, effectively\ncapturing users' real-time needs and enhancing personalized experiences.\nHowever, due to limited planning and generalization capabilities, existing\nformulations of LLM-powered interactive recommender agents struggle to\neffectively address diverse and complex user intents, such as intuitive,\nunrefined, or occasionally ambiguous requests. To tackle this challenge, we\npropose a novel thought-augmented interactive recommender agent system (TAIRA)\nthat addresses complex user intents through distilled thought patterns.\nSpecifically, TAIRA is designed as an LLM-powered multi-agent system featuring\na manager agent that orchestrates recommendation tasks by decomposing user\nneeds and planning subtasks, with its planning capacity strengthened through\nThought Pattern Distillation (TPD), a thought-augmentation method that extracts\nhigh-level thoughts from the agent's and human experts' experiences. Moreover,\nwe designed a set of user simulation schemes to generate personalized queries\nof different difficulties and evaluate the recommendations based on specific\ndatasets. Through comprehensive experiments conducted across multiple datasets,\nTAIRA exhibits significantly enhanced performance compared to existing methods.\nNotably, TAIRA shows a greater advantage on more challenging tasks while\ngeneralizing effectively on novel tasks, further validating its superiority in\nmanaging complex user intents within interactive recommendation systems. The\ncode is publicly available at:https://github.com/Alcein/TAIRA.", "AI": {"tldr": "The paper introduces TAIRA, a thought-augmented interactive recommender agent that improves user intent management in personalized recommendations through planning and decomposition.", "motivation": "To enhance the effectiveness of LLM-powered interactive recommendation systems, especially in addressing complex user intents.", "method": "The proposed TAIRA system uses a multi-agent approach with a manager agent that employs Thought Pattern Distillation to decompose and plan recommendation tasks.", "result": "TAIRA significantly outperforms existing methods across various datasets, showing improved performance particularly on challenging tasks and effectively generalizes to novel tasks.", "conclusion": "TAIRA's design allows it to better manage complex user intents in interactive recommendations, making it a superior choice compared to current approaches.", "key_contributions": ["Introduction of the TAIRA system for interactive recommendations", "Use of Thought Pattern Distillation to enhance planning capabilities", "Empirical validation showing improved performance across multiple datasets."], "limitations": "", "keywords": ["interactive recommendations", "large language models", "thought-augmented systems"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.23508", "pdf": "https://arxiv.org/pdf/2506.23508.pdf", "abs": "https://arxiv.org/abs/2506.23508", "title": "Reinforcement Fine-Tuning Enables MLLMs Learning Novel Tasks Stably", "authors": ["Zhihao Zhang", "Qiaole Dong", "Qi Zhang", "Jun Zhao", "Enyu Zhou", "Zhiheng Xi", "Senjie Jin", "Xiaoran Fan", "Yuhao Zhou", "Yanwei Fu", "Tao Ji", "Tao Gui", "Xuanjing Huang"], "categories": ["cs.CL", "cs.AI"], "comment": "18 pages (Preprint. Work in progress)", "summary": "Post-training algorithms such as Supervised Fine-Tuning (SFT) and\nReinforcement Fine-Tuning (RFT) are widely used to adapt multimodal large\nlanguage models to downstream tasks. While effective at task adaptation, their\nimpact on prior knowledge remains unclear. In this paper, we introduce jigsaw\npuzzles as a novel task absent from existing pretraining corpora and\nsystematically study the behavior of SFT and RFT on an open-source multimodal\nmodel, Qwen2.5-VL. Our experiments reveal a sharp trade-off: SFT enables rapid\ntask acquisition but leads to catastrophic forgetting, whereas RFT learns more\nslowly on novel tasks but maintains prior knowledge. We analyze this phenomenon\nthrough the lens of learning dynamics, showing that RFT reinforces correct\nsamples that are naturally aligned with the base model's probability landscape,\nmitigating interference with prior knowledge. Moreover, supervised training on\ncorrect RFT-simulated rollouts allows SFT to preserve knowledge while rapidly\nlearning new tasks. These findings suggest that data distribution, rather than\nalgorithmic differences, plays a central role in forgetting, and highlight\nRFT's potential for stable continual learning in multimodal large language\nmodels.", "AI": {"tldr": "This paper studies the effects of Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning (RFT) on multimodal large language models, specifically focusing on their impact on task acquisition and prior knowledge retention using a novel jigsaw puzzle task.", "motivation": "To examine how post-training algorithms like SFT and RFT affect the retention of prior knowledge when adapting multimodal large language models to new tasks.", "method": "We systematically study the behavior of SFT and RFT on the multimodal model Qwen2.5-VL using a new task involving jigsaw puzzles, analyzing their impact through learning dynamics and performance metrics.", "result": "SFT allows for rapid acquisition of new tasks but causes catastrophic forgetting of previous knowledge, while RFT learns more slowly but maintains prior knowledge, suggesting that the data distribution influences forgetting more than the algorithms themselves.", "conclusion": "The choice of training approach and data distribution is critical for retaining knowledge in continual learning scenarios in multimodal large language models, particularly highlighting the benefits of RFT.", "key_contributions": ["Introduced jigsaw puzzles as a novel training task for multimodal models.", "Demonstrated the trade-off between SFT and RFT regarding task learning and knowledge retention.", "Provided insights into learning dynamics that explain the effectiveness of RFT in preventing catastrophic forgetting."], "limitations": "The paper is a work in progress, and results may change with further experimentation and analysis.", "keywords": ["Supervised Fine-Tuning", "Reinforcement Fine-Tuning", "multimodal models", "task acquisition", "knowledge retention"], "importance_score": 8, "read_time_minutes": 18}}
{"id": "2506.23524", "pdf": "https://arxiv.org/pdf/2506.23524.pdf", "abs": "https://arxiv.org/abs/2506.23524", "title": "NEU-ESC: A Comprehensive Vietnamese dataset for Educational Sentiment analysis and topic Classification toward multitask learning", "authors": ["Phan Quoc Hung Mai", "Quang Hung Nguyen", "Phuong Giang Duong", "Hong Hanh Nguyen", "Nguyen Tuan Long"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In the field of education, understanding students' opinions through their\ncomments is crucial, especially in the Vietnamese language, where resources\nremain limited. Existing educational datasets often lack domain relevance and\nstudent slang. To address these gaps, we introduce NEU-ESC, a new Vietnamese\ndataset for Educational Sentiment Classification and Topic Classification,\ncurated from university forums, which offers more samples, richer class\ndiversity, longer texts, and broader vocabulary. In addition, we explore\nmultitask learning using encoder-only language models (BERT), in which we\nshowed that it achieves performance up to 83.7% and 79.8% accuracy for\nsentiment and topic classification tasks. We also benchmark our dataset and\nmodel with other datasets and models, including Large Language Models, and\ndiscuss these benchmarks. The dataset is publicly available at:\nhttps://huggingface.co/datasets/hung20gg/NEU-ESC.", "AI": {"tldr": "Introduction of a new Vietnamese dataset for educational sentiment and topic classification, showing improved performance with multitask learning using BERT.", "motivation": "To address the lack of relevant datasets and understand students’ opinions in Vietnamese educational contexts, especially concerning student slang.", "method": "Development of NEU-ESC, a Vietnamese dataset created from university forums, and implementation of multitask learning with encoder-only language models (BERT) for sentiment and topic classification.", "result": "Achieved performance accuracy of 83.7% for sentiment classification and 79.8% for topic classification using the new dataset.", "conclusion": "The NEU-ESC dataset provides a more relevant and diverse resource for educational sentiment analysis in Vietnamese, with effective model performance evidenced through benchmarking.", "key_contributions": ["Introduction of the NEU-ESC dataset for educational sentiment analysis in Vietnamese.", "Demonstration of effective multitask learning using BERT for sentiment and topic classification.", "Public availability of the dataset for further research."], "limitations": "", "keywords": ["Vietnamese", "sentiment analysis", "topic classification", "educational dataset", "BERT"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.23527", "pdf": "https://arxiv.org/pdf/2506.23527.pdf", "abs": "https://arxiv.org/abs/2506.23527", "title": "On Recipe Memorization and Creativity in Large Language Models: Is Your Model a Creative Cook, a Bad Cook, or Merely a Plagiator?", "authors": ["Jan Kvapil", "Martin Fajcik"], "categories": ["cs.CL"], "comment": "13 pages, 5 figures", "summary": "This work-in-progress investigates the memorization, creativity, and nonsense\nfound in cooking recipes generated from Large Language Models (LLMs).\nPrecisely, we aim (i) to analyze memorization, creativity, and non-sense in\nLLMs using a small, high-quality set of human judgments and (ii) to evaluate\npotential approaches to automate such a human annotation in order to scale our\nstudy to hundreds of recipes. To achieve (i), we conduct a detailed human\nannotation on 20 preselected recipes generated by LLM (Mixtral), extracting\neach recipe's ingredients and step-by-step actions to assess which elements are\nmemorized--i.e., directly traceable to online sources possibly seen during\ntraining--and which arise from genuine creative synthesis or outright nonsense.\nWe find that Mixtral consistently reuses ingredients that can be found in\nonline documents, potentially seen during model training, suggesting strong\nreliance on memorized content. To achieve aim (ii) and scale our analysis\nbeyond small sample sizes and single LLM validation, we design an\n``LLM-as-judge'' pipeline that automates recipe generation, nonsense detection,\nparsing ingredients and recipe steps, and their annotation. For instance,\ncomparing its output against human annotations, the best ingredient extractor\nand annotator is Llama 3.1+Gemma 2 9B, achieving up to 78% accuracy on\ningredient matching. This automated framework enables large-scale\nquantification of memorization, creativity, and nonsense in generated recipes,\nproviding rigorous evidence of the models' creative capacities.", "AI": {"tldr": "This study analyzes memorization, creativity, and nonsensical elements in cooking recipes generated by LLMs, focusing on automating human annotation for scalability.", "motivation": "To investigate the extent of memorization, creativity, and nonsense in LLM-generated cooking recipes and to develop automated approaches for large-scale assessment.", "method": "Conducted detailed human annotations on 20 LLM-generated recipes, analyzing ingredients and steps to identify memorized content versus creative synthesis, and developed an 'LLM-as-judge' pipeline for automation.", "result": "The study found that the LLM Mixtral relies heavily on memorized content from online sources. The automated framework designed achieved up to 78% accuracy in ingredient matching, demonstrating potential for large-scale analysis.", "conclusion": "The research provides evidence of mixing memorization with creativity in LLM outputs and offers a viable method for automating large-scale evaluations in recipe generation.", "key_contributions": ["Detailed analysis of LLM-generated recipe elements", "Development of an automated framework for annotation", "Insights into the balance of memorization and creativity in LLMs"], "limitations": "Limited to 20 preselected recipes which may not represent the full range of LLM capabilities.", "keywords": ["large language models", "recipe generation", "creativity", "memorization", "nonsense detection"], "importance_score": 8, "read_time_minutes": 13}}
{"id": "2506.23601", "pdf": "https://arxiv.org/pdf/2506.23601.pdf", "abs": "https://arxiv.org/abs/2506.23601", "title": "Semantic-guided Diverse Decoding for Large Language Model", "authors": ["Weijie Shi", "Yue Cui", "Yaguang Wu", "Jingzhi Fang", "Shibo Zhang", "Mengze Li", "Sirui Han", "Jia Zhu", "Jiajie Xu", "Xiaofang Zhou"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Diverse decoding of large language models is crucial for applications\nrequiring multiple semantically distinct responses, yet existing methods\nprimarily achieve lexical rather than semantic diversity. This limitation\nsignificantly constrains Best-of-N strategies, group-based reinforcement\nlearning, and data synthesis. While temperature sampling and diverse beam\nsearch modify token distributions or apply n-gram penalties, they fail to\nensure meaningful semantic differentiation. We introduce Semantic-guided\nDiverse Decoding (SemDiD), operating directly in embedding space that balances\nquality with diversity through three complementary mechanisms: orthogonal\ndirectional guidance, dynamic inter-group repulsion, and position-debiased\nprobability assessment. SemDiD harmonizes these competing objectives using\nadaptive gain functions and constraint optimization, ensuring both quality\nthresholds and maximal semantic differentiation. Experiments show SemDiD\nconsistently outperforms existing methods, improving Best-of-N coverage by\n1.4-5.2% across diverse tasks and accelerating RLHF training convergence by 15%\nwhile increasing accuracy by up to 2.1%.", "AI": {"tldr": "Introducing Semantic-guided Diverse Decoding (SemDiD) which enhances semantic diversity in responses generated by large language models.", "motivation": "Current methods for achieving diversity in language model outputs mostly focus on lexical rather than semantic differences, limiting their effectiveness in various applications.", "method": "SemDiD operates in embedding space and employs three mechanisms: orthogonal directional guidance, dynamic inter-group repulsion, and position-debiased probability assessment to balance quality and diversity.", "result": "SemDiD outperforms existing methods, improving Best-of-N coverage by 1.4-5.2% and accelerating RLHF training convergence by 15%, with a potential accuracy increase of up to 2.1%.", "conclusion": "The proposed method demonstrates a significant improvement in generating semantically diverse responses while maintaining output quality.", "key_contributions": ["Introduction of a novel decoding method that focuses on semantic diversity.", "Demonstrated improvements in RLHF training efficiency and response accuracy.", "Utilization of embedding space for direct semantic guidance."], "limitations": "", "keywords": ["Semantic diversity", "Large language models", "Decoding strategies", "Natural language processing"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.23610", "pdf": "https://arxiv.org/pdf/2506.23610.pdf", "abs": "https://arxiv.org/abs/2506.23610", "title": "Evaluating the Simulation of Human Personality-Driven Susceptibility to Misinformation with LLMs", "authors": ["Manuel Pratelli", "Marinella Petrocchi"], "categories": ["cs.CL", "cs.CY"], "comment": "pre-print version - paper actually under submission", "summary": "Large language models (LLMs) make it possible to generate synthetic\nbehavioural data at scale, offering an ethical and low-cost alternative to\nhuman experiments. Whether such data can faithfully capture psychological\ndifferences driven by personality traits, however, remains an open question. We\nevaluate the capacity of LLM agents, conditioned on Big-Five profiles, to\nreproduce personality-based variation in susceptibility to misinformation,\nfocusing on news discernment, the ability to judge true headlines as true and\nfalse headlines as false. Leveraging published datasets in which human\nparticipants with known personality profiles rated headline accuracy, we create\nmatching LLM agents and compare their responses to the original human patterns.\nCertain trait-misinformation associations, notably those involving\nAgreeableness and Conscientiousness, are reliably replicated, whereas others\ndiverge, revealing systematic biases in how LLMs internalize and express\npersonality. The results underscore both the promise and the limits of\npersonality-aligned LLMs for behavioral simulation, and offer new insight into\nmodeling cognitive diversity in artificial agents.", "AI": {"tldr": "This paper evaluates the ability of LLMs to simulate personality-based variations in susceptibility to misinformation as influenced by the Big-Five personality traits.", "motivation": "To explore whether LLMs can ethically and cost-effectively generate synthetic behavioral data that reflects psychological differences driven by personality traits.", "method": "The study conditions LLM agents on Big-Five personality profiles and compares their judgments of headline accuracy to human participants with similar profiles, using existing datasets.", "result": "Certain personality traits like Agreeableness and Conscientiousness show reliable associations with susceptibility to misinformation in the simulated LLM agents, while others exhibit discrepancies, indicating biases in LLM behavior.", "conclusion": "The findings highlight both the potential of personality-aligned LLMs in behavioral simulations and their inherent limitations, providing insights into cognitive diversity in artificial agents.", "key_contributions": ["Demonstrated the capacity of LLMs to reflect personality traits in misinformation susceptibility.", "Identified systematic biases in LLMs regarding personality expression.", "Provided new insights into behavioral simulation using personality-aligned LLMs."], "limitations": "The study may not account for all factors influencing how LLMs simulate personality, and discrepancies in trait representation might affect generalizability.", "keywords": ["Large Language Models", "Personality Traits", "Misinformation", "Behavioral Simulation", "Cognitive Diversity"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.23661", "pdf": "https://arxiv.org/pdf/2506.23661.pdf", "abs": "https://arxiv.org/abs/2506.23661", "title": "Robustness of Misinformation Classification Systems to Adversarial Examples Through BeamAttack", "authors": ["Arnisa Fazla", "Lucas Krauter", "David Guzman Piedrahita", "Andrianos Michail"], "categories": ["cs.CL"], "comment": "12 pages main text, 27 pages total including references and\n  appendices. 13 figures, 10 tables. Accepted for publication in the LNCS\n  proceedings of CLEF 2025 (Best-of-Labs track)", "summary": "We extend BeamAttack, an adversarial attack algorithm designed to evaluate\nthe robustness of text classification systems through word-level modifications\nguided by beam search. Our extensions include support for word deletions and\nthe option to skip substitutions, enabling the discovery of minimal\nmodifications that alter model predictions. We also integrate LIME to better\nprioritize word replacements. Evaluated across multiple datasets and victim\nmodels (BiLSTM, BERT, and adversarially trained RoBERTa) within the BODEGA\nframework, our approach achieves over a 99\\% attack success rate while\npreserving the semantic and lexical similarity of the original texts. Through\nboth quantitative and qualitative analysis, we highlight BeamAttack's\neffectiveness and its limitations. Our implementation is available at\nhttps://github.com/LucK1Y/BeamAttack", "AI": {"tldr": "The paper extends the BeamAttack adversarial attack algorithm for text classification by adding features for word deletions and substitution skipping, achieving high success rates while maintaining text similarity.", "motivation": "To evaluate the robustness of text classification systems and improve adversarial attack methodologies.", "method": "The extended BeamAttack algorithm incorporates word deletions, omission of certain substitutions, and utilizes LIME to enhance word replacement prioritization.", "result": "Achieves over 99% attack success rates across various datasets and models while preserving the semantic and lexical integrity of the texts.", "conclusion": "The extended BeamAttack is effective for creating minimal modifications that alter model predictions, although some limitations exist which are discussed.", "key_contributions": ["Extension of BeamAttack to include word deletions.", "Integration of LIME for improved word replacement prioritization.", "Demonstration of effectiveness across multiple datasets and models."], "limitations": "Some limitations are discussed but not specified in detail in the abstract.", "keywords": ["adversarial attack", "text classification", "BeamAttack", "LIME", "robustness evaluation"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2506.23662", "pdf": "https://arxiv.org/pdf/2506.23662.pdf", "abs": "https://arxiv.org/abs/2506.23662", "title": "Zero-Shot Contextual Embeddings via Offline Synthetic Corpus Generation", "authors": ["Philip Lippmann", "Jie Yang"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Context-aware embedding methods boost retrieval accuracy by conditioning on\ncorpus statistics (e.g., term co-occurrence and topical patterns) extracted\nfrom neighboring documents. However, this context-aware approach requires\naccess to the target corpus or requires domain-specific finetuning, posing\npractical barriers in privacy-sensitive or resource-constrained settings. We\npresent ZEST, a zero-shot contextual adaptation framework that replaces real\ncorpus access with a one-time offline synthesis of a compact proxy. Given only\na handful exemplar documents representative of the general target domain, we\nuse a multi-step hierarchical procedure to generate a synthetic context corpus\nof several hundred documents that aims to emulate key domain-specific\ndistributions. At inference, the frozen context-aware encoder uses this proxy\ncorpus -- without any finetuning or target corpus access -- to produce\ndomain-adapted embeddings. Across the MTEB benchmark, ZEST's zero-shot\nsynthetic context adaptation using only five example documents performs within\n0.5% of models leveraging full target corpus access -- demonstrating remarkable\nefficacy without any retraining. ZEST thus provides a practical method for\ndeploying high-performance, adaptable embeddings in constrained environments.", "AI": {"tldr": "ZEST is a zero-shot contextual adaptation framework that enables domain-adapted embeddings without real corpus access or finetuning, utilizing a synthetic context corpus generated from exemplar documents.", "motivation": "The need for context-aware embedding methods often requires access to specific corpora or extensive finetuning, which can be impractical in privacy-sensitive or resource-constrained environments.", "method": "ZEST creates a synthetic context corpus from a small number of representative documents using a multi-step hierarchical procedure, producing domain-adapted embeddings during inference without needing further retraining or access to the target corpus.", "result": "ZEST performs within 0.5% of traditional models using full target corpus access on the MTEB benchmark, demonstrating its efficacy in producing high-quality embeddings in constrained settings.", "conclusion": "ZEST provides an effective alternative for generating adaptable embeddings in situations where accessing target corpora is challenging or impossible.", "key_contributions": ["Introduction of a zero-shot contextual adaptation framework (ZEST).", "Demonstrates the efficacy of synthetic context corpus across benchmarks with minimal input.", "Enables practical high-performance embedding solutions in sensitive environments."], "limitations": "Performance may be sensitive to the quality of the handful of exemplar documents used for synthesis.", "keywords": ["context-aware embeddings", "synthetic corpus", "zero-shot adaptation", "domain adaptation", "MTEB benchmark"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2506.23667", "pdf": "https://arxiv.org/pdf/2506.23667.pdf", "abs": "https://arxiv.org/abs/2506.23667", "title": "L0: Reinforcement Learning to Become General Agents", "authors": ["Junjie Zhang", "Jingyi Xi", "Zhuoyang Song", "Junyu Lu", "Yuhua Ke", "Ting Sun", "Yukun Yang", "Jiaxing Zhang", "Songxin Zhang", "Zejian Xie"], "categories": ["cs.CL"], "comment": null, "summary": "Training large language models (LLMs) to act as autonomous agents for\nmulti-turn, long-horizon tasks remains significant challenges in scalability\nand training efficiency. To address this, we introduce L-Zero (L0), a scalable,\nend-to-end training pipeline for general-purpose agents. Featuring a low-cost,\nextensible, and sandboxed concurrent agent worker pool, L0 lowers the barrier\nfor applying reinforcement learning in complex environments. We also introduce\nNB-Agent, the agent scaffold within L0, which operates in a \"code-as-action\"\nfashion via a Read-Eval-Print-Loop (REPL). We evaluate L0 on factuality\nquestion-answering benchmarks. Our experiments demonstrate that a base model\ncan develop robust problem-solving skills using solely Reinforcement Learning\nwith Verifiable Rewards (RLVR). On the Qwen2.5-7B-Instruct model, our method\nboosts accuracy on SimpleQA from 30 % to 80 % and on HotpotQA from 22 % to 41\n%. We have open-sourced the entire L0 system, including our L0 series models,\nthe NB-Agent, a complete training pipeline, and the corresponding training\nrecipes on (https://github.com/cmriat/l0).", "AI": {"tldr": "L-Zero (L0) is a scalable training pipeline for large language models (LLMs) that facilitates training autonomous agents using reinforcement learning.", "motivation": "To improve scalability and training efficiency for LLMs acting as autonomous agents in multi-turn, long-horizon tasks.", "method": "L0 features a concurrent agent worker pool and uses a 'code-as-action' approach with the NB-Agent operating through a Read-Eval-Print-Loop (REPL).", "result": "L0 significantly enhances the problem-solving skills of a base model, achieving accuracy improvements on factuality question-answering benchmarks (SimpleQA from 30% to 80%, HotpotQA from 22% to 41%).", "conclusion": "The open-sourced L0 system demonstrates its viability for training general-purpose agents in complex environments using Reinforcement Learning with Verifiable Rewards.", "key_contributions": ["Introduction of the L-Zero (L0) training pipeline.", "Development of NB-Agent using 'code-as-action'.", "Open-sourcing the entire system and training recipes."], "limitations": "", "keywords": ["large language models", "autonomous agents", "reinforcement learning"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2506.23735", "pdf": "https://arxiv.org/pdf/2506.23735.pdf", "abs": "https://arxiv.org/abs/2506.23735", "title": "AutoEvoEval: An Automated Framework for Evolving Close-Ended LLM Evaluation Data", "authors": ["JiaRu Wu", "Mingwei Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have shown remarkable performance on various\ntasks, but existing evaluation benchmarks are often static and insufficient to\nfully assess their robustness and generalization in realistic scenarios. Prior\nwork using evolutionary or adversarial data augmentation has improved\nevaluation diversity but lacks systematic control over perturbation types and\nmulti-step complexity, limiting comprehensive robustness analysis. To address\nthese gaps, we propose AutoEvoEval, an evolution-based evaluation framework for\nclose-ended tasks such as multi-choice question answering. AutoEvoEval\nintroduces 22 interpretable atomic evolution operations and supports\nmulti-round compositions, enabling controlled generation of diverse,\nchallenging, and realistic test samples. We conduct extensive experiments\naddressing four research questions on a broad set of open- and closed-source\nLLMs. Our results show that atomic operations cause an average accuracy drop of\n7.283\\%, with structure-disrupting or misleading semantic edits causing the\nlargest declines. Model sensitivities vary significantly for the same\nperturbation, and combining multiple evolution steps amplifies adversarial\neffects by up to 52.932\\%. These findings suggest current benchmarks may\noverestimate true model generalization and emphasize the need for\nevolution-aware robustness evaluation. Code and resources are available at:\nhttps://github.com/SYSUSELab/AutoEvoEval.", "AI": {"tldr": "Introducing AutoEvoEval, an evolution-based framework for evaluating LLM robustness through diverse test sample generation.", "motivation": "Current LLM evaluation benchmarks are insufficient for assessing robustness and generalization, especially under realistic perturbations.", "method": "AutoEvoEval employs 22 interpretable atomic evolution operations and supports multi-round compositions to generate challenging test samples.", "result": "Experiments reveal that atomic operations lead to an average accuracy drop of 7.283%, with significant model sensitivity variations and adversarial effects amplifying by up to 52.932%.", "conclusion": "The findings highlight the ineffectiveness of static benchmarks and the importance of evolution-aware evaluations for accurate model assessments.", "key_contributions": ["Proposed AutoEvoEval framework for LLM evaluation using atomic evolution operations.", "Introduced multi-round compositions for generating realistic perturbations.", "Demonstrated significant variances in model robustness against different perturbations."], "limitations": "The framework focuses specifically on close-ended tasks, limiting its applicability in other domains.", "keywords": ["Large language models", "evaluation benchmarks", "robustness analysis", "evolution-based evaluation", "adversarial testing"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.23743", "pdf": "https://arxiv.org/pdf/2506.23743.pdf", "abs": "https://arxiv.org/abs/2506.23743", "title": "Positional Bias in Binary Question Answering: How Uncertainty Shapes Model Preferences", "authors": ["Tiziano Labruna", "Simone Gallo", "Giovanni Da San Martino"], "categories": ["cs.CL"], "comment": null, "summary": "Positional bias in binary question answering occurs when a model\nsystematically favors one choice over another based solely on the ordering of\npresented options. In this study, we quantify and analyze positional bias\nacross five large language models under varying degrees of answer uncertainty.\nWe re-adapted the SQuAD-it dataset by adding an extra incorrect answer option\nand then created multiple versions with progressively less context and more\nout-of-context answers, yielding datasets that range from low to high\nuncertainty. Additionally, we evaluate two naturally higher-uncertainty\nbenchmarks: (1) WebGPT - question pairs with unequal human-assigned quality\nscores, and (2) Winning Arguments - where models predict the more persuasive\nargument in Reddit's r/ChangeMyView exchanges. Across each dataset, the order\nof the \"correct\" (or higher-quality/persuasive) option is systematically\nflipped (first placed in position 1, then in position 2) to compute both\nPreference Fairness and Position Consistency. We observe that positional bias\nis nearly absent under low-uncertainty conditions, but grows exponentially when\nit becomes doubtful to decide which option is correct.", "AI": {"tldr": "The study investigates positional bias in binary question answering across large language models, examining how bias correlates with answer uncertainty.", "motivation": "To understand how positional bias affects model performance in binary question answering scenarios and its dependence on answer uncertainty.", "method": "The SQuAD-it dataset was modified to include extra incorrect answers, generating versions with varying context quality and option placement. The study also evaluated existing benchmarks under different uncertainty conditions.", "result": "Positional bias was found to be minimal in low-uncertainty scenarios but increased significantly as uncertainty about the correct answer grew, indicating a systematic preference based on answer positioning.", "conclusion": "The findings highlight the need to consider positional bias, especially in situations with ambiguous answers, affecting fairness in model predictions.", "key_contributions": ["Quantitative analysis of positional bias in language models", "Creation of datasets to study varying contextual uncertainty", "Evaluation of positional bias using different real-world benchmarks"], "limitations": "The study may not cover all possible contexts and biases in different applications of language models.", "keywords": ["positional bias", "binary question answering", "large language models", "answer uncertainty", "fairness"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.23840", "pdf": "https://arxiv.org/pdf/2506.23840.pdf", "abs": "https://arxiv.org/abs/2506.23840", "title": "Do Thinking Tokens Help or Trap? Towards More Efficient Large Reasoning Model", "authors": ["Bowen Ding", "Yuhan Chen", "Futing Wang", "Lingfeng Ming", "Tao Lin"], "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 5 figures", "summary": "Large Reasoning Models (LRMs) excel at solving complex problems but face an\noverthinking dilemma. When handling simple tasks, they often produce verbose\nresponses overloaded with thinking tokens (e.g., wait, however). These tokens\ntrigger unnecessary high-level reasoning behaviors like reflection and\nbacktracking, reducing efficiency. In this work, our pilot study reveals that\nthese thinking-token-induced behaviors are not essential for effective\nproblem-solving and may even hinder correct reasoning within constrained token\nbudgets. We identify this phenomenon as the thinking trap. To mitigate this\nissue, we propose Dual Policy Preference Optimization (DuP-PO), a novel\nalgorithm featuring: (1) A rollout sampling strategy that guarantees balanced\nexposure to responses with and without thinking tokens; (2) A fine-grained\nadvantage control technique to dynamically regulate the prediction of target\ntokens; (3) A policy shaping method ensuring stable gradient contributions from\nthinking tokens. Experimental results on five popular math reasoning benchmarks\nshow that DuP-PO performs well on the popular LRM, which significantly improves\ntheir token efficiency during reasoning, while achieving superior performance\nof the base model.", "AI": {"tldr": "This paper addresses the inefficiencies of Large Reasoning Models (LRMs) caused by excessive use of thinking tokens, presenting a novel algorithm, Dual Policy Preference Optimization (DuP-PO), to enhance token efficiency and reasoning performance.", "motivation": "To understand and address the inefficiencies of LRMs when processing simple tasks, specifically focusing on the overuse of thinking tokens that lead to unnecessary high-level reasoning behaviors.", "method": "The paper introduces Dual Policy Preference Optimization (DuP-PO), which employs a rollout sampling strategy for balanced exposure to responses, an advantage control technique for dynamic token prediction regulation, and a policy shaping method for stable gradient contributions.", "result": "DuP-PO demonstrates significant improvements in token efficiency and overall reasoning performance on five popular math reasoning benchmarks compared to the base model.", "conclusion": "The results indicate that reducing unnecessary thinking tokens through DuP-PO can enhance both efficiency and effectiveness in problem-solving for LRMs.", "key_contributions": ["Introduction of the thinking trap phenomenon", "Development of the Dual Policy Preference Optimization (DuP-PO) algorithm", "Empirical validation of DuP-PO's effectiveness on math reasoning benchmarks."], "limitations": "", "keywords": ["Large Reasoning Models", "thinking tokens", "Dual Policy Preference Optimization", "token efficiency", "math reasoning benchmarks"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.23864", "pdf": "https://arxiv.org/pdf/2506.23864.pdf", "abs": "https://arxiv.org/abs/2506.23864", "title": "Garbage In, Reasoning Out? Why Benchmark Scores are Unreliable and What to Do About It", "authors": ["Seyed Mahed Mousavi", "Edoardo Cecchinato", "Lucia Hornikova", "Giuseppe Riccardi"], "categories": ["cs.CL"], "comment": null, "summary": "We conduct a systematic audit of three widely used reasoning benchmarks,\nSocialIQa, FauxPas-EAI, and ToMi, and uncover pervasive flaws in both benchmark\nitems and evaluation methodology. Using five LLMs (GPT-{3, 3.5, 4, o1}, and\nLLaMA 3.1) as diagnostic tools, we identify structural, semantic, and pragmatic\nissues in benchmark design (e.g., duplicated items, ambiguous wording, and\nimplausible answers), as well as scoring procedures that prioritize output form\nover reasoning process. Through systematic human annotation and re-evaluation\non cleaned benchmark subsets, we find that model scores often improve not due\nto due to erratic surface wording variations and not to improved reasoning.\nInfact, further analyses show that model performance is highly sensitive to\nminor input variations such as context availability and phrasing, revealing\nthat high scores may reflect alignment with format-specific cues rather than\nconsistent inference based on the input. These findings challenge the validity\nof current benchmark-based claims about reasoning in LLMs, and highlight the\nneed for evaluation protocols that assess reasoning as a process of drawing\ninference from available information, rather than as static output selection.\nWe release audited data and evaluation tools to support more interpretable and\ndiagnostic assessments of model reasoning.", "AI": {"tldr": "This paper audits widely used reasoning benchmarks and uncovers flaws in their design and evaluation, revealing that model scores may reflect alignment with format-specific cues rather than actual reasoning abilities.", "motivation": "To address flaws in existing benchmarks that assess reasoning in large language models (LLMs).", "method": "A systematic audit of three reasoning benchmarks using five LLMs to identify design flaws and evaluate model performance through human annotation and re-evaluation on cleaned subsets.", "result": "The study reveals structural, semantic, and pragmatic issues in benchmark design that lead to misleading model performance scores, which often improve due to superficial factors rather than genuine reasoning ability.", "conclusion": "The findings challenge the validity of benchmark-based claims about LLM reasoning and call for improved evaluation protocols that focus on reasoning processes rather than static outputs.", "key_contributions": ["Identification of pervasive flaws in reasoning benchmarks", "Demonstration of sensitivity of model performance to minor input variations", "Provision of audited data and evaluation tools for better reasoning assessments"], "limitations": "Focuses on a limited number of LLMs and benchmarks; may not generalize to all models or tasks.", "keywords": ["reasoning benchmarks", "large language models", "evaluation methodology"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.23888", "pdf": "https://arxiv.org/pdf/2506.23888.pdf", "abs": "https://arxiv.org/abs/2506.23888", "title": "Advancing Multi-Step Mathematical Reasoning in Large Language Models through Multi-Layered Self-Reflection with Auto-Prompting", "authors": ["André de Souza Loureiro", "Jorge Valverde-Rebaza", "Julieta Noguez", "David Escarcega", "Ricardo Marcacini"], "categories": ["cs.CL"], "comment": "Accepted for publication in: European Conference on Machine Learning\n  and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD\n  2025). Research Track", "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nimproved their problem-solving capabilities. However, these models still\nstruggle when faced with complex multi-step reasoning tasks. In this paper, we\npropose the Multi-Layered Self-Reflection with Auto-Prompting (MAPS) framework,\na novel approach designed to enhance multi-step mathematical reasoning in LLMs\nby integrating techniques such as Chain of Thought (CoT), Self-Reflection, and\nAuto-Prompting. Unlike traditional static prompting methods, MAPS employs an\niterative refinement process. Initially, the model generates a solution using\nCoT prompting. When errors are detected, an adaptive self-reflection mechanism\nidentifies and analyzes them, generating tailored prompts to guide corrections.\nThese dynamically adjusted prompts enable the model to iteratively refine its\nreasoning. Experiments on four well-established benchmarks across multiple LLMs\nshow that MAPS significantly outperforms standard CoT and achieves competitive\nresults with reasoning-optimized models. In addition, MAPS enables\ngeneral-purpose LLMs to reach performance levels comparable to specialized\nreasoning models. While deeper reflection layers improve accuracy, they also\nincrease token usage and costs. To balance this trade-off, MAPS strategically\nlimits reflection depth, ensuring an optimal balance between cost and reasoning\nperformance.", "AI": {"tldr": "MAPS framework enhances LLMs' multi-step reasoning by integrating CoT, Self-Reflection, and Auto-Prompting for iterative refinement.", "motivation": "To address the limitations of LLMs in performing complex multi-step reasoning tasks.", "method": "The MAPS framework utilizes an iterative refinement process involving Chain of Thought prompting, an adaptive self-reflection mechanism, and dynamic prompt adjustments.", "result": "Experiments show that MAPS outperforms standard CoT prompting and competes with specialized reasoning models across multiple benchmarks.", "conclusion": "While deeper reflection improves accuracy, it raises costs; MAPS manages this with strategic depth limitation for optimal performance versus cost.", "key_contributions": ["Introduction of MAPS framework for iterative reasoning improvements", "Demonstration of MAPS's effectiveness on multiple benchmarks", "Balanced approach to reflection depth for cost efficiency"], "limitations": "Deeper reflection layers increase token usage and costs, necessitating a balance.", "keywords": ["Large Language Models", "Self-Reflection", "Multi-step Reasoning", "Auto-Prompting", "Chain of Thought"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.23921", "pdf": "https://arxiv.org/pdf/2506.23921.pdf", "abs": "https://arxiv.org/abs/2506.23921", "title": "The Trilemma of Truth in Large Language Models", "authors": ["Germans Savcisens", "Tina Eliassi-Rad"], "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": null, "summary": "We often attribute human characteristics to large language models (LLMs) and\nclaim that they \"know\" certain things. LLMs have an internal probabilistic\nknowledge that represents information retained during training. How can we\nassess the veracity of this knowledge? We examine two common methods for\nprobing the veracity of LLMs and discover several assumptions that are flawed.\nTo address these flawed assumptions, we introduce sAwMIL (short for Sparse\nAware Multiple-Instance Learning), a probing method that utilizes the internal\nactivations of LLMs to separate statements into true, false, and neither.\nsAwMIL is based on multiple-instance learning and conformal prediction. We\nevaluate sAwMIL on 5 validity criteria across 16 open-source LLMs, including\nboth default and chat-based variants, as well as on 3 new datasets. Among the\ninsights we provide are: (1) the veracity signal is often concentrated in the\nthird quarter of an LLM's depth; (2) truth and falsehood signals are not always\nsymmetric; (3) linear probes perform better on chat models than on default\nmodels; (4) nonlinear probes may be required to capture veracity signals for\nsome LLMs with reinforcement learning from human feedback or knowledge\ndistillation; and (5) LLMs capture a third type of signal that is distinct from\ntrue and false and is neither true nor false. These findings provide a reliable\nmethod for verifying what LLMs \"know\" and how certain they are of their\nprobabilistic internal knowledge.", "AI": {"tldr": "This paper introduces sAwMIL, a new method for probing the veracity of large language models (LLMs), addressing flaws in existing probing methods by utilizing LLM internal activations to separate statements into true, false, and neither.", "motivation": "The study aims to better assess the veracity of knowledge retained by LLMs, challenging current probing methods that have flawed assumptions.", "method": "sAwMIL uses multiple-instance learning and conformal prediction on the internal activations of LLMs to categorize statements.", "result": "Evaluated on 5 criteria across 16 LLMs and 3 new datasets, findings indicate that the veracity signals concentrate at specific depths with asymmetrical truth and falsehood signals, highlighting the need for nonlinear probes for certain models.", "conclusion": "The method offers a reliable way to determine what LLMs 'know' and to gauge their certainty in probabilistic knowledge representation.", "key_contributions": ["Introduction of sAwMIL for probing LLMs", "Analysis of depth-related veracity signal concentration", "Insights into truth/falsehood asymmetry and necessity of nonlinear probes"], "limitations": "The method may not apply to all types of LLMs, especially those not covered in the evaluation.", "keywords": ["Large Language Models", "Veracity Assessment", "Multiple-Instance Learning", "Conformal Prediction", "Internal Activations"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2506.23929", "pdf": "https://arxiv.org/pdf/2506.23929.pdf", "abs": "https://arxiv.org/abs/2506.23929", "title": "IMPACT: Inflectional Morphology Probes Across Complex Typologies", "authors": ["Mohammed J. Saeed", "Tommi Vehvilainen", "Evgeny Fedoseev", "Sevil Caliskan", "Tatiana Vodolazova"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have shown significant progress on various\nmultilingual benchmarks and are increasingly used to generate and evaluate text\nin non-English languages. However, while they may produce fluent outputs, it\nremains unclear to what extent these models truly grasp the underlying\nlinguistic complexity of those languages, particularly in morphology. To\ninvestigate this, we introduce IMPACT, a synthetically generated evaluation\nframework focused on inflectional morphology, which we publicly release,\ndesigned to evaluate LLM performance across five morphologically rich\nlanguages: Arabic, Russian, Finnish, Turkish, and Hebrew. IMPACT includes\nunit-test-style cases covering both shared and language-specific phenomena,\nfrom basic verb inflections (e.g., tense, number, gender) to unique features\nlike Arabic's reverse gender agreement and vowel harmony in Finnish and\nTurkish. We assess eight multilingual LLMs that, despite strong English\nperformance, struggle with other languages and uncommon morphological patterns,\nespecially when judging ungrammatical examples. We also show that Chain of\nThought and Thinking Models can degrade performance. Our work exposes gaps in\nLLMs' handling of linguistic complexity, pointing to clear room for\nimprovement. To support further research, we publicly release the IMPACT\nframework.", "AI": {"tldr": "The paper introduces IMPACT, an evaluation framework assessing LLM performance in inflectional morphology across five languages, highlighting gaps in LLMs' linguistic understanding.", "motivation": "To investigate the extent to which LLMs grasp linguistic complexities in morphologically rich languages despite their overall performance.", "method": "The IMPACT framework includes unit-test-style evaluations for Arabic, Russian, Finnish, Turkish, and Hebrew, focusing on various morphological phenomena.", "result": "The framework reveals that while LLMs perform well in English, they struggle significantly with inflectional morphology in other languages and with ungrammatical constructions.", "conclusion": "The findings indicate marked deficiencies in LLMs' handling of linguistic intricacies, emphasizing the need for improvement in multilingual and morphological contexts.", "key_contributions": ["Introduction of the IMPACT framework for LLM evaluation", "Assessment of eight multilingual LLMs on morphology", "Public release of the IMPACT evaluation methods"], "limitations": "The framework may not cover all linguistic phenomena and is limited to five specific languages.", "keywords": ["Large Language Models", "Inflectional Morphology", "Evaluation Framework", "Multilingual NLP", "Linguistic Complexity"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.23930", "pdf": "https://arxiv.org/pdf/2506.23930.pdf", "abs": "https://arxiv.org/abs/2506.23930", "title": "Leveraging the Potential of Prompt Engineering for Hate Speech Detection in Low-Resource Languages", "authors": ["Ruhina Tabasshum Prome", "Tarikul Islam Tamiti", "Anomadarshi Barua"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid expansion of social media leads to a marked increase in hate\nspeech, which threatens personal lives and results in numerous hate crimes.\nDetecting hate speech presents several challenges: diverse dialects, frequent\ncode-mixing, and the prevalence of misspelled words in user-generated content\non social media platforms. Recent progress in hate speech detection is\ntypically concentrated on high-resource languages. However, low-resource\nlanguages still face significant challenges due to the lack of large-scale,\nhigh-quality datasets. This paper investigates how we can overcome this\nlimitation via prompt engineering on large language models (LLMs) focusing on\nlow-resource Bengali language. We investigate six prompting strategies -\nzero-shot prompting, refusal suppression, flattering the classifier, multi-shot\nprompting, role prompting, and finally our innovative metaphor prompting to\ndetect hate speech effectively in low-resource languages. We pioneer the\nmetaphor prompting to circumvent the built-in safety mechanisms of LLMs that\nmarks a significant departure from existing jailbreaking methods. We\ninvestigate all six different prompting strategies on the Llama2-7B model and\ncompare the results extensively with three pre-trained word embeddings - GloVe,\nWord2Vec, and FastText for three different deep learning models - multilayer\nperceptron (MLP), convolutional neural network (CNN), and bidirectional gated\nrecurrent unit (BiGRU). To prove the effectiveness of our metaphor prompting in\nthe low-resource Bengali language, we also evaluate it in another low-resource\nlanguage - Hindi, and two high-resource languages - English and German. The\nperformance of all prompting techniques is evaluated using the F1 score, and\nenvironmental impact factor (IF), which measures CO$_2$ emissions, electricity\nusage, and computational time.", "AI": {"tldr": "This paper explores prompt engineering techniques for hate speech detection in low-resource languages, particularly Bengali, using large language models.", "motivation": "The rise of hate speech on social media necessitates effective detection methods, especially for low-resource languages lacking quality datasets.", "method": "The study employs six prompting strategies on the Llama2-7B model, including zero-shot prompting, refusal suppression, flattering the classifier, multi-shot prompting, role prompting, and metaphor prompting, comparing their effectiveness with various deep learning models and pre-trained word embeddings.", "result": "The metaphor prompting technique demonstrates significant improvement in detecting hate speech in Bengali and Hindi, compared to high-resource languages like English and German.", "conclusion": "Innovative prompting strategies, especially metaphor prompting, are effective in enhancing hate speech detection performance in low-resource languages while addressing environmental impact factors.", "key_contributions": ["Introduction of metaphor prompting for hate speech detection", "Evaluation of multiple prompting strategies in low-resource languages", "Assessment of environmental impacts of different detection methods"], "limitations": "", "keywords": ["hate speech detection", "prompt engineering", "low-resource languages", "Bengali", "metaphor prompting"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.23940", "pdf": "https://arxiv.org/pdf/2506.23940.pdf", "abs": "https://arxiv.org/abs/2506.23940", "title": "Graft: Integrating the Domain Knowledge via Efficient Parameter Synergy for MLLMs", "authors": ["Yang Dai", "Jianxiang An", "Tianwei Lin", "Hongyang He", "Hongzhe Huang", "Wenqiao Zhang", "Zheqi Lv", "Siliang Tang", "Yueting Zhuang"], "categories": ["cs.CL"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have achieved success across various\ndomains. However, their applicability tends to degrade when confronted with\ndifferent types of data inputs, especially for MLLMs that have been fine-tuned\nfor specific tasks. Despite its importance, the study of knowledge sharing\namong domain-specific MLLMs--such as those trained for mathematics or\ncode--remains largely underexplored. To address the fragmentation of knowledge\nacross domain-specialized MLLMs, we propose a unified parameter integration\nframework that enables modular composition of expert capabilities. Our method\nis grounded in a novel Compatibility-Aware Parameter Splicing (CAPS) strategy,\nwhich leverages both local functional attribution and global\ninformation-theoretic signals to guide selective parameter fusion. By extending\nthis mechanism to the low-rank adaptation layer granularity, we ensure\nefficient integration with minimal inference overhead. Furthermore, we\nintroduce a domain compatibility scoring mechanism that quantifies inter-expert\nalignment at the activation level and correlates with downstream task utility.\nThis principled fusion protocol allows the final model to synergize\nheterogeneous expertise while preserving structural modularity. Extensive\nevaluations across diverse multimodal benchmarks validate the effectiveness of\nour framework, offering a scalable path toward compositional, domain-adaptive\nMLLMs.", "AI": {"tldr": "A framework for integrating domain-specific expertise in Multimodal Large Language Models (MLLMs) to enhance performance across varied data inputs.", "motivation": "To address the fragmentation of knowledge in domain-specialized MLLMs and improve their applicability to diverse data.", "method": "Introducing a Compatibility-Aware Parameter Splicing (CAPS) strategy for modular composition of expert capabilities and efficient parameter fusion.", "result": "The proposed framework demonstrates effective knowledge sharing and improved performance across various multimodal benchmarks with minimal inference overhead.", "conclusion": "The approach facilitates compositional and domain-adaptive MLLMs, validating the importance of synergizing heterogeneous expertise while maintaining structural modularity.", "key_contributions": ["Unified parameter integration framework for MLLMs", "Compatibility-Aware Parameter Splicing (CAPS) strategy", "Domain compatibility scoring mechanism"], "limitations": "", "keywords": ["Multimodal Large Language Models", "knowledge integration", "parameter splicing", "domain adaptation", "HCI"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.23951", "pdf": "https://arxiv.org/pdf/2506.23951.pdf", "abs": "https://arxiv.org/abs/2506.23951", "title": "Unveiling Decision-Making in LLMs for Text Classification : Extraction of influential and interpretable concepts with Sparse Autoencoders", "authors": ["Mathis Le Bail", "Jérémie Dentan", "Davide Buscaldi", "Sonia Vanier"], "categories": ["cs.CL"], "comment": null, "summary": "Sparse Autoencoders (SAEs) have been successfully used to probe Large\nLanguage Models (LLMs) and extract interpretable concepts from their internal\nrepresentations. These concepts are linear combinations of neuron activations\nthat correspond to human-interpretable features. In this paper, we investigate\nthe effectiveness of SAE-based explainability approaches for sentence\nclassification, a domain where such methods have not been extensively explored.\nWe present a novel SAE-based architecture tailored for text classification,\nleveraging a specialized classifier head and incorporating an activation rate\nsparsity loss. We benchmark this architecture against established methods such\nas ConceptShap, Independent Component Analysis, and other SAE-based concept\nextraction techniques. Our evaluation covers two classification benchmarks and\nfour fine-tuned LLMs from the Pythia family. We further enrich our analysis\nwith two novel metrics for measuring the precision of concept-based\nexplanations, using an external sentence encoder. Our empirical results show\nthat our architecture improves both the causality and interpretability of the\nextracted features.", "AI": {"tldr": "This paper presents a novel Sparse Autoencoder (SAE) architecture for text classification, enhancing explainability by extracting interpretable concepts from large language models (LLMs).", "motivation": "The study aims to explore the effectiveness of SAE-based approaches for text classification, a previously underexplored domain in this context.", "method": "A specialized SAE architecture for sentence classification was developed, incorporating a classifier head and an activation rate sparsity loss. The architecture was benchmarked against existing methods like ConceptShap and Independent Component Analysis.", "result": "The experimental results demonstrated that the proposed SAE architecture enhances the causality and interpretability of features extracted from fine-tuned LLMs.", "conclusion": "The findings suggest that SAE-based explainability approaches can significantly improve the interpretability of sentence classification in large language models.", "key_contributions": ["Introduction of a novel SAE architecture for text classification", "Benchmarking against established explainability methods", "Development of two new metrics for measuring concept explanation precision"], "limitations": "", "keywords": ["Sparse Autoencoder", "Large Language Models", "text classification", "explainability", "interpretability"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.23979", "pdf": "https://arxiv.org/pdf/2506.23979.pdf", "abs": "https://arxiv.org/abs/2506.23979", "title": "TaP: A Taxonomy-Guided Framework for Automated and Scalable Preference Data Generation", "authors": ["Renren Jin", "Tianhao Shen", "Xinwei Wu", "Dan Shi", "Haoran Sun", "Wuwei Huang", "Quandong Wang", "Wei Liu", "Jian Luan", "Bin Wang", "Deyi Xiong"], "categories": ["cs.CL"], "comment": "33 pages, 15 tables, 11 figures", "summary": "Conducting supervised fine-tuning and preference fine-tuning on large\nlanguage models (LLMs) requires high-quality datasets to improve their ability\nto follow instructions and align with human preferences and values. However,\nconstructing such datasets is resource-intensive, and most available datasets\nfor supervised and preference fine-tuning are in English. To address these\nchallenges, we propose the \\underline{\\textbf{Ta}}xonomy-Guided\n\\underline{\\textbf{P}}reference Data Generation (TaP) framework, which\nfacilitates automated and scalable construction of preference datasets across\nvarious languages. TaP is grounded in a structured taxonomy that allows\nfine-grained control over dataset composition, thereby ensuring both diversity\nand comprehensive coverage. We employ TaP-generated datasets to perform\nsupervised and preference fine-tuning on various LLMs. Experimental results\ndemonstrate that LLMs trained on TaP-generated datasets outperform those\ntrained on existing open-source datasets. Remarkably, LLMs trained on\nTaP-generated datasets surpass the performance of those trained on an\nopen-source dataset that is 180 times larger.", "AI": {"tldr": "The TaP framework facilitates the automated generation of high-quality multilingual datasets for fine-tuning LLMs, demonstrating superior performance compared to larger existing datasets.", "motivation": "Creating high-quality datasets for fine-tuning large language models is resource-intensive, and existing datasets are predominantly in English, limiting their applicability.", "method": "The TaP framework uses a structured taxonomy to automate the generation of diverse preference datasets across multiple languages, allowing for scalable dataset construction.", "result": "LLMs trained on TaP-generated datasets outperform those trained on existing open-source datasets, even exceeding the performance of a dataset 180 times larger.", "conclusion": "The TaP framework provides a significant advancement in dataset generation for LLMs, enhancing their ability to align with human values in multiple languages.", "key_contributions": ["Automated generation of multilingual preference datasets", "Use of a structured taxonomy for dataset diversity", "Improved performance of LLMs trained on TaP datasets compared to larger datasets"], "limitations": "The framework's efficiency and effectiveness across less represented languages are yet to be fully analyzed.", "keywords": ["large language models", "dataset generation", "preference datasets", "multilingual", "taxonomy"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.23990", "pdf": "https://arxiv.org/pdf/2506.23990.pdf", "abs": "https://arxiv.org/abs/2506.23990", "title": "Machine Understanding of Scientific Language", "authors": ["Dustin Wright"], "categories": ["cs.CL", "cs.LG"], "comment": "PhD Thesis, 210 pages", "summary": "Scientific information expresses human understanding of nature. This\nknowledge is largely disseminated in different forms of text, including\nscientific papers, news articles, and discourse among people on social media.\nWhile important for accelerating our pursuit of knowledge, not all scientific\ntext is faithful to the underlying science. As the volume of this text has\nburgeoned online in recent years, it has become a problem of societal\nimportance to be able to identify the faithfulness of a given piece of\nscientific text automatically. This thesis is concerned with the cultivation of\ndatasets, methods, and tools for machine understanding of scientific language,\nin order to analyze and understand science communication at scale. To arrive at\nthis, I present several contributions in three areas of natural language\nprocessing and machine learning: automatic fact checking, learning with limited\ndata, and scientific text processing. These contributions include new methods\nand resources for identifying check-worthy claims, adversarial claim\ngeneration, multi-source domain adaptation, learning from crowd-sourced labels,\ncite-worthiness detection, zero-shot scientific fact checking, detecting\nexaggerated scientific claims, and modeling degrees of information change in\nscience communication. Critically, I demonstrate how the research outputs of\nthis thesis are useful for effectively learning from limited amounts of\nscientific text in order to identify misinformative scientific statements and\ngenerate new insights into the science communication process", "AI": {"tldr": "This thesis addresses the automatic identification of faithfulness in scientific text by developing datasets, methods, and tools in NLP and machine learning.", "motivation": "To address the societal importance of identifying the faithfulness of scientific information amidst the growing volume of textual content online.", "method": "Development of new datasets and methods for NLP focused on automatic fact checking, learning from limited data, and processing scientific texts.", "result": "Introduction of novel methods and resources for tasks such as identifying check-worthy claims and conducting zero-shot scientific fact checking.", "conclusion": "The research demonstrates effective approaches for learning from limited scientific texts to identify misinformation and enhance the understanding of science communication.", "key_contributions": ["New methods for automatic fact checking", "Adversarial claim generation techniques", "Zero-shot scientific fact checking mechanisms"], "limitations": "", "keywords": ["natural language processing", "machine learning", "scientific text processing"], "importance_score": 8, "read_time_minutes": 45}}
{"id": "2506.23998", "pdf": "https://arxiv.org/pdf/2506.23998.pdf", "abs": "https://arxiv.org/abs/2506.23998", "title": "Auto-TA: Towards Scalable Automated Thematic Analysis (TA) via Multi-Agent Large Language Models with Reinforcement Learning", "authors": ["Seungjun Yi", "Joakim Nguyen", "Huimin Xu", "Terence Lim", "Andrew Well", "Mia Markey", "Ying Ding"], "categories": ["cs.CL"], "comment": "Presented at ACL 2025 SRW", "summary": "Congenital heart disease (CHD) presents complex, lifelong challenges often\nunderrepresented in traditional clinical metrics. While unstructured narratives\noffer rich insights into patient and caregiver experiences, manual thematic\nanalysis (TA) remains labor-intensive and unscalable. We propose a fully\nautomated large language model (LLM) pipeline that performs end-to-end TA on\nclinical narratives, which eliminates the need for manual coding or full\ntranscript review. Our system employs a novel multi-agent framework, where\nspecialized LLM agents assume roles to enhance theme quality and alignment with\nhuman analysis. To further improve thematic relevance, we optionally integrate\nreinforcement learning from human feedback (RLHF). This supports scalable,\npatient-centered analysis of large qualitative datasets and allows LLMs to be\nfine-tuned for specific clinical contexts.", "AI": {"tldr": "A proposed automated LLM pipeline for thematic analysis of clinical narratives in CHD, utilizing a multi-agent framework and optionally incorporating RLHF for improved patient-centered analysis.", "motivation": "To address the challenges of manual thematic analysis in clinical narratives related to congenital heart disease by providing a scalable and efficient solution.", "method": "A fully automated large language model pipeline utilizing a multi-agent framework to perform end-to-end thematic analysis, with an optional reinforcement learning component.", "result": "The proposed system improves the quality of theme analysis in patient and caregiver narratives and allows for the analysis of large qualitative datasets without manual intervention.", "conclusion": "This automated approach enhances patient-centered analysis in clinical contexts, making it easier to derive insights from narratives related to congenital heart disease.", "key_contributions": ["Development of a fully automated LLM pipeline for thematic analysis.", "Implementation of a multi-agent framework for enhanced analysis.", "Integration of RLHF for improved thematic relevance."], "limitations": "", "keywords": ["Congenital Heart Disease", "Thematic Analysis", "Large Language Models", "Reinforcement Learning", "Patient-Centered Care"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2506.24006", "pdf": "https://arxiv.org/pdf/2506.24006.pdf", "abs": "https://arxiv.org/abs/2506.24006", "title": "Large Language Models Don't Make Sense of Word Problems. A Scoping Review from a Mathematics Education Perspective", "authors": ["Anselm R. Strohmaier", "Wim Van Dooren", "Kathrin Seßler", "Brian Greer", "Lieven Verschaffel"], "categories": ["cs.CL", "math.HO"], "comment": null, "summary": "The progress of Large Language Models (LLMs) like ChatGPT raises the question\nof how they can be integrated into education. One hope is that they can support\nmathematics learning, including word-problem solving. Since LLMs can handle\ntextual input with ease, they appear well-suited for solving mathematical word\nproblems. Yet their real competence, whether they can make sense of the\nreal-world context, and the implications for classrooms remain unclear. We\nconducted a scoping review from a mathematics-education perspective, including\nthree parts: a technical overview, a systematic review of word problems used in\nresearch, and a state-of-the-art empirical evaluation of LLMs on mathematical\nword problems. First, in the technical overview, we contrast the\nconceptualization of word problems and their solution processes between LLMs\nand students. In computer-science research this is typically labeled\nmathematical reasoning, a term that does not align with usage in mathematics\neducation. Second, our literature review of 213 studies shows that the most\npopular word-problem corpora are dominated by s-problems, which do not require\na consideration of realities of their real-world context. Finally, our\nevaluation of GPT-3.5-turbo, GPT-4o-mini, GPT-4.1, and o3 on 287 word problems\nshows that most recent LLMs solve these s-problems with near-perfect accuracy,\nincluding a perfect score on 20 problems from PISA. LLMs still showed\nweaknesses in tackling problems where the real-world context is problematic or\nnon-sensical. In sum, we argue based on all three aspects that LLMs have\nmastered a superficial solution process but do not make sense of word problems,\nwhich potentially limits their value as instructional tools in mathematics\nclassrooms.", "AI": {"tldr": "This paper reviews the integration of Large Language Models (LLMs) in mathematics education, emphasizing their performance in word problem-solving and the disconnect between LLMs' and students' reasoning processes.", "motivation": "To explore how LLMs like ChatGPT can support mathematics learning, particularly in solving word problems, and to assess their limitations in understanding real-world contexts.", "method": "The study conducts a scoping review consisting of a technical overview comparing LLMs and student reasoning, a systematic literature review of 213 studies, and an empirical evaluation of several LLMs' performances on 287 mathematical word problems.", "result": "The review reveals that LLMs demonstrate near-perfect accuracy on specific problem types (s-problems) but struggle with word problems that involve complex real-world contexts, highlighting their limitations as instructional tools.", "conclusion": "LLMs excel in superficial solution processes but fail to genuinely comprehend the context of word problems, which may restrict their effectiveness in educational settings.", "key_contributions": ["Technical overview contrasting LLMs and students' approaches to word problems.", "Literature review identifying gaps in current research on word problems used in studies.", "Empirical evaluation showing varying performance of LLMs on mathematical problems based on context."], "limitations": "LLMs do not effectively understand or process the real-world context of certain mathematical problems, limiting their usefulness in educational contexts.", "keywords": ["Large Language Models", "mathematics education", "word problems", "empirical evaluation", "educational tools"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.24016", "pdf": "https://arxiv.org/pdf/2506.24016.pdf", "abs": "https://arxiv.org/abs/2506.24016", "title": "EXPERT: An Explainable Image Captioning Evaluation Metric with Structured Explanations", "authors": ["Hyunjong Kim", "Sangyeop Kim", "Jongheon Jeong", "Yeongjae Cho", "Sungzoon Cho"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Accepted at ACL 2025 Findings", "summary": "Recent advances in large language models and vision-language models have led\nto growing interest in explainable evaluation metrics for image captioning.\nHowever, these metrics generate explanations without standardized criteria, and\nthe overall quality of the generated explanations remains unverified. In this\npaper, we propose EXPERT, a reference-free evaluation metric that provides\nstructured explanations based on three fundamental criteria: fluency,\nrelevance, and descriptiveness. By constructing large-scale datasets of\nhigh-quality structured explanations, we develop a two-stage evaluation\ntemplate to effectively supervise a vision-language model for both scoring and\nexplanation generation. EXPERT achieves state-of-the-art results on benchmark\ndatasets while providing significantly higher-quality explanations than\nexisting metrics, as validated through comprehensive human evaluation. Our code\nand datasets are available at https://github.com/hjkim811/EXPERT.", "AI": {"tldr": "EXPERT is a new evaluation metric for image captioning that offers structured explanations based on fluency, relevance, and descriptiveness, surpassing existing metrics in quality and verification.", "motivation": "To address the lack of standardized criteria in current evaluation metrics for image captioning and to verify the quality of generated explanations.", "method": "Developed a two-stage evaluation template and constructed large-scale datasets of high-quality structured explanations to supervise vision-language models for scoring and explanation generation.", "result": "EXPERT achieves state-of-the-art results on benchmark datasets and provides significantly higher-quality explanations verified through comprehensive human evaluation.", "conclusion": "EXPERT offers a more reliable and structured approach to evaluating image captioning, beneficial for future research and applications.", "key_contributions": ["Introduced a reference-free evaluation metric for image captioning.", "Established structured criteria for evaluation: fluency, relevance, and descriptiveness.", "Achieved state-of-the-art results while providing superior explanation quality."], "limitations": "", "keywords": ["image captioning", "evaluation metrics", "explainable AI", "vision-language models", "natural language processing"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2506.24068", "pdf": "https://arxiv.org/pdf/2506.24068.pdf", "abs": "https://arxiv.org/abs/2506.24068", "title": "STACK: Adversarial Attacks on LLM Safeguard Pipelines", "authors": ["Ian R. McKenzie", "Oskar J. Hollinsworth", "Tom Tseng", "Xander Davies", "Stephen Casper", "Aaron D. Tucker", "Robert Kirk", "Adam Gleave"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Frontier AI developers are relying on layers of safeguards to protect against\ncatastrophic misuse of AI systems. Anthropic guards their latest Claude 4 Opus\nmodel using one such defense pipeline, and other frontier developers including\nGoogle DeepMind and OpenAI pledge to soon deploy similar defenses. However, the\nsecurity of such pipelines is unclear, with limited prior work evaluating or\nattacking these pipelines. We address this gap by developing and red-teaming an\nopen-source defense pipeline. First, we find that a novel few-shot-prompted\ninput and output classifier outperforms state-of-the-art open-weight safeguard\nmodel ShieldGemma across three attacks and two datasets, reducing the attack\nsuccess rate (ASR) to 0% on the catastrophic misuse dataset ClearHarm. Second,\nwe introduce a STaged AttaCK (STACK) procedure that achieves 71% ASR on\nClearHarm in a black-box attack against the few-shot-prompted classifier\npipeline. Finally, we also evaluate STACK in a transfer setting, achieving 33%\nASR, providing initial evidence that it is feasible to design attacks with no\naccess to the target pipeline. We conclude by suggesting specific mitigations\nthat developers could use to thwart staged attacks.", "AI": {"tldr": "This paper evaluates the security of AI defense pipelines and introduces a novel few-shot prompted classifier that outperforms existing safeguards against misuse, while also analyzing the feasibility of black-box attacks.", "motivation": "The motivation is to assess and improve the security of AI defense pipelines that are crucial for preventing catastrophic misuse of AI systems, which are becoming standard among frontier AI developers.", "method": "We developed and red-teamed an open-source defense pipeline, comparing a few-shot-prompted input and output classifier against existing safeguard models and implementing a novel black-box attack method (STACK).", "result": "The few-shot-prompted classifier outperformed the ShieldGemma model with an attack success rate of 0% on the ClearHarm dataset, while the STACK procedure achieved 71% ASR in a black-box attack.", "conclusion": "The research concludes that it is possible to devise effective attacks on AI pipeline defenses, and recommends specific mitigations for developers to counter such staged attacks.", "key_contributions": ["Development of an open-source defense pipeline", "Introduction of a few-shot prompted input/output classifier", "Proposing the STaged AttaCK (STACK) procedure for evaluating pipeline vulnerabilities"], "limitations": "Limited prior work on evaluating or attacking AI defense pipelines.", "keywords": ["AI Defense Pipelines", "Catastrophic Misuse", "Few-Shot Learning", "Red-Teaming", "Black-Box Attacks"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.24106", "pdf": "https://arxiv.org/pdf/2506.24106.pdf", "abs": "https://arxiv.org/abs/2506.24106", "title": "On the Predictive Power of Representation Dispersion in Language Models", "authors": ["Yanhong Li", "Ming Li", "Karen Livescu", "Jiawei Zhou"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We show that a language model's ability to predict text is tightly linked to\nthe breadth of its embedding space: models that spread their contextual\nrepresentations more widely tend to achieve lower perplexity. Concretely, we\nfind that representation dispersion - the average pairwise cosine distance\namong hidden vectors - strongly and negatively correlates with perplexity\nacross diverse model families (LLaMA, Qwen, and others) and domains (Wikipedia,\nnews, scientific abstracts). Beyond illustrating this link, we show how\ndispersion can be leveraged for a range of practical tasks without requiring\nlabeled data. First, measuring dispersion on unlabeled text allows us to\npredict downstream accuracy in new domains, offering a data-efficient tool for\nmodel selection. Next, we find that identifying layers with higher dispersion\npinpoints the best representations for retrieval-based methods such as kNN-LM,\nbypassing exhaustive layer-by-layer searches. Finally, we integrate a simple\npush-away objective into training, which increases dispersion in both\nsingle-domain and cross-domain scenarios and directly improves perplexity in\neach.", "AI": {"tldr": "The paper demonstrates that the dispersion of a language model's embeddings is correlated with its text prediction performance, and suggests methods to leverage this for model selection and improved accuracy.", "motivation": "To explore the relationship between the embedding space of language models and their text prediction capabilities, and to find practical applications for this insight.", "method": "We analyze the correlation between representation dispersion (average pairwise cosine distance among hidden vectors) and perplexity across various models and domains. We also propose a training objective to enhance dispersion.", "result": "High representation dispersion is linked to lower perplexity. Measuring dispersion on unlabeled texts can predict accuracy in new domains. We identified layers with higher dispersion for improved retrieval accuracy and proposed a training objective that enhances dispersion and reduces perplexity.", "conclusion": "Increasing embedding dispersion can directly improve language model performance, providing a novel tool for model assessment and training strategies.", "key_contributions": ["Establishes a strong correlation between embedding dispersion and perplexity in language models.", "Introduces methods for leveraging dispersion for model selection using unlabeled data.", "Proposes a training objective to enhance embedding dispersion, resulting in improved performance."], "limitations": "", "keywords": ["language models", "embedding dispersion", "perplexity", "model selection", "training objectives"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.24117", "pdf": "https://arxiv.org/pdf/2506.24117.pdf", "abs": "https://arxiv.org/abs/2506.24117", "title": "Computational Detection of Intertextual Parallels in Biblical Hebrew: A Benchmark Study Using Transformer-Based Language Models", "authors": ["David M. Smiley"], "categories": ["cs.CL"], "comment": null, "summary": "Identifying parallel passages in biblical Hebrew is foundational in biblical\nscholarship for uncovering intertextual relationships. Traditional methods rely\non manual comparison, which is labor-intensive and prone to human error. This\nstudy evaluates the potential of pre-trained transformer-based language models,\nincluding E5, AlephBERT, MPNet, and LaBSE, for detecting textual parallels in\nthe Hebrew Bible. Focusing on known parallels between the books of Samuel/Kings\nand Chronicles, I assessed each model's capability to generate word embeddings\nthat delineate parallel from non-parallel passages. Utilizing cosine similarity\nand Wasserstein Distance measures, I found that E5 and AlephBERT show\nsignificant promise, with E5 excelling in parallel detection and AlephBERT\ndemonstrating stronger non-parallel differentiation. These findings indicate\nthat pre-trained models can enhance the efficiency and accuracy of detecting\nintertextual parallels in ancient texts, suggesting broader applications for\nancient language studies.", "AI": {"tldr": "This study evaluates the use of transformer-based models for identifying parallel passages in biblical Hebrew, finding E5 and AlephBERT particularly effective.", "motivation": "The paper addresses the labor-intensive and error-prone nature of traditional methods for identifying intertextual relationships in biblical scholarship.", "method": "The study assesses pre-trained transformer models (E5, AlephBERT, MPNet, LaBSE) for their ability to generate word embeddings that identify parallel versus non-parallel passages using cosine similarity and Wasserstein Distance.", "result": "E5 and AlephBERT showed significant promise in parallel detection, with E5 excelling in identifying parallels and AlephBERT better at differentiating non-parallel texts.", "conclusion": "Pre-trained models can enhance the efficiency and accuracy of detecting intertextual parallels in ancient texts, indicating potential broader applications in ancient language studies.", "key_contributions": ["Demonstration of the effectiveness of transformer models in biblical text analysis.", "Quantitative evaluation of model performance using cosine similarity and Wasserstein Distance.", "Implications for efficiency improvements in ancient language studies."], "limitations": "", "keywords": ["human-computer interaction", "natural language processing", "transformer models"], "importance_score": 2, "read_time_minutes": 10}}
{"id": "2305.16264", "pdf": "https://arxiv.org/pdf/2305.16264.pdf", "abs": "https://arxiv.org/abs/2305.16264", "title": "Scaling Data-Constrained Language Models", "authors": ["Niklas Muennighoff", "Alexander M. Rush", "Boaz Barak", "Teven Le Scao", "Aleksandra Piktus", "Nouamane Tazi", "Sampo Pyysalo", "Thomas Wolf", "Colin Raffel"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "50 pages (9 main), 39 figures, 15 tables", "summary": "The current trend of scaling language models involves increasing both\nparameter count and training dataset size. Extrapolating this trend suggests\nthat training dataset size may soon be limited by the amount of text data\navailable on the internet. Motivated by this limit, we investigate scaling\nlanguage models in data-constrained regimes. Specifically, we run a large set\nof experiments varying the extent of data repetition and compute budget,\nranging up to 900 billion training tokens and 9 billion parameter models. We\nfind that with constrained data for a fixed compute budget, training with up to\n4 epochs of repeated data yields negligible changes to loss compared to having\nunique data. However, with more repetition, the value of adding compute\neventually decays to zero. We propose and empirically validate a scaling law\nfor compute optimality that accounts for the decreasing value of repeated\ntokens and excess parameters. Finally, we experiment with approaches mitigating\ndata scarcity, including augmenting the training dataset with code data or\nremoving commonly used filters. Models and datasets from our 400 training runs\nare freely available at https://github.com/huggingface/datablations.", "AI": {"tldr": "This paper explores the effects of data repetition and compute budget on training large language models in data-constrained scenarios, finding optimal training strategies and introducing a new scaling law for compute efficiency.", "motivation": "The study is motivated by the impending limits on available training data from the internet, as language models scale in both parameter counts and dataset sizes.", "method": "A large set of experiments were conducted, varying data repetition and compute budgets across models with up to 900 billion training tokens and 9 billion parameters.", "result": "Results indicate that training with up to 4 epochs of repeated data leads to negligible loss changes compared to unique data, but additional repetition diminishes the returns of added compute resources.", "conclusion": "The paper proposes a new scaling law for compute optimality, highlighting strategies to address data scarcity through dataset augmentation and modifications in filtering practices.", "key_contributions": ["Investigation of training large language models in data-constrained environments.", "Empirical validation of a scaling law for compute optimality based on repeated tokens.", "Release of models and datasets from extensive training experiments."], "limitations": "", "keywords": ["Language Models", "Data Repetition", "Compute Budget", "Scaling Laws", "Data Scarcity"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2404.19543", "pdf": "https://arxiv.org/pdf/2404.19543.pdf", "abs": "https://arxiv.org/abs/2404.19543", "title": "RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing", "authors": ["Yucheng Hu", "Yuxing Lu"], "categories": ["cs.CL", "cs.AI"], "comment": "30 pages, 7 figures. Draft version 1", "summary": "Large Language Models (LLMs) have catalyzed significant advancements in\nNatural Language Processing (NLP), yet they encounter challenges such as\nhallucination and the need for domain-specific knowledge. To mitigate these,\nrecent methodologies have integrated information retrieved from external\nresources with LLMs, substantially enhancing their performance across NLP\ntasks. This survey paper addresses the absence of a comprehensive overview on\nRetrieval-Augmented Language Models (RALMs), both Retrieval-Augmented\nGeneration (RAG) and Retrieval-Augmented Understanding (RAU), providing an\nin-depth examination of their paradigm, evolution, taxonomy, and applications.\nThe paper discusses the essential components of RALMs, including Retrievers,\nLanguage Models, and Augmentations, and how their interactions lead to diverse\nmodel structures and applications. RALMs demonstrate utility in a spectrum of\ntasks, from translation and dialogue systems to knowledge-intensive\napplications. The survey includes several evaluation methods of RALMs,\nemphasizing the importance of robustness, accuracy, and relevance in their\nassessment. It also acknowledges the limitations of RALMs, particularly in\nretrieval quality and computational efficiency, offering directions for future\nresearch. In conclusion, this survey aims to offer a structured insight into\nRALMs, their potential, and the avenues for their future development in NLP.\nThe paper is supplemented with a Github Repository containing the surveyed\nworks and resources for further study:\nhttps://github.com/2471023025/RALM_Survey.", "AI": {"tldr": "This survey paper reviews Retrieval-Augmented Language Models (RALMs), covering their taxonomy, components, applications, and evaluation methods, while discussing challenges and future research directions.", "motivation": "To fill the gap in comprehensive understanding of Retrieval-Augmented Language Models (RALMs) and their applications in Natural Language Processing (NLP).", "method": "The paper surveys existing literature on RALMs, discussing their components such as Retrievers, Language Models, and Augmentations, and how these interact to form various models.", "result": "RALMs show significant performance improvements in tasks like translation, dialogue systems, and knowledge-intensive applications through the integration of external information.", "conclusion": "The paper provides a structured overview of RALMs, highlighting their potential and suggesting future research directions to address challenges in retrieval quality and computational efficiency.", "key_contributions": ["Comprehensive overview of Retrieval-Augmented Language Models (RALMs)", "Discussion on applications and evaluation methods for RALMs", "Identification of key challenges and future research avenues in the field."], "limitations": "Limitations related to retrieval quality and computational efficiency of RALMs.", "keywords": ["Retrieval-Augmented Language Models", "Natural Language Processing", "Evaluation methods", "Domain-specific knowledge", "NLP tasks"], "importance_score": 9, "read_time_minutes": 30}}
{"id": "2405.01299", "pdf": "https://arxiv.org/pdf/2405.01299.pdf", "abs": "https://arxiv.org/abs/2405.01299", "title": "The Effectiveness of LLMs as Annotators: A Comparative Overview and Empirical Analysis of Direct Representation", "authors": ["Maja Pavlovic", "Massimo Poesio"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "LREC-COLING NLPerspectives workshop", "summary": "Large Language Models (LLMs) have emerged as powerful support tools across\nvarious natural language tasks and a range of application domains. Recent\nstudies focus on exploring their capabilities for data annotation. This paper\nprovides a comparative overview of twelve studies investigating the potential\nof LLMs in labelling data. While the models demonstrate promising cost and\ntime-saving benefits, there exist considerable limitations, such as\nrepresentativeness, bias, sensitivity to prompt variations and English language\npreference. Leveraging insights from these studies, our empirical analysis\nfurther examines the alignment between human and GPT-generated opinion\ndistributions across four subjective datasets. In contrast to the studies\nexamining representation, our methodology directly obtains the opinion\ndistribution from GPT. Our analysis thereby supports the minority of studies\nthat are considering diverse perspectives when evaluating data annotation tasks\nand highlights the need for further research in this direction.", "AI": {"tldr": "This paper reviews twelve studies on the use of Large Language Models (LLMs) for data annotation, noting their potential benefits and significant limitations.", "motivation": "To explore the role of LLMs in data annotation and their effectiveness in labeling datasets compared to human insights.", "method": "A comparative overview of twelve studies on LLMs for data labeling is presented, with an empirical analysis of GPT-generated opinion distributions against human opinions across four subjective datasets.", "result": "LLMs show cost and time-saving benefits in data labeling but face issues like bias, prompt sensitivity, and limited language representativeness.", "conclusion": "Further research is necessary to ensure diverse perspectives in data annotation tasks utilizing LLMs.", "key_contributions": ["Comparative overview of twelve studies on LLM data annotation", "Empirical analysis of alignment between human and GPT-generated opinions", "Highlighting limitations and the need for diversity in annotation tasks"], "limitations": "Considerable limitations include bias, sensitivity to prompt variations, and a preference for English language.", "keywords": ["Large Language Models", "data annotation", "human-computer interaction", "opinion distribution", "empirical analysis"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2406.15627", "pdf": "https://arxiv.org/pdf/2406.15627.pdf", "abs": "https://arxiv.org/abs/2406.15627", "title": "Benchmarking Uncertainty Quantification Methods for Large Language Models with LM-Polygraph", "authors": ["Roman Vashurin", "Ekaterina Fadeeva", "Artem Vazhentsev", "Lyudmila Rvanova", "Akim Tsvigun", "Daniil Vasilev", "Rui Xing", "Abdelrahman Boda Sadallah", "Kirill Grishchenkov", "Sergey Petrakov", "Alexander Panchenko", "Timothy Baldwin", "Preslav Nakov", "Maxim Panov", "Artem Shelmanov"], "categories": ["cs.CL", "cs.LG"], "comment": "Published at TACL 2025, presented at ACL 2025. Roman Vashurin,\n  Ekaterina Fadeeva, Artem Vazhentsev contributed equally", "summary": "The rapid proliferation of large language models (LLMs) has stimulated\nresearchers to seek effective and efficient approaches to deal with LLM\nhallucinations and low-quality outputs. Uncertainty quantification (UQ) is a\nkey element of machine learning applications in dealing with such challenges.\nHowever, research to date on UQ for LLMs has been fragmented in terms of\ntechniques and evaluation methodologies. In this work, we address this issue by\nintroducing a novel benchmark that implements a collection of state-of-the-art\nUQ baselines and offers an environment for controllable and consistent\nevaluation of novel UQ techniques over various text generation tasks. Our\nbenchmark also supports the assessment of confidence normalization methods in\nterms of their ability to provide interpretable scores. Using our benchmark, we\nconduct a large-scale empirical investigation of UQ and normalization\ntechniques across eleven tasks, identifying the most effective approaches.\nCode: https://github.com/IINemo/lm-polygraph Benchmark:\nhttps://huggingface.co/LM-Polygraph", "AI": {"tldr": "This paper introduces a benchmark for uncertainty quantification (UQ) in large language models (LLMs), addressing the issue of LLM hallucinations and low-quality outputs.", "motivation": "The rapid growth of large language models has led to significant challenges related to hallucinations and unreliable outputs, necessitating effective uncertainty quantification methods.", "method": "The authors developed a comprehensive benchmark that consolidates state-of-the-art UQ techniques and provides a controlled environment for evaluating these methods across various text generation tasks.", "result": "The empirical study revealed the most effective UQ and normalization techniques across eleven tasks, enhancing understanding of their effectiveness.", "conclusion": "This benchmark enables consistent evaluation of UQ methods and aids in the development of interpretable scoring systems for LLM outputs.", "key_contributions": ["Introduction of a novel benchmark for UQ in LLMs.", "Implementation of state-of-the-art UQ techniques for evaluation.", "Large-scale empirical investigation of UQ techniques across eleven tasks."], "limitations": "", "keywords": ["uncertainty quantification", "large language models", "benchmarking", "text generation", "confidence normalization"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2407.01461", "pdf": "https://arxiv.org/pdf/2407.01461.pdf", "abs": "https://arxiv.org/abs/2407.01461", "title": "Enhancing the Capability and Robustness of Large Language Models through Reinforcement Learning-Driven Query Refinement", "authors": ["Xiaohua Wang", "Zisu Huang", "Feiran Zhang", "Zhibo Xu", "Cenyuan Zhang", "Qi Qian", "Xiaoqing Zheng", "Xuanjing Huang"], "categories": ["cs.CL"], "comment": null, "summary": "The capacity of large language models (LLMs) to generate honest, harmless,\nand helpful responses heavily relies on the quality of user prompts. However,\nthese prompts often tend to be brief and vague, thereby significantly limiting\nthe full potential of LLMs. Moreover, harmful prompts can be meticulously\ncrafted and manipulated by adversaries to jailbreak LLMs, inducing them to\nproduce potentially toxic content. To enhance the capabilities of LLMs while\nmaintaining strong robustness against harmful jailbreak inputs, this study\nproposes a transferable and pluggable framework that refines user prompts\nbefore they are input into LLMs. This strategy improves the quality of the\nqueries, empowering LLMs to generate more truthful, benign and useful\nresponses. Specifically, a lightweight query refinement model is introduced and\ntrained using a specially designed reinforcement learning approach that\nincorporates multiple objectives to enhance particular capabilities of LLMs.\nExtensive experiments demonstrate that the refinement model not only improves\nthe quality of responses but also strengthens their robustness against\njailbreak attacks. Code is available at:\nhttps://github.com/Huangzisu/query-refinement .", "AI": {"tldr": "This paper proposes a framework for refining user prompts to enhance large language models' (LLMs) performance and robustness against harmful inputs.", "motivation": "The quality of user prompts significantly affects the performance of LLMs, with vague prompts limiting their potential and harmful prompts posing security risks.", "method": "A transferable and pluggable query refinement framework is proposed, utilizing a lightweight model trained with a reinforcement learning approach focused on multiple objectives.", "result": "Experiments show that the refinement model enhances response quality and improves robustness against jailbreak attacks on LLMs.", "conclusion": "Refining user prompts can lead to more truthful and useful outputs from LLMs while mitigating the risks of adversarial prompts.", "key_contributions": ["Introduction of a framework for query refinement in LLMs.", "Development of a lightweight model trained with multi-objective reinforcement learning.", "Demonstrated enhancement of response quality and robustness against attacks."], "limitations": "", "keywords": ["Large Language Models", "Prompt Engineering", "Reinforcement Learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2407.12749", "pdf": "https://arxiv.org/pdf/2407.12749.pdf", "abs": "https://arxiv.org/abs/2407.12749", "title": "ChipXplore: Natural Language Exploration of Hardware Designs and Libraries", "authors": ["Manar Abdelatty", "Jacob Rosenstein", "Sherief Reda"], "categories": ["cs.CL"], "comment": "10 pages", "summary": "Hardware design workflows rely on Process Design Kits (PDKs) from different\nfabrication nodes, each containing standard cell libraries optimized for speed,\npower, or density. Engineers typically navigate between the design and target\nPDK to make informed decisions, such as selecting gates for area optimization\nor enhancing the speed of the critical path. However, this process is often\nmanual, time-consuming, and prone to errors. To address this, we present\nChipXplore, a multi-agent collaborative framework powered by large language\nmodels that enables engineers to query hardware designs and PDKs using natural\nlanguage. By exploiting the structured nature of PDK and hardware design data,\nChipXplore retrieves relevant information through text-to-SQL and\ntext-to-Cypher customized workflows. The framework achieves an execution\naccuracy of 97.39\\% in complex natural language queries and improves\nproductivity by making retrieval 5.63x faster while reducing errors by 5.25x in\nuser studies. Compared to generic workflows, ChipXplore's customized workflow\nis capable of orchestrating reasoning and planning over multiple databases,\nimproving accuracy by 29.78\\%. ChipXplore lays the foundation for building\nautonomous agents capable of tackling diverse physical design tasks that\nrequire PDK and hardware design awareness.", "AI": {"tldr": "ChipXplore is a multi-agent framework using large language models to enhance hardware design workflows by enabling natural language queries for PDKs, thereby improving accuracy and productivity.", "motivation": "To streamline the manual and error-prone process of querying hardware designs and PDKs, facilitating quicker and more accurate decision-making for engineers.", "method": "The framework utilizes text-to-SQL and text-to-Cypher workflows to retrieve information from structured PDK and hardware design data, enabling natural language queries.", "result": "ChipXplore achieves a 97.39% execution accuracy in complex queries, speeds up retrieval by 5.63x, and reduces errors by 5.25x in user studies, improving overall productivity.", "conclusion": "ChipXplore sets the groundwork for future autonomous agents designed to perform various physical design tasks involving hardware design and PDKs more efficiently.", "key_contributions": ["Development of a multi-agent collaborative framework for hardware design queries", "Implementation of tailored text-to-SQL and text-to-Cypher workflows for improved accuracy", "Demonstration of significant productivity improvements and error reduction compared to traditional methods."], "limitations": "", "keywords": ["hardware design", "process design kits", "multi-agent", "large language models", "natural language processing"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2407.21536", "pdf": "https://arxiv.org/pdf/2407.21536.pdf", "abs": "https://arxiv.org/abs/2407.21536", "title": "Tracing Intricate Cues in Dialogue: Joint Graph Structure and Sentiment Dynamics for Multimodal Emotion Recognition", "authors": ["Jiang Li", "Xiaoping Wang", "Zhigang Zeng"], "categories": ["cs.CL"], "comment": "Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence", "summary": "Multimodal emotion recognition in conversation (MERC) has garnered\nsubstantial research attention recently. Existing MERC methods face several\nchallenges: (1) they fail to fully harness direct inter-modal cues, possibly\nleading to less-than-thorough cross-modal modeling; (2) they concurrently\nextract information from the same and different modalities at each network\nlayer, potentially triggering conflicts from the fusion of multi-source data;\n(3) they lack the agility required to detect dynamic sentimental changes,\nperhaps resulting in inaccurate classification of utterances with abrupt\nsentiment shifts. To address these issues, a novel approach named GraphSmile is\nproposed for tracking intricate emotional cues in multimodal dialogues.\nGraphSmile comprises two key components, i.e., GSF and SDP modules. GSF\ningeniously leverages graph structures to alternately assimilate inter-modal\nand intra-modal emotional dependencies layer by layer, adequately capturing\ncross-modal cues while effectively circumventing fusion conflicts. SDP is an\nauxiliary task to explicitly delineate the sentiment dynamics between\nutterances, promoting the model's ability to distinguish sentimental\ndiscrepancies. GraphSmile is effortlessly applied to multimodal sentiment\nanalysis in conversation (MSAC), thus enabling simultaneous execution of MERC\nand MSAC tasks. Empirical results on multiple benchmarks demonstrate that\nGraphSmile can handle complex emotional and sentimental patterns, significantly\noutperforming baseline models.", "AI": {"tldr": "GraphSmile is a novel approach for multimodal emotion recognition in conversation that effectively captures emotional cues while addressing existing challenges in the field.", "motivation": "Current methods in multimodal emotion recognition face challenges such as ineffective cross-modal modeling, conflicts from data fusion, and inability to track dynamic emotional changes.", "method": "GraphSmile employs GSF and SDP modules; GSF uses graph structures to capture emotional dependencies, while SDP delineates sentiment dynamics between utterances.", "result": "GraphSmile significantly outperformed baseline models on various benchmarks in terms of handling complex emotional patterns.", "conclusion": "GraphSmile enables simultaneous multimodal sentiment analysis and emotion recognition in conversation tasks with improved accuracy.", "key_contributions": ["Introduction of GraphSmile for addressing challenges in MERC", "Utilization of graph structures for better cross-modal cue capturing", "Enhancement of sentiment dynamics tracking through the SDP module"], "limitations": "", "keywords": ["Multimodal Emotion Recognition", "Graph Neural Networks", "Sentiment Analysis"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2408.06576", "pdf": "https://arxiv.org/pdf/2408.06576.pdf", "abs": "https://arxiv.org/abs/2408.06576", "title": "CTISum: A New Benchmark Dataset For Cyber Threat Intelligence Summarization", "authors": ["Wei Peng", "Junmei Ding", "Wei Wang", "Lei Cui", "Wei Cai", "Zhiyu Hao", "Xiaochun Yun"], "categories": ["cs.CL"], "comment": null, "summary": "Cyber Threat Intelligence (CTI) summarization involves generating concise and\naccurate highlights from web intelligence data, which is critical for providing\ndecision-makers with actionable insights to swiftly detect and respond to cyber\nthreats in the cybersecurity domain. Despite that, the development of efficient\ntechniques for summarizing CTI reports, comprising facts, analytical insights,\nattack processes, and more, has been hindered by the lack of suitable datasets.\nTo address this gap, we introduce CTISum, a new benchmark dataset designed for\nthe CTI summarization task. Recognizing the significance of understanding\nattack processes, we also propose a novel fine-grained subtask: attack process\nsummarization, which aims to help defenders assess risks, identify security\ngaps, and uncover vulnerabilities. Specifically, a multi-stage annotation\npipeline is designed to collect and annotate CTI data from diverse web sources,\nalongside a comprehensive benchmarking of CTISum using both extractive,\nabstractive and LLMs-based summarization methods. Experimental results reveal\nthat current state-of-the-art models face significant challenges when applied\nto CTISum, highlighting that automatic summarization of CTI reports remains an\nopen research problem. The code and example dataset can be made publicly\navailable at https://github.com/pengwei-iie/CTISum.", "AI": {"tldr": "CTI summarization aims to generate highlights from web intelligence data for decision-makers in cybersecurity. This paper introduces CTISum, a dataset for CTI summarization, and a subtask focused on attack process summarization, addressing the challenges of summarizing CTI reports.", "motivation": "There is a lack of suitable datasets for summarizing Cyber Threat Intelligence (CTI) reports, which are crucial for quick detection and response to cyber threats.", "method": "The authors propose CTISum, a new benchmark dataset for CTI summarization, and a multi-stage annotation pipeline to collect and annotate CTI data. They also benchmark this dataset using various summarization methods, including extractive, abstractive, and LLM-based techniques.", "result": "Experimental results indicate that current state-of-the-art models struggle significantly with the CTISum dataset, underlining the challenges in automatic summarization of CTI reports.", "conclusion": "The research highlights the ongoing open problem of CTI report summarization and the need for more effective techniques and datasets.", "key_contributions": ["Introduction of CTISum, a benchmark dataset for CTI summarization.", "Proposal of attack process summarization as a novel fine-grained subtask.", "Identification of significant challenges faced by current models in CTI summarization."], "limitations": "The paper primarily focuses on the dataset and does not provide new algorithms; challenges in summarization remain.", "keywords": ["Cyber Threat Intelligence", "CTI Summarization", "Dataset", "Machine Learning", "Natural Language Processing"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2408.11189", "pdf": "https://arxiv.org/pdf/2408.11189.pdf", "abs": "https://arxiv.org/abs/2408.11189", "title": "Emotional RAG LLMs: Reading Comprehension for the Open Internet", "authors": ["Benjamin Reichman", "Adar Avsian", "Kartik Talamadupula", "Toshish Jawale", "Larry Heck"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "Queries to large language models (LLMs) can be divided into two parts: the\ninstruction/question and the accompanying context. The context for\nretrieval-augmented generation (RAG) systems in most benchmarks comes from\nWikipedia-like texts written in a neutral and factual tone. However, real-world\nRAG applications often retrieve internet-based text with diverse tones and\nlinguistic styles, posing challenges for downstream tasks. This paper\nintroduces (a) a dataset that transforms RAG-retrieved passages into\nemotionally inflected and sarcastic text, (b) an emotion translation model for\nadapting text to different tones, and (c) a prompt-based method to improve\nLLMs' pragmatic interpretation of retrieved text.", "AI": {"tldr": "This paper presents a dataset to enhance the emotional and stylistic diversity of RAG-retrieved text, along with an emotion translation model and a prompt-based method for improving LLMs' interpretation of such text.", "motivation": "The need to adapt LLMs and RAG systems to handle diverse tones and linguistic styles found in real-world texts rather than neutral, Wikipedia-like content.", "method": "Introduction of a dataset with emotionally inflected and sarcastic text, development of an emotion translation model, and a prompt-based approach to improve LLMs' understanding of context.", "result": "The proposed methods demonstrate stronger performance in interpreting varied tones and styles in LLM applications.", "conclusion": "Enhancing LLMs with emotional diversity in retrieval contexts can improve their pragmatic understanding and applicability in real-world scenarios.", "key_contributions": ["Dataset for emotionally varied RAG retrievals", "Emotion translation model for tone adaptation", "Prompt-based techniques for LLM interpretation"], "limitations": "The dataset’s effectiveness in broader contexts and potential biases in emotional translation models need further exploration.", "keywords": ["retrieval-augmented generation", "emotion translation", "large language models", "pragmatic interpretation", "text diversity"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2409.01524", "pdf": "https://arxiv.org/pdf/2409.01524.pdf", "abs": "https://arxiv.org/abs/2409.01524", "title": "S^3cMath: Spontaneous Step-level Self-correction Makes Large Language Models Better Mathematical Reasoners", "authors": ["Yuchen Yan", "Jin Jiang", "Yang Liu", "Yixin Cao", "Xin Xu", "Mengdi Zhang", "Xunliang Cai", "Jian Shao"], "categories": ["cs.CL", "cs.AI"], "comment": "AAAI 2025: https://ojs.aaai.org/index.php/AAAI/article/view/34749", "summary": "Self-correction is a novel method that can stimulate the potential reasoning\nabilities of large language models (LLMs). It involves detecting and correcting\nerrors during the inference process when LLMs solve reasoning problems.\nHowever, recent works do not regard self-correction as a spontaneous and\nintrinsic capability of LLMs. Instead, such correction is achieved through\npost-hoc generation, external knowledge introduction, multi-model\ncollaboration, and similar techniques. In this paper, we propose a series of\nmathematical LLMs called S$^3$c-Math, which are able to perform Spontaneous\nStep-level Self-correction for Mathematical reasoning. This capability helps\nLLMs to recognize whether their ongoing inference tends to contain errors and\nsimultaneously correct these errors to produce a more reliable response. We\nproposed a method, which employs a step-level sampling approach to construct\nstep-wise self-correction data for achieving such ability. Additionally, we\nimplement a training strategy that uses above constructed data to equip LLMs\nwith spontaneous step-level self-correction capacities. Our data and methods\nhave been demonstrated to be effective across various foundation LLMs,\nconsistently showing significant progress in evaluations on GSM8K, MATH, and\nother mathematical benchmarks. To the best of our knowledge, we are the first\nto introduce the spontaneous step-level self-correction ability of LLMs in\nmathematical reasoning.", "AI": {"tldr": "Introducing S$^3$c-Math, a method for spontaneous step-level self-correction in large language models during mathematical reasoning.", "motivation": "To improve the reasoning accuracy of large language models by enabling them to self-correct during inference.", "method": "A step-level sampling approach is used to create self-correction data, and a training strategy is employed to develop spontaneous self-correction capabilities in LLMs.", "result": "The S$^3$c-Math models are shown to significantly improve performance on benchmarks such as GSM8K and MATH by correctly identifying and rectifying errors in real-time.", "conclusion": "The introduction of spontaneous step-level self-correction in LLMs represents a significant advancement in mathematical reasoning capabilities.", "key_contributions": ["Development of spontaneous step-level self-correction method for LLMs", "Creation of self-correction data through step-level sampling", "Demonstration of effectiveness across various mathematical benchmarks"], "limitations": "", "keywords": ["large language models", "self-correction", "mathematical reasoning", "machine learning", "NLU"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2409.15380", "pdf": "https://arxiv.org/pdf/2409.15380.pdf", "abs": "https://arxiv.org/abs/2409.15380", "title": "Kalahi: A handcrafted, grassroots cultural LLM evaluation suite for Filipino", "authors": ["Jann Railey Montalan", "Jian Gang Ngui", "Wei Qi Leong", "Yosephine Susanto", "Hamsawardhini Rengarajan", "Alham Fikri Aji", "William Chandra Tjhi"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted for presentation at Paclic 38, 2024", "summary": "Multilingual large language models (LLMs) today may not necessarily provide\nculturally appropriate and relevant responses to its Filipino users. We\nintroduce Kalahi, a cultural LLM evaluation suite collaboratively created by\nnative Filipino speakers. It is composed of 150 high-quality, handcrafted and\nnuanced prompts that test LLMs for generations that are relevant to shared\nFilipino cultural knowledge and values. Strong LLM performance in Kalahi\nindicates a model's ability to generate responses similar to what an average\nFilipino would say or do in a given situation. We conducted experiments on LLMs\nwith multilingual and Filipino language support. Results show that Kalahi,\nwhile trivial for Filipinos, is challenging for LLMs, with the best model\nanswering only 46.0% of the questions correctly compared to native Filipino\nperformance of 89.10%. Thus, Kalahi can be used to accurately and reliably\nevaluate Filipino cultural representation in LLMs.", "AI": {"tldr": "Kalahi is an evaluation suite for LLMs that focuses on culturally relevant responses for Filipino users, highlighting a gap in current LLM performance.", "motivation": "To assess how well multilingual LLMs generate culturally appropriate responses for Filipino users, given current inadequacies.", "method": "Kalahi consists of 150 nuanced prompts designed by native Filipino speakers to evaluate LLMs' capability in generating culturally relevant responses.", "result": "The best LLM answered only 46.0% of the prompt questions correctly, compared to 89.10% for native Filipinos, indicating a significant disparity in cultural understanding.", "conclusion": "Kalahi provides a reliable measure for evaluating the cultural representation of LLMs in relation to Filipino culture.", "key_contributions": ["Introduction of a culturally-focused LLM evaluation suite (Kalahi)", "Creation of 150 nuanced prompts based on Filipino culture", "Empirical results showcasing LLM performance gaps in cultural relevance"], "limitations": "", "keywords": ["multilingual LLMs", "Filipino culture", "cultural evaluation"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2410.03145", "pdf": "https://arxiv.org/pdf/2410.03145.pdf", "abs": "https://arxiv.org/abs/2410.03145", "title": "Margin Matching Preference Optimization: Enhanced Model Alignment with Granular Feedback", "authors": ["Kyuyoung Kim", "Ah Jeong Seo", "Hao Liu", "Jinwoo Shin", "Kimin Lee"], "categories": ["cs.CL"], "comment": "EMNLP 2024 Findings", "summary": "Large language models (LLMs) fine-tuned with alignment techniques, such as\nreinforcement learning from human feedback, have been instrumental in\ndeveloping some of the most capable AI systems to date. Despite their success,\nexisting methods typically rely on simple binary labels, such as those\nindicating preferred outputs in pairwise preferences, which fail to capture the\nsubtle differences in relative quality between pairs. To address this\nlimitation, we introduce an approach called Margin Matching Preference\nOptimization (MMPO), which incorporates relative quality margins into\noptimization, leading to improved LLM policies and reward models. Specifically,\ngiven quality margins in pairwise preferences, we design soft target\nprobabilities based on the Bradley-Terry model, which are then used to train\nmodels with the standard cross-entropy objective. Experiments with both human\nand AI feedback data demonstrate that MMPO consistently outperforms baseline\nmethods, often by a substantial margin, on popular benchmarks including\nMT-bench and RewardBench. Notably, the 7B model trained with MMPO achieves\nstate-of-the-art performance on RewardBench as of June 2024, outperforming\nother models of the same scale. Our analysis also shows that MMPO is more\nrobust to overfitting, leading to better-calibrated models.", "AI": {"tldr": "Introducing Margin Matching Preference Optimization (MMPO) for improving LLMs by incorporating relative quality margins in pairwise preferences for training.", "motivation": "Existing methods in LLM alignment rely on simplistic binary labels, failing to capture nuanced differences in output quality.", "method": "MMPO incorporates relative quality margins into optimization by designing soft target probabilities based on the Bradley-Terry model, using standard cross-entropy for training.", "result": "MMPO outperforms baseline methods on benchmarks like MT-bench and RewardBench; the 7B model trained with MMPO achieves state-of-the-art performance on RewardBench as of June 2024.", "conclusion": "MMPO leads to better-calibrated, robust models and improved policy performance in LLMs.", "key_contributions": ["Introduction of Margin Matching Preference Optimization (MMPO) approach", "Incorporation of relative quality margins into LLM training", "Demonstrated superior performance on multiple benchmarks"], "limitations": "", "keywords": ["Large Language Models", "Alignment Techniques", "Machine Learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2410.06735", "pdf": "https://arxiv.org/pdf/2410.06735.pdf", "abs": "https://arxiv.org/abs/2410.06735", "title": "Which Programming Language and What Features at Pre-training Stage Affect Downstream Logical Inference Performance?", "authors": ["Fumiya Uchiyama", "Takeshi Kojima", "Andrew Gambardella", "Qi Cao", "Yusuke Iwasawa", "Yutaka Matsuo"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to EMNLP2024", "summary": "Recent large language models (LLMs) have demonstrated remarkable\ngeneralization abilities in mathematics and logical reasoning tasks. Prior\nresearch indicates that LLMs pre-trained with programming language data exhibit\nhigh mathematical and reasoning abilities; however, this causal relationship\nhas not been rigorously tested. Our research aims to verify which programming\nlanguages and features during pre-training affect logical inference\nperformance. Specifically, we pre-trained decoder-based language models from\nscratch using datasets from ten programming languages (e.g., Python, C, Java)\nand three natural language datasets (Wikipedia, Fineweb, C4) under identical\nconditions. Thereafter, we evaluated the trained models in a few-shot\nin-context learning setting on logical reasoning tasks: FLD and bAbi, which do\nnot require commonsense or world knowledge. The results demonstrate that nearly\nall models trained with programming languages consistently outperform those\ntrained with natural languages, indicating that programming languages contain\nfactors that elicit logic inference performance. In addition, we found that\nmodels trained with programming languages exhibit a better ability to follow\ninstructions compared to those trained with natural languages. Further analysis\nreveals that the depth of Abstract Syntax Trees representing parsed results of\nprograms also affects logical reasoning performance. These findings will offer\ninsights into the essential elements of pre-training for acquiring the\nfoundational abilities of LLMs.", "AI": {"tldr": "This paper investigates the impact of programming language pre-training on the logical inference performance of large language models (LLMs).", "motivation": "To rigorously test the causal relationship between programming language features in pre-training and logical reasoning performance in LLMs.", "method": "The authors pre-trained decoder-based language models from scratch using datasets from ten programming languages and evaluated them on logical reasoning tasks in a few-shot context.", "result": "Models trained with programming languages consistently outperformed those trained with natural languages in logical reasoning tasks, and showed better instruction-following abilities.", "conclusion": "The results suggest that programming languages offer unique factors that enhance logical inference, with the structure of Abstract Syntax Trees also influencing performance.", "key_contributions": ["Demonstrated that programming languages significantly improve LLMs' logical reasoning capabilities.", "Identified specific programming features that enhance instruction-following abilities.", "Provided insights into pre-training elements vital for foundational LLM capabilities."], "limitations": "", "keywords": ["large language models", "programming languages", "logical inference", "pre-training", "machine learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2410.08174", "pdf": "https://arxiv.org/pdf/2410.08174.pdf", "abs": "https://arxiv.org/abs/2410.08174", "title": "Sample then Identify: A General Framework for Risk Control and Assessment in Multimodal Large Language Models", "authors": ["Qingni Wang", "Tiantian Geng", "Zhiyuan Wang", "Teng Wang", "Bo Fu", "Feng Zheng"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MM"], "comment": "Accepted by ICLR 2025 Spotlights", "summary": "Multimodal Large Language Models (MLLMs) exhibit promising advancements\nacross various tasks, yet they still encounter significant trustworthiness\nissues. Prior studies apply Split Conformal Prediction (SCP) in language\nmodeling to construct prediction sets with statistical guarantees. However,\nthese methods typically rely on internal model logits or are restricted to\nmultiple-choice settings, which hampers their generalizability and adaptability\nin dynamic, open-ended environments. In this paper, we introduce TRON, a\ntwo-step framework for risk control and assessment, applicable to any MLLM that\nsupports sampling in both open-ended and closed-ended scenarios. TRON comprises\ntwo main components: (1) a novel conformal score to sample response sets of\nminimum size, and (2) a nonconformity score to identify high-quality responses\nbased on self-consistency theory, controlling the error rates by two specific\nrisk levels. Furthermore, we investigate semantic redundancy in prediction sets\nwithin open-ended contexts for the first time, leading to a promising\nevaluation metric for MLLMs based on average set size. Our comprehensive\nexperiments across four Video Question-Answering (VideoQA) datasets utilizing\neight MLLMs show that TRON achieves desired error rates bounded by two\nuser-specified risk levels. Additionally, deduplicated prediction sets maintain\nadaptiveness while being more efficient and stable for risk assessment under\ndifferent risk levels.", "AI": {"tldr": "TRON is a two-step framework for improving trustworthiness in Multimodal Large Language Models (MLLMs) by applying risk control and assessment strategies to enhance their adaptability in various environments.", "motivation": "To address significant trustworthiness issues in current MLLMs, particularly in open-ended tasks, and to improve generalizability and adaptability in risk assessment.", "method": "TRON consists of two components: a novel conformal score for minimum size response sampling and a nonconformity score for assessing high-quality responses, both aimed at managing error rates across specified risk levels.", "result": "TRON yielded effective error rate control in predictions across four VideoQA datasets using eight MLLMs, demonstrating adaptability and stability while maintaining efficiency.", "conclusion": "The framework provides a significant enhancement in the assessment and control of MLLM predictions, introducing new evaluation metrics that are applicable in open-ended contexts.", "key_contributions": ["Introduction of TRON framework for MLLM risk management", "Novel conformal and nonconformity scores for response assessment", "New evaluation metric based on semantic redundancy in prediction sets"], "limitations": "", "keywords": ["Multi-modal Large Language Models", "Trustworthiness", "Risk Assessment", "Conformal Prediction", "Semantic Redundancy"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2410.10360", "pdf": "https://arxiv.org/pdf/2410.10360.pdf", "abs": "https://arxiv.org/abs/2410.10360", "title": "Parenting: Optimizing Knowledge Selection of Retrieval-Augmented Language Models with Parameter Decoupling and Tailored Tuning", "authors": ["Yongxin Xu", "Ruizhe Zhang", "Xinke Jiang", "Yujie Feng", "Yuzhen Xiao", "Xinyu Ma", "Runchuan Zhu", "Xu Chu", "Junfeng Zhao", "Yasha Wang"], "categories": ["cs.CL", "cs.IR"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Retrieval-Augmented Generation (RAG) offers an effective solution to the\nissues faced by Large Language Models (LLMs) in hallucination generation and\nknowledge obsolescence by incorporating externally retrieved knowledge.\nHowever, existing methods lack effective control mechanisms for integrating\ninternal and external knowledge. Inspired by human cognitive processes, we\npropose Parenting, a novel framework that decouples, identifies, and\npurposefully optimizes parameter subspaces related to adherence and robustness.\nSpecifically, Parenting utilizes a key parameter mining method that combines\nforward and backward propagation signals to localize subspaces representing\ndifferent capabilities. Then, Parenting employs a type-tailored tuning\nstrategy, applying specific and appropriate optimizations to different\nsubspaces, aiming to achieve a balanced enhancement of both adherence and\nrobustness. Extensive experiments on various datasets and models validate the\neffectiveness and generalizability of our method.", "AI": {"tldr": "Parenting is a novel framework for integrating internal and external knowledge in Retrieval-Augmented Generation (RAG), enhancing adherence and robustness in Large Language Models (LLMs) without hallucination.", "motivation": "Existing methods lack effective control mechanisms for integrating internal and external knowledge, leading to issues in LLMs like hallucination and knowledge obsolescence.", "method": "Parenting decouples, identifies, and optimizes parameter subspaces related to adherence and robustness using a key parameter mining method that combines forward and backward propagation signals.", "result": "Extensive experiments ensure the effectiveness and generalizability of Parenting across various datasets and models.", "conclusion": "The Parenting framework offers a structured approach to optimize LLMs' adherence and robustness by leveraging both internal and external knowledge.", "key_contributions": ["Introduction of the Parenting framework for optimizing LLMs", "Decoupling parameter subspaces for targeted tuning", "Key parameter mining method that enhances model robustness"], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Large Language Models", "parameter optimization"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2410.17711", "pdf": "https://arxiv.org/pdf/2410.17711.pdf", "abs": "https://arxiv.org/abs/2410.17711", "title": "Beware of Calibration Data for Pruning Large Language Models", "authors": ["Yixin Ji", "Yang Xiang", "Juntao Li", "Qingrong Xia", "Ping Li", "Xinyu Duan", "Zhefeng Wang", "Min Zhang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Published as a conference paper at ICLR 2025", "summary": "As large language models (LLMs) are widely applied across various fields,\nmodel compression has become increasingly crucial for reducing costs and\nimproving inference efficiency. Post-training pruning is a promising method\nthat does not require resource-intensive iterative training and only needs a\nsmall amount of calibration data to assess the importance of parameters. Recent\nresearch has enhanced post-training pruning from different aspects but few of\nthem systematically explore the effects of calibration data, and it is unclear\nif there exist better calibration data construction strategies. We fill this\nblank and surprisingly observe that calibration data is also crucial to\npost-training pruning, especially for high sparsity. Through controlled\nexperiments on important influence factors of calibration data, including the\npruning settings, the amount of data, and its similarity with pre-training\ndata, we observe that a small size of data is adequate, and more similar data\nto its pre-training stage can yield better performance. As pre-training data is\nusually inaccessible for advanced LLMs, we further provide a self-generating\ncalibration data synthesis strategy to construct feasible calibration data.\nExperimental results on recent strong open-source LLMs (e.g., DCLM, and\nLLaMA-3) show that the proposed strategy can enhance the performance of strong\npruning methods (e.g., Wanda, DSnoT, OWL) by a large margin (up to $2.68\\%$).\nCode is available at https://github.com/Dereck0602/calibration_data.", "AI": {"tldr": "This paper investigates the impact of calibration data on post-training pruning in large language models, providing a synthesis strategy for constructing effective calibration data.", "motivation": "Model compression is essential for large language models to reduce costs and improve inference efficiency, yet the effects of calibration data on post-training pruning remain underexplored.", "method": "The study conducts controlled experiments to analyze the influence factors of calibration data, such as pruning settings, data size, and similarity to pre-training data, while also proposing a self-generating strategy for calibration data synthesis.", "result": "The findings reveal that a small amount of calibration data, especially similar to pre-training data, significantly improves pruning performance, with experimental results showing up to a 2.68% enhancement in model accuracy.", "conclusion": "The proposed calibration data synthesis strategy offers a promising solution to enhance post-training pruning methods in large language models.", "key_contributions": ["Systematic exploration of calibration data effects on pruning performance.", "Introduction of a self-generating strategy for calibration data synthesis.", "Empirical evidence showing the importance of data similarity for improved model performance."], "limitations": "", "keywords": ["large language models", "post-training pruning", "calibration data", "model compression", "deep learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2411.06660", "pdf": "https://arxiv.org/pdf/2411.06660.pdf", "abs": "https://arxiv.org/abs/2411.06660", "title": "Bridge: A Unified Framework to Knowledge Graph Completion via Language Models and Knowledge Representation", "authors": ["Qiao Qiao", "Yuepei Li", "Qing Wang", "Kang Zhou", "Qi Li"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Knowledge graph completion (KGC) is a task of inferring missing triples based\non existing Knowledge Graphs (KGs). Both structural and semantic information\nare vital for successful KGC. However, existing methods only use either the\nstructural knowledge from the KG embeddings or the semantic information from\npre-trained language models (PLMs), leading to suboptimal model performance.\nMoreover, since PLMs are not trained on KGs, directly using PLMs to encode\ntriples may be inappropriate. To overcome these limitations, we propose a novel\nframework called Bridge, which jointly encodes structural and semantic\ninformation of KGs. Specifically, we strategically encode entities and\nrelations separately by PLMs to better utilize the semantic knowledge of PLMs\nand enable structured representation learning via a structural learning\nprinciple. Furthermore, to bridge the gap between KGs and PLMs, we employ a\nself-supervised representation learning method called BYOL to fine-tune PLMs\nwith two different views of a triple. Unlike BYOL, which uses augmentation\nmethods to create two semantically similar views of the same image, potentially\naltering the semantic information. We strategically separate the triple into\ntwo parts to create different views, thus avoiding semantic alteration.\nExperiments demonstrate that Bridge outperforms the SOTA models on three\nbenchmark datasets.", "AI": {"tldr": "Bridge is a novel framework that enhances knowledge graph completion by jointly encoding structural and semantic information from knowledge graphs and pre-trained language models, outperforming state-of-the-art models.", "motivation": "Existing methods for knowledge graph completion either focus on structural information or semantic information, leading to suboptimal performance.", "method": "The Bridge framework encodes entities and relations separately using pre-trained language models, employs a self-supervised learning method called BYOL to fine-tune PLMs with distinct views of triples, and avoids semantic alteration.", "result": "Bridge significantly outperforms state-of-the-art models on three benchmark datasets.", "conclusion": "The proposed framework effectively integrates both structural and semantic knowledge for superior knowledge graph completion.", "key_contributions": ["Introduction of the Bridge framework for KGC", "Separate encoding of entities and relations for better PLM utilization", "Use of BYOL in a novel way to enhance KGC without semantic alteration"], "limitations": "", "keywords": ["Knowledge Graph Completion", "Pre-trained Language Models", "Self-supervised Learning"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2411.08870", "pdf": "https://arxiv.org/pdf/2411.08870.pdf", "abs": "https://arxiv.org/abs/2411.08870", "title": "The Limited Impact of Medical Adaptation of Large Language and Vision-Language Models", "authors": ["Daniel P. Jeong", "Pranav Mani", "Saurabh Garg", "Zachary C. Lipton", "Michael Oberst"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Extended version of EMNLP 2024 paper arXiv:2411.04118. Includes\n  additional results on clinical note QA tasks and supervised fine-tuning\n  evaluations", "summary": "Several recent works seek to adapt general-purpose large language models\n(LLMs) and vision-language models (VLMs) for medical applications through\ncontinued pretraining on publicly available biomedical corpora. These works\ntypically claim that such domain-adaptive pretraining improves performance on\nvarious downstream medical tasks, such as answering medical exam questions. In\nthis paper, we compare ten \"medical\" LLMs and two VLMs against their\ncorresponding base models, arriving at a different conclusion: all medical VLMs\nand nearly all medical LLMs fail to consistently improve over their base models\nin the zero-/few-shot prompting and supervised fine-tuning regimes for medical\nquestion answering (QA). For instance, on clinical-note-based QA tasks in the\n3-shot setting, medical LLMs outperform their base models in only 26.7% of\ncases, reach a (statistical) tie in 16.7% of cases, and perform significantly\nworse in the remaining 56.7% of cases. Our conclusions are based on (i)\ncomparing each medical model directly against its base model; (ii) optimizing\nthe prompts for each model separately in zero-/few-shot prompting; and (iii)\naccounting for statistical uncertainty in comparisons. Our findings suggest\nthat state-of-the-art general-domain models may already exhibit strong medical\nknowledge and reasoning capabilities, and offer recommendations to strengthen\nthe conclusions of future studies.", "AI": {"tldr": "This paper evaluates the effectiveness of medical large language models (LLMs) and vision-language models (VLMs) by comparing them to their base models in medical question answering tasks, finding that many do not outperform general models.", "motivation": "To assess the performance of domain-adapted medical LLMs and VLMs in medical question answering and determine if they truly provide better results compared to their base models.", "method": "Comparison of ten medical LLMs and two VLMs against their respective base models in zero-/few-shot prompting and supervised fine-tuning, including optimization of prompts and accounting for statistical uncertainty.", "result": "Medical LLMs outperformed their base models only in 26.7% of cases and performed worse in 56.7% of cases during the evaluation of clinical-note-based QA tasks.", "conclusion": "General-domain models may possess sufficient medical knowledge and reasoning capabilities, suggesting that medical domain adaptation may not be necessary and recommending improvements for future studies.", "key_contributions": ["Comparison of medical models directly against their base models", "Optimized prompts for individual models in zero-/few-shot settings", "Accounted for statistical uncertainty in performance comparisons"], "limitations": "", "keywords": ["medical LLMs", "VLMs", "medical question answering", "domain adaptation", "statistical uncertainty"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2411.10557", "pdf": "https://arxiv.org/pdf/2411.10557.pdf", "abs": "https://arxiv.org/abs/2411.10557", "title": "MLAN: Language-Based Instruction Tuning Preserves and Transfers Knowledge in Multimodal Language Models", "authors": ["Jianhong Tu", "Zhuohao Ni", "Nicholas Crispino", "Zihao Yu", "Michael Bendersky", "Beliz Gunel", "Ruoxi Jia", "Xin Liu", "Lingjuan Lyu", "Dawn Song", "Chenguang Wang"], "categories": ["cs.CL"], "comment": null, "summary": "We present a novel visual instruction tuning strategy to improve the\nzero-shot task generalization of multimodal large language models by building a\nfirm text-only knowledge base. Existing work lacks sufficient experimentation\non the importance of each modality in the instruction tuning stage, often using\na majority of vision-language data while keeping text-only data limited and\nfixing mixtures of modalities. By incorporating diverse text-only data in the\nvisual instruction tuning stage, we vary vision-language data in various\ncontrolled experiments to investigate the importance of modality in visual\ninstruction tuning. Our comprehensive evaluation shows that the text-heavy\ninstruction tuning approach is able to perform on-par with traditional\nvision-heavy mixtures on both modalities across 12 general datasets while using\nas low as half the total training tokens. We find that simply increasing\nsufficiently diverse text-only data enables transfer of instruction following\nability and domain knowledge across modalities while being more efficient than\nthe vision-language approach.", "AI": {"tldr": "This paper proposes a new visual instruction tuning strategy that emphasizes the use of text-only data to enhance the performance of multimodal large language models in zero-shot generalization tasks.", "motivation": "To address the imbalance in the types of data used for instruction tuning in multimodal large language models, this paper investigates the impact of text-only data on task generalization.", "method": "The authors conduct controlled experiments by varying the mixture of vision-language and text-only data during the instruction tuning stage, focusing on the role of modality in performance.", "result": "The text-heavy instruction tuning strategy achieves comparable performance to traditional vision-heavy methods across multiple datasets while using fewer training tokens, indicating higher efficiency.", "conclusion": "Increasing the diversity of text-only data during instruction tuning facilitates better transfer of instruction following and domain knowledge between modalities, making the tuning process more effective.", "key_contributions": ["Introduces a novel text-heavy visual instruction tuning strategy", "Demonstrates the effectiveness of diverse text-only data for multimodal tasks", "Shows that using less training data can yield competitive results compared to traditional methods"], "limitations": "The study may not cover all potential modalities or dataset combinations, limiting generalizability to some specific tasks.", "keywords": ["visual instruction tuning", "multimodal models", "text-only data", "zero-shot generalization", "efficiency"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2412.01131", "pdf": "https://arxiv.org/pdf/2412.01131.pdf", "abs": "https://arxiv.org/abs/2412.01131", "title": "A Comprehensive Evaluation of Semantic Relation Knowledge of Pretrained Language Models and Humans", "authors": ["Zhihan Cao", "Hiroaki Yamada", "Simone Teufel", "Takenobu Tokunaga"], "categories": ["cs.CL"], "comment": "This manuscript is currently under review at Language Resources and\n  Evaluation", "summary": "Recently, much work has concerned itself with the enigma of what exactly PLMs\n(pretrained language models) learn about different aspects of language, and how\nthey learn it. One stream of this type of research investigates the knowledge\nthat PLMs have about semantic relations. However, many aspects of semantic\nrelations were left unexplored. Only one relation was considered, namely\nhypernymy. Furthermore, previous work did not measure humans' performance on\nthe same task as that solved by the PLMs. This means that at this point in\ntime, there is only an incomplete view of models' semantic relation knowledge.\nTo address this gap, we introduce a comprehensive evaluation framework covering\nfive relations beyond hypernymy, namely hyponymy, holonymy, meronymy, antonymy,\nand synonymy. We use six metrics (two newly introduced here) for recently\nuntreated aspects of semantic relation knowledge, namely soundness,\ncompleteness, symmetry, asymmetry, prototypicality, and distinguishability and\nfairly compare humans and models on the same task. Our extensive experiments\ninvolve 16 PLMs, eight masked and eight causal language models. Up to now only\nmasked language models had been tested although causal and masked language\nmodels treat context differently. Our results reveal a significant knowledge\ngap between humans and models for almost all semantic relations. Antonymy is\nthe outlier relation where all models perform reasonably well. In general,\nmasked language models perform significantly better than causal language\nmodels. Nonetheless, both masked and causal language models are likely to\nconfuse non-antonymy relations with antonymy.", "AI": {"tldr": "The paper evaluates the semantic relation knowledge of pretrained language models (PLMs) beyond hypernymy by using a new framework and metrics, comparing human performance to that of various PLMs across five relations.", "motivation": "To address the incomplete view of semantic relations knowledge in PLMs, particularly focusing on unexplored aspects of semantic relations beyond hypernymy.", "method": "A comprehensive evaluation framework covering hyponymy, holonymy, meronymy, antonymy, and synonymy was introduced, utilizing six metrics for assessment and comparing 16 PLMs, including masked and causal models, against human performance.", "result": "The results show a significant knowledge gap between humans and PLMs in understanding semantic relations, with models confusing non-antonymy relations with antonymy. Masked language models outperformed causal models overall.", "conclusion": "Despite some models' reasonable performance in antonymy, the findings highlight the general limitations of PLMs in grasping semantic relations when contrasted with human understanding.", "key_contributions": ["Introduction of a comprehensive evaluation framework for semantic relations", "Comparative analysis between human performance and PLM capabilities", "Identification of performance gaps particularly in non-antonymy relations"], "limitations": "The study is limited to semantic relations tested and may not generalize to other forms of knowledge.", "keywords": ["pretrained language models", "semantic relations", "evaluation framework", "human performance", "antonymy"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2412.04205", "pdf": "https://arxiv.org/pdf/2412.04205.pdf", "abs": "https://arxiv.org/abs/2412.04205", "title": "A Context-aware Framework for Translation-mediated Conversations", "authors": ["José Pombal", "Sweta Agrawal", "Patrick Fernandes", "Emmanouil Zaranis", "André F. T. Martins"], "categories": ["cs.CL"], "comment": null, "summary": "Automatic translation systems offer a powerful solution to bridge language\nbarriers in scenarios where participants do not share a common language.\nHowever, these systems can introduce errors leading to misunderstandings and\nconversation breakdown. A key issue is that current systems fail to incorporate\nthe rich contextual information necessary to resolve ambiguities and omitted\ndetails, resulting in literal, inappropriate, or misaligned translations. In\nthis work, we present a framework to improve large language model-based\ntranslation systems by incorporating contextual information in bilingual\nconversational settings during training and inference. We validate our proposed\nframework on two task-oriented domains: customer chat and user-assistant\ninteraction. Across both settings, the system produced by our\nframework-TowerChat-consistently results in better translations than\nstate-of-the-art systems like GPT-4o and TowerInstruct, as measured by multiple\nautomatic translation quality metrics on several language pairs. We also show\nthat the resulting model leverages context in an intended and interpretable\nway, improving consistency between the conveyed message and the generated\ntranslations.", "AI": {"tldr": "This paper presents a framework, TowerChat, that enhances large language model-based translation systems by integrating contextual information in bilingual conversations, demonstrating improved translation quality over state-of-the-art systems.", "motivation": "To address the limitations of current automatic translation systems that fail to incorporate contextual information, which leads to errors and misunderstandings.", "method": "The authors propose a framework, TowerChat, that includes contextual information during the training and inference stages in bilingual conversational settings.", "result": "Validation on customer chat and user-assistant interactions shows that TowerChat consistently yields better translation quality than existing systems like GPT-4o and TowerInstruct, measured by various automatic metrics.", "conclusion": "The proposed framework improves consistency between the message conveyed and the generated translations, leveraging context effectively.", "key_contributions": ["Development of the TowerChat framework", "Demonstrated improvement over state-of-the-art translation systems", "Showed effective context leveraging in translations"], "limitations": "", "keywords": ["automatic translation", "contextual information", "large language models", "bilingual conversation", "translation quality"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2412.10266", "pdf": "https://arxiv.org/pdf/2412.10266.pdf", "abs": "https://arxiv.org/abs/2412.10266", "title": "Reasoner Outperforms: Generative Stance Detection with Rationalization for Social Media", "authors": ["Jiaqing Yuan", "Ruijie Xi", "Munindar P. Singh"], "categories": ["cs.CL"], "comment": "Accepted by ACM Hypertext 2025", "summary": "Stance detection is crucial for fostering a human-centric Web by analyzing\nuser-generated content to identify biases and harmful narratives that undermine\ntrust. With the development of Large Language Models (LLMs), existing\napproaches treat stance detection as a classification problem, providing robust\nmethodologies for modeling complex group interactions and advancing\ncapabilities in natural language tasks. However, these methods often lack\ninterpretability, limiting their ability to offer transparent and\nunderstandable justifications for predictions. This study adopts a generative\napproach, where stance predictions include explicit, interpretable rationales,\nand integrates them into smaller language models through single-task and\nmultitask learning. We find that incorporating reasoning into stance detection\nenables the smaller model (FlanT5) to outperform GPT-3.5's zero-shot\nperformance, achieving an improvement of up to 9.57%. Moreover, our results\nshow that reasoning capabilities enhance multitask learning performance but may\nreduce effectiveness in single-task settings. Crucially, we demonstrate that\nfaithful rationales improve rationale distillation into SLMs, advancing efforts\nto build interpretable, trustworthy systems for addressing discrimination,\nfostering trust, and promoting equitable engagement on social media.", "AI": {"tldr": "This study presents a generative approach to stance detection that enhances interpretability through explicit rationales, integrating them into smaller language models to outperform existing state-of-the-art models.", "motivation": "The research aims to improve stance detection to foster a human-centric Web, addressing biases and harmful narratives in user-generated content and emphasizing the need for interpretability in AI models.", "method": "The study employs a generative approach to stance detection, integrating rationale-based predictions into smaller language models using both single-task and multitask learning methodologies.", "result": "Incorporating reasoning into stance detection allows the smaller model (FlanT5) to exceed GPT-3.5's zero-shot performance by up to 9.57%, while enhancing multitask learning performance but potentially hindering single-task efficacy.", "conclusion": "The integration of faithful rationales into stance detection systems can lead to more interpretable and trustworthy AI systems, which are essential for promoting equity and reducing discrimination on social media.", "key_contributions": ["Introduction of a generative approach to stance detection with interpretable rationales.", "Achievement of improved performance over GPT-3.5 by a smaller model (FlanT5).", "Advancement in prototype models that foster trust and reduce biases in social media applications."], "limitations": "The study indicates that while multitask learning benefits from reasoning capabilities, single-task performance may decline.", "keywords": ["stance detection", "generative models", "interpretability", "fairness in AI", "social media bias"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2412.12386", "pdf": "https://arxiv.org/pdf/2412.12386.pdf", "abs": "https://arxiv.org/abs/2412.12386", "title": "Interpretable LLM-based Table Question Answering", "authors": ["Giang Nguyen", "Ivan Brugere", "Shubham Sharma", "Sanjay Kariyappa", "Anh Totti Nguyen", "Freddy Lecue"], "categories": ["cs.CL", "cs.LG"], "comment": "Published in Transactions on Machine Learning Research (TMLR) in\n  06/2025. Reviews at: https://openreview.net/forum?id=2eTsZBoU2W", "summary": "Interpretability in Table Question Answering (Table QA) is critical,\nespecially in high-stakes domains like finance and healthcare. While recent\nTable QA approaches based on Large Language Models (LLMs) achieve high\naccuracy, they often produce ambiguous explanations of how answers are derived.\n  We propose Plan-of-SQLs (POS), a new Table QA method that makes the model's\ndecision-making process interpretable. POS decomposes a question into a\nsequence of atomic steps, each directly translated into an executable SQL\ncommand on the table, thereby ensuring that every intermediate result is\ntransparent. Through extensive experiments, we show that: First, POS generates\nthe highest-quality explanations among compared methods, which markedly\nimproves the users' ability to simulate and verify the model's decisions.\nSecond, when evaluated on standard Table QA benchmarks (TabFact, WikiTQ, and\nFeTaQA), POS achieves QA accuracy that is competitive to existing methods,\nwhile also offering greater efficiency-requiring significantly fewer LLM calls\nand table database queries (up to 25x fewer)-and more robust performance on\nlarge-sized tables. Finally, we observe high agreement (up to 90.59% in forward\nsimulation) between LLMs and human users when making decisions based on the\nsame explanations, suggesting that LLMs could serve as an effective proxy for\nhumans in evaluating Table QA explanations.", "AI": {"tldr": "This paper introduces Plan-of-SQLs (POS), an interpretable Table Question Answering method utilizing Large Language Models, which generates clear explanations and efficient queries.", "motivation": "The need for interpretability in Table QA is essential, particularly in high-stakes fields such as finance and healthcare, where understanding the reasoning behind answers is critical.", "method": "Plan-of-SQLs (POS) breaks down questions into atomic steps that are executable SQL commands, allowing for transparency in the decision-making process.", "result": "POS outperforms other methods in generating high-quality explanations and achieves competitive accuracy on Table QA benchmarks while being significantly more efficient.", "conclusion": "POS demonstrates that LLMs can provide interpretable and reliable explanations for Table QA, enhancing the user’s ability to understand and trust the answers provided by AI systems.", "key_contributions": ["Introduces a novel method for interpretable Table QA using atomic SQL commands", "Demonstrates significant efficiency improvements (up to 25x fewer LLM calls)", "Shows strong agreement in decision-making between LLMs and human users."], "limitations": "", "keywords": ["Table Question Answering", "Large Language Models", "Interpretability", "SQL", "Healthcare"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2501.01644", "pdf": "https://arxiv.org/pdf/2501.01644.pdf", "abs": "https://arxiv.org/abs/2501.01644", "title": "Multimodal Contrastive Representation Learning in Augmented Biomedical Knowledge Graphs", "authors": ["Tien Dang", "Viet Thanh Duy Nguyen", "Minh Tuan Le", "Truong-Son Hy"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Biomedical Knowledge Graphs (BKGs) integrate diverse datasets to elucidate\ncomplex relationships within the biomedical field. Effective link prediction on\nthese graphs can uncover valuable connections, such as potential novel\ndrug-disease relations. We introduce a novel multimodal approach that unifies\nembeddings from specialized Language Models (LMs) with Graph Contrastive\nLearning (GCL) to enhance intra-entity relationships while employing a\nKnowledge Graph Embedding (KGE) model to capture inter-entity relationships for\neffective link prediction. To address limitations in existing BKGs, we present\nPrimeKG++, an enriched knowledge graph incorporating multimodal data, including\nbiological sequences and textual descriptions for each entity type. By\ncombining semantic and relational information in a unified representation, our\napproach demonstrates strong generalizability, enabling accurate link\npredictions even for unseen nodes. Experimental results on PrimeKG++ and the\nDrugBank drug-target interaction dataset demonstrate the effectiveness and\nrobustness of our method across diverse biomedical datasets. Our source code,\npre-trained models, and data are publicly available at\nhttps://github.com/HySonLab/BioMedKG", "AI": {"tldr": "This paper presents PrimeKG++, a novel multimodal approach for enhancing link prediction in Biomedical Knowledge Graphs (BKGs) by unifying embeddings from language models with Graph Contrastive Learning.", "motivation": "The need for effective link prediction in Biomedical Knowledge Graphs to uncover valuable relationships such as novel drug-disease relations.", "method": "A multimodal approach that combines Language Models embeddings with Graph Contrastive Learning, along with a Knowledge Graph Embedding model for effective intra- and inter-entity relationship capturing.", "result": "The proposed method shows strong generalizability and accuracy in link predictions, even for unseen nodes, validated through experiments on PrimeKG++ and the DrugBank dataset.", "conclusion": "The integration of multimodal data significantly enhances link prediction performance and robustness in biomedical applications.", "key_contributions": ["Introduction of PrimeKG++, an enriched knowledge graph incorporating multimodal data.", "Use of a novel multimodal approach that combines LMs with Graph Contrastive Learning for link prediction.", "Public availability of source code, pre-trained models, and enriched dataset for further research."], "limitations": "", "keywords": ["Biomedical Knowledge Graphs", "Link Prediction", "Graph Contrastive Learning", "Knowledge Graph Embedding", "Multimodal Data"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.04397", "pdf": "https://arxiv.org/pdf/2502.04397.pdf", "abs": "https://arxiv.org/abs/2502.04397", "title": "Multimodal Medical Code Tokenizer", "authors": ["Xiaorui Su", "Shvat Messica", "Yepeng Huang", "Ruth Johnson", "Lukas Fesser", "Shanghua Gao", "Faryad Sahneh", "Marinka Zitnik"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ICML'25", "summary": "Foundation models trained on patient electronic health records (EHRs) require\ntokenizing medical data into sequences of discrete vocabulary items. Existing\ntokenizers treat medical codes from EHRs as isolated textual tokens. However,\neach medical code is defined by its textual description, its position in\nontological hierarchies, and its relationships to other codes, such as disease\nco-occurrences and drug-treatment associations. Medical vocabularies contain\nmore than 600,000 codes with critical information for clinical reasoning. We\nintroduce MedTok, a multimodal medical code tokenizer that uses the text\ndescriptions and relational context of codes. MedTok processes text using a\nlanguage model encoder and encodes the relational structure with a graph\nencoder. It then quantizes both modalities into a unified token space,\npreserving modality-specific and cross-modality information. We integrate\nMedTok into five EHR models and evaluate it on operational and clinical tasks\nacross in-patient and out-patient datasets, including outcome prediction,\ndiagnosis classification, drug recommendation, and risk stratification.\nSwapping standard EHR tokenizers with MedTok improves AUPRC across all EHR\nmodels, by 4.10% on MIMIC-III, 4.78% on MIMIC-IV, and 11.32% on EHRShot, with\nthe largest gains in drug recommendation. Beyond EHR modeling, we demonstrate\nusing MedTok tokenizer with medical QA systems. Our results demonstrate the\npotential of MedTok as a unified tokenizer for medical codes, improving\ntokenization for medical foundation models.", "AI": {"tldr": "MedTok is a multimodal medical code tokenizer that enhances tokenization of EHR data by incorporating text descriptions and relational context, leading to improved performance in clinical tasks.", "motivation": "Foundation models trained on EHRs need effective tokenization of medical data, which traditional tokenizers inadequately address by treating medical codes as isolated tokens.", "method": "MedTok processes text with a language model encoder for descriptions and utilizes a graph encoder for relational structures, quantizing both into a unified token space.", "result": "MedTok significantly improves AUPRC across various EHR models: 4.10% on MIMIC-III, 4.78% on MIMIC-IV, and 11.32% on EHRShot, especially enhancing drug recommendation tasks.", "conclusion": "MedTok serves as a unified tokenizer for medical codes that improves tokenization processes for medical foundation models, demonstrating broad utility in various operational and clinical applications.", "key_contributions": ["Introduction of MedTok for multimodal medical code tokenization", "Improved model performance on EHR datasets", "Demonstrable application in medical QA systems"], "limitations": "", "keywords": ["medical code tokenizer", "EHR models", "machine learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.05489", "pdf": "https://arxiv.org/pdf/2502.05489.pdf", "abs": "https://arxiv.org/abs/2502.05489", "title": "Mechanistic Interpretability of Emotion Inference in Large Language Models", "authors": ["Ala N. Tak", "Amin Banayeeanzade", "Anahita Bolourani", "Mina Kian", "Robin Jia", "Jonathan Gratch"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 camera-ready version. First two authors contributed equally", "summary": "Large language models (LLMs) show promising capabilities in predicting human\nemotions from text. However, the mechanisms through which these models process\nemotional stimuli remain largely unexplored. Our study addresses this gap by\ninvestigating how autoregressive LLMs infer emotions, showing that emotion\nrepresentations are functionally localized to specific regions in the model.\nOur evaluation includes diverse model families and sizes and is supported by\nrobustness checks. We then show that the identified representations are\npsychologically plausible by drawing on cognitive appraisal theory, a\nwell-established psychological framework positing that emotions emerge from\nevaluations (appraisals) of environmental stimuli. By causally intervening on\nconstrued appraisal concepts, we steer the generation and show that the outputs\nalign with theoretical and intuitive expectations. This work highlights a novel\nway to causally intervene and precisely shape emotional text generation,\npotentially benefiting safety and alignment in sensitive affective domains.", "AI": {"tldr": "This study explores how autoregressive large language models (LLMs) predict emotions from text and identifies localized emotion representations, providing a psychologically plausible framework for emotional text generation.", "motivation": "To investigate the mechanisms through which autoregressive LLMs process and infer emotions from text, an area that has not been thoroughly examined.", "method": "The study employs diverse model families and sizes and includes robustness checks to evaluate how LLMs represent emotions functionally localized in specific regions of the model.", "result": "Identified emotion representations align with cognitive appraisal theory and demonstrate causally intervening on appraisal concepts can influence emotional text generation outcomes.", "conclusion": "This research shows a novel method for precise emotional text generation with LLMs, which may enhance safety and alignment in sensitive emotional contexts.", "key_contributions": ["Identification of localized emotion representations in LLMs.", "Application of cognitive appraisal theory to emotion generation in LLMs.", "Demonstration of causal interventions to steer emotional outputs."], "limitations": "", "keywords": ["large language models", "emotions", "cognitive appraisal theory", "affective computing", "text generation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.05651", "pdf": "https://arxiv.org/pdf/2502.05651.pdf", "abs": "https://arxiv.org/abs/2502.05651", "title": "KMI: A Dataset of Korean Motivational Interviewing Dialogues for Psychotherapy", "authors": ["Hyunjong Kim", "Suyeon Lee", "Yeongjae Cho", "Eunseo Ryu", "Yohan Jo", "Suran Seong", "Sungzoon Cho"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at NAACL 2025 Main Conference", "summary": "The increasing demand for mental health services has led to the rise of\nAI-driven mental health chatbots, though challenges related to privacy, data\ncollection, and expertise persist. Motivational Interviewing (MI) is gaining\nattention as a theoretical basis for boosting expertise in the development of\nthese chatbots. However, existing datasets are showing limitations for training\nchatbots, leading to a substantial demand for publicly available resources in\nthe field of MI and psychotherapy. These challenges are even more pronounced in\nnon-English languages, where they receive less attention. In this paper, we\npropose a novel framework that simulates MI sessions enriched with the\nexpertise of professional therapists. We train an MI forecaster model that\nmimics the behavioral choices of professional therapists and employ Large\nLanguage Models (LLMs) to generate utterances through prompt engineering. Then,\nwe present KMI, the first synthetic dataset theoretically grounded in MI,\ncontaining 1,000 high-quality Korean Motivational Interviewing dialogues.\nThrough an extensive expert evaluation of the generated dataset and the\ndialogue model trained on it, we demonstrate the quality, expertise, and\npracticality of KMI. We also introduce novel metrics derived from MI theory in\norder to evaluate dialogues from the perspective of MI.", "AI": {"tldr": "Proposes a framework for AI-driven mental health chatbots using Motivational Interviewing (MI) techniques, including a synthetic dataset for training.", "motivation": "To address the limitations of existing datasets for mental health chatbots and improve their effectiveness using MI principles.", "method": "Developed a framework simulating MI sessions with an MI forecaster model and utilized LLMs for generating utterances, creating the KMI dataset with 1,000 dialogues in Korean.", "result": "Expert evaluation shows that the KMI dataset and dialogue model have high quality and practical applicability for training mental health chatbots.", "conclusion": "The study highlights the potential of MI-based approaches in enhancing AI-driven mental health services and introduces new metrics for evaluating chatbot dialogues.", "key_contributions": ["Introduction of the KMI dataset with 1,000 synthetic MI dialogues in Korean", "Development of an MI forecaster model to simulate therapist behavior", "Novel evaluation metrics based on MI theory for chatbot dialogues."], "limitations": "Focus on Korean language, which may limit applicability to English and other languages.", "keywords": ["Motivational Interviewing", "AI Chatbots", "Mental Health", "Synthetic Dataset", "Large Language Models"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.07004", "pdf": "https://arxiv.org/pdf/2502.07004.pdf", "abs": "https://arxiv.org/abs/2502.07004", "title": "Demystifying Singular Defects in Large Language Models", "authors": ["Haoqi Wang", "Tong Zhang", "Mathieu Salzmann"], "categories": ["cs.CL"], "comment": "ICML 2025", "summary": "Large transformer models are known to produce high-norm tokens. In vision\ntransformers (ViTs), such tokens have been mathematically modeled through the\nsingular vectors of the linear approximations of layers. However, in large\nlanguage models (LLMs), the underlying causes of high-norm tokens remain\nlargely unexplored, and their different properties from those of ViTs require a\nnew analysis framework. In this paper, we provide both theoretical insights and\nempirical validation across a range of recent models, leading to the following\nobservations: i) The layer-wise singular direction predicts the abrupt\nexplosion of token norms in LLMs. ii) The negative eigenvalues of a layer\nexplain its sudden decay. iii) The computational pathways leading to high-norm\ntokens differ between initial and noninitial tokens. iv) High-norm tokens are\ntriggered by the right leading singular vector of the matrix approximating the\ncorresponding modules. We showcase two practical applications of these\nfindings: the improvement of quantization schemes and the design of LLM\nsignatures. Our findings not only advance the understanding of singular defects\nin LLMs but also open new avenues for their application. We expect that this\nwork will stimulate further research into the internal mechanisms of LLMs. Code\nis released at https://github.com/haoqiwang/singular_defect.", "AI": {"tldr": "This paper explores the causes and implications of high-norm tokens in large language models (LLMs), providing theoretical and empirical insights.", "motivation": "Understanding the mechanics behind high-norm tokens in LLMs is crucial as they differ from those in vision transformers and impact model performance.", "method": "The authors analyze layer-wise singular directions, eigenvalues, and token pathways in LLMs to evaluate high-norm tokens and their implications.", "result": "The study identifies key factors behind high-norm tokens and showcases improvements in quantization schemes and design of LLM signatures based on their findings.", "conclusion": "The findings expand the understanding of singular defects in LLMs and suggest new directions for research in their internal mechanisms.", "key_contributions": ["Theoretical insights into high-norm tokens in LLMs", "Empirical validation across various models", "Practical applications for quantization and LLM design"], "limitations": "", "keywords": ["large language models", "high-norm tokens", "singular values", "quantization schemes", "internal mechanisms"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.10341", "pdf": "https://arxiv.org/pdf/2502.10341.pdf", "abs": "https://arxiv.org/abs/2502.10341", "title": "Organize the Web: Constructing Domains Enhances Pre-Training Data Curation", "authors": ["Alexander Wettig", "Kyle Lo", "Sewon Min", "Hannaneh Hajishirzi", "Danqi Chen", "Luca Soldaini"], "categories": ["cs.CL"], "comment": "Accepted at ICML 2025. Project page: https://weborganizer.allen.ai", "summary": "Modern language models are trained on large, unstructured datasets consisting\nof trillions of tokens and obtained by crawling the web. The unstructured\nnature makes it difficult to reason about their contents and develop systematic\napproaches to data curation. In this paper, we unpack monolithic web corpora by\ndeveloping taxonomies of their contents and organizing them into domains. We\nintroduce WebOrganizer, a framework for organizing web pages in terms of both\ntheir topic and format. Using these two complementary notions of domains, we\nautomatically annotate pre-training data by distilling annotations from a large\nlanguage model into efficient classifiers. This allows us to study how data\nfrom different domains should be mixed to improve models on downstream tasks,\nand we show that we can combine insights about effective topics and formats to\nfurther boost performance. We demonstrate that our domain mixing also improves\nexisting methods that select data based on quality. Furthermore, we study and\ncompare how quality-based methods will implicitly change the domain mixture.\nOverall, our work demonstrates that constructing and mixing domains provides a\nvaluable complement to quality-based data curation methods, opening new avenues\nfor effective and insightful pre-training data curation.", "AI": {"tldr": "The paper introduces WebOrganizer, a framework for organizing unstructured web data into taxonomies based on topic and format, improving language model pre-training data curation.", "motivation": "To improve the systematic approach to curating large unstructured datasets used for training language models.", "method": "Developing taxonomies of web corpus contents and automatically annotating pre-training data using a large language model to create efficient classifiers for organizing data by topic and format.", "result": "WebOrganizer allows for effective mixing of different domain data, enhancing the performance of language models on downstream tasks compared to traditional quality-based data curation methods.", "conclusion": "Constructing and mixing domains is a valuable complement to quality-based methods, leading to better insights and performance in pre-training data curation.", "key_contributions": ["Introduction of the WebOrganizer framework for domain organization.", "Development of efficient classifiers for annotating pre-training data.", "Evidence that domain mixing improves model performance over traditional quality selection methods."], "limitations": "", "keywords": ["language models", "data curation", "domain mixing", "pre-training data", "automated annotation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.18282", "pdf": "https://arxiv.org/pdf/2502.18282.pdf", "abs": "https://arxiv.org/abs/2502.18282", "title": "Better Aligned with Survey Respondents or Training Data? Unveiling Political Leanings of LLMs on U.S. Supreme Court Cases", "authors": ["Shanshan Xu", "T. Y. S. S Santosh", "Yanai Elazar", "Quirin Vogel", "Barbara Plank", "Matthias Grabmair"], "categories": ["cs.CL"], "comment": null, "summary": "Recent works have shown that Large Language Models (LLMs) have a tendency to\nmemorize patterns and biases present in their training data, raising important\nquestions about how such memorized content influences model behavior. One such\nconcern is the emergence of political bias in LLM outputs. In this paper, we\ninvestigate the extent to which LLMs' political leanings reflect memorized\npatterns from their pretraining corpora. We propose a method to quantitatively\nevaluate political leanings embedded in the large pretraining corpora.\nSubsequently we investigate to whom are the LLMs' political leanings more\naligned with, their pretrainig corpora or the surveyed human opinions. As a\ncase study, we focus on probing the political leanings of LLMs in 32 US Supreme\nCourt cases, addressing contentious topics such as abortion and voting rights.\nOur findings reveal that LLMs strongly reflect the political leanings in their\ntraining data, and no strong correlation is observed with their alignment to\nhuman opinions as expressed in surveys. These results underscore the importance\nof responsible curation of training data, and the methodology for auditing the\nmemorization in LLMs to ensure human-AI alignment.", "AI": {"tldr": "This paper investigates the influence of training data on political biases in Large Language Models (LLMs) and evaluates their alignment with human political opinions through a case study of US Supreme Court cases.", "motivation": "To understand how memorized patterns in training data impact the behavior of LLMs, specifically regarding political bias.", "method": "A quantitative method was proposed to evaluate political leanings in the LLM's pretraining corpora, along with a case study examining 32 US Supreme Court cases.", "result": "LLMs strongly reflect the political leanings in their training data, with no strong correlation found between LLM outputs and human opinions.", "conclusion": "The study highlights the need for responsible curation of training data and the importance of auditing LLM memorization to achieve human-AI alignment.", "key_contributions": ["Proposed a method to evaluate political leanings in LLMs' training data.", "Investigated LLM political biases using US Supreme Court case studies.", "Revealed misalignment between LLM outputs and surveyed human opinions."], "limitations": "", "keywords": ["Large Language Models", "Political Bias", "Training Data", "Human-AI Alignment", "Audit Methodology"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.18435", "pdf": "https://arxiv.org/pdf/2502.18435.pdf", "abs": "https://arxiv.org/abs/2502.18435", "title": "What Makes the Preferred Thinking Direction for LLMs in Multiple-choice Questions?", "authors": ["Yizhe Zhang", "Richard Bai", "Zijin Gu", "Ruixiang Zhang", "Jiatao Gu", "Emmanuel Abbe", "Samy Bengio", "Navdeep Jaitly"], "categories": ["cs.CL", "cs.IT", "cs.LG", "math.IT"], "comment": "10 pages for the main text", "summary": "Language models usually use left-to-right (L2R) autoregressive factorization.\nHowever, L2R factorization may not always be the best inductive bias.\nTherefore, we investigate whether alternative factorizations of the text\ndistribution could be beneficial in some tasks. We investigate right-to-left\n(R2L) training as a compelling alternative, focusing on multiple-choice\nquestions (MCQs) as a test bed for knowledge extraction and reasoning. Through\nextensive experiments across various model sizes (2B-8B parameters) and\ntraining datasets, we find that R2L models can significantly outperform L2R\nmodels on several MCQ benchmarks, including logical reasoning, commonsense\nunderstanding, and truthfulness assessment tasks. Our analysis reveals that\nthis performance difference may be fundamentally linked to multiple factors\nincluding calibration, computability, and directional conditional entropy. We\nanalyze the impact of these factors through controlled simulation studies using\narithmetic tasks, where the impacting factors can be better disentangled. Our\nwork demonstrates that exploring alternative factorizations of the text\ndistribution can lead to improvements in LLM capabilities and provides\ntheoretical insights into optimal factorization towards approximating human\nlanguage distribution, and when each reasoning order might be more\nadvantageous. Our code and checkpoints are released at\nhttps://github.com/apple/ml-reversal-blessing.", "AI": {"tldr": "This paper investigates right-to-left (R2L) training as an alternative to left-to-right (L2R) autoregressive factorization in language models, particularly on multiple-choice questions (MCQs), finding that R2L can significantly outperform L2R models in various reasoning benchmarks.", "motivation": "To explore whether alternative text distribution factorizations, specifically right-to-left (R2L), can provide better performance than the standard left-to-right (L2R) autoregressive approach in language models.", "method": "Extensive experiments were conducted across various model sizes (2B-8B parameters) and training datasets, focusing on MCQ benchmarks and analyzing performance differences through controlled simulations.", "result": "R2L models significantly outperform L2R models on several MCQ benchmarks, including logical reasoning, commonsense understanding, and truthfulness assessment tasks.", "conclusion": "Exploring alternative factorizations of text distributions can improve LLM capabilities, providing insights into when each reasoning order might be advantageous.", "key_contributions": ["Demonstrated the effectiveness of R2L models over L2R models for MCQs.", "Provided theoretical insights into text distribution factorization and its impact on language modeling.", "Released code and checkpoints for further research."], "limitations": "The study primarily focuses on MCQs and controlled simulation studies, which may not cover all real-world applications.", "keywords": ["Language Models", "Right-to-Left Training", "Multiple-Choice Questions", "Knowledge Extraction", "Reasoning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2503.01875", "pdf": "https://arxiv.org/pdf/2503.01875.pdf", "abs": "https://arxiv.org/abs/2503.01875", "title": "Time-MQA: Time Series Multi-Task Question Answering with Context Enhancement", "authors": ["Yaxuan Kong", "Yiyuan Yang", "Yoontae Hwang", "Wenjie Du", "Stefan Zohren", "Zhangyang Wang", "Ming Jin", "Qingsong Wen"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Annual Meeting of the Association for Computational Linguistics (ACL\n  2025, Main)", "summary": "Time series data are foundational in finance, healthcare, and energy domains.\nHowever, most existing methods and datasets remain focused on a narrow spectrum\nof tasks, such as forecasting or anomaly detection. To bridge this gap, we\nintroduce Time Series Multi-Task Question Answering (Time-MQA), a unified\nframework that enables natural language queries across multiple time series\ntasks - numerical analytical tasks and open-ended question answering with\nreasoning. Central to Time-MQA is the TSQA dataset, a large-scale dataset\ncontaining $\\sim$200k question-answer pairs derived from diverse time series\nspanning environment, traffic, etc. This comprehensive resource covers various\ntime series lengths and promotes robust model development. We further\ndemonstrate how continually pre-training large language models (Mistral 7B,\nLlama-3 8B, and Qwen-2.5 7B) on the TSQA dataset enhanced time series reasoning\ncapabilities, moving beyond mere numeric tasks and enabling more advanced and\nintuitive interactions with temporal data. The complete TSQA dataset, models,\nuser study questionnaires for evaluation, and other related materials have been\nopen-sourced.", "AI": {"tldr": "Introducing Time Series Multi-Task Question Answering (Time-MQA) framework for natural language queries on time series data, emphasizing on enhanced reasoning capabilities using a comprehensive dataset.", "motivation": "To address the limited focus of existing methods on narrow tasks such as forecasting or anomaly detection in time series data by enabling a wider range of natural language queries.", "method": "Developed a unified framework called Time-MQA that utilizes a large-scale TSQA dataset with approximately 200k question-answer pairs to facilitate various tasks in time series.", "result": "The framework demonstrated that continual pre-training of large language models on the TSQA dataset significantly improved their reasoning capabilities, expanding beyond numeric tasks to support intuitive interactions with temporal data.", "conclusion": "The TSQA dataset and the Time-MQA framework provide a foundation for robust model development and open-source resources for further research in time series analysis.", "key_contributions": ["Introduction of Time-MQA framework for multi-task question answering in time series", "Creation of the TSQA dataset with diverse time series and extensive question-answer pairs", "Enhanced reasoning abilities in language models when pre-trained on TSQA dataset"], "limitations": "", "keywords": ["time series", "multi-task learning", "question answering", "language models", "TSQA dataset"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2503.04722", "pdf": "https://arxiv.org/pdf/2503.04722.pdf", "abs": "https://arxiv.org/abs/2503.04722", "title": "Enough Coin Flips Can Make LLMs Act Bayesian", "authors": ["Ritwik Gupta", "Rodolfo Corona", "Jiaxin Ge", "Eric Wang", "Dan Klein", "Trevor Darrell", "David M. Chan"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL 2025 Main", "summary": "Large language models (LLMs) exhibit the ability to generalize given few-shot\nexamples in their input prompt, an emergent capability known as in-context\nlearning (ICL). We investigate whether LLMs use ICL to perform structured\nreasoning in ways that are consistent with a Bayesian framework or rely on\npattern matching. Using a controlled setting of biased coin flips, we find\nthat: (1) LLMs often possess biased priors, causing initial divergence in\nzero-shot settings, (2) in-context evidence outweighs explicit bias\ninstructions, (3) LLMs broadly follow Bayesian posterior updates, with\ndeviations primarily due to miscalibrated priors rather than flawed updates,\nand (4) attention magnitude has negligible effect on Bayesian inference. With\nsufficient demonstrations of biased coin flips via ICL, LLMs update their\npriors in a Bayesian manner.", "AI": {"tldr": "This paper investigates the reasoning capabilities of large language models (LLMs) through in-context learning, finding that LLMs exhibit Bayesian reasoning with biased priors and demonstrate structured updates based on evidence provided through few-shot examples.", "motivation": "To understand how large language models utilize in-context learning for reasoning, specifically whether they employ Bayesian frameworks or pattern matching in their decision-making processes.", "method": "Controlled experiments were conducted using biased coin flips to assess LLMs' reasoning capabilities, examining their prior biases, reliance on evidence, adherence to Bayesian updates, and the effects of attention magnitude.", "result": "The study reveals that LLMs exhibit biased priors leading to divergence in zero-shot contexts, that in-context evidence predominantly influences their reasoning over explicit biases, that they generally follow Bayesian updating procedures, and that attention magnitude has little impact on inference outcomes.", "conclusion": "LLMs are capable of updating their priors in a Bayesian framework when provided with sufficient in-context demonstrations, suggesting their reasoning ability is more structured than mere pattern matching.", "key_contributions": ["Investigation of LLMs' in-context learning and reasoning", "Demonstration of biased priors in zero-shot settings", "Evidence supporting Bayesian posterior updates in LLMs."], "limitations": "The experiments are limited to a controlled setting and may not fully represent LLM behavior in more complex or varied contexts.", "keywords": ["Large Language Models", "In-Context Learning", "Bayesian Reasoning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2503.10135", "pdf": "https://arxiv.org/pdf/2503.10135.pdf", "abs": "https://arxiv.org/abs/2503.10135", "title": "Gumiho: A Hybrid Architecture to Prioritize Early Tokens in Speculative Decoding", "authors": ["Jinze Li", "Yixing Xu", "Haiduo Huang", "Xuanwu Yin", "Dong Li", "Edith C. H. Ngai", "Emad Barsoum"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to the 42nd International Conference on Machine Learning\n  (ICML 2025). Code: https://github.com/AMD-AIG-AIMA/Gumiho", "summary": "Speculative decoding (SPD) aims to accelerate the auto-regressive token\ngeneration process of a target Large Language Model (LLM). Some approaches\nemploy a draft model with multiple heads to predict a sequence of future\ntokens, where each head handles a token in the sequence. The target LLM\nverifies the predicted sequence and accepts aligned tokens, enabling efficient\nmulti-token generation. However, existing methods assume that all tokens within\na sequence are equally important, employing identical head structures and\nrelying on a single-generation paradigm, either serial or parallel. To this\nend, we theoretically demonstrate that initial tokens in the draft sequence are\nmore important than later ones. Building on this insight, we propose Gumiho, a\nhybrid model combining serial and parallel heads. Specifically, given the\ncritical importance of early tokens, we employ a sophisticated Transformer\narchitecture for the early draft heads in a serial configuration to improve\naccuracy. For later tokens, we utilize multiple lightweight MLP heads operating\nin parallel to enhance efficiency. By allocating more advanced model structures\nand longer running times to the early heads, Gumiho achieves improved overall\nperformance. The experimental results demonstrate that our method outperforms\nexisting approaches, fully validating its effectiveness.", "AI": {"tldr": "Gumiho is a hybrid model for efficient auto-regressive token generation in LLMs, balancing accuracy and efficiency through a unique architecture.", "motivation": "To improve the efficiency of multi-token generation in LLMs by addressing the unequal importance of tokens in a sequence during prediction.", "method": "Gumiho employs a combination of serial heads with advanced Transformer architecture for early tokens and lightweight parallel MLP heads for later tokens.", "result": "Gumiho demonstrates improved performance over existing methods in the token generation process of LLMs, validating its effectiveness through experimental results.", "conclusion": "The proposed hybrid model enhances both accuracy and efficiency in LLM token generation by appropriately prioritizing early tokens.", "key_contributions": ["Introduction of Gumiho, a hybrid model for token generation", "Theoretical analysis of token importance in sequences", "Experimental results showing improved performance compared to existing methods."], "limitations": "", "keywords": ["Speculative Decoding", "Transformer", "Auto-regressive Models", "Machine Learning", "Token Generation"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2503.11655", "pdf": "https://arxiv.org/pdf/2503.11655.pdf", "abs": "https://arxiv.org/abs/2503.11655", "title": "Explainable Sentiment Analysis with DeepSeek-R1: Performance, Efficiency, and Few-Shot Learning", "authors": ["Donghao Huang", "Zhaoxia Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, 2 figures, 6 tables, revised and re-submitted to an IEEE\n  journal", "summary": "Large language models (LLMs) have transformed sentiment analysis, yet\nbalancing accuracy, efficiency, and explainability remains a critical\nchallenge. This study presents the first comprehensive evaluation of\nDeepSeek-R1--an open-source reasoning model--against OpenAI's GPT-4o and\nGPT-4o-mini. We test the full 671B model and its distilled variants,\nsystematically documenting few-shot learning curves. Our experiments show\nDeepSeek-R1 achieves a 91.39\\% F1 score on 5-class sentiment and 99.31\\%\naccuracy on binary tasks with just 5 shots, an eightfold improvement in\nfew-shot efficiency over GPT-4o. Architecture-specific distillation effects\nemerge, where a 32B Qwen2.5-based model outperforms the 70B Llama-based variant\nby 6.69 percentage points. While its reasoning process reduces throughput,\nDeepSeek-R1 offers superior explainability via transparent, step-by-step\ntraces, establishing it as a powerful, interpretable open-source alternative.", "AI": {"tldr": "DeepSeek-R1 is an open-source reasoning model that outperforms GPT-4o and GPT-4o-mini in few-shot sentiment analysis tasks, achieving higher accuracy and explainability.", "motivation": "The study addresses the challenging balance between accuracy, efficiency, and explainability in sentiment analysis using large language models.", "method": "A comprehensive evaluation of DeepSeek-R1 is performed against OpenAI's models using systematic few-shot learning experiments documenting their performance metrics.", "result": "DeepSeek-R1 achieves a 91.39% F1 score on 5-class sentiment and 99.31% accuracy on binary sentiment analysis with just 5 shots, significantly outperforming GPT-4o in few-shot efficiency.", "conclusion": "While DeepSeek-R1 has reduced throughput due to its reasoning process, it is established as a powerful and interpretable alternative model for sentiment analysis tasks.", "key_contributions": ["First comprehensive evaluation of DeepSeek-R1 against GPT-4o models in sentiment analysis.", "Demonstrates superior few-shot learning efficiency with significant performance improvements.", "Offers enhanced explainability through transparent reasoning traces."], "limitations": "Reduced throughput due to the reasoning process.", "keywords": ["DeepSeek-R1", "sentiment analysis", "few-shot learning", "explainability", "large language models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.16334", "pdf": "https://arxiv.org/pdf/2503.16334.pdf", "abs": "https://arxiv.org/abs/2503.16334", "title": "LLM Braces: Straightening Out LLM Predictions with Relevant Sub-Updates", "authors": ["Ying Shen", "Lifu Huang"], "categories": ["cs.CL"], "comment": "ACL 2025, 16 pages, 2 figures", "summary": "Recent findings reveal that much of the knowledge in a Transformer-based\nLarge Language Model (LLM) is encoded in its feed-forward (FFN) layers, where\neach FNN layer can be interpreted as the summation of sub-updates, each\ncorresponding to a weighted column vector from the FFN's value parameter matrix\nthat often encodes human-interpretable concepts. In light of this, we\nhypothesize that model performance and behaviors can be further enhanced and\ncontrolled by modulating the contributions of these sub-updates based on their\nrelevance to the input or target output style, and propose LLMBRACES, a novel\nand efficient method that computes relevance scores associated with value\nvectors in FFN layers and leverages these scores to dynamically adjust the\ncontribution of sub-updates. By optimizing sub-update contributions, LLMBRACES\nrefines the prediction process, leading to more accurate and reliable outputs,\nmuch like a 'brace' providing support and stability. Moreover, LLMBRACES can be\nextended to support conditional control over generation characteristics, such\nas sentiment, thereby offering fine-grained steering of LLM outputs. Extensive\nexperiments on various LLMs-including Qwen2.5-1.5B, Llama2-7B, and\nLlama3-8B-demonstrate that LLMBRACES outperforms baseline approaches in both\nfine-tuning and zero-shot settings while requiring significantly fewer tunable\nparameters, up to 75% fewer compared to LoRA. Furthermore, LLMBRACES excels in\nsentiment-controlled generation and toxicity reduction, highlighting its\npotential for flexible, controlled text generation across applications.", "AI": {"tldr": "LLMBRACES enhances Transformer-based LLMs by modulating sub-update contributions in FFN layers to improve accuracy and control over outputs, particularly for sentiment and toxicity.", "motivation": "To improve performance and controllability of Transformer-based LLMs by optimizing contributions of value vectors in FFN layers to enhance prediction and generation characteristics.", "method": "LLMBRACES computes relevance scores for value vectors in FFN layers and uses these scores to dynamically adjust the contribution of sub-updates during text generation.", "result": "LLMBRACES outperforms baseline methods in fine-tuning and zero-shot scenarios while using 75% fewer tunable parameters, demonstrating excellence in sentiment control and toxicity reduction.", "conclusion": "LLMBRACES provides a novel approach for enhancing LLM output accuracy and customizing generation settings through relevant control mechanisms, showing promising applications in various contexts.", "key_contributions": ["Introduction of LLMBRACES for relevance-based adjustment of sub-update contributions in LLMs.", "Substantial improvements in fine-tuning and zero-shot performance across multiple LLM architectures.", "Capability for controlled output generation, including sentiment moderation and toxicity reduction."], "limitations": "", "keywords": ["Large Language Models", "Human-Computer Interaction", "Sentiment Control", "Transformer Models", "Text Generation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2503.17222", "pdf": "https://arxiv.org/pdf/2503.17222.pdf", "abs": "https://arxiv.org/abs/2503.17222", "title": "Automating Adjudication of Cardiovascular Events Using Large Language Models", "authors": ["Sonish Sivarajkumar", "Kimia Ameri", "Chuqin Li", "Yanshan Wang", "Min Jiang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Cardiovascular events, such as heart attacks and strokes, remain a leading\ncause of mortality globally, necessitating meticulous monitoring and\nadjudication in clinical trials. This process, traditionally performed manually\nby clinical experts, is time-consuming, resource-intensive, and prone to\ninter-reviewer variability, potentially introducing bias and hindering trial\nprogress. This study addresses these critical limitations by presenting a novel\nframework for automating the adjudication of cardiovascular events in clinical\ntrials using Large Language Models (LLMs). We developed a two-stage approach:\nfirst, employing an LLM-based pipeline for event information extraction from\nunstructured clinical data and second, using an LLM-based adjudication process\nguided by a Tree of Thoughts approach and clinical endpoint committee (CEC)\nguidelines. Using cardiovascular event-specific clinical trial data, the\nframework achieved an F1-score of 0.82 for event extraction and an accuracy of\n0.68 for adjudication. Furthermore, we introduce the CLEART score, a novel,\nautomated metric specifically designed for evaluating the quality of\nAI-generated clinical reasoning in adjudicating cardiovascular events. This\napproach demonstrates significant potential for substantially reducing\nadjudication time and costs while maintaining high-quality, consistent, and\nauditable outcomes in clinical trials. The reduced variability and enhanced\nstandardization also allow for faster identification and mitigation of risks\nassociated with cardiovascular therapies.", "AI": {"tldr": "The study presents a framework using LLMs to automate the adjudication of cardiovascular events in clinical trials, improving efficiency and consistency.", "motivation": "Address the limitations of manual adjudication in clinical trials for cardiovascular events, which is time-consuming and biased.", "method": "A two-stage approach: LLM-based event information extraction from unstructured data, followed by LLM-guided adjudication based on clinical guidelines.", "result": "Achieved an F1-score of 0.82 for event extraction and an accuracy of 0.68 for adjudication; introduced the CLEART score for evaluating AI-generated clinical reasoning.", "conclusion": "The framework shows significant potential for reducing adjudication time and costs while ensuring high-quality outcomes and faster risk identification in clinical trials.", "key_contributions": ["Development of a novel framework for LLM-based adjudication of cardiovascular events", "Introduction of the CLEART score for evaluating clinical reasoning quality", "Demonstrated substantial reduction in adjudication time and enhanced consistency."], "limitations": "The study does not explore the generalizability of the approach to other medical domains or types of events.", "keywords": ["Cardiovascular Events", "Large Language Models", "Clinical Trials", "Event Adjudication", "AI in Healthcare"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2504.12563", "pdf": "https://arxiv.org/pdf/2504.12563.pdf", "abs": "https://arxiv.org/abs/2504.12563", "title": "MetaSynth: Meta-Prompting-Driven Agentic Scaffolds for Diverse Synthetic Data Generation", "authors": ["Haris Riaz", "Sourav Bhabesh", "Vinayak Arannil", "Miguel Ballesteros", "Graham Horwood"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "33 pages, 17 figures. Findings of ACL 2025", "summary": "Recent smaller language models such Phi-3.5 and Phi-4 rely on synthetic data\ngenerated using larger Language models. Questions remain about leveraging\nsynthetic data for other use cases, such as adapting LLMs to specific domains.\nA key limitation of synthetic data is low diversity, which negatively impacts\nits downstream applicability for improving other models. To address this, we\npropose MetaSynth, a method for generating synthetic data that enhances\ndiversity through meta-prompting, where a language model orchestrates multiple\n\"expert\" LLM agents to collaboratively generate data. Using only 25 million\ntokens of synthetic data generated with MetaSynth, we successfully adapt a\nwell-trained LLM (Mistral-7B-v0.3) to two specialized domains-Finance and\nBiomedicine-without compromising the capabilities of the resulting model in\ngeneral tasks. In addition, we evaluate the diversity of our synthetic data\nusing seven automated metrics, and find that it approaches the diversity of LLM\npre-training corpora.\n  Continually pre-training Mistral-7B-v0.3 with MetaSynth notably outperforms\nthe base LLM, showing improvements of up to 4.08% in Finance and 13.75% in\nBiomedicine. The same model shows degraded performance when trained on data\ngenerated using a template prompt, even when the template includes prior\ngenerations and varying In-Context exemplars of real data. Our findings suggest\nthat a few million tokens of diverse synthetic data without mixing any real\ndata, is sufficient for effective domain adaptation when using MetaSynth.", "AI": {"tldr": "MetaSynth enhances the diversity of synthetic data for domain adaptation in LLMs, demonstrating effective improvements in Finance and Biomedicine with limited token usage.", "motivation": "Address the challenge of low diversity in synthetic data generated by LLMs, which limits its utility in adapting models to specific domains.", "method": "MetaSynth employs meta-prompting, where a language model orchestrates multiple expert LLM agents to collaboratively generate synthetic data, improving its diversity.", "result": "Using 25 million tokens of synthetic data from MetaSynth, we adapted Mistral-7B-v0.3 to Finance and Biomedicine, achieving performance improvements of 4.08% and 13.75% respectively, compared to base LLM.", "conclusion": "Few million tokens of diverse synthetic data can effectively support domain adaptation in LLMs without integrating real data, as evidenced by improved model performance.", "key_contributions": ["Introduction of MetaSynth for generating diverse synthetic data", "Demonstrated effective domain adaptation for Finance and Biomedicine", "Evaluation using seven automated metrics to assess diversity"], "limitations": "Focus on only two specialized domains; results may vary in other domains or tasks.", "keywords": ["synthetic data", "domain adaptation", "language models", "meta-prompting", "diversity"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.14154", "pdf": "https://arxiv.org/pdf/2504.14154.pdf", "abs": "https://arxiv.org/abs/2504.14154", "title": "SConU: Selective Conformal Uncertainty in Large Language Models", "authors": ["Zhiyuan Wang", "Qingni Wang", "Yue Zhang", "Tianlong Chen", "Xiaofeng Zhu", "Xiaoshuang Shi", "Kaidi Xu"], "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "comment": "Accepted by ACL 2025 Main", "summary": "As large language models are increasingly utilized in real-world\napplications, guarantees of task-specific metrics are essential for their\nreliable deployment. Previous studies have introduced various criteria of\nconformal uncertainty grounded in split conformal prediction, which offer\nuser-specified correctness coverage. However, existing frameworks often fail to\nidentify uncertainty data outliers that violate the exchangeability assumption,\nleading to unbounded miscoverage rates and unactionable prediction sets. In\nthis paper, we propose a novel approach termed Selective Conformal Uncertainty\n(SConU), which, for the first time, implements significance tests, by\ndeveloping two conformal p-values that are instrumental in determining whether\na given sample deviates from the uncertainty distribution of the calibration\nset at a specific manageable risk level. Our approach not only facilitates\nrigorous management of miscoverage rates across both single-domain and\ninterdisciplinary contexts, but also enhances the efficiency of predictions.\nFurthermore, we comprehensively analyze the components of the conformal\nprocedures, aiming to approximate conditional coverage, particularly in\nhigh-stakes question-answering tasks.", "AI": {"tldr": "The paper introduces Selective Conformal Uncertainty (SConU), a method for managing uncertainty in large language model applications by implementing significance tests to detect outliers and improve prediction reliability.", "motivation": "To address the challenges of miscoverage rates and unactionable predictions in existing conformal uncertainty frameworks for large language models.", "method": "The paper proposes Selective Conformal Uncertainty (SConU), which utilizes conformal p-values to determine deviations from the uncertainty distribution of the calibration set.", "result": "SConU enhances the management of miscoverage rates and the efficiency of predictions in both single-domain and interdisciplinary contexts, especially for high-stakes question-answering tasks.", "conclusion": "The method provides a rigorous approach to approximating conditional coverage, improving the performance of large language models in real-world applications.", "key_contributions": ["Introduction of Selective Conformal Uncertainty (SConU) methodology", "Implementation of significance tests to identify uncertainty data outliers", "Enhanced prediction reliability and efficiency across various contexts"], "limitations": "", "keywords": ["Selective Conformal Uncertainty", "large language models", "uncertainty quantification"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.07160", "pdf": "https://arxiv.org/pdf/2506.07160.pdf", "abs": "https://arxiv.org/abs/2506.07160", "title": "GeometryZero: Improving Geometry Solving for LLM with Group Contrastive Policy Optimization", "authors": ["Yikun Wang", "Yibin Wang", "Dianyi Wang", "Zimian Peng", "Qipeng Guo", "Dacheng Tao", "Jiaqi Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large language models (LLMs) have demonstrated remarkable\ncapabilities across diverse domains, particularly in mathematical reasoning,\namid which geometry problem solving remains a challenging area where auxiliary\nconstruction plays a enssential role. Existing approaches either achieve\nsuboptimal performance or rely on massive LLMs (e.g., GPT-4o), incurring\nmassive computational costs. We posit that reinforcement learning with\nverifiable reward (e.g., GRPO) offers a promising direction for training\nsmaller models that effectively combine auxiliary construction with robust\ngeometric reasoning. However, directly applying GRPO to geometric reasoning\npresents fundamental limitations due to its dependence on unconditional\nrewards, which leads to indiscriminate and counterproductive auxiliary\nconstructions. To address these challenges, we propose Group Contrastive Policy\nOptimization (GCPO), a novel reinforcement learning framework featuring two key\ninnovations: (1) Group Contrastive Masking, which adaptively provides positive\nor negative reward signals for auxiliary construction based on contextual\nutility, and a (2) length reward that promotes longer reasoning chains.\nBuilding on GCPO, we develop GeometryZero, a family of affordable-size\ngeometric reasoning models that judiciously determine when to employ auxiliary\nconstruction. Our extensive empirical evaluation across popular geometric\nbenchmarks (Geometry3K, MathVista) demonstrates that GeometryZero models\nconsistently outperform baselines (e.g. GRPO), achieving an average improvement\nof 4.29% across all benchmarks.", "AI": {"tldr": "The paper introduces Group Contrastive Policy Optimization (GCPO), a reinforcement learning framework that enhances geometric reasoning in smaller models by effectively using auxiliary constructions.", "motivation": "To improve geometric problem-solving capabilities in LLMs without incurring high computational costs associated with large models.", "method": "The authors developed the Group Contrastive Policy Optimization (GCPO) framework, which implements Group Contrastive Masking for contextual rewards and a length reward to encourage longer reasoning chains.", "result": "GeometryZero models, built on GCPO, significantly outperform existing models like GRPO with an average improvement of 4.29% on geometric benchmarks.", "conclusion": "GCPO enables effective training of smaller models for geometric reasoning while leveraging auxiliary constructions and providing relevant reward signals.", "key_contributions": ["Introduction of Group Contrastive Policy Optimization (GCPO) framework", "Development of GeometryZero models for geometric reasoning", "Demonstrated performance improvements over existing methods on geometric benchmarks"], "limitations": "", "keywords": ["Large Language Models", "Geometric Reasoning", "Reinforcement Learning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.20199", "pdf": "https://arxiv.org/pdf/2506.20199.pdf", "abs": "https://arxiv.org/abs/2506.20199", "title": "How to Retrieve Examples in In-context Learning to Improve Conversational Emotion Recognition using Large Language Models?", "authors": ["Mengqi Wang", "Tiantian Feng", "Shrikanth Narayanan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have enabled a wide variety of real-world\napplications in various domains. However, creating a high-performing\napplication with high accuracy remains challenging, particularly for subjective\ntasks like emotion recognition. Inspired by the SLT 2024 GenSER Challenge, this\nstudy investigates approaches to improving conversational emotion recognition\n(CER) by LLMs. Specifically, we explore how to retrieve high-quality examples\nin in-context learning (ICL) to enhance CER. We propose various strategies\nbased on random and augmented example retrieval and also analyze the impact of\nconversational context on CER accuracy. Experiments were conducted on the three\ndatasets including IEMOCAP, MELD and EmoryNLP. The results show that augmented\nexample retrieval consistently outperforms other techniques under investigation\nacross all datasets, highlighting the importance of retrieving coherent\ntargeted examples and enhancing them through paraphrasing.", "AI": {"tldr": "This study explores improving conversational emotion recognition (CER) using large language models (LLMs) through high-quality example retrieval for in-context learning.", "motivation": "Creating high-performing applications with high accuracy remains challenging for subjective tasks like emotion recognition.", "method": "Various strategies for retrieving high-quality examples, including random and augmented example retrieval, were proposed and analyzed, focusing on the impact of conversational context on CER accuracy.", "result": "Augmented example retrieval consistently outperformed other techniques across three datasets (IEMOCAP, MELD, EmoryNLP), emphasizing the need for coherent targeted examples and paraphrasing.", "conclusion": "Enhancing CER effectiveness requires innovative example retrieval strategies which directly improve the accuracy of LLMs in conversational applications.", "key_contributions": ["Investigation of in-context learning for emotional recognition using LLMs", "Proposed augmented example retrieval methods", "Analysis of conversational context impact on emotion recognition accuracy"], "limitations": "Study limited to specific datasets and might not generalize to all conversational contexts or emotional nuances.", "keywords": ["Conversational Emotion Recognition", "Large Language Models", "In-Context Learning", "Example Retrieval", "Emotion Recognition"], "importance_score": 9, "read_time_minutes": 10}}
