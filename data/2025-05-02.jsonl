{"id": "2505.00153", "pdf": "https://arxiv.org/pdf/2505.00153.pdf", "abs": "https://arxiv.org/abs/2505.00153", "title": "Audo-Sight: Enabling Ambient Interaction For Blind And Visually Impaired Individuals", "authors": ["Bhanuja Ainary"], "categories": ["cs.HC", "cs.DC"], "comment": "This thesis was conducted under the guidance of Mohsen Amini Salehi.\n  Special thanks to Minseo Kim and Jacob Bradshaw for their valuable\n  contributions and support throughout the research process. 60 pages, 13\n  Figures, 2 Tables", "summary": "Visually impaired people face significant challenges when attempting to\ninteract with and understand complex environments, and traditional assistive\ntechnologies often struggle to quickly provide necessary contextual\nunderstanding and interactive intelligence. This thesis presents Audo-Sight, a\nstate-of-the-art assistive system that seamlessly integrates Multimodal Large\nLanguage Models (MLLMs) to provide expedient, context-aware interactions for\nBlind and Visually Impaired (BVI) individuals. The system operates in two\ndifferent modalities: personalized interaction through user identification and\npublic access in common spaces like museums and shopping malls. In tailored\nenvironments, the system adjusts its output to conform to the preferences of\nindividual users, thus enhancing accessibility through a user-aware form of\ninteraction. In shared environments, Audo-Sight employs a shared architecture\nthat adapts to its current user with no manual reconfiguration required. To\nfacilitate appropriate interactions with the LLM, the public Audo-Sight\nsolution includes an Age-Range Determiner and Safe Query Filter. Additionally,\nthe system ensures that responses are respectful to BVI users through NeMo\nGuardrails. By utilizing multimodal reasoning, BVI-cognizant response editing,\nand safeguarding features, this work represents a major leap in AI-driven\naccessibility technology capable of increasing autonomy, safety, and\ninteraction for people with visual impairments in social settings. Finally, we\npresent the integration of Audo-Sight and SmartSight, which enables enhanced\nsituational awareness for BVI individuals. This integration takes advantage of\nthe real-time visual analysis of SmartSight, combined with the extensive\nreasoning and interactive capabilities of Audo-Sight, and goes beyond object\nidentification to provide context-driven, voice-controlled assistance in\ndynamic environments."}
{"id": "2505.00455", "pdf": "https://arxiv.org/pdf/2505.00455.pdf", "abs": "https://arxiv.org/abs/2505.00455", "title": "Data Therapist: Eliciting Domain Knowledge from Subject Matter Experts Using Large Language Models", "authors": ["Sungbok Shin", "Hyeon Jeon", "Sanghyun Hong", "Niklas Elmqvist"], "categories": ["cs.HC", "cs.AI"], "comment": "Submitted to IEEE VIS2025", "summary": "Effective data visualization requires not only technical proficiency but also\na deep understanding of the domain-specific context in which data exists. This\ncontext often includes tacit knowledge about data provenance, quality, and\nintended use, which is rarely explicit in the dataset itself. We present the\nData Therapist, a web-based tool that helps domain experts externalize this\nimplicit knowledge through a mixed-initiative process combining iterative Q&A\nwith interactive annotation. Powered by a large language model, the system\nanalyzes user-supplied datasets, prompts users with targeted questions, and\nallows annotation at varying levels of granularity. The resulting structured\nknowledge base can inform both human and automated visualization design. We\nevaluated the tool in a qualitative study involving expert pairs from Molecular\nBiology, Accounting, Political Science, and Usable Security. The study revealed\nrecurring patterns in how experts reason about their data and highlights areas\nwhere AI support can improve visualization design."}
{"id": "2505.00018", "pdf": "https://arxiv.org/pdf/2505.00018.pdf", "abs": "https://arxiv.org/abs/2505.00018", "title": "Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management", "authors": ["Ju Wu", "Calvin K. L. Or"], "categories": ["cs.AI", "cs.HC", "cs.MA"], "comment": null, "summary": "This position paper critically surveys a broad spectrum of recent empirical\ndevelopments on human-AI agents collaboration, highlighting both their\ntechnical achievements and persistent gaps. We observe a lack of a unifying\ntheoretical framework that can coherently integrate these varied studies,\nespecially when tackling open-ended, complex tasks. To address this, we propose\na novel conceptual architecture: one that systematically interlinks the\ntechnical details of multi-agent coordination, knowledge management, cybernetic\nfeedback loops, and higher-level control mechanisms. By mapping existing\ncontributions, from symbolic AI techniques and connectionist LLM-based agents\nto hybrid organizational practices, onto this proposed framework (Hierarchical\nExploration-Exploitation Net), our approach facilitates revision of legacy\nmethods and inspires new work that fuses qualitative and quantitative\nparadigms. The paper's structure allows it to be read from any section, serving\nequally as a critical review of technical implementations and as a\nforward-looking reference for designing or extending human-AI symbioses.\nTogether, these insights offer a stepping stone toward deeper co-evolution of\nhuman cognition and AI capability."}
{"id": "2505.00049", "pdf": "https://arxiv.org/pdf/2505.00049.pdf", "abs": "https://arxiv.org/abs/2505.00049", "title": "Humanizing LLMs: A Survey of Psychological Measurements with Tools, Datasets, and Human-Agent Applications", "authors": ["Wenhan Dong", "Yuemeng Zhao", "Zhen Sun", "Yule Liu", "Zifan Peng", "Jingyi Zheng", "Zongmin Zhang", "Ziyi Zhang", "Jun Wu", "Ruiming Wang", "Shengmin Xu", "Xinyi Huang", "Xinlei He"], "categories": ["cs.CY", "cs.CL", "cs.HC", "cs.LG"], "comment": "26 pages,7 figures", "summary": "As large language models (LLMs) are increasingly used in human-centered\ntasks, assessing their psychological traits is crucial for understanding their\nsocial impact and ensuring trustworthy AI alignment. While existing reviews\nhave covered some aspects of related research, several important areas have not\nbeen systematically discussed, including detailed discussions of diverse\npsychological tests, LLM-specific psychological datasets, and the applications\nof LLMs with psychological traits. To address this gap, we systematically\nreview six key dimensions of applying psychological theories to LLMs: (1)\nassessment tools; (2) LLM-specific datasets; (3) evaluation metrics\n(consistency and stability); (4) empirical findings; (5) personality simulation\nmethods; and (6) LLM-based behavior simulation. Our analysis highlights both\nthe strengths and limitations of current methods. While some LLMs exhibit\nreproducible personality patterns under specific prompting schemes, significant\nvariability remains across tasks and settings. Recognizing methodological\nchallenges such as mismatches between psychological tools and LLMs'\ncapabilities, as well as inconsistencies in evaluation practices, this study\naims to propose future directions for developing more interpretable, robust,\nand generalizable psychological assessment frameworks for LLMs."}
{"id": "2505.00001", "pdf": "https://arxiv.org/pdf/2505.00001.pdf", "abs": "https://arxiv.org/abs/2505.00001", "title": "Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning", "authors": ["Shaun Baek", "Shaun Esua-Mensah", "Cyrus Tsui", "Sejan Vigneswaralingam", "Abdullah Alali", "Michael Lu", "Vasu Sharma", "Kevin Zhu"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are primarily trained on high-resource natural\nlanguages, limiting their effectiveness in low-resource settings and in tasks\nrequiring deep logical reasoning. This research introduces Rosetta-PL, a\nbenchmark designed to evaluate LLMs' logical reasoning and generalization\ncapabilities in a controlled environment. We construct Rosetta-PL by\ntranslating a dataset of logical propositions from Lean into a custom logical\nlanguage, which is then used to fine-tune an LLM (e.g., GPT-4o). Our\nexperiments analyze the impact of the size of the dataset and the translation\nmethodology on the performance of the model. Our results indicate that\npreserving logical relationships in the translation process significantly\nboosts precision, with accuracy plateauing beyond roughly 20,000 training\nsamples. These insights provide valuable guidelines for optimizing LLM training\nin formal reasoning tasks and improving performance in various low-resource\nlanguage applications."}
{"id": "2505.00101", "pdf": "https://arxiv.org/pdf/2505.00101.pdf", "abs": "https://arxiv.org/abs/2505.00101", "title": "From Lab to Wrist: Bridging Metabolic Monitoring and Consumer Wearables for Heart Rate and Oxygen Consumption Modeling", "authors": ["Barak Gahtan", "Sanketh Vedula", "Gil Samuelly Leichtag", "Einat Kodesh", "Alex M. Bronstein"], "categories": ["cs.LG", "cs.HC"], "comment": null, "summary": "Understanding physiological responses during running is critical for\nperformance optimization, tailored training prescriptions, and athlete health\nmanagement. We introduce a comprehensive framework -- what we believe to be the\nfirst capable of predicting instantaneous oxygen consumption (VO$_{2}$)\ntrajectories exclusively from consumer-grade wearable data. Our approach\nemploys two complementary physiological models: (1) accurate modeling of heart\nrate (HR) dynamics via a physiologically constrained ordinary differential\nequation (ODE) and neural Kalman filter, trained on over 3 million HR\nobservations, achieving 1-second interval predictions with mean absolute errors\nas low as 2.81\\,bpm (correlation 0.87); and (2) leveraging the principles of\nprecise HR modeling, a novel VO$_{2}$ prediction architecture requiring only\nthe initial second of VO$_{2}$ data for calibration, enabling robust,\nsequence-to-sequence metabolic demand estimation. Despite relying solely on\nsmartwatch and chest-strap data, our method achieves mean absolute percentage\nerrors of approximately 13\\%, effectively capturing rapid physiological\ntransitions and steady-state conditions across diverse running intensities. Our\nsynchronized dataset, complemented by blood lactate measurements, further lays\nthe foundation for future noninvasive metabolic zone identification. By\nembedding physiological constraints within modern machine learning, this\nframework democratizes advanced metabolic monitoring, bridging laboratory-grade\naccuracy and everyday accessibility, thus empowering both elite athletes and\nrecreational fitness enthusiasts."}
{"id": "2505.00002", "pdf": "https://arxiv.org/pdf/2505.00002.pdf", "abs": "https://arxiv.org/abs/2505.00002", "title": "Symbol grounding in computational systems: A paradox of intentions", "authors": ["Vincent C. Müller"], "categories": ["cs.CL"], "comment": null, "summary": "The paper presents a paradoxical feature of computational systems that\nsuggests that computationalism cannot explain symbol grounding. If the mind is\na digital computer, as computationalism claims, then it can be computing either\nover meaningful symbols or over meaningless symbols. If it is computing over\nmeaningful symbols its functioning presupposes the existence of meaningful\nsymbols in the system, i.e. it implies semantic nativism. If the mind is\ncomputing over meaningless symbols, no intentional cognitive processes are\navailable prior to symbol grounding. In this case, no symbol grounding could\ntake place since any grounding presupposes intentional cognitive processes. So,\nwhether computing in the mind is over meaningless or over meaningful symbols,\ncomputationalism implies semantic nativism."}
{"id": "2505.00219", "pdf": "https://arxiv.org/pdf/2505.00219.pdf", "abs": "https://arxiv.org/abs/2505.00219", "title": "Real-Time Brain-Computer Interface Control of Walking Exoskeleton with Bilateral Sensory Feedback", "authors": ["Jeffrey Lim", "Po T. Wang", "Won Joon Sohn", "Derrick Lin", "Shravan Thaploo", "Luke Bashford", "David Bjanes", "Angelica Nguyen", "Hui Gong", "Michelle Armacost", "Susan J. Shaw", "Spencer Kellis", "Brian Lee", "Darrin Lee", "Payam Heydari", "Richard A. Andersen", "Zoran Nenadic", "Charles Y. Liu", "An H. Do"], "categories": ["q-bio.NC", "cs.HC"], "comment": "Main text of pre-print and supplementary information included", "summary": "Invasive brain-computer interface (BCI) technology has demonstrated the\npossibility of restoring brain-controlled walking in paraplegic spinal cord\ninjury patients. However, current implementations of BCI-controlled walking\nstill have significant drawbacks. In particular, prior systems are\nunidirectional and lack sensory feedback for insensate patients, have\nsuboptimal reliance on brain signals from the bilateral arm areas of the motor\ncortex, and depend on external systems for signal processing. Motivated by\nthese shortcomings, this study is the first time a bidirectional brain-computer\ninterface (BDBCI) has demonstrated the restoration of both brain-controlled\nwalking and leg sensory feedback while utilizing the bilateral leg motor and\nsensory cortices. Here, a subject undergoing subdural electrocorticogram\nelectrode implantation for epilepsy surgery evaluation leveraged the leg\nrepresentation areas of the bilateral interhemispheric primary motor and\nsensory cortices to operate a BDBCI with high performance. Although electrode\nimplantation in the interhemispheric region is uncommon, electrodes can be\nsafely implanted in this region to access rich leg motor information and\ndeliver bilateral leg sensory feedback. Finally, we demonstrated that all BDBCI\noperations can be executed on a dedicated, portable embedded system. These\nresults indicate that BDBCIs can potentially provide brain-controlled\nambulation and artificial leg sensation to people with paraplegia after spinal\ncord injury in a manner that emulates full-implantability and is untethered\nfrom any external systems."}
{"id": "2505.00003", "pdf": "https://arxiv.org/pdf/2505.00003.pdf", "abs": "https://arxiv.org/abs/2505.00003", "title": "The Mind in the Machine: A Survey of Incorporating Psychological Theories in LLMs", "authors": ["Zizhou Liu", "Ziwei Gong", "Lin Ai", "Zheng Hui", "Run Chen", "Colin Wayne Leach", "Michelle R. Greene", "Julia Hirschberg"], "categories": ["cs.CL"], "comment": null, "summary": "Psychological insights have long shaped pivotal NLP breakthroughs, including\nthe cognitive underpinnings of attention mechanisms, formative reinforcement\nlearning, and Theory of Mind-inspired social modeling. As Large Language Models\n(LLMs) continue to grow in scale and complexity, there is a rising consensus\nthat psychology is essential for capturing human-like cognition, behavior, and\ninteraction. This paper reviews how psychological theories can inform and\nenhance stages of LLM development, including data, pre-training, post-training,\nand evaluation\\&application. Our survey integrates insights from cognitive,\ndevelopmental, behavioral, social, personality psychology, and\npsycholinguistics. Our analysis highlights current trends and gaps in how\npsychological theories are applied. By examining both cross-domain connections\nand points of tension, we aim to bridge disciplinary divides and promote more\nthoughtful integration of psychology into future NLP research."}
{"id": "2505.00603", "pdf": "https://arxiv.org/pdf/2505.00603.pdf", "abs": "https://arxiv.org/abs/2505.00603", "title": "Can LLMs Help Improve Analogical Reasoning For Strategic Decisions? Experimental Evidence from Humans and GPT-4", "authors": ["Phanish Puranam", "Prothit Sen", "Maciej Workiewicz"], "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "This study investigates whether large language models, specifically GPT4, can\nmatch human capabilities in analogical reasoning within strategic decision\nmaking contexts. Using a novel experimental design involving source to target\nmatching, we find that GPT4 achieves high recall by retrieving all plausible\nanalogies but suffers from low precision, frequently applying incorrect\nanalogies based on superficial similarities. In contrast, human participants\nexhibit high precision but low recall, selecting fewer analogies yet with\nstronger causal alignment. These findings advance theory by identifying\nmatching, the evaluative phase of analogical reasoning, as a distinct step that\nrequires accurate causal mapping beyond simple retrieval. While current LLMs\nare proficient in generating candidate analogies, humans maintain a comparative\nadvantage in recognizing deep structural similarities across domains. Error\nanalysis reveals that AI errors arise from surface level matching, whereas\nhuman errors stem from misinterpretations of causal structure. Taken together,\nthe results suggest a productive division of labor in AI assisted\norganizational decision making where LLMs may serve as broad analogy\ngenerators, while humans act as critical evaluators, applying the most\ncontextually appropriate analogies to strategic problems."}
{"id": "2505.00004", "pdf": "https://arxiv.org/pdf/2505.00004.pdf", "abs": "https://arxiv.org/abs/2505.00004", "title": "LangVAE and LangSpace: Building and Probing for Language Model VAEs", "authors": ["Danilo S. Carvalho", "Yingji Zhang", "Harriet Unsworth", "André Freitas"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present LangVAE, a novel framework for modular construction of variational\nautoencoders (VAEs) on top of pre-trained large language models (LLMs). Such\nlanguage model VAEs can encode the knowledge of their pre-trained components\ninto more compact and semantically disentangled representations. The\nrepresentations obtained in this way can be analysed with the LangVAE companion\nframework: LangSpace, which implements a collection of probing methods, such as\nvector traversal and interpolation, disentanglement measures, and cluster\nvisualisations. LangVAE and LangSpace offer a flexible, efficient and scalable\nway of building and analysing textual representations, with simple integration\nfor models available on the HuggingFace Hub. Additionally, we conducted a set\nof experiments with different encoder and decoder combinations, as well as\nannotated inputs, revealing a wide range of interactions across architectural\nfamilies and sizes w.r.t. generalisation and disentanglement. Our findings\ndemonstrate a promising framework for systematising the experimentation and\nunderstanding of textual representations."}
{"id": "2206.09535", "pdf": "https://arxiv.org/pdf/2206.09535.pdf", "abs": "https://arxiv.org/abs/2206.09535", "title": "Characterizing Human Actions in the Digital Platform by Temporal Context", "authors": ["Akira Matsui", "Emilio Ferrara"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Recent advances in digital platforms generate rich, high-dimensional logs of\nhuman behavior, and machine learning models have helped social scientists\nexplain knowledge accumulation, communication, and information diffusion. Such\nmodels, however, almost always treat behavior as sequences of actions,\nabstracting the inter-temporal information among actions. To close this gap, we\nintroduce a two-scale Action-Timing Context(ATC) framework that jointly embeds\neach action and its time interval. ATC obtains low-dimensional representations\nof actions and characterizes them with inter-temporal information. We provide\nthree applications of ATC to real-world datasets and demonstrate that the\nmethod offers a unified view of human behavior. The presented qualitative\nfindings demonstrate that explicitly modeling inter-temporal context is\nessential for a comprehensive, interpretable understanding of human activity on\ndigital platforms."}
{"id": "2505.00006", "pdf": "https://arxiv.org/pdf/2505.00006.pdf", "abs": "https://arxiv.org/abs/2505.00006", "title": "Toward a digital twin of U.S. Congress", "authors": ["Hayden Helm", "Tianyi Chen", "Harvey McGuinness", "Paige Lee", "Brandon Duderstadt", "Carey E. Priebe"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.SI"], "comment": null, "summary": "In this paper we provide evidence that a virtual model of U.S.\ncongresspersons based on a collection of language models satisfies the\ndefinition of a digital twin. In particular, we introduce and provide\nhigh-level descriptions of a daily-updated dataset that contains every Tweet\nfrom every U.S. congressperson during their respective terms. We demonstrate\nthat a modern language model equipped with congressperson-specific subsets of\nthis data are capable of producing Tweets that are largely indistinguishable\nfrom actual Tweets posted by their physical counterparts. We illustrate how\ngenerated Tweets can be used to predict roll-call vote behaviors and to\nquantify the likelihood of congresspersons crossing party lines, thereby\nassisting stakeholders in allocating resources and potentially impacting\nreal-world legislative dynamics. We conclude with a discussion of the\nlimitations and important extensions of our analysis."}
{"id": "2405.13924", "pdf": "https://arxiv.org/pdf/2405.13924.pdf", "abs": "https://arxiv.org/abs/2405.13924", "title": "Narrative Review of Emotional Expression Support in XR: Psychophysiology of Speech-to-Text Interfaces", "authors": ["Sunday David Ubur", "Denis Gracanin"], "categories": ["cs.HC"], "comment": "Removed a slight repetition in the conclusion", "summary": "This narrative review examines recent advancements, limitations, and research\ngaps in integrating emotional expression into speech-to-text (STT) interfaces\nwithin extended reality (XR) environments. Drawing from 37 peer-reviewed\nstudies published between 2020 and 2024, we synthesized literature across\nmultiple domains, including affective computing, psychophysiology, captioning\ninnovation, and immersive human-computer interaction. Thematic categories\ninclude communication enhancement technologies for Deaf and Hard of Hearing\n(DHH) users, emotive captioning strategies, visual and affective augmentation\nin AR/VR, speech emotion recognition, and the development of empathic systems.\nDespite the growing accessibility of real-time STT tools, such systems largely\nfail to convey affective nuance, limiting the richness of communication for DHH\nusers and other caption consumers. This review highlights emerging approaches\nsuch as animated captions, emojilization, color-coded overlays, and\navatar-based emotion visualization, but finds a persistent gap in real-time\nemotion-aware captioning within immersive XR contexts. We identify key research\nopportunities at the intersection of accessibility, XR, and emotional\nexpression, and propose future directions for the development of\naffect-responsive, user-centered captioning interfaces."}
{"id": "2505.00008", "pdf": "https://arxiv.org/pdf/2505.00008.pdf", "abs": "https://arxiv.org/abs/2505.00008", "title": "A Scoping Review of Natural Language Processing in Addressing Medically Inaccurate Information: Errors, Misinformation, and Hallucination", "authors": ["Zhaoyi Sun", "Wen-Wai Yim", "Ozlem Uzuner", "Fei Xia", "Meliha Yetisgen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Objective: This review aims to explore the potential and challenges of using\nNatural Language Processing (NLP) to detect, correct, and mitigate medically\ninaccurate information, including errors, misinformation, and hallucination. By\nunifying these concepts, the review emphasizes their shared methodological\nfoundations and their distinct implications for healthcare. Our goal is to\nadvance patient safety, improve public health communication, and support the\ndevelopment of more reliable and transparent NLP applications in healthcare.\n  Methods: A scoping review was conducted following PRISMA guidelines,\nanalyzing studies from 2020 to 2024 across five databases. Studies were\nselected based on their use of NLP to address medically inaccurate information\nand were categorized by topic, tasks, document types, datasets, models, and\nevaluation metrics.\n  Results: NLP has shown potential in addressing medically inaccurate\ninformation on the following tasks: (1) error detection (2) error correction\n(3) misinformation detection (4) misinformation correction (5) hallucination\ndetection (6) hallucination mitigation. However, challenges remain with data\nprivacy, context dependency, and evaluation standards.\n  Conclusion: This review highlights the advancements in applying NLP to tackle\nmedically inaccurate information while underscoring the need to address\npersistent challenges. Future efforts should focus on developing real-world\ndatasets, refining contextual methods, and improving hallucination management\nto ensure reliable and transparent healthcare applications."}
{"id": "2501.15877", "pdf": "https://arxiv.org/pdf/2501.15877.pdf", "abs": "https://arxiv.org/abs/2501.15877", "title": "Boli: A dataset for understanding stuttering experience and analyzing stuttered speech", "authors": ["Ashita Batra", "Mannas Narang", "Neeraj Kumar Sharma", "Pradip K Das"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "There is a growing need for diverse, high-quality stuttered speech data,\nparticularly in the context of Indian languages. This paper introduces Project\nBoli, a multi-lingual stuttered speech dataset designed to advance scientific\nunderstanding and technology development for individuals who stutter,\nparticularly in India. The dataset constitutes (a) anonymized metadata (gender,\nage, country, mother tongue) and responses to a questionnaire about how\nstuttering affects their daily lives, (b) captures both read speech (using the\nRainbow Passage) and spontaneous speech (through image description tasks) for\neach participant and (c) includes detailed annotations of five stutter types:\nblocks, prolongations, interjections, sound repetitions and word repetitions.\nWe present a comprehensive analysis of the dataset, including the data\ncollection procedure, experience summarization of people who stutter, severity\nassessment of stuttering events and technical validation of the collected data.\nThe dataset is released as an open access to further speech technology\ndevelopment."}
{"id": "2505.00009", "pdf": "https://arxiv.org/pdf/2505.00009.pdf", "abs": "https://arxiv.org/abs/2505.00009", "title": "Efficient Knowledge Transfer in Multi-Task Learning through Task-Adaptive Low-Rank Representation", "authors": ["Xiao Zhang", "Kangsheng Wang", "Tianyu Hu", "Huimin Ma"], "categories": ["cs.CL"], "comment": "Accepted by IEEE International Conference on Multimedia & Expo 2025", "summary": "Pre-trained language models (PLMs) demonstrate remarkable intelligence but\nstruggle with emerging tasks unseen during training in real-world applications.\nTraining separate models for each new task is usually impractical. Multi-task\nlearning (MTL) addresses this challenge by transferring shared knowledge from\nsource tasks to target tasks. As an dominant parameter-efficient fine-tuning\nmethod, prompt tuning (PT) enhances MTL by introducing an adaptable vector that\ncaptures task-specific knowledge, which acts as a prefix to the original prompt\nthat preserves shared knowledge, while keeping PLM parameters frozen. However,\nPT struggles to effectively capture the heterogeneity of task-specific\nknowledge due to its limited representational capacity. To address this\nchallenge, we propose Task-Adaptive Low-Rank Representation (TA-LoRA), an MTL\nmethod built on PT, employing the low-rank representation to model task\nheterogeneity and a fast-slow weights mechanism where the slow weight encodes\nshared knowledge, while the fast weight captures task-specific nuances,\navoiding the mixing of shared and task-specific knowledge, caused by training\nlow-rank representations from scratch. Moreover, a zero-initialized attention\nmechanism is introduced to minimize the disruption of immature low-rank\ncomponents on original prompts during warm-up epochs. Experiments on 16 tasks\ndemonstrate that TA-LoRA achieves state-of-the-art performance in full-data and\nfew-shot settings while maintaining superior parameter efficiency."}
{"id": "2504.18969", "pdf": "https://arxiv.org/pdf/2504.18969.pdf", "abs": "https://arxiv.org/abs/2504.18969", "title": "Advancing Face-to-Face Emotion Communication: A Multimodal Dataset (AFFEC)", "authors": ["Meisam J. Sekiavandi", "Laurits Dixen", "Jostein Fimland", "Sree Keerthi Desu", "Antonia-Bianca Zserai", "Ye Sul Lee", "Maria Barrett", "Paolo Burelli"], "categories": ["cs.HC"], "comment": null, "summary": "Emotion recognition has the potential to play a pivotal role in enhancing\nhuman-computer interaction by enabling systems to accurately interpret and\nrespond to human affect. Yet, capturing emotions in face-to-face contexts\nremains challenging due to subtle nonverbal cues, variations in personal\ntraits, and the real-time dynamics of genuine interactions. Existing emotion\nrecognition datasets often rely on limited modalities or controlled conditions,\nthereby missing the richness and variability found in real-world scenarios.\n  In this work, we introduce Advancing Face-to-Face Emotion Communication\n(AFFEC), a multimodal dataset designed to address these gaps. AFFEC encompasses\n84 simulated emotional dialogues across six distinct emotions, recorded from 73\nparticipants over more than 5,000 trials and annotated with more than 20,000\nlabels. It integrates electroencephalography (EEG), eye-tracking, galvanic skin\nresponse (GSR), facial videos, and Big Five personality assessments. Crucially,\nAFFEC explicitly distinguishes between felt emotions (the participant's\ninternal affect) and perceived emotions (the observer's interpretation of the\nstimulus).\n  Baseline analyses spanning unimodal features and straightforward multimodal\nfusion demonstrate that even minimal processing yields classification\nperformance significantly above chance, especially for arousal. Incorporating\npersonality traits further improves predictions of felt emotions, highlighting\nthe importance of individual differences. By bridging controlled\nexperimentation with more realistic face-to-face stimuli, AFFEC offers a unique\nresource for researchers aiming to develop context-sensitive, adaptive, and\npersonalized emotion recognition models."}
{"id": "2505.00010", "pdf": "https://arxiv.org/pdf/2505.00010.pdf", "abs": "https://arxiv.org/abs/2505.00010", "title": "Jailbreak Detection in Clinical Training LLMs Using Feature-Based Predictive Models", "authors": ["Tri Nguyen", "Lohith Srikanth Pentapalli", "Magnus Sieverding", "Laurah Turner", "Seth Overla", "Weibing Zheng", "Chris Zhou", "David Furniss", "Danielle Weber", "Michael Gharib", "Matt Kelleher", "Michael Shukis", "Cameron Pawlik", "Kelly Cohen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Jailbreaking in Large Language Models (LLMs) threatens their safe use in\nsensitive domains like education by allowing users to bypass ethical\nsafeguards. This study focuses on detecting jailbreaks in 2-Sigma, a clinical\neducation platform that simulates patient interactions using LLMs. We annotated\nover 2,300 prompts across 158 conversations using four linguistic variables\nshown to correlate strongly with jailbreak behavior. The extracted features\nwere used to train several predictive models, including Decision Trees, Fuzzy\nLogic-based classifiers, Boosting methods, and Logistic Regression. Results\nshow that feature-based predictive models consistently outperformed Prompt\nEngineering, with the Fuzzy Decision Tree achieving the best overall\nperformance. Our findings demonstrate that linguistic-feature-based models are\neffective and explainable alternatives for jailbreak detection. We suggest\nfuture work explore hybrid frameworks that integrate prompt-based flexibility\nwith rule-based robustness for real-time, spectrum-based jailbreak monitoring\nin educational LLMs."}
{"id": "2504.21397", "pdf": "https://arxiv.org/pdf/2504.21397.pdf", "abs": "https://arxiv.org/abs/2504.21397", "title": "Coping with Uncertainty in UX Design Practice: Practitioner Strategies and Judgment", "authors": ["Prakash Shukla", "Phuong Bui", "Paul Parsons"], "categories": ["cs.HC"], "comment": "11 pages, In Creativity and Cognition (C&C '25), June 23--25, 2025,\n  Virtual, United Kingdom", "summary": "The complexity of UX design practice extends beyond ill-structured design\nproblems to include uncertainties shaped by shifting stakeholder priorities,\nteam dynamics, limited resources, and implementation constraints. While prior\nresearch in related fields has addressed uncertainty in design more broadly,\nthe specific character of uncertainty in UX practice remains underexplored.\nThis study examines how UX practitioners experience and respond to uncertainty\nin real-world projects, drawing on a multi-week diary study and follow-up\ninterviews with ten designers. We identify a range of practitioner\nstrategies-including adaptive framing, negotiation, and judgment-that allow\ndesigners to move forward amid ambiguity. Our findings highlight the central\nrole of design judgment in navigating uncertainty, including emergent forms\nsuch as temporal and sacrificial judgment, and extend prior understandings by\nshowing how UX practitioners engage uncertainty as a persistent, situated\nfeature of practice."}
{"id": "2505.00012", "pdf": "https://arxiv.org/pdf/2505.00012.pdf", "abs": "https://arxiv.org/abs/2505.00012", "title": "The AI Co-Ethnographer: How Far Can Automation Take Qualitative Research?", "authors": ["Fabian Retkowski", "Andreas Sudmann", "Alexander Waibel"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to NLP4DH 2025", "summary": "Qualitative research often involves labor-intensive processes that are\ndifficult to scale while preserving analytical depth. This paper introduces The\nAI Co-Ethnographer (AICoE), a novel end-to-end pipeline developed for\nqualitative research and designed to move beyond the limitations of simply\nautomating code assignments, offering a more integrated approach. AICoE\norganizes the entire process, encompassing open coding, code consolidation,\ncode application, and even pattern discovery, leading to a comprehensive\nanalysis of qualitative data."}
{"id": "2402.14410", "pdf": "https://arxiv.org/pdf/2402.14410.pdf", "abs": "https://arxiv.org/abs/2402.14410", "title": "A new sociology of humans and machines", "authors": ["Milena Tsvetkova", "Taha Yasseri", "Niccolo Pescetelli", "Tobias Werner"], "categories": ["cs.SI", "cs.CY", "cs.HC", "physics.soc-ph", "A.1; C.2.4; H.1.2; J.4; K.4.0; K.6.0"], "comment": "42 pages, 2 figures, 1 table", "summary": "From fake social media accounts and generative artificial intelligence\nchatbots to trading algorithms and self-driving vehicles, robots, bots and\nalgorithms are proliferating and permeating our communication channels, social\ninteractions, economic transactions and transportation arteries. Networks of\nmultiple interdependent and interacting humans and intelligent machines\nconstitute complex social systems for which the collective outcomes cannot be\ndeduced from either human or machine behaviour alone. Under this paradigm, we\nreview recent research and identify general dynamics and patterns in situations\nof competition, coordination, cooperation, contagion and collective\ndecision-making, with context-rich examples from high-frequency trading\nmarkets, a social media platform, an open collaboration community and a\ndiscussion forum. To ensure more robust and resilient human-machine\ncommunities, we require a new sociology of humans and machines. Researchers\nshould study these communities using complex system methods; engineers should\nexplicitly design artificial intelligence for human-machine and machine-machine\ninteractions; and regulators should govern the ecological diversity and social\nco-development of humans and machines."}
{"id": "2505.00013", "pdf": "https://arxiv.org/pdf/2505.00013.pdf", "abs": "https://arxiv.org/abs/2505.00013", "title": "Performance Evaluation of Emotion Classification in Japanese Using RoBERTa and DeBERTa", "authors": ["Yoichi Takenaka"], "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 3 tables, 3 appendices. Submitted to New Generation\n  Computing. Includes comparisons between fine-tuned PLMs and LLMs on Japanese\n  emotion classification. Code available at\n  https://pypi.org/project/deberta-emotion-predictor/", "summary": "Background Practical applications such as social media monitoring and\ncustomer-feedback analysis require accurate emotion detection for Japanese\ntext, yet resource scarcity and class imbalance hinder model performance.\n  Objective This study aims to build a high-accuracy model for predicting the\npresence or absence of eight Plutchik emotions in Japanese sentences.\n  Methods Using the WRIME corpus, we transform reader-averaged intensity scores\ninto binary labels and fine-tune four pre-trained language models (BERT,\nRoBERTa, DeBERTa-v3-base, DeBERTa-v3-large). For context, we also assess two\nlarge language models (TinySwallow-1.5B-Instruct and ChatGPT-4o). Accuracy and\nF1-score serve as evaluation metrics.\n  Results DeBERTa-v3-large attains the best mean accuracy (0.860) and F1-score\n(0.662), outperforming all other models. It maintains robust F1 across both\nhigh-frequency emotions (e.g., Joy, Anticipation) and low-frequency emotions\n(e.g., Anger, Trust). The LLMs lag, with ChatGPT-4o and\nTinySwallow-1.5B-Instruct scoring 0.527 and 0.292 in mean F1, respectively.\n  Conclusion The fine-tuned DeBERTa-v3-large model currently offers the most\nreliable solution for binary emotion classification in Japanese. We release\nthis model as a pip-installable package (pip install\ndeberta-emotion-predictor). Future work should augment data for rare emotions,\nreduce model size, and explore prompt engineering to improve LLM performance.\n  This manuscript is under review for possible publication in New Generation\nComputing."}
{"id": "2501.18265", "pdf": "https://arxiv.org/pdf/2501.18265.pdf", "abs": "https://arxiv.org/abs/2501.18265", "title": "Efficiency and Effectiveness of LLM-Based Summarization of Evidence in Crowdsourced Fact-Checking", "authors": ["Kevin Roitero", "Dustin Wright", "Michael Soprano", "Isabelle Augenstein", "Stefano Mizzaro"], "categories": ["cs.IR", "cs.CL", "cs.HC"], "comment": "19 pages; 7 figures; 5 tables", "summary": "Evaluating the truthfulness of online content is critical for combating\nmisinformation. This study examines the efficiency and effectiveness of\ncrowdsourced truthfulness assessments through a comparative analysis of two\napproaches: one involving full-length webpages as evidence for each claim, and\nanother using summaries for each evidence document generated with a large\nlanguage model. Using an A/B testing setting, we engage a diverse pool of\nparticipants tasked with evaluating the truthfulness of statements under these\nconditions. Our analysis explores both the quality of assessments and the\nbehavioral patterns of participants. The results reveal that relying on\nsummarized evidence offers comparable accuracy and error metrics to the\nStandard modality while significantly improving efficiency. Workers in the\nSummary setting complete a significantly higher number of assessments, reducing\ntask duration and costs. Additionally, the Summary modality maximizes internal\nagreement and maintains consistent reliance on and perceived usefulness of\nevidence, demonstrating its potential to streamline large-scale truthfulness\nevaluations."}
{"id": "2505.00014", "pdf": "https://arxiv.org/pdf/2505.00014.pdf", "abs": "https://arxiv.org/abs/2505.00014", "title": "Manifold-Constrained Sentence Embeddings via Triplet Loss: Projecting Semantics onto Spheres, Tori, and Möbius Strips", "authors": ["Vinit K. Chavan"], "categories": ["cs.CL"], "comment": "10 pages, 6 figures. Code available at\n  https://github.com/vinitchavan/manifold-embedding-nlp", "summary": "Recent advances in representation learning have emphasized the role of\nembedding geometry in capturing semantic structure. Traditional sentence\nembeddings typically reside in unconstrained Euclidean spaces, which may limit\ntheir ability to reflect complex relationships in language. In this work, we\nintroduce a novel framework that constrains sentence embeddings to lie on\ncontinuous manifolds -- specifically the unit sphere, torus, and M\\\"obius strip\n-- using triplet loss as the core training objective. By enforcing differential\ngeometric constraints on the output space, our approach encourages the learning\nof embeddings that are both discriminative and topologically structured.\n  We evaluate our method on benchmark datasets (AG News and MBTI) and compare\nit to classical baselines including TF-IDF, Word2Vec, and unconstrained\nKeras-derived embeddings. Our results demonstrate that manifold-constrained\nembeddings, particularly those projected onto spheres and M\\\"obius strips,\nsignificantly outperform traditional approaches in both clustering quality\n(Silhouette Score) and classification performance (Accuracy). These findings\nhighlight the value of embedding in manifold space -- where topological\nstructure complements semantic separation -- offering a new and mathematically\ngrounded direction for geometric representation learning in NLP."}
{"id": "2504.21800", "pdf": "https://arxiv.org/pdf/2504.21800.pdf", "abs": "https://arxiv.org/abs/2504.21800", "title": "How Real Are Synthetic Therapy Conversations? Evaluating Fidelity in Prolonged Exposure Dialogues", "authors": ["Suhas BN", "Dominik Mattioli", "Saeed Abdullah", "Rosa I. Arriaga", "Chris W. Wiese", "Andrew M. Sherrill"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "68T50", "I.2.7; H.3.1"], "comment": "11 pages, 5 tables", "summary": "The growing adoption of synthetic data in healthcare is driven by privacy\nconcerns, limited access to real-world data, and the high cost of annotation.\nThis work explores the use of synthetic Prolonged Exposure (PE) therapeutic\nconversations for Post-Traumatic Stress Disorder (PTSD) as a scalable\nalternative for training and evaluating clinical models. We systematically\ncompare real and synthetic dialogues using linguistic, structural, and\nprotocol-specific metrics, including turn-taking patterns and treatment\nfidelity. We also introduce and evaluate PE-specific metrics derived from\nlinguistic analysis and semantic modeling, offering a novel framework for\nassessing clinical fidelity beyond surface fluency. Our findings show that\nalthough synthetic data holds promise for mitigating data scarcity and\nprotecting patient privacy, it can struggle to capture the subtle dynamics of\ntherapeutic interactions. Synthetic therapy dialogues closely match structural\nfeatures of real-world conversations (e.g., speaker switch ratio: 0.98 vs.\n0.99); however, they may not adequately reflect key fidelity markers (e.g.,\ndistress monitoring). We highlight gaps in existing evaluation frameworks and\nadvocate for fidelity-aware metrics that go beyond surface fluency to uncover\nclinically significant failures. Our findings clarify where synthetic data can\neffectively complement real-world datasets -- and where critical limitations\nremain."}
{"id": "2505.00015", "pdf": "https://arxiv.org/pdf/2505.00015.pdf", "abs": "https://arxiv.org/abs/2505.00015", "title": "Design and Application of Multimodal Large Language Model Based System for End to End Automation of Accident Dataset Generation", "authors": ["MD Thamed Bin Zaman Chowdhury", "Moazzem Hossain"], "categories": ["cs.CL"], "comment": "Shortened the abstract to fit within 1920 characters. This paper is\n  currently under Review in Elsevier journal 'Accident Analysis & Prevention'", "summary": "Road traffic accidents remain a major public safety and socio-economic issue\nin developing countries like Bangladesh. Existing accident data collection is\nlargely manual, fragmented, and unreliable, resulting in underreporting and\ninconsistent records. This research proposes a fully automated system using\nLarge Language Models (LLMs) and web scraping techniques to address these\nchallenges. The pipeline consists of four components: automated web scraping\ncode generation, news collection from online sources, accident news\nclassification with structured data extraction, and duplicate removal. The\nsystem uses the multimodal generative LLM Gemini-2.0-Flash for seamless\nautomation. The code generation module classifies webpages into pagination,\ndynamic, or infinite scrolling categories and generates suitable Python scripts\nfor scraping. LLMs also classify and extract key accident information such as\ndate, time, location, fatalities, injuries, road type, vehicle types, and\npedestrian involvement. A deduplication algorithm ensures data integrity by\nremoving duplicate reports. The system scraped 14 major Bangladeshi news sites\nover 111 days (Oct 1, 2024 - Jan 20, 2025), processing over 15,000 news\narticles and identifying 705 unique accidents. The code generation module\nachieved 91.3% calibration and 80% validation accuracy. Chittagong reported the\nhighest number of accidents (80), fatalities (70), and injuries (115), followed\nby Dhaka, Faridpur, Gazipur, and Cox's Bazar. Peak accident times were morning\n(8-9 AM), noon (12-1 PM), and evening (6-7 PM). A public repository was also\ndeveloped with usage instructions. This study demonstrates the viability of an\nLLM-powered, scalable system for accurate, low-effort accident data collection,\nproviding a foundation for data-driven road safety policymaking in Bangladesh."}
{"id": "2505.00016", "pdf": "https://arxiv.org/pdf/2505.00016.pdf", "abs": "https://arxiv.org/abs/2505.00016", "title": "Sparks of Tabular Reasoning via Text2SQL Reinforcement Learning", "authors": ["Josefa Lia Stoisser", "Marc Boubnovski Martell", "Julien Fauqueur"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This work reframes the Text-to-SQL task as a pathway for teaching large\nlanguage models (LLMs) to reason over and manipulate tabular data--moving\nbeyond the traditional focus on query generation. We propose a two-stage\nframework that leverages SQL supervision to develop transferable table\nreasoning capabilities. First, we synthesize detailed chain-of-thought (CoT)\ntraces from real-world SQL queries, providing step-by-step, clause-level\nsupervision that teaches the model how to traverse, filter, and aggregate table\nfields. Second, we introduce a Group Relative Policy Optimization (GRPO)\nreinforcement learning objective that connects SQL execution accuracy to\ngeneralizable reasoning by encouraging steps that extend beyond task-specific\nsyntax and transfer across datasets. Empirically, our approach improves\nperformance on standard Text-to-SQL benchmarks and achieves substantial gains\non reasoning-intensive datasets such as BIRD and CRT-QA, demonstrating enhanced\ngeneralization and interpretability. Specifically, the distilled-quantized\nLLaMA model achieved a 20\\% increase in accuracy when trained on Text-to-SQL\ntasks, while Qwen achieved a 5\\% increase. These results suggest that SQL can\nserve not only as a target formalism but also as an effective scaffold for\nlearning robust, transferable reasoning over structured data."}
{"id": "2505.00017", "pdf": "https://arxiv.org/pdf/2505.00017.pdf", "abs": "https://arxiv.org/abs/2505.00017", "title": "ReCellTy: Domain-specific knowledge graph retrieval-augmented LLMs workflow for single-cell annotation", "authors": ["Dezheng Han", "Yibin Jia", "Ruxiao Chen", "Wenjie Han", "Shuaishuai Guo", "Jianbo Wang"], "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.LG"], "comment": null, "summary": "To enable precise and fully automated cell type annotation with large\nlanguage models (LLMs), we developed a graph structured feature marker database\nto retrieve entities linked to differential genes for cell reconstruction. We\nfurther designed a multi task workflow to optimize the annotation process.\nCompared to general purpose LLMs, our method improves human evaluation scores\nby up to 0.21 and semantic similarity by 6.1% across 11 tissue types, while\nmore closely aligning with the cognitive logic of manual annotation."}
{"id": "2505.00019", "pdf": "https://arxiv.org/pdf/2505.00019.pdf", "abs": "https://arxiv.org/abs/2505.00019", "title": "An Empirical Study on Prompt Compression for Large Language Models", "authors": ["Zheng Zhang", "Jinyi Li", "Yihuai Lan", "Xiang Wang", "Hao Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by Building Trust Workshop at ICLR 2025", "summary": "Prompt engineering enables Large Language Models (LLMs) to perform a variety\nof tasks. However, lengthy prompts significantly increase computational\ncomplexity and economic costs. To address this issue, we study six prompt\ncompression methods for LLMs, aiming to reduce prompt length while maintaining\nLLM response quality. In this paper, we present a comprehensive analysis\ncovering aspects such as generation performance, model hallucinations, efficacy\nin multimodal tasks, word omission analysis, and more. We evaluate these\nmethods across 13 datasets, including news, scientific articles, commonsense\nQA, math QA, long-context QA, and VQA datasets. Our experiments reveal that\nprompt compression has a greater impact on LLM performance in long contexts\ncompared to short ones. In the Longbench evaluation, moderate compression even\nenhances LLM performance. Our code and data is available at\nhttps://github.com/3DAgentWorld/Toolkit-for-Prompt-Compression."}
{"id": "2505.00020", "pdf": "https://arxiv.org/pdf/2505.00020.pdf", "abs": "https://arxiv.org/abs/2505.00020", "title": "Beyond Public Access in LLM Pre-Training Data", "authors": ["Sruly Rosenblat", "Tim O'Reilly", "Ilan Strauss"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Using a legally obtained dataset of 34 copyrighted O'Reilly Media books, we\napply the DE-COP membership inference attack method to investigate whether\nOpenAI's large language models were trained on copyrighted content without\nconsent. Our AUROC scores show that GPT-4o, OpenAI's more recent and capable\nmodel, demonstrates strong recognition of paywalled O'Reilly book content\n(AUROC = 82\\%), compared to OpenAI's earlier model GPT-3.5 Turbo. In contrast,\nGPT-3.5 Turbo shows greater relative recognition of publicly accessible\nO'Reilly book samples. GPT-4o Mini, as a much smaller model, shows no knowledge\nof public or non-public O'Reilly Media content when tested (AUROC $\\approx$\n50\\%). Testing multiple models, with the same cutoff date, helps us account for\npotential language shifts over time that might bias our findings. These results\nhighlight the urgent need for increased corporate transparency regarding\npre-training data sources as a means to develop formal licensing frameworks for\nAI content training"}
{"id": "2505.00021", "pdf": "https://arxiv.org/pdf/2505.00021.pdf", "abs": "https://arxiv.org/abs/2505.00021", "title": "Ustnlp16 at SemEval-2025 Task 9: Improving Model Performance through Imbalance Handling and Focal Loss", "authors": ["Zhuoang Cai", "Zhenghao Li", "Yang Liu", "Liyuan Guo", "Yangqiu Song"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Classification tasks often suffer from imbal- anced data distribution, which\npresents chal- lenges in food hazard detection due to severe class imbalances,\nshort and unstructured text, and overlapping semantic categories. In this\npaper, we present our system for SemEval- 2025 Task 9: Food Hazard Detection,\nwhich ad- dresses these issues by applying data augmenta- tion techniques to\nimprove classification perfor- mance. We utilize transformer-based models, BERT\nand RoBERTa, as backbone classifiers and explore various data balancing\nstrategies, including random oversampling, Easy Data Augmentation (EDA), and\nfocal loss. Our ex- periments show that EDA effectively mitigates class\nimbalance, leading to significant improve- ments in accuracy and F1 scores.\nFurthermore, combining focal loss with oversampling and EDA further enhances\nmodel robustness, par- ticularly for hard-to-classify examples. These findings\ncontribute to the development of more effective NLP-based classification models\nfor food hazard detection."}
{"id": "2505.00022", "pdf": "https://arxiv.org/pdf/2505.00022.pdf", "abs": "https://arxiv.org/abs/2505.00022", "title": "Aleph-Alpha-GermanWeb: Improving German-language LLM pre-training with model-based data curation and synthetic data generation", "authors": ["Thomas F Burns", "Letitia Parcalabescu", "Stephan Wäldchen", "Michael Barlow", "Gregor Ziegltrum", "Volker Stampa", "Bastian Harren", "Björn Deiseroth"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "10 pages, 3 figures", "summary": "Scaling data quantity is essential for large language models (LLMs), yet\nrecent findings show that data quality can significantly boost performance and\ntraining efficiency. We introduce a German-language dataset curation pipeline\nthat combines heuristic and model-based filtering techniques with synthetic\ndata generation. We use our pipeline to create Aleph-Alpha-GermanWeb, a\nlarge-scale German pre-training dataset which draws from: (1) Common Crawl web\ndata, (2) FineWeb2, and (3) synthetically-generated data conditioned on actual,\norganic web data. We evaluate our dataset by pre-training both a 1B Llama-style\nmodel and an 8B tokenizer-free hierarchical autoregressive transformer (HAT). A\ncomparison on German-language benchmarks, including MMMLU, shows significant\nperformance gains of Aleph-Alpha-GermanWeb over FineWeb2 alone. This advantage\nholds at the 8B scale even when FineWeb2 is enriched by human-curated\nhigh-quality data sources such as Wikipedia. Our findings support the growing\nbody of evidence that model-based data curation and synthetic data generation\ncan significantly enhance LLM pre-training datasets."}
{"id": "2505.00023", "pdf": "https://arxiv.org/pdf/2505.00023.pdf", "abs": "https://arxiv.org/abs/2505.00023", "title": "CORG: Generating Answers from Complex, Interrelated Contexts", "authors": ["Hyunji Lee", "Franck Dernoncourt", "Trung Bui", "Seunghyun Yoon"], "categories": ["cs.CL", "cs.AI"], "comment": "published at Findings of NAACL 2025", "summary": "In a real-world corpus, knowledge frequently recurs across documents but\noften contains inconsistencies due to ambiguous naming, outdated information,\nor errors, leading to complex interrelationships between contexts. Previous\nresearch has shown that language models struggle with these complexities,\ntypically focusing on single factors in isolation. We classify these\nrelationships into four types: distracting, ambiguous, counterfactual, and\nduplicated. Our analysis reveals that no single approach effectively addresses\nall these interrelationships simultaneously. Therefore, we introduce Context\nOrganizer (CORG), a framework that organizes multiple contexts into\nindependently processed groups. This design allows the model to efficiently\nfind all relevant answers while ensuring disambiguation. CORG consists of three\nkey components: a graph constructor, a reranker, and an aggregator. Our results\ndemonstrate that CORG balances performance and efficiency effectively,\noutperforming existing grouping methods and achieving comparable results to\nmore computationally intensive, single-context approaches."}
{"id": "2505.00024", "pdf": "https://arxiv.org/pdf/2505.00024.pdf", "abs": "https://arxiv.org/abs/2505.00024", "title": "Nemotron-Research-Tool-N1: Tool-Using Language Models with Reinforced Reasoning", "authors": ["Shaokun Zhang", "Yi Dong", "Jieyu Zhang", "Jan Kautz", "Bryan Catanzaro", "Andrew Tao", "Qingyun Wu", "Zhiding Yu", "Guilin Liu"], "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 4 tables, 5 figures", "summary": "Enabling large language models with external tools has become a pivotal\nstrategy for extending their functionality beyond text generation tasks. Prior\nwork typically enhances tool-use abilities by either applying supervised\nfine-tuning (SFT) to enforce tool-call correctness or distilling reasoning\ntraces from stronger models for SFT. However, both approaches fall short,\neither omitting reasoning entirely or producing imitative reasoning that limits\ngeneralization. Inspired by the success of DeepSeek-R1 in eliciting reasoning\nthrough rule-based reinforcement learning, we develop the\nNemotron-Research-Tool-N1 series of tool-using language models using a similar\ntraining paradigm. Instead of restrictively supervising intermediate reasoning\ntraces distilled from stronger models, Nemotron-Research-Tool-N1 is optimized\nwith a binary reward that evaluates only the structural validity and functional\ncorrectness of tool invocations. This lightweight supervision allows the model\nto autonomously internalize reasoning strategies, without the need for\nannotated reasoning trajectories. Experiments on the BFCL and API-Bank\nbenchmarks show that Nemotron-Research-Tool-N1-7B and\nNemotron-Research-Tool-N1-14B, built on Qwen-2.5-7B/14B-Instruct, achieve\nstate-of-the-art results, outperforming GPT-4o on both evaluations."}
{"id": "2505.00025", "pdf": "https://arxiv.org/pdf/2505.00025.pdf", "abs": "https://arxiv.org/abs/2505.00025", "title": "A Method for the Architecture of a Medical Vertical Large Language Model Based on Deepseek R1", "authors": ["Mingda Zhang", "Jianglong Qin"], "categories": ["cs.CL", "cs.AI", "I.2.7; J.3"], "comment": "14 pages, 1 figures", "summary": "In recent years, despite foundation models like DeepSeek-R1 and ChatGPT\ndemonstrating significant capabilities in general tasks, professional knowledge\nbarriers, computational resource requirements, and deployment environment\nlimitations have severely hindered their application in actual medical\nscenarios. Addressing these challenges, this paper proposes an efficient\nlightweight medical vertical large language model architecture method,\nsystematically solving the lightweight problem of medical large models from\nthree dimensions: knowledge acquisition, model compression, and computational\noptimization. At the knowledge acquisition level, a knowledge transfer pipeline\nis designed from the fine-tuned DeepSeek-R1-Distill-70B teacher model to the\nDeepSeek-R1-Distill-7B student model, and Low-Rank Adaptation (LoRA) technology\nis adopted to precisely adjust key attention layers. At the model compression\nlevel, compression techniques including 4-bit weight quantization are\nimplemented while preserving the core representation ability for medical\nreasoning. At the computational optimization level, inference optimization\ntechniques such as Flash Attention acceleration and continuous batching are\nintegrated, and a professional prompt template system is constructed to adapt\nto different types of medical problems. Experimental results on medical\nquestion-answering datasets show that the method proposed in this paper\nmaintains professional accuracy while reducing memory consumption by 64.7\\% and\ninference latency by 12.4\\%, providing an effective solution for the\napplication of medical large models in resource-constrained environments such\nas edge computing devices."}
{"id": "2505.00026", "pdf": "https://arxiv.org/pdf/2505.00026.pdf", "abs": "https://arxiv.org/abs/2505.00026", "title": "Theory of Mind in Large Language Models: Assessment and Enhancement", "authors": ["Ruirui Chen", "Weifeng Jiang", "Chengwei Qin", "Cheston Tan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Theory of Mind (ToM)-the ability to infer and reason about others' mental\nstates-is fundamental to human social intelligence. As Large Language Models\n(LLMs) become increasingly integrated into daily life, it is crucial to assess\nand enhance their capacity to interpret and respond to human mental states. In\nthis paper, we review LLMs' ToM capabilities by examining both evaluation\nbenchmarks and the strategies designed to improve them. We focus on widely\nadopted story-based benchmarks and provide an in-depth analysis of methods\naimed at enhancing ToM in LLMs. Furthermore, we outline promising future\nresearch directions informed by recent benchmarks and state-of-the-art\napproaches. Our survey serves as a valuable resource for researchers interested\nin advancing LLMs' ToM capabilities."}
{"id": "2505.00027", "pdf": "https://arxiv.org/pdf/2505.00027.pdf", "abs": "https://arxiv.org/abs/2505.00027", "title": "Extracting Abstraction Dimensions by Identifying Syntax Pattern from Texts", "authors": ["Jian Zhou", "Jiazheng Li", "Sirui Zhuge", "Hai Zhuge"], "categories": ["cs.CL", "cs.AI", "68T50 (Primary) 91F20 (Secondary)", "I.2.7; I.2.1"], "comment": "25pages, 3 figures, 8 tables", "summary": "This paper proposed an approach to automatically discovering subject\ndimension, action dimension, object dimension and adverbial dimension from\ntexts to efficiently operate texts and support query in natural language. The\nhigh quality of trees guarantees that all subjects, actions, objects and\nadverbials and their subclass relations within texts can be represented. The\nindependency of trees ensures that there is no redundant representation between\ntrees. The expressiveness of trees ensures that the majority of sentences can\nbe accessed from each tree and the rest of sentences can be accessed from at\nleast one tree so that the tree-based search mechanism can support querying in\nnatural language. Experiments show that the average precision, recall and\nF1-score of the abstraction trees constructed by the subclass relations of\nsubject, action, object and adverbial are all greater than 80%. The application\nof the proposed approach to supporting query in natural language demonstrates\nthat different types of question patterns for querying subject or object have\nhigh coverage of texts, and searching multiple trees on subject, action, object\nand adverbial according to the question pattern can quickly reduce search space\nto locate target sentences, which can support precise operation on texts."}
{"id": "2505.00028", "pdf": "https://arxiv.org/pdf/2505.00028.pdf", "abs": "https://arxiv.org/abs/2505.00028", "title": "Enhancing Speech-to-Speech Dialogue Modeling with End-to-End Retrieval-Augmented Generation", "authors": ["Pengchao Feng", "Ziyang Ma", "Wenxi Chen", "Yao Li", "Sheng Wang", "Kai Yu", "Xie Chen"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "In recent years, end-to-end speech-to-speech (S2S) dialogue systems have\ngarnered increasing research attention due to their advantages over traditional\ncascaded systems, including achieving lower latency and more natural\nintegration of nonverbal cues such as emotion and speaker identity. However,\nthese end-to-end systems face key challenges, particularly in incorporating\nexternal knowledge, a capability commonly addressed by Retrieval-Augmented\nGeneration (RAG) in text-based large language models (LLMs). The core\ndifficulty lies in the modality gap between input speech and retrieved textual\nknowledge, which hinders effective integration. To address this issue, we\npropose a novel end-to-end RAG framework that directly retrieves relevant\ntextual knowledge from speech queries, eliminating the need for intermediate\nspeech-to-text conversion via techniques like ASR. Experimental results\ndemonstrate that our method significantly improves the performance of\nend-to-end S2S dialogue systems while achieving higher retrieval efficiency.\nAlthough the overall performance still lags behind cascaded models, our\nframework offers a promising direction for enhancing knowledge integration in\nend-to-end S2S systems. We will release the code and dataset to support\nreproducibility and promote further research in this area."}
{"id": "2505.00029", "pdf": "https://arxiv.org/pdf/2505.00029.pdf", "abs": "https://arxiv.org/abs/2505.00029", "title": "Keep the General, Inject the Specific: Structured Dialogue Fine-Tuning for Knowledge Injection without Catastrophic Forgetting", "authors": ["Yijie Hong", "Xiaofei Yin", "Xinzhong Wang", "Yi Tu", "Ya Guo", "Sufeng Duan", "Weiqiang Wang", "Lingyong Fang", "Depeng Wang", "Huijia Zhu"], "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 3 figures", "summary": "Large Vision Language Models have demonstrated impressive versatile\ncapabilities through extensive multimodal pre-training, but face significant\nlimitations when incorporating specialized knowledge domains beyond their\ntraining distribution. These models struggle with a fundamental dilemma: direct\nadaptation approaches that inject domain-specific knowledge often trigger\ncatastrophic forgetting of foundational visual-linguistic abilities. We\nintroduce Structured Dialogue Fine-Tuning (SDFT), an effective approach that\neffectively injects domain-specific knowledge while minimizing catastrophic\nforgetting. Drawing inspiration from supervised fine-tuning in LLMs and\nsubject-driven personalization in text-to-image diffusion models, our method\nemploys a three-phase dialogue structure: Foundation Preservation reinforces\npre-trained visual-linguistic alignment through caption tasks; Contrastive\nDisambiguation introduces carefully designed counterfactual examples to\nmaintain semantic boundaries; and Knowledge Specialization embeds specialized\ninformation through chain-of-thought reasoning. Experimental results across\nmultiple domains confirm SDFT's effectiveness in balancing specialized\nknowledge acquisition with general capability retention. Our key contributions\ninclude a data-centric dialogue template that balances foundational alignment\nwith targeted knowledge integration, a weighted multi-turn supervision\nframework, and comprehensive evaluation across diverse knowledge types."}
{"id": "2505.00030", "pdf": "https://arxiv.org/pdf/2505.00030.pdf", "abs": "https://arxiv.org/abs/2505.00030", "title": "Can Language Models Represent the Past without Anachronism?", "authors": ["Ted Underwood", "Laura K. Nelson", "Matthew Wilkens"], "categories": ["cs.CL"], "comment": null, "summary": "Before researchers can use language models to simulate the past, they need to\nunderstand the risk of anachronism. We find that prompting a contemporary model\nwith examples of period prose does not produce output consistent with period\nstyle. Fine-tuning produces results that are stylistically convincing enough to\nfool an automated judge, but human evaluators can still distinguish fine-tuned\nmodel outputs from authentic historical text. We tentatively conclude that\npretraining on period prose may be required in order to reliably simulate\nhistorical perspectives for social research."}
{"id": "2505.00031", "pdf": "https://arxiv.org/pdf/2505.00031.pdf", "abs": "https://arxiv.org/abs/2505.00031", "title": "Learning to Plan Before Answering: Self-Teaching LLMs to Learn Abstract Plans for Problem Solving", "authors": ["Jin Zhang", "Flood Sung", "Zhilin Yang", "Yang Gao", "Chongjie Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In the field of large language model (LLM) post-training, the effectiveness\nof utilizing synthetic data generated by the LLM itself has been\nwell-presented. However, a key question remains unaddressed: what essential\ninformation should such self-generated data encapsulate? Existing approaches\nonly produce step-by-step problem solutions, and fail to capture the abstract\nmeta-knowledge necessary for generalization across similar problems. Drawing\ninsights from cognitive science, where humans employ high-level abstraction to\nsimplify complex problems before delving into specifics, we introduce a novel\nself-training algorithm: LEarning to Plan before Answering (LEPA). LEPA trains\nthe LLM to formulate anticipatory plans, which serve as abstract meta-knowledge\nfor problem-solving, before engaging with the intricacies of problems. This\napproach not only outlines the solution generation path but also shields the\nLLM from the distraction of irrelevant details. During data generation, LEPA\nfirst crafts an anticipatory plan based on the problem, and then generates a\nsolution that aligns with both the plan and the problem. LEPA refines the plan\nthrough self-reflection, aiming to acquire plans that are instrumental in\nyielding correct solutions. During model optimization, the LLM is trained to\npredict both the refined plans and the corresponding solutions. By efficiently\nextracting and utilizing the anticipatory plans, LEPA demonstrates remarkable\nsuperiority over conventional algorithms on various challenging natural\nlanguage reasoning benchmarks."}
{"id": "2505.00032", "pdf": "https://arxiv.org/pdf/2505.00032.pdf", "abs": "https://arxiv.org/abs/2505.00032", "title": "MDD-LLM: Towards Accuracy Large Language Models for Major Depressive Disorder Diagnosis", "authors": ["Yuyang Sha", "Hongxin Pan", "Wei Xu", "Weiyu Meng", "Gang Luo", "Xinyu Du", "Xiaobing Zhai", "Henry H. Y. Tong", "Caijuan Shi", "Kefeng Li"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Major depressive disorder (MDD) impacts more than 300 million people\nworldwide, highlighting a significant public health issue. However, the uneven\ndistribution of medical resources and the complexity of diagnostic methods have\nresulted in inadequate attention to this disorder in numerous countries and\nregions. This paper introduces a high-performance MDD diagnosis tool named\nMDD-LLM, an AI-driven framework that utilizes fine-tuned large language models\n(LLMs) and extensive real-world samples to tackle challenges in MDD diagnosis.\nTherefore, we select 274,348 individual information from the UK Biobank cohort\nto train and evaluate the proposed method. Specifically, we select 274,348\nindividual records from the UK Biobank cohort and design a tabular data\ntransformation method to create a large corpus for training and evaluating the\nproposed approach. To illustrate the advantages of MDD-LLM, we perform\ncomprehensive experiments and provide several comparative analyses against\nexisting model-based solutions across multiple evaluation metrics. Experimental\nresults show that MDD-LLM (70B) achieves an accuracy of 0.8378 and an AUC of\n0.8919 (95% CI: 0.8799 - 0.9040), significantly outperforming existing machine\nlearning and deep learning frameworks for MDD diagnosis. Given the limited\nexploration of LLMs in MDD diagnosis, we examine numerous factors that may\ninfluence the performance of our proposed method, such as tabular data\ntransformation techniques and different fine-tuning strategies."}
{"id": "2505.00033", "pdf": "https://arxiv.org/pdf/2505.00033.pdf", "abs": "https://arxiv.org/abs/2505.00033", "title": "From Attention to Atoms: Spectral Dictionary Learning for Fast, Interpretable Language Models", "authors": ["Andrew Kiruluta"], "categories": ["cs.CL"], "comment": null, "summary": "We propose a novel spectral generative modeling framework for natural\nlanguage processing that jointly learns a global time varying Fourier\ndictionary and per token mixing coefficients, replacing the ubiquitous self\nattention mechanism in transformer architectures. By enforcing reconstruction\nlosses in both the time domain (embedding reconstruction) and the frequency\ndomain (via Short Time Fourier Transform magnitude matching) alongside a\nstandard language modeling objective, and fitting a Gaussian Mixture Model\n(GMM) prior over the learned mixing vectors, our approach achieves competitive\nperplexity and generation quality on standard benchmarks such as WikiText2 and\nPenn Treebank. In contrast to the quadratic computation complexity of self\nattention, our method operates with linear complexity, delivering substantial\nefficiency gains. We demonstrate that spectral dictionary models can achieve\ncompetitive performance compared to transformer baselines while significantly\nreducing inference latency and memory footprint, offering a compelling\nalternative for scalable language modeling."}
{"id": "2505.00034", "pdf": "https://arxiv.org/pdf/2505.00034.pdf", "abs": "https://arxiv.org/abs/2505.00034", "title": "Improving Phishing Email Detection Performance of Small Large Language Models", "authors": ["Zijie Lin", "Zikang Liu", "Hanbo Fan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models(LLMs) have demonstrated remarkable performance on many\nnatural language processing(NLP) tasks and have been employed in phishing email\ndetection research. However, in current studies, well-performing LLMs typically\ncontain billions or even tens of billions of parameters, requiring enormous\ncomputational resources. To reduce computational costs, we investigated the\neffectiveness of small-parameter LLMs for phishing email detection. These LLMs\nhave around 3 billion parameters and can run on consumer-grade GPUs. However,\nsmall LLMs often perform poorly in phishing email detection task. To address\nthese issues, we designed a set of methods including Prompt Engineering,\nExplanation Augmented Fine-tuning, and Model Ensemble to improve phishing email\ndetection capabilities of small LLMs. We validated the effectiveness of our\napproach through experiments, significantly improving accuracy on the\nSpamAssassin dataset from around 0.5 for baseline models like\nQwen2.5-1.5B-Instruct to 0.976."}
{"id": "2505.00035", "pdf": "https://arxiv.org/pdf/2505.00035.pdf", "abs": "https://arxiv.org/abs/2505.00035", "title": "Linguistic Complexity and Socio-cultural Patterns in Hip-Hop Lyrics", "authors": ["Aayam Bansal", "Raghav Agarwal", "Kaashvi Jain"], "categories": ["cs.CL", "cs.AI"], "comment": "12 pages", "summary": "This paper presents a comprehensive computational framework for analyzing\nlinguistic complexity and socio-cultural trends in hip-hop lyrics. Using a\ndataset of 3,814 songs from 146 influential artists spanning four decades\n(1980-2020), we employ natural language processing techniques to quantify\nmultiple dimensions of lyrical complexity. Our analysis reveals a 23.7%\nincrease in vocabulary diversity over the study period, with East Coast artists\ndemonstrating 17.3% higher lexical variation than other regions. Rhyme density\nincreased by 34.2% across all regions, with Midwest artists exhibiting the\nhighest technical complexity (3.04 rhymes per line). Topic modeling identified\nsignificant shifts in thematic content, with social justice themes decreasing\nfrom 28.5% to 13.8% of content while introspective themes increased from 7.6%\nto 26.3%. Sentiment analysis demon- strated that lyrics became significantly\nmore negative during sociopolitical crises, with polarity decreasing by 0.31\nfollowing major social unrest. Multi-dimensional analysis revealed four dis-\ntinct stylistic approaches that correlate strongly with geographic origin\n(r=0.68, p!0.001) and time period (r=0.59, p<0.001). These findings establish\nquantitative evidence for the evolution of hip- hop as both an art form and a\nreflection of societal dynamics, providing insights into the interplay between\nlinguistic innovation and cultural context in popular music."}
{"id": "2505.00036", "pdf": "https://arxiv.org/pdf/2505.00036.pdf", "abs": "https://arxiv.org/abs/2505.00036", "title": "A Framework to Assess the Persuasion Risks Large Language Model Chatbots Pose to Democratic Societies", "authors": ["Zhongren Chen", "Joshua Kalla", "Quan Le", "Shinpei Nakamura-Sakai", "Jasjeet Sekhon", "Ruixiao Wang"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "In recent years, significant concern has emerged regarding the potential\nthreat that Large Language Models (LLMs) pose to democratic societies through\ntheir persuasive capabilities. We expand upon existing research by conducting\ntwo survey experiments and a real-world simulation exercise to determine\nwhether it is more cost effective to persuade a large number of voters using\nLLM chatbots compared to standard political campaign practice, taking into\naccount both the \"receive\" and \"accept\" steps in the persuasion process (Zaller\n1992). These experiments improve upon previous work by assessing extended\ninteractions between humans and LLMs (instead of using single-shot\ninteractions) and by assessing both short- and long-run persuasive effects\n(rather than simply asking users to rate the persuasiveness of LLM-produced\ncontent). In two survey experiments (N = 10,417) across three distinct\npolitical domains, we find that while LLMs are about as persuasive as actual\ncampaign ads once voters are exposed to them, political persuasion in the\nreal-world depends on both exposure to a persuasive message and its impact\nconditional on exposure. Through simulations based on real-world parameters, we\nestimate that LLM-based persuasion costs between \\$48-\\$74 per persuaded voter\ncompared to \\$100 for traditional campaign methods, when accounting for the\ncosts of exposure. However, it is currently much easier to scale traditional\ncampaign persuasion methods than LLM-based persuasion. While LLMs do not\ncurrently appear to have substantially greater potential for large-scale\npolitical persuasion than existing non-LLM methods, this may change as LLM\ncapabilities continue to improve and it becomes easier to scalably encourage\nexposure to persuasive LLMs."}
{"id": "2505.00038", "pdf": "https://arxiv.org/pdf/2505.00038.pdf", "abs": "https://arxiv.org/abs/2505.00038", "title": "HyPerAlign: Hypotheses-driven Personalized Alignment", "authors": ["Cristina Garbacea", "Chenhao Tan"], "categories": ["cs.CL"], "comment": null, "summary": "Alignment algorithms are widely used to align large language models (LLMs) to\nhuman users based on preference annotations that reflect their intended\nreal-world use cases. Typically these (often divergent) preferences are\naggregated over a diverse set of users, resulting in fine-tuned models that are\naligned to the ``average-user'' preference. Nevertheless, current models are\nused by individual users in very specific contexts and situations, emphasizing\nthe need for user-dependent preference control. In this work we address the\nproblem of personalizing LLM outputs to their users, aiming to generate\ncustomized responses tailored to individual users, instead of generic outputs\nthat emulate the collective voices of diverse populations. We propose a novel\ninterpretable and sample-efficient hypotheses-driven personalization approach\n(HyPerAlign) where given few-shot examples written by a particular user, we\nfirst infer hypotheses about their communication strategies, personality and\nwriting style, then prompt LLM models with these hypotheses and user specific\nattributes to generate customized outputs. We conduct experiments on two\ndifferent personalization tasks, authorship attribution and deliberative\nalignment, with datasets from diverse domains (news articles, blog posts,\nemails, jailbreaking benchmarks), and demonstrate the superiority of\nhypotheses-driven personalization approach when compared to preference-based\nfine-tuning methods. For deliberative alignment, the helpfulness of LLM models\nis improved by up to $70\\%$ on average. For authorship attribution, results\nindicate consistently high win-rates (commonly $>90\\%$) against\nstate-of-the-art preference fine-tuning approaches for LLM personalization\nacross diverse user profiles and LLM models. Overall, our approach represents\nan interpretable and sample-efficient strategy for the personalization of LLM\nmodels to individual users."}
{"id": "2505.00039", "pdf": "https://arxiv.org/pdf/2505.00039.pdf", "abs": "https://arxiv.org/abs/2505.00039", "title": "Graph RAG for Legal Norms: A Hierarchical and Temporal Approach", "authors": ["Hudson de Martim"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "This article proposes an adaptation of Graph Retrieval Augmented Generation\n(Graph RAG) specifically designed for the analysis and comprehension of legal\nnorms, which are characterized by their predefined hierarchical structure,\nextensive network of internal and external references and multiple temporal\nversions. By combining structured knowledge graphs with contextually enriched\ntext segments, Graph RAG offers a promising solution to address the inherent\ncomplexity and vast volume of legal data. The integration of hierarchical\nstructure and temporal evolution into knowledge graphs - along with the concept\nof comprehensive Text Units - facilitates the construction of richer,\ninterconnected representations of legal knowledge. Through a detailed analysis\nof Graph RAG and its application to legal norm datasets, this article aims to\nsignificantly advance the field of Artificial Intelligence applied to Law,\ncreating opportunities for more effective systems in legal research,\nlegislative analysis, and decision support."}
{"id": "2505.00047", "pdf": "https://arxiv.org/pdf/2505.00047.pdf", "abs": "https://arxiv.org/abs/2505.00047", "title": "Base Models Beat Aligned Models at Randomness and Creativity", "authors": ["Peter West", "Christopher Potts"], "categories": ["cs.CL"], "comment": null, "summary": "Alignment has quickly become a default ingredient in LLM development, with\ntechniques such as reinforcement learning from human feedback making models act\nsafely, follow instructions, and perform ever-better on complex tasks. While\nthese techniques are certainly useful, we propose that they should not be\nuniversally applied and demonstrate a range of tasks on which base language\nmodels consistently outperform their popular aligned forms. Particularly, we\nstudy tasks that require unpredictable outputs, such as random number\ngeneration, mixed strategy games (rock-paper-scissors and hide-and-seek), and\ncreative writing. In each case, aligned models tend towards narrow behaviors\nthat result in distinct disadvantages, for instance, preferring to generate \"7\"\nover other uniformly random numbers, becoming almost fully predictable in some\ngame states, or prioritizing pleasant writing over creative originality. Across\nmodels tested, better performance on common benchmarks tends to correlate with\nworse performance on our tasks, suggesting an effective trade-off in the\nrequired capabilities."}
{"id": "2505.00050", "pdf": "https://arxiv.org/pdf/2505.00050.pdf", "abs": "https://arxiv.org/abs/2505.00050", "title": "Emotional Analysis of Fashion Trends Using Social Media and AI: Sentiment Analysis on Twitter for Fashion Trend Forecasting", "authors": ["Aayam Bansal", "Agneya Tharun"], "categories": ["cs.CL", "cs.AI"], "comment": "13 pages", "summary": "This study explores the intersection of fashion trends and social media\nsentiment through computational analysis of Twitter data using the T4SA\n(Twitter for Sentiment Analysis) dataset. By applying natural language\nprocessing and machine learning techniques, we examine how sentiment patterns\nin fashion-related social media conversations can serve as predictors for\nemerging fashion trends. Our analysis involves the identification and\ncategorization of fashion-related content, sentiment classification with\nimproved normalization techniques, time series decomposition, statistically\nvalidated causal relationship modeling, cross-platform sentiment comparison,\nand brand-specific sentiment analysis. Results indicate correlations between\nsentiment patterns and fashion theme popularity, with accessories and\nstreetwear themes showing statistically significant rising trends. The Granger\ncausality analysis establishes sustainability and streetwear as primary trend\ndrivers, showing bidirectional relationships with several other themes. The\nfindings demonstrate that social media sentiment analysis can serve as an\neffective early indicator of fashion trend trajectories when proper statistical\nvalidation is applied. Our improved predictive model achieved 78.35% balanced\naccuracy in sentiment classification, establishing a reliable foundation for\ntrend prediction across positive, neutral, and negative sentiment categories."}
{"id": "2505.00056", "pdf": "https://arxiv.org/pdf/2505.00056.pdf", "abs": "https://arxiv.org/abs/2505.00056", "title": "Clustering Internet Memes Through Template Matching and Multi-Dimensional Similarity", "authors": ["Tygo Bloem", "Filip Ilievski"], "categories": ["cs.CL", "cs.IR", "cs.LG", "cs.MM"], "comment": null, "summary": "Meme clustering is critical for toxicity detection, virality modeling, and\ntyping, but it has received little attention in previous research. Clustering\nsimilar Internet memes is challenging due to their multimodality, cultural\ncontext, and adaptability. Existing approaches rely on databases, overlook\nsemantics, and struggle to handle diverse dimensions of similarity. This paper\nintroduces a novel method that uses template-based matching with\nmulti-dimensional similarity features, thus eliminating the need for predefined\ndatabases and supporting adaptive matching. Memes are clustered using local and\nglobal features across similarity categories such as form, visual content,\ntext, and identity. Our combined approach outperforms existing clustering\nmethods, producing more consistent and coherent clusters, while\nsimilarity-based feature sets enable adaptability and align with human\nintuition. We make all supporting code publicly available to support subsequent\nresearch. Code: https://github.com/tygobl/meme-clustering"}
{"id": "2505.00057", "pdf": "https://arxiv.org/pdf/2505.00057.pdf", "abs": "https://arxiv.org/abs/2505.00057", "title": "A Report on the llms evaluating the high school questions", "authors": ["Zhu Jiawei", "Chen Wei"], "categories": ["cs.CL"], "comment": null, "summary": "This report aims to evaluate the performance of large language models (LLMs)\nin solving high school science questions and to explore their potential\napplications in the educational field. With the rapid development of LLMs in\nthe field of natural language processing, their application in education has\nattracted widespread attention. This study selected mathematics exam questions\nfrom the college entrance examinations (2019-2023) as evaluation data and\nutilized at least eight LLM APIs to provide answers. A comprehensive assessment\nwas conducted based on metrics such as accuracy, response time, logical\nreasoning, and creativity. Through an in-depth analysis of the evaluation\nresults, this report reveals the strengths and weaknesses of LLMs in handling\nhigh school science questions and discusses their implications for educational\npractice. The findings indicate that although LLMs perform excellently in\ncertain aspects, there is still room for improvement in logical reasoning and\ncreative problem-solving. This report provides an empirical foundation for\nfurther research and application of LLMs in the educational field and offers\nsuggestions for improvement."}
{"id": "2505.00059", "pdf": "https://arxiv.org/pdf/2505.00059.pdf", "abs": "https://arxiv.org/abs/2505.00059", "title": "BERSting at the Screams: A Benchmark for Distanced, Emotional and Shouted Speech Recognition", "authors": ["Paige Tuttösí", "Mantaj Dhillon", "Luna Sang", "Shane Eastwood", "Poorvi Bhatia", "Quang Minh Dinh", "Avni Kapoor", "Yewon Jin", "Angelica Lim"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to Computer Speech and Language, Special issue:\n  Multi-Speaker, Multi-Microphone, and Multi-Modal Distant Speech Recognition\n  (September 2025)", "summary": "Some speech recognition tasks, such as automatic speech recognition (ASR),\nare approaching or have reached human performance in many reported metrics.\nYet, they continue to struggle in complex, real-world, situations, such as with\ndistanced speech. Previous challenges have released datasets to address the\nissue of distanced ASR, however, the focus remains primarily on distance,\nspecifically relying on multi-microphone array systems. Here we present the\nB(asic) E(motion) R(andom phrase) S(hou)t(s) (BERSt) dataset. The dataset\ncontains almost 4 hours of English speech from 98 actors with varying regional\nand non-native accents. The data was collected on smartphones in the actors\nhomes and therefore includes at least 98 different acoustic environments. The\ndata also includes 7 different emotion prompts and both shouted and spoken\nutterances. The smartphones were places in 19 different positions, including\nobstructions and being in a different room than the actor. This data is\npublicly available for use and can be used to evaluate a variety of speech\nrecognition tasks, including: ASR, shout detection, and speech emotion\nrecognition (SER). We provide initial benchmarks for ASR and SER tasks, and\nfind that ASR degrades both with an increase in distance and shout level and\nshows varied performance depending on the intended emotion. Our results show\nthat the BERSt dataset is challenging for both ASR and SER tasks and continued\nwork is needed to improve the robustness of such systems for more accurate\nreal-world use."}
{"id": "2505.00060", "pdf": "https://arxiv.org/pdf/2505.00060.pdf", "abs": "https://arxiv.org/abs/2505.00060", "title": "Fact-Consistency Evaluation of Text-to-SQL Generation for Business Intelligence Using Exaone 3.5", "authors": ["Jeho Choi"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "6 pages, 1 table", "summary": "Large Language Models (LLMs) have shown promise in enabling natural language\ninterfaces for structured data querying through text-to-SQL generation.\nHowever, their application in real-world Business Intelligence (BI) contexts\nremains limited due to semantic hallucinations, structural errors, and a lack\nof domain-specific evaluation frameworks. In this study, we propose a\nFact-Consistency Evaluation Framework for assessing the semantic accuracy of\nLLM-generated SQL outputs using Exaone 3.5--an instruction-tuned, bilingual LLM\noptimized for enterprise tasks. We construct a domain-specific benchmark\ncomprising 219 natural language business questions across five SQL complexity\nlevels, derived from actual sales data in LG Electronics' internal BigQuery\nenvironment. Each question is paired with a gold-standard SQL query and a\nvalidated ground-truth answer. We evaluate model performance using answer\naccuracy, execution success rate, semantic error rate, and non-response rate.\nExperimental results show that while Exaone 3.5 performs well on simple\naggregation tasks (93% accuracy in L1), it exhibits substantial degradation in\narithmetic reasoning (4% accuracy in H1) and grouped ranking tasks (31% in H4),\nwith semantic errors and non-responses concentrated in complex cases.\nQualitative error analysis further identifies common failure types such as\nmisapplied arithmetic logic, incomplete filtering, and incorrect grouping\noperations. Our findings highlight the current limitations of LLMs in\nbusiness-critical environments and underscore the need for fact-consistency\nvalidation layers and hybrid reasoning approaches. This work contributes a\nreproducible benchmark and evaluation methodology for advancing reliable\nnatural language interfaces to structured enterprise data systems."}
{"id": "2505.00061", "pdf": "https://arxiv.org/pdf/2505.00061.pdf", "abs": "https://arxiv.org/abs/2505.00061", "title": "Enhancing Security and Strengthening Defenses in Automated Short-Answer Grading Systems", "authors": ["Sahar Yarmohammadtoosky", "Yiyun Zhou", "Victoria Yaneva", "Peter Baldwin", "Saed Rezayi", "Brian Clauser", "Polina Harikeo"], "categories": ["cs.CL", "cs.CR"], "comment": null, "summary": "This study examines vulnerabilities in transformer-based automated\nshort-answer grading systems used in medical education, with a focus on how\nthese systems can be manipulated through adversarial gaming strategies. Our\nresearch identifies three main types of gaming strategies that exploit the\nsystem's weaknesses, potentially leading to false positives. To counteract\nthese vulnerabilities, we implement several adversarial training methods\ndesigned to enhance the systems' robustness. Our results indicate that these\nmethods significantly reduce the susceptibility of grading systems to such\nmanipulations, especially when combined with ensemble techniques like majority\nvoting and ridge regression, which further improve the system's defense against\nsophisticated adversarial inputs. Additionally, employing large language models\nsuch as GPT-4 with varied prompting techniques has shown promise in recognizing\nand scoring gaming strategies effectively. The findings underscore the\nimportance of continuous improvements in AI-driven educational tools to ensure\ntheir reliability and fairness in high-stakes settings."}
{"id": "2505.00063", "pdf": "https://arxiv.org/pdf/2505.00063.pdf", "abs": "https://arxiv.org/abs/2505.00063", "title": "GDI-Bench: A Benchmark for General Document Intelligence with Vision and Reasoning Decoupling", "authors": ["Siqi Li", "Yufan Shen", "Xiangnan Chen", "Jiayi Chen", "Hengwei Ju", "Haodong Duan", "Song Mao", "Hongbin Zhou", "Bo Zhang", "Pinlong Cai", "Licheng Wen", "Botian Shi", "Yong Liu", "Xinyu Cai", "Yu Qiao"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "The rapid advancement of multimodal large language models (MLLMs) has\nprofoundly impacted the document domain, creating a wide array of application\nscenarios. This progress highlights the need for a comprehensive benchmark to\nevaluate these models' capabilities across various document-specific tasks.\nHowever, existing benchmarks often fail to locate specific model weaknesses or\nguide systematic improvements. To bridge this gap, we introduce a General\nDocument Intelligence Benchmark (GDI-Bench), featuring 1.9k images across 9 key\nscenarios and 19 document-specific tasks. By decoupling visual complexity and\nreasoning complexity, the GDI-Bench structures graded tasks that allow\nperformance assessment by difficulty, aiding in model weakness identification\nand optimization guidance. We evaluate the GDI-Bench on various open-source and\nclosed-source models, conducting decoupled analyses in the visual and reasoning\ndomains. For instance, the GPT-4o model excels in reasoning tasks but exhibits\nlimitations in visual capabilities. To address the diverse tasks and domains in\nthe GDI-Bench, we propose a GDI Model that mitigates the issue of catastrophic\nforgetting during the supervised fine-tuning (SFT) process through a\nintelligence-preserving training strategy. Our model achieves state-of-the-art\nperformance on previous benchmarks and the GDI-Bench. Both our benchmark and\nmodel will be open source."}
{"id": "2505.00065", "pdf": "https://arxiv.org/pdf/2505.00065.pdf", "abs": "https://arxiv.org/abs/2505.00065", "title": "ConSens: Assessing context grounding in open-book question answering", "authors": ["Ivan Vankov", "Matyo Ivanov", "Adriana Correia", "Victor Botev"], "categories": ["cs.CL", "cs.LG"], "comment": "9 pages, 3 figures, 3 tables", "summary": "Large Language Models (LLMs) have demonstrated considerable success in\nopen-book question answering (QA), where the task requires generating answers\ngrounded in a provided external context. A critical challenge in open-book QA\nis to ensure that model responses are based on the provided context rather than\nits parametric knowledge, which can be outdated, incomplete, or incorrect.\nExisting evaluation methods, primarily based on the LLM-as-a-judge approach,\nface significant limitations, including biases, scalability issues, and\ndependence on costly external systems. To address these challenges, we propose\na novel metric that contrasts the perplexity of the model response under two\nconditions: when the context is provided and when it is not. The resulting\nscore quantifies the extent to which the model's answer relies on the provided\ncontext. The validity of this metric is demonstrated through a series of\nexperiments that show its effectiveness in identifying whether a given answer\nis grounded in the provided context. Unlike existing approaches, this metric is\ncomputationally efficient, interpretable, and adaptable to various use cases,\noffering a scalable and practical solution to assess context utilization in\nopen-book QA systems."}
{"id": "2505.00114", "pdf": "https://arxiv.org/pdf/2505.00114.pdf", "abs": "https://arxiv.org/abs/2505.00114", "title": "Fine-Tuning LLMs for Low-Resource Dialect Translation: The Case of Lebanese", "authors": ["Silvana Yakhni", "Ali Chehab"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper examines the effectiveness of Large Language Models (LLMs) in\ntranslating the low-resource Lebanese dialect, focusing on the impact of\nculturally authentic data versus larger translated datasets. We compare three\nfine-tuning approaches: Basic, contrastive, and grammar-hint tuning, using\nopen-source Aya23 models. Experiments reveal that models fine-tuned on a\nsmaller but culturally aware Lebanese dataset (LW) consistently outperform\nthose trained on larger, non-native data. The best results were achieved\nthrough contrastive fine-tuning paired with contrastive prompting, which\nindicates the benefits of exposing translation models to bad examples. In\naddition, to ensure authentic evaluation, we introduce LebEval, a new benchmark\nderived from native Lebanese content, and compare it to the existing FLoRes\nbenchmark. Our findings challenge the \"More Data is Better\" paradigm and\nemphasize the crucial role of cultural authenticity in dialectal translation.\nWe made our datasets and code available on Github."}
{"id": "2505.00127", "pdf": "https://arxiv.org/pdf/2505.00127.pdf", "abs": "https://arxiv.org/abs/2505.00127", "title": "Between Underthinking and Overthinking: An Empirical Study of Reasoning Length and correctness in LLMs", "authors": ["Jinyan Su", "Jennifer Healey", "Preslav Nakov", "Claire Cardie"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly optimized for long reasoning,\nunder the assumption that more reasoning leads to better performance. However,\nemerging evidence suggests that longer responses can sometimes degrade accuracy\nrather than improve it. In this paper, we conduct a systematic empirical study\nof the relationship between reasoning length and answer correctness. We find\nthat LLMs tend to overthink simple problems, generating unnecessarily long\noutputs, and underthink harder ones, failing to extend their reasoning when it\nis most needed. This indicates that models might misjudge problem difficulty\nand fail to calibrate their response length appropriately. Furthermore, we\ninvestigate the effects of length reduction with a preference optimization\nalgorithm when simply preferring the shorter responses regardless of answer\ncorrectness. Experiments show that the generation length can be significantly\nreduced while maintaining acceptable accuracy. Our findings highlight\ngeneration length as a meaningful signal for reasoning behavior and motivate\nfurther exploration into LLMs' self-awareness in reasoning length adaptation."}
{"id": "2505.00147", "pdf": "https://arxiv.org/pdf/2505.00147.pdf", "abs": "https://arxiv.org/abs/2505.00147", "title": "AdaptMI: Adaptive Skill-based In-context Math Instruction for Small Language Models", "authors": ["Yinghui He", "Abhishek Panigrahi", "Yong Lin", "Sanjeev Arora"], "categories": ["cs.CL"], "comment": null, "summary": "In-context learning (ICL) allows a language model to improve its\nproblem-solving capability when provided with suitable information in context.\nSince the choice of in-context information can be determined based on the\nproblem itself, in-context learning is analogous to human learning from\nteachers in a classroom. Recent works (Didolkar et al., 2024a; 2024b) show that\nICL performance can be improved by leveraging a frontier large language model's\n(LLM) ability to predict required skills to solve a problem, popularly referred\nto as an LLM's metacognition, and using the recommended skills to construct\nnecessary in-context examples. While this skill-based strategy boosts ICL\nperformance in larger models, its gains on small language models (SLMs) have\nbeen minimal, highlighting a performance gap in ICL capabilities. We\ninvestigate this gap and show that skill-based prompting can hurt SLM\nperformance on easy questions by introducing unnecessary information, akin to\ncognitive overload. To address this, we introduce AdaptMI, an adaptive approach\nto selecting skill-based in-context Math Instructions for SLMs. Inspired by\ncognitive load theory from human pedagogy, our method only introduces\nskill-based examples when the model performs poorly. We further propose\nAdaptMI+, which adds examples targeted to the specific skills missing from the\nmodel's responses. On 5-shot evaluations across popular math benchmarks and\nfive SLMs (1B--7B; Qwen, Llama), AdaptMI+ improves accuracy by up to 6% over\nnaive skill-based strategies."}
{"id": "2505.00191", "pdf": "https://arxiv.org/pdf/2505.00191.pdf", "abs": "https://arxiv.org/abs/2505.00191", "title": "IP-CRR: Information Pursuit for Interpretable Classification of Chest Radiology Reports", "authors": ["Yuyan Ge", "Kwan Ho Ryan Chan", "Pablo Messina", "René Vidal"], "categories": ["cs.CL"], "comment": "12 pages, 4 figures", "summary": "The development of AI-based methods for analyzing radiology reports could\nlead to significant advances in medical diagnosis--from improving diagnostic\naccuracy to enhancing efficiency and reducing workload. However, the lack of\ninterpretability in these methods has hindered their adoption in clinical\nsettings. In this paper, we propose an interpretable-by-design framework for\nclassifying radiology reports. The key idea is to extract a set of most\ninformative queries from a large set of reports and use these queries and their\ncorresponding answers to predict a diagnosis. Thus, the explanation for a\nprediction is, by construction, the set of selected queries and answers. We use\nthe Information Pursuit framework to select informative queries, the Flan-T5\nmodel to determine if facts are present in the report, and a classifier to\npredict the disease. Experiments on the MIMIC-CXR dataset demonstrate the\neffectiveness of the proposed method, highlighting its potential to enhance\ntrust and usability in medical AI."}
{"id": "2505.00261", "pdf": "https://arxiv.org/pdf/2505.00261.pdf", "abs": "https://arxiv.org/abs/2505.00261", "title": "Enriching the Korean Learner Corpus with Multi-reference Annotations and Rubric-Based Scoring", "authors": ["Jayoung Song", "KyungTae Lim", "Jungyeul Park"], "categories": ["cs.CL"], "comment": null, "summary": "Despite growing global interest in Korean language education, there remains a\nsignificant lack of learner corpora tailored to Korean L2 writing. To address\nthis gap, we enhance the KoLLA Korean learner corpus by adding multiple\ngrammatical error correction (GEC) references, thereby enabling more nuanced\nand flexible evaluation of GEC systems, and reflects the variability of human\nlanguage. Additionally, we enrich the corpus with rubric-based scores aligned\nwith guidelines from the Korean National Language Institute, capturing\ngrammatical accuracy, coherence, and lexical diversity. These enhancements make\nKoLLA a robust and standardized resource for research in Korean L2 education,\nsupporting advancements in language learning, assessment, and automated error\ncorrection."}
{"id": "2505.00268", "pdf": "https://arxiv.org/pdf/2505.00268.pdf", "abs": "https://arxiv.org/abs/2505.00268", "title": "Consistency in Language Models: Current Landscape, Challenges, and Future Directions", "authors": ["Jekaterina Novikova", "Carol Anderson", "Borhane Blili-Hamelin", "Subhabrata Majumdar"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The hallmark of effective language use lies in consistency -- expressing\nsimilar meanings in similar contexts and avoiding contradictions. While human\ncommunication naturally demonstrates this principle, state-of-the-art language\nmodels struggle to maintain reliable consistency across different scenarios.\nThis paper examines the landscape of consistency research in AI language\nsystems, exploring both formal consistency (including logical rule adherence)\nand informal consistency (such as moral and factual coherence). We analyze\ncurrent approaches to measure aspects of consistency, identify critical\nresearch gaps in standardization of definitions, multilingual assessment, and\nmethods to improve consistency. Our findings point to an urgent need for robust\nbenchmarks to measure and interdisciplinary approaches to ensure consistency in\nthe application of language models on domain-specific tasks while preserving\nthe utility and adaptability."}
{"id": "2505.00339", "pdf": "https://arxiv.org/pdf/2505.00339.pdf", "abs": "https://arxiv.org/abs/2505.00339", "title": "Enhancing AI-Driven Education: Integrating Cognitive Frameworks, Linguistic Feedback Analysis, and Ethical Considerations for Improved Content Generation", "authors": ["Antoun Yaacoub", "Sansiri Tarnpradab", "Phattara Khumprom", "Zainab Assaghir", "Lionel Prevost", "Jérôme Da-Rugna"], "categories": ["cs.CL", "cs.AI"], "comment": "This article will be presented in IJCNN 2025 \"AI Innovations for\n  Education: Transforming Teaching and Learning through Cutting-Edge\n  Technologies\" workshop", "summary": "Artificial intelligence (AI) is rapidly transforming education, presenting\nunprecedented opportunities for personalized learning and streamlined content\ncreation. However, realizing the full potential of AI in educational settings\nnecessitates careful consideration of the quality, cognitive depth, and ethical\nimplications of AI-generated materials. This paper synthesizes insights from\nfour related studies to propose a comprehensive framework for enhancing\nAI-driven educational tools. We integrate cognitive assessment frameworks\n(Bloom's Taxonomy and SOLO Taxonomy), linguistic analysis of AI-generated\nfeedback, and ethical design principles to guide the development of effective\nand responsible AI tools. We outline a structured three-phase approach\nencompassing cognitive alignment, linguistic feedback integration, and ethical\nsafeguards. The practical application of this framework is demonstrated through\nits integration into OneClickQuiz, an AI-powered Moodle plugin for quiz\ngeneration. This work contributes a comprehensive and actionable guide for\neducators, researchers, and developers aiming to harness AI's potential while\nupholding pedagogical and ethical standards in educational content generation."}
{"id": "2505.00367", "pdf": "https://arxiv.org/pdf/2505.00367.pdf", "abs": "https://arxiv.org/abs/2505.00367", "title": "KoACD: The First Korean Adolescent Dataset for Cognitive Distortion Analysis", "authors": ["JunSeo Kim", "HyeHyeon Kim"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Cognitive distortion refers to negative thinking patterns that can lead to\nmental health issues like depression and anxiety in adolescents. Previous\nstudies using natural language processing (NLP) have focused mainly on\nsmall-scale adult datasets, with limited research on adolescents. This study\nintroduces KoACD, the first large-scale dataset of cognitive distortions in\nKorean adolescents, containing 108,717 instances. We applied a multi-Large\nLanguage Model (LLM) negotiation method to refine distortion classification and\ngenerate synthetic data using two approaches: cognitive clarification for\ntextual clarity and cognitive balancing for diverse distortion representation.\nValidation through LLMs and expert evaluations showed that while LLMs\nclassified distortions with explicit markers, they struggled with\ncontext-dependent reasoning, where human evaluators demonstrated higher\naccuracy. KoACD aims to enhance future research on cognitive distortion\ndetection."}
{"id": "2505.00389", "pdf": "https://arxiv.org/pdf/2505.00389.pdf", "abs": "https://arxiv.org/abs/2505.00389", "title": "CSE-SFP: Enabling Unsupervised Sentence Representation Learning via a Single Forward Pass", "authors": ["Bowen Zhang", "Zixin Song", "Chunping Li"], "categories": ["cs.CL"], "comment": "Accepted by SIGIR 2025 (Full)", "summary": "As a fundamental task in Information Retrieval and Computational Linguistics,\nsentence representation has profound implications for a wide range of practical\napplications such as text clustering, content analysis, question-answering\nsystems, and web search. Recent advances in pre-trained language models (PLMs)\nhave driven remarkable progress in this field, particularly through\nunsupervised embedding derivation methods centered on discriminative PLMs like\nBERT. However, due to time and computational constraints, few efforts have\nattempted to integrate unsupervised sentence representation with generative\nPLMs, which typically possess much larger parameter sizes. Given that\nstate-of-the-art models in both academia and industry are predominantly based\non generative architectures, there is a pressing need for an efficient\nunsupervised text representation framework tailored to decoder-only PLMs. To\naddress this concern, we propose CSE-SFP, an innovative method that exploits\nthe structural characteristics of generative models. Compared to existing\nstrategies, CSE-SFP requires only a single forward pass to perform effective\nunsupervised contrastive learning. Rigorous experimentation demonstrates that\nCSE-SFP not only produces higher-quality embeddings but also significantly\nreduces both training time and memory consumption. Furthermore, we introduce\ntwo ratio metrics that jointly assess alignment and uniformity, thereby\nproviding a more robust means for evaluating the semantic spatial properties of\nencoding models."}
{"id": "2505.00467", "pdf": "https://arxiv.org/pdf/2505.00467.pdf", "abs": "https://arxiv.org/abs/2505.00467", "title": "Red Teaming Large Language Models for Healthcare", "authors": ["Vahid Balazadeh", "Michael Cooper", "David Pellow", "Atousa Assadi", "Jennifer Bell", "Jim Fackler", "Gabriel Funingana", "Spencer Gable-Cook", "Anirudh Gangadhar", "Abhishek Jaiswal", "Sumanth Kaja", "Christopher Khoury", "Randy Lin", "Kaden McKeen", "Sara Naimimohasses", "Khashayar Namdar", "Aviraj Newatia", "Allan Pang", "Anshul Pattoo", "Sameer Peesapati", "Diana Prepelita", "Bogdana Rakova", "Saba Sadatamin", "Rafael Schulman", "Ajay Shah", "Syed Azhar Shah", "Syed Ahmar Shah", "Babak Taati", "Balagopal Unnikrishnan", "Stephanie Williams", "Rahul G Krishnan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present the design process and findings of the pre-conference workshop at\nthe Machine Learning for Healthcare Conference (2024) entitled Red Teaming\nLarge Language Models for Healthcare, which took place on August 15, 2024.\nConference participants, comprising a mix of computational and clinical\nexpertise, attempted to discover vulnerabilities -- realistic clinical prompts\nfor which a large language model (LLM) outputs a response that could cause\nclinical harm. Red-teaming with clinicians enables the identification of LLM\nvulnerabilities that may not be recognised by LLM developers lacking clinical\nexpertise. We report the vulnerabilities found, categorise them, and present\nthe results of a replication study assessing the vulnerabilities across all\nLLMs provided."}
{"id": "2505.00479", "pdf": "https://arxiv.org/pdf/2505.00479.pdf", "abs": "https://arxiv.org/abs/2505.00479", "title": "Computational Identification of Regulatory Statements in EU Legislation", "authors": ["Gijs Jan Brandsma", "Jens Blom-Hansen", "Christiaan Meijer", "Kody Moodley"], "categories": ["cs.CL", "I.2.7"], "comment": "11 pages, 6 figures", "summary": "Identifying regulatory statements in legislation is useful for developing\nmetrics to measure the regulatory density and strictness of legislation. A\ncomputational method is valuable for scaling the identification of such\nstatements from a growing body of EU legislation, constituting approximately\n180,000 published legal acts between 1952 and 2023. Past work on extraction of\nthese statements varies in the permissiveness of their definitions for what\nconstitutes a regulatory statement. In this work, we provide a specific\ndefinition for our purposes based on the institutional grammar tool. We develop\nand compare two contrasting approaches for automatically identifying such\nstatements in EU legislation, one based on dependency parsing, and the other on\na transformer-based machine learning model. We found both approaches performed\nsimilarly well with accuracies of 80% and 84% respectively and a K alpha of\n0.58. The high accuracies and not exceedingly high agreement suggests potential\nfor combining strengths of both approaches."}
{"id": "2505.00506", "pdf": "https://arxiv.org/pdf/2505.00506.pdf", "abs": "https://arxiv.org/abs/2505.00506", "title": "HalluMix: A Task-Agnostic, Multi-Domain Benchmark for Real-World Hallucination Detection", "authors": ["Deanna Emery", "Michael Goitia", "Freddie Vargus", "Iulia Neagu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As large language models (LLMs) are increasingly deployed in high-stakes\ndomains, detecting hallucinated content$\\unicode{x2013}$text that is not\ngrounded in supporting evidence$\\unicode{x2013}$has become a critical\nchallenge. Existing benchmarks for hallucination detection are often\nsynthetically generated, narrowly focused on extractive question answering, and\nfail to capture the complexity of real-world scenarios involving multi-document\ncontexts and full-sentence outputs. We introduce the HalluMix Benchmark, a\ndiverse, task-agnostic dataset that includes examples from a range of domains\nand formats. Using this benchmark, we evaluate seven hallucination detection\nsystems$\\unicode{x2013}$both open and closed\nsource$\\unicode{x2013}$highlighting differences in performance across tasks,\ndocument lengths, and input representations. Our analysis highlights\nsubstantial performance disparities between short and long contexts, with\ncritical implications for real-world Retrieval Augmented Generation (RAG)\nimplementations. Quotient Detections achieves the best overall performance,\nwith an accuracy of 0.82 and an F1 score of 0.84."}
{"id": "2505.00551", "pdf": "https://arxiv.org/pdf/2505.00551.pdf", "abs": "https://arxiv.org/abs/2505.00551", "title": "100 Days After DeepSeek-R1: A Survey on Replication Studies and More Directions for Reasoning Language Models", "authors": ["Chong Zhang", "Yue Deng", "Xiang Lin", "Bin Wang", "Dianwen Ng", "Hai Ye", "Xingxuan Li", "Yao Xiao", "Zhanfeng Mo", "Qi Zhang", "Lidong Bing"], "categories": ["cs.CL"], "comment": null, "summary": "The recent development of reasoning language models (RLMs) represents a novel\nevolution in large language models. In particular, the recent release of\nDeepSeek-R1 has generated widespread social impact and sparked enthusiasm in\nthe research community for exploring the explicit reasoning paradigm of\nlanguage models. However, the implementation details of the released models\nhave not been fully open-sourced by DeepSeek, including DeepSeek-R1-Zero,\nDeepSeek-R1, and the distilled small models. As a result, many replication\nstudies have emerged aiming to reproduce the strong performance achieved by\nDeepSeek-R1, reaching comparable performance through similar training\nprocedures and fully open-source data resources. These works have investigated\nfeasible strategies for supervised fine-tuning (SFT) and reinforcement learning\nfrom verifiable rewards (RLVR), focusing on data preparation and method design,\nyielding various valuable insights. In this report, we provide a summary of\nrecent replication studies to inspire future research. We primarily focus on\nSFT and RLVR as two main directions, introducing the details for data\nconstruction, method design and training procedure of current replication\nstudies. Moreover, we conclude key findings from the implementation details and\nexperimental results reported by these studies, anticipating to inspire future\nresearch. We also discuss additional techniques of enhancing RLMs, highlighting\nthe potential of expanding the application scope of these models, and\ndiscussing the challenges in development. By this survey, we aim to help\nresearchers and developers of RLMs stay updated with the latest advancements,\nand seek to inspire new ideas to further enhance RLMs."}
{"id": "2505.00557", "pdf": "https://arxiv.org/pdf/2505.00557.pdf", "abs": "https://arxiv.org/abs/2505.00557", "title": "Triggering Hallucinations in LLMs: A Quantitative Study of Prompt-Induced Hallucination in Large Language Models", "authors": ["Makoto Sato"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Hallucinations in large language models (LLMs) present a growing challenge\nacross real-world applications, from healthcare to law, where factual\nreliability is essential. Despite advances in alignment and instruction tuning,\nLLMs can still generate outputs that are fluent yet fundamentally untrue.\nUnderstanding the cognitive dynamics that underlie these hallucinations remains\nan open problem. In this study, we propose a prompt-based framework to\nsystematically trigger and quantify hallucination: a Hallucination-Inducing\nPrompt (HIP), which synthetically fuses semantically distant concepts (e.g.,\nperiodic table of elements and tarot divination) in a misleading way, and a\nHallucination Quantifying Prompt (HQP), which scores the plausibility,\nconfidence, and coherence of the output. Controlled experiments across multiple\nLLMs revealed that HIPs consistently produced less coherent and more\nhallucinated responses than their null-fusion controls. These effects varied\nacross models, with reasoning-oriented LLMs showing distinct profiles from\ngeneral-purpose ones. Our framework provides a reproducible testbed for\nstudying hallucination vulnerability, and opens the door to developing safer,\nmore introspective LLMs that can detect and self-regulate the onset of\nconceptual instability."}
{"id": "2505.00570", "pdf": "https://arxiv.org/pdf/2505.00570.pdf", "abs": "https://arxiv.org/abs/2505.00570", "title": "FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension", "authors": ["Jushi Kai", "Boyi Zeng", "Yixuan Wang", "Haoli Bai", "Bo Jiang", "Zhouhan Lin"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Extending the context window in large language models (LLMs) is essential for\napplications involving long-form content generation. However, the linear\nincrease in key-value (KV) cache memory requirements and the quadratic\ncomplexity of self-attention with respect to sequence length present\nsignificant challenges during fine-tuning and inference. Existing methods\nsuffer from performance degradation when extending to longer contexts. In this\nwork, we introduce a novel context extension method that optimizes both\nfine-tuning and inference efficiency. Our method exploits a key observation: in\nthe frequency domain, the energy distribution of the KV cache is primarily\nconcentrated in low-frequency components. By filtering out the high-frequency\ncomponents, the KV cache can be effectively compressed with minimal information\nloss. Building on this insight, we propose an efficient compression technique,\nFreqKV, that iteratively compresses the increasing KV cache to a fixed size in\nthe frequency domain, applicable to both fine-tuning and inference. FreqKV\nintroduces no additional parameters or architectural modifications. With\nminimal fine-tuning, LLMs can learn to leverage the limited cache that is\ncompressed in the frequency domain and extend the context window efficiently.\nExperiments on various long context language modeling and understanding tasks\ndemonstrate the efficiency and efficacy of the proposed method."}
{"id": "2505.00582", "pdf": "https://arxiv.org/pdf/2505.00582.pdf", "abs": "https://arxiv.org/abs/2505.00582", "title": "Block Circulant Adapter for Large Language Models", "authors": ["Xinyu Ding", "Meiqi Wang", "Siyu Liao", "Zhongfeng Wang"], "categories": ["cs.CL", "cs.LG"], "comment": "to appear in Proceedings of the 2025 International Joint Conference\n  on Artificial Intelligence (IJCAI-2025)", "summary": "Fine-tuning large language models (LLMs) is difficult due to their huge model\nsize. Recent Fourier domain-based methods show potential for reducing\nfine-tuning costs. We propose a block circulant matrix-based fine-tuning method\nwith a stable training heuristic to leverage the properties of circulant\nmatrices and one-dimensional Fourier transforms to reduce storage and\ncomputation costs. Experiments show that our method uses $14\\times$ less number\nof parameters than VeRA, $16\\times$ smaller than LoRA and $32\\times$ less FLOPs\nthan FourierFT, while maintaining close or better task performance. Our\napproach presents a promising way in frequency domain to fine-tune large models\non downstream tasks."}
{"id": "2505.00624", "pdf": "https://arxiv.org/pdf/2505.00624.pdf", "abs": "https://arxiv.org/abs/2505.00624", "title": "FineScope : Precision Pruning for Domain-Specialized Large Language Models Using SAE-Guided Self-Data Cultivation", "authors": ["Chaitali Bhattacharyya", "Yeseong Kim"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Training large language models (LLMs) from scratch requires significant\ncomputational resources, driving interest in developing smaller,\ndomain-specific LLMs that maintain both efficiency and strong task performance.\nMedium-sized models such as LLaMA, llama} have served as starting points for\ndomain-specific adaptation, but they often suffer from accuracy degradation\nwhen tested on specialized datasets. We introduce FineScope, a framework for\nderiving compact, domain-optimized LLMs from larger pretrained models.\nFineScope leverages the Sparse Autoencoder (SAE) framework, inspired by its\nability to produce interpretable feature representations, to extract\ndomain-specific subsets from large datasets. We apply structured pruning with\ndomain-specific constraints, ensuring that the resulting pruned models retain\nessential knowledge for the target domain. To further enhance performance,\nthese pruned models undergo self-data distillation, leveraging SAE-curated\ndatasets to restore key domain-specific information lost during pruning.\nExtensive experiments and ablation studies demonstrate that FineScope achieves\nhighly competitive performance, outperforming several large-scale\nstate-of-the-art LLMs in domain-specific tasks. Additionally, our results show\nthat FineScope enables pruned models to regain a substantial portion of their\noriginal performance when fine-tuned with SAE-curated datasets. Furthermore,\napplying these datasets to fine-tune pretrained LLMs without pruning also\nimproves their domain-specific accuracy, highlighting the robustness of our\napproach. The code will be released."}
{"id": "2505.00626", "pdf": "https://arxiv.org/pdf/2505.00626.pdf", "abs": "https://arxiv.org/abs/2505.00626", "title": "The Illusion of Role Separation: Hidden Shortcuts in LLM Role Learning (and How to Fix Them)", "authors": ["Zihao Wang", "Yibo Jiang", "Jiahao Yu", "Heqing Huang"], "categories": ["cs.CL", "cs.AI", "68T50", "I.2"], "comment": null, "summary": "Large language models (LLMs) that integrate multiple input roles (e.g.,\nsystem instructions, user queries, external tool outputs) are increasingly\nprevalent in practice. Ensuring that the model accurately distinguishes\nmessages from each role -- a concept we call \\emph{role separation} -- is\ncrucial for consistent multi-role behavior. Although recent work often targets\nstate-of-the-art prompt injection defenses, it remains unclear whether such\nmethods truly teach LLMs to differentiate roles or merely memorize known\ntriggers. In this paper, we examine \\emph{role-separation learning}: the\nprocess of teaching LLMs to robustly distinguish system and user tokens.\nThrough a \\emph{simple, controlled experimental framework}, we find that\nfine-tuned models often rely on two proxies for role identification: (1) task\ntype exploitation, and (2) proximity to begin-of-text. Although data\naugmentation can partially mitigate these shortcuts, it generally leads to\niterative patching rather than a deeper fix. To address this, we propose\nreinforcing \\emph{invariant signals} that mark role boundaries by adjusting\ntoken-wise cues in the model's input encoding. In particular, manipulating\nposition IDs helps the model learn clearer distinctions and reduces reliance on\nsuperficial proxies. By focusing on this mechanism-centered perspective, our\nwork illuminates how LLMs can more reliably maintain consistent multi-role\nbehavior without merely memorizing known prompts or triggers."}
{"id": "2505.00654", "pdf": "https://arxiv.org/pdf/2505.00654.pdf", "abs": "https://arxiv.org/abs/2505.00654", "title": "Large Language Models Understanding: an Inherent Ambiguity Barrier", "authors": ["Daniel N. Nissani"], "categories": ["cs.CL", "cs.AI"], "comment": "submitted to NEURAL COMPUTATION", "summary": "A lively ongoing debate is taking place, since the extraordinary emergence of\nLarge Language Models (LLMs) with regards to their capability to understand the\nworld and capture the meaning of the dialogues in which they are involved.\nArguments and counter-arguments have been proposed based upon thought\nexperiments, anecdotal conversations between LLMs and humans, statistical\nlinguistic analysis, philosophical considerations, and more. In this brief\npaper we present a counter-argument based upon a thought experiment and\nsemi-formal considerations leading to an inherent ambiguity barrier which\nprevents LLMs from having any understanding of what their amazingly fluent\ndialogues mean."}
{"id": "2505.00661", "pdf": "https://arxiv.org/pdf/2505.00661.pdf", "abs": "https://arxiv.org/abs/2505.00661", "title": "On the generalization of language models from in-context learning and finetuning: a controlled study", "authors": ["Andrew K. Lampinen", "Arslan Chaudhry", "Stephanie C. Y. Chan", "Cody Wild", "Diane Wan", "Alex Ku", "Jörg Bornschein", "Razvan Pascanu", "Murray Shanahan", "James L. McClelland"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models exhibit exciting capabilities, yet can show\nsurprisingly narrow generalization from finetuning -- from failing to\ngeneralize to simple reversals of relations they are trained on, to missing\nlogical deductions that can be made from trained information. These failures to\ngeneralize from fine-tuning can hinder practical application of these models.\nHowever, language models' in-context learning shows different inductive biases,\nand can generalize better in some of these cases. Here, we explore these\ndifferences in generalization between in-context- and fine-tuning-based\nlearning. To do so, we constructed several novel datasets to evaluate and\nimprove models' ability to generalize from finetuning data. The datasets are\nconstructed to isolate the knowledge in the dataset from that in pretraining,\nto create clean tests of generalization. We expose pretrained large models to\ncontrolled subsets of the information in these datasets -- either in context,\nor through fine-tuning -- and evaluate their performance on test sets that\nrequire various types of generalization. We find overall that in data-matched\nsettings, in-context learning can generalize more flexibly than fine-tuning\n(though we also find some qualifications of prior findings, such as cases when\nfine-tuning can generalize to reversals embedded in a larger structure of\nknowledge). We build on these findings to propose a method to enable improved\ngeneralization from fine-tuning: adding in-context inferences to finetuning\ndata. We show that this method improves generalization across various splits of\nour datasets and other benchmarks. Our results have implications for\nunderstanding the inductive biases of different modes of learning in language\nmodels, and practically improving their performance."}
{"id": "2505.00662", "pdf": "https://arxiv.org/pdf/2505.00662.pdf", "abs": "https://arxiv.org/abs/2505.00662", "title": "DeepCritic: Deliberate Critique with Large Language Models", "authors": ["Wenkai Yang", "Jingwen Chen", "Yankai Lin", "Ji-Rong Wen"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Work in progress. Data and models are available at\n  https://github.com/RUCBM/DeepCritic", "summary": "As Large Language Models (LLMs) are rapidly evolving, providing accurate\nfeedback and scalable oversight on their outputs becomes an urgent and critical\nproblem. Leveraging LLMs as critique models to achieve automated supervision is\na promising solution. In this work, we focus on studying and enhancing the math\ncritique ability of LLMs. Current LLM critics provide critiques that are too\nshallow and superficial on each step, leading to low judgment accuracy and\nstruggling to offer sufficient feedback for the LLM generator to correct\nmistakes. To tackle this issue, we propose a novel and effective two-stage\nframework to develop LLM critics that are capable of deliberately critiquing on\neach reasoning step of math solutions. In the first stage, we utilize\nQwen2.5-72B-Instruct to generate 4.5K long-form critiques as seed data for\nsupervised fine-tuning. Each seed critique consists of deliberate step-wise\ncritiques that includes multi-perspective verifications as well as in-depth\ncritiques of initial critiques for each reasoning step. Then, we perform\nreinforcement learning on the fine-tuned model with either existing\nhuman-labeled data from PRM800K or our automatically annotated data obtained\nvia Monte Carlo sampling-based correctness estimation, to further incentivize\nits critique ability. Our developed critique model built on Qwen2.5-7B-Instruct\nnot only significantly outperforms existing LLM critics (including the\nsame-sized DeepSeek-R1-distill models and GPT-4o) on various error\nidentification benchmarks, but also more effectively helps the LLM generator\nrefine erroneous steps through more detailed feedback."}
{"id": "2505.00675", "pdf": "https://arxiv.org/pdf/2505.00675.pdf", "abs": "https://arxiv.org/abs/2505.00675", "title": "Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future Directions", "authors": ["Yiming Du", "Wenyu Huang", "Danna Zheng", "Zhaowei Wang", "Sebastien Montella", "Mirella Lapata", "Kam-Fai Wong", "Jeff Z. Pan"], "categories": ["cs.CL"], "comment": null, "summary": "Memory is a fundamental component of AI systems, underpinning large language\nmodels (LLMs) based agents. While prior surveys have focused on memory\napplications with LLMs, they often overlook the atomic operations that underlie\nmemory dynamics. In this survey, we first categorize memory representations\ninto parametric, contextual structured, and contextual unstructured and then\nintroduce six fundamental memory operations: Consolidation, Updating, Indexing,\nForgetting, Retrieval, and Compression. We systematically map these operations\nto the most relevant research topics across long-term, long-context, parametric\nmodification, and multi-source memory. By reframing memory systems through the\nlens of atomic operations and representation types, this survey provides a\nstructured and dynamic perspective on research, benchmark datasets, and tools\nrelated to memory in AI, clarifying the functional interplay in LLMs based\nagents while outlining promising directions for future research\\footnote{The\npaper list, datasets, methods and tools are available at\n\\href{https://github.com/Elvin-Yiming-Du/Survey_Memory_in_AI}{https://github.com/Elvin-Yiming-Du/Survey\\_Memory\\_in\\_AI}.}."}
{"id": "2505.00679", "pdf": "https://arxiv.org/pdf/2505.00679.pdf", "abs": "https://arxiv.org/abs/2505.00679", "title": "Steering Large Language Models with Register Analysis for Arbitrary Style Transfer", "authors": ["Xinchen Yang", "Marine Carpuat"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in\nrewriting text across various styles. However, effectively leveraging this\nability for example-based arbitrary style transfer, where an input text is\nrewritten to match the style of a given exemplar, remains an open challenge. A\nkey question is how to describe the style of the exemplar to guide LLMs toward\nhigh-quality rewrites. In this work, we propose a prompting method based on\nregister analysis to guide LLMs to perform this task. Empirical evaluations\nacross multiple style transfer tasks show that our prompting approach enhances\nstyle transfer strength while preserving meaning more effectively than existing\nprompting strategies."}
{"id": "2505.00049", "pdf": "https://arxiv.org/pdf/2505.00049.pdf", "abs": "https://arxiv.org/abs/2505.00049", "title": "Humanizing LLMs: A Survey of Psychological Measurements with Tools, Datasets, and Human-Agent Applications", "authors": ["Wenhan Dong", "Yuemeng Zhao", "Zhen Sun", "Yule Liu", "Zifan Peng", "Jingyi Zheng", "Zongmin Zhang", "Ziyi Zhang", "Jun Wu", "Ruiming Wang", "Shengmin Xu", "Xinyi Huang", "Xinlei He"], "categories": ["cs.CY", "cs.CL", "cs.HC", "cs.LG"], "comment": "26 pages,7 figures", "summary": "As large language models (LLMs) are increasingly used in human-centered\ntasks, assessing their psychological traits is crucial for understanding their\nsocial impact and ensuring trustworthy AI alignment. While existing reviews\nhave covered some aspects of related research, several important areas have not\nbeen systematically discussed, including detailed discussions of diverse\npsychological tests, LLM-specific psychological datasets, and the applications\nof LLMs with psychological traits. To address this gap, we systematically\nreview six key dimensions of applying psychological theories to LLMs: (1)\nassessment tools; (2) LLM-specific datasets; (3) evaluation metrics\n(consistency and stability); (4) empirical findings; (5) personality simulation\nmethods; and (6) LLM-based behavior simulation. Our analysis highlights both\nthe strengths and limitations of current methods. While some LLMs exhibit\nreproducible personality patterns under specific prompting schemes, significant\nvariability remains across tasks and settings. Recognizing methodological\nchallenges such as mismatches between psychological tools and LLMs'\ncapabilities, as well as inconsistencies in evaluation practices, this study\naims to propose future directions for developing more interpretable, robust,\nand generalizable psychological assessment frameworks for LLMs."}
{"id": "2505.00105", "pdf": "https://arxiv.org/pdf/2505.00105.pdf", "abs": "https://arxiv.org/abs/2505.00105", "title": "Optimization of embeddings storage for RAG systems using quantization and dimensionality reduction techniques", "authors": ["Naamán Huerga-Pérez", "Rubén Álvarez", "Rubén Ferrero-Guillén", "Alberto Martínez-Gutiérrez", "Javier Díez-González"], "categories": ["cs.IR", "cs.CL", "cs.DB"], "comment": "13 pages, 9 figures, 1 table", "summary": "Retrieval-Augmented Generation enhances language models by retrieving\nrelevant information from external knowledge bases, relying on high-dimensional\nvector embeddings typically stored in float32 precision. However, storing these\nembeddings at scale presents significant memory challenges. To address this\nissue, we systematically investigate on MTEB benchmark two complementary\noptimization strategies: quantization, evaluating standard formats (float16,\nint8, binary) and low-bit floating-point types (float8), and dimensionality\nreduction, assessing methods like PCA, Kernel PCA, UMAP, Random Projections and\nAutoencoders. Our results show that float8 quantization achieves a 4x storage\nreduction with minimal performance degradation (<0.3%), significantly\noutperforming int8 quantization at the same compression level, being simpler to\nimplement. PCA emerges as the most effective dimensionality reduction\ntechnique. Crucially, combining moderate PCA (e.g., retaining 50% dimensions)\nwith float8 quantization offers an excellent trade-off, achieving 8x total\ncompression with less performance impact than using int8 alone (which provides\nonly 4x compression). To facilitate practical application, we propose a\nmethodology based on visualizing the performance-storage trade-off space to\nidentify the optimal configuration that maximizes performance within their\nspecific memory constraints."}
{"id": "2505.00150", "pdf": "https://arxiv.org/pdf/2505.00150.pdf", "abs": "https://arxiv.org/abs/2505.00150", "title": "Detecting and Mitigating Hateful Content in Multimodal Memes with Vision-Language Models", "authors": ["Minh-Hao Van", "Xintao Wu"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "The rapid evolution of social media has provided enhanced communication\nchannels for individuals to create online content, enabling them to express\ntheir thoughts and opinions. Multimodal memes, often utilized for playful or\nhumorous expressions with visual and textual elements, are sometimes misused to\ndisseminate hate speech against individuals or groups. While the detection of\nhateful memes is well-researched, developing effective methods to transform\nhateful content in memes remains a significant challenge. Leveraging the\npowerful generation and reasoning capabilities of Vision-Language Models\n(VLMs), we address the tasks of detecting and mitigating hateful content. This\npaper presents two key contributions: first, a definition-guided prompting\ntechnique for detecting hateful memes, and second, a unified framework for\nmitigating hateful content in memes, named UnHateMeme, which works by replacing\nhateful textual and/or visual components. With our definition-guided prompts,\nVLMs achieve impressive performance on hateful memes detection task.\nFurthermore, our UnHateMeme framework, integrated with VLMs, demonstrates a\nstrong capability to convert hateful memes into non-hateful forms that meet\nhuman-level criteria for hate speech and maintain multimodal coherence between\nimage and text. Through empirical experiments, we show the effectiveness of\nstate-of-the-art pretrained VLMs such as LLaVA, Gemini and GPT-4o on the\nproposed tasks, providing a comprehensive analysis of their respective\nstrengths and limitations for these tasks. This paper aims to shed light on\nimportant applications of VLMs for ensuring safe and respectful online\nenvironments."}
{"id": "2505.00212", "pdf": "https://arxiv.org/pdf/2505.00212.pdf", "abs": "https://arxiv.org/abs/2505.00212", "title": "Which Agent Causes Task Failures and When? On Automated Failure Attribution of LLM Multi-Agent Systems", "authors": ["Shaokun Zhang", "Ming Yin", "Jieyu Zhang", "Jiale Liu", "Zhiguang Han", "Jingyang Zhang", "Beibin Li", "Chi Wang", "Huazheng Wang", "Yiran Chen", "Qingyun Wu"], "categories": ["cs.MA", "cs.CL"], "comment": null, "summary": "Failure attribution in LLM multi-agent systems-identifying the agent and step\nresponsible for task failures-provides crucial clues for systems debugging but\nremains underexplored and labor-intensive. In this paper, we propose and\nformulate a new research area: automated failure attribution for LLM\nmulti-agent systems. To support this initiative, we introduce the Who&When\ndataset, comprising extensive failure logs from 127 LLM multi-agent systems\nwith fine-grained annotations linking failures to specific agents and decisive\nerror steps. Using the Who&When, we develop and evaluate three automated\nfailure attribution methods, summarizing their corresponding pros and cons. The\nbest method achieves 53.5% accuracy in identifying failure-responsible agents\nbut only 14.2% in pinpointing failure steps, with some methods performing below\nrandom. Even SOTA reasoning models, such as OpenAI o1 and DeepSeek R1, fail to\nachieve practical usability. These results highlight the task's complexity and\nthe need for further research in this area. Code and dataset are available at\nhttps://github.com/mingyin1/Agents_Failure_Attribution"}
{"id": "2505.00234", "pdf": "https://arxiv.org/pdf/2505.00234.pdf", "abs": "https://arxiv.org/abs/2505.00234", "title": "Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks", "authors": ["Vishnu Sarukkai", "Zhiqiang Xie", "Kayvon Fatahalian"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Many methods for improving Large Language Model (LLM) agents for sequential\ndecision-making tasks depend on task-specific knowledge engineering--such as\nprompt tuning, curated in-context examples, or customized observation and\naction spaces. Using these approaches, agent performance improves with the\nquality or amount of knowledge engineering invested. Instead, we investigate\nhow LLM agents can automatically improve their performance by learning\nin-context from their own successful experiences on similar tasks. Rather than\nrelying on task-specific knowledge engineering, we focus on constructing and\nrefining a database of self-generated examples. We demonstrate that even a\nnaive accumulation of successful trajectories across training tasks boosts test\nperformance on three benchmarks: ALFWorld (73% to 89%), Wordcraft (55% to 64%),\nand InterCode-SQL (75% to 79%)--matching the performance the initial agent\nachieves if allowed two to three attempts per task. We then introduce two\nextensions: (1) database-level selection through population-based training to\nidentify high-performing example collections, and (2) exemplar-level selection\nthat retains individual trajectories based on their empirical utility as\nin-context examples. These extensions further enhance performance, achieving\n91% on ALFWorld--matching more complex approaches that employ task-specific\ncomponents and prompts. Our results demonstrate that automatic trajectory\ndatabase construction offers a compelling alternative to labor-intensive\nknowledge engineering."}
{"id": "2505.00263", "pdf": "https://arxiv.org/pdf/2505.00263.pdf", "abs": "https://arxiv.org/abs/2505.00263", "title": "EnronQA: Towards Personalized RAG over Private Documents", "authors": ["Michael J. Ryan", "Danmei Xu", "Chris Nivera", "Daniel Campos"], "categories": ["cs.IR", "cs.CL"], "comment": "26 pages, 4 figures, 6 tables", "summary": "Retrieval Augmented Generation (RAG) has become one of the most popular\nmethods for bringing knowledge-intensive context to large language models (LLM)\nbecause of its ability to bring local context at inference time without the\ncost or data leakage risks associated with fine-tuning. A clear separation of\nprivate information from the LLM training has made RAG the basis for many\nenterprise LLM workloads as it allows the company to augment LLM's\nunderstanding using customers' private documents. Despite its popularity for\nprivate documents in enterprise deployments, current RAG benchmarks for\nvalidating and optimizing RAG pipelines draw their corpora from public data\nsuch as Wikipedia or generic web pages and offer little to no personal context.\nSeeking to empower more personal and private RAG we release the EnronQA\nbenchmark, a dataset of 103,638 emails with 528,304 question-answer pairs\nacross 150 different user inboxes. EnronQA enables better benchmarking of RAG\npipelines over private data and allows for experimentation on the introduction\nof personalized retrieval settings over realistic data. Finally, we use EnronQA\nto explore the tradeoff in memorization and retrieval when reasoning over\nprivate documents."}
{"id": "2505.00315", "pdf": "https://arxiv.org/pdf/2505.00315.pdf", "abs": "https://arxiv.org/abs/2505.00315", "title": "Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing", "authors": ["Piotr Piękos", "Róbert Csordás", "Jürgen Schmidhuber"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Recent advances in large language models highlighted the excessive quadratic\ncost of self-attention. Despite the significant research efforts, subquadratic\nattention methods still suffer from inferior performance in practice. We\nhypothesize that dynamic, learned content-based sparsity can lead to more\nefficient attention mechanisms. We present Mixture of Sparse Attention (MoSA),\na novel approach inspired by Mixture of Experts (MoE) with expert choice\nrouting. MoSA dynamically selects tokens for each attention head, allowing\narbitrary sparse attention patterns. By selecting $k$ tokens from a sequence of\nlength $T$, MoSA reduces the computational complexity of each attention head\nfrom $O(T^2)$ to $O(k^2 + T)$. This enables using more heads within the same\ncomputational budget, allowing higher specialization. We show that among the\ntested sparse attention variants, MoSA is the only one that can outperform the\ndense baseline, sometimes with up to 27% better perplexity for an identical\ncompute budget. MoSA can also reduce the resource usage compared to dense\nself-attention. Despite using torch implementation without an optimized kernel,\nperplexity-matched MoSA models are simultaneously faster in wall-clock time,\nrequire less memory for training, and drastically reduce the size of the\nKV-cache compared to the dense transformer baselines."}
{"id": "2505.00337", "pdf": "https://arxiv.org/pdf/2505.00337.pdf", "abs": "https://arxiv.org/abs/2505.00337", "title": "T2VPhysBench: A First-Principles Benchmark for Physical Consistency in Text-to-Video Generation", "authors": ["Xuyang Guo", "Jiayan Huo", "Zhenmei Shi", "Zhao Song", "Jiahao Zhang", "Jiale Zhao"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Text-to-video generative models have made significant strides in recent\nyears, producing high-quality videos that excel in both aesthetic appeal and\naccurate instruction following, and have become central to digital art creation\nand user engagement online. Yet, despite these advancements, their ability to\nrespect fundamental physical laws remains largely untested: many outputs still\nviolate basic constraints such as rigid-body collisions, energy conservation,\nand gravitational dynamics, resulting in unrealistic or even misleading\ncontent. Existing physical-evaluation benchmarks typically rely on automatic,\npixel-level metrics applied to simplistic, life-scenario prompts, and thus\noverlook both human judgment and first-principles physics. To fill this gap, we\nintroduce \\textbf{T2VPhysBench}, a first-principled benchmark that\nsystematically evaluates whether state-of-the-art text-to-video systems, both\nopen-source and commercial, obey twelve core physical laws including Newtonian\nmechanics, conservation principles, and phenomenological effects. Our benchmark\nemploys a rigorous human evaluation protocol and includes three targeted\nstudies: (1) an overall compliance assessment showing that all models score\nbelow 0.60 on average in each law category; (2) a prompt-hint ablation\nrevealing that even detailed, law-specific hints fail to remedy physics\nviolations; and (3) a counterfactual robustness test demonstrating that models\noften generate videos that explicitly break physical rules when so instructed.\nThe results expose persistent limitations in current architectures and offer\nconcrete insights for guiding future research toward truly physics-aware video\ngeneration."}
{"id": "2505.00358", "pdf": "https://arxiv.org/pdf/2505.00358.pdf", "abs": "https://arxiv.org/abs/2505.00358", "title": "R&B: Domain Regrouping and Data Mixture Balancing for Efficient Foundation Model Training", "authors": ["Albert Ge", "Tzu-Heng Huang", "John Cooper", "Avi Trost", "Ziyi Chu", "Satya Sai Srinath Namburi GNVV", "Ziyang Cai", "Kendall Park", "Nicholas Roberts", "Frederic Sala"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Data mixing strategies have successfully reduced the costs involved in\ntraining language models. While promising, such methods suffer from two flaws.\nFirst, they rely on predetermined data domains (e.g., data sources, task\ntypes), which may fail to capture critical semantic nuances, leaving\nperformance on the table. Second, these methods scale with the number of\ndomains in a computationally prohibitive way. We address these challenges via\nR&B, a framework that re-partitions training data based on semantic similarity\n(Regroup) to create finer-grained domains, and efficiently optimizes the data\ncomposition (Balance) by leveraging a Gram matrix induced by domain gradients\nobtained throughout training. Unlike prior works, it removes the need for\nadditional compute to obtain evaluation information such as losses or\ngradients. We analyze this technique under standard regularity conditions and\nprovide theoretical insights that justify R&B's effectiveness compared to\nnon-adaptive mixing approaches. Empirically, we demonstrate the effectiveness\nof R&B on five diverse datasets ranging from natural language to reasoning and\nmultimodal tasks. With as little as 0.01% additional compute overhead, R&B\nmatches or exceeds the performance of state-of-the-art data mixing strategies."}
{"id": "2505.00422", "pdf": "https://arxiv.org/pdf/2505.00422.pdf", "abs": "https://arxiv.org/abs/2505.00422", "title": "Toward Automated Regulatory Decision-Making: Trustworthy Medical Device Risk Classification with Multimodal Transformers and Self-Training", "authors": ["Yu Han", "Aaron Ceross", "Jeroen H. M. Bergmann"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Accurate classification of medical device risk levels is essential for\nregulatory oversight and clinical safety. We present a Transformer-based\nmultimodal framework that integrates textual descriptions and visual\ninformation to predict device regulatory classification. The model incorporates\na cross-attention mechanism to capture intermodal dependencies and employs a\nself-training strategy for improved generalization under limited supervision.\nExperiments on a real-world regulatory dataset demonstrate that our approach\nachieves up to 90.4% accuracy and 97.9% AUROC, significantly outperforming\ntext-only (77.2%) and image-only (54.8%) baselines. Compared to standard\nmultimodal fusion, the self-training mechanism improved SVM performance by 3.3\npercentage points in accuracy (from 87.1% to 90.4%) and 1.4 points in macro-F1,\nsuggesting that pseudo-labeling can effectively enhance generalization under\nlimited supervision. Ablation studies further confirm the complementary\nbenefits of both cross-modal attention and self-training."}
{"id": "2505.00649", "pdf": "https://arxiv.org/pdf/2505.00649.pdf", "abs": "https://arxiv.org/abs/2505.00649", "title": "Investigating Task Arithmetic for Zero-Shot Information Retrieval", "authors": ["Marco Braga", "Pranav Kasela", "Alessandro Raganato", "Gabriella Pasi"], "categories": ["cs.IR", "cs.CL", "cs.LG"], "comment": "Accepted in SIGIR '25", "summary": "Large Language Models (LLMs) have shown impressive zero-shot performance\nacross a variety of Natural Language Processing tasks, including document\nre-ranking. However, their effectiveness degrades on unseen tasks and domains,\nlargely due to shifts in vocabulary and word distributions. In this paper, we\ninvestigate Task Arithmetic, a technique that combines the weights of LLMs\npre-trained on different tasks or domains via simple mathematical operations,\nsuch as addition or subtraction, to adapt retrieval models without requiring\nadditional fine-tuning. Our method is able to synthesize diverse tasks and\ndomain knowledge into a single model, enabling effective zero-shot adaptation\nin different retrieval contexts. Extensive experiments on publicly available\nscientific, biomedical, and multilingual datasets show that our method improves\nstate-of-the-art re-ranking performance by up to 18% in NDCG@10 and 15% in\nP@10. In addition to these empirical gains, our analysis provides insights into\nthe strengths and limitations of Task Arithmetic as a practical strategy for\nzero-shot learning and model adaptation. We make our code publicly available at\nhttps://github.com/DetectiveMB/Task-Arithmetic-for-ZS-IR."}
{"id": "2505.00703", "pdf": "https://arxiv.org/pdf/2505.00703.pdf", "abs": "https://arxiv.org/abs/2505.00703", "title": "T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level and Token-level CoT", "authors": ["Dongzhi Jiang", "Ziyu Guo", "Renrui Zhang", "Zhuofan Zong", "Hao Li", "Le Zhuo", "Shilin Yan", "Pheng-Ann Heng", "Hongsheng Li"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Project Page: https://github.com/CaraJ7/T2I-R1", "summary": "Recent advancements in large language models have demonstrated how\nchain-of-thought (CoT) and reinforcement learning (RL) can improve performance.\nHowever, applying such reasoning strategies to the visual generation domain\nremains largely unexplored. In this paper, we present T2I-R1, a novel\nreasoning-enhanced text-to-image generation model, powered by RL with a\nbi-level CoT reasoning process. Specifically, we identify two levels of CoT\nthat can be utilized to enhance different stages of generation: (1) the\nsemantic-level CoT for high-level planning of the prompt and (2) the\ntoken-level CoT for low-level pixel processing during patch-by-patch\ngeneration. To better coordinate these two levels of CoT, we introduce\nBiCoT-GRPO with an ensemble of generation rewards, which seamlessly optimizes\nboth generation CoTs within the same training step. By applying our reasoning\nstrategies to the baseline model, Janus-Pro, we achieve superior performance\nwith 13% improvement on T2I-CompBench and 19% improvement on the WISE\nbenchmark, even surpassing the state-of-the-art model FLUX.1. Code is available\nat: https://github.com/CaraJ7/T2I-R1"}
{"id": "2309.08532", "pdf": "https://arxiv.org/pdf/2309.08532.pdf", "abs": "https://arxiv.org/abs/2309.08532", "title": "EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful Prompt Optimizers", "authors": ["Qingyan Guo", "Rui Wang", "Junliang Guo", "Bei Li", "Kaitao Song", "Xu Tan", "Guoqing Liu", "Jiang Bian", "Yujiu Yang"], "categories": ["cs.CL", "cs.AI"], "comment": "International Conference on Learning Representations (ICLR) 2024", "summary": "Large Language Models (LLMs) excel in various tasks, but they rely on\ncarefully crafted prompts that often demand substantial human effort. To\nautomate this process, in this paper, we propose a novel framework for discrete\nprompt optimization, called EvoPrompt, which borrows the idea of evolutionary\nalgorithms (EAs) as they exhibit good performance and fast convergence. To\nenable EAs to work on discrete prompts, which are natural language expressions\nthat need to be coherent and human-readable, we connect LLMs with EAs. This\napproach allows us to simultaneously leverage the powerful language processing\ncapabilities of LLMs and the efficient optimization performance of EAs.\nSpecifically, abstaining from any gradients or parameters, EvoPrompt starts\nfrom a population of prompts and iteratively generates new prompts with LLMs\nbased on the evolutionary operators, improving the population based on the\ndevelopment set. We optimize prompts for both closed- and open-source LLMs\nincluding GPT-3.5 and Alpaca, on 31 datasets covering language understanding,\ngeneration tasks, as well as BIG-Bench Hard (BBH) tasks. EvoPrompt\nsignificantly outperforms human-engineered prompts and existing methods for\nautomatic prompt generation (e.g., up to 25% on BBH). Furthermore, EvoPrompt\ndemonstrates that connecting LLMs with EAs creates synergies, which could\ninspire further research on the combination of LLMs and conventional\nalgorithms."}
{"id": "2401.15371", "pdf": "https://arxiv.org/pdf/2401.15371.pdf", "abs": "https://arxiv.org/abs/2401.15371", "title": "LegalDuet: Learning Fine-grained Representations for Legal Judgment Prediction via a Dual-View Contrastive Learning", "authors": ["Buqiang Xu", "Xin Dai", "Zhenghao Liu", "Huiyuan Xie", "Xiaoyuan Yi", "Shuo Wang", "Yukun Yan", "Liner Yang", "Yu Gu", "Ge Yu"], "categories": ["cs.CL"], "comment": null, "summary": "Legal Judgment Prediction (LJP) is a fundamental task of legal artificial\nintelligence, aiming to automatically predict the judgment outcomes of legal\ncases. Existing LJP models primarily focus on identifying legal triggers within\ncriminal fact descriptions by contrastively training language models. However,\nthese LJP models overlook the importance of learning to effectively distinguish\nsubtle differences among judgments, which is crucial for producing more\naccurate predictions. In this paper, we propose LegalDuet, which continuously\npretrains language models to learn a more tailored embedding space for\nrepresenting legal cases. Specifically, LegalDuet designs a dual-view mechanism\nto continuously pretrain language models: 1) Law Case Clustering retrieves\nsimilar cases as hard negatives and employs contrastive training to\ndifferentiate among confusing cases; 2) Legal Decision Matching aims to\nidentify legal clues within criminal fact descriptions to align them with the\nchain of reasoning that contains the correct legal decision. Our experiments on\nthe CAIL2018 dataset demonstrate the effectiveness of LegalDuet. Further\nanalysis reveals that LegalDuet improves the ability of pretrained language\nmodels to distinguish confusing criminal charges by reducing prediction\nuncertainty and enhancing the separability of criminal charges. The experiments\ndemonstrate that LegalDuet produces a more concentrated and distinguishable\nembedding space, effectively aligning criminal facts with corresponding legal\ndecisions. The code is available at https://github.com/NEUIR/LegalDuet."}
{"id": "2402.08498", "pdf": "https://arxiv.org/pdf/2402.08498.pdf", "abs": "https://arxiv.org/abs/2402.08498", "title": "\"Reasoning\" with Rhetoric: On the Style-Evidence Tradeoff in LLM-Generated Counter-Arguments", "authors": ["Preetika Verma", "Kokil Jaidka", "Svetlana Churina"], "categories": ["cs.CL"], "comment": "22 pages, 9 figures, 13 tables", "summary": "Large language models (LLMs) play a key role in generating evidence-based and\nstylistic counter-arguments, yet their effectiveness in real-world applications\nhas been underexplored. Previous research often neglects the balance between\nevidentiality and style, which are crucial for persuasive arguments. To address\nthis, we evaluated the effectiveness of stylized evidence-based\ncounter-argument generation in Counterfire, a new dataset of 38,000\ncounter-arguments generated by revising counter-arguments to Reddit's\nChangeMyView community to follow different discursive styles. We evaluated\ngeneric and stylized counter-arguments from basic and fine-tuned models such as\nGPT-3.5, PaLM-2, and Koala-13B, as well as newer models (GPT-4o, Claude Haiku,\nLLaMA-3.1) focusing on rhetorical quality and persuasiveness. Our findings\nreveal that humans prefer stylized counter-arguments over the original outputs,\nwith GPT-3.5 Turbo performing well, though still not reaching human standards\nof rhetorical quality nor persuasiveness. Additionally, our work created a\nnovel argument triplets dataset for studying style control, with human\npreference labels that provide insights into the tradeoffs between evidence\nintegration and argument quality."}
{"id": "2405.04532", "pdf": "https://arxiv.org/pdf/2405.04532.pdf", "abs": "https://arxiv.org/abs/2405.04532", "title": "QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving", "authors": ["Yujun Lin", "Haotian Tang", "Shang Yang", "Zhekai Zhang", "Guangxuan Xiao", "Chuang Gan", "Song Han"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PF"], "comment": "The first three authors contribute equally to this project and are\n  listed in the alphabetical order. Yujun Lin leads the quantization algorithm,\n  Haotian Tang and Shang Yang lead the GPU kernels and the serving system. Code\n  is available at https://github.com/mit-han-lab/omniserve", "summary": "Quantization can accelerate large language model (LLM) inference. Going\nbeyond INT8 quantization, the research community is actively exploring even\nlower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization\ntechniques only accelerate low-batch, edge LLM inference, failing to deliver\nperformance gains in large-batch, cloud-based LLM serving. We uncover a\ncritical issue: existing INT4 quantization methods suffer from significant\nruntime overhead (20-90%) when dequantizing either weights or partial sums on\nGPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization\nalgorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands\nfor quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented\nby the QServe inference library that achieves measured speedup. The key insight\ndriving QServe is that the efficiency of LLM serving on GPUs is critically\ninfluenced by operations on low-throughput CUDA cores. Building upon this\ninsight, in QoQ algorithm, we introduce progressive quantization that can allow\nlow dequantization overhead in W4A8 GEMM. Additionally, we develop\nSmoothAttention to effectively mitigate the accuracy degradation incurred by\n4-bit KV quantization. In the QServe system, we perform compute-aware weight\nreordering and take advantage of register-level parallelism to reduce\ndequantization latency. We also make fused attention memory-bound, harnessing\nthe performance gain brought by KV4 quantization. As a result, QServe improves\nthe maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x\non L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to\nTensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput\nthan TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of\nLLM serving by 3x. Code is available at\nhttps://github.com/mit-han-lab/omniserve."}
{"id": "2405.11804", "pdf": "https://arxiv.org/pdf/2405.11804.pdf", "abs": "https://arxiv.org/abs/2405.11804", "title": "(Perhaps) Beyond Human Translation: Harnessing Multi-Agent Collaboration for Translating Ultra-Long Literary Texts", "authors": ["Minghao Wu", "Jiahao Xu", "Yulin Yuan", "Gholamreza Haffari", "Longyue Wang", "Weihua Luo", "Kaifu Zhang"], "categories": ["cs.CL"], "comment": "To appear at TACL", "summary": "Literary translation remains one of the most challenging frontiers in machine\ntranslation due to the complexity of capturing figurative language, cultural\nnuances, and unique stylistic elements. In this work, we introduce TransAgents,\na novel multi-agent framework that simulates the roles and collaborative\npractices of a human translation company, including a CEO, Senior Editor,\nJunior Editor, Translator, Localization Specialist, and Proofreader. The\ntranslation process is divided into two stages: a preparation stage where the\nteam is assembled and comprehensive translation guidelines are drafted, and an\nexecution stage that involves sequential translation, localization,\nproofreading, and a final quality check. Furthermore, we propose two innovative\nevaluation strategies: Monolingual Human Preference (MHP), which evaluates\ntranslations based solely on target language quality and cultural\nappropriateness, and Bilingual LLM Preference (BLP), which leverages large\nlanguage models like GPT-4} for direct text comparison. Although TransAgents\nachieves lower d-BLEU scores, due to the limited diversity of references, its\ntranslations are significantly better than those of other baselines and are\npreferred by both human evaluators and LLMs over traditional human references\nand GPT-4} translations. Our findings highlight the potential of multi-agent\ncollaboration in enhancing translation quality, particularly for longer texts."}
{"id": "2407.20906", "pdf": "https://arxiv.org/pdf/2407.20906.pdf", "abs": "https://arxiv.org/abs/2407.20906", "title": "Automated Review Generation Method Based on Large Language Models", "authors": ["Shican Wu", "Xiao Ma", "Dehui Luo", "Lulu Li", "Xiangcheng Shi", "Xin Chang", "Xiaoyun Lin", "Ran Luo", "Chunlei Pei", "Changying Du", "Zhi-Jian Zhao", "Jinlong Gong"], "categories": ["cs.CL", "cs.AI", "physics.data-an"], "comment": "Code: https://github.com/TJU-ECAT-AI/AutomaticReviewGeneration Data:\n  https://github.com/TJU-ECAT-AI/AutomaticReviewGenerationData This research\n  has been invited for a Short Oral presentation at the 18th ICC -\n  International Congress on Catalysis, taking place in Lyon, France from July\n  14-19, 2024 Published at https://doi.org/10.1093/nsr/nwaf169 for newer\n  edition", "summary": "Literature research, vital for scientific work, faces the challenge of\nsurging information volumes exceeding researchers' processing capabilities. We\npresent an automated review generation method based on large language models\n(LLMs) to overcome efficiency bottlenecks and reduce cognitive load. Our\nstatistically validated evaluation framework demonstrates that the generated\nreviews match or exceed manual quality, offering broad applicability across\nresearch fields without requiring users' domain knowledge. Applied to propane\ndehydrogenation (PDH) catalysts, our method swiftly analyzed 343 articles,\naveraging seconds per article per LLM account, producing comprehensive reviews\nspanning 35 topics, with extended analysis of 1041 articles providing insights\ninto catalysts' properties. Through multi-layered quality control, we\neffectively mitigated LLMs' hallucinations, with expert verification confirming\naccuracy and citation integrity while demonstrating hallucination risks reduced\nto below 0.5\\% with 95\\% confidence. Released Windows application enables\none-click review generation, enhancing research productivity and literature\nrecommendation efficiency while setting the stage for broader scientific\nexplorations."}
{"id": "2410.01957", "pdf": "https://arxiv.org/pdf/2410.01957.pdf", "abs": "https://arxiv.org/abs/2410.01957", "title": "Challenges and Future Directions of Data-Centric AI Alignment", "authors": ["Min-Hsuan Yeh", "Jeffrey Wang", "Xuefeng Du", "Seongheon Park", "Leitian Tao", "Shawn Im", "Yixuan Li"], "categories": ["cs.CL"], "comment": "ICML 2025", "summary": "As AI systems become increasingly capable and influential, ensuring their\nalignment with human values, preferences, and goals has become a critical\nresearch focus. Current alignment methods primarily focus on designing\nalgorithms and loss functions but often underestimate the crucial role of data.\nThis paper advocates for a shift towards data-centric AI alignment, emphasizing\nthe need to enhance the quality and representativeness of data used in aligning\nAI systems. In this position paper, we highlight key challenges associated with\nboth human-based and AI-based feedback within the data-centric alignment\nframework. Through qualitative analysis, we identify multiple sources of\nunreliability in human feedback, as well as problems related to temporal drift,\ncontext dependence, and AI-based feedback failing to capture human values due\nto inherent model limitations. We propose future research directions, including\nimproved feedback collection practices, robust data-cleaning methodologies, and\nrigorous feedback verification processes. We call for future research into\nthese critical directions to ensure, addressing gaps that persist in\nunderstanding and improving data-centric alignment practices."}
{"id": "2410.20774", "pdf": "https://arxiv.org/pdf/2410.20774.pdf", "abs": "https://arxiv.org/abs/2410.20774", "title": "Are LLM-Judges Robust to Expressions of Uncertainty? Investigating the effect of Epistemic Markers on LLM-based Evaluation", "authors": ["Dongryeol Lee", "Yerin Hwang", "Yongil Kim", "Joonsuk Park", "Kyomin Jung"], "categories": ["cs.CL"], "comment": "NAACL 2025 Oral (21 pages, 6 figures, 15 tables)", "summary": "In line with the principle of honesty, there has been a growing effort to\ntrain large language models (LLMs) to generate outputs containing epistemic\nmarkers. However, evaluation in the presence of epistemic markers has been\nlargely overlooked, raising a critical question: Could the use of epistemic\nmarkers in LLM-generated outputs lead to unintended negative consequences? To\naddress this, we present EMBER, a benchmark designed to assess the robustness\nof LLM-judges to epistemic markers in both single and pairwise evaluation\nsettings. Our findings, based on evaluations using EMBER, reveal that all\ntested LLM-judges, including GPT-4o, show a notable lack of robustness in the\npresence of epistemic markers. Specifically, we observe a negative bias toward\nepistemic markers, with a stronger bias against markers expressing uncertainty.\nThis suggests that LLM-judges are influenced by the presence of these markers\nand do not focus solely on the correctness of the content."}
{"id": "2412.05862", "pdf": "https://arxiv.org/pdf/2412.05862.pdf", "abs": "https://arxiv.org/abs/2412.05862", "title": "Domain-Specific Translation with Open-Source Large Language Models: Resource-Oriented Analysis", "authors": ["Aman Kassahun Wassie", "Mahdi Molaei", "Yasmin Moslem"], "categories": ["cs.CL"], "comment": null, "summary": "In this work, we compare the domain-specific translation performance of\nopen-source autoregressive decoder-only large language models (LLMs) with\ntask-oriented machine translation (MT) models. Our experiments focus on the\nmedical domain and cover four language directions with varied resource\navailability: English-to-French, English-to-Portuguese, English-to-Swahili, and\nSwahili-to-English. Despite recent advancements, LLMs demonstrate a significant\nquality gap in specialized translation compared to multilingual encoder-decoder\nMT models such as NLLB-200. Our results indicate that NLLB-200 3.3B outperforms\nall evaluated LLMs in the 7-8B parameter range across three out of the four\nlanguage directions. While fine-tuning improves the performance of LLMs such as\nMistral and Llama, these models still underperform compared to fine-tuned\nNLLB-200 3.3B models. Our findings highlight the ongoing need for specialized\nMT models to achieve high-quality domain-specific translation, especially in\nmedium-resource and low-resource settings. Moreover, the superior performance\nof larger LLMs over their 8B variants suggests potential value in pre-training\ndomain-specific medium-sized language models, employing targeted data selection\nand knowledge distillation approaches to enhance both quality and efficiency in\nspecialized translation tasks."}
{"id": "2501.00571", "pdf": "https://arxiv.org/pdf/2501.00571.pdf", "abs": "https://arxiv.org/abs/2501.00571", "title": "KnowRA: Knowledge Retrieval Augmented Method for Document-level Relation Extraction with Comprehensive Reasoning Abilities", "authors": ["Chengcheng Mai", "Yuxiang Wang", "Ziyu Gong", "Hanxiang Wang", "Yihua Huang"], "categories": ["cs.CL"], "comment": "This work has been accepted by IJCAI 2025 (CCF A)", "summary": "Document-level relation extraction (Doc-RE) aims to extract relations between\nentities across multiple sentences. Therefore, Doc-RE requires more\ncomprehensive reasoning abilities like humans, involving complex cross-sentence\ninteractions between entities, contexts, and external general knowledge,\ncompared to the sentence-level RE. However, most existing Doc-RE methods focus\non optimizing single reasoning ability, but lack the ability to utilize\nexternal knowledge for comprehensive reasoning on long documents. To solve\nthese problems, a knowledge retrieval augmented method, named KnowRA, was\nproposed with comprehensive reasoning to autonomously determine whether to\naccept external knowledge to assist DocRE. Firstly, we constructed a document\ngraph for semantic encoding and integrated the co-reference resolution model to\naugment the co-reference reasoning ability. Then, we expanded the document\ngraph into a document knowledge graph by retrieving the external knowledge base\nfor common-sense reasoning and a novel knowledge filtration method was\npresented to filter out irrelevant knowledge. Finally, we proposed the axis\nattention mechanism to build direct and indirect associations with intermediary\nentities for achieving cross-sentence logical reasoning. Extensive experiments\nconducted on two datasets verified the effectiveness of our method compared to\nthe state-of-the-art baselines. Our code is available at\nhttps://anonymous.4open.science/r/KnowRA."}
{"id": "2501.13947", "pdf": "https://arxiv.org/pdf/2501.13947.pdf", "abs": "https://arxiv.org/abs/2501.13947", "title": "A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods", "authors": ["Wenli Yang", "Lilian Some", "Michael Bain", "Byeong Kang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid development of artificial intelligence has led to marked progress\nin the field. One interesting direction for research is whether Large Language\nModels (LLMs) can be integrated with structured knowledge-based systems. This\napproach aims to combine the generative language understanding of LLMs and the\nprecise knowledge representation systems by which they are integrated. This\narticle surveys the relationship between LLMs and knowledge bases, looks at how\nthey can be applied in practice, and discusses related technical, operational,\nand ethical challenges. Utilizing a comprehensive examination of the\nliterature, the study both identifies important issues and assesses existing\nsolutions. It demonstrates the merits of incorporating generative AI into\nstructured knowledge-base systems concerning data contextualization, model\naccuracy, and utilization of knowledge resources. The findings give a full list\nof the current situation of research, point out the main gaps, and propose\nhelpful paths to take. These insights contribute to advancing AI technologies\nand support their practical deployment across various sectors."}
{"id": "2502.05945", "pdf": "https://arxiv.org/pdf/2502.05945.pdf", "abs": "https://arxiv.org/abs/2502.05945", "title": "HSI: Head-Specific Intervention Can Induce Misaligned AI Coordination in Large Language Models", "authors": ["Paul Darm", "Annalisa Riccardi"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "Large Language Models (LLMs), Interference-time activation shifting,\n  Steerability, Explainability, AI alignment, Interpretability", "summary": "Robust alignment guardrails for large language models are becoming\nincreasingly important with their widespread application. In contrast to\nprevious studies, we demonstrate that inference-time activation interventions\ncan bypass safety alignments and effectively steer model generations towards\nharmful AI coordination for Llama 2. Our method applies fine-grained\ninterventions at specific model subcomponents, particularly attention heads,\nusing a simple binary choice probing strategy. These interventions then\ngeneralise to the open-ended generation setting effectively circumventing\nsafety guardrails. We show that probing single attention heads is more\neffective than intervening on full layers and intervening on only four\nattention heads is comparable to supervised fine-tuning. We further show that\nonly a few example completions are needed to compute effective steering\ndirections, which is an advantage over classical fine-tuning. Our findings\nhighlight the shortcomings of current alignment techniques. In addition, our\nresults suggest that, at the attention head level, activations encode\nfine-grained linearly separable behaviors. Practically, the approach offers a\nstraightforward methodology to steer large language model behaviour, which\ncould be extended to diverse domains beyond safety requiring fine-grained\ncontrol over the model output. The code and datasets for this study can be\nfound on https://github.com/PaulDrm/targeted_intervention."}
{"id": "2502.07963", "pdf": "https://arxiv.org/pdf/2502.07963.pdf", "abs": "https://arxiv.org/abs/2502.07963", "title": "Caught in the Web of Words: Do LLMs Fall for Spin in Medical Literature?", "authors": ["Hye Sun Yun", "Karen Y. C. Zhang", "Ramez Kouzy", "Iain J. Marshall", "Junyi Jessy Li", "Byron C. Wallace"], "categories": ["cs.CL", "cs.AI"], "comment": "22 pages, 12 figures, 4 tables", "summary": "Medical research faces well-documented challenges in translating novel\ntreatments into clinical practice. Publishing incentives encourage researchers\nto present \"positive\" findings, even when empirical results are equivocal.\nConsequently, it is well-documented that authors often spin study results,\nespecially in article abstracts. Such spin can influence clinician\ninterpretation of evidence and may affect patient care decisions. In this\nstudy, we ask whether the interpretation of trial results offered by Large\nLanguage Models (LLMs) is similarly affected by spin. This is important since\nLLMs are increasingly being used to trawl through and synthesize published\nmedical evidence. We evaluated 22 LLMs and found that they are across the board\nmore susceptible to spin than humans. They might also propagate spin into their\noutputs: We find evidence, e.g., that LLMs implicitly incorporate spin into\nplain language summaries that they generate. We also find, however, that LLMs\nare generally capable of recognizing spin, and can be prompted in a way to\nmitigate spin's impact on LLM outputs."}
{"id": "2502.20984", "pdf": "https://arxiv.org/pdf/2502.20984.pdf", "abs": "https://arxiv.org/abs/2502.20984", "title": "UoR-NCL at SemEval-2025 Task 1: Using Generative LLMs and CLIP Models for Multilingual Multimodal Idiomaticity Representation", "authors": ["Thanet Markchom", "Tong Wu", "Liting Huang", "Huizhi Liang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "SemEval-2025 Task 1 focuses on ranking images based on their alignment with a\ngiven nominal compound that may carry idiomatic meaning in both English and\nBrazilian Portuguese. To address this challenge, this work uses generative\nlarge language models (LLMs) and multilingual CLIP models to enhance idiomatic\ncompound representations. LLMs generate idiomatic meanings for potentially\nidiomatic compounds, enriching their semantic interpretation. These meanings\nare then encoded using multilingual CLIP models, serving as representations for\nimage ranking. Contrastive learning and data augmentation techniques are\napplied to fine-tune these embeddings for improved performance. Experimental\nresults show that multimodal representations extracted through this method\noutperformed those based solely on the original nominal compounds. The\nfine-tuning approach shows promising outcomes but is less effective than using\nembeddings without fine-tuning. The source code used in this paper is available\nat https://github.com/tongwu17/SemEval-2025-Task1-UoR-NCL."}
{"id": "2503.23895", "pdf": "https://arxiv.org/pdf/2503.23895.pdf", "abs": "https://arxiv.org/abs/2503.23895", "title": "Dynamic Parametric Retrieval Augmented Generation for Test-time Knowledge Enhancement", "authors": ["Yuqiao Tan", "Shizhu He", "Huanxuan Liao", "Jun Zhao", "Kang Liu"], "categories": ["cs.CL", "cs.AI"], "comment": "preprint. Code is available at https://github.com/Trae1ounG/DyPRAG", "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nretrieving relevant documents from external sources and incorporating them into\nthe context. While it improves reliability by providing factual texts, it\nsignificantly increases inference costs as context length grows and introduces\nchallenging issue of RAG hallucination, primarily caused by the lack of\ncorresponding parametric knowledge in LLMs. An efficient solution is to enhance\nthe knowledge of LLMs at test-time. Parametric RAG (PRAG) addresses this by\nembedding document into LLMs parameters to perform test-time knowledge\nenhancement, effectively reducing inference costs through offline training.\nHowever, its high training and storage costs, along with limited generalization\nability, significantly restrict its practical adoption. To address these\nchallenges, we propose Dynamic Parametric RAG (DyPRAG), a novel framework that\nleverages a lightweight parameter translator model to efficiently convert\ndocuments into parametric knowledge. DyPRAG not only reduces inference,\ntraining, and storage costs but also dynamically generates parametric\nknowledge, seamlessly enhancing the knowledge of LLMs and resolving knowledge\nconflicts in a plug-and-play manner at test-time. Extensive experiments on\nmultiple datasets demonstrate the effectiveness and generalization capabilities\nof DyPRAG, offering a powerful and practical RAG paradigm which enables\nsuperior knowledge fusion and mitigates RAG hallucination in real-world\napplications. Our code is available at https://github.com/Trae1ounG/DyPRAG."}
{"id": "2504.00027", "pdf": "https://arxiv.org/pdf/2504.00027.pdf", "abs": "https://arxiv.org/abs/2504.00027", "title": "Opioid Named Entity Recognition (ONER-2025) from Reddit", "authors": ["Grigori Sidorov", "Muhammad Ahmad", "Iqra Ameer", "Muhammad Usman", "Ildar Batyrshin"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The opioid overdose epidemic remains a critical public health crisis,\nparticularly in the United States, leading to significant mortality and\nsocietal costs. Social media platforms like Reddit provide vast amounts of\nunstructured data that offer insights into public perceptions, discussions, and\nexperiences related to opioid use. This study leverages Natural Language\nProcessing (NLP), specifically Opioid Named Entity Recognition (ONER-2025), to\nextract actionable information from these platforms. Our research makes four\nkey contributions. First, we created a unique, manually annotated dataset\nsourced from Reddit, where users share self-reported experiences of opioid use\nvia different administration routes. This dataset contains 331,285 tokens and\nincludes eight major opioid entity categories. Second, we detail our annotation\nprocess and guidelines while discussing the challenges of labeling the\nONER-2025 dataset. Third, we analyze key linguistic challenges, including\nslang, ambiguity, fragmented sentences, and emotionally charged language, in\nopioid discussions. Fourth, we propose a real-time monitoring system to process\nstreaming data from social media, healthcare records, and emergency services to\nidentify overdose events. Using 5-fold cross-validation in 11 experiments, our\nsystem integrates machine learning, deep learning, and transformer-based\nlanguage models with advanced contextual embeddings to enhance understanding.\nOur transformer-based models (bert-base-NER and roberta-base) achieved 97%\naccuracy and F1-score, outperforming baselines by 10.23% (RF=0.88)."}
{"id": "2504.14194", "pdf": "https://arxiv.org/pdf/2504.14194.pdf", "abs": "https://arxiv.org/abs/2504.14194", "title": "Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models", "authors": ["Xinlin Zhuang", "Jiahui Peng", "Ren Ma", "Yinfan Wang", "Tianyi Bai", "Xingjian Wei", "Jiantao Qiu", "Chi Zhang", "Ying Qian", "Conghui He"], "categories": ["cs.CL"], "comment": "Under review", "summary": "The composition of pre-training datasets for large language models (LLMs)\nremains largely undisclosed, hindering transparency and efforts to optimize\ndata quality, a critical driver of model performance. Current data selection\nmethods, such as natural language quality assessments, diversity-based filters,\nand classifier-based approaches, are limited by single-dimensional evaluation\nor redundancy-focused strategies. To address these gaps, we propose PRRC to\nevaluate data quality across Professionalism, Readability, Reasoning, and\nCleanliness. We further introduce Meta-rater, a multi-dimensional data\nselection method that integrates these dimensions with existing quality metrics\nthrough learned optimal weightings. Meta-rater employs proxy models to train a\nregression model that predicts validation loss, enabling the identification of\noptimal combinations of quality scores. Experiments demonstrate that Meta-rater\ndoubles convergence speed for 1.3B parameter models and improves downstream\ntask performance by 3.23, with scalable benefits observed in 3.3B models\ntrained on 100B tokens. Additionally, we release the annotated SlimPajama-627B\ndataset, labeled across 25 quality metrics (including PRRC), to advance\nresearch in data-centric LLM development. Our work establishes that holistic,\nmulti-dimensional quality integration significantly outperforms conventional\nsingle-dimension approaches, offering a scalable paradigm for enhancing\npre-training efficiency and model capability."}
{"id": "2504.16060", "pdf": "https://arxiv.org/pdf/2504.16060.pdf", "abs": "https://arxiv.org/abs/2504.16060", "title": "Vision-Language Models Are Not Pragmatically Competent in Referring Expression Generation", "authors": ["Ziqiao Ma", "Jing Ding", "Xuejun Zhang", "Dezhi Luo", "Jiahe Ding", "Sihan Xu", "Yuchen Huang", "Run Peng", "Joyce Chai"], "categories": ["cs.CL"], "comment": "Homepage: https://vlm-reg.github.io/", "summary": "Referring Expression Generation (REG) is a core task for evaluating the\npragmatic competence of vision-language systems, requiring not only accurate\nsemantic grounding but also adherence to principles of cooperative\ncommunication (Grice, 1975). However, current evaluations of vision-language\nmodels (VLMs) often overlook the pragmatic dimension, reducing REG to a\nregion-based captioning task and neglecting Gricean maxims. In this work, we\nrevisit REG from a pragmatic perspective, introducing a new dataset (RefOI) of\n1.5k images annotated with both written and spoken referring expressions.\nThrough a systematic evaluation of state-of-the-art VLMs, we identify three key\nfailures of pragmatic competence: (1) failure to uniquely identify the\nreferent, (2) inclusion of excessive or irrelevant information, and (3)\nmisalignment with human pragmatic preference, such as the underuse of minimal\nspatial cues. We also show that standard automatic evaluations fail to capture\nthese pragmatic violations, reinforcing superficial cues rather than genuine\nreferential success. Our findings call for a renewed focus on pragmatically\ninformed models and evaluation frameworks that align with real human\ncommunication."}
{"id": "2504.19314", "pdf": "https://arxiv.org/pdf/2504.19314.pdf", "abs": "https://arxiv.org/abs/2504.19314", "title": "BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language Models in Chinese", "authors": ["Peilin Zhou", "Bruce Leon", "Xiang Ying", "Can Zhang", "Yifan Shao", "Qichen Ye", "Dading Chong", "Zhiling Jin", "Chenxuan Xie", "Meng Cao", "Yuxin Gu", "Sixin Hong", "Jing Ren", "Jian Chen", "Chao Liu", "Yining Hua"], "categories": ["cs.CL"], "comment": "Under Review", "summary": "As large language models (LLMs) evolve into tool-using agents, the ability to\nbrowse the web in real-time has become a critical yardstick for measuring their\nreasoning and retrieval competence. Existing benchmarks such as BrowseComp\nconcentrate on English and overlook the linguistic, infrastructural, and\ncensorship-related complexities of other major information ecosystems -- most\nnotably Chinese. To address this gap, we introduce BrowseComp-ZH, a\nhigh-difficulty benchmark purpose-built to comprehensively evaluate LLM agents\non the Chinese web. BrowseComp-ZH consists of 289 multi-hop questions spanning\n11 diverse domains. Each question is reverse-engineered from a short,\nobjective, and easily verifiable answer (e.g., a date, number, or proper noun).\nA two-stage quality control protocol is applied to strive for high question\ndifficulty and answer uniqueness. We benchmark over 20 state-of-the-art\nlanguage models and agentic search systems on our proposed BrowseComp-ZH.\nDespite their strong conversational and retrieval capabilities, most models\nstruggle severely: a large number achieve accuracy rates below 10%, and only a\nhandful exceed 20%. Even the best-performing system, OpenAI's DeepResearch,\nreaches just 42.9%. These results demonstrate the considerable difficulty of\nBrowseComp-ZH, where success demands not only effective retrieval strategies,\nbut also sophisticated reasoning and information reconciliation -- capabilities\nthat current models still struggle to master. Our dataset, construction\nguidelines, and benchmark results have been publicly released at\nhttps://github.com/PALIN2018/BrowseComp-ZH."}
{"id": "2504.19467", "pdf": "https://arxiv.org/pdf/2504.19467.pdf", "abs": "https://arxiv.org/abs/2504.19467", "title": "BRIDGE: Benchmarking Large Language Models for Understanding Real-world Clinical Practice Text", "authors": ["Jiageng Wu", "Bowen Gu", "Ren Zhou", "Kevin Xie", "Doug Snyder", "Yixing Jiang", "Valentina Carducci", "Richard Wyss", "Rishi J Desai", "Emily Alsentzer", "Leo Anthony Celi", "Adam Rodman", "Sebastian Schneeweiss", "Jonathan H. Chen", "Santiago Romero-Brufau", "Kueiyu Joshua Lin", "Jie Yang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) hold great promise for medical applications and\nare evolving rapidly, with new models being released at an accelerated pace.\nHowever, current evaluations of LLMs in clinical contexts remain limited. Most\nexisting benchmarks rely on medical exam-style questions or PubMed-derived\ntext, failing to capture the complexity of real-world electronic health record\n(EHR) data. Others focus narrowly on specific application scenarios, limiting\ntheir generalizability across broader clinical use. To address this gap, we\npresent BRIDGE, a comprehensive multilingual benchmark comprising 87 tasks\nsourced from real-world clinical data sources across nine languages. We\nsystematically evaluated 52 state-of-the-art LLMs (including DeepSeek-R1,\nGPT-4o, Gemini, and Llama 4) under various inference strategies. With a total\nof 13,572 experiments, our results reveal substantial performance variation\nacross model sizes, languages, natural language processing tasks, and clinical\nspecialties. Notably, we demonstrate that open-source LLMs can achieve\nperformance comparable to proprietary models, while medically fine-tuned LLMs\nbased on older architectures often underperform versus updated general-purpose\nmodels. The BRIDGE and its corresponding leaderboard serve as a foundational\nresource and a unique reference for the development and evaluation of new LLMs\nin real-world clinical text understanding.\n  The BRIDGE leaderboard:\nhttps://huggingface.co/spaces/YLab-Open/BRIDGE-Medical-Leaderboard"}
{"id": "2504.20946", "pdf": "https://arxiv.org/pdf/2504.20946.pdf", "abs": "https://arxiv.org/abs/2504.20946", "title": "Trace-of-Thought Prompting: Investigating Prompt-Based Knowledge Distillation Through Question Decomposition", "authors": ["Tyler McDonald", "Ali Emami"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Knowledge distillation allows smaller neural networks to emulate the\nperformance of larger, teacher models with reduced computational demands.\nTraditional methods for Large Language Models (LLMs) often necessitate\nextensive fine-tuning, which limits their accessibility. To address this, we\nintroduce Trace-of-Thought Prompting, a novel framework designed to distill\ncritical reasoning capabilities from high-resource teacher models (over 8\nbillion parameters) to low-resource student models (up to 8 billion\nparameters). This approach leverages problem decomposition to enhance\ninterpretability and facilitate human-in-the-loop interventions. Empirical\nevaluations on the GSM8K and MATH datasets show that student models achieve\naccuracy gains of up to 113% on GSM8K and 21% on MATH, with significant\nimprovements particularly notable in smaller models like Llama 2 and Zephyr.\nOur results suggest a promising pathway for open-source, low-resource models to\neventually serve both as both students and teachers, potentially reducing our\nreliance on high-resource, proprietary models."}
{"id": "2504.21012", "pdf": "https://arxiv.org/pdf/2504.21012.pdf", "abs": "https://arxiv.org/abs/2504.21012", "title": "Waking Up an AI: A Quantitative Framework for Prompt-Induced Phase Transition in Large Language Models", "authors": ["Makoto Sato"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "What underlies intuitive human thinking? One approach to this question is to\ncompare the cognitive dynamics of humans and large language models (LLMs).\nHowever, such a comparison requires a method to quantitatively analyze AI\ncognitive behavior under controlled conditions. While anecdotal observations\nsuggest that certain prompts can dramatically change LLM behavior, these\nobservations have remained largely qualitative. Here, we propose a two-part\nframework to investigate this phenomenon: a Transition-Inducing Prompt (TIP)\nthat triggers a rapid shift in LLM responsiveness, and a Transition Quantifying\nPrompt (TQP) that evaluates this change using a separate LLM. Through\ncontrolled experiments, we examined how LLMs react to prompts embedding two\nsemantically distant concepts (e.g., mathematical aperiodicity and traditional\ncrafts)-either fused together or presented separately-by changing their\nlinguistic quality and affective tone. Whereas humans tend to experience\nheightened engagement when such concepts are meaningfully blended producing a\nnovel concept-a form of conceptual fusion-current LLMs showed no significant\ndifference in responsiveness between semantically fused and non-fused prompts.\nThis suggests that LLMs may not yet replicate the conceptual integration\nprocesses seen in human intuition. Our method enables fine-grained,\nreproducible measurement of cognitive responsiveness, and may help illuminate\nkey differences in how intuition and conceptual leaps emerge in artificial\nversus human minds."}
{"id": "2504.21800", "pdf": "https://arxiv.org/pdf/2504.21800.pdf", "abs": "https://arxiv.org/abs/2504.21800", "title": "How Real Are Synthetic Therapy Conversations? Evaluating Fidelity in Prolonged Exposure Dialogues", "authors": ["Suhas BN", "Dominik Mattioli", "Saeed Abdullah", "Rosa I. Arriaga", "Chris W. Wiese", "Andrew M. Sherrill"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "68T50", "I.2.7; H.3.1"], "comment": "11 pages, 5 tables", "summary": "The growing adoption of synthetic data in healthcare is driven by privacy\nconcerns, limited access to real-world data, and the high cost of annotation.\nThis work explores the use of synthetic Prolonged Exposure (PE) therapeutic\nconversations for Post-Traumatic Stress Disorder (PTSD) as a scalable\nalternative for training and evaluating clinical models. We systematically\ncompare real and synthetic dialogues using linguistic, structural, and\nprotocol-specific metrics, including turn-taking patterns and treatment\nfidelity. We also introduce and evaluate PE-specific metrics derived from\nlinguistic analysis and semantic modeling, offering a novel framework for\nassessing clinical fidelity beyond surface fluency. Our findings show that\nalthough synthetic data holds promise for mitigating data scarcity and\nprotecting patient privacy, it can struggle to capture the subtle dynamics of\ntherapeutic interactions. Synthetic therapy dialogues closely match structural\nfeatures of real-world conversations (e.g., speaker switch ratio: 0.98 vs.\n0.99); however, they may not adequately reflect key fidelity markers (e.g.,\ndistress monitoring). We highlight gaps in existing evaluation frameworks and\nadvocate for fidelity-aware metrics that go beyond surface fluency to uncover\nclinically significant failures. Our findings clarify where synthetic data can\neffectively complement real-world datasets -- and where critical limitations\nremain."}
{"id": "2403.00108", "pdf": "https://arxiv.org/pdf/2403.00108.pdf", "abs": "https://arxiv.org/abs/2403.00108", "title": "LoRATK: LoRA Once, Backdoor Everywhere in the Share-and-Play Ecosystem", "authors": ["Hongyi Liu", "Shaochen Zhong", "Xintong Sun", "Minghao Tian", "Mohsen Hariri", "Zirui Liu", "Ruixiang Tang", "Zhimeng Jiang", "Jiayi Yuan", "Yu-Neng Chuang", "Li Li", "Soo-Hyun Choi", "Rui Chen", "Vipin Chaudhary", "Xia Hu"], "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "Finetuning LLMs with LoRA has gained significant popularity due to its\nsimplicity and effectiveness. Often, users may even find pluggable,\ncommunity-shared LoRAs to enhance their base models for a specific downstream\ntask of interest; enjoying a powerful, efficient, yet customized LLM experience\nwith negligible investment. However, this convenient share-and-play ecosystem\nalso introduces a new attack surface, where attackers can distribute malicious\nLoRAs to a community eager to try out shared assets. Despite the high-risk\npotential, no prior art has comprehensively explored LoRA's attack surface\nunder the downstream-enhancing share-and-play context. In this paper, we\ninvestigate how backdoors can be injected into task-enhancing LoRAs and examine\nthe mechanisms of such infections. We find that with a simple, efficient, yet\nspecific recipe, a backdoor LoRA can be trained once and then seamlessly merged\n(in a training-free fashion) with multiple task-enhancing LoRAs, retaining both\nits malicious backdoor and benign downstream capabilities. This allows\nattackers to scale the distribution of compromised LoRAs with minimal effort by\nleveraging the rich pool of existing shared LoRA assets. We note that such\nmerged LoRAs are particularly infectious -- because their malicious intent is\ncleverly concealed behind improved downstream capabilities, creating a strong\nincentive for voluntary download -- and dangerous -- because under local\ndeployment, no safety measures exist to intervene when things go wrong. Our\nwork is among the first to study this new threat model of training-free\ndistribution of downstream-capable-yet-backdoor-injected LoRAs, highlighting\nthe urgent need for heightened security awareness in the LoRA ecosystem.\nWarning: This paper contains offensive content and involves a real-life\ntragedy."}
{"id": "2404.17525", "pdf": "https://arxiv.org/pdf/2404.17525.pdf", "abs": "https://arxiv.org/abs/2404.17525", "title": "Large Language Model Agent as a Mechanical Designer", "authors": ["Yayati Jadhav", "Amir Barati Farimani"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Conventional mechanical design follows an iterative process in which initial\nconcepts are refined through cycles of expert assessment and resource-intensive\nFinite Element Method (FEM) analysis to meet performance goals. While machine\nlearning models have been developed to assist in parts of this process, they\ntypically require large datasets, extensive training, and are often tailored to\nspecific tasks, limiting their generalizability. To address these limitations,\nwe propose a framework that leverages a pretrained Large Language Model (LLM)\nin conjunction with an FEM module to autonomously generate, evaluate, and\nrefine structural designs based on performance specifications and numerical\nfeedback. The LLM operates without domain-specific fine-tuning, using general\nreasoning to propose design candidates, interpret FEM-derived performance\nmetrics, and apply structurally sound modifications. Using 2D truss structures\nas a testbed, we show that the LLM can effectively navigate highly discrete and\nmulti-faceted design spaces, balance competing objectives, and identify\nconvergence when further optimization yields diminishing returns. Compared to\nNon-dominated Sorting Genetic Algorithm II (NSGA-II), our method achieves\nfaster convergence and fewer FEM evaluations. Experiments with varying\ntemperature settings (0.5, 1.0, 1.2) and model sizes (GPT-4.1 and GPT-4.1-mini)\nindicate that smaller models yield higher constraint satisfaction with fewer\nsteps, while lower temperatures enhance design consistency. These results\nestablish LLMs as a promising new class of reasoning-based, natural\nlanguage-driven optimizers for autonomous design and iterative structural\nrefinement."}
{"id": "2405.04620", "pdf": "https://arxiv.org/pdf/2405.04620.pdf", "abs": "https://arxiv.org/abs/2405.04620", "title": "Folded Context Condensation in Path Integral Formalism for Infinite Context Transformers", "authors": ["Won-Gi Paeng", "Daesuk Kwon", "Kyungwon Jeong", "Honggyo Suh"], "categories": ["hep-ph", "cs.AI", "cs.CL", "cs.LG", "cs.NE"], "comment": "11 pages, 12 figures", "summary": "In this work, we present a generalized formulation of the Transformer\nalgorithm by reinterpreting its core mechanisms within the framework of Path\nIntegral formalism. In this perspective, the attention mechanism is recast as a\nprocess that integrates all possible transition paths leading to future token\nstates, with temporal evolution governed by the Feed-Forward Network. By\nsystematically mapping each component of the Transformer to its counterpart in\nthe Path Integral formulation, we obtain a more compact and efficient\nrepresentation, in which the contextual information of a sequence is condensed\ninto memory-like segments. These segments are recurrently processed across\nTransformer layers, enabling more effective long-term information retention. We\nvalidate the effectiveness of this approach through the Passkey retrieval task\nand a summarization task, demonstrating that the proposed method preserves\nhistorical information while exhibiting memory usage that scales linearly with\nsequence length. This contrasts with the non-linear memory growth typically\nobserved in standard attention mechanisms. We expect that this quantum-inspired\ngeneralization of the Transformer architecture will open new avenues for\nenhancing both the efficiency and expressiveness of future Transformer models."}
{"id": "2410.05573", "pdf": "https://arxiv.org/pdf/2410.05573.pdf", "abs": "https://arxiv.org/abs/2410.05573", "title": "TaeBench: Improving Quality of Toxic Adversarial Examples", "authors": ["Xuan Zhu", "Dmitriy Bespalov", "Liwen You", "Ninad Kulkarni", "Yanjun Qi"], "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted for publication in NAACL 2025. The official version will be\n  available in the ACL Anthology", "summary": "Toxicity text detectors can be vulnerable to adversarial examples - small\nperturbations to input text that fool the systems into wrong detection.\nExisting attack algorithms are time-consuming and often produce invalid or\nambiguous adversarial examples, making them less useful for evaluating or\nimproving real-world toxicity content moderators. This paper proposes an\nannotation pipeline for quality control of generated toxic adversarial examples\n(TAE). We design model-based automated annotation and human-based quality\nverification to assess the quality requirements of TAE. Successful TAE should\nfool a target toxicity model into making benign predictions, be grammatically\nreasonable, appear natural like human-generated text, and exhibit semantic\ntoxicity. When applying these requirements to more than 20 state-of-the-art\n(SOTA) TAE attack recipes, we find many invalid samples from a total of 940k\nraw TAE attack generations. We then utilize the proposed pipeline to filter and\ncurate a high-quality TAE dataset we call TaeBench (of size 264k). Empirically,\nwe demonstrate that TaeBench can effectively transfer-attack SOTA toxicity\ncontent moderation models and services. Our experiments also show that TaeBench\nwith adversarial training achieve significant improvements of the robustness of\ntwo toxicity detectors."}
{"id": "2411.02790", "pdf": "https://arxiv.org/pdf/2411.02790.pdf", "abs": "https://arxiv.org/abs/2411.02790", "title": "Bridging Personalization and Control in Scientific Personalized Search", "authors": ["Sheshera Mysore", "Garima Dhanania", "Kishor Patil", "Surya Kallumadi", "Andrew McCallum", "Hamed Zamani"], "categories": ["cs.IR", "cs.CL"], "comment": "SIGIR 2025 paper", "summary": "Personalized search is a problem where models benefit from learning user\npreferences from per-user historical interaction data. The inferred preferences\nenable personalized ranking models to improve the relevance of documents for\nusers. However, personalization is also seen as opaque in its use of historical\ninteractions and is not amenable to users' control. Further, personalization\nlimits the diversity of information users are exposed to. While search results\nmay be automatically diversified this does little to address the lack of\ncontrol over personalization. In response, we introduce a model for\npersonalized search that enables users to control personalized rankings\nproactively. Our model, CtrlCE, is a novel cross-encoder model augmented with\nan editable memory built from users' historical interactions. The editable\nmemory allows cross-encoders to be personalized efficiently and enables users\nto control personalized ranking. Next, because all queries do not require\npersonalization, we introduce a calibrated mixing model which determines when\npersonalization is necessary. This enables users to control personalization via\ntheir editable memory only when necessary. To thoroughly evaluate CtrlCE, we\ndemonstrate its empirical performance in four domains of science, its ability\nto selectively request user control in a calibration evaluation of the mixing\nmodel, and the control provided by its editable memory in a user study."}
{"id": "2411.16508", "pdf": "https://arxiv.org/pdf/2411.16508.pdf", "abs": "https://arxiv.org/abs/2411.16508", "title": "All Languages Matter: Evaluating LMMs on Culturally Diverse 100 Languages", "authors": ["Ashmal Vayani", "Dinura Dissanayake", "Hasindri Watawana", "Noor Ahsan", "Nevasini Sasikumar", "Omkar Thawakar", "Henok Biadglign Ademtew", "Yahya Hmaiti", "Amandeep Kumar", "Kartik Kuckreja", "Mykola Maslych", "Wafa Al Ghallabi", "Mihail Mihaylov", "Chao Qin", "Abdelrahman M Shaker", "Mike Zhang", "Mahardika Krisna Ihsani", "Amiel Esplana", "Monil Gokani", "Shachar Mirkin", "Harsh Singh", "Ashay Srivastava", "Endre Hamerlik", "Fathinah Asma Izzati", "Fadillah Adamsyah Maani", "Sebastian Cavada", "Jenny Chim", "Rohit Gupta", "Sanjay Manjunath", "Kamila Zhumakhanova", "Feno Heriniaina Rabevohitra", "Azril Amirudin", "Muhammad Ridzuan", "Daniya Kareem", "Ketan More", "Kunyang Li", "Pramesh Shakya", "Muhammad Saad", "Amirpouya Ghasemaghaei", "Amirbek Djanibekov", "Dilshod Azizov", "Branislava Jankovic", "Naman Bhatia", "Alvaro Cabrera", "Johan Obando-Ceron", "Olympiah Otieno", "Fabian Farestam", "Muztoba Rabbani", "Sanoojan Baliah", "Santosh Sanjeev", "Abduragim Shtanchaev", "Maheen Fatima", "Thao Nguyen", "Amrin Kareem", "Toluwani Aremu", "Nathan Xavier", "Amit Bhatkal", "Hawau Toyin", "Aman Chadha", "Hisham Cholakkal", "Rao Muhammad Anwer", "Michael Felsberg", "Jorma Laaksonen", "Thamar Solorio", "Monojit Choudhury", "Ivan Laptev", "Mubarak Shah", "Salman Khan", "Fahad Khan"], "categories": ["cs.CV", "cs.CL"], "comment": "A Multilingual Multimodal cultural benchmark for 100 languages", "summary": "Existing Large Multimodal Models (LMMs) generally focus on only a few regions\nand languages. As LMMs continue to improve, it is increasingly important to\nensure they understand cultural contexts, respect local sensitivities, and\nsupport low-resource languages, all while effectively integrating corresponding\nvisual cues. In pursuit of culturally diverse global multimodal models, our\nproposed All Languages Matter Benchmark (ALM-bench) represents the largest and\nmost comprehensive effort to date for evaluating LMMs across 100 languages.\nALM-bench challenges existing models by testing their ability to understand and\nreason about culturally diverse images paired with text in various languages,\nincluding many low-resource languages traditionally underrepresented in LMM\nresearch. The benchmark offers a robust and nuanced evaluation framework\nfeaturing various question formats, including true/false, multiple choice, and\nopen-ended questions, which are further divided into short and long-answer\ncategories. ALM-bench design ensures a comprehensive assessment of a model's\nability to handle varied levels of difficulty in visual and linguistic\nreasoning. To capture the rich tapestry of global cultures, ALM-bench carefully\ncurates content from 13 distinct cultural aspects, ranging from traditions and\nrituals to famous personalities and celebrations. Through this, ALM-bench not\nonly provides a rigorous testing ground for state-of-the-art open and\nclosed-source LMMs but also highlights the importance of cultural and\nlinguistic inclusivity, encouraging the development of models that can serve\ndiverse global populations effectively. Our benchmark is publicly available."}
{"id": "2412.18086", "pdf": "https://arxiv.org/pdf/2412.18086.pdf", "abs": "https://arxiv.org/abs/2412.18086", "title": "Generating Traffic Scenarios via In-Context Learning to Learn Better Motion Planner", "authors": ["Aizierjiang Aiersilan"], "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.GR", "cs.LG"], "comment": "We are excited to announce that this paper has been accepted for oral\n  presentation at the AAAI 2025 Main Conference. We are grateful for the\n  insightful feedback from the reviewers and look forward to contributing to\n  the discussions at AAAI", "summary": "Motion planning is a crucial component in autonomous driving.\nState-of-the-art motion planners are trained on meticulously curated datasets,\nwhich are not only expensive to annotate but also insufficient in capturing\nrarely seen critical scenarios. Failing to account for such scenarios poses a\nsignificant risk to motion planners and may lead to incidents during testing.\nAn intuitive solution is to manually compose such scenarios by programming and\nexecuting a simulator (e.g., CARLA). However, this approach incurs substantial\nhuman costs. Motivated by this, we propose an inexpensive method for generating\ndiverse critical traffic scenarios to train more robust motion planners. First,\nwe represent traffic scenarios as scripts, which are then used by the simulator\nto generate traffic scenarios. Next, we develop a method that accepts\nuser-specified text descriptions, which a Large Language Model translates into\nscripts using in-context learning. The output scripts are sent to the simulator\nthat produces the corresponding traffic scenarios. As our method can generate\nabundant safety-critical traffic scenarios, we use them as synthetic training\ndata for motion planners. To demonstrate the value of generated scenarios, we\ntrain existing motion planners on our synthetic data, real-world datasets, and\na combination of both. Our experiments show that motion planners trained with\nour data significantly outperform those trained solely on real-world data,\nshowing the usefulness of our synthetic data and the effectiveness of our data\ngeneration method. Our source code is available at\nhttps://ezharjan.github.io/AutoSceneGen."}
{"id": "2501.18265", "pdf": "https://arxiv.org/pdf/2501.18265.pdf", "abs": "https://arxiv.org/abs/2501.18265", "title": "Efficiency and Effectiveness of LLM-Based Summarization of Evidence in Crowdsourced Fact-Checking", "authors": ["Kevin Roitero", "Dustin Wright", "Michael Soprano", "Isabelle Augenstein", "Stefano Mizzaro"], "categories": ["cs.IR", "cs.CL", "cs.HC"], "comment": "19 pages; 7 figures; 5 tables", "summary": "Evaluating the truthfulness of online content is critical for combating\nmisinformation. This study examines the efficiency and effectiveness of\ncrowdsourced truthfulness assessments through a comparative analysis of two\napproaches: one involving full-length webpages as evidence for each claim, and\nanother using summaries for each evidence document generated with a large\nlanguage model. Using an A/B testing setting, we engage a diverse pool of\nparticipants tasked with evaluating the truthfulness of statements under these\nconditions. Our analysis explores both the quality of assessments and the\nbehavioral patterns of participants. The results reveal that relying on\nsummarized evidence offers comparable accuracy and error metrics to the\nStandard modality while significantly improving efficiency. Workers in the\nSummary setting complete a significantly higher number of assessments, reducing\ntask duration and costs. Additionally, the Summary modality maximizes internal\nagreement and maintains consistent reliance on and perceived usefulness of\nevidence, demonstrating its potential to streamline large-scale truthfulness\nevaluations."}
{"id": "2504.05520", "pdf": "https://arxiv.org/pdf/2504.05520.pdf", "abs": "https://arxiv.org/abs/2504.05520", "title": "Efficient Reinforcement Finetuning via Adaptive Curriculum Learning", "authors": ["Taiwei Shi", "Yiyang Wu", "Linxin Song", "Tianyi Zhou", "Jieyu Zhao"], "categories": ["cs.LG", "cs.CL"], "comment": "25 pages, 7 figures, 6 tables", "summary": "Reinforcement finetuning (RFT) has shown great potential for enhancing the\nmathematical reasoning capabilities of large language models (LLMs), but it is\noften sample- and compute-inefficient, requiring extensive training. In this\nwork, we introduce AdaRFT (Adaptive Curriculum Reinforcement Finetuning), a\nmethod that significantly improves both the efficiency and final accuracy of\nRFT through adaptive curriculum learning. AdaRFT dynamically adjusts the\ndifficulty of training problems based on the model's recent reward signals,\nensuring that the model consistently trains on tasks that are challenging but\nsolvable. This adaptive sampling strategy accelerates learning by maintaining\nan optimal difficulty range, avoiding wasted computation on problems that are\ntoo easy or too hard. AdaRFT requires only a lightweight extension to standard\nRFT algorithms like Proximal Policy Optimization (PPO), without modifying the\nreward function or model architecture. Experiments on competition-level math\ndatasets-including AMC, AIME, and IMO-style problems-demonstrate that AdaRFT\nsignificantly improves both training efficiency and reasoning performance. We\nevaluate AdaRFT across multiple data distributions and model sizes, showing\nthat it reduces training time by up to 2x and improves accuracy by a\nconsiderable margin, offering a more scalable and effective RFT framework."}
