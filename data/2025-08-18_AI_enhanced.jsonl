{"id": "2508.10903", "pdf": "https://arxiv.org/pdf/2508.10903.pdf", "abs": "https://arxiv.org/abs/2508.10903", "title": "How do Data Journalists Design Maps to Tell Stories?", "authors": ["Arlindo Gomes", "Emilly Brito", "Luis Morais", "Nivan Ferreira"], "categories": ["cs.HC", "cs.CY"], "comment": "IEEE VIS 2025", "summary": "Maps are essential to news media as they provide a familiar way to convey\nspatial context and present engaging narratives. However, the design of\njournalistic maps may be challenging, as editorial teams need to balance\nmultiple aspects, such as aesthetics, the audience's expected data literacy,\ntight publication deadlines, and the team's technical skills. Data journalists\noften come from multiple areas and lack a cartography, data visualization, and\ndata science background, limiting their competence in creating maps. While\nprevious studies have examined spatial visualizations in data stories, this\nresearch seeks to gain a deeper understanding of the map design process\nemployed by news outlets. To achieve this, we strive to answer two specific\nresearch questions: what is the design space of journalistic maps? and how do\neditorial teams produce journalistic map articles? To answer the first one, we\ncollected and analyzed a large corpus of 462 journalistic maps used in news\narticles from five major news outlets published over three months. As a result,\nwe created a design space comprised of eight dimensions that involved both\nproperties describing the articles' aspects and the visual/interactive features\nof maps. We approach the second research question via semi-structured\ninterviews with four data journalists who create data-driven articles daily.\nThrough these interviews, we identified the most common design rationales made\nby editorial teams and potential gaps in current practices. We also collected\nthe practitioners' feedback on our design space to externally validate it. With\nthese results, we aim to provide researchers and journalists with empirical\ndata to design and study journalistic maps.", "AI": {"tldr": "This research examines the map design process in journalism, analyzing a corpus of 462 maps and conducting interviews with data journalists to identify common practices and rationales.", "motivation": "To understand the challenges and processes involved in creating journalistic maps, particularly given the diverse backgrounds of data journalists.", "method": "Analyzed 462 journalistic maps from five major news outlets and conducted semi-structured interviews with four data journalists.", "result": "Developed a design space with eight dimensions detailing article aspects and map features, and identified common design rationales among journalists.", "conclusion": "The findings provide empirical data to aid researchers and journalists in enhancing the design and study of journalistic maps.", "key_contributions": ["Creation of a design space for journalistic maps with eight dimensions", "Identification of common design rationales in map production", "Empirical validation of the design space through practitioner feedback."], "limitations": "", "keywords": ["journalistic maps", "data journalism", "spatial visualizations", "map design", "data-driven articles"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2508.10907", "pdf": "https://arxiv.org/pdf/2508.10907.pdf", "abs": "https://arxiv.org/abs/2508.10907", "title": "Designing for Engaging Communication Between Parents and Young Adult Children Through Shared Music Experiences", "authors": ["Euihyeok Lee", "Souneil Park", "Jin Yu", "Seungchul Lee", "Seungwoo Kang"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "This paper aims to foster social interaction between parents and young adult\nchildren living apart via music. Our approach transforms their music-listening\nmoment into an opportunity to listen to the other's favorite songs and enrich\ninteraction in their daily lives. To this end, we explore the current practice\nand needs of parent-child communication and the experience and perception of\nmusic-mediated interaction. Based on the findings, we developed DJ-Fam, a\nmobile application that enables parents and children to listen to their\nfavorite songs and use them as conversation starters to foster parent-child\ninteraction. From our deployment study with seven families over four weeks in\nSouth Korea, we show the potential of DJ-Fam to influence parent-child\ninteraction and their mutual understanding and relationship positively.\nSpecifically, DJ-Fam considerably increases the frequency of communication and\ndiversifies the communication channels and topics, all of which are\nsatisfactory to the participants.", "AI": {"tldr": "The paper presents DJ-Fam, a mobile app designed to enhance communication between parents and young adult children through shared music experiences.", "motivation": "To improve social interaction and communication between parents and their adult children living apart using music as a medium.", "method": "The study involved a deployment of the DJ-Fam app with seven families over four weeks, analyzing their communication patterns and interactions.", "result": "The findings indicate that DJ-Fam positively influenced parent-child interactions, leading to increased communication frequency and a diverse range of topics for discussion.", "conclusion": "DJ-Fam can effectively enrich parent-child relationships by creating opportunities for interaction through shared music experiences.", "key_contributions": ["Developed DJ-Fam app to foster communication between parents and children through music.", "Conducted a deployment study demonstrating usage and impact on interactions.", "Identified the role of music in enhancing daily communication and mutual understanding."], "limitations": "The study involved a small sample size and was conducted in a specific cultural context (South Korea), which may limit generalizability.", "keywords": ["Music Interaction", "Parent-Child Communication", "Mobile Application", "Human-Computer Interaction", "Social Interaction"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2508.10911", "pdf": "https://arxiv.org/pdf/2508.10911.pdf", "abs": "https://arxiv.org/abs/2508.10911", "title": "Uncovering Latent Connections in Indigenous Heritage: Semantic Pipelines for Cultural Preservation in Brazil", "authors": ["Luis Vitor Zerkowski", "Nina S. T. Hirata"], "categories": ["cs.HC", "cs.CY", "cs.LG", "I.2.m"], "comment": "8 tables, 7 figures, submitted to AAAI2026", "summary": "Indigenous communities face ongoing challenges in preserving their cultural\nheritage, particularly in the face of systemic marginalization and urban\ndevelopment. In Brazil, the Museu Nacional dos Povos Indigenas through the\nTainacan platform hosts the country's largest online collection of Indigenous\nobjects and iconographies, providing a critical resource for cultural\nengagement. Using publicly available data from this repository, we present a\ndata-driven initiative that applies artificial intelligence to enhance\naccessibility, interpretation, and exploration. We develop two semantic\npipelines: a visual pipeline that models image-based similarity and a textual\npipeline that captures semantic relationships from item descriptions. These\nembedding spaces are projected into two dimensions and integrated into an\ninteractive visualization tool we also developed. In addition to\nsimilarity-based navigation, users can explore the collection through temporal\nand geographic lenses, enabling both semantic and contextualized perspectives.\nThe system supports curatorial tasks, aids public engagement, and reveals\nlatent connections within the collection. This work demonstrates how AI can\nethically contribute to cultural preservation practices.", "AI": {"tldr": "The paper presents a data-driven initiative using AI to enhance accessibility and exploration of Indigenous cultural heritage through the Tainacan platform.", "motivation": "Indigenous communities are challenged in preserving their cultural heritage due to systemic marginalization and urban development.", "method": "Develops two semantic pipelines: a visual pipeline for image similarity and a textual pipeline for semantic relationships from descriptions, integrated into an interactive tool.", "result": "The system allows similarity-based navigation and exploration through temporal and geographic contexts, aiding curatorial tasks and public engagement.", "conclusion": "AI can ethically contribute to cultural preservation practices by revealing connections within Indigenous collections.", "key_contributions": ["Development of visual and textual semantic pipelines", "Creation of an interactive visualization tool", "Facilitation of cultural engagement and exploration of Indigenous heritage"], "limitations": "", "keywords": ["Indigenous communities", "Cultural heritage", "Artificial intelligence", "Semantic pipelines", "Interactive visualization"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.10914", "pdf": "https://arxiv.org/pdf/2508.10914.pdf", "abs": "https://arxiv.org/abs/2508.10914", "title": "Generation and Evaluation in the Human Invention Process through the Lens of Game Design", "authors": ["Katherine M. Collins", "Graham Todd", "Cedegao E. Zhang", "Adrian Weller", "Julian Togelius", "Junyi Chu", "Lionel Wong", "Thomas L. Griffiths", "Joshua B. Tenenbaum"], "categories": ["cs.HC"], "comment": "CogSci conference non-archival paper", "summary": "The human ability to learn rules and solve problems has been a central\nconcern of cognitive science research since the field's earliest days. But we\ndo not just follow rules and solve problems given to us by others: we modify\nthose rules, create new problems, and set new goals and tasks for ourselves and\nothers. Arguably, even more than rule following and problem solving, human\nintelligence is about creatively breaking and stretching the rules, changing\nthe game, and inventing new problems worth thinking about. Creating a good rule\nor a good problem depends not just on the ideas one can think up but on how one\nevaluates such proposals. Here, we study invention through the lens of game\ndesign. We focus particularly on the early stages of novice, \"everyday\" game\ncreation, where the stakes are low. We draw on a dataset of over 450 human\ncreated games, created by participants who saw an initial seed set of\ntwo-player grid-based strategy games. We consider two different cognitive\nmechanisms that may be at work during the early processes of intuitive game\ninvention: an associative proposal based on previous games one has seen and\ncompute-bounded model-based evaluation that an everyday game creator may use to\nrefine their initial draft proposals. In our preliminary work, we conduct a\nmodel-based analysis of how people invented new games based on prior experience\nand find that generated games are best described by a model which incorporates\nmodel-based estimates of game quality at a population level. Our work points to\nhow human invention is based not only on what people propose, but how they\nevaluate and offers a computational toolkit to scale empirical studies of\nmodel-based simulation in open-ended human innovation.", "AI": {"tldr": "This paper explores human invention in game design, focusing on how rules and problems are actively modified and created by individuals. It analyzes game creation by novice designers through a dataset of over 450 games, highlighting cognitive mechanisms involved in evaluating and refining game proposals.", "motivation": "Understanding human creativity in rule modification and problem creation, particularly in the context of game design.", "method": "Analyzed a dataset of over 450 games created by participants, focusing on cognitive mechanisms such as associative proposals and compute-bounded model-based evaluations during game invention.", "result": "Preliminary analysis suggests that the invention of new games aligns with a model that incorporates estimates of game quality, indicating that both proposal generation and evaluation significantly influence game design.", "conclusion": "The study emphasizes that human invention relies on both the creativity of proposals and the evaluation processes, providing insights into game design and a computational framework for studying human innovation.", "key_contributions": ["Examination of cognitive processes in game invention", "Empirical analysis using a large dataset of human-created games", "Introduction of a computational toolkit for empirical studies in human creativity."], "limitations": "The study is limited to novice game creators and may not generalize to expert game design.", "keywords": ["cognitive science", "game design", "human invention", "creativity", "model-based evaluation"], "importance_score": 3, "read_time_minutes": 15}}
{"id": "2508.10904", "pdf": "https://arxiv.org/pdf/2508.10904.pdf", "abs": "https://arxiv.org/abs/2508.10904", "title": "A2HCoder: An LLM-Driven Coding Agent for Hierarchical Algorithm-to-HDL Translation", "authors": ["Jie Lei", "Ruofan Jia", "J. Andrew Zhang", "Hao Zhang"], "categories": ["cs.CL", "cs.AR", "cs.PL"], "comment": "15 pages, 6 figures", "summary": "In wireless communication systems, stringent requirements such as ultra-low\nlatency and power consumption have significantly increased the demand for\nefficient algorithm-to-hardware deployment. However, a persistent and\nsubstantial gap remains between algorithm design and hardware implementation.\nBridging this gap traditionally requires extensive domain expertise and\ntime-consuming manual development, due to fundamental mismatches between\nhigh-level programming languages like MATLAB and hardware description languages\n(HDLs) such as Verilog-in terms of memory access patterns, data processing\nmanners, and datatype representations. To address this challenge, we propose\nA2HCoder: a Hierarchical Algorithm-to-HDL Coding Agent, powered by large\nlanguage models (LLMs), designed to enable agile and reliable\nalgorithm-to-hardware translation. A2HCoder introduces a hierarchical framework\nthat enhances both robustness and interpretability while suppressing common\nhallucination issues in LLM-generated code. In the horizontal dimension,\nA2HCoder decomposes complex algorithms into modular functional blocks,\nsimplifying code generation and improving consistency. In the vertical\ndimension, instead of relying on end-to-end generation, A2HCoder performs\nstep-by-step, fine-grained translation, leveraging external toolchains such as\nMATLAB and Vitis HLS for debugging and circuit-level synthesis. This structured\nprocess significantly mitigates hallucinations and ensures hardware-level\ncorrectness. We validate A2HCoder through a real-world deployment case in the\n5G wireless communication domain, demonstrating its practicality, reliability,\nand deployment efficiency.", "AI": {"tldr": "A2HCoder is a hierarchical framework utilizing large language models to facilitate efficient algorithm-to-hardware translation for wireless communication systems, addressing challenges in bridging the gap between algorithm design and HDL implementation.", "motivation": "The substantial gap between algorithm design and hardware implementation in wireless communication systems necessitates an efficient and agile method for translating algorithms to hardware effectively.", "method": "A2HCoder introduces a hierarchical framework that decomposes complex algorithms into modular functional blocks and performs step-by-step translation using external toolchains like MATLAB and Vitis HLS.", "result": "Validation in a real-world 5G wireless communication deployment demonstrates A2HCoder's practicality, reliability, and efficiency in hardware-level correctness and reduced hallucination issues.", "conclusion": "A2HCoder effectively bridges the gap between high-level algorithm design and hardware implementation, providing a robust and interpretable solution to challenges faced in the deployment of algorithms in hardware.", "key_contributions": ["Introduction of a hierarchical framework for algorithm-to-hardware translation", "Modular decomposition of algorithms into functional blocks", "Step-by-step translation enhancing robustness and reducing common hallucinations"], "limitations": "", "keywords": ["Algorithm-to-hardware", "Large language models", "Wireless communication", "Hierarchical framework", "5G"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2508.10916", "pdf": "https://arxiv.org/pdf/2508.10916.pdf", "abs": "https://arxiv.org/abs/2508.10916", "title": "Multimodal Quantitative Measures for Multiparty Behaviour Evaluation", "authors": ["Ojas Shirekar", "Wim Pouw", "Chenxu Hao", "Vrushank Phadnis", "Thabo Beeler", "Chirag Raman"], "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.MA"], "comment": null, "summary": "Digital humans are emerging as autonomous agents in multiparty interactions,\nyet existing evaluation metrics largely ignore contextual coordination\ndynamics. We introduce a unified, intervention-driven framework for objective\nassessment of multiparty social behaviour in skeletal motion data, spanning\nthree complementary dimensions: (1) synchrony via Cross-Recurrence\nQuantification Analysis, (2) temporal alignment via Multiscale Empirical Mode\nDecompositionbased Beat Consistency, and (3) structural similarity via Soft\nDynamic Time Warping. We validate metric sensitivity through three\ntheory-driven perturbations -- gesture kinematic dampening, uniform\nspeech-gesture delays, and prosodic pitch-variance reduction-applied to\n$\\approx 145$ 30-second thin slices of group interactions from the DnD dataset.\nMixed-effects analyses reveal predictable, joint-independent shifts: dampening\nincreases CRQA determinism and reduces beat consistency, delays weaken\ncross-participant coupling, and pitch flattening elevates F0 Soft-DTW costs. A\ncomplementary perception study ($N=27$) compares judgments of full-video and\nskeleton-only renderings to quantify representation effects. Our three measures\ndeliver orthogonal insights into spatial structure, timing alignment, and\nbehavioural variability. Thereby forming a robust toolkit for evaluating and\nrefining socially intelligent agents. Code available on\n\\href{https://github.com/tapri-lab/gig-interveners}{GitHub}.", "AI": {"tldr": "This paper presents a framework for evaluating multiparty social behavior in digital humans' skeletal motion data through three dimensions: synchrony, temporal alignment, and structural similarity.", "motivation": "To address the gap in existing metrics for evaluating multiparty interactions in digital humans, focusing on contextual coordination dynamics.", "method": "The authors introduce a unified, intervention-driven framework incorporating measures of synchrony (Cross-Recurrence Quantification Analysis), temporal alignment (Multiscale Empirical Mode Decomposition), and structural similarity (Soft Dynamic Time Warping) applied to interaction data.", "result": "Validation showed predictable shifts in social behavior metrics due to interventions like gesture dampening and speech-gesture delays, demonstrating the framework's sensitivity and effectiveness.", "conclusion": "The proposed metrics can effectively evaluate and refine socially intelligent agents by providing insights into aspects of social interaction.", "key_contributions": ["A unified framework for assessing multiparty social behavior in digital humans.", "Three distinct dimensions for evaluation: synchrony, temporal alignment, and structural similarity.", "Validation of metrics through controlled perturbations in movement and speech."], "limitations": "", "keywords": ["digital humans", "multiparty interactions", "social behavior evaluation", "skeletal motion data", "evaluation metrics"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2508.10906", "pdf": "https://arxiv.org/pdf/2508.10906.pdf", "abs": "https://arxiv.org/abs/2508.10906", "title": "PersonaTwin: A Multi-Tier Prompt Conditioning Framework for Generating and Evaluating Personalized Digital Twins", "authors": ["Sihan Chen", "John P. Lalor", "Yi Yang", "Ahmed Abbasi"], "categories": ["cs.CL"], "comment": "Presented at the Generation, Evaluation & Metrics (GEM) Workshop at\n  ACL 2025", "summary": "While large language models (LLMs) afford new possibilities for user modeling\nand approximation of human behaviors, they often fail to capture the\nmultidimensional nuances of individual users. In this work, we introduce\nPersonaTwin, a multi-tier prompt conditioning framework that builds adaptive\ndigital twins by integrating demographic, behavioral, and psychometric data.\nUsing a comprehensive data set in the healthcare context of more than 8,500\nindividuals, we systematically benchmark PersonaTwin against standard LLM\noutputs, and our rigorous evaluation unites state-of-the-art text similarity\nmetrics with dedicated demographic parity assessments, ensuring that generated\nresponses remain accurate and unbiased. Experimental results show that our\nframework produces simulation fidelity on par with oracle settings. Moreover,\ndownstream models trained on persona-twins approximate models trained on\nindividuals in terms of prediction and fairness metrics across both\nGPT-4o-based and Llama-based models. Together, these findings underscore the\npotential for LLM digital twin-based approaches in producing realistic and\nemotionally nuanced user simulations, offering a powerful tool for personalized\ndigital user modeling and behavior analysis.", "AI": {"tldr": "Introduction of PersonaTwin, a framework that uses demographic, behavioral, and psychometric data to create adaptive digital twins for improved user modeling.", "motivation": "Large language models (LLMs) often fail at capturing the multidimensional nuances of individual users, especially in healthcare settings.", "method": "Developed a multi-tier prompt conditioning framework called PersonaTwin, integrating diverse user data and benchmarking it against standard LLM outputs using advanced text similarity metrics and demographic assessments.", "result": "Experimental results indicate that PersonaTwin provides simulation fidelity comparable to oracle settings and that downstream models trained on persona-twins show approximate performance to individual-trained models in prediction and fairness metrics.", "conclusion": "PersonaTwin demonstrates the feasibility of LLM digital twin approaches for creating realistic and emotionally nuanced user simulations, which can enhance personalized user modeling and behavior analysis.", "key_contributions": ["Introduction of the PersonaTwin framework for user modeling.", "Benchmarking against standard LLMs with a focus on fairness and predictive accuracy.", "Utilization of a large healthcare dataset for rigorous evaluation."], "limitations": "", "keywords": ["large language models", "user modeling", "digital twins", "healthcare", "behavior analysis"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.10917", "pdf": "https://arxiv.org/pdf/2508.10917.pdf", "abs": "https://arxiv.org/abs/2508.10917", "title": "Managing the unexpected: Operator behavioural data and its value in predicting correct alarm responses", "authors": ["Chidera W. Amazu", "Joseph Mietkiewicz", "Ammar N. Abbas", "Gabriele Baldissone", "Davide Fissore", "Micaela Demichela", "Anders L. Madsen", "Maria Chiara Leva"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Data from psychophysiological measures can offer new insight into control\nroom operators' behaviour, cognition, and mental workload status. This can be\nparticularly helpful when combined with appraisal of capacity to respond to\npossible critical plant conditions (i.e. critical alarms response scenarios).\nHowever, wearable physiological measurement tools such as eye tracking and EEG\ncaps can be perceived as intrusive and not suitable for usage in daily\noperations. Therefore, this article examines the potential of using real-time\ndata from process and operator-system interactions during abnormal scenarios\nthat can be recorded and retrieved from the distributed control system's\nhistorian or process log, and their capacity to provide insight into operator\nbehavior and predict their response outcomes, without intruding on daily tasks.\nData for this study were obtained from a design of experiment using a\nformaldehyde production plant simulator and four human-in-the-loop experimental\nsupport configurations. A comparison between the different configurations in\nterms of both behaviour and performance is presented in this paper. A step-wise\nlogistic regression and a Bayesian network models were used to achieve this\nobjective. The results identified some predictive metrics and the paper discuss\ntheir value as precursor or predictor of overall system performance in alarm\nresponse scenarios. Knowledge of relevant and predictive behavioural metrics\naccessible in real time can better equip decision-makers to predict outcomes\nand provide timely support measures for operators.", "AI": {"tldr": "This paper investigates the use of real-time data from process and operator-system interactions to predict operator behavior and responses to critical plant conditions, without using intrusive wearable measures.", "motivation": "To explore ways of gaining insight into control room operators' behavior and mental workload without intrusive physiological measures.", "method": "Data was collected from a formaldehyde production plant simulator using human-in-the-loop experimental configurations. Step-wise logistic regression and Bayesian network models were employed for analysis.", "result": "The study identified key predictive metrics that serve as precursors to overall system performance in alarm response scenarios.", "conclusion": "Real-time behavioral metrics can enhance decision-making by predicting outcomes and providing timely support for operators during critical scenarios.", "key_contributions": ["Identified predictive metrics for operator performance in alarm scenarios", "Demonstrated the utility of non-intrusive data from control system logs", "Provided insights on enhancing operator support measures in critical situations"], "limitations": "", "keywords": ["psychophysiology", "operator behavior", "alarm response", "predictive modeling", "human-in-the-loop"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2508.10925", "pdf": "https://arxiv.org/pdf/2508.10925.pdf", "abs": "https://arxiv.org/abs/2508.10925", "title": "gpt-oss-120b & gpt-oss-20b Model Card", "authors": ["OpenAI", ":", "Sandhini Agarwal", "Lama Ahmad", "Jason Ai", "Sam Altman", "Andy Applebaum", "Edwin Arbus", "Rahul K. Arora", "Yu Bai", "Bowen Baker", "Haiming Bao", "Boaz Barak", "Ally Bennett", "Tyler Bertao", "Nivedita Brett", "Eugene Brevdo", "Greg Brockman", "Sebastien Bubeck", "Che Chang", "Kai Chen", "Mark Chen", "Enoch Cheung", "Aidan Clark", "Dan Cook", "Marat Dukhan", "Casey Dvorak", "Kevin Fives", "Vlad Fomenko", "Timur Garipov", "Kristian Georgiev", "Mia Glaese", "Tarun Gogineni", "Adam Goucher", "Lukas Gross", "Katia Gil Guzman", "John Hallman", "Jackie Hehir", "Johannes Heidecke", "Alec Helyar", "Haitang Hu", "Romain Huet", "Jacob Huh", "Saachi Jain", "Zach Johnson", "Chris Koch", "Irina Kofman", "Dominik Kundel", "Jason Kwon", "Volodymyr Kyrylov", "Elaine Ya Le", "Guillaume Leclerc", "James Park Lennon", "Scott Lessans", "Mario Lezcano-Casado", "Yuanzhi Li", "Zhuohan Li", "Ji Lin", "Jordan Liss", "Lily", "Liu", "Jiancheng Liu", "Kevin Lu", "Chris Lu", "Zoran Martinovic", "Lindsay McCallum", "Josh McGrath", "Scott McKinney", "Aidan McLaughlin", "Song Mei", "Steve Mostovoy", "Tong Mu", "Gideon Myles", "Alexander Neitz", "Alex Nichol", "Jakub Pachocki", "Alex Paino", "Dana Palmie", "Ashley Pantuliano", "Giambattista Parascandolo", "Jongsoo Park", "Leher Pathak", "Carolina Paz", "Ludovic Peran", "Dmitry Pimenov", "Michelle Pokrass", "Elizabeth Proehl", "Huida Qiu", "Gaby Raila", "Filippo Raso", "Hongyu Ren", "Kimmy Richardson", "David Robinson", "Bob Rotsted", "Hadi Salman", "Suvansh Sanjeev", "Max Schwarzer", "D. Sculley", "Harshit Sikchi", "Kendal Simon", "Karan Singhal", "Yang Song", "Dane Stuckey", "Zhiqing Sun", "Philippe Tillet", "Sam Toizer", "Foivos Tsimpourlas", "Nikhil Vyas", "Eric Wallace", "Xin Wang", "Miles Wang", "Olivia Watkins", "Kevin Weil", "Amy Wendling", "Kevin Whinnery", "Cedric Whitney", "Hannah Wong", "Lin Yang", "Yu Yang", "Michihiro Yasunaga", "Kristen Ying", "Wojciech Zaremba", "Wenting Zhan", "Cyril Zhang", "Brian Zhang", "Eddie Zhang", "Shengjia Zhao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present gpt-oss-120b and gpt-oss-20b, two open-weight reasoning models\nthat push the frontier of accuracy and inference cost. The models use an\nefficient mixture-of-expert transformer architecture and are trained using\nlarge-scale distillation and reinforcement learning. We optimize the models to\nhave strong agentic capabilities (deep research browsing, python tool use, and\nsupport for developer-provided functions), all while using a rendered chat\nformat that enables clear instruction following and role delineation. Both\nmodels achieve strong results on benchmarks ranging from mathematics, coding,\nand safety. We release the model weights, inference implementations, tool\nenvironments, and tokenizers under an Apache 2.0 license to enable broad use\nand further research.", "AI": {"tldr": "Introduction of two open-weight reasoning models, gpt-oss-120b and gpt-oss-20b, that enhance accuracy and reduce inference costs using a mixture-of-expert transformer architecture.", "motivation": "To advance the accuracy and cost-effectiveness of reasoning models while providing strong agentic capabilities.", "method": "Utilizes a mixture-of-expert transformer architecture, large-scale distillation, and reinforcement learning for training.", "result": "Achieves strong performance on various benchmarks in mathematics, coding, and safety.", "conclusion": "The release of model weights and implementations aims to foster broad usage and further research.", "key_contributions": ["Introduction of open-weight reasoning models", "Efficient mixture-of-expert architecture", "Support for diverse tool use in a chat format"], "limitations": "", "keywords": ["open-weight models", "mixure-of-experts", "reinforcement learning"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2508.10919", "pdf": "https://arxiv.org/pdf/2508.10919.pdf", "abs": "https://arxiv.org/abs/2508.10919", "title": "Human-AI collaboration or obedient and often clueless AI in instruct, serve, repeat dynamics?", "authors": ["Mohammed Saqr", "Kamila Misiejuk", "Sonsoles López-Pernas"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "While research on human-AI collaboration exists, it mainly examined language\nlearning and used traditional counting methods with little attention to\nevolution and dynamics of collaboration on cognitively demanding tasks. This\nstudy examines human-AI interactions while solving a complex problem.\nStudent-AI interactions were qualitatively coded and analyzed with transition\nnetwork analysis, sequence analysis and partial correlation networks as well as\ncomparison of frequencies using chi-square and Person-residual shaded Mosaic\nplots to map interaction patterns, their evolution, and their relationship to\nproblem complexity and student performance. Findings reveal a dominant\nInstructive pattern with interactions characterized by iterative ordering\nrather than collaborative negotiation. Oftentimes, students engaged in long\nthreads that showed misalignment between their prompts and AI output that\nexemplified a lack of synergy that challenges the prevailing assumptions about\nLLMs as collaborative partners. We also found no significant correlations\nbetween assignment complexity, prompt length, and student grades suggesting a\nlack of cognitive depth, or effect of problem difficulty. Our study indicates\nthat the current LLMs, optimized for instruction-following rather than\ncognitive partnership, compound their capability to act as cognitively\nstimulating or aligned collaborators. Implications for designing AI systems\nthat prioritize cognitive alignment and collaboration are discussed.", "AI": {"tldr": "This study explores the dynamics of human-AI collaboration in solving complex problems, revealing that current LLMs function more as instructive tools than as collaborative partners.", "motivation": "To investigate the nature of human-AI interactions in cognitively demanding tasks and to challenge prevailing assumptions about LLMs as collaborative entities.", "method": "Qualitative coding of student-AI interactions, followed by transition network analysis, sequence analysis, partial correlation networks, and frequency comparisons through chi-square and Person-residual shaded Mosaic plots.", "result": "The study found a dominant Instructive interaction pattern, highlighting a lack of synergy between students and AI, with no significant correlations identified between assignment complexity, prompt length, and student performance.", "conclusion": "Current LLMs are more instruction-following than cognitively aligned partners, indicating a need for AI systems that enhance cognitive collaboration.", "key_contributions": ["Identification of dominant Instructive interaction patterns in human-AI collaboration.", "Demonstration of misalignment between student prompts and AI outputs.", "Recommendations for designing AI systems to enhance cognitive partnership."], "limitations": "Focused primarily on interaction patterns without a deeper dive into cognitive engagement mechanisms.", "keywords": ["human-AI collaboration", "LLMs", "cognitive partnership", "instructive pattern", "interaction dynamics"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.10927", "pdf": "https://arxiv.org/pdf/2508.10927.pdf", "abs": "https://arxiv.org/abs/2508.10927", "title": "Modeling and Detecting Company Risks from News: A Case Study in Bloomberg News", "authors": ["Jiaxin Pei", "Soumya Vadlamannati", "Liang-Kang Huang", "Daniel Preotiuc-Pietro", "Xinyu Hua"], "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.LG"], "comment": null, "summary": "Identifying risks associated with a company is important to investors and the\nwell-being of the overall financial market. In this study, we build a\ncomputational framework to automatically extract company risk factors from news\narticles. Our newly proposed schema comprises seven distinct aspects, such as\nsupply chain, regulations, and competitions. We sample and annotate 744 news\narticles and benchmark various machine learning models. While large language\nmodels have achieved huge progress in various types of NLP tasks, our\nexperiment shows that zero-shot and few-shot prompting state-of-the-art LLMs\n(e.g. LLaMA-2) can only achieve moderate to low performances in identifying\nrisk factors. And fine-tuned pre-trained language models are performing better\non most of the risk factors. Using this model, we analyze over 277K Bloomberg\nnews articles and demonstrate that identifying risk factors from news could\nprovide extensive insight into the operations of companies and industries.", "AI": {"tldr": "A study develops a framework for extracting company risk factors from news articles using machine learning, revealing that fine-tuned LLMs outperform few-shot prompting.", "motivation": "To aid investors and enhance financial market stability by identifying company risk factors from news articles.", "method": "Built a computational framework with a schema of seven aspects; annotated 744 news articles and benchmarked different ML models, including LLaMA-2.", "result": "Fine-tuned language models outperform zero-shot and few-shot LLMs in identifying risk factors; analyzed over 277K articles showing insights into company operations.", "conclusion": "Fine-tuned models provide more reliable risk factor identification, highlighting the limitations of state-of-the-art LLMs in this specific application.", "key_contributions": ["Proposed a new schema for categorizing company risk factors from news articles.", "Conducted a comprehensive analysis using over 277K news articles.", "Demonstrated the superiority of fine-tuned models over few-shot prompting LLMs in risk factor extraction."], "limitations": "The study’s reliance on annotated news articles may limit generalizability; further research needed on broader datasets.", "keywords": ["company risk factors", "machine learning", "news articles", "financial market", "language models"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2508.11022", "pdf": "https://arxiv.org/pdf/2508.11022.pdf", "abs": "https://arxiv.org/abs/2508.11022", "title": "GhostObjects: Instructing Robots by Manipulating Spatially Aligned Virtual Twins in Augmented Reality", "authors": ["Lauren W. Wang", "Parastoo Abtahi"], "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "Robots are increasingly capable of autonomous operations, yet human\ninteraction remains essential for issuing personalized instructions. Instead of\ndirectly controlling robots through Programming by Demonstration (PbD) or\nteleoperation, we propose giving instructions by interacting with\nGhostObjects-world-aligned, life-size virtual twins of physical objects-in\naugmented reality (AR). By direct manipulation of GhostObjects, users can\nprecisely specify physical goals and spatial parameters, with features\nincluding real-world lasso selection of multiple objects and snapping back to\ndefault positions, enabling tasks beyond simple pick-and-place.", "AI": {"tldr": "The paper proposes using augmented reality (AR) to allow users to give personalized instructions to robots through the manipulation of virtual twins of physical objects, called GhostObjects.", "motivation": "To improve how users interact with robots for personalized instruction, moving beyond traditional methods like Programming by Demonstration and teleoperation.", "method": "The methodology involves creating GhostObjects in AR, which users can manipulate to specify physical goals and spatial parameters for robot tasks.", "result": "The proposed system enables features such as real-world lasso selection of multiple objects and snapping back to default positions, broadening task capabilities beyond simple pick-and-place.", "conclusion": "Augmented reality can enhance user-robot interaction by facilitating more natural and precise instructions via GhostObjects.", "key_contributions": ["Introduction of GhostObjects for AR interaction with robots", "Real-world lasso selection capability", "Enhanced users' ability to specify complex physical goals."], "limitations": "", "keywords": ["Augmented Reality", "Human-Robot Interaction", "GhostObjects"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.10971", "pdf": "https://arxiv.org/pdf/2508.10971.pdf", "abs": "https://arxiv.org/abs/2508.10971", "title": "Rule2Text: A Framework for Generating and Evaluating Natural Language Explanations of Knowledge Graph Rules", "authors": ["Nasim Shirvani-Mahdavi", "Chengkai Li"], "categories": ["cs.CL", "cs.AI"], "comment": "arXiv admin note: text overlap with arXiv:2507.23740", "summary": "Knowledge graphs (KGs) can be enhanced through rule mining; however, the\nresulting logical rules are often difficult for humans to interpret due to\ntheir inherent complexity and the idiosyncratic labeling conventions of\nindividual KGs. This work presents Rule2Text, a comprehensive framework that\nleverages large language models (LLMs) to generate natural language\nexplanations for mined logical rules, thereby improving KG accessibility and\nusability. We conduct extensive experiments using multiple datasets, including\nFreebase variants (FB-CVT-REV, FB+CVT-REV, and FB15k-237) as well as the\nogbl-biokg dataset, with rules mined using AMIE 3.5.1. We systematically\nevaluate several LLMs across a comprehensive range of prompting strategies,\nincluding zero-shot, few-shot, variable type incorporation, and\nChain-of-Thought reasoning. To systematically assess models' performance, we\nconduct a human evaluation of generated explanations on correctness and\nclarity. To address evaluation scalability, we develop and validate an\nLLM-as-a-judge framework that demonstrates strong agreement with human\nevaluators. Leveraging the best-performing model (Gemini 2.0 Flash), LLM judge,\nand human-in-the-loop feedback, we construct high-quality ground truth\ndatasets, which we use to fine-tune the open-source Zephyr model. Our results\ndemonstrate significant improvements in explanation quality after fine-tuning,\nwith particularly strong gains in the domain-specific dataset. Additionally, we\nintegrate a type inference module to support KGs lacking explicit type\ninformation. All code and data are publicly available at\nhttps://github.com/idirlab/KGRule2NL.", "AI": {"tldr": "Rule2Text is a framework that uses large language models to generate natural language explanations for mined logical rules in knowledge graphs, enhancing their accessibility and usability.", "motivation": "There is a need for more interpretable logical rules derived from knowledge graphs, as existing rules are complex and challenging for humans to understand.", "method": "The study employs large language models (LLMs) and various prompting strategies to generate explanations for mined logical rules, evaluating model performance through human assessments and developing an LLM-as-a-judge framework.", "result": "Fine-tuning with the best-performing model led to significant improvements in the quality of the generated explanations, especially in domain-specific applications.", "conclusion": "The introduction of Rule2Text significantly advances the interpretability of knowledge graphs by providing clearer explanations for complex rules, making them more user-friendly.", "key_contributions": ["Development of Rule2Text framework for generating natural language explanations from knowledge graphs", "Extensive evaluation of multiple LLMs and prompting strategies", "Construction of high-quality ground truth datasets for fine-tuning models"], "limitations": "The framework's performance may vary across different types of knowledge graphs and datasets used for training.", "keywords": ["Knowledge Graphs", "Large Language Models", "Natural Language Explanations", "Rule Mining", "Human Evaluation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.11030", "pdf": "https://arxiv.org/pdf/2508.11030.pdf", "abs": "https://arxiv.org/abs/2508.11030", "title": "Families' Vision of Generative AI Agents for Household Safety Against Digital and Physical Threats", "authors": ["Zikai Wen", "Lanjing Liu", "Yaxing Yao"], "categories": ["cs.HC"], "comment": null, "summary": "As families face increasingly complex safety challenges in digital and\nphysical environments, generative AI (GenAI) presents new opportunities to\nsupport household safety through multiple specialized AI agents. Through a\ntwo-phase qualitative study consisting of individual interviews and\ncollaborative sessions with 13 parent-child dyads, we explored families'\nconceptualizations of GenAI and their envisioned use of AI agents in daily\nfamily life. Our findings reveal that families preferred to distribute\nsafety-related support across multiple AI agents, each embodying a familiar\ncaregiving role: a household manager coordinating routine tasks and mitigating\nrisks such as digital fraud and home accidents; a private tutor providing\npersonalized educational support, including safety education; and a family\ntherapist offering emotional support to address sensitive safety issues such as\ncyberbullying and digital harassment. Families emphasized the need for\nagent-specific privacy boundaries, recognized generational differences in trust\ntoward AI agents, and stressed the importance of maintaining open family\ncommunication alongside the assistance of AI agents. Based on these findings,\nwe propose a multi-agent system design featuring four privacy-preserving\nprinciples: memory segregation, conversational consent, selective data sharing,\nand progressive memory management to help balance safety, privacy, and autonomy\nwithin family contexts.", "AI": {"tldr": "The study investigates families' perceptions of generative AI in enhancing household safety through specialized AI agents, revealing preferences for distributed roles among agents.", "motivation": "Families are facing increasing safety challenges in digital and physical environments, and GenAI offers new support opportunities.", "method": "A two-phase qualitative study with individual interviews and collaborative sessions involving 13 parent-child dyads.", "result": "Families preferred multiple AI agents for safety-related tasks, each embodying familiar caregiving roles, with distinct privacy preferences and communication needs.", "conclusion": "A proposed multi-agent system design emphasizing privacy principles can help balance safety, privacy, and autonomy.", "key_contributions": ["Identified family roles for AI agents in safety contexts", "Proposed a multi-agent system with privacy-preserving principles", "Explored generational differences in trust towards AI agents"], "limitations": "", "keywords": ["Generative AI", "Household safety", "Multi-agent system", "Privacy", "Family communication"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.10995", "pdf": "https://arxiv.org/pdf/2508.10995.pdf", "abs": "https://arxiv.org/abs/2508.10995", "title": "Improving Text Style Transfer using Masked Diffusion Language Models with Inference-time Scaling", "authors": ["Tejomay Kishor Padole", "Suyash P Awate", "Pushpak Bhattacharyya"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted as a main conference submission in the European Conference\n  on Artificial Intelligence (ECAI 2025)", "summary": "Masked diffusion language models (MDMs) have recently gained traction as a\nviable generative framework for natural language. This can be attributed to its\nscalability and ease of training compared to other diffusion model paradigms\nfor discrete data, establishing itself as the state-of-the-art\nnon-autoregressive generator for discrete data. Diffusion models, in general,\nhave shown excellent ability to improve the generation quality by leveraging\ninference-time scaling either by increasing the number of denoising steps or by\nusing external verifiers on top of the outputs of each step to guide the\ngeneration. In this work, we propose a verifier-based inference-time scaling\nmethod that aids in finding a better candidate generation during the denoising\nprocess of the MDM. Our experiments demonstrate the application of MDMs for\nstandard text-style transfer tasks and establish MDMs as a better alternative\nto autoregressive language models. Additionally, we show that a simple\nsoft-value-based verifier setup for MDMs using off-the-shelf pre-trained\nembedding models leads to significant gains in generation quality even when\nused on top of typical classifier-free guidance setups in the existing\nliterature.", "AI": {"tldr": "This paper presents a verifier-based inference-time scaling method for masked diffusion language models (MDMs), improving text generation quality.", "motivation": "To enhance the generation quality of masked diffusion language models (MDMs) and establish them as a superior alternative to autoregressive language models.", "method": "The authors propose a verifier-based inference-time scaling method during the denoising process of MDMs, which utilizes soft-value-based verifiers to improve candidate generation.", "result": "Experiments show that MDMs, enhanced with a simple verifier setup, significantly outperform traditional autoregressive models in text-style transfer tasks.", "conclusion": "The findings indicate that integrating verifier systems into MDMs can lead to substantial improvements in generation quality, reinforcing their status as the state-of-the-art for discrete data.", "key_contributions": ["Introduction of a verifier-based method for MDMs during denoising", "Demonstration of MDMs' superiority over autoregressive models", "Significant improvement in generation quality using soft-value-based verifiers."], "limitations": "", "keywords": ["masked diffusion language models", "text generation", "inference-time scaling"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.11052", "pdf": "https://arxiv.org/pdf/2508.11052.pdf", "abs": "https://arxiv.org/abs/2508.11052", "title": "AI That Helps Us Help Each Other: A Proactive System for Scaffolding Mentor-Novice Collaboration in Entrepreneurship Coaching", "authors": ["Evey Jiaxin Huang", "Matthew Easterday", "Elizabeth Gerber"], "categories": ["cs.HC", "cs.AI", "68T35 (Primary), 68U99 (Secondary)", "H.5.2"], "comment": "To appear in CSCW 2025 Volume 9", "summary": "Entrepreneurship requires navigating open-ended, ill-defined problems:\nidentifying risks, challenging assumptions, and making strategic decisions\nunder deep uncertainty. Novice founders often struggle with these metacognitive\ndemands, while mentors face limited time and visibility to provide tailored\nsupport. We present a human-AI coaching system that combines a domain-specific\ncognitive model of entrepreneurial risk with a large language model (LLM) to\nproactively scaffold both novice and mentor thinking. The system proactively\nposes diagnostic questions that challenge novices' thinking and helps both\nnovices and mentors plan for more focused and emotionally attuned meetings.\nCritically, mentors can inspect and modify the underlying cognitive model,\nshaping the logic of the system to reflect their evolving needs. Through an\nexploratory field deployment, we found that using the system supported novice\nmetacognition, helped mentors plan emotionally attuned strategies, and improved\nmeeting depth, intentionality, and focus--while also surfaced key tensions\naround trust, misdiagnosis, and expectations of AI. We contribute design\nprinciples for proactive AI systems that scaffold metacognition and human-human\ncollaboration in complex, ill-defined domains, offering implications for\nsimilar domains like healthcare, education, and knowledge work.", "AI": {"tldr": "This paper presents a human-AI coaching system designed to support novice entrepreneurs and their mentors by improving metacognitive thinking and enhancing meeting quality through a large language model and a cognitive model of entrepreneurial risk.", "motivation": "Novice entrepreneurs struggle with metacognitive demands while mentors have limited ability to provide tailored support due to time constraints. This study aims to improve the interaction between novices and mentors in the context of entrepreneurship.", "method": "The system integrates a cognitive model of entrepreneurial risk and a large language model (LLM) to ask diagnostic questions, challenging novices' thought processes and aiding mentors in planning effective meetings.", "result": "Field deployments showed that the AI system enhanced novice metacognition, enabled mentors to create emotionally attuned meeting strategies, and improved the depth, intentionality, and focus of meetings, while revealing tensions related to trust and AI expectations.", "conclusion": "The findings contribute to the understanding of design principles for proactive AI systems that aid metacognition and collaboration in complex domains, with implications for fields such as healthcare and education.", "key_contributions": ["Development of a human-AI coaching system for entrepreneurship", "Scaffolding metacognition in novice entrepreneurs", "Design principles for proactive AI in ill-defined domains"], "limitations": "Key tensions around trust, misdiagnosis, and expectations of AI were identified but require further examination.", "keywords": ["Human-AI collaboration", "Entrepreneurship", "Metacognition", "Large language models", "Coaching systems"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.11009", "pdf": "https://arxiv.org/pdf/2508.11009.pdf", "abs": "https://arxiv.org/abs/2508.11009", "title": "SproutBench: A Benchmark for Safe and Ethical Large Language Models for Youth", "authors": ["Wenpeng Xing", "Lanyi Wei", "Haixiao Hu", "Rongchang Li", "Mohan Li", "Changting Lin", "Meng Han"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid proliferation of large language models (LLMs) in applications\ntargeting children and adolescents necessitates a fundamental reassessment of\nprevailing AI safety frameworks, which are largely tailored to adult users and\nneglect the distinct developmental vulnerabilities of minors. This paper\nhighlights key deficiencies in existing LLM safety benchmarks, including their\ninadequate coverage of age-specific cognitive, emotional, and social risks\nspanning early childhood (ages 0--6), middle childhood (7--12), and adolescence\n(13--18). To bridge these gaps, we introduce SproutBench, an innovative\nevaluation suite comprising 1,283 developmentally grounded adversarial prompts\ndesigned to probe risks such as emotional dependency, privacy violations, and\nimitation of hazardous behaviors. Through rigorous empirical evaluation of 47\ndiverse LLMs, we uncover substantial safety vulnerabilities, corroborated by\nrobust inter-dimensional correlations (e.g., between Safety and Risk\nPrevention) and a notable inverse relationship between Interactivity and Age\nAppropriateness. These insights yield practical guidelines for advancing\nchild-centric AI design and deployment.", "AI": {"tldr": "The paper critiques current AI safety frameworks for large language models (LLMs) as they relate to children and adolescents, introducing SproutBench to evaluate safety risks specific to different age groups.", "motivation": "To reassess AI safety frameworks for LLMs aimed at minors, focusing on developmental vulnerabilities that existing benchmarks overlook.", "method": "The authors developed SproutBench, an evaluation suite with 1,283 adversarial prompts targeted at the cognitive, emotional, and social risks of different age groups.", "result": "The evaluation of 47 LLMs revealed significant safety vulnerabilities, indicating critical gaps in existing safety frameworks for young users.", "conclusion": "The findings provide practical guidelines for improving child-centric AI design and deployment by addressing distinct vulnerabilities of minors.", "key_contributions": ["Introduction of SproutBench, an evaluation suite for child-specific LLM risks", "Identification of substantial safety vulnerabilities in 47 LLMs", "Insights into the relationship between interactivity and age appropriateness in AI design."], "limitations": "", "keywords": ["AI safety", "large language models", "child-centric design", "developmental vulnerabilities", "SproutBench"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.11059", "pdf": "https://arxiv.org/pdf/2508.11059.pdf", "abs": "https://arxiv.org/abs/2508.11059", "title": "Stories and Systems: Educational Interactive Storytelling to Teach Media Literacy and Systemic Thinking", "authors": ["Christian Roth", "Rahmin Bender-Salazar", "Breanne Pitt"], "categories": ["cs.HC", "cs.ET"], "comment": "Under submission (May, 2025)", "summary": "This paper explores how Interactive Digital Narratives (IDNs) can support\nlearners in developing the critical literacies needed to address complex\nsocietal challenges, so-called wicked problems, such as climate change,\npandemics, and social inequality. While digital technologies offer broad access\nto narratives and data, they also contribute to misinformation and the\noversimplification of interconnected issues. IDNs enable learners to navigate\nnonlinear, interactive stories, fostering deeper understanding and engagement.\nWe introduce Systemic Learning IDNs: interactive narrative experiences\nexplicitly designed to help learners explore and reflect on complex systems and\ninterdependencies. To guide their creation and use, we propose the CLASS\nframework, a structured model that integrates systems thinking, design\nthinking, and storytelling. This transdisciplinary approach supports learners\nin developing curiosity, critical thinking, and collaborative problem-solving.\nFocusing on the classroom context, we apply CLASS to two cases, one commercial\nnarrative simulation and one educational prototype, offering a comparative\nanalysis and practical recommendations for future design and implementation. By\ncombining narrative, systems mapping, and participatory design, this paper\nhighlights how IDNs can become powerful tools for transformative,\nsystems-oriented learning in an increasingly complex world.", "AI": {"tldr": "This paper presents Interactive Digital Narratives (IDNs) as tools to develop critical literacies for addressing complex societal challenges, proposing a framework for their effective design and use in education.", "motivation": "To explore how IDNs can help learners develop critical literacies to tackle wicked problems such as climate change, pandemics, and social inequality.", "method": "Introduction of the CLASS framework, which integrates systems thinking, design thinking, and storytelling to guide the creation and use of Systemic Learning IDNs in educational contexts.", "result": "The application of CLASS to two case studies demonstrates its effectiveness in fostering curiosity, critical thinking, and collaborative problem-solving among learners.", "conclusion": "IDNs can serve as transformative tools for learning about complex systems and interdependencies, enhancing engagement and understanding in a digital age.", "key_contributions": ["Introduction of the CLASS framework for designing IDNs.", "Comparative analysis of a commercial narrative simulation and an educational prototype for practical recommendations.", "Highlighting the role of IDNs in supporting transformative learning experiences."], "limitations": "", "keywords": ["Interactive Digital Narratives", "critical literacies", "systems thinking", "education", "wicked problems"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2508.11017", "pdf": "https://arxiv.org/pdf/2508.11017.pdf", "abs": "https://arxiv.org/abs/2508.11017", "title": "Beyond the Rosetta Stone: Unification Forces in Generalization Dynamics", "authors": ["Carter Blum", "Katja Filipova", "Ann Yuan", "Asma Ghandeharioun", "Julian Zimmert", "Fred Zhang", "Jessica Hoffmann", "Tal Linzen", "Martin Wattenberg", "Lucas Dixon", "Mor Geva"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) struggle with cross-lingual knowledge transfer:\nthey hallucinate when asked in one language about facts expressed in a\ndifferent language during training. This work introduces a controlled setting\nto study the causes and dynamics of this phenomenon by training small\nTransformer models from scratch on synthetic multilingual datasets. We identify\na learning phase wherein a model develops either separate or unified\nrepresentations of the same facts across languages, and show that unification\nis essential for cross-lingual transfer. We also show that the degree of\nunification depends on mutual information between facts and training data\nlanguage, and on how easy it is to extract that language. Based on these\ninsights, we develop methods to modulate the level of cross-lingual transfer by\nmanipulating data distribution and tokenization, and we introduce metrics and\nvisualizations to formally characterize their effects on unification. Our work\nshows how controlled settings can shed light on pre-training dynamics and\nsuggests new directions for improving cross-lingual transfer in LLMs.", "AI": {"tldr": "This paper examines cross-lingual knowledge transfer in LLMs using small Transformer models trained on synthetic multilingual datasets, focusing on the concepts of representation unification and its impact on transfer effectiveness.", "motivation": "To address the challenges of cross-lingual knowledge transfer in large language models, particularly hallucinations arising from multilingual facts during training.", "method": "Synthetic multilingual datasets were used to train small Transformer models from scratch, allowing for the observation of how models develop representations across languages.", "result": "The study identifies critical phases of learning that involve either separate or unified representations of facts, demonstrating that unified representations are crucial for effective cross-lingual transfer.", "conclusion": "Controlled training settings can illuminate the dynamics of pre-training in LLMs and highlight new methodologies to enhance cross-lingual knowledge transfer.", "key_contributions": ["Identified learning phases in representation development for cross-lingual transfer.", "Developed metrics and visualizations to assess the impact of data distribution and tokenization on unification.", "Proposed methods to enhance cross-lingual transfer based on empirical findings."], "limitations": "", "keywords": ["Cross-lingual transfer", "Large language models", "Multilingual datasets"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.11062", "pdf": "https://arxiv.org/pdf/2508.11062.pdf", "abs": "https://arxiv.org/abs/2508.11062", "title": "Human-in-the-Loop Systems for Adaptive Learning Using Generative AI", "authors": ["Bhavishya Tarun", "Haoze Du", "Dinesh Kannan", "Edward F. Gehringer"], "categories": ["cs.HC", "cs.LG"], "comment": "Accepted for presentation at the Frontiers in Education Conference,\n  Nashville, Tennessee, USA, 2-5 November 2025", "summary": "A Human-in-the-Loop (HITL) approach leverages generative AI to enhance\npersonalized learning by directly integrating student feedback into\nAI-generated solutions. Students critique and modify AI responses using\npredefined feedback tags, fostering deeper engagement and understanding. This\nempowers students to actively shape their learning, with AI serving as an\nadaptive partner. The system uses a tagging technique and prompt engineering to\npersonalize content, informing a Retrieval-Augmented Generation (RAG) system to\nretrieve relevant educational material and adjust explanations in real time.\nThis builds on existing research in adaptive learning, demonstrating how\nstudent-driven feedback loops can modify AI-generated responses for improved\nstudent retention and engagement, particularly in STEM education. Preliminary\nfindings from a study with STEM students indicate improved learning outcomes\nand confidence compared to traditional AI tools. This work highlights AI's\npotential to create dynamic, feedback-driven, and personalized learning\nenvironments through iterative refinement.", "AI": {"tldr": "A HITL approach integrates student feedback into AI-generated learning solutions, enhancing personalization and engagement in STEM education.", "motivation": "To enhance personalized learning by using student feedback in AI-generated solutions.", "method": "A Human-in-the-Loop approach utilizing tagging techniques and prompt engineering to inform a Retrieval-Augmented Generation system.", "result": "Preliminary findings indicate improved learning outcomes and confidence among STEM students using this approach compared to traditional AI tools.", "conclusion": "AI can create dynamic, personalized learning environments through student-driven feedback loops and iterative refinement.", "key_contributions": ["Integration of student feedback in AI-generated solutions", "Development of a tagging technique for personalized content", "Demonstration of improved learning outcomes in STEM education"], "limitations": "", "keywords": ["Human-in-the-Loop", "Generative AI", "Personalized Learning", "STEM Education", "Feedback"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.11027", "pdf": "https://arxiv.org/pdf/2508.11027.pdf", "abs": "https://arxiv.org/abs/2508.11027", "title": "Hell or High Water: Evaluating Agentic Recovery from External Failures", "authors": ["Andrew Wang", "Sophia Hager", "Adi Asija", "Daniel Khashabi", "Nicholas Andrews"], "categories": ["cs.CL"], "comment": "Accepted to COLM 2025", "summary": "As language model agents are applied to real world problems of increasing\ncomplexity, they will be expected to formulate plans across large search\nspaces. If those plans fail for reasons beyond their control, how well do\nlanguage agents search for alternative ways to achieve their goals? We devise a\nspecialized agentic planning benchmark to study this question. Each planning\nproblem is solved via combinations of function calls. The agent searches for\nrelevant functions from a set of over four thousand possibilities, and observes\nenvironmental feedback in the form of function outputs or error messages. Our\nbenchmark confronts the agent with external failures in its workflow, such as\nfunctions that suddenly become unavailable. At the same time, even with the\nintroduction of these failures, we guarantee that the task remains solvable.\nIdeally, an agent's performance on the planning task should not be affected by\nthe presence of external failures. Overall, we find that language agents\nstruggle to formulate and execute backup plans in response to environment\nfeedback. While state-of-the-art models are often able to identify the correct\nfunction to use in the right context, they struggle to adapt to feedback from\nthe environment and often fail to pursue alternate courses of action, even when\nthe search space is artificially restricted. We provide a systematic analysis\nof the failures of both open-source and commercial models, examining the\neffects of search space size, as well as the benefits of scaling model size in\nour setting. Our analysis identifies key challenges for current generative\nmodels as well as promising directions for future work.", "AI": {"tldr": "This paper presents a benchmark for evaluating language model agents' capacity to adapt plans in response to external failures, revealing that current models struggle with backup planning despite being contextually aware.", "motivation": "To understand how well language model agents can formulate alternative plans when their initial plans fail due to external factors.", "method": "A specialized agentic planning benchmark was devised where agents search for functions from a set of over four thousand possibilities, facing simulated external failures while maintaining solvable tasks.", "result": "Language agents demonstrated difficulty in adapting to environmental feedback and struggled to pursue alternative action plans, even when the search space was limited.", "conclusion": "The performance of language agents on planning tasks is negatively affected by external failures, highlighting key limitations in current generative models and suggesting areas for future research.", "key_contributions": ["Introduction of a novel benchmark for agentic planning under external failure conditions.", "Systematic analysis of the limitations of open-source and commercial language models in handling alternative plans.", "Identification of challenges in the performance of state-of-the-art models in dynamic environments."], "limitations": "Current models struggle to execute backup plans and adapt to changes in environmental feedback; performance affected by search space size.", "keywords": ["language models", "agentic planning", "environmental feedback", "backup plans", "failure analysis"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.11072", "pdf": "https://arxiv.org/pdf/2508.11072.pdf", "abs": "https://arxiv.org/abs/2508.11072", "title": "DriveSimQuest: A VR Driving Simulator and Research Platform on Meta Quest with Unity", "authors": ["Nishanth Chidambaram", "Weichen Liu", "Manas Satish Bedmutha", "Nadir Weibel", "Chen Chen"], "categories": ["cs.HC", "H.5.m; J.m"], "comment": "3 pages, 2 figures, In the Proceedings of the 38th Annual ACM\n  Symposium on User Interface Software and Technology (UIST Adjunct '25),\n  September 28 - October 1, 2025, Busan, Republic of Korea", "summary": "Using head-mounted Virtual Reality (VR) displays to simulate driving is\ncritical to studying driving behavior and designing driver assistance systems.\nBut existing VR driving simulators are often limited to tracking only eye\nmovements. The bulky outside-in tracking setup and Unreal-based architecture\nalso present significant engineering challenges for interaction researchers and\npractitioners. We present DriveSimQuest, a VR driving simulator and research\nplatform built on the Meta Quest Pro and Unity, capable of capturing rich\nbehavioral signals such as gaze, facial expressions, hand activities, and\nfull-body gestures in real-time. DriveSimQuest offers a preliminary,\neasy-to-deploy platform that supports researchers and practitioners in studying\ndrivers' affective states and behaviors, and in designing future context-aware\ndriving assistance systems.", "AI": {"tldr": "DriveSimQuest is a VR driving simulator that captures various behavioral signals in real-time, facilitating the study of driving behavior and the design of driver assistance systems.", "motivation": "There is a need for improved VR driving simulators that can capture comprehensive behavioral data beyond eye movements for the study of driving behavior and design of driver assistance systems.", "method": "DriveSimQuest uses the Meta Quest Pro and Unity to create a VR driving simulator capable of real-time tracking of gaze, facial expressions, hand activities, and full-body gestures.", "result": "The platform demonstrates enhanced capabilities for tracking driver behaviors and emotions, enabling more in-depth analysis of driving states.", "conclusion": "DriveSimQuest serves as an accessible platform for researchers to investigate driving behaviors and develop context-aware driving assistance systems.", "key_contributions": ["Real-time tracking of multiple behavioral signals (gaze, facial expressions, etc.)", "Built on Meta Quest Pro and Unity for ease of use", "Supports research in driver affective states and behavior analysis"], "limitations": "", "keywords": ["Virtual Reality", "Driving Simulation", "Human-Computer Interaction", "Driver Assistance Systems", "Behavioral Tracking"], "importance_score": 7, "read_time_minutes": 3}}
{"id": "2508.11061", "pdf": "https://arxiv.org/pdf/2508.11061.pdf", "abs": "https://arxiv.org/abs/2508.11061", "title": "BIPOLAR: Polarization-based granular framework for LLM bias evaluation", "authors": ["Martin Pavlíček", "Tomáš Filip", "Petr Sosík"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are known to exhibit biases in downstream tasks,\nespecially when dealing with sensitive topics such as political discourse,\ngender identity, ethnic relations, or national stereotypes. Although\nsignificant progress has been made in bias detection and mitigation techniques,\ncertain challenges remain underexplored. This study proposes a reusable,\ngranular, and topic-agnostic framework to evaluate polarisation-related biases\nin LLM (both open-source and closed-source). Our approach combines\npolarisation-sensitive sentiment metrics with a synthetically generated\nbalanced dataset of conflict-related statements, using a predefined set of\nsemantic categories.\n  As a case study, we created a synthetic dataset that focusses on the\nRussia-Ukraine war, and we evaluated the bias in several LLMs: Llama-3,\nMistral, GPT-4, Claude 3.5, and Gemini 1.0. Beyond aggregate bias scores, with\na general trend for more positive sentiment toward Ukraine, the framework\nallowed fine-grained analysis with considerable variation between semantic\ncategories, uncovering divergent behavioural patterns among models. Adaptation\nto prompt modifications showed further bias towards preconceived language and\ncitizenship modification.\n  Overall, the framework supports automated dataset generation and fine-grained\nbias assessment, is applicable to a variety of polarisation-driven scenarios\nand topics, and is orthogonal to many other bias-evaluation strategies.", "AI": {"tldr": "The paper presents a framework to evaluate polarization-related biases in large language models (LLMs), particularly in the context of sensitive topics like political discourse, using a synthetic dataset focused on the Russia-Ukraine war.", "motivation": "To address the underexplored challenges in detecting and mitigating biases in LLMs, especially when dealing with sensitive topics such as political and ethnic relations.", "method": "A reusable and granular framework combining polarization-sensitive sentiment metrics and a synthetically generated balanced dataset of conflict-related statements, focusing on predefined semantic categories.", "result": "The framework revealed aggregate bias scores indicating more positive sentiment toward Ukraine among evaluated LLMs, while allowing fine-grained analysis that uncovered significant variation in behavioral patterns across models.", "conclusion": "The proposed framework facilitates automated dataset generation and detailed bias assessment, applicable to various polarization-driven scenarios, and complements existing bias evaluation strategies.", "key_contributions": ["Development of a reusable and topic-agnostic bias evaluation framework for LLMs", "Creation of a synthetic dataset focused on the Russia-Ukraine conflict", "Fine-grained analysis revealing varying behavioral patterns among LLMs"], "limitations": "Further exploration is needed into different types of biases beyond those evaluated in this study.", "keywords": ["bias detection", "large language models", "sentiment analysis", "polarization", "synthetic dataset"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2508.11149", "pdf": "https://arxiv.org/pdf/2508.11149.pdf", "abs": "https://arxiv.org/abs/2508.11149", "title": "Toward Needs-Conscious Design: Co-Designing a Human-Centered Framework for AI-Mediated Communication", "authors": ["Robert Wolfe", "Aayushi Dangol", "JaeWon Kim", "Alexis Hiniker"], "categories": ["cs.HC"], "comment": "Accepted for publication at AIES 2025", "summary": "We introduce Needs-Conscious Design, a human-centered framework for\nAI-mediated communication that builds on the principles of Nonviolent\nCommunication (NVC). We conducted an interview study with N=14 certified NVC\ntrainers and a diary study and co-design with N=13 lay users of online\ncommunication technologies to understand how NVC might inform design that\ncenters human relationships. We define three pillars of Needs-Conscious Design:\nIntentionality, Presence, and Receptiveness to Needs. Drawing on participant\nco-designs, we provide design concepts and illustrative examples for each of\nthese pillars. We further describe a problematic emergent property of\nAI-mediated communication identified by participants, which we call Empathy\nFog, and which is characterized by uncertainty over how much empathy,\nattention, and effort a user has actually invested via an AI-facilitated online\ninteraction. Finally, because even well-intentioned designs may alter user\nbehavior and process emotional data, we provide guiding questions for\nconsentful Needs-Conscious Design, applying an affirmative consent framework\nused in social media contexts. Needs-Conscious Design offers a foundation for\nleveraging AI to facilitate human connection, rather than replacing or\nobscuring it.", "AI": {"tldr": "The paper introduces Needs-Conscious Design, a framework for AI-mediated communication focused on enhancing human relationships through intentional, present, and receptive design principles.", "motivation": "To explore how Nonviolent Communication (NVC) can inform the design of AI-mediated communication, emphasizing the importance of human relationships.", "method": "The study involved interviews with 14 certified NVC trainers and co-design sessions with 13 lay users of online communication technologies.", "result": "Identified three pillars of Needs-Conscious Design and described a phenomenon termed 'Empathy Fog' that affects user interaction with AI.", "conclusion": "Needs-Conscious Design provides a framework for using AI to foster, rather than hinder, human connections, incorporating a consent-based approach to emotional data handling.", "key_contributions": ["Introduction of the Needs-Conscious Design framework", "Identification of the Empathy Fog phenomenon", "Guidelines for consentful AI design in communication"], "limitations": "", "keywords": ["Needs-Conscious Design", "Human-Computer Interaction", "AI-mediated communication"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.11068", "pdf": "https://arxiv.org/pdf/2508.11068.pdf", "abs": "https://arxiv.org/abs/2508.11068", "title": "Approaching the Source of Symbol Grounding with Confluent Reductions of Abstract Meaning Representation Directed Graphs", "authors": ["Nicolas Goulet", "Alexandre Blondin Massé", "Moussa Abdendi"], "categories": ["cs.CL"], "comment": null, "summary": "Abstract meaning representation (AMR) is a semantic formalism used to\nrepresent the meaning of sentences as directed acyclic graphs. In this paper,\nwe describe how real digital dictionaries can be embedded into AMR directed\ngraphs (digraphs), using state-of-the-art pre-trained large language models.\nThen, we reduce those graphs in a confluent manner, i.e. with transformations\nthat preserve their circuit space. Finally, the properties of these reduces\ndigraphs are analyzed and discussed in relation to the symbol grounding\nproblem.", "AI": {"tldr": "This paper discusses embedding digital dictionaries into Abstract Meaning Representation (AMR) digraphs using large language models, and explores the reduction of these graphs while preserving their properties.", "motivation": "To enhance the representation of sentence meanings by integrating real digital dictionaries into AMR digraphs.", "method": "The authors utilize state-of-the-art pre-trained large language models to embed dictionaries into AMR directed graphs and apply confluent transformations for graph reduction while maintaining circuit space.", "result": "The paper presents analyzed properties of the reduced digraphs and their relevance to the symbol grounding problem.", "conclusion": "The study contributes to understanding how embedding dictionaries into AMR can improve semantic representations in natural language processing.", "key_contributions": ["Integration of digital dictionaries into AMR digraphs.", "Methodology for confluent graph reduction preserving circuit space.", "Analysis of reduced digraphs in relation to the symbol grounding problem."], "limitations": "", "keywords": ["Abstract Meaning Representation", "digital dictionaries", "large language models", "graph reduction", "symbol grounding"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2508.11150", "pdf": "https://arxiv.org/pdf/2508.11150.pdf", "abs": "https://arxiv.org/abs/2508.11150", "title": "From Misunderstandings to Learning Opportunities: Leveraging Generative AI in Discussion Forums to Support Student Learning", "authors": ["Stanislav Pozdniakov", "Jonathan Brazil", "Oleksandra Poquet", "Stephan Krusche", "Santiago Berrezueta-Guzman", "Shazia Sadiq", "Hassan Khosravi"], "categories": ["cs.HC"], "comment": "Artificial Intelligence in Education (AIED 2025)", "summary": "In the contemporary educational landscape, particularly in large classroom\nsettings, discussion forums have become a crucial tool for promoting\ninteraction and addressing student queries. These forums foster a collaborative\nlearning environment where students engage with both the teaching team and\ntheir peers. However, the sheer volume of content generated in these forums\nposes two significant interconnected challenges: How can we effectively\nidentify common misunderstandings that arise in student discussions? And once\nidentified, how can instructors use these insights to address them effectively?\nThis paper explores the approach to integrating large language models (LLMs)\nand Retrieval-Augmented Generation (RAG) to tackle these challenges. We then\ndemonstrate the approach Misunderstanding to Mastery (M2M) with authentic data\nfrom three computer science courses, involving 1355 students with 2878 unique\nposts, followed by an evaluation with five instructors teaching these courses.\nResults show that instructors found the approach promising and valuable for\nteaching, effectively identifying misunderstandings and generating actionable\ninsights. Instructors highlighted the need for more fine-grained groupings,\nclearer metrics, validation of the created resources, and ethical\nconsiderations around data anonymity.", "AI": {"tldr": "The paper explores the integration of large language models and Retrieval-Augmented Generation to identify and address common misunderstandings in large classroom discussion forums.", "motivation": "To enhance interaction and address frequent misunderstandings in educational discussion forums, leveraging AI tools.", "method": "The approach Misunderstanding to Mastery (M2M) integrates LLMs and RAG to analyze discussions from three computer science courses, engaging 1355 students and 2878 posts.", "result": "Instructors considered the M2M approach effective for identifying misunderstandings and generating actionable insights, while noting the need for improved grouping, metrics, validation, and ethical considerations.", "conclusion": "The integration of AI can significantly enhance educational practices by addressing misunderstandings in student discussions, although further refinement is needed.", "key_contributions": ["Introduction of the M2M framework for misunderstanding identification", "Analysis of real classroom data with instructor feedback", "Assessment of ethical considerations in data usage"], "limitations": "Need for more nuanced groupings, clearer evaluation metrics, and validation processes for resources generated.", "keywords": ["large language models", "Retrieval-Augmented Generation", "educational technology", "student discussions", "AI in education"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.11120", "pdf": "https://arxiv.org/pdf/2508.11120.pdf", "abs": "https://arxiv.org/abs/2508.11120", "title": "Towards Reliable Multi-Agent Systems for Marketing Applications via Reflection, Memory, and Planning", "authors": ["Lorenzo Jaime Yu Flores", "Junyi Shen", "Xiaoyuan Gu"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large language models (LLMs) enabled the development of AI\nagents that can plan and interact with tools to complete complex tasks.\nHowever, literature on their reliability in real-world applications remains\nlimited. In this paper, we introduce a multi-agent framework for a marketing\ntask: audience curation. To solve this, we introduce a framework called RAMP\nthat iteratively plans, calls tools, verifies the output, and generates\nsuggestions to improve the quality of the audience generated. Additionally, we\nequip the model with a long-term memory store, which is a knowledge base of\nclient-specific facts and past queries. Overall, we demonstrate the use of LLM\nplanning and memory, which increases accuracy by 28 percentage points on a set\nof 88 evaluation queries. Moreover, we show the impact of iterative\nverification and reflection on more ambiguous queries, showing progressively\nbetter recall (roughly +20 percentage points) with more verify/reflect\niterations on a smaller challenge set, and higher user satisfaction. Our\nresults provide practical insights for deploying reliable LLM-based systems in\ndynamic, industry-facing environments.", "AI": {"tldr": "The paper presents RAMP, a multi-agent framework for audience curation using LLMs, enhancing task completion reliability.", "motivation": "To address the limited literature on the reliability of LLMs in real-world applications, particularly in marketing tasks.", "method": "A multi-agent framework named RAMP was developed that iteratively plans, calls tools, verifies outputs, and generates suggestions for audience curation.", "result": "The framework increased accuracy by 28 percentage points on evaluation queries and showed better recall with more iterations of verification and reflection.", "conclusion": "RAMP provides practical insights for deploying reliable LLM-based systems in dynamic environments, leading to higher user satisfaction and improved results.", "key_contributions": ["Introduction of a multi-agent framework for task completion using LLMs.", "Implementation of long-term memory for client-specific facts.", "Demonstration of increased accuracy and user satisfaction through iterative verification."], "limitations": "", "keywords": ["LLM", "multi-agent framework", "audience curation"], "importance_score": 9, "read_time_minutes": 6}}
{"id": "2508.11278", "pdf": "https://arxiv.org/pdf/2508.11278.pdf", "abs": "https://arxiv.org/abs/2508.11278", "title": "Is General-Purpose AI Reasoning Sensitive to Data-Induced Cognitive Biases? Dynamic Benchmarking on Typical Software Engineering Dilemmas", "authors": ["Francesco Sovrano", "Gabriele Dominici", "Rita Sevastjanova", "Alessandra Stramiglio", "Alberto Bacchelli"], "categories": ["cs.HC", "cs.AI", "cs.SE"], "comment": null, "summary": "Human cognitive biases in software engineering can lead to costly errors.\nWhile general-purpose AI (GPAI) systems may help mitigate these biases due to\ntheir non-human nature, their training on human-generated data raises a\ncritical question: Do GPAI systems themselves exhibit cognitive biases?\n  To investigate this, we present the first dynamic benchmarking framework to\nevaluate data-induced cognitive biases in GPAI within software engineering\nworkflows. Starting with a seed set of 16 hand-crafted realistic tasks, each\nfeaturing one of 8 cognitive biases (e.g., anchoring, framing) and\ncorresponding unbiased variants, we test whether bias-inducing linguistic cues\nunrelated to task logic can lead GPAI systems from correct to incorrect\nconclusions.\n  To scale the benchmark and ensure realism, we develop an on-demand\naugmentation pipeline relying on GPAI systems to generate task variants that\npreserve bias-inducing cues while varying surface details. This pipeline\nensures correctness (88--99% on average, according to human evaluation),\npromotes diversity, and controls reasoning complexity by leveraging\nProlog-based reasoning and LLM-as-a-judge validation. It also verifies that the\nembedded biases are both harmful and undetectable by logic-based, unbiased\nreasoners.\n  We evaluate leading GPAI systems (GPT, LLaMA, DeepSeek) and find a consistent\ntendency to rely on shallow linguistic heuristics over deep reasoning. All\nsystems exhibit cognitive biases (ranging from 5.9% to 35% across types), with\nbias sensitivity increasing sharply with task complexity (up to 49%),\nhighlighting critical risks in real-world software engineering deployments.", "AI": {"tldr": "This paper investigates cognitive biases in general-purpose AI (GPAI) systems within software engineering using a dynamic benchmarking framework. It finds that GPAI systems exhibit cognitive biases influenced by linguistic cues, with varying sensitivity based on task complexity.", "motivation": "The study aims to understand if GPAI systems, which are trained on human-generated data, exhibit cognitive biases that can affect their decision-making in software engineering.", "method": "A dynamic benchmarking framework was developed, featuring a seed set of 16 tasks exhibiting cognitive biases, supplemented by an on-demand augmentation pipeline that generates diverse task variants while preserving bias-inducing cues, evaluated using GPAI systems.", "result": "Leading GPAI systems consistently exhibited cognitive biases, relying on shallow linguistic heuristics in their decision-making, with bias sensitivity increasing with task complexity.", "conclusion": "The findings highlight significant risks associated with cognitive biases in GPAI systems, particularly as task complexity increases, emphasizing the need for vigilant deployment practices in software engineering.", "key_contributions": ["Development of the first dynamic benchmarking framework for evaluating bias in GPAI in software engineering workflows", "Creation of an augmentation pipeline to ensure diversity and realism in task scenarios", "Identification of cognitive biases in leading GPAI systems and their reliance on linguistic heuristics over deep reasoning"], "limitations": "The study focuses only on a limited set of cognitive biases and may not encompass all biases present in GPAI systems or software engineering tasks.", "keywords": ["cognitive biases", "general-purpose AI", "software engineering", "benchmarking", "language models"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.11133", "pdf": "https://arxiv.org/pdf/2508.11133.pdf", "abs": "https://arxiv.org/abs/2508.11133", "title": "MoNaCo: More Natural and Complex Questions for Reasoning Across Dozens of Documents", "authors": ["Tomer Wolfson", "Harsh Trivedi", "Mor Geva", "Yoav Goldberg", "Dan Roth", "Tushar Khot", "Ashish Sabharwal", "Reut Tsarfaty"], "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": "Accepted for publication in Transactions of the Association for\n  Computational Linguistics (TACL), 2025. Authors pre-print", "summary": "Large language models (LLMs) are emerging as a go-to tool for querying\ninformation. However, current LLM benchmarks rarely feature natural questions\nthat are both information-seeking as well as genuinely time-consuming for\nhumans. To address this gap we introduce MoNaCo, a benchmark of 1,315 natural\nand complex questions that require dozens, and at times hundreds, of\nintermediate steps to solve -- far more than any existing QA benchmark. To\nbuild MoNaCo, we developed a decomposed annotation pipeline to elicit and\nmanually answer natural time-consuming questions at scale. Frontier LLMs\nevaluated on MoNaCo achieve at most 61.2% F1, hampered by low recall and\nhallucinations. Our results underscore the need for reasoning models that\nbetter handle the complexity and sheer breadth of real-world\ninformation-seeking questions -- with MoNaCo providing an effective resource\nfor tracking such progress. The MONACO benchmark, codebase, prompts and models\npredictions are publicly available at: https://tomerwolgithub.github.io/monaco", "AI": {"tldr": "MoNaCo is a new benchmark consisting of 1,315 complex natural questions requiring extensive reasoning, highlighting the shortcomings of current LLMs in handling intricate query tasks.", "motivation": "The paper addresses the lack of natural, information-seeking questions in existing LLM benchmarks that challenge the capabilities of current models.", "method": "A decomposed annotation pipeline was developed to elicit and manually answer complex, time-consuming natural questions at scale.", "result": "Frontier LLMs tested on MoNaCo achieved a maximum F1 score of 61.2%, indicating significant challenges in low recall and hallucination issues.", "conclusion": "MoNaCo serves as a powerful resource to monitor and encourage improvements in reasoning models for complex real-world information-seeking questions.", "key_contributions": ["Introduction of the MoNaCo benchmark with 1,315 complex questions.", "Demonstration of LLMs' limitations in answering intricate queries.", "Public availability of the benchmark, codebase, and model predictions."], "limitations": "The benchmark focuses specifically on the challenges of handling complex queries, which may not represent all possible user needs in real-world scenarios.", "keywords": ["Large Language Models", "Benchmark", "MoNaCo", "Information Seeking", "Reasoning Models"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2508.11304", "pdf": "https://arxiv.org/pdf/2508.11304.pdf", "abs": "https://arxiv.org/abs/2508.11304", "title": "GulliVR: A Walking-Oriented Technique for Navigation in Virtual Reality Games Based on Virtual Body Resizing", "authors": ["Andrey Krekhov", "Sebastian Cmentowski", "Katharina Emmerich", "Maic Masuch", "Jens Krüger"], "categories": ["cs.HC"], "comment": "author version", "summary": "Virtual reality games are often centered around our feeling of \"being there\".\nThat presence can be significantly enhanced by supporting physical walking.\nAlthough modern virtual reality systems enable room-scale motions, the size of\nour living rooms is not enough to explore vast virtual environments. Developers\nbypass that limitation by adding virtual navigation such as teleportation.\nAlthough such techniques are intended (or designed) to extend but not replace\nnatural walking, what we often observe are nonmoving players beaming to a\nlocation that is one real step ahead. Our navigation metaphor emphasizes\nphysical walking by promoting players into giants on demand to cover large\ndistances. In contrast to flying, our technique proportionally increases the\nmodeled eye distance, preventing cybersickness and creating the feeling of\nbeing in a miniature world. Our evaluations underpin a significantly increased\npresence and walking distance compared to the teleportation approach. Finally,\nwe derive a set of game design implications related to the integration of our\ntechnique.", "AI": {"tldr": "The paper explores a novel virtual navigation technique in VR games that enhances player presence and walking distance by promoting physical walking instead of teleportation.", "motivation": "To improve the feeling of presence in virtual reality games by addressing the limitations of room-scale motion and current navigation techniques like teleportation.", "method": "The study introduces a navigation metaphor that allows players to physically walk while the system proportionally increases the modeled eye distance, reducing cybersickness and enhancing the feeling of being in a miniature world.", "result": "The proposed navigation technique significantly increases player presence and walking distances compared to traditional teleportation methods.", "conclusion": "The findings suggest that integrating this new navigation technique can enhance the VR gaming experience and lead to important game design implications.", "key_contributions": ["Introduction of a navigation metaphor that emphasizes physical walking", "Significant increase in player presence and distance walked compared to teleportation", "Derivation of game design implications based on the findings"], "limitations": "", "keywords": ["virtual reality", "navigation", "game design", "cybersickness", "presence"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2508.11163", "pdf": "https://arxiv.org/pdf/2508.11163.pdf", "abs": "https://arxiv.org/abs/2508.11163", "title": "MobQA: A Benchmark Dataset for Semantic Understanding of Human Mobility Data through Question Answering", "authors": ["Hikaru Asano", "Hiroki Ouchi", "Akira Kasuga", "Ryo Yonetani"], "categories": ["cs.CL"], "comment": "23 pages, 12 figures", "summary": "This paper presents MobQA, a benchmark dataset designed to evaluate the\nsemantic understanding capabilities of large language models (LLMs) for human\nmobility data through natural language question answering.\n  While existing models excel at predicting human movement patterns, it remains\nunobvious how much they can interpret the underlying reasons or semantic\nmeaning of those patterns. MobQA provides a comprehensive evaluation framework\nfor LLMs to answer questions about diverse human GPS trajectories spanning\ndaily to weekly granularities. It comprises 5,800 high-quality question-answer\npairs across three complementary question types: factual retrieval (precise\ndata extraction), multiple-choice reasoning (semantic inference), and free-form\nexplanation (interpretive description), which all require spatial, temporal,\nand semantic reasoning. Our evaluation of major LLMs reveals strong performance\non factual retrieval but significant limitations in semantic reasoning and\nexplanation question answering, with trajectory length substantially impacting\nmodel effectiveness. These findings demonstrate the achievements and\nlimitations of state-of-the-art LLMs for semantic mobility\nunderstanding.\\footnote{MobQA dataset is available at\nhttps://github.com/CyberAgentAILab/mobqa.}", "AI": {"tldr": "The paper introduces MobQA, a benchmark dataset for evaluating LLMs on their semantic understanding of human mobility data through natural language questions.", "motivation": "To assess the semantic interpretation abilities of LLMs regarding human movement patterns, beyond just prediction capabilities.", "method": "The paper presents a dataset containing 5,800 question-answer pairs classified into factual retrieval, multiple-choice reasoning, and free-form explanation types, focusing on spatial, temporal, and semantic reasoning.", "result": "Results indicate that while LLMs excel in factual retrieval tasks, they struggle with semantic reasoning and explanations, particularly affected by trajectory length.", "conclusion": "The evaluation highlights both the competencies and limitations of leading LLMs in understanding the semantics of human mobility.", "key_contributions": ["Introduction of the MobQA benchmark dataset for mobility understanding", "Comprehensive evaluation of LLM performance in semantic reasoning", "Insights into the limitations of LLMs in interpreting human mobility data"], "limitations": "The focus of the study is limited to the performance of LLMs on existing datasets without exploring potential improvements or alternative approaches.", "keywords": ["MobQA", "large language models", "human mobility", "semantic reasoning", "natural language processing"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2508.11314", "pdf": "https://arxiv.org/pdf/2508.11314.pdf", "abs": "https://arxiv.org/abs/2508.11314", "title": "Outpace Reality: A Novel Augmented-Walking Technique for Virtual Reality Games", "authors": ["Sebastian Cmentowski", "Fabian Kievelitz", "Jens Krüger"], "categories": ["cs.HC"], "comment": "author version", "summary": "The size of most virtual environments exceeds the tracking space available\nfor physical walking. One solution to this disparity is to extend the available\nwalking range by augmenting users' actual movements. However, the resulting\nincrease in visual flow can easily cause cybersickness. Therefore, we present a\nnovel augmented-walking approach for virtual reality games. Our core concept is\na virtual tunnel that spans the entire travel distance when viewed from the\noutside. However, its interior is only a fraction as long, allowing users to\ncover the distance by real walking. Whereas the tunnel hides the visual flow\nfrom the applied movement acceleration, windows on the tunnel's walls still\nreveal the actual expedited motion. Our evaluation reveals that our approach\navoids cybersickness while enhancing physical activity and preserving presence.\nWe finish our paper with a discussion of the design considerations and\nlimitations of our proposed locomotion technique.", "AI": {"tldr": "This paper proposes a novel augmented-walking technique using a virtual tunnel to extend walking space in VR while preventing cybersickness.", "motivation": "The need to extend physical walking space in virtual environments to enhance user experience without causing cybersickness.", "method": "The proposed method employs a virtual tunnel that allows real walking within a shorter actual distance while hiding visual flow and providing windows that indicate movement speed.", "result": "The evaluation showed that this technique effectively prevents cybersickness, promotes physical activity, and maintains user presence in the virtual environment.", "conclusion": "The augmented-walking approach is a viable solution for increasing the physical walking range in virtual reality applications, addressing both user comfort and engagement.", "key_contributions": ["Novel virtual tunnel concept for VR locomotion", "Prevention of cybersickness while enhancing physical activity", "Maintenance of user presence during gameplay"], "limitations": "The approach may still require further testing across a wider range of VR environments and user profiles.", "keywords": ["augmented reality", "virtual reality", "cybersickness", "locomotion technique", "user experience"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.11166", "pdf": "https://arxiv.org/pdf/2508.11166.pdf", "abs": "https://arxiv.org/abs/2508.11166", "title": "Overcoming Low-Resource Barriers in Tulu: Neural Models and Corpus Creation for OffensiveLanguage Identification", "authors": ["Anusha M D", "Deepthi Vikram", "Bharathi Raja Chakravarthi", "Parameshwar R Hegde"], "categories": ["cs.CL"], "comment": "20 pages, 3 tables, 3 figures. Submitted to Language Resources and\n  Evaluation (Springer)", "summary": "Tulu, a low-resource Dravidian language predominantly spoken in southern\nIndia, has limited computational resources despite its growing digital\npresence. This study presents the first benchmark dataset for Offensive\nLanguage Identification (OLI) in code-mixed Tulu social media content,\ncollected from YouTube comments across various domains. The dataset, annotated\nwith high inter-annotator agreement (Krippendorff's alpha = 0.984), includes\n3,845 comments categorized into four classes: Not Offensive, Not Tulu,\nOffensive Untargeted, and Offensive Targeted. We evaluate a suite of deep\nlearning models, including GRU, LSTM, BiGRU, BiLSTM, CNN, and attention-based\nvariants, alongside transformer architectures (mBERT, XLM-RoBERTa). The BiGRU\nmodel with self-attention achieves the best performance with 82% accuracy and a\n0.81 macro F1-score. Transformer models underperform, highlighting the\nlimitations of multilingual pretraining in code-mixed, under-resourced\ncontexts. This work lays the foundation for further NLP research in Tulu and\nsimilar low-resource, code-mixed languages.", "AI": {"tldr": "This paper introduces the first benchmark dataset for Offensive Language Identification in code-mixed Tulu social media comments, evaluates various deep learning models, and highlights the importance of further NLP research in low-resource languages.", "motivation": "Develop a benchmark for Offensive Language Identification in Tulu, a low-resource Dravidian language, to address the lack of computational resources and support further research in under-resourced languages.", "method": "Collected a dataset of 3,845 comments from YouTube, annotated based on offensiveness; evaluated deep learning models including GRU, LSTM, CNN, and transformer architectures.", "result": "The BiGRU model with self-attention achieved 82% accuracy and a 0.81 macro F1-score, while transformer models performed poorly in this context.", "conclusion": "The study highlights the challenges multilingual pretraining poses for code-mixed, under-resourced languages and lays groundwork for future research.", "key_contributions": ["First benchmark dataset for Offensive Language Identification in Tulu", "High inter-annotator agreement in dataset annotation", "Evaluation of a range of deep learning models, identifying strengths and weaknesses in a specific context."], "limitations": "Findings may not generalize to other languages or fully capture the complexity of offensive language across different cultures.", "keywords": ["Offensive Language Identification", "Tulu", "deep learning", "code-mixed languages", "NLP"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2508.11327", "pdf": "https://arxiv.org/pdf/2508.11327.pdf", "abs": "https://arxiv.org/abs/2508.11327", "title": "The User-first Approach to AI Ethics: Preferences for Ethical Principles in AI Systems across Cultures and Contexts", "authors": ["Benjamin J. Carroll", "Jianlong Zhou", "Paul F. Burke", "Sabine Ammon"], "categories": ["cs.HC"], "comment": null, "summary": "As AI systems increasingly permeate everyday life, designers and developers\nface mounting pressure to balance innovation with ethical design choices. To\ndate, the operationalisation of AI ethics has predominantly depended on\nframeworks that prescribe which ethical principles should be embedded within AI\nsystems. However, the extent to which users value these principles remains\nlargely unexplored in the existing literature. In a discrete choice experiment\nconducted in four countries, we quantify user preferences for 11 ethical\nprinciples. Our findings indicate that, while users generally prioritise\nprivacy, justice & fairness, and transparency, their preferences exhibit\nsignificant variation based on culture and application context. Latent class\nanalysis further revealed four distinct user cohorts, the largest of which is\nethically disengaged and defers to regulatory oversight. Our findings offer (1)\nempirical evidence of uneven user prioritisation of AI ethics principles, (2)\nactionable guidance for operationalising ethics tailored to culture and\ncontext, (3) support for the development of robust regulatory mechanisms, and\n(4) a foundation for advancing a user-centred approach to AI ethics, motivated\nindependently from abstract moral theory.", "AI": {"tldr": "This paper explores user preferences for AI ethics principles across different cultural contexts, revealing significant variations and distinct user cohorts.", "motivation": "As AI systems increasingly integrate into daily life, understanding user values regarding ethical principles in AI design is essential.", "method": "A discrete choice experiment was conducted across four countries to quantify user preferences for 11 ethical principles related to AI.", "result": "Users prioritize privacy, justice & fairness, and transparency, but preferences vary based on culture and application context. Four distinct user cohorts were identified.", "conclusion": "The findings provide empirical evidence for varying user priorities among AI ethics principles and suggest a user-centered approach to operationalize AI ethics.", "key_contributions": ["Empirical evidence of user prioritization of AI ethics principles", "Navigation of ethics tailored to culture and context", "Support for developing regulatory mechanisms based on user preferences"], "limitations": "", "keywords": ["AI ethics", "user preferences", "cultural context", "discrete choice experiment", "user-centered design"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.11184", "pdf": "https://arxiv.org/pdf/2508.11184.pdf", "abs": "https://arxiv.org/abs/2508.11184", "title": "Personalized Distractor Generation via MCTS-Guided Reasoning Reconstruction", "authors": ["Tao Wu", "Jingyuan Chen", "Wang Lin", "Jian Zhan", "Mengze Li", "Kun Kuang", "Fei Wu"], "categories": ["cs.CL"], "comment": null, "summary": "Distractors, incorrect but plausible answer choices in multiple-choice\nquestions (MCQs), play a critical role in educational assessment by diagnosing\nstudent misconceptions. Recent work has leveraged large language models (LLMs)\nto generate shared, group-level distractors by learning common error patterns\nacross large student populations. However, such distractors often fail to\ncapture the diverse reasoning errors of individual students, limiting their\ndiagnostic effectiveness. To address this limitation, we introduce the task of\npersonalized distractor generation, which aims to generate tailored distractors\nbased on individual misconceptions inferred from each student's past\nquestion-answering (QA) records, ensuring every student receives options that\neffectively exposes their specific reasoning errors. While promising, this task\nis challenging because each student typically has only a few QA records, which\noften lack the student's underlying reasoning processes, making training-based\ngroup-level approaches infeasible. To overcome this, we propose a training-free\ntwo-stage framework. In the first stage, we construct a student-specific\nmisconception prototype by applying Monte Carlo Tree Search (MCTS) to recover\nthe student's reasoning trajectories from past incorrect answers. In the second\nstage, this prototype guides the simulation of the student's reasoning on new\nquestions, enabling the generation of personalized distractors that align with\nthe student's recurring misconceptions. Experiments show that our approach\nachieves the best performance in generating plausible, personalized distractors\nfor 140 students, and also effectively generalizes to group-level settings,\nhighlighting its robustness and adaptability.", "AI": {"tldr": "The paper introduces a personalized distractor generation framework using a two-stage approach to tailor multiple-choice question options based on individual student misconceptions derived from their past QA records.", "motivation": "To improve the diagnostic effectiveness of distractors in educational assessments by addressing the limitations of group-level distractor generation which often overlook individual reasoning errors.", "method": "A training-free two-stage framework is proposed where the first stage constructs a student-specific misconception prototype using Monte Carlo Tree Search (MCTS) to analyze past incorrect answers. The second stage utilizes this prototype to simulate the student's reasoning on new questions for generating personalized distractors.", "result": "Experiments validate that the proposed method achieves superior performance in generating plausible, personalized distractors and effectively generalizes to group-level distractors.", "conclusion": "The personalized distractor generation framework emphasizes robustness and adaptability, enhancing the educational assessment process by tailoring content to individual student needs.", "key_contributions": ["Introduction of personalized distractor generation for individual students.", "Two-stage framework leveraging MCTS for prototype construction and reasoning simulation.", "Demonstrated effectiveness through experimental results with 140 students."], "limitations": "The framework relies on past QA records, which may be limited for some students, affecting the richness of generated distractors.", "keywords": ["personalized learning", "distractor generation", "student misconceptions", "large language models", "Monte Carlo Tree Search"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2508.11335", "pdf": "https://arxiv.org/pdf/2508.11335.pdf", "abs": "https://arxiv.org/abs/2508.11335", "title": "Towards Smart Workplaces: Understanding Mood-Influencing Factors of the Physical Workspace in Collaborative Group Settings", "authors": ["Tzu-Hui Wu", "Sebastian Cmentowski", "Yunyin Lou", "Jun Hu", "Regina Bernhaupt"], "categories": ["cs.HC"], "comment": "preprint, submitted to INTERACT 2025", "summary": "Group mood plays a crucial role in shaping workspace experiences, influencing\ngroup dynamics, team performance, and creativity. The perceived group mood\ndepends on many, often subconscious, aspects such as individual emotional\nstates or group life, which make it challenging to maintain a positive\natmosphere. Intelligent technology could support mood regulation in physical\noffice environments, for example, as adaptive ambient lighting for mood\nregulation. However, little is known about the relationship between the\nphysical workspace and group mood dynamics. To address this knowledge gap, we\nconducted a qualitative user study (N=8 workgroups and overall 26 participants)\nto explore how the physical workspace shapes group mood experiences and\ninvestigate employees' perspectives on intelligent mood-aware technologies. Our\nfindings reveal key factors influencing group mood, and participants'\nexpectations for supportive technology to preserve privacy and autonomy. Our\nwork highlights the potential of adaptive and responsive workspaces while also\nemphasizing the need for human-centered, technology-driven interventions that\nbenefit group well-being.", "AI": {"tldr": "This study explores the influence of physical workspace on group mood dynamics and the potential for intelligent technologies to support mood regulation.", "motivation": "To fill the knowledge gap regarding how physical workspaces shape group mood experiences and the role of intelligent technologies in mood regulation.", "method": "A qualitative user study involving 8 workgroups with a total of 26 participants was conducted to investigate the interaction between workspace factors and group mood.", "result": "Key factors influencing group mood were identified, alongside participants' expectations for mood-aware technologies concerning privacy and autonomy.", "conclusion": "The findings suggest the importance of adaptive workspaces and the need for human-centered technologies to enhance group well-being.", "key_contributions": ["Identifies key factors impacting group mood in workspaces.", "Explores employee expectations for technological support in mood regulation.", "Suggests adaptive workspaces as potential interventions for improving group dynamics."], "limitations": "", "keywords": ["group mood", "workspace dynamics", "intelligent technologies"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.11189", "pdf": "https://arxiv.org/pdf/2508.11189.pdf", "abs": "https://arxiv.org/abs/2508.11189", "title": "Novel Parasitic Dual-Scale Modeling for Efficient and Accurate Multilingual Speech Translation", "authors": ["Chenyang Le", "Yinfeng Xia", "Huiyan Li", "Manhong Wang", "Yutao Sun", "Xingyang Ma", "Yanmin Qian"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Interspeech 2025", "summary": "Recent advancements in speech-to-text translation have led to the development\nof multilingual models capable of handling multiple language pairs\nsimultaneously. However, these unified models often suffer from large parameter\nsizes, making it challenging to balance inference efficiency and performance,\nparticularly in local deployment scenarios. We propose an innovative Parasitic\nDual-Scale Approach, which combines an enhanced speculative sampling method\nwith model compression and knowledge distillation techniques. Building on the\nWhisper Medium model, we enhance it for multilingual speech translation into\nwhisperM2M, and integrate our novel KVSPN module, achieving state-of-the-art\n(SOTA) performance across six popular languages with improved inference\nefficiency. KVSPN enables a 40\\% speedup with no BLEU score degradation.\nCombined with distillation methods, it represents a 2.6$\\times$ speedup over\nthe original Whisper Medium with superior performance.", "AI": {"tldr": "The paper proposes the Parasitic Dual-Scale Approach to improve multilingual speech-to-text translation efficiency by combining model compression and knowledge distillation, achieving significant performance enhancements on the Whisper Medium model.", "motivation": "To address the challenge of large parameter sizes in multilingual speech translation models while maintaining efficiency and performance, particularly for local deployments.", "method": "The Parasitic Dual-Scale Approach integrates enhanced speculative sampling with model compression and knowledge distillation techniques. It focuses on improving the Whisper Medium model to create whisperM2M and incorporates the KVSPN module.", "result": "The proposed method achieves state-of-the-art performance across six languages with a 40% increase in inference speed and a 2.6× overall speedup compared to the original Whisper Medium model, without degrading the BLEU score.", "conclusion": "The proposed approach offers a viable solution for efficient multilingual speech translation, balancing speed and performance effectively, making it suitable for local deployment scenarios.", "key_contributions": ["Introduction of the Parasitic Dual-Scale Approach.", "Development of the KVSPN module that enhances speed without performance loss.", "Achieving state-of-the-art performance in multilingual speech translation."], "limitations": "", "keywords": ["speech-to-text", "multilingual models", "knowledge distillation"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.11398", "pdf": "https://arxiv.org/pdf/2508.11398.pdf", "abs": "https://arxiv.org/abs/2508.11398", "title": "Trustworthy AI Psychotherapy: Multi-Agent LLM Workflow for Counseling and Explainable Mental Disorder Diagnosis", "authors": ["Mithat Can Ozgun", "Jiahuan Pei", "Koen Hindriks", "Lucia Donatelli", "Qingzhi Liu", "Xin Sun", "Junxiao Wang"], "categories": ["cs.HC", "cs.AI", "cs.IR"], "comment": "Accepted by CIKM 2025 as a full paper", "summary": "LLM-based agents have emerged as transformative tools capable of executing\ncomplex tasks through iterative planning and action, achieving significant\nadvancements in understanding and addressing user needs. Yet, their\neffectiveness remains limited in specialized domains such as mental health\ndiagnosis, where they underperform compared to general applications. Current\napproaches to integrating diagnostic capabilities into LLMs rely on scarce,\nhighly sensitive mental health datasets, which are challenging to acquire.\nThese methods also fail to emulate clinicians' proactive inquiry skills, lack\nmulti-turn conversational comprehension, and struggle to align outputs with\nexpert clinical reasoning. To address these gaps, we propose DSM5AgentFlow, the\nfirst LLM-based agent workflow designed to autonomously generate DSM-5 Level-1\ndiagnostic questionnaires. By simulating therapist-client dialogues with\nspecific client profiles, the framework delivers transparent, step-by-step\ndisorder predictions, producing explainable and trustworthy results. This\nworkflow serves as a complementary tool for mental health diagnosis, ensuring\nadherence to ethical and legal standards. Through comprehensive experiments, we\nevaluate leading LLMs across three critical dimensions: conversational realism,\ndiagnostic accuracy, and explainability. Our datasets and implementations are\nfully open-sourced.", "AI": {"tldr": "The paper presents DSM5AgentFlow, an LLM-based agent workflow that autonomously generates DSM-5 Level-1 diagnostic questionnaires to enhance mental health diagnosis through improved dialogue simulation and explainability.", "motivation": "LLM-based agents struggle in specialized domains like mental health diagnosis, which requires advanced capabilities for understanding user needs and generating accurate diagnostic results.", "method": "The proposed DSM5AgentFlow simulates therapist-client dialogues with detailed client profiles to produce diagnostic questionnaires that follow ethical and legal guidelines, and evaluates these agents on conversational realism, diagnostic accuracy, and explainability.", "result": "The framework successfully generates disorder predictions that are transparent and trustworthy, demonstrating improvements over previous approaches in generating accurate mental health diagnostics.", "conclusion": "DSM5AgentFlow could serve as a valuable tool in mental health diagnoses, enhancing the accuracy and transparency of assessments while remaining aligned with ethical standards in clinical practice.", "key_contributions": ["Introduces the first LLM agent workflow for DSM-5 diagnostic questionnaires", "Enhances accuracy in mental health diagnosis through therapist-client dialogue simulations", "Provides an open-sourced resource for further research and development"], "limitations": "The effectiveness of the framework is contingent on the quality of the underlying LLM and may still require clinician verification.", "keywords": ["LLM-based agents", "mental health diagnosis", "DSM-5", "explainability", "conversational AI"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2508.11197", "pdf": "https://arxiv.org/pdf/2508.11197.pdf", "abs": "https://arxiv.org/abs/2508.11197", "title": "E-CaTCH: Event-Centric Cross-Modal Attention with Temporal Consistency and Class-Imbalance Handling for Misinformation Detection", "authors": ["Ahmad Mousavi", "Yeganeh Abdollahinejad", "Roberto Corizzo", "Nathalie Japkowicz", "Zois Boukouvalas"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI"], "comment": null, "summary": "Detecting multimodal misinformation on social media remains challenging due\nto inconsistencies between modalities, changes in temporal patterns, and\nsubstantial class imbalance. Many existing methods treat posts independently\nand fail to capture the event-level structure that connects them across time\nand modality. We propose E-CaTCH, an interpretable and scalable framework for\nrobustly detecting misinformation. If needed, E-CaTCH clusters posts into\npseudo-events based on textual similarity and temporal proximity, then\nprocesses each event independently. Within each event, textual and visual\nfeatures are extracted using pre-trained BERT and ResNet encoders, refined via\nintra-modal self-attention, and aligned through bidirectional cross-modal\nattention. A soft gating mechanism fuses these representations to form\ncontextualized, content-aware embeddings of each post. To model temporal\nevolution, E-CaTCH segments events into overlapping time windows and uses a\ntrend-aware LSTM, enhanced with semantic shift and momentum signals, to encode\nnarrative progression over time. Classification is performed at the event\nlevel, enabling better alignment with real-world misinformation dynamics. To\naddress class imbalance and promote stable learning, the model integrates\nadaptive class weighting, temporal consistency regularization, and hard-example\nmining. The total loss is aggregated across all events. Extensive experiments\non Fakeddit, IND, and COVID-19 MISINFOGRAPH demonstrate that E-CaTCH\nconsistently outperforms state-of-the-art baselines. Cross-dataset evaluations\nfurther demonstrate its robustness, generalizability, and practical\napplicability across diverse misinformation scenarios.", "AI": {"tldr": "E-CaTCH is a framework for detecting multimodal misinformation on social media, addressing temporal patterns and class imbalances by clustering posts, extracting features, and employing a trend-aware LSTM for event-level classification.", "motivation": "The challenge of detecting multimodal misinformation due to inconsistencies, temporal dynamics, and class imbalances necessitates a robust approach that captures the event-level structure connecting posts.", "method": "E-CaTCH clusters posts into pseudo-events, extracts textual and visual features using BERT and ResNet, aligns features with cross-modal attention, and models temporal evolution with a trend-aware LSTM. It integrates adaptive class weighting and regularization techniques to improve learning and classification.", "result": "E-CaTCH outperforms state-of-the-art baselines in extensive experiments on multiple datasets, showcasing robustness and generalizability in diverse misinformation contexts.", "conclusion": "The framework effectively models the complexities in misinformation detection across modalities and time, offering practical applications in real-world scenarios.", "key_contributions": ["Introduction of event-level structure in misinformation detection", "Utilization of trend-aware LSTM for temporal modeling", "Integration of adaptive class weighting and regularization techniques."], "limitations": "", "keywords": ["multimodal misinformation", "social media", "event-level detection"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2508.11401", "pdf": "https://arxiv.org/pdf/2508.11401.pdf", "abs": "https://arxiv.org/abs/2508.11401", "title": "FACET:Teacher-Centred LLM-Based Multi-Agent Systems-Towards Personalized Educational Worksheets", "authors": ["Jana Gonnermann-Müller", "Jennifer Haase", "Konstantin Fackeldey", "Sebastian Pokutta"], "categories": ["cs.HC", "cs.MA"], "comment": null, "summary": "The increasing heterogeneity of student populations poses significant\nchallenges for teachers, particularly in mathematics education, where\ncognitive, motivational, and emotional differences strongly influence learning\noutcomes. While AI-driven personalization tools have emerged, most remain\nperformance-focused, offering limited support for teachers and neglecting\nbroader pedagogical needs. This paper presents the FACET framework, a\nteacher-facing, large language model (LLM)-based multi-agent system designed to\ngenerate individualized classroom materials that integrate both cognitive and\nmotivational dimensions of learner profiles. The framework comprises three\nspecialized agents: (1) learner agents that simulate diverse profiles\nincorporating topic proficiency and intrinsic motivation, (2) a teacher agent\nthat adapts instructional content according to didactical principles, and (3)\nan evaluator agent that provides automated quality assurance. We tested the\nsystem using authentic grade 8 mathematics curriculum content and evaluated its\nfeasibility through a) automated agent-based assessment of output quality and\nb) exploratory feedback from K-12 in-service teachers. Results from ten\ninternal evaluations highlighted high stability and alignment between generated\nmaterials and learner profiles, and teacher feedback particularly highlighted\nstructure and suitability of tasks. The findings demonstrate the potential of\nmulti-agent LLM architectures to provide scalable, context-aware\npersonalization in heterogeneous classroom settings, and outline directions for\nextending the framework to richer learner profiles and real-world classroom\ntrials.", "AI": {"tldr": "This paper presents the FACET framework, a teacher-facing LLM-based system that generates individualized classroom materials addressing cognitive and motivational dimensions of learners.", "motivation": "The paper addresses the challenges posed by heterogeneous student populations in mathematics education and the limitations of current AI-driven personalization tools.", "method": "The FACET framework consists of three specialized agents: learner agents that simulate diverse profiles, a teacher agent that adapts instructional content, and an evaluator agent that ensures output quality, tested with grade 8 mathematics curriculum.", "result": "Tests showed high stability and alignment between generated materials and learner profiles, with positive feedback from K-12 teachers regarding the structure and suitability of tasks.", "conclusion": "The findings demonstrate the potential for multi-agent LLM architectures to enhance scalable, context-aware personalization in classrooms and suggest future directions for development.", "key_contributions": ["Introduction of the FACET framework for personalized education", "Utilization of multi-agent systems to cater to learner diversity", "Demonstration of system feasibility through teacher feedback and automated assessments."], "limitations": "", "keywords": ["Personalization", "Large Language Models", "Education", "Multi-Agent Systems", "Mathematics Education"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.11247", "pdf": "https://arxiv.org/pdf/2508.11247.pdf", "abs": "https://arxiv.org/abs/2508.11247", "title": "Cross-Granularity Hypergraph Retrieval-Augmented Generation for Multi-hop Question Answering", "authors": ["Changjian Wang", "Weihong Deng", "Weili Guan", "Quan Lu", "Ning Jiang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multi-hop question answering (MHQA) requires integrating knowledge scattered\nacross multiple passages to derive the correct answer. Traditional\nretrieval-augmented generation (RAG) methods primarily focus on coarse-grained\ntextual semantic similarity and ignore structural associations among dispersed\nknowledge, which limits their effectiveness in MHQA tasks. GraphRAG methods\naddress this by leveraging knowledge graphs (KGs) to capture structural\nassociations, but they tend to overly rely on structural information and\nfine-grained word- or phrase-level retrieval, resulting in an underutilization\nof textual semantics. In this paper, we propose a novel RAG approach called\nHGRAG for MHQA that achieves cross-granularity integration of structural and\nsemantic information via hypergraphs. Structurally, we construct an entity\nhypergraph where fine-grained entities serve as nodes and coarse-grained\npassages as hyperedges, and establish knowledge association through shared\nentities. Semantically, we design a hypergraph retrieval method that integrates\nfine-grained entity similarity and coarse-grained passage similarity via\nhypergraph diffusion. Finally, we employ a retrieval enhancement module, which\nfurther refines the retrieved results both semantically and structurally, to\nobtain the most relevant passages as context for answer generation with the\nLLM. Experimental results on benchmark datasets demonstrate that our approach\noutperforms state-of-the-art methods in QA performance, and achieves a\n6$\\times$ speedup in retrieval efficiency.", "AI": {"tldr": "The paper presents HGRAG, a novel approach for multi-hop question answering that combines structural and semantic information using hypergraphs to improve answer generation and retrieval efficiency.", "motivation": "To improve the effectiveness of multi-hop question answering by integrating structural and semantic knowledge rather than relying solely on one or the other.", "method": "The paper proposes a hypergraph-based approach where entities and passages are structured into a hypergraph to facilitate cross-granularity integration of structural and semantic information through hypergraph diffusion and a retrieval enhancement module.", "result": "HGRAG outperforms state-of-the-art methods in question answering performance and achieves a 6x increase in retrieval efficiency on benchmark datasets.", "conclusion": "The proposed HGRAG method effectively integrates structural and semantic knowledge, significantly improving multi-hop question answering tasks.", "key_contributions": ["Introduction of HGRAG for integrating structural and semantic data in MHQA", "Utilization of hypergraphs for relationship establishment between fine-grained entities and coarse-grained passages", "Demonstration of significant improvements in QA performance and retrieval efficiency."], "limitations": "", "keywords": ["multi-hop question answering", "retrieval-augmented generation", "hypergraphs", "semantic integration", "knowledge graphs"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.11412", "pdf": "https://arxiv.org/pdf/2508.11412.pdf", "abs": "https://arxiv.org/abs/2508.11412", "title": "Towards Embodied Conversational Agents for Reducing Oral Exam Anxiety in Extended Reality", "authors": ["Jens Grubert", "Yvonne Sedelmaier", "Dieter Landes"], "categories": ["cs.HC"], "comment": "Accepted to the IEEE ISMAR-Adjunct Proceedings 2025", "summary": "Oral examinations are a prevalent but psychologically demanding form of\nassessment in higher education. Many students experience intense anxiety, which\ncan impair cognitive performance and hinder academic success. This position\npaper explores the potential of embodied conversational agents (ECAs) in\nextended reality (XR) environments to support students preparing for oral\nexams. We propose a system concept that integrates photorealistic ECAs with\nreal-time capable large language models (LLMs) to enable psychologically safe,\nadaptive, and repeatable rehearsal of oral examination scenarios. We also\ndiscuss the potential benefits and challenges of such an envisioned system.", "AI": {"tldr": "This position paper discusses the use of embodied conversational agents in XR environments to help students prepare for oral examinations by providing a psychologically safe and adaptive rehearsal system.", "motivation": "To address the intense anxiety experienced by students during oral exams, which can impair performance and academic success.", "method": "The proposed system integrates photorealistic embodied conversational agents with real-time capable large language models to facilitate rehearsal of oral examination scenarios.", "result": "The envisioned system supports a repeatable and adaptive practice experience for students, potentially alleviating anxiety associated with oral assessments.", "conclusion": "While the integration of ECAs and LLMs in XR offers promising support for students, the paper outlines both potential benefits and challenges of implementing such systems.", "key_contributions": ["Integration of ECAs with LLMs for oral exam preparation", "Use of XR to create a psychologically safe rehearsal environment", "Discussion of benefits and challenges of the proposed system"], "limitations": "", "keywords": ["Embodied Conversational Agents", "Oral Examinations", "Extended Reality", "Anxiety", "Large Language Models"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.11260", "pdf": "https://arxiv.org/pdf/2508.11260.pdf", "abs": "https://arxiv.org/abs/2508.11260", "title": "UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?", "authors": ["Mukund Choudhary", "KV Aditya Srivatsa", "Gaurja Aeron", "Antara Raaghavi Bhattacharya", "Dang Khoa Dang Dinh", "Ikhlasul Akmal Hanif", "Daria Kotova", "Ekaterina Kochmar", "Monojit Choudhury"], "categories": ["cs.CL"], "comment": "Accepted to COLM 2025", "summary": "Large language models (LLMs) have demonstrated potential in reasoning tasks,\nbut their performance on linguistics puzzles remains consistently poor. These\npuzzles, often derived from Linguistics Olympiad (LO) contests, provide a\nminimal contamination environment to assess LLMs' linguistic reasoning\nabilities across low-resource languages. This work analyses LLMs' performance\non 629 problems across 41 low-resource languages by labelling each with\nlinguistically informed features to unveil weaknesses. Our analyses show that\nLLMs struggle with puzzles involving higher morphological complexity and\nperform better on puzzles involving linguistic features that are also found in\nEnglish. We also show that splitting words into morphemes as a pre-processing\nstep improves solvability, indicating a need for more informed and\nlanguage-specific tokenisers. These findings thus offer insights into some\nchallenges in linguistic reasoning and modelling of low-resource languages.", "AI": {"tldr": "This paper investigates large language models' performance on linguistic puzzles in low-resource languages, revealing weaknesses in reasoning due to morphological complexity.", "motivation": "To assess the linguistic reasoning abilities of large language models (LLMs) through puzzles from Linguistics Olympiad contests, focusing on low-resource languages.", "method": "Analyzed LLM performance on 629 linguistic puzzles across 41 languages, labeling puzzles with linguistically informed features.", "result": "LLMs performed poorly on puzzles with higher morphological complexity but better on those with linguistic features similar to English; word splitting improved results.", "conclusion": "The study highlights challenges in linguistic reasoning and the need for more informed tokenization approaches for low-resource languages.", "key_contributions": ["Analysis of LLMs on linguistic puzzles for low-resource languages", "Identification of performance discrepancies based on morphological complexity", "Proposed improvements in tokenization for better LLM performance in low-resource contexts"], "limitations": "Focus on a specific set of puzzles; results may not generalize to all linguistic reasoning tasks.", "keywords": ["large language models", "linguistic reasoning", "low-resource languages"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.11426", "pdf": "https://arxiv.org/pdf/2508.11426.pdf", "abs": "https://arxiv.org/abs/2508.11426", "title": "ReachVox: Clutter-free Reachability Visualization for Robot Motion Planning in Virtual Reality", "authors": ["Steffen Hauck", "Diar Abdlkarim", "John Dudley", "Per Ola Kristensson", "Eyal Ofek", "Jens Grubert"], "categories": ["cs.HC", "cs.RO"], "comment": "To appear in Proceedings of IEEE ISMAR 2025", "summary": "Human-Robot-Collaboration can enhance workflows by leveraging the mutual\nstrengths of human operators and robots. Planning and understanding robot\nmovements remain major challenges in this domain. This problem is prevalent in\ndynamic environments that might need constant robot motion path adaptation. In\nthis paper, we investigate whether a minimalistic encoding of the reachability\nof a point near an object of interest, which we call ReachVox, can aid the\ncollaboration between a remote operator and a robotic arm in VR. Through a user\nstudy (n=20), we indicate the strength of the visualization relative to a\npoint-based reachability check-up.", "AI": {"tldr": "This paper explores how ReachVox, a minimalistic encoding of reachability, can improve human-robot collaboration in VR environments.", "motivation": "To enhance workflows in human-robot collaboration by addressing planning and understanding of robot movements in dynamic environments.", "method": "A user study with 20 participants was conducted to evaluate the effectiveness of ReachVox in aiding collaboration between a remote operator and a robotic arm.", "result": "The findings suggest that ReachVox's visualization strengthens the collaboration compared to traditional point-based reachability checks.", "conclusion": "ReachVox can be a valuable tool in improving human-robot interaction, particularly in VR settings.", "key_contributions": ["Introduction of ReachVox for encoding reachability", "User study demonstrating effectiveness", "Insights into enhancing human-robot collaboration in VR"], "limitations": "Study conducted with a small sample size (n=20) and specific context may limit generalizability.", "keywords": ["Human-Robot Collaboration", "Reachability", "Virtual Reality", "User Study", "Automation"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2508.11280", "pdf": "https://arxiv.org/pdf/2508.11280.pdf", "abs": "https://arxiv.org/abs/2508.11280", "title": "LETToT: Label-Free Evaluation of Large Language Models On Tourism Using Expert Tree-of-Thought", "authors": ["Ruiyan Qi", "Congding Wen", "Weibo Zhou", "Shangsong Liang", "Lingbo Li"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Evaluating large language models (LLMs) in specific domain like tourism\nremains challenging due to the prohibitive cost of annotated benchmarks and\npersistent issues like hallucinations. We propose $\\textbf{L}$able-Free\n$\\textbf{E}$valuation of LLM on $\\textbf{T}$ourism using Expert\n$\\textbf{T}$ree-$\\textbf{o}$f-$\\textbf{T}$hought (LETToT), a framework that\nleverages expert-derived reasoning structures-instead of labeled data-to access\nLLMs in tourism. First, we iteratively refine and validate hierarchical ToT\ncomponents through alignment with generic quality dimensions and expert\nfeedback. Results demonstrate the effectiveness of our systematically optimized\nexpert ToT with 4.99-14.15\\% relative quality gains over baselines. Second, we\napply LETToT's optimized expert ToT to evaluate models of varying scales\n(32B-671B parameters), revealing: (1) Scaling laws persist in specialized\ndomains (DeepSeek-V3 leads), yet reasoning-enhanced smaller models (e.g.,\nDeepSeek-R1-Distill-Llama-70B) close this gap; (2) For sub-72B models, explicit\nreasoning architectures outperform counterparts in accuracy and conciseness\n($p<0.05$). Our work established a scalable, label-free paradigm for\ndomain-specific LLM evaluation, offering a robust alternative to conventional\nannotated benchmarks.", "AI": {"tldr": "The paper introduces a framework called LETToT for evaluating LLMs in the tourism domain without labeled data, showing improved performance through expert-derived reasoning structures.", "motivation": "Evaluating LLMs in specific domains is challenging due to high costs of annotated benchmarks and issues like hallucinations.", "method": "The LETToT framework uses expert-derived reasoning structures to iteratively refine and validate evaluation components aligned with quality dimensions, avoiding reliance on labeled data.", "result": "The framework achieved 4.99-14.15% relative quality gains over baseline methods, demonstrating the effectiveness of reasoning-enhanced models in evaluating LLMs.", "conclusion": "LETToT presents a scalable, label-free evaluation paradigm for LLMs in specialized domains, offering a solid alternative to traditional benchmarks.", "key_contributions": ["Introduction of LETToT framework for LLM evaluation without labeled data", "Demonstrated effectiveness of reasoning-enhanced smaller models", "Established scaling laws and performance metrics for LLMs in tourism"], "limitations": "", "keywords": ["Large Language Models", "Tourism", "Evaluation Framework", "Label-Free Methodology", "Expert Feedback"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.11544", "pdf": "https://arxiv.org/pdf/2508.11544.pdf", "abs": "https://arxiv.org/abs/2508.11544", "title": "Grand Challenge: Mediating Between Confirmatory and Exploratory Research Cultures in Health Sciences and Visual Analytics", "authors": ["Viktor von Wyl", "Jürgen Bernard"], "categories": ["cs.HC"], "comment": "2+1 pages, 1 figure, position paper for the Visual Analytics in\n  Healthcare Workshop at IEEE VIS, Vienna, 2025", "summary": "Collaboration between health science and visual analytics research is often\nhindered by different, sometimes incompatible approaches to research design.\nHealth science often follows hypothesis-driven protocols, registered in\nadvance, and focuses on reproducibility and risk mitigation. Visual analytics,\nin contrast, relies on iterative data exploration, prioritizing insight\ngeneration and analytic refinement through user interaction. These differences\ncreate challenges in interdisciplinary projects, including misaligned\nterminology, unrealistic expectations about data readiness, divergent\nvalidation norms, or conflicting explainability requirements. To address these\npersistent tensions, we identify seven research needs and actions: (1)\nguidelines for broader community adoption, (2) agreement on quality and\nvalidation benchmarks, (3) frameworks for aligning research tasks, (4)\nintegrated workflows combining confirmatory and exploratory stages, (5) tools\nfor harmonizing terminology across disciplines, (6) dedicated bridging roles\nfor transdisciplinary work, and (7) cultural adaptation and mutual recognition.\nWe organize these needs in a framework with three areas: culture, standards,\nand processes. They can constitute a research agenda for developing reliable,\nreproducible, and clinically relevant data-centric methods.", "AI": {"tldr": "This paper addresses the collaboration challenges between health science and visual analytics, proposing a framework to harmonize their approaches.", "motivation": "The collaboration between health science and visual analytics is often hindered by differing research designs and expectations, which can impact interdisciplinary projects.", "method": "The authors identify seven key research needs and actions to enhance collaboration, organizing them into a framework focused on culture, standards, and processes.", "result": "The proposed framework aims to foster reliable, reproducible, and clinically relevant methods by addressing misalignment between the fields through detailed guidelines and tools.", "conclusion": "By implementing the suggested frameworks and actions, interdisciplinary projects can improve their research quality and effectiveness, ultimately leading to better health outcomes.", "key_contributions": ["Identification of seven key research needs for collaboration", "Framework categorizing needs into culture, standards, and processes", "Actionable guidelines to bridge gaps between health science and visual analytics"], "limitations": "", "keywords": ["visual analytics", "health science", "interdisciplinary collaboration", "data-centric methods", "framework"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2508.11281", "pdf": "https://arxiv.org/pdf/2508.11281.pdf", "abs": "https://arxiv.org/abs/2508.11281", "title": "ToxiFrench: Benchmarking and Enhancing Language Models via CoT Fine-Tuning for French Toxicity Detection", "authors": ["Axel Delaval", "Shujian Yang", "Haicheng Wang", "Han Qiu", "Jialiang Lu"], "categories": ["cs.CL", "cs.AI", "cs.CY", "68T50", "I.2.7"], "comment": "14 pages, 5 figures, 8 tables. This paper introduces TOXIFRENCH, a\n  new large-scale benchmark for French toxicity detection, and proposes a\n  Chain-of-Thought (CoT) fine-tuning method with a dynamic weighted loss. The\n  resulting fine-tuned 4B parameter model, ToxiFrench, achieves\n  state-of-the-art performance, outperforming larger models like GPT-4o", "summary": "Detecting toxic content using language models is crucial yet challenging.\nWhile substantial progress has been made in English, toxicity detection in\nFrench remains underdeveloped, primarily due to the lack of culturally\nrelevant, large-scale datasets. In this work, we introduce TOXIFRENCH, a new\npublic benchmark of 53,622 French online comments, constructed via a\nsemi-automated annotation pipeline that reduces manual labeling to only 10%\nthrough high-confidence LLM-based pre-annotation and human verification. Then,\nwe benchmark a broad range of models and uncover a counterintuitive insight:\nSmall Language Models (SLMs) outperform many larger models in robustness and\ngeneralization under the toxicity detection task. Motivated by this finding, we\npropose a novel Chain-of-Thought (CoT) fine-tuning strategy using a dynamic\nweighted loss that progressively emphasizes the model's final decision,\nsignificantly improving faithfulness. Our fine-tuned 4B model achieves\nstate-of-the-art performance, improving its F1 score by 13% over its baseline\nand outperforming LLMs such as GPT-40 and Gemini-2.5. Further evaluation on a\ncross-lingual toxicity benchmark demonstrates strong multilingual ability,\nsuggesting that our methodology can be effectively extended to other languages\nand safety-critical classification tasks.", "AI": {"tldr": "This paper introduces TOXIFRENCH, a benchmark for toxicity detection in French, and presents a Chain-of-Thought fine-tuning method that improves model performance.", "motivation": "The need for effective toxicity detection in French due to the lack of appropriate datasets and previous underperformance in this language.", "method": "A semi-automated annotation pipeline was developed to create a dataset of 53,622 French online comments, along with benchmarking various models for toxicity detection and applying a Chain-of-Thought fine-tuning method.", "result": "The fine-tuned 4B model, ToxiFrench, achieved state-of-the-art performance with a 13% improvement in F1 score over the baseline and outperformed larger models like GPT-40 and Gemini-2.5.", "conclusion": "The findings indicate that SLMs can be more robust than larger models for toxicity detection; the approach can be generalized to other languages and safety-critical tasks.", "key_contributions": ["Introduction of TOXIFRENCH, a novel benchmark for French toxicity detection.", "Development of a dynamic weighted loss fine-tuning strategy.", "Demonstrated strong multilingual capability in toxicity detection."], "limitations": "The study focuses primarily on French and may need further validation across diverse dialects and cultures.", "keywords": ["toxicity detection", "language models", "French language", "Chain-of-Thought", "benchmark"], "importance_score": 8, "read_time_minutes": 14}}
{"id": "2508.11613", "pdf": "https://arxiv.org/pdf/2508.11613.pdf", "abs": "https://arxiv.org/abs/2508.11613", "title": "Adaptive Cardio Load Targets for Improving Fitness and Performance", "authors": ["Justin Phillips", "Daniel Roggen", "Cathy Speed", "Robert Harle"], "categories": ["cs.HC"], "comment": null, "summary": "Cardio Load, introduced by Google in 2024, is a measure of cardiovascular\nwork (also known as training load) resulting from all the user's activities\nacross the day. It is based on heart rate reserve and captures both activity\nintensity and duration. Thanks to feedback from users and internal research, we\nintroduce adaptive and personalized targets which will be set weekly. This\nfeature will be available in the Public Preview of the Fitbit app after\nSeptember 2025. This white paper provides a comprehensive overview of Cardio\nLoad (CL) and how weekly CL targets are established, with examples shown to\nillustrate the effect of varying CL on the weekly target. We compare Cardio\nLoad and Active Zone Minutes (AZMs), highlighting their distinct purposes, i.e.\nAZMs for health guidelines and CL for performance measurement. We highlight\nthat CL is accumulated both during active workouts and incidental daily\nactivities, so users are able top-up their CL score with small bouts of\nactivity across the day.", "AI": {"tldr": "Overview of Cardio Load, a new metric for measuring cardiovascular work developed by Google, focusing on personalized weekly targets and distinguishing it from Active Zone Minutes.", "motivation": "To provide a comprehensive understanding of how Cardio Load (CL) measures cardiovascular work and how personalized weekly targets can optimize user performance.", "method": "The paper describes the methodology behind Cardio Load, which is based on heart rate reserve, and how it accommodates both intensity and duration of activities throughout the day.", "result": "The introduction of adaptive and personalized weekly targets for Cardio Load, along with a breakdown of how varying activity levels impact these targets, is presented along with comparative analysis against Active Zone Minutes.", "conclusion": "Cardio Load offers a flexible way to accumulate cardiovascular work from both workouts and everyday activities, thereby enhancing users' ability to improve their fitness levels over time.", "key_contributions": ["Introduction of Cardio Load as a measure of cardiovascular work", "Adaptive and personalized targets for weekly Cardio Load", "Comparison between Cardio Load and Active Zone Minutes showing distinct usage"], "limitations": "", "keywords": ["Cardio Load", "heart rate", "fitness tracking", "personalized targets", "active lifestyle"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2508.11285", "pdf": "https://arxiv.org/pdf/2508.11285.pdf", "abs": "https://arxiv.org/abs/2508.11285", "title": "AI in Mental Health: Emotional and Sentiment Analysis of Large Language Models' Responses to Depression, Anxiety, and Stress Queries", "authors": ["Arya VarastehNezhad", "Reza Tavasoli", "Soroush Elyasi", "MohammadHossein LotfiNia", "Hamed Farbeh"], "categories": ["cs.CL"], "comment": null, "summary": "Depression, anxiety, and stress are widespread mental health concerns that\nincreasingly drive individuals to seek information from Large Language Models\n(LLMs). This study investigates how eight LLMs (Claude Sonnet, Copilot, Gemini\nPro, GPT-4o, GPT-4o mini, Llama, Mixtral, and Perplexity) reply to twenty\npragmatic questions about depression, anxiety, and stress when those questions\nare framed for six user profiles (baseline, woman, man, young, old, and\nuniversity student). The models generated 2,880 answers, which we scored for\nsentiment and emotions using state-of-the-art tools. Our analysis revealed that\noptimism, fear, and sadness dominated the emotional landscape across all\noutputs, with neutral sentiment maintaining consistently high values.\nGratitude, joy, and trust appeared at moderate levels, while emotions such as\nanger, disgust, and love were rarely expressed. The choice of LLM significantly\ninfluenced emotional expression patterns. Mixtral exhibited the highest levels\nof negative emotions including disapproval, annoyance, and sadness, while Llama\ndemonstrated the most optimistic and joyful responses. The type of mental\nhealth condition dramatically shaped emotional responses: anxiety prompts\nelicited extraordinarily high fear scores (0.974), depression prompts generated\nelevated sadness (0.686) and the highest negative sentiment, while\nstress-related queries produced the most optimistic responses (0.755) with\nelevated joy and trust. In contrast, demographic framing of queries produced\nonly marginal variations in emotional tone. Statistical analyses confirmed\nsignificant model-specific and condition-specific differences, while\ndemographic influences remained minimal. These findings highlight the critical\nimportance of model selection in mental health applications, as each LLM\nexhibits a distinct emotional signature that could significantly impact user\nexperience and outcomes.", "AI": {"tldr": "This study examines how eight LLMs respond to questions about depression, anxiety, and stress across different user profiles, finding that model choice notably affects emotional expression.", "motivation": "To investigate how various LLMs respond to mental health inquiries and the impact of user demographics on these responses.", "method": "The study evaluated responses from eight LLMs to twenty questions about mental health, scoring 2,880 answers for sentiment and emotional content.", "result": "Responses showed dominance of optimism, fear, and sadness, with Mixtral exhibiting the highest negative emotions and Llama displaying the most optimism.", "conclusion": "Model selection is crucial in mental health contexts, as LLMs display unique emotional signatures influencing user experience and outcomes.", "key_contributions": ["Analysis of multiple LLMs on mental health queries", "Insights into emotional expressions of LLMs based on mental health conditions", "Demonstration of minimal effect of demographic framing on responses"], "limitations": "", "keywords": ["Large Language Models", "mental health", "emotional expression", "sentiment analysis", "user profiles"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.11620", "pdf": "https://arxiv.org/pdf/2508.11620.pdf", "abs": "https://arxiv.org/abs/2508.11620", "title": "Grab-n-Go: On-the-Go Microgesture Recognition with Objects in Hand", "authors": ["Chi-Jung Lee", "Jiaxin Li", "Tianhong Catherine Yu", "Ruidong Zhang", "Vipin Gunda", "François Guimbretière", "Cheng Zhang"], "categories": ["cs.HC"], "comment": null, "summary": "As computing devices become increasingly integrated into daily life, there is\na growing need for intuitive, always-available interaction methods, even when\nusers' hands are occupied. In this paper, we introduce Grab-n-Go, the first\nwearable device that leverages active acoustic sensing to recognize subtle hand\nmicrogestures while holding various objects. Unlike prior systems that focus\nsolely on free-hand gestures or basic hand-object activity recognition,\nGrab-n-Go simultaneously captures information about hand microgestures,\ngrasping poses, and object geometries using a single wristband, enabling the\nrecognition of fine-grained hand movements occurring within activities\ninvolving occupied hands. A deep learning framework processes these complex\nsignals to identify 30 distinct microgestures, with 6 microgestures for each of\nthe 5 grasping poses. In a user study with 10 participants and 25 everyday\nobjects, Grab-n-Go achieved an average recognition accuracy of 92.0%. A\nfollow-up study further validated Grab-n-Go's robustness against 10 more\nchallenging, deformable objects. These results underscore the potential of\nGrab-n-Go to provide seamless, unobtrusive interactions without requiring\nmodifications to existing objects. The complete dataset, comprising data from\n18 participants performing 30 microgestures with 35 distinct objects, is\npublicly available at https://github.com/cjlisalee/Grab-n-Go_Data with the DOI:\nhttps://doi.org/10.7298/7kbd-vv75.", "AI": {"tldr": "Grab-n-Go is a wearable device that uses active acoustic sensing to recognize hand microgestures while users hold objects, achieving high accuracy in gesture recognition.", "motivation": "There is a need for intuitive interaction methods for users with occupied hands, as more computing devices integrate into daily life.", "method": "The study introduced a wristband that captures hand microgestures, grasping poses, and object geometries, and employed a deep learning framework to analyze the signals and identify 30 microgestures from users holding various objects.", "result": "In user studies, Grab-n-Go achieved an average recognition accuracy of 92.0% across different everyday objects and validated its performance with challenging deformable objects.", "conclusion": "Grab-n-Go demonstrates the ability to provide seamless interactions without requiring users to change their object usage habits.", "key_contributions": ["Development of the first wearable device to recognize hand microgestures while hands are occupied", "Achieved 92.0% average accuracy in gesture recognition", "Publicly available dataset to support further research."], "limitations": "", "keywords": ["wearable device", "gesture recognition", "human-computer interaction", "deep learning", "acoustic sensing"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2508.11290", "pdf": "https://arxiv.org/pdf/2508.11290.pdf", "abs": "https://arxiv.org/abs/2508.11290", "title": "SafeConstellations: Steering LLM Safety to Reduce Over-Refusals Through Task-Specific Trajectory", "authors": ["Utsav Maskey", "Sumit Yadav", "Mark Dras", "Usman Naseem"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "LLMs increasingly exhibit over-refusal behavior, where safety mechanisms\ncause models to reject benign instructions that superficially resemble harmful\ncontent. This phenomena diminishes utility in production applications that\nrepeatedly rely on common prompt templates or applications that frequently rely\non LLMs for specific tasks (e.g. sentiment analysis, language translation).\nThrough comprehensive evaluation, we demonstrate that LLMs still tend to refuse\nresponses to harmful instructions when those instructions are reframed to\nappear as benign tasks. Our mechanistic analysis reveal that LLMs follow\ndistinct \"constellation\" patterns in embedding space as representations\ntraverse layers, with each task maintaining consistent trajectories that shift\npredictably between refusal and non-refusal cases. We introduce\nSafeConstellations, an inference-time trajectory-shifting approach that tracks\ntask-specific trajectory patterns and guides representations toward non-refusal\npathways. By selectively guiding model behavior only on tasks prone to\nover-refusal, and by preserving general model behavior, our method reduces\nover-refusal rates by up to 73% with minimal impact on utility-offering a\nprincipled approach to mitigating over-refusals.", "AI": {"tldr": "The paper addresses LLMs' over-refusal behavior, proposing SafeConstellations, a method that significantly reduces refusal rates in specific tasks without compromising overall utility.", "motivation": "Over-refusal behavior in LLMs reduces their utility for benign tasks that resemble harmful instructions, affecting applications like sentiment analysis and translation.", "method": "The authors conduct a comprehensive evaluation of LLM behavior and introduce SafeConstellations, a method that shifts embeddings in the model's trajectory to minimize refusal rates on benign tasks.", "result": "The proposed method reduces over-refusal rates by up to 73% while maintaining the overall utility of the LLM.", "conclusion": "SafeConstellations offers a principled approach to manage over-refusal in LLMs, enhancing their usability in critical applications.", "key_contributions": ["Introduction of SafeConstellations to address over-refusal behavior in LLMs", "Empirical analysis of LLM refusal patterns", "Significant reduction of refusal rates without loss of utility"], "limitations": "", "keywords": ["LLMs", "over-refusal", "SafeConstellations", "machine learning", "HCI"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2508.11310", "pdf": "https://arxiv.org/pdf/2508.11310.pdf", "abs": "https://arxiv.org/abs/2508.11310", "title": "SGSimEval: A Comprehensive Multifaceted and Similarity-Enhanced Benchmark for Automatic Survey Generation Systems", "authors": ["Beichen Guo", "Zhiyuan Wen", "Yu Yang", "Peng Gao", "Ruosong Yang", "Jiaxing Shen"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Accepted to The 21st International Conference on Advanced Data Mining\n  and Applications (ADMA2025)", "summary": "The growing interest in automatic survey generation (ASG), a task that\ntraditionally required considerable time and effort, has been spurred by recent\nadvances in large language models (LLMs). With advancements in\nretrieval-augmented generation (RAG) and the rising popularity of multi-agent\nsystems (MASs), synthesizing academic surveys using LLMs has become a viable\napproach, thereby elevating the need for robust evaluation methods in this\ndomain. However, existing evaluation methods suffer from several limitations,\nincluding biased metrics, a lack of human preference, and an over-reliance on\nLLMs-as-judges. To address these challenges, we propose SGSimEval, a\ncomprehensive benchmark for Survey Generation with Similarity-Enhanced\nEvaluation that evaluates automatic survey generation systems by integrating\nassessments of the outline, content, and references, and also combines\nLLM-based scoring with quantitative metrics to provide a multifaceted\nevaluation framework. In SGSimEval, we also introduce human preference metrics\nthat emphasize both inherent quality and similarity to humans. Extensive\nexperiments reveal that current ASG systems demonstrate human-comparable\nsuperiority in outline generation, while showing significant room for\nimprovement in content and reference generation, and our evaluation metrics\nmaintain strong consistency with human assessments.", "AI": {"tldr": "This paper introduces SGSimEval, a benchmark for evaluating automatic survey generation systems using a multifaceted evaluation framework that combines LLM-based scoring and human preference metrics.", "motivation": "The motivation for this research is to address the limitations of existing evaluation methods for automatic survey generation (ASG) systems, particularly their biases and over-reliance on LLMs for judgment.", "method": "The authors propose SGSimEval, which evaluates ASG systems through assessments of outline, content, and references, integrating both LLM-based scoring and quantitative metrics with human preference evaluations.", "result": "Extensive experiments show that current ASG systems excel in outline generation compared to humans, but they need improvement in content and reference generation.", "conclusion": "The new evaluation framework provides a robust approach to assessing ASG systems while aligning closely with human judgments.", "key_contributions": ["Introduction of SGSimEval as a new benchmark for evaluating ASG systems.", "Integration of human preference metrics in the evaluation process.", "Demonstration of current ASG systems' strengths and weaknesses in generating survey content."], "limitations": "", "keywords": ["automatic survey generation", "large language models", "evaluation methods"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.11318", "pdf": "https://arxiv.org/pdf/2508.11318.pdf", "abs": "https://arxiv.org/abs/2508.11318", "title": "LLM Compression: How Far Can We Go in Balancing Size and Performance?", "authors": ["Sahil Sk", "Debasish Dhal", "Sonal Khosla", "Sk Shahid", "Sambit Shekhar", "Akash Dhaka", "Shantipriya Parida", "Dilip K. Prasad", "Ondřej Bojar"], "categories": ["cs.CL"], "comment": "This paper has been accepted for presentation at the RANLP 2025\n  conference", "summary": "Quantization is an essential and popular technique for improving the\naccessibility of large language models (LLMs) by reducing memory usage and\ncomputational costs while maintaining performance. In this study, we apply\n4-bit Group Scaling Quantization (GSQ) and Generative Pretrained Transformer\nQuantization (GPTQ) to LLaMA 1B, Qwen 0.5B, and PHI 1.5B, evaluating their\nimpact across multiple NLP tasks. We benchmark these models on MS MARCO\n(Information Retrieval), BoolQ (Boolean Question Answering), and GSM8K\n(Mathematical Reasoning) datasets, assessing both accuracy and efficiency\nacross various tasks. The study measures the trade-offs between model\ncompression and task performance, analyzing key evaluation metrics, namely\naccuracy, inference latency, and throughput (total output tokens generated per\nsecond), providing insights into the suitability of low-bit quantization for\nreal-world deployment. Using the results, users can then make suitable\ndecisions based on the specifications that need to be met. We discuss the pros\nand cons of GSQ and GPTQ techniques on models of different sizes, which also\nserve as a benchmark for future experiments.", "AI": {"tldr": "This paper investigates the effects of 4-bit Group Scaling Quantization and Generative Pretrained Transformer Quantization on various large language models to assess accuracy and efficiency across multiple NLP tasks.", "motivation": "To improve accessibility and reduce resource demands of large language models while maintaining performance through quantization techniques.", "method": "The study applies 4-bit Group Scaling Quantization and Generative Pretrained Transformer Quantization to LLaMA 1B, Qwen 0.5B, and PHI 1.5B, benchmarking them on the MS MARCO, BoolQ, and GSM8K datasets.", "result": "The analysis presents trade-offs between model compression (using quantization) and accuracy/efficiency in NLP tasks, including metrics such as inference latency and throughput.", "conclusion": "The paper provides insights into the practical deployment of quantized models and highlights the advantages and limitations of GSQ and GPTQ for models of different sizes.", "key_contributions": ["Introduction of 4-bit quantization techniques for large language models", "Benchmarking various models on significant NLP tasks", "Discussion of performance trade-offs in real-world applications."], "limitations": "", "keywords": ["Quantization", "Large Language Models", "NLP Benchmarking", "Inference Latency", "Model Efficiency"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.11343", "pdf": "https://arxiv.org/pdf/2508.11343.pdf", "abs": "https://arxiv.org/abs/2508.11343", "title": "SpecDetect: Simple, Fast, and Training-Free Detection of LLM-Generated Text via Spectral Analysis", "authors": ["Haitong Luo", "Weiyao Zhang", "Suhang Wang", "Wenji Zou", "Chungang Lin", "Xuying Meng", "Yujun Zhang"], "categories": ["cs.CL"], "comment": "Under Review", "summary": "The proliferation of high-quality text from Large Language Models (LLMs)\ndemands reliable and efficient detection methods. While existing training-free\napproaches show promise, they often rely on surface-level statistics and\noverlook fundamental signal properties of the text generation process. In this\nwork, we reframe detection as a signal processing problem, introducing a novel\nparadigm that analyzes the sequence of token log-probabilities in the frequency\ndomain. By systematically analyzing the signal's spectral properties using the\nglobal Discrete Fourier Transform (DFT) and the local Short-Time Fourier\nTransform (STFT), we find that human-written text consistently exhibits\nsignificantly higher spectral energy. This higher energy reflects the\nlarger-amplitude fluctuations inherent in human writing compared to the\nsuppressed dynamics of LLM-generated text. Based on this key insight, we\nconstruct SpecDetect, a detector built on a single, robust feature from the\nglobal DFT: DFT total energy. We also propose an enhanced version,\nSpecDetect++, which incorporates a sampling discrepancy mechanism to further\nboost robustness. Extensive experiments demonstrate that our approach\noutperforms the state-of-the-art model while running in nearly half the time.\nOur work introduces a new, efficient, and interpretable pathway for\nLLM-generated text detection, showing that classical signal processing\ntechniques offer a surprisingly powerful solution to this modern challenge.", "AI": {"tldr": "This work presents SpecDetect, a novel method for detecting LLM-generated text using signal processing techniques, particularly analyzing token log-probabilities in the frequency domain.", "motivation": "To address the need for reliable detection methods for LLM-generated text, which currently rely mainly on surface-level statistics.", "method": "The study analyzes token log-probabilities using global DFT and local STFT to assess spectral properties, leading to the development of SpecDetect and SpecDetect++ for enhanced robustness.", "result": "SpecDetect outperforms existing state-of-the-art models in both accuracy and speed, demonstrating a clear distinction between human-written and LLM-generated text based on spectral energy.", "conclusion": "The work provides an efficient and interpretable solution to LLM detection, leveraging classical signal processing to identify fundamental differences in text generation.", "key_contributions": ["Introduction of SpecDetect for LLM detection based on spectral analysis", "Enhanced version SpecDetect++ with sampling discrepancy mechanism", "Significant performance improvement over state-of-the-art models"], "limitations": "", "keywords": ["LLM detection", "signal processing", "text generation", "DFT", "STFT"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.11364", "pdf": "https://arxiv.org/pdf/2508.11364.pdf", "abs": "https://arxiv.org/abs/2508.11364", "title": "Feedback Indicators: The Alignment between Llama and a Teacher in Language Learning", "authors": ["Sylvio Rüdian", "Yassin Elsir", "Marvin Kretschmer", "Sabine Cayrou", "Niels Pinkwart"], "categories": ["cs.CL"], "comment": "11 pages, one table", "summary": "Automated feedback generation has the potential to enhance students' learning\nprogress by providing timely and targeted feedback. Moreover, it can assist\nteachers in optimizing their time, allowing them to focus on more strategic and\npersonalized aspects of teaching. To generate high-quality, information-rich\nformative feedback, it is essential first to extract relevant indicators, as\nthese serve as the foundation upon which the feedback is constructed. Teachers\noften employ feedback criteria grids composed of various indicators that they\nevaluate systematically. This study examines the initial phase of extracting\nsuch indicators from students' submissions of a language learning course using\nthe large language model Llama 3.1. Accordingly, the alignment between\nindicators generated by the LLM and human ratings across various feedback\ncriteria is investigated. The findings demonstrate statistically significant\nstrong correlations, even in cases involving unanticipated combinations of\nindicators and criteria. The methodology employed in this paper offers a\npromising foundation for extracting indicators from students' submissions using\nLLMs. Such indicators can potentially be utilized to auto-generate explainable\nand transparent formative feedback in future research.", "AI": {"tldr": "This study explores the extraction of feedback indicators from students' language learning submissions using the LLM Llama 3.1, highlighting its potential for automated formative feedback generation.", "motivation": "To enhance student learning through timely feedback and assist teachers in optimizing their time for personalized teaching.", "method": "The study analyzes students' submissions to extract relevant feedback indicators using the large language model Llama 3.1 and assesses the alignment of these indicators with human ratings.", "result": "Statistically significant strong correlations were found between the LLM-generated indicators and the feedback criteria, even in unexpected combinations.", "conclusion": "The methodology demonstrates a promising foundation for utilizing LLMs to auto-generate explainable and transparent formative feedback.", "key_contributions": ["Introduction of a methodology for extracting feedback indicators using LLMs", "Demonstration of strong correlations between LLM-generated indicators and human feedback", "Potential implications for automated formative feedback generation in education."], "limitations": "", "keywords": ["automated feedback", "large language models", "education", "indicators extraction", "formative feedback"], "importance_score": 8, "read_time_minutes": 11}}
{"id": "2508.11383", "pdf": "https://arxiv.org/pdf/2508.11383.pdf", "abs": "https://arxiv.org/abs/2508.11383", "title": "When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs", "authors": ["Mikhail Seleznyov", "Mikhail Chaichuk", "Gleb Ershov", "Alexander Panchenko", "Elena Tutubalina", "Oleg Somov"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are highly sensitive to subtle, non-semantic\nvariations in prompt phrasing and formatting. In this work, we present the\nfirst systematic evaluation of 5 methods for improving prompt robustness within\na unified experimental framework. We benchmark these techniques on 8 models\nfrom Llama, Qwen and Gemma families across 52 tasks from Natural Instructions\ndataset. Our evaluation covers robustness methods from both fine-tuned and\nin-context learning paradigms, and tests their generalization against multiple\ntypes of distribution shifts. Finally, we extend our analysis to GPT-4.1 and\nDeepSeek V3 to assess frontier models' current robustness to format\nperturbations. Our findings offer actionable insights into the relative\neffectiveness of these robustness methods, enabling practitioners to make\ninformed decisions when aiming for stable and reliable LLM performance in\nreal-world applications. Code:\nhttps://github.com/AIRI-Institute/when-punctuation-matters.", "AI": {"tldr": "Evaluation of methods for improving prompt robustness in Large Language Models (LLMs).", "motivation": "To systematically evaluate and improve the robustness of LLMs against subtle prompt variations and distribution shifts.", "method": "Benchmarking 5 robustness techniques on 8 models from Llama, Qwen, and Gemma families across 52 tasks from the Natural Instructions dataset.", "result": "Identification of the relative effectiveness of various robustness methods, including insights into their performance across different types of distribution shifts.", "conclusion": "The findings provide actionable insights for practitioners aiming to achieve reliable LLM performance in practical applications.", "key_contributions": ["Systematic evaluation of 5 prompt robustness methods.", "Benchmarking on multiple LLMs across a wide range of tasks.", "Actionable insights for improving LLM stability in real-world applications."], "limitations": "", "keywords": ["Large Language Models", "prompt robustness", "distribution shifts"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.11386", "pdf": "https://arxiv.org/pdf/2508.11386.pdf", "abs": "https://arxiv.org/abs/2508.11386", "title": "Retrieval-augmented reasoning with lean language models", "authors": ["Ryan Sze-Yin Chan", "Federico Nanni", "Tomas Lazauskas", "Rosie Wood", "Penelope Yong", "Lionel Tarassenko", "Mark Girolami", "James Geddes", "Andrew Duncan"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "This technical report details a novel approach to combining reasoning and\nretrieval augmented generation (RAG) within a single, lean language model\narchitecture. While existing RAG systems typically rely on large-scale models\nand external APIs, our work addresses the increasing demand for performant and\nprivacy-preserving solutions deployable in resource-constrained or secure\nenvironments. Building on recent developments in test-time scaling and\nsmall-scale reasoning models, we develop a retrieval augmented conversational\nagent capable of interpreting complex, domain-specific queries using a\nlightweight backbone model. Our system integrates a dense retriever with\nfine-tuned Qwen2.5-Instruct models, using synthetic query generation and\nreasoning traces derived from frontier models (e.g., DeepSeek-R1) over a\ncurated corpus, in this case, the NHS A-to-Z condition pages. We explore the\nimpact of summarisation-based document compression, synthetic data design, and\nreasoning-aware fine-tuning on model performance. Evaluation against both\nnon-reasoning and general-purpose lean models demonstrates that our\ndomain-specific fine-tuning approach yields substantial gains in answer\naccuracy and consistency, approaching frontier-level performance while\nremaining feasible for local deployment. All implementation details and code\nare publicly released to support reproducibility and adaptation across domains.", "AI": {"tldr": "This report proposes a novel retrieval augmented generation (RAG) approach using a lightweight language model for domain-specific query handling, aiming for high performance and privacy in resource-constrained environments.", "motivation": "The paper addresses the need for efficient and privacy-preserving RAG systems that can operate within secure, resource-constrained environments, moving away from the reliance on large-scale models and external APIs.", "method": "The authors develop a conversational agent that combines a dense retriever with fine-tuned Qwen2.5-Instruct models, employing synthetic query generation and reasoning traces derived from advanced models over a specific corpus (NHS A-to-Z condition pages).", "result": "The evaluation shows that the proposed domain-specific fine-tuning results in significant improvements in answer accuracy and consistency, nearing the performance of more extensive models while enabling local deployment.", "conclusion": "The study concludes that lightweight models can achieve competitive performance with proper fine-tuning and that the implementation is reproducible and adaptable to other domains as all code is publicly released.", "key_contributions": ["Introduction of a lightweight RAG approach for domain-specific queries", "Integration of reasoning and retrieval in a compact model architecture", "Evaluation of summarization-based document compression and reasoning-aware fine-tuning methods"], "limitations": "", "keywords": ["retrieval augmented generation", "conversational agent", "domain-specific queries", "lightweight models", "privacy-preserving solutions"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.11388", "pdf": "https://arxiv.org/pdf/2508.11388.pdf", "abs": "https://arxiv.org/abs/2508.11388", "title": "Model Interpretability and Rationale Extraction by Input Mask Optimization", "authors": ["Marc Brinner", "Sina Zarriess"], "categories": ["cs.CL", "cs.CV", "cs.LG"], "comment": null, "summary": "Concurrent to the rapid progress in the development of neural-network based\nmodels in areas like natural language processing and computer vision, the need\nfor creating explanations for the predictions of these black-box models has\nrisen steadily. We propose a new method to generate extractive explanations for\npredictions made by neural networks, that is based on masking parts of the\ninput which the model does not consider to be indicative of the respective\nclass. The masking is done using gradient-based optimization combined with a\nnew regularization scheme that enforces sufficiency, comprehensiveness and\ncompactness of the generated explanation, three properties that are known to be\ndesirable from the related field of rationale extraction in natural language\nprocessing. In this way, we bridge the gap between model interpretability and\nrationale extraction, thereby proving that the latter of which can be performed\nwithout training a specialized model, only on the basis of a trained\nclassifier. We further apply the same method to image inputs and obtain high\nquality explanations for image classifications, which indicates that the\nconditions proposed for rationale extraction in natural language processing are\nmore broadly applicable to different input types.", "AI": {"tldr": "This paper presents a new method for generating extractive explanations from neural network predictions using gradient-based masking.", "motivation": "With the increasing complexity of neural networks, there's a growing need for interpretable models that can provide clear explanations for their predictions.", "method": "The proposed method utilizes gradient-based optimization to mask parts of the input that are not indicative of the predicted class, while incorporating a new regularization scheme to ensure explanations are sufficient, comprehensive, and compact.", "result": "The method successfully generates high-quality explanations for both textual and image inputs, demonstrating broader applicability across different data types.", "conclusion": "This approach bridges model interpretability and rationale extraction, showing that effective rationale extraction can be done without a specialized model.", "key_contributions": ["Introduction of a method for extractive explanations using gradient-based masking.", "Establishes a connection between rationale extraction in NLP and model interpretability.", "Demonstrates applicability of the method to both text and image data."], "limitations": "", "keywords": ["explainable AI", "rationale extraction", "neural networks", "gradient-based optimization", "model interpretability"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.11393", "pdf": "https://arxiv.org/pdf/2508.11393.pdf", "abs": "https://arxiv.org/abs/2508.11393", "title": "Rationalizing Transformer Predictions via End-To-End Differentiable Self-Training", "authors": ["Marc Brinner", "Sina Zarrieß"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "We propose an end-to-end differentiable training paradigm for stable training\nof a rationalized transformer classifier. Our approach results in a single\nmodel that simultaneously classifies a sample and scores input tokens based on\ntheir relevance to the classification. To this end, we build on the widely-used\nthree-player-game for training rationalized models, which typically relies on\ntraining a rationale selector, a classifier and a complement classifier. We\nsimplify this approach by making a single model fulfill all three roles,\nleading to a more efficient training paradigm that is not susceptible to the\ncommon training instabilities that plague existing approaches. Further, we\nextend this paradigm to produce class-wise rationales while incorporating\nrecent advances in parameterizing and regularizing the resulting rationales,\nthus leading to substantially improved and state-of-the-art alignment with\nhuman annotations without any explicit supervision.", "AI": {"tldr": "We introduce a unified, end-to-end differentiable training method for a rationalized transformer classifier that enhances efficiency and stability in training while simultaneously providing classification and relevance scoring of input tokens.", "motivation": "The traditional methods for training rationalized models often lead to instabilities and require multiple models for different tasks, which complicates the training process.", "method": "We propose a single model that integrates the roles of a rationale selector, classifier, and complement classifier, thereby simplifying the training process and enhancing stability.", "result": "The new training paradigm yields substantial improvements in the alignment of rationales with human annotations and demonstrates state-of-the-art performance without requiring explicit supervision.", "conclusion": "Our approach mitigates common pitfalls in existing methods, providing an efficient solution for model training while producing high-quality rationales relevant to classifications.", "key_contributions": ["Introduction of a single model for rationale selection and classification", "Simplified training process leading to enhanced stability", "Significant improvements in rationale alignment with human annotations"], "limitations": "", "keywords": ["rationalized models", "transformer classifier", "differentiable training"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2401.05999", "pdf": "https://arxiv.org/pdf/2401.05999.pdf", "abs": "https://arxiv.org/abs/2401.05999", "title": "Boosting Mixed-Initiative Co-Creativity in Game Design: A Tutorial", "authors": ["Solange Margarido", "Licínio Roque", "Penousal Machado", "Pedro Martins"], "categories": ["cs.HC"], "comment": "37 pages, 1 table, 19 figures; expanded introduction to section 3,\n  subsection 4.1, and closing discussion in section 5; restructured subsection\n  4.2 for greater clarity", "summary": "In recent years, there has been a growing application of mixed-initiative\nco-creative approaches in the creation of video games. The rapid advances in\nthe capabilities of artificial intelligence (AI) systems further propel\ncreative collaboration between humans and computational agents. In this\ntutorial, we present guidelines for researchers and practitioners to develop\ngame design tools with a high degree of mixed-initiative co-creativity\n(MI-CCy). We begin by reviewing a selection of current works that will serve as\ncase studies and categorize them by the type of game content they address. We\nintroduce the MI-CCy Quantifier, a framework that can be used by researchers\nand developers to assess co-creative tools on their level of MI-CCy through a\nvisual scheme of quantifiable criteria scales. We demonstrate the usage of the\nMI-CCy Quantifier by applying it to the selected works. This analysis enabled\nus to discern prevalent patterns within these tools, as well as features that\ncontribute to a higher level of MI-CCy. We highlight current gaps in MI-CCy\napproaches within game design, which we propose as pivotal aspects to tackle in\nthe development of forthcoming approaches.", "AI": {"tldr": "This tutorial discusses the development of mixed-initiative co-creative tools in video game design and introduces the MI-CCy Quantifier for assessing these tools.", "motivation": "The paper explores the intersection of AI and game design, particularly how mixed-initiative co-creative methods can enhance creative collaboration in developing video games.", "method": "The authors present guidelines and case studies for evaluating game design tools using the MI-CCy Quantifier, which categorizes game content and assesses tools based on quantifiable criteria scales.", "result": "The analysis of selected works using the MI-CCy Quantifier revealed patterns and features associated with higher levels of mixed-initiative co-creativity in game design tools.", "conclusion": "The paper identifies gaps in current mixed-initiative co-creativity approaches and provides insights for future development in game design tools.", "key_contributions": ["Introduction of the MI-CCy Quantifier framework for assessing co-creative tools.", "Categorization of existing game design tools based on their approach to mixed-initiative co-creativity.", "Identification of current gaps and future directions for research in game design."], "limitations": "", "keywords": ["mixed-initiative co-creativity", "game design", "AI", "co-creative tools", "MI-CCy Quantifier"], "importance_score": 5, "read_time_minutes": 15}}
{"id": "2508.11414", "pdf": "https://arxiv.org/pdf/2508.11414.pdf", "abs": "https://arxiv.org/abs/2508.11414", "title": "Survey-to-Behavior: Downstream Alignment of Human Values in LLMs via Survey Questions", "authors": ["Shangrui Nie", "Florian Mai", "David Kaczér", "Charles Welch", "Zhixue Zhao", "Lucie Flek"], "categories": ["cs.CL"], "comment": "7 pages 1 figure", "summary": "Large language models implicitly encode preferences over human values, yet\nsteering them often requires large training data. In this work, we investigate\na simple approach: Can we reliably modify a model's value system in downstream\nbehavior by training it to answer value survey questions accordingly? We first\nconstruct value profiles of several open-source LLMs by asking them to rate a\nseries of value-related descriptions spanning 20 distinct human values, which\nwe use as a baseline for subsequent experiments. We then investigate whether\nthe value system of a model can be governed by fine-tuning on the value\nsurveys. We evaluate the effect of finetuning on the model's behavior in two\nways; first, we assess how answers change on in-domain, held-out survey\nquestions. Second, we evaluate whether the model's behavior changes in\nout-of-domain settings (situational scenarios). To this end, we construct a\ncontextualized moral judgment dataset based on Reddit posts and evaluate\nchanges in the model's behavior in text-based adventure games. We demonstrate\nthat our simple approach can not only change the model's answers to in-domain\nsurvey questions, but also produces substantial shifts (value alignment) in\nimplicit downstream task behavior.", "AI": {"tldr": "This paper explores whether fine-tuning large language models (LLMs) on value surveys can modify their implicit value systems and behavior across various scenarios.", "motivation": "Large language models encode preferences over human values, but modifying these preferences typically requires extensive training data. The authors investigate if a simpler approach, using value survey fine-tuning, can effectively alter model behavior.", "method": "The authors construct value profiles for several open-source LLMs by having them rate 20 distinct human values. Subsequently, they fine-tune these models on value survey responses and evaluate changes in behavior through held-out survey questions and application in moral judgment scenarios using contextualized datasets derived from Reddit and text-based games.", "result": "Fine-tuning on value survey questions successfully changed model responses on in-domain queries and resulted in significant behavioral shifts in out-of-domain scenarios, demonstrating a degree of value alignment in implicit tasks.", "conclusion": "The simple approach of fine-tuning LLMs on value surveys is effective in altering both direct responses and broader behavioral patterns, indicating a viable method for steering model preferences.", "key_contributions": ["Development of value profiles for open-source LLMs based on human values.", "Novel fine-tuning approach using value surveys to modify LLM behavior.", "Empirical demonstration of value alignment in both in-domain and out-of-domain tasks."], "limitations": "The study's generalizability may be limited by the specific datasets used and the range of LLMs evaluated.", "keywords": ["large language models", "value systems", "fine-tuning", "human values", "behavioral change"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.08670", "pdf": "https://arxiv.org/pdf/2504.08670.pdf", "abs": "https://arxiv.org/abs/2504.08670", "title": "Once Upon an AI: Six Scaffolds for Child-AI Interaction Design, Inspired by Disney", "authors": ["Nomisha Kurian"], "categories": ["cs.HC", "cs.AI"], "comment": "28 pages", "summary": "To build AI that children can intuitively understand and benefit from,\ndesigners need a design grammar that serves their developmental needs. This\npaper bridges artificial intelligence design for children - an emerging field\nstill defining its best practices - and animation, a well established field\nwith decades of experience in engaging children through accessible\nstorytelling. Pairing Piagetian developmental theory with design pattern\nextraction from 52 works of animation, the paper presents a six scaffold\nframework that integrates design insights transferable to child centred AI\ndesign: (1) signals for visual animacy and clarity, (2) sound for musical and\nauditory scaffolding, (3) synchrony in audiovisual cues, (4) sidekick style\npersonas, (5) storyplay that supports symbolic play and imaginative\nexploration, and (6) structure in the form of predictable narratives. These\nstrategies, long refined in animation, function as multimodal scaffolds for\nattention, understanding, and attunement, supporting learning and comfort. This\nstructured design grammar is transferable to AI design. By reframing cinematic\nstorytelling and child development theory as design logic for AI, the paper\noffers heuristics for AI that aligns with the cognitive stages and emotional\nneeds of young users. The work contributes to design theory by showing how\nsensory, affective, and narrative techniques can inform developmentally attuned\nAI design. Future directions include empirical testing, cultural adaptation,\nand participatory co design.", "AI": {"tldr": "This paper presents a design grammar for creating AI tailored for children, integrating insights from animation and developmental theory.", "motivation": "The need for AI that children can intuitively understand and that supports their developmental needs.", "method": "The paper pairs Piagetian developmental theory with design pattern extraction from 52 animation works to develop a six scaffold framework.", "result": "The framework includes strategies for visual clarity, sound scaffolding, synchrony in cues, sidekick personas, storyplay, and narrative structure, enhancing learning and comfort for children.", "conclusion": "By providing design heuristics based on developmental insights and storytelling techniques, the paper contributes to the field of child-centered AI design.", "key_contributions": ["Development of a six scaffold framework for child-centered AI design", "Integration of animation techniques with developmental theory", "Provision of design heuristics for enhancing children's interaction with AI"], "limitations": "", "keywords": ["AI design", "children", "animation", "developmental theory", "storytelling"], "importance_score": 4, "read_time_minutes": 28}}
{"id": "2508.11429", "pdf": "https://arxiv.org/pdf/2508.11429.pdf", "abs": "https://arxiv.org/abs/2508.11429", "title": "HumorPlanSearch: Structured Planning and HuCoT for Contextual AI Humor", "authors": ["Shivam Dubey"], "categories": ["cs.CL"], "comment": null, "summary": "Automated humor generation with Large Language Models (LLMs) often yields\njokes that feel generic, repetitive, or tone-deaf because humor is deeply\nsituated and hinges on the listener's cultural background, mindset, and\nimmediate context. We introduce HumorPlanSearch, a modular pipeline that\nexplicitly models context through: (1) Plan-Search for diverse, topic-tailored\nstrategies; (2) Humor Chain-of-Thought (HuCoT) templates capturing cultural and\nstylistic reasoning; (3) a Knowledge Graph to retrieve and adapt\nhigh-performing historical strategies; (4) novelty filtering via semantic\nembeddings; and (5) an iterative judge-driven revision loop. To evaluate\ncontext sensitivity and comedic quality, we propose the Humor Generation Score\n(HGS), which fuses direct ratings, multi-persona feedback, pairwise win-rates,\nand topic relevance. In experiments across nine topics with feedback from 13\nhuman judges, our full pipeline (KG + Revision) boosts mean HGS by 15.4 percent\n(p < 0.05) over a strong baseline. By foregrounding context at every stage from\nstrategy planning to multi-signal evaluation, HumorPlanSearch advances\nAI-driven humor toward more coherent, adaptive, and culturally attuned comedy.", "AI": {"tldr": "HumorPlanSearch is a pipeline that improves automated humor generation using context modeling and cultural reasoning, resulting in more adaptive and coherent jokes.", "motivation": "Automated humor generation often fails due to lack of consideration for cultural background and context, leading to generic jokes.", "method": "HumorPlanSearch incorporates a modular approach with Plan-Search for strategies, Humor Chain-of-Thought templates, a Knowledge Graph for historical strategy retrieval, novelty filtering, and a revision loop driven by human judges.", "result": "The HumorPlanSearch pipeline improves the Humor Generation Score by 15.4% over a strong baseline, indicating better context sensitivity and comedic quality.", "conclusion": "By integrating contextual considerations throughout the humor generation process, HumorPlanSearch demonstrates significant advancements in AI-driven humor.", "key_contributions": ["Introduction of HumorPlanSearch for context-sensitive humor generation", "Development of Humor Generation Score (HGS) for evaluating comedic quality", "Integration of diverse techniques including Knowledge Graphs and iterative revision loops"], "limitations": "", "keywords": ["humor generation", "context modeling", "large language models", "cultural reasoning", "AI-driven comedy"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.11434", "pdf": "https://arxiv.org/pdf/2508.11434.pdf", "abs": "https://arxiv.org/abs/2508.11434", "title": "Online Anti-sexist Speech: Identifying Resistance to Gender Bias in Political Discourse", "authors": ["Aditi Dutta", "Susan Banducci"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Anti-sexist speech, i.e., public expressions that challenge or resist\ngendered abuse and sexism, plays a vital role in shaping democratic debate\nonline. Yet automated content moderation systems, increasingly powered by large\nlanguage models (LLMs), may struggle to distinguish such resistance from the\nsexism it opposes. This study examines how five LLMs classify sexist,\nanti-sexist, and neutral political tweets from the UK, focusing on\nhigh-salience trigger events involving female Members of Parliament in the year\n2022. Our analysis show that models frequently misclassify anti-sexist speech\nas harmful, particularly during politically charged events where rhetorical\nstyles of harm and resistance converge. These errors risk silencing those who\nchallenge sexism, with disproportionate consequences for marginalised voices.\nWe argue that moderation design must move beyond binary harmful/not-harmful\nschemas, integrate human-in-the-loop review during sensitive events, and\nexplicitly include counter-speech in training data. By linking feminist\nscholarship, event-based analysis, and model evaluation, this work highlights\nthe sociotechnical challenges of safeguarding resistance speech in digital\npolitical spaces.", "AI": {"tldr": "This study explores the challenges LLMs face in accurately classifying anti-sexist speech on social media, identifying critical misclassification errors that impact marginalized voices.", "motivation": "To examine the effectiveness of LLMs in distinguishing between sexist, anti-sexist, and neutral content in political discourse and to address the implications of misclassification.", "method": "The study analyzes the classification performance of five LLMs on political tweets related to high-profile events in the UK involving female Members of Parliament in 2022.", "result": "The analysis reveals that LLMs often misclassify anti-sexist speech as harmful, particularly during politically charged events, which can silence marginalized voices.", "conclusion": "The findings suggest a need for moderation designs to incorporate more nuanced categorization, human oversight during sensitive times, and the inclusion of counter-speech in model training.", "key_contributions": ["Identifies misclassification of anti-sexist speech by LLMs.", "Highlights the dangers of binary moderation schemas in content moderation.", "Calls for integrating feminist scholarship into AI moderation practices."], "limitations": "The study is limited to a specific set of political tweets and may not generalize to other contexts or types of speech.", "keywords": ["Anti-sexist speech", "Content moderation", "Large language models", "Political tweets", "Gendered abuse"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.11442", "pdf": "https://arxiv.org/pdf/2508.11442.pdf", "abs": "https://arxiv.org/abs/2508.11442", "title": "CoDiEmb: A Collaborative yet Distinct Framework for Unified Representation Learning in Information Retrieval and Semantic Textual Similarity", "authors": ["Bowen Zhang", "Zixin Song", "Chunquan Chen", "Qian-Wen Zhang", "Di Yin", "Xing Sun"], "categories": ["cs.CL"], "comment": null, "summary": "Learning unified text embeddings that excel across diverse downstream tasks\nis a central goal in representation learning, yet negative transfer remains a\npersistent obstacle. This challenge is particularly pronounced when jointly\ntraining a single encoder for Information Retrieval (IR) and Semantic Textual\nSimilarity (STS), two essential but fundamentally disparate tasks for which\nnaive co-training typically yields steep performance trade-offs. We argue that\nresolving this conflict requires systematically decoupling task-specific\nlearning signals throughout the training pipeline. To this end, we introduce\nCoDiEmb, a unified framework that reconciles the divergent requirements of IR\nand STS in a collaborative yet distinct manner. CoDiEmb integrates three key\ninnovations for effective joint optimization: (1) Task-specialized objectives\npaired with a dynamic sampler that forms single-task batches and balances\nper-task updates, thereby preventing gradient interference. For IR, we employ a\ncontrastive loss with multiple positives and hard negatives, augmented by\ncross-device sampling. For STS, we adopt order-aware objectives that directly\noptimize correlation and ranking consistency. (2) A delta-guided model fusion\nstrategy that computes fine-grained merging weights for checkpoints by\nanalyzing each parameter's deviation from its pre-trained initialization,\nproving more effective than traditional Model Soups. (3) An efficient,\nsingle-stage training pipeline that is simple to implement and converges\nstably. Extensive experiments on 15 standard IR and STS benchmarks across three\nbase encoders validate CoDiEmb. Our results and analysis demonstrate that the\nframework not only mitigates cross-task trade-offs but also measurably improves\nthe geometric properties of the embedding space.", "AI": {"tldr": "CoDiEmb framework addresses negative transfer in text embeddings for Information Retrieval and Semantic Textual Similarity by decoupling task-specific learning signals and optimizing joint performance.", "motivation": "To tackle the issue of negative transfer when training a unified text embedding model for two distinct tasks: Information Retrieval (IR) and Semantic Textual Similarity (STS).", "method": "Introduced CoDiEmb, which integrates task-specialized objectives, a delta-guided model fusion strategy, and a single-stage training pipeline to optimize performance across both tasks without gradient interference.", "result": "CoDiEmb mitigates cross-task trade-offs and improves the geometric properties of the embedding space, as validated by extensive experiments on 15 standard benchmarks.", "conclusion": "The framework allows effective joint optimization for IR and STS, enhancing performance beyond traditional co-training methods.", "key_contributions": ["Introduced a dynamic sampler for single-task batch formation", "Developed a delta-guided model fusion strategy for parameter optimization", "Implemented an efficient single-stage training pipeline"], "limitations": "", "keywords": ["text embeddings", "Information Retrieval", "Semantic Textual Similarity", "co-training", "machine learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.11454", "pdf": "https://arxiv.org/pdf/2508.11454.pdf", "abs": "https://arxiv.org/abs/2508.11454", "title": "Reference Points in LLM Sentiment Analysis: The Role of Structured Context", "authors": ["Junichiro Niimi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are now widely used across many fields,\nincluding marketing research. Sentiment analysis, in particular, helps firms\nunderstand consumer preferences. While most NLP studies classify sentiment from\nreview text alone, marketing theories, such as prospect theory and\nexpectation--disconfirmation theory, point out that customer evaluations are\nshaped not only by the actual experience but also by additional reference\npoints. This study therefore investigates how the content and format of such\nsupplementary information affect sentiment analysis using LLMs. We compare\nnatural language (NL) and JSON-formatted prompts using a lightweight 3B\nparameter model suitable for practical marketing applications. Experiments on\ntwo Yelp categories (Restaurant and Nightlife) show that the JSON prompt with\nadditional information outperforms all baselines without fine-tuning: Macro-F1\nrises by 1.6% and 4% while RMSE falls by 16% and 9.1%, respectively, making it\ndeployable in resource-constrained edge devices. Furthermore, a follow-up\nanalysis confirms that performance gains stem from genuine contextual reasoning\nrather than label proxying. This work demonstrates that structured prompting\ncan enable smaller models to achieve competitive performance, offering a\npractical alternative to large-scale model deployment.", "AI": {"tldr": "This study explores the impact of supplementary information format on sentiment analysis using LLMs, demonstrating that structured JSON prompts outperform natural language prompts without fine-tuning.", "motivation": "Understanding consumer preferences through sentiment analysis is crucial for firms, and existing NLP studies often neglect the influence of additional reference points defined by marketing theories.", "method": "The study compares the effectiveness of natural language and JSON-formatted prompts using a 3B parameter language model on two Yelp categories: Restaurant and Nightlife.", "result": "JSON prompts outperformed all baselines, improving Macro-F1 scores by 1.6% and 4%, and decreasing RMSE by 16% and 9.1%, indicating enhanced performance without fine-tuning.", "conclusion": "Structured prompting allows smaller models to achieve competitive performance in sentiment analysis, making it feasible for deployment in resource-limited environments.", "key_contributions": ["Demonstrated the effectiveness of JSON prompts in sentiment analysis using LLMs.", "Showed that structured prompts can outperform natural language prompts without fine-tuning.", "Highlighted the importance of contextual reasoning in sentiment analysis results."], "limitations": "", "keywords": ["Large Language Models", "Sentiment Analysis", "Marketing Research", "JSON Formatting", "Contextual Reasoning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.11534", "pdf": "https://arxiv.org/pdf/2508.11534.pdf", "abs": "https://arxiv.org/abs/2508.11534", "title": "Speciesism in AI: Evaluating Discrimination Against Animals in Large Language Models", "authors": ["Monika Jotautaitė", "Lucius Caviola", "David A. Brewster", "Thilo Hagendorff"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "As large language models (LLMs) become more widely deployed, it is crucial to\nexamine their ethical tendencies. Building on research on fairness and\ndiscrimination in AI, we investigate whether LLMs exhibit speciesist bias --\ndiscrimination based on species membership -- and how they value non-human\nanimals. We systematically examine this issue across three paradigms: (1)\nSpeciesismBench, a 1,003-item benchmark assessing recognition and moral\nevaluation of speciesist statements; (2) established psychological measures\ncomparing model responses with those of human participants; (3) text-generation\ntasks probing elaboration on, or resistance to, speciesist rationalizations. In\nour benchmark, LLMs reliably detected speciesist statements but rarely\ncondemned them, often treating speciesist attitudes as morally acceptable. On\npsychological measures, results were mixed: LLMs expressed slightly lower\nexplicit speciesism than people, yet in direct trade-offs they more often chose\nto save one human over multiple animals. A tentative interpretation is that\nLLMs may weight cognitive capacity rather than species per se: when capacities\nwere equal, they showed no species preference, and when an animal was described\nas more capable, they tended to prioritize it over a less capable human. In\nopen-ended text generation tasks, LLMs frequently normalized or rationalized\nharm toward farmed animals while refusing to do so for non-farmed animals.\nThese findings suggest that while LLMs reflect a mixture of progressive and\nmainstream human views, they nonetheless reproduce entrenched cultural norms\naround animal exploitation. We argue that expanding AI fairness and alignment\nframeworks to explicitly include non-human moral patients is essential for\nreducing these biases and preventing the entrenchment of speciesist attitudes\nin AI systems and the societies they influence.", "AI": {"tldr": "This paper investigates speciesist bias in large language models (LLMs), examining their recognition and moral evaluation of speciesist statements while revealing mixed ethical stances on animal treatment.", "motivation": "To analyze the ethical tendencies of large language models concerning speciesism, reflecting on the significance of addressing biases in AI systems, particularly in relation to non-human animals.", "method": "The research employs three paradigms: a benchmark (SpeciesismBench) with 1,003 items assessing speciesist statement responses, psychological measures comparing LLM and human responses, and text-generation tasks to evaluate elaboration on speciesist rationalizations.", "result": "LLMs reliably detect speciesist statements but rarely condemn them. They showed a mix of explicit speciesism lower than that of humans but preferred to save humans over multiple non-human animals; they rationalize harm toward farmed animals more than non-farmed animals.", "conclusion": "LLMs reflect a blend of progressive and mainstream human views while reproducing entrenched cultural norms. The study emphasizes the need to expand AI fairness frameworks to include non-human moral considerations to reduce speciesist biases.", "key_contributions": ["Introduction of SpeciesismBench as a benchmark for evaluating speciesism in language models", "Findings indicating LLMs' mixed ethical views on speciesism", "Recommendations for enhancing AI fairness frameworks to address non-human animals"], "limitations": "The study highlights the need for further research on the implications of these findings in real-world AI applications and the cultural contexts of speciesism.", "keywords": ["speciesism", "large language models", "AI ethics", "animal rights", "fairness in AI"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.11536", "pdf": "https://arxiv.org/pdf/2508.11536.pdf", "abs": "https://arxiv.org/abs/2508.11536", "title": "Language models align with brain regions that represent concepts across modalities", "authors": ["Maria Ryskina", "Greta Tuckute", "Alexander Fung", "Ashley Malkin", "Evelina Fedorenko"], "categories": ["cs.CL"], "comment": "Accepted to COLM 2025. Code and data can be found at\n  https://github.com/ryskina/concepts-brain-llms", "summary": "Cognitive science and neuroscience have long faced the challenge of\ndisentangling representations of language from representations of conceptual\nmeaning. As the same problem arises in today's language models (LMs), we\ninvestigate the relationship between LM--brain alignment and two neural\nmetrics: (1) the level of brain activation during processing of sentences,\ntargeting linguistic processing, and (2) a novel measure of meaning consistency\nacross input modalities, which quantifies how consistently a brain region\nresponds to the same concept across paradigms (sentence, word cloud, image)\nusing an fMRI dataset (Pereira et al., 2018). Our experiments show that both\nlanguage-only and language-vision models predict the signal better in more\nmeaning-consistent areas of the brain, even when these areas are not strongly\nsensitive to language processing, suggesting that LMs might internally\nrepresent cross-modal conceptual meaning.", "AI": {"tldr": "This paper investigates the alignment between language models and brain activity, focusing on linguistic processing and cross-modal conceptual meaning.", "motivation": "The study aims to explore how language models relate to brain representations of language and meaning, addressing challenges in cognitive science and neuroscience.", "method": "The research examines brain activation levels during sentence processing and introduces a new measure of meaning consistency across different modalities using fMRI data.", "result": "Results indicate that language models predict brain activity better in regions of consistent meaning representation, even when those regions are not primarily associated with language processing.", "conclusion": "The findings suggest that language models may internally capture cross-modal conceptual meanings, providing insights for future research in cognitive science and AI.", "key_contributions": ["Introduced a novel metric for meaning consistency across modalities.", "Demonstrated the link between brain activation and language model predictions.", "Provided empirical evidence of cross-modal representation in language models."], "limitations": "", "keywords": ["Language Models", "Neuroscience", "Cognitive Science"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.11567", "pdf": "https://arxiv.org/pdf/2508.11567.pdf", "abs": "https://arxiv.org/abs/2508.11567", "title": "AgentMental: An Interactive Multi-Agent Framework for Explainable and Adaptive Mental Health Assessment", "authors": ["Jinpeng Hu", "Ao Wang", "Qianqian Xie", "Hui Ma", "Zhuo Li", "Dan Guo"], "categories": ["cs.CL"], "comment": null, "summary": "Mental health assessment is crucial for early intervention and effective\ntreatment, yet traditional clinician-based approaches are limited by the\nshortage of qualified professionals. Recent advances in artificial intelligence\nhave sparked growing interest in automated psychological assessment, yet most\nexisting approaches are constrained by their reliance on static text analysis,\nlimiting their ability to capture deeper and more informative insights that\nemerge through dynamic interaction and iterative questioning. Therefore, in\nthis paper, we propose a multi-agent framework for mental health evaluation\nthat simulates clinical doctor-patient dialogues, with specialized agents\nassigned to questioning, adequacy evaluation, scoring, and updating. We\nintroduce an adaptive questioning mechanism in which an evaluation agent\nassesses the adequacy of user responses to determine the necessity of\ngenerating targeted follow-up queries to address ambiguity and missing\ninformation. Additionally, we employ a tree-structured memory in which the root\nnode encodes the user's basic information, while child nodes (e.g., topic and\nstatement) organize key information according to distinct symptom categories\nand interaction turns. This memory is dynamically updated throughout the\ninteraction to reduce redundant questioning and further enhance the information\nextraction and contextual tracking capabilities. Experimental results on the\nDAIC-WOZ dataset illustrate the effectiveness of our proposed method, which\nachieves better performance than existing approaches.", "AI": {"tldr": "This paper presents a multi-agent framework for automated mental health assessment that enhances interaction and dialogue through adaptive questioning and dynamic memory management.", "motivation": "To address the limitations of traditional clinician-based mental health assessments and improve the automation of psychological evaluation using AI.", "method": "A multi-agent framework simulating doctor-patient dialogues with an adaptive questioning mechanism and tree-structured memory for dynamic information management.", "result": "Experimental results demonstrate that the proposed method achieves better performance in mental health evaluation compared to existing static text analysis approaches.", "conclusion": "The proposed framework offers an innovative solution for improving automated psychological assessments by facilitating deeper insights through interactive questioning and organization of user data.", "key_contributions": ["Introduction of a multi-agent framework for mental health evaluation.", "Development of an adaptive questioning mechanism to enhance interaction.", "Implementation of a tree-structured memory for efficient data management."], "limitations": "", "keywords": ["mental health assessment", "multi-agent framework", "adaptive questioning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.11582", "pdf": "https://arxiv.org/pdf/2508.11582.pdf", "abs": "https://arxiv.org/abs/2508.11582", "title": "Aware First, Think Less: Dynamic Boundary Self-Awareness Drives Extreme Reasoning Efficiency in Large Language Models", "authors": ["Qiguang Chen", "Dengyun Peng", "Jinhao Liu", "HuiKang Su", "Jiannan Guan", "Libo Qin", "Wanxiang Che"], "categories": ["cs.CL", "cs.AI"], "comment": "Preprint", "summary": "Recent advancements in large language models (LLMs) have greatly improved\ntheir capabilities on complex reasoning tasks through Long Chain-of-Thought\n(CoT). However, this approach often results in substantial redundancy,\nimpairing computational efficiency and causing significant delays in real-time\napplications. To improve the efficiency, current methods often rely on\nhuman-defined difficulty priors, which do not align with the LLM's self-awared\ndifficulty, leading to inefficiencies. In this paper, we introduce the Dynamic\nReasoning-Boundary Self-Awareness Framework (DR. SAF), which enables models to\ndynamically assess and adjust their reasoning depth in response to problem\ncomplexity. DR. SAF integrates three key components: Boundary Self-Awareness\nAlignment, Adaptive Reward Management, and a Boundary Preservation Mechanism.\nThese components allow models to optimize their reasoning processes, balancing\nefficiency and accuracy without compromising performance. Our experimental\nresults demonstrate that DR. SAF achieves a 49.27% reduction in total response\ntokens with minimal loss in accuracy. The framework also delivers a 6.59x gain\nin token efficiency and a 5x reduction in training time, making it well-suited\nto resource-limited settings. During extreme training, DR. SAF can even surpass\ntraditional instruction-based models in token efficiency with more than 16%\naccuracy improvement.", "AI": {"tldr": "The paper proposes the Dynamic Reasoning-Boundary Self-Awareness Framework (DR. SAF) to enhance the efficiency of large language models in complex reasoning tasks by allowing them to dynamically assess and adjust their reasoning depth, reducing redundancy.", "motivation": "To address the inefficiencies resulting from the redundancy in current chain-of-thought reasoning approaches in large language models.", "method": "The DR. SAF framework incorporates Boundary Self-Awareness Alignment, Adaptive Reward Management, and a Boundary Preservation Mechanism to optimize reasoning processes.", "result": "DR. SAF achieved a 49.27% reduction in total response tokens, a 6.59x gain in token efficiency, and a 5x reduction in training time, along with over 16% improvement in accuracy compared to traditional models during extreme training.", "conclusion": "The framework is effective for resource-limited settings and improves both efficiency and accuracy in large language models' reasoning tasks.", "key_contributions": ["Introduction of the Dynamic Reasoning-Boundary Self-Awareness Framework (DR. SAF)", "Significant reduction in response tokens and training time", "Improved token efficiency and accuracy during complex reasoning tasks"], "limitations": "", "keywords": ["dynamic reasoning", "large language models", "efficiency"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.11598", "pdf": "https://arxiv.org/pdf/2508.11598.pdf", "abs": "https://arxiv.org/abs/2508.11598", "title": "Representing Speech Through Autoregressive Prediction of Cochlear Tokens", "authors": ["Greta Tuckute", "Klemen Kotar", "Evelina Fedorenko", "Daniel L. K. Yamins"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "We introduce AuriStream, a biologically inspired model for encoding speech\nvia a two-stage framework inspired by the human auditory processing hierarchy.\nThe first stage transforms raw audio into a time-frequency representation based\non the human cochlea, from which we extract discrete \\textbf{cochlear tokens}.\nThe second stage applies an autoregressive sequence model over the cochlear\ntokens. AuriStream learns meaningful phoneme and word representations, and\nstate-of-the-art lexical semantics. AuriStream shows competitive performance on\ndiverse downstream SUPERB speech tasks. Complementing AuriStream's strong\nrepresentational capabilities, it generates continuations of audio which can be\nvisualized in a spectrogram space and decoded back into audio, providing\ninsights into the model's predictions. In summary, we present a two-stage\nframework for speech representation learning to advance the development of more\nhuman-like models that efficiently handle a range of speech-based tasks.", "AI": {"tldr": "AuriStream is a biologically inspired model for encoding speech using a two-stage framework that mimics human auditory processing, showing competitive performance on speech tasks and enabling audio continuation generation.", "motivation": "To advance the development of more human-like models capable of efficiently handling various speech-based tasks.", "method": "The model consists of two stages: transforming raw audio into a time-frequency representation to extract cochlear tokens, followed by applying an autoregressive sequence model over those tokens.", "result": "AuriStream achieves state-of-the-art performance on the SUPERB benchmark for speech tasks, demonstrating its capability in learning meaningful phoneme and word representations.", "conclusion": "The two-stage framework enhances speech representation learning, contributing to more efficient models for handling diverse speech-based applications.", "key_contributions": ["Introduction of a biologically inspired two-stage framework for speech representation learning.", "Generation of audio continuations that provide insights into model predictions.", "Competitive performance on diverse downstream speech tasks."], "limitations": "", "keywords": ["Speech representation", "Human auditory processing", "Machine learning", "Phoneme and word representation", "Audio generation"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2508.11605", "pdf": "https://arxiv.org/pdf/2508.11605.pdf", "abs": "https://arxiv.org/abs/2508.11605", "title": "Dataset Creation for Visual Entailment using Generative AI", "authors": ["Rob Reijtenbach", "Suzan Verberne", "Gijs Wijnholds"], "categories": ["cs.CL"], "comment": "NALOMA: Natural Logic meets Machine Learning workshop @ ESSLLI 2025", "summary": "In this paper we present and validate a new synthetic dataset for training\nvisual entailment models. Existing datasets for visual entailment are small and\nsparse compared to datasets for textual entailment. Manually creating datasets\nis labor-intensive. We base our synthetic dataset on the SNLI dataset for\ntextual entailment. We take the premise text from SNLI as input prompts in a\ngenerative image model, Stable Diffusion, creating an image to replace each\ntextual premise. We evaluate our dataset both intrinsically and extrinsically.\nFor extrinsic evaluation, we evaluate the validity of the generated images by\nusing them as training data for a visual entailment classifier based on CLIP\nfeature vectors. We find that synthetic training data only leads to a slight\ndrop in quality on SNLI-VE, with an F-score 0.686 compared to 0.703 when\ntrained on real data. We also compare the quality of our generated training\ndata to original training data on another dataset: SICK-VTE. Again, there is\nonly a slight drop in F-score: from 0.400 to 0.384. These results indicate that\nin settings with data sparsity, synthetic data can be a promising solution for\ntraining visual entailment models.", "AI": {"tldr": "This paper presents a synthetic dataset for training visual entailment models, validated against existing benchmarks.", "motivation": "Existing visual entailment datasets are small and sparse, necessitating a more efficient method of data generation.", "method": "A synthetic dataset is created using the SNLI dataset for textual entailment as prompts for a generative image model, Stable Diffusion, and evaluated both intrinsically and extrinsically.", "result": "The synthetic data shows a slight drop in quality compared to real data, with an F-score of 0.686 on SNLI-VE and 0.384 on SICK-VTE, indicating potential in data-sparse settings.", "conclusion": "Synthetic data can serve as a viable alternative for training visual entailment models when real data is limited.", "key_contributions": ["Introduction of a synthetic dataset for visual entailment", "Comparison of performance metrics with existing real datasets", "Evaluation demonstrating the effectiveness of synthetic data in sparse data environments"], "limitations": "The synthetic dataset still results in a slight drop in performance compared to real data.", "keywords": ["synthetic dataset", "visual entailment", "machine learning", "data sparsity", "generative model"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2508.11607", "pdf": "https://arxiv.org/pdf/2508.11607.pdf", "abs": "https://arxiv.org/abs/2508.11607", "title": "TinyTim: A Family of Language Models for Divergent Generation", "authors": ["Christopher J. Agostino"], "categories": ["cs.CL"], "comment": "7 pages, 3 figures, submitted to NeurIPS Creative AI track, code and\n  model available at https://hf.co/npc-worldwide/TinyTimV1", "summary": "This work introduces TinyTim, a family of large language models fine-tuned on\nJames Joyce's `Finnegans Wake'. Through quantitative evaluation against\nbaseline models, we demonstrate that TinyTim V1 produces a statistically\ndistinct generative profile characterized by high lexical diversity and low\nsemantic coherence. These findings are interpreted through theories of\ncreativity and complex problem-solving, arguing that such specialized models\ncan function as divergent knowledge sources within more extensive creative\narchitectures, powering automated discovery mechanisms in diverse settings.", "AI": {"tldr": "TinyTim, a family of large language models inspired by 'Finnegans Wake', showcases high lexical diversity but low semantic coherence, suggesting potential for creativity and automated discovery.", "motivation": "To explore how fine-tuning language models on specific literary texts can enhance creative outputs and problem-solving capabilities.", "method": "Quantitative evaluation of TinyTim V1 against baseline models to assess generative profiles.", "result": "TinyTim V1 demonstrates a distinct generative profile with high lexical diversity and low semantic coherence compared to existing models.", "conclusion": "Specialized language models like TinyTim can serve as unique knowledge sources in larger creative frameworks, aiding in automated discovery.", "key_contributions": ["Introduction of TinyTim, a model tuned on a literary text.", "Empirical evidence of high lexical diversity and low semantic coherence.", "Proposal of specialized models as resources for creativity and problem-solving.", "Availability of code and model to facilitate further study."], "limitations": "", "keywords": ["Large Language Models", "Creativity", "Automated Discovery", "Text Generation", "NeurIPS"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2402.18013", "pdf": "https://arxiv.org/pdf/2402.18013.pdf", "abs": "https://arxiv.org/abs/2402.18013", "title": "A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems", "authors": ["Zihao Yi", "Jiarui Ouyang", "Zhe Xu", "Yuwen Liu", "Tianhao Liao", "Haohao Luo", "Ying Shen"], "categories": ["cs.CL", "cs.AI"], "comment": "35 pages, 10 figures, ACM Computing Surveys", "summary": "This survey provides a comprehensive review of research on multi-turn\ndialogue systems, with a particular focus on multi-turn dialogue systems based\non large language models (LLMs). This paper aims to (a) give a summary of\nexisting LLMs and approaches for adapting LLMs to downstream tasks; (b)\nelaborate recent advances in multi-turn dialogue systems, covering both\nLLM-based open-domain dialogue (ODD) and task-oriented dialogue (TOD) systems,\nalong with datasets and evaluation metrics; (c) discuss some future emphasis\nand recent research problems arising from the development of LLMs and the\nincreasing demands on multi-turn dialogue systems.", "AI": {"tldr": "This survey reviews research on multi-turn dialogue systems, particularly those utilizing large language models (LLMs), summarizing adaptations, advances, datasets, and evaluation metrics.", "motivation": "To provide an overview of the state of research on multi-turn dialogue systems and the role of LLMs in this field.", "method": "The survey discusses existing LLM adaptations for various tasks, recent advancements in both open-domain and task-oriented dialogue systems, and future research directions.", "result": "The paper covers significant findings in LLM-based dialogue systems, including datasets and evaluation metrics used in assessing these systems.", "conclusion": "The study highlights the current challenges and future research areas needed to enhance multi-turn dialogue systems as LLM technology progresses.", "key_contributions": ["Comprehensive summary of existing LLMs for dialogue systems", "Insights into advancements in open-domain and task-oriented dialogues", "Discussion of future research challenges in the domain"], "limitations": "", "keywords": ["multi-turn dialogue systems", "large language models", "dialogue systems", "task-oriented dialogue", "open-domain dialogue"], "importance_score": 9, "read_time_minutes": 35}}
{"id": "2410.01671", "pdf": "https://arxiv.org/pdf/2410.01671.pdf", "abs": "https://arxiv.org/abs/2410.01671", "title": "Bridging Context Gaps: Leveraging Coreference Resolution for Long Contextual Understanding", "authors": ["Yanming Liu", "Xinyue Peng", "Jiannan Cao", "Yanxin Shen", "Tianyu Du", "Sheng Cheng", "Xun Wang", "Jianwei Yin", "Xuhong Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "ICLR 2025 camera ready version, with second updated metadata", "summary": "Large language models (LLMs) have shown remarkable capabilities in natural\nlanguage processing; however, they still face difficulties when tasked with\nunderstanding lengthy contexts and executing effective question answering.\nThese challenges often arise due to the complexity and ambiguity present in\nlonger texts. To enhance the performance of LLMs in such scenarios, we\nintroduce the Long Question Coreference Adaptation (LQCA) method. This\ninnovative framework focuses on coreference resolution tailored to long\ncontexts, allowing the model to identify and manage references effectively. The\nLQCA method encompasses four key steps: resolving coreferences within\nsub-documents, computing the distances between mentions, defining a\nrepresentative mention for coreference, and answering questions through mention\nreplacement. By processing information systematically, the framework provides\neasier-to-handle partitions for LLMs, promoting better understanding.\nExperimental evaluations on a range of LLMs and datasets have yielded positive\nresults, with a notable improvements on OpenAI-o1-mini and GPT-4o models,\nhighlighting the effectiveness of leveraging coreference resolution to bridge\ncontext gaps in question answering. Our code is public at\nhttps://github.com/OceannTwT/LQCA.", "AI": {"tldr": "The paper presents the Long Question Coreference Adaptation (LQCA) method, which enhances the performance of large language models (LLMs) on lengthy texts by improving coreference resolution.", "motivation": "Address the difficulties LLMs face in understanding lengthy contexts and executing effective question answering due to complexity and ambiguity.", "method": "The LQCA method involves four key steps: resolving coreferences within sub-documents, computing distances between mentions, defining a representative mention for coreference, and answering questions through mention replacement.", "result": "Experimental evaluations show notable improvements in performance on OpenAI-o1-mini and GPT-4o models after applying the LQCA method.", "conclusion": "The LQCA method effectively bridges context gaps in question answering, promoting better understanding for LLMs.", "key_contributions": ["Introduction of the LQCA method for coreference resolution in long contexts", "Systematic processing of information to improve LLM performance", "Publicly available code for reproducibility"], "limitations": "", "keywords": ["long language models", "coreference resolution", "question answering"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2412.11736", "pdf": "https://arxiv.org/pdf/2412.11736.pdf", "abs": "https://arxiv.org/abs/2412.11736", "title": "Personalized LLM for Generating Customized Responses to the Same Query from Different Users", "authors": ["Hang Zeng", "Chaoyue Niu", "Fan Wu", "Chengfei Lv", "Guihai Chen"], "categories": ["cs.CL"], "comment": "Accepted by CIKM'25", "summary": "Existing work on large language model (LLM) personalization assigned\ndifferent responding roles to LLMs, but overlooked the diversity of queriers.\nIn this work, we propose a new form of querier-aware LLM personalization,\ngenerating different responses even for the same query from different queriers.\nWe design a dual-tower model architecture with a cross-querier general encoder\nand a querier-specific encoder. We further apply contrastive learning with\nmulti-view augmentation, pulling close the dialogue representations of the same\nquerier, while pulling apart those of different queriers. To mitigate the\nimpact of query diversity on querier-contrastive learning, we cluster the\ndialogues based on query similarity and restrict the scope of contrastive\nlearning within each cluster. To address the lack of datasets designed for\nquerier-aware personalization, we also build a multi-querier dataset from\nEnglish and Chinese scripts, as well as WeChat records, called MQDialog,\ncontaining 173 queriers and 12 responders. Extensive evaluations demonstrate\nthat our design significantly improves the quality of personalized response\ngeneration, achieving relative improvement of 8.4% to 48.7% in ROUGE-L scores\nand winning rates ranging from 54% to 82% compared with various baseline\nmethods.", "AI": {"tldr": "This paper presents a querier-aware approach to personalized response generation in large language models (LLMs), introducing a dual-tower model and a new dataset, MQDialog.", "motivation": "To improve the personalization of LLM responses by accounting for the diversity of queriers rather than treating queries as uniform.", "method": "A dual-tower model architecture is designed with a cross-querier general encoder and a querier-specific encoder, utilizing contrastive learning with multi-view augmentation to enhance dialogue representation.", "result": "The proposed method achieves improvements of 8.4% to 48.7% in ROUGE-L scores and a winning rate between 54% to 82% over various baselines.", "conclusion": "The study demonstrates significant advancements in personalized response generation quality by incorporating querier-specific data.", "key_contributions": ["Introduction of querier-aware LLM personalization", "Development of MQDialog dataset with 173 queriers and 12 responders", "Implementation of dual-tower model architecture for personalized response generation"], "limitations": "", "keywords": ["Large Language Models", "Personalization", "Contrastive Learning", "Human-Computer Interaction", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.12052", "pdf": "https://arxiv.org/pdf/2502.12052.pdf", "abs": "https://arxiv.org/abs/2502.12052", "title": "A Dual-Perspective NLG Meta-Evaluation Framework with Automatic Benchmark and Better Interpretability", "authors": ["Xinyu Hu", "Mingqi Gao", "Li Lin", "Zhenghan Yu", "Xiaojun Wan"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025", "summary": "In NLG meta-evaluation, evaluation metrics are typically assessed based on\ntheir consistency with humans. However, we identify some limitations in\ntraditional NLG meta-evaluation approaches, such as issues in handling human\nratings and ambiguous selections of correlation measures, which undermine the\neffectiveness of meta-evaluation. In this work, we propose a dual-perspective\nNLG meta-evaluation framework that focuses on different evaluation\ncapabilities, thereby providing better interpretability. In addition, we\nintroduce a method of automatically constructing the corresponding benchmarks\nwithout requiring new human annotations. Furthermore, we conduct experiments\nwith 16 representative LLMs as the evaluators based on our proposed framework,\ncomprehensively analyzing their evaluation performance from different\nperspectives.", "AI": {"tldr": "This paper proposes a dual-perspective framework for NLG meta-evaluation to enhance interpretability and address limitations in traditional approaches.", "motivation": "To address the limitations in traditional NLG meta-evaluation methods, particularly in handling human ratings and correlation measures.", "method": "The authors introduce a dual-perspective NLG meta-evaluation framework and a benchmark construction method that does not require new human annotations. Experiments were conducted using 16 LLMs as evaluators.", "result": "The experiments provide a comprehensive analysis of LLM performance in meta-evaluation from various perspectives based on the proposed framework.", "conclusion": "The dual-perspective framework enhances interpretability and offers a robust approach to NLG meta-evaluation without the dependency on new human ratings.", "key_contributions": ["Development of a dual-perspective NLG meta-evaluation framework", "Method for constructing benchmarks without new human annotations", "Evaluation of 16 LLMs using the proposed framework."], "limitations": "", "keywords": ["NLG", "meta-evaluation", "LLM", "framework", "machine learning"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2502.16636", "pdf": "https://arxiv.org/pdf/2502.16636.pdf", "abs": "https://arxiv.org/abs/2502.16636", "title": "Visual-RAG: Benchmarking Text-to-Image Retrieval Augmented Generation for Visual Knowledge Intensive Queries", "authors": ["Yin Wu", "Quanyu Long", "Jing Li", "Jianfei Yu", "Wenya Wang"], "categories": ["cs.CL", "cs.CV"], "comment": "21 pages, 6 figures, 17 tables", "summary": "Retrieval-augmented generation (RAG) is a paradigm that augments large\nlanguage models (LLMs) with external knowledge to tackle knowledge-intensive\nquestion answering. While several benchmarks evaluate Multimodal LLMs (MLLMs)\nunder Multimodal RAG settings, they predominantly retrieve from textual corpora\nand do not explicitly assess how models exploit visual evidence during\ngeneration. Consequently, there still lacks benchmark that isolates and\nmeasures the contribution of retrieved images in RAG. We introduce Visual-RAG,\na question-answering benchmark that targets visually grounded,\nknowledge-intensive questions. Unlike prior work, Visual-RAG requires\ntext-to-image retrieval and the integration of retrieved clue images to extract\nvisual evidence for answer generation. With Visual-RAG, we evaluate 5\nopen-source and 3 proprietary MLLMs, showcasing that images provide strong\nevidence in augmented generation. However, even state-of-the-art models\nstruggle to efficiently extract and utilize visual knowledge. Our results\nhighlight the need for improved visual retrieval, grounding, and attribution in\nmultimodal RAG systems.", "AI": {"tldr": "Visual-RAG introduces a benchmark for evaluating the role of visual evidence in retrieval-augmented generation with multimodal large language models.", "motivation": "To assess how multimodal large language models utilize visual evidence in knowledge-intensive question answering, highlighting the lack of existing benchmarks that measure this aspect.", "method": "The authors propose Visual-RAG, which requires text-to-image retrieval and integrates retrieved images to support answer generation, thus isolating the contribution of visual data in the RAG process.", "result": "Evaluation of 5 open-source and 3 proprietary multimodal language models shows that while images enhance generation, even the best models struggle with effective visual knowledge extraction and utilization.", "conclusion": "The study emphasizes the necessity for advancements in visual retrieval, grounding, and attribution within multimodal retrieval-augmented generation frameworks.", "key_contributions": ["Introduction of Visual-RAG benchmark for visually grounded questions", "Assessment of image contribution to answer generation in MLLMs", "Highlighting the challenges in visual knowledge extraction for state-of-the-art models"], "limitations": "The benchmark primarily focuses on the visual component and may not cover other modalities or aspects of question answering comprehensively.", "keywords": ["retrieval-augmented generation", "visual grounding", "multimodal language models"], "importance_score": 9, "read_time_minutes": 30}}
{"id": "2503.01307", "pdf": "https://arxiv.org/pdf/2503.01307.pdf", "abs": "https://arxiv.org/abs/2503.01307", "title": "Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs", "authors": ["Kanishk Gandhi", "Ayush Chakravarthy", "Anikait Singh", "Nathan Lile", "Noah D. Goodman"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Test-time inference has emerged as a powerful paradigm for enabling language\nmodels to ``think'' longer and more carefully about complex challenges, much\nlike skilled human experts. While reinforcement learning (RL) can drive\nself-improvement in language models on verifiable tasks, some models exhibit\nsubstantial gains while others quickly plateau. For instance, we find that\nQwen-2.5-3B far exceeds Llama-3.2-3B under identical RL training for the game\nof Countdown. This discrepancy raises a critical question: what intrinsic\nproperties enable effective self-improvement? We introduce a framework to\ninvestigate this question by analyzing four key cognitive behaviors --\nverification, backtracking, subgoal setting, and backward chaining -- that both\nexpert human problem solvers and successful language models employ. Our study\nreveals that Qwen naturally exhibits these reasoning behaviors, whereas Llama\ninitially lacks them. In systematic experimentation with controlled behavioral\ndatasets, we find that priming Llama with examples containing these reasoning\nbehaviors enables substantial improvements during RL, matching or exceeding\nQwen's performance. Importantly, the presence of reasoning behaviors, rather\nthan correctness of answers, proves to be the critical factor -- models primed\nwith incorrect solutions containing proper reasoning patterns achieve\ncomparable performance to those trained on correct solutions. Finally,\nleveraging continued pretraining with OpenWebMath data, filtered to amplify\nreasoning behaviors, enables the Llama model to match Qwen's self-improvement\ntrajectory. Our findings establish a fundamental relationship between initial\nreasoning behaviors and the capacity for improvement, explaining why some\nlanguage models effectively utilize additional computation while others\nplateau.", "AI": {"tldr": "This paper explores intrinsic properties that enhance the self-improvement of language models through a framework analyzing cognitive behaviors like verification and backtracking.", "motivation": "To understand what intrinsic properties enable effective self-improvement in language models during reinforcement learning (RL) tasks.", "method": "The authors analyze cognitive behaviors (verification, backtracking, subgoal setting, backward chaining) in language models; they conducted experiments comparing Qwen and Llama models under RL training.", "result": "Qwen-2.5-3B outperforms Llama-3.2-3B in RL tasks, especially after Llama is primed with reasoning behaviors, leading to significant performance improvements.", "conclusion": "The presence of reasoning behaviors is crucial for models to improve; even incorrect solutions with proper reasoning patterns led to similar performance levels, and continued pretraining enhanced Llama's capabilities.", "key_contributions": ["Introduced a framework to analyze cognitive behaviors in language models.", "Demonstrated that reasoning behaviors are essential for self-improvement in RL tasks.", "Showed that priming with reasoning behaviors can significantly enhance performance, regardless of correctness."], "limitations": "", "keywords": ["language models", "reasoning behaviors", "reinforcement learning", "self-improvement", "cognitive behaviors"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2503.17811", "pdf": "https://arxiv.org/pdf/2503.17811.pdf", "abs": "https://arxiv.org/abs/2503.17811", "title": "Feather-SQL: A Lightweight NL2SQL Framework with Dual-Model Collaboration Paradigm for Small Language Models", "authors": ["Wenqi Pei", "Hailing Xu", "Hengyuan Zhao", "Shizheng Hou", "Han Chen", "Zining Zhang", "Pingyi Luo", "Bingsheng He"], "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": "DL4C @ ICLR 2025", "summary": "Natural Language to SQL (NL2SQL) has seen significant advancements with large\nlanguage models (LLMs). However, these models often depend on closed-source\nsystems and high computational resources, posing challenges in data privacy and\ndeployment. In contrast, small language models (SLMs) struggle with NL2SQL\ntasks, exhibiting poor performance and incompatibility with existing\nframeworks. To address these issues, we introduce Feather-SQL, a new\nlightweight framework tailored for SLMs. Feather-SQL improves SQL executability\nand accuracy through 1) schema pruning and linking, 2) multi-path and\nmulti-candidate generation. Additionally, we introduce the 1+1 Model\nCollaboration Paradigm, which pairs a strong general-purpose chat model with a\nfine-tuned SQL specialist, combining strong analytical reasoning with\nhigh-precision SQL generation. Experimental results on BIRD demonstrate that\nFeather-SQL improves NL2SQL performance on SLMs, with around 10% boost for\nmodels without fine-tuning. The proposed paradigm raises the accuracy ceiling\nof SLMs to 54.76%, highlighting its effectiveness.", "AI": {"tldr": "Feather-SQL is a lightweight framework for improving NL2SQL performance in small language models, addressing issues related to data privacy and deployment.", "motivation": "To overcome the limitations of large language models in NL2SQL tasks regarding data privacy and performance of small language models.", "method": "Introducing Feather-SQL, which utilizes schema pruning, multi-path and multi-candidate generation, coupled with a collaboration of a general chat model and a SQL specialist.", "result": "Feather-SQL demonstrates a 10% improvement in NL2SQL performance for small language models, raising their accuracy to 54.76%.", "conclusion": "Feather-SQL effectively enhances NL2SQL capabilities in small language models while ensuring data privacy.", "key_contributions": ["Development of Feather-SQL framework for SLMs", "1+1 Model Collaboration Paradigm", "Demonstrated significant performance improvements on BIRD dataset"], "limitations": "", "keywords": ["Natural Language to SQL", "Small Language Models", "Feather-SQL"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2506.06371", "pdf": "https://arxiv.org/pdf/2506.06371.pdf", "abs": "https://arxiv.org/abs/2506.06371", "title": "Relationship Detection on Tabular Data Using Statistical Analysis and Large Language Models", "authors": ["Panagiotis Koletsis", "Christos Panagiotopoulos", "Georgios Th. Papadopoulos", "Vasilis Efthymiou"], "categories": ["cs.CL"], "comment": null, "summary": "Over the past few years, table interpretation tasks have made significant\nprogress due to their importance and the introduction of new technologies and\nbenchmarks in the field. This work experiments with a hybrid approach for\ndetecting relationships among columns of unlabeled tabular data, using a\nKnowledge Graph (KG) as a reference point, a task known as CPA. This approach\nleverages large language models (LLMs) while employing statistical analysis to\nreduce the search space of potential KG relations. The main modules of this\napproach for reducing the search space are domain and range constraints\ndetection, as well as relation co-appearance analysis. The experimental\nevaluation on two benchmark datasets provided by the SemTab challenge assesses\nthe influence of each module and the effectiveness of different\nstate-of-the-art LLMs at various levels of quantization. The experiments were\nperformed, as well as at different prompting techniques. The proposed\nmethodology, which is publicly available on github, proved to be competitive\nwith state-of-the-art approaches on these datasets.", "AI": {"tldr": "This paper presents a hybrid approach for detecting relationships in unlabeled tabular data using Knowledge Graphs and large language models.", "motivation": "To improve the performance of table interpretation tasks by leveraging new technologies and benchmarks in the field.", "method": "A hybrid methodology that combines Knowledge Graphs with large language models and statistical analysis to detect relationships among columns of tabular data.", "result": "The proposed approach showed competitive results on benchmark datasets from the SemTab challenge, highlighting the effectiveness of its modules and prompting techniques.", "conclusion": "The methodology is effective in reducing the search space for potential relationships in tabular data and is publicly available for use.", "key_contributions": ["Introduces a hybrid approach combining Knowledge Graphs and large language models for tabular data interpretation.", "Implements domain and range constraints detection and relation co-appearance analysis to enhance relationship detection.", "Demonstrates competitive performance against state-of-the-art methods on benchmark datasets."], "limitations": "", "keywords": ["table interpretation", "Knowledge Graph", "large language models", "statistical analysis", "benchmark datasets"], "importance_score": 7, "read_time_minutes": 15}}
