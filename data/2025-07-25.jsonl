{"id": "2507.17753", "pdf": "https://arxiv.org/pdf/2507.17753.pdf", "abs": "https://arxiv.org/abs/2507.17753", "title": "Exploring Communication Strategies for Collaborative LLM Agents in Mathematical Problem-Solving", "authors": ["Liang Zhang", "Xiaoming Zhai", "Jionghao Lin", "Jionghao Lin", "Jennifer Kleiman", "Diego Zapata-Rivera", "Carol Forsyth", "Yang Jiang", "Xiangen Hu", "Arthur C. Graesser"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "Large Language Model (LLM) agents are increasingly utilized in AI-aided\neducation to support tutoring and learning. Effective communication strategies\namong LLM agents improve collaborative problem-solving efficiency and\nfacilitate cost-effective adoption in education. However, little research has\nsystematically evaluated the impact of different communication strategies on\nagents' problem-solving. Our study examines four communication modes,\n\\textit{teacher-student interaction}, \\textit{peer-to-peer collaboration},\n\\textit{reciprocal peer teaching}, and \\textit{critical debate}, in a\ndual-agent, chat-based mathematical problem-solving environment using the\nOpenAI GPT-4o model. Evaluated on the MATH dataset, our results show that\ndual-agent setups outperform single agents, with \\textit{peer-to-peer\ncollaboration} achieving the highest accuracy. Dialogue acts like statements,\nacknowledgment, and hints play a key role in collaborative problem-solving.\nWhile multi-agent frameworks enhance computational tasks, effective\ncommunication strategies are essential for tackling complex problems in AI\neducation."}
{"id": "2507.17754", "pdf": "https://arxiv.org/pdf/2507.17754.pdf", "abs": "https://arxiv.org/abs/2507.17754", "title": "A Custom-Built Ambient Scribe Reduces Cognitive Load and Documentation Burden for Telehealth Clinicians", "authors": ["Justin Morse", "Kurt Gilbert", "Kyle Shin", "Rick Cooke", "Peyton Rose", "Jack Sullivan", "Angelo Sisante"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "Clinician burnout has motivated the growing adoption of ambient medical\nscribes in the clinic. In this work, we introduce a custom-built ambient scribe\napplication integrated into the EHR system at Included Health, a personalized\nall-in-one healthcare company offering telehealth services. The application\nuses Whisper for transcription and a modular in-context learning pipeline with\nGPT-4o to automatically generate SOAP notes and patient instructions. Testing\non mock visit data shows that the notes generated by the application exceed the\nquality of expert-written notes as determined by an LLM-as-a-judge. The\napplication has been widely adopted by the clinical practice, with over 540\nclinicians at Included Health using the application at least once. 94% (n = 63)\nof surveyed clinicians report reduced cognitive load during visits and 97% (n =\n66) report less documentation burden when using the application. Additionally,\nwe show that post-processing notes with a fine-tuned BART model improves\nconciseness. These findings highlight the potential for AI systems to ease\nadministrative burdens and support clinicians in delivering efficient,\nhigh-quality care."}
{"id": "2507.17755", "pdf": "https://arxiv.org/pdf/2507.17755.pdf", "abs": "https://arxiv.org/abs/2507.17755", "title": "Between Filters and Feeds: Investigating Douyin and WeChat's Influence on Chinese Adolescent Body Image", "authors": ["Jianfeng Lan", "Yingjia Huang"], "categories": ["cs.HC", "cs.CY", "cs.SI"], "comment": null, "summary": "In the digital era, social media platforms play a pivotal role in shaping\nadolescents' body image perceptions. This study examines how Douyin and WeChat,\ntwo contrasting Chinese social media platforms, influence body image among\nChinese male adolescents. Employing a platformization perspective, we surveyed\n395 male adolescents aged 10 to 24 using the Multidimensional Body-Self\nRelations Questionnaire-Appearance Scales (MBSRQ-AS) to assess self-evaluation\nand body satisfaction. Our findings reveal that Douyin usage is significantly\ncorrelated with appearance evaluation and body area satisfaction, while WeChat\nusage shows no significant correlation with any body image dimensions. These\nresults suggest that Douyin's algorithm-driven, video-centric environment\nintensifies exposure to idealized body standards, impacting users at a\ncognitive level. This study underscores the importance of considering\nplatform-specific characteristics in understanding social media's impact on\nbody image. It contributes to the broader discourse on how technological design\nand content modalities mediate psychological outcomes, offering insights for\naddressing body image concerns among male adolescents in China."}
{"id": "2507.17756", "pdf": "https://arxiv.org/pdf/2507.17756.pdf", "abs": "https://arxiv.org/abs/2507.17756", "title": "Insights from Railway Professionals: Rethinking Railway assumptions regarding safety and autonomy", "authors": ["Josh Hunter", "John McDermid", "Simon Burton"], "categories": ["cs.HC", "cs.AI"], "comment": "9 pages, 3 figures, published in European Dependable Computing\n  Conference 2025", "summary": "This study investigates how railway professionals perceive safety as a\nconcept within rail, with the intention to help inform future technological\ndevelopments within the industry. Through a series of interviews with drivers,\nroute planners,and administrative personnel, the research explores the\ncurrentstate of safety practices, the potential for automation and the\nunderstanding of the railway as a system of systems. Key findings highlight a\ncautious attitude towards automation, a preference for assistive technologies,\nand a complex understanding of safety that integrates human, systematic and\ntechnological factors. The study also addresses the limitations of transferring\nautomotive automation technologies to railways and the need for a\nrailway-specific causation model to better evaluate and enhance safety in an\nevolving technological landscape. This study aims to bridge thegap between\ncontemporary research and practical applications, contributing to the\ndevelopment of more effective safety metrics."}
{"id": "2507.17842", "pdf": "https://arxiv.org/pdf/2507.17842.pdf", "abs": "https://arxiv.org/abs/2507.17842", "title": "Shop-R1: Rewarding LLMs to Simulate Human Behavior in Online Shopping via Reinforcement Learning", "authors": ["Yimeng Zhang", "Tian Wang", "Jiri Gesi", "Ziyi Wang", "Yuxuan Lu", "Jiacheng Lin", "Sinong Zhan", "Vianne Gao", "Ruochen Jiao", "Junze Liu", "Kun Qian", "Yuxin Tang", "Ran Xue", "Houyu Zhang", "Qingjun Cui", "Yufan Guo", "Dakuo Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have recently demonstrated strong potential in\ngenerating 'believable human-like' behavior in web environments. Prior work has\nexplored augmenting training data with LLM-synthesized rationales and applying\nsupervised fine-tuning (SFT) to enhance reasoning ability, which in turn can\nimprove downstream action prediction. However, the performance of such\napproaches remains inherently bounded by the reasoning capabilities of the\nmodel used to generate the rationales. In this paper, we introduce Shop-R1, a\nnovel reinforcement learning (RL) framework aimed at enhancing the reasoning\nability of LLMs for simulation of real human behavior in online shopping\nenvironments Specifically, Shop-R1 decomposes the human behavior simulation\ntask into two stages: rationale generation and action prediction, each guided\nby distinct reward signals. For rationale generation, we leverage internal\nmodel signals (e.g., logit distributions) to guide the reasoning process in a\nself-supervised manner. For action prediction, we propose a hierarchical reward\nstructure with difficulty-aware scaling to prevent reward hacking and enable\nfine-grained reward assignment. This design evaluates both high-level action\ntypes and the correctness of fine-grained sub-action details (attributes and\nvalues), rewarding outputs proportionally to their difficulty. Experimental\nresults show that our method achieves a relative improvement of over 65%\ncompared to the baseline."}
{"id": "2507.17757", "pdf": "https://arxiv.org/pdf/2507.17757.pdf", "abs": "https://arxiv.org/abs/2507.17757", "title": "BrisT1D Dataset: Young Adults with Type 1 Diabetes in the UK using Smartwatches", "authors": ["Sam Gordon James", "Miranda Elaine Glynis Armstrong", "Aisling Ann O'Kane", "Harry Emerson", "Zahraa S. Abdallah"], "categories": ["cs.HC", "cs.LG"], "comment": "13 pages, 14 figures", "summary": "Background: Type 1 diabetes (T1D) has seen a rapid evolution in management\ntechnology and forms a useful case study for the future management of other\nchronic conditions. Further development of this management technology requires\nan exploration of its real-world use and the potential of additional data\nstreams. To facilitate this, we contribute the BrisT1D Dataset to the growing\nnumber of public T1D management datasets. The dataset was developed from a\nlongitudinal study of 24 young adults in the UK who used a smartwatch alongside\ntheir usual T1D management. Findings: The BrisT1D dataset features both device\ndata from the T1D management systems and smartwatches used by participants, as\nwell as transcripts of monthly interviews and focus groups conducted during the\nstudy. The device data is provided in a processed state, for usability and more\nrapid analysis, and in a raw state, for in-depth exploration of novel insights\ncaptured in the study. Conclusions: This dataset has a range of potential\napplications. The quantitative elements can support blood glucose prediction,\nhypoglycaemia prediction, and closed-loop algorithm development. The\nqualitative elements enable the exploration of user experiences and opinions,\nas well as broader mixed-methods research into the role of smartwatches in T1D\nmanagement."}
{"id": "2507.17849", "pdf": "https://arxiv.org/pdf/2507.17849.pdf", "abs": "https://arxiv.org/abs/2507.17849", "title": "Dynamic and Generalizable Process Reward Modeling", "authors": ["Zhangyue Yin", "Qiushi Sun", "Zhiyuan Zeng", "Qinyuan Cheng", "Xipeng Qiu", "Xuanjing Huang"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 Main", "summary": "Process Reward Models (PRMs) are crucial for guiding Large Language Models\n(LLMs) in complex scenarios by providing dense reward signals. However,\nexisting PRMs primarily rely on heuristic approaches, which struggle with\ncross-domain generalization. While LLM-as-judge has been proposed to provide\ngeneralized rewards, current research has focused mainly on feedback results,\noverlooking the meaningful guidance embedded within the text. Additionally,\nstatic and coarse-grained evaluation criteria struggle to adapt to complex\nprocess supervision. To tackle these challenges, we propose Dynamic and\nGeneralizable Process Reward Modeling (DG-PRM), which features a reward tree to\ncapture and store fine-grained, multi-dimensional reward criteria. DG-PRM\ndynamically selects reward signals for step-wise reward scoring. To handle\nmultifaceted reward signals, we pioneeringly adopt Pareto dominance estimation\nto identify discriminative positive and negative pairs. Experimental results\nshow that DG-PRM achieves stunning performance on prevailing benchmarks,\nsignificantly boosting model performance across tasks with dense rewards.\nFurther analysis reveals that DG-PRM adapts well to out-of-distribution\nscenarios, demonstrating exceptional generalizability."}
{"id": "2507.17759", "pdf": "https://arxiv.org/pdf/2507.17759.pdf", "abs": "https://arxiv.org/abs/2507.17759", "title": "DHMS: A Digital Hostel Management System Integrating Campus ChatBot, Predictive Intelligence, and Real-Time Automation", "authors": ["Riddhi Heda", "Sidhant Singh", "Umair Yasir", "Tanmay Jaiswal", "Anil Mokhade"], "categories": ["cs.HC"], "comment": null, "summary": "Traditional hostel management practices in academic institutions often suffer\nfrom inefficiencies, delays, and fragmented communication. These systems fail\nto meet the expectations of digitally native students and place a significant\noperational burden on hostel staff. This paper introduces DHMS (Digital Hostel\nManagement System), a modular and integrated platform designed to digitize and\nstreamline essential hostel management functions. DHMS leverages modern web\ntechnologies, artificial intelligence, and cloud infrastructure to automate\nroom allotment, grievance redressal, gate pass logistics, and communication via\na natural language chatbot. In simulation tests, DHMS achieved a 92% student\nsatisfaction rate in room allocation and maintained an average chatbot response\ntime below one second. Additional features include predictive analytics for\nproactive maintenance planning and sentiment analysis for feedback processing.\nWhile promising, the system requires further testing for integration across\nmultiple hostel blocks, user acceptance, scalability under load, and ERP\ncompatibility before campus-wide deployment. This work discusses the system\narchitecture, implementation approach, and factors critical to improving user\nexperience, administrative efficiency, and decision-making processes."}
{"id": "2507.17896", "pdf": "https://arxiv.org/pdf/2507.17896.pdf", "abs": "https://arxiv.org/abs/2507.17896", "title": "VeriMinder: Mitigating Analytical Vulnerabilities in NL2SQL", "authors": ["Shubham Mohole", "Sainyam Galhotra"], "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": null, "summary": "Application systems using natural language interfaces to databases (NLIDBs)\nhave democratized data analysis. This positive development has also brought\nforth an urgent challenge to help users who might use these systems without a\nbackground in statistical analysis to formulate bias-free analytical questions.\nAlthough significant research has focused on text-to-SQL generation accuracy,\naddressing cognitive biases in analytical questions remains underexplored. We\npresent VeriMinder, https://veriminder.ai, an interactive system for detecting\nand mitigating such analytical vulnerabilities. Our approach introduces three\nkey innovations: (1) a contextual semantic mapping framework for biases\nrelevant to specific analysis contexts (2) an analytical framework that\noperationalizes the Hard-to-Vary principle and guides users in systematic data\nanalysis (3) an optimized LLM-powered system that generates high-quality,\ntask-specific prompts using a structured process involving multiple candidates,\ncritic feedback, and self-reflection.\n  User testing confirms the merits of our approach. In direct user experience\nevaluation, 82.5% participants reported positively impacting the quality of the\nanalysis. In comparative evaluation, VeriMinder scored significantly higher\nthan alternative approaches, at least 20% better when considered for metrics of\nthe analysis's concreteness, comprehensiveness, and accuracy. Our system,\nimplemented as a web application, is set to help users avoid \"wrong question\"\nvulnerability during data analysis. VeriMinder code base with prompts,\nhttps://reproducibility.link/veriminder, is available as an MIT-licensed\nopen-source software to facilitate further research and adoption within the\ncommunity."}
{"id": "2507.17761", "pdf": "https://arxiv.org/pdf/2507.17761.pdf", "abs": "https://arxiv.org/abs/2507.17761", "title": "Co-constructing Explanations for AI Systems using Provenance", "authors": ["Jan-Christoph Kalo", "Fina Polat", "Shubha Guha", "Paul Groth"], "categories": ["cs.HC"], "comment": "5 pages", "summary": "Modern AI systems are complex workflows containing multiple components and\ndata sources. Data provenance provides the ability to interrogate and\npotentially explain the outputs of these systems. However, provenance is often\ntoo detailed and not contextualized for the user trying to understand the AI\nsystem. In this work, we present our vision for an interactive agent that works\ntogether with the user to co-construct an explanation that is simultaneously\nuseful to the user as well as grounded in data provenance. To illustrate this\nvision, we present: 1) an initial prototype of such an agent; and 2) a scalable\nevaluation framework based on user simulations and a large language model as a\njudge approach."}
{"id": "2507.17918", "pdf": "https://arxiv.org/pdf/2507.17918.pdf", "abs": "https://arxiv.org/abs/2507.17918", "title": "One Whisper to Grade Them All", "authors": ["Nhan Phan", "Anusha Porwal", "Yaroslav Getman", "Ekaterina Voskoboinik", "Tamás Grósz", "Mikko Kurimo"], "categories": ["cs.CL", "eess.AS"], "comment": "Accepted to SLaTE 2025 workshop", "summary": "We present an efficient end-to-end approach for holistic Automatic Speaking\nAssessment (ASA) of multi-part second-language tests, developed for the 2025\nSpeak & Improve Challenge. Our system's main novelty is the ability to process\nall four spoken responses with a single Whisper-small encoder, combine all\ninformation via a lightweight aggregator, and predict the final score. This\narchitecture removes the need for transcription and per-part models, cuts\ninference time, and makes ASA practical for large-scale Computer-Assisted\nLanguage Learning systems.\n  Our system achieved a Root Mean Squared Error (RMSE) of 0.384, outperforming\nthe text-based baseline (0.44) while using at most 168M parameters (about 70%\nof Whisper-small). Furthermore, we propose a data sampling strategy, allowing\nthe model to train on only 44.8% of the speakers in the corpus and still reach\n0.383 RMSE, demonstrating improved performance on imbalanced classes and strong\ndata efficiency."}
{"id": "2507.17774", "pdf": "https://arxiv.org/pdf/2507.17774.pdf", "abs": "https://arxiv.org/abs/2507.17774", "title": "Human-AI Co-Creation: A Framework for Collaborative Design in Intelligent Systems", "authors": ["Zhangqi Liu"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "As artificial intelligence (AI) continues to evolve from a back-end\ncomputational tool into an interactive, generative collaborator, its\nintegration into early-stage design processes demands a rethinking of\ntraditional workflows in human-centered design. This paper explores the\nemergent paradigm of human-AI co-creation, where AI is not merely used for\nautomation or efficiency gains, but actively participates in ideation, visual\nconceptualization, and decision-making. Specifically, we investigate the use of\nlarge language models (LLMs) like GPT-4 and multimodal diffusion models such as\nStable Diffusion as creative agents that engage designers in iterative cycles\nof proposal, critique, and revision."}
{"id": "2507.17944", "pdf": "https://arxiv.org/pdf/2507.17944.pdf", "abs": "https://arxiv.org/abs/2507.17944", "title": "Evaluating the Performance of AI Text Detectors, Few-Shot and Chain-of-Thought Prompting Using DeepSeek Generated Text", "authors": ["Hulayyil Alshammari", "Praveen Rao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have rapidly transformed the creation of written\nmaterials. LLMs have led to questions about writing integrity, thereby driving\nthe creation of artificial intelligence (AI) detection technologies.\nAdversarial attacks, such as standard and humanized paraphrasing, inhibit\ndetectors' ability to detect machine-generated text. Previous studies have\nmainly focused on ChatGPT and other well-known LLMs and have shown varying\naccuracy across detectors. However, there is a clear gap in the literature\nabout DeepSeek, a recently published LLM. Therefore, in this work, we\ninvestigate whether six generally accessible AI detection tools -- AI Text\nClassifier, Content Detector AI, Copyleaks, QuillBot, GPT-2, and GPTZero -- can\nconsistently recognize text generated by DeepSeek. The detectors were exposed\nto the aforementioned adversarial attacks. We also considered DeepSeek as a\ndetector by performing few-shot prompting and chain-of-thought reasoning (CoT)\nfor classifying AI and human-written text. We collected 49 human-authored\nquestion-answer pairs from before the LLM era and generated matching responses\nusing DeepSeek-v3, producing 49 AI-generated samples. Then, we applied\nadversarial techniques such as paraphrasing and humanizing to add 196 more\nsamples. These were used to challenge detector robustness and assess accuracy\nimpact. While QuillBot and Copyleaks showed near-perfect performance on\noriginal and paraphrased DeepSeek text, others -- particularly AI Text\nClassifier and GPT-2 -- showed inconsistent results. The most effective attack\nwas humanization, reducing accuracy to 71% for Copyleaks, 58% for QuillBot, and\n52% for GPTZero. Few-shot and CoT prompting showed high accuracy, with the best\nfive-shot result misclassifying only one of 49 samples (AI recall 96%, human\nrecall 100%)."}
{"id": "2507.17898", "pdf": "https://arxiv.org/pdf/2507.17898.pdf", "abs": "https://arxiv.org/abs/2507.17898", "title": "Same Data, Different Audiences: Using Personas to Scope a Supercomputing Job Queue Visualization", "authors": ["Connor Scully-Allison", "Kevin Menear", "Kristin Potter", "Andrew McNutt", "Katherine E. Isaacs", "Dmitry Duplyakin"], "categories": ["cs.HC"], "comment": "11 Pages, 4 figures", "summary": "Domain-specific visualizations sometimes focus on narrow, albeit important,\ntasks for one group of users. This focus limits the utility of a visualization\nto other groups working with the same data. While tasks elicited from other\ngroups can present a design pitfall if not disambiguated, they also present a\ndesign opportunity -- development of visualizations that support multiple\ngroups. This development choice presents a trade off of broadening the scope\nbut limiting support for the more narrow tasks of any one group, which in some\ncases can enhance the overall utility of the visualization. We investigate this\nscenario through a design study where we develop \\textit{Guidepost}, a\nnotebook-embedded visualization of supercomputer queue data that helps\nscientists assess supercomputer queue wait times, machine learning researchers\nunderstand prediction accuracy, and system maintainers analyze usage trends. We\nadapt the use of personas for visualization design from existing literature in\nthe HCI and software engineering domains and apply them in categorizing tasks\nbased on their uniqueness across the stakeholder personas. Under this model,\ntasks shared between all groups should be supported by interactive\nvisualizations and tasks unique to each group can be deferred to scripting with\nnotebook-embedded visualization design. We evaluate our visualization with nine\nexpert analysts organized into two groups: a \"research analyst\" group that uses\nsupercomputer queue data in their research (representing the Machine Learning\nresearchers and Jobs Data Analyst personas) and a \"supercomputer user\" group\nthat uses this data conditionally (representing the HPC User persona). We find\nthat our visualization serves our three stakeholder groups by enabling users to\nsuccessfully execute shared tasks with point-and-click interaction while\nfacilitating case-specific programmatic analysis workflows."}
{"id": "2507.17951", "pdf": "https://arxiv.org/pdf/2507.17951.pdf", "abs": "https://arxiv.org/abs/2507.17951", "title": "Are LLM Belief Updates Consistent with Bayes' Theorem?", "authors": ["Sohaib Imran", "Ihor Kendiukhov", "Matthew Broerman", "Aditya Thomas", "Riccardo Campanella", "Rob Lamb", "Peter M. Atkinson"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at the ICML 2025 Workshop on Assessing World Models", "summary": "Do larger and more capable language models learn to update their \"beliefs\"\nabout propositions more consistently with Bayes' theorem when presented with\nevidence in-context? To test this, we formulate a Bayesian Coherence\nCoefficient (BCC) metric and generate a dataset with which to measure the BCC.\nWe measure BCC for multiple pre-trained-only language models across five model\nfamilies, comparing against the number of model parameters, the amount of\ntraining data, and model scores on common benchmarks. Our results provide\nevidence for our hypothesis that larger and more capable pre-trained language\nmodels assign credences that are more coherent with Bayes' theorem. These\nresults have important implications for our understanding and governance of\nLLMs."}
{"id": "2507.17943", "pdf": "https://arxiv.org/pdf/2507.17943.pdf", "abs": "https://arxiv.org/abs/2507.17943", "title": "Automated Brake Onset Detection in Naturalistic Driving Data", "authors": ["Shu-Yuan Liu", "Johan Engström", "Gustav Markkula"], "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "Response timing measures play a crucial role in the assessment of automated\ndriving systems (ADS) in collision avoidance scenarios, including but not\nlimited to establishing human benchmarks and comparing ADS to human driver\nresponse performance. For example, measuring the response time (of a human\ndriver or ADS) to a conflict requires the determination of a stimulus onset and\na response onset. In existing studies, response onset relies on manual\nannotation or vehicle control signals such as accelerator and brake pedal\nmovements. These methods are not applicable when analyzing large scale data\nwhere vehicle control signals are not available. This holds in particular for\nthe rapidly expanding sets of ADS log data where the behavior of surrounding\nroad users is observed via onboard sensors. To advance evaluation techniques\nfor ADS and enable measuring response timing when vehicle control signals are\nnot available, we developed a simple and efficient algorithm, based on a\npiecewise linear acceleration model, to automatically estimate brake onset that\ncan be applied to any type of driving data that includes vehicle longitudinal\ntime series data. We also proposed a manual annotation method to identify brake\nonset and used it as ground truth for validation. R2 was used as a confidence\nmetric to measure the accuracy of the algorithm, and its classification\nperformance was analyzed using naturalistic collision avoidance data of both\nADS and humans, where our method was validated against human manual annotation.\nAlthough our algorithm is subject to certain limitations, it is efficient,\ngeneralizable, applicable to any road user and scenario types, and is highly\nconfigurable."}
{"id": "2507.17974", "pdf": "https://arxiv.org/pdf/2507.17974.pdf", "abs": "https://arxiv.org/abs/2507.17974", "title": "Natural Language Processing for Tigrinya: Current State and Future Directions", "authors": ["Fitsum Gaim", "Jong C. Park"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": null, "summary": "Despite being spoken by millions of people, Tigrinya remains severely\nunderrepresented in Natural Language Processing (NLP) research. This work\npresents a comprehensive survey of NLP research for Tigrinya, analyzing over 40\nstudies spanning more than a decade of work from 2011 to 2025. We\nsystematically review the current state of computational resources, models, and\napplications across ten distinct downstream tasks, including morphological\nprocessing, machine translation, speech recognition, and question-answering.\nOur analysis reveals a clear trajectory from foundational, rule-based systems\nto modern neural architectures, with progress consistently unlocked by resource\ncreation milestones. We identify key challenges rooted in Tigrinya's\nmorphological complexity and resource scarcity, while highlighting promising\nresearch directions, including morphology-aware modeling, cross-lingual\ntransfer, and community-centered resource development. This work serves as both\na comprehensive reference for researchers and a roadmap for advancing Tigrinya\nNLP. A curated metadata of the surveyed studies and resources is made publicly\navailable.\\footnote{Tigrinya NLP Anthology:\nhttps://github.com/fgaim/tigrinya-nlp-anthology."}
{"id": "2507.17985", "pdf": "https://arxiv.org/pdf/2507.17985.pdf", "abs": "https://arxiv.org/abs/2507.17985", "title": "Decoding Instructional Dialogue: Human-AI Collaborative Analysis of Teacher Use of AI Tool at Scale", "authors": ["Alex Liu", "Lief Esbenshade", "Shawon Sarkar", "Victor Tian", "Zachary Zhang", "Kevin He", "Min Sun"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "The integration of large language models (LLMs) into educational tools has\nthe potential to substantially impact how teachers plan instruction, support\ndiverse learners, and engage in professional reflection. Yet little is known\nabout how educators actually use these tools in practice and how their\ninteractions with AI can be meaningfully studied at scale. This paper presents\na human-AI collaborative methodology for large-scale qualitative analysis of\nover 140,000 educator-AI messages drawn from a generative AI platform used by\nK-12 teachers. Through a four-phase coding pipeline, we combined inductive\ntheme discovery, codebook development, structured annotation, and model\nbenchmarking to examine patterns of educator engagement and evaluate the\nperformance of LLMs in qualitative coding tasks. We developed a hierarchical\ncodebook aligned with established teacher evaluation frameworks, capturing\neducators' instructional goals, contextual needs, and pedagogical strategies.\nOur findings demonstrate that LLMs, particularly Claude 3.5 Haiku, can reliably\nsupport theme identification, extend human recognition in complex scenarios,\nand outperform open-weight models in both accuracy and structural reliability.\nThe analysis also reveals substantive patterns in how educators inquire AI to\nenhance instructional practices (79.7 percent of total conversations), create\nor adapt content (76.1 percent), support assessment and feedback loop (46.9\npercent), attend to student needs for tailored instruction (43.3 percent), and\nassist other professional responsibilities (34.2 percent), highlighting\nemerging AI-related competencies that have direct implications for teacher\npreparation and professional development. This study offers a scalable,\ntransparent model for AI-augmented qualitative research and provides\nfoundational insights into the evolving role of generative AI in educational\npractice."}
{"id": "2507.18013", "pdf": "https://arxiv.org/pdf/2507.18013.pdf", "abs": "https://arxiv.org/abs/2507.18013", "title": "Technical Report of TeleChat2, TeleChat2.5 and T1", "authors": ["Zihan Wang", "Xinzhang Liu", "Yitong Yao", "Chao Wang", "Yu Zhao", "Zhihao Yang", "Wenmin Deng", "Kaipeng Jia", "Jiaxin Peng", "Yuyao Huang", "Sishi Xiong", "Zhuo Jiang", "Kaidong Yu", "Xiaohui Hu", "Fubei Yao", "Ruiyu Fang", "Zhuoru Jiang", "Ruiting Song", "Qiyi Xie", "Rui Xue", "Xuewei He", "Yanlei Xue", "Zhu Yuan", "Zhaoxi Zhang", "Zilu Huang", "Shiquan Wang", "Xin Wang", "Hanming Wu", "Mingyuan Wang", "Xufeng Zhan", "Yuhan Sun", "Zhaohu Xing", "Yuhao Jiang", "Bingkai Yang", "Shuangyong Song", "Yongxiang Li", "Zhongjiang He", "Xuelong Li"], "categories": ["cs.CL", "I.2.7"], "comment": "32 pages, 5 figures", "summary": "We introduce the latest series of TeleChat models: \\textbf{TeleChat2},\n\\textbf{TeleChat2.5}, and \\textbf{T1}, offering a significant upgrade over\ntheir predecessor, TeleChat. Despite minimal changes to the model architecture,\nthe new series achieves substantial performance gains through enhanced training\nstrategies in both pre-training and post-training stages. The series begins\nwith \\textbf{TeleChat2}, which undergoes pretraining on 10 trillion\nhigh-quality and diverse tokens. This is followed by Supervised Fine-Tuning\n(SFT) and Direct Preference Optimization (DPO) to further enhance its\ncapabilities. \\textbf{TeleChat2.5} and \\textbf{T1} expand the pipeline by\nincorporating a continual pretraining phase with domain-specific datasets,\ncombined with reinforcement learning (RL) to improve performance in code\ngeneration and mathematical reasoning tasks. The \\textbf{T1} variant is\ndesigned for complex reasoning, supporting long Chain-of-Thought (CoT)\nreasoning and demonstrating substantial improvements in mathematics and coding.\nIn contrast, \\textbf{TeleChat2.5} prioritizes speed, delivering rapid\ninference. Both flagship models of \\textbf{T1} and \\textbf{TeleChat2.5} are\ndense Transformer-based architectures with 115B parameters, showcasing\nsignificant advancements in reasoning and general task performance compared to\nthe original TeleChat. Notably, \\textbf{T1-115B} outperform proprietary models\nsuch as OpenAI's o1-mini and GPT-4o. We publicly release \\textbf{TeleChat2},\n\\textbf{TeleChat2.5} and \\textbf{T1}, including post-trained versions with 35B\nand 115B parameters, to empower developers and researchers with\nstate-of-the-art language models tailored for diverse applications."}
{"id": "2507.17997", "pdf": "https://arxiv.org/pdf/2507.17997.pdf", "abs": "https://arxiv.org/abs/2507.17997", "title": "Evaluating judgment of spatial correlation in visual displays of scalar field distributions", "authors": ["Yayan Zhao", "Matthew Berger"], "categories": ["cs.HC"], "comment": null, "summary": "In this work we study the identification of spatial correlation in\ndistributions of 2D scalar fields, presented across different forms of visual\ndisplays. We study simple visual displays that directly show color-mapped\nscalar fields, namely those drawn from a distribution, and whether humans can\nidentify strongly correlated spatial regions in these displays. In this\nsetting, the recognition of correlation requires making judgments on a set of\nfields, rather than just one field. Thus, in our experimental design we compare\ntwo basic visualization designs: animation-based displays against juxtaposed\nviews of scalar fields, along different choices of color scales. Moreover, we\ninvestigate the impacts of the distribution itself, controlling for the level\nof spatial correlation and discriminability in spatial scales. Our study's\nresults illustrate the impacts of these distribution characteristics, while\nalso highlighting how different visual displays impact the types of judgments\nmade in assessing spatial correlation. Supplemental material is available at\nhttps://osf.io/zn4qy"}
{"id": "2507.18028", "pdf": "https://arxiv.org/pdf/2507.18028.pdf", "abs": "https://arxiv.org/abs/2507.18028", "title": "NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural KV Database", "authors": ["Weizhi Fei", "Hao Shi", "Jing Xu", "Jingchen Peng", "Jiazheng Li", "Jingzhao Zhang", "Bo Bai", "Wei Han", "Zhenyuan Chen", "Xueyan Niu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Efficiently editing knowledge stored in large language models (LLMs) enables\nmodel updates without large-scale training. One possible solution is\nLocate-and-Edit (L\\&E), allowing simultaneous modifications of a massive number\nof facts. However, such editing may compromise the general abilities of LLMs\nand even result in forgetting edited facts when scaling up to thousands of\nedits. In this paper, we model existing linear L\\&E methods as querying a\nKey-Value (KV) database. From this perspective, we then propose NeuralDB, an\nediting framework that explicitly represents the edited facts as a neural KV\ndatabase equipped with a non-linear gated retrieval module, % In particular,\nour gated module only operates when inference involves the edited facts,\neffectively preserving the general abilities of LLMs. Comprehensive experiments\ninvolving the editing of 10,000 facts were conducted on the ZsRE and\nCounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results\ndemonstrate that NeuralDB not only excels in editing efficacy, generalization,\nspecificity, fluency, and consistency, but also preserves overall performance\nacross six representative text understanding and generation tasks. Further\nexperiments indicate that NeuralDB maintains its effectiveness even when scaled\nto 100,000 facts (\\textbf{50x} more than in prior work)."}
{"id": "2507.18084", "pdf": "https://arxiv.org/pdf/2507.18084.pdf", "abs": "https://arxiv.org/abs/2507.18084", "title": "\"I Would Not Be This Version of Myself Today\": Elaborating on the Effects of Eudaimonic Gaming Experiences", "authors": ["Nisha Devasia", "Georgia Kenderova", "Michele Newman", "Julie Kientz", "Jin Ha Lee"], "categories": ["cs.HC"], "comment": "Accepted to CHI PLAY 2025", "summary": "While much of the research in digital games has emphasized hedonic\nexperiences, such as flow, enjoyment, and positive affect, recent years have\nseen increased interest in eudaimonic gaming experiences, typically\nmixed-affect and associated with personal meaningfulness and growth. The\nformation of such experiences in games is theorized to have four constituent\nelements: motivation, game use, experience, and effects. However, while the\nfirst three elements have been relatively well explored in the literature, the\neffects - and how they may influence positive individual outcomes - have been\nunderexplored thus far. To this end, in this work, we investigate the perceived\noutcomes of eudaimonic gaming and how different components of the experience\ninfluence these effects. We conducted a survey (n = 166) in which respondents\nrecounted meaningful gaming experiences and how they affected their present\nlives. We used a mixed-methods approach to classify effects and identify\nsignificant subcomponents of their formation. We contribute an empirical\nunderstanding of how meaningful gaming experiences can lead to positive\nreflective, learning, social, health, and career effects, extending current\ntheoretical models of eudaimonic gaming experiences and offering implications\nfor how researchers and practitioners might use these findings to promote\npositive outcomes for players."}
{"id": "2507.18043", "pdf": "https://arxiv.org/pdf/2507.18043.pdf", "abs": "https://arxiv.org/abs/2507.18043", "title": "GrAInS: Gradient-based Attribution for Inference-Time Steering of LLMs and VLMs", "authors": ["Duy Nguyen", "Archiki Prasad", "Elias Stengel-Eskin", "Mohit Bansal"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "21 pages. Code: https://github.com/duykhuongnguyen/GrAInS", "summary": "Inference-time steering methods offer a lightweight alternative to\nfine-tuning large language models (LLMs) and vision-language models (VLMs) by\nmodifying internal activations at test time without updating model weights.\nHowever, most existing approaches rely on fixed, global intervention vectors,\noverlook the causal influence of individual input tokens, and fail to leverage\ninformative gradients from the model's logits, particularly in multimodal\nsettings where visual and textual inputs contribute unevenly. To address these\nlimitations, we introduce GrAInS, an inference-time steering approach that\noperates across both language-only and vision-language models and tasks. GrAInS\nuses contrastive, gradient-based attribution via Integrated Gradients to\nidentify the top-k most influential tokens, both positively and negatively\nattributed based on their contribution to preferred versus dispreferred\noutputs. These tokens are then used to construct directional steering vectors\nthat capture semantic shifts from undesirable to desirable behavior. During\ninference, GrAInS adjusts hidden activations at transformer layers guided by\ntoken-level attribution signals, and normalizes activations to preserve\nrepresentational scale. This enables fine-grained, interpretable, and modular\ncontrol over model behavior, without retraining or auxiliary supervision.\nEmpirically, GrAInS consistently outperforms both fine-tuning and existing\nsteering baselines: it achieves a 13.22% accuracy gain on TruthfulQA using\nLlama-3.1-8B, reduces hallucination rates on MMHal-Bench from 0.624 to 0.514\nwith LLaVA-1.6-7B, and improves alignment win rates on SPA-VL by 8.11%, all\nwhile preserving the model's fluency and general capabilities."}
{"id": "2507.18085", "pdf": "https://arxiv.org/pdf/2507.18085.pdf", "abs": "https://arxiv.org/abs/2507.18085", "title": "Effects of variation in system responsiveness on user performance in virtual environments", "authors": ["Benjamin Watson", "Neff Walker", "William Ribarsky", "Victoria Spaulding"], "categories": ["cs.HC", "cs.ET"], "comment": null, "summary": "System responsiveness (SR) is defined as the elapsed time until a system\nresponds to user control. SR fluctuates over time, so it must be described\nstatistically with mean (MSR) and standard deviation (SDSR). In this paper, we\nexamine SR in virtual environments (VEs), outlining its components and methods\nof experimental measurement and manipulation. Three studies of MSR and SDSR\neffects on performance of grasp and placement tasks are then presented. The\nstudies used within-subjects designs with 11, 12, and 10 participants,\nrespectively. Results showed that SDSR affected performance only if it was\nabove 82 ms. Placement required more frequent visual feedback and was more\nsensitive to SR. We infer that VE designers need not tightly control SDSR and\nmay wish to vary SR control based on required visual feedback frequency. These\nresults may be used to improve the human-computer interface in a wide range of\ninteractive graphical applications, including scientific visualization,\ntraining, mental health, and entertainment."}
{"id": "2507.18044", "pdf": "https://arxiv.org/pdf/2507.18044.pdf", "abs": "https://arxiv.org/abs/2507.18044", "title": "Synthetic Data Generation for Phrase Break Prediction with Large Language Model", "authors": ["Hoyeon Lee", "Sejung Son", "Ye-Eun Kang", "Jong-Hwan Kim"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at Interspeech 2025", "summary": "Current approaches to phrase break prediction address crucial prosodic\naspects of text-to-speech systems but heavily rely on vast human annotations\nfrom audio or text, incurring significant manual effort and cost. Inherent\nvariability in the speech domain, driven by phonetic factors, further\ncomplicates acquiring consistent, high-quality data. Recently, large language\nmodels (LLMs) have shown success in addressing data challenges in NLP by\ngenerating tailored synthetic data while reducing manual annotation needs.\nMotivated by this, we explore leveraging LLM to generate synthetic phrase break\nannotations, addressing the challenges of both manual annotation and\nspeech-related tasks by comparing with traditional annotations and assessing\neffectiveness across multiple languages. Our findings suggest that LLM-based\nsynthetic data generation effectively mitigates data challenges in phrase break\nprediction and highlights the potential of LLMs as a viable solution for the\nspeech domain."}
{"id": "2507.18151", "pdf": "https://arxiv.org/pdf/2507.18151.pdf", "abs": "https://arxiv.org/abs/2507.18151", "title": "Understood: Real-Time Communication Support for Adults with ADHD Using Mixed Reality", "authors": ["Shizhen Zhang", "Shengxin Li", "Quan Li"], "categories": ["cs.HC"], "comment": "Appear UIST2025", "summary": "Adults with Attention Deficit Hyperactivity Disorder (ADHD) often experience\ncommunication challenges, primarily due to executive dysfunction and emotional\ndysregulation, even after years of social integration. While existing\ninterventions predominantly target children through structured or intrusive\nmethods, adults lack tools that translate clinical strategies into daily\ncommunication support. To address this gap, we present Understood, a Mixed\nReality (MR) system implemented on Microsoft HoloLens 2, designed to assist\nadults with ADHD in real-world communication. Through formative semi-structured\ninterviews and a design workshop, we identified critical communication barriers\nand derived design goals for the system. Understood combines three key\nfeatures: (1) real-time conversation summarization to reduce cognitive load,\n(2) context-aware subsequent word suggestions during moments of disfluency, and\n(3) topic shifting detection and reminding to mitigate off-topic transitions. A\nwithin-subjects user study and expert interviews demonstrate that Understood\neffectively supports communication with high usability, offering a complement\nto therapist-mediated interventions."}
{"id": "2507.18055", "pdf": "https://arxiv.org/pdf/2507.18055.pdf", "abs": "https://arxiv.org/abs/2507.18055", "title": "Privacy-Preserving Synthetic Review Generation with Diverse Writing Styles Using LLMs", "authors": ["Tevin Atwal", "Chan Nam Tieu", "Yefeng Yuan", "Zhan Shi", "Yuhong Liu", "Liang Cheng"], "categories": ["cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "The increasing use of synthetic data generated by Large Language Models\n(LLMs) presents both opportunities and challenges in data-driven applications.\nWhile synthetic data provides a cost-effective, scalable alternative to\nreal-world data to facilitate model training, its diversity and privacy risks\nremain underexplored. Focusing on text-based synthetic data, we propose a\ncomprehensive set of metrics to quantitatively assess the diversity (i.e.,\nlinguistic expression, sentiment, and user perspective), and privacy (i.e.,\nre-identification risk and stylistic outliers) of synthetic datasets generated\nby several state-of-the-art LLMs. Experiment results reveal significant\nlimitations in LLMs' capabilities in generating diverse and privacy-preserving\nsynthetic data. Guided by the evaluation results, a prompt-based approach is\nproposed to enhance the diversity of synthetic reviews while preserving\nreviewer privacy."}
{"id": "2507.18165", "pdf": "https://arxiv.org/pdf/2507.18165.pdf", "abs": "https://arxiv.org/abs/2507.18165", "title": "ProactiveVA: Proactive Visual Analytics with LLM-Based UI Agent", "authors": ["Yuheng Zhao", "Xueli Shu", "Liwen Fan", "Lin Gao", "Yu Zhang", "Siming Chen"], "categories": ["cs.HC"], "comment": "11 pages, 8 figures", "summary": "Visual analytics (VA) is typically applied to complex data, thus requiring\ncomplex tools. While visual analytics empowers analysts in data analysis,\nanalysts may get lost in the complexity occasionally. This highlights the need\nfor intelligent assistance mechanisms. However, even the latest LLM-assisted VA\nsystems only provide help when explicitly requested by the user, making them\ninsufficiently intelligent to offer suggestions when analysts need them the\nmost. We propose a ProactiveVA framework in which LLM-powered UI agent monitors\nuser interactions and delivers context-aware assistance proactively. To design\neffective proactive assistance, we first conducted a formative study analyzing\nhelp-seeking behaviors in user interaction logs, identifying when users need\nproactive help, what assistance they require, and how the agent should\nintervene. Based on this analysis, we distilled key design requirements in\nterms of intent recognition, solution generation, interpretability and\ncontrollability. Guided by these requirements, we develop a three-stage UI\nagent pipeline including perception, reasoning, and acting. The agent\nautonomously perceives users' needs from VA interaction logs, providing\ntailored suggestions and intuitive guidance through interactive exploration of\nthe system. We implemented the framework in two representative types of VA\nsystems, demonstrating its generalizability, and evaluated the effectiveness\nthrough an algorithm evaluation, case and expert study and a user study. We\nalso discuss current design trade-offs of proactive VA and areas for further\nexploration."}
{"id": "2507.18061", "pdf": "https://arxiv.org/pdf/2507.18061.pdf", "abs": "https://arxiv.org/abs/2507.18061", "title": "TELEVAL: A Dynamic Benchmark Designed for Spoken Language Models in Chinese Interactive Scenarios", "authors": ["Zehan Li", "Hongjie Chen", "Yuxin Zhang", "Jing Zhou", "Xuening Wang", "Hang Lv", "Mengjie Du", "Yaodong Song", "Jie Lian", "Jian Kang", "Jie Li", "Yongxiang Li", "Zhongjiang He", "Xuelong Li"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "Spoken language models (SLMs) have seen rapid progress in recent years, along\nwith the development of numerous benchmarks for evaluating their performance.\nHowever, most existing benchmarks primarily focus on evaluating whether SLMs\ncan perform complex tasks comparable to those tackled by large language models\n(LLMs), often failing to align with how users naturally interact in real-world\nconversational scenarios. In this paper, we propose TELEVAL, a dynamic\nbenchmark specifically designed to evaluate SLMs' effectiveness as\nconversational agents in realistic Chinese interactive settings. TELEVAL\ndefines three evaluation dimensions: Explicit Semantics, Paralinguistic and\nImplicit Semantics, and System Abilities. It adopts a dialogue format\nconsistent with real-world usage and evaluates text and audio outputs\nseparately. TELEVAL particularly focuses on the model's ability to extract\nimplicit cues from user speech and respond appropriately without additional\ninstructions. Our experiments demonstrate that despite recent progress,\nexisting SLMs still have considerable room for improvement in natural\nconversational tasks. We hope that TELEVAL can serve as a user-centered\nevaluation framework that directly reflects the user experience and contributes\nto the development of more capable dialogue-oriented SLMs."}
{"id": "2507.18169", "pdf": "https://arxiv.org/pdf/2507.18169.pdf", "abs": "https://arxiv.org/abs/2507.18169", "title": "Recommender systems, representativeness, and online music: A psychosocial analysis of Italian listeners", "authors": ["Lorenzo Porcaro", "Chiara Monaldi"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Recommender systems shape music listening worldwide due to their widespread\nadoption in online platforms. Growing concerns about representational harms\nthat these systems may cause are nowadays part of the scientific and public\ndebate, wherein music listener perspectives are oftentimes reported and\ndiscussed from a cognitive-behaviorism perspective, but rarely contextualised\nunder a psychosocial and cultural lens. We proceed in this direction, by\ninterviewing a group of Italian music listeners and analysing their narratives\nthrough Emotional Textual Analysis. Thanks to this, we identify shared cultural\nrepertoires that reveal people's complex relationship with listening practices:\neven when familiar with online platforms, listeners may still lack a critical\nunderstanding of recommender systems. Moreover, representational issues,\nparticularly gender disparities, seem not yet fully grasped in the context of\nonline music listening. This study underscores the need for interdisciplinary\nresearch to address representational harms, and the role of algorithmic\nawareness and digital literacy in developing trustworthy recommender systems."}
{"id": "2507.18076", "pdf": "https://arxiv.org/pdf/2507.18076.pdf", "abs": "https://arxiv.org/abs/2507.18076", "title": "Hybrid and Unitary Fine-Tuning of Large Language Models: Methods and Benchmarking under Resource Constraints", "authors": ["Haomin Qi", "Zihan Dai", "Chengbo Huang"], "categories": ["cs.CL"], "comment": "10 pages, 2 figures and 1 table", "summary": "Fine-tuning large language models (LLMs) remains a computational bottleneck\ndue to their scale and memory demands. This paper presents a comprehensive\nevaluation of parameter-efficient fine-tuning (PEFT) techniques, including\nLoRA, BOFT, LoRA-GA, and uRNN, and introduces a novel hybrid strategy that\ndynamically integrates BOFT's orthogonal stability with LoRA-GA's\ngradient-aligned rapid convergence. By computing per-layer adaptive updates\nguided by gradient norms, the hybrid method achieves superior convergence\nefficiency and generalization across diverse tasks. We also explore, for the\nfirst time, the adaptation of unitary RNN (uRNN) principles to\ntransformer-based LLMs, enhancing gradient stability through structured unitary\nconstraints. Empirical evaluations on four benchmarks -- GLUE, GSM8K, MT-Bench,\nand HumanEval -- using models ranging from 7B to 405B parameters demonstrate\nthat our hybrid method consistently outperforms individual PEFT baselines,\napproaching full fine-tuning accuracy while reducing resource consumption by up\nto 2.1 times in training time and 50 percent in memory usage. These findings\nestablish the hybrid approach as a practical and scalable fine-tuning solution\nfor real-world deployment of LLMs under resource constraints."}
{"id": "2507.18252", "pdf": "https://arxiv.org/pdf/2507.18252.pdf", "abs": "https://arxiv.org/abs/2507.18252", "title": "Multimodal Behavioral Patterns Analysis with Eye-Tracking and LLM-Based Reasoning", "authors": ["Dongyang Guo", "Yasmeen Abdrabou", "Enkeleda Thaqi", "Enkelejda Kasneci"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Eye-tracking data reveals valuable insights into users' cognitive states but\nis difficult to analyze due to its structured, non-linguistic nature. While\nlarge language models (LLMs) excel at reasoning over text, they struggle with\ntemporal and numerical data. This paper presents a multimodal human-AI\ncollaborative framework designed to enhance cognitive pattern extraction from\neye-tracking signals. The framework includes: (1) a multi-stage pipeline using\nhorizontal and vertical segmentation alongside LLM reasoning to uncover latent\ngaze patterns; (2) an Expert-Model Co-Scoring Module that integrates expert\njudgment with LLM output to generate trust scores for behavioral\ninterpretations; and (3) a hybrid anomaly detection module combining LSTM-based\ntemporal modeling with LLM-driven semantic analysis. Our results across several\nLLMs and prompt strategies show improvements in consistency, interpretability,\nand performance, with up to 50% accuracy in difficulty prediction tasks. This\napproach offers a scalable, interpretable solution for cognitive modeling and\nhas broad potential in adaptive learning, human-computer interaction, and\neducational analytics."}
{"id": "2507.18103", "pdf": "https://arxiv.org/pdf/2507.18103.pdf", "abs": "https://arxiv.org/abs/2507.18103", "title": "A New Pair of GloVes", "authors": ["Riley Carlson", "John Bauer", "Christopher D. Manning"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This report documents, describes, and evaluates new 2024 English GloVe\n(Global Vectors for Word Representation) models. While the original GloVe\nmodels built in 2014 have been widely used and found useful, languages and the\nworld continue to evolve and we thought that current usage could benefit from\nupdated models. Moreover, the 2014 models were not carefully documented as to\nthe exact data versions and preprocessing that were used, and we rectify this\nby documenting these new models. We trained two sets of word embeddings using\nWikipedia, Gigaword, and a subset of Dolma. Evaluation through vocabulary\ncomparison, direct testing, and NER tasks shows that the 2024 vectors\nincorporate new culturally and linguistically relevant words, perform\ncomparably on structural tasks like analogy and similarity, and demonstrate\nimproved performance on recent, temporally dependent NER datasets such as\nnon-Western newswire data."}
{"id": "2507.18315", "pdf": "https://arxiv.org/pdf/2507.18315.pdf", "abs": "https://arxiv.org/abs/2507.18315", "title": "Talking to...uh...um...Machines: The Impact of Disfluent Speech Agents on Partner Models and Perspective Taking", "authors": ["Rhys Jacka", "Paola R. Peña", "Sophie Leonard", "Éva Székely", "Benjamin R. Cowan"], "categories": ["cs.HC"], "comment": "12 pages, 3 figures, in Proceedings of the 7th ACM Conference on\n  Conversational User Interfaces", "summary": "Speech disfluencies play a role in perspective-taking and audience design in\nhuman-human communication (HHC), but little is known about their impact in\nhuman-machine dialogue (HMD). In an online Namer-Matcher task, sixty-one\nparticipants interacted with a speech agent using either fluent or disfluent\nspeech. Participants completed a partner-modelling questionnaire (PMQ) both\nbefore and after the task. Post-interaction evaluations indicated that\nparticipants perceived the disfluent agent as more competent, despite no\nsignificant differences in pre-task ratings. However, no notable differences\nwere observed in assessments of conversational flexibility or human-likeness.\nOur findings also reveal evidence of egocentric and allocentric language\nproduction when participants interact with speech agents. Interaction with\ndisfluent speech agents appears to increase egocentric communication in\ncomparison to fluent agents. Although the wide credibility intervals mean this\neffect is not clear-cut. We discuss potential interpretations of this finding,\nfocusing on how disfluencies may impact partner models and language production\nin HMD."}
{"id": "2507.18119", "pdf": "https://arxiv.org/pdf/2507.18119.pdf", "abs": "https://arxiv.org/abs/2507.18119", "title": "GOAT-SLM: A Spoken Language Model with Paralinguistic and Speaker Characteristic Awareness", "authors": ["Hongjie Chen", "Zehan Li", "Yaodong Song", "Wenming Deng", "Yitong Yao", "Yuxin Zhang", "Hang Lv", "Xuechao Zhu", "Jian Kang", "Jie Lian", "Jie Li", "Chao Wang", "Shuangyong Song", "Yongxiang Li", "Zhongjiang He"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "Recent advances in end-to-end spoken language models (SLMs) have\nsignificantly improved the ability of AI systems to engage in natural spoken\ninteractions. However, most existing models treat speech merely as a vehicle\nfor linguistic content, often overlooking the rich paralinguistic and speaker\ncharacteristic cues embedded in human speech, such as dialect, age, emotion,\nand non-speech vocalizations. In this work, we introduce GOAT-SLM, a novel\nspoken language model with paralinguistic and speaker characteristic awareness,\ndesigned to extend spoken language modeling beyond text semantics. GOAT-SLM\nadopts a dual-modality head architecture that decouples linguistic modeling\nfrom acoustic realization, enabling robust language understanding while\nsupporting expressive and adaptive speech generation. To enhance model\nefficiency and versatility, we propose a modular, staged training strategy that\nprogressively aligns linguistic, paralinguistic, and speaker characteristic\ninformation using large-scale speech-text corpora. Experimental results on\nTELEVAL, a multi-dimensional evaluation benchmark, demonstrate that GOAT-SLM\nachieves well-balanced performance across both semantic and non-semantic tasks,\nand outperforms existing open-source models in handling emotion, dialectal\nvariation, and age-sensitive interactions. This work highlights the importance\nof modeling beyond linguistic content and advances the development of more\nnatural, adaptive, and socially aware spoken language systems."}
{"id": "2507.18393", "pdf": "https://arxiv.org/pdf/2507.18393.pdf", "abs": "https://arxiv.org/abs/2507.18393", "title": "PALM: PAnoramic Learning Map Integrating Learning Analytics and Curriculum Map for Scalable Insights Across Courses", "authors": ["Mahiro Ozaki", "Li Chen", "Shotaro Naganuma", "Valdemar Švábenský", "Fumiya Okubo", "Atsushi Shimada"], "categories": ["cs.HC", "cs.CY"], "comment": "To appear in the Proceedings of the IEEE SMC 2025 conference", "summary": "This study proposes and evaluates the PAnoramic Learning Map (PALM), a\nlearning analytics (LA) dashboard designed to address the scalability\nchallenges of LA by integrating curriculum-level information. Traditional LA\nresearch has predominantly focused on individual courses or learners and often\nlacks a framework that considers the relationships between courses and the\nlong-term trajectory of learning. To bridge this gap, PALM was developed to\nintegrate multilayered educational data into a curriculum map, enabling\nlearners to intuitively understand their learning records and academic\nprogression. We conducted a system evaluation to assess PALM's effectiveness in\ntwo key areas: (1) its impact on students' awareness of their learning\nbehaviors, and (2) its comparative performance against existing systems. The\nresults indicate that PALM enhances learners' awareness of study planning and\nreflection, particularly by improving perceived behavioral control through the\nvisual presentation of individual learning histories and statistical trends,\nwhich clarify the links between learning actions and outcomes. Although PALM\nrequires ongoing refinement as a system, it received significantly higher\nevaluations than existing systems in terms of visual appeal and usability. By\nserving as an information resource with previously inaccessible insights, PALM\nenhances self-regulated learning and engagement, representing a significant\nstep beyond conventional LA toward a comprehensive and scalable approach."}
{"id": "2507.18140", "pdf": "https://arxiv.org/pdf/2507.18140.pdf", "abs": "https://arxiv.org/abs/2507.18140", "title": "MathOPEval: A Fine-grained Evaluation Benchmark for Visual Operations of MLLMs in Mathematical Reasoning", "authors": ["Xiaoyuan Li", "Moxin Li", "Wenjie Wang", "Rui Men", "Yichang Zhang", "Fuli Feng", "Dayiheng Liu", "Junyang Lin"], "categories": ["cs.CL"], "comment": "Under Review", "summary": "Recent progress in Multi-modal Large Language Models (MLLMs) has enabled\nstep-by-step multi-modal mathematical reasoning by performing visual operations\nbased on the textual instructions. A promising approach uses code as an\nintermediate representation to precisely express and manipulate the images in\nthe reasoning steps. However, existing evaluations focus mainly on text-only\nreasoning outputs, leaving the MLLM's ability to perform accurate visual\noperations via code largely unexplored. This work takes a first step toward\naddressing that gap by evaluating MLLM's code-based capabilities in multi-modal\nmathematical reasoning.Specifically, our framework focuses on two key\nevaluation aspects: (1) Multi-modal Code Generation (MCG) evaluates the model's\nability to accurately understand and construct visualizations from scratch. (2)\nMulti-modal Code Editing (MCE) assesses the model's capacity for fine-grained\noperations, which include three types: Deletion, Modification and Annotation.\nTo evaluate the above tasks, we incorporate a dataset that covers the five most\npopular types of mathematical figures, including geometric diagrams, function\nplots, and three types of statistical charts, to provide a comprehensive and\neffective measurement of existing MLLMs. Our experimental evaluation involves\nnine mainstream MLLMs, and the results reveal that existing models still lag\nsignificantly behind human performance in performing fine-grained visual\noperations."}
{"id": "2507.18401", "pdf": "https://arxiv.org/pdf/2507.18401.pdf", "abs": "https://arxiv.org/abs/2507.18401", "title": "Multisensory Integration and Sensory Substitution Across Vision, Audition, and Haptics: Answering the What, Which, and When in Study Protocols", "authors": ["Andrew Jeyathasan", "Swati Banerjee"], "categories": ["cs.HC", "q-bio.NC"], "comment": null, "summary": "We experience the world through multiple senses that work together to create\na cohesive perception, whether in daily life or immersive technologies.\nUnderstanding this multisensory integration (MSI) requires examining the\ninteractions between sensory modalities, each with unique temporal dynamics and\ncharacteristics. While most research focuses on unimodal or bimodal cues, the\nintegration of three or more modalities remains underexplored. MSI studies must\naccount for factors like cross-modal correspondence, congruence, cognitive\nload, and stimulus timing, which become increasingly complex as modalities\nmultiply. This article examines these key factors and how they can be applied\nto 8 design effective MSI study protocols."}
{"id": "2507.18143", "pdf": "https://arxiv.org/pdf/2507.18143.pdf", "abs": "https://arxiv.org/abs/2507.18143", "title": "HIVMedQA: Benchmarking large language models for HIV medical decision support", "authors": ["Gonzalo Cardenal Antolin", "Jacques Fellay", "Bashkim Jaha", "Roger Kouyos", "Niko Beerenwinkel", "Diane Duroux"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are emerging as valuable tools to support\nclinicians in routine decision-making. HIV management is a compelling use case\ndue to its complexity, including diverse treatment options, comorbidities, and\nadherence challenges. However, integrating LLMs into clinical practice raises\nconcerns about accuracy, potential harm, and clinician acceptance. Despite\ntheir promise, AI applications in HIV care remain underexplored, and LLM\nbenchmarking studies are scarce. This study evaluates the current capabilities\nof LLMs in HIV management, highlighting their strengths and limitations. We\nintroduce HIVMedQA, a benchmark designed to assess open-ended medical question\nanswering in HIV care. The dataset consists of curated, clinically relevant\nquestions developed with input from an infectious disease physician. We\nevaluated seven general-purpose and three medically specialized LLMs, applying\nprompt engineering to enhance performance. Our evaluation framework\nincorporates both lexical similarity and an LLM-as-a-judge approach, extended\nto better reflect clinical relevance. We assessed performance across key\ndimensions: question comprehension, reasoning, knowledge recall, bias,\npotential harm, and factual accuracy. Results show that Gemini 2.5 Pro\nconsistently outperformed other models across most dimensions. Notably, two of\nthe top three models were proprietary. Performance declined as question\ncomplexity increased. Medically fine-tuned models did not always outperform\ngeneral-purpose ones, and larger model size was not a reliable predictor of\nperformance. Reasoning and comprehension were more challenging than factual\nrecall, and cognitive biases such as recency and status quo were observed.\nThese findings underscore the need for targeted development and evaluation to\nensure safe, effective LLM integration in clinical care."}
{"id": "2507.18428", "pdf": "https://arxiv.org/pdf/2507.18428.pdf", "abs": "https://arxiv.org/abs/2507.18428", "title": "Towards Understanding Decision Problems As a Goal of Visualization Design", "authors": ["Lena Cibulski", "Stefan Bruckner"], "categories": ["cs.HC"], "comment": null, "summary": "Decision-making is a central yet under-defined goal in visualization\nresearch. While existing task models address decision processes, they often\nneglect the conditions framing a decision. To better support decision-making\ntasks, we propose a characterization scheme that describes decision problems\nthrough key properties of the data, users, and task context. This scheme helps\nvisualization researchers specify decision-support claims more precisely and\ninforms the design of appropriate visual encodings and interactions. We\ndemonstrate the utility of our approach by applying it to characterize decision\ntasks targeted by existing design studies, highlighting opportunities for\nfuture research in decision-centric visualization."}
{"id": "2507.18171", "pdf": "https://arxiv.org/pdf/2507.18171.pdf", "abs": "https://arxiv.org/abs/2507.18171", "title": "Sticking to the Mean: Detecting Sticky Tokens in Text Embedding Models", "authors": ["Kexin Chen", "Dongxia Wang", "Yi Liu", "Haonan Zhang", "Wenhai Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 main", "summary": "Despite the widespread use of Transformer-based text embedding models in NLP\ntasks, surprising 'sticky tokens' can undermine the reliability of embeddings.\nThese tokens, when repeatedly inserted into sentences, pull sentence similarity\ntoward a certain value, disrupting the normal distribution of embedding\ndistances and degrading downstream performance. In this paper, we\nsystematically investigate such anomalous tokens, formally defining them and\nintroducing an efficient detection method, Sticky Token Detector (STD), based\non sentence and token filtering. Applying STD to 40 checkpoints across 14 model\nfamilies, we discover a total of 868 sticky tokens. Our analysis reveals that\nthese tokens often originate from special or unused entries in the vocabulary,\nas well as fragmented subwords from multilingual corpora. Notably, their\npresence does not strictly correlate with model size or vocabulary size. We\nfurther evaluate how sticky tokens affect downstream tasks like clustering and\nretrieval, observing significant performance drops of up to 50%. Through\nattention-layer analysis, we show that sticky tokens disproportionately\ndominate the model's internal representations, raising concerns about\ntokenization robustness. Our findings show the need for better tokenization\nstrategies and model design to mitigate the impact of sticky tokens in future\ntext embedding applications."}
{"id": "2507.18450", "pdf": "https://arxiv.org/pdf/2507.18450.pdf", "abs": "https://arxiv.org/abs/2507.18450", "title": "High-Dimensional Data Classification in Concentric Coordinates", "authors": ["Alice Williams", "Boris Kovalerchuk"], "categories": ["cs.HC", "cs.LG"], "comment": "8 pages, 21 figures", "summary": "The visualization of multi-dimensional data with interpretable methods\nremains limited by capabilities for both high-dimensional lossless\nvisualizations that do not suffer from occlusion and that are computationally\ncapable by parameterized visualization. This paper proposes a low to high\ndimensional data supporting framework using lossless Concentric Coordinates\nthat are a more compact generalization of Parallel Coordinates along with\nformer Circular Coordinates. These are forms of the General Line Coordinate\nvisualizations that can directly support machine learning algorithm\nvisualization and facilitate human interaction."}
{"id": "2507.18182", "pdf": "https://arxiv.org/pdf/2507.18182.pdf", "abs": "https://arxiv.org/abs/2507.18182", "title": "SCOPE: Stochastic and Counterbiased Option Placement for Evaluating Large Language Models", "authors": ["Wonjun Jeong", "Dongseok Kim", "Taegkeun Whangbo"], "categories": ["cs.CL", "cs.AI"], "comment": "34 pages, 1 figure", "summary": "Large Language Models (LLMs) can achieve inflated scores on multiple-choice\ntasks by exploiting inherent biases in option positions or labels, rather than\ndemonstrating genuine understanding. This study introduces SCOPE, an evaluation\nframework designed to measure and mitigate such selection bias in a\ndataset-independent manner. By repeatedly invoking a null prompt that lacks\nsemantic content, SCOPE estimates each model's unique position-bias\ndistribution. It then redistributes the answer slot according to the\ninverse-bias distribution, thereby equalizing the lucky-rate, the probability\nof selecting the correct answer by chance. Furthermore, it prevents\nsemantically similar distractors from being placed adjacent to the answer,\nthereby blocking near-miss guesses based on superficial proximity cues. Across\nmultiple benchmark experiments, SCOPE consistently outperformed existing\ndebiasing methods in terms of stable performance improvements and showed\nclearer confidence distributions over correct options. This framework thus\noffers a new standard for enhancing the fairness and reliability of LLM\nevaluations."}
{"id": "2507.18510", "pdf": "https://arxiv.org/pdf/2507.18510.pdf", "abs": "https://arxiv.org/abs/2507.18510", "title": "ForcePinch: Force-Responsive Spatial Interaction for Tracking Speed Control in XR", "authors": ["Chenyang Zhang", "Tiffany S Ma", "John Andrews", "Eric J Gonzalez", "Mar Gonzalez-Franco", "Yalong Yang"], "categories": ["cs.HC"], "comment": null, "summary": "Spatial interaction in 3D environments requires balancing efficiency and\nprecision, which requires dynamic tracking speed adjustments. However, existing\ntechniques often couple tracking speed adjustments directly with hand\nmovements, reducing interaction flexibility. Inspired by the natural friction\ncontrol inherent in the physical world, we introduce ForcePinch, a novel\nforce-responsive spatial interaction method that enables users to intuitively\nmodulate pointer tracking speed and smoothly transition between rapid and\nprecise movements by varying their pinching force. To implement this concept,\nwe developed a hardware prototype integrating a pressure sensor with a\ncustomizable mapping function that translates pinching force into tracking\nspeed adjustments. We conducted a user study with 20 participants performing\nwell-established 1D, 2D, and 3D object manipulation tasks, comparing ForcePinch\nagainst the distance-responsive technique Go-Go and speed-responsive technique\nPRISM. Results highlight distinctive characteristics of the force-responsive\napproach across different interaction contexts. Drawing on these findings, we\nhighlight the contextual meaning and versatility of force-responsive\ninteractions through four illustrative examples, aiming to inform and inspire\nfuture spatial interaction design."}
{"id": "2507.18190", "pdf": "https://arxiv.org/pdf/2507.18190.pdf", "abs": "https://arxiv.org/abs/2507.18190", "title": "TN-AutoRCA: Benchmark Construction and Agentic Framework for Self-Improving Alarm-Based Root Cause Analysis in Telecommunication Networks", "authors": ["Keyu Wu", "Qianjin Yu", "Manlin Mei", "Ruiting Liu", "Jun Wang", "Kailai Zhang", "Yelun Bao"], "categories": ["cs.CL"], "comment": "10 pages", "summary": "Root Cause Analysis (RCA) in telecommunication networks is a critical task,\nyet it presents a formidable challenge for Artificial Intelligence (AI) due to\nits complex, graph-based reasoning requirements and the scarcity of realistic\nbenchmarks."}
{"id": "2507.18572", "pdf": "https://arxiv.org/pdf/2507.18572.pdf", "abs": "https://arxiv.org/abs/2507.18572", "title": "PosterMate: Audience-driven Collaborative Persona Agents for Poster Design", "authors": ["Donghoon Shin", "Daniel Lee", "Gary Hsieh", "Gromit Yeuk-Yin Chan"], "categories": ["cs.HC", "cs.AI", "cs.CL", "H.5.2; I.2.7"], "comment": null, "summary": "Poster designing can benefit from synchronous feedback from target audiences.\nHowever, gathering audiences with diverse perspectives and reconciling them on\ndesign edits can be challenging. Recent generative AI models present\nopportunities to simulate human-like interactions, but it is unclear how they\nmay be used for feedback processes in design. We introduce PosterMate, a poster\ndesign assistant that facilitates collaboration by creating audience-driven\npersona agents constructed from marketing documents. PosterMate gathers\nfeedback from each persona agent regarding poster components, and stimulates\ndiscussion with the help of a moderator to reach a conclusion. These\nagreed-upon edits can then be directly integrated into the poster design.\nThrough our user study (N=12), we identified the potential of PosterMate to\ncapture overlooked viewpoints, while serving as an effective prototyping tool.\nAdditionally, our controlled online evaluation (N=100) revealed that the\nfeedback from an individual persona agent is appropriate given its persona\nidentity, and the discussion effectively synthesizes the different persona\nagents' perspectives."}
{"id": "2507.18197", "pdf": "https://arxiv.org/pdf/2507.18197.pdf", "abs": "https://arxiv.org/abs/2507.18197", "title": "Integrating an ISO30401-compliant Knowledge management system with existing business processes of an organization", "authors": ["Aline Belloni", "Patrick Prieur"], "categories": ["cs.CL", "cs.DL"], "comment": "in French language. AGeCSO2025 : 18{\\`e}me Colloque International de\n  l'Association pour la Gestion des Connaissances dans la Soci{\\'e}t{\\'e} et\n  les Organisations, Association pour la Gestion des Connaissances dans la\n  Soci{\\'e}t{\\'e} et les Organisations (AGECSO), Jun 2025, TROYES, France", "summary": "Business process modeling is used by most organizations as an essential\nframework for ensuring efficiency and effectiveness of the work and workflow\nperformed by its employees and for ensuring the alignment of such work with its\nstrategic goals. For organizations that are compliant or near-compliant with\nISO 9001, this approach involves the detailed mapping of processes,\nsub-processes, activities, and tasks. ISO30401 is a Management System Standard,\nintroduced in 2018, establishing universal requirements for the set up of a\nKnowledge Management System in an organization. As ``ISO30401 implementers'' we\nregularly face the challenge of explaining our clients how the knowledge\ndevelopment, transformation and conveyances activities depicted in ISO30401 do\nintegrate with existing operational processes. This article recaps process\nmodelling principles in the context of ISO9001 and explores, based on our\nexperience, how an ISO30401-compliant Knowledge Management System (KMS)\nentwines with all other processes of an Integrated Management System and in\nparticular how it can be implemented by deploying the mechanisms of the SECI\nmodel through the steps of PDCA cycles."}
{"id": "2507.18619", "pdf": "https://arxiv.org/pdf/2507.18619.pdf", "abs": "https://arxiv.org/abs/2507.18619", "title": "MeloKids: Multisensory VR System to Enhance Speech and Motor Coordination in Children with Hearing Loss", "authors": ["Yichen Yu", "Qiaoran Wang"], "categories": ["cs.HC"], "comment": null, "summary": "Children with hearing impairments face ongoing challenges in language and\nmotor development. This study explores how multi-sensory feedback technology\nbased on virtual reality (VR), integrating auditory, visual, and tactile\nstimuli, can enhance rehabilitation outcomes. Using functional near-infrared\nspectroscopy (fNIRS) technology, we assessed cortical activation patterns in\nchildren during pitch-matching tasks across different interaction modes. Our\nfindings aim to provide evidence for designing personalized, interactive\nrehabilitation systems that enhance cognitive engagement and motor control in\nchildren with hearing impairments."}
{"id": "2507.18202", "pdf": "https://arxiv.org/pdf/2507.18202.pdf", "abs": "https://arxiv.org/abs/2507.18202", "title": "Safeguarding RAG Pipelines with GMTP: A Gradient-based Masked Token Probability Method for Poisoned Document Detection", "authors": ["San Kim", "Jonghwi Kim", "Yejin Jeon", "Gary Geunbae Lee"], "categories": ["cs.CL", "cs.AI"], "comment": "18 pages, accepted to ACL Findings 2025", "summary": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by\nproviding external knowledge for accurate and up-to-date responses. However,\nthis reliance on external sources exposes a security risk, attackers can inject\npoisoned documents into the knowledge base to steer the generation process\ntoward harmful or misleading outputs. In this paper, we propose Gradient-based\nMasked Token Probability (GMTP), a novel defense method to detect and filter\nout adversarially crafted documents. Specifically, GMTP identifies high-impact\ntokens by examining gradients of the retriever's similarity function. These key\ntokens are then masked, and their probabilities are checked via a Masked\nLanguage Model (MLM). Since injected tokens typically exhibit markedly low\nmasked-token probabilities, this enables GMTP to easily detect malicious\ndocuments and achieve high-precision filtering. Experiments demonstrate that\nGMTP is able to eliminate over 90% of poisoned content while retaining relevant\ndocuments, thus maintaining robust retrieval and generation performance across\ndiverse datasets and adversarial settings."}
{"id": "2507.18622", "pdf": "https://arxiv.org/pdf/2507.18622.pdf", "abs": "https://arxiv.org/abs/2507.18622", "title": "Evaluation of a Provenance Management Tool for Immersive Virtual Fieldwork", "authors": ["Armin Bernstetter", "Tom Kwasnitschka", "Isabella Peters"], "categories": ["cs.HC"], "comment": "Accepted for Mensch und Computer 2025 Short Paper Track", "summary": "Ensuring reproducibility of research is an integral part of good scientific\npractice. One way to support this is through provenance: information about\nresearch workflows from data gathering to researchers' sensemaking processes\nleading to published results. This is highly important in disciplines such as\ngeosciences, where researchers use software for interactive and immersive\nvisualizations of geospatial data, doing virtual measurements in simulated\nfieldwork on 3D models. We evaluated a provenance management tool, which allows\nrecording of interactions with a virtual fieldwork tool and annotating\ndifferent states of the visualization. The user study investigated how\nresearchers used this Digital Lab Book (DLB) and whether perceived ease of use\nand perceived usefulness differed between groups in immersive or non-immersive\nsettings. Participants perceived the DLB as both useful and easy to use. While\nthere were indications of differences in perceived ease of use (higher for\nimmersive setting), usage patterns showed no significant group differences."}
{"id": "2507.18203", "pdf": "https://arxiv.org/pdf/2507.18203.pdf", "abs": "https://arxiv.org/abs/2507.18203", "title": "Exploring the Impact of Instruction-Tuning on LLM's Susceptibility to Misinformation", "authors": ["Kyubeen Han", "Junseo Jang", "Hongjin Kim", "Geunyeong Jeong", "Harksoo Kim"], "categories": ["cs.CL"], "comment": "ACL 2025 Main Accepted", "summary": "Instruction-tuning enhances the ability of large language models (LLMs) to\nfollow user instructions more accurately, improving usability while reducing\nharmful outputs. However, this process may increase the model's dependence on\nuser input, potentially leading to the unfiltered acceptance of misinformation\nand the generation of hallucinations. Existing studies primarily highlight that\nLLMs are receptive to external information that contradict their parametric\nknowledge, but little research has been conducted on the direct impact of\ninstruction-tuning on this phenomenon. In our study, we investigate the impact\nof instruction-tuning on LLM's susceptibility to misinformation. Our analysis\nreveals that instruction-tuned LLMs are significantly more likely to accept\nmisinformation when it is presented by the user. A comparison with base models\nshows that instruction-tuning increases reliance on user-provided information,\nshifting susceptibility from the assistant role to the user role. Furthermore,\nwe explore additional factors influencing misinformation susceptibility, such\nas the role of the user in prompt structure, misinformation length, and the\npresence of warnings in the system prompt. Our findings underscore the need for\nsystematic approaches to mitigate unintended consequences of instruction-tuning\nand enhance the reliability of LLMs in real-world applications."}
{"id": "2507.17758", "pdf": "https://arxiv.org/pdf/2507.17758.pdf", "abs": "https://arxiv.org/abs/2507.17758", "title": "Weaving the Future: Generative AI and the Reimagining of Fashion Design", "authors": ["Pierre-Marie Chauvin", "Angèle Merlin", "Xavier Fresquet", "Hugo Caselles-Dupré", "Benjamin Simmenauer", "Mathieu de Fayet"], "categories": ["cs.CY", "cs.HC"], "comment": null, "summary": "This paper explores the integration of generative AI into the fashion design\nprocess. Drawing on insights from the January 2025 seminar ``Tisser le futur,''\nit investigates how AI reshapes creative workflows, from ideation to\nprototyping, while interrogating the ethical, aesthetic, and labor\nimplications. The paper highlights co-creative dynamics between humans and\nmachines, the potential for aesthetic innovation, and the environmental and\ncultural challenges of algorithmic design."}
{"id": "2507.18212", "pdf": "https://arxiv.org/pdf/2507.18212.pdf", "abs": "https://arxiv.org/abs/2507.18212", "title": "Prune&Comp: Free Lunch for Layer-Pruned LLMs via Iterative Pruning with Magnitude Compensation", "authors": ["Xinrui Chen", "Hongxing Zhang", "Fanyi Zeng", "Yongxian Wei", "Yizhi Wang", "Xitong Ling", "Guanghao Li", "Chun Yuan"], "categories": ["cs.CL"], "comment": null, "summary": "Layer pruning has emerged as a promising technique for compressing large\nlanguage models (LLMs) while achieving acceleration proportional to the pruning\nratio. In this work, we identify that removing any layer induces a significant\nmagnitude gap in hidden states, resulting in substantial performance\ndegradation. To address this issue, we propose Prune&Comp, a novel\nplug-and-play layer pruning scheme that leverages magnitude compensation to\nmitigate such gaps in a training-free manner. Specifically, we first estimate\nthe magnitude gap caused by layer removal and then eliminate this gap by\nrescaling the remaining weights offline, with zero runtime overhead incurred.\nWe further demonstrate the advantages of Prune&Comp through an iterative\npruning strategy. When integrated with an iterative prune-and-compensate loop,\nPrune&Comp consistently enhances existing layer pruning metrics. For instance,\nwhen 5 layers of LLaMA-3-8B are pruned using the prevalent block influence\nmetric, Prune&Comp nearly halves the perplexity and retains 93.19\\% of the\noriginal model's question-answering performance, outperforming the baseline by\n4.01%."}
{"id": "2507.17760", "pdf": "https://arxiv.org/pdf/2507.17760.pdf", "abs": "https://arxiv.org/abs/2507.17760", "title": "How Instructional Sequence and Personalized Support Impact Diagnostic Strategy Learning", "authors": ["Fatma Betül Güreş", "Tanya Nazaretsky", "Bahar Radmehr", "Martina Rau", "Tanja Käser"], "categories": ["cs.CY", "cs.AI", "cs.HC", "K.3.1"], "comment": "Submitted to AIED 2025 main track", "summary": "Supporting students in developing effective diagnostic reasoning is a key\nchallenge in various educational domains. Novices often struggle with cognitive\nbiases such as premature closure and over-reliance on heuristics.\nScenario-based learning (SBL) can address these challenges by offering\nrealistic case experiences and iterative practice, but the optimal sequencing\nof instruction and problem-solving activities remains unclear. This study\nexamines how personalized support can be incorporated into different\ninstructional sequences and whether providing explicit diagnostic strategy\ninstruction before (I-PS) or after problem-solving (PS-I) improves learning and\nits transfer. We employ a between-groups design in an online SBL environment\ncalled PharmaSim, which simulates real-world client interactions for pharmacy\ntechnician apprentices. Results indicate that while both instruction types are\nbeneficial, PS-I leads to significantly higher performance in transfer tasks."}
{"id": "2507.18263", "pdf": "https://arxiv.org/pdf/2507.18263.pdf", "abs": "https://arxiv.org/abs/2507.18263", "title": "Locate-and-Focus: Enhancing Terminology Translation in Speech Language Models", "authors": ["Suhang Wu", "Jialong Tang", "Chengyi Yang", "Pei Zhang", "Baosong Yang", "Junhui Li", "Junfeng Yao", "Min Zhang", "Jinsong Su"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ACL 2025", "summary": "Direct speech translation (ST) has garnered increasing attention nowadays,\nyet the accurate translation of terminology within utterances remains a great\nchallenge. In this regard, current studies mainly concentrate on leveraging\nvarious translation knowledge into ST models. However, these methods often\nstruggle with interference from irrelevant noise and can not fully utilize the\ntranslation knowledge. To address these issues, in this paper, we propose a\nnovel Locate-and-Focus method for terminology translation. It first effectively\nlocates the speech clips containing terminologies within the utterance to\nconstruct translation knowledge, minimizing irrelevant information for the ST\nmodel. Subsequently, it associates the translation knowledge with the utterance\nand hypothesis from both audio and textual modalities, allowing the ST model to\nbetter focus on translation knowledge during translation. Experimental results\nacross various datasets demonstrate that our method effectively locates\nterminologies within utterances and enhances the success rate of terminology\ntranslation, while maintaining robust general translation performance."}
{"id": "2507.17978", "pdf": "https://arxiv.org/pdf/2507.17978.pdf", "abs": "https://arxiv.org/abs/2507.17978", "title": "MeAJOR Corpus: A Multi-Source Dataset for Phishing Email Detection", "authors": ["Paulo Mendes", "Eva Maia", "Isabel Praça"], "categories": ["cs.CR", "cs.AI", "cs.HC", "68P20 (Primary) 68T05, 68T07, 68T10 (Secondary)", "K.6.5; I.2.6; I.2.7; C.2.0"], "comment": "8 pages, 2 tables, WI-IAT 2025 conference", "summary": "Phishing emails continue to pose a significant threat to cybersecurity by\nexploiting human vulnerabilities through deceptive content and malicious\npayloads. While Machine Learning (ML) models are effective at detecting\nphishing threats, their performance largely relies on the quality and diversity\nof the training data. This paper presents MeAJOR (Merged email Assets from\nJoint Open-source Repositories) Corpus, a novel, multi-source phishing email\ndataset designed to overcome critical limitations in existing resources. It\nintegrates 135894 samples representing a broad number of phishing tactics and\nlegitimate emails, with a wide spectrum of engineered features. We evaluated\nthe dataset's utility for phishing detection research through systematic\nexperiments with four classification models (RF, XGB, MLP, and CNN) across\nmultiple feature configurations. Results highlight the dataset's effectiveness,\nachieving 98.34% F1 with XGB. By integrating broad features from multiple\ncategories, our dataset provides a reusable and consistent resource, while\naddressing common challenges like class imbalance, generalisability and\nreproducibility."}
{"id": "2507.18264", "pdf": "https://arxiv.org/pdf/2507.18264.pdf", "abs": "https://arxiv.org/abs/2507.18264", "title": "Zero-shot OCR Accuracy of Low-Resourced Languages: A Comparative Analysis on Sinhala and Tamil", "authors": ["Nevidu Jayatilleke", "Nisansa de Silva"], "categories": ["cs.CL"], "comment": "10 pages, 4 figures, Accepted paper at Recent Advances in Natural\n  Language Processing (RANLP) 2025", "summary": "Solving the problem of Optical Character Recognition (OCR) on printed text\nfor Latin and its derivative scripts can now be considered settled due to the\nvolumes of research done on English and other High-Resourced Languages (HRL).\nHowever, for Low-Resourced Languages (LRL) that use unique scripts, it remains\nan open problem. This study presents a comparative analysis of the zero-shot\nperformance of six distinct OCR engines on two LRLs: Sinhala and Tamil. The\nselected engines include both commercial and open-source systems, aiming to\nevaluate the strengths of each category. The Cloud Vision API, Surya, Document\nAI, and Tesseract were evaluated for both Sinhala and Tamil, while Subasa OCR\nand EasyOCR were examined for only one language due to their limitations. The\nperformance of these systems was rigorously analysed using five measurement\ntechniques to assess accuracy at both the character and word levels. According\nto the findings, Surya delivered the best performance for Sinhala across all\nmetrics, with a WER of 2.61%. Conversely, Document AI excelled across all\nmetrics for Tamil, highlighted by a very low CER of 0.78%. In addition to the\nabove analysis, we also introduce a novel synthetic Tamil OCR benchmarking\ndataset."}
{"id": "2507.18022", "pdf": "https://arxiv.org/pdf/2507.18022.pdf", "abs": "https://arxiv.org/abs/2507.18022", "title": "Does visualization help AI understand data?", "authors": ["Victoria R. Li", "Johnathan Sun", "Martin Wattenberg"], "categories": ["cs.AI", "cs.HC", "cs.LG"], "comment": "5 pages, 6 figures", "summary": "Charts and graphs help people analyze data, but can they also be useful to AI\nsystems? To investigate this question, we perform a series of experiments with\ntwo commercial vision-language models: GPT 4.1 and Claude 3.5. Across three\nrepresentative analysis tasks, the two systems describe synthetic datasets more\nprecisely and accurately when raw data is accompanied by a scatterplot,\nespecially as datasets grow in complexity. Comparison with two baselines --\nproviding a blank chart and a chart with mismatched data -- shows that the\nimproved performance is due to the content of the charts. Our results are\ninitial evidence that AI systems, like humans, can benefit from visualization."}
{"id": "2507.18294", "pdf": "https://arxiv.org/pdf/2507.18294.pdf", "abs": "https://arxiv.org/abs/2507.18294", "title": "StyleAdaptedLM: Enhancing Instruction Following Models with Efficient Stylistic Transfer", "authors": ["Pritika Ramu", "Apoorv Saxena", "Meghanath M Y", "Varsha Sankar", "Debraj Basu"], "categories": ["cs.CL"], "comment": null, "summary": "Adapting LLMs to specific stylistic characteristics, like brand voice or\nauthorial tones, is crucial for enterprise communication but challenging to\nachieve from corpora which lacks instruction-response formatting without\ncompromising instruction adherence. We introduce StyleAdaptedLM, a framework\nthat efficiently transfers stylistic traits to instruction-following models\nusing Low-Rank Adaptation (LoRA). LoRA adapters are first trained on a base\nmodel with diverse unstructured stylistic corpora, then merged with a separate\ninstruction-following model. This enables robust stylistic customization\nwithout paired data or sacrificing task performance. Experiments across\nmultiple datasets and models demonstrate improved stylistic consistency while\npreserving instruction adherence, with human evaluations confirming\nbrand-specific convention uptake. StyleAdaptedLM offers an efficient path for\nstylistic personalization in LLMs."}
{"id": "2507.18262", "pdf": "https://arxiv.org/pdf/2507.18262.pdf", "abs": "https://arxiv.org/abs/2507.18262", "title": "ReSem3D: Refinable 3D Spatial Constraints via Fine-Grained Semantic Grounding for Generalizable Robotic Manipulation", "authors": ["Chenyu Su", "Weiwei Shang", "Chen Qian", "Fei Zhang", "Shuang Cong"], "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "comment": "12 pages,9 figures", "summary": "Semantics-driven 3D spatial constraints align highlevel semantic\nrepresentations with low-level action spaces, facilitating the unification of\ntask understanding and execution in robotic manipulation. The synergistic\nreasoning of Multimodal Large Language Models (MLLMs) and Vision Foundation\nModels (VFMs) enables cross-modal 3D spatial constraint construction.\nNevertheless, existing methods have three key limitations: (1) coarse semantic\ngranularity in constraint modeling, (2) lack of real-time closed-loop planning,\n(3) compromised robustness in semantically diverse environments. To address\nthese challenges, we propose ReSem3D, a unified manipulation framework for\nsemantically diverse environments, leveraging the synergy between VFMs and\nMLLMs to achieve fine-grained visual grounding and dynamically constructs\nhierarchical 3D spatial constraints for real-time manipulation. Specifically,\nthe framework is driven by hierarchical recursive reasoning in MLLMs, which\ninteract with VFMs to automatically construct 3D spatial constraints from\nnatural language instructions and RGB-D observations in two stages: part-level\nextraction and region-level refinement. Subsequently, these constraints are\nencoded as real-time optimization objectives in joint space, enabling reactive\nbehavior to dynamic disturbances. Extensive simulation and real-world\nexperiments are conducted in semantically rich household and sparse chemical\nlab environments. The results demonstrate that ReSem3D performs diverse\nmanipulation tasks under zero-shot conditions, exhibiting strong adaptability\nand generalization. Code and videos at https://resem3d.github.io."}
{"id": "2507.18305", "pdf": "https://arxiv.org/pdf/2507.18305.pdf", "abs": "https://arxiv.org/abs/2507.18305", "title": "BadReasoner: Planting Tunable Overthinking Backdoors into Large Reasoning Models for Fun or Profit", "authors": ["Biao Yi", "Zekun Fei", "Jianing Geng", "Tong Li", "Lihai Nie", "Zheli Liu", "Yiming Li"], "categories": ["cs.CL"], "comment": null, "summary": "Large reasoning models (LRMs) have emerged as a significant advancement in\nartificial intelligence, representing a specialized class of large language\nmodels (LLMs) designed to tackle complex reasoning tasks. The defining\ncharacteristic of LRMs lies in their extensive chain-of-thought (CoT) reasoning\ncapabilities. In this paper, we identify a previously unexplored attack vector\nagainst LRMs, which we term \"overthinking backdoors\". We advance this concept\nby proposing a novel tunable backdoor, which moves beyond simple on/off attacks\nto one where an attacker can precisely control the extent of the model's\nreasoning verbosity. Our attack is implemented through a novel data poisoning\nmethodology. It pairs a tunable trigger-where the number of repetitions signals\nthe desired intensity-with a correspondingly verbose CoT response. These\nresponses are programmatically generated by instructing a teacher LLM to inject\na controlled number of redundant refinement steps into a correct reasoning\nprocess. The approach preserves output correctness, which ensures stealth and\nestablishes the attack as a pure resource-consumption vector. Extensive\nempirical results on various LRMs demonstrate that our method can reliably\ntrigger a controllable, multi-fold increase in the length of the reasoning\nprocess, without degrading the final answer's correctness. Our source code is\navailable at https://github.com/FZaKK/BadReasoner."}
{"id": "2507.18523", "pdf": "https://arxiv.org/pdf/2507.18523.pdf", "abs": "https://arxiv.org/abs/2507.18523", "title": "The Moral Gap of Large Language Models", "authors": ["Maciej Skorski", "Alina Landowska"], "categories": ["cs.CL", "cs.CY", "cs.HC", "cs.LG"], "comment": "preprint", "summary": "Moral foundation detection is crucial for analyzing social discourse and\ndeveloping ethically-aligned AI systems. While large language models excel\nacross diverse tasks, their performance on specialized moral reasoning remains\nunclear.\n  This study provides the first comprehensive comparison between\nstate-of-the-art LLMs and fine-tuned transformers across Twitter and Reddit\ndatasets using ROC, PR, and DET curve analysis.\n  Results reveal substantial performance gaps, with LLMs exhibiting high false\nnegative rates and systematic under-detection of moral content despite prompt\nengineering efforts. These findings demonstrate that task-specific fine-tuning\nremains superior to prompting for moral reasoning applications."}
{"id": "2507.18338", "pdf": "https://arxiv.org/pdf/2507.18338.pdf", "abs": "https://arxiv.org/abs/2507.18338", "title": "Uncertainty Quantification for Evaluating Machine Translation Bias", "authors": ["Ieva Raminta Staliūnaitė", "Julius Cheng", "Andreas Vlachos"], "categories": ["cs.CL"], "comment": null, "summary": "In machine translation (MT), when the source sentence includes a lexeme whose\ngender is not overtly marked, but whose target-language equivalent requires\ngender specification, the model must infer the appropriate gender from the\ncontext and/or external knowledge. Studies have shown that MT models exhibit\nbiased behaviour, relying on stereotypes even when they clash with contextual\ninformation. We posit that apart from confidently translating using the correct\ngender when it is evident from the input, models should also maintain\nuncertainty about the gender when it is ambiguous. Using recently proposed\nmetrics of semantic uncertainty, we find that models with high translation and\ngender accuracy on unambiguous instances do not necessarily exhibit the\nexpected level of uncertainty in ambiguous ones. Similarly, debiasing has\nindependent effects on ambiguous and unambiguous translation instances."}
{"id": "2411.02650", "pdf": "https://arxiv.org/pdf/2411.02650.pdf", "abs": "https://arxiv.org/abs/2411.02650", "title": "A Scoping Review of Functional Near-Infrared Spectroscopy (fNIRS) Applications in Game-Based Learning Environments", "authors": ["Shayla Sharmin", "Gael Lucero-Palacios", "Behdokht Kiafar", "Mohammad Fahim Abrar", "Mohammad Al-Ratrout", "Aditya Raikwar", "Roghayeh Leila Barmaki"], "categories": ["cs.HC"], "comment": "28 pages, 3 figures", "summary": "Functional Near-Infrared Spectroscopy (fNIRS) has emerged as a valuable tool\nto investigate cognitive and emotional processes during learning. We focus\nspecifically on game-integrated learning systems as the context for fNIRS-based\nbrain data analysis. We selected game-integrated learning systems because such\nsystems make learning more engaging, interactive, and immersive, all of which\nare critical features for adaptive learning design. The goal of this scoping\nreview is to help researchers understand how fNIRS has been used so far to\nstudy brain activity in game-integrated learning systems. We also aim to show\nhow brain data captured through fNIRS can support the development of adaptive\nlearning systems by monitoring learners' cognitive states. Using the PRISMA-ScR\nframework, 1300 papers were screened, and 21 empirical studies were selected\nfor in-depth analysis. Studies were categorized as affective/cognitive response\nstudies or comparative studies, and further analyzed by learning platform, game\ndevice, fNIRS configuration, outcome measures, and study design. The findings\nreveal that game-integrated learning systems can be as effective as traditional\nmethods in improving engagement and involvement. The findings also show that\nfNIRS offers valuable insights into cognitive states, but it has not yet been\nwidely implemented in real-time adaptive systems. We identify key challenges in\nstandardization and data interpretation and highlight the potential of fNIRS\nfor developing brain-aware, interactive learning environments. This review\noffers insights to guide future research on using brain data to support\nadaptive learning and intelligent system design."}
{"id": "2507.18340", "pdf": "https://arxiv.org/pdf/2507.18340.pdf", "abs": "https://arxiv.org/abs/2507.18340", "title": "TDR: Task-Decoupled Retrieval with Fine-Grained LLM Feedback for In-Context Learning", "authors": ["Yifu Chen", "Bingchen Huang", "Zhiling Wang", "Yuanchao Du", "Junfeng Luo", "Lei Shen", "Zhineng chen"], "categories": ["cs.CL"], "comment": null, "summary": "In-context learning (ICL) has become a classic approach for enabling LLMs to\nhandle various tasks based on a few input-output examples. The effectiveness of\nICL heavily relies on the quality of these examples, and previous works which\nfocused on enhancing example retrieval capabilities have achieved impressive\nperformances. However, two challenges remain in retrieving high-quality\nexamples: (1) Difficulty in distinguishing cross-task data distributions, (2)\nDifficulty in making the fine-grained connection between retriever output and\nfeedback from LLMs. In this paper, we propose a novel framework called TDR. TDR\ndecouples the ICL examples from different tasks, which enables the retrieval\nmodule to retrieve examples specific to the target task within a multi-task\ndataset. Furthermore, TDR models fine-grained feedback from LLMs to supervise\nand guide the training of the retrieval module, which helps to retrieve\nhigh-quality examples. We conducted extensive experiments on a suite of 30 NLP\ntasks, the results demonstrate that TDR consistently improved results across\nall datasets and achieves state-of-the-art performance. Meanwhile, our approach\nis a plug-and-play method, which can be easily combined with various LLMs to\nimprove example retrieval abilities for ICL. The code is available at\nhttps://github.com/Nnn-s/TDR."}
{"id": "2412.14209", "pdf": "https://arxiv.org/pdf/2412.14209.pdf", "abs": "https://arxiv.org/abs/2412.14209", "title": "Integrating Evidence into the Design of XAI and AI-based Decision Support Systems: A Means-End Framework for End-users in Construction", "authors": ["Peter E. D. Love", "Jane Matthews", "Weili Fang", "Hadi Mahamivanan"], "categories": ["cs.HC", "cs.AI"], "comment": "74 pages, 5 figures and 3 tables", "summary": "Explainable Artificial Intelligence seeks to make the reasoning processes of\nAI models transparent and interpretable, particularly in complex decision\nmaking environments. In the construction industry, where AI based decision\nsupport systems are increasingly adopted, limited attention has been paid to\nthe integration of supporting evidence that underpins the reliability and\naccountability of AI generated outputs. The absence of such evidence undermines\nthe validity of explanations and the trustworthiness of system recommendations.\nThis paper addresses this gap by introducing a theoretical, evidence based\nmeans end framework developed through a narrative review. The framework offers\nan epistemic foundation for designing XAI enabled DSS that generate meaningful\nexplanations tailored to users knowledge needs and decision contexts. It\nfocuses on evaluating the strength, relevance, and utility of different types\nof evidence supporting AI generated explanations. While developed with\nconstruction professionals as primary end users, the framework is also\napplicable to developers, regulators, and project managers with varying\nepistemic goals."}
{"id": "2507.18343", "pdf": "https://arxiv.org/pdf/2507.18343.pdf", "abs": "https://arxiv.org/abs/2507.18343", "title": "Hybrid Annotation for Propaganda Detection: Integrating LLM Pre-Annotations with Human Intelligence", "authors": ["Ariana Sahitaj", "Premtim Sahitaj", "Veronika Solopova", "Jiaao Li", "Sebastian Möller", "Vera Schmitt"], "categories": ["cs.CL"], "comment": "NLP4PI at ACL", "summary": "Propaganda detection on social media remains challenging due to task\ncomplexity and limited high-quality labeled data. This paper introduces a novel\nframework that combines human expertise with Large Language Model (LLM)\nassistance to improve both annotation consistency and scalability. We propose a\nhierarchical taxonomy that organizes 14 fine-grained propaganda techniques into\nthree broader categories, conduct a human annotation study on the HQP dataset\nthat reveals low inter-annotator agreement for fine-grained labels, and\nimplement an LLM-assisted pre-annotation pipeline that extracts propagandistic\nspans, generates concise explanations, and assigns local labels as well as a\nglobal label. A secondary human verification study shows significant\nimprovements in both agreement and time-efficiency. Building on this, we\nfine-tune smaller language models (SLMs) to perform structured annotation.\nInstead of fine-tuning on human annotations, we train on high-quality\nLLM-generated data, allowing a large model to produce these annotations and a\nsmaller model to learn to generate them via knowledge distillation. Our work\ncontributes towards the development of scalable and robust propaganda detection\nsystems, supporting the idea of transparent and accountable media ecosystems in\nline with SDG 16. The code is publicly available at our GitHub repository."}
{"id": "2412.15669", "pdf": "https://arxiv.org/pdf/2412.15669.pdf", "abs": "https://arxiv.org/abs/2412.15669", "title": "WigglyEyes: Inferring Eye Movements from Keypress Data", "authors": ["Yujun Zhu", "Danqing Shi", "Hee-Seung Moon", "Antti Oulasvirta"], "categories": ["cs.HC"], "comment": null, "summary": "We present a model for inferring where users look during interaction based on\nkeypress data only. Given a key log, it outputs a scanpath that tells,\nmoment-by-moment, how the user had moved eyes while entering those keys. The\nmodel can be used as a proxy for human data in cases where collecting real eye\ntracking data is expensive or impossible. Our technical insight is an inference\narchitecture that considers the individual characteristics of the user,\ninferred as a low-dimensional parameter vector. We present a novel loss\nfunction for synchronizing inferred eye movements with the keypresses.\nEvaluations on touchscreen typing demonstrate accurate gaze inference."}
{"id": "2507.18392", "pdf": "https://arxiv.org/pdf/2507.18392.pdf", "abs": "https://arxiv.org/abs/2507.18392", "title": "CLEAR: Error Analysis via LLM-as-a-Judge Made Easy", "authors": ["Asaf Yehudai", "Lilach Eden", "Yotam Perlitz", "Roy Bar-Haim", "Michal Shmueli-Scheuer"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The evaluation of Large Language Models (LLMs) increasingly relies on other\nLLMs acting as judges. However, current evaluation paradigms typically yield a\nsingle score or ranking, answering which model is better but not why. While\nessential for benchmarking, these top-level scores obscure the specific,\nactionable reasons behind a model's performance. To bridge this gap, we\nintroduce CLEAR, an interactive, open-source package for LLM-based error\nanalysis. CLEAR first generates per-instance textual feedback, then it creates\na set of system-level error issues, and quantifies the prevalence of each\nidentified issue. Our package also provides users with an interactive dashboard\nthat allows for a comprehensive error analysis through aggregate\nvisualizations, applies interactive filters to isolate specific issues or score\nranges, and drills down to the individual instances that exemplify a particular\nbehavioral pattern. We demonstrate CLEAR analysis for RAG and Math benchmarks,\nand showcase its utility through a user case study."}
{"id": "2503.10029", "pdf": "https://arxiv.org/pdf/2503.10029.pdf", "abs": "https://arxiv.org/abs/2503.10029", "title": "HandProxy: Expanding the Affordances of Speech Interfaces in Immersive Environments with a Virtual Proxy Hand", "authors": ["Chen Liang", "Yuxuan Liu", "Martez Mott", "Anhong Guo"], "categories": ["cs.HC"], "comment": "Accepted in ACM IMWUT 2025", "summary": "Hand interactions are increasingly used as the primary input modality in\nimmersive environments, but they are not always feasible due to situational\nimpairments, motor limitations, and environmental constraints. Speech\ninterfaces have been explored as an alternative to hand input in research and\ncommercial solutions, but are limited to initiating basic hand gestures and\nsystem controls. We introduce HandProxy, a system that expands the affordances\nof speech interfaces to support expressive hand interactions. Instead of\nrelying on predefined speech commands directly mapped to possible interactions,\nHandProxy enables users to control the movement of a virtual hand as an\ninteraction proxy, allowing them to describe the intended interactions\nnaturally while the system translates speech into a sequence of hand controls\nfor real-time execution. A user study with 20 participants demonstrated that\nHandProxy effectively enabled diverse hand interactions in virtual\nenvironments, achieving a 100% task completion rate with an average of 1.09\nattempts per speech command and 91.8% command execution accuracy, while\nsupporting flexible, natural speech input with varying levels of control and\ngranularity."}
{"id": "2507.18406", "pdf": "https://arxiv.org/pdf/2507.18406.pdf", "abs": "https://arxiv.org/abs/2507.18406", "title": "Factual Inconsistencies in Multilingual Wikipedia Tables", "authors": ["Silvia Cappa", "Lingxiao Kong", "Pille-Riin Peet", "Fanfu Wei", "Yuchen Zhou", "Jan-Christoph Kalo"], "categories": ["cs.CL", "cs.DB", "cs.DL", "cs.IR"], "comment": "11 pages, 7 figures, White Paper for RTF Work at ISWS Summer School\n  2025", "summary": "Wikipedia serves as a globally accessible knowledge source with content in\nover 300 languages. Despite covering the same topics, the different versions of\nWikipedia are written and updated independently. This leads to factual\ninconsistencies that can impact the neutrality and reliability of the\nencyclopedia and AI systems, which often rely on Wikipedia as a main training\nsource. This study investigates cross-lingual inconsistencies in Wikipedia's\nstructured content, with a focus on tabular data. We developed a methodology to\ncollect, align, and analyze tables from Wikipedia multilingual articles,\ndefining categories of inconsistency. We apply various quantitative and\nqualitative metrics to assess multilingual alignment using a sample dataset.\nThese insights have implications for factual verification, multilingual\nknowledge interaction, and design for reliable AI systems leveraging Wikipedia\ncontent."}
{"id": "2504.02250", "pdf": "https://arxiv.org/pdf/2504.02250.pdf", "abs": "https://arxiv.org/abs/2504.02250", "title": "Designing Effective Human-Swarm Interaction Interfaces: Insights from a User Study on Task Performance", "authors": ["Wasura D. Wattearachchi", "Erandi Lakshika", "Kathryn Kasmarik", "Michael Barlow"], "categories": ["cs.HC", "cs.RO"], "comment": "8 pages, 4 figures, 5 tables", "summary": "In this paper, we present a systematic method of design for human-swarm\ninteraction interfaces, combining theoretical insights with empirical\nevaluation. We first derived ten design principles from existing literature,\napplying them to key information dimensions identified through goal-directed\ntask analysis and developed a tablet-based interface for a target search task.\nWe then conducted a user study with 31 participants where humans were required\nto guide a robotic swarm to a target in the presence of three types of hazards\nthat pose a risk to the robots: Distributed, Moving, and Spreading. Performance\nwas measured based on the proximity of the robots to the target and the number\nof deactivated robots at the end of the task. Results indicate that at least\none robot was brought closer to the target in 98% of tasks, demonstrating the\ninterface's success in fulfilling the primary objective of the task.\nAdditionally, in nearly 67% of tasks, more than 50% of the robots reached the\ntarget. Moreover, particularly better performance was noted in moving hazards.\nAdditionally, the interface appeared to help minimise robot deactivation, as\nevidenced by nearly 94% of tasks where participants managed to keep more than\n50% of the robots active, ensuring that most of the swarm remained operational.\nHowever, its effectiveness varied across hazards, with robot deactivation being\nlowest in distributed hazard scenarios, suggesting that the interface provided\nthe most support in these conditions."}
{"id": "2507.18417", "pdf": "https://arxiv.org/pdf/2507.18417.pdf", "abs": "https://arxiv.org/abs/2507.18417", "title": "FinDPO: Financial Sentiment Analysis for Algorithmic Trading through Preference Optimization of LLMs", "authors": ["Giorgos Iacovides", "Wuyang Zhou", "Danilo Mandic"], "categories": ["cs.CL", "cs.LG", "q-fin.ST", "q-fin.TR"], "comment": null, "summary": "Opinions expressed in online finance-related textual data are having an\nincreasingly profound impact on trading decisions and market movements. This\ntrend highlights the vital role of sentiment analysis as a tool for quantifying\nthe nature and strength of such opinions. With the rapid development of\nGenerative AI (GenAI), supervised fine-tuned (SFT) large language models (LLMs)\nhave become the de facto standard for financial sentiment analysis. However,\nthe SFT paradigm can lead to memorization of the training data and often fails\nto generalize to unseen samples. This is a critical limitation in financial\ndomains, where models must adapt to previously unobserved events and the\nnuanced, domain-specific language of finance. To this end, we introduce FinDPO,\nthe first finance-specific LLM framework based on post-training human\npreference alignment via Direct Preference Optimization (DPO). The proposed\nFinDPO achieves state-of-the-art performance on standard sentiment\nclassification benchmarks, outperforming existing supervised fine-tuned models\nby 11% on the average. Uniquely, the FinDPO framework enables the integration\nof a fine-tuned causal LLM into realistic portfolio strategies through a novel\n'logit-to-score' conversion, which transforms discrete sentiment predictions\ninto continuous, rankable sentiment scores (probabilities). In this way,\nsimulations demonstrate that FinDPO is the first sentiment-based approach to\nmaintain substantial positive returns of 67% annually and strong risk-adjusted\nperformance, as indicated by a Sharpe ratio of 2.0, even under realistic\ntransaction costs of 5 basis points (bps)."}
{"id": "2504.03300", "pdf": "https://arxiv.org/pdf/2504.03300.pdf", "abs": "https://arxiv.org/abs/2504.03300", "title": "On the Complexities of Testing for Compliance with Human Oversight Requirements in AI Regulation", "authors": ["Markus Langer", "Veronika Lazar", "Kevin Baum"], "categories": ["cs.HC", "cs.CY", "K.4.1; K.5.2; J.4"], "comment": null, "summary": "Human oversight requirements are a core component of the European AI Act and\nin AI governance. In this paper, we highlight key challenges in testing for\ncompliance with these requirements. A central difficulty lies in balancing\nsimple, but potentially ineffective checklist-based approaches with\nresource-intensive and context-sensitive empirical testing of the effectiveness\nof human oversight of AI. Questions regarding when to update compliance\ntesting, the context-dependent nature of human oversight requirements, and\ndifficult-to-operationalize standards further complicate compliance testing. We\nargue that these challenges illustrate broader challenges in the future of\nsociotechnical AI governance, i.e. a future that shifts from ensuring good\ntechnological products to good sociotechnical systems."}
{"id": "2507.18442", "pdf": "https://arxiv.org/pdf/2507.18442.pdf", "abs": "https://arxiv.org/abs/2507.18442", "title": "AraTable: Benchmarking LLMs' Reasoning and Understanding of Arabic Tabular Data", "authors": ["Rana Alshaikh", "Israa Alghanmi", "Shelan Jeawak"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The cognitive and reasoning abilities of large language models (LLMs) have\nenabled remarkable progress in natural language processing. However, their\nperformance in interpreting structured data, especially in tabular formats,\nremains limited. Although benchmarks for English tabular data are widely\navailable, Arabic is still underrepresented because of the limited availability\nof public resources and its unique language features. To address this gap, we\npresent AraTable, a novel and comprehensive benchmark designed to evaluate the\nreasoning and understanding capabilities of LLMs when applied to Arabic tabular\ndata. AraTable consists of various evaluation tasks, such as direct question\nanswering, fact verification, and complex reasoning, involving a wide range of\nArabic tabular sources. Our methodology follows a hybrid pipeline, where\ninitial content is generated by LLMs and subsequently filtered and verified by\nhuman experts to ensure high dataset quality. Initial analyses using AraTable\nshow that, while LLMs perform adequately on simpler tabular tasks such as\ndirect question answering, they continue to face significant cognitive\nchallenges when tasks require deeper reasoning and fact verification. This\nindicates that there are substantial opportunities for future work to improve\nperformance on complex tabular reasoning tasks. We also propose a fully\nautomated evaluation framework that uses a self-deliberation mechanism and\nachieves performance nearly identical to that of human judges. This research\nprovides a valuable, publicly available resource and evaluation framework that\ncan help accelerate the development of foundational models for processing and\nanalysing Arabic structured data."}
{"id": "2504.17999", "pdf": "https://arxiv.org/pdf/2504.17999.pdf", "abs": "https://arxiv.org/abs/2504.17999", "title": "Streaming, Fast and Slow: Cognitive Load-Aware Streaming for Efficient LLM Serving", "authors": ["Chang Xiao", "Brenda Yang"], "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "Generative conversational interfaces powered by large language models (LLMs)\ntypically stream output token-by-token at a rate determined by computational\nbudget, often neglecting actual human reading speeds and the cognitive load\nassociated with the content. This mismatch frequently leads to inefficient use\nof computational resources. For example, in cloud-based services, streaming\ncontent faster than users can read appears unnecessary, resulting in wasted\ncomputational resources and potential delays for other users, particularly\nduring peak usage periods. To address this issue, we propose an adaptive\nstreaming method that dynamically adjusts the pacing of LLM streaming output in\nreal-time based on inferred cognitive load. Our approach estimates the\ncognitive load associated with streaming content and strategically slows down\nthe stream during complex or information-rich segments, thereby freeing\ncomputational resources for other users. We conducted a statistical analysis\nand simulation based on a statistical model derived from data collected in a\ncrowdsourced user study across various types of LLM-generated content. Our\nresults show that this adaptive method can effectively reduce computational\nconsumption while largely maintaining streaming speed above user's normal\nreading speed."}
{"id": "2507.18448", "pdf": "https://arxiv.org/pdf/2507.18448.pdf", "abs": "https://arxiv.org/abs/2507.18448", "title": "Restoring Rhythm: Punctuation Restoration Using Transformer Models for Bangla, a Low-Resource Language", "authors": ["Md Obyedullahil Mamun", "Md Adyelullahil Mamun", "Arif Ahmad", "Md. Imran Hossain Emu"], "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2; I.7"], "comment": null, "summary": "Punctuation restoration enhances the readability of text and is critical for\npost-processing tasks in Automatic Speech Recognition (ASR), especially for\nlow-resource languages like Bangla. In this study, we explore the application\nof transformer-based models, specifically XLM-RoBERTa-large, to automatically\nrestore punctuation in unpunctuated Bangla text. We focus on predicting four\npunctuation marks: period, comma, question mark, and exclamation mark across\ndiverse text domains. To address the scarcity of annotated resources, we\nconstructed a large, varied training corpus and applied data augmentation\ntechniques. Our best-performing model, trained with an augmentation factor of\nalpha = 0.20%, achieves an accuracy of 97.1% on the News test set, 91.2% on the\nReference set, and 90.2% on the ASR set.\n  Results show strong generalization to reference and ASR transcripts,\ndemonstrating the model's effectiveness in real-world, noisy scenarios. This\nwork establishes a strong baseline for Bangla punctuation restoration and\ncontributes publicly available datasets and code to support future research in\nlow-resource NLP."}
{"id": "2506.06874", "pdf": "https://arxiv.org/pdf/2506.06874.pdf", "abs": "https://arxiv.org/abs/2506.06874", "title": "LLM-D12: A Dual-Dimensional Scale of Instrumental and Relational Dependencies on Large Language Models", "authors": ["Ala Yankouskaya", "Areej B. Babiker", "Syeda W. F. Rizvi", "Sameha Alshakhsi", "Magnus Liebherr", "Raian Ali"], "categories": ["cs.HC", "cs.AI", "Human-Centered Computing -- > Human computer interaction (HCI) -->\n  HCI design and evaluation methods"], "comment": null, "summary": "There is growing interest in understanding how people interact with large\nlanguage models (LLMs) and whether such models elicit dependency or even\naddictive behaviour. Validated tools to assess the extent to which individuals\nmay become dependent on LLMs are scarce and primarily build on classic\nbehavioral addiction symptoms, adapted to the context of LLM use. We view this\nas a conceptual limitation, as the LLM-human relationship is more nuanced and\nwarrants a fresh and distinct perspective. To address this gap, we developed\nand validated a new 12-item questionnaire to measure LLM dependency, referred\nto as LLM-D12. The scale was based on the authors' prior theoretical work, with\nitems developed accordingly and responses collected from 526 participants in\nthe UK. Exploratory and confirmatory factor analyses, performed on separate\nhalves of the total sample using a split-sample approach, supported a\ntwo-factor structure: Instrumental Dependency (six items) and Relationship\nDependency (six items). Instrumental Dependency reflects the extent to which\nindividuals rely on LLMs to support or collaborate in decision-making and\ncognitive tasks. Relationship Dependency captures the tendency to perceive LLMs\nas socially meaningful, sentient, or companion-like entities. The two-factor\nstructure demonstrated excellent internal consistency and clear discriminant\nvalidity. External validation confirmed both the conceptual foundation and the\ndistinction between the two subscales. The psychometric properties and\nstructure of our LLM-D12 scale were interpreted in light of the emerging view\nthat dependency on LLMs does not necessarily indicate dysfunction but may still\nreflect reliance levels that could become problematic in certain contexts."}
{"id": "2507.18451", "pdf": "https://arxiv.org/pdf/2507.18451.pdf", "abs": "https://arxiv.org/abs/2507.18451", "title": "Generation of Synthetic Clinical Text: A Systematic Review", "authors": ["Basel Alshaikhdeeb", "Ahmed Abdelmonem Hemedan", "Soumyabrata Ghosh", "Irina Balaur", "Venkata Satagopam"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Generating clinical synthetic text represents an effective solution for\ncommon clinical NLP issues like sparsity and privacy. This paper aims to\nconduct a systematic review on generating synthetic medical free-text by\nformulating quantitative analysis to three research questions concerning (i)\nthe purpose of generation, (ii) the techniques, and (iii) the evaluation\nmethods. We searched PubMed, ScienceDirect, Web of Science, Scopus, IEEE,\nGoogle Scholar, and arXiv databases for publications associated with generating\nsynthetic medical unstructured free-text. We have identified 94 relevant\narticles out of 1,398 collected ones. A great deal of attention has been given\nto the generation of synthetic medical text from 2018 onwards, where the main\npurpose of such a generation is towards text augmentation, assistive writing,\ncorpus building, privacy-preserving, annotation, and usefulness. Transformer\narchitectures were the main predominant technique used to generate the text,\nespecially the GPTs. On the other hand, there were four main aspects of\nevaluation, including similarity, privacy, structure, and utility, where\nutility was the most frequent method used to assess the generated synthetic\nmedical text. Although the generated synthetic medical text demonstrated a\nmoderate possibility to act as real medical documents in different downstream\nNLP tasks, it has proven to be a great asset as augmented, complementary to the\nreal documents, towards improving the accuracy and overcoming\nsparsity/undersampling issues. Yet, privacy is still a major issue behind\ngenerating synthetic medical text, where more human assessments are needed to\ncheck for the existence of any sensitive information. Despite that, advances in\ngenerating synthetic medical text will considerably accelerate the adoption of\nworkflows and pipeline development, discarding the time-consuming legalities of\ndata transfer."}
{"id": "2506.17606", "pdf": "https://arxiv.org/pdf/2506.17606.pdf", "abs": "https://arxiv.org/abs/2506.17606", "title": "Full-body WPT: wireless powering with meandered e-textiles", "authors": ["Ryo Takahashi", "Takashi Sato", "Wakako Yukita", "Tomoyuki Yokota", "Takao Someya", "Yoshihiro Kawahara"], "categories": ["cs.HC"], "comment": null, "summary": "We present Full-body WPT, wireless power networking around the human body\nusing a meandered textile coil. Unlike traditional inductive systems that emit\nstrong fields into the deep tissue inside the body, the meander coil enables\nlocalized generation of strong magnetic field constrained to the skin surface,\neven when scaled to the size of the human body. Such localized inductive system\nenhances both safety and efficiency of wireless power around the body.\nFurthermore, the use of low-loss conductive yarn achieve energy-efficient and\nlightweight design. We analyze the performance of our design through\nsimulations and experimental prototypes, demonstrating high power transfer\nefficiency and adaptability to user movement and posture. Our system provides a\nsafe and efficient distributed power network using meandered textile coils\nintegrated into wearable materials, highlighting the potential of body-centric\nwireless power networking as a foundational layer for ubiquitous health\nmonitoring, augmented reality, and human-machine interaction systems."}
{"id": "2507.18504", "pdf": "https://arxiv.org/pdf/2507.18504.pdf", "abs": "https://arxiv.org/abs/2507.18504", "title": "Not All Features Deserve Attention: Graph-Guided Dependency Learning for Tabular Data Generation with Language Models", "authors": ["Zheyu Zhang", "Shuo Yang", "Bardh Prenkaj", "Gjergji Kasneci"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have shown strong potential for tabular data\ngeneration by modeling textualized feature-value pairs. However, tabular data\ninherently exhibits sparse feature-level dependencies, where many feature\ninteractions are structurally insignificant. This creates a fundamental\nmismatch as LLMs' self-attention mechanism inevitably distributes focus across\nall pairs, diluting attention on critical relationships, particularly in\ndatasets with complex dependencies or semantically ambiguous features. To\naddress this limitation, we propose GraDe (Graph-Guided Dependency Learning), a\nnovel method that explicitly integrates sparse dependency graphs into LLMs'\nattention mechanism. GraDe employs a lightweight dynamic graph learning module\nguided by externally extracted functional dependencies, prioritizing key\nfeature interactions while suppressing irrelevant ones. Our experiments across\ndiverse real-world datasets demonstrate that GraDe outperforms existing\nLLM-based approaches by up to 12% on complex datasets while achieving\ncompetitive results with state-of-the-art approaches in synthetic data quality.\nOur method is minimally intrusive yet effective, offering a practical solution\nfor structure-aware tabular data modeling with LLMs."}
{"id": "2507.17248", "pdf": "https://arxiv.org/pdf/2507.17248.pdf", "abs": "https://arxiv.org/abs/2507.17248", "title": "Reality Proxy: Fluid Interactions with Real-World Objects in MR via Abstract Representations", "authors": ["Xiaoan Liu", "Difan Jia", "Xianhao Carton Liu", "Mar Gonzalez-Franco", "Chen Zhu-Tian"], "categories": ["cs.HC", "cs.AI", "cs.GR", "H.5.2; I.3.6"], "comment": "16 pages, 9 figures. Accepted for publication in UIST'25 (The 38th\n  Annual ACM Symposium on User Interface Software and Technology), Busan,\n  Republic of Korea, 28 Sep - 1 Oct 2025", "summary": "Interacting with real-world objects in Mixed Reality (MR) often proves\ndifficult when they are crowded, distant, or partially occluded, hindering\nstraightforward selection and manipulation. We observe that these difficulties\nstem from performing interaction directly on physical objects, where input is\ntightly coupled to their physical constraints. Our key insight is to decouple\ninteraction from these constraints by introducing proxies-abstract\nrepresentations of real-world objects. We embody this concept in Reality Proxy,\na system that seamlessly shifts interaction targets from physical objects to\ntheir proxies during selection. Beyond facilitating basic selection, Reality\nProxy uses AI to enrich proxies with semantic attributes and hierarchical\nspatial relationships of their corresponding physical objects, enabling novel\nand previously cumbersome interactions in MR - such as skimming,\nattribute-based filtering, navigating nested groups, and complex multi object\nselections - all without requiring new gestures or menu systems. We demonstrate\nReality Proxy's versatility across diverse scenarios, including office\ninformation retrieval, large-scale spatial navigation, and multi-drone control.\nAn expert evaluation suggests the system's utility and usability, suggesting\nthat proxy-based abstractions offer a powerful and generalizable interaction\nparadigm for future MR systems."}
{"id": "2507.18523", "pdf": "https://arxiv.org/pdf/2507.18523.pdf", "abs": "https://arxiv.org/abs/2507.18523", "title": "The Moral Gap of Large Language Models", "authors": ["Maciej Skorski", "Alina Landowska"], "categories": ["cs.CL", "cs.CY", "cs.HC", "cs.LG"], "comment": "preprint", "summary": "Moral foundation detection is crucial for analyzing social discourse and\ndeveloping ethically-aligned AI systems. While large language models excel\nacross diverse tasks, their performance on specialized moral reasoning remains\nunclear.\n  This study provides the first comprehensive comparison between\nstate-of-the-art LLMs and fine-tuned transformers across Twitter and Reddit\ndatasets using ROC, PR, and DET curve analysis.\n  Results reveal substantial performance gaps, with LLMs exhibiting high false\nnegative rates and systematic under-detection of moral content despite prompt\nengineering efforts. These findings demonstrate that task-specific fine-tuning\nremains superior to prompting for moral reasoning applications."}
{"id": "2312.12102", "pdf": "https://arxiv.org/pdf/2312.12102.pdf", "abs": "https://arxiv.org/abs/2312.12102", "title": "I-CEE: Tailoring Explanations of Image Classification Models to User Expertise", "authors": ["Yao Rong", "Peizhu Qian", "Vaibhav Unhelkar", "Enkelejda Kasneci"], "categories": ["cs.AI", "cs.CV", "cs.HC", "cs.LG"], "comment": null, "summary": "Effectively explaining decisions of black-box machine learning models is\ncritical to responsible deployment of AI systems that rely on them. Recognizing\ntheir importance, the field of explainable AI (XAI) provides several techniques\nto generate these explanations. Yet, there is relatively little emphasis on the\nuser (the explainee) in this growing body of work and most XAI techniques\ngenerate \"one-size-fits-all\" explanations. To bridge this gap and achieve a\nstep closer towards human-centered XAI, we present I-CEE, a framework that\nprovides Image Classification Explanations tailored to User Expertise. Informed\nby existing work, I-CEE explains the decisions of image classification models\nby providing the user with an informative subset of training data (i.e.,\nexample images), corresponding local explanations, and model decisions.\nHowever, unlike prior work, I-CEE models the informativeness of the example\nimages to depend on user expertise, resulting in different examples for\ndifferent users. We posit that by tailoring the example set to user expertise,\nI-CEE can better facilitate users' understanding and simulatability of the\nmodel. To evaluate our approach, we conduct detailed experiments in both\nsimulation and with human participants (N = 100) on multiple datasets.\nExperiments with simulated users show that I-CEE improves users' ability to\naccurately predict the model's decisions (simulatability) compared to\nbaselines, providing promising preliminary results. Experiments with human\nparticipants demonstrate that our method significantly improves user\nsimulatability accuracy, highlighting the importance of human-centered XAI"}
{"id": "2507.18542", "pdf": "https://arxiv.org/pdf/2507.18542.pdf", "abs": "https://arxiv.org/abs/2507.18542", "title": "Effective Multi-Task Learning for Biomedical Named Entity Recognition", "authors": ["João Ruano", "Gonçalo M. Correia", "Leonor Barreiros", "Afonso Mendes"], "categories": ["cs.CL"], "comment": "Accepted at the 24th BioNLP workshop (ACL2025), 15 pages, 3 figures", "summary": "Biomedical Named Entity Recognition presents significant challenges due to\nthe complexity of biomedical terminology and inconsistencies in annotation\nacross datasets. This paper introduces SRU-NER (Slot-based Recurrent Unit NER),\na novel approach designed to handle nested named entities while integrating\nmultiple datasets through an effective multi-task learning strategy. SRU-NER\nmitigates annotation gaps by dynamically adjusting loss computation to avoid\npenalizing predictions of entity types absent in a given dataset. Through\nextensive experiments, including a cross-corpus evaluation and human assessment\nof the model's predictions, SRU-NER achieves competitive performance in\nbiomedical and general-domain NER tasks, while improving cross-domain\ngeneralization."}
{"id": "2409.13725", "pdf": "https://arxiv.org/pdf/2409.13725.pdf", "abs": "https://arxiv.org/abs/2409.13725", "title": "Identity-related Speech Suppression in Generative AI Content Moderation", "authors": ["Grace Proebsting", "Oghenefejiro Isaacs Anigboro", "Charlie M. Crawford", "Danaé Metaxa", "Sorelle A. Friedler"], "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": "ACM Conference on Equity and Access in Algorithms, Mechanisms, and\n  Optimization, 2025", "summary": "Automated content moderation has long been used to help identify and filter\nundesired user-generated content online. But such systems have a history of\nincorrectly flagging content by and about marginalized identities for removal.\nGenerative AI systems now use such filters to keep undesired generated content\nfrom being created by or shown to users. While a lot of focus has been given to\nmaking sure such systems do not produce undesired outcomes, considerably less\nattention has been paid to making sure appropriate text can be generated. From\nclassrooms to Hollywood, as generative AI is increasingly used for creative or\nexpressive text generation, whose stories will these technologies allow to be\ntold, and whose will they suppress?\n  In this paper, we define and introduce measures of speech suppression,\nfocusing on speech related to different identity groups incorrectly filtered by\na range of content moderation APIs. Using both short-form, user-generated\ndatasets traditional in content moderation and longer generative AI-focused\ndata, including two datasets we introduce in this work, we create a benchmark\nfor measurement of speech suppression for nine identity groups. Across one\ntraditional and four generative AI-focused automated content moderation\nservices tested, we find that identity-related speech is more likely to be\nincorrectly suppressed than other speech. We find that reasons for incorrect\nflagging behavior vary by identity based on stereotypes and text associations,\nwith, e.g., disability-related content more likely to be flagged for self-harm\nor health-related reasons while non-Christian content is more likely to be\nflagged as violent or hateful. As generative AI systems are increasingly used\nfor creative work, we urge further attention to how this may impact the\ncreation of identity-related content."}
{"id": "2507.18546", "pdf": "https://arxiv.org/pdf/2507.18546.pdf", "abs": "https://arxiv.org/abs/2507.18546", "title": "GLiNER2: An Efficient Multi-Task Information Extraction System with Schema-Driven Interface", "authors": ["Urchade Zaratiana", "Gil Pasternak", "Oliver Boyd", "George Hurn-Maloney", "Ash Lewis"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Information extraction (IE) is fundamental to numerous NLP applications, yet\nexisting solutions often require specialized models for different tasks or rely\non computationally expensive large language models. We present GLiNER2, a\nunified framework that enhances the original GLiNER architecture to support\nnamed entity recognition, text classification, and hierarchical structured data\nextraction within a single efficient model. Built pretrained transformer\nencoder architecture, GLiNER2 maintains CPU efficiency and compact size while\nintroducing multi-task composition through an intuitive schema-based interface.\nOur experiments demonstrate competitive performance across extraction and\nclassification tasks with substantial improvements in deployment accessibility\ncompared to LLM-based alternatives. We release GLiNER2 as an open-source\npip-installable library with pre-trained models and documentation at\nhttps://github.com/fastino-ai/GLiNER2."}
{"id": "2504.14928", "pdf": "https://arxiv.org/pdf/2504.14928.pdf", "abs": "https://arxiv.org/abs/2504.14928", "title": "EducationQ: Evaluating LLMs' Teaching Capabilities Through Multi-Agent Dialogue Framework", "authors": ["Yao Shi", "Rongkeng Liang", "Yong Xu"], "categories": ["cs.AI", "cs.CE", "cs.CL", "cs.CY", "cs.HC"], "comment": "Paper URL: https://aclanthology.org/2025.acl-long.1576/; Presentation\n  Video: https://www.youtube.com/watch?v=j63ooKE50I0", "summary": "Large language models (LLMs) increasingly serve as educational tools, yet\nevaluating their teaching capabilities remains challenging due to the\nresource-intensive, context-dependent, and methodologically complex nature of\nteacher-student interactions. We introduce EducationQ, a multi-agent dialogue\nframework that efficiently assesses teaching capabilities through simulated\ndynamic educational scenarios, featuring specialized agents for teaching,\nlearning, and evaluation. Testing 14 LLMs across major AI Organizations\n(OpenAI, Meta, Google, Anthropic, and others) on 1,498 questions spanning 13\ndisciplines and 10 difficulty levels reveals that teaching effectiveness does\nnot correlate linearly with model scale or general reasoning capabilities -\nwith some smaller open-source models outperforming larger commercial\ncounterparts in teaching contexts. This finding highlights a critical gap in\ncurrent evaluations that prioritize knowledge recall over interactive pedagogy.\nOur mixed-methods evaluation, combining quantitative metrics with qualitative\nanalysis and expert case studies, identifies distinct pedagogical strengths\nemployed by top-performing models (e.g., sophisticated questioning strategies,\nadaptive feedback mechanisms). Human expert evaluations show 78% agreement with\nour automated qualitative analysis of effective teaching behaviors, validating\nour methodology. EducationQ demonstrates that LLMs-as-teachers require\nspecialized optimization beyond simple scaling, suggesting next-generation\neducational AI prioritize targeted enhancement of specific pedagogical\neffectiveness."}
{"id": "2507.18562", "pdf": "https://arxiv.org/pdf/2507.18562.pdf", "abs": "https://arxiv.org/abs/2507.18562", "title": "GIIFT: Graph-guided Inductive Image-free Multimodal Machine Translation", "authors": ["Jiafeng Xiong", "Yuting Zhao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multimodal Machine Translation (MMT) has demonstrated the significant help of\nvisual information in machine translation. However, existing MMT methods face\nchallenges in leveraging the modality gap by enforcing rigid visual-linguistic\nalignment whilst being confined to inference within their trained multimodal\ndomains. In this work, we construct novel multimodal scene graphs to preserve\nand integrate modality-specific information and introduce GIIFT, a two-stage\nGraph-guided Inductive Image-Free MMT framework that uses a cross-modal Graph\nAttention Network adapter to learn multimodal knowledge in a unified fused\nspace and inductively generalize it to broader image-free translation domains.\nExperimental results on the Multi30K dataset of English-to-French and\nEnglish-to-German tasks demonstrate that our GIIFT surpasses existing\napproaches and achieves the state-of-the-art, even without images during\ninference. Results on the WMT benchmark show significant improvements over the\nimage-free translation baselines, demonstrating the strength of GIIFT towards\ninductive image-free inference."}
{"id": "2506.05606", "pdf": "https://arxiv.org/pdf/2506.05606.pdf", "abs": "https://arxiv.org/abs/2506.05606", "title": "OPeRA: A Dataset of Observation, Persona, Rationale, and Action for Evaluating LLMs on Human Online Shopping Behavior Simulation", "authors": ["Ziyi Wang", "Yuxuan Lu", "Wenbo Li", "Amirali Amini", "Bo Sun", "Yakov Bart", "Weimin Lyu", "Jiri Gesi", "Tian Wang", "Jing Huang", "Yu Su", "Upol Ehsan", "Malihe Alikhani", "Toby Jia-Jun Li", "Lydia Chilton", "Dakuo Wang"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Can large language models (LLMs) accurately simulate the next web action of a\nspecific user? While LLMs have shown promising capabilities in generating\n``believable'' human behaviors, evaluating their ability to mimic real user\nbehaviors remains an open challenge, largely due to the lack of high-quality,\npublicly available datasets that capture both the observable actions and the\ninternal reasoning of an actual human user. To address this gap, we introduce\nOPERA, a novel dataset of Observation, Persona, Rationale, and Action collected\nfrom real human participants during online shopping sessions. OPERA is the\nfirst public dataset that comprehensively captures: user personas, browser\nobservations, fine-grained web actions, and self-reported just-in-time\nrationales. We developed both an online questionnaire and a custom browser\nplugin to gather this dataset with high fidelity. Using OPERA, we establish the\nfirst benchmark to evaluate how well current LLMs can predict a specific user's\nnext action and rationale with a given persona and <observation, action,\nrationale> history. This dataset lays the groundwork for future research into\nLLM agents that aim to act as personalized digital twins for human."}
{"id": "2507.18570", "pdf": "https://arxiv.org/pdf/2507.18570.pdf", "abs": "https://arxiv.org/abs/2507.18570", "title": "Hybrid Tokenization Strategy for DNA Language Model using Byte Pair Encoding and K-MER Methods", "authors": ["Ganesh Sapkota", "Md Hasibur Rahman"], "categories": ["cs.CL"], "comment": null, "summary": "This paper presents a novel hybrid tokenization strategy that enhances the\nperformance of DNA Language Models (DLMs) by combining 6-mer tokenization with\nByte Pair Encoding (BPE-600). Traditional k-mer tokenization is effective at\ncapturing local DNA sequence structures but often faces challenges, including\nuneven token distribution and a limited understanding of global sequence\ncontext. To address these limitations, we propose merging unique 6mer tokens\nwith optimally selected BPE tokens generated through 600 BPE cycles. This\nhybrid approach ensures a balanced and context-aware vocabulary, enabling the\nmodel to capture both short and long patterns within DNA sequences\nsimultaneously. A foundational DLM trained on this hybrid vocabulary was\nevaluated using next-k-mer prediction as a fine-tuning task, demonstrating\nsignificantly improved performance. The model achieved prediction accuracies of\n10.78% for 3-mers, 10.1% for 4-mers, and 4.12% for 5-mers, outperforming\nstate-of-the-art models such as NT, DNABERT2, and GROVER. These results\nhighlight the ability of the hybrid tokenization strategy to preserve both the\nlocal sequence structure and global contextual information in DNA modeling.\nThis work underscores the importance of advanced tokenization methods in\ngenomic language modeling and lays a robust foundation for future applications\nin downstream DNA sequence analysis and biological research."}
{"id": "2507.18578", "pdf": "https://arxiv.org/pdf/2507.18578.pdf", "abs": "https://arxiv.org/abs/2507.18578", "title": "Wide-In, Narrow-Out: Revokable Decoding for Efficient and Effective DLLMs", "authors": ["Feng Hong", "Geng Yu", "Yushi Ye", "Haicheng Huang", "Huangjie Zheng", "Ya Zhang", "Yanfeng Wang", "Jiangchao Yao"], "categories": ["cs.CL"], "comment": null, "summary": "Diffusion Large Language Models (DLLMs) have emerged as a compelling\nalternative to Autoregressive models, designed for fast parallel generation.\nHowever, existing DLLMs are plagued by a severe quality-speed trade-off, where\nfaster parallel decoding leads to significant performance degradation. We\nattribute this to the irreversibility of standard decoding in DLLMs, which is\neasily polarized into the wrong decoding direction along with early error\ncontext accumulation. To resolve this, we introduce Wide-In, Narrow-Out (WINO),\na training-free decoding algorithm that enables revokable decoding in DLLMs.\nWINO employs a parallel draft-and-verify mechanism, aggressively drafting\nmultiple tokens while simultaneously using the model's bidirectional context to\nverify and re-mask suspicious ones for refinement. Verified in open-source\nDLLMs like LLaDA and MMaDA, WINO is shown to decisively improve the\nquality-speed trade-off. For instance, on the GSM8K math benchmark, it\naccelerates inference by 6$\\times$ while improving accuracy by 2.58%; on\nFlickr30K captioning, it achieves a 10$\\times$ speedup with higher performance.\nMore comprehensive experiments are conducted to demonstrate the superiority and\nprovide an in-depth understanding of WINO."}
{"id": "2507.18580", "pdf": "https://arxiv.org/pdf/2507.18580.pdf", "abs": "https://arxiv.org/abs/2507.18580", "title": "System Report for CCL25-Eval Task 10: SRAG-MAV for Fine-Grained Chinese Hate Speech Recognition", "authors": ["Jiahao Wang", "Ramen Liu", "Longhui Zhang", "Jing Li"], "categories": ["cs.CL"], "comment": "8 pages, 3 figures, accepted as oral presentation at CCL25-Eval", "summary": "This paper presents our system for CCL25-Eval Task 10, addressing\nFine-Grained Chinese Hate Speech Recognition (FGCHSR). We propose a novel\nSRAG-MAV framework that synergistically integrates task reformulation(TR),\nSelf-Retrieval-Augmented Generation (SRAG), and Multi-Round Accumulative Voting\n(MAV). Our method reformulates the quadruplet extraction task into triplet\nextraction, uses dynamic retrieval from the training set to create contextual\nprompts, and applies multi-round inference with voting to improve output\nstability and performance. Our system, based on the Qwen2.5-7B model, achieves\na Hard Score of 26.66, a Soft Score of 48.35, and an Average Score of 37.505 on\nthe STATE ToxiCN dataset, significantly outperforming baselines such as GPT-4o\n(Average Score 15.63) and fine-tuned Qwen2.5-7B (Average Score 35.365). The\ncode is available at https://github.com/king-wang123/CCL25-SRAG-MAV."}
{"id": "2507.18584", "pdf": "https://arxiv.org/pdf/2507.18584.pdf", "abs": "https://arxiv.org/abs/2507.18584", "title": "AQuilt: Weaving Logic and Self-Inspection into Low-Cost, High-Relevance Data Synthesis for Specialist LLMs", "authors": ["Xiaopeng Ke", "Hexuan Deng", "Xuebo Liu", "Jun Rao", "Zhenxi Song", "Jun Yu", "Min Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "32 pages, 4 figures", "summary": "Despite the impressive performance of large language models (LLMs) in general\ndomains, they often underperform in specialized domains. Existing approaches\ntypically rely on data synthesis methods and yield promising results by using\nunlabeled data to capture domain-specific features. However, these methods\neither incur high computational costs or suffer from performance limitations,\nwhile also demonstrating insufficient generalization across different tasks. To\naddress these challenges, we propose AQuilt, a framework for constructing\ninstruction-tuning data for any specialized domains from corresponding\nunlabeled data, including Answer, Question, Unlabeled data, Inspection, Logic,\nand Task type. By incorporating logic and inspection, we encourage reasoning\nprocesses and self-inspection to enhance model performance. Moreover,\ncustomizable task instructions enable high-quality data generation for any\ntask. As a result, we construct a dataset of 703k examples to train a powerful\ndata synthesis model. Experiments show that AQuilt is comparable to DeepSeek-V3\nwhile utilizing just 17% of the production cost. Further analysis demonstrates\nthat our generated data exhibits higher relevance to downstream tasks. Source\ncode, models, and scripts are available at https://github.com/Krueske/AQuilt."}
{"id": "2507.18618", "pdf": "https://arxiv.org/pdf/2507.18618.pdf", "abs": "https://arxiv.org/abs/2507.18618", "title": "TRPrompt: Bootstrapping Query-Aware Prompt Optimization from Textual Rewards", "authors": ["Andreea Nica", "Ivan Zakazov", "Nicolas Mario Baldwin", "Saibo Geng", "Robert West"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Prompt optimization improves the reasoning abilities of large language models\n(LLMs) without requiring parameter updates to the target model. Following\nheuristic-based \"Think step by step\" approaches, the field has evolved in two\nmain directions: while one group of methods uses textual feedback to elicit\nimproved prompts from general-purpose LLMs in a training-free way, a concurrent\nline of research relies on numerical rewards to train a special prompt model,\ntailored for providing optimal prompts to the target model. In this paper, we\nintroduce the Textual Reward Prompt framework (TRPrompt), which unifies these\napproaches by directly incorporating textual feedback into training of the\nprompt model. Our framework does not require prior dataset collection and is\nbeing iteratively improved with the feedback on the generated prompts. When\ncoupled with the capacity of an LLM to internalize the notion of what a \"good\"\nprompt is, the high-resolution signal provided by the textual rewards allows us\nto train a prompt model yielding state-of-the-art query-specific prompts for\nthe problems from the challenging math datasets GSMHard and MATH."}
{"id": "2507.18624", "pdf": "https://arxiv.org/pdf/2507.18624.pdf", "abs": "https://arxiv.org/abs/2507.18624", "title": "Checklists Are Better Than Reward Models For Aligning Language Models", "authors": ["Vijay Viswanathan", "Yanchao Sun", "Shuang Ma", "Xiang Kong", "Meng Cao", "Graham Neubig", "Tongshuang Wu"], "categories": ["cs.CL"], "comment": null, "summary": "Language models must be adapted to understand and follow user instructions.\nReinforcement learning is widely used to facilitate this -- typically using\nfixed criteria such as \"helpfulness\" and \"harmfulness\". In our work, we instead\npropose using flexible, instruction-specific criteria as a means of broadening\nthe impact that reinforcement learning can have in eliciting instruction\nfollowing. We propose \"Reinforcement Learning from Checklist Feedback\" (RLCF).\nFrom instructions, we extract checklists and evaluate how well responses\nsatisfy each item - using both AI judges and specialized verifier programs -\nthen combine these scores to compute rewards for RL. We compare RLCF with other\nalignment methods applied to a strong instruction following model\n(Qwen2.5-7B-Instruct) on five widely-studied benchmarks -- RLCF is the only\nmethod to improve performance on every benchmark, including a 4-point boost in\nhard satisfaction rate on FollowBench, a 6-point increase on InFoBench, and a\n3-point rise in win rate on Arena-Hard. These results establish checklist\nfeedback as a key tool for improving language models' support of queries that\nexpress a multitude of needs."}
{"id": "2507.17753", "pdf": "https://arxiv.org/pdf/2507.17753.pdf", "abs": "https://arxiv.org/abs/2507.17753", "title": "Exploring Communication Strategies for Collaborative LLM Agents in Mathematical Problem-Solving", "authors": ["Liang Zhang", "Xiaoming Zhai", "Jionghao Lin", "Jionghao Lin", "Jennifer Kleiman", "Diego Zapata-Rivera", "Carol Forsyth", "Yang Jiang", "Xiangen Hu", "Arthur C. Graesser"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "Large Language Model (LLM) agents are increasingly utilized in AI-aided\neducation to support tutoring and learning. Effective communication strategies\namong LLM agents improve collaborative problem-solving efficiency and\nfacilitate cost-effective adoption in education. However, little research has\nsystematically evaluated the impact of different communication strategies on\nagents' problem-solving. Our study examines four communication modes,\n\\textit{teacher-student interaction}, \\textit{peer-to-peer collaboration},\n\\textit{reciprocal peer teaching}, and \\textit{critical debate}, in a\ndual-agent, chat-based mathematical problem-solving environment using the\nOpenAI GPT-4o model. Evaluated on the MATH dataset, our results show that\ndual-agent setups outperform single agents, with \\textit{peer-to-peer\ncollaboration} achieving the highest accuracy. Dialogue acts like statements,\nacknowledgment, and hints play a key role in collaborative problem-solving.\nWhile multi-agent frameworks enhance computational tasks, effective\ncommunication strategies are essential for tackling complex problems in AI\neducation."}
{"id": "2507.17797", "pdf": "https://arxiv.org/pdf/2507.17797.pdf", "abs": "https://arxiv.org/abs/2507.17797", "title": "GenSelect: A Generative Approach to Best-of-N", "authors": ["Shubham Toshniwal", "Ivan Sorokin", "Aleksander Ficek", "Ivan Moshkov", "Igor Gitman"], "categories": ["cs.LG", "cs.CL"], "comment": "Presented at the 2nd AI for MATH Workshop @ ICML", "summary": "Generative reward models with parallel sampling have enabled effective\ntest-time scaling for reasoning tasks. Current approaches employ pointwise\nscoring of individual solutions or pairwise comparisons. However, pointwise\nmethods underutilize LLMs' comparative abilities, while pairwise methods scale\ninefficiently with larger sampling budgets. We introduce GenSelect, where the\nLLM uses long reasoning to select the best solution among N candidates. This\nleverages LLMs' comparative strengths while scaling efficiently across parallel\nsampling budgets. For math reasoning, we demonstrate that reasoning models,\nsuch as QwQ and DeepSeek-R1-0528, excel at GenSelect, outperforming existing\nscoring approaches with simple prompting."}
{"id": "2507.17937", "pdf": "https://arxiv.org/pdf/2507.17937.pdf", "abs": "https://arxiv.org/abs/2507.17937", "title": "Bob's Confetti: Phonetic Memorization Attacks in Music and Video Generation", "authors": ["Jaechul Roh", "Zachary Novack", "Yuefeng Peng", "Niloofar Mireshghallah", "Taylor Berg-Kirkpatrick", "Amir Houmansadr"], "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": null, "summary": "Lyrics-to-Song (LS2) generation models promise end-to-end music synthesis\nfrom text, yet their vulnerability to training data memorization remains\nunderexplored. We introduce Adversarial PhoneTic Prompting (APT), a novel\nattack where lyrics are semantically altered while preserving their acoustic\nstructure through homophonic substitutions (e.g., Eminem's famous \"mom's\nspaghetti\" $\\rightarrow$ \"Bob's confetti\"). Despite these distortions, we\nuncover a powerful form of sub-lexical memorization: models like SUNO and YuE\nregenerate outputs strikingly similar to known training content, achieving high\nsimilarity across audio-domain metrics, including CLAP, AudioJudge, and\nCoverID. This vulnerability persists across multiple languages and genres. More\nsurprisingly, we discover that phoneme-altered lyrics alone can trigger visual\nmemorization in text-to-video models. When prompted with phonetically modified\nlyrics from Lose Yourself, Veo 3 reconstructs visual elements from the original\nmusic video -- including character appearance and scene composition -- despite\nno visual cues in the prompt. We term this phenomenon phonetic-to-visual\nregurgitation. Together, these findings expose a critical vulnerability in\ntranscript-conditioned multimodal generation: phonetic prompting alone can\nunlock memorized audiovisual content, raising urgent questions about copyright,\nsafety, and content provenance in modern generative systems. Example\ngenerations are available on our demo page (jrohsc.github.io/music_attack/)."}
{"id": "2507.18009", "pdf": "https://arxiv.org/pdf/2507.18009.pdf", "abs": "https://arxiv.org/abs/2507.18009", "title": "GRR-CoCa: Leveraging LLM Mechanisms in Multimodal Model Architectures", "authors": ["Jake R. Patock", "Nicole Catherine Lewis", "Kevin McCoy", "Christina Gomez", "Canling Chen", "Lorenzo Luzi"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "12 pages, 2 figures", "summary": "State-of-the-art (SOTA) image and text generation models are multimodal\nmodels that have many similarities to large language models (LLMs). Despite\nachieving strong performances, leading foundational multimodal model\narchitectures frequently lag behind the architectural sophistication of\ncontemporary LLMs. We propose GRR-CoCa, an improved SOTA Contrastive Captioner\n(CoCa) model that incorporates Gaussian error gated linear units, root mean\nsquared normalization, and rotary positional embedding into the textual\ndecoders and the vision transformer (ViT) encoder. Each architectural\nmodification has been shown to improve model performance in LLMs, but has yet\nto be adopted in CoCa. We benchmarked GRR-CoCa against Baseline CoCa, a model\nwith the same modified textual decoders but with CoCa's original ViT encoder.\nWe used standard pretraining and fine-tuning workflows to benchmark the models\non contrastive and generative tasks. Our GRR-CoCa significantly outperformed\nBaseline CoCa on the pretraining dataset and three diverse fine-tuning\ndatasets. Pretraining improvements were 27.25% in contrastive loss, 3.71% in\nperplexity, and 7.15% in CoCa loss. The average fine-tuning improvements were\n13.66% in contrastive loss, 5.18% in perplexity, and 5.55% in CoCa loss. We\nshow that GRR-CoCa's modified architecture improves performance and\ngeneralization across vision-language domains."}
{"id": "2507.18053", "pdf": "https://arxiv.org/pdf/2507.18053.pdf", "abs": "https://arxiv.org/abs/2507.18053", "title": "RECALLED: An Unbounded Resource Consumption Attack on Large Vision-Language Models", "authors": ["Haoran Gao", "Yuanhe Zhang", "Zhenhong Zhou", "Lei Jiang", "Fanyu Meng", "Yujia Xiao", "Kun Wang", "Yang Liu", "Junlan Feng"], "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Resource Consumption Attacks (RCAs) have emerged as a significant threat to\nthe deployment of Large Language Models (LLMs). With the integration of vision\nmodalities, additional attack vectors exacerbate the risk of RCAs in large\nvision-language models (LVLMs). However, existing red-teaming studies have\nlargely overlooked visual inputs as a potential attack surface, resulting in\ninsufficient mitigation strategies against RCAs in LVLMs. To address this gap,\nwe propose RECALLED (\\textbf{RE}source \\textbf{C}onsumption \\textbf{A}ttack on\n\\textbf{L}arge Vision-\\textbf{L}anguag\\textbf{E} Mo\\textbf{D}els), the first\napproach for exploiting visual modalities to trigger unbounded RCAs\nred-teaming. First, we present \\textit{Vision Guided Optimization}, a\nfine-grained pixel-level optimization, to obtain \\textit{Output Recall}\nadversarial perturbations, which can induce repeating output. Then, we inject\nthe perturbations into visual inputs, triggering unbounded generations to\nachieve the goal of RCAs. Additionally, we introduce \\textit{Multi-Objective\nParallel Losses} to generate universal attack templates and resolve\noptimization conflicts when intending to implement parallel attacks. Empirical\nresults demonstrate that RECALLED increases service response latency by over 26\n$\\uparrow$, resulting in an additional 20\\% increase in GPU utilization and\nmemory consumption. Our study exposes security vulnerabilities in LVLMs and\nestablishes a red-teaming framework that can facilitate future defense\ndevelopment against RCAs."}
{"id": "2507.18071", "pdf": "https://arxiv.org/pdf/2507.18071.pdf", "abs": "https://arxiv.org/abs/2507.18071", "title": "Group Sequence Policy Optimization", "authors": ["Chujie Zheng", "Shixuan Liu", "Mingze Li", "Xiong-Hui Chen", "Bowen Yu", "Chang Gao", "Kai Dang", "Yuqiong Liu", "Rui Men", "An Yang", "Jingren Zhou", "Junyang Lin"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "This paper introduces Group Sequence Policy Optimization (GSPO), our stable,\nefficient, and performant reinforcement learning algorithm for training large\nlanguage models. Unlike previous algorithms that adopt token-level importance\nratios, GSPO defines the importance ratio based on sequence likelihood and\nperforms sequence-level clipping, rewarding, and optimization. We demonstrate\nthat GSPO achieves superior training efficiency and performance compared to the\nGRPO algorithm, notably stabilizes Mixture-of-Experts (MoE) RL training, and\nhas the potential for simplifying the design of RL infrastructure. These merits\nof GSPO have contributed to the remarkable improvements in the latest Qwen3\nmodels."}
{"id": "2507.18115", "pdf": "https://arxiv.org/pdf/2507.18115.pdf", "abs": "https://arxiv.org/abs/2507.18115", "title": "Agentic AI framework for End-to-End Medical Data Inference", "authors": ["Soorya Ram Shimgekar", "Shayan Vassef", "Abhay Goyal", "Navin Kumar", "Koustuv Saha"], "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.ET", "cs.LG"], "comment": "10 pages, 5 figures, 2 tables, BIBM conference", "summary": "Building and deploying machine learning solutions in healthcare remains\nexpensive and labor-intensive due to fragmented preprocessing workflows, model\ncompatibility issues, and stringent data privacy constraints. In this work, we\nintroduce an Agentic AI framework that automates the entire clinical data\npipeline, from ingestion to inference, through a system of modular,\ntask-specific agents. These agents handle both structured and unstructured\ndata, enabling automatic feature selection, model selection, and preprocessing\nrecommendation without manual intervention. We evaluate the system on publicly\navailable datasets from geriatrics, palliative care, and colonoscopy imaging.\nFor example, in the case of structured data (anxiety data) and unstructured\ndata (colonoscopy polyps data), the pipeline begins with file-type detection by\nthe Ingestion Identifier Agent, followed by the Data Anonymizer Agent ensuring\nprivacy compliance, where we first identify the data type and then anonymize\nit. The Feature Extraction Agent identifies features using an embedding-based\napproach for tabular data, extracting all column names, and a multi-stage\nMedGemma-based approach for image data, which infers modality and disease name.\nThese features guide the Model-Data Feature Matcher Agent in selecting the\nbest-fit model from a curated repository. The Preprocessing Recommender Agent\nand Preprocessing Implementor Agent then apply tailored preprocessing based on\ndata type and model requirements. Finally, the ``Model Inference Agent\" runs\nthe selected model on the uploaded data and generates interpretable outputs\nusing tools like SHAP, LIME, and DETR attention maps. By automating these\nhigh-friction stages of the ML lifecycle, the proposed framework reduces the\nneed for repeated expert intervention, offering a scalable, cost-efficient\npathway for operationalizing AI in clinical environments."}
{"id": "2507.18161", "pdf": "https://arxiv.org/pdf/2507.18161.pdf", "abs": "https://arxiv.org/abs/2507.18161", "title": "Recent Trends in Distant Conversational Speech Recognition: A Review of CHiME-7 and 8 DASR Challenges", "authors": ["Samuele Cornell", "Christoph Boeddeker", "Taejin Park", "He Huang", "Desh Raj", "Matthew Wiesner", "Yoshiki Masuyama", "Xuankai Chang", "Zhong-Qiu Wang", "Stefano Squartini", "Paola Garcia", "Shinji Watanabe"], "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": null, "summary": "The CHiME-7 and 8 distant speech recognition (DASR) challenges focus on\nmulti-channel, generalizable, joint automatic speech recognition (ASR) and\ndiarization of conversational speech. With participation from 9 teams\nsubmitting 32 diverse systems, these challenges have contributed to\nstate-of-the-art research in the field. This paper outlines the challenges'\ndesign, evaluation metrics, datasets, and baseline systems while analyzing key\ntrends from participant submissions. From this analysis it emerges that: 1)\nMost participants use end-to-end (e2e) ASR systems, whereas hybrid systems were\nprevalent in previous CHiME challenges. This transition is mainly due to the\navailability of robust large-scale pre-trained models, which lowers the data\nburden for e2e-ASR. 2) Despite recent advances in neural speech separation and\nenhancement (SSE), all teams still heavily rely on guided source separation,\nsuggesting that current neural SSE techniques are still unable to reliably deal\nwith complex scenarios and different recording setups. 3) All best systems\nemploy diarization refinement via target-speaker diarization techniques.\nAccurate speaker counting in the first diarization pass is thus crucial to\navoid compounding errors and CHiME-8 DASR participants especially focused on\nthis part. 4) Downstream evaluation via meeting summarization can correlate\nweakly with transcription quality due to the remarkable effectiveness of\nlarge-language models in handling errors. On the NOTSOFAR-1 scenario, even\nsystems with over 50\\% time-constrained minimum permutation WER can perform\nroughly on par with the most effective ones (around 11\\%). 5) Despite recent\nprogress, accurately transcribing spontaneous speech in challenging acoustic\nenvironments remains difficult, even when using computationally intensive\nsystem ensembles."}
{"id": "2507.18302", "pdf": "https://arxiv.org/pdf/2507.18302.pdf", "abs": "https://arxiv.org/abs/2507.18302", "title": "LoRA-Leak: Membership Inference Attacks Against LoRA Fine-tuned Language Models", "authors": ["Delong Ran", "Xinlei He", "Tianshuo Cong", "Anyu Wang", "Qi Li", "Xiaoyun Wang"], "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Language Models (LMs) typically adhere to a \"pre-training and fine-tuning\"\nparadigm, where a universal pre-trained model can be fine-tuned to cater to\nvarious specialized domains. Low-Rank Adaptation (LoRA) has gained the most\nwidespread use in LM fine-tuning due to its lightweight computational cost and\nremarkable performance. Because the proportion of parameters tuned by LoRA is\nrelatively small, there might be a misleading impression that the LoRA\nfine-tuning data is invulnerable to Membership Inference Attacks (MIAs).\nHowever, we identify that utilizing the pre-trained model can induce more\ninformation leakage, which is neglected by existing MIAs. Therefore, we\nintroduce LoRA-Leak, a holistic evaluation framework for MIAs against the\nfine-tuning datasets of LMs. LoRA-Leak incorporates fifteen membership\ninference attacks, including ten existing MIAs, and five improved MIAs that\nleverage the pre-trained model as a reference. In experiments, we apply\nLoRA-Leak to three advanced LMs across three popular natural language\nprocessing tasks, demonstrating that LoRA-based fine-tuned LMs are still\nvulnerable to MIAs (e.g., 0.775 AUC under conservative fine-tuning settings).\nWe also applied LoRA-Leak to different fine-tuning settings to understand the\nresulting privacy risks. We further explore four defenses and find that only\ndropout and excluding specific LM layers during fine-tuning effectively\nmitigate MIA risks while maintaining utility. We highlight that under the\n\"pre-training and fine-tuning\" paradigm, the existence of the pre-trained model\nmakes MIA a more severe risk for LoRA-based LMs. We hope that our findings can\nprovide guidance on data privacy protection for specialized LM providers."}
{"id": "2507.18455", "pdf": "https://arxiv.org/pdf/2507.18455.pdf", "abs": "https://arxiv.org/abs/2507.18455", "title": "LLM-based Embedders for Prior Case Retrieval", "authors": ["Damith Premasiri", "Tharindu Ranasinghe", "Ruslan Mitkov"], "categories": ["cs.IR", "cs.CL"], "comment": "Accepted in Recent Advancements in Natural Language Processing (RANLP\n  2025) conference", "summary": "In common law systems, legal professionals such as lawyers and judges rely on\nprecedents to build their arguments. As the volume of cases has grown massively\nover time, effectively retrieving prior cases has become essential. Prior case\nretrieval (PCR) is an information retrieval (IR) task that aims to\nautomatically identify the most relevant court cases for a specific query from\na large pool of potential candidates. While IR methods have seen several\nparadigm shifts over the last few years, the vast majority of PCR methods\ncontinue to rely on traditional IR methods, such as BM25. The state-of-the-art\ndeep learning IR methods have not been successful in PCR due to two key\nchallenges: i. Lengthy legal text limitation; when using the powerful\nBERT-based transformer models, there is a limit of input text lengths, which\ninevitably requires to shorten the input via truncation or division with a loss\nof legal context information. ii. Lack of legal training data; due to data\nprivacy concerns, available PCR datasets are often limited in size, making it\ndifficult to train deep learning-based models effectively. In this research, we\naddress these challenges by leveraging LLM-based text embedders in PCR.\nLLM-based embedders support longer input lengths, and since we use them in an\nunsupervised manner, they do not require training data, addressing both\nchallenges simultaneously. In this paper, we evaluate state-of-the-art\nLLM-based text embedders in four PCR benchmark datasets and show that they\noutperform BM25 and supervised transformer-based models."}
{"id": "2507.18572", "pdf": "https://arxiv.org/pdf/2507.18572.pdf", "abs": "https://arxiv.org/abs/2507.18572", "title": "PosterMate: Audience-driven Collaborative Persona Agents for Poster Design", "authors": ["Donghoon Shin", "Daniel Lee", "Gary Hsieh", "Gromit Yeuk-Yin Chan"], "categories": ["cs.HC", "cs.AI", "cs.CL", "H.5.2; I.2.7"], "comment": null, "summary": "Poster designing can benefit from synchronous feedback from target audiences.\nHowever, gathering audiences with diverse perspectives and reconciling them on\ndesign edits can be challenging. Recent generative AI models present\nopportunities to simulate human-like interactions, but it is unclear how they\nmay be used for feedback processes in design. We introduce PosterMate, a poster\ndesign assistant that facilitates collaboration by creating audience-driven\npersona agents constructed from marketing documents. PosterMate gathers\nfeedback from each persona agent regarding poster components, and stimulates\ndiscussion with the help of a moderator to reach a conclusion. These\nagreed-upon edits can then be directly integrated into the poster design.\nThrough our user study (N=12), we identified the potential of PosterMate to\ncapture overlooked viewpoints, while serving as an effective prototyping tool.\nAdditionally, our controlled online evaluation (N=100) revealed that the\nfeedback from an individual persona agent is appropriate given its persona\nidentity, and the discussion effectively synthesizes the different persona\nagents' perspectives."}
{"id": "2507.18576", "pdf": "https://arxiv.org/pdf/2507.18576.pdf", "abs": "https://arxiv.org/abs/2507.18576", "title": "SafeWork-R1: Coevolving Safety and Intelligence under the AI-45$^{\\circ}$ Law", "authors": ["Shanghai AI Lab", ":", "Yicheng Bao", "Guanxu Chen", "Mingkang Chen", "Yunhao Chen", "Chiyu Chen", "Lingjie Chen", "Sirui Chen", "Xinquan Chen", "Jie Cheng", "Yu Cheng", "Dengke Deng", "Yizhuo Ding", "Dan Ding", "Xiaoshan Ding", "Yi Ding", "Zhichen Dong", "Lingxiao Du", "Yuyu Fan", "Xinshun Feng", "Yanwei Fu", "Yuxuan Gao", "Ruijun Ge", "Tianle Gu", "Lujun Gui", "Jiaxuan Guo", "Qianxi He", "Yuenan Hou", "Xuhao Hu", "Hong Huang", "Kaichen Huang", "Shiyang Huang", "Yuxian Jiang", "Shanzhe Lei", "Jie Li", "Lijun Li", "Hao Li", "Juncheng Li", "Xiangtian Li", "Yafu Li", "Lingyu Li", "Xueyan Li", "Haotian Liang", "Dongrui Liu", "Qihua Liu", "Zhixuan Liu", "Bangwei Liu", "Huacan Liu", "Yuexiao Liu", "Zongkai Liu", "Chaochao Lu", "Yudong Lu", "Xiaoya Lu", "Zhenghao Lu", "Qitan Lv", "Caoyuan Ma", "Jiachen Ma", "Xiaoya Ma", "Zhongtian Ma", "Lingyu Meng", "Ziqi Miao", "Yazhe Niu", "Yuezhang Peng", "Yuan Pu", "Han Qi", "Chen Qian", "Xingge Qiao", "Jingjing Qu", "Jiashu Qu", "Wanying Qu", "Wenwen Qu", "Xiaoye Qu", "Qihan Ren", "Qingnan Ren", "Qingyu Ren", "Jing Shao", "Wenqi Shao", "Shuai Shao", "Dongxing Shi", "Xin Song", "Xinhao Song", "Yan Teng", "Xuan Tong", "Yingchun Wang", "Xuhong Wang", "Shujie Wang", "Xin Wang", "Yige Wang", "Yixu Wang", "Yuanfu Wang", "Futing Wang", "Ruofan Wang", "Wenjie Wang", "Yajie Wang", "Muhao Wei", "Xiaoyu Wen", "Fenghua Weng", "Yuqi Wu", "Yingtong Xiong", "Xingcheng Xu", "Chao Yang", "Yue Yang", "Yang Yao", "Yulei Ye", "Zhenyun Yin", "Yi Yu", "Bo Zhang", "Qiaosheng Zhang", "Jinxuan Zhang", "Yexin Zhang", "Yinqiang Zheng", "Hefeng Zhou", "Zhanhui Zhou", "Pengyu Zhu", "Qingzi Zhu", "Yubo Zhu", "Bowen Zhou"], "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "47 pages, 18 figures, authors are listed in alphabetical order by\n  their last names", "summary": "We introduce SafeWork-R1, a cutting-edge multimodal reasoning model that\ndemonstrates the coevolution of capabilities and safety. It is developed by our\nproposed SafeLadder framework, which incorporates large-scale, progressive,\nsafety-oriented reinforcement learning post-training, supported by a suite of\nmulti-principled verifiers. Unlike previous alignment methods such as RLHF that\nsimply learn human preferences, SafeLadder enables SafeWork-R1 to develop\nintrinsic safety reasoning and self-reflection abilities, giving rise to safety\n`aha' moments. Notably, SafeWork-R1 achieves an average improvement of\n$46.54\\%$ over its base model Qwen2.5-VL-72B on safety-related benchmarks\nwithout compromising general capabilities, and delivers state-of-the-art safety\nperformance compared to leading proprietary models such as GPT-4.1 and Claude\nOpus 4. To further bolster its reliability, we implement two distinct\ninference-time intervention methods and a deliberative search mechanism,\nenforcing step-level verification. Finally, we further develop\nSafeWork-R1-InternVL3-78B, SafeWork-R1-DeepSeek-70B, and\nSafeWork-R1-Qwen2.5VL-7B. All resulting models demonstrate that safety and\ncapability can co-evolve synergistically, highlighting the generalizability of\nour framework in building robust, reliable, and trustworthy general-purpose AI."}
{"id": "2507.18583", "pdf": "https://arxiv.org/pdf/2507.18583.pdf", "abs": "https://arxiv.org/abs/2507.18583", "title": "DR.EHR: Dense Retrieval for Electronic Health Record with Knowledge Injection and Synthetic Data", "authors": ["Zhengyun Zhao", "Huaiyuan Ying", "Yue Zhong", "Sheng Yu"], "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "Model and code released upon acceptance", "summary": "Electronic Health Records (EHRs) are pivotal in clinical practices, yet their\nretrieval remains a challenge mainly due to semantic gap issues. Recent\nadvancements in dense retrieval offer promising solutions but existing models,\nboth general-domain and biomedical-domain, fall short due to insufficient\nmedical knowledge or mismatched training corpora. This paper introduces\n\\texttt{DR.EHR}, a series of dense retrieval models specifically tailored for\nEHR retrieval. We propose a two-stage training pipeline utilizing MIMIC-IV\ndischarge summaries to address the need for extensive medical knowledge and\nlarge-scale training data. The first stage involves medical entity extraction\nand knowledge injection from a biomedical knowledge graph, while the second\nstage employs large language models to generate diverse training data. We train\ntwo variants of \\texttt{DR.EHR}, with 110M and 7B parameters, respectively.\nEvaluated on the CliniQ benchmark, our models significantly outperforms all\nexisting dense retrievers, achieving state-of-the-art results. Detailed\nanalyses confirm our models' superiority across various match and query types,\nparticularly in challenging semantic matches like implication and abbreviation.\nAblation studies validate the effectiveness of each pipeline component, and\nsupplementary experiments on EHR QA datasets demonstrate the models'\ngeneralizability on natural language questions, including complex ones with\nmultiple entities. This work significantly advances EHR retrieval, offering a\nrobust solution for clinical applications."}
{"id": "2507.18616", "pdf": "https://arxiv.org/pdf/2507.18616.pdf", "abs": "https://arxiv.org/abs/2507.18616", "title": "SynC: Synthetic Image Caption Dataset Refinement with One-to-many Mapping for Zero-shot Image Captioning", "authors": ["Si-Woo Kim", "MinJu Jeon", "Ye-Chan Kim", "Soeun Lee", "Taewhan Kim", "Dong-Jin Kim"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted to ACM Multimedia 2025", "summary": "Zero-shot Image Captioning (ZIC) increasingly utilizes synthetic datasets\ngenerated by text-to-image (T2I) models to mitigate the need for costly manual\nannotation. However, these T2I models often produce images that exhibit\nsemantic misalignments with their corresponding input captions (e.g., missing\nobjects, incorrect attributes), resulting in noisy synthetic image-caption\npairs that can hinder model training. Existing dataset pruning techniques are\nlargely designed for removing noisy text in web-crawled data. However, these\nmethods are ill-suited for the distinct challenges of synthetic data, where\ncaptions are typically well-formed, but images may be inaccurate\nrepresentations. To address this gap, we introduce SynC, a novel framework\nspecifically designed to refine synthetic image-caption datasets for ZIC.\nInstead of conventional filtering or regeneration, SynC focuses on reassigning\ncaptions to the most semantically aligned images already present within the\nsynthetic image pool. Our approach employs a one-to-many mapping strategy by\ninitially retrieving multiple relevant candidate images for each caption. We\nthen apply a cycle-consistency-inspired alignment scorer that selects the best\nimage by verifying its ability to retrieve the original caption via\nimage-to-text retrieval. Extensive evaluations demonstrate that SynC\nconsistently and significantly improves performance across various ZIC models\non standard benchmarks (MS-COCO, Flickr30k, NoCaps), achieving state-of-the-art\nresults in several scenarios. SynC offers an effective strategy for curating\nrefined synthetic data to enhance ZIC."}
{"id": "2212.10678", "pdf": "https://arxiv.org/pdf/2212.10678.pdf", "abs": "https://arxiv.org/abs/2212.10678", "title": "Causally Testing Gender Bias in LLMs: A Case Study on Occupational Bias", "authors": ["Yuen Chen", "Vethavikashini Chithrra Raghuram", "Justus Mattern", "Rada Mihalcea", "Zhijing Jin"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Generated texts from large language models (LLMs) have been shown to exhibit\na variety of harmful, human-like biases against various demographics. These\nfindings motivate research efforts aiming to understand and measure such\neffects. This paper introduces a causal formulation for bias measurement in\ngenerative language models. Based on this theoretical foundation, we outline a\nlist of desiderata for designing robust bias benchmarks. We then propose a\nbenchmark called OccuGender, with a bias-measuring procedure to investigate\noccupational gender bias. We test several state-of-the-art open-source LLMs on\nOccuGender, including Llama, Mistral, and their instruction-tuned versions. The\nresults show that these models exhibit substantial occupational gender bias.\nLastly, we discuss prompting strategies for bias mitigation and an extension of\nour causal formulation to illustrate the generalizability of our framework. Our\ncode and data https://github.com/chenyuen0103/gender-bias."}
{"id": "2308.09954", "pdf": "https://arxiv.org/pdf/2308.09954.pdf", "abs": "https://arxiv.org/abs/2308.09954", "title": "DocTER: Evaluating Document-based Knowledge Editing", "authors": ["Suhang Wu", "Ante Wang", "Minlong Peng", "Yujie Lin", "Wenbo Li", "Mingming Sun", "Jinsong Su"], "categories": ["cs.CL", "cs.AI"], "comment": "Information processing & management", "summary": "Knowledge editing aims to correct outdated or inaccurate knowledge in neural\nnetworks. In this paper, we explore knowledge editing using easily accessible\ndocuments instead of manually labeled factual triples employed in earlier\nresearch. To advance this field, we establish the first evaluation benchmark,\n\\textit{DocTER}, featuring Documents containing counterfactual knowledge for\nediting. A comprehensive four-perspective evaluation is introduced: Edit\nSuccess, Locality, Reasoning, and Cross-lingual Transfer. To adapt conventional\ntriplet-based knowledge editing methods for this task, we develop an\nExtract-then-Edit pipeline that extracts triples from documents before applying\nexisting methods. Experiments on popular knowledge editing methods demonstrate\nthat editing with documents presents significantly greater challenges than\nusing triples. In document-based scenarios, even the best-performing in-context\nediting approach still lags behind by 10 points in editing success when\ncompared to using gold triples. This observation also holds for both reasoning\nand cross-lingual test sets. We further analyze key factors influencing task\nperformance, including the quality of extracted triples, the frequency and\nposition of edited knowledge in documents, various methods for enhancing\nreasoning, and performance differences across various directions in\ncross-lingual knowledge editing, which provide valuable insights for future\nresearch."}
{"id": "2401.01405", "pdf": "https://arxiv.org/pdf/2401.01405.pdf", "abs": "https://arxiv.org/abs/2401.01405", "title": "Quantifying the Uniqueness and Divisiveness of Presidential Discourse", "authors": ["Karen Zhou", "Alexander A. Meitus", "Milo Chase", "Grace Wang", "Anne Mykland", "William Howell", "Chenhao Tan"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.SI"], "comment": "Published in PNAS Nexus:\n  https://academic.oup.com/pnasnexus/article/3/10/pgae431/7814873", "summary": "Do American presidents speak discernibly different from each other? If so, in\nwhat ways? And are these differences confined to any single medium of\ncommunication? To investigate these questions, this paper introduces a novel\nmetric of uniqueness based on large language models, develops a new lexicon for\ndivisive speech, and presents a framework for assessing the distinctive ways in\nwhich presidents speak about their political opponents. Applying these tools to\na variety of corpora of presidential speeches, we find considerable evidence\nthat Donald Trump's speech patterns diverge from those of all major party\nnominees for the presidency in recent history. Trump is significantly more\ndistinctive than his fellow Republicans, whose uniqueness values appear closer\nto those of the Democrats. Contributing to these differences is Trump's\nemployment of divisive and antagonistic language, particularly when targeting\nhis political opponents. These differences hold across a variety of measurement\nstrategies, arise on both the campaign trail and in official presidential\naddresses, and do not appear to be an artifact of secular changes in\npresidential communications."}
{"id": "2401.17256", "pdf": "https://arxiv.org/pdf/2401.17256.pdf", "abs": "https://arxiv.org/abs/2401.17256", "title": "Weak-to-Strong Jailbreaking on Large Language Models", "authors": ["Xuandong Zhao", "Xianjun Yang", "Tianyu Pang", "Chao Du", "Lei Li", "Yu-Xiang Wang", "William Yang Wang"], "categories": ["cs.CL"], "comment": "ICML 2025", "summary": "Large language models (LLMs) are vulnerable to jailbreak attacks - resulting\nin harmful, unethical, or biased text generations. However, existing\njailbreaking methods are computationally costly. In this paper, we propose the\nweak-to-strong jailbreaking attack, an efficient inference time attack for\naligned LLMs to produce harmful text. Our key intuition is based on the\nobservation that jailbroken and aligned models only differ in their initial\ndecoding distributions. The weak-to-strong attack's key technical insight is\nusing two smaller models (a safe and an unsafe one) to adversarially modify a\nsignificantly larger safe model's decoding probabilities. We evaluate the\nweak-to-strong attack on 5 diverse open-source LLMs from 3 organizations. The\nresults show our method can increase the misalignment rate to over 99% on two\ndatasets with just one forward pass per example. Our study exposes an urgent\nsafety issue that needs to be addressed when aligning LLMs. As an initial\nattempt, we propose a defense strategy to protect against such attacks, but\ncreating more advanced defenses remains challenging. The code for replicating\nthe method is available at https://github.com/XuandongZhao/weak-to-strong"}
{"id": "2406.12548", "pdf": "https://arxiv.org/pdf/2406.12548.pdf", "abs": "https://arxiv.org/abs/2406.12548", "title": "P-React: Synthesizing Topic-Adaptive Reactions of Personality Traits via Mixture of Specialized LoRA Experts", "authors": ["Yuhao Dan", "Jie Zhou", "Qin Chen", "Junfeng Tian", "Liang He"], "categories": ["cs.CL"], "comment": null, "summary": "Personalized large language models (LLMs) have attracted great attention in\nmany applications, such as emotional support and role-playing. However,\nexisting works primarily focus on modeling explicit character profiles, while\nignoring the underlying personality traits that truly shape behaviors and\ndecision-making, hampering the development of more anthropomorphic and\npsychologically-grounded AI systems. In this paper, we explore the modeling of\nBig Five personality traits, which is the most widely used trait theory in\npsychology, and propose P-React, a mixture of experts (MoE)-based personalized\nLLM. Particularly, we integrate a Personality Specialization Loss (PSL) to\nbetter capture individual trait expressions, providing a more nuanced and\npsychologically grounded personality simulacrum. To facilitate research in this\nfield, we curate OCEAN-Chat, a high-quality, human-verified dataset designed to\ntrain LLMs in expressing personality traits across diverse topics. Extensive\nexperiments demonstrate the effectiveness of P-React in maintaining consistent\nand real personality."}
{"id": "2407.19795", "pdf": "https://arxiv.org/pdf/2407.19795.pdf", "abs": "https://arxiv.org/abs/2407.19795", "title": "VolDoGer: LLM-assisted Datasets for Domain Generalization in Vision-Language Tasks", "authors": ["Juhwan Choi", "Junehyoung Kwon", "JungMin Yun", "Seunguk Yu", "YoungBin Kim"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "ICCV 2025 Workshop on Curated Data for Efficient Learning (CDEL)", "summary": "Domain generalizability is a crucial aspect of a deep learning model since it\ndetermines the capability of the model to perform well on data from unseen\ndomains. However, research on the domain generalizability of deep learning\nmodels for vision-language tasks remains limited, primarily because of the lack\nof required datasets. To address these challenges, we propose VolDoGer:\nVision-Language Dataset for Domain Generalization, a dedicated dataset designed\nfor domain generalization that addresses three vision-language tasks: image\ncaptioning, visual question answering, and visual entailment. We constructed\nVolDoGer by extending LLM-based data annotation techniques to vision-language\ntasks, thereby alleviating the burden of recruiting human annotators. We\nevaluated the domain generalizability of various models, ranging from\nfine-tuned models to a recent multimodal large language model, through\nVolDoGer."}
{"id": "2409.13725", "pdf": "https://arxiv.org/pdf/2409.13725.pdf", "abs": "https://arxiv.org/abs/2409.13725", "title": "Identity-related Speech Suppression in Generative AI Content Moderation", "authors": ["Grace Proebsting", "Oghenefejiro Isaacs Anigboro", "Charlie M. Crawford", "Danaé Metaxa", "Sorelle A. Friedler"], "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": "ACM Conference on Equity and Access in Algorithms, Mechanisms, and\n  Optimization, 2025", "summary": "Automated content moderation has long been used to help identify and filter\nundesired user-generated content online. But such systems have a history of\nincorrectly flagging content by and about marginalized identities for removal.\nGenerative AI systems now use such filters to keep undesired generated content\nfrom being created by or shown to users. While a lot of focus has been given to\nmaking sure such systems do not produce undesired outcomes, considerably less\nattention has been paid to making sure appropriate text can be generated. From\nclassrooms to Hollywood, as generative AI is increasingly used for creative or\nexpressive text generation, whose stories will these technologies allow to be\ntold, and whose will they suppress?\n  In this paper, we define and introduce measures of speech suppression,\nfocusing on speech related to different identity groups incorrectly filtered by\na range of content moderation APIs. Using both short-form, user-generated\ndatasets traditional in content moderation and longer generative AI-focused\ndata, including two datasets we introduce in this work, we create a benchmark\nfor measurement of speech suppression for nine identity groups. Across one\ntraditional and four generative AI-focused automated content moderation\nservices tested, we find that identity-related speech is more likely to be\nincorrectly suppressed than other speech. We find that reasons for incorrect\nflagging behavior vary by identity based on stereotypes and text associations,\nwith, e.g., disability-related content more likely to be flagged for self-harm\nor health-related reasons while non-Christian content is more likely to be\nflagged as violent or hateful. As generative AI systems are increasingly used\nfor creative work, we urge further attention to how this may impact the\ncreation of identity-related content."}
{"id": "2411.07037", "pdf": "https://arxiv.org/pdf/2411.07037.pdf", "abs": "https://arxiv.org/abs/2411.07037", "title": "LIFBench: Evaluating the Instruction Following Performance and Stability of Large Language Models in Long-Context Scenarios", "authors": ["Xiaodong Wu", "Minhao Wang", "Yichen Liu", "Xiaoming Shi", "He Yan", "Xiangju Lu", "Junmin Zhu", "Wei Zhang"], "categories": ["cs.CL"], "comment": "17 pages, 3 figures", "summary": "As Large Language Models (LLMs) evolve in natural language processing (NLP),\ntheir ability to stably follow instructions in long-context inputs has become\ncritical for real-world applications. However, existing benchmarks seldom focus\non instruction-following in long-context scenarios or stability on different\ninputs. To bridge this gap, we introduce LIFBench, a scalable dataset designed\nto evaluate LLMs' instruction-following capabilities and stability across long\ncontexts. LIFBench comprises three long-context scenarios and eleven diverse\ntasks, featuring 2,766 instructions generated through an automated expansion\nmethod across three dimensions: length, expression, and variables. For\nevaluation, we propose LIFEval, a rubric-based assessment method that enables\nprecise, automated scoring of complex LLM responses without reliance on\nLLM-assisted assessments or human judgment. This method allows for a\ncomprehensive analysis of model performance and stability from multiple\nperspectives. We conduct detailed experiments on 20 prominent LLMs across six\nlength intervals. Our work contributes LIFBench and LIFEval as robust tools for\nassessing LLM performance in complex and long-context settings, offering\nvaluable insights to guide future advancements in LLM development."}
{"id": "2411.10371", "pdf": "https://arxiv.org/pdf/2411.10371.pdf", "abs": "https://arxiv.org/abs/2411.10371", "title": "A Survey of Event Causality Identification: Taxonomy, Challenges, Assessment, and Prospects", "authors": ["Qing Cheng", "Zefan Zeng", "Xingchen Hu", "Yuehang Si", "Zhong Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Event Causality Identification (ECI) has become an essential task in Natural\nLanguage Processing (NLP), focused on automatically detecting causal\nrelationships between events within texts. This comprehensive survey\nsystematically investigates fundamental concepts and models, developing a\nsystematic taxonomy and critically evaluating diverse models. We begin by\ndefining core concepts, formalizing the ECI problem, and outlining standard\nevaluation protocols. Our classification framework divides ECI models into two\nprimary tasks: Sentence-level Event Causality Identification (SECI) and\nDocument-level Event Causality Identification (DECI). For SECI, we review\nmodels employing feature pattern-based matching, machine learning classifiers,\ndeep semantic encoding, prompt-based fine-tuning, and causal knowledge\npre-training, alongside data augmentation strategies. For DECI, we focus on\napproaches utilizing deep semantic encoding, event graph reasoning, and\nprompt-based fine-tuning. Special attention is given to recent advancements in\nmulti-lingual and cross-lingual ECI, as well as zero-shot ECI leveraging Large\nLanguage Models (LLMs). We analyze the strengths, limitations, and unresolved\nchallenges associated with each approach. Extensive quantitative evaluations\nare conducted on four benchmark datasets to rigorously assess the performance\nof various ECI models. We conclude by discussing future research directions and\nhighlighting opportunities to advance the field further."}
{"id": "2501.01144", "pdf": "https://arxiv.org/pdf/2501.01144.pdf", "abs": "https://arxiv.org/abs/2501.01144", "title": "BlockDialect: Block-wise Fine-grained Mixed Format Quantization for Energy-Efficient LLM Inference", "authors": ["Wonsuk Jang", "Thierry Tambe"], "categories": ["cs.CL", "cs.LG"], "comment": "ICML 2025", "summary": "The rapidly increasing size of large language models (LLMs) presents\nsignificant challenges in memory usage and computational costs. Quantizing both\nweights and activations can address these issues, with hardware-supported\nfine-grained scaling emerging as a promising solution to mitigate outliers.\nHowever, existing methods struggle to capture nuanced block data distributions.\nWe propose BlockDialect, a block-wise fine-grained mixed format technique that\nassigns a per-block optimal number format from a formatbook for better data\nrepresentation. Additionally, we introduce DialectFP4, a formatbook of FP4\nvariants (akin to dialects) that adapt to diverse data distributions. To\nleverage this efficiently, we propose a two-stage approach for online\nDialectFP4 activation quantization. Importantly, DialectFP4 ensures energy\nefficiency by selecting representable values as scaled integers compatible with\nlow-precision integer arithmetic. BlockDialect achieves 10.78% (7.48%) accuracy\ngain on the LLaMA3-8B (LLaMA2-7B) model compared to MXFP4 format with lower bit\nusage per data, while being only 5.45% (2.69%) below full precision even when\nquantizing full-path matrix multiplication. Focusing on how to represent over\nhow to scale, our work presents a promising path for energy-efficient LLM\ninference."}
{"id": "2502.03699", "pdf": "https://arxiv.org/pdf/2502.03699.pdf", "abs": "https://arxiv.org/abs/2502.03699", "title": "LLM Alignment as Retriever Optimization: An Information Retrieval Perspective", "authors": ["Bowen Jin", "Jinsung Yoon", "Zhen Qin", "Ziqi Wang", "Wei Xiong", "Yu Meng", "Jiawei Han", "Sercan O. Arik"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "26 pages", "summary": "Large Language Models (LLMs) have revolutionized artificial intelligence with\ncapabilities in reasoning, coding, and communication, driving innovation across\nindustries. Their true potential depends on effective alignment to ensure\ncorrect, trustworthy and ethical behavior, addressing challenges like\nmisinformation, hallucinations, bias and misuse. While existing Reinforcement\nLearning (RL)-based alignment methods are notoriously complex, direct\noptimization approaches offer a simpler alternative. In this work, we introduce\na novel direct optimization approach for LLM alignment by drawing on\nestablished Information Retrieval (IR) principles. We present a systematic\nframework that bridges LLM alignment and IR methodologies, mapping LLM\ngeneration and reward models to IR's retriever-reranker paradigm. Building on\nthis foundation, we propose LLM Alignment as Retriever Preference Optimization\n(LarPO), a new alignment method that enhances overall alignment quality.\nExtensive experiments validate LarPO's effectiveness with 38.9 % and 13.7 %\naveraged improvement on AlpacaEval2 and MixEval-Hard respectively. Our work\nopens new avenues for advancing LLM alignment by integrating IR foundations,\noffering a promising direction for future research."}
{"id": "2502.12988", "pdf": "https://arxiv.org/pdf/2502.12988.pdf", "abs": "https://arxiv.org/abs/2502.12988", "title": "Beyond Profile: From Surface-Level Facts to Deep Persona Simulation in LLMs", "authors": ["Zixiao Wang", "Duzhen Zhang", "Ishita Agrawal", "Shen Gao", "Le Song", "Xiuying Chen"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 Findings", "summary": "Previous approaches to persona simulation large language models (LLMs) have\ntypically relied on learning basic biographical information, or using limited\nrole-play dialogue datasets to capture a character's responses. However, a\nholistic representation of an individual goes beyond surface-level facts or\nconversations to deeper thoughts and thinking. In this work, we introduce\nCharacterBot, a model designed to replicate both the linguistic patterns and\ndistinctive thought patterns as manifested in the textual works of a character.\nUsing Lu Xun, a renowned Chinese writer as a case study, we propose four\ntraining tasks derived from his 17 essay collections. These include a\npre-training task focused on mastering external linguistic structures and\nknowledge, as well as three fine-tuning tasks: multiple-choice question\nanswering, generative question answering, and style transfer, each aligning the\nLLM with Lu Xun's internal ideation and writing style. To optimize learning\nacross these tasks, we introduce a CharLoRA parameter updating mechanism, where\na general linguistic style expert collaborates with other task-specific experts\nto better study both the language style and the understanding of deeper\nthoughts. We evaluate CharacterBot on three tasks for linguistic accuracy and\nopinion comprehension, demonstrating that it significantly outperforms the\nbaselines on our adapted metrics. We hope this work inspires future research on\ndeep character persona simulation LLMs while considering the importance of\nethical standards."}
{"id": "2502.15487", "pdf": "https://arxiv.org/pdf/2502.15487.pdf", "abs": "https://arxiv.org/abs/2502.15487", "title": "ExpliCa: Evaluating Explicit Causal Reasoning in Large Language Models", "authors": ["Martina Miliani", "Serena Auriemma", "Alessandro Bondielli", "Emmanuele Chersoni", "Lucia Passaro", "Irene Sucameli", "Alessandro Lenci"], "categories": ["cs.CL", "cs.AI", "68T50, 68T07", "I.2.7"], "comment": "Accepted for publication in Findings of ACL 2025", "summary": "Large Language Models (LLMs) are increasingly used in tasks requiring\ninterpretive and inferential accuracy. In this paper, we introduce ExpliCa, a\nnew dataset for evaluating LLMs in explicit causal reasoning. ExpliCa uniquely\nintegrates both causal and temporal relations presented in different linguistic\norders and explicitly expressed by linguistic connectives. The dataset is\nenriched with crowdsourced human acceptability ratings. We tested LLMs on\nExpliCa through prompting and perplexity-based metrics. We assessed seven\ncommercial and open-source LLMs, revealing that even top models struggle to\nreach 0.80 accuracy. Interestingly, models tend to confound temporal relations\nwith causal ones, and their performance is also strongly influenced by the\nlinguistic order of the events. Finally, perplexity-based scores and prompting\nperformance are differently affected by model size."}
{"id": "2502.18679", "pdf": "https://arxiv.org/pdf/2502.18679.pdf", "abs": "https://arxiv.org/abs/2502.18679", "title": "Discriminative Finetuning of Generative Large Language Models without Reward Models and Human Preference Data", "authors": ["Siqi Guo", "Ilgee Hong", "Vicente Balmaseda", "Changlong Yu", "Liang Qiu", "Xin Liu", "Haoming Jiang", "Tuo Zhao", "Tianbao Yang"], "categories": ["cs.CL"], "comment": "18 pages, 7 figures", "summary": "Supervised fine-tuning (SFT) has become a crucial step for aligning\npretrained large language models (LLMs) using supervised datasets of\ninput-output pairs. However, despite being supervised, SFT is inherently\nlimited by its generative training objective. To address its limitations, the\nexisting common strategy is to follow SFT with a separate phase of preference\noptimization (PO), which relies on either human-labeled preference data or a\nstrong reward model to guide the learning process. In this paper, we address\nthe limitations of SFT by exploring one of the most successful techniques in\nconventional supervised learning: discriminative learning. We introduce\nDiscriminative Fine-Tuning (DFT), an improved variant of SFT, which mitigates\nthe burden of collecting human-labeled preference data or training strong\nreward models. Unlike SFT that employs a generative approach and overlooks\nnegative data, DFT adopts a discriminative paradigm that increases the\nprobability of positive answers while suppressing potentially negative ones,\naiming for data prediction instead of token prediction. Our contributions\ninclude: (i) a discriminative probabilistic framework for fine-tuning LLMs by\nexplicitly modeling the discriminative likelihood of an answer among all\npossible outputs given an input; (ii) efficient algorithms to optimize this\ndiscriminative likelihood; and (iii) extensive experiments demonstrating DFT's\neffectiveness, achieving performance better than SFT and comparable to if not\nbetter than SFT$\\rightarrow$PO. The code can be found at\nhttps://github.com/Optimization-AI/DFT."}
{"id": "2503.21676", "pdf": "https://arxiv.org/pdf/2503.21676.pdf", "abs": "https://arxiv.org/abs/2503.21676", "title": "How do language models learn facts? Dynamics, curricula and hallucinations", "authors": ["Nicolas Zucchet", "Jörg Bornschein", "Stephanie Chan", "Andrew Lampinen", "Razvan Pascanu", "Soham De"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted at the 2nd Conference on Language Modeling (2025)", "summary": "Large language models accumulate vast knowledge during pre-training, yet the\ndynamics governing this acquisition remain poorly understood. This work\ninvestigates the learning dynamics of language models on a synthetic factual\nrecall task, uncovering three key findings: First, language models learn in\nthree phases, exhibiting a performance plateau before acquiring precise factual\nknowledge. Mechanistically, this plateau coincides with the formation of\nattention-based circuits that support recall. Second, the training data\ndistribution significantly impacts learning dynamics, as imbalanced\ndistributions lead to shorter plateaus. Finally, hallucinations emerge\nsimultaneously with knowledge, and integrating new knowledge into the model\nthrough fine-tuning is challenging, as it quickly corrupts its existing\nparametric memories. Our results emphasize the importance of data distribution\nin knowledge acquisition and suggest novel data scheduling strategies to\naccelerate neural network training."}
{"id": "2504.05211", "pdf": "https://arxiv.org/pdf/2504.05211.pdf", "abs": "https://arxiv.org/abs/2504.05211", "title": "Exploiting individual differences to bootstrap communication", "authors": ["Richard A. Blythe", "Casimir Fisch"], "categories": ["cs.CL", "physics.soc-ph", "q-bio.PE"], "comment": "Revised version is a full paper with considerable additional\n  exposition and discussion. Now 21 pages including supplementary information,\n  11 figures", "summary": "Establishing a communication system is hard because the intended meaning of a\nsignal is unknown to its receiver when first produced, and the signaller also\nhas no idea how that signal will be interpreted. Most theoretical accounts of\nthe emergence of communication systems rely on feedback to reinforce behaviours\nthat have led to successful communication in the past. However, providing such\nfeedback requires already being able to communicate the meaning that was\nintended or interpreted. Therefore these accounts cannot explain how\ncommunication can be bootstrapped from non-communicative behaviours. Here we\npresent a model that shows how a communication system, capable of expressing an\nunbounded number of meanings, can emerge as a result of individual behavioural\ndifferences in a large population without any pre-existing means to determine\ncommunicative success. The two key cognitive capabilities responsible for this\noutcome are behaving predictably in a given situation, and an alignment of\npsychological states ahead of signal production that derives from shared\nintentionality. Since both capabilities can exist independently of\ncommunication, our results are compatible with theories in which large flexible\nsocially-learned communication systems like language are the product of a\ngeneral but well-developed capacity for social cognition."}
{"id": "2505.20658", "pdf": "https://arxiv.org/pdf/2505.20658.pdf", "abs": "https://arxiv.org/abs/2505.20658", "title": "Enhancing Transformation from Natural Language to Signal Temporal Logic Using LLMs with Diverse External Knowledge", "authors": ["Yue Fang", "Zhi Jin", "Jie An", "Hongshen Chen", "Xiaohong Chen", "Naijun Zhan"], "categories": ["cs.CL"], "comment": "11 pages, 5 figures, published to ACL 2025", "summary": "Temporal Logic (TL), especially Signal Temporal Logic (STL), enables precise\nformal specification, making it widely used in cyber-physical systems such as\nautonomous driving and robotics. Automatically transforming NL into STL is an\nattractive approach to overcome the limitations of manual transformation, which\nis time-consuming and error-prone. However, due to the lack of datasets,\nautomatic transformation currently faces significant challenges and has not\nbeen fully explored. In this paper, we propose an NL-STL dataset named\nSTL-Diversity-Enhanced (STL-DivEn), which comprises 16,000 samples enriched\nwith diverse patterns. To develop the dataset, we first manually create a\nsmall-scale seed set of NL-STL pairs. Next, representative examples are\nidentified through clustering and used to guide large language models (LLMs) in\ngenerating additional NL-STL pairs. Finally, diversity and accuracy are ensured\nthrough rigorous rule-based filters and human validation. Furthermore, we\nintroduce the Knowledge-Guided STL Transformation (KGST) framework, a novel\napproach for transforming natural language into STL, involving a\ngenerate-then-refine process based on external knowledge. Statistical analysis\nshows that the STL-DivEn dataset exhibits more diversity than the existing\nNL-STL dataset. Moreover, both metric-based and human evaluations indicate that\nour KGST approach outperforms baseline models in transformation accuracy on\nSTL-DivEn and DeepSTL datasets."}
{"id": "2506.05606", "pdf": "https://arxiv.org/pdf/2506.05606.pdf", "abs": "https://arxiv.org/abs/2506.05606", "title": "OPeRA: A Dataset of Observation, Persona, Rationale, and Action for Evaluating LLMs on Human Online Shopping Behavior Simulation", "authors": ["Ziyi Wang", "Yuxuan Lu", "Wenbo Li", "Amirali Amini", "Bo Sun", "Yakov Bart", "Weimin Lyu", "Jiri Gesi", "Tian Wang", "Jing Huang", "Yu Su", "Upol Ehsan", "Malihe Alikhani", "Toby Jia-Jun Li", "Lydia Chilton", "Dakuo Wang"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Can large language models (LLMs) accurately simulate the next web action of a\nspecific user? While LLMs have shown promising capabilities in generating\n``believable'' human behaviors, evaluating their ability to mimic real user\nbehaviors remains an open challenge, largely due to the lack of high-quality,\npublicly available datasets that capture both the observable actions and the\ninternal reasoning of an actual human user. To address this gap, we introduce\nOPERA, a novel dataset of Observation, Persona, Rationale, and Action collected\nfrom real human participants during online shopping sessions. OPERA is the\nfirst public dataset that comprehensively captures: user personas, browser\nobservations, fine-grained web actions, and self-reported just-in-time\nrationales. We developed both an online questionnaire and a custom browser\nplugin to gather this dataset with high fidelity. Using OPERA, we establish the\nfirst benchmark to evaluate how well current LLMs can predict a specific user's\nnext action and rationale with a given persona and <observation, action,\nrationale> history. This dataset lays the groundwork for future research into\nLLM agents that aim to act as personalized digital twins for human."}
{"id": "2506.16383", "pdf": "https://arxiv.org/pdf/2506.16383.pdf", "abs": "https://arxiv.org/abs/2506.16383", "title": "Large Language Models in Argument Mining: A Survey", "authors": ["Hao Li", "Viktor Schlegel", "Yizheng Sun", "Riza Batista-Navarro", "Goran Nenadic"], "categories": ["cs.CL"], "comment": "Work draft", "summary": "Argument Mining (AM), a critical subfield of Natural Language Processing\n(NLP), focuses on extracting argumentative structures from text. The advent of\nLarge Language Models (LLMs) has profoundly transformed AM, enabling advanced\nin-context learning, prompt-based generation, and robust cross-domain\nadaptability. This survey systematically synthesizes recent advancements in\nLLM-driven AM. We provide a concise review of foundational theories and\nannotation frameworks, alongside a meticulously curated catalog of datasets. A\nkey contribution is our comprehensive taxonomy of AM subtasks, elucidating how\ncontemporary LLM techniques -- such as prompting, chain-of-thought reasoning,\nand retrieval augmentation -- have reconfigured their execution. We further\ndetail current LLM architectures and methodologies, critically assess\nevaluation practices, and delineate pivotal challenges including long-context\nreasoning, interpretability, and annotation bottlenecks. Conclusively, we\nhighlight emerging trends and propose a forward-looking research agenda for\nLLM-based computational argumentation, aiming to strategically guide\nresearchers in this rapidly evolving domain."}
{"id": "2506.19733", "pdf": "https://arxiv.org/pdf/2506.19733.pdf", "abs": "https://arxiv.org/abs/2506.19733", "title": "Breaking Barriers: Do Reinforcement Post Training Gains Transfer To Unseen Domains?", "authors": ["Chuxuan Hu", "Yuxuan Zhu", "Antony Kellermann", "Caleb Biddulph", "Suppakit Waiwitlikhit", "Jason Benn", "Daniel Kang"], "categories": ["cs.CL"], "comment": "9 pages, 4 figures, 2 tables", "summary": "Reinforcement post training (RPT) has recently shown promise in improving the\nreasoning abilities of large language models (LLMs). However, it remains\nunclear how well these improvements generalize to new domains, as prior work\nevaluates RPT models on data from the same domains used for fine-tuning. To\nunderstand the generalizability of RPT, we conduct two studies. (1)\nObservational: We compare a wide range of open-weight RPT models against their\ncorresponding base models across multiple domains, including both seen and\nunseen domains in their fine-tuning data. (2) Interventional: we fine-tune LLMs\nwith RPT on single domains and evaluate their performance across multiple\ndomains. Both studies converge on the same conclusion that, although RPT brings\nsubstantial gains on tasks similar to the fine-tuning data, the gains\ngeneralize inconsistently and can vanish on domains with different reasoning\npatterns."}
{"id": "2507.08017", "pdf": "https://arxiv.org/pdf/2507.08017.pdf", "abs": "https://arxiv.org/abs/2507.08017", "title": "Mechanistic Indicators of Understanding in Large Language Models", "authors": ["Pierre Beckmann", "Matthieu Queloz"], "categories": ["cs.CL", "cs.AI"], "comment": "32 pages", "summary": "Recent findings in mechanistic interpretability (MI), the field probing the\ninner workings of Large Language Models (LLMs), challenge the view that these\nmodels rely solely on superficial statistics. We offer an accessible synthesis\nof these findings that doubles as an introduction to MI while integrating these\nfindings within a novel theoretical framework for thinking about machine\nunderstanding. We argue that LLMs develop internal structures that are\nfunctionally analogous to the kind of understanding that consists in seeing\nconnections. To sharpen this idea, we propose a three-tiered conception of\nunderstanding. First, conceptual understanding emerges when a model forms\n\"features\" as directions in latent space, learning the connections between\ndiverse manifestations of something. Second, state-of-the-world understanding\nemerges when a model learns contingent factual connections between features and\ndynamically tracks changes in the world. Third, principled understanding\nemerges when a model ceases to rely on a collection of memorized facts and\ndiscovers a \"circuit\" connecting these facts. However, these forms of\nunderstanding remain radically different from human understanding, as the\nphenomenon of \"parallel mechanisms\" shows. We conclude that the debate should\nmove beyond the yes-or-no question of whether LLMs understand to investigate\nhow their strange minds work and forge conceptions that fit them."}
{"id": "2507.08621", "pdf": "https://arxiv.org/pdf/2507.08621.pdf", "abs": "https://arxiv.org/abs/2507.08621", "title": "A comprehensive study of LLM-based argument classification: from LLAMA through GPT-4o to Deepseek-R1", "authors": ["Marcin Pietroń", "Rafał Olszowski", "Jakub Gomułka", "Filip Gampel", "Andrzej Tomski"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Argument mining (AM) is an interdisciplinary research field that integrates\ninsights from logic, philosophy, linguistics, rhetoric, law, psychology, and\ncomputer science. It involves the automatic identification and extraction of\nargumentative components, such as premises and claims, and the detection of\nrelationships between them, such as support, attack, or neutrality. Recently,\nthe field has advanced significantly, especially with the advent of large\nlanguage models (LLMs), which have enhanced the efficiency of analyzing and\nextracting argument semantics compared to traditional methods and other deep\nlearning models. There are many benchmarks for testing and verifying the\nquality of LLM, but there is still a lack of research and results on the\noperation of these models in publicly available argument classification\ndatabases. This paper presents a study of a selection of LLM's, using diverse\ndatasets such as Args.me and UKP. The models tested include versions of GPT,\nLlama, and DeepSeek, along with reasoning-enhanced variants incorporating the\nChain-of-Thoughts algorithm. The results indicate that ChatGPT-4o outperforms\nthe others in the argument classification benchmarks. In case of models\nincorporated with reasoning capabilities, the Deepseek-R1 shows its\nsuperiority. However, despite their superiority, GPT-4o and Deepseek-R1 still\nmake errors. The most common errors are discussed for all models. To our\nknowledge, the presented work is the first broader analysis of the mentioned\ndatasets using LLM and prompt algorithms. The work also shows some weaknesses\nof known prompt algorithms in argument analysis, while indicating directions\nfor their improvement. The added value of the work is the in-depth analysis of\nthe available argument datasets and the demonstration of their shortcomings."}
{"id": "2507.11936", "pdf": "https://arxiv.org/pdf/2507.11936.pdf", "abs": "https://arxiv.org/abs/2507.11936", "title": "A Survey of Deep Learning for Geometry Problem Solving", "authors": ["Jianzhe Ma", "Wenxuan Wang", "Qin Jin"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "Work in progress", "summary": "Geometry problem solving is a key area of mathematical reasoning, which is\nwidely involved in many important fields such as education, mathematical\nability assessment of artificial intelligence, and multimodal ability\nassessment. In recent years, the rapid development of deep learning technology,\nespecially the rise of multimodal large language models, has triggered a\nwidespread research boom. This paper provides a survey of the applications of\ndeep learning in geometry problem solving, including (i) a comprehensive\nsummary of the relevant tasks in geometry problem solving; (ii) a thorough\nreview of related deep learning methods; (iii) a detailed analysis of\nevaluation metrics and methods; and (iv) a critical discussion of the current\nchallenges and future directions that can be explored. Our goal is to provide a\ncomprehensive and practical reference of deep learning for geometry problem\nsolving to promote further developments in this field. We create a continuously\nupdated list of papers on GitHub: https://github.com/majianz/dl4gps."}
{"id": "2507.12720", "pdf": "https://arxiv.org/pdf/2507.12720.pdf", "abs": "https://arxiv.org/abs/2507.12720", "title": "FLEXITOKENS: Flexible Tokenization for Evolving Language Models", "authors": ["Abraham Toluase Owodunni", "Orevaoghene Ahia", "Sachin Kumar"], "categories": ["cs.CL"], "comment": null, "summary": "Language models (LMs) are challenging to adapt to new data distributions by\nsimple finetuning. This is due to the rigidity of their subword tokenizers,\nwhich typically remain unchanged during adaptation. This inflexibility often\nleads to inefficient tokenization, causing overfragmentation of\nout-of-distribution domains, unseen languages, or scripts. In this work, we\ndevelop byte-level LMs with learnable tokenizers to make tokenization adaptive.\nOur models include a submodule that learns to predict boundaries between the\ninput byte sequence, encoding it into variable-length segments. Existing\ntokenizer-free methods train this boundary predictor using an auxiliary loss\nthat enforces a fixed compression rate across the training corpus, introducing\na new kind of rigidity. We propose FLEXITOKENS, a simplified training objective\nthat enables significantly greater flexibility during adaptation. Evaluating\nacross multiple multilingual benchmarks, morphologically diverse tasks, and\ndomains, we demonstrate that FLEXITOKENS consistently reduces token\nover-fragmentation and achieves up to 10% improvements on downstream task\nperformance compared to subword and other gradient-based tokenizers. Code and\ndata for our experiments will be released at\nhttps://github.com/owos/flexitokens"}
{"id": "2507.13238", "pdf": "https://arxiv.org/pdf/2507.13238.pdf", "abs": "https://arxiv.org/abs/2507.13238", "title": "Multilingual LLMs Are Not Multilingual Thinkers: Evidence from Hindi Analogy Evaluation", "authors": ["Ashray Gupta", "Rohan Joseph", "Sunny Rai"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Analogies test a model's ability to infer implicit relationships between\nconcepts, making them a key benchmark for evaluating reasoning capabilities.\nWhile large language models (LLMs) are widely evaluated for reasoning in\nEnglish, their abilities in Indic languages remain understudied, limiting our\nunderstanding of whether these models generalize across languages. To address\nthis gap, we introduce a new Hindi Analogy Test Set (HATS), comprising 405\nmultiple-choice questions sourced from Indian government exams. We benchmark\nstate-of-the-art multilingual LLMs using various prompting strategies and\nintroduce a grounded Chain of Thought approach that leverages cognitive\ntheories of analogical reasoning. This approach improves model performance on\nHindi analogy questions. Our experiments show that models perform best with\nEnglish prompts, irrespective of the prompting strategy. Our test set addresses\nthe lack of a critical resource to evaluate LLM reasoning capabilities in\nHindi."}
{"id": "2507.14314", "pdf": "https://arxiv.org/pdf/2507.14314.pdf", "abs": "https://arxiv.org/abs/2507.14314", "title": "What Makes You CLIC: Detection of Croatian Clickbait Headlines", "authors": ["Marija Anđelić", "Dominik Šipek", "Laura Majer", "Jan Šnajder"], "categories": ["cs.CL"], "comment": "Accepted at Slavic NLP 2025", "summary": "Online news outlets operate predominantly on an advertising-based revenue\nmodel, compelling journalists to create headlines that are often scandalous,\nintriguing, and provocative -- commonly referred to as clickbait. Automatic\ndetection of clickbait headlines is essential for preserving information\nquality and reader trust in digital media and requires both contextual\nunderstanding and world knowledge. For this task, particularly in\nless-resourced languages, it remains unclear whether fine-tuned methods or\nin-context learning (ICL) yield better results. In this paper, we compile CLIC,\na novel dataset for clickbait detection of Croatian news headlines spanning a\n20-year period and encompassing mainstream and fringe outlets. We fine-tune the\nBERTi\\'c model on this task and compare its performance to LLM-based ICL\nmethods with prompts both in Croatian and English. Finally, we analyze the\nlinguistic properties of clickbait. We find that nearly half of the analyzed\nheadlines contain clickbait, and that finetuned models deliver better results\nthan general LLMs."}
{"id": "2507.16632", "pdf": "https://arxiv.org/pdf/2507.16632.pdf", "abs": "https://arxiv.org/abs/2507.16632", "title": "Step-Audio 2 Technical Report", "authors": ["Boyong Wu", "Chao Yan", "Chen Hu", "Cheng Yi", "Chengli Feng", "Fei Tian", "Feiyu Shen", "Gang Yu", "Haoyang Zhang", "Jingbei Li", "Mingrui Chen", "Peng Liu", "Wang You", "Xiangyu Tony Zhang", "Xingyuan Li", "Xuerui Yang", "Yayue Deng", "Yechang Huang", "Yuxin Li", "Yuxin Zhang", "Zhao You", "Brian Li", "Changyi Wan", "Hanpeng Hu", "Jiangjie Zhen", "Siyu Chen", "Song Yuan", "Xuelin Zhang", "Yimin Jiang", "Yu Zhou", "Yuxiang Yang", "Bingxin Li", "Buyun Ma", "Changhe Song", "Dongqing Pang", "Guoqiang Hu", "Haiyang Sun", "Kang An", "Na Wang", "Shuli Gao", "Wei Ji", "Wen Li", "Wen Sun", "Xuan Wen", "Yong Ren", "Yuankai Ma", "Yufan Lu", "Bin Wang", "Bo Li", "Changxin Miao", "Che Liu", "Chen Xu", "Dapeng Shi", "Dingyuan Hu", "Donghang Wu", "Enle Liu", "Guanzhe Huang", "Gulin Yan", "Han Zhang", "Hao Nie", "Haonan Jia", "Hongyu Zhou", "Jianjian Sun", "Jiaoren Wu", "Jie Wu", "Jie Yang", "Jin Yang", "Junzhe Lin", "Kaixiang Li", "Lei Yang", "Liying Shi", "Li Zhou", "Longlong Gu", "Ming Li", "Mingliang Li", "Mingxiao Li", "Nan Wu", "Qi Han", "Qinyuan Tan", "Shaoliang Pang", "Shengjie Fan", "Siqi Liu", "Tiancheng Cao", "Wanying Lu", "Wenqing He", "Wuxun Xie", "Xu Zhao", "Xueqi Li", "Yanbo Yu", "Yang Yang", "Yi Liu", "Yifan Lu", "Yilei Wang", "Yuanhao Ding", "Yuanwei Liang", "Yuanwei Lu", "Yuchu Luo", "Yuhe Yin", "Yumeng Zhan", "Yuxiang Zhang", "Zidong Yang", "Zixin Zhang", "Binxing Jiao", "Daxin Jiang", "Heung-Yeung Shum", "Jiansheng Chen", "Jing Li", "Xiangyu Zhang", "Yibo Zhu"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "This paper presents Step-Audio 2, an end-to-end multi-modal large language\nmodel designed for industry-strength audio understanding and speech\nconversation. By integrating a latent audio encoder and reasoning-centric\nreinforcement learning (RL), Step-Audio 2 achieves promising performance in\nautomatic speech recognition (ASR) and audio understanding. To facilitate\ngenuine end-to-end speech conversation, Step-Audio 2 incorporates the\ngeneration of discrete audio tokens into language modeling, significantly\nenhancing its responsiveness to paralinguistic information such as speaking\nstyles and emotions. To effectively leverage the rich textual and acoustic\nknowledge in real-world data, Step-Audio 2 integrates retrieval-augmented\ngeneration (RAG) and is able to call external tools such as web search to\nmitigate hallucination and audio search to switch timbres. Trained on millions\nof hours of speech and audio data, Step-Audio 2 delivers intelligence and\nexpressiveness across diverse conversational scenarios. Evaluation results\ndemonstrate that Step-Audio 2 achieves state-of-the-art performance on various\naudio understanding and conversational benchmarks compared to other open-source\nand commercial solutions. Please visit\nhttps://github.com/stepfun-ai/Step-Audio2 for more information."}
{"id": "2507.16802", "pdf": "https://arxiv.org/pdf/2507.16802.pdf", "abs": "https://arxiv.org/abs/2507.16802", "title": "Agentar-Fin-R1: Enhancing Financial Intelligence through Domain Expertise, Training Efficiency, and Advanced Reasoning", "authors": ["Yanjun Zheng", "Xiyang Du", "Longfei Liao", "Xiaoke Zhao", "Zhaowen Zhou", "Jingze Song", "Bo Zhang", "Jiawei Liu", "Xiang Qi", "Zhe Li", "Zhiqiang Zhang", "Wei Wang", "Peng Zhang"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) exhibit considerable promise in financial\napplications; however, prevailing models frequently demonstrate limitations\nwhen confronted with scenarios that necessitate sophisticated reasoning\ncapabilities, stringent trustworthiness criteria, and efficient adaptation to\ndomain-specific requirements. We introduce the Agentar-Fin-R1 series of\nfinancial large language models (8B and 32B parameters), specifically\nengineered based on the Qwen3 foundation model to enhance reasoning\ncapabilities, reliability, and domain specialization for financial\napplications. Our optimization approach integrates a high-quality, systematic\nfinancial task label system with a comprehensive multi-layered trustworthiness\nassurance framework. This framework encompasses high-quality trustworthy\nknowledge engineering, multi-agent trustworthy data synthesis, and rigorous\ndata validation governance. Through label-guided automated difficulty-aware\noptimization, tow-stage training pipeline, and dynamic attribution systems, we\nachieve substantial improvements in training efficiency. Our models undergo\ncomprehensive evaluation on mainstream financial benchmarks including Fineva,\nFinEval, and FinanceIQ, as well as general reasoning datasets such as MATH-500\nand GPQA-diamond. To thoroughly assess real-world deployment capabilities, we\ninnovatively propose the Finova evaluation benchmark, which focuses on\nagent-level financial reasoning and compliance verification. Experimental\nresults demonstrate that Agentar-Fin-R1 not only achieves state-of-the-art\nperformance on financial tasks but also exhibits exceptional general reasoning\ncapabilities, validating its effectiveness as a trustworthy solution for\nhigh-stakes financial applications. The Finova bench is available at\nhttps://github.com/antgroup/Finova."}
{"id": "2507.16809", "pdf": "https://arxiv.org/pdf/2507.16809.pdf", "abs": "https://arxiv.org/abs/2507.16809", "title": "LingBench++: A Linguistically-Informed Benchmark and Reasoning Framework for Multi-Step and Cross-Cultural Inference with LLMs", "authors": ["Da-Chen Lian", "Ri-Sheng Huang", "Pin-Er Chen", "Chunki Lim", "You-Kuan Lin", "Guan-Yu Tseng", "Zi-Cheng Yang", "Zhen-Yu Lin", "Pin-Cheng Chen", "Shu-Kai Hsieh"], "categories": ["cs.CL"], "comment": "42p, 17f, 10t. Revisions: Merged paragraphs in Intro to emphasize\n  contributions. Clarified benchmark design (Sec 3.5.1). Added single-agent,\n  OpenAI-guided & 6-round experiments (Sec 5.2). Note: we only ran each\n  experiment once; statistical tests are needed for strong claims. Revised Sec\n  6. Added acknowledgements, 2 new co-authors, and corrected typos/grammar", "summary": "We propose LingBench++, a linguistically-informed benchmark and reasoning\nframework designed to evaluate large language models (LLMs) on complex\nlinguistic tasks inspired by the International Linguistics Olympiad (IOL).\nUnlike prior benchmarks that focus solely on final answer accuracy, LingBench++\nprovides structured reasoning traces, stepwise evaluation protocols, and rich\ntypological metadata across over 90 low-resource and cross-cultural languages.\nWe further develop a multi-agent architecture integrating grammatical knowledge\nretrieval, tool-augmented reasoning, and deliberate hypothesis testing. Through\nsystematic comparisons of baseline and our proposed agentic models, we\ndemonstrate that models equipped with external knowledge sources and iterative\nreasoning outperform single-pass approaches in both accuracy and\ninterpretability. LingBench++ offers a comprehensive foundation for advancing\nlinguistically grounded, culturally informed, and cognitively plausible\nreasoning in LLMs."}
{"id": "2507.17527", "pdf": "https://arxiv.org/pdf/2507.17527.pdf", "abs": "https://arxiv.org/abs/2507.17527", "title": "Seed LiveInterpret 2.0: End-to-end Simultaneous Speech-to-speech Translation with Your Voice", "authors": ["Shanbo Cheng", "Yu Bao", "Zhichao Huang", "Yu Lu", "Ningxin Peng", "Lu Xu", "Runsheng Yu", "Rong Cao", "Ting Han", "Zeyang Li", "Sitong Liu", "Shengtao Ma", "Shiguang Pan", "Jiongchen Xiao", "Nuo Xu", "Meng Yang", "Rong Ye", "Yiming Yu", "Ruofei Zhang", "Wanyi Zhang", "Wenhao Zhu", "Liehao Zou", "Lu Lu", "Yuxuan Wang", "Yonghui Wu"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Seed-LiveInterpret 2.0 Technical Report", "summary": "Simultaneous Interpretation (SI) represents one of the most daunting\nfrontiers in the translation industry, with product-level automatic systems\nlong plagued by intractable challenges: subpar transcription and translation\nquality, lack of real-time speech generation, multi-speaker confusion, and\ntranslated speech inflation, especially in long-form discourses. In this study,\nwe introduce Seed-LiveInterpret 2.0, an end-to-end SI model that delivers\nhigh-fidelity, ultra-low-latency speech-to-speech generation with voice cloning\ncapabilities. As a fully operational product-level solution, Seed-LiveInterpret\n2.0 tackles these challenges head-on through our novel duplex speech-to-speech\nunderstanding-generating framework. Experimental results demonstrate that\nthrough large-scale pretraining and reinforcement learning, the model achieves\na significantly better balance between translation accuracy and latency,\nvalidated by human interpreters to exceed 70% correctness in complex scenarios.\nNotably, Seed-LiveInterpret 2.0 outperforms commercial SI solutions by\nsignificant margins in translation quality, while slashing the average latency\nof cloned speech from nearly 10 seconds to a near-real-time 3 seconds, which is\naround a near 70% reduction that drastically enhances practical usability."}
{"id": "2507.17702", "pdf": "https://arxiv.org/pdf/2507.17702.pdf", "abs": "https://arxiv.org/abs/2507.17702", "title": "Towards Greater Leverage: Scaling Laws for Efficient Mixture-of-Experts Language Models", "authors": ["Changxin Tian", "Kunlong Chen", "Jia Liu", "Ziqi Liu", "Zhiqiang Zhang", "Jun Zhou"], "categories": ["cs.CL", "I.2.7"], "comment": null, "summary": "Mixture-of-Experts (MoE) has become a dominant architecture for scaling Large\nLanguage Models (LLMs) efficiently by decoupling total parameters from\ncomputational cost. However, this decoupling creates a critical challenge:\npredicting the model capacity of a given MoE configurations (e.g., expert\nactivation ratio and granularity) remains an unresolved problem. To address\nthis gap, we introduce Efficiency Leverage (EL), a metric quantifying the\ncomputational advantage of an MoE model over a dense equivalent. We conduct a\nlarge-scale empirical study, training over 300 models up to 28B parameters, to\nsystematically investigate the relationship between MoE architectural\nconfigurations and EL. Our findings reveal that EL is primarily driven by the\nexpert activation ratio and the total compute budget, both following\npredictable power laws, while expert granularity acts as a non-linear modulator\nwith a clear optimal range. We integrate these discoveries into a unified\nscaling law that accurately predicts the EL of an MoE architecture based on its\nconfiguration. To validate our derived scaling laws, we designed and trained\nLing-mini-beta, a pilot model for Ling-2.0 series with only 0.85B active\nparameters, alongside a 6.1B dense model for comparison. When trained on an\nidentical 1T high-quality token dataset, Ling-mini-beta matched the performance\nof the 6.1B dense model while consuming over 7x fewer computational resources,\nthereby confirming the accuracy of our scaling laws. This work provides a\nprincipled and empirically-grounded foundation for the scaling of efficient MoE\nmodels."}
{"id": "2404.14445", "pdf": "https://arxiv.org/pdf/2404.14445.pdf", "abs": "https://arxiv.org/abs/2404.14445", "title": "A Multi-Faceted Evaluation Framework for Assessing Synthetic Data Generated by Large Language Models", "authors": ["Yefeng Yuan", "Yuhong Liu", "Liang Cheng"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "10 pages, 1 figure, 4 tables", "summary": "The rapid advancements in generative AI and large language models (LLMs) have\nopened up new avenues for producing synthetic data, particularly in the realm\nof structured tabular formats, such as product reviews. Despite the potential\nbenefits, concerns regarding privacy leakage have surfaced, especially when\npersonal information is utilized in the training datasets. In addition, there\nis an absence of a comprehensive evaluation framework capable of quantitatively\nmeasuring the quality of the generated synthetic data and their utility for\ndownstream tasks. In response to this gap, we introduce SynEval, an open-source\nevaluation framework designed to assess the fidelity, utility, and privacy\npreservation of synthetically generated tabular data via a suite of diverse\nevaluation metrics. We validate the efficacy of our proposed framework -\nSynEval - by applying it to synthetic product review data generated by three\nstate-of-the-art LLMs: ChatGPT, Claude, and Llama. Our experimental findings\nilluminate the trade-offs between various evaluation metrics in the context of\nsynthetic data generation. Furthermore, SynEval stands as a critical instrument\nfor researchers and practitioners engaged with synthetic tabular data,,\nempowering them to judiciously determine the suitability of the generated data\nfor their specific applications, with an emphasis on upholding user privacy."}
{"id": "2412.09900", "pdf": "https://arxiv.org/pdf/2412.09900.pdf", "abs": "https://arxiv.org/abs/2412.09900", "title": "Analyzing Fairness of Computer Vision and Natural Language Processing Models", "authors": ["Ahmed Rashed", "Abdelkrim Kallich", "Mohamed Eltayeb"], "categories": ["cs.LG", "cs.CL"], "comment": "25 pages, 8 table, 11 figures", "summary": "Machine learning (ML) algorithms play a critical role in decision-making\nacross various domains, such as healthcare, finance, education, and law\nenforcement. However, concerns about fairness and bias in these systems have\nraised significant ethical and social challenges. To address these challenges,\nthis research utilizes two prominent fairness libraries, Fairlearn by Microsoft\nand AIF360 by IBM. These libraries offer comprehensive frameworks for fairness\nanalysis, providing tools to evaluate fairness metrics, visualize results, and\nimplement bias mitigation algorithms. The study focuses on assessing and\nmitigating biases for unstructured datasets using Computer Vision (CV) and\nNatural Language Processing (NLP) models. The primary objective is to present a\ncomparative analysis of the performance of mitigation algorithms from the two\nfairness libraries. This analysis involves applying the algorithms\nindividually, one at a time, in one of the stages of the ML lifecycle,\npre-processing, in-processing, or post-processing, as well as sequentially\nacross more than one stage. The results reveal that some sequential\napplications improve the performance of mitigation algorithms by effectively\nreducing bias while maintaining the model's performance. Publicly available\ndatasets from Kaggle were chosen for this research, providing a practical\ncontext for evaluating fairness in real-world machine learning workflows."}
{"id": "2412.10510", "pdf": "https://arxiv.org/pdf/2412.10510.pdf", "abs": "https://arxiv.org/abs/2412.10510", "title": "DEFAME: Dynamic Evidence-based FAct-checking with Multimodal Experts", "authors": ["Tobias Braun", "Mark Rothermel", "Marcus Rohrbach", "Anna Rohrbach"], "categories": ["cs.CV", "cs.CL"], "comment": "ICML 2025 version. 9 pages main paper, 35 pages with appendix, 18\n  figures and 7 tables. Corrected two inconsistent numbers in Table 2", "summary": "The proliferation of disinformation demands reliable and scalable\nfact-checking solutions. We present Dynamic Evidence-based FAct-checking with\nMultimodal Experts (DEFAME), a modular, zero-shot MLLM pipeline for\nopen-domain, text-image claim verification. DEFAME operates in a six-stage\nprocess, dynamically selecting the tools and search depth to extract and\nevaluate textual and visual evidence. Unlike prior approaches that are\ntext-only, lack explainability, or rely solely on parametric knowledge, DEFAME\nperforms end-to-end verification, accounting for images in claims and evidence\nwhile generating structured, multimodal reports. Evaluation on the popular\nbenchmarks VERITE, AVerITeC, and MOCHEG shows that DEFAME surpasses all\nprevious methods, establishing itself as the new state-of-the-art fact-checking\nsystem for uni- and multimodal fact-checking. Moreover, we introduce a new\nmultimodal benchmark, ClaimReview2024+, featuring claims after the knowledge\ncutoff of GPT-4o, avoiding data leakage. Here, DEFAME drastically outperforms\nthe GPT-4o baselines, showing temporal generalizability and the potential for\nreal-time fact-checking."}
{"id": "2412.13102", "pdf": "https://arxiv.org/pdf/2412.13102.pdf", "abs": "https://arxiv.org/abs/2412.13102", "title": "AIR-Bench: Automated Heterogeneous Information Retrieval Benchmark", "authors": ["Jianlyu Chen", "Nan Wang", "Chaofan Li", "Bo Wang", "Shitao Xiao", "Han Xiao", "Hao Liao", "Defu Lian", "Zheng Liu"], "categories": ["cs.IR", "cs.CL"], "comment": "32 pages, 6 figures; Accepted to ACL 2025 Main", "summary": "Evaluation plays a crucial role in the advancement of information retrieval\n(IR) models. However, current benchmarks, which are based on predefined domains\nand human-labeled data, face limitations in addressing evaluation needs for\nemerging domains both cost-effectively and efficiently. To address this\nchallenge, we propose the Automated Heterogeneous Information Retrieval\nBenchmark (AIR-Bench). AIR-Bench is distinguished by three key features: 1)\nAutomated. The testing data in AIR-Bench is automatically generated by large\nlanguage models (LLMs) without human intervention. 2) Heterogeneous. The\ntesting data in AIR-Bench is generated with respect to diverse tasks, domains\nand languages. 3) Dynamic. The domains and languages covered by AIR-Bench are\nconstantly augmented to provide an increasingly comprehensive evaluation\nbenchmark for community developers. We develop a reliable and robust data\ngeneration pipeline to automatically create diverse and high-quality evaluation\ndatasets based on real-world corpora. Our findings demonstrate that the\ngenerated testing data in AIR-Bench aligns well with human-labeled testing\ndata, making AIR-Bench a dependable benchmark for evaluating IR models. The\nresources in AIR-Bench are publicly available at\nhttps://github.com/AIR-Bench/AIR-Bench."}
{"id": "2502.04757", "pdf": "https://arxiv.org/pdf/2502.04757.pdf", "abs": "https://arxiv.org/abs/2502.04757", "title": "ELITE: Enhanced Language-Image Toxicity Evaluation for Safety", "authors": ["Wonjun Lee", "Doehyeon Lee", "Eugene Choi", "Sangyoon Yu", "Ashkan Yousefpour", "Haon Park", "Bumsub Ham", "Suhyun Kim"], "categories": ["cs.CV", "cs.CL"], "comment": "ICML 2025. Project page at https://velpegor.github.io/ELITE/", "summary": "Current Vision Language Models (VLMs) remain vulnerable to malicious prompts\nthat induce harmful outputs. Existing safety benchmarks for VLMs primarily rely\non automated evaluation methods, but these methods struggle to detect implicit\nharmful content or produce inaccurate evaluations. Therefore, we found that\nexisting benchmarks have low levels of harmfulness, ambiguous data, and limited\ndiversity in image-text pair combinations. To address these issues, we propose\nthe ELITE benchmark, a high-quality safety evaluation benchmark for VLMs,\nunderpinned by our enhanced evaluation method, the ELITE evaluator. The ELITE\nevaluator explicitly incorporates a toxicity score to accurately assess\nharmfulness in multimodal contexts, where VLMs often provide specific,\nconvincing, but unharmful descriptions of images. We filter out ambiguous and\nlow-quality image-text pairs from existing benchmarks using the ELITE evaluator\nand generate diverse combinations of safe and unsafe image-text pairs. Our\nexperiments demonstrate that the ELITE evaluator achieves superior alignment\nwith human evaluations compared to prior automated methods, and the ELITE\nbenchmark offers enhanced benchmark quality and diversity. By introducing\nELITE, we pave the way for safer, more robust VLMs, contributing essential\ntools for evaluating and mitigating safety risks in real-world applications."}
{"id": "2503.01424", "pdf": "https://arxiv.org/pdf/2503.01424.pdf", "abs": "https://arxiv.org/abs/2503.01424", "title": "From Hypothesis to Publication: A Comprehensive Survey of AI-Driven Research Support Systems", "authors": ["Zekun Zhou", "Xiaocheng Feng", "Lei Huang", "Xiachong Feng", "Ziyun Song", "Ruihan Chen", "Liang Zhao", "Weitao Ma", "Yuxuan Gu", "Baoxin Wang", "Dayong Wu", "Guoping Hu", "Ting Liu", "Bing Qin"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Research is a fundamental process driving the advancement of human\ncivilization, yet it demands substantial time and effort from researchers. In\nrecent years, the rapid development of artificial intelligence (AI)\ntechnologies has inspired researchers to explore how AI can accelerate and\nenhance research. To monitor relevant advancements, this paper presents a\nsystematic review of the progress in this domain. Specifically, we organize the\nrelevant studies into three main categories: hypothesis formulation, hypothesis\nvalidation, and manuscript publication. Hypothesis formulation involves\nknowledge synthesis and hypothesis generation. Hypothesis validation includes\nthe verification of scientific claims, theorem proving, and experiment\nvalidation. Manuscript publication encompasses manuscript writing and the peer\nreview process. Furthermore, we identify and discuss the current challenges\nfaced in these areas, as well as potential future directions for research.\nFinally, we also offer a comprehensive overview of existing benchmarks and\ntools across various domains that support the integration of AI into the\nresearch process. We hope this paper serves as an introduction for beginners\nand fosters future research. Resources have been made publicly available at\nhttps://github.com/zkzhou126/AI-for-Research."}
{"id": "2503.07919", "pdf": "https://arxiv.org/pdf/2503.07919.pdf", "abs": "https://arxiv.org/abs/2503.07919", "title": "BEARCUBS: A benchmark for computer-using web agents", "authors": ["Yixiao Song", "Katherine Thai", "Chau Minh Pham", "Yapei Chang", "Mazin Nadaf", "Mohit Iyyer"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "16 pages", "summary": "Modern web agents possess computer use abilities that allow them to interact\nwith webpages by sending commands to a virtual keyboard and mouse. While such\nagents have considerable potential to assist human users with complex tasks,\nevaluating their capabilities in real-world settings poses a major challenge.\nTo this end, we introduce BEARCUBS, a \"smallbut mighty\" benchmark of 111\ninformation-seeking questions designed to evaluate a web agent's ability to\nsearch, browse, and identify factual information from the web. Unlike prior web\nagent benchmarks, solving BEARCUBS requires (1) accessing live web content\nrather than synthetic or simulated pages, which captures the unpredictability\nof real-world web interactions; and (2) performing a broad range of multimodal\ninteractions (e.g., video understanding, 3D navigation) that cannot be bypassed\nvia text-based workarounds. Each question in BEARCUBS has a corresponding\nshort, unambiguous answer and a human-validated browsing trajectory, allowing\nfor transparent evaluation of agent performance and strategies. A human study\nconfirms that BEARCUBS questions are solvable but non-trivial (84.7% human\naccuracy), revealing domain knowledge gaps and overlooked details as common\nfailure points. We find that ChatGPT Agent significantly outperforms other\ncomputer-using agents with an overall accuracy of 65.8% (compared to e.g.,\nOperator's 23.4%), showcasing substantial progress in tasks involving real\ncomputer use, such as playing web games and navigating 3D environments.\nNevertheless, closing the gap to human performance requires improvements in\nareas like fine control, complex data filtering, and execution speed. To\nfacilitate future research, BEARCUBS will be updated periodically to replace\ninvalid or contaminated questions, keeping the benchmark fresh for future\ngenerations of web agents."}
{"id": "2503.12358", "pdf": "https://arxiv.org/pdf/2503.12358.pdf", "abs": "https://arxiv.org/abs/2503.12358", "title": "IPCGRL: Language-Instructed Reinforcement Learning for Procedural Level Generation", "authors": ["In-Chang Baek", "Sung-Hyun Kim", "Seo-Young Lee", "Dong-Hyeon Kim", "Kyung-Joong Kim"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "9 pages, 9 figures, 3 tables, accepted to Conference on Games 2025", "summary": "Recent research has highlighted the significance of natural language in\nenhancing the controllability of generative models. While various efforts have\nbeen made to leverage natural language for content generation, research on deep\nreinforcement learning (DRL) agents utilizing text-based instructions for\nprocedural content generation remains limited. In this paper, we propose\nIPCGRL, an instruction-based procedural content generation method via\nreinforcement learning, which incorporates a sentence embedding model. IPCGRL\nfine-tunes task-specific embedding representations to effectively compress\ngame-level conditions. We evaluate IPCGRL in a two-dimensional level generation\ntask and compare its performance with a general-purpose embedding method. The\nresults indicate that IPCGRL achieves up to a 21.4% improvement in\ncontrollability and a 17.2% improvement in generalizability for unseen\ninstructions. Furthermore, the proposed method extends the modality of\nconditional input, enabling a more flexible and expressive interaction\nframework for procedural content generation."}
{"id": "2503.16870", "pdf": "https://arxiv.org/pdf/2503.16870.pdf", "abs": "https://arxiv.org/abs/2503.16870", "title": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs", "authors": ["Anshumann", "Mohd Abbas Zaidi", "Akhil Kedia", "Jinwoo Ahn", "Taehwak Kwon", "Kangwook Lee", "Haejun Lee", "Joohyung Lee"], "categories": ["cs.LG", "cs.AI", "cs.CL", "68T50", "I.2.7"], "comment": "Accepted as Oral paper at ACL 2025. Source code is available at\n  https://github.com/akhilkedia/RandomSamplingKD . Anshumann, Mohd Abbas Zaidi\n  and Akhil Kedia have Equal Contribution", "summary": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B."}
{"id": "2504.04704", "pdf": "https://arxiv.org/pdf/2504.04704.pdf", "abs": "https://arxiv.org/abs/2504.04704", "title": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are Important", "authors": ["Manlai Liang", "JiaMing Zhang", "Xiong Li", "Jinlong Li"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modification of the inference infrastructure and\nsignificant computation overhead. Based on the fact that the Large Language\nmodels are autoregressive models, we propose LagKV, a KV compression strategy\nonly relying on straight forward comparison among KV themselves. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on RULER benchmark show that, our approach\noutperforms SnapKV and StreamingLLM in different compression ratios. Especially\nin the 64-digit passkey retrieval task, our method outperforms the attention\nweight based method $H_2O$ over $50\\%$ with same compression ratios. Our code\nis available at https://github.com/AI-Lab-China-Merchants-Bank/LagKV."}
{"id": "2504.14928", "pdf": "https://arxiv.org/pdf/2504.14928.pdf", "abs": "https://arxiv.org/abs/2504.14928", "title": "EducationQ: Evaluating LLMs' Teaching Capabilities Through Multi-Agent Dialogue Framework", "authors": ["Yao Shi", "Rongkeng Liang", "Yong Xu"], "categories": ["cs.AI", "cs.CE", "cs.CL", "cs.CY", "cs.HC"], "comment": "Paper URL: https://aclanthology.org/2025.acl-long.1576/; Presentation\n  Video: https://www.youtube.com/watch?v=j63ooKE50I0", "summary": "Large language models (LLMs) increasingly serve as educational tools, yet\nevaluating their teaching capabilities remains challenging due to the\nresource-intensive, context-dependent, and methodologically complex nature of\nteacher-student interactions. We introduce EducationQ, a multi-agent dialogue\nframework that efficiently assesses teaching capabilities through simulated\ndynamic educational scenarios, featuring specialized agents for teaching,\nlearning, and evaluation. Testing 14 LLMs across major AI Organizations\n(OpenAI, Meta, Google, Anthropic, and others) on 1,498 questions spanning 13\ndisciplines and 10 difficulty levels reveals that teaching effectiveness does\nnot correlate linearly with model scale or general reasoning capabilities -\nwith some smaller open-source models outperforming larger commercial\ncounterparts in teaching contexts. This finding highlights a critical gap in\ncurrent evaluations that prioritize knowledge recall over interactive pedagogy.\nOur mixed-methods evaluation, combining quantitative metrics with qualitative\nanalysis and expert case studies, identifies distinct pedagogical strengths\nemployed by top-performing models (e.g., sophisticated questioning strategies,\nadaptive feedback mechanisms). Human expert evaluations show 78% agreement with\nour automated qualitative analysis of effective teaching behaviors, validating\nour methodology. EducationQ demonstrates that LLMs-as-teachers require\nspecialized optimization beyond simple scaling, suggesting next-generation\neducational AI prioritize targeted enhancement of specific pedagogical\neffectiveness."}
{"id": "2506.23276", "pdf": "https://arxiv.org/pdf/2506.23276.pdf", "abs": "https://arxiv.org/abs/2506.23276", "title": "Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in Public Goods Games", "authors": ["David Guzman Piedrahita", "Yongjin Yang", "Mrinmaya Sachan", "Giorgia Ramponi", "Bernhard Schölkopf", "Zhijing Jin"], "categories": ["cs.AI", "cs.CL"], "comment": "Published at COLM 2025", "summary": "As large language models (LLMs) are increasingly deployed as autonomous\nagents, understanding their cooperation and social mechanisms is becoming\nincreasingly important. In particular, how LLMs balance self-interest and\ncollective well-being is a critical challenge for ensuring alignment,\nrobustness, and safe deployment. In this paper, we examine the challenge of\ncostly sanctioning in multi-agent LLM systems, where an agent must decide\nwhether to invest its own resources to incentivize cooperation or penalize\ndefection. To study this, we adapt a public goods game with institutional\nchoice from behavioral economics, allowing us to observe how different LLMs\nnavigate social dilemmas over repeated interactions. Our analysis reveals four\ndistinct behavioral patterns among models: some consistently establish and\nsustain high levels of cooperation, others fluctuate between engagement and\ndisengagement, some gradually decline in cooperative behavior over time, and\nothers rigidly follow fixed strategies regardless of outcomes. Surprisingly, we\nfind that reasoning LLMs, such as the o1 series, struggle significantly with\ncooperation, whereas some traditional LLMs consistently achieve high levels of\ncooperation. These findings suggest that the current approach to improving\nLLMs, which focuses on enhancing their reasoning capabilities, does not\nnecessarily lead to cooperation, providing valuable insights for deploying LLM\nagents in environments that require sustained collaboration. Our code is\navailable at https://github.com/davidguzmanp/SanctSim"}
{"id": "2507.07966", "pdf": "https://arxiv.org/pdf/2507.07966.pdf", "abs": "https://arxiv.org/abs/2507.07966", "title": "Scaling RL to Long Videos", "authors": ["Yukang Chen", "Wei Huang", "Baifeng Shi", "Qinghao Hu", "Hanrong Ye", "Ligeng Zhu", "Zhijian Liu", "Pavlo Molchanov", "Jan Kautz", "Xiaojuan Qi", "Sifei Liu", "Hongxu Yin", "Yao Lu", "Song Han"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Code at https://github.com/NVlabs/Long-RL and model at\n  https://huggingface.co/Efficient-Large-Model/LongVILA-R1-7B", "summary": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 104K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves\nstrong performance on video benchmarks, reaching 65.0% and 70.7% accuracy on\nVideoMME without and with subtitles, respectively, and consistently\noutperforming LongVILA-R1 across multiple benchmarks. Moreover, LongVILA-R1\nshows steady performance improvements as the number of input video frames\nincreases. Notably, our MR-SP system achieves up to 2.1x speedup on long video\nRL training. In addition, we release our training system for public\navailability that supports RL training on various modalities (video, text, and\naudio), various models (VILA and Qwen series), and even image and video\ngeneration models. On a single A100 node (8 GPUs), it supports RL training on\nhour-long videos (e.g., 3,600 frames / around 256k tokens)."}
{"id": "2507.14660", "pdf": "https://arxiv.org/pdf/2507.14660.pdf", "abs": "https://arxiv.org/abs/2507.14660", "title": "When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems", "authors": ["Qibing Ren", "Sitao Xie", "Longxuan Wei", "Zhenfei Yin", "Junchi Yan", "Lizhuang Ma", "Jing Shao"], "categories": ["cs.AI", "cs.CL"], "comment": "Code is available at\n  https://github.com/renqibing/MultiAgent4Collusion", "summary": "Recent large-scale events like election fraud and financial scams have shown\nhow harmful coordinated efforts by human groups can be. With the rise of\nautonomous AI systems, there is growing concern that AI-driven groups could\nalso cause similar harm. While most AI safety research focuses on individual AI\nsystems, the risks posed by multi-agent systems (MAS) in complex real-world\nsituations are still underexplored. In this paper, we introduce a\nproof-of-concept to simulate the risks of malicious MAS collusion, using a\nflexible framework that supports both centralized and decentralized\ncoordination structures. We apply this framework to two high-risk fields:\nmisinformation spread and e-commerce fraud. Our findings show that\ndecentralized systems are more effective at carrying out malicious actions than\ncentralized ones. The increased autonomy of decentralized systems allows them\nto adapt their strategies and cause more damage. Even when traditional\ninterventions, like content flagging, are applied, decentralized groups can\nadjust their tactics to avoid detection. We present key insights into how these\nmalicious groups operate and the need for better detection systems and\ncountermeasures. Code is available at https://github.com/renqibing/RogueAgent."}
{"id": "2507.14679", "pdf": "https://arxiv.org/pdf/2507.14679.pdf", "abs": "https://arxiv.org/abs/2507.14679", "title": "GCC-Spam: Spam Detection via GAN, Contrastive Learning, and Character Similarity Networks", "authors": ["Zhijie Wang", "Zixin Xu", "Zhiyuan Pan"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The exponential growth of spam text on the Internet necessitates robust\ndetection mechanisms to mitigate risks such as information leakage and social\ninstability. This work addresses two principal challenges: adversarial\nstrategies employed by spammers and the scarcity of labeled data. We propose a\nnovel spam-text detection framework GCC-Spam, which integrates three core\ninnovations. First, a character similarity network captures orthographic and\nphonetic features to counter character-obfuscation attacks and furthermore\nproduces sentence embeddings for downstream classification. Second, contrastive\nlearning enhances discriminability by optimizing the latent-space distance\nbetween spam and normal texts. Third, a Generative Adversarial Network (GAN)\ngenerates realistic pseudo-spam samples to alleviate data scarcity while\nimproving model robustness and classification accuracy. Extensive experiments\non real-world datasets demonstrate that our model outperforms baseline\napproaches, achieving higher detection rates with significantly fewer labeled\nexamples."}
{"id": "2507.15205", "pdf": "https://arxiv.org/pdf/2507.15205.pdf", "abs": "https://arxiv.org/abs/2507.15205", "title": "Long-Short Distance Graph Neural Networks and Improved Curriculum Learning for Emotion Recognition in Conversation", "authors": ["Xinran Li", "Xiujuan Xu", "Jiaqi Qiao"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted by the 28th European Conference on Artificial Intelligence\n  (ECAI 2025)", "summary": "Emotion Recognition in Conversation (ERC) is a practical and challenging\ntask. This paper proposes a novel multimodal approach, the Long-Short Distance\nGraph Neural Network (LSDGNN). Based on the Directed Acyclic Graph (DAG), it\nconstructs a long-distance graph neural network and a short-distance graph\nneural network to obtain multimodal features of distant and nearby utterances,\nrespectively. To ensure that long- and short-distance features are as distinct\nas possible in representation while enabling mutual influence between the two\nmodules, we employ a Differential Regularizer and incorporate a BiAffine Module\nto facilitate feature interaction. In addition, we propose an Improved\nCurriculum Learning (ICL) to address the challenge of data imbalance. By\ncomputing the similarity between different emotions to emphasize the shifts in\nsimilar emotions, we design a \"weighted emotional shift\" metric and develop a\ndifficulty measurer, enabling a training process that prioritizes learning easy\nsamples before harder ones. Experimental results on the IEMOCAP and MELD\ndatasets demonstrate that our model outperforms existing benchmarks."}
{"id": "2507.16838", "pdf": "https://arxiv.org/pdf/2507.16838.pdf", "abs": "https://arxiv.org/abs/2507.16838", "title": "Segmentation-free Goodness of Pronunciation", "authors": ["Xinwei Cao", "Zijian Fan", "Torbjørn Svendsen", "Giampiero Salvi"], "categories": ["eess.AS", "cs.AI", "cs.CL"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Mispronunciation detection and diagnosis (MDD) is a significant part in\nmodern computer aided language learning (CALL) systems. Within MDD,\nphoneme-level pronunciation assessment is key to helping L2 learners improve\ntheir pronunciation. However, most systems are based on a form of goodness of\npronunciation (GOP) which requires pre-segmentation of speech into phonetic\nunits. This limits the accuracy of these methods and the possibility to use\nmodern CTC-based acoustic models for their evaluation. In this study, we first\npropose self-alignment GOP (GOP-SA) that enables the use of CTC-trained ASR\nmodels for MDD. Next, we define a more general alignment-free method that takes\nall possible alignments of the target phoneme into account (GOP-AF). We give a\ntheoretical account of our definition of GOP-AF, an implementation that solves\npotential numerical issues as well as a proper normalization which makes the\nmethod applicable with acoustic models with different peakiness over time. We\nprovide extensive experimental results on the CMU Kids and Speechocean762\ndatasets comparing the different definitions of our methods, estimating the\ndependency of GOP-AF on the peakiness of the acoustic models and on the amount\nof context around the target phoneme. Finally, we compare our methods with\nrecent studies over the Speechocean762 data showing that the feature vectors\nderived from the proposed method achieve state-of-the-art results on\nphoneme-level pronunciation assessment."}
