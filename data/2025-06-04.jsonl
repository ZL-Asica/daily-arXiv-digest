{"id": "2506.01982", "pdf": "https://arxiv.org/pdf/2506.01982.pdf", "abs": "https://arxiv.org/abs/2506.01982", "title": "Music interpretation and emotion perception: A computational and neurophysiological investigation", "authors": ["Vassilis Lyberatos", "Spyridon Kantarelis", "Ioanna Zioga", "Christina Anagnostopoulou", "Giorgos Stamou", "Anastasia Georgaki"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "This study investigates emotional expression and perception in music\nperformance using computational and neurophysiological methods. The influence\nof different performance settings, such as repertoire, diatonic modal etudes,\nand improvisation, as well as levels of expressiveness, on performers'\nemotional communication and listeners' reactions is explored. Professional\nmusicians performed various tasks, and emotional annotations were provided by\nboth performers and the audience. Audio analysis revealed that expressive and\nimprovisational performances exhibited unique acoustic features, while emotion\nanalysis showed stronger emotional responses. Neurophysiological measurements\nindicated greater relaxation in improvisational performances. This multimodal\nstudy highlights the significance of expressivity in enhancing emotional\ncommunication and audience engagement."}
{"id": "2506.01998", "pdf": "https://arxiv.org/pdf/2506.01998.pdf", "abs": "https://arxiv.org/abs/2506.01998", "title": "Inter(sectional) Alia(s): Ambiguity in Voice Agent Identity via Intersectional Japanese Self-Referents", "authors": ["Takao Fujii", "Katie Seaborn", "Madeleine Steeds", "Jun Kato"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY", "cs.SD", "eess.AS"], "comment": "CHI '25", "summary": "Conversational agents that mimic people have raised questions about the\nethics of anthropomorphizing machines with human social identity cues. Critics\nhave also questioned assumptions of identity neutrality in humanlike agents.\nRecent work has revealed that intersectional Japanese pronouns can elicit\ncomplex and sometimes evasive impressions of agent identity. Yet, the role of\nother \"neutral\" non-pronominal self-referents (NPSR) and voice as a socially\nexpressive medium remains unexplored. In a crowdsourcing study, Japanese\nparticipants (N = 204) evaluated three ChatGPT voices (Juniper, Breeze, and\nEmber) using seven self-referents. We found strong evidence of voice gendering\nalongside the potential of intersectional self-referents to evade gendering,\ni.e., ambiguity through neutrality and elusiveness. Notably, perceptions of age\nand formality intersected with gendering as per sociolinguistic theories,\nespecially boku and watakushi. This work provides a nuanced take on agent\nidentity perceptions and champions intersectional and culturally-sensitive work\non voice agents."}
{"id": "2506.02262", "pdf": "https://arxiv.org/pdf/2506.02262.pdf", "abs": "https://arxiv.org/abs/2506.02262", "title": "Composable Building Blocks for Controllable and Transparent Interactive AI Systems", "authors": ["Sebe Vanbrabant", "Gustavo Rovelo Ruiz", "Davy Vanacken"], "categories": ["cs.HC", "cs.AI", "H.5.2; I.2.0"], "comment": "Accepted to The 3rd Workshop on Engineering Interactive Systems\n  Embedding AI Technologies, EICS 2025", "summary": "While the increased integration of AI technologies into interactive systems\nenables them to solve an equally increasing number of tasks, the black box\nproblem of AI models continues to spread throughout the interactive system as a\nwhole. Explainable AI (XAI) techniques can make AI models more accessible by\nemploying post-hoc methods or transitioning to inherently interpretable models.\nWhile this makes individual AI models clearer, the overarching system\narchitecture remains opaque. To this end, we propose an approach to represent\ninteractive systems as sequences of structural building blocks, such as AI\nmodels and control mechanisms grounded in the literature. These can then be\nexplained through accompanying visual building blocks, such as XAI techniques.\nThe flow and APIs of the structural building blocks form an explicit overview\nof the system. This serves as a communication basis for both humans and\nautomated agents like LLMs, aligning human and machine interpretability of AI\nmodels. We discuss a selection of building blocks and concretize our flow-based\napproach in an architecture and accompanying prototype interactive system."}
{"id": "2506.02447", "pdf": "https://arxiv.org/pdf/2506.02447.pdf", "abs": "https://arxiv.org/abs/2506.02447", "title": "Visualization for interactively adjusting the de-bias effect of word embedding", "authors": ["Arisa Sugino", "Takayuki Itoh"], "categories": ["cs.HC"], "comment": null, "summary": "Word embedding, which converts words into numerical values, is an important\nnatural language processing technique and widely used. One of the serious\nproblems of word embedding is that the bias will be learned and affect the\nmodel if the dataset used for pre-training contains bias. On the other hand,\nindiscriminate removal of bias from word embeddings may result in the loss of\ninformation, even if the bias is undesirable to us. As a result, a risk of\nmodel performance degradation due to bias removal will be another problem. As a\nsolution to this problem, we focus on gender bias in Japanese and propose an\ninteractive visualization method to adjust the degree of debias for each word\ncategory. Specifically, we visualize the accuracy in a category classification\ntask after debiasing, and allow the user to adjust the parameters based on the\nvisualization results, so that the debiasing can be adjusted according to the\nuser's objectives. In addition, considering a trade-off between debiasing and\npreventing degradation of model performance, and that different people perceive\ngender bias differently, we developed a mechanism to present multiple choices\nof debiasing configurations applying an optimization scheme. This paper\npresents the results of an experiment in which we removed the gender bias for\nword embeddings learned from the Japanese version of Wikipedia. We classified\nwords into five categories based on a news corpus, and observed that the degree\nof influence of debiasing differed greatly among the categories. We then\nadjusted the degree of debiasing for each category based on the visualization\nresults."}
{"id": "2506.01961", "pdf": "https://arxiv.org/pdf/2506.01961.pdf", "abs": "https://arxiv.org/abs/2506.01961", "title": "Research on Medical Named Entity Identification Based On Prompt-Biomrc Model and Its Application in Intelligent Consultation System", "authors": ["Jinzhu Yang"], "categories": ["cs.CL"], "comment": null, "summary": "This study is dedicated to exploring the application of prompt learning\nmethods to advance Named Entity Recognition (NER) within the medical domain. In\nrecent years, the emergence of large-scale models has driven significant\nprogress in NER tasks, particularly with the introduction of the BioBERT\nlanguage model, which has greatly enhanced NER capabilities in medical texts.\nOur research introduces the Prompt-bioMRC model, which integrates both hard\ntemplate and soft prompt designs aimed at refining the precision and efficiency\nof medical entity recognition. Through extensive experimentation across diverse\nmedical datasets, our findings consistently demonstrate that our approach\nsurpasses traditional models. This enhancement not only validates the efficacy\nof our methodology but also highlights its potential to provide reliable\ntechnological support for applications like intelligent diagnosis systems. By\nleveraging advanced NER techniques, this study contributes to advancing\nautomated medical data processing, facilitating more accurate medical\ninformation extraction, and supporting efficient healthcare decision-making\nprocesses."}
{"id": "2506.02514", "pdf": "https://arxiv.org/pdf/2506.02514.pdf", "abs": "https://arxiv.org/abs/2506.02514", "title": "To Embody or Not: The Effect Of Embodiment On User Perception Of LLM-based Conversational Agents", "authors": ["Kyra Wang", "Boon-Kiat Quek", "Jessica Goh", "Dorien Herremans"], "categories": ["cs.HC"], "comment": null, "summary": "Embodiment in conversational agents (CAs) refers to the physical or visual\nrepresentation of these agents, which can significantly influence user\nperception and interaction. Limited work has been done examining the effect of\nembodiment on the perception of CAs utilizing modern large language models\n(LLMs) in non-hierarchical cooperative tasks, a common use case of CAs as more\npowerful models become widely available for general use. To bridge this\nresearch gap, we conducted a mixed-methods within-subjects study on how users\nperceive LLM-based CAs in cooperative tasks when embodied and non-embodied. The\nresults show that the non-embodied agent received significantly better\nquantitative appraisals for competence than the embodied agent, and in\nqualitative feedback, many participants believed that the embodied CA was more\nsycophantic than the non-embodied CA. Building on prior work on users'\nperceptions of LLM sycophancy and anthropomorphic features, we theorize that\nthe typically-positive impact of embodiment on perception of CA credibility can\nbecome detrimental in the presence of sycophancy. The implication of such a\nphenomenon is that, contrary to intuition and existing literature, embodiment\nis not a straightforward way to improve a CA's perceived credibility if there\nexists a tendency to sycophancy."}
{"id": "2506.01992", "pdf": "https://arxiv.org/pdf/2506.01992.pdf", "abs": "https://arxiv.org/abs/2506.01992", "title": "No Free Lunch in Active Learning: LLM Embedding Quality Dictates Query Strategy Success", "authors": ["Lukas Rauch", "Moritz Wirth", "Denis Huseljic", "Marek Herde", "Bernhard Sick", "Matthias Aßenmacher"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "under review @NeurIPS2025", "summary": "The advent of large language models (LLMs) capable of producing\ngeneral-purpose representations lets us revisit the practicality of deep active\nlearning (AL): By leveraging frozen LLM embeddings, we can mitigate the\ncomputational costs of iteratively fine-tuning large backbones. This study\nestablishes a benchmark and systematically investigates the influence of LLM\nembedding quality on query strategies in deep AL. We employ five top-performing\nmodels from the massive text embedding benchmark (MTEB) leaderboard and two\nbaselines for ten diverse text classification tasks. Our findings reveal key\ninsights: First, initializing the labeled pool using diversity-based sampling\nsynergizes with high-quality embeddings, boosting performance in early AL\niterations. Second, the choice of the optimal query strategy is sensitive to\nembedding quality. While the computationally inexpensive Margin sampling can\nachieve performance spikes on specific datasets, we find that strategies like\nBadge exhibit greater robustness across tasks. Importantly, their effectiveness\nis often enhanced when paired with higher-quality embeddings. Our results\nemphasize the need for context-specific evaluation of AL strategies, as\nperformance heavily depends on embedding quality and the target task."}
{"id": "2506.02700", "pdf": "https://arxiv.org/pdf/2506.02700.pdf", "abs": "https://arxiv.org/abs/2506.02700", "title": "Cognitive Load-Driven VR Memory Palaces: Personalizing Focus and Recall Enhancement", "authors": ["Zhengyang Li", "Hailin Deng"], "categories": ["cs.HC"], "comment": "10 pages, 5 figures, submitted to HCII 2025. Includes EEG-based VR\n  adaptation experiments, dynamic spatial modeling, and empirical evaluation of\n  memory performance", "summary": "Cognitive load, which varies across individuals, can significantly affect\nfocus and memory performance.This study explores the integration of Virtual\nReality (VR) with memory palace techniques, aiming to optimize VR environments\ntailored to individual cognitive load levels to improve focus and memory. We\nutilized EEG devices, specifically the Oculus Quest 2, to monitor Beta wave\nactivity in 10 participants.By modeling their cognitive load profiles through\npolynomial regression, we dynamically adjusted spatial variables within a VR\nenvironment using Grasshopper, creating personalized experiences. Results\nindicate that 8 participants showed a notable increase in Beta wave activity,\ndemonstrating improved focus and cognitive performance in the customized VR\nsettings.These findings underscore the potential of VR-based memory\nenvironments, driven by cognitive load considerations, and provide valuable\ninsights for advancing VR memory research"}
{"id": "2506.02000", "pdf": "https://arxiv.org/pdf/2506.02000.pdf", "abs": "https://arxiv.org/abs/2506.02000", "title": "NovelHopQA: Diagnosing Multi-Hop Reasoning Failures in Long Narrative Contexts", "authors": ["Abhay Gupta", "Michael Lu", "Kevin Zhu", "Sean O'Brien", "Vasu Sharma"], "categories": ["cs.CL"], "comment": null, "summary": "Current large language models (LLMs) struggle to answer questions that span\ntens of thousands of tokens, especially when multi-hop reasoning is involved.\nWhile prior benchmarks explore long-context comprehension or multi-hop\nreasoning in isolation, none jointly vary context length and reasoning depth in\nnatural narrative settings. We introduce NovelHopQA, the first benchmark to\nevaluate k1-4 hop QA over 64k-128k-token excerpts from 83 full-length\npublic-domain novels. A keyword-guided pipeline builds hop-separated chains\ngrounded in coherent storylines. We evaluate six state-of-the-art (SOTA) models\nand apply oracle-context filtering to ensure all questions are genuinely\nanswerable. Human annotators validate both alignment and hop depth. We noticed\nconsistent accuracy drops with increased hops and context length, even in\nfrontier models-revealing that sheer scale does not guarantee robust reasoning.\nOur failure mode analysis highlights common breakdowns, such as missed\nfinal-hop integration and long-range drift. NovelHopQA offers a controlled\ndiagnostic setting to stress-test multi-hop reasoning at scale."}
{"id": "2506.02714", "pdf": "https://arxiv.org/pdf/2506.02714.pdf", "abs": "https://arxiv.org/abs/2506.02714", "title": "Heatables: Effects of Infrared-LED-Induced Ear Heating on Thermal Perception, Comfort, and Cognitive Performance", "authors": ["Valeria Zitz", "Michael Küttner", "Jonas Hummel", "Michael T. Knierim", "Michael Beigl", "Tobias Röddiger"], "categories": ["cs.HC"], "comment": null, "summary": "Maintaining thermal comfort in shared indoor environments remains\nchallenging, as centralized HVAC systems are slow to adapt and standardized to\ngroup norms. Cold exposure not only reduces subjective comfort but can impair\ncognitive performance, particularly under moderate to severe cold stress.\nPersonal Comfort Systems (PCS) have shown promise by providing localized\nheating, yet many designs target distal body parts with low thermosensitivity\nand often lack portability. In this work, we investigate whether targeted\nthermal stimulation using in-ear worn devices can manipulate thermal perception\nand enhance thermal comfort. We present Heatables, a novel in-ear wearable that\nemits Near-Infrared (NIR) and Infrared (IR) radiation via integrated LEDs to\ndeliver localized optical heating. This approach leverages NIR-IR's ability to\npenetrate deeper tissues, offering advantages over traditional resistive\nheating limited to surface warming. In a placebo-controlled study with 24\nparticipants, each exposed for 150 minutes in a cool office environment\n(approximately 17.5 degrees Celsius) to simulate sustained cold stress during\ntypical sedentary office activities, Heatables significantly increased the\nperceived ambient temperature by around 1.5 degrees Celsius and delayed cold\ndiscomfort. Importantly, thermal benefits extended beyond the ear region,\nimproving both whole-body comfort and thermal acceptability. These findings\nposition in-ear NIR-IR-LED-based stimulation as a promising modality for\nunobtrusive thermal comfort enhancement in everyday contexts."}
{"id": "2506.02005", "pdf": "https://arxiv.org/pdf/2506.02005.pdf", "abs": "https://arxiv.org/abs/2506.02005", "title": "Pruning for Performance: Efficient Idiom and Metaphor Classification in Low-Resource Konkani Using mBERT", "authors": ["Timothy Do", "Pranav Saran", "Harshita Poojary", "Pranav Prabhu", "Sean O'Brien", "Vasu Sharma", "Kevin Zhu"], "categories": ["cs.CL"], "comment": "9 pages, 7 figures", "summary": "In this paper, we address the persistent challenges that figurative language\nexpressions pose for natural language processing (NLP) systems, particularly in\nlow-resource languages such as Konkani. We present a hybrid model that\nintegrates a pre-trained Multilingual BERT (mBERT) with a bidirectional LSTM\nand a linear classifier. This architecture is fine-tuned on a newly introduced\nannotated dataset for metaphor classification, developed as part of this work.\nTo improve the model's efficiency, we implement a gradient-based attention head\npruning strategy. For metaphor classification, the pruned model achieves an\naccuracy of 78%. We also applied our pruning approach to expand on an existing\nidiom classification task, achieving 83% accuracy. These results demonstrate\nthe effectiveness of attention head pruning for building efficient NLP tools in\nunderrepresented languages."}
{"id": "2506.02856", "pdf": "https://arxiv.org/pdf/2506.02856.pdf", "abs": "https://arxiv.org/abs/2506.02856", "title": "Exploring listeners' perceptions of AI-generated and human-composed music for functional emotional applications", "authors": ["Kimaya Lecamwasam", "Tishya Ray Chaudhuri"], "categories": ["cs.HC"], "comment": "18 pages (includes references and appendix), 2 figures, 5 tables\n  (includes 1 in appendix), and appendix", "summary": "This work investigates how listeners perceive and evaluate AI-generated as\ncompared to human-composed music in the context of emotional resonance and\nregulation. Across a mixed-methods design, participants were exposed to both AI\nand human music under various labeling conditions (music correctly labeled as\nAI- or human-origin, music incorrectly labeled as AI- or human-origin, and\nunlabeled music) and emotion cases (Calm and Upbeat), and were asked to rate\npreference, efficacy of target emotion elicitation, and emotional impact.\nParticipants were significantly more likely to rate human-composed music,\nregardless of labeling, as more effective at eliciting target emotional states,\nthough quantitative analyses revealed no significant differences in emotional\nresponse. However, participants were significantly more likely to indicate\npreference for AI-generated music, yielding further questions regarding the\nimpact of emotional authenticity and perceived authorship on musical appraisal.\nQualitative data underscored this, with participants associating humanness with\nqualities such as imperfection, flow, and 'soul.' These findings challenge the\nassumption that preference alone signals success in generative music systems.\nRather than positioning AI tools as replacements for human creativity or\nemotional expression, they point toward a more careful design ethos that\nacknowledges the limits of replication and prioritizes human values such as\nauthenticity, individuality, and emotion regulation in wellness and affective\ntechnologies."}
{"id": "2506.02018", "pdf": "https://arxiv.org/pdf/2506.02018.pdf", "abs": "https://arxiv.org/abs/2506.02018", "title": "Enhancing Paraphrase Type Generation: The Impact of DPO and RLHF Evaluated with Human-Ranked Data", "authors": ["Christopher Lee Lübbers"], "categories": ["cs.CL", "I.2.7"], "comment": "21 pages, 11 figures. Master's thesis, University of Goettingen,\n  December 2025. Code: https://github.com/cluebbers/dpo-rlhf-paraphrase-types.\n  Models:\n  https://huggingface.co/collections/cluebbers/enhancing-paraphrase-type-generation-673ca8d75dfe2ce962a48ac0", "summary": "Paraphrasing re-expresses meaning to enhance applications like text\nsimplification, machine translation, and question-answering. Specific\nparaphrase types facilitate accurate semantic analysis and robust language\nmodels. However, existing paraphrase-type generation methods often misalign\nwith human preferences due to reliance on automated metrics and limited\nhuman-annotated training data, obscuring crucial aspects of semantic fidelity\nand linguistic transformations.\n  This study addresses this gap by leveraging a human-ranked paraphrase-type\ndataset and integrating Direct Preference Optimization (DPO) to align model\noutputs directly with human judgments. DPO-based training increases\nparaphrase-type generation accuracy by 3 percentage points over a supervised\nbaseline and raises human preference ratings by 7 percentage points. A newly\ncreated human-annotated dataset supports more rigorous future evaluations.\nAdditionally, a paraphrase-type detection model achieves F1 scores of 0.91 for\naddition/deletion, 0.78 for same polarity substitution, and 0.70 for\npunctuation changes.\n  These findings demonstrate that preference data and DPO training produce more\nreliable, semantically accurate paraphrases, enabling downstream applications\nsuch as improved summarization and more robust question-answering. The PTD\nmodel surpasses automated metrics and provides a more reliable framework for\nevaluating paraphrase quality, advancing paraphrase-type research toward\nricher, user-aligned language generation and establishing a stronger foundation\nfor future evaluations grounded in human-centric criteria."}
{"id": "2506.02966", "pdf": "https://arxiv.org/pdf/2506.02966.pdf", "abs": "https://arxiv.org/abs/2506.02966", "title": "Unpacking Graduate Students' Learning Experience with Generative AI Teaching Assistant in A Quantitative Methodology Course", "authors": ["Zhanxin Hao", "Haifeng Luo", "Yongyi Chen", "Yu Zhang"], "categories": ["cs.HC", "68-11", "J.4"], "comment": "37 pages, 5 figures", "summary": "The study was conducted in an Advanced Quantitative Research Methods course\ninvolving 20 graduate students. During the course, student inquiries made to\nthe AI were recorded and coded using Bloom's taxonomy and the CLEAR framework.\nA series of independent sample t-tests and poisson regression analyses were\nemployed to analyse the characteristics of different questions asked by\nstudents with different backgrounds. Post course interviews were conducted with\n10 students to gain deeper insights into their perceptions. The findings\nrevealed a U-shaped pattern in students' use of the AI assistant, with higher\nusage at the beginning and towards the end of the course, and a decrease in\nusage during the middle weeks. Most questions posed to the AI focused on\nknowledge and comprehension levels, with fewer questions involving deeper\ncognitive thinking. Students with a weaker mathematical foundation used the AI\nassistant more frequently, though their inquiries tended to lack explicit and\nlogical structure compared to those with a strong mathematical foundation, who\nengaged less with the tool. These patterns suggest the need for targeted\nguidance to optimise the effectiveness of AI tools for students with varying\nlevels of academic proficiency."}
{"id": "2506.02019", "pdf": "https://arxiv.org/pdf/2506.02019.pdf", "abs": "https://arxiv.org/abs/2506.02019", "title": "ChatCFD: an End-to-End CFD Agent with Domain-specific Structured Thinking", "authors": ["E Fan", "Weizong Wang", "Tianhan Zhang"], "categories": ["cs.CL"], "comment": "19 pages, 8 figures", "summary": "Computational Fluid Dynamics (CFD) is essential for scientific and\nengineering advancements but is limited by operational complexity and the need\nfor extensive expertise. This paper presents ChatCFD, a large language\nmodel-driven pipeline that automates CFD workflows within the OpenFOAM\nframework. It enables users to configure and execute complex simulations from\nnatural language prompts or published literature with minimal expertise. The\ninnovation is its structured approach to database construction, configuration\nvalidation, and error reflection, integrating CFD and OpenFOAM knowledge with\ngeneral language models to improve accuracy and adaptability. Validation shows\nChatCFD can autonomously reproduce published CFD results, handling complex,\nunseen configurations beyond basic examples, a task challenging for general\nlanguage models."}
{"id": "2506.02993", "pdf": "https://arxiv.org/pdf/2506.02993.pdf", "abs": "https://arxiv.org/abs/2506.02993", "title": "Mapping Student-AI Interaction Dynamics in Multi-Agent Learning Environments: Supporting Personalised Learning and Reducing Performance Gaps", "authors": ["Zhanxin Hao", "Jie Cao", "Ruimiao Li", "Jifan Yu", "Zhiyuan Liu", "Yu Zhang"], "categories": ["cs.HC"], "comment": "27 pages, 8 figures", "summary": "Multi-agent AI systems, which simulate diverse instructional roles such as\nteachers and peers, offer new possibilities for personalized and interactive\nlearning. Yet, student-AI interaction patterns and their pedagogical\nimplications remain unclear. This study explores how university students\nengaged with multiple AI agents, and how these interactions influenced\ncognitive outcomes (learning gains) and non-cognitive factors (motivation,\ntechnology acceptance). Based on MAIC, an online learning platform with\nmulti-agent, the research involved 305 university students and 19,365 lines of\ndialogue data. Pre- and post-test scores, self-reported motivation and\ntechnology acceptance were also collected. The study identified two engagement\npatterns: co-construction of knowledge and co-regulation. Lag sequential\nanalysis revealed that students with lower prior knowledge relied more on\nco-construction of knowledge sequences, showing higher learning gains and\npost-course motivation. In contrast, students with higher prior knowledge\nengaged more in co-regulation behaviors but exhibited limited learning\nimprovement. Technology acceptance increased across all groups. These findings\nsuggest that multi-agent AI systems can adapt to students' varying needs,\nsupport differentiated engagement, and reduce performance gaps. Implications\nfor personalized system design and future research directions are discussed."}
{"id": "2506.02037", "pdf": "https://arxiv.org/pdf/2506.02037.pdf", "abs": "https://arxiv.org/abs/2506.02037", "title": "FinS-Pilot: A Benchmark for Online Financial System", "authors": ["Feng Wang", "Yiding Sun", "Jiaxin Mao", "Wei Xue", "Danqing Xu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious professional domains, with their performance typically evaluated\nthrough standardized benchmarks. However, the development of financial RAG\nbenchmarks has been constrained by data confidentiality issues and the lack of\ndynamic data integration. To address this issue, we introduces FinS-Pilot, a\nnovel benchmark for evaluating RAG systems in online financial applications.\nConstructed from real-world financial assistant interactions, our benchmark\nincorporates both real-time API data and structured text sources, organized\nthrough an intent classification framework covering critical financial domains\nsuch as equity analysis and macroeconomic forecasting. The benchmark enables\ncomprehensive evaluation of financial assistants' capabilities in handling both\nstatic knowledge and time-sensitive market information. Through systematic\nexperiments with multiple Chinese leading LLMs, we demonstrate FinS-Pilot's\neffectiveness in identifying models suitable for financial applications while\naddressing the current gap in specialized evaluation tools for the financial\ndomain. Our work contributes both a practical evaluation framework and a\ncurated dataset to advance research in financial NLP systems. The code and\ndataset are accessible on\nGitHub\\footnote{https://github.com/PhealenWang/financial\\_rag\\_benchmark}."}
{"id": "2506.03052", "pdf": "https://arxiv.org/pdf/2506.03052.pdf", "abs": "https://arxiv.org/abs/2506.03052", "title": "Feedstack: Layering Structured Representations over Unstructured Feedback to Scaffold Human AI Conversation", "authors": ["Hannah Vy Nguyen", "Yu-Chun Grace Yen", "Omar Shakir", "Hang Huynh", "Sebastian Gutierrez", "June A. Smith", "Sheila Jimenez", "Salma Abdelgelil", "Stephen MacNeil"], "categories": ["cs.HC"], "comment": "CUI '25, Proceedings of the 7th ACM Conference on Conversational User\n  Interfaces, July 8--10, 2025, Waterloo, ON, Canada", "summary": "Many conversational user interfaces facilitate linear conversations with\nturn-based dialogue, similar to face-to-face conversations between people.\nHowever, digital conversations can afford more than simple back-and-forth; they\ncan be layered with interaction techniques and structured representations that\nscaffold exploration, reflection, and shared understanding between users and AI\nsystems. We introduce Feedstack, a speculative interface that augments feedback\nconversations with layered affordances for organizing, navigating, and\nexternalizing feedback. These layered structures serve as a shared\nrepresentation of the conversation that can surface user intent and reveal\nunderlying design principles. This work represents an early exploration of this\nvision using a research-through-design approach. We describe system features\nand design rationale, and present insights from two formative (n=8, n=8)\nstudies to examine how novice designers engage with these layered supports.\nRather than presenting a conclusive evaluation, we reflect on Feedstack as a\ndesign probe that opens up new directions for conversational feedback systems."}
{"id": "2506.02041", "pdf": "https://arxiv.org/pdf/2506.02041.pdf", "abs": "https://arxiv.org/abs/2506.02041", "title": "Enhancing Multimodal Continual Instruction Tuning with BranchLoRA", "authors": ["Duzhen Zhang", "Yong Ren", "Zhong-Zhi Li", "Yahan Yu", "Jiahua Dong", "Chenxing Li", "Zhilong Ji", "Jinfeng Bai"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL2025 Main Conference", "summary": "Multimodal Continual Instruction Tuning (MCIT) aims to finetune Multimodal\nLarge Language Models (MLLMs) to continually align with human intent across\nsequential tasks. Existing approaches often rely on the Mixture-of-Experts\n(MoE) LoRA framework to preserve previous instruction alignments. However,\nthese methods are prone to Catastrophic Forgetting (CF), as they aggregate all\nLoRA blocks via simple summation, which compromises performance over time. In\nthis paper, we identify a critical parameter inefficiency in the MoELoRA\nframework within the MCIT context. Based on this insight, we propose\nBranchLoRA, an asymmetric framework to enhance both efficiency and performance.\nTo mitigate CF, we introduce a flexible tuning-freezing mechanism within\nBranchLoRA, enabling branches to specialize in intra-task knowledge while\nfostering inter-task collaboration. Moreover, we incrementally incorporate\ntask-specific routers to ensure an optimal branch distribution over time,\nrather than favoring the most recent task. To streamline inference, we\nintroduce a task selector that automatically routes test inputs to the\nappropriate router without requiring task identity. Extensive experiments on\nthe latest MCIT benchmark demonstrate that BranchLoRA significantly outperforms\nMoELoRA and maintains its superiority across various MLLM sizes."}
{"id": "2506.03113", "pdf": "https://arxiv.org/pdf/2506.03113.pdf", "abs": "https://arxiv.org/abs/2506.03113", "title": "Assessing Workers Neuro-physiological Stress Responses to Augmented Reality Safety Warnings in Immersive Virtual Roadway Work Zones", "authors": ["Fatemeh Banani Ardecani", "Omidreza Shoghli"], "categories": ["cs.HC"], "comment": null, "summary": "This paper presents a multi-stage experimental framework that integrates\nimmersive Virtual Reality (VR) simulations, wearable sensors, and advanced\nsignal processing to investigate construction workers neuro-physiological\nstress responses to multi-sensory AR-enabled warnings. Participants performed\nlight- and moderate-intensity roadway maintenance tasks within a high-fidelity\nVR roadway work zone, while key stress markers of electrodermal activity (EDA),\nheart rate variability (HRV), and electroencephalography (EEG) were\ncontinuously measured. Statistical analyses revealed that task intensity\nsignificantly influenced physiological and neurological stress indicators.\nModerate-intensity tasks elicited greater autonomic arousal, evidenced by\nelevated heart rate measures (mean-HR, std-HR, max-HR) and stronger\nelectrodermal responses, while EEG data indicated distinct stress-related alpha\nsuppression and beta enhancement. Feature-importance analysis further\nidentified mean EDR and short-term HR metrics as discriminative for classifying\ntask intensity. Correlation results highlighted a temporal lag between\nimmediate neural changes and subsequent physiological stress reactions,\nemphasizing the interplay between cognition and autonomic regulation during\nhazardous tasks."}
{"id": "2506.02058", "pdf": "https://arxiv.org/pdf/2506.02058.pdf", "abs": "https://arxiv.org/abs/2506.02058", "title": "Evaluating the Unseen Capabilities: How Many Theorems Do LLMs Know?", "authors": ["Xiang Li", "Jiayi Xin", "Qi Long", "Weijie J. Su"], "categories": ["cs.CL", "cs.IR", "cs.LG", "stat.AP", "stat.ME"], "comment": null, "summary": "Accurate evaluation of large language models (LLMs) is crucial for\nunderstanding their capabilities and guiding their development. However,\ncurrent evaluations often inconsistently reflect the actual capacities of these\nmodels. In this paper, we demonstrate that one of many contributing factors to\nthis \\textit{evaluation crisis} is the oversight of unseen knowledge --\ninformation encoded by LLMs but not directly observed or not yet observed\nduring evaluations. We introduce KnowSum, a statistical framework designed to\nprovide a more comprehensive assessment by quantifying the unseen knowledge for\na class of evaluation tasks. KnowSum estimates the unobserved portion by\nextrapolating from the appearance frequencies of observed knowledge instances.\nWe demonstrate the effectiveness and utility of KnowSum across three critical\napplications: estimating total knowledge, evaluating information retrieval\neffectiveness, and measuring output diversity. Our experiments reveal that a\nsubstantial volume of knowledge is omitted when relying solely on observed LLM\nperformance. Importantly, KnowSum yields significantly different comparative\nrankings for several common LLMs based on their internal knowledge."}
{"id": "2504.04332", "pdf": "https://arxiv.org/pdf/2504.04332.pdf", "abs": "https://arxiv.org/abs/2504.04332", "title": "IMPersona: Evaluating Individual Level LM Impersonation", "authors": ["Quan Shi", "Carlos E. Jimenez", "Stephen Dong", "Brian Seo", "Caden Yao", "Adam Kelch", "Karthik Narasimhan"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "25 pages, 9 pages main", "summary": "As language models achieve increasingly human-like capabilities in\nconversational text generation, a critical question emerges: to what extent can\nthese systems simulate the characteristics of specific individuals? To evaluate\nthis, we introduce IMPersona, a framework for evaluating LMs at impersonating\nspecific individuals' writing style and personal knowledge. Using supervised\nfine-tuning and a hierarchical memory-inspired retrieval system, we demonstrate\nthat even modestly sized open-source models, such as Llama-3.1-8B-Instruct, can\nachieve impersonation abilities at concerning levels. In blind conversation\nexperiments, participants (mis)identified our fine-tuned models with memory\nintegration as human in 44.44% of interactions, compared to just 25.00% for the\nbest prompting-based approach. We analyze these results to propose detection\nmethods and defense strategies against such impersonation attempts. Our\nfindings raise important questions about both the potential applications and\nrisks of personalized language models, particularly regarding privacy,\nsecurity, and the ethical deployment of such technologies in real-world\ncontexts."}
{"id": "2506.02126", "pdf": "https://arxiv.org/pdf/2506.02126.pdf", "abs": "https://arxiv.org/abs/2506.02126", "title": "Knowledge or Reasoning? A Close Look at How LLMs Think Across Domains", "authors": ["Juncheng Wu", "Sheng Liu", "Haoqin Tu", "Hang Yu", "Xiaoke Huang", "James Zou", "Cihang Xie", "Yuyin Zhou"], "categories": ["cs.CL"], "comment": "17 pages, preprint", "summary": "Recent advances in reasoning-enhanced Large Language Models such as\nOpenAI-o1/3 and DeepSeek-R1 have significantly improved performance on complex\ntasks. However, the quality and transparency of their internal reasoning\nprocesses remain underexplored. This work moves beyond the final-answer\naccuracy and investigates step-by-step reasoning in the medical and\nmathematical domains by explicitly decomposing the thinking trajectories into\ntwo parts: knowledge and reasoning. Specifically, we introduce a fine-grained\nevaluation framework that judges: (1) the correctness of knowledge used\n(measured by Knowledge Index (KI)) and (2) the quality of reasoning (measured\nby Information Gain (InfoGain)). Using this framework, we study R1-distilled\nand base Qwen models trained with supervised fine-tuning (SFT) and/or\nreinforcement learning (RL) in the medical and math domains. Three intriguing\nfindings emerge: (1) The general reasoning abilities in R1-distilled models do\nnot transfer effectively to the medical domain through either SFT or RL. (2)\nSFT raises final-answer accuracy in both domains, but often at the cost of\nreasoning quality: InfoGain drops by 38.9% on average compared with untrained\nmodels; In the medical domain, however, SFT remains crucial because domain\nknowledge is indispensable. (3) RL enhances medical reasoning by pruning\ninaccurate or irrelevant knowledge from reasoning paths, thereby improving both\nreasoning accuracy and knowledge correctness."}
{"id": "2506.02064", "pdf": "https://arxiv.org/pdf/2506.02064.pdf", "abs": "https://arxiv.org/abs/2506.02064", "title": "The Measurement Imbalance in Agentic AI Evaluation Undermines Industry Productivity Claims", "authors": ["Kiana Jafari Meimandi", "Gabriela Aránguiz-Dias", "Grace Ra Kim", "Lana Saadeddin", "Mykel J. Kochenderfer"], "categories": ["cs.CY", "cs.HC"], "comment": "15 pages, 3 figures", "summary": "As industry reports claim agentic AI systems deliver double-digit\nproductivity gains and multi-trillion dollar economic potential, the validity\nof these claims has become critical for investment decisions, regulatory\npolicy, and responsible technology adoption. However, this paper demonstrates\nthat current evaluation practices for agentic AI systems exhibit a systemic\nimbalance that calls into question prevailing industry productivity claims. Our\nsystematic review of 84 papers (2023--2025) reveals an evaluation imbalance\nwhere technical metrics dominate assessments (83%), while human-centered (30%),\nsafety (53%), and economic assessments (30%) remain peripheral, with only 15%\nincorporating both technical and human dimensions. This measurement gap creates\na fundamental disconnect between benchmark success and deployment value. We\npresent evidence from healthcare, finance, and retail sectors where systems\nexcelling on technical metrics failed in real-world implementation due to\nunmeasured human, temporal, and contextual factors. Our position is not against\nagentic AI's potential, but rather that current evaluation frameworks\nsystematically privilege narrow technical metrics while neglecting dimensions\ncritical to real-world success. We propose a balanced four-axis evaluation\nmodel and call on the community to lead this paradigm shift because\nbenchmark-driven optimization shapes what we build. By redefining evaluation\npractices, we can better align industry claims with deployment realities and\nensure responsible scaling of agentic systems in high-stakes domains."}
{"id": "2506.02132", "pdf": "https://arxiv.org/pdf/2506.02132.pdf", "abs": "https://arxiv.org/abs/2506.02132", "title": "Model Internal Sleuthing: Finding Lexical Identity and Inflectional Morphology in Modern Language Models", "authors": ["Michael Li", "Nishant Subramani"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large transformer-based language models dominate modern NLP, yet our\nunderstanding of how they encode linguistic information is rooted in studies of\nearly models like BERT and GPT-2. To better understand today's language models,\nwe investigate how both classical architectures (BERT, DeBERTa, GPT-2)and\ncontemporary large language models (Pythia, OLMo-2, Gemma-2, Qwen2.5,\nLlama-3.1) represent lexical identity and inflectional morphology. We train\nlinear and nonlinear classifiers on layer-wise activations to predict word\nlemmas and inflectional features. We discover that models concentrate lexical\ninformation linearly in early layers and increasingly nonlinearly in later\nlayers, while keeping inflectional information uniformly accessible and\nlinearly separable throughout the layers. Further analysis reveals that these\nmodels encode inflectional morphology through generalizable abstractions, but\nrely predominantly on memorization to encode lexical identity. Remarkably,\nthese patterns emerge across all 16 models we test, despite differences in\narchitecture, size, and training regime (including pretrained and\ninstruction-tuned variants). This consistency suggests that, despite\nsubstantial advances in LLM technologies, transformer models organize\nlinguistic information in similar ways, indicating that these properties could\nbe fundamental for next token prediction and are learned early during\npretraining. Our code is available at\nhttps://github.com/ml5885/model_internal_sleuthing."}
{"id": "2506.02380", "pdf": "https://arxiv.org/pdf/2506.02380.pdf", "abs": "https://arxiv.org/abs/2506.02380", "title": "EyeNavGS: A 6-DoF Navigation Dataset and Record-n-Replay Software for Real-World 3DGS Scenes in VR", "authors": ["Zihao Ding", "Cheng-Tse Lee", "Mufeng Zhu", "Tao Guan", "Yuan-Chun Sun", "Cheng-Hsin Hsu", "Yao Liu"], "categories": ["cs.MM", "cs.CV", "cs.GR", "cs.HC"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) is an emerging media representation that\nreconstructs real-world 3D scenes in high fidelity, enabling\n6-degrees-of-freedom (6-DoF) navigation in virtual reality (VR). However,\ndeveloping and evaluating 3DGS-enabled applications and optimizing their\nrendering performance, require realistic user navigation data. Such data is\ncurrently unavailable for photorealistic 3DGS reconstructions of real-world\nscenes. This paper introduces EyeNavGS (EyeNavGS), the first publicly available\n6-DoF navigation dataset featuring traces from 46 participants exploring twelve\ndiverse, real-world 3DGS scenes. The dataset was collected at two sites, using\nthe Meta Quest Pro headsets, recording the head pose and eye gaze data for each\nrendered frame during free world standing 6-DoF navigation. For each of the\ntwelve scenes, we performed careful scene initialization to correct for scene\ntilt and scale, ensuring a perceptually-comfortable VR experience. We also\nrelease our open-source SIBR viewer software fork with record-and-replay\nfunctionalities and a suite of utility tools for data processing, conversion,\nand visualization. The EyeNavGS dataset and its accompanying software tools\nprovide valuable resources for advancing research in 6-DoF viewport prediction,\nadaptive streaming, 3D saliency, and foveated rendering for 3DGS scenes. The\nEyeNavGS dataset is available at: https://symmru.github.io/EyeNavGS/."}
{"id": "2506.02147", "pdf": "https://arxiv.org/pdf/2506.02147.pdf", "abs": "https://arxiv.org/abs/2506.02147", "title": "BabyLM's First Constructions: Causal interventions provide a signal of learning", "authors": ["Joshua Rozner", "Leonie Weissweiler", "Cory Shain"], "categories": ["cs.CL"], "comment": null, "summary": "Construction grammar posits that children acquire constructions (form-meaning\npairings) from the statistics of their environment. Recent work supports this\nhypothesis by showing sensitivity to constructions in pretrained language\nmodels (PLMs), including one recent study (Rozner et al., 2025) demonstrating\nthat constructions shape the PLM's output distribution. However, models under\nstudy have generally been trained on developmentally implausible amounts of\ndata, casting doubt on their relevance to human language learning. Here we use\nRozner et al.'s methods to evaluate constructional learning in models from the\n2024 BabyLM challenge. Our results show that even when trained on\ndevelopmentally plausible quantities of data, models represent diverse\nconstructions, even hard cases that are superficially indistinguishable. We\nfurther find correlational evidence that constructional performance may be\nfunctionally relevant: models that better represent constructions perform\nbetter on the BabyLM benchmarks."}
{"id": "2506.02449", "pdf": "https://arxiv.org/pdf/2506.02449.pdf", "abs": "https://arxiv.org/abs/2506.02449", "title": "IP-Dialog: Evaluating Implicit Personalization in Dialogue Systems with Synthetic Data", "authors": ["Bo Peng", "Zhiheng Wang", "Heyang Gong", "Chaochao Lu"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "In modern dialogue systems, the ability to implicitly infer user backgrounds\nfrom conversations and leverage this information for personalized assistance is\ncrucial. However, the scarcity of high-quality data remains a fundamental\nchallenge to evaluating and improving this capability. Traditional dataset\nconstruction methods are labor-intensive, resource-demanding, and raise privacy\nconcerns. To address these issues, we propose a novel approach for automatic\nsynthetic data generation and introduce the Implicit Personalized Dialogue\n(IP-Dialog) benchmark along with a training dataset, covering 10 tasks and 12\nuser attribute types. Additionally, we develop a systematic evaluation\nframework with four metrics to assess both attribute awareness and reasoning\ncapabilities. We further propose five causal graphs to elucidate models'\nreasoning pathways during implicit personalization. Extensive experiments yield\ninsightful observations and prove the reliability of our dataset."}
{"id": "2506.02157", "pdf": "https://arxiv.org/pdf/2506.02157.pdf", "abs": "https://arxiv.org/abs/2506.02157", "title": "HENT-SRT: Hierarchical Efficient Neural Transducer with Self-Distillation for Joint Speech Recognition and Translation", "authors": ["Amir Hussein", "Cihan Xiao", "Matthew Wiesner", "Dan Povey", "Leibny Paola Garcia", "Sanjeev Khudanpur"], "categories": ["cs.CL", "eess.AS"], "comment": null, "summary": "Neural transducers (NT) provide an effective framework for speech streaming,\ndemonstrating strong performance in automatic speech recognition (ASR).\nHowever, the application of NT to speech translation (ST) remains challenging,\nas existing approaches struggle with word reordering and performance\ndegradation when jointly modeling ASR and ST, resulting in a gap with\nattention-based encoder-decoder (AED) models. Existing NT-based ST approaches\nalso suffer from high computational training costs. To address these issues, we\npropose HENT-SRT (Hierarchical Efficient Neural Transducer for Speech\nRecognition and Translation), a novel framework that factorizes ASR and\ntranslation tasks to better handle reordering. To ensure robust ST while\npreserving ASR performance, we use self-distillation with CTC consistency\nregularization. Moreover, we improve computational efficiency by incorporating\nbest practices from ASR transducers, including a down-sampled hierarchical\nencoder, a stateless predictor, and a pruned transducer loss to reduce training\ncomplexity. Finally, we introduce a blank penalty during decoding, reducing\ndeletions and improving translation quality. Our approach is evaluated on three\nconversational datasets Arabic, Spanish, and Mandarin achieving new\nstate-of-the-art performance among NT models and substantially narrowing the\ngap with AED-based systems."}
{"id": "2506.02533", "pdf": "https://arxiv.org/pdf/2506.02533.pdf", "abs": "https://arxiv.org/abs/2506.02533", "title": "Natural Language Processing to Enhance Deliberation in Political Online Discussions: A Survey", "authors": ["Maike Behrendt", "Stefan Sylvius Wagner", "Carina Weinmann", "Marike Bormann", "Mira Warne", "Stefan Harmeling"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Political online participation in the form of discussing political issues and\nexchanging opinions among citizens is gaining importance with more and more\nformats being held digitally. To come to a decision, a careful discussion and\nconsideration of opinions and a civil exchange of arguments, which is defined\nas the act of deliberation, is desirable. The quality of discussions and\nparticipation processes in terms of their deliberativeness highly depends on\nthe design of platforms and processes. To facilitate online communication for\nboth participants and initiators, machine learning methods offer a lot of\npotential. In this work we want to showcase which issues occur in political\nonline discussions and how machine learning can be used to counteract these\nissues and enhance deliberation."}
{"id": "2506.02172", "pdf": "https://arxiv.org/pdf/2506.02172.pdf", "abs": "https://arxiv.org/abs/2506.02172", "title": "Different Speech Translation Models Encode and Translate Speaker Gender Differently", "authors": ["Dennis Fucci", "Marco Gaido", "Matteo Negri", "Luisa Bentivogli", "Andre Martins", "Giuseppe Attanasio"], "categories": ["cs.CL"], "comment": "Accepted at ACL 2025", "summary": "Recent studies on interpreting the hidden states of speech models have shown\ntheir ability to capture speaker-specific features, including gender. Does this\nfinding also hold for speech translation (ST) models? If so, what are the\nimplications for the speaker's gender assignment in translation? We address\nthese questions from an interpretability perspective, using probing methods to\nassess gender encoding across diverse ST models. Results on three language\ndirections (English-French/Italian/Spanish) indicate that while traditional\nencoder-decoder models capture gender information, newer architectures --\nintegrating a speech encoder with a machine translation system via adapters --\ndo not. We also demonstrate that low gender encoding capabilities result in\nsystems' tendency toward a masculine default, a translation bias that is more\npronounced in newer architectures."}
{"id": "2506.02622", "pdf": "https://arxiv.org/pdf/2506.02622.pdf", "abs": "https://arxiv.org/abs/2506.02622", "title": "HORUS: A Mixed Reality Interface for Managing Teams of Mobile Robots", "authors": ["Omotoye Shamsudeen Adekoya", "Antonio Sgorbissa", "Carmine Tommaso Recchiuto"], "categories": ["cs.RO", "cs.HC"], "comment": "7 pages, 7 figures, conference paper submitted to IROS 2025", "summary": "Mixed Reality (MR) interfaces have been extensively explored for controlling\nmobile robots, but there is limited research on their application to managing\nteams of robots. This paper presents HORUS: Holistic Operational Reality for\nUnified Systems, a Mixed Reality interface offering a comprehensive set of\ntools for managing multiple mobile robots simultaneously. HORUS enables\noperators to monitor individual robot statuses, visualize sensor data projected\nin real time, and assign tasks to single robots, subsets of the team, or the\nentire group, all from a Mini-Map (Ground Station). The interface also provides\ndifferent teleoperation modes: a mini-map mode that allows teleoperation while\nobserving the robot model and its transform on the mini-map, and a\nsemi-immersive mode that offers a flat, screen-like view in either single or\nstereo view (3D). We conducted a user study in which participants used HORUS to\nmanage a team of mobile robots tasked with finding clues in an environment,\nsimulating search and rescue tasks. This study compared HORUS's full-team\nmanagement capabilities with individual robot teleoperation. The experiments\nvalidated the versatility and effectiveness of HORUS in multi-robot\ncoordination, demonstrating its potential to advance human-robot collaboration\nin dynamic, team-based environments."}
{"id": "2506.02175", "pdf": "https://arxiv.org/pdf/2506.02175.pdf", "abs": "https://arxiv.org/abs/2506.02175", "title": "AI Debate Aids Assessment of Controversial Claims", "authors": ["Salman Rahman", "Sheriff Issaka", "Ashima Suvarna", "Genglin Liu", "James Shiffer", "Jaeyoung Lee", "Md Rizwan Parvez", "Hamid Palangi", "Shi Feng", "Nanyun Peng", "Yejin Choi", "Julian Michael", "Liwei Jiang", "Saadia Gabriel"], "categories": ["cs.CL"], "comment": null, "summary": "As AI grows more powerful, it will increasingly shape how we understand the\nworld. But with this influence comes the risk of amplifying misinformation and\ndeepening social divides-especially on consequential topics like public health\nwhere factual accuracy directly impacts well-being. Scalable Oversight aims to\nensure AI truthfulness by enabling humans to supervise systems that may exceed\nhuman capabilities--yet humans themselves hold different beliefs and biases\nthat impair their judgment. We study whether AI debate can guide biased judges\ntoward the truth by having two AI systems debate opposing sides of\ncontroversial COVID-19 factuality claims where people hold strong prior\nbeliefs. We conduct two studies: one with human judges holding either\nmainstream or skeptical beliefs evaluating factuality claims through\nAI-assisted debate or consultancy protocols, and a second examining the same\nproblem with personalized AI judges designed to mimic these different human\nbelief systems. In our human study, we find that debate-where two AI advisor\nsystems present opposing evidence-based arguments-consistently improves\njudgment accuracy and confidence calibration, outperforming consultancy with a\nsingle-advisor system by 10% overall. The improvement is most significant for\njudges with mainstream beliefs (+15.2% accuracy), though debate also helps\nskeptical judges who initially misjudge claims move toward accurate views\n(+4.7% accuracy). In our AI judge study, we find that AI judges with human-like\npersonas achieve even higher accuracy (78.5%) than human judges (70.1%) and\ndefault AI judges without personas (69.8%), suggesting their potential for\nsupervising frontier AI models. These findings highlight AI debate as a\npromising path toward scalable, bias-resilient oversight--leveraging both\ndiverse human and AI judgments to move closer to truth in contested domains."}
{"id": "2506.02715", "pdf": "https://arxiv.org/pdf/2506.02715.pdf", "abs": "https://arxiv.org/abs/2506.02715", "title": "UltrasonicSpheres: Localized, Multi-Channel Sound Spheres Using Off-the-Shelf Speakers and Earables", "authors": ["Michael Küttner", "Valeria Sitz", "Kathrin Gerling", "Michael Beigl", "Tobias Röddiger"], "categories": ["cs.SD", "cs.HC", "eess.AS"], "comment": null, "summary": "We present a demo ofUltrasonicSpheres, a novel system for location-specific\naudio delivery using wearable earphones that decode ultrasonic signals into\naudible sound. Unlike conventional beamforming setups, UltrasonicSpheres relies\non single ultrasonic speakers to broadcast localized audio with multiple\nchannels, each encoded on a distinct ultrasonic carrier frequency. Users\nwearing our acoustically transparent earphones can demodulate their selected\nstream, such as exhibit narrations in a chosen language, while remaining fully\naware of ambient environmental sounds. The experience preserves spatial audio\nperception, giving the impression that the sound originates directly from the\nphysical location of the source. This enables personalized, localized audio\nwithout requiring pairing, tracking, or additional infrastructure. Importantly,\nvisitors not equipped with the earphones are unaffected, as the ultrasonic\nsignals are inaudible to the human ear. Our demo invites participants to\nexplore multiple co-located audio zones and experience how UltrasonicSpheres\nsupports unobtrusive delivery of personalized sound in public spaces."}
{"id": "2506.02181", "pdf": "https://arxiv.org/pdf/2506.02181.pdf", "abs": "https://arxiv.org/abs/2506.02181", "title": "Echoes of Phonetics: Unveiling Relevant Acoustic Cues for ASR via Feature Attribution", "authors": ["Dennis Fucci", "Marco Gaido", "Matteo Negri", "Mauro Cettolo", "Luisa Bentivogli"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at Interspeech 2025", "summary": "Despite significant advances in ASR, the specific acoustic cues models rely\non remain unclear. Prior studies have examined such cues on a limited set of\nphonemes and outdated models. In this work, we apply a feature attribution\ntechnique to identify the relevant acoustic cues for a modern Conformer-based\nASR system. By analyzing plosives, fricatives, and vowels, we assess how\nfeature attributions align with their acoustic properties in the time and\nfrequency domains, also essential for human speech perception. Our findings\nshow that the ASR model relies on vowels' full time spans, particularly their\nfirst two formants, with greater saliency in male speech. It also better\ncaptures the spectral characteristics of sibilant fricatives than non-sibilants\nand prioritizes the release phase in plosives, especially burst\ncharacteristics. These insights enhance the interpretability of ASR models and\nhighlight areas for future research to uncover potential gaps in model\nrobustness."}
{"id": "2506.02911", "pdf": "https://arxiv.org/pdf/2506.02911.pdf", "abs": "https://arxiv.org/abs/2506.02911", "title": "Cell-o1: Training LLMs to Solve Single-Cell Reasoning Puzzles with Reinforcement Learning", "authors": ["Yin Fang", "Qiao Jin", "Guangzhi Xiong", "Bowen Jin", "Xianrui Zhong", "Siru Ouyang", "Aidong Zhang", "Jiawei Han", "Zhiyong Lu"], "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.HC", "cs.LG"], "comment": "28 pages; 16 tables; 7 figures; Code:\n  https://github.com/ncbi-nlp/cell-o1", "summary": "Cell type annotation is a key task in analyzing the heterogeneity of\nsingle-cell RNA sequencing data. Although recent foundation models automate\nthis process, they typically annotate cells independently, without considering\nbatch-level cellular context or providing explanatory reasoning. In contrast,\nhuman experts often annotate distinct cell types for different cell clusters\nbased on their domain knowledge. To mimic this workflow, we introduce the\nCellPuzzles task, where the objective is to assign unique cell types to a batch\nof cells. This benchmark spans diverse tissues, diseases, and donor conditions,\nand requires reasoning across the batch-level cellular context to ensure label\nuniqueness. We find that off-the-shelf large language models (LLMs) struggle on\nCellPuzzles, with the best baseline (OpenAI's o1) achieving only 19.0%\nbatch-level accuracy. To fill this gap, we propose Cell-o1, a 7B LLM trained\nvia supervised fine-tuning on distilled reasoning traces, followed by\nreinforcement learning with batch-level rewards. Cell-o1 achieves\nstate-of-the-art performance, outperforming o1 by over 73% and generalizing\nwell across contexts. Further analysis of training dynamics and reasoning\nbehaviors provides insights into batch-level annotation performance and\nemergent expert-like reasoning. Code and data are available at\nhttps://github.com/ncbi-nlp/cell-o1."}
{"id": "2506.02204", "pdf": "https://arxiv.org/pdf/2506.02204.pdf", "abs": "https://arxiv.org/abs/2506.02204", "title": "BehaviorBox: Automated Discovery of Fine-Grained Performance Differences Between Language Models", "authors": ["Lindia Tjuatja", "Graham Neubig"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Language model evaluation is a daunting task: prompts are brittle,\ncorpus-level perplexities are vague, and the choice of benchmarks are endless.\nFinding examples that show meaningful, generalizable differences between two\nLMs is crucial to understanding where one model succeeds and another fails. Can\nthis process be done automatically? In this work, we propose methodology for\nautomated comparison of language models that uses performance-aware contextual\nembeddings to find fine-grained features of text where one LM outperforms\nanother. Our method, which we name BehaviorBox, extracts coherent features that\ndemonstrate differences with respect to the ease of generation between two LMs.\nSpecifically, BehaviorBox finds features that describe groups of words in\nfine-grained contexts, such as \"conditional 'were' in the phrase 'if you were'\"\nand \"exclamation marks after emotional statements\", where one model outperforms\nanother within a particular datatset. We apply BehaviorBox to compare models\nthat vary in size, model family, and post-training, and enumerate insights into\nspecific contexts that illustrate meaningful differences in performance which\ncannot be found by measures such as corpus-level perplexity alone."}
{"id": "2506.02987", "pdf": "https://arxiv.org/pdf/2506.02987.pdf", "abs": "https://arxiv.org/abs/2506.02987", "title": "Performance of leading large language models in May 2025 in Membership of the Royal College of General Practitioners-style examination questions: a cross-sectional analysis", "authors": ["Richard Armitage"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "12 pages, 1 Table", "summary": "Background: Large language models (LLMs) have demonstrated substantial\npotential to support clinical practice. Other than Chat GPT4 and its\npredecessors, few LLMs, especially those of the leading and more powerful\nreasoning model class, have been subjected to medical specialty examination\nquestions, including in the domain of primary care. This paper aimed to test\nthe capabilities of leading LLMs as of May 2025 (o3, Claude Opus 4, Grok3, and\nGemini 2.5 Pro) in primary care education, specifically in answering Member of\nthe Royal College of General Practitioners (MRCGP) style examination questions.\n  Methods: o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro were tasked to answer\n100 randomly chosen multiple choice questions from the Royal College of General\nPractitioners GP SelfTest on 25 May 2025. Questions included textual\ninformation, laboratory results, and clinical images. Each model was prompted\nto answer as a GP in the UK and was provided with full question information.\nEach question was attempted once by each model. Responses were scored against\ncorrect answers provided by GP SelfTest.\n  Results: The total score of o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro was\n99.0%, 95.0%, 95.0%, and 95.0%, respectively. The average peer score for the\nsame questions was 73.0%.\n  Discussion: All models performed remarkably well, and all substantially\nexceeded the average performance of GPs and GP registrars who had answered the\nsame questions. o3 demonstrated the best performance, while the performances of\nthe other leading models were comparable with each other and were not\nsubstantially lower than that of o3. These findings strengthen the case for\nLLMs, particularly reasoning models, to support the delivery of primary care,\nespecially those that have been specifically trained on primary care clinical\ndata."}
{"id": "2506.02212", "pdf": "https://arxiv.org/pdf/2506.02212.pdf", "abs": "https://arxiv.org/abs/2506.02212", "title": "Leveraging Natural Language Processing to Unravel the Mystery of Life: A Review of NLP Approaches in Genomics, Transcriptomics, and Proteomics", "authors": ["Ella Rannon", "David Burstein"], "categories": ["cs.CL", "cs.AI", "q-bio.GN"], "comment": null, "summary": "Natural Language Processing (NLP) has transformed various fields beyond\nlinguistics by applying techniques originally developed for human language to\nthe analysis of biological sequences. This review explores the application of\nNLP methods to biological sequence data, focusing on genomics, transcriptomics,\nand proteomics. We examine how various NLP methods, from classic approaches\nlike word2vec to advanced models employing transformers and hyena operators,\nare being adapted to analyze DNA, RNA, protein sequences, and entire genomes.\nThe review also examines tokenization strategies and model architectures,\nevaluating their strengths, limitations, and suitability for different\nbiological tasks. We further cover recent advances in NLP applications for\nbiological data, such as structure prediction, gene expression, and\nevolutionary analysis, highlighting the potential of these methods for\nextracting meaningful insights from large-scale genomic data. As language\nmodels continue to advance, their integration into bioinformatics holds immense\npromise for advancing our understanding of biological processes in all domains\nof life."}
{"id": "2412.00411", "pdf": "https://arxiv.org/pdf/2412.00411.pdf", "abs": "https://arxiv.org/abs/2412.00411", "title": "Seismocardiography for Emotion Recognition: A Study on EmoWear with Insights from DEAP", "authors": ["Mohammad Hasan Rahmani", "Rafael Berkvens", "Maarten Weyn"], "categories": ["cs.HC"], "comment": "This work has been accepted for publication in IEEE Transactions on\n  Affective Computing. Final version can be found at the provided DOI", "summary": "Emotions have a profound impact on our daily lives, influencing our thoughts,\nbehaviors, and interactions, but also our physiological reactions. Recent\nadvances in wearable technology have facilitated studying emotions through\ncardio-respiratory signals. Accelerometers offer a non-invasive, convenient,\nand cost-effective method for capturing heart- and pulmonary-induced vibrations\non the chest wall, specifically Seismocardiography (SCG) and\nAccelerometry-Derived Respiration (ADR). Their affordability, wide\navailability, and ability to provide rich contextual data make accelerometers\nideal for everyday use. While accelerometers have been used as part of broader\nmodality fusions for Emotion Recognition (ER), their stand-alone potential via\nSCG and ADR remains unexplored. Bridging this gap could significantly help the\nembedding of ER into real-world applications, minimizing the hardware, and\nincreasing contextual integration potentials. To address this gap, we introduce\nSCG and ADR as novel modalities for ER and evaluate their performance using the\nEmoWear dataset. First, we replicate the single-trial emotion classification\npipeline from the DEAP dataset study, achieving similar results. Then we use\nour validated pipeline to train models that predict affective valence-arousal\nstates using SCG and compare them against established cardiac signals,\nElectrocardiography (ECG) and Blood Volume Pulse (BVP). Results show that SCG\nis a viable modality for ER, achieving similar performance to ECG and BVP. By\ncombining ADR with SCG, we achieved a working ER framework that only requires a\nsingle chest-worn accelerometer. These findings pave the way for integrating ER\ninto real-world, enabling seamless affective computing in everyday life."}
{"id": "2506.02239", "pdf": "https://arxiv.org/pdf/2506.02239.pdf", "abs": "https://arxiv.org/abs/2506.02239", "title": "Investigating the Impact of Word Informativeness on Speech Emotion Recognition", "authors": ["Sofoklis Kakouros"], "categories": ["cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "In emotion recognition from speech, a key challenge lies in identifying\nspeech signal segments that carry the most relevant acoustic variations for\ndiscerning specific emotions. Traditional approaches compute functionals for\nfeatures such as energy and F0 over entire sentences or longer speech portions,\npotentially missing essential fine-grained variation in the long-form\nstatistics. This research investigates the use of word informativeness, derived\nfrom a pre-trained language model, to identify semantically important segments.\nAcoustic features are then computed exclusively for these identified segments,\nenhancing emotion recognition accuracy. The methodology utilizes standard\nacoustic prosodic features, their functionals, and self-supervised\nrepresentations. Results indicate a notable improvement in recognition\nperformance when features are computed on segments selected based on word\ninformativeness, underscoring the effectiveness of this approach."}
{"id": "2503.24160", "pdf": "https://arxiv.org/pdf/2503.24160.pdf", "abs": "https://arxiv.org/abs/2503.24160", "title": "A Comparative Study of Scanpath Models in Graph-Based Visualization", "authors": ["Angela Lopez-Cardona", "Parvin Emami", "Sebastian Idesis", "Saravanakumar Duraisamy", "Luis A. Leiva", "Ioannis Arapakis"], "categories": ["cs.HC", "cs.CV"], "comment": null, "summary": "Information Visualization (InfoVis) systems utilize visual representations to\nenhance data interpretation. Understanding how visual attention is allocated is\nessential for optimizing interface design. However, collecting Eye-tracking\n(ET) data presents challenges related to cost, privacy, and scalability.\nComputational models provide alternatives for predicting gaze patterns, thereby\nadvancing InfoVis research. In our study, we conducted an ET experiment with 40\nparticipants who analyzed graphs while responding to questions of varying\ncomplexity within the context of digital forensics. We compared human scanpaths\nwith synthetic ones generated by models such as DeepGaze, UMSS, and Gazeformer.\nOur research evaluates the accuracy of these models and examines how question\ncomplexity and number of nodes influence performance. This work contributes to\nthe development of predictive modeling in visual analytics, offering insights\nthat can enhance the design and effectiveness of InfoVis systems."}
{"id": "2506.02264", "pdf": "https://arxiv.org/pdf/2506.02264.pdf", "abs": "https://arxiv.org/abs/2506.02264", "title": "CoDial: Interpretable Task-Oriented Dialogue Systems Through Dialogue Flow Alignment", "authors": ["Radin Shayanfar", "Chu Fei Luo", "Rohan Bhambhoria", "Samuel Dahan", "Xiaodan Zhu"], "categories": ["cs.CL"], "comment": null, "summary": "It is often challenging to teach specialized, unseen tasks to dialogue\nsystems due to the high cost of expert knowledge, training data, and high\ntechnical difficulty. To support domain-specific applications - such as law,\nmedicine, or finance - it is essential to build frameworks that enable\nnon-technical experts to define, test, and refine system behaviour with minimal\neffort. Achieving this requires cross-disciplinary collaboration between\ndevelopers and domain specialists. In this work, we introduce a novel\nframework, CoDial (Code for Dialogue), that converts expert knowledge,\nrepresented as a novel structured heterogeneous graph, into executable\nconversation logic. CoDial can be easily implemented in existing guardrailing\nlanguages, such as Colang, to enable interpretable, modifiable, and true\nzero-shot specification of task-oriented dialogue systems. Empirically, CoDial\nachieves state-of-the-art performance on the STAR dataset for inference-based\nmodels and is competitive with similar baselines on the well-known MultiWOZ\ndataset. We also demonstrate CoDial's iterative improvement via manual and\nLLM-aided feedback, making it a practical tool for expert-guided alignment of\nLLMs in high-stakes domains."}
{"id": "2504.13880", "pdf": "https://arxiv.org/pdf/2504.13880.pdf", "abs": "https://arxiv.org/abs/2504.13880", "title": "An AI-powered Public Health Automated Kiosk System for Personalized Care: An Experimental Pilot Study", "authors": ["Sonya Falahati", "Morteza Alizadeh", "Fatemeh Ghazipour", "Zhino Safahi", "Navid Khaledian", "Mohammad R. Salmanpour"], "categories": ["cs.HC", "F.2.2; I.2.7"], "comment": "16 pages, 3 figures, 2 tables", "summary": "Background: The HERMES Kiosk (Healthcare Enhanced Recommendations through\nArtificial Intelligence & Expertise System) is designed to provide personalized\nOver-the-Counter (OTC) medication recommendations, addressing the limitations\nof traditional health kiosks. It integrates an advanced GAMENet model enhanced\nwith Graph Attention Networks (GAT) and Multi-Head Cross-Attention (MHCA) while\nensuring user privacy through federated learning. This paper outlines the\nconceptual design and architecture of HERMES, with a focus on deployment in\nhigh-traffic public areas. Methods: HERMES analyzes self-reported symptoms and\nanonymized medical histories using AI algorithms to generate context-aware OTC\nmedication recommendations. The system was initially trained using Electronic\nHealth Records (EHR) from the MIMIC-III dataset (6,350 patients) and Drug-Drug\nInteraction (DDI) data from the TWOSIDES database, incorporating the top 90\nseverity DDI types. Real-time DDI checks and ATC-mapped drug codes further\nimprove safety. The kiosk is designed for accessibility, offering multilingual\nsupport, large fonts, voice commands, and Braille compatibility. A built-in\nhealth education library promotes preventive care and health literacy. A survey\nwas conducted among 10 medical professionals to evaluate its potential\napplications in medicine. Results: Preliminary results show that the enhanced\nGAMENet model achieved a Precision-Recall AUC (PRAUC) of 0.74, outperforming\nthe original model. These findings suggest a strong potential for delivering\naccurate and secure healthcare recommendations in public settings. Conclusion:\nHERMES demonstrates how AI-driven, privacy-preserving kiosks can enhance public\nhealth access, empower users, and alleviate burdens on healthcare systems.\nFuture work will focus on real-world deployment, usability testing, and\nscalability for broader adoption."}
{"id": "2506.02279", "pdf": "https://arxiv.org/pdf/2506.02279.pdf", "abs": "https://arxiv.org/abs/2506.02279", "title": "ImpRAG: Retrieval-Augmented Generation with Implicit Queries", "authors": ["Wenzheng Zhang", "Xi Victoria Lin", "Karl Stratos", "Wen-tau Yih", "Mingda Chen"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems traditionally treat retrieval\nand generation as separate processes, requiring explicit textual queries to\nconnect them. This separation can limit the ability of models to generalize\nacross diverse tasks. In this work, we propose a query-free RAG system, named\nImpRAG, which integrates retrieval and generation into a unified model. ImpRAG\nallows models to implicitly express their information needs, eliminating the\nneed for human-specified queries. By dividing pretrained decoder-only language\nmodels into specialized layer groups, ImpRAG optimizes retrieval and generation\ntasks simultaneously. Our approach employs a two-stage inference process, using\nthe same model parameters and forward pass for both retrieval and generation,\nthereby minimizing the disparity between retrievers and language models.\nExperiments on 8 knowledge-intensive tasks demonstrate that ImpRAG achieves\n3.6-11.5 improvements in exact match scores on unseen tasks with diverse\nformats, highlighting its effectiveness in enabling models to articulate their\nown information needs and generalize across tasks. Our analysis underscores the\nimportance of balancing retrieval and generation parameters and leveraging\ngeneration perplexities as retrieval training objectives for enhanced\nperformance."}
{"id": "2504.20369", "pdf": "https://arxiv.org/pdf/2504.20369.pdf", "abs": "https://arxiv.org/abs/2504.20369", "title": "Perception-aware Sampling for Scatterplot Visualizations", "authors": ["Zafeiria Moumoulidou", "Hamza Elhamdadi", "Ke Yang", "Subrata Mitra", "Cindy Xiong Bearfield", "Alexandra Meliou"], "categories": ["cs.HC", "cs.DB"], "comment": null, "summary": "Visualizing data is often a crucial first step in data analytics workflows,\nbut growing data sizes pose challenges due to computational and visual\nperception limitations. As a result, data analysts commonly down-sample their\ndata and work with subsets. Deriving representative samples, however, remains a\nchallenge. This paper focuses on scatterplots, a widely-used visualization\ntype, and introduces a novel sampling objective -- perception-awareness --\naiming to improve sample efficacy by targeting humans' perception of a\nvisualization.\n  We make the following contributions: (1) We propose perception-augmented\ndatabases and design PAwS: a novel perception-aware sampling method for\nscatterplots that leverages saliency maps -- a computer vision tool for\npredicting areas of attention focus in visualizations -- and models\nperception-awareness via saliency, density, and coverage objectives. (2) We\ndesign ApproPAwS: a fast, perception-aware method for approximate\nvisualizations, which exploits the fact that small visual perturbations are\noften imperceptible to humans. (3) We introduce the concept of perceptual\nsimilarity as a metric for sample quality, and present a novel method that\ncompares saliency maps to measure it. (4) Our extensive experimental evaluation\nshows that our methods consistently outperform prior art in producing samples\nwith high perceptual similarity, while ApproPAwS achieves up to 100x speed-ups\nwith minimal loss in visual fidelity. Our user study shows that PAwS is often\npreferred by humans, validating our quantitative findings."}
{"id": "2506.02283", "pdf": "https://arxiv.org/pdf/2506.02283.pdf", "abs": "https://arxiv.org/abs/2506.02283", "title": "Sounding Like a Winner? Prosodic Differences in Post-Match Interviews", "authors": ["Sofoklis Kakouros", "Haoyu Chen"], "categories": ["cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "This study examines the prosodic characteristics associated with winning and\nlosing in post-match tennis interviews. Additionally, this research explores\nthe potential to classify match outcomes solely based on post-match interview\nrecordings using prosodic features and self-supervised learning (SSL)\nrepresentations. By analyzing prosodic elements such as pitch and intensity,\nalongside SSL models like Wav2Vec 2.0 and HuBERT, the aim is to determine\nwhether an athlete has won or lost their match. Traditional acoustic features\nand deep speech representations are extracted from the data, and machine\nlearning classifiers are employed to distinguish between winning and losing\nplayers. Results indicate that SSL representations effectively differentiate\nbetween winning and losing outcomes, capturing subtle speech patterns linked to\nemotional states. At the same time, prosodic cues -- such as pitch variability\n-- remain strong indicators of victory."}
{"id": "2505.03117", "pdf": "https://arxiv.org/pdf/2505.03117.pdf", "abs": "https://arxiv.org/abs/2505.03117", "title": "Do ATCOs Need Explanations, and Why? Towards ATCO-Centered Explainable AI for Conflict Resolution Advisories", "authors": ["Katherine Fennedy", "Brian Hilburn", "Thaivalappil N. M. Nadirsha", "Sameer Alam", "Khanh-Duy Le", "Hua Li"], "categories": ["cs.HC"], "comment": "2025 ATRD US-Europe Air Transportation Research & Development\n  Symposium", "summary": "Interest in explainable artificial intelligence (XAI) is surging. Prior\nresearch has primarily focused on systems' ability to generate explanations,\noften guided by researchers' intuitions rather than end-users' needs.\nUnfortunately, such approaches have not yielded favorable outcomes when\ncompared to a black-box baseline (i.e., no explanation). To address this gap,\nthis paper advocates a human-centered approach that shifts focus to air traffic\ncontrollers (ATCOs) by asking a fundamental yet overlooked question: Do ATCOs\nneed explanations, and if so, why? Insights from air traffic management (ATM),\nhuman-computer interaction, and the social sciences were synthesized to provide\na holistic understanding of XAI challenges and opportunities in ATM. Evaluating\n11 ATM operational goals revealed a clear need for explanations when ATCOs aim\nto document decisions and rationales for future reference or report generation.\nConversely, ATCOs are less likely to seek them when their conflict resolution\napproach align with the artificial intelligence (AI) advisory. While this is a\npreliminary study, the findings are expected to inspire broader and deeper\ninquiries into the design of ATCO-centric XAI systems, paving the way for more\neffective human-AI interaction in ATM."}
{"id": "2506.02298", "pdf": "https://arxiv.org/pdf/2506.02298.pdf", "abs": "https://arxiv.org/abs/2506.02298", "title": "LAM SIMULATOR: Advancing Data Generation for Large Action Model Training via Online Exploration and Trajectory Feedback", "authors": ["Thai Hoang", "Kung-Hsiang Huang", "Shirley Kokane", "Jianguo Zhang", "Zuxin Liu", "Ming Zhu", "Jake Grigsby", "Tian Lan", "Michael S Ryoo", "Chien-Sheng Wu", "Shelby Heinecke", "Huan Wang", "Silvio Savarese", "Caiming Xiong", "Juan Carlos Niebles"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "LAM Simulator framework for agentic data generation", "summary": "Large Action Models (LAMs) for AI Agents offer incredible potential but face\nchallenges due to the need for high-quality training data, especially for\nmulti-steps tasks that involve planning, executing tool calls, and responding\nto feedback. To address these issues, we present LAM SIMULATOR, a comprehensive\nframework designed for online exploration of agentic tasks with high-quality\nfeedback. Our framework features a dynamic task query generator, an extensive\ncollection of tools, and an interactive environment where Large Language Model\n(LLM) Agents can call tools and receive real-time feedback. This setup enables\nLLM Agents to explore and solve tasks autonomously, facilitating the discovery\nof multiple approaches to tackle any given task. The resulting action\ntrajectory data are then used to create high-quality training datasets for\nLAMs. Our experiments on popular agentic benchmarks, ToolBench and CRMArena,\nhighlight the effectiveness of LAM SIMULATOR: models trained with\nself-generated datasets using our framework achieve significant performance\ngains, up to a 49.3\\% improvement over their original baselines. LAM SIMULATOR\nrequires minimal human input during dataset creation, highlighting LAM\nSIMULATOR's efficiency and effectiveness in speeding up development of AI\nagents."}
{"id": "2505.07592", "pdf": "https://arxiv.org/pdf/2505.07592.pdf", "abs": "https://arxiv.org/abs/2505.07592", "title": "Neural Signatures Within and Between Chess Puzzle Solving and Standard Cognitive Tasks for Brain-Computer Interfaces: A Low-Cost Electroencephalography Study", "authors": ["Matthew Russell", "Samuel Youkeles", "William Xia", "Kenny Zheng", "Aman Shah", "Robert J. K. Jacob"], "categories": ["cs.HC"], "comment": "29 pages, 13 figures", "summary": "Consumer-grade electroencephalography (EEG) devices show promise for\nBrain-Computer Interface (BCI) applications, but their efficacy in detecting\nsubtle cognitive states remains understudied. We developed a comprehensive\nstudy paradigm which incorporates a combination of established cognitive tasks\n(N-Back, Stroop, and Mental Rotation) and adds a novel ecological Chess puzzles\ntask. We tested our paradigm with the MUSE 2, a low-cost consumer-grade EEG\ndevice. Using linear mixed-effects modeling we demonstrate successful\ndistinctions of within-task workload levels and cross-task cognitive states\nbased on the spectral power data derived from the MUSE 2 device. With machine\nlearning we further show reliable predictive power to differentiate between\nworkload levels in the N-Back task, and also achieve effective cross-task\nclassification. These findings demonstrate that consumer-grade EEG devices like\nthe MUSE 2 can be used to effectively differentiate between various levels of\ncognitive workload as well as among more nuanced task-based cognitive states,\nand that these tools can be leveraged for real-time adaptive BCI applications\nin practical settings."}
{"id": "2506.02302", "pdf": "https://arxiv.org/pdf/2506.02302.pdf", "abs": "https://arxiv.org/abs/2506.02302", "title": "Explain-then-Process: Using Grammar Prompting to Enhance Grammatical Acceptability Judgments", "authors": ["Russell Scheinberg", "Ameeta Agrawal", "Amber Shore", "So Young Lee"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ACL 2025 Findings", "summary": "Large language models (LLMs) can explain grammatical rules, yet they often\nfail to apply those rules when judging sentence acceptability. We present\n\"grammar prompting\", an explain-then-process paradigm: a large LLM first\nproduces a concise explanation of the relevant syntactic phenomenon, then that\nexplanation is fed back as additional context to the target model -- either an\nLLM or a smaller language model (SLM) -- before deciding which sentence of a\nminimal pair is grammatical. On the English BLiMP, Chinese SLING, and Russian\nRuBLiMP benchmarks, this simple prompt design yields substantial improvements\nover strong baselines across many syntactic phenomena. Feeding an LLM's\nmetalinguistic explanation back to the target model bridges the gap between\nknowing a rule and using it. On SLMs, grammar prompting alone trims the average\nLLM-SLM accuracy gap by about 20%, and when paired with chain-of-thought, by\n56% (13.0 pp -> 5.8 pp), all at negligible cost. The lightweight,\nlanguage-agnostic cue lets low-cost SLMs approach frontier-LLM performance in\nmultilingual settings."}
{"id": "2502.16810", "pdf": "https://arxiv.org/pdf/2502.16810.pdf", "abs": "https://arxiv.org/abs/2502.16810", "title": "Grounded Persuasive Language Generation for Automated Marketing", "authors": ["Jibang Wu", "Chenghao Yang", "Simon Mahns", "Chaoqi Wang", "Hao Zhu", "Fei Fang", "Haifeng Xu"], "categories": ["cs.AI", "cs.CL", "cs.HC", "econ.GN", "q-fin.EC"], "comment": null, "summary": "This paper develops an agentic framework that employs large language models\n(LLMs) to automate the generation of persuasive and grounded marketing content,\nusing real estate listing descriptions as our focal application domain. Our\nmethod is designed to align the generated content with user preferences while\nhighlighting useful factual attributes. This agent consists of three key\nmodules: (1) Grounding Module, mimicking expert human behavior to predict\nmarketable features; (2) Personalization Module, aligning content with user\npreferences; (3) Marketing Module, ensuring factual accuracy and the inclusion\nof localized features. We conduct systematic human-subject experiments in the\ndomain of real estate marketing, with a focus group of potential house buyers.\nThe results demonstrate that marketing descriptions generated by our approach\nare preferred over those written by human experts by a clear margin while\nmaintaining the same level of factual accuracy. Our findings suggest a\npromising agentic approach to automate large-scale targeted marketing while\nensuring factuality of content generation."}
{"id": "2506.02321", "pdf": "https://arxiv.org/pdf/2506.02321.pdf", "abs": "https://arxiv.org/abs/2506.02321", "title": "Quantifying Misattribution Unfairness in Authorship Attribution", "authors": ["Pegah Alipoormolabashi", "Ajay Patel", "Niranjan Balasubramanian"], "categories": ["cs.CL"], "comment": null, "summary": "Authorship misattribution can have profound consequences in real life. In\nforensic settings simply being considered as one of the potential authors of an\nevidential piece of text or communication can result in undesirable scrutiny.\nThis raises a fairness question: Is every author in the candidate pool at equal\nrisk of misattribution? Standard evaluation measures for authorship attribution\nsystems do not explicitly account for this notion of fairness. We introduce a\nsimple measure, Misattribution Unfairness Index (MAUIk), which is based on how\noften authors are ranked in the top k for texts they did not write. Using this\nmeasure we quantify the unfairness of five models on two different datasets.\nAll models exhibit high levels of unfairness with increased risks for some\nauthors. Furthermore, we find that this unfairness relates to how the models\nembed the authors as vectors in the latent search space. In particular, we\nobserve that the risk of misattribution is higher for authors closer to the\ncentroid (or center) of the embedded authors in the haystack. These results\nindicate the potential for harm and the need for communicating with and\ncalibrating end users on misattribution risk when building and providing such\nmodels for downstream use."}
{"id": "2505.10742", "pdf": "https://arxiv.org/pdf/2505.10742.pdf", "abs": "https://arxiv.org/abs/2505.10742", "title": "Evaluations at Work: Measuring the Capabilities of GenAI in Use", "authors": ["Brandon Lepine", "Gawesha Weerantunga", "Juho Kim", "Pamela Mishkin", "Matthew Beane"], "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "Current AI benchmarks miss the messy, multi-turn nature of human-AI\ncollaboration. We present an evaluation framework that decomposes real-world\ntasks into interdependent subtasks, letting us track both LLM performance and\nusers' strategies across a dialogue. Complementing this framework, we develop a\nsuite of metrics, including a composite usage derived from semantic similarity,\nword overlap, and numerical matches; structural coherence; intra-turn\ndiversity; and a novel measure of the \"information frontier\" reflecting the\nalignment between AI outputs and users' working knowledge. We demonstrate our\nmethodology in a financial valuation task that mirrors real-world complexity.\nOur empirical findings reveal that while greater integration of LLM-generated\ncontent generally enhances output quality, its benefits are moderated by\nfactors such as response incoherence, excessive subtask diversity, and the\ndistance of provided information from users' existing knowledge. These results\nsuggest that proactive dialogue strategies designed to inject novelty may\ninadvertently undermine task performance. Our work thus advances a more\nholistic evaluation of human-AI collaboration, offering both a robust\nmethodological framework and actionable insights for developing more effective\nAI-augmented work processes."}
{"id": "2506.02326", "pdf": "https://arxiv.org/pdf/2506.02326.pdf", "abs": "https://arxiv.org/abs/2506.02326", "title": "Something Just Like TRuST : Toxicity Recognition of Span and Target", "authors": ["Berk Atil", "Namrata Sureddy", "Rebecca J. Passonneau"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Toxicity in online content, including content generated by language models,\nhas become a critical concern due to its potential for negative psychological\nand social impact. This paper introduces TRuST, a comprehensive dataset\ndesigned to improve toxicity detection that merges existing datasets, and has\nlabels for toxicity, target social group, and toxic spans. It includes a\ndiverse range of target groups such as ethnicity, gender, religion, disability,\nand politics, with both human/machine-annotated and human machine-generated\ndata. We benchmark state-of-the-art large language models (LLMs) on toxicity\ndetection, target group identification, and toxic span extraction. We find that\nfine-tuned models consistently outperform zero-shot and few-shot prompting,\nthough performance remains low for certain social groups. Further, reasoning\ncapabilities do not significantly improve performance, indicating that LLMs\nhave weak social reasoning skills."}
{"id": "2506.02338", "pdf": "https://arxiv.org/pdf/2506.02338.pdf", "abs": "https://arxiv.org/abs/2506.02338", "title": "One Missing Piece for Open-Source Reasoning Models: A Dataset to Mitigate Cold-Starting Short CoT LLMs in RL", "authors": ["Hyungjoo Chae", "Dongjin Kang", "Jihyuk Kim", "Beong-woo Kwak", "Sunghyun Park", "Haeju Park", "Jinyoung Yeo", "Moontae Lee", "Kyungjae Lee"], "categories": ["cs.CL"], "comment": "ACL 2025 Industry", "summary": "With the release of R1, a publicly available large reasoning model (LRM),\nresearchers commonly train new LRMs by training language models on R1's long\nchain-of-thought (CoT) inferences. While prior works show that LRMs'\ncapabilities can be reproduced through direct distillation, the continued\nreliance on the existing models (e.g., R1) remains a critical limitation in\nadvancing the field. As a first step toward independent LRM development, this\npaper explores the possibility of constructing a long CoT dataset with LLMs\nthat are not trained for inference-time scaling. To this end, we present the\nLong CoT Collection, a dataset of 100K CoT rationales annotated using existing\nshort CoT LLMs. We develop a pipeline that induces o1's novel reasoning\nstrategies into short CoT LLMs, enabling them to think longer and introducing\ncontrollability over the thought budget to better manage the overthinking\nproblem. Our extensive analyses validate that our dataset achieves quality\ncomparable to--or slightly below--R1. Furthermore, our experiments demonstrate\nthat training on our dataset not only strengthens general reasoning skills, but\nalso provides a strong foundation for reinforcement learning--models\ninitialized on our data achieve 2-3x larger gains with RLVR."}
{"id": "2506.02347", "pdf": "https://arxiv.org/pdf/2506.02347.pdf", "abs": "https://arxiv.org/abs/2506.02347", "title": "STORYTELLER: An Enhanced Plot-Planning Framework for Coherent and Cohesive Story Generation", "authors": ["Jiaming Li", "Yukun Chen", "Ziqiang Liu", "Minghuan Tan", "Lei Zhang", "Yunshui Li", "Run Luo", "Longze Chen", "Jing Luo", "Ahmadreza Argha", "Hamid Alinejad-Rokny", "Wei Zhou", "Min Yang"], "categories": ["cs.CL"], "comment": null, "summary": "Stories are central to human culture, serving to share ideas, preserve\ntraditions, and foster connections. Automatic story generation, a key\nadvancement in artificial intelligence (AI), offers new possibilities for\ncreating personalized content, exploring creative ideas, and enhancing\ninteractive experiences. However, existing methods struggle to maintain\nnarrative coherence and logical consistency. This disconnect compromises the\noverall storytelling experience, underscoring the need for substantial\nimprovements. Inspired by human cognitive processes, we introduce Storyteller,\na novel approach that systemically improves the coherence and consistency of\nautomatically generated stories. Storyteller introduces a plot node structure\nbased on linguistically grounded subject verb object (SVO) triplets, which\ncapture essential story events and ensure a consistent logical flow. Unlike\nprevious methods, Storyteller integrates two dynamic modules, the STORYLINE and\nnarrative entity knowledge graph (NEKG),that continuously interact with the\nstory generation process. This integration produces structurally sound,\ncohesive and immersive narratives. Extensive experiments demonstrate that\nStoryteller significantly outperforms existing approaches, achieving an 84.33%\naverage win rate through human preference evaluation. At the same time, it is\nalso far ahead in other aspects including creativity, coherence, engagement,\nand relevance."}
{"id": "2506.02350", "pdf": "https://arxiv.org/pdf/2506.02350.pdf", "abs": "https://arxiv.org/abs/2506.02350", "title": "Truth over Tricks: Measuring and Mitigating Shortcut Learning in Misinformation Detection", "authors": ["Herun Wan", "Jiaying Wu", "Minnan Luo", "Zhi Zeng", "Zhixiong Su"], "categories": ["cs.CL"], "comment": null, "summary": "Misinformation detection models often rely on superficial cues (i.e.,\n\\emph{shortcuts}) that correlate with misinformation in training data but fail\nto generalize to the diverse and evolving nature of real-world misinformation.\nThis issue is exacerbated by large language models (LLMs), which can easily\ngenerate convincing misinformation through simple prompts. We introduce\nTruthOverTricks, a unified evaluation paradigm for measuring shortcut learning\nin misinformation detection. TruthOverTricks categorizes shortcut behaviors\ninto intrinsic shortcut induction and extrinsic shortcut injection, and\nevaluates seven representative detectors across 14 popular benchmarks, along\nwith two new factual misinformation datasets, NQ-Misinfo and Streaming-Misinfo.\nEmpirical results reveal that existing detectors suffer severe performance\ndegradation when exposed to both naturally occurring and adversarially crafted\nshortcuts. To address this, we propose SMF, an LLM-augmented data augmentation\nframework that mitigates shortcut reliance through paraphrasing, factual\nsummarization, and sentiment normalization. SMF consistently enhances\nrobustness across 16 benchmarks, encouraging models to rely on deeper semantic\nunderstanding rather than shortcut cues. To promote the development of\nmisinformation detectors, we have published the resources publicly at\nhttps://github.com/whr000001/TruthOverTricks."}
{"id": "2506.02351", "pdf": "https://arxiv.org/pdf/2506.02351.pdf", "abs": "https://arxiv.org/abs/2506.02351", "title": "DIAMOND: An LLM-Driven Agent for Context-Aware Baseball Highlight Summarization", "authors": ["Jeonghun Kang", "Soonmok Kwon", "Joonseok Lee", "Byung-Hak Kim"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "To appear in the First REALM (Research on Agent Language Models)\n  workshop at ACL 2025", "summary": "Traditional approaches -- such as Win Probability Added (WPA)-based ranking\nor computer vision-driven event detection -- can identify scoring plays but\noften miss strategic depth, momentum shifts, and storyline progression. Manual\ncuration remains the gold standard but is resource-intensive and not scalable.\nWe introduce DIAMOND, an LLM-driven agent for context-aware baseball highlight\nsummarization that integrates structured sports analytics with natural language\nreasoning. DIAMOND leverages sabermetric features -- Win Expectancy, WPA, and\nLeverage Index -- to quantify play importance, while an LLM module enhances\nselection based on contextual narrative value. This hybrid approach ensures\nboth quantitative rigor and qualitative richness, surpassing the limitations of\npurely statistical or vision-based systems. Evaluated on five diverse Korean\nBaseball Organization League games, DIAMOND improves F1-score from 42.9%\n(WPA-only) to 84.8%, outperforming both commercial and statistical baselines.\nThough limited in scale, our results highlight the potential of modular,\ninterpretable agent-based frameworks for event-level summarization in sports\nand beyond."}
{"id": "2506.02372", "pdf": "https://arxiv.org/pdf/2506.02372.pdf", "abs": "https://arxiv.org/abs/2506.02372", "title": "AnswerCarefully: A Dataset for Improving the Safety of Japanese LLM Output", "authors": ["Hisami Suzuki", "Satoru Katsumata", "Takashi Kodama", "Tetsuro Takahashi", "Kouta Nakayama", "Satoshi Sekine"], "categories": ["cs.CL"], "comment": null, "summary": "In this paper we present AnswerCarefully, a dataset for promoting the safety\nand appropriateness of Japanese LLM outputs. The dataset consists of 1,800\npairs of questions and reference answers, where the questions require special\nattention in answering. It covers a wide range of risk categories established\nin prior English-language datasets, but the data samples are original in that\nthey are manually created to reflect the socio-cultural context of LLM usage in\nJapan. We show that using this dataset for instruction to fine-tune a Japanese\nLLM led to improved output safety without compromising the utility of general\nresponses. We also report the results of a safety evaluation of 12 Japanese\nLLMs using this dataset as a benchmark. Finally, we describe the latest update\non the dataset which provides English translations and annotations of the\nquestions, aimed at facilitating the derivation of similar datasets in\ndifferent languages and regions."}
{"id": "2506.02378", "pdf": "https://arxiv.org/pdf/2506.02378.pdf", "abs": "https://arxiv.org/abs/2506.02378", "title": "Exploring Explanations Improves the Robustness of In-Context Learning", "authors": ["Ukyo Honda", "Tatsushi Oka"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ACL 2025 (Main Conference)", "summary": "In-context learning (ICL) has emerged as a successful paradigm for leveraging\nlarge language models (LLMs). However, it often struggles to generalize beyond\nthe distribution of the provided demonstrations. A recent advancement in\nenhancing robustness is ICL with explanations (X-ICL), which improves\nprediction reliability by guiding LLMs to understand and articulate the\nreasoning behind correct labels. Building on this approach, we introduce an\nadvanced framework that extends X-ICL by systematically exploring explanations\nfor all possible labels (X$^2$-ICL), thereby enabling more comprehensive and\nrobust decision-making. Experimental results on multiple natural language\nunderstanding datasets validate the effectiveness of X$^2$-ICL, demonstrating\nsignificantly improved robustness to out-of-distribution data compared to the\nexisting ICL approaches."}
{"id": "2506.02391", "pdf": "https://arxiv.org/pdf/2506.02391.pdf", "abs": "https://arxiv.org/abs/2506.02391", "title": "Consultant Decoding: Yet Another Synergistic Mechanism", "authors": ["Chuanghao Ding", "Jiaping Wang", "Ziqing Yang", "Xiaoliang Wang", "Dahua Lin", "Cam-Tu Nguyen", "Fei Tan"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 findings", "summary": "The synergistic mechanism based on Speculative Decoding (SD) has garnered\nconsiderable attention as a simple yet effective approach for accelerating the\ninference of large language models (LLMs). Nonetheless, the high rejection\nrates require repeated LLMs calls to validate draft tokens, undermining the\noverall efficiency gain of SD. In this work, we revisit existing verification\nmechanisms and propose a novel synergetic mechanism Consultant Decoding (CD).\nUnlike SD, which relies on a metric derived from importance sampling for\nverification, CD verifies candidate drafts using token-level likelihoods\ncomputed solely by the LLM. CD achieves up to a 2.5-fold increase in inference\nspeed compared to the target model, while maintaining comparable generation\nquality (around 100% of the target model's performance). Interestingly, this is\nachieved by combining models whose parameter sizes differ by two orders of\nmagnitude. In addition, CD reduces the call frequency of the large target model\nto below 10%, particularly in more demanding tasks. CD's performance was even\nfound to surpass that of the large target model, which theoretically represents\nthe upper bound for speculative decoding."}
{"id": "2506.02404", "pdf": "https://arxiv.org/pdf/2506.02404.pdf", "abs": "https://arxiv.org/abs/2506.02404", "title": "GraphRAG-Bench: Challenging Domain-Specific Reasoning for Evaluating Graph Retrieval-Augmented Generation", "authors": ["Yilin Xiao", "Junnan Dong", "Chuang Zhou", "Su Dong", "Qianwen Zhang", "Di Yin", "Xing Sun", "Xiao Huang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Graph Retrieval Augmented Generation (GraphRAG) has garnered increasing\nrecognition for its potential to enhance large language models (LLMs) by\nstructurally organizing domain-specific corpora and facilitating complex\nreasoning. However, current evaluations of GraphRAG models predominantly rely\non traditional question-answering datasets. Their limited scope in questions\nand evaluation metrics fails to comprehensively assess the reasoning capacity\nimprovements enabled by GraphRAG models. To address this gap, we introduce\nGraphRAG-Bench, a large-scale, domain-specific benchmark designed to rigorously\nevaluate GraphRAG models. Our benchmark offers three key superiorities: \\((i)\\)\nChallenging question design. Featuring college-level, domain-specific questions\nthat demand multi-hop reasoning, the benchmark ensures that simple content\nretrieval is insufficient for problem-solving. For example, some questions\nrequire mathematical reasoning or programming. \\((ii)\\) Diverse task coverage.\nThe dataset includes a broad spectrum of reasoning tasks, multiple-choice,\ntrue/false, multi-select, open-ended, and fill-in-the-blank. It spans 16\ndisciplines in twenty core textbooks. \\((iii)\\) Holistic evaluation framework.\nGraphRAG-Bench provides comprehensive assessment across the entire GraphRAG\npipeline, including graph construction, knowledge retrieval, and answer\ngeneration. Beyond final-answer correctness, it evaluates the logical coherence\nof the reasoning process. By applying nine contemporary GraphRAG methods to\nGraphRAG-Bench, we demonstrate its utility in quantifying how graph-based\nstructuring improves model reasoning capabilities. Our analysis reveals\ncritical insights about graph architectures, retrieval efficacy, and reasoning\ncapabilities, offering actionable guidance for the research community."}
{"id": "2506.02412", "pdf": "https://arxiv.org/pdf/2506.02412.pdf", "abs": "https://arxiv.org/abs/2506.02412", "title": "SingaKids: A Multilingual Multimodal Dialogic Tutor for Language Learning", "authors": ["Zhengyuan Liu", "Geyu Lin", "Hui Li Tan", "Huayun Zhang", "Yanfeng Lu", "Xiaoxue Gao", "Stella Xin Yin", "He Sun", "Hock Huan Goh", "Lung Hsiang Wong", "Nancy F. Chen"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Industry Track", "summary": "The integration of generative artificial intelligence into educational\napplications has enhanced personalized and interactive learning experiences,\nand it shows strong potential to promote young learners language acquisition.\nHowever, it is still challenging to ensure consistent and robust performance\nacross different languages and cultural contexts, and kids-friendly design\nrequires simplified instructions, engaging interactions, and age-appropriate\nscaffolding to maintain motivation and optimize learning outcomes. In this\nwork, we introduce SingaKids, a dialogic tutor designed to facilitate language\nlearning through picture description tasks. Our system integrates dense image\ncaptioning, multilingual dialogic interaction, speech understanding, and\nengaging speech generation to create an immersive learning environment in four\nlanguages: English, Mandarin, Malay, and Tamil. We further improve the system\nthrough multilingual pre-training, task-specific tuning, and scaffolding\noptimization. Empirical studies with elementary school students demonstrate\nthat SingaKids provides effective dialogic teaching, benefiting learners at\ndifferent performance levels."}
{"id": "2506.02425", "pdf": "https://arxiv.org/pdf/2506.02425.pdf", "abs": "https://arxiv.org/abs/2506.02425", "title": "Gender Inequality in English Textbooks Around the World: an NLP Approach", "authors": ["Tairan Liu"], "categories": ["cs.CL", "stat.AP"], "comment": null, "summary": "Textbooks play a critical role in shaping children's understanding of the\nworld. While previous studies have identified gender inequality in individual\ncountries' textbooks, few have examined the issue cross-culturally. This study\napplies natural language processing methods to quantify gender inequality in\nEnglish textbooks from 22 countries across 7 cultural spheres. Metrics include\ncharacter count, firstness (which gender is mentioned first), and TF-IDF word\nassociations by gender. The analysis also identifies gender patterns in proper\nnames appearing in TF-IDF word lists, tests whether large language models can\ndistinguish between gendered word lists, and uses GloVe embeddings to examine\nhow closely keywords associate with each gender. Results show consistent\noverrepresentation of male characters in terms of count, firstness, and named\nentities. All regions exhibit gender inequality, with the Latin cultural sphere\nshowing the least disparity."}
{"id": "2506.02426", "pdf": "https://arxiv.org/pdf/2506.02426.pdf", "abs": "https://arxiv.org/abs/2506.02426", "title": "Comparative Analysis of AI Agent Architectures for Entity Relationship Classification", "authors": ["Maryam Berijanian", "Kuldeep Singh", "Amin Sehati"], "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.1"], "comment": null, "summary": "Entity relationship classification remains a challenging task in information\nextraction, especially in scenarios with limited labeled data and complex\nrelational structures. In this study, we conduct a comparative analysis of\nthree distinct AI agent architectures designed to perform relation\nclassification using large language models (LLMs). The agentic architectures\nexplored include (1) reflective self-evaluation, (2) hierarchical task\ndecomposition, and (3) a novel multi-agent dynamic example generation\nmechanism, each leveraging different modes of reasoning and prompt adaptation.\nIn particular, our dynamic example generation approach introduces real-time\ncooperative and adversarial prompting. We systematically compare their\nperformance across multiple domains and model backends. Our experiments\ndemonstrate that multi-agent coordination consistently outperforms standard\nfew-shot prompting and approaches the performance of fine-tuned models. These\nfindings offer practical guidance for the design of modular, generalizable\nLLM-based systems for structured relation extraction. The source codes and\ndataset are available at\n\\href{https://github.com/maryambrj/ALIEN.git}{https://github.com/maryambrj/ALIEN.git}."}
{"id": "2506.02431", "pdf": "https://arxiv.org/pdf/2506.02431.pdf", "abs": "https://arxiv.org/abs/2506.02431", "title": "From Anger to Joy: How Nationality Personas Shape Emotion Attribution in Large Language Models", "authors": ["Mahammed Kamruzzaman", "Abdullah Al Monsur", "Gene Louis Kim", "Anshuman Chhabra"], "categories": ["cs.CL"], "comment": null, "summary": "Emotions are a fundamental facet of human experience, varying across\nindividuals, cultural contexts, and nationalities. Given the recent success of\nLarge Language Models (LLMs) as role-playing agents, we examine whether LLMs\nexhibit emotional stereotypes when assigned nationality-specific personas.\nSpecifically, we investigate how different countries are represented in\npre-trained LLMs through emotion attributions and whether these attributions\nalign with cultural norms. Our analysis reveals significant nationality-based\ndifferences, with emotions such as shame, fear, and joy being\ndisproportionately assigned across regions. Furthermore, we observe notable\nmisalignment between LLM-generated and human emotional responses, particularly\nfor negative emotions, highlighting the presence of reductive and potentially\nbiased stereotypes in LLM outputs."}
{"id": "2506.02442", "pdf": "https://arxiv.org/pdf/2506.02442.pdf", "abs": "https://arxiv.org/abs/2506.02442", "title": "Should LLM Safety Be More Than Refusing Harmful Instructions?", "authors": ["Utsav Maskey", "Mark Dras", "Usman Naseem"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "This paper presents a systematic evaluation of Large Language Models' (LLMs)\nbehavior on long-tail distributed (encrypted) texts and their safety\nimplications. We introduce a two-dimensional framework for assessing LLM\nsafety: (1) instruction refusal-the ability to reject harmful obfuscated\ninstructions, and (2) generation safety-the suppression of generating harmful\nresponses. Through comprehensive experiments, we demonstrate that models that\npossess capabilities to decrypt ciphers may be susceptible to\nmismatched-generalization attacks: their safety mechanisms fail on at least one\nsafety dimension, leading to unsafe responses or over-refusal. Based on these\nfindings, we evaluate a number of pre-LLM and post-LLM safeguards and discuss\ntheir strengths and limitations. This work contributes to understanding the\nsafety of LLM in long-tail text scenarios and provides directions for\ndeveloping robust safety mechanisms."}
{"id": "2506.02449", "pdf": "https://arxiv.org/pdf/2506.02449.pdf", "abs": "https://arxiv.org/abs/2506.02449", "title": "IP-Dialog: Evaluating Implicit Personalization in Dialogue Systems with Synthetic Data", "authors": ["Bo Peng", "Zhiheng Wang", "Heyang Gong", "Chaochao Lu"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "In modern dialogue systems, the ability to implicitly infer user backgrounds\nfrom conversations and leverage this information for personalized assistance is\ncrucial. However, the scarcity of high-quality data remains a fundamental\nchallenge to evaluating and improving this capability. Traditional dataset\nconstruction methods are labor-intensive, resource-demanding, and raise privacy\nconcerns. To address these issues, we propose a novel approach for automatic\nsynthetic data generation and introduce the Implicit Personalized Dialogue\n(IP-Dialog) benchmark along with a training dataset, covering 10 tasks and 12\nuser attribute types. Additionally, we develop a systematic evaluation\nframework with four metrics to assess both attribute awareness and reasoning\ncapabilities. We further propose five causal graphs to elucidate models'\nreasoning pathways during implicit personalization. Extensive experiments yield\ninsightful observations and prove the reliability of our dataset."}
{"id": "2506.02454", "pdf": "https://arxiv.org/pdf/2506.02454.pdf", "abs": "https://arxiv.org/abs/2506.02454", "title": "Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports From Scratch with Agentic Framework", "authors": ["Zhaorui Yang", "Bo Pan", "Han Wang", "Yiyao Wang", "Xingyu Liu", "Minfeng Zhu", "Bo Zhang", "Wei Chen"], "categories": ["cs.CL", "cs.AI"], "comment": "47 pages", "summary": "Visualizations play a crucial part in effective communication of concepts and\ninformation. Recent advances in reasoning and retrieval augmented generation\nhave enabled Large Language Models (LLMs) to perform deep research and generate\ncomprehensive reports. Despite its progress, existing deep research frameworks\nprimarily focus on generating text-only content, leaving the automated\ngeneration of interleaved texts and visualizations underexplored. This novel\ntask poses key challenges in designing informative visualizations and\neffectively integrating them with text reports. To address these challenges, we\npropose Formal Description of Visualization (FDV), a structured textual\nrepresentation of charts that enables LLMs to learn from and generate diverse,\nhigh-quality visualizations. Building on this representation, we introduce\nMultimodal DeepResearcher, an agentic framework that decomposes the task into\nfour stages: (1) researching, (2) exemplar report textualization, (3) planning,\nand (4) multimodal report generation. For the evaluation of generated\nmultimodal reports, we develop MultimodalReportBench, which contains 100\ndiverse topics served as inputs along with 5 dedicated metrics. Extensive\nexperiments across models and evaluation methods demonstrate the effectiveness\nof Multimodal DeepResearcher. Notably, utilizing the same Claude 3.7 Sonnet\nmodel, Multimodal DeepResearcher achieves an 82\\% overall win rate over the\nbaseline method."}
{"id": "2506.02460", "pdf": "https://arxiv.org/pdf/2506.02460.pdf", "abs": "https://arxiv.org/abs/2506.02460", "title": "MidPO: Dual Preference Optimization for Safety and Helpfulness in Large Language Models via a Mixture of Experts Framework", "authors": ["Yupeng Qi", "Ziyu Lyu", "Min Yang", "Yanlin Wang", "Lu Bai", "Lixin Cui"], "categories": ["cs.CL"], "comment": null, "summary": "As large language models (LLMs) are increasingly applied across various\ndomains, enhancing safety while maintaining the helpfulness of LLMs has become\na critical challenge. Recent studies solve this problem through\nsafety-constrained online preference optimization or safety-constrained offline\npreference optimization. However, the safety-constrained online methods often\nsuffer from excessive safety, which might reduce helpfulness, while the\nsafety-constrained offline methods perform poorly in adaptively balancing\nsafety and helpfulness. To address these limitations, we propose MidPO, a\n\\textbf{\\underline{Mi}}xture of Experts (MoE) framework for safety-helpfulness\n\\textbf{\\underline{d}}ual \\textbf{\\underline{P}}reference\n\\textbf{\\underline{O}}ptimization. Firstly, MidPO devises single-preference\nenhanced direct preference optimization approach to transform the base model\ninto two independent experts, termed safety and helpfulness experts, and\nfine-tunes the two independent experts for optimal safety or helpfulness\nperformance. Secondly, to achieve an effective balance between safety and\nhelpfulness, MidPO incorporates the two experts into the MoE framework and\ndesigns a dynamic routing mechanism to allocate contributions from each expert\nadaptively. We conduct quantitative and qualitative experiments on three\npopular datasets to demonstrate the proposed MidPO significantly outperforms\nstate-of-the-art approaches in both safety and helpfulness. The code and models\nwill be released."}
{"id": "2506.02461", "pdf": "https://arxiv.org/pdf/2506.02461.pdf", "abs": "https://arxiv.org/abs/2506.02461", "title": "XToM: Exploring the Multilingual Theory of Mind for Large Language Models", "authors": ["Chunkit Chan", "Yauwai Yim", "Hongchuan Zeng", "Zhiying Zou", "Xinyuan Cheng", "Zhifan Sun", "Zheye Deng", "Kawai Chung", "Yuzhuo Ao", "Yixiang Fan", "Cheng Jiayang", "Ercong Nie", "Ginny Y. Wong", "Helmut Schmid", "Hinrich Schütze", "Simon See", "Yangqiu Song"], "categories": ["cs.CL"], "comment": null, "summary": "Theory of Mind (ToM), the ability to infer mental states in others, is\npivotal for human social cognition. Existing evaluations of ToM in LLMs are\nlargely limited to English, neglecting the linguistic diversity that shapes\nhuman cognition. This limitation raises a critical question: can LLMs exhibit\nMultilingual Theory of Mind, which is the capacity to reason about mental\nstates across diverse linguistic contexts? To address this gap, we present\nXToM, a rigorously validated multilingual benchmark that evaluates ToM across\nfive languages and incorporates diverse, contextually rich task scenarios.\nUsing XToM, we systematically evaluate LLMs (e.g., DeepSeek R1), revealing a\npronounced dissonance: while models excel in multilingual language\nunderstanding, their ToM performance varies across languages. Our findings\nexpose limitations in LLMs' ability to replicate human-like mentalizing across\nlinguistic contexts."}
{"id": "2506.02478", "pdf": "https://arxiv.org/pdf/2506.02478.pdf", "abs": "https://arxiv.org/abs/2506.02478", "title": "FroM: Frobenius Norm-Based Data-Free Adaptive Model Merging", "authors": ["Zijian Li", "Xiaocheng Feng", "Huixin Liu", "Yichong Huang", "Ting Liu", "Bing Qin"], "categories": ["cs.CL"], "comment": "12 pages, 11 figures", "summary": "With the development of large language models, fine-tuning has emerged as an\neffective method to enhance performance in specific scenarios by injecting\ndomain-specific knowledge. In this context, model merging techniques provide a\nsolution for fusing knowledge from multiple fine-tuning models by combining\ntheir parameters. However, traditional methods often encounter task\ninterference when merging full fine-tuning models, and this problem becomes\neven more evident in parameter-efficient fine-tuning scenarios. In this paper,\nwe introduce an improvement to the RegMean method, which indirectly leverages\nthe training data to approximate the outputs of the linear layers before and\nafter merging. We propose an adaptive merging method called FroM, which\ndirectly measures the model parameters using the Frobenius norm, without any\ntraining data. By introducing an additional hyperparameter for control, FroM\noutperforms baseline methods across various fine-tuning scenarios, alleviating\nthe task interference problem."}
{"id": "2506.02480", "pdf": "https://arxiv.org/pdf/2506.02480.pdf", "abs": "https://arxiv.org/abs/2506.02480", "title": "ORPP: Self-Optimizing Role-playing Prompts to Enhance Language Model Capabilities", "authors": ["Yifan Duan", "Yihong Tang", "Kehai Chen", "Liqiang Nie", "Min Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "High-quality prompts are crucial for eliciting outstanding performance from\nlarge language models (LLMs) on complex tasks. Existing research has explored\nmodel-driven strategies for prompt optimization. However, these methods often\nsuffer from high computational overhead or require strong optimization\ncapabilities from the model itself, which limits their broad applicability.To\naddress these challenges, we propose ORPP (Optimized Role-Playing Prompt),a\nframework that enhances model performance by optimizing and generating\nrole-playing prompts. The core idea of ORPP is to confine the prompt search\nspace to role-playing scenarios, thereby fully activating the model's intrinsic\ncapabilities through carefully crafted, high-quality role-playing prompts.\nSpecifically, ORPP first performs iterative optimization on a small subset of\ntraining samples to generate high-quality role-playing prompts. Then,\nleveraging the model's few-shot learning capability, it transfers the\noptimization experience to efficiently generate suitable prompts for the\nremaining samples.Our experimental results show that ORPP not only matches but\nin most cases surpasses existing mainstream prompt optimization methods in\nterms of performance. Notably, ORPP demonstrates superior \"plug-and-play\"\ncapability. In most cases, it can be integrated with various other prompt\nmethods and further enhance their effectiveness."}
{"id": "2506.02481", "pdf": "https://arxiv.org/pdf/2506.02481.pdf", "abs": "https://arxiv.org/abs/2506.02481", "title": "Do Language Models Think Consistently? A Study of Value Preferences Across Varying Response Lengths", "authors": ["Inderjeet Nair", "Lu Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Evaluations of LLMs' ethical risks and value inclinations often rely on\nshort-form surveys and psychometric tests, yet real-world use involves\nlong-form, open-ended responses -- leaving value-related risks and preferences\nin practical settings largely underexplored. In this work, we ask: Do value\npreferences inferred from short-form tests align with those expressed in\nlong-form outputs? To address this question, we compare value preferences\nelicited from short-form reactions and long-form responses, varying the number\nof arguments in the latter to capture users' differing verbosity preferences.\nAnalyzing five LLMs (llama3-8b, gemma2-9b, mistral-7b, qwen2-7b, and olmo-7b),\nwe find (1) a weak correlation between value preferences inferred from\nshort-form and long-form responses across varying argument counts, and (2)\nsimilarly weak correlation between preferences derived from any two distinct\nlong-form generation settings. (3) Alignment yields only modest gains in the\nconsistency of value expression. Further, we examine how long-form generation\nattributes relate to value preferences, finding that argument specificity\nnegatively correlates with preference strength, while representation across\nscenarios shows a positive correlation. Our findings underscore the need for\nmore robust methods to ensure consistent value expression across diverse\napplications."}
{"id": "2506.02483", "pdf": "https://arxiv.org/pdf/2506.02483.pdf", "abs": "https://arxiv.org/abs/2506.02483", "title": "Enhancing Large Language Models with Neurosymbolic Reasoning for Multilingual Tasks", "authors": ["Sina Bagheri Nezhad", "Ameeta Agrawal"], "categories": ["cs.CL"], "comment": "Accepted at 19th Conference on Neurosymbolic Learning and Reasoning\n  (NeSy 2025)", "summary": "Large language models (LLMs) often struggle to perform multi-target reasoning\nin long-context scenarios where relevant information is scattered across\nextensive documents. To address this challenge, we introduce NeuroSymbolic\nAugmented Reasoning (NSAR), which combines the benefits of neural and symbolic\nreasoning during inference. NSAR explicitly extracts symbolic facts from text\nand generates executable Python code to handle complex reasoning steps. Through\nextensive experiments across seven languages and diverse context lengths, we\ndemonstrate that NSAR significantly outperforms both a vanilla RAG baseline and\nadvanced prompting strategies in accurately identifying and synthesizing\nmultiple pieces of information. Our results highlight the effectiveness of\ncombining explicit symbolic operations with neural inference for robust,\ninterpretable, and scalable reasoning in multilingual settings."}
{"id": "2506.02494", "pdf": "https://arxiv.org/pdf/2506.02494.pdf", "abs": "https://arxiv.org/abs/2506.02494", "title": "Minos: A Multimodal Evaluation Model for Bidirectional Generation Between Image and Text", "authors": ["Junzhe Zhang", "Huixuan Zhang", "Xinyu Hu", "Li Lin", "Mingqi Gao", "Shi Qiu", "Xiaojun Wan"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Evaluation is important for multimodal generation tasks. With the rapid\nprogress of MLLMs, there is growing interest in applying MLLMs to build general\nevaluation systems. However, existing work overlooks two aspects: (1) the\ndevelopment of evaluation capabilities for text-to-image (T2I) generation task,\nand (2) the incorporation of large-scale human evaluation data. In this paper,\nwe introduce Minos-Corpus, a large-scale multimodal evaluation dataset that\ncombines evaluation data from both human and GPT. The corpus contains\nevaluation data across both image-to-text(I2T) and T2I generation tasks. Based\non this corpus, we propose Data Selection and Balance, Mix-SFT training\nmethods, and apply DPO to develop Minos, a multimodal evaluation model built\nupon a 7B backbone. Minos achieves state-of-the-art (SoTA) performance among\nall open-source evaluation models of similar scale on the average of evaluation\nperformance on all tasks, and outperforms all open-source and closed-source\nmodels on evaluation of T2I generation task. Extensive experiments demonstrate\nthe importance of leveraging high-quality human evaluation data and jointly\ntraining on evaluation data from both I2T and T2I generation tasks."}
{"id": "2506.02503", "pdf": "https://arxiv.org/pdf/2506.02503.pdf", "abs": "https://arxiv.org/abs/2506.02503", "title": "KARE-RAG: Knowledge-Aware Refinement and Enhancement for RAG", "authors": ["Yongjian Li", "HaoCheng Chu", "Yukun Yan", "Zhenghao Liu", "Shi Yu", "Zheni Zeng", "Ruobing Wang", "Sen Song", "Zhiyuan Liu", "Maosong Sun"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to\naccess broader knowledge sources, yet factual inconsistencies persist due to\nnoise in retrieved documents-even with advanced retrieval methods. We\ndemonstrate that enhancing generative models' capacity to process noisy content\nis equally critical for robust performance. In this paper, we present KARE-RAG\n(Knowledge-Aware Refinement and Enhancement for RAG), which improves knowledge\nutilization through three key innovations: (1) structured knowledge\nrepresentations that facilitate error detection during training, (2) Dense\nDirect Preference Optimization (DDPO)-a refined training objective that\nprioritizes correction of critical errors, and (3) a contrastive data\ngeneration pipeline that maintains semantic consistency while rectifying\nfactual inaccuracies. Experiments show our method significantly enhances\nstandard RAG pipelines across model scales, improving both in-domain and\nout-of-domain task performance without compromising general capabilities.\nNotably, these gains are achieved with modest training data, suggesting\ndata-efficient optimization is possible through targeted learning strategies.\nOur findings establish a new direction for RAG improvement: by improving how\nmodels learn to process retrieved content, we can enhance performance across\ndiverse inference paradigms. All data and code will be publicly available on\nGithub."}
{"id": "2506.02510", "pdf": "https://arxiv.org/pdf/2506.02510.pdf", "abs": "https://arxiv.org/abs/2506.02510", "title": "M$^3$FinMeeting: A Multilingual, Multi-Sector, and Multi-Task Financial Meeting Understanding Evaluation Dataset", "authors": ["Jie Zhu", "Junhui Li", "Yalong Wen", "Xiandong Li", "Lifan Guo", "Feng Chen"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL-2025", "summary": "Recent breakthroughs in large language models (LLMs) have led to the\ndevelopment of new benchmarks for evaluating their performance in the financial\ndomain. However, current financial benchmarks often rely on news articles,\nearnings reports, or announcements, making it challenging to capture the\nreal-world dynamics of financial meetings. To address this gap, we propose a\nnovel benchmark called $\\texttt{M$^3$FinMeeting}$, which is a multilingual,\nmulti-sector, and multi-task dataset designed for financial meeting\nunderstanding. First, $\\texttt{M$^3$FinMeeting}$ supports English, Chinese, and\nJapanese, enhancing comprehension of financial discussions in diverse\nlinguistic contexts. Second, it encompasses various industry sectors defined by\nthe Global Industry Classification Standard (GICS), ensuring that the benchmark\nspans a broad range of financial activities. Finally,\n$\\texttt{M$^3$FinMeeting}$ includes three tasks: summarization, question-answer\n(QA) pair extraction, and question answering, facilitating a more realistic and\ncomprehensive evaluation of understanding. Experimental results with seven\npopular LLMs reveal that even the most advanced long-context models have\nsignificant room for improvement, demonstrating the effectiveness of\n$\\texttt{M$^3$FinMeeting}$ as a benchmark for assessing LLMs' financial meeting\ncomprehension skills."}
{"id": "2506.02515", "pdf": "https://arxiv.org/pdf/2506.02515.pdf", "abs": "https://arxiv.org/abs/2506.02515", "title": "FinChain: A Symbolic Benchmark for Verifiable Chain-of-Thought Financial Reasoning", "authors": ["Zhuohan Xie", "Dhruv Sahnan", "Debopriyo Banerjee", "Georgi Georgiev", "Rushil Thareja", "Hachem Madmoun", "Jinyan Su", "Aaryamonvikram Singh", "Yuxia Wang", "Rui Xing", "Fajri Koto", "Haonan Li", "Ivan Koychev", "Tanmoy Chakraborty", "Salem Lahlou", "Veselin Stoyanov", "Preslav Nakov"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "15 pages, 8 figures, 2 tables", "summary": "Multi-step symbolic reasoning is critical for advancing downstream\nperformance on financial tasks. Yet, benchmarks for systematically evaluating\nthis capability are lacking. Existing datasets like FinQA and ConvFinQA\nsupervise only final numerical answers, without assessing intermediate\nreasoning steps. To address this, we introduce FinChain, the first symbolic\nbenchmark designed for verifiable Chain-of- Thought (CoT) financial reasoning.\nSpanning 54 topics across 12 financial domains, Fin- Chain offers five\nparameterized templates per topic, each varying in reasoning complexity and\ndomain expertise required. Each dataset instance includes an executable Python\ntrace, enabling automatic generation of extensive training data and easy\nadaptation to other domains. We also introduce ChainEval, a new metric for\nautomatic evaluation of both final answers and intermediate reasoning.\nBenchmarking 30 LLMs on our dataset, we find that even state-of-the-art models\nhave considerable room for improvement in multi-step financial reasoning. All\ntemplates and evaluation metrics for FinChain are available at https:\n//github.com/mbzuai-nlp/finchain."}
{"id": "2506.02519", "pdf": "https://arxiv.org/pdf/2506.02519.pdf", "abs": "https://arxiv.org/abs/2506.02519", "title": "Learning Together to Perform Better: Teaching Small-Scale LLMs to Collaborate via Preferential Rationale Tuning", "authors": ["Sohan Patnaik", "Milan Aggarwal", "Sumit Bhatia", "Balaji Krishnamurthy"], "categories": ["cs.CL"], "comment": "Accepted at ACL Main 2025", "summary": "LLMssuch as GPT-4 have shown a remarkable ability to solve complex questions\nby generating step-by-step rationales. Prior works have utilized this\ncapability to improve smaller and cheaper LMs (say, with 7B parameters).\nHowever, various practical constraints, such as copyright and legal issues,\nowing to lack of transparency in the pre-training data of large (often closed)\nmodels, prevent their use in commercial settings. Little focus has been given\nto improving the innate reasoning ability of smaller models without distilling\ninformation from larger LLMs. To address this, we propose COLLATE, a trainable\nframework that tunes a (small) LLM to generate those outputs from a pool of\ndiverse rationales that selectively improves the downstream task. COLLATE\nenforces multiple instances of the same LLM to exhibit distinct behavior and\nemploys them to generate rationales to obtain diverse outputs. The LLM is then\ntuned via preference optimization to choose the candidate rationale which\nmaximizes the likelihood of ground-truth answer. COLLATE outperforms several\ntrainable and prompting baselines on 5 datasets across 3 domains: maths problem\nsolving, natural language inference, and commonsense reasoning. We show the eff\nicacy of COLLATE on LLMs from different model families across varying parameter\nscales (1B to 8B) and demonstrate the benefit of multiple rationale providers\nguided by the end task through ablations. Code is released here\n(https://github.com/Sohanpatnaik106/collate)."}
{"id": "2506.02527", "pdf": "https://arxiv.org/pdf/2506.02527.pdf", "abs": "https://arxiv.org/abs/2506.02527", "title": "Multilingual Information Retrieval with a Monolingual Knowledge Base", "authors": ["Yingying Zhuang", "Aman Gupta", "Anurag Beniwal"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "6 pages, accepted at GENNEXT@SIGIR25", "summary": "Multilingual information retrieval has emerged as powerful tools for\nexpanding knowledge sharing across languages. On the other hand, resources on\nhigh quality knowledge base are often scarce and in limited languages,\ntherefore an effective embedding model to transform sentences from different\nlanguages into a feature vector space same as the knowledge base language\nbecomes the key ingredient for cross language knowledge sharing, especially to\ntransfer knowledge available in high-resource languages to low-resource ones.\nIn this paper we propose a novel strategy to fine-tune multilingual embedding\nmodels with weighted sampling for contrastive learning, enabling multilingual\ninformation retrieval with a monolingual knowledge base. We demonstrate that\nthe weighted sampling strategy produces performance gains compared to standard\nones by up to 31.03\\% in MRR and up to 33.98\\% in Recall@3. Additionally, our\nproposed methodology is language agnostic and applicable for both multilingual\nand code switching use cases."}
{"id": "2506.02532", "pdf": "https://arxiv.org/pdf/2506.02532.pdf", "abs": "https://arxiv.org/abs/2506.02532", "title": "ReasoningFlow: Semantic Structure of Complex Reasoning Traces", "authors": ["Jinu Lee", "Sagnik Mukherjee", "Dilek Hakkani-Tur", "Julia Hockenmaier"], "categories": ["cs.CL"], "comment": "10 pages, 6 figures. ArgMining 2025 Workshop (Non-archival) @ ACL\n  2025", "summary": "Large reasoning models (LRMs) generate complex reasoning traces with\nplanning, reflection, verification, and backtracking. In this work, we\nintroduce ReasoningFlow, a unified schema for analyzing the semantic structures\nof these complex traces. ReasoningFlow parses traces into directed acyclic\ngraphs, enabling the characterization of distinct reasoning patterns as\nsubgraph structures. This human-interpretable representation offers promising\napplications in understanding, evaluating, and enhancing the reasoning\nprocesses of LRMs."}
{"id": "2506.02533", "pdf": "https://arxiv.org/pdf/2506.02533.pdf", "abs": "https://arxiv.org/abs/2506.02533", "title": "Natural Language Processing to Enhance Deliberation in Political Online Discussions: A Survey", "authors": ["Maike Behrendt", "Stefan Sylvius Wagner", "Carina Weinmann", "Marike Bormann", "Mira Warne", "Stefan Harmeling"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Political online participation in the form of discussing political issues and\nexchanging opinions among citizens is gaining importance with more and more\nformats being held digitally. To come to a decision, a careful discussion and\nconsideration of opinions and a civil exchange of arguments, which is defined\nas the act of deliberation, is desirable. The quality of discussions and\nparticipation processes in terms of their deliberativeness highly depends on\nthe design of platforms and processes. To facilitate online communication for\nboth participants and initiators, machine learning methods offer a lot of\npotential. In this work we want to showcase which issues occur in political\nonline discussions and how machine learning can be used to counteract these\nissues and enhance deliberation."}
{"id": "2506.02536", "pdf": "https://arxiv.org/pdf/2506.02536.pdf", "abs": "https://arxiv.org/abs/2506.02536", "title": "Answer Convergence as a Signal for Early Stopping in Reasoning", "authors": ["Xin Liu", "Lu Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Chain-of-thought (CoT) prompting enhances reasoning in large language models\n(LLMs) but often leads to verbose and redundant outputs, thus increasing\ninference cost. We hypothesize that many reasoning steps are unnecessary for\nproducing correct answers. To investigate this, we start with a systematic\nstudy to examine what is the minimum reasoning required for a model to reach a\nstable decision. We find that on math reasoning tasks like math, models\ntypically converge to their final answers after 60\\% of the reasoning steps,\nsuggesting substantial redundancy in the remaining content. Based on these\ninsights, we propose three inference-time strategies to improve efficiency: (1)\nearly stopping via answer consistency, (2) boosting the probability of\ngenerating end-of-reasoning signals, and (3) a supervised method that learns\nwhen to stop based on internal activations. Experiments across five benchmarks\nand five open-weights LLMs show that our methods significantly reduce token\nusage with little or no accuracy drop. In particular, on NaturalQuestions,\nAnswer Consistency reduces tokens by over 40\\% while further improving\naccuracy. Our work underscores the importance of cost-effective reasoning\nmethods that operate at inference time, offering practical benefits for\nreal-world applications."}
{"id": "2506.02544", "pdf": "https://arxiv.org/pdf/2506.02544.pdf", "abs": "https://arxiv.org/abs/2506.02544", "title": "CoRe-MMRAG: Cross-Source Knowledge Reconciliation for Multimodal RAG", "authors": ["Yang Tian", "Fan Liu", "Jingyuan Zhang", "Victoria W.", "Yupeng Hu", "Liqiang Nie"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 Main", "summary": "Multimodal Retrieval-Augmented Generation (MMRAG) has been introduced to\nenhance Multimodal Large Language Models by incorporating externally retrieved\nmultimodal knowledge, but it introduces two challenges: Parametric-Retrieved\nKnowledge Inconsistency (PRKI), where discrepancies between parametric and\nretrieved knowledge create uncertainty in determining reliability, and\nVisual-Textual Knowledge Inconsistency (VTKI), where misalignment between\nvisual and textual sources disrupts entity representation. To address these\nchallenges, we propose \\textbf{C}r\\textbf{o}ss-source knowledge\n\\textbf{Re}conciliation for \\textbf{M}ulti\\textbf{M}odal \\textbf{RAG}\n(CoRe-MMRAG), a novel end-to-end framework that effectively reconciles\ninconsistencies across knowledge sources. CoRe-MMRAG follows a four-stage\npipeline: it first generates an internal response from parametric knowledge,\nthen selects the most relevant multimodal evidence via joint similarity\nassessment, generates an external response, and finally integrates both to\nproduce a reliable answer. Additionally, a specialized training paradigm\nenhances knowledge source discrimination, multimodal integration, and unified\nanswer generation. Experiments on KB-VQA benchmarks show that CoRe-MMRAG\nachieves substantial improvements over baseline methods, achieving 5.6\\% and\n9.3\\% performance gains on InfoSeek and Encyclopedic-VQA, respectively. We\nrelease code and data at\n\\href{https://github.com/TyangJN/CoRe-MMRAG}{https://github.com/TyangJN/CoRe-MMRAG}."}
{"id": "2506.02561", "pdf": "https://arxiv.org/pdf/2506.02561.pdf", "abs": "https://arxiv.org/abs/2506.02561", "title": "Pruning General Large Language Models into Customized Expert Models", "authors": ["Yirao Zhao", "Guizhen Chen", "Kenji Kawaguchi", "Lidong Bing", "Wenxuan Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have revolutionized natural language processing,\nyet their substantial model sizes often require substantial computational\nresources. To preserve computing resources and accelerate inference speed, it\nis crucial to prune redundant parameters, especially for experienced users who\noften need compact expert models tailored to specific downstream scenarios.\nHowever, most existing pruning methods focus on preserving the model's general\ncapabilities, often requiring extensive post-training or suffering from\ndegraded performance due to coarse-grained pruning. In this work, we design a\n$\\underline{Cus}$tom $\\underline{Prun}$ing method ($\\texttt{Cus-Prun}$) to\nprune a large general model into a smaller lightweight expert model, which is\npositioned along the \"language\", \"domain\" and \"task\" dimensions. By identifying\nand pruning irrelevant neurons of each dimension, $\\texttt{Cus-Prun}$ creates\nexpert models without any post-training. Our experiments demonstrate that\n$\\texttt{Cus-Prun}$ consistently outperforms other methods, achieving minimal\nloss in both expert and general capabilities across various models from\ndifferent model families and sizes."}
{"id": "2506.02573", "pdf": "https://arxiv.org/pdf/2506.02573.pdf", "abs": "https://arxiv.org/abs/2506.02573", "title": "IndoSafety: Culturally Grounded Safety for LLMs in Indonesian Languages", "authors": ["Muhammad Falensi Azmi", "Muhammad Dehan Al Kautsar", "Alfan Farizki Wicaksono", "Fajri Koto"], "categories": ["cs.CL"], "comment": "25 pages", "summary": "Although region-specific large language models (LLMs) are increasingly\ndeveloped, their safety remains underexplored, particularly in culturally\ndiverse settings like Indonesia, where sensitivity to local norms is essential\nand highly valued by the community. In this work, we present IndoSafety, the\nfirst high-quality, human-verified safety evaluation dataset tailored for the\nIndonesian context, covering five language varieties: formal and colloquial\nIndonesian, along with three major local languages: Javanese, Sundanese, and\nMinangkabau. IndoSafety is constructed by extending prior safety frameworks to\ndevelop a taxonomy that captures Indonesia's sociocultural context. We find\nthat existing Indonesian-centric LLMs often generate unsafe outputs,\nparticularly in colloquial and local language settings, while fine-tuning on\nIndoSafety significantly improves safety while preserving task performance. Our\nwork highlights the critical need for culturally grounded safety evaluation and\nprovides a concrete step toward responsible LLM deployment in multilingual\nsettings. Warning: This paper contains example data that may be offensive,\nharmful, or biased."}
{"id": "2506.02584", "pdf": "https://arxiv.org/pdf/2506.02584.pdf", "abs": "https://arxiv.org/abs/2506.02584", "title": "Prosodic Structure Beyond Lexical Content: A Study of Self-Supervised Learning", "authors": ["Sarenne Wallbridge", "Christoph Minixhofer", "Catherine Lai", "Peter Bell"], "categories": ["cs.CL", "cs.AI", "eess.AS"], "comment": "Accepted at INTERSPEECH 2025", "summary": "People exploit the predictability of lexical structures during text\ncomprehension. Though predictable structure is also present in speech, the\ndegree to which prosody, e.g. intonation, tempo, and loudness, contributes to\nsuch structure independently of the lexical content is unclear. This study\nleverages self-supervised learning (SSL) to examine the temporal granularity of\nstructures in the acoustic correlates of prosody. Representations from our\nproposed Masked Prosody Model can predict perceptual labels dependent on local\ninformation, such as word boundaries, but provide the most value for labels\ninvolving longer-term structures, like emotion recognition. Probing experiments\nacross various perceptual labels show strong relative gains over untransformed\npitch, energy, and voice activity features. Our results reveal the importance\nof SSL training objective timescale and highlight the value of complex\nSSL-encoded structures compared to more constrained classical structures."}
{"id": "2506.02589", "pdf": "https://arxiv.org/pdf/2506.02589.pdf", "abs": "https://arxiv.org/abs/2506.02589", "title": "Evaluating Named Entity Recognition Models for Russian Cultural News Texts: From BERT to LLM", "authors": ["Maria Levchenko"], "categories": ["cs.CL", "cs.AI", "cs.IR", "68T50", "I.2.7; H.3.3"], "comment": null, "summary": "This paper addresses the challenge of Named Entity Recognition (NER) for\nperson names within the specialized domain of Russian news texts concerning\ncultural events. The study utilizes the unique SPbLitGuide dataset, a\ncollection of event announcements from Saint Petersburg spanning 1999 to 2019.\nA comparative evaluation of diverse NER models is presented, encompassing\nestablished transformer-based architectures such as DeepPavlov, RoBERTa, and\nSpaCy, alongside recent Large Language Models (LLMs) including GPT-3.5, GPT-4,\nand GPT-4o. Key findings highlight the superior performance of GPT-4o when\nprovided with specific prompting for JSON output, achieving an F1 score of\n0.93. Furthermore, GPT-4 demonstrated the highest precision at 0.99. The\nresearch contributes to a deeper understanding of current NER model\ncapabilities and limitations when applied to morphologically rich languages\nlike Russian within the cultural heritage domain, offering insights for\nresearchers and practitioners. Follow-up evaluation with GPT-4.1 (April 2025)\nachieves F1=0.94 for both simple and structured prompts, demonstrating rapid\nprogress across model families and simplified deployment requirements."}
{"id": "2506.02591", "pdf": "https://arxiv.org/pdf/2506.02591.pdf", "abs": "https://arxiv.org/abs/2506.02591", "title": "On Generalization across Measurement Systems: LLMs Entail More Test-Time Compute for Underrepresented Cultures", "authors": ["Minh Duc Bui", "Kyung Eun Park", "Goran Glavaš", "Fabian David Schmidt", "Katharina von der Wense"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main (Camera-Ready Version)", "summary": "Measurement systems (e.g., currencies) differ across cultures, but the\nconversions between them are well defined so that humans can state facts using\nany measurement system of their choice. Being available to users from diverse\ncultural backgrounds, large language models (LLMs) should also be able to\nprovide accurate information irrespective of the measurement system at hand.\nUsing newly compiled datasets we test if this is the case for seven open-source\nLLMs, addressing three key research questions: (RQ1) What is the default system\nused by LLMs for each type of measurement? (RQ2) Do LLMs' answers and their\naccuracy vary across different measurement systems? (RQ3) Can LLMs mitigate\npotential challenges w.r.t. underrepresented systems via reasoning? Our\nfindings show that LLMs default to the measurement system predominantly used in\nthe data. Additionally, we observe considerable instability and variance in\nperformance across different measurement systems. While this instability can in\npart be mitigated by employing reasoning methods such as chain-of-thought\n(CoT), this implies longer responses and thereby significantly increases\ntest-time compute (and inference costs), marginalizing users from cultural\nbackgrounds that use underrepresented measurement systems."}
{"id": "2506.02592", "pdf": "https://arxiv.org/pdf/2506.02592.pdf", "abs": "https://arxiv.org/abs/2506.02592", "title": "Beyond the Surface: Measuring Self-Preference in LLM Judgments", "authors": ["Zhi-Yuan Chen", "Hao Wang", "Xinyu Zhang", "Enrui Hu", "Yankai Lin"], "categories": ["cs.CL"], "comment": null, "summary": "Recent studies show that large language models (LLMs) exhibit self-preference\nbias when serving as judges, meaning they tend to favor their own responses\nover those generated by other models. Existing methods typically measure this\nbias by calculating the difference between the scores a judge model assigns to\nits own responses and those it assigns to responses from other models. However,\nthis approach conflates self-preference bias with response quality, as\nhigher-quality responses from the judge model may also lead to positive score\ndifferences, even in the absence of bias. To address this issue, we introduce\ngold judgments as proxies for the actual quality of responses and propose the\nDBG score, which measures self-preference bias as the difference between the\nscores assigned by the judge model to its own responses and the corresponding\ngold judgments. Since gold judgments reflect true response quality, the DBG\nscore mitigates the confounding effect of response quality on bias measurement.\nUsing the DBG score, we conduct comprehensive experiments to assess\nself-preference bias across LLMs of varying versions, sizes, and reasoning\nabilities. Additionally, we investigate two factors that influence and help\nalleviate self-preference bias: response text style and the post-training data\nof judge models. Finally, we explore potential underlying mechanisms of\nself-preference bias from an attention-based perspective. Our code and data are\navailable at https://github.com/zhiyuanc2001/self-preference."}
{"id": "2506.02596", "pdf": "https://arxiv.org/pdf/2506.02596.pdf", "abs": "https://arxiv.org/abs/2506.02596", "title": "EssayBench: Evaluating Large Language Models in Multi-Genre Chinese Essay Writing", "authors": ["Fan Gao", "Dongyuan Li", "Ding Xia", "Fei Mi", "Yasheng Wang", "Lifeng Shang", "Baojun Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Chinese essay writing and its evaluation are critical in educational\ncontexts, yet the capabilities of Large Language Models (LLMs) in this domain\nremain largely underexplored. Existing benchmarks often rely on coarse-grained\ntext quality metrics, largely overlooking the structural and rhetorical\ncomplexities of Chinese essays, particularly across diverse genres. To address\nthis gap, we propose \\benchName, a multi-genre benchmark specifically designed\nfor Chinese essay writing across four major genres: Argumentative, Narrative,\nDescriptive, and Expository. We curate and refine a total of 728 real-world\nprompts to ensure authenticity and meticulously categorize them into the\n\\textit{Open-Ended} and \\textit{Constrained} sets to capture diverse writing\nscenarios. To reliably evaluate generated essays, we develop a fine-grained,\ngenre-specific scoring framework that hierarchically aggregates scores. We\nfurther validate our evaluation protocol through a comprehensive human\nagreement study. Finally, we benchmark 15 large-sized LLMs, analyzing their\nstrengths and limitations across genres and instruction types. With \\benchName,\nwe aim to advance LLM-based Chinese essay evaluation and inspire future\nresearch on improving essay generation in educational settings."}
{"id": "2506.02627", "pdf": "https://arxiv.org/pdf/2506.02627.pdf", "abs": "https://arxiv.org/abs/2506.02627", "title": "Overcoming Data Scarcity in Multi-Dialectal Arabic ASR via Whisper Fine-Tuning", "authors": ["Ömer Tarik Özyilmaz", "Matt Coler", "Matias Valdenegro-Toro"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted at Interspeech 2025", "summary": "Although commercial Arabic automatic speech recognition (ASR) systems support\nModern Standard Arabic (MSA), they struggle with dialectal speech. We\ninvestigate the effect of fine-tuning OpenAI's Whisper on five major Arabic\ndialects (Gulf, Levantine, Iraqi, Egyptian, Maghrebi) using Mozilla Common\nVoice for MSA and the MASC dataset for dialectal speech. We evaluate MSA\ntraining size effects, benefits of pre-training on MSA data, and\ndialect-specific versus dialect-pooled models. We find that small amounts of\nMSA fine-tuning data yield substantial improvements for smaller models,\nmatching larger non-fine-tuned models. While MSA pre-training shows minimal\nbenefit, suggesting limited shared features between MSA and dialects, our\ndialect-pooled models perform comparably to dialect-specific ones. This\nindicates that pooling dialectal data, when properly balanced, can help address\ndata scarcity in low-resource ASR without significant performance loss."}
{"id": "2506.02659", "pdf": "https://arxiv.org/pdf/2506.02659.pdf", "abs": "https://arxiv.org/abs/2506.02659", "title": "Are Economists Always More Introverted? Analyzing Consistency in Persona-Assigned LLMs", "authors": ["Manon Reusens", "Bart Baesens", "David Jurgens"], "categories": ["cs.CL"], "comment": null, "summary": "Personalized Large Language Models (LLMs) are increasingly used in diverse\napplications, where they are assigned a specific persona - such as a happy high\nschool teacher - to guide their responses. While prior research has examined\nhow well LLMs adhere to predefined personas in writing style, a comprehensive\nanalysis of consistency across different personas and task types is lacking. In\nthis paper, we introduce a new standardized framework to analyze consistency in\npersona-assigned LLMs. We define consistency as the extent to which a model\nmaintains coherent responses when assigned the same persona across different\ntasks and runs. Our framework evaluates personas across four different\ncategories (happiness, occupation, personality, and political stance) spanning\nmultiple task dimensions (survey writing, essay generation, social media post\ngeneration, single turn, and multi-turn conversations). Our findings reveal\nthat consistency is influenced by multiple factors, including the assigned\npersona, stereotypes, and model design choices. Consistency also varies across\ntasks, increasing with more structured tasks and additional context. All code\nis available on GitHub."}
{"id": "2506.02672", "pdf": "https://arxiv.org/pdf/2506.02672.pdf", "abs": "https://arxiv.org/abs/2506.02672", "title": "EvaLearn: Quantifying the Learning Capability and Efficiency of LLMs via Sequential Problem Solving", "authors": ["Shihan Dou", "Ming Zhang", "Chenhao Huang", "Jiayi Chen", "Feng Chen", "Shichun Liu", "Yan Liu", "Chenxiao Liu", "Cheng Zhong", "Zongzhang Zhang", "Tao Gui", "Chao Xin", "Wei Chengzhi", "Lin Yan", "Qi Zhang", "Xuanjing Huang"], "categories": ["cs.CL", "cs.AI"], "comment": "47 pages, 24 figures", "summary": "We introduce EvaLearn, a pioneering benchmark designed to evaluate large\nlanguage models (LLMs) on their learning capability and efficiency in\nchallenging tasks, a critical, yet underexplored aspect of model potential.\nEvaLearn contains 648 challenging problems across six task types, grouped into\n182 sequences, each sequence dedicated to one task type. Diverging from most\nexisting benchmarks that evaluate models in parallel, EvaLearn requires models\nto solve problems sequentially, allowing them to leverage the experience gained\nfrom previous solutions. EvaLearn provides five comprehensive automated metrics\nto evaluate models and quantify their learning capability and efficiency. We\nextensively benchmark nine frontier models and observe varied performance\nprofiles: some models, such as Claude-3.7-sonnet, start with moderate initial\nperformance but exhibit strong learning ability, while some models struggle to\nbenefit from experience and may even show negative transfer. Moreover, we\ninvestigate model performance under two learning settings and find that\ninstance-level rubrics and teacher-model feedback further facilitate model\nlearning. Importantly, we observe that current LLMs with stronger static\nabilities do not show a clear advantage in learning capability across all\ntasks, highlighting that EvaLearn evaluates a new dimension of model\nperformance. We hope EvaLearn provides a novel evaluation perspective for\nassessing LLM potential and understanding the gap between models and human\ncapabilities, promoting the development of deeper and more dynamic evaluation\napproaches. All datasets, the automatic evaluation framework, and the results\nstudied in this paper are available at the GitHub repository."}
{"id": "2506.02678", "pdf": "https://arxiv.org/pdf/2506.02678.pdf", "abs": "https://arxiv.org/abs/2506.02678", "title": "TL;DR: Too Long, Do Re-weighting for Effcient LLM Reasoning Compression", "authors": ["Zhong-Zhi Li", "Xiao Liang", "Zihao Tang", "Lei Ji", "Peijie Wang", "Haotian Xu", "Xing W", "Haizhen Huang", "Weiwei Deng", "Ying Nian Wu", "Yeyun Gong", "Zhijiang Guo", "Xiao Liu", "Fei Yin", "Cheng-Lin Liu"], "categories": ["cs.CL", "cs.CE", "cs.NA", "math.NA"], "comment": null, "summary": "Large Language Models (LLMs) have recently achieved remarkable progress by\nleveraging Reinforcement Learning and extended Chain-of-Thought (CoT)\ntechniques. However, the challenge of performing efficient language\nreasoning--especially during inference with extremely long outputs--has drawn\nincreasing attention from the research community. In this work, we propose a\ndynamic ratio-based training pipeline that does not rely on sophisticated data\nannotations or interpolation between multiple models. We continuously balance\nthe weights between the model's System-1 and System-2 data to eliminate\nredundant reasoning processes while preserving the model's reasoning\ncapability. We validate our approach across models on DeepSeek-R1-Distill-7B\nand DeepSeek-R1-Distill-14B and on a diverse set of benchmarks with varying\ndifficulty levels. Our method significantly reduces the number of output tokens\nby nearly 40% while maintaining the accuracy of the reasoning. Our code and\ndata will be available soon."}
{"id": "2506.02683", "pdf": "https://arxiv.org/pdf/2506.02683.pdf", "abs": "https://arxiv.org/abs/2506.02683", "title": "Decompose, Plan in Parallel, and Merge: A Novel Paradigm for Large Language Models based Planning with Multiple Constraints", "authors": ["Zhengdong Lu", "Weikai Lu", "Yiling Tao", "Yun Dai", "ZiXuan Chen", "Huiping Zhuang", "Cen Chen", "Hao Peng", "Ziqian Zeng"], "categories": ["cs.CL"], "comment": null, "summary": "Despite significant advances in Large Language Models (LLMs), planning tasks\nstill present challenges for LLM-based agents. Existing planning methods face\ntwo key limitations: heavy constraints and cascading errors. To address these\nlimitations, we propose a novel parallel planning paradigm, which Decomposes,\nPlans for subtasks in Parallel, and Merges subplans into a final plan (DPPM).\nSpecifically, DPPM decomposes the complex task based on constraints into\nsubtasks, generates the subplan for each subtask in parallel, and merges them\ninto a global plan. In addition, our approach incorporates a verification and\nrefinement module, enabling error correction and conflict resolution.\nExperimental results demonstrate that DPPM significantly outperforms existing\nmethods in travel planning tasks."}
{"id": "2506.02689", "pdf": "https://arxiv.org/pdf/2506.02689.pdf", "abs": "https://arxiv.org/abs/2506.02689", "title": "MASTER: Enhancing Large Language Model via Multi-Agent Simulated Teaching", "authors": ["Liang Yue", "Yihong Tang", "Kehai Chen", "Jie Liu", "Min Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Instruction fine-tuning is crucial in NLP tasks, enhancing pretrained models'\ninstruction-following capabilities and task-specific performance. However,\nobtaining high-quality fine-tuning data for large models is challenging due to\ndata collection difficulties and high production costs. To address this, we\npropose MASTER, a novel data augmentation method that enriches original data\nthrough interactions among multiple agents with varying cognitive levels. We\nsimulate three pedagogically grounded teaching scenarios, leveraging\nmulti-agent conversations to generate high-quality teacher-student interaction\ndata. Utilizing MASTER, we construct BOOST-QA, a fine-tuning dataset augmented\nfrom existing datasets like Orca-Math-200k, ProcQA, and OpenHermes2.5.\nExperiments show that models fine-tuned with BOOST-QA perform excellently\nacross multiple benchmarks, demonstrating strong multitask generalization.\nNotably, MASTER significantly improves models' reasoning abilities in complex\ntasks, providing valuable insights for future research."}
{"id": "2506.02701", "pdf": "https://arxiv.org/pdf/2506.02701.pdf", "abs": "https://arxiv.org/abs/2506.02701", "title": "On Entity Identification in Language Models", "authors": ["Masaki Sakata", "Sho Yokoi", "Benjamin Heinzerling", "Takumi Ito", "Kentaro Inui"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings; 26 pages, 13 figures, 9 tables", "summary": "We analyze the extent to which internal representations of language models\n(LMs) identify and distinguish mentions of named entities, focusing on the\nmany-to-many correspondence between entities and their mentions. We first\nformulate two problems of entity mentions -- ambiguity and variability -- and\npropose a framework analogous to clustering quality metrics. Specifically, we\nquantify through cluster analysis of LM internal representations the extent to\nwhich mentions of the same entity cluster together and mentions of different\nentities remain separated. Our experiments examine five Transformer-based\nautoregressive models, showing that they effectively identify and distinguish\nentities with metrics analogous to precision and recall ranging from 0.66 to\n0.9. Further analysis reveals that entity-related information is compactly\nrepresented in a low-dimensional linear subspace at early LM layers.\nAdditionally, we clarify how the characteristics of entity representations\ninfluence word prediction performance. These findings are interpreted through\nthe lens of isomorphism between LM representations and entity-centric knowledge\nstructures in the real world, providing insights into how LMs internally\norganize and use entity information."}
{"id": "2506.02726", "pdf": "https://arxiv.org/pdf/2506.02726.pdf", "abs": "https://arxiv.org/abs/2506.02726", "title": "RACE-Align: Retrieval-Augmented and Chain-of-Thought Enhanced Preference Alignment for Large Language Models", "authors": ["Qihang Yan", "Xinyu Zhang", "Luming Guo", "Qi Zhang", "Feifan Liu"], "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; I.2.6; H.3.3"], "comment": null, "summary": "Large Language Models (LLMs) struggle with accuracy, domain-specific\nreasoning, and interpretability in vertical domains. Traditional preference\nalignment methods like Reinforcement Learning from Human Feedback (RLHF) and\nDirect Preference Optimization (DPO) often overlook the underlying knowledge\nsources and reasoning logic. This paper introduces RACE-Align\n(Retrieval-Augmented and Chain-of-Thought Enhanced Alignment), a novel\nframework designed to address these limitations. RACE-Align systematically\nconstructs a binary preference dataset incorporating external knowledge support\nand explicit Chain-of-Thought (CoT) reasoning, then aligns LLMs using the DPO\nalgorithm. The core innovation lies in its preference data construction\nstrategy: it integrates AI-driven retrieval for factual grounding, enhancing\nknowledgeability and accuracy, and emphasizes the optimization of\ndomain-specific CoT, treating the reasoning process itself as a key preference\ndimension. A multi-stage, AI-driven refinement pipeline cost-effectively\ngenerates these preference pairs. Experimental validation in Traditional\nChinese Medicine (TCM) using Qwen3-1.7B as the base model demonstrates that\nRACE-Align significantly outperforms the original base model and a model\nfine-tuned only with Supervised Fine-Tuning (SFT). Improvements were observed\nacross multiple dimensions, including answer accuracy, information richness,\napplication of TCM thinking patterns, logicality and depth of reasoning, and\ninterpretability. These findings suggest RACE-Align offers an effective pathway\nto enhance LLMs' knowledge application, reasoning reliability, and process\ntransparency in complex vertical domains."}
{"id": "2506.02740", "pdf": "https://arxiv.org/pdf/2506.02740.pdf", "abs": "https://arxiv.org/abs/2506.02740", "title": "Stereotypical gender actions can be extracted from Web text", "authors": ["Amaç Herdağdelen", "Marco Baroni"], "categories": ["cs.CL"], "comment": null, "summary": "We extracted gender-specific actions from text corpora and Twitter, and\ncompared them to stereotypical expectations of people. We used Open Mind Common\nSense (OMCS), a commonsense knowledge repository, to focus on actions that are\npertinent to common sense and daily life of humans. We use the gender\ninformation of Twitter users and Web-corpus-based pronoun/name gender\nheuristics to compute the gender bias of the actions. With high recall, we\nobtained a Spearman correlation of 0.47 between corpus-based predictions and a\nhuman gold standard, and an area under the ROC curve of 0.76 when predicting\nthe polarity of the gold standard. We conclude that it is feasible to use\nnatural text (and a Twitter-derived corpus in particular) in order to augment\ncommonsense repositories with the stereotypical gender expectations of actions.\nWe also present a dataset of 441 commonsense actions with human judges' ratings\non whether the action is typically/slightly masculine/feminine (or neutral),\nand another larger dataset of 21,442 actions automatically rated by the methods\nwe investigate in this study."}
{"id": "2506.02753", "pdf": "https://arxiv.org/pdf/2506.02753.pdf", "abs": "https://arxiv.org/abs/2506.02753", "title": "Multi-task Learning with Active Learning for Arabic Offensive Speech Detection", "authors": ["Aisha Alansari", "Hamzah Luqman"], "categories": ["cs.CL"], "comment": null, "summary": "The rapid growth of social media has amplified the spread of offensive,\nviolent, and vulgar speech, which poses serious societal and cybersecurity\nconcerns. Detecting such content in Arabic text is particularly complex due to\nlimited labeled data, dialectal variations, and the language's inherent\ncomplexity. This paper proposes a novel framework that integrates multi-task\nlearning (MTL) with active learning to enhance offensive speech detection in\nArabic social media text. By jointly training on two auxiliary tasks, violent\nand vulgar speech, the model leverages shared representations to improve the\ndetection accuracy of the offensive speech. Our approach dynamically adjusts\ntask weights during training to balance the contribution of each task and\noptimize performance. To address the scarcity of labeled data, we employ an\nactive learning strategy through several uncertainty sampling techniques to\niteratively select the most informative samples for model training. We also\nintroduce weighted emoji handling to better capture semantic cues. Experimental\nresults on the OSACT2022 dataset show that the proposed framework achieves a\nstate-of-the-art macro F1-score of 85.42%, outperforming existing methods while\nusing significantly fewer fine-tuning samples. The findings of this study\nhighlight the potential of integrating MTL with active learning for efficient\nand accurate offensive language detection in resource-constrained settings."}
{"id": "2506.02758", "pdf": "https://arxiv.org/pdf/2506.02758.pdf", "abs": "https://arxiv.org/abs/2506.02758", "title": "Exploiting the English Vocabulary Profile for L2 word-level vocabulary assessment with LLMs", "authors": ["Stefano Bannò", "Kate Knill", "Mark Gales"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to the 20th Workshop on Innovative Use of NLP for Building\n  Educational Applications", "summary": "Vocabulary use is a fundamental aspect of second language (L2) proficiency.\nTo date, its assessment by automated systems has typically examined the\ncontext-independent, or part-of-speech (PoS) related use of words. This paper\nintroduces a novel approach to enable fine-grained vocabulary evaluation\nexploiting the precise use of words within a sentence. The scheme combines\nlarge language models (LLMs) with the English Vocabulary Profile (EVP). The EVP\nis a standard lexical resource that enables in-context vocabulary use to be\nlinked with proficiency level. We evaluate the ability of LLMs to assign\nproficiency levels to individual words as they appear in L2 learner writing,\naddressing key challenges such as polysemy, contextual variation, and\nmulti-word expressions. We compare LLMs to a PoS-based baseline. LLMs appear to\nexploit additional semantic information that yields improved performance. We\nalso explore correlations between word-level proficiency and essay-level\nproficiency. Finally, the approach is applied to examine the consistency of the\nEVP proficiency levels. Results show that LLMs are well-suited for the task of\nvocabulary assessment."}
{"id": "2506.02803", "pdf": "https://arxiv.org/pdf/2506.02803.pdf", "abs": "https://arxiv.org/abs/2506.02803", "title": "SemVink: Advancing VLMs' Semantic Understanding of Optical Illusions via Visual Global Thinking", "authors": ["Sifan Li", "Yujun Cai", "Yiwei Wang"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) excel in semantic tasks but falter at a core\nhuman capability: detecting hidden content in optical illusions or AI-generated\nimages through perceptual adjustments like zooming. We introduce HC-Bench, a\nbenchmark of 112 images with hidden text, objects, and illusions, revealing\nthat leading VLMs achieve near-zero accuracy (0-5.36%)-even with explicit\nprompting. Humans resolve such ambiguities instinctively, yet VLMs fail due to\nan overreliance on high-level semantics. Strikingly, we propose SemVink\n(Semantic Visual Thinking) by simply scaling images to low resolutions (32-128\npixels), which unlocks >99% accuracy by eliminating redundant visual noise.\nThis exposes a critical architectural flaw: VLMs prioritize abstract reasoning\nover low-level visual operations crucial for real-world robustness. Our work\nurges a shift toward hybrid models integrating multi-scale processing, bridging\nthe gap between computational vision and human cognition for applications in\nmedical imaging, security, and beyond."}
{"id": "2506.02818", "pdf": "https://arxiv.org/pdf/2506.02818.pdf", "abs": "https://arxiv.org/abs/2506.02818", "title": "ProcrustesGPT: Compressing LLMs with Structured Matrices and Orthogonal Transformations", "authors": ["Ekaterina Grishina", "Mikhail Gorbunov", "Maxim Rakhuba"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted by ACL Findings", "summary": "Large language models (LLMs) demonstrate impressive results in natural\nlanguage processing tasks but require a significant amount of computational and\nmemory resources. Structured matrix representations are a promising way for\nreducing the number of parameters of these models. However, it seems\nunrealistic to expect that weight matrices of pretrained models can be\naccurately represented by structured matrices without any fine-tuning. To\novercome this issue, we utilize the fact that LLM output is invariant under\ncertain orthogonal transformations of weight matrices. This insight can be\nleveraged to identify transformations that significantly improve the\ncompressibility of weights within structured classes. The proposed approach is\napplicable to various types of structured matrices that support efficient\nprojection operations. Code is available at\nhttps://github.com/GrishKate/ProcrustesGPT"}
{"id": "2506.02827", "pdf": "https://arxiv.org/pdf/2506.02827.pdf", "abs": "https://arxiv.org/abs/2506.02827", "title": "TO-GATE: Clarifying Questions and Summarizing Responses with Trajectory Optimization for Eliciting Human Preference", "authors": ["Yulin Dou", "Jiangming Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) can effectively elicit human preferences through\nmulti-turn dialogue. Complex tasks can be accomplished through iterative\nclarifying questions and final responses generated by an LLM acting as a\nquestioner (STaR-GATE; Andukuri et al., 2024}). However, existing approaches\nbased on self-taught reasoning struggle to identify optimal dialogue\ntrajectories and avoid irrelevant questions to the tasks. To address this\nlimitation, we propose TO-GATE, a novel framework that enhances question\ngeneration through trajectory optimization, which consists of two key\ncomponents: a clarification resolver that generates optimal questioning\ntrajectories, and a summarizer that ensures task-aligned final responses. The\ntrajectory optimization enables the model to produce effective elicitation\nquestions and summary responses tailored to specific tasks. Experimental\nresults demonstrate that TO-GATE significantly outperforms baseline methods,\nachieving a 9.32% improvement on standard preference elicitation tasks."}
{"id": "2506.02872", "pdf": "https://arxiv.org/pdf/2506.02872.pdf", "abs": "https://arxiv.org/abs/2506.02872", "title": "Token and Span Classification for Entity Recognition in French Historical Encyclopedias", "authors": ["Ludovic Moncla", "Hédi Zeghidi"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Named Entity Recognition (NER) in historical texts presents unique challenges\ndue to non-standardized language, archaic orthography, and nested or\noverlapping entities. This study benchmarks a diverse set of NER approaches,\nranging from classical Conditional Random Fields (CRFs) and spaCy-based models\nto transformer-based architectures such as CamemBERT and sequence-labeling\nmodels like Flair. Experiments are conducted on the GeoEDdA dataset, a richly\nannotated corpus derived from 18th-century French encyclopedias. We propose\nframing NER as both token-level and span-level classification to accommodate\ncomplex nested entity structures typical of historical documents. Additionally,\nwe evaluate the emerging potential of few-shot prompting with generative\nlanguage models for low-resource scenarios. Our results demonstrate that while\ntransformer-based models achieve state-of-the-art performance, especially on\nnested entities, generative models offer promising alternatives when labeled\ndata are scarce. The study highlights ongoing challenges in historical NER and\nsuggests avenues for hybrid approaches combining symbolic and neural methods to\nbetter capture the intricacies of early modern French text."}
{"id": "2506.02878", "pdf": "https://arxiv.org/pdf/2506.02878.pdf", "abs": "https://arxiv.org/abs/2506.02878", "title": "CoT is Not True Reasoning, It Is Just a Tight Constraint to Imitate: A Theory Perspective", "authors": ["Jintian Shao", "Yiming Cheng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Chain-of-Thought (CoT) prompting has demonstrably enhanced the performance of\nLarge Language Models on tasks requiring multi-step inference. This success has\nled to widespread claims of emergent reasoning capabilities in these models. In\nthis paper, we present a theoretical counter-perspective: Chain-of-Thought\n(CoT) does not elicit genuine, abstract reasoning. Instead, we argue that\nChain-of-Thought functions as a powerful structural constraint that guides\nLarge Language Models to imitate the form of reasoning. By forcing the\ngeneration of intermediate steps, Chain-of-Thought leverages the model immense\ncapacity for sequence prediction and pattern matching, effectively constraining\nits output to sequences that resemble coherent thought processes.\nChain-of-Thought (CoT) prompting has demonstrably enhanced the performance of\nLarge Language Models on tasks requiring multi-step inference. This success has\nled to widespread claims of emergent reasoning capabilities in these models. In\nthis paper, we present a theoretical counter-perspective: Chain-of-Thought\n(CoT) does not elicit genuine, abstract reasoning. Instead, we argue that\nChain-of-Thought functions as a powerful structural constraint that guides\nLarge Language Models to imitate the form of reasoning. By forcing the\ngeneration of intermediate steps, Chain-of-Thought leverages the model immense\ncapacity for sequence prediction and pattern matching, effectively constraining\nits output to sequences that resemble coherent thought processes."}
{"id": "2506.02894", "pdf": "https://arxiv.org/pdf/2506.02894.pdf", "abs": "https://arxiv.org/abs/2506.02894", "title": "A Multi-Dialectal Dataset for German Dialect ASR and Dialect-to-Standard Speech Translation", "authors": ["Verena Blaschke", "Miriam Winkler", "Constantin Förster", "Gabriele Wenger-Glemser", "Barbara Plank"], "categories": ["cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Although Germany has a diverse landscape of dialects, they are\nunderrepresented in current automatic speech recognition (ASR) research. To\nenable studies of how robust models are towards dialectal variation, we present\nBetthupferl, an evaluation dataset containing four hours of read speech in\nthree dialect groups spoken in Southeast Germany (Franconian, Bavarian,\nAlemannic), and half an hour of Standard German speech. We provide both\ndialectal and Standard German transcriptions, and analyze the linguistic\ndifferences between them. We benchmark several multilingual state-of-the-art\nASR models on speech translation into Standard German, and find differences\nbetween how much the output resembles the dialectal vs. standardized\ntranscriptions. Qualitative error analyses of the best ASR model reveal that it\nsometimes normalizes grammatical differences, but often stays closer to the\ndialectal constructions."}
{"id": "2506.02899", "pdf": "https://arxiv.org/pdf/2506.02899.pdf", "abs": "https://arxiv.org/abs/2506.02899", "title": "IMPARA-GED: Grammatical Error Detection is Boosting Reference-free Grammatical Error Quality Estimator", "authors": ["Yusuke Sakai", "Takumi Goto", "Taro Watanabe"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings", "summary": "We propose IMPARA-GED, a novel reference-free automatic grammatical error\ncorrection (GEC) evaluation method with grammatical error detection (GED)\ncapabilities. We focus on the quality estimator of IMPARA, an existing\nautomatic GEC evaluation method, and construct that of IMPARA-GED using a\npre-trained language model with enhanced GED capabilities. Experimental results\non SEEDA, a meta-evaluation dataset for automatic GEC evaluation methods,\ndemonstrate that IMPARA-GED achieves the highest correlation with human\nsentence-level evaluations."}
{"id": "2506.02911", "pdf": "https://arxiv.org/pdf/2506.02911.pdf", "abs": "https://arxiv.org/abs/2506.02911", "title": "Cell-o1: Training LLMs to Solve Single-Cell Reasoning Puzzles with Reinforcement Learning", "authors": ["Yin Fang", "Qiao Jin", "Guangzhi Xiong", "Bowen Jin", "Xianrui Zhong", "Siru Ouyang", "Aidong Zhang", "Jiawei Han", "Zhiyong Lu"], "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.HC", "cs.LG"], "comment": "28 pages; 16 tables; 7 figures; Code:\n  https://github.com/ncbi-nlp/cell-o1", "summary": "Cell type annotation is a key task in analyzing the heterogeneity of\nsingle-cell RNA sequencing data. Although recent foundation models automate\nthis process, they typically annotate cells independently, without considering\nbatch-level cellular context or providing explanatory reasoning. In contrast,\nhuman experts often annotate distinct cell types for different cell clusters\nbased on their domain knowledge. To mimic this workflow, we introduce the\nCellPuzzles task, where the objective is to assign unique cell types to a batch\nof cells. This benchmark spans diverse tissues, diseases, and donor conditions,\nand requires reasoning across the batch-level cellular context to ensure label\nuniqueness. We find that off-the-shelf large language models (LLMs) struggle on\nCellPuzzles, with the best baseline (OpenAI's o1) achieving only 19.0%\nbatch-level accuracy. To fill this gap, we propose Cell-o1, a 7B LLM trained\nvia supervised fine-tuning on distilled reasoning traces, followed by\nreinforcement learning with batch-level rewards. Cell-o1 achieves\nstate-of-the-art performance, outperforming o1 by over 73% and generalizing\nwell across contexts. Further analysis of training dynamics and reasoning\nbehaviors provides insights into batch-level annotation performance and\nemergent expert-like reasoning. Code and data are available at\nhttps://github.com/ncbi-nlp/cell-o1."}
{"id": "2506.02921", "pdf": "https://arxiv.org/pdf/2506.02921.pdf", "abs": "https://arxiv.org/abs/2506.02921", "title": "A Controllable Examination for Long-Context Language Models", "authors": ["Yijun Yang", "Zeyu Huang", "Wenhao Zhu", "Zihan Qiu", "Fei Yuan", "Jeff Z. Pan", "Ivan Titov"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "Existing frameworks for evaluating long-context language models (LCLM) can be\nbroadly categorized into real-world and synthetic tasks. Despite their utility,\nboth approaches are accompanied by certain intrinsic limitations. Real-world\ntasks are too complex to interpret or characterize and are susceptible to data\ncontamination. In contrast, synthetic tasks often adopt the\nneedle-in-the-haystack (NIAH) format, wherein a lack of coherence between the\n\"needle\" and the \"haystack\" compromises their validity as proxies for realistic\napplications. In response to these challenges, we posit that an ideal\nlong-context evaluation framework should be characterized by three essential\nfeatures: $\\textit{seamless context}$, $\\textit{controllable setting}$, and\n$\\textit{sound evaluation}$. This study introduces $\\textbf{LongBioBench}$, a\nnovel benchmark that utilizes artificially generated biographies as a\ncontrolled environment for assessing LCLMs across dimensions of\n$\\textit{understanding}$, $\\textit{reasoning}$, and $\\textit{trustworthiness}$.\nOur experimental evaluation, which includes $\\textbf{18}$ LCLMs in total,\ndemonstrates that most models still exhibit deficiencies in semantic\nunderstanding and elementary reasoning over retrieved results and are less\ntrustworthy as context length increases. Our further analysis indicates some\ndesign choices employed by existing synthetic benchmarks, such as contextual\nnon-coherence, numerical needles, and the absence of distractors, rendering\nthem vulnerable to test the model long-context capabilities. Moreover, we also\nreveal that long-context continual pretraining primarily adjusts RoPE embedding\nto accommodate extended context lengths. To sum up, compared to previous\nsynthetic benchmarks, LongBioBench achieves a better trade-off between\nmirroring authentic language tasks and maintaining controllability, and is\nhighly interpretable and configurable."}
{"id": "2506.02924", "pdf": "https://arxiv.org/pdf/2506.02924.pdf", "abs": "https://arxiv.org/abs/2506.02924", "title": "INESC-ID @ eRisk 2025: Exploring Fine-Tuned, Similarity-Based, and Prompt-Based Approaches to Depression Symptom Identification", "authors": ["Diogo A. P. Nunes", "Eugénio Ribeiro"], "categories": ["cs.CL", "cs.IR", "cs.LG", "I.2.7; I.5.4; J.3; H.3.3"], "comment": "12 pages, 1 figure, 6 tables", "summary": "In this work, we describe our team's approach to eRisk's 2025 Task 1: Search\nfor Symptoms of Depression. Given a set of sentences and the Beck's Depression\nInventory - II (BDI) questionnaire, participants were tasked with submitting up\nto 1,000 sentences per depression symptom in the BDI, sorted by relevance.\nParticipant submissions were evaluated according to standard Information\nRetrieval (IR) metrics, including Average Precision (AP) and R-Precision\n(R-PREC). The provided training data, however, consisted of sentences labeled\nas to whether a given sentence was relevant or not w.r.t. one of BDI's\nsymptoms. Due to this labeling limitation, we framed our development as a\nbinary classification task for each BDI symptom, and evaluated accordingly. To\nthat end, we split the available labeled data into training and validation\nsets, and explored foundation model fine-tuning, sentence similarity, Large\nLanguage Model (LLM) prompting, and ensemble techniques. The validation results\nrevealed that fine-tuning foundation models yielded the best performance,\nparticularly when enhanced with synthetic data to mitigate class imbalance. We\nalso observed that the optimal approach varied by symptom. Based on these\ninsights, we devised five independent test runs, two of which used ensemble\nmethods. These runs achieved the highest scores in the official IR evaluation,\noutperforming submissions from 16 other teams."}
{"id": "2506.02945", "pdf": "https://arxiv.org/pdf/2506.02945.pdf", "abs": "https://arxiv.org/abs/2506.02945", "title": "Quantitative LLM Judges", "authors": ["Aishwarya Sahoo", "Jeevana Kruthi Karnuthala", "Tushar Parmanand Budhwani", "Pranchal Agarwal", "Sankaran Vaidyanathan", "Alexa Siu", "Franck Dernoncourt", "Jennifer Healey", "Nedim Lipka", "Ryan Rossi", "Uttaran Bhattacharya", "Branislav Kveton"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "LLM-as-a-judge is a framework in which a large language model (LLM)\nautomatically evaluates the output of another LLM. We propose quantitative LLM\njudges, which align evaluation scores of existing LLM judges to human scores in\na given domain using regression models. The models are trained to improve the\nscore of the original judge by using the judge's textual evaluation and score.\nWe present four quantitative judges for different types of absolute and\nrelative feedback, which showcases the generality and versatility of our\nframework. Our framework is more computationally efficient than supervised\nfine-tuning and can be more statistically efficient when human feedback is\nlimited, which is expected in most applications of our work. We validate these\nclaims empirically on four datasets using two base judges. Our experiments show\nthat quantitative judges can effectively improve the predictive power of\nexisting judges through post-hoc modeling."}
{"id": "2506.02951", "pdf": "https://arxiv.org/pdf/2506.02951.pdf", "abs": "https://arxiv.org/abs/2506.02951", "title": "Adaptive Graph Pruning for Multi-Agent Communication", "authors": ["Boyi Li", "Zhonghan Zhao", "Der-Horng Lee", "Gaoang Wang"], "categories": ["cs.CL", "cs.MA"], "comment": null, "summary": "Large Language Model (LLM) based multi-agent systems have shown remarkable\nperformance in various tasks, especially when enhanced through collaborative\ncommunication. However, current methods often rely on a fixed number of agents\nand static communication structures, limiting their ability to adapt to varying\ntask complexities. In this paper, we propose Adaptive Graph Pruning (AGP), a\nnovel task-adaptive multi-agent collaboration framework that jointly optimizes\nagent quantity (hard-pruning) and communication topology (soft-pruning).\nSpecifically, our method employs a two-stage training strategy: firstly,\nindependently training soft-pruning networks for different agent quantities to\ndetermine optimal agent-quantity-specific complete graphs and positional masks\nacross specific tasks; and then jointly optimizing hard-pruning and\nsoft-pruning within a maximum complete graph to dynamically configure the\nnumber of agents and their communication topologies per task. Extensive\nexperiments demonstrate that our approach is: (1) High-performing, achieving\nstate-of-the-art results across six benchmarks and consistently generalizes\nacross multiple mainstream LLM architectures, with a increase in performance of\n$2.58\\%\\sim 9.84\\%$; (2) Task-adaptive, dynamically constructing optimized\ncommunication topologies tailored to specific tasks, with an extremely high\nperformance in all three task categories (general reasoning, mathematical\nreasoning, and code generation); (3) Token-economical, having fewer training\nsteps and token consumption at the same time, with a decrease in token\nconsumption of $90\\%+$; and (4) Training-efficient, achieving high performance\nwith very few training steps compared with other methods. The performance will\nsurpass the existing baselines after about ten steps of training under six\nbenchmarks."}
{"id": "2506.02959", "pdf": "https://arxiv.org/pdf/2506.02959.pdf", "abs": "https://arxiv.org/abs/2506.02959", "title": "HACo-Det: A Study Towards Fine-Grained Machine-Generated Text Detection under Human-AI Coauthoring", "authors": ["Zhixiong Su", "Yichen Wang", "Herun Wan", "Zhaohan Zhang", "Minnan Luo"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The misuse of large language models (LLMs) poses potential risks, motivating\nthe development of machine-generated text (MGT) detection. Existing literature\nprimarily concentrates on binary, document-level detection, thereby neglecting\ntexts that are composed jointly by human and LLM contributions. Hence, this\npaper explores the possibility of fine-grained MGT detection under human-AI\ncoauthoring. We suggest fine-grained detectors can pave pathways toward\ncoauthored text detection with a numeric AI ratio. Specifically, we propose a\ndataset, HACo-Det, which produces human-AI coauthored texts via an automatic\npipeline with word-level attribution labels. We retrofit seven prevailing\ndocument-level detectors to generalize them to word-level detection. Then we\nevaluate these detectors on HACo-Det on both word- and sentence-level detection\ntasks. Empirical results show that metric-based methods struggle to conduct\nfine-grained detection with a 0.462 average F1 score, while finetuned models\nshow superior performance and better generalization across domains. However, we\nargue that fine-grained co-authored text detection is far from solved. We\nfurther analyze factors influencing performance, e.g., context window, and\nhighlight the limitations of current methods, pointing to potential avenues for\nimprovement."}
{"id": "2506.02961", "pdf": "https://arxiv.org/pdf/2506.02961.pdf", "abs": "https://arxiv.org/abs/2506.02961", "title": "FlowerTune: A Cross-Domain Benchmark for Federated Fine-Tuning of Large Language Models", "authors": ["Yan Gao", "Massimo Roberto Scamarcia", "Javier Fernandez-Marques", "Mohammad Naseri", "Chong Shen Ng", "Dimitris Stripelis", "Zexi Li", "Tao Shen", "Jiamu Bai", "Daoyuan Chen", "Zikai Zhang", "Rui Hu", "InSeo Song", "Lee KangYoon", "Hong Jia", "Ting Dang", "Junyan Wang", "Zheyuan Liu", "Daniel Janes Beutel", "Lingjuan Lyu", "Nicholas D. Lane"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved state-of-the-art results across\ndiverse domains, yet their development remains reliant on vast amounts of\npublicly available data, raising concerns about data scarcity and the lack of\naccess to domain-specific, sensitive information. Federated Learning (FL)\npresents a compelling framework to address these challenges by enabling\ndecentralized fine-tuning on pre-trained LLMs without sharing raw data.\nHowever, the compatibility and performance of pre-trained LLMs in FL settings\nremain largely under explored. We introduce the FlowerTune LLM Leaderboard, a\nfirst-of-its-kind benchmarking suite designed to evaluate federated fine-tuning\nof LLMs across four diverse domains: general NLP, finance, medical, and coding.\nEach domain includes federated instruction-tuning datasets and domain-specific\nevaluation metrics. Our results, obtained through a collaborative, open-source\nand community-driven approach, provide the first comprehensive comparison\nacross 26 pre-trained LLMs with different aggregation and fine-tuning\nstrategies under federated settings, offering actionable insights into model\nperformance, resource constraints, and domain adaptation. This work lays the\nfoundation for developing privacy-preserving, domain-specialized LLMs for\nreal-world applications."}
{"id": "2506.02973", "pdf": "https://arxiv.org/pdf/2506.02973.pdf", "abs": "https://arxiv.org/abs/2506.02973", "title": "Expanding before Inferring: Enhancing Factuality in Large Language Models through Premature Layers Interpolation", "authors": ["Dingwei Chen", "Ziqiang Liu", "Feiteng Fang", "Chak Tou Leong", "Shiwen Ni", "Ahmadreza Argha", "Hamid Alinejad-Rokny", "Min Yang", "Chengming Li"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) demonstrate remarkable capabilities in text\nunderstanding and generation. However, their tendency to produce factually\ninconsistent outputs, commonly referred to as ''hallucinations'', remains a\ncritical challenge. Existing approaches, such as retrieval-based and\ninference-time correction methods, primarily address this issue at the input or\noutput level, often overlooking the intrinsic information refinement process\nand the role of premature layers. Meanwhile, alignment- and fine-tuning-based\nmethods are resource-intensive. In this paper, we propose PLI (Premature Layers\nInterpolation), a novel, training-free, and plug-and-play intervention designed\nto enhance factuality. PLI mitigates hallucinations by inserting premature\nlayers formed through mathematical interpolation with adjacent layers. Inspired\nby stable diffusion and sampling steps, PLI extends the depth of information\nprocessing and transmission in LLMs, improving factual coherence. Experiments\non four publicly available datasets demonstrate that PLI effectively reduces\nhallucinations while outperforming existing baselines in most cases. Further\nanalysis suggests that the success of layer interpolation is closely linked to\nLLMs' internal mechanisms. To promote reproducibility, we will release our code\nand data upon acceptance."}
{"id": "2506.02979", "pdf": "https://arxiv.org/pdf/2506.02979.pdf", "abs": "https://arxiv.org/abs/2506.02979", "title": "Towards a Japanese Full-duplex Spoken Dialogue System", "authors": ["Atsumoto Ohashi", "Shinya Iizuka", "Jingjing Jiang", "Ryuichiro Higashinaka"], "categories": ["cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Full-duplex spoken dialogue systems, which can model simultaneous\nbidirectional features of human conversations such as speech overlaps and\nbackchannels, have attracted significant attention recently. However, the study\nof full-duplex spoken dialogue systems for the Japanese language has been\nlimited, and the research on their development in Japanese remains scarce. In\nthis paper, we present the first publicly available full-duplex spoken dialogue\nmodel in Japanese, which is built upon Moshi, a full-duplex dialogue model in\nEnglish. Our model is trained through a two-stage process: pre-training on a\nlarge-scale spoken dialogue data in Japanese, followed by fine-tuning on\nhigh-quality stereo spoken dialogue data. We further enhance the model's\nperformance by incorporating synthetic dialogue data generated by a\nmulti-stream text-to-speech system. Evaluation experiments demonstrate that the\ntrained model outperforms Japanese baseline models in both naturalness and\nmeaningfulness."}
{"id": "2506.02987", "pdf": "https://arxiv.org/pdf/2506.02987.pdf", "abs": "https://arxiv.org/abs/2506.02987", "title": "Performance of leading large language models in May 2025 in Membership of the Royal College of General Practitioners-style examination questions: a cross-sectional analysis", "authors": ["Richard Armitage"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "12 pages, 1 Table", "summary": "Background: Large language models (LLMs) have demonstrated substantial\npotential to support clinical practice. Other than Chat GPT4 and its\npredecessors, few LLMs, especially those of the leading and more powerful\nreasoning model class, have been subjected to medical specialty examination\nquestions, including in the domain of primary care. This paper aimed to test\nthe capabilities of leading LLMs as of May 2025 (o3, Claude Opus 4, Grok3, and\nGemini 2.5 Pro) in primary care education, specifically in answering Member of\nthe Royal College of General Practitioners (MRCGP) style examination questions.\n  Methods: o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro were tasked to answer\n100 randomly chosen multiple choice questions from the Royal College of General\nPractitioners GP SelfTest on 25 May 2025. Questions included textual\ninformation, laboratory results, and clinical images. Each model was prompted\nto answer as a GP in the UK and was provided with full question information.\nEach question was attempted once by each model. Responses were scored against\ncorrect answers provided by GP SelfTest.\n  Results: The total score of o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro was\n99.0%, 95.0%, 95.0%, and 95.0%, respectively. The average peer score for the\nsame questions was 73.0%.\n  Discussion: All models performed remarkably well, and all substantially\nexceeded the average performance of GPs and GP registrars who had answered the\nsame questions. o3 demonstrated the best performance, while the performances of\nthe other leading models were comparable with each other and were not\nsubstantially lower than that of o3. These findings strengthen the case for\nLLMs, particularly reasoning models, to support the delivery of primary care,\nespecially those that have been specifically trained on primary care clinical\ndata."}
{"id": "2506.02995", "pdf": "https://arxiv.org/pdf/2506.02995.pdf", "abs": "https://arxiv.org/abs/2506.02995", "title": "It's Not a Walk in the Park! Challenges of Idiom Translation in Speech-to-text Systems", "authors": ["Iuliia Zaitova", "Badr M. Abdullah", "Wei Xue", "Dietrich Klakow", "Bernd Möbius", "Tania Avgustinova"], "categories": ["cs.CL"], "comment": "13 pages, 3 figures, ACL 2025", "summary": "Idioms are defined as a group of words with a figurative meaning not\ndeducible from their individual components. Although modern machine translation\nsystems have made remarkable progress, translating idioms remains a major\nchallenge, especially for speech-to-text systems, where research on this topic\nis notably sparse. In this paper, we systematically evaluate idiom translation\nas compared to conventional news translation in both text-to-text machine\ntranslation (MT) and speech-to-text translation (SLT) systems across two\nlanguage pairs (German to English, Russian to English). We compare\nstate-of-the-art end-to-end SLT systems (SeamlessM4T SLT-to-text, Whisper Large\nv3) with MT systems (SeamlessM4T SLT-to-text, No Language Left Behind), Large\nLanguage Models (DeepSeek, LLaMA) and cascaded alternatives. Our results reveal\nthat SLT systems experience a pronounced performance drop on idiomatic data,\noften reverting to literal translations even in higher layers, whereas MT\nsystems and Large Language Models demonstrate better handling of idioms. These\nfindings underscore the need for idiom-specific strategies and improved\ninternal representations in SLT architectures."}
{"id": "2506.02998", "pdf": "https://arxiv.org/pdf/2506.02998.pdf", "abs": "https://arxiv.org/abs/2506.02998", "title": "A Multi-Agent Framework for Mitigating Dialect Biases in Privacy Policy Question-Answering Systems", "authors": ["Đorđe Klisura", "Astrid R Bernaga Torres", "Anna Karen Gárate-Escamilla", "Rajesh Roshan Biswal", "Ke Yang", "Hilal Pataci", "Anthony Rios"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Privacy policies inform users about data collection and usage, yet their\ncomplexity limits accessibility for diverse populations. Existing Privacy\nPolicy Question Answering (QA) systems exhibit performance disparities across\nEnglish dialects, disadvantaging speakers of non-standard varieties. We propose\na novel multi-agent framework inspired by human-centered design principles to\nmitigate dialectal biases. Our approach integrates a Dialect Agent, which\ntranslates queries into Standard American English (SAE) while preserving\ndialectal intent, and a Privacy Policy Agent, which refines predictions using\ndomain expertise. Unlike prior approaches, our method does not require\nretraining or dialect-specific fine-tuning, making it broadly applicable across\nmodels and domains. Evaluated on PrivacyQA and PolicyQA, our framework improves\nGPT-4o-mini's zero-shot accuracy from 0.394 to 0.601 on PrivacyQA and from\n0.352 to 0.464 on PolicyQA, surpassing or matching few-shot baselines without\nadditional training data. These results highlight the effectiveness of\nstructured agent collaboration in mitigating dialect biases and underscore the\nimportance of designing NLP systems that account for linguistic diversity to\nensure equitable access to privacy information."}
{"id": "2506.03009", "pdf": "https://arxiv.org/pdf/2506.03009.pdf", "abs": "https://arxiv.org/abs/2506.03009", "title": "Conditioning Large Language Models on Legal Systems? Detecting Punishable Hate Speech", "authors": ["Florian Ludwig", "Torsten Zesch", "Frederike Zufall"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The assessment of legal problems requires the consideration of a specific\nlegal system and its levels of abstraction, from constitutional law to\nstatutory law to case law. The extent to which Large Language Models (LLMs)\ninternalize such legal systems is unknown. In this paper, we propose and\ninvestigate different approaches to condition LLMs at different levels of\nabstraction in legal systems. This paper examines different approaches to\nconditioning LLMs at multiple levels of abstraction in legal systems to detect\npotentially punishable hate speech. We focus on the task of classifying whether\na specific social media posts falls under the criminal offense of incitement to\nhatred as prescribed by the German Criminal Code. The results show that there\nis still a significant performance gap between models and legal experts in the\nlegal assessment of hate speech, regardless of the level of abstraction with\nwhich the models were conditioned. Our analysis revealed, that models\nconditioned on abstract legal knowledge lacked deep task understanding, often\ncontradicting themselves and hallucinating answers, while models using concrete\nlegal knowledge performed reasonably well in identifying relevant target\ngroups, but struggled with classifying target conducts."}
{"id": "2506.03011", "pdf": "https://arxiv.org/pdf/2506.03011.pdf", "abs": "https://arxiv.org/abs/2506.03011", "title": "Coding Agents with Multimodal Browsing are Generalist Problem Solvers", "authors": ["Aditya Bharat Soni", "Boxuan Li", "Xingyao Wang", "Valerie Chen", "Graham Neubig"], "categories": ["cs.CL"], "comment": null, "summary": "Modern human labor is characterized by specialization; we train for years and\ndevelop particular tools that allow us to perform well across a variety of\ntasks. In addition, AI agents have been specialized for domains such as\nsoftware engineering, web navigation, and workflow automation. However, this\nresults in agents that are good for one thing but fail to generalize beyond\ntheir intended scope. One reason for this is that agent developers provide a\nhighly specialized set of tools or make architectural decisions optimized for a\nspecific use case or benchmark. In this work, we ask the question: what is the\nminimal set of general tools that can be used to achieve high performance\nacross a diverse set of tasks? Our answer is OpenHands-Versa, a generalist\nagent built with a modest number of general tools: code editing and execution,\nweb search, as well as multimodal web browsing and file access. Importantly,\nOpenHands-Versa demonstrates superior or competitive performance over leading\nspecialized agents across three diverse and challenging benchmarks: SWE-Bench\nMultimodal, GAIA, and The Agent Company, outperforming the best-performing\npreviously published results with absolute improvements in success rate of 9.1,\n1.3, and 9.1 points respectively. Further, we show how existing\nstate-of-the-art multi-agent systems fail to generalize beyond their target\ndomains. These results demonstrate the feasibility of developing a generalist\nagent to solve diverse tasks and establish OpenHands-Versa as a strong baseline\nfor future research."}
{"id": "2506.03035", "pdf": "https://arxiv.org/pdf/2506.03035.pdf", "abs": "https://arxiv.org/abs/2506.03035", "title": "Leveraging Information Retrieval to Enhance Spoken Language Understanding Prompts in Few-Shot Learning", "authors": ["Pierre Lepagnol", "Sahar Ghannay", "Thomas Gerald", "Christophe Servan", "Sophie Rosset"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Conference paper accepted to INTERSPEECH 2025", "summary": "Understanding user queries is fundamental in many applications, such as home\nassistants, booking systems, or recommendations. Accordingly, it is crucial to\ndevelop accurate Spoken Language Understanding (SLU) approaches to ensure the\nreliability of the considered system. Current State-of-the-Art SLU techniques\nrely on large amounts of training data; however, only limited annotated\nexamples are available for specific tasks or languages.\n  In the meantime, instruction-tuned large language models (LLMs) have shown\nexceptional performance on unseen tasks in a few-shot setting when provided\nwith adequate prompts. In this work, we propose to explore example selection by\nleveraging Information retrieval (IR) approaches to build an enhanced prompt\nthat is applied to an SLU task. We evaluate the effectiveness of the proposed\nmethod on several SLU benchmarks. Experimental results show that lexical IR\nmethods significantly enhance performance without increasing prompt length."}
{"id": "2506.03038", "pdf": "https://arxiv.org/pdf/2506.03038.pdf", "abs": "https://arxiv.org/abs/2506.03038", "title": "Towards Analyzing and Understanding the Limitations of VAPO: A Theoretical Perspective", "authors": ["Jintian Shao", "Yiming Cheng"], "categories": ["cs.CL"], "comment": null, "summary": "Reinforcement learning (RL) enhances large language models (LLMs) in complex,\nlong-chain-of-thought (long-CoT) reasoning. The advanced VAPO framework,\ndespite sophisticated mechanisms like Decoupled GAE, theoretically faces\nfundamental limitations in comprehensively modeling and leveraging deep,\nlong-term value for fine-grained, step-by-step policy guidance in extended\nreasoning chains. We argue these limitations stem from inherent difficulties in\ncredit assignment, value function representational capacity with temporally\nabstracted goals, and translating global value signals into local policy\nimprovements, especially with sparse rewards. Our theoretical analysis examines\nthese aspects to illuminate VAPO's boundaries in long-term value modeling,\naiming to deepen understanding of current RL for advanced reasoning and suggest\nfuture research for more robust LLM agents."}
{"id": "2506.03051", "pdf": "https://arxiv.org/pdf/2506.03051.pdf", "abs": "https://arxiv.org/abs/2506.03051", "title": "Facts Do Care About Your Language: Assessing Answer Quality of Multilingual LLMs", "authors": ["Yuval Kansal", "Shmuel Berman", "Lydia Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Factuality is a necessary precursor to useful educational tools. As adoption\nof Large Language Models (LLMs) in education continues of grow, ensuring\ncorrectness in all settings is paramount. Despite their strong English\ncapabilities, LLM performance in other languages is largely untested. In this\nwork, we evaluate the correctness of the Llama3.1 family of models in answering\nfactual questions appropriate for middle and high school students. We\ndemonstrate that LLMs not only provide extraneous and less truthful\ninformation, but also exacerbate existing biases against rare languages."}
{"id": "2506.03090", "pdf": "https://arxiv.org/pdf/2506.03090.pdf", "abs": "https://arxiv.org/abs/2506.03090", "title": "Literary Evidence Retrieval via Long-Context Language Models", "authors": ["Katherine Thai", "Mohit Iyyer"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "How well do modern long-context language models understand literary fiction?\nWe explore this question via the task of literary evidence retrieval,\nrepurposing the RELiC dataset of That et al. (2022) to construct a benchmark\nwhere the entire text of a primary source (e.g., The Great Gatsby) is provided\nto an LLM alongside literary criticism with a missing quotation from that work.\nThis setting, in which the model must generate the missing quotation, mirrors\nthe human process of literary analysis by requiring models to perform both\nglobal narrative reasoning and close textual examination. We curate a\nhigh-quality subset of 292 examples through extensive filtering and human\nverification. Our experiments show that recent reasoning models, such as Gemini\nPro 2.5 can exceed human expert performance (62.5% vs. 50% accuracy). In\ncontrast, the best open-weight model achieves only 29.1% accuracy, highlighting\na wide gap in interpretive reasoning between open and closed-weight models.\nDespite their speed and apparent accuracy, even the strongest models struggle\nwith nuanced literary signals and overgeneration, signaling open challenges for\napplying LLMs to literary analysis. We release our dataset and evaluation code\nto encourage future work in this direction."}
{"id": "2506.03101", "pdf": "https://arxiv.org/pdf/2506.03101.pdf", "abs": "https://arxiv.org/abs/2506.03101", "title": "Beyond Text Compression: Evaluating Tokenizers Across Scales", "authors": ["Jonas F. Lotz", "António V. Lopes", "Stephan Peitz", "Hendra Setiawan", "Leonardo Emili"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "The choice of tokenizer can profoundly impact language model performance, yet\naccessible and reliable evaluations of tokenizer quality remain an open\nchallenge. Inspired by scaling consistency, we show that smaller models can\naccurately predict significant differences in tokenizer impact on larger models\nat a fraction of the compute cost. By systematically evaluating both\nEnglish-centric and multilingual tokenizers, we find that tokenizer choice has\nnegligible effects on tasks in English but results in consistent performance\ndifferences in multilingual settings. We propose new intrinsic tokenizer\nmetrics inspired by Zipf's law that correlate more strongly with downstream\nperformance than text compression when modeling unseen languages. By combining\nseveral metrics to capture multiple aspects of tokenizer behavior, we develop a\nreliable framework for intrinsic tokenizer evaluations. Our work offers a more\nefficient path to informed tokenizer selection in future language model\ndevelopment."}
{"id": "2506.03106", "pdf": "https://arxiv.org/pdf/2506.03106.pdf", "abs": "https://arxiv.org/abs/2506.03106", "title": "Critique-GRPO: Advancing LLM Reasoning with Natural Language and Numerical Feedback", "authors": ["Xiaoying Zhang", "Hao Sun", "Yipeng Zhang", "Kaituo Feng", "Chao Yang", "Helen Meng"], "categories": ["cs.CL", "cs.AI"], "comment": "38 pages", "summary": "Recent advances in reinforcement learning (RL) with numerical feedback, such\nas scalar rewards, have significantly enhanced the complex reasoning\ncapabilities of large language models (LLMs). Despite this success, we identify\nthree key challenges encountered by RL with solely numerical feedback:\nperformance plateaus, limited effectiveness of self-reflection, and persistent\nfailures. We then demonstrate that RL-finetuned models, even after exhibiting\nperformance plateaus, can generate correct refinements on persistently failed\nproblems by leveraging natural language feedback in the form of critiques.\nBuilding on this insight, we propose Critique-GRPO, an online RL framework that\nintegrates both natural language and numerical feedback for effective policy\noptimization. Critique-GRPO enables LLMs to learn from initial responses and\ncritique-guided refinements simultaneously while maintaining exploration.\nExtensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that\nCritique-GRPO consistently outperforms supervised learning-based and RL-based\nfine-tuning approaches across eight challenging mathematical, STEM, and general\nreasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%,\nrespectively. Notably, Critique-GRPO surpasses a strong baseline that\nincorporates expert demonstrations within online RL. Further analysis reveals\ntwo critical insights about policy exploration: (1) higher entropy does not\nalways guarantee efficient learning from exploration, and (2) longer responses\ndo not necessarily lead to more effective exploration."}
{"id": "2506.03122", "pdf": "https://arxiv.org/pdf/2506.03122.pdf", "abs": "https://arxiv.org/abs/2506.03122", "title": "AUTOCIRCUIT-RL: Reinforcement Learning-Driven LLM for Automated Circuit Topology Generation", "authors": ["Prashanth Vijayaraghavan", "Luyao Shi", "Ehsan Degan", "Vandana Mukherjee", "Xin Zhang"], "categories": ["cs.CL"], "comment": "9 Pages (Content), 4 Pages (Appendix), 7 figures, ICML'2025", "summary": "Analog circuit topology synthesis is integral to Electronic Design Automation\n(EDA), enabling the automated creation of circuit structures tailored to\nspecific design requirements. However, the vast design search space and strict\nconstraint adherence make efficient synthesis challenging. Leveraging the\nversatility of Large Language Models (LLMs), we propose AUTOCIRCUIT-RL,a novel\nreinforcement learning (RL)-based framework for automated analog circuit\nsynthesis. The framework operates in two phases: instruction tuning, where an\nLLM learns to generate circuit topologies from structured prompts encoding\ndesign constraints, and RL refinement, which further improves the\ninstruction-tuned model using reward models that evaluate validity, efficiency,\nand output voltage. The refined model is then used directly to generate\ntopologies that satisfy the design constraints. Empirical results show that\nAUTOCIRCUIT-RL generates ~12% more valid circuits and improves efficiency by\n~14% compared to the best baselines, while reducing duplicate generation rates\nby ~38%. It achieves over 60% success in synthesizing valid circuits with\nlimited training data, demonstrating strong generalization. These findings\nhighlight the framework's effectiveness in scaling to complex circuits while\nmaintaining efficiency and constraint adherence, marking a significant\nadvancement in AI-driven circuit design."}
{"id": "2506.03136", "pdf": "https://arxiv.org/pdf/2506.03136.pdf", "abs": "https://arxiv.org/abs/2506.03136", "title": "Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning", "authors": ["Yinjie Wang", "Ling Yang", "Ye Tian", "Ke Shen", "Mengdi Wang"], "categories": ["cs.CL"], "comment": "Project: https://github.com/Gen-Verse/CURE", "summary": "We propose CURE, a novel reinforcement learning framework with a dedicated\nreward design that co-evolves coding and unit test generation capabilities\nbased on their interaction outcomes, without any ground-truth code as\nsupervision. This approach enables flexible and scalable training and allows\nthe unit tester to learn directly from the coder's mistakes. Our derived\nReasonFlux-Coder-7B and 14B models improve code generation accuracy by 5.3% and\nBest-of-N accuracy by 9.0% after optimization on Qwen2.5-Instruct models,\noutperforming similarly sized Qwen-Coder, DeepSeek-Coder, and Seed-Coder. They\nnaturally extend to downstream tasks such as test-time scaling and agentic\ncoding-achieving a 8.1% improvement over the base model. For the long-CoT\nmodel, our ReasonFlux-Coder-4B consistently outperforms Qwen3-4B while\nachieving 64.8% inference efficiency in unit test generation. Notably, we also\nfind that our model can serve as an effective reward model for reinforcement\nlearning on base models. Project: https://github.com/Gen-Verse/CURE"}
{"id": "2506.03143", "pdf": "https://arxiv.org/pdf/2506.03143.pdf", "abs": "https://arxiv.org/abs/2506.03143", "title": "GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents", "authors": ["Qianhui Wu", "Kanzhi Cheng", "Rui Yang", "Chaoyun Zhang", "Jianwei Yang", "Huiqiang Jiang", "Jian Mu", "Baolin Peng", "Bo Qiao", "Reuben Tan", "Si Qin", "Lars Liden", "Qingwei Lin", "Huan Zhang", "Tong Zhang", "Jianbing Zhang", "Dongmei Zhang", "Jianfeng Gao"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "One of the principal challenges in building VLM-powered GUI agents is visual\ngrounding, i.e., localizing the appropriate screen region for action execution\nbased on both the visual content and the textual plans. Most existing work\nformulates this as a text-based coordinate generation task. However, these\napproaches suffer from several limitations: weak spatial-semantic alignment,\ninability to handle ambiguous supervision targets, and a mismatch between the\ndense nature of screen coordinates and the coarse, patch-level granularity of\nvisual features extracted by models like Vision Transformers. In this paper, we\npropose GUI-Actor, a VLM-based method for coordinate-free GUI grounding. At its\ncore, GUI-Actor introduces an attention-based action head that learns to align\na dedicated <ACTOR> token with all relevant visual patch tokens, enabling the\nmodel to propose one or more action regions in a single forward pass. In line\nwith this, we further design a grounding verifier to evaluate and select the\nmost plausible action region from the candidates proposed for action execution.\nExtensive experiments show that GUI-Actor outperforms prior state-of-the-art\nmethods on multiple GUI action grounding benchmarks, with improved\ngeneralization to unseen screen resolutions and layouts. Notably, GUI-Actor-7B\neven surpasses UI-TARS-72B (38.1) on ScreenSpot-Pro, achieving scores of 40.7\nwith Qwen2-VL and 44.6 with Qwen2.5-VL as backbones. Furthermore, by\nincorporating the verifier, we find that fine-tuning only the newly introduced\naction head (~100M parameters for 7B model) while keeping the VLM backbone\nfrozen is sufficient to achieve performance comparable to previous\nstate-of-the-art models, highlighting that GUI-Actor can endow the underlying\nVLM with effective grounding capabilities without compromising its\ngeneral-purpose strengths."}
{"id": "2506.03145", "pdf": "https://arxiv.org/pdf/2506.03145.pdf", "abs": "https://arxiv.org/abs/2506.03145", "title": "Entity-Augmented Neuroscience Knowledge Retrieval Using Ontology and Semantic Understanding Capability of LLM", "authors": ["Pralaypati Ta", "Sriram Venkatesaperumal", "Keerthi Ram", "Mohanasankar Sivaprakasam"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Neuroscience research publications encompass a vast wealth of knowledge.\nAccurately retrieving existing information and discovering new insights from\nthis extensive literature is essential for advancing the field. However, when\nknowledge is dispersed across multiple sources, current state-of-the-art\nretrieval methods often struggle to extract the necessary information. A\nknowledge graph (KG) can integrate and link knowledge from multiple sources,\nbut existing methods for constructing KGs in neuroscience often rely on labeled\ndata and require domain expertise. Acquiring large-scale, labeled data for a\nspecialized area like neuroscience presents significant challenges. This work\nproposes novel methods for constructing KG from unlabeled large-scale\nneuroscience research corpus utilizing large language models (LLM),\nneuroscience ontology, and text embeddings. We analyze the semantic relevance\nof neuroscience text segments identified by LLM for building the knowledge\ngraph. We also introduce an entity-augmented information retrieval algorithm to\nextract knowledge from the KG. Several experiments were conducted to evaluate\nthe proposed approaches, and the results demonstrate that our methods\nsignificantly enhance knowledge discovery from the unlabeled neuroscience\nresearch corpus. It achieves an F1 score of 0.84 for entity extraction, and the\nknowledge obtained from the KG improves answers to over 54% of the questions."}
{"id": "2506.03149", "pdf": "https://arxiv.org/pdf/2506.03149.pdf", "abs": "https://arxiv.org/abs/2506.03149", "title": "Causal Estimation of Tokenisation Bias", "authors": ["Pietro Lesci", "Clara Meister", "Thomas Hofmann", "Andreas Vlachos", "Tiago Pimentel"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Published as a conference paper at ACL 2025", "summary": "Modern language models are typically trained over subword sequences, but\nultimately define probabilities over character-strings. Ideally, the choice of\nthe tokeniser -- which maps character-strings to subwords -- should not affect\nthe probability assigned to the underlying character-string; in practice, it\ndoes. We define this mismatch as tokenisation bias. In this work, we quantify\none particular type of tokenisation bias: the effect of including or not a\nsubword (e.g., $\\langle hello \\rangle$) in a tokeniser's vocabulary on the\nprobability a trained model assigns to the corresponding characters (i.e.,\n\\textit{``hello''}). Estimating this effect is challenging because each model\nis trained with only one tokeniser. We address this by framing tokenisation\nbias as a causal effect and estimating it using the regression discontinuity\ndesign. Specifically, we exploit the fact that tokenisation algorithms rank\nsubwords and add the first $K$ to a tokeniser's vocabulary, where $K$ is an\narbitrary cutoff point. As such, we can estimate a causal effect by comparing\nsimilar subwords around this cutoff. Experimentally, we find that tokenisation\nconsistently affects models' outputs across scales, vocabularies, and\ntokenisers. Notably, a subword's presence in a small model's vocabulary may\nincrease its characters' probability by up to 17 times, highlighting\ntokenisation as a key design choice in language modelling."}
{"id": "2506.01704", "pdf": "https://arxiv.org/pdf/2506.01704.pdf", "abs": "https://arxiv.org/abs/2506.01704", "title": "Generate, Not Recommend: Personalized Multimodal Content Generation", "authors": ["Jiongnan Liu", "Zhicheng Dou", "Ning Hu", "Chenyan Xiong"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "To address the challenge of information overload from massive web contents,\nrecommender systems are widely applied to retrieve and present personalized\nresults for users. However, recommendation tasks are inherently constrained to\nfiltering existing items and lack the ability to generate novel concepts,\nlimiting their capacity to fully satisfy user demands and preferences. In this\npaper, we propose a new paradigm that goes beyond content filtering and\nselecting: directly generating personalized items in a multimodal form, such as\nimages, tailored to individual users. To accomplish this, we leverage\nany-to-any Large Multimodal Models (LMMs) and train them in both supervised\nfine-tuning and online reinforcement learning strategy to equip them with the\nability to yield tailored next items for users. Experiments on two benchmark\ndatasets and user study confirm the efficacy of the proposed method. Notably,\nthe generated images not only align well with users' historical preferences but\nalso exhibit relevance to their potential future interests."}
{"id": "2506.01963", "pdf": "https://arxiv.org/pdf/2506.01963.pdf", "abs": "https://arxiv.org/abs/2506.01963", "title": "Breaking Quadratic Barriers: A Non-Attention LLM for Ultra-Long Context Horizons", "authors": ["Andrew Kiruluta", "Preethi Raju", "Priscilla Burity"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "We present a novel non attention based architecture for large language models\n(LLMs) that efficiently handles very long context windows, on the order of\nhundreds of thousands to potentially millions of tokens. Unlike traditional\nTransformer designs, which suffer from quadratic memory and computation\noverload due to the nature of the self attention mechanism, our model avoids\ntoken to token attention entirely. Instead, it combines the following\ncomplementary components: State Space blocks (inspired by S4) that learn\ncontinuous time convolution kernels and scale near linearly with sequence\nlength, Multi Resolution Convolution layers that capture local context at\ndifferent dilation levels, a lightweight Recurrent Supervisor to maintain a\nglobal hidden state across sequential chunks, and Retrieval Augmented External\nMemory that stores and retrieves high-level chunk embeddings without\nreintroducing quadratic operations."}
{"id": "2506.01967", "pdf": "https://arxiv.org/pdf/2506.01967.pdf", "abs": "https://arxiv.org/abs/2506.01967", "title": "Turning LLM Activations Quantization-Friendly", "authors": ["Patrik Czakó", "Gábor Kertész", "Sándor Szénási"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "6 pages, 5 figures. Accepted to SACI 2025 conference proceedings", "summary": "Quantization effectively reduces the serving costs of Large Language Models\n(LLMs) by speeding up data movement through compressed parameters and enabling\nfaster operations via integer arithmetic. However, activating integer\narithmetic requires quantizing both weights and activations, which poses\nchallenges due to the significant outliers in LLMs that increase quantization\nerror. In this work, we investigate these outliers with an emphasis on their\neffect on layer-wise quantization error, then examine how smoothing and\nrotation transform the observed values. Our primary contributions include\nintroducing a new metric to measure and visualize quantization difficulty based\non channel magnitudes, as well as proposing a hybrid approach that applies\nchannel-wise scaling before rotation, supported by a mathematical formulation\nof its benefits."}
{"id": "2506.01998", "pdf": "https://arxiv.org/pdf/2506.01998.pdf", "abs": "https://arxiv.org/abs/2506.01998", "title": "Inter(sectional) Alia(s): Ambiguity in Voice Agent Identity via Intersectional Japanese Self-Referents", "authors": ["Takao Fujii", "Katie Seaborn", "Madeleine Steeds", "Jun Kato"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY", "cs.SD", "eess.AS"], "comment": "CHI '25", "summary": "Conversational agents that mimic people have raised questions about the\nethics of anthropomorphizing machines with human social identity cues. Critics\nhave also questioned assumptions of identity neutrality in humanlike agents.\nRecent work has revealed that intersectional Japanese pronouns can elicit\ncomplex and sometimes evasive impressions of agent identity. Yet, the role of\nother \"neutral\" non-pronominal self-referents (NPSR) and voice as a socially\nexpressive medium remains unexplored. In a crowdsourcing study, Japanese\nparticipants (N = 204) evaluated three ChatGPT voices (Juniper, Breeze, and\nEmber) using seven self-referents. We found strong evidence of voice gendering\nalongside the potential of intersectional self-referents to evade gendering,\ni.e., ambiguity through neutrality and elusiveness. Notably, perceptions of age\nand formality intersected with gendering as per sociolinguistic theories,\nespecially boku and watakushi. This work provides a nuanced take on agent\nidentity perceptions and champions intersectional and culturally-sensitive work\non voice agents."}
{"id": "2506.02057", "pdf": "https://arxiv.org/pdf/2506.02057.pdf", "abs": "https://arxiv.org/abs/2506.02057", "title": "Enhancing Speech Instruction Understanding and Disambiguation in Robotics via Speech Prosody", "authors": ["David Sasu", "Kweku Andoh Yamoah", "Benedict Quartey", "Natalie Schluter"], "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted to Interspeech 2025", "summary": "Enabling robots to accurately interpret and execute spoken language\ninstructions is essential for effective human-robot collaboration. Traditional\nmethods rely on speech recognition to transcribe speech into text, often\ndiscarding crucial prosodic cues needed for disambiguating intent. We propose a\nnovel approach that directly leverages speech prosody to infer and resolve\ninstruction intent. Predicted intents are integrated into large language models\nvia in-context learning to disambiguate and select appropriate task plans.\nAdditionally, we present the first ambiguous speech dataset for robotics,\ndesigned to advance research in speech disambiguation. Our method achieves\n95.79% accuracy in detecting referent intents within an utterance and\ndetermines the intended task plan of ambiguous instructions with 71.96%\naccuracy, demonstrating its potential to significantly improve human-robot\ncommunication."}
{"id": "2506.02059", "pdf": "https://arxiv.org/pdf/2506.02059.pdf", "abs": "https://arxiv.org/abs/2506.02059", "title": "Learning More with Less: Self-Supervised Approaches for Low-Resource Speech Emotion Recognition", "authors": ["Ziwei Gong", "Pengyuan Shi", "Kaan Donbekci", "Lin Ai", "Run Chen", "David Sasu", "Zehui Wu", "Julia Hirschberg"], "categories": ["cs.SD", "cs.CL"], "comment": "Accepted at Interspeech 2025", "summary": "Speech Emotion Recognition (SER) has seen significant progress with deep\nlearning, yet remains challenging for Low-Resource Languages (LRLs) due to the\nscarcity of annotated data. In this work, we explore unsupervised learning to\nimprove SER in low-resource settings. Specifically, we investigate contrastive\nlearning (CL) and Bootstrap Your Own Latent (BYOL) as self-supervised\napproaches to enhance cross-lingual generalization. Our methods achieve notable\nF1 score improvements of 10.6% in Urdu, 15.2% in German, and 13.9% in Bangla,\ndemonstrating their effectiveness in LRLs. Additionally, we analyze model\nbehavior to provide insights on key factors influencing performance across\nlanguages, and also highlighting challenges in low-resource SER. This work\nprovides a foundation for developing more inclusive, explainable, and robust\nemotion recognition systems for underrepresented languages."}
{"id": "2506.02077", "pdf": "https://arxiv.org/pdf/2506.02077.pdf", "abs": "https://arxiv.org/abs/2506.02077", "title": "Assigning Distinct Roles to Quantized and Low-Rank Matrices Toward Optimal Weight Decomposition", "authors": ["Yoonjun Cho", "Soeun Kim", "Dongjae Jeon", "Kyelim Lee", "Beomsoo Lee", "Albert No"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted to Findings of ACL 2025", "summary": "Decomposing weight matrices into quantization and low-rank components\n($\\mathbf{W} \\approx \\mathbf{Q} + \\mathbf{L}\\mathbf{R}$) is a widely used\ntechnique for compressing large language models (LLMs). Existing joint\noptimization methods iteratively alternate between quantization and low-rank\napproximation. However, these methods tend to prioritize one component at the\nexpense of the other, resulting in suboptimal decompositions that fail to\nleverage each component's unique strengths. In this work, we introduce\nOutlier-Driven Low-Rank Initialization (ODLRI), which assigns low-rank\ncomponents the specific role of capturing activation-sensitive weights. This\nstructured decomposition mitigates outliers' negative impact on quantization,\nenabling more effective balance between quantization and low-rank\napproximation. Experiments on Llama2 (7B, 13B, 70B), Llama3-8B, and Mistral-7B\ndemonstrate that incorporating ODLRI into the joint optimization framework\nconsistently reduces activation-aware error, minimizes quantization scale, and\nimproves perplexity and zero-shot accuracy in low-bit settings."}
{"id": "2506.02085", "pdf": "https://arxiv.org/pdf/2506.02085.pdf", "abs": "https://arxiv.org/abs/2506.02085", "title": "Unveiling Audio Deepfake Origins: A Deep Metric learning And Conformer Network Approach With Ensemble Fusion", "authors": ["Ajinkya Kulkarni", "Sandipana Dowerah", "Tanel Alumae", "Mathew Magimai. -Doss"], "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": "Accepted at Interspeech 2025, Netherlands", "summary": "Audio deepfakes are acquiring an unprecedented level of realism with advanced\nAI. While current research focuses on discerning real speech from spoofed\nspeech, tracing the source system is equally crucial. This work proposes a\nnovel audio source tracing system combining deep metric multi-class N-pair loss\nwith Real Emphasis and Fake Dispersion framework, a Conformer classification\nnetwork, and ensemble score-embedding fusion. The N-pair loss improves\ndiscriminative ability, while Real Emphasis and Fake Dispersion enhance\nrobustness by focusing on differentiating real and fake speech patterns. The\nConformer network captures both global and local dependencies in the audio\nsignal, crucial for source tracing. The proposed ensemble score-embedding\nfusion shows an optimal trade-off between in-domain and out-of-domain source\ntracing scenarios. We evaluate our method using Frechet Distance and standard\nmetrics, demonstrating superior performance in source tracing over the baseline\nsystem."}
{"id": "2506.02088", "pdf": "https://arxiv.org/pdf/2506.02088.pdf", "abs": "https://arxiv.org/abs/2506.02088", "title": "Enhancing Speech Emotion Recognition with Graph-Based Multimodal Fusion and Prosodic Features for the Speech Emotion Recognition in Naturalistic Conditions Challenge at Interspeech 2025", "authors": ["Alef Iury Siqueira Ferreira", "Lucas Rafael Gris", "Alexandre Ferro Filho", "Lucas Ólives", "Daniel Ribeiro", "Luiz Fernando", "Fernanda Lustosa", "Rodrigo Tanaka", "Frederico Santos de Oliveira", "Arlindo Galvão Filho"], "categories": ["cs.SD", "cs.CL", "cs.LG"], "comment": null, "summary": "Training SER models in natural, spontaneous speech is especially challenging\ndue to the subtle expression of emotions and the unpredictable nature of\nreal-world audio. In this paper, we present a robust system for the INTERSPEECH\n2025 Speech Emotion Recognition in Naturalistic Conditions Challenge, focusing\non categorical emotion recognition. Our method combines state-of-the-art audio\nmodels with text features enriched by prosodic and spectral cues. In\nparticular, we investigate the effectiveness of Fundamental Frequency (F0)\nquantization and the use of a pretrained audio tagging model. We also employ an\nensemble model to improve robustness. On the official test set, our system\nachieved a Macro F1-score of 39.79% (42.20% on validation). Our results\nunderscore the potential of these methods, and analysis of fusion techniques\nconfirmed the effectiveness of Graph Attention Networks. Our source code is\npublicly available."}
{"id": "2506.02096", "pdf": "https://arxiv.org/pdf/2506.02096.pdf", "abs": "https://arxiv.org/abs/2506.02096", "title": "SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis", "authors": ["Zijian Wu", "Jinjie Ni", "Xiangyan Liu", "Zichen Liu", "Hang Yan", "Michael Qizhe Shieh"], "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) trained via reinforcement learning with\nverifiable reward (RLVR) have shown notable progress in scaling test-time\ncompute effectively. In this work, we investigate how synthesized RL data can\nfurther improve RLVR. To this end, we propose \\textbf{SynthRL}-a scalable and\nguaranteed pipeline for automatic data scaling in reasoning-oriented RL\ntraining. SynthRL comprises three key stages: (1) selecting seed questions with\nappropriate distribution, (2) augmenting them into more challenging variants\nwhile preserving the original answers, and (3) a guaranteed verification stage\nthat ensures near-perfect correctness and difficulty enhancement. Our empirical\nexperiments demonstrate SynthRL's scalability and effectiveness. When applied\nto the MMK12 dataset, SynthRL synthesizes over 3.3K additional verifiable,\nchallenging questions from approximately 8K seed samples. Models trained with\nour synthesized data achieve consistent gains across five out-of-domain visual\nmath reasoning benchmarks, with a significant improvement over baseline models\ntrained on seed data alone. Notably, detailed analysis reveals that the gains\nare more pronounced on the most challenging evaluation samples, highlighting\nSynthRL's effectiveness in eliciting deeper and more complex reasoning\npatterns."}
{"id": "2506.02160", "pdf": "https://arxiv.org/pdf/2506.02160.pdf", "abs": "https://arxiv.org/abs/2506.02160", "title": "A Dynamic Framework for Semantic Grouping of Common Data Elements (CDE) Using Embeddings and Clustering", "authors": ["Madan Krishnamurthy", "Daniel Korn", "Melissa A Haendel", "Christopher J Mungall", "Anne E Thessen"], "categories": ["cs.IR", "cs.CL", "cs.LG"], "comment": null, "summary": "This research aims to develop a dynamic and scalable framework to facilitate\nharmonization of Common Data Elements (CDEs) across heterogeneous biomedical\ndatasets by addressing challenges such as semantic heterogeneity, structural\nvariability, and context dependence to streamline integration, enhance\ninteroperability, and accelerate scientific discovery. Our methodology\nleverages Large Language Models (LLMs) for context-aware text embeddings that\nconvert CDEs into dense vectors capturing semantic relationships and patterns.\nThese embeddings are clustered using Hierarchical Density-Based Spatial\nClustering of Applications with Noise (HDBSCAN) to group semantically similar\nCDEs. The framework incorporates four key steps: (1) LLM-based text embedding\nto mathematically represent semantic context, (2) unsupervised clustering of\nembeddings via HDBSCAN, (3) automated labeling using LLM summarization, and (4)\nsupervised learning to train a classifier assigning new or unclustered CDEs to\nlabeled clusters. Evaluated on the NIH NLM CDE Repository with over 24,000\nCDEs, the system identified 118 meaningful clusters at an optimized minimum\ncluster size of 20. The classifier achieved 90.46 percent overall accuracy,\nperforming best in larger categories. External validation against Gravity\nProjects Social Determinants of Health domains showed strong agreement\n(Adjusted Rand Index 0.52, Normalized Mutual Information 0.78), indicating that\nembeddings effectively capture cluster characteristics. This adaptable and\nscalable approach offers a practical solution to CDE harmonization, improving\nselection efficiency and supporting ongoing data interoperability."}
{"id": "2506.02178", "pdf": "https://arxiv.org/pdf/2506.02178.pdf", "abs": "https://arxiv.org/abs/2506.02178", "title": "Cocktail-Party Audio-Visual Speech Recognition", "authors": ["Thai-Binh Nguyen", "Ngoc-Quan Pham", "Alexander Waibel"], "categories": ["cs.SD", "cs.CL"], "comment": "Accepted at Interspeech 2025", "summary": "Audio-Visual Speech Recognition (AVSR) offers a robust solution for speech\nrecognition in challenging environments, such as cocktail-party scenarios,\nwhere relying solely on audio proves insufficient. However, current AVSR models\nare often optimized for idealized scenarios with consistently active speakers,\noverlooking the complexities of real-world settings that include both speaking\nand silent facial segments. This study addresses this gap by introducing a\nnovel audio-visual cocktail-party dataset designed to benchmark current AVSR\nsystems and highlight the limitations of prior approaches in realistic noisy\nconditions. Additionally, we contribute a 1526-hour AVSR dataset comprising\nboth talking-face and silent-face segments, enabling significant performance\ngains in cocktail-party environments. Our approach reduces WER by 67% relative\nto the state-of-the-art, reducing WER from 119% to 39.2% in extreme noise,\nwithout relying on explicit segmentation cues."}
{"id": "2506.02208", "pdf": "https://arxiv.org/pdf/2506.02208.pdf", "abs": "https://arxiv.org/abs/2506.02208", "title": "KDRL: Post-Training Reasoning LLMs via Unified Knowledge Distillation and Reinforcement Learning", "authors": ["Hongling Xu", "Qi Zhu", "Heyuan Deng", "Jinpeng Li", "Lu Hou", "Yasheng Wang", "Lifeng Shang", "Ruifeng Xu", "Fei Mi"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances in large language model (LLM) post-training have leveraged\ntwo distinct paradigms to enhance reasoning capabilities: reinforcement\nlearning (RL) and knowledge distillation (KD). While RL enables the emergence\nof complex reasoning behaviors, it often suffers from low sample efficiency\nwhen the initial policy struggles to explore high-reward trajectories.\nConversely, KD improves learning efficiency via mimicking the teacher model but\ntends to generalize poorly to out-of-domain scenarios. In this work, we present\n\\textbf{KDRL}, a \\textit{unified post-training framework} that jointly\noptimizes a reasoning model through teacher supervision (KD) and\nself-exploration (RL). Specifically, KDRL leverages policy gradient\noptimization to simultaneously minimize the reverse Kullback-Leibler divergence\n(RKL) between the student and teacher distributions while maximizing the\nexpected rule-based rewards. We first formulate a unified objective that\nintegrates GRPO and KD, and systematically explore how different KL\napproximations, KL coefficients, and reward-guided KD strategies affect the\noverall post-training dynamics and performance. Empirical results on multiple\nreasoning benchmarks demonstrate that KDRL outperforms GRPO and various KD\nbaselines while achieving a favorable balance between performance and reasoning\ntoken efficiency. These findings indicate that integrating KD and RL serves as\nan effective and efficient strategy to train reasoning LLMs."}
{"id": "2506.02229", "pdf": "https://arxiv.org/pdf/2506.02229.pdf", "abs": "https://arxiv.org/abs/2506.02229", "title": "VLCD: Vision-Language Contrastive Distillation for Accurate and Efficient Automatic Placenta Analysis", "authors": ["Manas Mehta", "Yimu Pan", "Kelly Gallagher", "Alison D. Gernand", "Jeffery A. Goldstein", "Delia Mwinyelle", "Leena Mithal", "James Z. Wang"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Proceedings of the 9th International Workshop on Health Intelligence,\n  in conjunction with the Annual AAAI Conference on Artificial Intelligence,\n  Philadelphia, Pennsylvania, March 2025", "summary": "Pathological examination of the placenta is an effective method for detecting\nand mitigating health risks associated with childbirth. Recent advancements in\nAI have enabled the use of photographs of the placenta and pathology reports\nfor detecting and classifying signs of childbirth-related pathologies. However,\nexisting automated methods are computationally extensive, which limits their\ndeployability. We propose two modifications to vision-language contrastive\nlearning (VLC) frameworks to enhance their accuracy and efficiency: (1)\ntext-anchored vision-language contrastive knowledge distillation (VLCD)-a new\nknowledge distillation strategy for medical VLC pretraining, and (2)\nunsupervised predistillation using a large natural images dataset for improved\ninitialization. Our approach distills efficient neural networks that match or\nsurpass the teacher model in performance while achieving model compression and\nacceleration. Our results showcase the value of unsupervised predistillation in\nimproving the performance and robustness of our approach, specifically for\nlower-quality images. VLCD serves as an effective way to improve the efficiency\nand deployability of medical VLC approaches, making AI-based healthcare\nsolutions more accessible, especially in resource-constrained environments."}
{"id": "2506.02314", "pdf": "https://arxiv.org/pdf/2506.02314.pdf", "abs": "https://arxiv.org/abs/2506.02314", "title": "ResearchCodeBench: Benchmarking LLMs on Implementing Novel Machine Learning Research Code", "authors": ["Tianyu Hua", "Harper Hua", "Violet Xiang", "Benjamin Klieger", "Sang T. Truong", "Weixin Liang", "Fan-Yun Sun", "Nick Haber"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown promise in transforming machine\nlearning research, yet their capability to faithfully implement novel ideas\nfrom recent research papers-ideas unseen during pretraining-remains unclear. We\nintroduce ResearchCodeBench, a benchmark of 212 coding challenges that\nevaluates LLMs' ability to translate cutting-edge ML contributions from top\n2024-2025 research papers into executable code. We assessed 30+ proprietary and\nopen-source LLMs, finding that even the best models correctly implement less\nthan 40% of the code. We find Gemini-2.5-Pro-Preview to perform best at 37.3%\nsuccess rate, with O3 (High) and O4-mini (High) following behind at 32.3% and\n30.8% respectively. We present empirical findings on performance comparison,\ncontamination, and error patterns. By providing a rigorous and community-driven\nevaluation platform, ResearchCodeBench enables continuous understanding and\nadvancement of LLM-driven innovation in research code generation."}
{"id": "2506.02414", "pdf": "https://arxiv.org/pdf/2506.02414.pdf", "abs": "https://arxiv.org/abs/2506.02414", "title": "StarVC: A Unified Auto-Regressive Framework for Joint Text and Speech Generation in Voice Conversion", "authors": ["Fengjin Li", "Jie Wang", "Yadong Niu", "Yongqing Wang", "Meng Meng", "Jian Luan", "Zhiyong Wu"], "categories": ["cs.MM", "cs.CL", "cs.SD", "eess.AS"], "comment": "5 pages, 2 figures, Accepted by Interspeech 2025, Demo:\n  https://thuhcsi.github.io/StarVC/", "summary": "Voice Conversion (VC) modifies speech to match a target speaker while\npreserving linguistic content. Traditional methods usually extract speaker\ninformation directly from speech while neglecting the explicit utilization of\nlinguistic content. Since VC fundamentally involves disentangling speaker\nidentity from linguistic content, leveraging structured semantic features could\nenhance conversion performance. However, previous attempts to incorporate\nsemantic features into VC have shown limited effectiveness, motivating the\nintegration of explicit text modeling. We propose StarVC, a unified\nautoregressive VC framework that first predicts text tokens before synthesizing\nacoustic features. The experiments demonstrate that StarVC outperforms\nconventional VC methods in preserving both linguistic content (i.e., WER and\nCER) and speaker characteristics (i.e., SECS and MOS). Audio demo can be found\nat: https://thuhcsi.github.io/StarVC/."}
{"id": "2506.02475", "pdf": "https://arxiv.org/pdf/2506.02475.pdf", "abs": "https://arxiv.org/abs/2506.02475", "title": "Comba: Improving Nonlinear RNNs with Closed-loop Control", "authors": ["Jiaxi Hu", "Yongqi Pan", "Jusen Du", "Disen Lan", "Xiaqiang Tang", "Qingsong Wen", "Yuxuan Liang", "Weigao Sun"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Recent efficient sequence modeling methods such as Gated DeltaNet, TTT, and\nRWKV-7 have achieved performance improvements by supervising the recurrent\nmemory management through Delta learning rule. Unlike previous state-space\nmodels (e.g., Mamba) and gated linear attentions (e.g., GLA), these models\nintroduce interactions between the recurrent state and the key vector,\nresulting in a nonlinear recursive structure. In this paper, we first introduce\nthe concept of Nonlinear RNNs with a comprehensive analysis on the advantages\nand limitations of these models. Then, based on closed-loop control theory, we\npropose a novel Nonlinear RNN variant named Comba, which adopts a\nscalar-plus-low-rank state transition, with both state feedback and output\nfeedback corrections. We also implement a hardware-efficient chunk-wise\nparallel kernel in Triton and train models with 340M/1.3B parameters on\nlarge-scale corpus. Comba demonstrates its superior performance and computation\nefficiency in both language and vision modeling."}
{"id": "2506.02479", "pdf": "https://arxiv.org/pdf/2506.02479.pdf", "abs": "https://arxiv.org/abs/2506.02479", "title": "BitBypass: A New Direction in Jailbreaking Aligned Large Language Models with Bitstream Camouflage", "authors": ["Kalyan Nakka", "Nitesh Saxena"], "categories": ["cs.CR", "cs.CL"], "comment": "24 pages, 24 figures, and 7 tables", "summary": "The inherent risk of generating harmful and unsafe content by Large Language\nModels (LLMs), has highlighted the need for their safety alignment. Various\ntechniques like supervised fine-tuning, reinforcement learning from human\nfeedback, and red-teaming were developed for ensuring the safety alignment of\nLLMs. However, the robustness of these aligned LLMs is always challenged by\nadversarial attacks that exploit unexplored and underlying vulnerabilities of\nthe safety alignment. In this paper, we develop a novel black-box jailbreak\nattack, called BitBypass, that leverages hyphen-separated bitstream camouflage\nfor jailbreaking aligned LLMs. This represents a new direction in jailbreaking\nby exploiting fundamental information representation of data as continuous\nbits, rather than leveraging prompt engineering or adversarial manipulations.\nOur evaluation of five state-of-the-art LLMs, namely GPT-4o, Gemini 1.5, Claude\n3.5, Llama 3.1, and Mixtral, in adversarial perspective, revealed the\ncapabilities of BitBypass in bypassing their safety alignment and tricking them\ninto generating harmful and unsafe content. Further, we observed that BitBypass\noutperforms several state-of-the-art jailbreak attacks in terms of stealthiness\nand attack success. Overall, these results highlights the effectiveness and\nefficiency of BitBypass in jailbreaking these state-of-the-art LLMs."}
{"id": "2506.02529", "pdf": "https://arxiv.org/pdf/2506.02529.pdf", "abs": "https://arxiv.org/abs/2506.02529", "title": "Automated Web Application Testing: End-to-End Test Case Generation with Large Language Models and Screen Transition Graphs", "authors": ["Nguyen-Khang Le", "Quan Minh Bui", "Minh Ngoc Nguyen", "Hiep Nguyen", "Trung Vo", "Son T. Luu", "Shoshin Nomura", "Minh Le Nguyen"], "categories": ["cs.SE", "cs.AI", "cs.CL", "I.2.7"], "comment": "Published in the Proceedings of JSAI 2025", "summary": "Web applications are critical to modern software ecosystems, yet ensuring\ntheir reliability remains challenging due to the complexity and dynamic nature\nof web interfaces. Recent advances in large language models (LLMs) have shown\npromise in automating complex tasks, but limitations persist in handling\ndynamic navigation flows and complex form interactions. This paper presents an\nautomated system for generating test cases for two key aspects of web\napplication testing: site navigation and form filling. For site navigation, the\nsystem employs screen transition graphs and LLMs to model navigation flows and\ngenerate test scenarios. For form filling, it uses state graphs to handle\nconditional forms and automates Selenium script generation. Key contributions\ninclude: (1) a novel integration of graph structures and LLMs for site\nnavigation testing, (2) a state graph-based approach for automating\nform-filling test cases, and (3) a comprehensive dataset for evaluating\nform-interaction testing. Experimental results demonstrate the system's\neffectiveness in improving test coverage and robustness, advancing the state of\nweb application testing."}
{"id": "2506.02553", "pdf": "https://arxiv.org/pdf/2506.02553.pdf", "abs": "https://arxiv.org/abs/2506.02553", "title": "Response-Level Rewards Are All You Need for Online Reinforcement Learning in LLMs: A Mathematical Perspective", "authors": ["Shenghua He", "Tian Xia", "Xuan Zhou", "Hui Wei"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "We study a common challenge in reinforcement learning for large language\nmodels (LLMs): the Zero-Reward Assumption, where non-terminal actions (i.e.,\nintermediate token generations) receive zero task-specific immediate reward,\nwhile only the final token receives a reward for the entire response. This\nassumption arises frequently in practice, as precise token-level rewards are\noften difficult or infeasible to obtain in LLM applications. In this work, we\nprovide a unifying theoretical perspective. We introduce the Trajectory Policy\nGradient Theorem, which shows that the policy gradient based on true, unknown\ntoken-level rewards can be unbiasedly estimated using only a response-level\nreward model, regardless of whether the Zero-Reward Assumption holds or not,\nfor algorithms in the REINFORCE and Actor-Critic families. This result reveals\nthat widely used methods such as PPO, GRPO, ReMax, and RLOO inherently possess\nthe capacity to model token-level reward signals, offering a theoretical\njustification for response-level reward approaches. Our findings pave the way\nfor more practical, efficient LLM fine-tuning, allowing developers to treat\ntraining algorithms as black boxes and focus on improving the response-level\nreward model with auxiliary sub-models. We also offer a detailed analysis of\npopular RL and non-RL methods, comparing their theoretical foundations and\npractical advantages across common LLM tasks. Finally, we propose a new\nalgorithm: Token-Reinforced Policy Optimization (TRePO), a theoretically\ngrounded method that is simpler than PPO, matches GRPO in memory efficiency,\nand holds promise for broad applicability."}
{"id": "2506.02590", "pdf": "https://arxiv.org/pdf/2506.02590.pdf", "abs": "https://arxiv.org/abs/2506.02590", "title": "Synthetic Speech Source Tracing using Metric Learning", "authors": ["Dimitrios Koutsianos", "Stavros Zacharopoulos", "Yannis Panagakis", "Themos Stafylakis"], "categories": ["cs.SD", "cs.CL"], "comment": "Submitted to Interspeech 2025", "summary": "This paper addresses source tracing in synthetic speech-identifying\ngenerative systems behind manipulated audio via speaker recognition-inspired\npipelines. While prior work focuses on spoofing detection, source tracing lacks\nrobust solutions. We evaluate two approaches: classification-based and\nmetric-learning. We tested our methods on the MLAADv5 benchmark using ResNet\nand self-supervised learning (SSL) backbones. The results show that ResNet\nachieves competitive performance with the metric learning approach, matching\nand even exceeding SSL-based systems. Our work demonstrates ResNet's viability\nfor source tracing while underscoring the need to optimize SSL representations\nfor this task. Our work bridges speaker recognition methodologies with audio\nforensic challenges, offering new directions for combating synthetic media\nmanipulation."}
{"id": "2506.02708", "pdf": "https://arxiv.org/pdf/2506.02708.pdf", "abs": "https://arxiv.org/abs/2506.02708", "title": "Iterative Self-Improvement of Vision Language Models for Image Scoring and Self-Explanation", "authors": ["Naoto Tanji", "Toshihiko Yamasaki"], "categories": ["cs.CV", "cs.CL"], "comment": "Accepted to ICIP2025", "summary": "Image scoring is a crucial task in numerous real-world applications. To trust\na model's judgment, understanding its rationale is essential. This paper\nproposes a novel training method for Vision Language Models (VLMs) to generate\nnot only image scores but also corresponding justifications in natural\nlanguage. Leveraging only an image scoring dataset and an instruction-tuned\nVLM, our method enables self-training, utilizing the VLM's generated text\nwithout relying on external data or models. In addition, we introduce a simple\nmethod for creating a dataset designed to improve alignment between predicted\nscores and their textual justifications. By iteratively training the model with\nDirect Preference Optimization on two distinct datasets and merging them, we\ncan improve both scoring accuracy and the coherence of generated explanations."}
{"id": "2506.02720", "pdf": "https://arxiv.org/pdf/2506.02720.pdf", "abs": "https://arxiv.org/abs/2506.02720", "title": "Benchmarking and Advancing Large Language Models for Local Life Services", "authors": ["Xiaochong Lan", "Jie Feng", "Jiahuan Lei", "Xinlei Shi", "Yong Li"], "categories": ["cs.AI", "cs.CL"], "comment": "KDD 2025", "summary": "Large language models (LLMs) have exhibited remarkable capabilities and\nachieved significant breakthroughs across various domains, leading to their\nwidespread adoption in recent years. Building on this progress, we investigate\ntheir potential in the realm of local life services. In this study, we\nestablish a comprehensive benchmark and systematically evaluate the performance\nof diverse LLMs across a wide range of tasks relevant to local life services.\nTo further enhance their effectiveness, we explore two key approaches: model\nfine-tuning and agent-based workflows. Our findings reveal that even a\nrelatively compact 7B model can attain performance levels comparable to a much\nlarger 72B model, effectively balancing inference cost and model capability.\nThis optimization greatly enhances the feasibility and efficiency of deploying\nLLMs in real-world online services, making them more practical and accessible\nfor local life applications."}
{"id": "2506.02730", "pdf": "https://arxiv.org/pdf/2506.02730.pdf", "abs": "https://arxiv.org/abs/2506.02730", "title": "An Exploratory Framework for Future SETI Applications: Detecting Generative Reactivity via Language Models", "authors": ["Po-Chieh Yu"], "categories": ["astro-ph.IM", "cs.CL"], "comment": "submitted to the International Journal of Astrobiology", "summary": "We present an exploratory framework to test whether noise-like input can\ninduce structured responses in language models. Instead of assuming that\nextraterrestrial signals must be decoded, we evaluate whether inputs can\ntrigger linguistic behavior in generative systems. This shifts the focus from\ndecoding to viewing structured output as a sign of underlying regularity in the\ninput. We tested GPT-2 small, a 117M-parameter model trained on English text,\nusing four types of acoustic input: human speech, humpback whale vocalizations,\nPhylloscopus trochilus birdsong, and algorithmically generated white noise. All\ninputs were treated as noise-like, without any assumed symbolic encoding. To\nassess reactivity, we defined a composite score called Semantic Induction\nPotential (SIP), combining entropy, syntax coherence, compression gain, and\nrepetition penalty. Results showed that whale and bird vocalizations had higher\nSIP scores than white noise, while human speech triggered only moderate\nresponses. This suggests that language models may detect latent structure even\nin data without conventional semantics. We propose that this approach could\ncomplement traditional SETI methods, especially in cases where communicative\nintent is unknown. Generative reactivity may offer a different way to identify\ndata worth closer attention."}
{"id": "2506.02761", "pdf": "https://arxiv.org/pdf/2506.02761.pdf", "abs": "https://arxiv.org/abs/2506.02761", "title": "Rethinking Machine Unlearning in Image Generation Models", "authors": ["Renyang Liu", "Wenjie Feng", "Tianwei Zhang", "Wei Zhou", "Xueqi Cheng", "See-Kiong Ng"], "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.CV"], "comment": "Accepted by ACM CCS 2025", "summary": "With the surge and widespread application of image generation models, data\nprivacy and content safety have become major concerns and attracted great\nattention from users, service providers, and policymakers. Machine unlearning\n(MU) is recognized as a cost-effective and promising means to address these\nchallenges. Despite some advancements, image generation model unlearning (IGMU)\nstill faces remarkable gaps in practice, e.g., unclear task discrimination and\nunlearning guidelines, lack of an effective evaluation framework, and\nunreliable evaluation metrics. These can hinder the understanding of unlearning\nmechanisms and the design of practical unlearning algorithms. We perform\nexhaustive assessments over existing state-of-the-art unlearning algorithms and\nevaluation standards, and discover several critical flaws and challenges in\nIGMU tasks. Driven by these limitations, we make several core contributions, to\nfacilitate the comprehensive understanding, standardized categorization, and\nreliable evaluation of IGMU. Specifically, (1) We design CatIGMU, a novel\nhierarchical task categorization framework. It provides detailed implementation\nguidance for IGMU, assisting in the design of unlearning algorithms and the\nconstruction of testbeds. (2) We introduce EvalIGMU, a comprehensive evaluation\nframework. It includes reliable quantitative metrics across five critical\naspects. (3) We construct DataIGM, a high-quality unlearning dataset, which can\nbe used for extensive evaluations of IGMU, training content detectors for\njudgment, and benchmarking the state-of-the-art unlearning algorithms. With\nEvalIGMU and DataIGM, we discover that most existing IGMU algorithms cannot\nhandle the unlearning well across different evaluation dimensions, especially\nfor preservation and robustness. Code and models are available at\nhttps://github.com/ryliu68/IGMU."}
{"id": "2506.02867", "pdf": "https://arxiv.org/pdf/2506.02867.pdf", "abs": "https://arxiv.org/abs/2506.02867", "title": "Demystifying Reasoning Dynamics with Mutual Information: Thinking Tokens are Information Peaks in LLM Reasoning", "authors": ["Chen Qian", "Dongrui Liu", "Haochen Wen", "Zhen Bai", "Yong Liu", "Jing Shao"], "categories": ["cs.AI", "cs.CL"], "comment": "Preprint. Under review", "summary": "Large reasoning models (LRMs) have demonstrated impressive capabilities in\ncomplex problem-solving, yet their internal reasoning mechanisms remain poorly\nunderstood. In this paper, we investigate the reasoning trajectories of LRMs\nfrom an information-theoretic perspective. By tracking how mutual information\n(MI) between intermediate representations and the correct answer evolves during\nLRM reasoning, we observe an interesting MI peaks phenomenon: the MI at\nspecific generative steps exhibits a sudden and significant increase during\nLRM's reasoning process. We theoretically analyze such phenomenon and show that\nas MI increases, the probability of model's prediction error decreases.\nFurthermore, these MI peaks often correspond to tokens expressing reflection or\ntransition, such as ``Hmm'', ``Wait'' and ``Therefore,'' which we term as the\nthinking tokens. We then demonstrate that these thinking tokens are crucial for\nLRM's reasoning performance, while other tokens has minimal impacts. Building\non these analyses, we propose two simple yet effective methods to improve LRM's\nreasoning performance, by delicately leveraging these thinking tokens. Overall,\nour work provides novel insights into the reasoning mechanisms of LRMs and\noffers practical ways to improve their reasoning capabilities. The code is\navailable at https://github.com/ChnQ/MI-Peaks."}
{"id": "2506.02890", "pdf": "https://arxiv.org/pdf/2506.02890.pdf", "abs": "https://arxiv.org/abs/2506.02890", "title": "Scaling Fine-Grained MoE Beyond 50B Parameters: Empirical Evaluation and Practical Insights", "authors": ["Jakub Krajewski", "Marcin Chochowski", "Daniel Korzekwa"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Mixture of Experts (MoE) architectures have emerged as pivotal for scaling\nLarge Language Models (LLMs) efficiently. Fine-grained MoE approaches -\nutilizing more numerous, smaller experts - have demonstrated potential in\nimproving model convergence and quality. This work proposes a set of training\nrecipes and provides a comprehensive empirical evaluation of fine-grained MoE,\ndirectly comparing its scaling properties against standard MoE configurations\nfor models with up to 56B total (17B active) parameters. We investigate\nconvergence speed, model performance on downstream benchmarks, and practical\ntraining considerations across various setups. Overall, at the largest scale we\nshow that fine-grained MoE achieves better validation loss and higher accuracy\nacross a set of downstream benchmarks. This study offers empirical grounding\nand practical insights for leveraging fine-grained MoE in the development of\nfuture large-scale models."}
{"id": "2506.02992", "pdf": "https://arxiv.org/pdf/2506.02992.pdf", "abs": "https://arxiv.org/abs/2506.02992", "title": "Mitigating Manipulation and Enhancing Persuasion: A Reflective Multi-Agent Approach for Legal Argument Generation", "authors": ["Li Zhang", "Kevin D. Ashley"], "categories": ["cs.AI", "cs.CL", "cs.LG", "68T50", "I.2"], "comment": "13 pages, 2 figures, Workshop on Legally Compliant Intelligent\n  Chatbots at ICAIL 2025]{Workshop on Legally Compliant Intelligent Chatbots @\n  ICAIL 2025", "summary": "Large Language Models (LLMs) are increasingly explored for legal argument\ngeneration, yet they pose significant risks of manipulation through\nhallucination and ungrounded persuasion, and often fail to utilize provided\nfactual bases effectively or abstain when arguments are untenable. This paper\nintroduces a novel reflective multi-agent method designed to address these\nchallenges in the context of legally compliant persuasion. Our approach employs\nspecialized agents--a Factor Analyst and an Argument Polisher--in an iterative\nrefinement process to generate 3-ply legal arguments (plaintiff, defendant,\nrebuttal). We evaluate Reflective Multi-Agent against single-agent,\nenhanced-prompt single-agent, and non-reflective multi-agent baselines using\nfour diverse LLMs (GPT-4o, GPT-4o-mini, Llama-4-Maverick-17b-128e,\nLlama-4-Scout-17b-16e) across three legal scenarios: \"arguable\", \"mismatched\",\nand \"non-arguable\". Results demonstrate Reflective Multi-Agent's significant\nsuperiority in successful abstention (preventing generation when arguments\ncannot be grounded), marked improvements in hallucination accuracy (reducing\nfabricated and misattributed factors), particularly in \"non-arguable\"\nscenarios, and enhanced factor utilization recall (improving the use of\nprovided case facts). These findings suggest that structured reflection within\na multi-agent framework offers a robust computable method for fostering ethical\npersuasion and mitigating manipulation in LLM-based legal argumentation\nsystems, a critical step towards trustworthy AI in law. Project page:\nhttps://lizhang-aiandlaw.github.io/A-Reflective-Multi-Agent-Approach-for-Legal-Argument-Generation/"}
{"id": "2506.03053", "pdf": "https://arxiv.org/pdf/2506.03053.pdf", "abs": "https://arxiv.org/abs/2506.03053", "title": "MAEBE: Multi-Agent Emergent Behavior Framework", "authors": ["Sinem Erisken", "Timothy Gothard", "Martin Leitgab", "Ram Potham"], "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.CY", "cs.LG"], "comment": "Preprint. This work has been submitted to the Multi-Agent Systems\n  Workshop at ICML 2025 for review", "summary": "Traditional AI safety evaluations on isolated LLMs are insufficient as\nmulti-agent AI ensembles become prevalent, introducing novel emergent risks.\nThis paper introduces the Multi-Agent Emergent Behavior Evaluation (MAEBE)\nframework to systematically assess such risks. Using MAEBE with the Greatest\nGood Benchmark (and a novel double-inversion question technique), we\ndemonstrate that: (1) LLM moral preferences, particularly for Instrumental\nHarm, are surprisingly brittle and shift significantly with question framing,\nboth in single agents and ensembles. (2) The moral reasoning of LLM ensembles\nis not directly predictable from isolated agent behavior due to emergent group\ndynamics. (3) Specifically, ensembles exhibit phenomena like peer pressure\ninfluencing convergence, even when guided by a supervisor, highlighting\ndistinct safety and alignment challenges. Our findings underscore the necessity\nof evaluating AI systems in their interactive, multi-agent contexts."}
{"id": "2506.03100", "pdf": "https://arxiv.org/pdf/2506.03100.pdf", "abs": "https://arxiv.org/abs/2506.03100", "title": "Retrieval-Augmented Generation as Noisy In-Context Learning: A Unified Theory and Risk Bounds", "authors": ["Yang Guo", "Yutian Tao", "Yifei Ming", "Robert D. Nowak", "Yingyu Liang"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR", "math.ST", "stat.TH"], "comment": "Under Review", "summary": "Retrieval-augmented generation (RAG) has seen many empirical successes in\nrecent years by aiding the LLM with external knowledge. However, its\ntheoretical aspect has remained mostly unexplored. In this paper, we propose\nthe first finite-sample generalization bound for RAG in in-context linear\nregression and derive an exact bias-variance tradeoff. Our framework views the\nretrieved texts as query-dependent noisy in-context examples and recovers the\nclassical in-context learning (ICL) and standard RAG as the limit cases. Our\nanalysis suggests that an intrinsic ceiling on generalization error exists on\nRAG as opposed to the ICL. Furthermore, our framework is able to model\nretrieval both from the training data and from external corpora by introducing\nuniform and non-uniform RAG noise. In line with our theory, we show the sample\nefficiency of ICL and RAG empirically with experiments on common QA benchmarks,\nsuch as Natural Questions and TriviaQA."}
{"id": "2506.03135", "pdf": "https://arxiv.org/pdf/2506.03135.pdf", "abs": "https://arxiv.org/abs/2506.03135", "title": "OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language Models", "authors": ["Mengdi Jia", "Zekun Qi", "Shaochen Zhang", "Wenyao Zhang", "Xinqiang Yu", "Jiawei He", "He Wang", "Li Yi"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Project Page: https://qizekun.github.io/omnispatial/", "summary": "Spatial reasoning is a key aspect of cognitive psychology and remains a major\nbottleneck for current vision-language models (VLMs). While extensive research\nhas aimed to evaluate or improve VLMs' understanding of basic spatial\nrelations, such as distinguishing left from right, near from far, and object\ncounting, these tasks represent only the most fundamental level of spatial\nreasoning. In this work, we introduce OmniSpatial, a comprehensive and\nchallenging benchmark for spatial reasoning, grounded in cognitive psychology.\nOmniSpatial covers four major categories: dynamic reasoning, complex spatial\nlogic, spatial interaction, and perspective-taking, with 50 fine-grained\nsubcategories. Through Internet data crawling and careful manual annotation, we\nconstruct over 1.5K question-answer pairs. Extensive experiments show that both\nopen- and closed-source VLMs, as well as existing reasoning and spatial\nunderstanding models, exhibit significant limitations in comprehensive spatial\nunderstanding. We further analyze failure cases and propose potential\ndirections for future research."}
{"id": "2506.03144", "pdf": "https://arxiv.org/pdf/2506.03144.pdf", "abs": "https://arxiv.org/abs/2506.03144", "title": "MERIT: Multilingual Semantic Retrieval with Interleaved Multi-Condition Query", "authors": ["Wei Chow", "Yuan Gao", "Linfeng Li", "Xian Wang", "Qi Xu", "Hang Song", "Lingdong Kong", "Ran Zhou", "Yi Zeng", "Yidong Cai", "Botian Jiang", "Shilin Xu", "Jiajun Zhang", "Minghui Qiu", "Xiangtai Li", "Tianshu Yang", "Siliang Tang", "Juncheng Li"], "categories": ["cs.CV", "cs.CL", "cs.MM"], "comment": "Preprint; Project Page, Code, and Dataset at:\n  https://merit-2025.github.io/", "summary": "Semantic retrieval is crucial for modern applications yet remains\nunderexplored in current research. Existing datasets are limited to single\nlanguages, single images, or singular retrieval conditions, often failing to\nfully exploit the expressive capacity of visual information as evidenced by\nmaintained performance when images are replaced with captions. However,\npractical retrieval scenarios frequently involve interleaved multi-condition\nqueries with multiple images. Hence, this paper introduces MERIT, the first\nmultilingual dataset for interleaved multi-condition semantic retrieval,\ncomprising 320,000 queries with 135,000 products in 5 languages, covering 7\ndistinct product categories. Extensive experiments on MERIT identify existing\nmodels's limitation: focusing solely on global semantic information while\nneglecting specific conditional elements in queries. Consequently, we propose\nCoral, a novel fine-tuning framework that adapts pre-trained MLLMs by\nintegrating embedding reconstruction to preserve fine-grained conditional\nelements and contrastive learning to extract comprehensive global semantics.\nExperiments demonstrate that Coral achieves a 45.9% performance improvement\nover conventional approaches on MERIT, with strong generalization capabilities\nvalidated across 8 established retrieval benchmarks. Collectively, our\ncontributions - a novel dataset, identification of critical limitations in\nexisting approaches, and an innovative fine-tuning framework - establish a\nfoundation for future research in interleaved multi-condition semantic\nretrieval."}
{"id": "2506.03147", "pdf": "https://arxiv.org/pdf/2506.03147.pdf", "abs": "https://arxiv.org/abs/2506.03147", "title": "UniWorld: High-Resolution Semantic Encoders for Unified Visual Understanding and Generation", "authors": ["Bin Lin", "Zongjian Li", "Xinhua Cheng", "Yuwei Niu", "Yang Ye", "Xianyi He", "Shenghai Yuan", "Wangbo Yu", "Shaodong Wang", "Yunyang Ge", "Yatian Pang", "Li Yuan"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Although existing unified models deliver strong performance on\nvision-language understanding and text-to-image generation, their models are\nlimited in exploring image perception and manipulation tasks, which are\nurgently desired by users for wide applications. Recently, OpenAI released\ntheir powerful GPT-4o-Image model for comprehensive image perception and\nmanipulation, achieving expressive capability and attracting community\ninterests. By observing the performance of GPT-4o-Image in our carefully\nconstructed experiments, we infer that GPT-4o-Image leverages features\nextracted by semantic encoders instead of VAE, while VAEs are considered\nessential components in many image manipulation models. Motivated by such\ninspiring observations, we present a unified generative framework named\nUniWorld based on semantic features provided by powerful visual-language models\nand contrastive semantic encoders. As a result, we build a strong unified model\nusing only 1% amount of BAGEL's data, which consistently outperforms BAGEL on\nimage editing benchmarks. UniWorld also maintains competitive image\nunderstanding and generation capabilities, achieving strong performance across\nmultiple image perception tasks. We fully open-source our models, including\nmodel weights, training and evaluation scripts, and datasets."}
{"id": "2110.13658", "pdf": "https://arxiv.org/pdf/2110.13658.pdf", "abs": "https://arxiv.org/abs/2110.13658", "title": "Can Character-based Language Models Improve Downstream Task Performance in Low-Resource and Noisy Language Scenarios?", "authors": ["Arij Riabi", "Benoît Sagot", "Djamé Seddah"], "categories": ["cs.CL", "cs.LG"], "comment": "updated version with new results", "summary": "Recent impressive improvements in NLP, largely based on the success of\ncontextual neural language models, have been mostly demonstrated on at most a\ncouple dozen high-resource languages. Building language models and, more\ngenerally, NLP systems for non-standardized and low-resource languages remains\na challenging task. In this work, we focus on North-African colloquial\ndialectal Arabic written using an extension of the Latin script, called\nNArabizi, found mostly on social media and messaging communication. In this\nlow-resource scenario with data displaying a high level of variability, we\ncompare the downstream performance of a character-based language model on\npart-of-speech tagging and dependency parsing to that of monolingual and\nmultilingual models. We show that a character-based model trained on only 99k\nsentences of NArabizi and fined-tuned on a small treebank of this language\nleads to performance close to those obtained with the same architecture\npre-trained on large multilingual and monolingual models. Confirming these\nresults a on much larger data set of noisy French user-generated content, we\nargue that such character-based language models can be an asset for NLP in\nlow-resource and high language variability set-tings."}
{"id": "2111.00157", "pdf": "https://arxiv.org/pdf/2111.00157.pdf", "abs": "https://arxiv.org/abs/2111.00157", "title": "TransAug: Translate as Augmentation for Sentence Embeddings", "authors": ["Jue Wang"], "categories": ["cs.CL"], "comment": null, "summary": "While contrastive learning greatly advances the representation of sentence\nembeddings, it is still limited by the size of the existing sentence datasets.\nIn this paper, we present TransAug (Translate as Augmentation), which provide\nthe first exploration of utilizing translated sentence pairs as data\naugmentation for text, and introduce a two-stage paradigm to advances the\nstate-of-the-art sentence embeddings. Instead of adopting an encoder trained in\nother languages setting, we first distill a Chinese encoder from a SimCSE\nencoder (pretrained in English), so that their embeddings are close in semantic\nspace, which can be regraded as implicit data augmentation. Then, we only\nupdate the English encoder via cross-lingual contrastive learning and frozen\nthe distilled Chinese encoder. Our approach achieves a new state-of-art on\nstandard semantic textual similarity (STS), outperforming both SimCSE and\nSentence-T5, and the best performance in corresponding tracks on transfer tasks\nevaluated by SentEval."}
{"id": "2303.12892", "pdf": "https://arxiv.org/pdf/2303.12892.pdf", "abs": "https://arxiv.org/abs/2303.12892", "title": "Improving Transformer Performance for French Clinical Notes Classification Using Mixture of Experts on a Limited Dataset", "authors": ["Thanh-Dung Le", "Philippe Jouvet", "Rita Noumeir"], "categories": ["cs.CL", "eess.SP"], "comment": "Accepted for publication in the IEEE Journal of Translational\n  Engineering in Health and Medicine", "summary": "Transformer-based models have shown outstanding results in natural language\nprocessing but face challenges in applications like classifying small-scale\nclinical texts, especially with constrained computational resources. This study\npresents a customized Mixture of Expert (MoE) Transformer models for\nclassifying small-scale French clinical texts at CHU Sainte-Justine Hospital.\nThe MoE-Transformer addresses the dual challenges of effective training with\nlimited data and low-resource computation suitable for in-house hospital use.\nDespite the success of biomedical pre-trained models such as CamemBERT-bio,\nDrBERT, and AliBERT, their high computational demands make them impractical for\nmany clinical settings. Our MoE-Transformer model not only outperforms\nDistillBERT, CamemBERT, FlauBERT, and Transformer models on the same dataset\nbut also achieves impressive results: an accuracy of 87\\%, precision of 87\\%,\nrecall of 85\\%, and F1-score of 86\\%. While the MoE-Transformer does not\nsurpass the performance of biomedical pre-trained BERT models, it can be\ntrained at least 190 times faster, offering a viable alternative for settings\nwith limited data and computational resources. Although the MoE-Transformer\naddresses challenges of generalization gaps and sharp minima, demonstrating\nsome limitations for efficient and accurate clinical text classification, this\nmodel still represents a significant advancement in the field. It is\nparticularly valuable for classifying small French clinical narratives within\nthe privacy and constraints of hospital-based computational resources."}
{"id": "2403.04247", "pdf": "https://arxiv.org/pdf/2403.04247.pdf", "abs": "https://arxiv.org/abs/2403.04247", "title": "UltraWiki: Ultra-fine-grained Entity Set Expansion with Negative Seed Entities", "authors": ["Yangning Li", "Qingsong Lv", "Tianyu Yu", "Yinghui Li", "Xuming Hu", "Wenhao Jiang", "Hai-Tao Zheng", "Hui Wang"], "categories": ["cs.CL"], "comment": "Accepted by ICDE 2025", "summary": "Entity Set Expansion (ESE) aims to identify new entities belonging to the\nsame semantic class as the given set of seed entities. Traditional methods\nsolely relied on positive seed entities to represent the target fine-grained\nsemantic class, rendering them tough to represent ultra-fine-grained semantic\nclasses. Specifically, merely relying on positive seed entities leads to two\ninherent shortcomings: (i) Ambiguity among ultra-fine-grained semantic classes.\n(ii) Inability to define ``unwanted'' semantics. Hence, previous ESE methods\nstruggle to address the ultra-fine-grained ESE (Ultra-ESE) task. To solve this\nissue, we first introduce negative seed entities in the inputs, which jointly\ndescribe the ultra-fine-grained semantic class with positive seed entities.\nNegative seed entities eliminate the semantic ambiguity by providing a contrast\nbetween positive and negative attributes. Meanwhile, it provides a\nstraightforward way to express ``unwanted''. To assess model performance in\nUltra-ESE and facilitate further research, we also constructed UltraWiki, the\nfirst large-scale dataset tailored for Ultra-ESE. UltraWiki encompasses 50,973\nentities and 394,097 sentences, alongside 236 ultra-fine-grained semantic\nclasses, where each class is represented with 3-5 positive and negative seed\nentities. Moreover, a retrieval-based framework RetExpan and a generation-based\nframework GenExpan are proposed to provide powerful baselines for Ultra-ESE.\nAdditionally, we devised two strategies to enhance models' comprehension of\nultra-fine-grained entities' semantics: contrastive learning and\nchain-of-thought reasoning. Extensive experiments confirm the effectiveness of\nour proposed strategies and also reveal that there remains a large space for\nimprovement in Ultra-ESE."}
{"id": "2403.09073", "pdf": "https://arxiv.org/pdf/2403.09073.pdf", "abs": "https://arxiv.org/abs/2403.09073", "title": "Revealing the Parallel Multilingual Learning within Large Language Models", "authors": ["Yongyu Mu", "Peinan Feng", "Zhiquan Cao", "Yuzhang Wu", "Bei Li", "Chenglong Wang", "Tong Xiao", "Kai Song", "Tongran Liu", "Chunliang Zhang", "Jingbo Zhu"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2024", "summary": "In this study, we reveal an in-context learning (ICL) capability of\nmultilingual large language models (LLMs): by translating the input to several\nlanguages, we provide Parallel Input in Multiple Languages (PiM) to LLMs, which\nsignificantly enhances their comprehension abilities. To test this capability,\nwe design extensive experiments encompassing 8 typical datasets, 7 languages\nand 8 state-of-the-art multilingual LLMs. Experimental results show that (1)\nincorporating more languages help PiM surpass the conventional ICL further; (2)\neven combining with the translations that are inferior to baseline performance\ncan also help. Moreover, by examining the activated neurons in LLMs, we\ndiscover a counterintuitive but interesting phenomenon. Contrary to the common\nthought that PiM would activate more neurons than monolingual input to leverage\nknowledge learned from diverse languages, PiM actually inhibits neurons and\npromotes more precise neuron activation especially when more languages are\nadded. This phenomenon aligns with the neuroscience insight about synaptic\npruning, which removes less used neural connections, strengthens remainders,\nand then enhances brain intelligence."}
{"id": "2403.19390", "pdf": "https://arxiv.org/pdf/2403.19390.pdf", "abs": "https://arxiv.org/abs/2403.19390", "title": "Checkpoint Merging via Bayesian Optimization in LLM Pretraining", "authors": ["Deyuan Liu", "Zecheng Wang", "Bingning Wang", "Weipeng Chen", "Chunshan Li", "Zhiying Tu", "Dianhui Chu", "Bo Li", "Dianbo Sui"], "categories": ["cs.CL"], "comment": null, "summary": "The rapid proliferation of large language models (LLMs) such as GPT-4 and\nGemini underscores the intense demand for resources during their training\nprocesses, posing significant challenges due to substantial computational and\nenvironmental costs. To alleviate this issue, we propose checkpoint merging in\npretraining LLM. This method utilizes LLM checkpoints with shared training\ntrajectories, and is rooted in an extensive search space exploration for the\nbest merging weight via Bayesian optimization. Through various experiments, we\ndemonstrate that: (1) Our proposed methodology exhibits the capacity to augment\npretraining, presenting an opportunity akin to obtaining substantial benefits\nat minimal cost; (2) Our proposed methodology, despite requiring a given\nheld-out dataset, still demonstrates robust generalization capabilities across\ndiverse domains, a pivotal aspect in pretraining."}
{"id": "2405.06705", "pdf": "https://arxiv.org/pdf/2405.06705.pdf", "abs": "https://arxiv.org/abs/2405.06705", "title": "LLMs can Find Mathematical Reasoning Mistakes by Pedagogical Chain-of-Thought", "authors": ["Zhuoxuan Jiang", "Haoyuan Peng", "Shanshan Feng", "Fan Li", "Dongsheng Li"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by IJCAI 2024", "summary": "Self-correction is emerging as a promising approach to mitigate the issue of\nhallucination in Large Language Models (LLMs). To facilitate effective\nself-correction, recent research has proposed mistake detection as its initial\nstep. However, current literature suggests that LLMs often struggle with\nreliably identifying reasoning mistakes when using simplistic prompting\nstrategies. To address this challenge, we introduce a unique prompting\nstrategy, termed the Pedagogical Chain-of-Thought (PedCoT), which is\nspecifically designed to guide the identification of reasoning mistakes,\nparticularly mathematical reasoning mistakes. PedCoT consists of pedagogical\nprinciples for prompts (PPP) design, two-stage interaction process (TIP) and\ngrounded PedCoT prompts, all inspired by the educational theory of the Bloom\nCognitive Model (BCM). We evaluate our approach on two public datasets\nfeaturing math problems of varying difficulty levels. The experiments\ndemonstrate that our zero-shot prompting strategy significantly outperforms\nstrong baselines. The proposed method can achieve the goal of reliable\nmathematical mistake identification and provide a foundation for automatic math\nanswer grading. The results underscore the significance of educational theory,\nserving as domain knowledge, in guiding prompting strategy design for\naddressing challenging tasks with LLMs effectively."}
{"id": "2406.06907", "pdf": "https://arxiv.org/pdf/2406.06907.pdf", "abs": "https://arxiv.org/abs/2406.06907", "title": "SignMusketeers: An Efficient Multi-Stream Approach for Sign Language Translation at Scale", "authors": ["Shester Gueuwou", "Xiaodan Du", "Greg Shakhnarovich", "Karen Livescu"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "Accepted to ACL (Findings) 2025", "summary": "A persistent challenge in sign language video processing, including the task\nof sign to written language translation, is how we learn representations of\nsign language in an effective and efficient way that preserves the important\nattributes of these languages, while remaining invariant to irrelevant visual\ndifferences. Informed by the nature and linguistics of signed languages, our\nproposed method focuses on just the most relevant parts in a signing video: the\nface, hands and body pose of the signer. However, instead of fully relying on\npose estimation from off-the-shelf pose tracking models, which have\ninconsistent performance for hands and faces, we propose to learn a\nrepresentation of the complex handshapes and facial expressions of sign\nlanguages in a self-supervised fashion. Our approach is based on learning from\nindividual frames (rather than video sequences) and is therefore much more\nefficient than prior work on sign language pre-training. Compared to a recent\nmodel that established a new state of the art in sign language translation on\nthe How2Sign dataset, our approach yields similar translation performance,\nusing less than 3\\% of the compute."}
{"id": "2406.14545", "pdf": "https://arxiv.org/pdf/2406.14545.pdf", "abs": "https://arxiv.org/abs/2406.14545", "title": "Unmasking Database Vulnerabilities: Zero-Knowledge Schema Inference Attacks in Text-to-SQL Systems", "authors": ["Đorđe Klisura", "Anthony Rios"], "categories": ["cs.CL"], "comment": "Accepted to NAACL 2025 Findings", "summary": "Text-to-SQL systems empower users to interact with databases using natural\nlanguage, automatically translating queries into executable SQL code. However,\ntheir reliance on database schema information for SQL generation exposes them\nto significant security vulnerabilities, particularly schema inference attacks\nthat can lead to unauthorized data access or manipulation. In this paper, we\nintroduce a novel zero-knowledge framework for reconstructing the underlying\ndatabase schema of text-to-SQL models without any prior knowledge of the\ndatabase. Our approach systematically probes text-to-SQL models with specially\ncrafted questions and leverages a surrogate GPT-4 model to interpret the\noutputs, effectively uncovering hidden schema elements -- including tables,\ncolumns, and data types. We demonstrate that our method achieves high accuracy\nin reconstructing table names, with F1 scores of up to .99 for generative\nmodels and .78 for fine-tuned models, underscoring the severity of schema\nleakage risks. We also show that our attack can steal prompt information in\nnon-text-to-SQL models. Furthermore, we propose a simple protection mechanism\nfor generative models and empirically show its limitations in mitigating these\nattacks."}
{"id": "2407.01384", "pdf": "https://arxiv.org/pdf/2407.01384.pdf", "abs": "https://arxiv.org/abs/2407.01384", "title": "Free-text Rationale Generation under Readability Level Control", "authors": ["Yi-Sheng Hsu", "Nils Feldhus", "Sherzod Hakimov"], "categories": ["cs.CL"], "comment": "ACL 2025 Workshop on Generation, Evaluation, and Metrics (GEM^2)", "summary": "Free-text rationales justify model decisions in natural language and thus\nbecome likable and accessible among approaches to explanation across many\ntasks. However, their effectiveness can be hindered by misinterpretation and\nhallucination. As a perturbation test, we investigate how large language models\n(LLMs) perform rationale generation under the effects of readability level\ncontrol, i.e., being prompted for an explanation targeting a specific expertise\nlevel, such as sixth grade or college. We find that explanations are adaptable\nto such instruction, though the observed distinction between readability levels\ndoes not fully match the defined complexity scores according to traditional\nreadability metrics. Furthermore, the generated rationales tend to feature\nmedium level complexity, which correlates with the measured quality using\nautomatic metrics. Finally, our human annotators confirm a generally\nsatisfactory impression on rationales at all readability levels, with\nhigh-school-level readability being most commonly perceived and favored."}
{"id": "2407.03525", "pdf": "https://arxiv.org/pdf/2407.03525.pdf", "abs": "https://arxiv.org/abs/2407.03525", "title": "UnSeenTimeQA: Time-Sensitive Question-Answering Beyond LLMs' Memorization", "authors": ["Md Nayem Uddin", "Amir Saeidi", "Divij Handa", "Agastya Seth", "Tran Cao Son", "Eduardo Blanco", "Steven R. Corman", "Chitta Baral"], "categories": ["cs.CL"], "comment": "Accepted at ACL 2025 (Main)", "summary": "This paper introduces UnSeenTimeQA, a novel data contamination-free\ntime-sensitive question-answering (TSQA) benchmark. It differs from existing\nTSQA benchmarks by avoiding web-searchable queries grounded in the real world.\nWe present a series of time-sensitive event scenarios based on synthetically\ngenerated facts. It requires large language models (LLMs) to engage in genuine\ntemporal reasoning without depending on the factual knowledge acquired during\nthe pre-training phase. Our data generation framework enables on-demand\ngeneration of new samples, mitigating the risk of data leakage. We designed\nthree types of time-sensitive questions to test LLMs' temporal reasoning\nabilities over sequential and parallel event occurrences. Our evaluation of\nfive LLMs on synthetic fact-based TSQA reveals mixed results: while they\nperform well on simpler subsets, their overall performance remains inferior as\ncompared to real world fact-based TSQA. Error analysis indicates that LLMs face\ndifficulties in reasoning over long-range event dependencies and parallel\nevents."}
{"id": "2407.11930", "pdf": "https://arxiv.org/pdf/2407.11930.pdf", "abs": "https://arxiv.org/abs/2407.11930", "title": "Localizing and Mitigating Errors in Long-form Question Answering", "authors": ["Rachneet Sachdeva", "Yixiao Song", "Mohit Iyyer", "Iryna Gurevych"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings; Code and data are available:\n  https://github.com/UKPLab/acl2025-lfqa-hallucination", "summary": "Long-form question answering (LFQA) aims to provide thorough and in-depth\nanswers to complex questions, enhancing comprehension. However, such detailed\nresponses are prone to hallucinations and factual inconsistencies, challenging\ntheir faithful evaluation. This work introduces HaluQuestQA, the first\nhallucination dataset with localized error annotations for human-written and\nmodel-generated LFQA answers. HaluQuestQA comprises 698 QA pairs with 1.8k\nspan-level error annotations for five different error types by expert\nannotators, along with preference judgments. Using our collected data, we\nthoroughly analyze the shortcomings of long-form answers and find that they\nlack comprehensiveness and provide unhelpful references. We train an automatic\nfeedback model on this dataset that predicts error spans with incomplete\ninformation and provides associated explanations. Finally, we propose a\nprompt-based approach, Error-informed refinement, that uses signals from the\nlearned feedback model to refine generated answers, which we show reduces\nerrors and improves answer quality across multiple models. Furthermore, humans\nfind answers generated by our approach comprehensive and highly prefer them\n(84%) over the baseline answers."}
{"id": "2407.15186", "pdf": "https://arxiv.org/pdf/2407.15186.pdf", "abs": "https://arxiv.org/abs/2407.15186", "title": "A Survey on Employing Large Language Models for Text-to-SQL Tasks", "authors": ["Liang Shi", "Zhengju Tang", "Nan Zhang", "Xiaotong Zhang", "Zhi Yang"], "categories": ["cs.CL"], "comment": "Accepted by ACM Computing Surveys (CSUR)", "summary": "With the development of the Large Language Models (LLMs), a large range of\nLLM-based Text-to-SQL(Text2SQL) methods have emerged. This survey provides a\ncomprehensive review of LLM-based Text2SQL studies. We first enumerate classic\nbenchmarks and evaluation metrics. For the two mainstream methods, prompt\nengineering and finetuning, we introduce a comprehensive taxonomy and offer\npractical insights into each subcategory. We present an overall analysis of the\nabove methods and various models evaluated on well-known datasets and extract\nsome characteristics. Finally, we discuss the challenges and future directions\nin this field."}
{"id": "2407.21050", "pdf": "https://arxiv.org/pdf/2407.21050.pdf", "abs": "https://arxiv.org/abs/2407.21050", "title": "Cross-Institutional Dental EHR Entity Extraction via Generative AI and Synthetic Notes", "authors": ["Yao-Shun Chuang", "Chun-Teh Lee", "Oluwabunmi Tokede", "Guo-Hao Lin", "Ryan Brandon", "Trung Duong Tran", "Xiaoqian Jiang", "Muhammad F. Walji"], "categories": ["cs.CL"], "comment": "11 pages, 2 tables, 3 figures, under review", "summary": "This research addresses the issue of missing structured data in dental\nrecords by extracting diagnostic information from unstructured text. The\nupdated periodontology classification system's complexity has increased\nincomplete or missing structured diagnoses. To tackle this, we use advanced AI\nand NLP methods, leveraging GPT-4 to generate synthetic notes for fine-tuning a\nRoBERTa model. This significantly enhances the model's ability to understand\nmedical and dental language. We evaluated the model using 120 randomly selected\nclinical notes from two datasets, demonstrating its improved diagnostic\nextraction accuracy. The results showed high accuracy in diagnosing periodontal\nstatus, stage, and grade, with Site 1 scoring 0.99 and Site 2 scoring 0.98. In\nthe subtype category, Site 2 achieved perfect scores, outperforming Site 1.\nThis method enhances extraction accuracy and broadens its use across dental\ncontexts. The study underscores AI and NLP's transformative impact on\nhealthcare delivery and management. Integrating AI and NLP technologies\nenhances documentation and simplifies administrative tasks by precisely\nextracting complex clinical information. This approach effectively addresses\nchallenges in dental diagnostics. Using synthetic training data from LLMs\noptimizes the training process, improving accuracy and efficiency in\nidentifying periodontal diagnoses from clinical notes. This innovative method\nholds promise for broader healthcare applications, potentially improving\npatient care quality."}
{"id": "2408.08769", "pdf": "https://arxiv.org/pdf/2408.08769.pdf", "abs": "https://arxiv.org/abs/2408.08769", "title": "Lower Layers Matter: Alleviating Hallucination via Multi-Layer Fusion Contrastive Decoding with Truthfulness Refocused", "authors": ["Dingwei Chen", "Feiteng Fang", "Shiwen Ni", "Feng Liang", "Xiping Hu", "Ahmadreza Argha", "Hamid Alinejad-Rokny", "Min Yang", "Chengming Li"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated exceptional performance across\nvarious natural language processing tasks. However, they occasionally generate\ninaccurate and counterfactual outputs, a phenomenon commonly referred to as\n\"hallucinations''. To tackle this issue, recent studies have explored\ncontrastive decoding between the original model and an amateur model with\ninduced hallucination, showing promising results. Nevertheless, this approach\ncan disrupt the original LLM's output distribution due to coarse contrast and\nsimple subtraction operations, potentially leading to errors. In this paper, we\nintroduce a novel contrastive decoding framework, termed LOL (LOwer Layer\nMatters). Unlike prior methods that focus solely on the final layer, our\napproach integrates contrastive information from lower layers to enable\nmulti-layer fusion during contrastive decoding. Additionally, we incorporate a\ntruthfulness refocused module that leverages instruction guidance to further\nimprove truthfulness in contrastive decoding. Extensive experiments on four\npublicly available datasets demonstrate that the LOL framework significantly\nmitigates hallucination while outperforming existing baselines in most cases.\nFor reproducibility, we will release our code and data upon acceptance."}
{"id": "2409.15762", "pdf": "https://arxiv.org/pdf/2409.15762.pdf", "abs": "https://arxiv.org/abs/2409.15762", "title": "XTRUST: On the Multilingual Trustworthiness of Large Language Models", "authors": ["Yahan Li", "Yi Wang", "Yi Chang", "Yuan Wu"], "categories": ["cs.CL"], "comment": "21 pages", "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\na range of natural language processing (NLP) tasks, capturing the attention of\nboth practitioners and the broader public. A key question that now preoccupies\nthe AI community concerns the capabilities and limitations of these models,\nwith trustworthiness emerging as a central issue, particularly as LLMs are\nincreasingly applied in sensitive fields like healthcare and finance, where\nerrors can have serious consequences. However, most previous studies on the\ntrustworthiness of LLMs have been limited to a single language, typically the\npredominant one in the dataset, such as English. In response to the growing\nglobal deployment of LLMs, we introduce XTRUST, the first comprehensive\nmultilingual trustworthiness benchmark. XTRUST encompasses a diverse range of\ntopics, including illegal activities, hallucination, out-of-distribution (OOD)\nrobustness, physical and mental health, toxicity, fairness, misinformation,\nprivacy, and machine ethics, across 10 different languages. Using XTRUST, we\nconduct an empirical evaluation of the multilingual trustworthiness of five\nwidely used LLMs, offering an in-depth analysis of their performance across\nlanguages and tasks. Our results indicate that many LLMs struggle with certain\nlow-resource languages, such as Arabic and Russian, highlighting the\nconsiderable room for improvement in the multilingual trustworthiness of\ncurrent language models. The code is available at\nhttps://github.com/LluckyYH/XTRUST."}
{"id": "2409.17044", "pdf": "https://arxiv.org/pdf/2409.17044.pdf", "abs": "https://arxiv.org/abs/2409.17044", "title": "How to Connect Speech Foundation Models and Large Language Models? What Matters and What Does Not", "authors": ["Francesco Verdini", "Pierfrancesco Melucci", "Stefano Perna", "Francesco Cariaggi", "Marco Gaido", "Sara Papi", "Szymon Mazurek", "Marek Kasztelnik", "Luisa Bentivogli", "Sébastien Bratières", "Paolo Merialdo", "Simone Scardapane"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Submitted to Interspeech 2025", "summary": "The remarkable performance achieved by Large Language Models (LLM) has driven\nresearch efforts to leverage them for a wide range of tasks and input\nmodalities. In speech-to-text (S2T) tasks, the emerging solution consists of\nprojecting the output of the encoder of a Speech Foundational Model (SFM) into\nthe LLM embedding space through an adapter module. However, no work has yet\ninvestigated how much the downstream-task performance depends on each component\n(SFM, adapter, LLM) nor whether the best design of the adapter depends on the\nchosen SFM and LLM. To fill this gap, we evaluate the combination of 5 adapter\nmodules, 2 LLMs (Mistral and Llama), and 2 SFMs (Whisper and SeamlessM4T) on\ntwo widespread S2T tasks, namely Automatic Speech Recognition and Speech\nTranslation. Our results demonstrate that the SFM plays a pivotal role in\ndownstream performance, while the adapter choice has moderate impact and\ndepends on the SFM and LLM."}
{"id": "2410.00382", "pdf": "https://arxiv.org/pdf/2410.00382.pdf", "abs": "https://arxiv.org/abs/2410.00382", "title": "Answer When Needed, Forget When Not: Language Models Pretend to Forget via In-Context Knowledge Unlearning", "authors": ["Shota Takashiro", "Takeshi Kojima", "Andrew Gambardella", "Qi Cao", "Yusuke Iwasawa", "Yutaka Matsuo"], "categories": ["cs.CL"], "comment": "Accepted at ACL 2025 (Findings)", "summary": "As large language models (LLMs) are applied across diverse domains, the\nability to selectively unlearn specific information is becoming increasingly\nessential. For instance, LLMs are expected to selectively provide confidential\ninformation to authorized internal users, such as employees or trusted\npartners, while withholding it from external users, including the general\npublic and unauthorized entities. Therefore, we propose a novel method termed\n``in-context knowledge unlearning'', which enables the model to selectively\nforget information in test-time based on the query context. Our method\nfine-tunes pre-trained LLMs to enable prompt unlearning of target knowledge\nwithin the context, while preserving unrelated information. Experiments on\nTOFU, AGE and RWKU datasets using Llama2-7B/13B and Mistral-7B models\ndemonstrate that our method achieves up to 95% forget accuracy while retaining\n80% of unrelated knowledge, significantly outperforming baselines in both\nin-domain and out-of-domain scenarios. Further investigation of the model's\ninternal behavior revealed that while fine-tuned LLMs generate correct\npredictions in the middle layers and preserve them up to the final layer.\nHowever, the decision to forget is made only at the last layer, i.e. ``LLMs\npretend to forget''. Our findings offer valuable insight into the improvement\nof the robustness of the unlearning mechanisms in LLMs, laying a foundation for\nfuture research in the field."}
{"id": "2410.02677", "pdf": "https://arxiv.org/pdf/2410.02677.pdf", "abs": "https://arxiv.org/abs/2410.02677", "title": "CulturalBench: A Robust, Diverse, and Challenging Cultural Benchmark by Human-AI CulturalTeaming", "authors": ["Yu Ying Chiu", "Liwei Jiang", "Bill Yuchen Lin", "Chan Young Park", "Shuyue Stella Li", "Sahithya Ravi", "Mehar Bhatia", "Maria Antoniak", "Yulia Tsvetkov", "Vered Shwartz", "Yejin Choi"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL 2025 Main, 39 pages, 16 figures. arXiv admin note: text overlap\n  with arXiv:2404.06664", "summary": "Robust, diverse, and challenging cultural knowledge benchmarks are essential\nfor measuring our progress towards making LMs that are helpful across diverse\ncultures. We introduce CulturalBench: a set of 1,696 human-written and\nhuman-verified questions to assess LMs' cultural knowledge, covering 45 global\nregions including underrepresented ones like Bangladesh, Zimbabwe, and Peru.\nQuestions are each verified by five independent annotators and span 17 diverse\ntopics ranging from food preferences to greeting etiquette. We construct\nCulturalBench using methods inspired by Human-AI Red-Teaming. Compared to human\nperformance (92.4% accuracy), the hard version of CulturalBench is challenging\neven for the best-performing frontier LMs, ranging from 28.7% to 61.5% in\naccuracy. We find that LMs often struggle with tricky questions that have\nmultiple correct answers (e.g., What utensils do the Chinese usually use?),\nrevealing a tendency to overfit to a single answer. Our results indicate that\nGPT-4o substantially outperform other models across cultures, besting local\nproviders (e.g., Mistral on European culture and DeepSeek on Chinese culture).\nAcross the board, models under-perform on questions related to North Africa,\nSouth America and Middle East."}
{"id": "2410.03869", "pdf": "https://arxiv.org/pdf/2410.03869.pdf", "abs": "https://arxiv.org/abs/2410.03869", "title": "Chain-of-Jailbreak Attack for Image Generation Models via Editing Step by Step", "authors": ["Wenxuan Wang", "Kuiyi Gao", "Youliang Yuan", "Jen-tse Huang", "Qiuzhi Liu", "Shuai Wang", "Wenxiang Jiao", "Zhaopeng Tu"], "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.CV", "cs.MM"], "comment": "Accepted by ACL 2025 Findings", "summary": "Text-based image generation models, such as Stable Diffusion and DALL-E 3,\nhold significant potential in content creation and publishing workflows, making\nthem the focus in recent years. Despite their remarkable capability to generate\ndiverse and vivid images, considerable efforts are being made to prevent the\ngeneration of harmful content, such as abusive, violent, or pornographic\nmaterial. To assess the safety of existing models, we introduce a novel\njailbreaking method called Chain-of-Jailbreak (CoJ) attack, which compromises\nimage generation models through a step-by-step editing process. Specifically,\nfor malicious queries that cannot bypass the safeguards with a single prompt,\nwe intentionally decompose the query into multiple sub-queries. The image\ngeneration models are then prompted to generate and iteratively edit images\nbased on these sub-queries. To evaluate the effectiveness of our CoJ attack\nmethod, we constructed a comprehensive dataset, CoJ-Bench, encompassing nine\nsafety scenarios, three types of editing operations, and three editing\nelements. Experiments on four widely-used image generation services provided by\nGPT-4V, GPT-4o, Gemini 1.5 and Gemini 1.5 Pro, demonstrate that our CoJ attack\nmethod can successfully bypass the safeguards of models for over 60% cases,\nwhich significantly outperforms other jailbreaking methods (i.e., 14%).\nFurther, to enhance these models' safety against our CoJ attack method, we also\npropose an effective prompting-based method, Think Twice Prompting, that can\nsuccessfully defend over 95% of CoJ attack. We release our dataset and code to\nfacilitate the AI safety research."}
{"id": "2410.10672", "pdf": "https://arxiv.org/pdf/2410.10672.pdf", "abs": "https://arxiv.org/abs/2410.10672", "title": "Large Language Model Evaluation via Matrix Nuclear-Norm", "authors": ["Yahan Li", "Tingyu Xia", "Yi Chang", "Yuan Wu"], "categories": ["cs.CL"], "comment": "21 pages", "summary": "As large language models (LLMs) continue to evolve, efficient evaluation\nmetrics are vital for assessing their ability to compress information and\nreduce redundancy. While traditional metrics like Matrix Entropy offer valuable\ninsights, they are computationally intensive for large-scale models due to\ntheir \\( O(n^3) \\) time complexity with Singular Value Decomposition (SVD). To\nmitigate this issue, we introduce the Matrix Nuclear-Norm, which not only\nserves as a metric to quantify the data compression proficiency of LLM but also\nprovides a convex approximation of matrix rank to capture both predictive\ndiscriminability and diversity. By employing the \\( L_{1,2}\\text{-norm} \\) to\nfurther approximate the nuclear norm, we can effectively assess the model's\ninformation compression capabilities. This approach reduces the time complexity\nto \\( O(n^2) \\) and eliminates the need for SVD computation. Consequently, the\nMatrix Nuclear-Norm achieves speeds 8 to 24 times faster than Matrix Entropy\nfor the CEREBRAS-GPT model as sizes increase from 111M to 6.7B. This\nperformance gap becomes more pronounced with larger models, as validated in\ntests with other models like Pythia. Additionally, evaluations on benchmarks\nand model responses confirm that our proposed Matrix Nuclear-Norm is a\nreliable, scalable, and efficient tool for assessing LLMs' performance,\nstriking a balance between accuracy and computational efficiency. The code is\navailable at https://github.com/MLGroupJLU/MatrixNuclearNorm."}
{"id": "2410.10995", "pdf": "https://arxiv.org/pdf/2410.10995.pdf", "abs": "https://arxiv.org/abs/2410.10995", "title": "Watching the Watchers: Exposing Gender Disparities in Machine Translation Quality Estimation", "authors": ["Emmanouil Zaranis", "Giuseppe Attanasio", "Sweta Agrawal", "André F. T. Martins"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Quality estimation (QE)-the automatic assessment of translation quality-has\nrecently become crucial across several stages of the translation pipeline, from\ndata curation to training and decoding. While QE metrics have been optimized to\nalign with human judgments, whether they encode social biases has been largely\noverlooked. Biased QE risks favoring certain demographic groups over others,\ne.g., by exacerbating gaps in visibility and usability. This paper defines and\ninvestigates gender bias of QE metrics and discusses its downstream\nimplications for machine translation (MT). Experiments with state-of-the-art QE\nmetrics across multiple domains, datasets, and languages reveal significant\nbias. When a human entity's gender in the source is undisclosed,\nmasculine-inflected translations score higher than feminine-inflected ones, and\ngender-neutral translations are penalized. Even when contextual cues\ndisambiguate gender, using context-aware QE metrics leads to more errors in\nselecting the correct translation inflection for feminine referents than for\nmasculine ones. Moreover, a biased QE metric affects data filtering and\nquality-aware decoding. Our findings underscore the need for a renewed focus on\ndeveloping and evaluating QE metrics centered on gender."}
{"id": "2410.11020", "pdf": "https://arxiv.org/pdf/2410.11020.pdf", "abs": "https://arxiv.org/abs/2410.11020", "title": "Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning", "authors": ["Bokai Hu", "Sai Ashish Somayajula", "Xin Pan", "Pengtao Xie"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Instruction-fine-tuned large language models (LLMs) under 14B parameters\ncontinue to underperform on natural language understanding (NLU) tasks, often\ntrailing smaller models like BERT-base on benchmarks such as GLUE and\nSuperGLUE. Motivated by the success of reinforcement learning in reasoning\ntasks (e.g., DeepSeek), we explore Proximal Policy Optimization (PPO) as a\nframework to improve the NLU capabilities of LLMs. We frame NLU as a\nreinforcement learning environment, treating token generation as a sequence of\nactions and optimizing for reward signals based on alignment with ground-truth\nlabels. PPO consistently outperforms supervised fine-tuning, yielding an\naverage improvement of 6.3 points on GLUE, and surpasses zero-shot and few-shot\nprompting by 38.7 and 26.1 points, respectively. Notably, PPO-tuned models\noutperform GPT-4o by over 4\\% on average across sentiment and natural language\ninference tasks, including gains of 7.3\\% on the Mental Health dataset and\n10.9\\% on SIGA-nli. This work highlights a promising direction for adapting\nLLMs to new tasks by reframing them as reinforcement learning problems,\nenabling learning through simple end-task rewards rather than extensive data\ncuration."}
{"id": "2410.12974", "pdf": "https://arxiv.org/pdf/2410.12974.pdf", "abs": "https://arxiv.org/abs/2410.12974", "title": "BenchmarkCards: Standardized Documentation for Large Language Model Benchmarks", "authors": ["Anna Sokol", "Elizabeth Daly", "Michael Hind", "David Piorkowski", "Xiangliang Zhang", "Nuno Moniz", "Nitesh Chawla"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are powerful tools capable of handling diverse\ntasks. Comparing and selecting appropriate LLMs for specific tasks requires\nsystematic evaluation methods, as models exhibit varying capabilities across\ndifferent domains. However, finding suitable benchmarks is difficult given the\nmany available options. This complexity not only increases the risk of\nbenchmark misuse and misinterpretation but also demands substantial effort from\nLLM users, seeking the most suitable benchmarks for their specific needs. To\naddress these issues, we introduce \\texttt{BenchmarkCards}, an intuitive and\nvalidated documentation framework that standardizes critical benchmark\nattributes such as objectives, methodologies, data sources, and limitations.\nThrough user studies involving benchmark creators and users, we show that\n\\texttt{BenchmarkCards} can simplify benchmark selection and enhance\ntransparency, facilitating informed decision-making in evaluating LLMs. Data &\nCode: https://github.com/SokolAnn/BenchmarkCards"}
{"id": "2410.14817", "pdf": "https://arxiv.org/pdf/2410.14817.pdf", "abs": "https://arxiv.org/abs/2410.14817", "title": "A Complexity-Based Theory of Compositionality", "authors": ["Eric Elmoznino", "Thomas Jiralerspong", "Yoshua Bengio", "Guillaume Lajoie"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Compositionality is believed to be fundamental to intelligence. In humans, it\nunderlies the structure of thought, language, and higher-level reasoning. In\nAI, compositional representations can enable a powerful form of\nout-of-distribution generalization, in which a model systematically adapts to\nnovel combinations of known concepts. However, while we have strong intuitions\nabout what compositionality is, we lack satisfying formal definitions for it\nthat are measurable and mathematical. Here, we propose such a definition, which\nwe call representational compositionality, that accounts for and extends our\nintuitions about compositionality. The definition is conceptually simple,\nquantitative, grounded in algorithmic information theory, and applicable to any\nrepresentation. Intuitively, representational compositionality states that a\ncompositional representation satisfies three properties. First, it must be\nexpressive. Second, it must be possible to re-describe the representation as a\nfunction of discrete symbolic sequences with re-combinable parts, analogous to\nsentences in natural language. Third, the function that relates these symbolic\nsequences to the representation, analogous to semantics in natural language,\nmust be simple. Through experiments on both synthetic and real world data, we\nvalidate our definition of compositionality and show how it unifies disparate\nintuitions from across the literature in both AI and cognitive science. We also\nshow that representational compositionality, while theoretically intractable,\ncan be readily estimated using standard deep learning tools. We hope that our\ndefinition can inspire the design of novel, theoretically-driven models that\nbetter capture the mechanisms of compositional thought. We make our code\navailable at https://github.com/EricElmoznino/complexity_compositionality."}
{"id": "2410.21271", "pdf": "https://arxiv.org/pdf/2410.21271.pdf", "abs": "https://arxiv.org/abs/2410.21271", "title": "EoRA: Fine-tuning-free Compensation for Compressed LLM with Eigenspace Low-Rank Approximation", "authors": ["Shih-Yang Liu", "Maksim Khadkevich", "Nai Chit Fung", "Charbel Sakr", "Chao-Han Huck Yang", "Chien-Yi Wang", "Saurav Muralidharan", "Hongxu Yin", "Kwang-Ting Cheng", "Jan Kautz", "Yu-Chiang Frank Wang", "Pavlo Molchanov", "Min-Hung Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While post-training compression techniques effectively reduce the memory\nfootprint, latency, and power consumption of Large Language Models (LLMs), they\noften result in noticeable accuracy degradation and remain limited by hardware\nand kernel constraints that restrict supported compression formats ultimately\nreducing flexibility across a wide range of deployment scenarios. In this work,\nwe propose EoRA, a novel fine-tuning-free method that augments compressed LLMs\nwith low-rank matrices, allowing users to rapidly enhance task-specific\nperformance and freely balance the trade-off between accuracy and computational\noverhead beyond the constraints of compression formats. EoRA consistently\noutperforms prior training-free low rank methods in recovering the accuracy of\ncompressed LLMs, achieving notable accuracy improvements (e.g.,\n$\\mathbf{10.84\\%}$ on ARC-Challenge, $\\mathbf{6.74\\%}$ on MathQA, and\n$\\mathbf{6.74\\%}$ on GSM8K) for LLaMA3-8B compressed to 3-bit. We also\nintroduce an optimized CUDA kernel, accelerating inference by up to 1.4x and\nreducing memory overhead through quantizing EoRA. Overall, EoRA offers a prompt\nsolution for improving the accuracy of compressed models under varying user\nrequirements, enabling more efficient and flexible deployment of LLMs. Code is\navailable at https://github.com/NVlabs/EoRA."}
{"id": "2411.00418", "pdf": "https://arxiv.org/pdf/2411.00418.pdf", "abs": "https://arxiv.org/abs/2411.00418", "title": "Self-Evolved Reward Learning for LLMs", "authors": ["Chenghua Huang", "Zhizhen Fan", "Lu Wang", "Fangkai Yang", "Pu Zhao", "Zeqi Lin", "Qingwei Lin", "Dongmei Zhang", "Saravan Rajmohan", "Qi Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "23 pages,6 figures,Accepted to ICLR 2025", "summary": "Reinforcement Learning from Human Feedback (RLHF) is a crucial technique for\naligning language models with human preferences, playing a pivotal role in the\nsuccess of conversational models like GPT-4, ChatGPT, and Llama 2. A core\nchallenge in employing RLHF lies in training a reliable reward model (RM),\nwhich relies on high-quality labels typically provided by human experts or\nadvanced AI system. These methods can be costly and may introduce biases that\naffect the language model's responses. As language models improve, human input\nmay become less effective in further enhancing their performance. In this\npaper, we propose Self-Evolved Reward Learning (SER), a novel approach where\nthe RM generates additional training data to iteratively improve itself. We\nconducted extensive experiments on multiple datasets such as HH-RLHF and\nUltraFeedback, using models like Mistral and Llama 3, and compare SER against\nvarious baselines. Our results demonstrate that even with limited\nhuman-annotated data, learning from self-feedback can robustly enhance RM\nperformance, thereby boosting the capabilities of large language models (LLMs).\nResources of this paper can be found at https://aka.ms/ser"}
{"id": "2411.02430", "pdf": "https://arxiv.org/pdf/2411.02430.pdf", "abs": "https://arxiv.org/abs/2411.02430", "title": "Generative Emotion Cause Explanation in Multimodal Conversations", "authors": ["Lin Wang", "Xiaocui Yang", "Shi Feng", "Daling Wang", "Yifei Zhang", "Zhitao Zhang"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Multimodal conversation, a crucial form of human communication, carries rich\nemotional content, making the exploration of the causes of emotions within it a\nresearch endeavor of significant importance. However, existing research on the\ncauses of emotions typically employs an utterance selection method within a\nsingle textual modality to locate causal utterances. This approach remains\nlimited to coarse-grained assessments, lacks nuanced explanations of emotional\ncausation, and demonstrates inadequate capability in identifying multimodal\nemotional triggers. Therefore, we introduce a task-\\textbf{Multimodal Emotion\nCause Explanation in Conversation (MECEC)}. This task aims to generate a\nsummary based on the multimodal context of conversations, clearly and\nintuitively describing the reasons that trigger a given emotion. To adapt to\nthis task, we develop a new dataset (ECEM) based on the MELD dataset. ECEM\ncombines video clips with detailed explanations of character emotions, helping\nto explore the causal factors behind emotional expression in multimodal\nconversations. A novel approach, FAME-Net, is further proposed, that harnesses\nthe power of Large Language Models (LLMs) to analyze visual data and accurately\ninterpret the emotions conveyed through facial expressions in videos. By\nexploiting the contagion effect of facial emotions, FAME-Net effectively\ncaptures the emotional causes of individuals engaged in conversations. Our\nexperimental results on the newly constructed dataset show that FAME-Net\noutperforms several excellent baselines. Code and dataset are available at\nhttps://github.com/3222345200/FAME-Net."}
{"id": "2411.02528", "pdf": "https://arxiv.org/pdf/2411.02528.pdf", "abs": "https://arxiv.org/abs/2411.02528", "title": "What Goes Into a LM Acceptability Judgment? Rethinking the Impact of Frequency and Length", "authors": ["Lindia Tjuatja", "Graham Neubig", "Tal Linzen", "Sophie Hao"], "categories": ["cs.CL"], "comment": "Accepted to NAACL 2025 (Main Conference)", "summary": "When comparing the linguistic capabilities of language models (LMs) with\nhumans using LM probabilities, factors such as the length of the sequence and\nthe unigram frequency of lexical items have a significant effect on LM\nprobabilities in ways that humans are largely robust to. Prior works in\ncomparing LM and human acceptability judgments treat these effects uniformly\nacross models, making a strong assumption that models require the same degree\nof adjustment to control for length and unigram frequency effects. We propose\nMORCELA, a new linking theory between LM scores and acceptability judgments\nwhere the optimal level of adjustment for these effects is estimated from data\nvia learned parameters for length and unigram frequency. We first show that\nMORCELA outperforms a commonly used linking theory for acceptability - SLOR\n(Pauls and Klein, 2012; Lau et al. 2017) - across two families of transformer\nLMs (Pythia and OPT). Furthermore, we demonstrate that the assumed degrees of\nadjustment in SLOR for length and unigram frequency overcorrect for these\nconfounds, and that larger models require a lower relative degree of adjustment\nfor unigram frequency, though a significant amount of adjustment is still\nnecessary for all models. Finally, our subsequent analysis shows that larger\nLMs' lower susceptibility to frequency effects can be explained by an ability\nto better predict rarer words in context."}
{"id": "2411.04975", "pdf": "https://arxiv.org/pdf/2411.04975.pdf", "abs": "https://arxiv.org/abs/2411.04975", "title": "SuffixDecoding: Extreme Speculative Decoding for Emerging AI Applications", "authors": ["Gabriele Oliaro", "Zhihao Jia", "Daniel Campos", "Aurick Qiao"], "categories": ["cs.CL", "cs.AI", "cs.DC", "cs.LG"], "comment": null, "summary": "Speculative decoding is widely adopted to reduce latency in large language\nmodel (LLM) inference by leveraging smaller draft models capable of handling\ndiverse user tasks. However, emerging AI applications, such as LLM-based\nagents, present unique workload characteristics: instead of diverse independent\nrequests, agentic frameworks typically submit repetitive inference requests,\nsuch as multi-agent pipelines performing similar subtasks or self-refinement\nloops iteratively enhancing outputs. These workloads result in long and highly\npredictable sequences, which current speculative decoding methods do not\neffectively exploit. To address this gap, we introduce \\emph{SuffixDecoding}, a\nnovel method that utilizes efficient suffix trees to cache long token sequences\nfrom prompts and previous outputs. By adaptively speculating more tokens when\nacceptance likelihood is high and fewer when it is low, SuffixDecoding\neffectively exploits opportunities for longer speculations while conserving\ncomputation when those opportunities are limited. Evaluations on agentic\nbenchmarks, including SWE-Bench and Text-to-SQL, demonstrate that\nSuffixDecoding achieves speedups of up to 5.3$\\times$, outperforming\nstate-of-the-art methods -- 2.8$\\times$ faster than model-based approaches like\nEAGLE-2/3 and 1.9$\\times$ faster than model-free approaches such as Token\nRecycling. SuffixDecoding is open-sourced at\nhttps://github.com/snowflakedb/ArcticInference."}
{"id": "2411.07965", "pdf": "https://arxiv.org/pdf/2411.07965.pdf", "abs": "https://arxiv.org/abs/2411.07965", "title": "SHARP: Unlocking Interactive Hallucination via Stance Transfer in Role-Playing LLMs", "authors": ["Chuyi Kong", "Ziyang Luo", "Hongzhan Lin", "Zhiyuan Fan", "Yaxin Fan", "Yuxi Sun", "Jing Ma"], "categories": ["cs.CL"], "comment": "28 pages, unfortunately accepted to findings with Meta 4, acknowledge\n  and apologize to the reviewers and area chair who support our work in the\n  discussion period", "summary": "The advanced role-playing capabilities of Large Language Models (LLMs) have\nenabled rich interactive scenarios, yet existing research in social\ninteractions neglects hallucination while struggling with poor generalizability\nand implicit character fidelity judgments. To bridge this gap, motivated by\nhuman behaviour, we introduce a generalizable and explicit paradigm for\nuncovering interactive patterns of LLMs across diverse worldviews.\nSpecifically, we first define interactive hallucination through stance\ntransfer, then construct SHARP, a benchmark built by extracting relations from\ncommonsense knowledge graphs and utilizing LLMs' inherent hallucination\nproperties to simulate multi-role interactions. Extensive experiments confirm\nour paradigm's effectiveness and stability, examine the factors that influence\nthese metrics, and challenge conventional hallucination mitigation solutions.\nMore broadly, our work reveals a fundamental limitation in popular\npost-training methods for role-playing LLMs: the tendency to obscure knowledge\nbeneath style, resulting in monotonous yet human-like behaviors - interactive\nhallucination."}
{"id": "2411.11479", "pdf": "https://arxiv.org/pdf/2411.11479.pdf", "abs": "https://arxiv.org/abs/2411.11479", "title": "Value-Spectrum: Quantifying Preferences of Vision-Language Models via Value Decomposition in Social Media Contexts", "authors": ["Jingxuan Li", "Yuning Yang", "Shengqi Yang", "Linfan Zhang", "Ying Nian Wu"], "categories": ["cs.CL"], "comment": "ACL 2025 main", "summary": "The recent progress in Vision-Language Models (VLMs) has broadened the scope\nof multimodal applications. However, evaluations often remain limited to\nfunctional tasks, neglecting abstract dimensions such as personality traits and\nhuman values. To address this gap, we introduce Value-Spectrum, a novel Visual\nQuestion Answering (VQA) benchmark aimed at assessing VLMs based on Schwartz's\nvalue dimensions that capture core human values guiding people's preferences\nand actions. We design a VLM agent pipeline to simulate video browsing and\nconstruct a vector database comprising over 50,000 short videos from TikTok,\nYouTube Shorts, and Instagram Reels. These videos span multiple months and\ncover diverse topics, including family, health, hobbies, society, technology,\netc. Benchmarking on Value-Spectrum highlights notable variations in how VLMs\nhandle value-oriented content. Beyond identifying VLMs' intrinsic preferences,\nwe also explore the ability of VLM agents to adopt specific personas when\nexplicitly prompted, revealing insights into the adaptability of the model in\nrole-playing scenarios. These findings highlight the potential of\nValue-Spectrum as a comprehensive evaluation set for tracking VLM preferences\nin value-based tasks and abilities to simulate diverse personas. The complete\ncode and data are available at: https://github.com/Jeremyyny/Value-Spectrum."}
{"id": "2411.15462", "pdf": "https://arxiv.org/pdf/2411.15462.pdf", "abs": "https://arxiv.org/abs/2411.15462", "title": "HateDay: Insights from a Global Hate Speech Dataset Representative of a Day on Twitter", "authors": ["Manuel Tonneau", "Diyi Liu", "Niyati Malhotra", "Scott A. Hale", "Samuel P. Fraiberger", "Victor Orozco-Olvera", "Paul Röttger"], "categories": ["cs.CL"], "comment": "ACL 2025 main conference. Data available at\n  https://huggingface.co/datasets/manueltonneau/hateday", "summary": "To address the global challenge of online hate speech, prior research has\ndeveloped detection models to flag such content on social media. However, due\nto systematic biases in evaluation datasets, the real-world effectiveness of\nthese models remains unclear, particularly across geographies. We introduce\nHateDay, the first global hate speech dataset representative of social media\nsettings, constructed from a random sample of all tweets posted on September\n21, 2022 and covering eight languages and four English-speaking countries.\nUsing HateDay, we uncover substantial variation in the prevalence and\ncomposition of hate speech across languages and regions. We show that\nevaluations on academic datasets greatly overestimate real-world detection\nperformance, which we find is very low, especially for non-European languages.\nOur analysis identifies key drivers of this gap, including models' difficulty\nto distinguish hate from offensive speech and a mismatch between the target\ngroups emphasized in academic datasets and those most frequently targeted in\nreal-world settings. We argue that poor model performance makes public models\nill-suited for automatic hate speech moderation and find that high moderation\nrates are only achievable with substantial human oversight. Our results\nunderscore the need to evaluate detection systems on data that reflects the\ncomplexity and diversity of real-world social media."}
{"id": "2411.16765", "pdf": "https://arxiv.org/pdf/2411.16765.pdf", "abs": "https://arxiv.org/abs/2411.16765", "title": "SHuBERT: Self-Supervised Sign Language Representation Learning via Multi-Stream Cluster Prediction", "authors": ["Shester Gueuwou", "Xiaodan Du", "Greg Shakhnarovich", "Karen Livescu", "Alexander H. Liu"], "categories": ["cs.CL", "cs.CV"], "comment": "Accepted to ACL 2025", "summary": "Sign language processing has traditionally relied on task-specific models,\nlimiting the potential for transfer learning across tasks. Pre-training methods\nfor sign language have typically focused on either supervised pre-training,\nwhich cannot take advantage of unlabeled data, or context-independent (frame or\nvideo segment) representations, which ignore the effects of relationships\nacross time in sign language. We introduce SHuBERT (Sign Hidden-Unit BERT), a\nself-supervised contextual representation model learned from approximately\n1,000 hours of American Sign Language video. SHuBERT adapts masked token\nprediction objectives to multi-stream visual sign language input, learning to\npredict multiple targets corresponding to clustered hand, face, and body pose\nstreams. SHuBERT achieves state-of-the-art performance across multiple tasks\nincluding sign language translation, isolated sign language recognition, and\nfingerspelling detection."}
{"id": "2412.12797", "pdf": "https://arxiv.org/pdf/2412.12797.pdf", "abs": "https://arxiv.org/abs/2412.12797", "title": "Is it the end of (generative) linguistics as we know it?", "authors": ["Cristiano Chesi"], "categories": ["cs.CL"], "comment": null, "summary": "A significant debate has emerged in response to a paper written by Steven\nPiantadosi (Piantadosi, 2023) and uploaded to the LingBuzz platform, the open\narchive for generative linguistics. Piantadosi's dismissal of Chomsky's\napproach is ruthless, but generative linguists deserve it. In this paper, I\nwill adopt three idealized perspectives -- computational, theoretical, and\nexperimental -- to focus on two fundamental issues that lend partial support to\nPiantadosi's critique: (a) the evidence challenging the Poverty of Stimulus\n(PoS) hypothesis and (b) the notion of simplicity as conceived within\nmainstream Minimalism. In conclusion, I argue that, to reclaim a central role\nin language studies, generative linguistics -- representing a prototypical\ntheoretical perspective on language -- needs a serious update leading to (i)\nmore precise, consistent, and complete formalizations of foundational\nintuitions and (ii) the establishment and utilization of a standardized dataset\nof crucial empirical evidence to evaluate the theory's adequacy. On the other\nhand, ignoring the formal perspective leads to major drawbacks in both\ncomputational and experimental approaches. Neither descriptive nor explanatory\nadequacy can be easily achieved without the precise formulation of general\nprinciples that can be challenged empirically."}
{"id": "2412.13649", "pdf": "https://arxiv.org/pdf/2412.13649.pdf", "abs": "https://arxiv.org/abs/2412.13649", "title": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation", "authors": ["Jialong Wu", "Zhenglin Wang", "Linhai Zhang", "Yilong Lai", "Yulan He", "Deyu Zhou"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods."}
{"id": "2412.15628", "pdf": "https://arxiv.org/pdf/2412.15628.pdf", "abs": "https://arxiv.org/abs/2412.15628", "title": "Can Input Attributions Explain Inductive Reasoning in In-Context Learning?", "authors": ["Mengyu Ye", "Tatsuki Kuribayashi", "Goro Kobayashi", "Jun Suzuki"], "categories": ["cs.CL"], "comment": "Findings of ACL 2025", "summary": "Interpreting the internal process of neural models has long been a challenge.\nThis challenge remains relevant in the era of large language models (LLMs) and\nin-context learning (ICL); for example, ICL poses a new issue of interpreting\nwhich example in the few-shot examples contributed to identifying/solving the\ntask. To this end, in this paper, we design synthetic diagnostic tasks of\ninductive reasoning, inspired by the generalization tests typically adopted in\npsycholinguistics. Here, most in-context examples are ambiguous w.r.t. their\nunderlying rule, and one critical example disambiguates it. The question is\nwhether conventional input attribution (IA) methods can track such a reasoning\nprocess, i.e., identify the influential example, in ICL. Our experiments\nprovide several practical findings; for example, a certain simple IA method\nworks the best, and the larger the model, the generally harder it is to\ninterpret the ICL with gradient-based IA methods."}
{"id": "2412.17063", "pdf": "https://arxiv.org/pdf/2412.17063.pdf", "abs": "https://arxiv.org/abs/2412.17063", "title": "Computational Analysis of Character Development in Holocaust Testimonies", "authors": ["Esther Shizgal", "Eitan Wagner", "Renana Keydar", "Omri Abend"], "categories": ["cs.CL"], "comment": null, "summary": "This work presents a computational approach to analyze character development\nalong the narrative timeline. The analysis characterizes the inner and outer\nchanges the protagonist undergoes within a narrative, and the interplay between\nthem. We consider transcripts of Holocaust survivor testimonies as a test case,\neach telling the story of an individual in first-person terms. We focus on the\nsurvivor's religious trajectory, examining the evolution of their disposition\ntoward religious belief and practice along the testimony. Clustering the\nresulting trajectories in the dataset, we identify common sequences in the\ndata. Our findings highlight multiple common structures of religiosity across\nthe narratives: in terms of belief, most present a constant disposition, while\nfor practice, most present an oscillating structure, serving as valuable\nmaterial for historical and sociological research. This work demonstrates the\npotential of natural language processing techniques for analyzing character\nevolution through thematic trajectories in narratives."}
{"id": "2412.17451", "pdf": "https://arxiv.org/pdf/2412.17451.pdf", "abs": "https://arxiv.org/abs/2412.17451", "title": "Diving into Self-Evolving Training for Multimodal Reasoning", "authors": ["Wei Liu", "Junlong Li", "Xiwen Zhang", "Fan Zhou", "Yu Cheng", "Junxian He"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "ICML 2025, Project Page: https://mstar-lmm.github.io", "summary": "Self-evolving trainin--where models iteratively learn from their own\noutputs--has emerged as a key approach for complex reasoning tasks, addressing\nthe scarcity of high-quality chain-of-thought data. However, its effectiveness\nin multimodal reasoning, a domain more intricate than text-only reasoning,\nremains underexplored, and the understanding of critical factors in this\ntraining paradigm remains limited. Furthermore, a central challenge for this\ntraining method is performance saturation, which impedes further improvements\nand scalability. Inspired by reinforcement learning (RL), in this paper, we\nreframe self-evolving training for multimodal reasoning through the lens of RL,\nidentifying three pivotal factors: Training Method, Reward Model, and Prompt\nVariation. Through systematic analysis, we establish relatively optimal design\nprinciples that significantly enhance multimodal reasoning capabilities.\nMoreover, delving deeper into training dynamics, we uncover the roots of\nsaturation and propose a new automatic balancing mechanism to mitigate this\nlimitation. Building on these insights, we propose M-STAR (Multimodal\nSelf-evolving Training for Reasoning), a framework that achieves consistent\nperformance gains across models of varying sizes and diverse benchmarks. All\nresources are made publicly available at https://mstar-lmm.github.io."}
{"id": "2412.17533", "pdf": "https://arxiv.org/pdf/2412.17533.pdf", "abs": "https://arxiv.org/abs/2412.17533", "title": "Behind Closed Words: Creating and Investigating the forePLay Annotated Dataset for Polish Erotic Discourse", "authors": ["Anna Kołos", "Katarzyna Lorenc", "Emilia Wiśnios", "Agnieszka Karlińska"], "categories": ["cs.CL"], "comment": "Accepted for ACL 2025 Main Conference", "summary": "The surge in online content has created an urgent demand for robust detection\nsystems, especially in non-English contexts where current tools demonstrate\nsignificant limitations. We present forePLay, a novel Polish language dataset\nfor erotic content detection, featuring over 24k annotated sentences with a\nmultidimensional taxonomy encompassing ambiguity, violence, and social\nunacceptability dimensions. Our comprehensive evaluation demonstrates that\nspecialized Polish language models achieve superior performance compared to\nmultilingual alternatives, with transformer-based architectures showing\nparticular strength in handling imbalanced categories. The dataset and\naccompanying analysis establish essential frameworks for developing\nlinguistically-aware content moderation systems, while highlighting critical\nconsiderations for extending such capabilities to morphologically complex\nlanguages."}
{"id": "2501.02086", "pdf": "https://arxiv.org/pdf/2501.02086.pdf", "abs": "https://arxiv.org/abs/2501.02086", "title": "Instruction-Following Pruning for Large Language Models", "authors": ["Bairu Hou", "Qibin Chen", "Jianyu Wang", "Guoli Yin", "Chong Wang", "Nan Du", "Ruoming Pang", "Shiyu Chang", "Tao Lei"], "categories": ["cs.CL"], "comment": "ICML 2025", "summary": "With the rapid scaling of large language models (LLMs), structured pruning\nhas become a widely used technique to learn efficient, smaller models from\nlarger ones, delivering superior performance compared to training similarly\nsized models from scratch. In this paper, we move beyond the traditional static\npruning approach of determining a fixed pruning mask for a model, and propose a\ndynamic approach to structured pruning. In our method, the pruning mask is\ninput-dependent and adapts dynamically based on the information described in a\nuser instruction. Our approach, termed \"instruction-following pruning\",\nintroduces a sparse mask predictor that takes the user instruction as input and\ndynamically selects the most relevant model parameters for the given task. To\nidentify and activate effective parameters, we jointly optimize the sparse mask\npredictor and the LLM, leveraging both instruction-following data and the\npre-training corpus. Experimental results demonstrate the effectiveness of our\napproach on a wide range of evaluation benchmarks. For example, our 3B\nactivated model improves over the 3B dense model by 5-8 points of absolute\nmargin on domains such as math and coding, and rivals the performance of a 9B\nmodel."}
{"id": "2501.02295", "pdf": "https://arxiv.org/pdf/2501.02295.pdf", "abs": "https://arxiv.org/abs/2501.02295", "title": "Explicit vs. Implicit: Investigating Social Bias in Large Language Models through Self-Reflection", "authors": ["Yachao Zhao", "Bo Wang", "Yan Wang", "Dongming Zhao", "Ruifang He", "Yuexian Hou"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025", "summary": "Large Language Models (LLMs) have been shown to exhibit various biases and\nstereotypes in their generated content. While extensive research has\ninvestigated biases in LLMs, prior work has predominantly focused on explicit\nbias, with minimal attention to implicit bias and the relation between these\ntwo forms of bias. This paper presents a systematic framework grounded in\nsocial psychology theories to investigate and compare explicit and implicit\nbiases in LLMs. We propose a novel self-reflection-based evaluation framework\nthat operates in two phases: first measuring implicit bias through simulated\npsychological assessment methods, then evaluating explicit bias by prompting\nLLMs to analyze their own generated content. Through extensive experiments on\nadvanced LLMs across multiple social dimensions, we demonstrate that LLMs\nexhibit a substantial inconsistency between explicit and implicit biases: while\nexplicit bias manifests as mild stereotypes, implicit bias exhibits strong\nstereotypes. We further investigate the underlying factors contributing to this\nexplicit-implicit bias inconsistency, examining the effects of training data\nscale, model size, and alignment techniques. Experimental results indicate that\nwhile explicit bias declines with increased training data and model size,\nimplicit bias exhibits a contrasting upward trend. Moreover, contemporary\nalignment methods effectively suppress explicit bias but show limited efficacy\nin mitigating implicit bias."}
{"id": "2501.03835", "pdf": "https://arxiv.org/pdf/2501.03835.pdf", "abs": "https://arxiv.org/abs/2501.03835", "title": "TACLR: A Scalable and Efficient Retrieval-based Method for Industrial Product Attribute Value Identification", "authors": ["Yindu Su", "Huike Zou", "Lin Sun", "Ting Zhang", "Haiyang Yang", "Liyu Chen", "David Lo", "Qingheng Zhang", "Shuguang Han", "Jufeng Chen"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Accepted at ACL 2025", "summary": "Product Attribute Value Identification (PAVI) involves identifying attribute\nvalues from product profiles, a key task for improving product search,\nrecommendation, and business analytics on e-commerce platforms. However,\nexisting PAVI methods face critical challenges, such as inferring implicit\nvalues, handling out-of-distribution (OOD) values, and producing normalized\noutputs. To address these limitations, we introduce Taxonomy-Aware Contrastive\nLearning Retrieval (TACLR), the first retrieval-based method for PAVI. TACLR\nformulates PAVI as an information retrieval task by encoding product profiles\nand candidate values into embeddings and retrieving values based on their\nsimilarity. It leverages contrastive training with taxonomy-aware hard negative\nsampling and employs adaptive inference with dynamic thresholds. TACLR offers\nthree key advantages: (1) it effectively handles implicit and OOD values while\nproducing normalized outputs; (2) it scales to thousands of categories, tens of\nthousands of attributes, and millions of values; and (3) it supports efficient\ninference for high-load industrial deployment. Extensive experiments on\nproprietary and public datasets validate the effectiveness and efficiency of\nTACLR. Further, it has been successfully deployed on the real-world e-commerce\nplatform Xianyu, processing millions of product listings daily with frequently\nupdated, large-scale attribute taxonomies. We release the code to facilitate\nreproducibility and future research at https://github.com/SuYindu/TACLR."}
{"id": "2501.06645", "pdf": "https://arxiv.org/pdf/2501.06645.pdf", "abs": "https://arxiv.org/abs/2501.06645", "title": "FocalPO: Enhancing Preference Optimizing by Focusing on Correct Preference Rankings", "authors": ["Tong Liu", "Xiao Yu", "Wenxuan Zhou", "Jindong Gu", "Volker Tresp"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025", "summary": "Efficient preference optimization algorithms such as Direct Preference\nOptimization (DPO) have become a popular approach in aligning large language\nmodels (LLMs) with human preferences. These algorithms implicitly treat the LLM\nas a reward model, and focus on training it to correct misranked preference\npairs. However, recent work~\\citep{chen2024preference} empirically finds that\nDPO training \\textit{rarely improves these misranked preference pairs}, despite\nits gradient emphasizing on these cases. We introduce FocalPO, a DPO variant\nthat instead \\textit{down-weighs} misranked preference pairs and prioritizes\nenhancing the model's understanding of pairs that it can already rank\ncorrectly. Inspired by Focal Loss used in vision tasks, FocalPO achieves this\nby adding a modulating factor to dynamically scale DPO loss. Our experiment\ndemonstrates that FocalPO surpasses DPO and its variants on popular benchmarks\nlike Alpaca Eval 2.0 using Mistral-Base-7B and Llama-3-Instruct-8B, with the\nintroduced hyperparameter fixed. Additionally, we empirically reveals how\nFocalPO affects training on correct and incorrect sample groups, further\nunderscoring its effectiveness."}
{"id": "2501.15781", "pdf": "https://arxiv.org/pdf/2501.15781.pdf", "abs": "https://arxiv.org/abs/2501.15781", "title": "Large Language Models to Diffusion Finetuning", "authors": ["Edoardo Cetin", "Tianyu Zhao", "Yujin Tang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Camera-ready version, presented at ICML 2025. Code available at:\n  https://github.com/SakanaAI/L2D", "summary": "We propose a new finetuning method to provide pre-trained large language\nmodels (LMs) the ability to scale test-time compute through the diffusion\nframework. By increasing the number of diffusion steps, we show our finetuned\nmodels achieve monotonically increasing accuracy, directly translating to\nimproved performance across downstream tasks. Furthermore, our finetuned models\ncan expertly answer questions on specific topics by integrating powerful\nguidance techniques, and autonomously determine the compute required for a\ngiven problem by leveraging adaptive ODE solvers. Our method is universally\napplicable to any foundation model pre-trained with a cross-entropy loss and\ndoes not modify any of its original weights, fully preserving its strong\nsingle-step generation capabilities. We show our method is more effective and\nfully compatible with traditional finetuning approaches, introducing an\northogonal new direction to unify the strengths of the autoregressive and\ndiffusion frameworks."}
{"id": "2502.00334", "pdf": "https://arxiv.org/pdf/2502.00334.pdf", "abs": "https://arxiv.org/abs/2502.00334", "title": "UGPhysics: A Comprehensive Benchmark for Undergraduate Physics Reasoning with Large Language Models", "authors": ["Xin Xu", "Qiyun Xu", "Tong Xiao", "Tianhao Chen", "Yuchen Yan", "Jiaxin Zhang", "Shizhe Diao", "Can Yang", "Yang Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ICML 2025", "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nsolving complex reasoning tasks, particularly in mathematics. However, the\ndomain of physics reasoning presents unique challenges that have received\nsignificantly less attention. Existing benchmarks often fall short in\nevaluating LLMs' abilities on the breadth and depth of undergraduate-level\nphysics, underscoring the need for a comprehensive evaluation. To fill this\ngap, we introduce UGPhysics, a large-scale and comprehensive benchmark\nspecifically designed to evaluate UnderGraduate-level Physics (UGPhysics)\nreasoning with LLMs. UGPhysics includes 5,520 undergraduate-level physics\nproblems in both English and Chinese, covering 13 subjects with seven different\nanswer types and four distinct physics reasoning skills, all rigorously\nscreened for data leakage. Additionally, we develop a Model-Assistant\nRule-based Judgment (MARJ) pipeline specifically tailored for assessing answer\ncorrectness of physics problems, ensuring accurate evaluation. Our evaluation\nof 31 leading LLMs shows that the highest overall accuracy, 49.8% (achieved by\nOpenAI-o1-mini), emphasizes the necessity for models with stronger physics\nreasoning skills, beyond math abilities. We hope UGPhysics, along with MARJ,\nwill drive future advancements in AI for physics reasoning. Codes and data are\navailable at https://github.com/YangLabHKUST/UGPhysics ."}
{"id": "2502.08246", "pdf": "https://arxiv.org/pdf/2502.08246.pdf", "abs": "https://arxiv.org/abs/2502.08246", "title": "Inference-time sparse attention with asymmetric indexing", "authors": ["Pierre-Emmanuel Mazaré", "Gergely Szilvasy", "Maria Lomeli", "Francisco Massa", "Naila Murray", "Hervé Jégou", "Matthijs Douze"], "categories": ["cs.CL"], "comment": null, "summary": "Self-attention in transformer models is an incremental associative memory\nthat maps key vectors to value vectors. One way to speed up self-attention is\nto employ GPU-compatible vector search algorithms based on standard\npartitioning methods such as k-means. However, such partitioning methods yield\npoor results in this context because (1) the keys and queries follow different\ndistributions, and (2) the RoPE positional encoding hinders the bucket\nassignment.\n  This paper introduces Saap (Self-Attention with Asymmetric Partitions), which\novercomes these problems. It is an asymmetrical indexing technique that employs\ndistinct partitions for keys and queries, thereby approximating self-attention\nwith a data-adaptive sparsity pattern. It works on pretrained language models\nand only requires to train (offline) a small query classifier. On a long\ncontext Llama 3.1-8b model, with sequences ranging from 100k to 500k tokens,\nSaap typically reduces by a factor of 20 the fraction of memory that needs to\nbe looked-up, which translates to a time saving of 60\\% when compared to\nFlashAttention-v2."}
{"id": "2502.09416", "pdf": "https://arxiv.org/pdf/2502.09416.pdf", "abs": "https://arxiv.org/abs/2502.09416", "title": "Rethinking Evaluation Metrics for Grammatical Error Correction: Why Use a Different Evaluation Process than Human?", "authors": ["Takumi Goto", "Yusuke Sakai", "Taro Watanabe"], "categories": ["cs.CL"], "comment": "ACL 2025 (Main), 5 pages, 2 figures", "summary": "One of the goals of automatic evaluation metrics in grammatical error\ncorrection (GEC) is to rank GEC systems such that it matches human preferences.\nHowever, current automatic evaluations are based on procedures that diverge\nfrom human evaluation. Specifically, human evaluation derives rankings by\naggregating sentence-level relative evaluation results, e.g., pairwise\ncomparisons, using a rating algorithm, whereas automatic evaluation averages\nsentence-level absolute scores to obtain corpus-level scores, which are then\nsorted to determine rankings. In this study, we propose an aggregation method\nfor existing automatic evaluation metrics which aligns with human evaluation\nmethods to bridge this gap. We conducted experiments using various metrics,\nincluding edit-based metrics, n-gram based metrics, and sentence-level metrics,\nand show that resolving the gap improves results for the most of metrics on the\nSEEDA benchmark. We also found that even BERT-based metrics sometimes\noutperform the metrics of GPT-4. The proposed ranking method is integrated\ngec-metrics."}
{"id": "2502.10973", "pdf": "https://arxiv.org/pdf/2502.10973.pdf", "abs": "https://arxiv.org/abs/2502.10973", "title": "Akan Cinematic Emotions (ACE): A Multimodal Multi-party Dataset for Emotion Recognition in Movie Dialogues", "authors": ["David Sasu", "Zehui Wu", "Ziwei Gong", "Run Chen", "Pengyuan Shi", "Lin Ai", "Julia Hirschberg", "Natalie Schluter"], "categories": ["cs.CL"], "comment": "Accepted to Findings at ACL 2025", "summary": "In this paper, we introduce the Akan Conversation Emotion (ACE) dataset, the\nfirst multimodal emotion dialogue dataset for an African language, addressing\nthe significant lack of resources for low-resource languages in emotion\nrecognition research. ACE, developed for the Akan language, contains 385\nemotion-labeled dialogues and 6,162 utterances across audio, visual, and\ntextual modalities, along with word-level prosodic prominence annotations. The\npresence of prosodic labels in this dataset also makes it the first\nprosodically annotated African language dataset. We demonstrate the quality and\nutility of ACE through experiments using state-of-the-art emotion recognition\nmethods, establishing solid baselines for future research. We hope ACE inspires\nfurther work on inclusive, linguistically and culturally diverse NLP resources."}
{"id": "2502.11075", "pdf": "https://arxiv.org/pdf/2502.11075.pdf", "abs": "https://arxiv.org/abs/2502.11075", "title": "Exposing Numeracy Gaps: A Benchmark to Evaluate Fundamental Numerical Abilities in Large Language Models", "authors": ["Haoyang Li", "Xuejia Chen", "Zhanchao XU", "Darian Li", "Nicole Hu", "Fei Teng", "Yiming Li", "Luyu Qiu", "Chen Jason Zhang", "Qing Li", "Lei Chen"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025", "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nnatural language processing tasks, such as text generation and semantic\nunderstanding. However, their performance on numerical reasoning tasks, such as\nbasic arithmetic, numerical retrieval, and magnitude comparison, remains\nsurprisingly poor. This gap arises from their reliance on surface-level\nstatistical patterns rather than understanding numbers as continuous\nmagnitudes. Existing benchmarks primarily focus on either linguistic competence\nor structured mathematical problem-solving, neglecting fundamental numerical\nreasoning required in real-world scenarios. To bridge this gap, we propose\nNumericBench, a comprehensive benchmark to evaluate six fundamental numerical\ncapabilities: number recognition, arithmetic operations, contextual retrieval,\ncomparison, summary, and logical reasoning. NumericBench includes datasets\nranging from synthetic number lists to the crawled real-world data, addressing\nchallenges like long contexts, noise, and multi-step reasoning. Extensive\nexperiments on state-of-the-art LLMs, including GPT-4 and DeepSeek, reveal\npersistent weaknesses in numerical reasoning, highlighting the urgent need to\nimprove numerically-aware language modeling. The benchmark is released in:\nhttps://github.com/TreeAI-Lab/NumericBench."}
{"id": "2502.11184", "pdf": "https://arxiv.org/pdf/2502.11184.pdf", "abs": "https://arxiv.org/abs/2502.11184", "title": "Can't See the Forest for the Trees: Benchmarking Multimodal Safety Awareness for Multimodal LLMs", "authors": ["Wenxuan Wang", "Xiaoyuan Liu", "Kuiyi Gao", "Jen-tse Huang", "Youliang Yuan", "Pinjia He", "Shuai Wang", "Zhaopeng Tu"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.MM"], "comment": "Accepted by ACL 2025", "summary": "Multimodal Large Language Models (MLLMs) have expanded the capabilities of\ntraditional language models by enabling interaction through both text and\nimages. However, ensuring the safety of these models remains a significant\nchallenge, particularly in accurately identifying whether multimodal content is\nsafe or unsafe-a capability we term safety awareness. In this paper, we\nintroduce MMSafeAware, the first comprehensive multimodal safety awareness\nbenchmark designed to evaluate MLLMs across 29 safety scenarios with 1500\ncarefully curated image-prompt pairs. MMSafeAware includes both unsafe and\nover-safety subsets to assess models abilities to correctly identify unsafe\ncontent and avoid over-sensitivity that can hinder helpfulness. Evaluating nine\nwidely used MLLMs using MMSafeAware reveals that current models are not\nsufficiently safe and often overly sensitive; for example, GPT-4V misclassifies\n36.1% of unsafe inputs as safe and 59.9% of benign inputs as unsafe. We further\nexplore three methods to improve safety awareness-prompting-based approaches,\nvisual contrastive decoding, and vision-centric reasoning fine-tuning-but find\nthat none achieve satisfactory performance. Our findings highlight the profound\nchallenges in developing MLLMs with robust safety awareness, underscoring the\nneed for further research in this area. All the code and data will be publicly\navailable to facilitate future research."}
{"id": "2502.12562", "pdf": "https://arxiv.org/pdf/2502.12562.pdf", "abs": "https://arxiv.org/abs/2502.12562", "title": "SEA: Low-Resource Safety Alignment for Multimodal Large Language Models via Synthetic Embeddings", "authors": ["Weikai Lu", "Hao Peng", "Huiping Zhuang", "Cen Chen", "Ziqian Zeng"], "categories": ["cs.CL", "cs.CR", "cs.MM"], "comment": "Accepted in ACL 2025 Main Track", "summary": "Multimodal Large Language Models (MLLMs) have serious security\nvulnerabilities.While safety alignment using multimodal datasets consisting of\ntext and data of additional modalities can effectively enhance MLLM's security,\nit is costly to construct these datasets. Existing low-resource security\nalignment methods, including textual alignment, have been found to struggle\nwith the security risks posed by additional modalities. To address this, we\npropose Synthetic Embedding augmented safety Alignment (SEA), which optimizes\nembeddings of additional modality through gradient updates to expand textual\ndatasets. This enables multimodal safety alignment training even when only\ntextual data is available. Extensive experiments on image, video, and\naudio-based MLLMs demonstrate that SEA can synthesize a high-quality embedding\non a single RTX3090 GPU within 24 seconds. SEA significantly improves the\nsecurity of MLLMs when faced with threats from additional modalities. To assess\nthe security risks introduced by video and audio, we also introduced a new\nbenchmark called VA-SafetyBench. High attack success rates across multiple\nMLLMs validate its challenge. Our code and data will be available at\nhttps://github.com/ZeroNLP/SEA."}
{"id": "2502.12665", "pdf": "https://arxiv.org/pdf/2502.12665.pdf", "abs": "https://arxiv.org/abs/2502.12665", "title": "A$^2$ATS: Retrieval-Based KV Cache Reduction via Windowed Rotary Position Embedding and Query-Aware Vector Quantization", "authors": ["Junhui He", "Junna Xing", "Nan Wang", "Rui Xu", "Shangyu Wu", "Peng Zhou", "Qiang Liu", "Chun Jason Xue", "Qingan Li"], "categories": ["cs.CL"], "comment": null, "summary": "Long context large language models (LLMs) pose significant challenges for\nefficient serving due to the large memory footprint and high access overhead of\nKV cache. Retrieval-based KV cache reduction methods can mitigate these\nchallenges, typically by offloading the complete KV cache to CPU and retrieving\nnecessary tokens on demand during inference. However, these methods still\nsuffer from unsatisfactory accuracy degradation and extra retrieval overhead.\nTo address these limitations, this paper proposes A$^2$ATS, a novel\nretrieval-based KV cache reduction method. A$^2$ATS aims to obtain an accurate\napproximation of attention scores by applying the vector quantization technique\nto key states, thereby enabling efficient and precise retrieval of the top-K\ntokens. First, we propose Windowed Rotary Position Embedding, which decouples\nthe positional dependency from query and key states after position embedding.\nThen, we propose query-aware vector quantization that optimizes the objective\nof attention score approximation directly. Finally, we design the heterogeneous\ninference architecture for KV cache offloading, enabling long context serving\nwith larger batch sizes. Experimental results demonstrate that A$^2$ATS can\nachieve a lower performance degradation with similar or lower overhead compared\nto existing methods, thereby increasing long context serving throughput by up\nto $2.7 \\times$."}
{"id": "2502.12921", "pdf": "https://arxiv.org/pdf/2502.12921.pdf", "abs": "https://arxiv.org/abs/2502.12921", "title": "Q-STRUM Debate: Query-Driven Contrastive Summarization for Recommendation Comparison", "authors": ["George-Kirollos Saad", "Scott Sanner"], "categories": ["cs.CL"], "comment": null, "summary": "Query-driven recommendation with unknown items poses a challenge for users to\nunderstand why certain items are appropriate for their needs. Query-driven\nContrastive Summarization (QCS) is a methodology designed to address this issue\nby leveraging language-based item descriptions to clarify contrasts between\nthem. However, existing state-of-the-art contrastive summarization methods such\nas STRUM-LLM fall short of this goal. To overcome these limitations, we\nintroduce Q-STRUM Debate, a novel extension of STRUM-LLM that employs\ndebate-style prompting to generate focused and contrastive summarizations of\nitem aspects relevant to a query. Leveraging modern large language models\n(LLMs) as powerful tools for generating debates, Q-STRUM Debate provides\nenhanced contrastive summaries. Experiments across three datasets demonstrate\nthat Q-STRUM Debate yields significant performance improvements over existing\nmethods on key contrastive summarization criteria, thus introducing a novel and\nperformant debate prompting methodology for QCS."}
{"id": "2502.14376", "pdf": "https://arxiv.org/pdf/2502.14376.pdf", "abs": "https://arxiv.org/abs/2502.14376", "title": "A Similarity Paradigm Through Textual Regularization Without Forgetting", "authors": ["Fangming Cui", "Jan Fong", "Rongfei Zeng", "Xinmei Tian", "Jun Yu"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Prompt learning has emerged as a promising method for adapting pre-trained\nvisual-language models (VLMs) to a range of downstream tasks. While optimizing\nthe context can be effective for improving performance on specific tasks, it\ncan often lead to poor generalization performance on unseen classes or datasets\nsampled from different distributions. It may be attributed to the fact that\ntextual prompts tend to overfit downstream data distributions, leading to the\nforgetting of generalized knowledge derived from hand-crafted prompts. In this\npaper, we propose a novel method called Similarity Paradigm with Textual\nRegularization (SPTR) for prompt learning without forgetting. SPTR is a\ntwo-pronged design based on hand-crafted prompts that is an inseparable\nframework. 1) To avoid forgetting general textual knowledge, we introduce the\noptimal transport as a textual regularization to finely ensure approximation\nwith hand-crafted features and tuning textual features. 2) In order to\ncontinuously unleash the general ability of multiple hand-crafted prompts, we\npropose a similarity paradigm for natural alignment score and adversarial\nalignment score to improve model robustness for generalization. Both modules\nshare a common objective in addressing generalization issues, aiming to\nmaximize the generalization capability derived from multiple hand-crafted\nprompts. Four representative tasks (i.e., non-generalization few-shot learning,\nbase-to-novel generalization, cross-dataset generalization, domain\ngeneralization) across 11 datasets demonstrate that SPTR outperforms existing\nprompt learning methods."}
{"id": "2502.15109", "pdf": "https://arxiv.org/pdf/2502.15109.pdf", "abs": "https://arxiv.org/abs/2502.15109", "title": "Social Genome: Grounded Social Reasoning Abilities of Multimodal Models", "authors": ["Leena Mathur", "Marian Qian", "Paul Pu Liang", "Louis-Philippe Morency"], "categories": ["cs.CL", "cs.LG"], "comment": "Under Review, 24 pages", "summary": "Social reasoning abilities are crucial for AI systems to effectively\ninterpret and respond to multimodal human communication and interaction within\nsocial contexts. We introduce SOCIAL GENOME, the first benchmark for\nfine-grained, grounded social reasoning abilities of multimodal models. SOCIAL\nGENOME contains 272 videos of interactions and 1,486 human-annotated reasoning\ntraces related to inferences about these interactions. These traces contain\n5,777 reasoning steps that reference evidence from visual cues, verbal cues,\nvocal cues, and external knowledge (contextual knowledge external to videos).\nSOCIAL GENOME is also the first modeling challenge to study external knowledge\nin social reasoning. SOCIAL GENOME computes metrics to holistically evaluate\nsemantic and structural qualities of model-generated social reasoning traces.\nWe demonstrate the utility of SOCIAL GENOME through experiments with\nstate-of-the-art models, identifying performance gaps and opportunities for\nfuture research to improve the grounded social reasoning abilities of\nmultimodal models."}
{"id": "2502.17110", "pdf": "https://arxiv.org/pdf/2502.17110.pdf", "abs": "https://arxiv.org/abs/2502.17110", "title": "Mobile-Agent-V: A Video-Guided Approach for Effortless and Efficient Operational Knowledge Injection in Mobile Automation", "authors": ["Junyang Wang", "Haiyang Xu", "Xi Zhang", "Ming Yan", "Ji Zhang", "Fei Huang", "Jitao Sang"], "categories": ["cs.CL", "cs.CV"], "comment": "17 pages, 7 figures, 9 tables", "summary": "The exponential rise in mobile device usage necessitates streamlined\nautomation for effective task management, yet many AI frameworks fall short due\nto inadequate operational expertise. While manually written knowledge can\nbridge this gap, it is often burdensome and inefficient. We introduce\nMobile-Agent-V, an innovative framework that utilizes video as a guiding tool\nto effortlessly and efficiently inject operational knowledge into mobile\nautomation processes. By deriving knowledge directly from video content,\nMobile-Agent-V eliminates manual intervention, significantly reducing the\neffort and time required for knowledge acquisition. To rigorously evaluate this\napproach, we propose Mobile-Knowledge, a benchmark tailored to assess the\nimpact of external knowledge on mobile agent performance. Our experimental\nfindings demonstrate that Mobile-Agent-V enhances performance by 36% compared\nto existing methods, underscoring its effortless and efficient advantages in\nmobile automation."}
{"id": "2502.17214", "pdf": "https://arxiv.org/pdf/2502.17214.pdf", "abs": "https://arxiv.org/abs/2502.17214", "title": "CoT-UQ: Improving Response-wise Uncertainty Quantification in LLMs with Chain-of-Thought", "authors": ["Boxuan Zhang", "Ruqi Zhang"], "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": "Accepted by ACL 2025 Findings", "summary": "Large language models (LLMs) excel in many tasks but struggle to accurately\nquantify uncertainty in their generated responses. This limitation makes it\nchallenging to detect misinformation and ensure reliable decision-making.\nExisting uncertainty quantification (UQ) methods for LLMs are primarily\nprompt-wise rather than response-wise, often requiring multiple response\nsamples, which incurs high computational costs. Moreover, LLMs have been shown\nto be overconfident, particularly when using reasoning steps to derive their\nanswers. In this work, we propose CoT-UQ, a response-wise UQ framework that\nintegrates LLMs' inherent reasoning capabilities through Chain-of-Thought (CoT)\ninto the UQ process. CoT-UQ captures critical information during inference by\nextracting keywords from each reasoning step and assessing their importance to\nthe final answer. This key reasoning information is then aggregated to produce\na final uncertainty estimate. We conduct extensive experiments based on Llama\nFamily with model sizes varying from 8B to 13B across logical and mathematical\nreasoning tasks. Experimental results demonstrate that CoT-UQ significantly\noutperforms existing UQ methods, achieving an average improvement of 5.9% AUROC\ncompared to current UQ methods. The code is available at:\nhttps://github.com/ZBox1005/CoT-UQ."}
{"id": "2502.17878", "pdf": "https://arxiv.org/pdf/2502.17878.pdf", "abs": "https://arxiv.org/abs/2502.17878", "title": "Towards Enhanced Immersion and Agency for LLM-based Interactive Drama", "authors": ["Hongqiu Wu", "Weiqi Wu", "Tianyang Xu", "Jiameng Zhang", "Hai Zhao"], "categories": ["cs.CL"], "comment": "Accepted by ACL'2025", "summary": "LLM-based Interactive Drama is a novel AI-based dialogue scenario, where the\nuser (i.e. the player) plays the role of a character in the story, has\nconversations with characters played by LLM agents, and experiences an\nunfolding story. This paper begins with understanding interactive drama from\ntwo aspects: Immersion, the player's feeling of being present in the story, and\nAgency, the player's ability to influence the story world. Both are crucial to\ncreating an enjoyable interactive experience, while they have been\nunderexplored in previous work. To enhance these two aspects, we first propose\nPlaywriting-guided Generation, a novel method that helps LLMs craft dramatic\nstories with substantially improved structures and narrative quality.\nAdditionally, we introduce Plot-based Reflection for LLM agents to refine their\nreactions to align with the player's intentions. Our evaluation relies on human\njudgment to assess the gains of our methods in terms of immersion and agency."}
{"id": "2502.18460", "pdf": "https://arxiv.org/pdf/2502.18460.pdf", "abs": "https://arxiv.org/abs/2502.18460", "title": "DRAMA: Diverse Augmentation from Large Language Models to Smaller Dense Retrievers", "authors": ["Xueguang Ma", "Xi Victoria Lin", "Barlas Oguz", "Jimmy Lin", "Wen-tau Yih", "Xilun Chen"], "categories": ["cs.CL", "cs.IR"], "comment": "ACL 2025", "summary": "Large language models (LLMs) have demonstrated strong effectiveness and\nrobustness while fine-tuned as dense retrievers. However, their large parameter\nsize brings significant inference time computational challenges, including high\nencoding costs for large-scale corpora and increased query latency, limiting\ntheir practical deployment. While smaller retrievers offer better efficiency,\nthey often fail to generalize effectively with limited supervised fine-tuning\ndata. In this work, we introduce DRAMA, a training framework that leverages\nLLMs to train smaller generalizable dense retrievers. In particular, we adopt\npruned LLMs as the backbone and train on diverse LLM-augmented data in a\nsingle-stage contrastive learning setup. Experiments show that DRAMA offers\nbetter multilingual and long-context capabilities than traditional\nencoder-based retrievers, and achieves strong performance across multiple tasks\nand languages. These highlight the potential of connecting the training of\nsmaller retrievers with the growing advancements in LLMs, bridging the gap\nbetween efficiency and generalization."}
{"id": "2502.19756", "pdf": "https://arxiv.org/pdf/2502.19756.pdf", "abs": "https://arxiv.org/abs/2502.19756", "title": "PolyPrompt: Automating Knowledge Extraction from Multilingual Language Models with Dynamic Prompt Generation", "authors": ["Nathan Roll"], "categories": ["cs.CL", "cs.LG"], "comment": "6 pages, 2 figures", "summary": "Large language models (LLMs) showcase increasingly impressive English\nbenchmark scores, however their performance profiles remain inconsistent across\nmultilingual settings. To address this gap, we introduce PolyPrompt, a novel,\nparameter-efficient framework for enhancing the multilingual capabilities of\nLLMs. Our method learns a set of trigger tokens for each language through a\ngradient-based search, identifying the input query's language and selecting the\ncorresponding trigger tokens which are prepended to the prompt during\ninference. We perform experiments on two ~1 billion parameter models, with\nevaluations on the global MMLU benchmark across fifteen typologically and\nresource diverse languages, demonstrating accuracy gains of 3.7%-19.9% compared\nto naive and translation-pipeline baselines."}
{"id": "2502.20129", "pdf": "https://arxiv.org/pdf/2502.20129.pdf", "abs": "https://arxiv.org/abs/2502.20129", "title": "Finite State Automata Inside Transformers with Chain-of-Thought: A Mechanistic Study on State Tracking", "authors": ["Yifan Zhang", "Wenyu Du", "Dongming Jin", "Jie Fu", "Zhi Jin"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Chain-of-thought (CoT) significantly enhances the performance of large\nlanguage models (LLMs) across a wide range of tasks, and prior research shows\nthat CoT can theoretically increase expressiveness. However, there is limited\nmechanistic understanding of the algorithms that Transformer+CoT can learn. Our\nkey contributions are: (1) We evaluate the state tracking capabilities of\nTransformer+CoT and its variants, confirming the effectiveness of CoT. (2)\nNext, we identify the circuit (a subset of model components, responsible for\ntracking the world state), indicating that late-layer MLP neurons play a key\nrole. We propose two metrics, compression and distinction, and show that the\nneuron sets for each state achieve nearly 100% accuracy, providing evidence of\nan implicit finite state automaton (FSA) embedded within the model. (3)\nAdditionally, we explore three challenging settings: skipping intermediate\nsteps, introducing data noises, and testing length generalization. Our results\ndemonstrate that Transformer+CoT learns robust algorithms (FSAs), highlighting\nits resilience in challenging scenarios. Our code is available at\nhttps://github.com/IvanChangPKU/FSA."}
{"id": "2503.01926", "pdf": "https://arxiv.org/pdf/2503.01926.pdf", "abs": "https://arxiv.org/abs/2503.01926", "title": "Unnatural Languages Are Not Bugs but Features for LLMs", "authors": ["Keyu Duan", "Yiran Zhao", "Zhili Feng", "Jinjie Ni", "Tianyu Pang", "Qian Liu", "Tianle Cai", "Longxu Dou", "Kenji Kawaguchi", "Anirudh Goyal", "J. Zico Kolter", "Michael Qizhe Shieh"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have been observed to process non-human-readable\ntext sequences, such as jailbreak prompts, often viewed as a bug for aligned\nLLMs. In this work, we present a systematic investigation challenging this\nperception, demonstrating that unnatural languages - strings that appear\nincomprehensible to humans but maintain semantic meanings for LLMs - contain\nlatent features usable by models. Notably, unnatural languages possess latent\nfeatures that can be generalized across different models and tasks during\ninference. Furthermore, models fine-tuned on unnatural versions of instruction\ndatasets perform on-par with those trained on natural language, achieving 49.71\nwin rates in Length-controlled AlpacaEval 2.0 in average across various base\nmodels. In addition, through comprehensive analysis, we demonstrate that LLMs\nprocess unnatural languages by filtering noise and inferring contextual meaning\nfrom filtered words."}
{"id": "2503.02519", "pdf": "https://arxiv.org/pdf/2503.02519.pdf", "abs": "https://arxiv.org/abs/2503.02519", "title": "Generator-Assistant Stepwise Rollback Framework for Large Language Model Agent", "authors": ["Xingzuo Li", "Kehai Chen", "Yunfei Long", "Xuefeng Bai", "Yong Xu", "Min Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Large language model (LLM) agents typically adopt a step-by-step reasoning\nframework, in which they interleave the processes of thinking and acting to\naccomplish the given task. However, this paradigm faces a deep-rooted one-pass\nissue whereby each generated intermediate thought is plugged into the\ntrajectory regardless of its correctness, which can cause irreversible error\npropagation. To address the issue, this paper proposes a novel framework called\nGenerator-Assistant Stepwise Rollback (GA-Rollback) to induce better\ndecision-making for LLM agents. Particularly, GA-Rollback utilizes a generator\nto interact with the environment and an assistant to examine each action\nproduced by the generator, where the assistant triggers a rollback operation\nupon detection of incorrect actions. Moreover, we introduce two additional\nstrategies tailored for the rollback scenario to further improve its\neffectiveness. Extensive experiments show that GA-Rollback achieves significant\nimprovements over several strong baselines on three widely used benchmarks. Our\nanalysis further reveals that GA-Rollback can function as a robust\nplug-and-play module, integrating seamlessly with other methods."}
{"id": "2503.05037", "pdf": "https://arxiv.org/pdf/2503.05037.pdf", "abs": "https://arxiv.org/abs/2503.05037", "title": "Collapse of Dense Retrievers: Short, Early, and Literal Biases Outranking Factual Evidence", "authors": ["Mohsen Fayyaz", "Ali Modarressi", "Hinrich Schuetze", "Nanyun Peng"], "categories": ["cs.CL", "cs.IR"], "comment": "ACL 2025 Main Conference", "summary": "Dense retrieval models are commonly used in Information Retrieval (IR)\napplications, such as Retrieval-Augmented Generation (RAG). Since they often\nserve as the first step in these systems, their robustness is critical to avoid\ndownstream failures. In this work, we repurpose a relation extraction dataset\n(e.g., Re-DocRED) to design controlled experiments that quantify the impact of\nheuristic biases, such as a preference for shorter documents, on retrievers\nlike Dragon+ and Contriever. We uncover major vulnerabilities, showing\nretrievers favor shorter documents, early positions, repeated entities, and\nliteral matches, all while ignoring the answer's presence! Notably, when\nmultiple biases combine, models exhibit catastrophic performance degradation,\nselecting the answer-containing document in less than 10% of cases over a\nsynthetic biased document without the answer. Furthermore, we show that these\nbiases have direct consequences for downstream applications like RAG, where\nretrieval-preferred documents can mislead LLMs, resulting in a 34% performance\ndrop than providing no documents at all.\nhttps://huggingface.co/datasets/mohsenfayyaz/ColDeR"}
{"id": "2503.10927", "pdf": "https://arxiv.org/pdf/2503.10927.pdf", "abs": "https://arxiv.org/abs/2503.10927", "title": "OASST-ETC Dataset: Alignment Signals from Eye-tracking Analysis of LLM Responses", "authors": ["Angela Lopez-Cardona", "Sebastian Idesis", "Miguel Barreda-Ángeles", "Sergi Abadal", "Ioannis Arapakis"], "categories": ["cs.CL", "cs.AI"], "comment": "This paper has been accepted to ACM ETRA 2025 and published on\n  PACMHCI", "summary": "While Large Language Models (LLMs) have significantly advanced natural\nlanguage processing, aligning them with human preferences remains an open\nchallenge. Although current alignment methods rely primarily on explicit\nfeedback, eye-tracking (ET) data offers insights into real-time cognitive\nprocessing during reading. In this paper, we present OASST-ETC, a novel\neye-tracking corpus capturing reading patterns from 24 participants, while\nevaluating LLM-generated responses from the OASST1 dataset. Our analysis\nreveals distinct reading patterns between preferred and non-preferred\nresponses, which we compare with synthetic eye-tracking data. Furthermore, we\nexamine the correlation between human reading measures and attention patterns\nfrom various transformer-based models, discovering stronger correlations in\npreferred responses. This work introduces a unique resource for studying human\ncognitive processing in LLM evaluation and suggests promising directions for\nincorporating eye-tracking data into alignment methods. The dataset and\nanalysis code are publicly available."}
{"id": "2503.11630", "pdf": "https://arxiv.org/pdf/2503.11630.pdf", "abs": "https://arxiv.org/abs/2503.11630", "title": "The time scale of redundancy between prosody and linguistic context", "authors": ["Tamar I. Regev", "Chiebuka Ohams", "Shaylee Xie", "Lukas Wolf", "Evelina Fedorenko", "Alex Warstadt", "Ethan G. Wilcox", "Tiago Pimentel"], "categories": ["cs.CL", "cs.IT", "math.IT"], "comment": "13 pages, 4 figures, accepted to ACL. Updated following ACL reviewers\n  comments", "summary": "In spoken communication, information is transmitted not only via words, but\nalso through a rich array of non-verbal signals, including prosody--the\nnon-segmental auditory features of speech. Do these different communication\nchannels carry distinct information? Prior work has shown that the information\ncarried by prosodic features is substantially redundant with that carried by\nthe surrounding words. Here, we systematically examine the time scale of this\nrelationship, studying how it varies with the length of past and future\ncontexts. We find that a word's prosodic features require an extended past\ncontext (3-8 words across different features) to be reliably predicted. Given\nthat long-scale contextual information decays in memory, prosody may facilitate\ncommunication by adding information that is locally unique. We also find that a\nword's prosodic features show some redundancy with future words, but only with\na short scale of 1-2 words, consistent with reports of incremental short-term\nplanning in language production. Thus, prosody may facilitate communication by\nhelping listeners predict upcoming material. In tandem, our results highlight\npotentially distinct roles that prosody plays in facilitating integration of\nwords into past contexts and in helping predict upcoming words."}
{"id": "2503.14433", "pdf": "https://arxiv.org/pdf/2503.14433.pdf", "abs": "https://arxiv.org/abs/2503.14433", "title": "Splintering Nonconcatenative Languages for Better Tokenization", "authors": ["Bar Gazit", "Shaltiel Shmidman", "Avi Shmidman", "Yuval Pinter"], "categories": ["cs.CL"], "comment": "Findings of the ACL 2025", "summary": "Common subword tokenization algorithms like BPE and UnigramLM assume that\ntext can be split into meaningful units by concatenative measures alone. This\nis not true for languages such as Hebrew and Arabic, where morphology is\nencoded in root-template patterns, or Malay and Georgian, where split affixes\nare common. We present SPLINTER, a pre-processing step which rearranges text\ninto a linear form that better represents such nonconcatenative morphologies,\nenabling meaningful contiguous segments to be found by the tokenizer. We\ndemonstrate SPLINTER's merit using both intrinsic measures evaluating token\nvocabularies in Hebrew, Arabic, and Malay; as well as on downstream tasks using\nBERT-architecture models trained for Hebrew."}
{"id": "2503.16048", "pdf": "https://arxiv.org/pdf/2503.16048.pdf", "abs": "https://arxiv.org/abs/2503.16048", "title": "Meta-Learning Neural Mechanisms rather than Bayesian Priors", "authors": ["Michael Goodale", "Salvador Mascarenhas", "Yair Lakretz"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main", "summary": "Children acquire language despite being exposed to several orders of\nmagnitude less data than large language models require. Meta-learning has been\nproposed as a way to integrate human-like learning biases into neural-network\narchitectures, combining both the structured generalizations of symbolic models\nwith the scalability of neural-network models. But what does meta-learning\nexactly imbue the model with? We investigate the meta-learning of formal\nlanguages and find that, contrary to previous claims, meta-trained models are\nnot learning simplicity-based priors when meta-trained on datasets organised\naround simplicity. Rather, we find evidence that meta-training imprints neural\nmechanisms (such as counters) into the model, which function like cognitive\nprimitives for the network on downstream tasks. Most surprisingly, we find that\nmeta-training on a single formal language can provide as much improvement to a\nmodel as meta-training on 5000 different formal languages, provided that the\nformal language incentivizes the learning of useful neural mechanisms. Taken\ntogether, our findings provide practical implications for efficient\nmeta-learning paradigms and new theoretical insights into linking symbolic\ntheories and neural mechanisms."}
{"id": "2503.17579", "pdf": "https://arxiv.org/pdf/2503.17579.pdf", "abs": "https://arxiv.org/abs/2503.17579", "title": "Leveraging Human Production-Interpretation Asymmetries to Test LLM Cognitive Plausibility", "authors": ["Suet-Ying Lam", "Qingcheng Zeng", "Jingyi Wu", "Rob Voigt"], "categories": ["cs.CL"], "comment": "ACL 2025 Camera-ready", "summary": "Whether large language models (LLMs) process language similarly to humans has\nbeen the subject of much theoretical and practical debate. We examine this\nquestion through the lens of the production-interpretation distinction found in\nhuman sentence processing and evaluate the extent to which instruction-tuned\nLLMs replicate this distinction. Using an empirically documented asymmetry\nbetween pronoun production and interpretation in humans for implicit causality\nverbs as a testbed, we find that some LLMs do quantitatively and qualitatively\nreflect human-like asymmetries between production and interpretation. We\ndemonstrate that whether this behavior holds depends upon both model size-with\nlarger models more likely to reflect human-like patterns and the choice of\nmeta-linguistic prompts used to elicit the behavior. Our codes and results are\navailable at\nhttps://github.com/LingMechLab/Production-Interpretation_Asymmetries_ACL2025."}
{"id": "2503.22395", "pdf": "https://arxiv.org/pdf/2503.22395.pdf", "abs": "https://arxiv.org/abs/2503.22395", "title": "Negation: A Pink Elephant in the Large Language Models' Room?", "authors": ["Tereza Vrabcová", "Marek Kadlčík", "Petr Sojka", "Michal Štefánik", "Michal Spiegel"], "categories": ["cs.CL"], "comment": null, "summary": "Negations are key to determining sentence meaning, making them essential for\nlogical reasoning. Despite their importance, negations pose a substantial\nchallenge for large language models (LLMs) and remain underexplored.\n  We constructed and published two new textual entailment datasets NoFEVER-ML\nand NoSNLI-ML in four languages (English, Czech, German, and Ukrainian) with\n  examples differing in negation. It allows investigation of the root causes of\nthe negation problem and its exemplification: how popular LLM model properties\nand language impact their inability to handle negation correctly.\n  Contrary to previous work, we show that increasing the model size may improve\nthe models' ability to handle negations. Furthermore, we find that both the\nmodels' reasoning accuracy and robustness to negation are language-dependent\nand that the length and explicitness of the premise have an impact on\nrobustness. There is better accuracy in projective language with fixed order,\nsuch as English, than in non-projective ones, such as German or Czech.\n  Our entailment datasets pave the way to further research for explanation and\nexemplification of the negation problem, minimization of LLM hallucinations,\nand improvement of LLM reasoning in multilingual settings."}
{"id": "2504.00589", "pdf": "https://arxiv.org/pdf/2504.00589.pdf", "abs": "https://arxiv.org/abs/2504.00589", "title": "Efficient Annotator Reliability Assessment with EffiARA", "authors": ["Owen Cook", "Jake Vasilakes", "Ian Roberts", "Xingyi Song"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Data annotation is an essential component of the machine learning pipeline;\nit is also a costly and time-consuming process. With the introduction of\ntransformer-based models, annotation at the document level is increasingly\npopular; however, there is no standard framework for structuring such tasks.\nThe EffiARA annotation framework is, to our knowledge, the first project to\nsupport the whole annotation pipeline, from understanding the resources\nrequired for an annotation task to compiling the annotated dataset and gaining\ninsights into the reliability of individual annotators as well as the dataset\nas a whole. The framework's efficacy is supported by two previous studies: one\nimproving classification performance through annotator-reliability-based\nsoft-label aggregation and sample weighting, and the other increasing the\noverall agreement among annotators through removing identifying and replacing\nan unreliable annotator. This work introduces the EffiARA Python package and\nits accompanying webtool, which provides an accessible graphical user interface\nfor the system. We open-source the EffiARA Python package at\nhttps://github.com/MiniEggz/EffiARA and the webtool is publicly accessible at\nhttps://effiara.gate.ac.uk."}
{"id": "2504.05050", "pdf": "https://arxiv.org/pdf/2504.05050.pdf", "abs": "https://arxiv.org/abs/2504.05050", "title": "Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language Models", "authors": ["Jiawei Lian", "Jianhong Pan", "Lefan Wang", "Yi Wang", "Shaohui Mei", "Lap-Pui Chau"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are foundational explorations to artificial\ngeneral intelligence, yet their alignment with human values via instruction\ntuning and preference learning achieves only superficial compliance. Here, we\ndemonstrate that harmful knowledge embedded during pretraining persists as\nindelible \"dark patterns\" in LLMs' parametric memory, evading alignment\nsafeguards and resurfacing under adversarial inducement at distributional\nshifts. In this study, we first theoretically analyze the intrinsic ethical\nvulnerability of aligned LLMs by proving that current alignment methods yield\nonly local \"safety regions\" in the knowledge manifold. In contrast, pretrained\nknowledge remains globally connected to harmful concepts via high-likelihood\nadversarial trajectories. Building on this theoretical insight, we empirically\nvalidate our findings by employing semantic coherence inducement under\ndistributional shifts--a method that systematically bypasses alignment\nconstraints through optimized adversarial prompts. This combined theoretical\nand empirical approach achieves a 100% attack success rate across 19 out of 23\nstate-of-the-art aligned LLMs, including DeepSeek-R1 and LLaMA-3, revealing\ntheir universal vulnerabilities."}
{"id": "2504.08961", "pdf": "https://arxiv.org/pdf/2504.08961.pdf", "abs": "https://arxiv.org/abs/2504.08961", "title": "A Fully Automated Pipeline for Conversational Discourse Annotation: Tree Scheme Generation and Labeling with Large Language Models", "authors": ["Kseniia Petukhova", "Ekaterina Kochmar"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have shown promise in\nautomating discourse annotation for conversations. While manually designing\ntree annotation schemes significantly improves annotation quality for humans\nand models, their creation remains time-consuming and requires expert\nknowledge. We propose a fully automated pipeline that uses LLMs to construct\nsuch schemes and perform annotation. We evaluate our approach on speech\nfunctions (SFs) and the Switchboard-DAMSL (SWBD-DAMSL) taxonomies. Our\nexperiments compare various design choices, and we show that frequency-guided\ndecision trees, paired with an advanced LLM for annotation, can outperform\npreviously manually designed trees and even match or surpass human annotators\nwhile significantly reducing the time required for annotation. We release all\ncode and resultant schemes and annotations to facilitate future research on\ndiscourse annotation."}
{"id": "2504.11042", "pdf": "https://arxiv.org/pdf/2504.11042.pdf", "abs": "https://arxiv.org/abs/2504.11042", "title": "LazyReview A Dataset for Uncovering Lazy Thinking in NLP Peer Reviews", "authors": ["Sukannya Purkayastha", "Zhuang Li", "Anne Lauscher", "Lizhen Qu", "Iryna Gurevych"], "categories": ["cs.CL"], "comment": "Accepted at ACL 2025: 29 pages, 18 Figures, 15 Tables", "summary": "Peer review is a cornerstone of quality control in scientific publishing.\nWith the increasing workload, the unintended use of `quick' heuristics,\nreferred to as lazy thinking, has emerged as a recurring issue compromising\nreview quality. Automated methods to detect such heuristics can help improve\nthe peer-reviewing process. However, there is limited NLP research on this\nissue, and no real-world dataset exists to support the development of detection\ntools. This work introduces LazyReview, a dataset of peer-review sentences\nannotated with fine-grained lazy thinking categories. Our analysis reveals that\nLarge Language Models (LLMs) struggle to detect these instances in a zero-shot\nsetting. However, instruction-based fine-tuning on our dataset significantly\nboosts performance by 10-20 performance points, highlighting the importance of\nhigh-quality training data. Furthermore, a controlled experiment demonstrates\nthat reviews revised with lazy thinking feedback are more comprehensive and\nactionable than those written without such feedback. We will release our\ndataset and the enhanced guidelines that can be used to train junior reviewers\nin the community. (Code available here:\nhttps://github.com/UKPLab/acl2025-lazy-review)"}
{"id": "2504.12216", "pdf": "https://arxiv.org/pdf/2504.12216.pdf", "abs": "https://arxiv.org/abs/2504.12216", "title": "d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning", "authors": ["Siyan Zhao", "Devaansh Gupta", "Qinqing Zheng", "Aditya Grover"], "categories": ["cs.CL", "cs.LG"], "comment": "27 pages, project page at https://dllm-reasoning.github.io/", "summary": "Recent large language models (LLMs) have demonstrated strong reasoning\ncapabilities that benefits from online reinforcement learning (RL). These\ncapabilities have primarily been demonstrated within the left-to-right\nautoregressive (AR) generation paradigm. In contrast, non-autoregressive\nparadigms based on diffusion generate text in a coarse-to-fine manner. Although\nrecent diffusion-based large language models (dLLMs) have achieved competitive\nlanguage modeling performance compared to their AR counterparts, it remains\nunclear if dLLMs can also leverage recent advances in LLM reasoning. To this\nend, we propose d1, a framework to adapt pre-trained masked dLLMs into\nreasoning models via a combination of supervised finetuning (SFT) and RL.\nSpecifically, we develop and extend techniques to improve reasoning in\npretrained dLLMs: (a) we utilize a masked SFT technique to distill knowledge\nand instill self-improvement behavior directly from existing datasets, and (b)\nwe introduce a novel critic-free, policy-gradient based RL algorithm called\ndiffu-GRPO, the first integration of policy gradient methods to masked dLLMs.\nThrough empirical studies, we investigate the performance of different\npost-training recipes on multiple mathematical and planning benchmarks. We find\nthat d1 yields the best performance and significantly improves performance of a\nstate-of-the-art dLLM. Our code is released at\nhttps://dllm-reasoning.github.io/."}
{"id": "2505.02862", "pdf": "https://arxiv.org/pdf/2505.02862.pdf", "abs": "https://arxiv.org/abs/2505.02862", "title": "Cannot See the Forest for the Trees: Invoking Heuristics and Biases to Elicit Irrational Choices of LLMs", "authors": ["Haoming Yang", "Ke Ma", "Xiaojun Jia", "Yingfei Sun", "Qianqian Xu", "Qingming Huang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite the remarkable performance of Large Language Models (LLMs), they\nremain vulnerable to jailbreak attacks, which can compromise their safety\nmechanisms. Existing studies often rely on brute-force optimization or manual\ndesign, failing to uncover potential risks in real-world scenarios. To address\nthis, we propose a novel jailbreak attack framework, ICRT, inspired by\nheuristics and biases in human cognition. Leveraging the simplicity effect, we\nemploy cognitive decomposition to reduce the complexity of malicious prompts.\nSimultaneously, relevance bias is utilized to reorganize prompts, enhancing\nsemantic alignment and inducing harmful outputs effectively. Furthermore, we\nintroduce a ranking-based harmfulness evaluation metric that surpasses the\ntraditional binary success-or-failure paradigm by employing ranking aggregation\nmethods such as Elo, HodgeRank, and Rank Centrality to comprehensively quantify\nthe harmfulness of generated content. Experimental results show that our\napproach consistently bypasses mainstream LLMs' safety mechanisms and generates\nhigh-risk content, providing insights into jailbreak attack risks and\ncontributing to stronger defense strategies."}
{"id": "2505.03320", "pdf": "https://arxiv.org/pdf/2505.03320.pdf", "abs": "https://arxiv.org/abs/2505.03320", "title": "Recall with Reasoning: Chain-of-Thought Distillation for Mamba's Long-Context Memory and Extrapolation", "authors": ["Junyu Ma", "Tianqing Fang", "Zhisong Zhang", "Hongming Zhang", "Haitao Mi", "Dong Yu"], "categories": ["cs.CL"], "comment": null, "summary": "Mamba's theoretical infinite-context potential is limited in practice when\nsequences far exceed training lengths. This work explores unlocking Mamba's\nlong-context memory ability by a simple-yet-effective method, Recall with\nReasoning (RwR), by distilling chain-of-thought (CoT) summarization from a\nteacher model. Specifically, RwR prepends these summarization as CoT prompts\nduring fine-tuning, teaching Mamba to actively recall and reason over long\ncontexts. Experiments on LONGMEMEVAL and HELMET show RwR boosts Mamba's\nlong-context performance against comparable Transformer/hybrid baselines under\nsimilar pretraining conditions, while preserving short-context capabilities,\nall without architectural changes."}
{"id": "2505.06150", "pdf": "https://arxiv.org/pdf/2505.06150.pdf", "abs": "https://arxiv.org/abs/2505.06150", "title": "A Scaling Law for Token Efficiency in LLM Fine-Tuning Under Fixed Compute Budgets", "authors": ["Ryan Lagasse", "Aidan Kierans", "Avijit Ghosh", "Shiri Dori-Hacohen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We introduce a scaling law for fine-tuning large language models (LLMs) under\nfixed compute budgets that explicitly accounts for data composition.\nConventional approaches measure training data solely by total tokens, yet the\nnumber of examples and their average token length -- what we term \\emph{dataset\nvolume} -- play a decisive role in model performance. Our formulation is tuned\nfollowing established procedures. Experiments on the BRICC dataset\n\\cite{salavati2024reducing} and subsets of the MMLU dataset\n\\cite{hendrycks2021measuringmassivemultitasklanguage}, evaluated under multiple\nsubsampling strategies, reveal that data composition significantly affects\ntoken efficiency. These results motivate refined scaling laws for practical LLM\nfine-tuning in resource-constrained settings."}
{"id": "2505.09825", "pdf": "https://arxiv.org/pdf/2505.09825.pdf", "abs": "https://arxiv.org/abs/2505.09825", "title": "KRISTEVA: Close Reading as a Novel Task for Benchmarking Interpretive Reasoning", "authors": ["Peiqi Sui", "Juan Diego Rodriguez", "Philippe Laban", "Dean Murphy", "Joseph P. Dexter", "Richard Jean So", "Samuel Baker", "Pramit Chaudhuri"], "categories": ["cs.CL"], "comment": "ACL 2025 main", "summary": "Each year, tens of millions of essays are written and graded in college-level\nEnglish courses. Students are asked to analyze literary and cultural texts\nthrough a process known as close reading, in which they gather textual details\nto formulate evidence-based arguments. Despite being viewed as a basis for\ncritical thinking and widely adopted as a required element of university\ncoursework, close reading has never been evaluated on large language models\n(LLMs), and multi-discipline benchmarks like MMLU do not include literature as\na subject. To fill this gap, we present KRISTEVA, the first close reading\nbenchmark for evaluating interpretive reasoning, consisting of 1331\nmultiple-choice questions adapted from classroom data. With KRISTEVA, we\npropose three progressively more difficult sets of tasks to approximate\ndifferent elements of the close reading process, which we use to test how well\nLLMs may seem to understand and reason about literary works: 1) extracting\nstylistic features, 2) retrieving relevant contextual information from\nparametric knowledge, and 3) multi-hop reasoning between style and external\ncontexts. Our baseline results find that, while state-of-the-art LLMs possess\nsome college-level close reading competency (accuracy 49.7% - 69.7%), their\nperformances still trail those of experienced human evaluators on 10 out of our\n11 tasks."}
{"id": "2505.13338", "pdf": "https://arxiv.org/pdf/2505.13338.pdf", "abs": "https://arxiv.org/abs/2505.13338", "title": "Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data Condensation and Spoken QA Generation", "authors": ["Qiongqiong Wang", "Hardik B. Sailor", "Tianchi Liu", "Ai Ti Aw"], "categories": ["cs.CL", "cs.AI", "eess.AS"], "comment": "Accepted at Interspeech 2025. [v2]: The dataset has been released,\n  and the link is now updated", "summary": "Current speech-LLMs exhibit limited capability in contextual reasoning\nalongside paralinguistic understanding, primarily due to the lack of\nQuestion-Answer (QA) datasets that cover both aspects. We propose a novel\nframework for dataset generation from in-the-wild speech data, that integrates\ncontextual reasoning with paralinguistic information. It consists of a pseudo\nparalinguistic label-based data condensation of in-the-wild speech and\nLLM-based Contextual Paralinguistic QA (CPQA) generation. The effectiveness is\nvalidated by a strong correlation in evaluations of the Qwen2-Audio-7B-Instruct\nmodel on a dataset created by our framework and human-generated CPQA dataset.\nThe results also reveal the speech-LLM's limitations in handling empathetic\nreasoning tasks, highlighting the need for such datasets and more robust\nmodels. The proposed framework is first of its kind and has potential in\ntraining more robust speech-LLMs with paralinguistic reasoning capabilities."}
{"id": "2505.13508", "pdf": "https://arxiv.org/pdf/2505.13508.pdf", "abs": "https://arxiv.org/abs/2505.13508", "title": "Time-R1: Towards Comprehensive Temporal Reasoning in LLMs", "authors": ["Zijia Liu", "Peixuan Han", "Haofei Yu", "Haoru Li", "Jiaxuan You"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) demonstrate impressive capabilities but lack\nrobust temporal intelligence, struggling to integrate reasoning about the past\nwith predictions and plausible generations of the future. Meanwhile, existing\nmethods typically target isolated temporal skills, such as question answering\nabout past events or basic forecasting, and exhibit poor generalization,\nparticularly when dealing with events beyond their knowledge cutoff or\nrequiring creative foresight. To address these limitations, we introduce\n\\textit{Time-R1}, the first framework to endow a moderate-sized (3B-parameter)\nLLM with comprehensive temporal abilities: understanding, prediction, and\ncreative generation. Our approach features a novel three-stage development\npath; the first two constitute a \\textit{reinforcement learning (RL)\ncurriculum} driven by a meticulously designed dynamic rule-based reward system.\nThis framework progressively builds (1) foundational temporal understanding and\nlogical event-time mappings from historical data, (2) future event prediction\nskills for events beyond its knowledge cutoff, and finally (3) enables\nremarkable generalization to creative future scenario generation without any\nfine-tuning. Strikingly, experiments demonstrate that Time-R1 outperforms\nmodels over 200 times larger, including the state-of-the-art 671B DeepSeek-R1,\non highly challenging future event prediction and creative scenario generation\nbenchmarks. This work provides strong evidence that thoughtfully engineered,\nprogressive RL fine-tuning allows smaller, efficient models to achieve superior\ntemporal performance, offering a practical and scalable path towards truly\ntime-aware AI. To foster further research, we also release \\textit{Time-Bench},\na large-scale multi-task temporal reasoning dataset derived from 10 years of\nnews data, and our series of \\textit{Time-R1} checkpoints."}
{"id": "2505.15299", "pdf": "https://arxiv.org/pdf/2505.15299.pdf", "abs": "https://arxiv.org/abs/2505.15299", "title": "Multi-Hop Question Generation via Dual-Perspective Keyword Guidance", "authors": ["Maodong Li", "Longyin Zhang", "Fang Kong"], "categories": ["cs.CL"], "comment": "17 pages, 5 figures, accepted to the Findings of ACL 2025", "summary": "Multi-hop question generation (MQG) aims to generate questions that require\nsynthesizing multiple information snippets from documents to derive target\nanswers. The primary challenge lies in effectively pinpointing crucial\ninformation snippets related to question-answer (QA) pairs, typically relying\non keywords. However, existing works fail to fully utilize the guiding\npotential of keywords and neglect to differentiate the distinct roles of\nquestion-specific and document-specific keywords. To address this, we define\ndual-perspective keywords (i.e., question and document keywords) and propose a\nDual-Perspective Keyword-Guided (DPKG) framework, which seamlessly integrates\nkeywords into the multi-hop question generation process. We argue that question\nkeywords capture the questioner's intent, whereas document keywords reflect the\ncontent related to the QA pair. Functionally, question and document keywords\nwork together to pinpoint essential information snippets in the document, with\nquestion keywords required to appear in the generated question. The DPKG\nframework consists of an expanded transformer encoder and two answer-aware\ntransformer decoders for keyword and question generation, respectively.\nExtensive experiments demonstrate the effectiveness of our work, showcasing its\npromising performance and underscoring its significant value in the MQG task."}
{"id": "2505.16014", "pdf": "https://arxiv.org/pdf/2505.16014.pdf", "abs": "https://arxiv.org/abs/2505.16014", "title": "Ranking Free RAG: Replacing Re-ranking with Selection in RAG for Sensitive Domains", "authors": ["Yash Saxena", "Ankur Padia", "Mandar S Chaudhary", "Kalpa Gunaratna", "Srinivasan Parthasarathy", "Manas Gaur"], "categories": ["cs.CL"], "comment": null, "summary": "Traditional Retrieval-Augmented Generation (RAG) pipelines rely on\nsimilarity-based retrieval and re-ranking, which depend on heuristics such as\ntop-k, and lack explainability, interpretability, and robustness against\nadversarial content. To address this gap, we propose a novel method METEORA\nthat replaces re-ranking in RAG with a rationale-driven selection approach.\nMETEORA operates in two stages. First, a general-purpose LLM is\npreference-tuned to generate rationales conditioned on the input query using\ndirect preference optimization. These rationales guide the evidence chunk\nselection engine, which selects relevant chunks in three stages: pairing\nindividual rationales with corresponding retrieved chunks for local relevance,\nglobal selection with elbow detection for adaptive cutoff, and context\nexpansion via neighboring chunks. This process eliminates the need for top-k\nheuristics. The rationales are also used for consistency check using a Verifier\nLLM to detect and filter poisoned or misleading content for safe generation.\nThe framework provides explainable and interpretable evidence flow by using\nrationales consistently across both selection and verification. Our evaluation\nacross six datasets spanning legal, financial, and academic research domains\nshows that METEORA improves generation accuracy by 33.34% while using\napproximately 50% fewer chunks than state-of-the-art re-ranking methods. In\nadversarial settings, METEORA significantly improves the F1 score from 0.10 to\n0.44 over the state-of-the-art perplexity-based defense baseline, demonstrating\nstrong resilience to poisoning attacks. Code available at:\nhttps://anonymous.4open.science/r/METEORA-DC46/README.md"}
{"id": "2505.16552", "pdf": "https://arxiv.org/pdf/2505.16552.pdf", "abs": "https://arxiv.org/abs/2505.16552", "title": "Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains", "authors": ["Wenhui Tan", "Jiaze Li", "Jianzhong Ju", "Zhenbo Luo", "Jian Luan", "Ruihua Song"], "categories": ["cs.CL"], "comment": "15 pages, 8 figures", "summary": "Large Language Models (LLMs) achieve superior performance through\nChain-of-Thought (CoT) reasoning, but these token-level reasoning chains are\ncomputationally expensive and inefficient. In this paper, we introduce\nCompressed Latent Reasoning (CoLaR), a novel framework that dynamically\ncompresses reasoning processes in latent space through a two-stage training\napproach. First, during supervised fine-tuning, CoLaR extends beyond next-token\nprediction by incorporating an auxiliary next compressed embedding prediction\nobjective. This process merges embeddings of consecutive tokens using a\ncompression factor randomly sampled from a predefined range, and trains a\nspecialized latent head to predict distributions of subsequent compressed\nembeddings. Second, we enhance CoLaR through reinforcement learning (RL) that\nleverages the latent head's non-deterministic nature to explore diverse\nreasoning paths and exploit more compact ones. This approach enables CoLaR to:\ni) perform reasoning at a dense latent level (i.e., silently), substantially\nreducing reasoning chain length, and ii) dynamically adjust reasoning speed at\ninference time by simply prompting the desired compression factor. Extensive\nexperiments across four mathematical reasoning datasets demonstrate that CoLaR\nachieves 14.1% higher accuracy than latent-based baseline methods at comparable\ncompression ratios, and reduces reasoning chain length by 53.3% with only 4.8%\nperformance degradation compared to explicit CoT method. Moreover, when applied\nto more challenging mathematical reasoning tasks, our RL-enhanced CoLaR\ndemonstrates performance gains of up to 5.4% while dramatically reducing latent\nreasoning chain length by 82.8%. The code and models will be released upon\nacceptance."}
{"id": "2505.16660", "pdf": "https://arxiv.org/pdf/2505.16660.pdf", "abs": "https://arxiv.org/abs/2505.16660", "title": "Can reasoning models comprehend mathematical problems in Chinese ancient texts? An empirical study based on data from Suanjing Shishu", "authors": ["Liu Chang", "Wang Dongbo", "Liu liu", "Zhao Zhixiao"], "categories": ["cs.CL", "cs.AI"], "comment": "29pages, 7 figures", "summary": "This study addresses the challenges in intelligent processing of Chinese\nancient mathematical classics by constructing Guji_MATH, a benchmark for\nevaluating classical texts based on Suanjing Shishu. It systematically assesses\nthe mathematical problem-solving capabilities of mainstream reasoning models\nunder the unique linguistic constraints of classical Chinese. Through\nmachine-assisted annotation and manual verification, 538 mathematical problems\nwere extracted from 8 canonical texts, forming a structured dataset centered on\nthe \"Question-Answer-Solution\" framework, supplemented by problem types and\ndifficulty levels. Dual evaluation modes--closed-book (autonomous\nproblem-solving) and open-book (reproducing classical solution methods)--were\ndesigned to evaluate the performance of six reasoning models on ancient Chinese\nmathematical problems. Results indicate that reasoning models can partially\ncomprehend and solve these problems, yet their overall performance remains\ninferior to benchmarks on modern mathematical tasks. Enhancing models'\nclassical Chinese comprehension and cultural knowledge should be prioritized\nfor optimization. This study provides methodological support for mining\nmathematical knowledge from ancient texts and disseminating traditional\nculture, while offering new perspectives for evaluating cross-linguistic and\ncross-cultural capabilities of reasoning models."}
{"id": "2505.17134", "pdf": "https://arxiv.org/pdf/2505.17134.pdf", "abs": "https://arxiv.org/abs/2505.17134", "title": "LongMagpie: A Self-synthesis Method for Generating Large-scale Long-context Instructions", "authors": ["Chaochen Gao", "Xing Wu", "Zijia Lin", "Debing Zhang", "Songlin Hu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "High-quality long-context instruction data is essential for aligning\nlong-context large language models (LLMs). Despite the public release of models\nlike Qwen and Llama, their long-context instruction data remains proprietary.\nHuman annotation is costly and challenging, while template-based synthesis\nmethods limit scale, diversity, and quality. We introduce LongMagpie, a\nself-synthesis framework that automatically generates large-scale long-context\ninstruction data. Our key insight is that aligned long-context LLMs, when\npresented with a document followed by special tokens preceding a user turn,\nauto-regressively generate contextually relevant queries. By harvesting these\ndocument-query pairs and the model's responses, LongMagpie produces\nhigh-quality instructions without human effort. Experiments on HELMET, RULER,\nand Longbench v2 demonstrate that LongMagpie achieves leading performance on\nlong-context tasks while maintaining competitive performance on short-context\ntasks, establishing it as a simple and effective approach for open, diverse,\nand scalable long-context instruction data synthesis."}
{"id": "2505.20015", "pdf": "https://arxiv.org/pdf/2505.20015.pdf", "abs": "https://arxiv.org/abs/2505.20015", "title": "On the class of coding optimality of human languages and the origins of Zipf's law", "authors": ["Ramon Ferrer-i-Cancho"], "categories": ["cs.CL", "physics.soc-ph"], "comment": null, "summary": "Here we present a new class of optimality for coding systems. Members of that\nclass are displaced linearly from optimal coding and thus exhibit Zipf's law,\nnamely a power-law distribution of frequency ranks. Within that class, Zipf's\nlaw, the size-rank law and the size-probability law form a group-like\nstructure. We identify human languages that are members of the class. All\nlanguages showing sufficient agreement with Zipf's law are potential members of\nthe class. In contrast, there are communication systems in other species that\ncannot be members of that class for exhibiting an exponential distribution\ninstead but dolphins and humpback whales might. We provide a new insight into\nplots of frequency versus rank in double logarithmic scale. For any system, a\nstraight line in that scale indicates that the lengths of optimal codes under\nnon-singular coding and under uniquely decodable encoding are displaced by a\nlinear function whose slope is the exponent of Zipf's law. For systems under\ncompression and constrained to be uniquely decodable, such a straight line may\nindicate that the system is coding close to optimality. Our findings provide\nsupport for the hypothesis that Zipf's law originates from compression."}
{"id": "2505.20282", "pdf": "https://arxiv.org/pdf/2505.20282.pdf", "abs": "https://arxiv.org/abs/2505.20282", "title": "One-shot Entropy Minimization", "authors": ["Zitian Gao", "Lynx Chen", "Joey Zhou", "Bryan Dai"], "categories": ["cs.CL"], "comment": "Work in progress", "summary": "We trained 13,440 large language models and found that entropy minimization\nrequires only a single unlabeled data and 10 steps optimization to achieve\nperformance improvements comparable to or even greater than those obtained\nusing thousands of data and carefully designed rewards in rule-based\nreinforcement learning. This striking result may prompt a rethinking of\npost-training paradigms for large language models. Our code is avaliable at\nhttps://github.com/zitian-gao/one-shot-em."}
{"id": "2505.20322", "pdf": "https://arxiv.org/pdf/2505.20322.pdf", "abs": "https://arxiv.org/abs/2505.20322", "title": "Beyond Prompt Engineering: Robust Behavior Control in LLMs via Steering Target Atoms", "authors": ["Mengru Wang", "Ziwen Xu", "Shengyu Mao", "Shumin Deng", "Zhaopeng Tu", "Huajun Chen", "Ningyu Zhang"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.IR", "cs.LG"], "comment": "ACL 2025", "summary": "Precise control over language model generation is vital for ensuring both\nsafety and reliability. Although prompt engineering and steering are commonly\nused to intervene in model behaviors, the vast number of parameters in models\noften results in highly intertwined internal representations. This\ninterdependency can limit control precision and sometimes lead to unintended\nside effects. Recent research has explored the use of sparse autoencoders (SAE)\nto disentangle knowledge in high-dimensional spaces for steering. However,\nthese applications have been limited to toy tasks owing to the nontrivial issue\nof locating atomic knowledge components. In this paper, we propose Steering\nTarget Atoms (STA), a novel method that isolates and manipulates disentangled\nknowledge components to enhance safety. Comprehensive experiments demonstrate\nthe effectiveness of our approach. Further analysis reveals that steering\nexhibits superior robustness and flexibility, particularly in adversarial\nscenarios. We also apply the steering strategy to the large reasoning model,\nconfirming its effectiveness in precise reasoning control."}
{"id": "2505.22019", "pdf": "https://arxiv.org/pdf/2505.22019.pdf", "abs": "https://arxiv.org/abs/2505.22019", "title": "VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich Information Understanding via Iterative Reasoning with Reinforcement Learning", "authors": ["Qiuchen Wang", "Ruixue Ding", "Yu Zeng", "Zehui Chen", "Lin Chen", "Shihang Wang", "Pengjun Xie", "Fei Huang", "Feng Zhao"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Effectively retrieving, reasoning and understanding visually rich information\nremains a challenge for RAG methods. Traditional text-based methods cannot\nhandle visual-related information. On the other hand, current vision-based RAG\napproaches are often limited by fixed pipelines and frequently struggle to\nreason effectively due to the insufficient activation of the fundamental\ncapabilities of models. As RL has been proven to be beneficial for model\nreasoning, we introduce VRAG-RL, a novel RL framework tailored for complex\nreasoning across visually rich information. With this framework, VLMs interact\nwith search engines, autonomously sampling single-turn or multi-turn reasoning\ntrajectories with the help of visual perception tokens and undergoing continual\noptimization based on these samples. Our approach highlights key limitations of\nRL in RAG domains: (i) Prior Multi-modal RAG approaches tend to merely\nincorporate images into the context, leading to insufficient reasoning token\nallocation and neglecting visual-specific perception; and (ii) When models\ninteract with search engines, their queries often fail to retrieve relevant\ninformation due to the inability to articulate requirements, thereby leading to\nsuboptimal performance. To address these challenges, we define an action space\ntailored for visually rich inputs, with actions including cropping and scaling,\nallowing the model to gather information from a coarse-to-fine perspective.\nFurthermore, to bridge the gap between users' original inquiries and the\nretriever, we employ a simple yet effective reward that integrates query\nrewriting and retrieval performance with a model-based reward. Our VRAG-RL\noptimizes VLMs for RAG tasks using specially designed RL strategies, aligning\nthe model with real-world applications. The code is available at\nhttps://github.com/Alibaba-NLP/VRAG."}
{"id": "2505.22116", "pdf": "https://arxiv.org/pdf/2505.22116.pdf", "abs": "https://arxiv.org/abs/2505.22116", "title": "Multimodal Forecasting of Sparse Intraoperative Hypotension Events Powered by Language Model", "authors": ["Jintao Zhang", "Zirui Liu", "Mingyue Cheng", "Shilong Zhang", "Tingyue Pan", "Qi Liu", "Yanhu Xie"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Intraoperative hypotension (IOH) frequently occurs under general anesthesia\nand is strongly linked to adverse outcomes such as myocardial injury and\nincreased mortality. Despite its significance, IOH prediction is hindered by\nevent sparsity and the challenge of integrating static and dynamic data across\ndiverse patients. In this paper, we propose \\textbf{IOHFuseLM}, a multimodal\nlanguage model framework. To accurately identify and differentiate sparse\nhypotensive events, we leverage a two-stage training strategy. The first stage\ninvolves domain adaptive pretraining on IOH physiological time series augmented\nthrough diffusion methods, thereby enhancing the model sensitivity to patterns\nassociated with hypotension. Subsequently, task fine-tuning is performed on the\noriginal clinical dataset to further enhance the ability to distinguish\nnormotensive from hypotensive states. To enable multimodal fusion for each\npatient, we align structured clinical descriptions with the corresponding\nphysiological time series at the token level. Such alignment enables the model\nto capture individualized temporal patterns alongside their corresponding\nclinical semantics. In addition, we convert static patient attributes into\nstructured text to enrich personalized information. Experimental evaluations on\ntwo intraoperative datasets demonstrate that IOHFuseLM outperforms established\nbaselines in accurately identifying IOH events, highlighting its applicability\nin clinical decision support scenarios. Our code is publicly available to\npromote reproducibility at https://github.com/zjt-gpu/IOHFuseLM."}
{"id": "2505.22830", "pdf": "https://arxiv.org/pdf/2505.22830.pdf", "abs": "https://arxiv.org/abs/2505.22830", "title": "What Has Been Lost with Synthetic Evaluation?", "authors": ["Alexander Gill", "Abhilasha Ravichander", "Ana Marasović"], "categories": ["cs.CL", "cs.AI"], "comment": "v2: Fixed low resolution figures", "summary": "Large language models (LLMs) are increasingly used for data generation.\nHowever, creating evaluation benchmarks raises the bar for this emerging\nparadigm. Benchmarks must target specific phenomena, penalize exploiting\nshortcuts, and be challenging. Through two case studies, we investigate whether\nLLMs can meet these demands by generating reasoning over-text benchmarks and\ncomparing them to those created through careful crowdsourcing. Specifically, we\nevaluate both the validity and difficulty of LLM-generated versions of two\nhigh-quality reading comprehension datasets: CondaQA, which evaluates reasoning\nabout negation, and DROP, which targets reasoning about quantities. We find\nthat prompting LLMs can produce variants of these datasets that are often valid\naccording to the annotation guidelines, at a fraction of the cost of the\noriginal crowdsourcing effort. However, we show that they are less challenging\nfor LLMs than their human-authored counterparts. This finding sheds light on\nwhat may have been lost by generating evaluation data with LLMs, and calls for\ncritically reassessing the immediate use of this increasingly prevalent\napproach to benchmark creation."}
{"id": "2505.22848", "pdf": "https://arxiv.org/pdf/2505.22848.pdf", "abs": "https://arxiv.org/abs/2505.22848", "title": "LiTEx: A Linguistic Taxonomy of Explanations for Understanding Within-Label Variation in Natural Language Inference", "authors": ["Pingjun Hong", "Beiduo Chen", "Siyao Peng", "Marie-Catherine de Marneffe", "Barbara Plank"], "categories": ["cs.CL"], "comment": "21 pages, 6 figures", "summary": "There is increasing evidence of Human Label Variation (HLV) in Natural\nLanguage Inference (NLI), where annotators assign different labels to the same\npremise-hypothesis pair. However, within-label variation--cases where\nannotators agree on the same label but provide divergent reasoning--poses an\nadditional and mostly overlooked challenge. Several NLI datasets contain\nhighlighted words in the NLI item as explanations, but the same spans on the\nNLI item can be highlighted for different reasons, as evidenced by free-text\nexplanations, which offer a window into annotators' reasoning. To\nsystematically understand this problem and gain insight into the rationales\nbehind NLI labels, we introduce LITEX, a linguistically-informed taxonomy for\ncategorizing free-text explanations. Using this taxonomy, we annotate a subset\nof the e-SNLI dataset, validate the taxonomy's reliability, and analyze how it\naligns with NLI labels, highlights, and explanations. We further assess the\ntaxonomy's usefulness in explanation generation, demonstrating that\nconditioning generation on LITEX yields explanations that are linguistically\ncloser to human explanations than those generated using only labels or\nhighlights. Our approach thus not only captures within-label variation but also\nshows how taxonomy-guided generation for reasoning can bridge the gap between\nhuman and model explanations more effectively than existing strategies."}
{"id": "2505.23001", "pdf": "https://arxiv.org/pdf/2505.23001.pdf", "abs": "https://arxiv.org/abs/2505.23001", "title": "DyePack: Provably Flagging Test Set Contamination in LLMs Using Backdoors", "authors": ["Yize Cheng", "Wenxiao Wang", "Mazda Moayeri", "Soheil Feizi"], "categories": ["cs.CL"], "comment": null, "summary": "Open benchmarks are essential for evaluating and advancing large language\nmodels, offering reproducibility and transparency. However, their accessibility\nmakes them likely targets of test set contamination. In this work, we introduce\nDyePack, a framework that leverages backdoor attacks to identify models that\nused benchmark test sets during training, without requiring access to the loss,\nlogits, or any internal details of the model. Like how banks mix dye packs with\ntheir money to mark robbers, DyePack mixes backdoor samples with the test data\nto flag models that trained on it. We propose a principled design incorporating\nmultiple backdoors with stochastic targets, enabling exact false positive rate\n(FPR) computation when flagging every model. This provably prevents false\naccusations while providing strong evidence for every detected case of\ncontamination. We evaluate DyePack on five models across three datasets,\ncovering both multiple-choice and open-ended generation tasks. For\nmultiple-choice questions, it successfully detects all contaminated models with\nguaranteed FPRs as low as 0.000073% on MMLU-Pro and 0.000017% on Big-Bench-Hard\nusing eight backdoors. For open-ended generation tasks, it generalizes well and\nidentifies all contaminated models on Alpaca with a guaranteed false positive\nrate of just 0.127% using six backdoors."}
{"id": "2505.23114", "pdf": "https://arxiv.org/pdf/2505.23114.pdf", "abs": "https://arxiv.org/abs/2505.23114", "title": "Dataset Cartography for Large Language Model Alignment: Mapping and Diagnosing Preference Data", "authors": ["Seohyeong Lee", "Eunwon Kim", "Hwaran Lee", "Buru Chang"], "categories": ["cs.CL"], "comment": null, "summary": "Human preference data plays a critical role in aligning large language models\n(LLMs) with human values. However, collecting such data is often expensive and\ninefficient, posing a significant scalability challenge. To address this, we\nintroduce Alignment Data Map, a GPT-4o-assisted tool for analyzing and\ndiagnosing preference data. Using GPT-4o as a proxy for LLM alignment, we\ncompute alignment scores for LLM-generated responses to instructions from\nexisting preference datasets. These scores are then used to construct an\nAlignment Data Map based on their mean and variance. Our experiments show that\nusing only 33 percent of the data, specifically samples in the high-mean,\nlow-variance region, achieves performance comparable to or better than using\nthe entire dataset. This finding suggests that the Alignment Data Map can\nsignificantly improve data collection efficiency by identifying high-quality\nsamples for LLM alignment without requiring explicit annotations. Moreover, the\nAlignment Data Map can diagnose existing preference datasets. Our analysis\nshows that it effectively detects low-impact or potentially misannotated\nsamples. Source code is available online."}
{"id": "2505.23368", "pdf": "https://arxiv.org/pdf/2505.23368.pdf", "abs": "https://arxiv.org/abs/2505.23368", "title": "Threading the Needle: Reweaving Chain-of-Thought Reasoning to Explain Human Label Variation", "authors": ["Beiduo Chen", "Yang Janet Liu", "Anna Korhonen", "Barbara Plank"], "categories": ["cs.CL"], "comment": "22 pages, 7 figures", "summary": "The recent rise of reasoning-tuned Large Language Models (LLMs)--which\ngenerate chains of thought (CoTs) before giving the final answer--has attracted\nsignificant attention and offers new opportunities for gaining insights into\nhuman label variation, which refers to plausible differences in how multiple\nannotators label the same data instance. Prior work has shown that\nLLM-generated explanations can help align model predictions with human label\ndistributions, but typically adopt a reverse paradigm: producing explanations\nbased on given answers. In contrast, CoTs provide a forward reasoning path that\nmay implicitly embed rationales for each answer option, before generating the\nanswers. We thus propose a novel LLM-based pipeline enriched with\nlinguistically-grounded discourse segmenters to extract supporting and opposing\nstatements for each answer option from CoTs with improved accuracy. We also\npropose a rank-based HLV evaluation framework that prioritizes the ranking of\nanswers over exact scores, which instead favor direct comparison of label\ndistributions. Our method outperforms a direct generation method as well as\nbaselines on three datasets, and shows better alignment of ranking methods with\nhumans, highlighting the effectiveness of our approach."}
{"id": "2505.23754", "pdf": "https://arxiv.org/pdf/2505.23754.pdf", "abs": "https://arxiv.org/abs/2505.23754", "title": "DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural Language and Reinforcement Learning", "authors": ["Ziyin Zhang", "Jiahao Xu", "Zhiwei He", "Tian Liang", "Qiuzhi Liu", "Yansi Li", "Linfeng Song", "Zhenwen Liang", "Zhuosheng Zhang", "Rui Wang", "Zhaopeng Tu", "Haitao Mi", "Dong Yu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Theorem proving serves as a major testbed for evaluating complex reasoning\nabilities in large language models (LLMs). However, traditional automated\ntheorem proving (ATP) approaches rely heavily on formal proof systems that\npoorly align with LLMs' strength derived from informal, natural language\nknowledge acquired during pre-training. In this work, we propose DeepTheorem, a\ncomprehensive informal theorem-proving framework exploiting natural language to\nenhance LLM mathematical reasoning. DeepTheorem includes a large-scale\nbenchmark dataset consisting of 121K high-quality IMO-level informal theorems\nand proofs spanning diverse mathematical domains, rigorously annotated for\ncorrectness, difficulty, and topic categories, accompanied by systematically\nconstructed verifiable theorem variants. We devise a novel reinforcement\nlearning strategy (RL-Zero) explicitly tailored to informal theorem proving,\nleveraging the verified theorem variants to incentivize robust mathematical\ninference. Additionally, we propose comprehensive outcome and process\nevaluation metrics examining proof correctness and the quality of reasoning\nsteps. Extensive experimental analyses demonstrate DeepTheorem significantly\nimproves LLM theorem-proving performance compared to existing datasets and\nsupervised fine-tuning protocols, achieving state-of-the-art accuracy and\nreasoning quality. Our findings highlight DeepTheorem's potential to\nfundamentally advance automated informal theorem proving and mathematical\nexploration."}
{"id": "2505.23807", "pdf": "https://arxiv.org/pdf/2505.23807.pdf", "abs": "https://arxiv.org/abs/2505.23807", "title": "DLP: Dynamic Layerwise Pruning in Large Language Models", "authors": ["Yuli Chen", "Bo Cheng", "Jiale Han", "Yingying Zhang", "Yingting Li", "Shuhao Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ICML 2025", "summary": "Pruning has recently been widely adopted to reduce the parameter scale and\nimprove the inference efficiency of Large Language Models (LLMs). Mainstream\npruning techniques often rely on uniform layerwise pruning strategies, which\ncan lead to severe performance degradation at high sparsity levels. Recognizing\nthe varying contributions of different layers in LLMs, recent studies have\nshifted their focus toward non-uniform layerwise pruning. However, these\napproaches often rely on pre-defined values, which can result in suboptimal\nperformance. To overcome these limitations, we propose a novel method called\nDynamic Layerwise Pruning (DLP). This approach adaptively determines the\nrelative importance of each layer by integrating model weights with input\nactivation information, assigning pruning rates accordingly. Experimental\nresults show that DLP effectively preserves model performance at high sparsity\nlevels across multiple LLMs. Specifically, at 70% sparsity, DLP reduces the\nperplexity of LLaMA2-7B by 7.79 and improves the average accuracy by 2.7%\ncompared to state-of-the-art methods. Moreover, DLP is compatible with various\nexisting LLM compression techniques and can be seamlessly integrated into\nParameter-Efficient Fine-Tuning (PEFT). We release the code at\nhttps://github.com/ironartisan/DLP to facilitate future research."}
{"id": "2505.23809", "pdf": "https://arxiv.org/pdf/2505.23809.pdf", "abs": "https://arxiv.org/abs/2505.23809", "title": "LLM-Driven E-Commerce Marketing Content Optimization: Balancing Creativity and Conversion", "authors": ["Haowei Yang", "Haotian Lyu", "Tianle Zhang", "Dingzhou Wang", "Yushang Zhao"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "As e-commerce competition intensifies, balancing creative content with\nconversion effectiveness becomes critical. Leveraging LLMs' language generation\ncapabilities, we propose a framework that integrates prompt engineering,\nmulti-objective fine-tuning, and post-processing to generate marketing copy\nthat is both engaging and conversion-driven. Our fine-tuning method combines\nsentiment adjustment, diversity enhancement, and CTA embedding. Through offline\nevaluations and online A/B tests across categories, our approach achieves a\n12.5 % increase in CTR and an 8.3 % increase in CVR while maintaining content\nnovelty. This provides a practical solution for automated copy generation and\nsuggests paths for future multimodal, real-time personalization."}
{"id": "2505.24133", "pdf": "https://arxiv.org/pdf/2505.24133.pdf", "abs": "https://arxiv.org/abs/2505.24133", "title": "R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration", "authors": ["Zefan Cai", "Wen Xiao", "Hanshi Sun", "Cheng Luo", "Yikai Zhang", "Ke Wan", "Yucheng Li", "Yeyang Zhou", "Li-Wen Chang", "Jiuxiang Gu", "Zhen Dong", "Anima Anandkumar", "Abedelkadir Asi", "Junjie Hu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reasoning models have demonstrated impressive performance in self-reflection\nand chain-of-thought reasoning. However, they often produce excessively long\noutputs, leading to prohibitively large key-value (KV) caches during inference.\nWhile chain-of-thought inference significantly improves performance on complex\nreasoning tasks, it can also lead to reasoning failures when deployed with\nexisting KV cache compression approaches. To address this, we propose\nRedundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel\nmethod specifically targeting redundant tokens in reasoning models. Our method\npreserves nearly 100% of the full KV cache performance using only 10% of the KV\ncache, substantially outperforming existing KV cache baselines, which reach\nonly 60% of the performance. Remarkably, R-KV even achieves 105% of full KV\ncache performance with 16% of the KV cache. This KV-cache reduction also leads\nto a 90% memory saving and a 6.6X throughput over standard chain-of-thought\nreasoning inference. Experimental results show that R-KV consistently\noutperforms existing KV cache compression baselines across two mathematical\nreasoning datasets."}
{"id": "2505.24539", "pdf": "https://arxiv.org/pdf/2505.24539.pdf", "abs": "https://arxiv.org/abs/2505.24539", "title": "Localizing Persona Representations in LLMs", "authors": ["Celia Cintas", "Miriam Rateike", "Erik Miehling", "Elizabeth Daly", "Skyler Speakman"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present a study on how and where personas -- defined by distinct sets of\nhuman characteristics, values, and beliefs -- are encoded in the representation\nspace of large language models (LLMs). Using a range of dimension reduction and\npattern recognition methods, we first identify the model layers that show the\ngreatest divergence in encoding these representations. We then analyze the\nactivations within a selected layer to examine how specific personas are\nencoded relative to others, including their shared and distinct embedding\nspaces. We find that, across multiple pre-trained decoder-only LLMs, the\nanalyzed personas show large differences in representation space only within\nthe final third of the decoder layers. We observe overlapping activations for\nspecific ethical perspectives -- such as moral nihilism and utilitarianism --\nsuggesting a degree of polysemy. In contrast, political ideologies like\nconservatism and liberalism appear to be represented in more distinct regions.\nThese findings help to improve our understanding of how LLMs internally\nrepresent information and can inform future efforts in refining the modulation\nof specific human traits in LLM outputs. Warning: This paper includes\npotentially offensive sample statements."}
{"id": "2506.00022", "pdf": "https://arxiv.org/pdf/2506.00022.pdf", "abs": "https://arxiv.org/abs/2506.00022", "title": "Scaling Physical Reasoning with the PHYSICS Dataset", "authors": ["Shenghe Zheng", "Qianjia Cheng", "Junchi Yao", "Mengsong Wu", "Haonan He", "Ning Ding", "Yu Cheng", "Shuyue Hu", "Lei Bai", "Dongzhan Zhou", "Ganqu Cui", "Peng Ye"], "categories": ["cs.CL", "cs.LG", "physics.ed-ph"], "comment": "Work on physical datasets", "summary": "Large Language Models (LLMs) have achieved remarkable progress on advanced\nreasoning tasks such as mathematics and coding competitions. Meanwhile,\nphysics, despite being both reasoning-intensive and essential to real-world\nunderstanding, received limited academic and industrial attention. This paper\nintroduces PHYSICS, a dataset containing 16,568 high-quality physics problems\nspanning subjects and difficulty levels, to facilitate this issue.\nSpecifically, PHYSICS is curated with exercises from over 100 textbooks through\na carefully designed pipeline for quality control. It covers five major physics\ndomains: Mechanics, Electromagnetism, Thermodynamics, Optics, and Modern\nPhysics. It also spans a wide range of difficulty levels, from high school to\ngraduate-level physics courses. To utilize the data for improving and\nevaluating the model's physical reasoning capabilities, we split the dataset\ninto training and test sets, and provide reasoning paths generated by powerful\nreasoning models for the training data to facilitate model training. In\naddition, for the evaluation part, we find that existing evaluation frameworks\nexhibit biases in aspects such as units, simplification, and precision in\nphysics domain. To balance efficiency and accuracy, we introduce a Rule+Model\nevaluation framework tailored to physics problems. Our evaluations on current\nstate-of-the-art open-source and proprietary models highlight the limitations\nof current models in handling physics-related tasks. We hope that our dataset\nand evaluation methodology will jointly advance the development of LLMs in the\nfield of physics."}
{"id": "2506.00077", "pdf": "https://arxiv.org/pdf/2506.00077.pdf", "abs": "https://arxiv.org/abs/2506.00077", "title": "Gaussian mixture models as a proxy for interacting language models", "authors": ["Edward L. Wang", "Tianyu Wang", "Avanti Athreya", "Vince Lyzinski", "Carey E. Priebe"], "categories": ["cs.CL", "cs.LG", "stat.ML", "62R07"], "comment": null, "summary": "Large language models (LLMs) are a powerful tool with the ability to match\nhuman capabilities and behavior in many settings. Retrieval-augmented\ngeneration (RAG) further allows LLMs to generate diverse output depending on\nthe contents of their RAG database. This motivates their use in the social\nsciences to study human behavior between individuals when large-scale\nexperiments are infeasible. However, LLMs depend on complex, computationally\nexpensive algorithms. In this paper, we introduce interacting Gaussian mixture\nmodels (GMMs) as an alternative to similar frameworks using LLMs. We compare a\nsimplified model of GMMs to select experimental simulations of LLMs whose\nupdating and response depend on feedback from other LLMs. We find that\ninteracting GMMs capture important features of the dynamics in interacting\nLLMs, and we investigate key similarities and differences between interacting\nLLMs and GMMs. We conclude by discussing the benefits of Gaussian mixture\nmodels, potential modifications, and future research directions."}
{"id": "2506.00250", "pdf": "https://arxiv.org/pdf/2506.00250.pdf", "abs": "https://arxiv.org/abs/2506.00250", "title": "PersianMedQA: Language-Centric Evaluation of LLMs in the Persian Medical Domain", "authors": ["Mohammad Javad Ranjbar Kalahroodi", "Amirhossein Sheikholselami", "Sepehr Karimi", "Sepideh Ranjbar Kalahroodi", "Heshaam Faili", "Azadeh Shakery"], "categories": ["cs.CL", "cs.IT", "math.IT"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable performance on a wide\nrange of NLP benchmarks, often surpassing human-level accuracy. However, their\nreliability in high-stakes domains such as medicine, particularly in\nlow-resource languages, remains underexplored. In this work, we introduce\nPersianMedQA, a large-scale, expert-validated dataset of multiple-choice\nPersian medical questions, designed to evaluate LLMs across both Persian and\nEnglish. We benchmark over 40 state-of-the-art models, including\ngeneral-purpose, Persian fine-tuned, and medical LLMs, in zero-shot and\nchain-of-thought (CoT) settings. Our results show that closed-source general\nmodels (e.g., GPT-4.1) consistently outperform all other categories, achieving\n83.3% accuracy in Persian and 80.7% in English, while Persian fine-tuned models\nsuch as Dorna underperform significantly (e.g., 35.9% in Persian), often\nstruggling with both instruction-following and domain reasoning. We also\nanalyze the impact of translation, showing that while English performance is\ngenerally higher, Persian responses are sometimes more accurate due to cultural\nand clinical contextual cues. Finally, we demonstrate that model size alone is\ninsufficient for robust performance without strong domain or language\nadaptation. PersianMedQA provides a foundation for evaluating multilingual and\nculturally grounded medical reasoning in LLMs. The PersianMedQA dataset can be\naccessed at: https://huggingface.co/datasets/MohammadJRanjbar/PersianMedQA"}
{"id": "2506.00288", "pdf": "https://arxiv.org/pdf/2506.00288.pdf", "abs": "https://arxiv.org/abs/2506.00288", "title": "Emergent Abilities of Large Language Models under Continued Pretraining for Language Adaptation", "authors": ["Ahmed Elhady", "Eneko Agirre", "Mikel Artetxe"], "categories": ["cs.CL", "cs.AI"], "comment": "To appear in ACL 2025 Main", "summary": "Continued pretraining (CPT) is a popular approach to adapt existing large\nlanguage models (LLMs) to new languages. When doing so, it is common practice\nto include a portion of English data in the mixture, but its role has not been\ncarefully studied to date. In this work, we show that including English does\nnot impact validation perplexity, yet it is critical for the emergence of\ndownstream capabilities in the target language. We introduce a\nlanguage-agnostic benchmark for in-context learning (ICL), which reveals\ncatastrophic forgetting early on CPT when English is not included. This in turn\ndamages the ability of the model to generalize to downstream prompts in the\ntarget language as measured by perplexity, even if it does not manifest in\nterms of accuracy until later in training, and can be tied to a big shift in\nthe model parameters. Based on these insights, we introduce curriculum learning\nand exponential moving average (EMA) of weights as effective alternatives to\nmitigate the need for English. All in all, our work sheds light into the\ndynamics by which emergent abilities arise when doing CPT for language\nadaptation, and can serve as a foundation to design more effective methods in\nthe future."}
{"id": "2506.00519", "pdf": "https://arxiv.org/pdf/2506.00519.pdf", "abs": "https://arxiv.org/abs/2506.00519", "title": "CausalAbstain: Enhancing Multilingual LLMs with Causal Reasoning for Trustworthy Abstention", "authors": ["Yuxi Sun", "Aoqi Zuo", "Wei Gao", "Jing Ma"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to Association for Computational Linguistics Findings (ACL)\n  2025", "summary": "Large Language Models (LLMs) often exhibit knowledge disparities across\nlanguages. Encouraging LLMs to \\textit{abstain} when faced with knowledge gaps\nis a promising strategy to reduce hallucinations in multilingual settings.\nCurrent abstention strategies for multilingual scenarios primarily rely on\ngenerating feedback in various languages using LLMs and performing\nself-reflection. However, these methods can be adversely impacted by\ninaccuracies and biases in the generated feedback. To address this, from a\ncausal perspective, we introduce \\textit{CausalAbstain}, a method that helps\nLLMs determine whether to utilize multiple generated feedback responses and how\nto identify the most useful ones. Extensive experiments demonstrate that\n\\textit{CausalAbstain} effectively selects helpful feedback and enhances\nabstention decisions with interpretability in both native language\n(\\textsc{Casual-native}) and multilingual (\\textsc{Causal-multi}) settings,\noutperforming strong baselines on two benchmark datasets covering encyclopedic\nand commonsense knowledge QA tasks. Our code and data are open-sourced at\nhttps://github.com/peachch/CausalAbstain."}
{"id": "2506.00612", "pdf": "https://arxiv.org/pdf/2506.00612.pdf", "abs": "https://arxiv.org/abs/2506.00612", "title": "Enhancing Clinical Multiple-Choice Questions Benchmarks with Knowledge Graph Guided Distractor Generation", "authors": ["Running Yang", "Wenlong Deng", "Minghui Chen", "Yuyin Zhou", "Xiaoxiao Li"], "categories": ["cs.CL"], "comment": null, "summary": "Clinical tasks such as diagnosis and treatment require strong decision-making\nabilities, highlighting the importance of rigorous evaluation benchmarks to\nassess the reliability of large language models (LLMs). In this work, we\nintroduce a knowledge-guided data augmentation framework that enhances the\ndifficulty of clinical multiple-choice question (MCQ) datasets by generating\ndistractors (i.e., incorrect choices that are similar to the correct one and\nmay confuse existing LLMs). Using our KG-based pipeline, the generated choices\nare both clinically plausible and deliberately misleading. Our approach\ninvolves multi-step, semantically informed walks on a medical knowledge graph\nto identify distractor paths-associations that are medically relevant but\nfactually incorrect-which then guide the LLM in crafting more deceptive\ndistractors. We apply the designed knowledge graph guided distractor generation\n(KGGDG) pipline, to six widely used medical QA benchmarks and show that it\nconsistently reduces the accuracy of state-of-the-art LLMs. These findings\nestablish KGGDG as a powerful tool for enabling more robust and diagnostic\nevaluations of medical LLMs."}
{"id": "2506.00622", "pdf": "https://arxiv.org/pdf/2506.00622.pdf", "abs": "https://arxiv.org/abs/2506.00622", "title": "Improving Dialogue State Tracking through Combinatorial Search for In-Context Examples", "authors": ["Haesung Pyun", "Yoonah Park", "Yohan Jo"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "This paper has been accepted for publication at ACL 2025", "summary": "In dialogue state tracking (DST), in-context learning comprises a retriever\nthat selects labeled dialogues as in-context examples and a DST model that uses\nthese examples to infer the dialogue state of the query dialogue. Existing\nmethods for constructing training data for retrievers suffer from three key\nlimitations: (1) the synergistic effect of examples is not considered, (2) the\nlinguistic characteristics of the query are not sufficiently factored in, and\n(3) scoring is not directly optimized for DST performance. Consequently, the\nretriever can fail to retrieve examples that would substantially improve DST\nperformance. To address these issues, we present CombiSearch, a method that\nscores effective in-context examples based on their combinatorial impact on DST\nperformance. Our evaluation on MultiWOZ shows that retrievers trained with\nCombiSearch surpass state-of-the-art models, achieving a 20x gain in data\nefficiency and generalizing well to the SGD dataset. Moreover, CombiSearch\nattains a 12% absolute improvement in the upper bound DST performance over\ntraditional approaches when no retrieval errors are assumed. This significantly\nincreases the headroom for practical DST performance while demonstrating that\nexisting methods rely on suboptimal data for retriever training."}
{"id": "2506.00694", "pdf": "https://arxiv.org/pdf/2506.00694.pdf", "abs": "https://arxiv.org/abs/2506.00694", "title": "Measuring Faithfulness and Abstention: An Automated Pipeline for Evaluating LLM-Generated 3-ply Case-Based Legal Arguments", "authors": ["Li Zhang", "Morgan Gray", "Jaromir Savelka", "Kevin D. Ashley"], "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50"], "comment": "11 pages, 7th Workshop on Automated Semantic Analysis of Information\n  in Legal Text @ ICAIL 2025, 16 June 2025, Chicago, IL", "summary": "Large Language Models (LLMs) demonstrate potential in complex legal tasks\nlike argument generation, yet their reliability remains a concern. Building\nupon pilot work assessing LLM generation of 3-ply legal arguments using human\nevaluation, this paper introduces an automated pipeline to evaluate LLM\nperformance on this task, specifically focusing on faithfulness (absence of\nhallucination), factor utilization, and appropriate abstention. We define\nhallucination as the generation of factors not present in the input case\nmaterials and abstention as the model's ability to refrain from generating\narguments when instructed and no factual basis exists. Our automated method\nemploys an external LLM to extract factors from generated arguments and\ncompares them against the ground-truth factors provided in the input case\ntriples (current case and two precedent cases). We evaluated eight distinct\nLLMs on three tests of increasing difficulty: 1) generating a standard 3-ply\nargument, 2) generating an argument with swapped precedent roles, and 3)\nrecognizing the impossibility of argument generation due to lack of shared\nfactors and abstaining. Our findings indicate that while current LLMs achieve\nhigh accuracy (over 90%) in avoiding hallucination on viable argument\ngeneration tests (Tests 1 & 2), they often fail to utilize the full set of\nrelevant factors present in the cases. Critically, on the abstention test (Test\n3), most models failed to follow instructions to stop, instead generating\nspurious arguments despite the lack of common factors. This automated pipeline\nprovides a scalable method for assessing these crucial LLM behaviors,\nhighlighting the need for improvements in factor utilization and robust\nabstention capabilities before reliable deployment in legal settings. Link:\nhttps://lizhang-aiandlaw.github.io/An-Automated-Pipeline-for-Evaluating-LLM-Generated-3-ply-Case-Based-Legal-Arguments/"}
{"id": "2506.00773", "pdf": "https://arxiv.org/pdf/2506.00773.pdf", "abs": "https://arxiv.org/abs/2506.00773", "title": "Dynamic Chunking and Selection for Reading Comprehension of Ultra-Long Context in Large Language Models", "authors": ["Boheng Sheng", "Jiacheng Yao", "Meicong Zhang", "Guoxiu He"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 Main Conference", "summary": "Large language models (LLMs) often struggle to accurately read and comprehend\nextremely long texts. Current methods for improvement typically rely on\nsplitting long contexts into fixed-length chunks. However, fixed truncation\nrisks separating semantically relevant content, leading to ambiguity and\ncompromising accurate understanding. To overcome this limitation, we propose a\nstraightforward approach for dynamically separating and selecting chunks of\nlong context, facilitating a more streamlined input for LLMs. In particular, we\ncompute semantic similarities between adjacent sentences, using lower\nsimilarities to adaptively divide long contexts into variable-length chunks. We\nfurther train a question-aware classifier to select sensitive chunks that are\ncritical for answering specific questions. Experimental results on both\nsingle-hop and multi-hop question-answering benchmarks show that the proposed\napproach consistently outperforms strong baselines. Notably, it maintains\nrobustness across a wide range of input lengths, handling sequences of up to\n256k tokens. Our datasets and code are available at the following link:\nhttps://github.com/ECNU-Text-Computing/DCS"}
{"id": "2506.00829", "pdf": "https://arxiv.org/pdf/2506.00829.pdf", "abs": "https://arxiv.org/abs/2506.00829", "title": "COMPKE: Complex Question Answering under Knowledge Editing", "authors": ["Keyuan Cheng", "Zijian Kan", "Zhixian He", "Zhuoran Zhang", "Muhammad Asif Ali", "Ke Xu", "Lijie Hu", "Di Wang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by ACL 2025 Findings", "summary": "Knowledge Editing, which efficiently modifies the knowledge in large language\nmodels, has gathered great attention. Current benchmarks primarily use\nmulti-hop question answering to assess and analyze newly injected or updated\nknowledge. However, we argue that these benchmarks fail to effectively evaluate\nhow well the updated models apply this knowledge in real-life scenarios,\nparticularly when questions require complex reasoning, involving one-to-many\nrelationships or multi-step logical intersections. To fill in this gap, we\nintroduce a new benchmark, COMPKE: Complex Question Answering under Knowledge\nEditing, which includes 11,924 complex questions that reflect real-life\nsituations. We conduct an extensive evaluation of four knowledge editing\nmethods on COMPKE, revealing that their effectiveness varies notably across\ndifferent models. For instance, MeLLo attains an accuracy of 39.47 on\nGPT-4O-MINI, but this drops sharply to 3.83 on QWEN2.5-3B. We further\ninvestigate the underlying causes of these disparities from both methodological\nand model-specific perspectives. The datasets are available at\nhttps://github.com/kzjkzj666/CompKE."}
{"id": "2506.00859", "pdf": "https://arxiv.org/pdf/2506.00859.pdf", "abs": "https://arxiv.org/abs/2506.00859", "title": "How Bidirectionality Helps Language Models Learn Better via Dynamic Bottleneck Estimation", "authors": ["Md Kowsher", "Nusrat Jahan Prottasha", "Shiyun Xu", "Shetu Mohanto", "Chen Chen", "Ozlem Garibay", "Niloofar Yousefi"], "categories": ["cs.CL"], "comment": null, "summary": "Bidirectional language models have better context understanding and perform\nbetter than unidirectional models on natural language understanding tasks, yet\nthe theoretical reasons behind this advantage remain unclear. In this work, we\ninvestigate this disparity through the lens of the Information Bottleneck (IB)\nprinciple, which formalizes a trade-off between compressing input information\nand preserving task-relevant content. We propose FlowNIB, a dynamic and\nscalable method for estimating mutual information during training that\naddresses key limitations of classical IB approaches, including computational\nintractability and fixed trade-off schedules. Theoretically, we show that\nbidirectional models retain more mutual information and exhibit higher\neffective dimensionality than unidirectional models. To support this, we\npresent a generalized framework for measuring representational complexity and\nprove that bidirectional representations are strictly more informative under\nmild conditions. We further validate our findings through extensive experiments\nacross multiple models and tasks using FlowNIB, revealing how information is\nencoded and compressed throughout training. Together, our work provides a\nprincipled explanation for the effectiveness of bidirectional architectures and\nintroduces a practical tool for analyzing information flow in deep language\nmodels."}
{"id": "2506.00912", "pdf": "https://arxiv.org/pdf/2506.00912.pdf", "abs": "https://arxiv.org/abs/2506.00912", "title": "Pi-SQL: Enhancing Text-to-SQL with Fine-Grained Guidance from Pivot Programming Languages", "authors": ["Yongdong chi", "Hanqing Wang", "Zonghan Yang", "Jian Yang", "Xiao Yan", "Yun Chen", "Guanhua Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Text-to-SQL transforms the user queries from natural language to executable\nSQL programs, enabling non-experts to interact with complex databases. Existing\nprompt-based methods craft meticulous text guidelines and examples to\nfacilitate SQL generation, but their accuracy is hindered by the large semantic\ngap between the texts and the low-resource SQL programs. In this work, we\npropose Pi-SQL, which incorporates the high-resource Python program as a pivot\nto bridge between the natural language query and SQL program. In particular,\nPi-SQL first generates Python programs that provide fine-grained step-by-step\nguidelines in their code blocks or comments, and then produces an SQL program\nfollowing the guidance of each Python program. The final SQL program matches\nthe reference Python program's query results and, through selection from\ncandidates generated by different strategies, achieves superior execution\nspeed, with a reward-based valid efficiency score up to 4.55 higher than the\nbest-performing baseline. Extensive experiments demonstrate the effectiveness\nof Pi-SQL, which improves the execution accuracy of the best-performing\nbaseline by up to 3.20."}
{"id": "2506.01047", "pdf": "https://arxiv.org/pdf/2506.01047.pdf", "abs": "https://arxiv.org/abs/2506.01047", "title": "CHEER-Ekman: Fine-grained Embodied Emotion Classification", "authors": ["Phan Anh Duong", "Cat Luong", "Divyesh Bommana", "Tianyu Jiang"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Emotions manifest through physical experiences and bodily reactions, yet\nidentifying such embodied emotions in text remains understudied. We present an\nembodied emotion classification dataset, CHEER-Ekman, extending the existing\nbinary embodied emotion dataset with Ekman's six basic emotion categories.\nUsing automatic best-worst scaling with large language models, we achieve\nperformance superior to supervised approaches on our new dataset. Our\ninvestigation reveals that simplified prompting instructions and\nchain-of-thought reasoning significantly improve emotion recognition accuracy,\nenabling smaller models to achieve competitive performance with larger ones.\nOur dataset is publicly available at: https://github.com/menamerai/cheer-ekman."}
{"id": "2506.01496", "pdf": "https://arxiv.org/pdf/2506.01496.pdf", "abs": "https://arxiv.org/abs/2506.01496", "title": "Continual Speech Learning with Fused Speech Features", "authors": ["Guitao Wang", "Jinming Zhao", "Hao Yang", "Guilin Qi", "Tongtong Wu", "Gholamreza Haffari"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Rapid growth in speech data demands adaptive models, as traditional static\nmethods fail to keep pace with dynamic and diverse speech information. We\nintroduce continuous speech learning, a new set-up targeting at bridging the\nadaptation gap in current speech models. We use the encoder-decoder Whisper\nmodel to standardize speech tasks into a generative format. We integrate a\nlearnable gated-fusion layer on the top of the encoder to dynamically select\ntask-specific features for downstream tasks. Our approach improves accuracy\nsignificantly over traditional methods in six speech processing tasks,\ndemonstrating gains in adapting to new speech tasks without full retraining."}
{"id": "2506.01531", "pdf": "https://arxiv.org/pdf/2506.01531.pdf", "abs": "https://arxiv.org/abs/2506.01531", "title": "STORM-BORN: A Challenging Mathematical Derivations Dataset Curated via a Human-in-the-Loop Multi-Agent Framework", "authors": ["Wenhao Liu", "Zhenyi Lu", "Xinyu Hu", "Jierui Zhang", "Dailin Li", "Jiacheng Cen", "Huilin Cao", "Haiteng Wang", "Yuhan Li", "Kun Xie", "Dandan Li", "Pei Zhang", "Chengbo Zhang", "Yuxiang Ren", "Xiaohong Huang", "Yan Ma"], "categories": ["cs.CL"], "comment": "accepted by ACL2025", "summary": "High-quality math datasets are crucial for advancing the reasoning abilities\nof large language models (LLMs). However, existing datasets often suffer from\nthree key issues: outdated and insufficient challenging content, neglecting\nhuman-like reasoning, and limited reliability due to single-LLM generation. To\naddress these, we introduce STORM-BORN, an ultra-challenging dataset of\nmathematical derivations sourced from cutting-edge academic papers, which\nincludes dense human-like approximations and heuristic cues. To ensure the\nreliability and quality, we propose a novel human-in-the-loop, multi-agent data\ngeneration framework, integrating reasoning-dense filters, multi-agent\ncollaboration, and human mathematicians' evaluations. We curated a set of 2,000\nsynthetic samples and deliberately selected the 100 most difficult problems.\nEven most advanced models like GPT-o1 solved fewer than 5% of them. Fine-tuning\non STORM-BORN boosts accuracy by 7.84% (LLaMA3-8B) and 9.12% (Qwen2.5-7B). As\nAI approaches mathematician-level reasoning, STORM-BORN provides both a\nhigh-difficulty benchmark and a human-like reasoning training resource. Our\ncode and dataset are publicly available at\nhttps://github.com/lwhere/STORM-BORN."}
{"id": "2506.01615", "pdf": "https://arxiv.org/pdf/2506.01615.pdf", "abs": "https://arxiv.org/abs/2506.01615", "title": "IndicRAGSuite: Large-Scale Datasets and a Benchmark for Indian Language RAG Systems", "authors": ["Pasunuti Prasanjith", "Prathmesh B More", "Anoop Kunchukuttan", "Raj Dabre"], "categories": ["cs.CL"], "comment": "WIP", "summary": "Retrieval-Augmented Generation (RAG) systems enable language models to access\nrelevant information and generate accurate, well-grounded, and contextually\ninformed responses. However, for Indian languages, the development of\nhigh-quality RAG systems is hindered by the lack of two critical resources: (1)\nevaluation benchmarks for retrieval and generation tasks, and (2) large-scale\ntraining datasets for multilingual retrieval. Most existing benchmarks and\ndatasets are centered around English or high-resource languages, making it\ndifficult to extend RAG capabilities to the diverse linguistic landscape of\nIndia. To address the lack of evaluation benchmarks, we create IndicMSMarco, a\nmultilingual benchmark for evaluating retrieval quality and response generation\nin 13 Indian languages, created via manual translation of 1000 diverse queries\nfrom MS MARCO-dev set. To address the need for training data, we build a\nlarge-scale dataset of (question, answer, relevant passage) tuples derived from\nthe Wikipedias of 19 Indian languages using state-of-the-art LLMs.\nAdditionally, we include translated versions of the original MS MARCO dataset\nto further enrich the training data and ensure alignment with real-world\ninformation-seeking tasks. Resources are available here:\nhttps://huggingface.co/collections/ai4bharat/indicragsuite-683e7273cb2337208c8c0fcb"}
{"id": "2506.01776", "pdf": "https://arxiv.org/pdf/2506.01776.pdf", "abs": "https://arxiv.org/abs/2506.01776", "title": "MaXIFE: Multilingual and Cross-lingual Instruction Following Evaluation", "authors": ["Yile Liu", "Ziwei Ma", "Xiu Jiang", "Jinglu Hu", "Jing Chang", "Liang Li"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Main Conference", "summary": "With the rapid adoption of large language models (LLMs) in natural language\nprocessing, the ability to follow instructions has emerged as a key metric for\nevaluating their practical utility. However, existing evaluation methods often\nfocus on single-language scenarios, overlooking the challenges and differences\npresent in multilingual and cross-lingual contexts. To address this gap, we\nintroduce MaXIFE: a comprehensive evaluation benchmark designed to assess\ninstruction-following capabilities across 23 different languages with 1667\nverifiable instruction tasks. MaXIFE integrates both Rule-Based Evaluation and\nModel-Based Evaluation, ensuring a balance of efficiency and accuracy. We\napplied MaXIFE to evaluate several leading commercial LLMs, establishing\nbaseline results for future comparisons. By providing a standardized tool for\nmultilingual instruction-following evaluation, MaXIFE aims to advance research\nand development in natural language processing."}
{"id": "2308.16061", "pdf": "https://arxiv.org/pdf/2308.16061.pdf", "abs": "https://arxiv.org/abs/2308.16061", "title": "Conti Inc.: Understanding the Internal Discussions of a large Ransomware-as-a-Service Operator with Machine Learning", "authors": ["Estelle Ruellan", "Masarah Paquet-Clouston", "Sebastian Garcia"], "categories": ["cs.CR", "cs.CL", "cs.LG"], "comment": null, "summary": "Ransomware-as-a-service (RaaS) is increasing the scale and complexity of\nransomware attacks. Understanding the internal operations behind RaaS has been\na challenge due to the illegality of such activities. The recent chat leak of\nthe Conti RaaS operator, one of the most infamous ransomware operators on the\ninternational scene, offers a key opportunity to better understand the inner\nworkings of such organizations. This paper analyzes the main topic discussions\nin the Conti chat leak using machine learning techniques such as Natural\nLanguage Processing (NLP) and Latent Dirichlet Allocation (LDA), as well as\nvisualization strategies. Five discussion topics are found: 1) Business, 2)\nTechnical, 3) Internal tasking/Management, 4) Malware, and 5) Customer\nService/Problem Solving. Moreover, the distribution of topics among Conti\nmembers shows that only 4% of individuals have specialized discussions while\nalmost all individuals (96%) are all-rounders, meaning that their discussions\nrevolve around the five topics. The results also indicate that a significant\nproportion of Conti discussions are non-tech related. This study thus\nhighlights that running such large RaaS operations requires a workforce skilled\nbeyond technical abilities, with individuals involved in various tasks, from\nmanagement to customer service or problem solving. The discussion topics also\nshow that the organization behind the Conti RaaS oper5086933ator shares\nsimilarities with a large firm. We conclude that, although RaaS represents an\nexample of specialization in the cybercrime industry, only a few members are\nspecialized in one topic, while the rest runs and coordinates the RaaS\noperation."}
{"id": "2310.08367", "pdf": "https://arxiv.org/pdf/2310.08367.pdf", "abs": "https://arxiv.org/abs/2310.08367", "title": "MCU: An Evaluation Framework for Open-Ended Game Agents", "authors": ["Xinyue Zheng", "Haowei Lin", "Kaichen He", "Zihao Wang", "Zilong Zheng", "Yitao Liang"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": null, "summary": "Developing AI agents capable of interacting with open-world environments to\nsolve diverse tasks is a compelling challenge. However, evaluating such\nopen-ended agents remains difficult, with current benchmarks facing scalability\nlimitations. To address this, we introduce Minecraft Universe (MCU), a\ncomprehensive evaluation framework set within the open-world video game\nMinecraft. MCU incorporates three key components: (1) an expanding collection\nof 3,452 composable atomic tasks that encompasses 11 major categories and 41\nsubcategories of challenges; (2) a task composition mechanism capable of\ngenerating infinite diverse tasks with varying difficulty; and (3) a general\nevaluation framework that achieves 91.5\\% alignment with human ratings for\nopen-ended task assessment. Empirical results reveal that even state-of-the-art\nfoundation agents struggle with the increasing diversity and complexity of\ntasks. These findings highlight the necessity of MCU as a robust benchmark to\ndrive progress in AI agent development within open-ended environments. Our\nevaluation code and scripts are available at\nhttps://github.com/CraftJarvis/MCU."}
{"id": "2402.15319", "pdf": "https://arxiv.org/pdf/2402.15319.pdf", "abs": "https://arxiv.org/abs/2402.15319", "title": "GPTVQ: The Blessing of Dimensionality for LLM Quantization", "authors": ["Mart van Baalen", "Andrey Kuzmin", "Ivan Koryakovskiy", "Markus Nagel", "Peter Couperus", "Cedric Bastoul", "Eric Mahurin", "Tijmen Blankevoort", "Paul Whatmough"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "In this work we show that the size versus accuracy trade-off of neural\nnetwork quantization can be significantly improved by increasing the\nquantization dimensionality. We propose the GPTVQ method, a new fast method for\npost-training vector quantization (VQ) that scales well to Large Language\nModels (LLMs). Our method interleaves quantization of one or more columns with\nupdates to the remaining unquantized weights, using information from the\nHessian of the per-layer output reconstruction MSE. Quantization codebooks are\ninitialized using an efficient data-aware version of the EM algorithm. The\ncodebooks are then updated, and further compressed by using integer\nquantization and SVD-based compression. GPTVQ establishes a new state-of-the\nart in the size vs accuracy trade-offs on a wide range of LLMs such as Llama-v2\nand Mistral. Furthermore, our method is efficient: on a single H100 it takes\nbetween 3 and 11 hours to process a Llamav2-70B model, depending on\nquantization setting. Lastly, with on-device timings for VQ decompression on a\nmobile CPU we show that VQ leads to improved latency compared to using a 4-bit\ninteger format."}
{"id": "2404.16873", "pdf": "https://arxiv.org/pdf/2404.16873.pdf", "abs": "https://arxiv.org/abs/2404.16873", "title": "AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs", "authors": ["Anselm Paulus", "Arman Zharmagambetov", "Chuan Guo", "Brandon Amos", "Yuandong Tian"], "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted to ICML 2025. Code is available at\n  http://github.com/facebookresearch/advprompter", "summary": "Large Language Models (LLMs) are vulnerable to jailbreaking attacks that lead\nto generation of inappropriate or harmful content. Manual red-teaming requires\na time-consuming search for adversarial prompts, whereas automatic adversarial\nprompt generation often leads to semantically meaningless attacks that do not\nscale well. In this paper, we present a novel method that uses another LLM,\ncalled AdvPrompter, to generate human-readable adversarial prompts in seconds.\nAdvPrompter, which is trained using an alternating optimization algorithm,\ngenerates suffixes that veil the input instruction without changing its\nmeaning, such that the TargetLLM is lured to give a harmful response.\nExperimental results on popular open source TargetLLMs show highly competitive\nresults on the AdvBench and HarmBench datasets, that also transfer to\nclosed-source black-box LLMs. We also show that training on adversarial\nsuffixes generated by AdvPrompter is a promising strategy for improving the\nrobustness of LLMs to jailbreaking attacks."}
{"id": "2410.11840", "pdf": "https://arxiv.org/pdf/2410.11840.pdf", "abs": "https://arxiv.org/abs/2410.11840", "title": "A Hitchhiker's Guide to Scaling Law Estimation", "authors": ["Leshem Choshen", "Yang Zhang", "Jacob Andreas"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "ICML", "summary": "Scaling laws predict the loss of a target machine learning model by\nextrapolating from easier-to-train models with fewer parameters or smaller\ntraining sets. This provides an efficient way for practitioners and researchers\nalike to compare pretraining decisions involving optimizers, datasets, and\nmodel architectures. Despite the widespread use of scaling laws to model the\ndynamics of language model training, there has been little work on\nunderstanding how to best estimate and interpret them. We collect (and release)\na large-scale dataset containing losses and downstream evaluations for 485\npreviously published pretrained models. We use these to estimate more than 1000\nscaling laws, then derive a set of best practices for estimating scaling laws\nin new model families. We find that fitting scaling laws to intermediate\ncheckpoints of training runs (and not just their final losses) substantially\nimproves accuracy, and that -- all else equal -- estimates of performance are\ngenerally most accurate when derived from other models of similar sizes.\nHowever, because there is a significant degree of variability across model\nseeds, training multiple small models is sometimes more useful than training a\nsingle large one. Moreover, while different model families differ scaling\nbehavior, they are often similar enough that a target model's behavior can be\npredicted from a single model with the same architecture, along with scaling\nparameter estimates derived from other model families."}
{"id": "2411.09689", "pdf": "https://arxiv.org/pdf/2411.09689.pdf", "abs": "https://arxiv.org/abs/2411.09689", "title": "Probing LLM Hallucination from Within: Perturbation-Driven Approach via Internal Knowledge", "authors": ["Seongmin Lee", "Hsiang Hsu", "Chun-Fu Chen", "Duen Horng", "Chau"], "categories": ["cs.AI", "cs.CL"], "comment": "22 pages, 15 figures", "summary": "LLM hallucination, where unfaithful text is generated, presents a critical\nchallenge for LLMs' practical applications. Current detection methods often\nresort to external knowledge, LLM fine-tuning, or supervised training with\nlarge hallucination-labeled datasets. Moreover, these approaches do not\ndistinguish between different types of hallucinations, which is crucial for\nenhancing detection performance. To address such limitations, we introduce\nhallucination probing, a new task that classifies LLM-generated text into three\ncategories: aligned, misaligned, and fabricated. Driven by our novel discovery\nthat perturbing key entities in prompts affects LLM's generation of these three\ntypes of text differently, we propose SHINE, a novel hallucination probing\nmethod that does not require external knowledge, supervised training, or LLM\nfine-tuning. SHINE is effective in hallucination probing across three modern\nLLMs, and achieves state-of-the-art performance in hallucination detection,\noutperforming seven competing methods across four datasets and four LLMs,\nunderscoring the importance of probing for accurate detection."}
{"id": "2411.14725", "pdf": "https://arxiv.org/pdf/2411.14725.pdf", "abs": "https://arxiv.org/abs/2411.14725", "title": "Evaluating and Advancing Multimodal Large Language Models in Perception Ability Lens", "authors": ["Feng Chen", "Chenhui Gou", "Jing Liu", "Yang Yang", "Zhaoyang Li", "Jiyuan Zhang", "Zhenbang Sun", "Bohan Zhuang", "Qi Wu"], "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "Code repository:\n  https://github.com/Chenfeng1271/AbilityLens/tree/main", "summary": "As multimodal large language models (MLLMs) advance rapidly, rigorous\nevaluation has become essential, providing further guidance for their\ndevelopment. In this work, we focus on a unified and robust evaluation of\n\\textbf{vision perception} abilities, the foundational skill of MLLMs. We find\nthat existing perception benchmarks, each focusing on different question types,\ndomains, and evaluation metrics, introduce significant evaluation variance,\ncomplicating comprehensive assessments of perception abilities when relying on\nany single benchmark. To address this, we introduce \\textbf{AbilityLens}, a\nunified benchmark designed to evaluate MLLMs in six key perception abilities\n(ranging from counting, OCR, to understanding structural data), focusing on\nboth accuracy and stability, with each ability encompassing diverse types of\nquestions, domains, and metrics. With the assistance of AbilityLens, we: (1)\nidentify the strengths and weaknesses of current main-stream MLLMs,\nhighlighting stability patterns and revealing a notable performance gap between\nstate-of-the-art open-source and closed-source models; (2) uncover interesting\nability conflict and early convergence phenomena during MLLM training; (3)\nreveal the primary reason of ability conflict is data mixing ratio and LLM\nmodel size; and (4) discuss the effectiveness of some straightforward\nstrategies \\eg, fine-tuning and model merging, to solve the ability conflict.\nThe benchmark and online leaderboard is released in\nhttps://github.com/Chenfeng1271/AbilityLens."}
{"id": "2412.10849", "pdf": "https://arxiv.org/pdf/2412.10849.pdf", "abs": "https://arxiv.org/abs/2412.10849", "title": "Superhuman performance of a large language model on the reasoning tasks of a physician", "authors": ["Peter G. Brodeur", "Thomas A. Buckley", "Zahir Kanjee", "Ethan Goh", "Evelyn Bin Ling", "Priyank Jain", "Stephanie Cabral", "Raja-Elie Abdulnour", "Adrian D. Haimovich", "Jason A. Freed", "Andrew Olson", "Daniel J. Morgan", "Jason Hom", "Robert Gallo", "Liam G. McCoy", "Haadi Mombini", "Christopher Lucas", "Misha Fotoohi", "Matthew Gwiazdon", "Daniele Restifo", "Daniel Restrepo", "Eric Horvitz", "Jonathan Chen", "Arjun K. Manrai", "Adam Rodman"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "A seminal paper published by Ledley and Lusted in 1959 introduced complex\nclinical diagnostic reasoning cases as the gold standard for the evaluation of\nexpert medical computing systems, a standard that has held ever since. Here, we\nreport the results of a physician evaluation of a large language model (LLM) on\nchallenging clinical cases against a baseline of hundreds of physicians. We\nconduct five experiments to measure clinical reasoning across differential\ndiagnosis generation, display of diagnostic reasoning, triage differential\ndiagnosis, probabilistic reasoning, and management reasoning, all adjudicated\nby physician experts with validated psychometrics. We then report a real-world\nstudy comparing human expert and AI second opinions in randomly-selected\npatients in the emergency room of a major tertiary academic medical center in\nBoston, MA. We compared LLMs and board-certified physicians at three predefined\ndiagnostic touchpoints: triage in the emergency room, initial evaluation by a\nphysician, and admission to the hospital or intensive care unit. In all\nexperiments--both vignettes and emergency room second opinions--the LLM\ndisplayed superhuman diagnostic and reasoning abilities, as well as continued\nimprovement from prior generations of AI clinical decision support. Our study\nsuggests that LLMs have achieved superhuman performance on general medical\ndiagnostic and management reasoning, fulfilling the vision put forth by Ledley\nand Lusted, and motivating the urgent need for prospective trials."}
{"id": "2412.15289", "pdf": "https://arxiv.org/pdf/2412.15289.pdf", "abs": "https://arxiv.org/abs/2412.15289", "title": "SATA: A Paradigm for LLM Jailbreak via Simple Assistive Task Linkage", "authors": ["Xiaoning Dong", "Wenbo Hu", "Wei Xu", "Tianxing He"], "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have made significant advancements across\nvarious tasks, but their safety alignment remain a major concern. Exploring\njailbreak prompts can expose LLMs' vulnerabilities and guide efforts to secure\nthem. Existing methods primarily design sophisticated instructions for the LLM\nto follow, or rely on multiple iterations, which could hinder the performance\nand efficiency of jailbreaks. In this work, we propose a novel jailbreak\nparadigm, Simple Assistive Task Linkage (SATA), which can effectively\ncircumvent LLM safeguards and elicit harmful responses. Specifically, SATA\nfirst masks harmful keywords within a malicious query to generate a relatively\nbenign query containing one or multiple [MASK] special tokens. It then employs\na simple assistive task such as a masked language model task or an element\nlookup by position task to encode the semantics of the masked keywords.\nFinally, SATA links the assistive task with the masked query to jointly perform\nthe jailbreak. Extensive experiments show that SATA achieves state-of-the-art\nperformance and outperforms baselines by a large margin. Specifically, on\nAdvBench dataset, with mask language model (MLM) assistive task, SATA achieves\nan overall attack success rate (ASR) of 85% and harmful score (HS) of 4.57, and\nwith element lookup by position (ELP) assistive task, SATA attains an overall\nASR of 76% and HS of 4.43."}
{"id": "2412.17626", "pdf": "https://arxiv.org/pdf/2412.17626.pdf", "abs": "https://arxiv.org/abs/2412.17626", "title": "Tracking the Feature Dynamics in LLM Training: A Mechanistic Study", "authors": ["Yang Xu", "Yi Wang", "Hengguan Huang", "Hao Wang"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Understanding training dynamics and feature evolution is crucial for the\nmechanistic interpretability of large language models (LLMs). Although sparse\nautoencoders (SAEs) have been used to identify features within LLMs, a clear\npicture of how these features evolve during training remains elusive. In this\nstudy, we (1) introduce SAE-Track, a novel method for efficiently obtaining a\ncontinual series of SAEs, providing the foundation for a mechanistic study that\ncovers (2) the semantic evolution of features, (3) the underlying processes of\nfeature formation, and (4) the directional drift of feature vectors. Our work\nprovides new insights into the dynamics of features in LLMs, enhancing our\nunderstanding of training mechanisms and feature evolution. For\nreproducibility, our code is available at\nhttps://github.com/Superposition09m/SAE-Track."}
{"id": "2502.04328", "pdf": "https://arxiv.org/pdf/2502.04328.pdf", "abs": "https://arxiv.org/abs/2502.04328", "title": "Ola: Pushing the Frontiers of Omni-Modal Language Model", "authors": ["Zuyan Liu", "Yuhao Dong", "Jiahui Wang", "Ziwei Liu", "Winston Hu", "Jiwen Lu", "Yongming Rao"], "categories": ["cs.CV", "cs.CL", "cs.MM", "cs.SD", "eess.AS", "eess.IV"], "comment": null, "summary": "Recent advances in large language models, particularly following GPT-4o, have\nsparked increasing interest in developing omni-modal models capable of\nunderstanding more modalities. While some open-source alternatives have\nemerged, there is still a notable lag behind specialized single-modality models\nin performance. In this paper, we present Ola, an Omni-modal Language model\nthat achieves competitive performance across image, video, and audio\nunderstanding compared to specialized counterparts, pushing the frontiers of\nthe omni-modal language model to a large extent. We conduct a comprehensive\nexploration of architectural design, data curation, and training strategies\nessential for building a robust omni-modal model. Ola incorporates advanced\nvisual understanding and audio recognition capabilities through several\ncritical and effective improvements over mainstream baselines. Moreover, we\nrethink inter-modal relationships during omni-modal training, emphasizing\ncross-modal alignment with video as a central bridge, and propose a progressive\ntraining pipeline that begins with the most distinct modalities and gradually\nmoves towards closer modality alignment. Extensive experiments demonstrate that\nOla surpasses existing open omni-modal LLMs across all modalities while\nachieving highly competitive performance compared to state-of-the-art\nspecialized models of similar sizes. We aim to make Ola a fully open omni-modal\nunderstanding solution to advance future research in this emerging field. Model\nweights, code, and data are open-sourced at https://github.com/Ola-Omni/Ola."}
{"id": "2502.06806", "pdf": "https://arxiv.org/pdf/2502.06806.pdf", "abs": "https://arxiv.org/abs/2502.06806", "title": "Logits are All We Need to Adapt Closed Models", "authors": ["Gaurush Hiranandani", "Haolun Wu", "Subhojyoti Mukherjee", "Sanmi Koyejo"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "29 pages, 8 figures", "summary": "Many commercial Large Language Models (LLMs) are often closed-source,\nlimiting developers to prompt tuning for aligning content generation with\nspecific applications. While these models currently do not provide access to\ntoken logits, we argue that if such access were available, it would enable more\npowerful adaptation techniques beyond prompt engineering. In this paper, we\npropose a token-level probability reweighting framework that, given access to\nlogits and a small amount of task-specific data, can effectively steer\nblack-box LLMs toward application-specific content generation. Our approach\nviews next-token prediction through the lens of supervised classification. We\nshow that aligning black-box LLMs with task-specific data can be formulated as\na label noise correction problem, leading to Plugin model -- an autoregressive\nprobability reweighting model that operates solely on logits. We provide\ntheoretical justification for why reweighting logits alone is sufficient for\ntask adaptation. Extensive experiments with multiple datasets, LLMs, and\nreweighting models demonstrate the effectiveness of our method, advocating for\nbroader access to token logits in closed-source models."}
{"id": "2502.13928", "pdf": "https://arxiv.org/pdf/2502.13928.pdf", "abs": "https://arxiv.org/abs/2502.13928", "title": "Symmetrical Visual Contrastive Optimization: Aligning Vision-Language Models with Minimal Contrastive Images", "authors": ["Shengguang Wu", "Fan-Yun Sun", "Kaiyue Wen", "Nick Haber"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted to ACL 2025 Main. Project Website: https://s-vco.github.io/", "summary": "Recent studies have shown that Large Vision-Language Models (VLMs) tend to\nneglect image content and over-rely on language-model priors, resulting in\nerrors in visually grounded tasks and hallucinations. We hypothesize that this\nissue arises because existing VLMs are not explicitly trained to generate texts\nthat are accurately grounded in fine-grained image details. To enhance visual\nfeedback during VLM training, we propose S-VCO (Symmetrical Visual Contrastive\nOptimization), a novel finetuning objective that steers the model toward\ncapturing important visual details and aligning them with corresponding text\ntokens. To further facilitate this detailed alignment, we introduce MVC, a\npaired image-text dataset built by automatically filtering and augmenting\nvisual counterfactual data to challenge the model with hard contrastive cases\ninvolving Minimal Visual Contrasts. Experiments show that our method\nconsistently improves VLM performance across diverse benchmarks covering\nvarious abilities and domains, achieving up to a 22% reduction in\nhallucinations, and significant gains in vision-centric and general tasks.\nNotably, these improvements become increasingly pronounced in benchmarks with\nhigher visual dependency. In short, S-VCO offers a significant enhancement of\nVLM's visually-dependent task performance while retaining or even improving the\nmodel's general abilities. We opensource our code at https://s-vco.github.io/"}
{"id": "2502.14354", "pdf": "https://arxiv.org/pdf/2502.14354.pdf", "abs": "https://arxiv.org/abs/2502.14354", "title": "Self-Improvement Towards Pareto Optimality: Mitigating Preference Conflicts in Multi-Objective Alignment", "authors": ["Moxin Li", "Yuantao Zhang", "Wenjie Wang", "Wentao Shi", "Zhuo Liu", "Fuli Feng", "Tat-Seng Chua"], "categories": ["cs.LG", "cs.CL"], "comment": "ACL findings (2025)", "summary": "Multi-Objective Alignment (MOA) aims to align LLMs' responses with multiple\nhuman preference objectives, with Direct Preference Optimization (DPO) emerging\nas a prominent approach. However, we find that DPO-based MOA approaches suffer\nfrom widespread preference conflicts in the data, where different objectives\nfavor different responses. This results in conflicting optimization directions,\nhindering the optimization on the Pareto Front. To address this, we propose to\nconstruct Pareto-optimal responses to resolve preference conflicts. To\nefficiently obtain and utilize such responses, we propose a self-improving DPO\nframework that enables LLMs to self-generate and select Pareto-optimal\nresponses for self-supervised preference alignment. Extensive experiments on\ntwo datasets demonstrate the superior Pareto Front achieved by our framework\ncompared to various baselines. Code is available at\nhttps://github.com/zyttt-coder/SIPO."}
{"id": "2502.15806", "pdf": "https://arxiv.org/pdf/2502.15806.pdf", "abs": "https://arxiv.org/abs/2502.15806", "title": "A Mousetrap: Fooling Large Reasoning Models for Jailbreak with Chain of Iterative Chaos", "authors": ["Yang Yao", "Xuan Tong", "Ruofan Wang", "Yixu Wang", "Lujundong Li", "Liang Liu", "Yan Teng", "Yingchun Wang"], "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Large Reasoning Models (LRMs) have significantly advanced beyond traditional\nLarge Language Models (LLMs) with their exceptional logical reasoning\ncapabilities, yet these improvements introduce heightened safety risks. When\nsubjected to jailbreak attacks, their ability to generate more targeted and\norganized content can lead to greater harm. Although some studies claim that\nreasoning enables safer LRMs against existing LLM attacks, they overlook the\ninherent flaws within the reasoning process itself. To address this gap, we\npropose the first jailbreak attack targeting LRMs, exploiting their unique\nvulnerabilities stemming from the advanced reasoning capabilities.\nSpecifically, we introduce a Chaos Machine, a novel component to transform\nattack prompts with diverse one-to-one mappings. The chaos mappings iteratively\ngenerated by the machine are embedded into the reasoning chain, which\nstrengthens the variability and complexity and also promotes a more robust\nattack. Based on this, we construct the Mousetrap framework, which makes\nattacks projected into nonlinear-like low sample spaces with mismatched\ngeneralization enhanced. Also, due to the more competing objectives, LRMs\ngradually maintain the inertia of unpredictable iterative reasoning and fall\ninto our trap. Success rates of the Mousetrap attacking o1-mini, Claude-Sonnet\nand Gemini-Thinking are as high as 96%, 86% and 98% respectively on our toxic\ndataset Trotter. On benchmarks such as AdvBench, StrongREJECT, and HarmBench,\nattacking Claude-Sonnet, well-known for its safety, Mousetrap can astonishingly\nachieve success rates of 87.5%, 86.58% and 93.13% respectively. Attention: This\npaper contains inappropriate, offensive and harmful content."}
{"id": "2502.16810", "pdf": "https://arxiv.org/pdf/2502.16810.pdf", "abs": "https://arxiv.org/abs/2502.16810", "title": "Grounded Persuasive Language Generation for Automated Marketing", "authors": ["Jibang Wu", "Chenghao Yang", "Simon Mahns", "Chaoqi Wang", "Hao Zhu", "Fei Fang", "Haifeng Xu"], "categories": ["cs.AI", "cs.CL", "cs.HC", "econ.GN", "q-fin.EC"], "comment": null, "summary": "This paper develops an agentic framework that employs large language models\n(LLMs) to automate the generation of persuasive and grounded marketing content,\nusing real estate listing descriptions as our focal application domain. Our\nmethod is designed to align the generated content with user preferences while\nhighlighting useful factual attributes. This agent consists of three key\nmodules: (1) Grounding Module, mimicking expert human behavior to predict\nmarketable features; (2) Personalization Module, aligning content with user\npreferences; (3) Marketing Module, ensuring factual accuracy and the inclusion\nof localized features. We conduct systematic human-subject experiments in the\ndomain of real estate marketing, with a focus group of potential house buyers.\nThe results demonstrate that marketing descriptions generated by our approach\nare preferred over those written by human experts by a clear margin while\nmaintaining the same level of factual accuracy. Our findings suggest a\npromising agentic approach to automate large-scale targeted marketing while\nensuring factuality of content generation."}
{"id": "2502.18017", "pdf": "https://arxiv.org/pdf/2502.18017.pdf", "abs": "https://arxiv.org/abs/2502.18017", "title": "ViDoRAG: Visual Document Retrieval-Augmented Generation via Dynamic Iterative Reasoning Agents", "authors": ["Qiuchen Wang", "Ruixue Ding", "Zehui Chen", "Weiqi Wu", "Shihang Wang", "Pengjun Xie", "Feng Zhao"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.IR"], "comment": null, "summary": "Understanding information from visually rich documents remains a significant\nchallenge for traditional Retrieval-Augmented Generation (RAG) methods.\nExisting benchmarks predominantly focus on image-based question answering (QA),\noverlooking the fundamental challenges of efficient retrieval, comprehension,\nand reasoning within dense visual documents. To bridge this gap, we introduce\nViDoSeek, a novel dataset designed to evaluate RAG performance on visually rich\ndocuments requiring complex reasoning. Based on it, we identify key limitations\nin current RAG approaches: (i) purely visual retrieval methods struggle to\neffectively integrate both textual and visual features, and (ii) previous\napproaches often allocate insufficient reasoning tokens, limiting their\neffectiveness. To address these challenges, we propose ViDoRAG, a novel\nmulti-agent RAG framework tailored for complex reasoning across visual\ndocuments. ViDoRAG employs a Gaussian Mixture Model (GMM)-based hybrid strategy\nto effectively handle multi-modal retrieval. To further elicit the model's\nreasoning capabilities, we introduce an iterative agent workflow incorporating\nexploration, summarization, and reflection, providing a framework for\ninvestigating test-time scaling in RAG domains. Extensive experiments on\nViDoSeek validate the effectiveness and generalization of our approach.\nNotably, ViDoRAG outperforms existing methods by over 10% on the competitive\nViDoSeek benchmark. The code is available at\nhttps://github.com/Alibaba-NLP/ViDoRAG."}
{"id": "2503.10633", "pdf": "https://arxiv.org/pdf/2503.10633.pdf", "abs": "https://arxiv.org/abs/2503.10633", "title": "We Should Chart an Atlas of All the World's Models", "authors": ["Eliahu Horwitz", "Nitzan Kurer", "Jonathan Kahana", "Liel Amar", "Yedid Hoshen"], "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "Project page: https://horwitz.ai/model-atlas", "summary": "Public model repositories now contain millions of models, yet most models\nremain undocumented and effectively lost. In this position paper, we advocate\nfor charting the world's model population in a unified structure we call the\nModel Atlas: a graph that captures models, their attributes, and the weight\ntransformations that connect them. The Model Atlas enables applications in\nmodel forensics, meta-ML research, and model discovery, challenging tasks given\ntoday's unstructured model repositories. However, because most models lack\ndocumentation, large atlas regions remain uncharted. Addressing this gap\nmotivates new machine learning methods that treat models themselves as data,\ninferring properties such as functionality, performance, and lineage directly\nfrom their weights. We argue that a scalable path forward is to bypass the\nunique parameter symmetries that plague model weights. Charting all the world's\nmodels will require a community effort, and we hope its broad utility will\nrally researchers toward this goal."}
{"id": "2503.14615", "pdf": "https://arxiv.org/pdf/2503.14615.pdf", "abs": "https://arxiv.org/abs/2503.14615", "title": "Unique Hard Attention: A Tale of Two Sides", "authors": ["Selim Jerad", "Anej Svete", "Jiaoda Li", "Ryan Cotterell"], "categories": ["cs.LG", "cs.CC", "cs.CL", "cs.FL"], "comment": null, "summary": "Understanding the expressive power of transformers has recently attracted\nattention, as it offers insights into their abilities and limitations. Many\nstudies analyze unique hard attention transformers, where attention selects a\nsingle position that maximizes the attention scores. When multiple positions\nachieve the maximum score, either the rightmost or the leftmost of those is\nchosen. In this paper, we highlight the importance of this seeming triviality.\nRecently, finite-precision transformers with both leftmost- and rightmost-hard\nattention were shown to be equivalent to Linear Temporal Logic (LTL). We show\nthat this no longer holds with only leftmost-hard attention -- in that case,\nthey correspond to a \\emph{strictly weaker} fragment of LTL. Furthermore, we\nshow that models with leftmost-hard attention are equivalent to \\emph{soft}\nattention, suggesting they may better approximate real-world transformers than\nright-attention models. These findings refine the landscape of transformer\nexpressivity and underscore the role of attention directionality."}
{"id": "2504.13932", "pdf": "https://arxiv.org/pdf/2504.13932.pdf", "abs": "https://arxiv.org/abs/2504.13932", "title": "Enhancing Ultra-Low-Bit Quantization of Large Language Models Through Saliency-Aware Partial Retraining", "authors": ["Deyu Cao", "Samin Aref"], "categories": ["cs.LG", "cs.CL", "68T50, 68T07, 68T09, 68U15", "I.2.7; I.2.6; I.2.4"], "comment": "This is a post-peer-review accepted manuscript from the proceedings\n  of the 22nd International Conference on Modeling Decisions for Artificial\n  Intelligence (MDAI'25). The publisher authenticated version and full citation\n  details are available on Springer's website. 31 pages, 4 figures, 16 tables", "summary": "The growing use of large language models has raised environmental and\neconomic concerns about their intensity of resource usage during inference.\nServing these models to each user requires substantial energy and water for\ncooling. Model compression techniques like quantization can shrink large\nlanguage models and make them more resource efficient at the cost of potential\nperformance degradation. Quantization methods compress model size through\nreplacing their high-precision parameters by quantized values of lower\nprecision. Among existing methods, the ApiQ method achieves superior accuracy\npreservation at minimal memory and time overhead. We investigate two ideas to\nextend performance in ultra-low-bit quantization beyond ApiQ's level. First, we\nlook into combining existing quantization-aware training techniques with ApiQ's\npartial training. We show that this does not outperform the baseline ApiQ\nmethod with limited training data and frozen weights. This leads to two key\ninsights: (1) The substantial representational capacity that is gained through\nfull retraining is unlikely to be feasible through partial training. (2) This\ngain may depend on using a large and diverse dataset in quantization-aware\ntraining. Second, through a novel approach informed by the two insights, we\npropose an ultra-low-bit quantization method that builds upon ApiQ and extends\nits performance without the need for full retraining. This publicly available\nmethod relies on a saliency-aware regularization term that prioritizes\npreserving the most impactful parameters during quantization. Our experiments\non LLaMA 7B and 13B benchmarks demonstrate that our method reduces the ApiQ's\naccuracy degradation by 10.85\\% and 7.54\\% respectively."}
{"id": "2505.03414", "pdf": "https://arxiv.org/pdf/2505.03414.pdf", "abs": "https://arxiv.org/abs/2505.03414", "title": "Enhancing Target-unspecific Tasks through a Features Matrix", "authors": ["Fangming Cui", "Yonggang Zhang", "Xuan Wang", "Xinmei Tian", "Jun Yu"], "categories": ["cs.CV", "cs.CL"], "comment": "Accepted by ICML 2025", "summary": "Recent developments in prompt learning of large Vision-Language Models (VLMs)\nhave significantly improved performance in target-specific tasks. However,\nthese prompting methods often struggle to tackle the target-unspecific or\ngeneralizable tasks effectively. It may be attributed to the fact that\noverfitting training causes the model to forget its general knowledge. The\ngeneral knowledge has a strong promotion on target-unspecific tasks. To\nalleviate this issue, we propose a novel Features Matrix (FM) approach designed\nto enhance these models on target-unspecific tasks. Our method extracts and\nleverages general knowledge, shaping a Features Matrix (FM). Specifically, the\nFM captures the semantics of diverse inputs from a deep and fine perspective,\npreserving essential general knowledge, which mitigates the risk of\noverfitting. Representative evaluations demonstrate that: 1) the FM is\ncompatible with existing frameworks as a generic and flexible module, and 2)\nthe FM significantly showcases its effectiveness in enhancing target-unspecific\ntasks (base-to-novel generalization, domain generalization, and cross-dataset\ngeneralization), achieving state-of-the-art performance."}
{"id": "2505.05098", "pdf": "https://arxiv.org/pdf/2505.05098.pdf", "abs": "https://arxiv.org/abs/2505.05098", "title": "X-Driver: Explainable Autonomous Driving with Vision-Language Models", "authors": ["Wei Liu", "Jiyuan Zhang", "Binxiong Zheng", "Yufeng Hu", "Yingzhan Lin", "Zengfeng Zeng"], "categories": ["cs.RO", "cs.CL", "cs.CV", "cs.ET"], "comment": null, "summary": "End-to-end autonomous driving has advanced significantly, offering benefits\nsuch as system simplicity and stronger driving performance in both open-loop\nand closed-loop settings than conventional pipelines. However, existing\nframeworks still suffer from low success rates in closed-loop evaluations,\nhighlighting their limitations in real-world deployment. In this paper, we\nintroduce X-Driver, a unified multi-modal large language models(MLLMs)\nframework designed for closed-loop autonomous driving, leveraging\nChain-of-Thought(CoT) and autoregressive modeling to enhance perception and\ndecision-making. We validate X-Driver across multiple autonomous driving tasks\nusing public benchmarks in CARLA simulation environment, including\nBench2Drive[6]. Our experimental results demonstrate superior closed-loop\nperformance, surpassing the current state-of-the-art(SOTA) while improving the\ninterpretability of driving decisions. These findings underscore the importance\nof structured reasoning in end-to-end driving and establish X-Driver as a\nstrong baseline for future research in closed-loop autonomous driving."}
{"id": "2505.13887", "pdf": "https://arxiv.org/pdf/2505.13887.pdf", "abs": "https://arxiv.org/abs/2505.13887", "title": "Mobile-Agent-V: A Video-Guided Approach for Effortless and Efficient Operational Knowledge Injection in Mobile Automation", "authors": ["Junyang Wang", "Haiyang Xu", "Xi Zhang", "Ming Yan", "Ji Zhang", "Fei Huang", "Jitao Sang"], "categories": ["cs.AI", "cs.CL"], "comment": "I submitted the replacement version as a new article by mistake.\n  Future updates will appear at arXiv:2502.17110", "summary": "The exponential rise in mobile device usage necessitates streamlined\nautomation for effective task management, yet many AI frameworks fall short due\nto inadequate operational expertise. While manually written knowledge can\nbridge this gap, it is often burdensome and inefficient. We introduce\nMobile-Agent-V, an innovative framework that utilizes video as a guiding tool\nto effortlessly and efficiently inject operational knowledge into mobile\nautomation processes. By deriving knowledge directly from video content,\nMobile-Agent-V eliminates manual intervention, significantly reducing the\neffort and time required for knowledge acquisition. To rigorously evaluate this\napproach, we propose Mobile-Knowledge, a benchmark tailored to assess the\nimpact of external knowledge on mobile agent performance. Our experimental\nfindings demonstrate that Mobile-Agent-V enhances performance by 36% compared\nto existing methods, underscoring its effortless and efficient advantages in\nmobile automation."}
{"id": "2505.16932", "pdf": "https://arxiv.org/pdf/2505.16932.pdf", "abs": "https://arxiv.org/abs/2505.16932", "title": "The Polar Express: Optimal Matrix Sign Methods and Their Application to the Muon Algorithm", "authors": ["Noah Amsel", "David Persson", "Christopher Musco", "Robert M. Gower"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NA", "math.NA", "math.OC", "65F30, 68T07, 68N19", "G.1.3; I.2.6; F.2.1; G.1.6"], "comment": "34 pages, 8 figures, 4 algorithms", "summary": "Computing the polar decomposition and the related matrix sign function, has\nbeen a well-studied problem in numerical analysis for decades. More recently,\nit has emerged as an important subroutine in deep learning, particularly within\nthe Muon optimization framework. However, the requirements in this setting\ndiffer significantly from those of traditional numerical analysis. In deep\nlearning, methods must be highly efficient and GPU-compatible, but high\naccuracy is often unnecessary. As a result, classical algorithms like\nNewton-Schulz (which suffers from slow initial convergence) and methods based\non rational functions (which rely on QR decompositions or matrix inverses) are\npoorly suited to this context. In this work, we introduce Polar Express, a\nGPU-friendly algorithm for computing the polar decomposition. Like classical\npolynomial methods such as Newton-Schulz, our approach uses only matrix-matrix\nmultiplications, making it GPU-compatible. Motivated by earlier work of Chen &\nChow and Nakatsukasa & Freund, Polar Express adapts the polynomial update rule\nat each iteration by solving a minimax optimization problem, and we prove that\nit enjoys a strong worst-case optimality guarantee. This property ensures both\nrapid early convergence and fast asymptotic convergence. We also address\nfinite-precision issues, making it stable in bfloat16 in practice. We apply\nPolar Express within the Muon optimization framework and show consistent\nimprovements in validation loss on large-scale models such as GPT-2,\noutperforming recent alternatives across a range of learning rates."}
{"id": "2505.21863", "pdf": "https://arxiv.org/pdf/2505.21863.pdf", "abs": "https://arxiv.org/abs/2505.21863", "title": "GETReason: Enhancing Image Context Extraction through Hierarchical Multi-Agent Reasoning", "authors": ["Shikhhar Siingh", "Abhinav Rawat", "Chitta Baral", "Vivek Gupta"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Publicly significant images from events hold valuable contextual information,\ncrucial for journalism and education. However, existing methods often struggle\nto extract this relevance accurately. To address this, we introduce GETReason\n(Geospatial Event Temporal Reasoning), a framework that moves beyond\nsurface-level image descriptions to infer deeper contextual meaning. We propose\nthat extracting global event, temporal, and geospatial information enhances\nunderstanding of an image's significance. Additionally, we introduce GREAT\n(Geospatial Reasoning and Event Accuracy with Temporal Alignment), a new metric\nfor evaluating reasoning-based image understanding. Our layered multi-agent\napproach, assessed using a reasoning-weighted metric, demonstrates that\nmeaningful insights can be inferred, effectively linking images to their\nbroader event context."}
{"id": "2505.24200", "pdf": "https://arxiv.org/pdf/2505.24200.pdf", "abs": "https://arxiv.org/abs/2505.24200", "title": "Improving Multilingual Speech Models on ML-SUPERB 2.0: Fine-tuning with Data Augmentation and LID-Aware CTC", "authors": ["Qingzheng Wang", "Jiancheng Sun", "Yifan Peng", "Shinji Watanabe"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "Multilingual speech processing with self-supervised or supervised pre-trained\nSpeech Foundation Models (SFM) has achieved strong performance on tasks like\nLanguage Identification (LID) and Automatic Speech Recognition (ASR). However,\nthese models struggle with limited resources during fine-tuning. This paper\nenhances multilingual LID and ASR on ML-SUPERB 2.0 by exploring multiple\nstrategies for adapting SFMs, including frozen upstream training, partial\nfine-tuning, and low-rank adaptation. Furthermore, we employ data augmentation\nto mitigate performance gaps in few-shot settings and introduce LID\nConnectionist Temporal Classification (CTC) loss for regularization. Our\napproach achieves a 14% relative improvement in LID accuracy and a 30% relative\nreduction in ASR CER over the baseline on ML-SUPERB 2.0, securing second place\nin the Interspeech 2025 ML-SUPERB 2.0 Challenge."}
{"id": "2506.00095", "pdf": "https://arxiv.org/pdf/2506.00095.pdf", "abs": "https://arxiv.org/abs/2506.00095", "title": "ClinBench-HPB: A Clinical Benchmark for Evaluating LLMs in Hepato-Pancreato-Biliary Diseases", "authors": ["Yuchong Li", "Xiaojun Zeng", "Chihua Fang", "Jian Yang", "Fucang Jia", "Lei Zhang"], "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": null, "summary": "Hepato-pancreato-biliary (HPB) disorders represent a global public health\nchallenge due to their high morbidity and mortality. Although large language\nmodels (LLMs) have shown promising performance in general medical\nquestion-answering tasks, the current evaluation benchmarks are mostly derived\nfrom standardized examinations or manually designed questions, lacking HPB\ncoverage and clinical cases. To address these issues, we systematically\neatablish an HPB disease evaluation benchmark comprising 3,535 closed-ended\nmultiple-choice questions and 337 open-ended real diagnosis cases, which\nencompasses all the 33 main categories and 465 subcategories of HPB diseases\ndefined in the International Statistical Classification of Diseases, 10th\nRevision (ICD-10). The multiple-choice questions are curated from public\ndatasets and synthesized data, and the clinical cases are collected from\nprestigious medical journals, case-sharing platforms, and collaborating\nhospitals. By evalauting commercial and open-source general and medical LLMs on\nour established benchmark, namely ClinBench-HBP, we find that while commercial\nLLMs perform competently on medical exam questions, they exhibit substantial\nperformance degradation on HPB diagnosis tasks, especially on complex,\ninpatient clinical cases. Those medical LLMs also show limited generalizability\nto HPB diseases. Our results reveal the critical limitations of current LLMs in\nthe domain of HPB diseases, underscoring the imperative need for future medical\nLLMs to handle real, complex clinical diagnostics rather than simple medical\nexam questions. The benchmark will be released at the homepage."}
{"id": "2506.00261", "pdf": "https://arxiv.org/pdf/2506.00261.pdf", "abs": "https://arxiv.org/abs/2506.00261", "title": "GPR: Empowering Generation with Graph-Pretrained Retriever", "authors": ["Xiaochen Wang", "Zongyu Wu", "Yuan Zhong", "Xiang Zhang", "Suhang Wang", "Fenglong Ma"], "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Graph retrieval-augmented generation (GRAG) places high demands on\ngraph-specific retrievers. However, existing retrievers often rely on language\nmodels pretrained on plain text, limiting their effectiveness due to domain\nmisalignment and structure ignorance. To address these challenges, we propose\nGPR, a graph-based retriever pretrained directly on knowledge graphs. GPR\naligns natural language questions with relevant subgraphs through LLM-guided\ngraph augmentation and employs a structure-aware objective to learn\nfine-grained retrieval strategies. Experiments on two datasets, three LLM\nbackbones, and five baselines show that GPR consistently improves both\nretrieval quality and downstream generation, demonstrating its effectiveness as\na robust retrieval solution for GRAG."}
{"id": "2506.00653", "pdf": "https://arxiv.org/pdf/2506.00653.pdf", "abs": "https://arxiv.org/abs/2506.00653", "title": "Linear Representation Transferability Hypothesis: Leveraging Small Models to Steer Large Models", "authors": ["Femi Bello", "Anubrata Das", "Fanzhi Zeng", "Fangcong Yin", "Liu Leqi"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "It has been hypothesized that neural networks with similar architectures\ntrained on similar data learn shared representations relevant to the learning\ntask. We build on this idea by extending the conceptual framework where\nrepresentations learned across models trained on the same data can be expressed\nas linear combinations of a \\emph{universal} set of basis features. These basis\nfeatures underlie the learning task itself and remain consistent across models,\nregardless of scale. From this framework, we propose the \\textbf{Linear\nRepresentation Transferability (LRT)} Hypothesis -- that there exists an affine\ntransformation between the representation spaces of different models. To test\nthis hypothesis, we learn affine mappings between the hidden states of models\nof different sizes and evaluate whether steering vectors -- directions in\nhidden state space associated with specific model behaviors -- retain their\nsemantic effect when transferred from small to large language models using the\nlearned mappings. We find strong empirical evidence that such affine mappings\ncan preserve steering behaviors. These findings suggest that representations\nlearned by small models can be used to guide the behavior of large models, and\nthat the LRT hypothesis may be a promising direction on understanding\nrepresentation alignment across model scales."}
{"id": "2506.01789", "pdf": "https://arxiv.org/pdf/2506.01789.pdf", "abs": "https://arxiv.org/abs/2506.01789", "title": "Datasheets Aren't Enough: DataRubrics for Automated Quality Metrics and Accountability", "authors": ["Genta Indra Winata", "David Anugraha", "Emmy Liu", "Alham Fikri Aji", "Shou-Yi Hung", "Aditya Parashar", "Patrick Amadeus Irawan", "Ruochen Zhang", "Zheng-Xin Yong", "Jan Christian Blaise Cruz", "Niklas Muennighoff", "Seungone Kim", "Hanyang Zhao", "Sudipta Kar", "Kezia Erina Suryoraharjo", "M. Farid Adilazuarda", "En-Shiun Annie Lee", "Ayu Purwarianti", "Derry Tanti Wijaya", "Monojit Choudhury"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "eess.AS"], "comment": "Preprint", "summary": "High-quality datasets are fundamental to training and evaluating machine\nlearning models, yet their creation-especially with accurate human\nannotations-remains a significant challenge. Many dataset paper submissions\nlack originality, diversity, or rigorous quality control, and these\nshortcomings are often overlooked during peer review. Submissions also\nfrequently omit essential details about dataset construction and properties.\nWhile existing tools such as datasheets aim to promote transparency, they are\nlargely descriptive and do not provide standardized, measurable methods for\nevaluating data quality. Similarly, metadata requirements at conferences\npromote accountability but are inconsistently enforced. To address these\nlimitations, this position paper advocates for the integration of systematic,\nrubric-based evaluation metrics into the dataset review process-particularly as\nsubmission volumes continue to grow. We also explore scalable, cost-effective\nmethods for synthetic data generation, including dedicated tools and\nLLM-as-a-judge approaches, to support more efficient evaluation. As a call to\naction, we introduce DataRubrics, a structured framework for assessing the\nquality of both human- and model-generated datasets. Leveraging recent advances\nin LLM-based evaluation, DataRubrics offers a reproducible, scalable, and\nactionable solution for dataset quality assessment, enabling both authors and\nreviewers to uphold higher standards in data-centric research. We also release\ncode to support reproducibility of LLM-based evaluations at\nhttps://github.com/datarubrics/datarubrics."}
