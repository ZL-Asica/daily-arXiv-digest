{"id": "2506.18919", "pdf": "https://arxiv.org/pdf/2506.18919.pdf", "abs": "https://arxiv.org/abs/2506.18919", "title": "MemeMind: A Large-Scale Multimodal Dataset with Chain-of-Thought Reasoning for Harmful Meme Detection", "authors": ["Hexiang Gu", "Qifan Yu", "Saihui Hou", "Zhiqin Fang", "Huijia Wu", "Zhaofeng He"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "The rapid development of social media has intensified the spread of harmful\ncontent. Harmful memes, which integrate both images and text, pose significant\nchallenges for automated detection due to their implicit semantics and complex\nmultimodal interactions. Although existing research has made progress in\ndetection accuracy and interpretability, the lack of a systematic, large-scale,\ndiverse, and highly explainable dataset continues to hinder further advancement\nin this field. To address this gap, we introduce MemeMind, a novel dataset\nfeaturing scientifically rigorous standards, large scale, diversity, bilingual\nsupport (Chinese and English), and detailed Chain-of-Thought (CoT) annotations.\nMemeMind fills critical gaps in current datasets by offering comprehensive\nlabeling and explicit reasoning traces, thereby providing a solid foundation\nfor enhancing harmful meme detection. In addition, we propose an innovative\ndetection framework, MemeGuard, which effectively integrates multimodal\ninformation with reasoning process modeling, significantly improving models'\nability to understand and identify harmful memes. Extensive experiments\nconducted on the MemeMind dataset demonstrate that MemeGuard consistently\noutperforms existing state-of-the-art methods in harmful meme detection tasks."}
{"id": "2506.18998", "pdf": "https://arxiv.org/pdf/2506.18998.pdf", "abs": "https://arxiv.org/abs/2506.18998", "title": "Mirage of Mastery: Memorization Tricks LLMs into Artificially Inflated Self-Knowledge", "authors": ["Sahil Kale", "Vijaykant Nadadur"], "categories": ["cs.CL"], "comment": "Accepted to the Pre-ACL Workshop 2025, Copenhagen", "summary": "When artificial intelligence mistakes memorization for intelligence, it\ncreates a dangerous mirage of reasoning. Existing studies treat memorization\nand self-knowledge deficits in LLMs as separate issues and do not recognize an\nintertwining link that degrades the trustworthiness of LLM responses. In our\nstudy, we utilize a novel framework to ascertain if LLMs genuinely learn\nreasoning patterns from training data or merely memorize them to assume\ncompetence across problems of similar complexity focused on STEM domains. Our\nanalysis shows a noteworthy problem in generalization: LLMs draw confidence\nfrom memorized solutions to infer a higher self-knowledge about their reasoning\nability, which manifests as an over 45% inconsistency in feasibility\nassessments when faced with self-validated, logically coherent task\nperturbations. This effect is most pronounced in science and medicine domains,\nwhich tend to have maximal standardized jargon and problems, further confirming\nour approach. Significant wavering within the self-knowledge of LLMs also shows\nflaws in current architectures and training patterns, highlighting the need for\ntechniques that ensure a balanced, consistent stance on models' perceptions of\ntheir own knowledge for maximum AI explainability and trustworthiness. Our code\nand results are available publicly at\nhttps://github.com/knowledge-verse-ai/LLM-Memorization_SK_Eval-."}
{"id": "2506.19004", "pdf": "https://arxiv.org/pdf/2506.19004.pdf", "abs": "https://arxiv.org/abs/2506.19004", "title": "Broken Tokens? Your Language Model can Secretly Handle Non-Canonical Tokenizations", "authors": ["Brian Siyuan Zheng", "Alisa Liu", "Orevaoghene Ahia", "Jonathan Hayase", "Yejin Choi", "Noah A. Smith"], "categories": ["cs.CL"], "comment": "preprint", "summary": "Modern tokenizers employ deterministic algorithms to map text into a single\n\"canonical\" token sequence, yet the same string can be encoded as many\nnon-canonical tokenizations using the tokenizer vocabulary. In this work, we\ninvestigate the robustness of LMs to text encoded with non-canonical\ntokenizations entirely unseen during training. Surprisingly, when evaluated\nacross 20 benchmarks, we find that instruction-tuned models retain up to 93.4%\nof their original performance when given a randomly sampled tokenization, and\n90.8% with character-level tokenization. We see that overall stronger models\ntend to be more robust, and robustness diminishes as the tokenization departs\nfarther from the canonical form. Motivated by these results, we then identify\nsettings where non-canonical tokenization schemes can *improve* performance,\nfinding that character-level segmentation improves string manipulation and code\nunderstanding tasks by up to +14%, and right-aligned digit grouping enhances\nlarge-number arithmetic by +33%. Finally, we investigate the source of this\nrobustness, finding that it arises in the instruction-tuning phase. We show\nthat while both base and post-trained models grasp the semantics of\nnon-canonical tokenizations (perceiving them as containing misspellings), base\nmodels try to mimic the imagined mistakes and degenerate into nonsensical\noutput, while post-trained models are committed to fluent responses. Overall,\nour findings suggest that models are less tied to their tokenizer than\npreviously believed, and demonstrate the promise of intervening on tokenization\nat inference time to boost performance."}
{"id": "2506.19028", "pdf": "https://arxiv.org/pdf/2506.19028.pdf", "abs": "https://arxiv.org/abs/2506.19028", "title": "Quantifying Fairness in LLMs Beyond Tokens: A Semantic and Statistical Perspective", "authors": ["Weijie Xu", "Yiwen Wang", "Chi Xue", "Xiangkun Hu", "Xi Fang", "Guimin Dong", "Chandan K. Reddy"], "categories": ["cs.CL", "cs.AI", "cs.CY", "68T50", "I.2.7"], "comment": "29 pages, 9 figures, 15 tables", "summary": "Large Language Models (LLMs) often generate responses with inherent biases,\nundermining their reliability in real-world applications. Existing evaluation\nmethods often overlook biases in long-form responses and the intrinsic\nvariability of LLM outputs. To address these challenges, we propose\nFiSCo(Fine-grained Semantic Computation), a novel statistical framework to\nevaluate group-level fairness in LLMs by detecting subtle semantic differences\nin long-form responses across demographic groups. Unlike prior work focusing on\nsentiment or token-level comparisons, FiSCo goes beyond surface-level analysis\nby operating at the claim level, leveraging entailment checks to assess the\nconsistency of meaning across responses. We decompose model outputs into\nsemantically distinct claims and apply statistical hypothesis testing to\ncompare inter- and intra-group similarities, enabling robust detection of\nsubtle biases. We formalize a new group counterfactual fairness definition and\nvalidate FiSCo on both synthetic and human-annotated datasets spanning gender,\nrace, and age. Experiments show that FiSco more reliably identifies nuanced\nbiases while reducing the impact of stochastic LLM variability, outperforming\nvarious evaluation metrics."}
{"id": "2506.18962", "pdf": "https://arxiv.org/pdf/2506.18962.pdf", "abs": "https://arxiv.org/abs/2506.18962", "title": "UniMind: Unleashing the Power of LLMs for Unified Multi-Task Brain Decoding", "authors": ["Weiheng Lu", "Chunfeng Song", "Jiamin Wu", "Pengyu Zhu", "Yuchen Zhou", "Weijian Mai", "Qihao Zheng", "Wanli Ouyang"], "categories": ["cs.HC"], "comment": "19pages,4 figures", "summary": "Decoding human brain activity from electroencephalography (EEG) signals is a\ncentral challenge at the intersection of neuroscience and artificial\nintelligence, enabling diverse applications in mental state assessment,\nclinical monitoring, and human-machine interaction. Recent efforts have\nextensively explored EEG-based brain foundation models for generalized brain\ndecoding, employing large-scale training on multiple datasets. However, most of\nthese attempts struggle with generalizability and fail to achieve satisfactory\nperformance without task-specific tuning due to pronounced inherent\nheterogeneity among decoding tasks. To address these challenges, we present\nUniMind, a general-purpose EEG foundation model for unified multi-task brain\ndecoding by uniquely unleashing the power of large language models to\ncomprehend complex neural patterns. UniMind offers several advantages. First,\nwe design a Neuro-Language Connector to bridge the modality gap between neural\nsignals and large language models, distilling and transforming the\nspatiotemporal neural patterns of EEG data into representations understandable\nby language models. Second, a Task-aware Query Selection module is proposed to\ninject task-awareness into the cross-modal alignment by dynamically generating\ntask-adaptive query tokens, enabling learning of task-relevant neural patterns\nacross diverse tasks. Extensive experiments across ten datasets demonstrate\nthat UniMind substantially outperforms state-of-the-art multi-task decoding\nmodels, with an average gain of 12 percent, while also offering valuable\nneuroscientific insights into neural functional correlations across tasks. The\ncode will be made publicly available."}
{"id": "2506.19037", "pdf": "https://arxiv.org/pdf/2506.19037.pdf", "abs": "https://arxiv.org/abs/2506.19037", "title": "Plan for Speed -- Dilated Scheduling for Masked Diffusion Language Models", "authors": ["Omer Luxembourg", "Haim Permuter", "Eliya Nachmani"], "categories": ["cs.CL", "cs.AI", "cs.IT", "cs.LG", "cs.NE", "math.IT"], "comment": null, "summary": "Masked diffusion language models (MDLM) have shown strong promise for\nnon-autoregressive text generation, yet existing samplers act as implicit\nplanners, selecting tokens to unmask via denoiser confidence or entropy scores.\nSuch heuristics falter under parallel unmasking - they ignore pairwise\ninteractions between tokens and cannot account for dependencies when unmasking\nmultiple positions at once, limiting their inference time to traditional\nauto-regressive (AR) models. We introduce the Dilated-scheduled Unmasking\nStrategy (DUS), an inference-only, planner-model-free method that requires no\nadditional training. DUS leverages a first-order Markov assumption to partition\nsequence positions into dilation-based groups of non-adjacent tokens, enabling\nindependent, parallel unmasking steps that respect local context that minimizes\nthe joint entropy of each iteration step. Unlike semi-AR block approaches\n(e.g., LLADA and Dream) that still invoke the denoiser per block, DUS reduces\nthe number of denoiser calls to O(log B) per generation block - yielding\nsubstantial speedup over the O(B) run time of state-of-the-art diffusion\nmodels, where B is the block size in the semi-AR inference process. In\nexperiments on math (GSM8K) and code completion (Humaneval, MBPP) benchmarks -\ndomains suited to non-ordinal generation - DUS improves scores over parallel\nconfidence-based planner, without modifying the underlying denoiser. DUS offers\na lightweight, budget-aware approach to efficient, high-quality text\ngeneration, paving the way to unlock the true capabilities of MDLMs."}
{"id": "2506.19017", "pdf": "https://arxiv.org/pdf/2506.19017.pdf", "abs": "https://arxiv.org/abs/2506.19017", "title": "Raise Awareness of the Environmental Impacts of Retail Food Products: A User-Centered Scenario-Based Approach", "authors": ["Lorenzo Porcelli", "Francesco Palmieri"], "categories": ["cs.HC"], "comment": null, "summary": "The climate is warming rapidly, and atmospheric concentrations of greenhouse\ngases (GHGs) are at their highest levels ever recorded. As a result of these\nclimate changes, caused mainly by human activities, disasters have increased\nfivefold over the past 50 years, causing death and economic loss. Civic\nengagement and awareness are essential to mitigate climate change and its\nimpacts. In this work, we proposed a user interface that makes users aware of\nthe environmental impact of the food products they buy when shopping. A\nuser-centered scenario-based design was followed in the development of the\ninterface. Gamification elements were added to increase civic participation in\nclimate action."}
{"id": "2506.19058", "pdf": "https://arxiv.org/pdf/2506.19058.pdf", "abs": "https://arxiv.org/abs/2506.19058", "title": "NLPnorth @ TalentCLEF 2025: Comparing Discriminative, Contrastive, and Prompt-Based Methods for Job Title and Skill Matching", "authors": ["Mike Zhang", "Rob van der Goot"], "categories": ["cs.CL"], "comment": "TalentCLEF 2025", "summary": "Matching job titles is a highly relevant task in the computational job market\ndomain, as it improves e.g., automatic candidate matching, career path\nprediction, and job market analysis. Furthermore, aligning job titles to job\nskills can be considered an extension to this task, with similar relevance for\nthe same downstream tasks. In this report, we outline NLPnorth's submission to\nTalentCLEF 2025, which includes both of these tasks: Multilingual Job Title\nMatching, and Job Title-Based Skill Prediction. For both tasks we compare\n(fine-tuned) classification-based, (fine-tuned) contrastive-based, and\nprompting methods. We observe that for Task A, our prompting approach performs\nbest with an average of 0.492 mean average precision (MAP) on test data,\naveraged over English, Spanish, and German. For Task B, we obtain an MAP of\n0.290 on test data with our fine-tuned classification-based approach.\nAdditionally, we made use of extra data by pulling all the language-specific\ntitles and corresponding \\emph{descriptions} from ESCO for each job and skill.\nOverall, we find that the largest multilingual language models perform best for\nboth tasks. Per the provisional results and only counting the unique teams, the\nranking on Task A is 5$^{\\text{th}}$/20 and for Task B 3$^{\\text{rd}}$/14."}
{"id": "2506.19107", "pdf": "https://arxiv.org/pdf/2506.19107.pdf", "abs": "https://arxiv.org/abs/2506.19107", "title": "Improving Student-AI Interaction Through Pedagogical Prompting: An Example in Computer Science Education", "authors": ["Ruiwei Xiao", "Xinying Hou", "Runlong Ye", "Majeed Kazemitabaar", "Nicholas Diana", "Michael Liut", "John Stamper"], "categories": ["cs.HC", "cs.AI"], "comment": "Under review for Computer & Education: Artificial Intelligence.\n  Journal policy allows submitting as preprint", "summary": "With the proliferation of large language model (LLM) applications since 2022,\ntheir use in education has sparked both excitement and concern. Recent studies\nconsistently highlight students' (mis)use of LLMs can hinder learning outcomes.\nThis work aims to teach students how to effectively prompt LLMs to improve\ntheir learning. We first proposed pedagogical prompting, a\ntheoretically-grounded new concept to elicit learning-oriented responses from\nLLMs. To move from concept design to a proof-of-concept learning intervention\nin real educational settings, we selected early undergraduate CS education\n(CS1/CS2) as the example context. We began with a formative survey study with\ninstructors (N=36) teaching early-stage undergraduate-level CS courses to\ninform the instructional design based on classroom needs. Based on their\ninsights, we designed and developed a learning intervention through an\ninteractive system with scenario-based instruction to train pedagogical\nprompting skills. Finally, we evaluated its instructional effectiveness through\na user study with CS novice students (N=22) using pre/post-tests. Through mixed\nmethods analyses, our results indicate significant improvements in learners'\nLLM-based pedagogical help-seeking skills, along with positive attitudes toward\nthe system and increased willingness to use pedagogical prompts in the future.\nOur contributions include (1) a theoretical framework of pedagogical prompting;\n(2) empirical insights into current instructor attitudes toward pedagogical\nprompting; and (3) a learning intervention design with an interactive learning\ntool and scenario-based instruction leading to promising results on teaching\nLLM-based help-seeking. Our approach is scalable for broader implementation in\nclassrooms and has the potential to be integrated into tools like ChatGPT as an\non-boarding experience to encourage learning-oriented use of generative AI."}
{"id": "2506.19073", "pdf": "https://arxiv.org/pdf/2506.19073.pdf", "abs": "https://arxiv.org/abs/2506.19073", "title": "MFTCXplain: A Multilingual Benchmark Dataset for Evaluating the Moral Reasoning of LLMs through Hate Speech Multi-hop Explanation", "authors": ["Jackson Trager", "Francielle Vargas", "Diego Alves", "Matteo Guida", "Mikel K. Ngueajio", "Ameeta Agrawal", "Flor Plaza-del-Arco", "Yalda Daryanai", "Farzan Karimi-Malekabadi"], "categories": ["cs.CL"], "comment": "Under Review", "summary": "Ensuring the moral reasoning capabilities of Large Language Models (LLMs) is\na growing concern as these systems are used in socially sensitive tasks.\nNevertheless, current evaluation benchmarks present two major shortcomings: a\nlack of annotations that justify moral classifications, which limits\ntransparency and interpretability; and a predominant focus on English, which\nconstrains the assessment of moral reasoning across diverse cultural settings.\nIn this paper, we introduce MFTCXplain, a multilingual benchmark dataset for\nevaluating the moral reasoning of LLMs via hate speech multi-hop explanation\nusing Moral Foundation Theory (MFT). The dataset comprises 3,000 tweets across\nPortuguese, Italian, Persian, and English, annotated with binary hate speech\nlabels, moral categories, and text span-level rationales. Empirical results\nhighlight a misalignment between LLM outputs and human annotations in moral\nreasoning tasks. While LLMs perform well in hate speech detection (F1 up to\n0.836), their ability to predict moral sentiments is notably weak (F1 < 0.35).\nFurthermore, rationale alignment remains limited mainly in underrepresented\nlanguages. These findings show the limited capacity of current LLMs to\ninternalize and reflect human moral reasoning."}
{"id": "2506.19210", "pdf": "https://arxiv.org/pdf/2506.19210.pdf", "abs": "https://arxiv.org/abs/2506.19210", "title": "Smart Glasses for CVI: Co-Designing Extended Reality Solutions to Support Environmental Perception by People with Cerebral Visual Impairment", "authors": ["Bhanuka Gamage", "Nicola McDowell", "Dijana Kovacic", "Leona Holloway", "Thanh-Toan Do", "Nicholas Price", "Arthur Lowery", "Kim Marriott"], "categories": ["cs.HC"], "comment": "Author's conditionally accepted version of a paper to be published at\n  ACM SIGACCESS Conference on Computers and Accessibility (ASSETS '25)", "summary": "Cerebral Visual Impairment (CVI) is the set to be the leading cause of vision\nimpairment, yet remains underrepresented in assistive technology research.\nUnlike ocular conditions, CVI affects higher-order visual processing-impacting\nobject recognition, facial perception, and attention in complex environments.\nThis paper presents a co-design study with two adults with CVI investigating\nhow smart glasses, i.e. head-mounted extended reality displays, can support\nunderstanding and interaction with the immediate environment. Guided by the\nDouble Diamond design framework, we conducted a two-week diary study, two\nideation workshops, and ten iterative development sessions using the Apple\nVision Pro. Our findings demonstrate that smart glasses can meaningfully\naddress key challenges in locating objects, reading text, recognising people,\nengaging in conversations, and managing sensory stress. With the rapid\nadvancement of smart glasses and increasing recognition of CVI as a distinct\nform of vision impairment, this research addresses a timely and under-explored\nintersection of technology and need."}
{"id": "2506.19089", "pdf": "https://arxiv.org/pdf/2506.19089.pdf", "abs": "https://arxiv.org/abs/2506.19089", "title": "Language Models Might Not Understand You: Evaluating Theory of Mind via Story Prompting", "authors": ["Nathaniel Getachew", "Abulhair Saparov"], "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 11 figures", "summary": "We introduce $\\texttt{StorySim}$, a programmable framework for synthetically\ngenerating stories to evaluate the theory of mind (ToM) and world modeling (WM)\ncapabilities of large language models (LLMs). Unlike prior benchmarks that may\nsuffer from contamination in pretraining data, $\\texttt{StorySim}$ produces\nnovel, compositional story prompts anchored by a highly controllable\n$\\texttt{Storyboard}$, enabling precise manipulation of character perspectives\nand events. We use this framework to design first- and second-order ToM tasks\nalongside WM tasks that control for the ability to track and model mental\nstates. Our experiments across a suite of state-of-the-art LLMs reveal that\nmost models perform better on WM tasks than ToM tasks, and that models tend to\nperform better reasoning with humans compared to inanimate objects.\nAdditionally, our framework enabled us to find evidence of heuristic behavior\nsuch as recency bias and an over-reliance on earlier events in the story. All\ncode for generating data and evaluations is freely available."}
{"id": "2506.19268", "pdf": "https://arxiv.org/pdf/2506.19268.pdf", "abs": "https://arxiv.org/abs/2506.19268", "title": "HARPT: A Corpus for Analyzing Consumers' Trust and Privacy Concerns in Mobile Health Apps", "authors": ["Timoteo Kelly", "Abdulkadir Korkmaz", "Samuel Mallet", "Connor Souders", "Sadra Aliakbarpour", "Praveen Rao"], "categories": ["cs.HC", "cs.CR", "cs.ET", "cs.LG"], "comment": "Under review at The 34th ACM International Conference on Information\n  and Knowledge Management (CIKM'25)", "summary": "We present HARPT, a large-scale annotated corpus of mobile health app store\nreviews aimed at advancing research in user privacy and trust. The dataset\ncomprises over 480,000 user reviews labeled into seven categories that capture\ncritical aspects of trust in applications, trust in providers and privacy\nconcerns. Creating HARPT required addressing multiple complexities, such as\ndefining a nuanced label schema, isolating relevant content from large volumes\nof noisy data, and designing an annotation strategy that balanced scalability\nwith accuracy. This strategy integrated rule-based filtering, iterative manual\nlabeling with review, targeted data augmentation, and weak supervision using\ntransformer-based classifiers to accelerate coverage. In parallel, a carefully\ncurated subset of 7,000 reviews was manually annotated to support model\ndevelopment and evaluation. We benchmark a broad range of classification\nmodels, demonstrating that strong performance is achievable and providing a\nbaseline for future research. HARPT is released as a public resource to support\nwork in health informatics, cybersecurity, and natural language processing."}
{"id": "2506.19113", "pdf": "https://arxiv.org/pdf/2506.19113.pdf", "abs": "https://arxiv.org/abs/2506.19113", "title": "Human-Aligned Faithfulness in Toxicity Explanations of LLMs", "authors": ["Ramaravind K. Mothilal", "Joanna Roy", "Syed Ishtiaque Ahmed", "Shion Guha"], "categories": ["cs.CL"], "comment": "21 pages, 5 figures, 7 tables", "summary": "The discourse around toxicity and LLMs in NLP largely revolves around\ndetection tasks. This work shifts the focus to evaluating LLMs' reasoning about\ntoxicity -- from their explanations that justify a stance -- to enhance their\ntrustworthiness in downstream tasks. Despite extensive research on\nexplainability, it is not straightforward to adopt existing methods to evaluate\nfree-form toxicity explanation due to their over-reliance on input text\nperturbations, among other challenges. To account for these, we propose a\nnovel, theoretically-grounded multi-dimensional criterion, Human-Aligned\nFaithfulness (HAF), that measures the extent to which LLMs' free-form toxicity\nexplanations align with those of a rational human under ideal conditions. We\ndevelop six metrics, based on uncertainty quantification, to comprehensively\nevaluate \\haf of LLMs' toxicity explanations with no human involvement, and\nhighlight how \"non-ideal\" the explanations are. We conduct several experiments\non three Llama models (of size up to 70B) and an 8B Ministral model on five\ndiverse toxicity datasets. Our results show that while LLMs generate plausible\nexplanations to simple prompts, their reasoning about toxicity breaks down when\nprompted about the nuanced relations between the complete set of reasons, the\nindividual reasons, and their toxicity stances, resulting in inconsistent and\nnonsensical responses. We open-source our code and LLM-generated explanations\nat https://github.com/uofthcdslab/HAF."}
{"id": "2506.19307", "pdf": "https://arxiv.org/pdf/2506.19307.pdf", "abs": "https://arxiv.org/abs/2506.19307", "title": "OpticalAging: Real-time Presbyopia Simulation for Inclusive Design via Tunable Lenses", "authors": ["Qing Zhang", "Zixiong Su", "Yoshihito Kondoh", "Kazunori Asada", "Thad Starner", "Kai Kunze", "Yuta Itoh", "Jun Rekimoto"], "categories": ["cs.HC"], "comment": "Under Submission", "summary": "Presbyopia, a common age-related vision condition affecting most people as\nthey age, often remains inadequately understood by those unaffected. To help\nbridge the gap between abstract accessibility knowledge and a more grounded\nappreciation of perceptual challenges, this study presents OpticalAging, an\noptical see-through simulation approach. Unlike VR-based methods, OpticalAging\nuses dynamically controlled tunable lenses to simulate the first-person visual\nperspective of presbyopia's distance-dependent blur during real-world\ninteraction, aiming to enhance awareness. While acknowledging critiques\nregarding simulation's limitations in fully capturing lived experience, we\nposition this tool as a complement to user-centered methods. Our user study (N\n= 19, 18-35 years old) provides validation: quantitative measurements show\nstatistically significant changes in near points across three age modes (40s,\n50s, 60s), while qualitative results suggest increases in reported\nunderstanding and empathy among participants. The integration of our tool into\na design task showcases its potential applicability within age-inclusive design\nworkflows when used critically alongside direct user engagement."}
{"id": "2506.19159", "pdf": "https://arxiv.org/pdf/2506.19159.pdf", "abs": "https://arxiv.org/abs/2506.19159", "title": "Enhanced Hybrid Transducer and Attention Encoder Decoder with Text Data", "authors": ["Yun Tang", "Eesung Kim", "Vijendra Raj Apsingekar"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted by Interspeech2025", "summary": "A joint speech and text optimization method is proposed for hybrid transducer\nand attention-based encoder decoder (TAED) modeling to leverage large amounts\nof text corpus and enhance ASR accuracy. The joint TAED (J-TAED) is trained\nwith both speech and text input modalities together, while it only takes speech\ndata as input during inference. The trained model can unify the internal\nrepresentations from different modalities, and be further extended to\ntext-based domain adaptation. It can effectively alleviate data scarcity for\nmismatch domain tasks since no speech data is required. Our experiments show\nJ-TAED successfully integrates speech and linguistic information into one\nmodel, and reduce the WER by 5.8 ~12.8% on the Librispeech dataset. The model\nis also evaluated on two out-of-domain datasets: one is finance and another is\nnamed entity focused. The text-based domain adaptation brings 15.3% and 17.8%\nWER reduction on those two datasets respectively."}
{"id": "2506.19364", "pdf": "https://arxiv.org/pdf/2506.19364.pdf", "abs": "https://arxiv.org/abs/2506.19364", "title": "Can theory-driven learning analytics dashboard enhance human-AI collaboration in writing learning? Insights from an empirical experiment", "authors": ["Angxuan Chen", "Jingjing Lian", "Xinran Kuang", "Jiyou Jia"], "categories": ["cs.HC"], "comment": null, "summary": "The integration of Generative AI (GenAI) into education has raised concerns\nabout over-reliance and superficial learning, particularly in writing tasks in\nhigher education. This study explores whether a theory-driven learning\nanalytics dashboard (LAD) can enhance human-AI collaboration in the academic\nwriting task by improving writing knowledge gains, fostering self-regulated\nlearning (SRL) skills and building different human-AI dialogue characteristics.\nGrounded in Zimmerman's SRL framework, the LAD provided real-time feedback on\nlearners' goal-setting, writing processes and reflection, while monitoring the\nquality of learner-AI interactions. A quasi-experiment was conducted involving\n52 postgraduate students divided into an experimental group (EG) using the LAD\nto a control group (CG) without it in a human-AI collaborative writing task.\nPre- and post- knowledge tests, questionnaires measuring SRL and cognitive\nload, and students' dialogue data with GenAI were collected and analyzed.\nResults showed that the EG achieved significantly higher writing knowledge\ngains and improved SRL skills, particularly in self-efficacy and cognitive\nstrategies. However, the EG also reported increased test anxiety and cognitive\nload, possibly due to heightened metacognitive awareness. Epistemic Network\nAnalysis revealed that the EG engaged in more reflective, evaluative\ninteractions with GenAI, while the CG focused on more transactional and\ninformation-seeking exchanges. These findings contribute to the growing body of\nliterature on the educational use of GenAI and highlight the importance of\ndesigning interventions that complement GenAI tools, ensuring that technology\nenhances rather than undermines the learning process."}
{"id": "2506.19187", "pdf": "https://arxiv.org/pdf/2506.19187.pdf", "abs": "https://arxiv.org/abs/2506.19187", "title": "Prompt, Translate, Fine-Tune, Re-Initialize, or Instruction-Tune? Adapting LLMs for In-Context Learning in Low-Resource Languages", "authors": ["Christopher Toukmaji", "Jeffrey Flanigan"], "categories": ["cs.CL"], "comment": "Accepted to ACL GEM 2025", "summary": "LLMs are typically trained in high-resource languages, and tasks in\nlower-resourced languages tend to underperform the higher-resource language\ncounterparts for in-context learning. Despite the large body of work on\nprompting settings, it is still unclear how LLMs should be adapted\ncross-lingually specifically for in-context learning in the low-resource target\nlanguages. We perform a comprehensive study spanning five diverse target\nlanguages, three base LLMs, and seven downstream tasks spanning over 4,100 GPU\ntraining hours (9,900+ TFLOPs) across various adaptation techniques: few-shot\nprompting, translate-test, fine-tuning, embedding re-initialization, and\ninstruction fine-tuning. Our results show that the few-shot prompting and\ntranslate-test settings tend to heavily outperform the gradient-based\nadaptation methods. To better understand this discrepancy, we design a novel\nmetric, Valid Output Recall (VOR), and analyze model outputs to empirically\nattribute the degradation of these trained models to catastrophic forgetting.\nTo the extent of our knowledge, this is the largest study done on in-context\nlearning for low-resource languages with respect to train compute and number of\nadaptation techniques considered. We make all our datasets and trained models\navailable for public use."}
{"id": "2506.19430", "pdf": "https://arxiv.org/pdf/2506.19430.pdf", "abs": "https://arxiv.org/abs/2506.19430", "title": "Integrating AIs With Body Tracking Technology for Human Behaviour Analysis: Challenges and Opportunities", "authors": ["Adrien Coppens", "Valérie Maquil"], "categories": ["cs.HC"], "comment": "This preprint has not undergone peer review (when applicable) or any\n  post-submission improvements or corrections. The Version of Record of this\n  contribution is published in Engineering Interactive Computer Systems (EICS)\n  2024 International Workshops, and is available online at\n  https://doi.org/10.1007/978-3-031-91760-8_4", "summary": "The automated analysis of human behaviour provides many opportunities for the\ncreation of interactive systems and the post-experiment investigations for user\nstudies. Commodity depth cameras offer reasonable body tracking accuracy at a\nlow price point, without the need for users to wear or hold any extra\nequipment. The resulting systems typically perform body tracking through a\ndedicated machine learning model, but they can be enhanced with additional AI\ncomponents providing extra capabilities. This leads to opportunities but also\nchallenges, for example regarding the orchestration of such AI components and\nthe engineering of the resulting tracking pipeline. In this paper, we discuss\nthese elements, based on our experience with the creation of a remote\ncollaboration system across distant wall-sized displays, that we built using\nexisting and readily available building blocks, including AI-based recognition\nmodels."}
{"id": "2506.19209", "pdf": "https://arxiv.org/pdf/2506.19209.pdf", "abs": "https://arxiv.org/abs/2506.19209", "title": "Augmenting Multi-Agent Communication with State Delta Trajectory", "authors": ["Yichen Tang", "Weihang Su", "Yujia Zhou", "Yiqun Liu", "Min Zhang", "Shaoping Ma", "Qingyao Ai"], "categories": ["cs.CL"], "comment": "22 pages, 5 figures", "summary": "Multi-agent techniques such as role playing or multi-turn debates have been\nshown to be effective in improving the performance of large language models\n(LLMs) in downstream tasks. Despite their differences in workflows, existing\nLLM-based multi-agent systems mostly use natural language for agent\ncommunication. While this is appealing for its simplicity and interpretability,\nit also introduces inevitable information loss as one model must down sample\nits continuous state vectors to concrete tokens before transferring them to the\nother model. Such losses are particularly significant when the information to\ntransfer is not simple facts, but reasoning logics or abstractive thoughts. To\ntackle this problem, we propose a new communication protocol that transfers\nboth natural language tokens and token-wise state transition trajectory from\none agent to another. Particularly, compared to the actual state value, we find\nthat the sequence of state changes in LLMs after generating each token can\nbetter reflect the information hidden behind the inference process, so we\npropose a State Delta Encoding (SDE) method to represent state transition\ntrajectories. The experimental results show that multi-agent systems with SDE\nachieve SOTA performance compared to other communication protocols,\nparticularly in tasks that involve complex reasoning. This shows the potential\nof communication augmentation for LLM-based multi-agent systems."}
{"id": "2506.19495", "pdf": "https://arxiv.org/pdf/2506.19495.pdf", "abs": "https://arxiv.org/abs/2506.19495", "title": "5 Days, 5 Stories: Using Technology to Promote Empathy in the Workplace", "authors": ["Russell Beale", "Eugenia Sergueeva"], "categories": ["cs.HC"], "comment": null, "summary": "Empathy is widely recognized as a vital attribute for effective collaboration\nand communication in the workplace, yet developing empathic skills and\nfostering it among colleagues remains a challenge. This study explores the\npotential of a collaborative digital storytelling platform - In Your Shoes -\ndesigned to promote empathic listening and interpersonal understanding through\nthe structured exchange of personal narratives. A one-week intervention was\nconducted with employees from multiple organizations using the platform.\nEmploying a mixed methods approach, we assessed quantitative changes in empathy\nusing the Empathy Quotient (EQ) and qualitatively analyzed participant\nexperiences through grounded theory. While quantitative analysis revealed no\nstatistically significant shift in dispositional empathy, qualitative findings\nsuggested the tool facilitated situational empathy, prompted self-reflection,\nimproved emotional resonance, and enhanced workplace relationships.\nParticipants reported feelings of psychological safety, connection, and, in\nsome cases, therapeutic benefits from sharing and responding to stories. These\nresults highlight the promise of asynchronous, structured narrative-based\ndigital tools for supporting empathic engagement in professional settings,\noffering insights for the design of emotionally intelligent workplace\ntechnologies."}
{"id": "2506.19258", "pdf": "https://arxiv.org/pdf/2506.19258.pdf", "abs": "https://arxiv.org/abs/2506.19258", "title": "Personality Prediction from Life Stories using Language Models", "authors": ["Rasiq Hussain", "Jerry Ma", "Rithik Khandelwal", "Joshua Oltmanns", "Mehak Gupta"], "categories": ["cs.CL", "cs.LG"], "comment": "13 pages, 5 figures", "summary": "Natural Language Processing (NLP) offers new avenues for personality\nassessment by leveraging rich, open-ended text, moving beyond traditional\nquestionnaires. In this study, we address the challenge of modeling long\nnarrative interview where each exceeds 2000 tokens so as to predict Five-Factor\nModel (FFM) personality traits. We propose a two-step approach: first, we\nextract contextual embeddings using sliding-window fine-tuning of pretrained\nlanguage models; then, we apply Recurrent Neural Networks (RNNs) with attention\nmechanisms to integrate long-range dependencies and enhance interpretability.\nThis hybrid method effectively bridges the strengths of pretrained transformers\nand sequence modeling to handle long-context data. Through ablation studies and\ncomparisons with state-of-the-art long-context models such as LLaMA and\nLongformer, we demonstrate improvements in prediction accuracy, efficiency, and\ninterpretability. Our results highlight the potential of combining\nlanguage-based features with long-context modeling to advance personality\nassessment from life narratives."}
{"id": "2506.19519", "pdf": "https://arxiv.org/pdf/2506.19519.pdf", "abs": "https://arxiv.org/abs/2506.19519", "title": "Examination of Eye-Tracking, Head-Gaze, and Controller-Based Ray-casting in TMT-VR: Performance and Usability Across Adulthood", "authors": ["Panagiotis Kourtesis", "Evgenia Giatzoglou", "Panagiotis Vorias", "Katerina Alkisti Gounari", "Eleni Orfanidou", "Chrysanthi Nega"], "categories": ["cs.HC", "B.8; C.4; D.0; J.4"], "comment": "29 pages, 11 Figures, 7 Tables", "summary": "Virtual reality (VR) can enrich neuropsychological testing, yet the ergonomic\ntrade-offs of its input modes remain under-examined. Seventy-seven healthy\nvolunteers-young (19-29 y) and middle-aged (27-56 y)-completed a VR\nTrail-Making Test with three pointing methods: eye-tracking, head-gaze, and a\nsix-degree-of-freedom hand controller. Completion time, spatial accuracy, and\nerror counts for the simple (Trail A) and alternating (Trail B) sequences were\nanalysed in 3 x 2 x 2 mixed-model ANOVAs; post-trial scales captured usability\n(SUS), user experience (UEQ-S), and acceptability. Age dominated behaviour:\nyounger adults were reliably faster, more precise, and less error-prone.\nAgainst this backdrop, input modality mattered. Eye-tracking yielded the best\nspatial accuracy and shortened Trail A time relative to manual control;\nhead-gaze matched eye-tracking on Trail A speed and became the quickest, least\nerror-prone option on Trail B. Controllers lagged on every metric. Subjective\nratings were high across the board, with only a small usability dip in\nmiddle-aged low-gamers. Overall, gaze-based ray-casting clearly outperformed\nmanual pointing, but optimal choice depended on task demands: eye-tracking\nmaximised spatial precision, whereas head-gaze offered calibration-free\nenhanced speed and error-avoidance under heavier cognitive load. TMT-VR appears\nto be accurate, engaging, and ergonomically adaptable assessment, yet it\nrequires age-specific-stratified norms."}
{"id": "2506.19262", "pdf": "https://arxiv.org/pdf/2506.19262.pdf", "abs": "https://arxiv.org/abs/2506.19262", "title": "What Matters in LLM-generated Data: Diversity and Its Effect on Model Fine-Tuning", "authors": ["Yuchang Zhu", "Zhonghua zhen", "Qunshu Lin", "Haotong Wei", "Xiaolong Sun", "Zixuan Yu", "Minghao Liu", "Zibin Zheng", "Liang Chen"], "categories": ["cs.CL", "cs.LG"], "comment": "Ongoing work", "summary": "With the remarkable generative capabilities of large language models (LLMs),\nusing LLM-generated data to train downstream models has emerged as a promising\napproach to mitigate data scarcity in specific domains and reduce\ntime-consuming annotations. However, recent studies have highlighted a critical\nissue: iterative training on self-generated data results in model collapse,\nwhere model performance degrades over time. Despite extensive research on the\nimplications of LLM-generated data, these works often neglect the importance of\ndata diversity, a key factor in data quality. In this work, we aim to\nunderstand the implications of the diversity of LLM-generated data on\ndownstream model performance. Specifically, we explore how varying levels of\ndiversity in LLM-generated data affect downstream model performance.\nAdditionally, we investigate the performance of models trained on data that\nmixes different proportions of LLM-generated data, which we refer to as\nsynthetic data. Our experimental results show that, with minimal distribution\nshift, moderately diverse LLM-generated data can enhance model performance in\nscenarios with insufficient labeled data, whereas highly diverse generated data\nhas a negative impact. We hope our empirical findings will offer valuable\nguidance for future studies on LLMs as data generators."}
{"id": "2506.19524", "pdf": "https://arxiv.org/pdf/2506.19524.pdf", "abs": "https://arxiv.org/abs/2506.19524", "title": "Beyond Wellbeing Apps: Co-Designing Immersive, Embodied, and Collective Digital Wellbeing Interventions for Healthcare Professionals", "authors": ["Zheyuan Zhang", "Jingjing Sun", "Dorian Peters", "Rafael A. Calvo"], "categories": ["cs.HC"], "comment": "21 pages, DIS '25: Designing Interactive Systems Conference, Funchal,\n  Portugal, July 2025", "summary": "Healthcare professionals (HCPs) face increasing levels of stress and burnout.\nTechnological wellbeing interventions provide accessible and flexible support\nfor HCPs. While most studies have focused on mobile- and web-based programs,\nalternative technologies like virtual reality (VR), augmented reality (AR),\ntangible interfaces, and embodied technologies are emerging as engaging and\neffective tools for wellbeing interventions. However, there is still a lack of\nresearch on how such technologies are perceived among HCPs. This study explored\nHCPs' perceptions and preferences for various types of wellbeing technologies,\nby conducting a 2-phase co-design study involving 26 HCPs in idea generation,\nconcept evaluation, prototype testing, and design iteration. From our findings,\nHCPs highly valued the potential of technologies to support mental health with\nimmersive, embodied, and collective experiences. Furthermore, we provided\ndesign recommendations for wellbeing technologies for HCPs that sustain user\nengagement by meeting their needs for autonomy, competence, and relatedness in\nthe experiences."}
{"id": "2506.19279", "pdf": "https://arxiv.org/pdf/2506.19279.pdf", "abs": "https://arxiv.org/abs/2506.19279", "title": "EmoStage: A Framework for Accurate Empathetic Response Generation via Perspective-Taking and Phase Recognition", "authors": ["Zhiyang Qi", "Keiko Takamizo", "Mariko Ukiyo", "Michimasa Inaba"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rising demand for mental health care has fueled interest in AI-driven\ncounseling systems. While large language models (LLMs) offer significant\npotential, current approaches face challenges, including limited understanding\nof clients' psychological states and counseling stages, reliance on\nhigh-quality training data, and privacy concerns associated with commercial\ndeployment. To address these issues, we propose EmoStage, a framework that\nenhances empathetic response generation by leveraging the inference\ncapabilities of open-source LLMs without additional training data. Our\nframework introduces perspective-taking to infer clients' psychological states\nand support needs, enabling the generation of emotionally resonant responses.\nIn addition, phase recognition is incorporated to ensure alignment with the\ncounseling process and to prevent contextually inappropriate or inopportune\nresponses. Experiments conducted in both Japanese and Chinese counseling\nsettings demonstrate that EmoStage improves the quality of responses generated\nby base models and performs competitively with data-driven methods."}
{"id": "2506.19611", "pdf": "https://arxiv.org/pdf/2506.19611.pdf", "abs": "https://arxiv.org/abs/2506.19611", "title": "Filters of Identity: AR Beauty and the Algorithmic Politics of the Digital Body", "authors": ["Miriam Doh", "Corinna Canali", "Nuria Oliver"], "categories": ["cs.HC"], "comment": "This work was presented at the \"Body Politics: Unpacking Tensions and\n  Future Perspectives For Body-Centric Design Research in HCI\" workshop at the\n  ACM (Association for Computing Machinery) CHI conference on Human Factors in\n  Computing Systems 2025", "summary": "This position paper situates AR beauty filters within the broader debate on\nBody Politics in HCI. We argue that these filters are not neutral tools but\ntechnologies of governance that reinforce racialized, gendered, and ableist\nbeauty standards. Through naming conventions, algorithmic bias, and platform\ngovernance, they impose aesthetic norms while concealing their influence. To\naddress these challenges, we advocate for transparency-driven interventions and\na critical rethinking of algorithmic aesthetics and digital embodiment."}
{"id": "2506.19315", "pdf": "https://arxiv.org/pdf/2506.19315.pdf", "abs": "https://arxiv.org/abs/2506.19315", "title": "JCAPT: A Joint Modeling Approach for CAPT", "authors": ["Tzu-Hsuan Yang", "Yue-Yang He", "Berlin Chen"], "categories": ["cs.CL", "cs.AI", "eess.AS"], "comment": "Submitted to the ISCA SLaTE-2025 Workshop", "summary": "Effective pronunciation feedback is critical in second language (L2)\nlearning, for which computer-assisted pronunciation training (CAPT) systems\noften encompass two key tasks: automatic pronunciation assessment (APA) and\nmispronunciation detection and diagnosis (MDD). Recent work has shown that\njoint modeling of these two tasks can yield mutual benefits. Our unified\nframework leverages Mamba, a selective state space model (SSM), while\nintegrating phonological features and think token strategies to jointly enhance\ninterpretability and fine-grained temporal reasoning in APA and MDD. To our\nknowledge, this is the first study to combine phonological attribution,\nSSM-based modeling, and prompting in CAPT. A series of experiments conducted on\nthe speechocean762 benchmark demonstrate that our model consistently\noutperforms prior methods, particularly on the MDD task."}
{"id": "2506.19644", "pdf": "https://arxiv.org/pdf/2506.19644.pdf", "abs": "https://arxiv.org/abs/2506.19644", "title": "Varif.ai to Vary and Verify User-Driven Diversity in Scalable Image Generation", "authors": ["M. Michelessa", "J. Ng", "C. Hurter", "B. Y. Lim"], "categories": ["cs.HC"], "comment": "DIS2025, code available at github.com/mario-michelessa/varifai", "summary": "Diversity in image generation is essential to ensure fair representations and\nsupport creativity in ideation. Hence, many text-to-image models have\nimplemented diversification mechanisms. Yet, after a few iterations of\ngeneration, a lack of diversity becomes apparent, because each user has their\nown diversity goals (e.g., different colors, brands of cars), and there are\ndiverse attributions to be specified. To support user-driven diversity control,\nwe propose Varif.ai that employs text-to-image and Large Language Models to\niteratively i) (re)generate a set of images, ii) verify if user-specified\nattributes have sufficient coverage, and iii) vary existing or new attributes.\nThrough an elicitation study, we uncovered user needs for diversity in image\ngeneration. A pilot validation showed that Varif.ai made achieving diverse\nimage sets easier. In a controlled evaluation with 20 participants, Varif.ai\nproved more effective than baseline methods across various scenarios. Thus,\nthis supports user control of diversity in image generation for creative\nideation and scalable image generation."}
{"id": "2506.19352", "pdf": "https://arxiv.org/pdf/2506.19352.pdf", "abs": "https://arxiv.org/abs/2506.19352", "title": "Spotting Out-of-Character Behavior: Atomic-Level Evaluation of Persona Fidelity in Open-Ended Generation", "authors": ["Jisu Shin", "Juhyun Oh", "Eunsu Kim", "Hoyun Song", "Alice Oh"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "Findings of ACL 2025; github repo:\n  https://github.com/ddindidu/atomic-persona-evaluation/", "summary": "Ensuring persona fidelity in large language models (LLMs) is essential for\nmaintaining coherent and engaging human-AI interactions. However, LLMs often\nexhibit Out-of-Character (OOC) behavior, where generated responses deviate from\nan assigned persona, leading to inconsistencies that affect model reliability.\nExisting evaluation methods typically assign single scores to entire responses,\nstruggling to capture subtle persona misalignment, particularly in long-form\ntext generation. To address this limitation, we propose an atomic-level\nevaluation framework that quantifies persona fidelity at a finer granularity.\nOur three key metrics measure the degree of persona alignment and consistency\nwithin and across generations. Our approach enables a more precise and\nrealistic assessment of persona fidelity by identifying subtle deviations that\nreal users would encounter. Through our experiments, we demonstrate that our\nframework effectively detects persona inconsistencies that prior methods\noverlook. By analyzing persona fidelity across diverse tasks and personality\ntypes, we reveal how task structure and persona desirability influence model\nadaptability, highlighting challenges in maintaining consistent persona\nexpression."}
{"id": "2506.18941", "pdf": "https://arxiv.org/pdf/2506.18941.pdf", "abs": "https://arxiv.org/abs/2506.18941", "title": "Can AI support student engagement in classroom activities in higher education?", "authors": ["Neha Rani", "Sharan Majumder", "Ishan Bhardwaj", "Pedro Guillermo Feijoo Garcia"], "categories": ["cs.CY", "cs.AI", "cs.ET", "cs.HC"], "comment": null, "summary": "Lucrative career prospects and creative opportunities often attract students\nto enroll in computer science majors and pursue advanced studies in the field.\nConsequently, there has been a significant surge in enrollment in computer\nscience courses, resulting in large class sizes that can range from hundreds to\neven thousands of students. A common challenge in such large classrooms is the\nlack of engagement between students and both the instructor and the learning\nmaterial. However, with advancements in technology and improvements in large\nlanguage models (LLMs), there is a considerable opportunity to utilize\nLLM-based AI models, such as conversational artificial intelligence (CAI), to\nenhance student engagement with learning content in large classes. To explore\nthe potential of CAI to support engagement, especially with learning content,\nwe designed an activity in a software Engineering course (with a large class\nsize) where students used CAI for an in-class activity. We conducted a\nwithin-subject investigation in a large classroom at a US university where we\ncompared student engagement during an in-class activity that used CAI tool vs.\none without CAI tool. The CAI tool we used was ChatGPT due to its widespread\npopularity and familiarity. Our results indicate that CAI (ChatGPT) has the\npotential to support engagement with learning content during in-class\nactivities, especially in large class sizes. We further discuss the\nimplications of our findings."}
{"id": "2506.19382", "pdf": "https://arxiv.org/pdf/2506.19382.pdf", "abs": "https://arxiv.org/abs/2506.19382", "title": "Measuring and Guiding Monosemanticity", "authors": ["Ruben Härle", "Felix Friedrich", "Manuel Brack", "Stephan Wäldchen", "Björn Deiseroth", "Patrick Schramowski", "Kristian Kersting"], "categories": ["cs.CL"], "comment": null, "summary": "There is growing interest in leveraging mechanistic interpretability and\ncontrollability to better understand and influence the internal dynamics of\nlarge language models (LLMs). However, current methods face fundamental\nchallenges in reliably localizing and manipulating feature representations.\nSparse Autoencoders (SAEs) have recently emerged as a promising direction for\nfeature extraction at scale, yet they, too, are limited by incomplete feature\nisolation and unreliable monosemanticity. To systematically quantify these\nlimitations, we introduce Feature Monosemanticity Score (FMS), a novel metric\nto quantify feature monosemanticity in latent representation. Building on these\ninsights, we propose Guided Sparse Autoencoders (G-SAE), a method that\nconditions latent representations on labeled concepts during training. We\ndemonstrate that reliable localization and disentanglement of target concepts\nwithin the latent space improve interpretability, detection of behavior, and\ncontrol. Specifically, our evaluations on toxicity detection, writing style\nidentification, and privacy attribute recognition show that G-SAE not only\nenhances monosemanticity but also enables more effective and fine-grained\nsteering with less quality degradation. Our findings provide actionable\nguidelines for measuring and advancing mechanistic interpretability and control\nof LLMs."}
{"id": "2506.19079", "pdf": "https://arxiv.org/pdf/2506.19079.pdf", "abs": "https://arxiv.org/abs/2506.19079", "title": "Reading Smiles: Proxy Bias in Foundation Models for Facial Emotion Recognition", "authors": ["Iosif Tsangko", "Andreas Triantafyllopoulos", "Adem Abdelmoula", "Adria Mallol-Ragolta", "Bjoern W. Schuller"], "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": null, "summary": "Foundation Models (FMs) are rapidly transforming Affective Computing (AC),\nwith Vision Language Models (VLMs) now capable of recognising emotions in zero\nshot settings. This paper probes a critical but underexplored question: what\nvisual cues do these models rely on to infer affect, and are these cues\npsychologically grounded or superficially learnt? We benchmark varying scale\nVLMs on a teeth annotated subset of AffectNet dataset and find consistent\nperformance shifts depending on the presence of visible teeth. Through\nstructured introspection of, the best-performing model, i.e., GPT-4o, we show\nthat facial attributes like eyebrow position drive much of its affective\nreasoning, revealing a high degree of internal consistency in its\nvalence-arousal predictions. These patterns highlight the emergent nature of\nFMs behaviour, but also reveal risks: shortcut learning, bias, and fairness\nissues especially in sensitive domains like mental health and education."}
{"id": "2506.19399", "pdf": "https://arxiv.org/pdf/2506.19399.pdf", "abs": "https://arxiv.org/abs/2506.19399", "title": "Automated Detection of Pre-training Text in Black-box LLMs", "authors": ["Ruihan Hu", "Yu-Ming Shang", "Jiankun Peng", "Wei Luo", "Yazhe Wang", "Xi Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "13 pages", "summary": "Detecting whether a given text is a member of the pre-training data of Large\nLanguage Models (LLMs) is crucial for ensuring data privacy and copyright\nprotection. Most existing methods rely on the LLM's hidden information (e.g.,\nmodel parameters or token probabilities), making them ineffective in the\nblack-box setting, where only input and output texts are accessible. Although\nsome methods have been proposed for the black-box setting, they rely on massive\nmanual efforts such as designing complicated questions or instructions. To\naddress these issues, we propose VeilProbe, the first framework for\nautomatically detecting LLMs' pre-training texts in a black-box setting without\nhuman intervention. VeilProbe utilizes a sequence-to-sequence mapping model to\ninfer the latent mapping feature between the input text and the corresponding\noutput suffix generated by the LLM. Then it performs the key token\nperturbations to obtain more distinguishable membership features. Additionally,\nconsidering real-world scenarios where the ground-truth training text samples\nare limited, a prototype-based membership classifier is introduced to alleviate\nthe overfitting issue. Extensive evaluations on three widely used datasets\ndemonstrate that our framework is effective and superior in the black-box\nsetting."}
{"id": "2506.19202", "pdf": "https://arxiv.org/pdf/2506.19202.pdf", "abs": "https://arxiv.org/abs/2506.19202", "title": "Preserving Sense of Agency: User Preferences for Robot Autonomy and User Control across Household Tasks", "authors": ["Claire Yang", "Heer Patel", "Max Kleiman-Weiner", "Maya Cakmak"], "categories": ["cs.RO", "cs.HC"], "comment": "Accepted by the 2025 34th IEEE International Conference on Robot and\n  Human Interactive Communication (RO-MAN)", "summary": "Roboticists often design with the assumption that assistive robots should be\nfully autonomous. However, it remains unclear whether users prefer highly\nautonomous robots, as prior work in assistive robotics suggests otherwise. High\nrobot autonomy can reduce the user's sense of agency, which represents feeling\nin control of one's environment. How much control do users, in fact, want over\nthe actions of robots used for in-home assistance? We investigate how robot\nautonomy levels affect users' sense of agency and the autonomy level they\nprefer in contexts with varying risks. Our study asked participants to rate\ntheir sense of agency as robot users across four distinct autonomy levels and\nranked their robot preferences with respect to various household tasks. Our\nfindings revealed that participants' sense of agency was primarily influenced\nby two factors: (1) whether the robot acts autonomously, and (2) whether a\nthird party is involved in the robot's programming or operation. Notably, an\nend-user programmed robot highly preserved users' sense of agency, even though\nit acts autonomously. However, in high-risk settings, e.g., preparing a snack\nfor a child with allergies, they preferred robots that prioritized their\ncontrol significantly more. Additional contextual factors, such as trust in a\nthird party operator, also shaped their preferences."}
{"id": "2506.19418", "pdf": "https://arxiv.org/pdf/2506.19418.pdf", "abs": "https://arxiv.org/abs/2506.19418", "title": "Learning to Disentangle Latent Reasoning Rules with Language VAEs: A Systematic Study", "authors": ["Yingji Zhang", "Marco Valentino", "Danilo S. Carvalho", "André Freitas"], "categories": ["cs.CL"], "comment": null, "summary": "Incorporating explicit reasoning rules within the latent space of language\nmodels (LMs) offers a promising pathway to enhance generalisation,\ninterpretability, and controllability. While current Transformer-based language\nmodels have shown strong performance on Natural Language Inference (NLI) tasks,\nthey often rely on memorisation rather than rule-based inference. This work\ninvestigates how reasoning rules can be explicitly embedded and memorised\nwithin the LMs through Language Variational Autoencoders (VAEs). We propose a\ncomplete pipeline for learning reasoning rules within Transformer-based\nlanguage VAEs. This pipeline encompasses three rule-based reasoning tasks, a\nsupporting theoretical framework, and a practical end-to-end architecture. The\nexperiment illustrates the following findings: Disentangled reasoning: Under\nexplicit signal supervision, reasoning rules - viewed as functional mappings -\ncan be disentangled within the encoder's parametric space. This separation\nresults in distinct clustering of rules in the output feature space. Prior\nknowledge injection: injecting reasoning information into the Query enables the\nmodel to more effectively retrieve the stored value Value from memory based on\nKey. This approach offers a simple method for integrating prior knowledge into\ndecoder-only language models. Performance bottleneck: In mathematical reasoning\ntasks using Qwen2.5(0.5B), increasing sample count doesn't improve performance\nbeyond a point. Moreover, ffn layers are better than attention layers at\npreserving the separation of reasoning rules in the model's parameters."}
{"id": "2506.19280", "pdf": "https://arxiv.org/pdf/2506.19280.pdf", "abs": "https://arxiv.org/abs/2506.19280", "title": "Emotion Detection on User Front-Facing App Interfaces for Enhanced Schedule Optimization: A Machine Learning Approach", "authors": ["Feiting Yang", "Antoine Moevus", "Steve Lévesque"], "categories": ["cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "Human-Computer Interaction (HCI) has evolved significantly to incorporate\nemotion recognition capabilities, creating unprecedented opportunities for\nadaptive and personalized user experiences. This paper explores the integration\nof emotion detection into calendar applications, enabling user interfaces to\ndynamically respond to users' emotional states and stress levels, thereby\nenhancing both productivity and engagement. We present and evaluate two\ncomplementary approaches to emotion detection: a biometric-based method\nutilizing heart rate (HR) data extracted from electrocardiogram (ECG) signals\nprocessed through Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU)\nneural networks to predict the emotional dimensions of Valence, Arousal, and\nDominance; and a behavioral method analyzing computer activity through multiple\nmachine learning models to classify emotions based on fine-grained user\ninteractions such as mouse movements, clicks, and keystroke patterns. Our\ncomparative analysis, from real-world datasets, reveals that while both\napproaches demonstrate effectiveness, the computer activity-based method\ndelivers superior consistency and accuracy, particularly for mouse-related\ninteractions, which achieved approximately 90\\% accuracy. Furthermore, GRU\nnetworks outperformed LSTM models in the biometric approach, with Valence\nprediction reaching 84.38\\% accuracy."}
{"id": "2506.19467", "pdf": "https://arxiv.org/pdf/2506.19467.pdf", "abs": "https://arxiv.org/abs/2506.19467", "title": "Can Large Language Models Capture Human Annotator Disagreements?", "authors": ["Jingwei Ni", "Yu Fan", "Vilém Zouhar", "Donya Rooein", "Alexander Hoyle", "Mrinmaya Sachan", "Markus Leippold", "Dirk Hovy", "Elliott Ash"], "categories": ["cs.CL", "cs.AI"], "comment": "Preprint Under Review", "summary": "Human annotation variation (i.e., annotation disagreements) is common in NLP\nand often reflects important information such as task subjectivity and sample\nambiguity. While Large Language Models (LLMs) are increasingly used for\nautomatic annotation to reduce human effort, their evaluation often focuses on\npredicting the majority-voted \"ground truth\" labels. It is still unclear,\nhowever, whether these models also capture informative human annotation\nvariation. Our work addresses this gap by extensively evaluating LLMs' ability\nto predict annotation disagreements without access to repeated human labels.\nOur results show that LLMs struggle with modeling disagreements, which can be\noverlooked by majority label-based evaluations. Notably, while RLVR-style\n(Reinforcement learning with verifiable rewards) reasoning generally boosts LLM\nperformance, it degrades performance in disagreement prediction. Our findings\nhighlight the critical need for evaluating and improving LLM annotators in\ndisagreement modeling. Code and data at\nhttps://github.com/EdisonNi-hku/Disagreement_Prediction."}
{"id": "2506.19352", "pdf": "https://arxiv.org/pdf/2506.19352.pdf", "abs": "https://arxiv.org/abs/2506.19352", "title": "Spotting Out-of-Character Behavior: Atomic-Level Evaluation of Persona Fidelity in Open-Ended Generation", "authors": ["Jisu Shin", "Juhyun Oh", "Eunsu Kim", "Hoyun Song", "Alice Oh"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "Findings of ACL 2025; github repo:\n  https://github.com/ddindidu/atomic-persona-evaluation/", "summary": "Ensuring persona fidelity in large language models (LLMs) is essential for\nmaintaining coherent and engaging human-AI interactions. However, LLMs often\nexhibit Out-of-Character (OOC) behavior, where generated responses deviate from\nan assigned persona, leading to inconsistencies that affect model reliability.\nExisting evaluation methods typically assign single scores to entire responses,\nstruggling to capture subtle persona misalignment, particularly in long-form\ntext generation. To address this limitation, we propose an atomic-level\nevaluation framework that quantifies persona fidelity at a finer granularity.\nOur three key metrics measure the degree of persona alignment and consistency\nwithin and across generations. Our approach enables a more precise and\nrealistic assessment of persona fidelity by identifying subtle deviations that\nreal users would encounter. Through our experiments, we demonstrate that our\nframework effectively detects persona inconsistencies that prior methods\noverlook. By analyzing persona fidelity across diverse tasks and personality\ntypes, we reveal how task structure and persona desirability influence model\nadaptability, highlighting challenges in maintaining consistent persona\nexpression."}
{"id": "2506.19468", "pdf": "https://arxiv.org/pdf/2506.19468.pdf", "abs": "https://arxiv.org/abs/2506.19468", "title": "MuBench: Assessment of Multilingual Capabilities of Large Language Models Across 61 Languages", "authors": ["Wenhan Han", "Yifan Zhang", "Zhixun Chen", "Binbin Liu", "Haobin Lin", "Bingni Zhang", "Taifeng Wang", "Mykola Pechenizkiy", "Meng Fang", "Yin Zheng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multilingual large language models (LLMs) are advancing rapidly, with new\nmodels frequently claiming support for an increasing number of languages.\nHowever, existing evaluation datasets are limited and lack cross-lingual\nalignment, leaving assessments of multilingual capabilities fragmented in both\nlanguage and skill coverage. To address this, we introduce MuBench, a benchmark\ncovering 61 languages and evaluating a broad range of capabilities. We evaluate\nseveral state-of-the-art multilingual LLMs and find notable gaps between\nclaimed and actual language coverage, particularly a persistent performance\ndisparity between English and low-resource languages. Leveraging MuBench's\nalignment, we propose Multilingual Consistency (MLC) as a complementary metric\nto accuracy for analyzing performance bottlenecks and guiding model\nimprovement. Finally, we pretrain a suite of 1.2B-parameter models on English\nand Chinese with 500B tokens, varying language ratios and parallel data\nproportions to investigate cross-lingual transfer dynamics."}
{"id": "2506.19415", "pdf": "https://arxiv.org/pdf/2506.19415.pdf", "abs": "https://arxiv.org/abs/2506.19415", "title": "Virtual Memory for 3D Gaussian Splatting", "authors": ["Jonathan Haberl", "Philipp Fleck", "Clemens Arth"], "categories": ["cs.GR", "cs.CV", "cs.HC"], "comment": "Based on the Master Thesis from Jonathan Haberl from 2024, Submitted\n  to TVCG in Feb. 2025;", "summary": "3D Gaussian Splatting represents a breakthrough in the field of novel view\nsynthesis. It establishes Gaussians as core rendering primitives for highly\naccurate real-world environment reconstruction. Recent advances have\ndrastically increased the size of scenes that can be created. In this work, we\npresent a method for rendering large and complex 3D Gaussian Splatting scenes\nusing virtual memory. By leveraging well-established virtual memory and virtual\ntexturing techniques, our approach efficiently identifies visible Gaussians and\ndynamically streams them to the GPU just in time for real-time rendering.\nSelecting only the necessary Gaussians for both storage and rendering results\nin reduced memory usage and effectively accelerates rendering, especially for\nhighly complex scenes. Furthermore, we demonstrate how level of detail can be\nintegrated into our proposed method to further enhance rendering speed for\nlarge-scale scenes. With an optimized implementation, we highlight key\npractical considerations and thoroughly evaluate the proposed technique and its\nimpact on desktop and mobile devices."}
{"id": "2506.19483", "pdf": "https://arxiv.org/pdf/2506.19483.pdf", "abs": "https://arxiv.org/abs/2506.19483", "title": "Commonsense Generation and Evaluation for Dialogue Systems using Large Language Models", "authors": ["Marcos Estecha-Garitagoitia", "Chen Zhang", "Mario Rodríguez-Cantelar", "Luis Fernando D'Haro"], "categories": ["cs.CL"], "comment": null, "summary": "This paper provides preliminary results on exploring the task of performing\nturn-level data augmentation for dialogue system based on different types of\ncommonsense relationships, and the automatic evaluation of the generated\nsynthetic turns. The proposed methodology takes advantage of the extended\nknowledge and zero-shot capabilities of pretrained Large Language Models (LLMs)\nto follow instructions, understand contextual information, and their\ncommonsense reasoning capabilities. The approach draws inspiration from\nmethodologies like Chain-of-Thought (CoT), applied more explicitly to the task\nof prompt-based generation for dialogue-based data augmentation conditioned on\ncommonsense attributes, and the automatic evaluation of the generated\ndialogues.\n  To assess the effectiveness of the proposed approach, first we extracted 200\nrandomly selected partial dialogues, from 5 different well-known dialogue\ndatasets, and generate alternative responses conditioned on different event\ncommonsense attributes. This novel dataset allows us to measure the proficiency\nof LLMs in generating contextually relevant commonsense knowledge, particularly\nup to 12 different specific ATOMIC [10] database relations. Secondly, we\npropose an evaluation framework to automatically detect the quality of the\ngenerated dataset inspired by the ACCENT [26] metric, which offers a nuanced\napproach to assess event commonsense. However, our method does not follow\nACCENT's complex eventrelation tuple extraction process. Instead, we propose an\ninstruction-based prompt for each commonsense attribute and use\nstate-of-the-art LLMs to automatically detect the original attributes used when\ncreating each augmented turn in the previous step.\n  Preliminary results suggest that our approach effectively harnesses LLMs\ncapabilities for commonsense reasoning and evaluation in dialogue systems."}
{"id": "2506.19484", "pdf": "https://arxiv.org/pdf/2506.19484.pdf", "abs": "https://arxiv.org/abs/2506.19484", "title": "Dialogic Pedagogy for Large Language Models: Aligning Conversational AI with Proven Theories of Learning", "authors": ["Russell Beale"], "categories": ["cs.CL", "cs.AI", "cs.HC", "K.3.2; I.2.6; H.4.m"], "comment": null, "summary": "Large Language Models (LLMs) are rapidly transforming education by enabling\nrich conversational learning experiences. This article provides a comprehensive\nreview of how LLM-based conversational agents are being used in higher\neducation, with extensions to secondary and lifelong learning contexts. We\nsynthesize existing literature on LLMs in education and theories of\nconversational and dialogic pedagogy - including Vygotsky's sociocultural\nlearning (scaffolding and the Zone of Proximal Development), the Socratic\nmethod, and Laurillard's conversational framework - and examine how prompting\nstrategies and retrieval-augmented generation (RAG) can align LLM behaviors\nwith these pedagogical theories, and how it can support personalized, adaptive\nlearning. We map educational theories to LLM capabilities, highlighting where\nLLM-driven dialogue supports established learning principles and where it\nchallenges or falls short of traditional pedagogical assumptions. Notable gaps\nin applying prior theories to LLMs are identified, such as the models tendency\nto provide direct answers instead of fostering co-construction of knowledge,\nand the need to account for the constant availability and broad but non-human\nexpertise of LLM tutors. In response, we propose practical strategies to better\nalign LLM interactions with sound pedagogy - for example, designing prompts\nthat encourage Socratic questioning, scaffolded guidance, and student\nreflection, as well as integrating retrieval mechanisms to ensure accuracy and\ncontextual relevance. Our aim is to bridge the gap between educational theory\nand the emerging practice of AI-driven conversational learning, offering\ninsights and tools for making LLM-based dialogues more educationally productive\nand theory-aligned."}
{"id": "2506.19484", "pdf": "https://arxiv.org/pdf/2506.19484.pdf", "abs": "https://arxiv.org/abs/2506.19484", "title": "Dialogic Pedagogy for Large Language Models: Aligning Conversational AI with Proven Theories of Learning", "authors": ["Russell Beale"], "categories": ["cs.CL", "cs.AI", "cs.HC", "K.3.2; I.2.6; H.4.m"], "comment": null, "summary": "Large Language Models (LLMs) are rapidly transforming education by enabling\nrich conversational learning experiences. This article provides a comprehensive\nreview of how LLM-based conversational agents are being used in higher\neducation, with extensions to secondary and lifelong learning contexts. We\nsynthesize existing literature on LLMs in education and theories of\nconversational and dialogic pedagogy - including Vygotsky's sociocultural\nlearning (scaffolding and the Zone of Proximal Development), the Socratic\nmethod, and Laurillard's conversational framework - and examine how prompting\nstrategies and retrieval-augmented generation (RAG) can align LLM behaviors\nwith these pedagogical theories, and how it can support personalized, adaptive\nlearning. We map educational theories to LLM capabilities, highlighting where\nLLM-driven dialogue supports established learning principles and where it\nchallenges or falls short of traditional pedagogical assumptions. Notable gaps\nin applying prior theories to LLMs are identified, such as the models tendency\nto provide direct answers instead of fostering co-construction of knowledge,\nand the need to account for the constant availability and broad but non-human\nexpertise of LLM tutors. In response, we propose practical strategies to better\nalign LLM interactions with sound pedagogy - for example, designing prompts\nthat encourage Socratic questioning, scaffolded guidance, and student\nreflection, as well as integrating retrieval mechanisms to ensure accuracy and\ncontextual relevance. Our aim is to bridge the gap between educational theory\nand the emerging practice of AI-driven conversational learning, offering\ninsights and tools for making LLM-based dialogues more educationally productive\nand theory-aligned."}
{"id": "2506.19757", "pdf": "https://arxiv.org/pdf/2506.19757.pdf", "abs": "https://arxiv.org/abs/2506.19757", "title": "Exploring Developer Experience Factors in Software Ecosystems", "authors": ["Rodrigo Oliveira Zacarias", "Léo Carvalho Ramos Antunes", "Márcio de Oliveira Barros", "Rodrigo Pereira dos Santos", "Patricia Lago"], "categories": ["cs.SE", "cs.HC"], "comment": "58 pages", "summary": "Context: Developer experience (DX) plays a key role in developers'\nperformance and their continued involvement in a software ecosystem (SECO)\nplatform. While researchers and practitioners have recognized several factors\naffecting DX in SECO platforms, a clear roadmap of the most influential factors\nis still missing. This is particularly important given the direct impact on\ndevelopers' interest in SECO and their ongoing engagement with the common\ntechnological platform. Goal: This work aims to identify key DX factors and\nunderstand how they influence third-party developers' decisions to adopt and\nkeep contributing to a SECO. Methods: We conducted a systematic mapping study\n(SMS), analyzing 29 studies to assess the state-of-the-art of DX in SECO.\nAdditionally, we conducted a Delphi study to evaluate the influence of 27 DX\nfactors (identified in our SMS) from the perspective of 21 third-party\ndevelopers to adopt and keep contributing to a SECO. Results: The factors that\nmost strongly influence developers' adoption and ongoing contributions to a\nSECO are: financial costs for using the platform, desired technical resources\nfor development, low barriers to entry into the applications market, and more\nfinancial gains. Conclusion: DX is essential for the success and sustainability\nof SECO. Our set of DX factors provides valuable insights and recommendations\nfor researchers and practitioners to address key DX concerns from the\nperspective of third-party developers."}
{"id": "2506.19492", "pdf": "https://arxiv.org/pdf/2506.19492.pdf", "abs": "https://arxiv.org/abs/2506.19492", "title": "Is Long-to-Short a Free Lunch? Investigating Inconsistency and Reasoning Efficiency in LRMs", "authors": ["Shu Yang", "Junchao Wu", "Xuansheng Wu", "Derek Wong", "Ninhao Liu", "Di Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Reasoning Models (LRMs) have achieved remarkable performance on complex\ntasks by engaging in extended reasoning before producing final answers, yet\nthis strength introduces the risk of overthinking, where excessive token\ngeneration occurs even for simple tasks. While recent work in efficient\nreasoning seeks to reduce reasoning length while preserving accuracy, it\nremains unclear whether such optimization is truly a free lunch. Drawing on the\nintuition that compressing reasoning may reduce the robustness of model\nresponses and lead models to omit key reasoning steps, we investigate whether\nefficient reasoning strategies introduce behavioral inconsistencies. To\nsystematically assess this, we introduce $ICBENCH$, a benchmark designed to\nmeasure inconsistency in LRMs across three dimensions: inconsistency across\ntask settings (ITS), inconsistency between training objectives and learned\nbehavior (TR-LB), and inconsistency between internal reasoning and\nself-explanations (IR-SE). Applying $ICBENCH$ to a range of open-source LRMs,\nwe find that while larger models generally exhibit greater consistency than\nsmaller ones, they all display widespread \"scheming\" behaviors, including\nself-disagreement, post-hoc rationalization, and the withholding of reasoning\ncues. Crucially, our results demonstrate that efficient reasoning strategies\nsuch as No-Thinking and Simple Token-Budget consistently increase all three\ndefined types of inconsistency. These findings suggest that although efficient\nreasoning enhances token-level efficiency, further investigation is imperative\nto ascertain whether it concurrently introduces the risk of models evading\neffective supervision."}
{"id": "2401.08405", "pdf": "https://arxiv.org/pdf/2401.08405.pdf", "abs": "https://arxiv.org/abs/2401.08405", "title": "Interrogating AI: Characterizing Emergent Playful Interactions with ChatGPT", "authors": ["Mohammad Ronagh Nikghalb", "Jinghui Cheng"], "categories": ["cs.HC", "cs.AI"], "comment": "Accepted to CSCW 2025; 23 pages", "summary": "In an era of AI's growing capabilities and influences, recent advancements\nare reshaping HCI and CSCW's view of AI. Playful interactions emerged as an\nimportant way for users to make sense of the ever-changing AI technologies, yet\nremained underexamined. We target this gap by investigating playful\ninteractions exhibited by users of a popular AI technology, ChatGPT. Through a\nthematic analysis of 372 user-generated posts on the ChatGPT subreddit, we\nfound that more than half (54\\%) of user discourse revolved around playful\ninteractions. The analysis further allowed us to construct a preliminary\nframework to describe these interactions, categorizing them into six types:\nreflecting, jesting, imitating, challenging, tricking, and contriving; each\nincluded sub-categories. This study contributes to HCI and CSCW by identifying\nthe diverse ways users engage in playful interactions with AI. It examines how\nthese interactions can help users understand AI's agency, shape human-AI\nrelationships, and provide insights for designing AI systems."}
{"id": "2506.19505", "pdf": "https://arxiv.org/pdf/2506.19505.pdf", "abs": "https://arxiv.org/abs/2506.19505", "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models", "authors": ["Zeyu Li", "Chuanfu Xiao", "Yang Wang", "Xiang Liu", "Zhenheng Tang", "Baotong Lu", "Mao Yang", "Xinyu Chen", "Xiaowen Chu"], "categories": ["cs.CL"], "comment": null, "summary": "Quantization has emerged as an effective and lightweight solution to reduce\nthe memory footprint of the KV cache in Large Language Models (LLMs).\nNevertheless, minimizing the performance degradation caused by ultra-low-bit KV\ncache quantization remains a significant challenge. We observe that quantizing\nthe KV cache of different tokens has varying impacts on the quality of\nattention outputs. To systematically investigate this phenomenon, we perform\nforward error propagation analysis on attention and propose the Anchor Score\n(AnS) that quantifies the sensitivity of each token's KV cache to\nquantization-induced error. Our analysis reveals significant disparities in AnS\nacross tokens, suggesting that preserving a small subset with full precision\n(FP16) of high-AnS tokens can greatly mitigate accuracy loss in aggressive\nquantization scenarios. Based on this insight, we introduce AnTKV, a novel\nframework that leverages Anchor Token-aware Vector Quantization to compress the\nKV cache. Furthermore, to support efficient deployment, we design and develop a\ntriton kernel that is fully compatible with FlashAttention, enabling fast\nonline Anchor Token selection. AnTKV enables LLaMA-3-8B to handle context\nlengths up to 840K tokens on a single 80GB A100 GPU, while achieving up to 3.5x\nhigher decoding throughput compared to the FP16 baseline. Our experiment\nresults demonstrate that AnTKV matches or outperforms prior works such as KIVI,\nSKVQ, KVQuant, and CQ under 4-bit settings. More importantly, AnTKV achieves\nsignificantly lower perplexity under ultra-low-bit quantization on Mistral-7B,\nwith only 6.32 at 1-bit and 8.87 at 0.375-bit, compared to the FP16 baseline of\n4.73."}
{"id": "2404.15615", "pdf": "https://arxiv.org/pdf/2404.15615.pdf", "abs": "https://arxiv.org/abs/2404.15615", "title": "M3D: Manifold-based Domain Adaptation with Dynamic Distribution for Non-Deep Transfer Learning in Cross-subject and Cross-session EEG-based Emotion Recognition", "authors": ["Ting Luo", "Jing Zhang", "Yingwei Qiu", "Li Zhang", "Yaohua Hu", "Zhuliang Yu", "Zhen Liang"], "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "Emotion decoding using Electroencephalography (EEG)-based affective\nbrain-computer interfaces (aBCIs) plays a crucial role in affective computing\nbut is limited by challenges such as EEG's non-stationarity, individual\nvariability, and the high cost of large labeled datasets. While deep learning\nmethods are effective, they require extensive computational resources and large\ndata volumes, limiting their practical application. To overcome these issues,\nwe propose Manifold-based Domain Adaptation with Dynamic Distribution (M3D), a\nlightweight, non-deep transfer learning framework. M3D consists of four key\nmodules: manifold feature transformation, dynamic distribution alignment,\nclassifier learning, and ensemble learning. The data is mapped to an optimal\nGrassmann manifold space, enabling dynamic alignment of source and target\ndomains. This alignment is designed to prioritize both marginal and conditional\ndistributions, improving adaptation efficiency across diverse datasets. In\nclassifier learning, the principle of structural risk minimization is applied\nto build robust classification models. Additionally, dynamic distribution\nalignment iteratively refines the classifier. The ensemble learning module\naggregates classifiers from different optimization stages to leverage diversity\nand enhance prediction accuracy. M3D is evaluated on two EEG emotion\nrecognition datasets using two validation protocols (cross-subject\nsingle-session and cross-subject cross-session) and a clinical EEG dataset for\nMajor Depressive Disorder (MDD). Experimental results show that M3D outperforms\ntraditional non-deep learning methods with a 4.47% average improvement and\nachieves deep learning-level performance with reduced data and computational\nrequirements, demonstrating its potential for real-world aBCI applications."}
{"id": "2506.19512", "pdf": "https://arxiv.org/pdf/2506.19512.pdf", "abs": "https://arxiv.org/abs/2506.19512", "title": "heiDS at ArchEHR-QA 2025: From Fixed-k to Query-dependent-k for Retrieval Augmented Generation", "authors": ["Ashish Chouhan", "Michael Gertz"], "categories": ["cs.CL"], "comment": "12 pages, 2 figures, 6 tables, Workshop on BioNLP and Shared Tasks at\n  ACL 2025", "summary": "This paper presents the approach of our team called heiDS for the ArchEHR-QA\n2025 shared task. A pipeline using a retrieval augmented generation (RAG)\nframework is designed to generate answers that are attributed to clinical\nevidence from the electronic health records (EHRs) of patients in response to\npatient-specific questions. We explored various components of a RAG framework,\nfocusing on ranked list truncation (RLT) retrieval strategies and attribution\napproaches. Instead of using a fixed top-k RLT retrieval strategy, we employ a\nquery-dependent-k retrieval strategy, including the existing surprise and\nautocut methods and two new methods proposed in this work, autocut* and elbow.\nThe experimental results show the benefits of our strategy in producing factual\nand relevant answers when compared to a fixed-$k$."}
{"id": "2501.02084", "pdf": "https://arxiv.org/pdf/2501.02084.pdf", "abs": "https://arxiv.org/abs/2501.02084", "title": "Simulated prosthetic vision confirms checkerboard as an effective raster pattern for epiretinal implants", "authors": ["Justin M. Kasowski", "Apurv Varshney", "Roksana Sadeghi", "Michael Beyeler"], "categories": ["cs.HC"], "comment": null, "summary": "Spatial scheduling of electrode activation (\"rastering\") is essential for\nsafely operating high-density retinal implants, yet its perceptual consequences\nremain poorly understood. This study systematically evaluates the impact of\nraster patterns, or spatial arrangements of sequential electrode activation, on\nperformance and perceived difficulty in simulated prosthetic vision (SPV). By\naddressing this gap, we aimed to identify patterns that optimize functional\nvision in retinal implants. Sighted participants completed letter recognition\nand motion discrimination tasks under four raster patterns (horizontal,\nvertical, checkerboard, and random) using an immersive SPV system. The\nsimulations emulated epiretinal implant perception and employed\npsychophysically validated models of electrode activation, phosphene\nappearance, nonlinear spatial summation, and temporal dynamics, ensuring\nrealistic representation of prosthetic vision. Performance accuracy and\nself-reported difficulty were analyzed to assess the effects of raster\npatterning. The checkerboard pattern consistently outperformed other raster\npatterns, yielding significantly higher accuracy and lower difficulty ratings\nacross both tasks. The horizontal and vertical patterns introduced biases\naligned with apparent motion artifacts, while the checkerboard minimized such\neffects. Random patterns resulted in the lowest performance, underscoring the\nimportance of structured activation. Notably, checkerboard matched performance\nin the \"No Raster\" condition, despite conforming to groupwise safety\nconstraints. This is the first quantitative, task-based evaluation of raster\npatterns in SPV. Checkerboard-style scheduling enhances perceptual clarity\nwithout increasing computational load, offering a low-overhead, clinically\nrelevant strategy for improving usability in next-generation retinal\nprostheses."}
{"id": "2506.19525", "pdf": "https://arxiv.org/pdf/2506.19525.pdf", "abs": "https://arxiv.org/abs/2506.19525", "title": "Automatic Posology Structuration : What role for LLMs?", "authors": ["Natalia Bobkova", "Laura Zanella-Calzada", "Anyes Tafoughalt", "Raphaël Teboul", "François Plesse", "Félix Gaschi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Automatically structuring posology instructions is essential for improving\nmedication safety and enabling clinical decision support. In French\nprescriptions, these instructions are often ambiguous, irregular, or\ncolloquial, limiting the effectiveness of classic ML pipelines. We explore the\nuse of Large Language Models (LLMs) to convert free-text posologies into\nstructured formats, comparing prompt-based methods and fine-tuning against a\n\"pre-LLM\" system based on Named Entity Recognition and Linking (NERL). Our\nresults show that while prompting improves performance, only fine-tuned LLMs\nmatch the accuracy of the baseline. Through error analysis, we observe\ncomplementary strengths: NERL offers structural precision, while LLMs better\nhandle semantic nuances. Based on this, we propose a hybrid pipeline that\nroutes low-confidence cases from NERL (<0.8) to the LLM, selecting outputs\nbased on confidence scores. This strategy achieves 91% structuration accuracy\nwhile minimizing latency and compute. Our results show that this hybrid\napproach improves structuration accuracy while limiting computational cost,\noffering a scalable solution for real-world clinical use."}
{"id": "2501.15276", "pdf": "https://arxiv.org/pdf/2501.15276.pdf", "abs": "https://arxiv.org/abs/2501.15276", "title": "Exploring the Collaborative Co-Creation Process with AI: A Case Study in Novice Music Production", "authors": ["Yue Fu", "Michele Newman", "Lewis Going", "Qiuzi Feng", "Jin Ha Lee"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Artificial intelligence is reshaping creative domains, yet its co-creative\nprocesses, especially in group settings with novice users, remain under\nexplored. To bridge this gap, we conducted a case study in a college-level\ncourse where nine undergraduate students were tasked with creating three\noriginal music tracks using AI tools over 10 weeks. The study spanned the\nentire creative journey from ideation to releasing these songs on Spotify.\nParticipants leveraged AI for music and lyric production, cover art, and\ndistribution. Our findings highlight how AI transforms creative workflows:\naccelerating ideation but compressing the traditional preparation stage, and\nrequiring novices to navigate a challenging idea selection and validation\nphase. We also identified a new \"collaging and refinement\" stage, where\nparticipants creatively combined diverse AI-generated outputs into cohesive\nworks. Furthermore, AI influenced group social dynamics and role division among\nhuman creators. Based on these insights, we propose the Human-AI Co-Creation\nStage Model and the Human-AI Agency Model, offering new perspectives on\ncollaborative co-creation with AI."}
{"id": "2506.19527", "pdf": "https://arxiv.org/pdf/2506.19527.pdf", "abs": "https://arxiv.org/abs/2506.19527", "title": "KnowMap: Efficient Knowledge-Driven Task Adaptation for LLMs", "authors": ["Kelin Fu", "Kaigui Bian"], "categories": ["cs.CL"], "comment": null, "summary": "While Large Language Models (LLMs) possess significant capabilities in\nopen-world agent tasks, they also face challenges in rapidly adapting to new,\nspecialized tasks due to their reliance on static pre-trained knowledge.\nTraditional methods such as fine-tuning are often costly, data-intensive, and\nmay lead to \"catastrophic forgetting.\" Therefore, we present KnowMap, a novel\napproach that dynamically constructs a knowledge base from environmental and\nexperiential data. KnowMap fine-tunes a small knowledge-embedding model to\nequip a larger LLM with valuable task-specific knowledge. Our experiments on\nthe ScienceWorld benchmark demonstrate 17.71% improvement for the performance\nof gpt-4-turbo model. KnowMap not only provides an efficient and effective\nmeans for LLM task-adapting, but also highlights how integrating environmental\nand experiential knowledge can enhance LLMs' reasoning capabilities."}
{"id": "2503.16484", "pdf": "https://arxiv.org/pdf/2503.16484.pdf", "abs": "https://arxiv.org/abs/2503.16484", "title": "AI-Facilitated Episodic Future Thinking For Adults with Obesity", "authors": ["Sareh Ahmadi", "Michelle Rockwell", "Megan Stuart", "Nicki Rohani", "Allison Tegge", "Xuan Wang", "Jeffrey Stein", "Edward A. Fox"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Episodic Future Thinking (EFT) involves vividly imagining personal future\nevents and experiences in detail. It has shown promise as an intervention to\nreduce delay discounting-the tendency to devalue delayed rewards in favor of\nimmediate gratification- and to promote behavior change in a range of\nmaladaptive health behaviors. We present EFTeacher, an AI chatbot powered by\nthe GPT-4-Turbo large language model, designed to generate EFT cues for users\nwith lifestyle-related conditions. To evaluate the feasibility and usability of\nEFTeacher, we conducted a mixed-methods study that included usability\nassessments, user evaluations based on content characteristics questionnaires,\nand semi-structured interviews. Qualitative findings indicate that participants\nperceived EFTeacher as communicative and supportive through an engaging\ndialogue. The chatbot facilitated imaginative thinking and reflection on future\ngoals. Participants appreciated its adaptability and personalization features,\nthough some noted challenges such as repetitive dialogue and verbose responses.\nOur findings underscore the potential of large language model-based chatbots in\nEFT interventions targeting maladaptive health behaviors."}
{"id": "2506.19548", "pdf": "https://arxiv.org/pdf/2506.19548.pdf", "abs": "https://arxiv.org/abs/2506.19548", "title": "Health Sentinel: An AI Pipeline For Real-time Disease Outbreak Detection", "authors": ["Devesh Pant", "Rishi Raj Grandhe", "Vipin Samaria", "Mukul Paul", "Sudhir Kumar", "Saransh Khanna", "Jatin Agrawal", "Jushaan Singh Kalra", "Akhil VSSG", "Satish V Khalikar", "Vipin Garg", "Himanshu Chauhan", "Pranay Verma", "Neha Khandelwal", "Soma S Dhavala", "Minesh Mathew"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Early detection of disease outbreaks is crucial to ensure timely intervention\nby the health authorities. Due to the challenges associated with traditional\nindicator-based surveillance, monitoring informal sources such as online media\nhas become increasingly popular. However, owing to the number of online\narticles getting published everyday, manual screening of the articles is\nimpractical. To address this, we propose Health Sentinel. It is a multi-stage\ninformation extraction pipeline that uses a combination of ML and non-ML\nmethods to extract events-structured information concerning disease outbreaks\nor other unusual health events-from online articles. The extracted events are\nmade available to the Media Scanning and Verification Cell (MSVC) at the\nNational Centre for Disease Control (NCDC), Delhi for analysis, interpretation\nand further dissemination to local agencies for timely intervention. From April\n2022 till date, Health Sentinel has processed over 300 million news articles\nand identified over 95,000 unique health events across India of which over\n3,500 events were shortlisted by the public health experts at NCDC as potential\noutbreaks."}
{"id": "2506.08892", "pdf": "https://arxiv.org/pdf/2506.08892.pdf", "abs": "https://arxiv.org/abs/2506.08892", "title": "Help or Hindrance: Understanding the Impact of Robot Communication in Action Teams", "authors": ["Tauhid Tanjim", "Jonathan St. George", "Kevin Ching", "Angelique Taylor"], "categories": ["cs.HC", "cs.RO"], "comment": "This is the author's original submitted version of the paper accepted\n  to the 2025 IEEE International Conference on Robot and Human Interactive\n  Communication (RO-MAN). \\c{opyright} 2025 IEEE. Personal use of this material\n  is permitted. For any other use, please contact IEEE", "summary": "The human-robot interaction (HRI) field has recognized the importance of\nenabling robots to interact with teams. Human teams rely on effective\ncommunication for successful collaboration in time-sensitive environments.\nRobots can play a role in enhancing team coordination through real-time\nassistance. Despite significant progress in human-robot teaming research, there\nremains an essential gap in how robots can effectively communicate with action\nteams using multimodal interaction cues in time-sensitive environments. This\nstudy addresses this knowledge gap in an experimental in-lab study to\ninvestigate how multimodal robot communication in action teams affects workload\nand human perception of robots. We explore team collaboration in a medical\ntraining scenario where a robotic crash cart (RCC) provides verbal and\nnon-verbal cues to help users remember to perform iterative tasks and search\nfor supplies. Our findings show that verbal cues for object search tasks and\nvisual cues for task reminders reduce team workload and increase perceived ease\nof use and perceived usefulness more effectively than a robot with no feedback.\nOur work contributes to multimodal interaction research in the HRI field,\nhighlighting the need for more human-robot teaming research to understand best\npractices for integrating collaborative robots in time-sensitive environments\nsuch as in hospitals, search and rescue, and manufacturing applications."}
{"id": "2506.19549", "pdf": "https://arxiv.org/pdf/2506.19549.pdf", "abs": "https://arxiv.org/abs/2506.19549", "title": "RCStat: A Statistical Framework for using Relative Contextualization in Transformers", "authors": ["Debabrata Mahapatra", "Shubham Agarwal", "Apoorv Saxena", "Subrata Mitra"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Prior work on input-token importance in auto-regressive transformers has\nrelied on Softmax-normalized attention weights, which obscure the richer\nstructure of pre-Softmax query-key logits. We introduce RCStat, a statistical\nframework that harnesses raw attention logits via Relative Contextualization\n(RC), a random variable measuring contextual alignment between token segments,\nand derive an efficient upper bound for RC. We demonstrate two applications:\n(i) Key-Value compression, where RC-based thresholds drive adaptive key-value\neviction for substantial cache reduction with minimal quality loss; and (ii)\nAttribution, where RC yields higher-fidelity token-, sentence-, and chunk-level\nexplanations than post-Softmax methods. Across question answering,\nsummarization, and attribution benchmarks, RCStat achieves significant\nempirical gains, delivering state-of-the-art compression and attribution\nperformance without any model retraining."}
{"id": "2506.14677", "pdf": "https://arxiv.org/pdf/2506.14677.pdf", "abs": "https://arxiv.org/abs/2506.14677", "title": "Human-Centered Editable Speech-to-Sign-Language Generation via Streaming Conformer-Transformer and Resampling Hook", "authors": ["Yingchao Li"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Existing end-to-end sign-language animation systems suffer from low\nnaturalness, limited facial/body expressivity, and no user control. We propose\na human-centered, real-time speech-to-sign animation framework that integrates\n(1) a streaming Conformer encoder with an autoregressive Transformer-MDN\ndecoder for synchronized upper-body and facial motion generation, (2) a\ntransparent, editable JSON intermediate representation empowering deaf users\nand experts to inspect and modify each sign segment, and (3) a\nhuman-in-the-loop optimization loop that refines the model based on user edits\nand ratings. Deployed on Unity3D, our system achieves a 13 ms average\nframe-inference time and a 103 ms end-to-end latency on an RTX 4070. Our key\ncontributions include the design of a JSON-centric editing mechanism for\nfine-grained sign-level personalization and the first application of an\nMDN-based feedback loop for continuous model adaptation. This combination\nestablishes a generalizable, explainable AI paradigm for user-adaptive,\nlow-latency multimodal systems. In studies with 20 deaf signers and 5\nprofessional interpreters, we observe a +13 point SUS improvement, 6.7 point\nreduction in cognitive load, and significant gains in naturalness and trust (p\n$<$ .001) over baselines. This work establishes a scalable, explainable AI\nparadigm for accessible sign-language technologies."}
{"id": "2506.19571", "pdf": "https://arxiv.org/pdf/2506.19571.pdf", "abs": "https://arxiv.org/abs/2506.19571", "title": "Has Machine Translation Evaluation Achieved Human Parity? The Human Reference and the Limits of Progress", "authors": ["Lorenzo Proietti", "Stefano Perrella", "Roberto Navigli"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ACL 2025 Main Conference. 24 pages", "summary": "In Machine Translation (MT) evaluation, metric performance is assessed based\non agreement with human judgments. In recent years, automatic metrics have\ndemonstrated increasingly high levels of agreement with humans. To gain a\nclearer understanding of metric performance and establish an upper bound, we\nincorporate human baselines in the MT meta-evaluation, that is, the assessment\nof MT metrics' capabilities. Our results show that human annotators are not\nconsistently superior to automatic metrics, with state-of-the-art metrics often\nranking on par with or higher than human baselines. Despite these findings\nsuggesting human parity, we discuss several reasons for caution. Finally, we\nexplore the broader implications of our results for the research field, asking:\nCan we still reliably measure improvements in MT evaluation? With this work, we\naim to shed light on the limits of our ability to measure progress in the\nfield, fostering discussion on an issue that we believe is crucial to the\nentire MT evaluation community."}
{"id": "2506.15525", "pdf": "https://arxiv.org/pdf/2506.15525.pdf", "abs": "https://arxiv.org/abs/2506.15525", "title": "\"How can we learn and use AI at the same time?\": Participatory Design of GenAI with High School Students", "authors": ["Isabella Pu", "Prerna Ravi", "Linh Dieu Dinh", "Chelsea Joe", "Caitlin Ogoe", "Zixuan Li", "Cynthia Breazeal", "Anastasia K. Ostrowski"], "categories": ["cs.HC"], "comment": "Copyright protected by ACM, 17 pages, 5 figures, 2 tables, in\n  proceedings of 24th annual ACM Interaction Design and Children Conference\n  (IDC 2025)", "summary": "As generative AI (GenAI) emerges as a transformative force, clear\nunderstanding of high school students' perspectives is essential for GenAI's\nmeaningful integration in high school environments. In this work, we draw\ninsights from a participatory design workshop where we engaged 17 high school\nstudents -- a group rarely involved in prior research in this area -- through\nthe design of novel GenAI tools and school policies addressing their key\nconcerns. Students identified challenges and developed solutions outlining\ntheir ideal features in GenAI tools, appropriate school use, and regulations.\nThese centered around the problem spaces of combating bias & misinformation,\ntackling crime & plagiarism, preventing over-reliance on AI, and handling false\naccusations of academic dishonesty. Building on our participants'\nunderrepresented perspectives, we propose new guidelines targeted at\neducational technology designers for development of GenAI technologies in high\nschools. We also argue for further incorporation of student voices in\ndevelopment of AI policies in their schools."}
{"id": "2506.19599", "pdf": "https://arxiv.org/pdf/2506.19599.pdf", "abs": "https://arxiv.org/abs/2506.19599", "title": "ECCoT: A Framework for Enhancing Effective Cognition via Chain of Thought in Large Language Model", "authors": ["Zhenke Duan", "Jiqun Pan", "Jiani Tu", "Xiaoyi Wang", "Yanqing Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In the era of large-scale artificial intelligence, Large Language Models\n(LLMs) have made significant strides in natural language processing. However,\nthey often lack transparency and generate unreliable outputs, raising concerns\nabout their interpretability. To address this, the Chain of Thought (CoT)\nprompting method structures reasoning into step-by-step deductions. Yet, not\nall reasoning chains are valid, and errors can lead to unreliable conclusions.\nWe propose ECCoT, an End-to-End Cognitive Chain of Thought Validation\nFramework, to evaluate and refine reasoning chains in LLMs. ECCoT integrates\nthe Markov Random Field-Embedded Topic Model (MRF-ETM) for topic-aware CoT\ngeneration and Causal Sentence-BERT (CSBert) for causal reasoning alignment. By\nfiltering ineffective chains using structured ordering statistics, ECCoT\nimproves interpretability, reduces biases, and enhances the trustworthiness of\nLLM-based decision-making. Key contributions include the introduction of ECCoT,\nMRF-ETM for topic-driven CoT generation, and CSBert for causal reasoning\nenhancement. Code is released at: https://github.com/erwinmsmith/ECCoT.git."}
{"id": "2506.18269", "pdf": "https://arxiv.org/pdf/2506.18269.pdf", "abs": "https://arxiv.org/abs/2506.18269", "title": "Co-persona: Leveraging LLMs and Expert Collaboration to Understand User Personas through Social Media Data Analysis", "authors": ["Min Yin", "Haoyu Liu", "Boyi Lian", "Chunlei Chai"], "categories": ["cs.HC"], "comment": "17pages,5figures,8tables", "summary": "This study introduces Co-Persona, a methodological framework bridging\nlarge-scale social media analysis with authentic user understanding through\nsystematic integration of Large Language Models and expert validation. Through\na case study of B.Co, a Chinese manufacturer, we investigated Co-Persona\napplication in bedside lamp development. Our methodology analyzed over 38\nmillion posts from Xiao Hongshu, employing multi-stage data processing\ncombining advanced NLP with expert validation. Analysis revealed five user\npersonas derived from bedtime behaviors: Health Aficionados, Night Owls,\nInterior Decorators, Child-care Workers, and Workaholics-each showing unique\npre-sleep activities and product preferences. Findings demonstrate Co-Persona\nenhances manufacturers' ability to process large datasets while maintaining\nuser understanding. The methodology provides structured approaches for targeted\nmarketing and product strategies. Research contributes to theoretical\nunderstanding of data-driven persona development and practical applications in\nconsumer-driven innovation. Code and data available at\nhttps://github.com/INFPa/LLMwithPersona."}
{"id": "2506.19603", "pdf": "https://arxiv.org/pdf/2506.19603.pdf", "abs": "https://arxiv.org/abs/2506.19603", "title": "Social Hatred: Efficient Multimodal Detection of Hatemongers", "authors": ["Tom Marzea", "Abraham Israeli", "Oren Tsur"], "categories": ["cs.CL", "cs.SI"], "comment": "To be published in WOAH, July 2025. arXiv admin note: text overlap\n  with arXiv:2409.14464", "summary": "Automatic detection of online hate speech serves as a crucial step in the\ndetoxification of the online discourse. Moreover, accurate classification can\npromote a better understanding of the proliferation of hate as a social\nphenomenon. While most prior work focus on the detection of hateful utterances,\nwe argue that focusing on the user level is as important, albeit challenging.\nIn this paper we consider a multimodal aggregative approach for the detection\nof hate-mongers, taking into account the potentially hateful texts, user\nactivity, and the user network. Evaluating our method on three unique datasets\nX (Twitter), Gab, and Parler we show that processing a user's texts in her\nsocial context significantly improves the detection of hate mongers, compared\nto previously used text and graph-based methods. We offer comprehensive set of\nresults obtained in different experimental settings as well as qualitative\nanalysis of illustrative cases. Our method can be used to improve the\nclassification of coded messages, dog-whistling, and racial gas-lighting, as\nwell as to inform intervention measures. Moreover, we demonstrate that our\nmultimodal approach performs well across very different content platforms and\nover large datasets and networks."}
{"id": "2407.02157", "pdf": "https://arxiv.org/pdf/2407.02157.pdf", "abs": "https://arxiv.org/abs/2407.02157", "title": "FineCLIPER: Multi-modal Fine-grained CLIP for Dynamic Facial Expression Recognition with AdaptERs", "authors": ["Haodong Chen", "Haojian Huang", "Junhao Dong", "Mingzhe Zheng", "Dian Shao"], "categories": ["cs.CV", "cs.HC"], "comment": "Accepted to ACM MM 2024", "summary": "Dynamic Facial Expression Recognition (DFER) is crucial for understanding\nhuman behavior. However, current methods exhibit limited performance mainly due\nto the scarcity of high-quality data, the insufficient utilization of facial\ndynamics, and the ambiguity of expression semantics, etc. To this end, we\npropose a novel framework, named Multi-modal Fine-grained CLIP for Dynamic\nFacial Expression Recognition with AdaptERs (FineCLIPER), incorporating the\nfollowing novel designs: 1) To better distinguish between similar facial\nexpressions, we extend the class labels to textual descriptions from both\npositive and negative aspects, and obtain supervision by calculating the\ncross-modal similarity based on the CLIP model; 2) Our FineCLIPER adopts a\nhierarchical manner to effectively mine useful cues from DFE videos.\nSpecifically, besides directly embedding video frames as input (low semantic\nlevel), we propose to extract the face segmentation masks and landmarks based\non each frame (middle semantic level) and utilize the Multi-modal Large\nLanguage Model (MLLM) to further generate detailed descriptions of facial\nchanges across frames with designed prompts (high semantic level).\nAdditionally, we also adopt Parameter-Efficient Fine-Tuning (PEFT) to enable\nefficient adaptation of large pre-trained models (i.e., CLIP) for this task.\nOur FineCLIPER achieves SOTA performance on the DFEW, FERV39k, and MAFW\ndatasets in both supervised and zero-shot settings with few tunable parameters.\nProject Page: https://haroldchen19.github.io/FineCLIPER-Page/"}
{"id": "2506.19607", "pdf": "https://arxiv.org/pdf/2506.19607.pdf", "abs": "https://arxiv.org/abs/2506.19607", "title": "Correcting Hallucinations in News Summaries: Exploration of Self-Correcting LLM Methods with External Knowledge", "authors": ["Juraj Vladika", "Ihsan Soydemir", "Florian Matthes"], "categories": ["cs.CL"], "comment": "Accepted to FEVER @ ACL 2025", "summary": "While large language models (LLMs) have shown remarkable capabilities to\ngenerate coherent text, they suffer from the issue of hallucinations --\nfactually inaccurate statements. Among numerous approaches to tackle\nhallucinations, especially promising are the self-correcting methods. They\nleverage the multi-turn nature of LLMs to iteratively generate verification\nquestions inquiring additional evidence, answer them with internal or external\nknowledge, and use that to refine the original response with the new\ncorrections. These methods have been explored for encyclopedic generation, but\nless so for domains like news summarization. In this work, we investigate two\nstate-of-the-art self-correcting systems by applying them to correct\nhallucinated summaries using evidence from three search engines. We analyze the\nresults and provide insights into systems' performance, revealing interesting\npractical findings on the benefits of search engine snippets and few-shot\nprompts, as well as high alignment of G-Eval and human evaluation."}
{"id": "2410.23478", "pdf": "https://arxiv.org/pdf/2410.23478.pdf", "abs": "https://arxiv.org/abs/2410.23478", "title": "Collage: Decomposable Rapid Prototyping for Information Extraction on Scientific PDFs", "authors": ["Sireesh Gururaja", "Yueheng Zhang", "Guannan Tang", "Tianhao Zhang", "Kevin Murphy", "Yu-Tsen Yi", "Junwon Seo", "Anthony Rollett", "Emma Strubell"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Recent years in NLP have seen the continued development of domain-specific\ninformation extraction tools for scientific documents, alongside the release of\nincreasingly multimodal pretrained transformer models. While the opportunity\nfor scientists outside of NLP to evaluate and apply such systems to their own\ndomains has never been clearer, these models are difficult to compare: they\naccept different input formats, are often black-box and give little insight\ninto processing failures, and rarely handle PDF documents, the most common\nformat of scientific publication. In this work, we present Collage, a tool\ndesigned for rapid prototyping, visualization, and evaluation of different\ninformation extraction models on scientific PDFs. Collage allows the use and\nevaluation of any HuggingFace token classifier, several LLMs, and multiple\nother task-specific models out of the box, and provides extensible software\ninterfaces to accelerate experimentation with new models. Further, we enable\nboth developers and users of NLP-based tools to inspect, debug, and better\nunderstand modeling pipelines by providing granular views of intermediate\nstates of processing. We demonstrate our system in the context of information\nextraction to assist with literature review in materials science."}
{"id": "2506.19652", "pdf": "https://arxiv.org/pdf/2506.19652.pdf", "abs": "https://arxiv.org/abs/2506.19652", "title": "Tailored Conversations beyond LLMs: A RL-Based Dialogue Manager", "authors": ["Lucie Galland", "Catherine Pelachaud", "Florian Pecune"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this work, we propose a novel framework that integrates large language\nmodels (LLMs) with an RL-based dialogue manager for open-ended dialogue with a\nspecific goal. By leveraging hierarchical reinforcement learning to model the\nstructured phases of dialogue and employ meta-learning to enhance adaptability\nacross diverse user profiles, our approach enhances adaptability and\nefficiency, enabling the system to learn from limited data, transition fluidly\nbetween dialogue phases, and personalize responses to heterogeneous patient\nneeds. We apply our framework to Motivational Interviews, aiming to foster\nbehavior change, and demonstrate that the proposed dialogue manager outperforms\na state-of-the-art LLM baseline in terms of reward, showing a potential benefit\nof conditioning LLMs to create open-ended dialogue systems with specific goals."}
{"id": "2412.00814", "pdf": "https://arxiv.org/pdf/2412.00814.pdf", "abs": "https://arxiv.org/abs/2412.00814", "title": "VR-Doh: Hands-on 3D Modeling in Virtual Reality", "authors": ["Zhaofeng Luo", "Zhitong Cui", "Shijian Luo", "Mengyu Chu", "Minchen Li"], "categories": ["cs.GR", "cs.HC"], "comment": "12 pages", "summary": "We introduce VR-Doh, an open-source, hands-on 3D modeling system that enables\nintuitive creation and manipulation of elastoplastic objects in Virtual Reality\n(VR). By customizing the Material Point Method (MPM) for real-time simulation\nof hand-induced large deformations and enhancing 3D Gaussian Splatting for\nseamless rendering, VR-Doh provides an interactive and immersive 3D modeling\nexperience. Users can naturally sculpt, deform, and edit objects through both\ncontact- and gesture-based hand-object interactions. To achieve real-time\nperformance, our system incorporates localized simulation techniques,\nparticle-level collision handling, and the decoupling of physical and\nappearance representations, ensuring smooth and responsive interactions. VR-Doh\nsupports both object creation and editing, enabling diverse modeling tasks such\nas designing food items, characters, and interlocking structures, all resulting\nin simulation-ready assets. User studies with both novice and experienced\nparticipants highlight the system's intuitive design, immersive feedback, and\ncreative potential. Compared to existing geometric modeling tools, VR-Doh\noffers enhanced accessibility and natural interaction, making it a powerful\ntool for creative exploration in VR."}
{"id": "2506.19733", "pdf": "https://arxiv.org/pdf/2506.19733.pdf", "abs": "https://arxiv.org/abs/2506.19733", "title": "Breaking Barriers: Do Reinforcement Post Training Gains Transfer To Unseen Domains?", "authors": ["Chuxuan Hu", "Yuxuan Zhu", "Antony Kellermann", "Caleb Biddulph", "Suppakit Waiwitlikhit", "Jason Benn", "Daniel Kang"], "categories": ["cs.CL"], "comment": "9 pages, 4 figures, 2 tables", "summary": "Reinforcement post training (RPT) has recently shown promise in improving the\nreasoning abilities of large language models (LLMs). However, it remains\nunclear how well these improvements generalize to new domains, as prior work\nevaluates RPT models on data from the same domains used for fine-tuning. To\nunderstand the generalizability of RPT, we conduct two studies. (1)\nObservational: We compare a wide range of open-weight RPT models against their\ncorresponding base models across multiple domains, including both seen and\nunseen domains in their fine-tuning data. (2) Interventional: we fine-tune LLMs\nwith RPT on single domains and evaluate their performance across multiple\ndomains. Both studies converge on the same conclusion that, although RPT brings\nsubstantial gains on tasks similar to the fine-tuning data, the gains\ngeneralize inconsistently and can vanish on domains with different reasoning\npatterns."}
{"id": "2506.08890", "pdf": "https://arxiv.org/pdf/2506.08890.pdf", "abs": "https://arxiv.org/abs/2506.08890", "title": "Human-Robot Teaming Field Deployments: A Comparison Between Verbal and Non-verbal Communication", "authors": ["Tauhid Tanjim", "Promise Ekpo", "Huajie Cao", "Jonathan St. George", "Kevin Ching", "Hee Rin Lee", "Angelique Taylor"], "categories": ["cs.RO", "cs.HC"], "comment": "This is the author's original submitted version of the paper accepted\n  to the 2025 IEEE International Conference on Robot and Human Interactive\n  Communication (RO-MAN). \\c{opyright} 2025 IEEE. Personal use of this material\n  is permitted. For any other use, please contact IEEE", "summary": "Healthcare workers (HCWs) encounter challenges in hospitals, such as\nretrieving medical supplies quickly from crash carts, which could potentially\nresult in medical errors and delays in patient care. Robotic crash carts (RCCs)\nhave shown promise in assisting healthcare teams during medical tasks through\nguided object searches and task reminders. Limited exploration has been done to\ndetermine what communication modalities are most effective and least disruptive\nto patient care in real-world settings. To address this gap, we conducted a\nbetween-subjects experiment comparing the RCC's verbal and non-verbal\ncommunication of object search with a standard crash cart in resuscitation\nscenarios to understand the impact of robot communication on workload and\nattitudes toward using robots in the workplace. Our findings indicate that\nverbal communication significantly reduced mental demand and effort compared to\nvisual cues and with a traditional crash cart. Although frustration levels were\nslightly higher during collaborations with the robot compared to a traditional\ncart, these research insights provide valuable implications for human-robot\nteamwork in high-stakes environments."}
{"id": "2506.19750", "pdf": "https://arxiv.org/pdf/2506.19750.pdf", "abs": "https://arxiv.org/abs/2506.19750", "title": "Evaluating Rare Disease Diagnostic Performance in Symptom Checkers: A Synthetic Vignette Simulation Approach", "authors": ["Takashi Nishibayashi", "Seiji Kanazawa", "Kumpei Yamada"], "categories": ["cs.CL"], "comment": null, "summary": "Background: Symptom Checkers (SCs) provide users with personalized medical\ninformation. To prevent performance degradation from algorithm updates, SC\ndevelopers must evaluate diagnostic performance changes for individual diseases\nbefore deployment. However, acquiring sufficient evaluation data for rare\ndiseases is difficult, and manually creating numerous clinical vignettes is\ncostly and impractical. Objective: This study proposes and validates a novel\nSynthetic Vignette Simulation Approach to evaluate diagnostic performance\nchanges for individual rare diseases following SC algorithm updates. Methods:\nWe used disease-phenotype annotations from the Human Phenotype Ontology (HPO),\na knowledge database for rare diseases, to generate synthetic vignettes. With\nthese, we simulated SC interviews to estimate the impact of algorithm updates\non real-world diagnostic performance. The method's effectiveness was evaluated\nretrospectively by comparing estimated values with actual metric changes using\nthe R 2(R-squared) coefficient. Results: The experiment included eight past SC\nalgorithm updates. For updates on diseases with frequency information in HPO\n(n=5), the R^2 for recall@8 change was 0.831 (p=0.031), and for precision@8\nchange, it was 0.78 (p=0.047), indicating the method can predict\npost-deployment performance. In contrast, large prediction errors occurred for\ndiseases without frequency information (n=3), highlighting its importance. The\nmanual effort to map HPO phenotypes to SC symptoms was approximately 2 hours\nper disease. Conclusions: Our method enables pre-deployment evaluation of SC\nalgorithm changes for individual rare diseases using a publicly available,\nexpert-created knowledge base. This transparent and low-cost approach allows\ndevelopers to efficiently improve diagnostic performance for rare diseases,\npotentially enhancing support for early diagnosis."}
{"id": "2506.09160", "pdf": "https://arxiv.org/pdf/2506.09160.pdf", "abs": "https://arxiv.org/abs/2506.09160", "title": "Understanding Human-AI Trust in Education", "authors": ["Griffin Pitts", "Sanaz Motamedi"], "categories": ["cs.CY", "cs.AI", "cs.ET", "cs.HC"], "comment": null, "summary": "As AI chatbots become increasingly integrated in education, students are\nturning to these systems for guidance, feedback, and information. However, the\nanthropomorphic characteristics of these chatbots create ambiguity regarding\nwhether students develop trust toward them as they would a human peer or\ninstructor, based in interpersonal trust, or as they would any other piece of\ntechnology, based in technology trust. This ambiguity presents theoretical\nchallenges, as interpersonal trust models may inappropriately ascribe human\nintentionality and morality to AI, while technology trust models were developed\nfor non-social technologies, leaving their applicability to anthropomorphic\nsystems unclear. To address this gap, we investigate how human-like and\nsystem-like trusting beliefs comparatively influence students' perceived\nenjoyment, trusting intention, behavioral intention to use, and perceived\nusefulness of an AI chatbot - factors associated with students' engagement and\nlearning outcomes. Through partial least squares structural equation modeling,\nwe found that human-like and system-like trust significantly influenced student\nperceptions, with varied effects. Human-like trust more strongly predicted\ntrusting intention, while system-like trust better predicted behavioral\nintention and perceived usefulness. Both had similar effects on perceived\nenjoyment. Given the partial explanatory power of each type of trust, we\npropose that students develop a distinct form of trust with AI chatbots\n(human-AI trust) that differs from human-human and human-technology models of\ntrust. Our findings highlight the need for new theoretical frameworks specific\nto human-AI trust and offer practical insights for fostering appropriately\ncalibrated trust, which is critical for the effective adoption and pedagogical\nimpact of AI in education."}
{"id": "2506.19753", "pdf": "https://arxiv.org/pdf/2506.19753.pdf", "abs": "https://arxiv.org/abs/2506.19753", "title": "Arabic Dialect Classification using RNNs, Transformers, and Large Language Models: A Comparative Analysis", "authors": ["Omar A. Essameldin", "Ali O. Elbeih", "Wael H. Gomaa", "Wael F. Elsersy"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The Arabic language is among the most popular languages in the world with a\nhuge variety of dialects spoken in 22 countries. In this study, we address the\nproblem of classifying 18 Arabic dialects of the QADI dataset of Arabic tweets.\nRNN models, Transformer models, and large language models (LLMs) via prompt\nengineering are created and tested. Among these, MARBERTv2 performed best with\n65% accuracy and 64% F1-score. Through the use of state-of-the-art\npreprocessing techniques and the latest NLP models, this paper identifies the\nmost significant linguistic issues in Arabic dialect identification. The\nresults corroborate applications like personalized chatbots that respond in\nusers' dialects, social media monitoring, and greater accessibility for Arabic\ncommunities."}
{"id": "2506.17364", "pdf": "https://arxiv.org/pdf/2506.17364.pdf", "abs": "https://arxiv.org/abs/2506.17364", "title": "AI-based Multimodal Biometrics for Detecting Smartphone Distractions: Application to Online Learning", "authors": ["Alvaro Becerra", "Roberto Daza", "Ruth Cobos", "Aythami Morales", "Mutlu Cukurova", "Julian Fierrez"], "categories": ["cs.CY", "cs.AI", "cs.CV", "cs.HC"], "comment": "Accepted in EC-TEL25: 20th European Conference on Technology Enhanced\n  Learning, Newcastle and Durham, UK, 15-19 September 2025", "summary": "This work investigates the use of multimodal biometrics to detect\ndistractions caused by smartphone use during tasks that require sustained\nattention, with a focus on computer-based online learning. Although the methods\nare applicable to various domains, such as autonomous driving, we concentrate\non the challenges learners face in maintaining engagement amid internal (e.g.,\nmotivation), system-related (e.g., course design) and contextual (e.g.,\nsmartphone use) factors. Traditional learning platforms often lack detailed\nbehavioral data, but Multimodal Learning Analytics (MMLA) and biosensors\nprovide new insights into learner attention. We propose an AI-based approach\nthat leverages physiological signals and head pose data to detect phone use.\nOur results show that single biometric signals, such as brain waves or heart\nrate, offer limited accuracy, while head pose alone achieves 87%. A multimodal\nmodel combining all signals reaches 91% accuracy, highlighting the benefits of\nintegration. We conclude by discussing the implications and limitations of\ndeploying these models for real-time support in online learning environments."}
{"id": "2506.19761", "pdf": "https://arxiv.org/pdf/2506.19761.pdf", "abs": "https://arxiv.org/abs/2506.19761", "title": "Accurate, fast, cheap: Choose three. Replacing Multi-Head-Attention with Bidirectional Recurrent Attention for Long-Form ASR", "authors": ["Martin Ratajczak", "Jean-Philippe Robichaud", "Jennifer Drexler Fox"], "categories": ["cs.CL"], "comment": "Accepted to Interspeech 2025", "summary": "Long-form speech recognition is an application area of increasing research\nfocus. ASR models based on multi-head attention (MHA) are ill-suited to\nlong-form ASR because of their quadratic complexity in sequence length. We\nbuild on recent work that has investigated linear complexity recurrent\nattention (RA) layers for ASR. We find that bidirectional RA layers can match\nthe accuracy of MHA for both short- and long-form applications. We present a\nstrong limited-context attention (LCA) baseline, and show that RA layers are\njust as accurate while being more efficient. We develop a long-form training\nparadigm which further improves RA performance, leading to better accuracy than\nLCA with 44% higher throughput. We also present Direction Dropout, a novel\nregularization method that improves accuracy, provides fine-grained control of\nthe accuracy/throughput trade-off of bidirectional RA, and enables a new\nalternating directions decoding mode with even higher throughput."}
{"id": "2506.17570", "pdf": "https://arxiv.org/pdf/2506.17570.pdf", "abs": "https://arxiv.org/abs/2506.17570", "title": "VReaves: Eavesdropping on Virtual Reality App Identity and Activity via Electromagnetic Side Channels", "authors": ["Wei Sun", "Minghong Fang", "Mengyuan Li"], "categories": ["cs.NI", "cs.CR", "cs.HC"], "comment": null, "summary": "Virtual reality (VR) has recently proliferated significantly, consisting of\nheadsets or head-mounted displays (HMDs) and hand controllers for an embodied\nand immersive experience. The VR device is usually embedded with different\nkinds of IoT sensors, such as cameras, microphones, communication sensors, etc.\nHowever, VR security has not been scrutinized from a physical hardware point of\nview, especially electromagnetic emanations (EM) that are automatically and\nunintentionally emitted from the VR headset. This paper presents VReaves, a\nsystem that can eavesdrop on the electromagnetic emanation side channel of a VR\nheadset for VR app identification and activity recognition. To do so, we first\ncharacterize the electromagnetic emanations from the embedded IoT sensors\n(e.g., cameras and microphones) in the VR headset through a signal processing\npipeline and further propose machine learning models to identify the VR app and\nrecognize the VR app activities. Our experimental evaluation with commercial\noff-the-shelf VR devices demonstrates the efficiency of VR app identification\nand activity recognition via electromagnetic emanation side channel."}
{"id": "2506.19767", "pdf": "https://arxiv.org/pdf/2506.19767.pdf", "abs": "https://arxiv.org/abs/2506.19767", "title": "SRFT: A Single-Stage Method with Supervised and Reinforcement Fine-Tuning for Reasoning", "authors": ["Yuqian Fu", "Tinghong Chen", "Jiajun Chai", "Xihuai Wang", "Songjun Tu", "Guojun Yin", "Wei Lin", "Qichao Zhang", "Yuanheng Zhu", "Dongbin Zhao"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have achieved remarkable progress in reasoning\ntasks, yet the optimal integration of Supervised Fine-Tuning (SFT) and\nReinforcement Learning (RL) remains a fundamental challenge. Through\ncomprehensive analysis of token distributions, learning dynamics, and\nintegration mechanisms from entropy-based perspectives, we reveal key\ndifferences between these paradigms: SFT induces coarse-grained global changes\nto LLM policy distributions, while RL performs fine-grained selective\noptimizations, with entropy serving as a critical indicator of training\neffectiveness. Building on these observations, we propose Supervised\nReinforcement Fine-Tuning (SRFT), a single-stage method that unifies both\nfine-tuning paradigms through entropy-aware weighting mechanisms. Our approach\nsimultaneously applies SFT and RL to directly optimize the LLM using\ndemonstrations and self-exploration rollouts rather than through two-stage\nsequential methods. Extensive experiments show that SRFT achieves 59.1% average\naccuracy, outperforming zero-RL methods by 9.0% on five mathematical reasoning\nbenchmarks and 10.9% on three out-of-distribution benchmarks."}
{"id": "2506.19794", "pdf": "https://arxiv.org/pdf/2506.19794.pdf", "abs": "https://arxiv.org/abs/2506.19794", "title": "Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic Empirical Study", "authors": ["Yuqi Zhu", "Yi Zhong", "Jintian Zhang", "Ziheng Zhang", "Shuofei Qiao", "Yujie Luo", "Lun Du", "Da Zheng", "Huajun Chen", "Ningyu Zhang"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "cs.MA"], "comment": "Work in progress", "summary": "Large Language Models (LLMs) hold promise in automating data analysis tasks,\nyet open-source models face significant limitations in these kinds of\nreasoning-intensive scenarios. In this work, we investigate strategies to\nenhance the data analysis capabilities of open-source LLMs. By curating a seed\ndataset of diverse, realistic scenarios, we evaluate models across three\ndimensions: data understanding, code generation, and strategic planning. Our\nanalysis reveals three key findings: (1) Strategic planning quality serves as\nthe primary determinant of model performance; (2) Interaction design and task\ncomplexity significantly influence reasoning capabilities; (3) Data quality\ndemonstrates a greater impact than diversity in achieving optimal performance.\nWe leverage these insights to develop a data synthesis methodology,\ndemonstrating significant improvements in open-source LLMs' analytical\nreasoning capabilities."}
{"id": "2506.19831", "pdf": "https://arxiv.org/pdf/2506.19831.pdf", "abs": "https://arxiv.org/abs/2506.19831", "title": "How Effectively Can BERT Models Interpret Context and Detect Bengali Communal Violent Text?", "authors": ["Abdullah Khondoker", "Enam Ahmed Taufik", "Md. Iftekhar Islam Tashik", "S M Ishtiak Mahmud", "Farig Sadeque"], "categories": ["cs.CL"], "comment": null, "summary": "The spread of cyber hatred has led to communal violence, fueling aggression\nand conflicts between various religious, ethnic, and social groups, posing a\nsignificant threat to social harmony. Despite its critical importance, the\nclassification of communal violent text remains an underexplored area in\nexisting research. This study aims to enhance the accuracy of detecting text\nthat incites communal violence, focusing specifically on Bengali textual data\nsourced from social media platforms. We introduce a fine-tuned BanglaBERT model\ntailored for this task, achieving a macro F1 score of 0.60. To address the\nissue of data imbalance, our dataset was expanded by adding 1,794 instances,\nwhich facilitated the development and evaluation of a fine-tuned ensemble\nmodel. This ensemble model demonstrated an improved performance, achieving a\nmacro F1 score of 0.63, thus highlighting its effectiveness in this domain. In\naddition to quantitative performance metrics, qualitative analysis revealed\ninstances where the models struggled with context understanding, leading to\noccasional misclassifications, even when predictions were made with high\nconfidence. Through analyzing the cosine similarity between words, we\nidentified certain limitations in the pre-trained BanglaBERT models,\nparticularly in their ability to distinguish between closely related communal\nand non-communal terms. To further interpret the model's decisions, we applied\nLIME, which helped to uncover specific areas where the model struggled in\nunderstanding context, contributing to errors in classification. These findings\nhighlight the promise of NLP and interpretability tools in reducing online\ncommunal violence. Our work contributes to the growing body of research in\ncommunal violence detection and offers a foundation for future studies aiming\nto refine these techniques for better accuracy and societal impact."}
{"id": "2506.19835", "pdf": "https://arxiv.org/pdf/2506.19835.pdf", "abs": "https://arxiv.org/abs/2506.19835", "title": "MAM: Modular Multi-Agent Framework for Multi-Modal Medical Diagnosis via Role-Specialized Collaboration", "authors": ["Yucheng Zhou", "Lingran Song", "Jianbing Shen"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Recent advancements in medical Large Language Models (LLMs) have showcased\ntheir powerful reasoning and diagnostic capabilities. Despite their success,\ncurrent unified multimodal medical LLMs face limitations in knowledge update\ncosts, comprehensiveness, and flexibility. To address these challenges, we\nintroduce the Modular Multi-Agent Framework for Multi-Modal Medical Diagnosis\n(MAM). Inspired by our empirical findings highlighting the benefits of role\nassignment and diagnostic discernment in LLMs, MAM decomposes the medical\ndiagnostic process into specialized roles: a General Practitioner, Specialist\nTeam, Radiologist, Medical Assistant, and Director, each embodied by an\nLLM-based agent. This modular and collaborative framework enables efficient\nknowledge updates and leverages existing medical LLMs and knowledge bases.\nExtensive experimental evaluations conducted on a wide range of publicly\naccessible multimodal medical datasets, incorporating text, image, audio, and\nvideo modalities, demonstrate that MAM consistently surpasses the performance\nof modality-specific LLMs. Notably, MAM achieves significant performance\nimprovements ranging from 18% to 365% compared to baseline models. Our code is\nreleased at https://github.com/yczhou001/MAM."}
{"id": "2506.18923", "pdf": "https://arxiv.org/pdf/2506.18923.pdf", "abs": "https://arxiv.org/abs/2506.18923", "title": "Mix-of-Language-Experts Architecture for Multilingual Programming", "authors": ["Yifan Zong", "Yuntian Deng", "Pengyu Nie"], "categories": ["cs.PL", "cs.CL", "cs.SE"], "comment": "Accepted at LLM4Code @ ICSE 2025", "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\naiding developers with tasks like code comprehension, generation, and\ntranslation. Supporting multilingual programming -- i.e., coding tasks across\nmultiple programming languages -- typically requires either (1) finetuning a\nsingle LLM across all programming languages, which is cost-efficient but\nsacrifices language-specific specialization and performance, or (2) finetuning\nseparate LLMs for each programming language, which allows for specialization\nbut is computationally expensive and storage-intensive due to the duplication\nof parameters. This paper introduces MoLE (Mix-of-Language-Experts), a novel\narchitecture that balances efficiency and specialization for multilingual\nprogramming. MoLE is composed of a base model, a shared LoRA (low-rank\nadaptation) module, and a collection of language-specific LoRA modules. These\nmodules are jointly optimized during the finetuning process, enabling effective\nknowledge sharing and specialization across programming languages. During\ninference, MoLE automatically routes to the language-specific LoRA module\ncorresponding to the programming language of the code token being generated.\nOur experiments demonstrate that MoLE achieves greater parameter efficiency\ncompared to training separate language-specific LoRAs, while outperforming a\nsingle shared LLM finetuned for all programming languages in terms of accuracy."}
{"id": "2506.18945", "pdf": "https://arxiv.org/pdf/2506.18945.pdf", "abs": "https://arxiv.org/abs/2506.18945", "title": "Chain-of-Experts: Unlocking the Communication Power of Mixture-of-Experts Models", "authors": ["Zihan Wang", "Rui Pan", "Jiarui Yao", "Robert Csordas", "Linjie Li", "Lu Yin", "Jiajun Wu", "Tong Zhang", "Manling Li", "Shiwei Liu"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "We propose Chain-of-Experts (CoE), a new Mixture-of-Experts (MoE)\narchitecture that introduces sequential expert communication within each layer.\nUnlike traditional MoE models, where experts operate independently in parallel,\nCoE processes tokens iteratively across a chain of experts inside a layer. To\nsupport dynamic expert selection across iterations, CoE employs a dedicated\nrouter at each iteration step within a layer. This design allows tokens to\nre-evaluate and select different experts during each iteration, rather than\nbeing statically assigned. As a result, CoE introduces a flexible routing\nmechanism that increases the diversity of expert combinations and enriches the\nmodel's representational capacity. CoE demonstrates improved performance under\nfixed compute: on math reasoning tasks, it reduces validation loss from 1.20 to\n1.12 compared to a standard MoE. Beyond performance, CoE offers a new scaling\naxis: depth through expert iteration, which complements conventional\nwidth/depth scaling. For example, using 2x iterations matches the performance\nof 3x expert selections (in width), while reducing memory usage by 17.6-42%\nrelative to other scaling strategies. Our analysis reveals that CoE's benefits\nstem from its iterative residual structure and enhanced expert specialization\nempowered by iterative routing, which together unlock more expressive\nrepresentations. Code is available at https://github.com/ZihanWang314/coe."}
{"id": "2506.18952", "pdf": "https://arxiv.org/pdf/2506.18952.pdf", "abs": "https://arxiv.org/abs/2506.18952", "title": "LLMs on a Budget? Say HOLA", "authors": ["Zohaib Hasan Siddiqui", "Jiechao Gao", "Ebad Shabbir", "Mohammad Anas Azeez", "Rafiq Ali", "Gautam Siddharth Kashyap", "Usman Naseem"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Running Large Language Models (LLMs) on edge devices is constrained by high\ncompute and memory demands posing a barrier for real-time applications in\nsectors like healthcare, education, and embedded systems. Current solutions\nsuch as quantization, pruning, and retrieval-augmented generation (RAG) offer\nonly partial optimizations and often compromise on speed or accuracy. We\nintroduce HOLA, an end-to-end optimization framework for efficient LLM\ndeployment. Internally, it leverages Hierarchical Speculative Decoding (HSD)\nfor faster inference without quality loss. Externally, AdaComp-RAG adjusts\nretrieval complexity based on context needs. Together with LoBi, which blends\nstructured pruning (LoRA) and quantization, HOLA delivers significant gains:\n17.6% EMA on GSM8K, 10.5% MCA on ARC, and reduced latency and memory on edge\ndevices like Jetson Nano--proving both scalable and production-ready."}
{"id": "2506.18957", "pdf": "https://arxiv.org/pdf/2506.18957.pdf", "abs": "https://arxiv.org/abs/2506.18957", "title": "A Comment On \"The Illusion of Thinking\": Reframing the Reasoning Cliff as an Agentic Gap", "authors": ["Sheraz Khan", "Subha Madhavan", "Kannan Natarajan"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "10 pages, 2 figures, Comment on \"The Illusion of Thinking:\n  Understanding the Strengths and Limitations of Reasoning Models via the Lens\n  of Problem Complexity\" (arXiv:2506.06941v1)", "summary": "The recent work by Shojaee et al. (2025), titled The Illusion of Thinking:\nUnderstanding the Strengths and Limitations of Reasoning Models via the Lens of\nProblem Complexity, presents a compelling empirical finding, a reasoning cliff,\nwhere the performance of Large Reasoning Models (LRMs) collapses beyond a\nspecific complexity threshold, which the authors posit as an intrinsic scaling\nlimitation of Chain-of-Thought (CoT) reasoning. This commentary, while\nacknowledging the study's methodological rigor, contends that this conclusion\nis confounded by experimental artifacts. We argue that the observed failure is\nnot evidence of a fundamental cognitive boundary, but rather a predictable\noutcome of system-level constraints in the static, text-only evaluation\nparadigm, including tool use restrictions, context window recall issues, the\nabsence of crucial cognitive baselines, inadequate statistical reporting, and\noutput generation limits. We reframe this performance collapse through the lens\nof an agentic gap, asserting that the models are not failing at reasoning, but\nat execution within a profoundly restrictive interface. We empirically\nsubstantiate this critique by demonstrating a striking reversal. A model,\ninitially declaring a puzzle impossible when confined to text-only generation,\nnow employs agentic tools to not only solve it but also master variations of\ncomplexity far beyond the reasoning cliff it previously failed to surmount.\nAdditionally, our empirical analysis of tool-enabled models like o4-mini and\nGPT-4o reveals a hierarchy of agentic reasoning, from simple procedural\nexecution to complex meta-cognitive self-correction, which has significant\nimplications for how we define and measure machine intelligence. The illusion\nof thinking attributed to LRMs is less a reasoning deficit and more a\nconsequence of an otherwise capable mind lacking the tools for action."}
{"id": "2506.18959", "pdf": "https://arxiv.org/pdf/2506.18959.pdf", "abs": "https://arxiv.org/abs/2506.18959", "title": "From Web Search towards Agentic Deep Research: Incentivizing Search with Reasoning Agents", "authors": ["Weizhi Zhang", "Yangning Li", "Yuanchen Bei", "Junyu Luo", "Guancheng Wan", "Liangwei Yang", "Chenxuan Xie", "Yuyao Yang", "Wei-Chieh Huang", "Chunyu Miao", "Henry Peng Zou", "Xiao Luo", "Yusheng Zhao", "Yankai Chen", "Chunkit Chan", "Peilin Zhou", "Xinyang Zhang", "Chenwei Zhang", "Jingbo Shang", "Ming Zhang", "Yangqiu Song", "Irwin King", "Philip S. Yu"], "categories": ["cs.IR", "cs.CL", "cs.LG"], "comment": null, "summary": "Information retrieval is a cornerstone of modern knowledge acquisition,\nenabling billions of queries each day across diverse domains. However,\ntraditional keyword-based search engines are increasingly inadequate for\nhandling complex, multi-step information needs. Our position is that Large\nLanguage Models (LLMs), endowed with reasoning and agentic capabilities, are\nushering in a new paradigm termed Agentic Deep Research. These systems\ntranscend conventional information search techniques by tightly integrating\nautonomous reasoning, iterative retrieval, and information synthesis into a\ndynamic feedback loop. We trace the evolution from static web search to\ninteractive, agent-based systems that plan, explore, and learn. We also\nintroduce a test-time scaling law to formalize the impact of computational\ndepth on reasoning and search. Supported by benchmark results and the rise of\nopen-source implementations, we demonstrate that Agentic Deep Research not only\nsignificantly outperforms existing approaches, but is also poised to become the\ndominant paradigm for future information seeking. All the related resources,\nincluding industry products, research papers, benchmark datasets, and\nopen-source implementations, are collected for the community in\nhttps://github.com/DavidZWZ/Awesome-Deep-Research."}
{"id": "2506.19072", "pdf": "https://arxiv.org/pdf/2506.19072.pdf", "abs": "https://arxiv.org/abs/2506.19072", "title": "HAWAII: Hierarchical Visual Knowledge Transfer for Efficient Vision-Language Models", "authors": ["Yimu Wang", "Mozhgan Nasr Azadani", "Sean Sedwards", "Krzysztof Czarnecki"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Work in progress", "summary": "Improving the visual understanding ability of vision-language models (VLMs)\nis crucial for enhancing their performance across various tasks. While using\nmultiple pretrained visual experts has shown great promise, it often incurs\nsignificant computational costs during training and inference. To address this\nchallenge, we propose HAWAII, a novel framework that distills knowledge from\nmultiple visual experts into a single vision encoder, enabling it to inherit\nthe complementary strengths of several experts with minimal computational\noverhead. To mitigate conflicts among different teachers and switch between\ndifferent teacher-specific knowledge, instead of using a fixed set of adapters\nfor multiple teachers, we propose to use teacher-specific Low-Rank Adaptation\n(LoRA) adapters with a corresponding router. Each adapter is aligned with a\nspecific teacher, avoiding noisy guidance during distillation. To enable\nefficient knowledge distillation, we propose fine-grained and coarse-grained\ndistillation. At the fine-grained level, token importance scores are employed\nto emphasize the most informative tokens from each teacher adaptively. At the\ncoarse-grained level, we summarize the knowledge from multiple teachers and\ntransfer it to the student using a set of general-knowledge LoRA adapters with\na router. Extensive experiments on various vision-language tasks demonstrate\nthe superiority of HAWAII, compared to the popular open-source VLMs."}
{"id": "2506.19143", "pdf": "https://arxiv.org/pdf/2506.19143.pdf", "abs": "https://arxiv.org/abs/2506.19143", "title": "Thought Anchors: Which LLM Reasoning Steps Matter?", "authors": ["Paul C. Bogdan", "Uzay Macar", "Neel Nanda", "Arthur Conmy"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Paul C. Bogdan and Uzay Macar contributed equally to this work, and\n  their listed order was determined by coinflip. Neel Nanda and Arthur Conmy\n  contributed equally to this work as senior authors, and their listed order\n  was determined by coinflip", "summary": "Reasoning large language models have recently achieved state-of-the-art\nperformance in many fields. However, their long-form chain-of-thought reasoning\ncreates interpretability challenges as each generated token depends on all\nprevious ones, making the computation harder to decompose. We argue that\nanalyzing reasoning traces at the sentence level is a promising approach to\nunderstanding reasoning processes. We present three complementary attribution\nmethods: (1) a black-box method measuring each sentence's counterfactual\nimportance by comparing final answers across 100 rollouts conditioned on the\nmodel generating that sentence or one with a different meaning; (2) a white-box\nmethod of aggregating attention patterns between pairs of sentences, which\nidentified ``broadcasting'' sentences that receive disproportionate attention\nfrom all future sentences via ``receiver'' attention heads; (3) a causal\nattribution method measuring logical connections between sentences by\nsuppressing attention toward one sentence and measuring the effect on each\nfuture sentence's tokens. Each method provides evidence for the existence of\nthought anchors, reasoning steps that have outsized importance and that\ndisproportionately influence the subsequent reasoning process. These thought\nanchors are typically planning or backtracking sentences. We provide an\nopen-source tool (www.thought-anchors.com) for visualizing the outputs of our\nmethods, and present a case study showing converging patterns across methods\nthat map how a model performs multi-step reasoning. The consistency across\nmethods demonstrates the potential of sentence-level analysis for a deeper\nunderstanding of reasoning models."}
{"id": "2506.19191", "pdf": "https://arxiv.org/pdf/2506.19191.pdf", "abs": "https://arxiv.org/abs/2506.19191", "title": "Bayesian Evolutionary Swarm Architecture: A Formal Epistemic System Grounded in Truth-Based Competition", "authors": ["Craig Steven Wright"], "categories": ["cs.AI", "cs.CL", "cs.GT", "math.LO", "68T05, 68Q87, 03E20", "I.2.6; I.2.3; F.1.1"], "comment": "83 pages, 14 sections, 92 formal results, no prior conference\n  publication", "summary": "We introduce a mathematically rigorous framework for an artificial\nintelligence system composed of probabilistic agents evolving through\nstructured competition and belief revision. The architecture, grounded in\nBayesian inference, measure theory, and population dynamics, defines agent\nfitness as a function of alignment with a fixed external oracle representing\nground truth. Agents compete in a discrete-time environment, adjusting\nposterior beliefs through observed outcomes, with higher-rated agents\nreproducing and lower-rated agents undergoing extinction. Ratings are updated\nvia pairwise truth-aligned utility comparisons, and belief updates preserve\nmeasurable consistency and stochastic convergence. We introduce hash-based\ncryptographic identity commitments to ensure traceability, alongside causal\ninference operators using do-calculus. Formal theorems on convergence,\nrobustness, and evolutionary stability are provided. The system establishes\ntruth as an evolutionary attractor, demonstrating that verifiable knowledge\narises from adversarial epistemic pressure within a computable, self-regulating\nswarm."}
{"id": "2506.19290", "pdf": "https://arxiv.org/pdf/2506.19290.pdf", "abs": "https://arxiv.org/abs/2506.19290", "title": "Skywork-SWE: Unveiling Data Scaling Laws for Software Engineering in LLMs", "authors": ["Liang Zeng", "Yongcong Li", "Yuzhen Xiao", "Changshi Li", "Chris Yuhao Liu", "Rui Yan", "Tianwen Wei", "Jujie He", "Xuchen Song", "Yang Liu", "Yahui Zhou"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Software engineering (SWE) has recently emerged as a crucial testbed for\nnext-generation LLM agents, demanding inherent capabilities in two critical\ndimensions: sustained iterative problem-solving (e.g., >50 interaction rounds)\nand long-context dependency resolution (e.g., >32k tokens). However, the data\ncuration process in SWE remains notoriously time-consuming, as it heavily\nrelies on manual annotation for code file filtering and the setup of dedicated\nruntime environments to execute and validate unit tests. Consequently, most\nexisting datasets are limited to only a few thousand GitHub-sourced instances.\nTo this end, we propose an incremental, automated data-curation pipeline that\nsystematically scales both the volume and diversity of SWE datasets. Our\ndataset comprises 10,169 real-world Python task instances from 2,531 distinct\nGitHub repositories, each accompanied by a task specified in natural language\nand a dedicated runtime-environment image for automated unit-test validation.\nWe have carefully curated over 8,000 successfully runtime-validated training\ntrajectories from our proposed SWE dataset. When fine-tuning the Skywork-SWE\nmodel on these trajectories, we uncover a striking data scaling phenomenon: the\ntrained model's performance for software engineering capabilities in LLMs\ncontinues to improve as the data size increases, showing no signs of\nsaturation. Notably, our Skywork-SWE model achieves 38.0% pass@1 accuracy on\nthe SWE-bench Verified benchmark without using verifiers or multiple rollouts,\nestablishing a new state-of-the-art (SOTA) among the Qwen2.5-Coder-32B-based\nLLMs built on the OpenHands agent framework. Furthermore, with the\nincorporation of test-time scaling techniques, the performance further improves\nto 47.0% accuracy, surpassing the previous SOTA results for sub-32B parameter\nmodels. We release the Skywork-SWE-32B model checkpoint to accelerate future\nresearch."}
{"id": "2506.19351", "pdf": "https://arxiv.org/pdf/2506.19351.pdf", "abs": "https://arxiv.org/abs/2506.19351", "title": "In-Context Occam's Razor: How Transformers Prefer Simpler Hypotheses on the Fly", "authors": ["Puneesh Deora", "Bhavya Vasudeva", "Tina Behnia", "Christos Thrampoulidis"], "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": "28 pages, 19 figures", "summary": "In-context learning (ICL) enables transformers to adapt to new tasks through\ncontextual examples without parameter updates. While existing research has\ntypically studied ICL in fixed-complexity environments, practical language\nmodels encounter tasks spanning diverse complexity levels. This paper\ninvestigates how transformers navigate hierarchical task structures where\nhigher-complexity categories can perfectly represent any pattern generated by\nsimpler ones. We design well-controlled testbeds based on Markov chains and\nlinear regression that reveal transformers not only identify the appropriate\ncomplexity level for each task but also accurately infer the corresponding\nparameters--even when the in-context examples are compatible with multiple\ncomplexity hypotheses. Notably, when presented with data generated by simpler\nprocesses, transformers consistently favor the least complex sufficient\nexplanation. We theoretically explain this behavior through a Bayesian\nframework, demonstrating that transformers effectively implement an in-context\nBayesian Occam's razor by balancing model fit against complexity penalties. We\nfurther ablate on the roles of model size, training mixture distribution,\ninference context length, and architecture. Finally, we validate this Occam's\nrazor-like inductive bias on a pretrained GPT-4 model with Boolean-function\ntasks as case study, suggesting it may be inherent to transformers trained on\ndiverse task distributions."}
{"id": "2506.19433", "pdf": "https://arxiv.org/pdf/2506.19433.pdf", "abs": "https://arxiv.org/abs/2506.19433", "title": "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments with a Hierarchical Spatial-Cognition Long-Short Memory System", "authors": ["Lixuan He", "Haoyu Dong", "Zhenxing Chen", "Yangcheng Yu", "Jie Feng", "Yong Li"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Vision-and-Language Navigation (VLN) in large-scale urban environments\nrequires embodied agents to ground linguistic instructions in complex scenes\nand recall relevant experiences over extended time horizons. Prior modular\npipelines offer interpretability but lack unified memory, while end-to-end\n(M)LLM agents excel at fusing vision and language yet remain constrained by\nfixed context windows and implicit spatial reasoning. We introduce\n\\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system\nthat can augment any VLN backbone. Mem4Nav fuses a sparse octree for\nfine-grained voxel indexing with a semantic topology graph for high-level\nlandmark connectivity, storing both in trainable memory tokens embedded via a\nreversible Transformer. Long-term memory (LTM) compresses and retains\nhistorical observations at both octree and graph nodes, while short-term memory\n(STM) caches recent multimodal entries in relative coordinates for real-time\nobstacle avoidance and local planning. At each step, STM retrieval sharply\nprunes dynamic context, and, when deeper history is needed, LTM tokens are\ndecoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and\nMap2Seq across three backbones (modular, state-of-the-art VLN with prompt-based\nLLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13\npp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW\nimprovement. Ablations confirm the indispensability of both the hierarchical\nmap and dual memory modules. Our codes are open-sourced via\nhttps://github.com/tsinghua-fib-lab/Mem4Nav."}
{"id": "2506.19441", "pdf": "https://arxiv.org/pdf/2506.19441.pdf", "abs": "https://arxiv.org/abs/2506.19441", "title": "TTSDS2: Resources and Benchmark for Evaluating Human-Quality Text to Speech Systems", "authors": ["Christoph Minixhofer", "Ondrej Klejch", "Peter Bell"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "Evaluation of Text to Speech (TTS) systems is challenging and\nresource-intensive. Subjective metrics such as Mean Opinion Score (MOS) are not\neasily comparable between works. Objective metrics are frequently used, but\nrarely validated against subjective ones. Both kinds of metrics are challenged\nby recent TTS systems capable of producing synthetic speech indistinguishable\nfrom real speech. In this work, we introduce Text to Speech Distribution Score\n2 (TTSDS2), a more robust and improved version of TTSDS. Across a range of\ndomains and languages, it is the only one out of 16 compared metrics to\ncorrelate with a Spearman correlation above 0.50 for every domain and\nsubjective score evaluated. We also release a range of resources for evaluating\nsynthetic speech close to real speech: A dataset with over 11,000 subjective\nopinion score ratings; a pipeline for continually recreating a multilingual\ntest dataset to avoid data leakage; and a continually updated benchmark for TTS\nin 14 languages."}
{"id": "2506.19500", "pdf": "https://arxiv.org/pdf/2506.19500.pdf", "abs": "https://arxiv.org/abs/2506.19500", "title": "NaviAgent: Bilevel Planning on Tool Dependency Graphs for Function Calling", "authors": ["Yan Jiang", "Hao Zhou", "LiZhong GU", "Ai Han", "TianLong Li"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "LLMs' reliance on static knowledge and fragile tool invocation severely\nhinders the orchestration of complex, heterogeneous toolchains, particularly at\nlarge scales. Existing methods typically use rigid single-path execution,\nresulting in poor error recovery and exponentially growing search spaces. We\nintroduce NaviAgent, a graph-navigated bilevel planning architecture for robust\nfunction calling, comprising a Multi-Path Decider and Graph-Encoded Navigator.\nAs an LLM-powered agent, the Multi-Path Decider defines a four-dimensional\ndecision space and continuously perceives environmental states, dynamically\nselecting the optimal action to fully cover all tool invocation scenarios. The\nGraph-Encoded Navigator constructs a Tool Dependency Heterogeneous Graph\n(TDHG), where node embeddings explicitly fuse API schema structure with\nhistorical invocation behavior. It also integrates a novel heuristic search\nstrategy that guides the Decider toward efficient and highly successful\ntoolchains, even for unseen tool combinations. Experiments show that NaviAgent\nconsistently achieves the highest task success rate (TSR) across all foundation\nmodels and task complexities, outperforming the average baselines (ReAct,\nToolLLM, {\\alpha}-UMI) by 13.5%, 16.4%, and 19.0% on Qwen2.5-14B, Qwen2.5-32B,\nand Deepseek-V3, respectively. Its execution steps are typically within one\nstep of the most efficient baseline, ensuring a strong balance between quality\nand efficiency. Notably, a fine-tuned Qwen2.5-14B model achieves a TSR of\n49.5%, surpassing the much larger 32B model (44.9%) under our architecture.\nIncorporating the Graph-Encoded Navigator further boosts TSR by an average of\n2.4 points, with gains up over 9 points on complex tasks for larger models\n(Deepseek-V3 and GPT-4o), highlighting its essential role in toolchain\norchestration."}
{"id": "2506.19579", "pdf": "https://arxiv.org/pdf/2506.19579.pdf", "abs": "https://arxiv.org/abs/2506.19579", "title": "Fake or Real, Can Robots Tell? Evaluating Embodied Vision-Language Models on Real and 3D-Printed Objects", "authors": ["Federico Tavella", "Kathryn Mearns", "Angelo Cangelosi"], "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": null, "summary": "Robotic scene understanding increasingly relies on vision-language models\n(VLMs) to generate natural language descriptions of the environment. In this\nwork, we present a comparative study of captioning strategies for tabletop\nscenes captured by a robotic arm equipped with an RGB camera. The robot\ncollects images of objects from multiple viewpoints, and we evaluate several\nmodels that generate scene descriptions. We compare the performance of various\ncaptioning models, like BLIP and VLMs. Our experiments examine the trade-offs\nbetween single-view and multi-view captioning, and difference between\nrecognising real-world and 3D printed objects. We quantitatively evaluate\nobject identification accuracy, completeness, and naturalness of the generated\ncaptions. Results show that VLMs can be used in robotic settings where common\nobjects need to be recognised, but fail to generalise to novel representations.\nOur findings provide practical insights into deploying foundation models for\nembodied agents in real-world settings."}
{"id": "2506.19665", "pdf": "https://arxiv.org/pdf/2506.19665.pdf", "abs": "https://arxiv.org/abs/2506.19665", "title": "Recurrent Visual Feature Extraction and Stereo Attentions for CT Report Generation", "authors": ["Yuanhe Tian", "Lei Mao", "Yan Song"], "categories": ["cs.CV", "cs.CL"], "comment": "7 pages, 3 figures", "summary": "Generating reports for computed tomography (CT) images is a challenging task,\nwhile similar to existing studies for medical image report generation, yet has\nits unique characteristics, such as spatial encoding of multiple images,\nalignment between image volume and texts, etc. Existing solutions typically use\ngeneral 2D or 3D image processing techniques to extract features from a CT\nvolume, where they firstly compress the volume and then divide the compressed\nCT slices into patches for visual encoding. These approaches do not explicitly\naccount for the transformations among CT slices, nor do they effectively\nintegrate multi-level image features, particularly those containing specific\norgan lesions, to instruct CT report generation (CTRG). In considering the\nstrong correlation among consecutive slices in CT scans, in this paper, we\npropose a large language model (LLM) based CTRG method with recurrent visual\nfeature extraction and stereo attentions for hierarchical feature modeling.\nSpecifically, we use a vision Transformer to recurrently process each slice in\na CT volume, and employ a set of attentions over the encoded slices from\ndifferent perspectives to selectively obtain important visual information and\nalign them with textual features, so as to better instruct an LLM for CTRG.\nExperiment results and further analysis on the benchmark M3D-Cap dataset show\nthat our method outperforms strong baseline models and achieves\nstate-of-the-art results, demonstrating its validity and effectiveness."}
{"id": "2506.19697", "pdf": "https://arxiv.org/pdf/2506.19697.pdf", "abs": "https://arxiv.org/abs/2506.19697", "title": "Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large Language Models", "authors": ["Jungwoo Park", "Taewhoo Lee", "Chanwoong Yoon", "Hyeon Hwang", "Jaewoo Kang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Extreme activation outliers in Large Language Models (LLMs) critically\ndegrade quantization performance, hindering efficient on-device deployment.\nWhile channel-wise operations and adaptive gradient scaling are recognized\ncauses, practical mitigation remains challenging. We introduce Outlier-Safe\nPre-Training (OSP), a practical guideline that proactively prevents outlier\nformation rather than relying on post-hoc mitigation. OSP combines three key\ninnovations: (1) the Muon optimizer, eliminating privileged bases while\nmaintaining training efficiency; (2) Single-Scale RMSNorm, preventing\nchannel-wise amplification; and (3) a learnable embedding projection,\nredistributing activation magnitudes originating from embedding matrices. We\nvalidate OSP by training a 1.4B-parameter model on 1 trillion tokens, which is\nthe first production-scale LLM trained without such outliers. Under aggressive\n4-bit quantization, our OSP model achieves a 35.7 average score across 10\nbenchmarks (compared to 26.5 for an Adam-trained model), with only a 2%\ntraining overhead. Remarkably, OSP models exhibit near-zero excess kurtosis\n(0.04) compared to extreme values (1818.56) in standard models, fundamentally\naltering LLM quantization behavior. Our work demonstrates that outliers are not\ninherent to LLMs but are consequences of training strategies, paving the way\nfor more efficient LLM deployment. The source code and pretrained checkpoints\nare available at https://github.com/dmis-lab/Outlier-Safe-Pre-Training."}
{"id": "2506.19743", "pdf": "https://arxiv.org/pdf/2506.19743.pdf", "abs": "https://arxiv.org/abs/2506.19743", "title": "NEAR$^2$: A Nested Embedding Approach to Efficient Product Retrieval and Ranking", "authors": ["Shenbin Qian", "Diptesh Kanojia", "Samarth Agrawal", "Hadeel Saadany", "Swapnil Bhosale", "Constantin Orasan", "Zhe Wu"], "categories": ["cs.IR", "cs.CL"], "comment": "This paper is accepted to the 2025 SIGIR Workshop on eCommerce", "summary": "E-commerce information retrieval (IR) systems struggle to simultaneously\nachieve high accuracy in interpreting complex user queries and maintain\nefficient processing of vast product catalogs. The dual challenge lies in\nprecisely matching user intent with relevant products while managing the\ncomputational demands of real-time search across massive inventories. In this\npaper, we propose a Nested Embedding Approach to product Retrieval and Ranking,\ncalled NEAR$^2$, which can achieve up to $12$ times efficiency in embedding\nsize at inference time while introducing no extra cost in training and\nimproving performance in accuracy for various encoder-based Transformer models.\nWe validate our approach using different loss functions for the retrieval and\nranking task, including multiple negative ranking loss and online contrastive\nloss, on four different test sets with various IR challenges such as short and\nimplicit queries. Our approach achieves an improved performance over a smaller\nembedding dimension, compared to any existing models."}
{"id": "2506.19774", "pdf": "https://arxiv.org/pdf/2506.19774.pdf", "abs": "https://arxiv.org/abs/2506.19774", "title": "Kling-Foley: Multimodal Diffusion Transformer for High-Quality Video-to-Audio Generation", "authors": ["Jun Wang", "Xijuan Zeng", "Chunyu Qiang", "Ruilong Chen", "Shiyao Wang", "Le Wang", "Wangjing Zhou", "Pengfei Cai", "Jiahui Zhao", "Nan Li", "Zihan Li", "Yuzhe Liang", "Xiaopeng Wang", "Haorui Zheng", "Ming Wen", "Kang Yin", "Yiran Wang", "Nan Li", "Feng Deng", "Liang Dong", "Chen Zhang", "Di Zhang", "Kun Gai"], "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": null, "summary": "We propose Kling-Foley, a large-scale multimodal Video-to-Audio generation\nmodel that synthesizes high-quality audio synchronized with video content. In\nKling-Foley, we introduce multimodal diffusion transformers to model the\ninteractions between video, audio, and text modalities, and combine it with a\nvisual semantic representation module and an audio-visual synchronization\nmodule to enhance alignment capabilities. Specifically, these modules align\nvideo conditions with latent audio elements at the frame level, thereby\nimproving semantic alignment and audio-visual synchronization. Together with\ntext conditions, this integrated approach enables precise generation of\nvideo-matching sound effects. In addition, we propose a universal latent audio\ncodec that can achieve high-quality modeling in various scenarios such as sound\neffects, speech, singing, and music. We employ a stereo rendering method that\nimbues synthesized audio with a spatial presence. At the same time, in order to\nmake up for the incomplete types and annotations of the open-source benchmark,\nwe also open-source an industrial-level benchmark Kling-Audio-Eval. Our\nexperiments show that Kling-Foley trained with the flow matching objective\nachieves new audio-visual SOTA performance among public models in terms of\ndistribution matching, semantic alignment, temporal alignment and audio\nquality."}
{"id": "2506.19806", "pdf": "https://arxiv.org/pdf/2506.19806.pdf", "abs": "https://arxiv.org/abs/2506.19806", "title": "LLM-Based Social Simulations Require a Boundary", "authors": ["Zengqing Wu", "Run Peng", "Takayuki Ito", "Chuan Xiao"], "categories": ["cs.CY", "cs.CL", "cs.MA"], "comment": null, "summary": "This position paper argues that large language model (LLM)-based social\nsimulations should establish clear boundaries to meaningfully contribute to\nsocial science research. While LLMs offer promising capabilities for modeling\nhuman-like agents compared to traditional agent-based modeling, they face\nfundamental limitations that constrain their reliability for social pattern\ndiscovery. The core issue lies in LLMs' tendency towards an ``average persona''\nthat lacks sufficient behavioral heterogeneity, a critical requirement for\nsimulating complex social dynamics. We examine three key boundary problems:\nalignment (simulated behaviors matching real-world patterns), consistency\n(maintaining coherent agent behavior over time), and robustness\n(reproducibility under varying conditions). We propose heuristic boundaries for\ndetermining when LLM-based simulations can reliably advance social science\nunderstanding. We believe that these simulations are more valuable when\nfocusing on (1) collective patterns rather than individual trajectories, (2)\nagent behaviors aligning with real population averages despite limited\nvariance, and (3) proper validation methods available for testing simulation\nrobustness. We provide a practical checklist to guide researchers in\ndetermining the appropriate scope and claims for LLM-based social simulations."}
{"id": "2506.19807", "pdf": "https://arxiv.org/pdf/2506.19807.pdf", "abs": "https://arxiv.org/abs/2506.19807", "title": "KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality", "authors": ["Baochang Ren", "Shuofei Qiao", "Wenhao Yu", "Huajun Chen", "Ningyu Zhang"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.MA"], "comment": "Work in progress", "summary": "Large Language Models (LLMs), particularly slow-thinking models, often\nexhibit severe hallucination, outputting incorrect content due to an inability\nto accurately recognize knowledge boundaries during reasoning. While\nReinforcement Learning (RL) can enhance complex reasoning abilities, its\noutcome-oriented reward mechanism often lacks factual supervision over the\nthinking process, further exacerbating the hallucination problem. To address\nthe high hallucination in slow-thinking models, we propose Knowledge-enhanced\nRL, KnowRL. KnowRL guides models to perform fact-based slow thinking by\nintegrating a factuality reward, based on knowledge verification, into the RL\ntraining process, helping them recognize their knowledge boundaries. KnowRL\nguides models to perform fact-based slow thinking by integrating a factuality\nreward, based on knowledge verification, into the RL training process, helping\nthem recognize their knowledge boundaries. This targeted factual input during\nRL training enables the model to learn and internalize fact-based reasoning\nstrategies. By directly rewarding adherence to facts within the reasoning\nsteps, KnowRL fosters a more reliable thinking process. Experimental results on\nthree hallucination evaluation datasets and two reasoning evaluation datasets\ndemonstrate that KnowRL effectively mitigates hallucinations in slow-thinking\nmodels while maintaining their original strong reasoning capabilities. Our code\nis available at https://github.com/zjunlp/KnowRL."}
{"id": "2506.19825", "pdf": "https://arxiv.org/pdf/2506.19825.pdf", "abs": "https://arxiv.org/abs/2506.19825", "title": "Evaluating Compliance with Visualization Guidelines in Diagrams for Scientific Publications Using Large Vision Language Models", "authors": ["Johannes Rückert", "Louise Bloch", "Christoph M. Friedrich"], "categories": ["cs.AI", "cs.CL"], "comment": "Accepted at ICDAR 2025", "summary": "Diagrams are widely used to visualize data in publications. The research\nfield of data visualization deals with defining principles and guidelines for\nthe creation and use of these diagrams, which are often not known or adhered to\nby researchers, leading to misinformation caused by providing inaccurate or\nincomplete information.\n  In this work, large Vision Language Models (VLMs) are used to analyze\ndiagrams in order to identify potential problems in regards to selected data\nvisualization principles and guidelines. To determine the suitability of VLMs\nfor these tasks, five open source VLMs and five prompting strategies are\ncompared using a set of questions derived from selected data visualization\nguidelines.\n  The results show that the employed VLMs work well to accurately analyze\ndiagram types (F1-score 82.49 %), 3D effects (F1-score 98.55 %), axes labels\n(F1-score 76.74 %), lines (RMSE 1.16), colors (RMSE 1.60) and legends (F1-score\n96.64 %, RMSE 0.70), while they cannot reliably provide feedback about the\nimage quality (F1-score 0.74 %) and tick marks/labels (F1-score 46.13 %). Among\nthe employed VLMs, Qwen2.5VL performs best, and the summarizing prompting\nstrategy performs best for most of the experimental questions.\n  It is shown that VLMs can be used to automatically identify a number of\npotential issues in diagrams, such as missing axes labels, missing legends, and\nunnecessary 3D effects. The approach laid out in this work can be extended for\nfurther aspects of data visualization."}
{"id": "2506.19830", "pdf": "https://arxiv.org/pdf/2506.19830.pdf", "abs": "https://arxiv.org/abs/2506.19830", "title": "Scaling Speculative Decoding with Lookahead Reasoning", "authors": ["Yichao Fu", "Rui Ge", "Zelei Shao", "Zhijie Deng", "Hao Zhang"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Reasoning models excel by generating long chain-of-thoughts, but decoding the\nresulting thousands of tokens is slow. Token-level speculative decoding (SD)\nhelps, but its benefit is capped, because the chance that an entire\n$\\gamma$-token guess is correct falls exponentially as $\\gamma$ grows. This\nmeans allocating more compute for longer token drafts faces an algorithmic\nceiling -- making the speedup modest and hardware-agnostic. We raise this\nceiling with Lookahead Reasoning, which exploits a second, step-level layer of\nparallelism. Our key insight is that reasoning models generate step-by-step,\nand each step needs only to be semantically correct, not exact token matching.\nIn Lookahead Reasoning, a lightweight draft model proposes several future\nsteps; the target model expands each proposal in one batched pass, and a\nverifier keeps semantically correct steps while letting the target regenerate\nany that fail. Token-level SD still operates within each reasoning step, so the\ntwo layers of parallelism multiply. We show Lookahead Reasoning lifts the peak\nspeedup of SD both theoretically and empirically. Across GSM8K, AIME, and other\nbenchmarks, Lookahead Reasoning improves the speedup of SD from 1.4x to 2.1x\nwhile preserving answer quality, and its speedup scales better with additional\nGPU throughput. Our code is available at\nhttps://github.com/hao-ai-lab/LookaheadReasoning"}
{"id": "2506.19847", "pdf": "https://arxiv.org/pdf/2506.19847.pdf", "abs": "https://arxiv.org/abs/2506.19847", "title": "Orthogonal Finetuning Made Scalable", "authors": ["Zeju Qiu", "Weiyang Liu", "Adrian Weller", "Bernhard Schölkopf"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": "Technical report (17 pages, 7 figures, project page:\n  https://spherelab.ai/oftv2/)", "summary": "Orthogonal finetuning (OFT) offers highly parameter-efficient adaptation\nwhile preventing catastrophic forgetting, but its high runtime and memory\ndemands limit practical deployment. We identify the core computational\nbottleneck in OFT as its weight-centric implementation, which relies on costly\nmatrix-matrix multiplications with cubic complexity. To overcome this, we\npropose OFTv2, an input-centric reformulation that instead uses matrix-vector\nmultiplications (i.e., matrix-free computation), reducing the computational\ncost to quadratic. We further introduce the Cayley-Neumann parameterization, an\nefficient orthogonal parameterization that approximates the matrix inversion in\nCayley transform via a truncated Neumann series. These modifications allow\nOFTv2 to achieve up to 10x faster training and 3x lower GPU memory usage\nwithout compromising performance. In addition, we extend OFTv2 to support\nfinetuning quantized foundation models and show that it outperforms the popular\nQLoRA in training stability, efficiency, and memory usage."}
{"id": "2506.19848", "pdf": "https://arxiv.org/pdf/2506.19848.pdf", "abs": "https://arxiv.org/abs/2506.19848", "title": "ScaleCap: Inference-Time Scalable Image Captioning via Dual-Modality Debiasing", "authors": ["Long Xing", "Qidong Huang", "Xiaoyi Dong", "Pan Zhang", "Yuhang Zang", "Yuhang Cao", "Jinsong Li", "Shuangrui Ding", "Weiming Zhang", "Nenghai Yu", "Jiaqi Wang", "Feng Wu", "Dahua Lin"], "categories": ["cs.CV", "cs.CL"], "comment": "Code is available at https://github.com/Cooperx521/ScaleCap", "summary": "This paper presents ScaleCap, an inference-time scalable image captioning\nstrategy that generates comprehensive and detailed image captions. The key\nchallenges of high-quality image captioning lie in the inherent biases of\nLVLMs: multimodal bias resulting in imbalanced descriptive granularity,\noffering detailed accounts of some elements while merely skimming over others;\nlinguistic bias leading to hallucinated descriptions of non-existent objects.\nTo address these issues, we propose a scalable debiased captioning strategy,\nwhich continuously enriches and calibrates the caption with increased inference\nbudget. Specifically, we propose two novel components: heuristic question\nanswering and contrastive sentence rating. The former generates\ncontent-specific questions based on the image and answers them to progressively\ninject relevant information into the caption. The latter employs sentence-level\noffline contrastive decoding to effectively identify and eliminate\nhallucinations caused by linguistic biases. With increased inference cost, more\nheuristic questions are raised by ScaleCap to progressively capture additional\nvisual details, generating captions that are more accurate, balanced, and\ninformative. Extensive modality alignment experiments demonstrate the\neffectiveness of ScaleCap. Annotating 450K images with ScaleCap and using them\nfor LVLM pretraining leads to consistent performance gains across 11 widely\nused benchmarks. Furthermore, ScaleCap showcases superb richness and fidelity\nof generated captions with two additional tasks: replacing images with captions\nin VQA task, and reconstructing images from captions to assess semantic\ncoverage. Code is available at https://github.com/Cooperx521/ScaleCap."}
{"id": "2308.16075", "pdf": "https://arxiv.org/pdf/2308.16075.pdf", "abs": "https://arxiv.org/abs/2308.16075", "title": "Impact of Visual Context on Noisy Multimodal NMT: An Empirical Study for English to Indian Languages", "authors": ["Baban Gain", "Dibyanayan Bandyopadhyay", "Samrat Mukherjee", "Chandranath Adak", "Asif Ekbal"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Neural Machine Translation (NMT) has made remarkable progress using\nlarge-scale textual data, but the potential of incorporating multimodal inputs,\nespecially visual information, remains underexplored in high-resource settings.\nWhile prior research has focused on using multimodal data in low-resource\nscenarios, this study examines how image features impact translation when added\nto a large-scale, pre-trained unimodal NMT system. Surprisingly, the study\nfinds that images might be redundant in this context. Additionally, the\nresearch introduces synthetic noise to assess whether images help the model\nhandle textual noise. Multimodal models slightly outperform text-only models in\nnoisy settings, even when random images are used. The study's experiments\ntranslate from English to Hindi, Bengali, and Malayalam, significantly\noutperforming state-of-the-art benchmarks. Interestingly, the effect of visual\ncontext varies with the level of source text noise: no visual context works\nbest for non-noisy translations, cropped image features are optimal for low\nnoise, and full image features perform better in high-noise scenarios. This\nsheds light on the role of visual context, especially in noisy settings, and\nopens up a new research direction for Noisy Neural Machine Translation in\nmultimodal setups. The research emphasizes the importance of combining visual\nand textual information to improve translation across various environments. Our\ncode is publicly available at https://github.com/babangain/indicMMT."}
{"id": "2404.01799", "pdf": "https://arxiv.org/pdf/2404.01799.pdf", "abs": "https://arxiv.org/abs/2404.01799", "title": "PATCH! {P}sychometrics-{A}ssis{T}ed Ben{CH}marking of Large Language Models against Human Populations: A Case Study of Proficiency in 8th Grade Mathematics", "authors": ["Qixiang Fang", "Daniel L. Oberski", "Dong Nguyen"], "categories": ["cs.CL", "cs.CY"], "comment": "Accepted to GEM2 Workshop: Generation, Evaluation & Metrics - ACL\n  2025", "summary": "Many existing benchmarks of large (multimodal) language models (LLMs) focus\non measuring LLMs' academic proficiency, often with also an interest in\ncomparing model performance with human test takers'. While such benchmarks have\nproven key to the development of LLMs, they suffer from several limitations,\nincluding questionable measurement quality (e.g., Do they measure what they are\nsupposed to in a reliable way?), lack of quality assessment on the item level\n(e.g., Are some items more important or difficult than others?) and unclear\nhuman population reference (e.g., To whom can the model be compared?). In\nresponse to these challenges, we propose leveraging knowledge from\npsychometrics -- a field dedicated to the measurement of latent variables like\nacademic proficiency -- into LLM benchmarking. We make four primary\ncontributions. First, we reflect on current LLM benchmark developments and\ncontrast them with psychometrics-based test development. Second, we introduce\nPATCH: a novel framework for {P}sychometrics-{A}ssis{T}ed ben{CH}marking of\nLLMs. PATCH addresses the aforementioned limitations. In particular, PATCH\nenables valid comparison between LLMs and human populations. Third, we\ndemonstrate PATCH by measuring several LLMs' proficiency in 8th grade\nmathematics against 56 human populations. We show that adopting a\npsychometrics-based approach yields evaluation outcomes that diverge from those\nbased on current benchmarking practices. Fourth, we release 4 high-quality\ndatasets to support measuring and comparing LLM proficiency in grade school\nmathematics and science with human populations."}
{"id": "2406.18259", "pdf": "https://arxiv.org/pdf/2406.18259.pdf", "abs": "https://arxiv.org/abs/2406.18259", "title": "Detecting Machine-Generated Texts: Not Just \"AI vs Humans\" and Explainability is Complicated", "authors": ["Jiazhou Ji", "Ruizhe Li", "Shujun Li", "Jie Guo", "Weidong Qiu", "Zheng Huang", "Chiyu Chen", "Xiaoyu Jiang", "Xinru Lu"], "categories": ["cs.CL", "cs.AI"], "comment": "19 pages, 2 figures", "summary": "As LLMs rapidly advance, increasing concerns arise regarding risks about\nactual authorship of texts we see online and in real world. The task of\ndistinguishing LLM-authored texts is complicated by the nuanced and overlapping\nbehaviors of both machines and humans. In this paper, we challenge the current\npractice of considering LLM-generated text detection a binary classification\ntask of differentiating human from AI. Instead, we introduce a novel ternary\ntext classification scheme, adding an \"undecided\" category for texts that could\nbe attributed to either source, and we show that this new category is crucial\nto understand how to make the detection result more explainable to lay users.\nThis research shifts the paradigm from merely classifying to explaining\nmachine-generated texts, emphasizing need for detectors to provide clear and\nunderstandable explanations to users. Our study involves creating four new\ndatasets comprised of texts from various LLMs and human authors. Based on new\ndatasets, we performed binary classification tests to ascertain the most\neffective SOTA detection methods and identified SOTA LLMs capable of producing\nharder-to-detect texts. We constructed a new dataset of texts generated by two\ntop-performing LLMs and human authors, and asked three human annotators to\nproduce ternary labels with explanation notes. This dataset was used to\ninvestigate how three top-performing SOTA detectors behave in new ternary\nclassification context. Our results highlight why \"undecided\" category is much\nneeded from the viewpoint of explainability. Additionally, we conducted an\nanalysis of explainability of the three best-performing detectors and the\nexplanation notes of the human annotators, revealing insights about the\ncomplexity of explainable detection of machine-generated texts. Finally, we\npropose guidelines for developing future detection systems with improved\nexplanatory power."}
{"id": "2407.06331", "pdf": "https://arxiv.org/pdf/2407.06331.pdf", "abs": "https://arxiv.org/abs/2407.06331", "title": "LEVOS: Leveraging Vocabulary Overlap with Sanskrit to Generate Technical Lexicons in Indian Languages", "authors": ["Karthika N J", "Krishnakant Bhatt", "Ganesh Ramakrishnan", "Preethi Jyothi"], "categories": ["cs.CL"], "comment": "20th Workshop on Innovative Use of NLP for Building Educational\n  Applications (Co-located with ACL2025)", "summary": "Translating technical terms into lexically similar, low-resource Indian\nlanguages remains a challenge due to limited parallel data and the complexity\nof linguistic structures. We propose a novel use-case of Sanskrit-based\nsegments for linguistically informed translation of such terms, leveraging\nsubword-level similarity and morphological alignment across related languages.\nOur approach uses character-level segmentation to identify meaningful subword\nunits, facilitating more accurate and context-aware translation. To enable\nthis, we utilize a Character-level Transformer model for Sanskrit Word\nSegmentation (CharSS), which addresses the complexities of sandhi and\nmorpho-phonemic changes during segmentation. We observe consistent improvements\nin two experimental settings for technical term translation using\nSanskrit-derived segments, averaging 8.46 and 6.79 chrF++ scores, respectively.\nFurther, we conduct a post hoc human evaluation to verify the quality\nassessment of the translated technical terms using automated metrics. This work\nhas important implications for the education field, especially in creating\naccessible, high-quality learning materials in Indian languages. By supporting\nthe accurate and linguistically rooted translation of technical content, our\napproach facilitates inclusivity and aids in bridging the resource gap for\nlearners in low-resource language communities."}
{"id": "2408.01933", "pdf": "https://arxiv.org/pdf/2408.01933.pdf", "abs": "https://arxiv.org/abs/2408.01933", "title": "Evaluating Transparent Reasoning in Large Language Models for Accountable Critical Tasks", "authors": ["Junhao Chen", "Bowen Wang", "Jiuyang Chang", "Yuta Nakashima"], "categories": ["cs.CL", "cs.AI"], "comment": "This paper is the journal extension of our NeurIPS 2024 paper\n  \"DiReCT: Diagnostic Reasoning for Clinical Notes via Large Language Models\"", "summary": "This paper introduces REACT, a benchmark designed to rigorously evaluate the\nreasoning capabilities of large language models (LLMs) within accountable,\nhigh-stakes decision-making tasks in medical and legal domains. Unlike\ntraditional benchmarks primarily focused on prediction accuracy, REACT\nemphasizes transparent and interpretable reasoning, requiring models to align\ntheir logic closely with expert-derived procedures. To assess whether LLM\nreasoning aligns closely with human experts, we annotated 511 clinical cases\nfrom the medical domain and 86 legal cases from the legal domain, each enriched\nwith detailed expert-extracted rationales and evidence supporting each step of\nthe reasoning process. These annotations were guided by carefully constructed\nreasoning graphs, which explicitly encode domain-specific inference structures\nand decision criteria derived by domain experts. These reasoning graphs serve\nnot only as standards for expert annotation but also as structured guidelines\nenabling models to reason transparently and step-by-step. To address the\nscalability challenges of manual annotation, we further developed a\nsemi-automatic annotation pipeline leveraging expert-defined reasoning graph\ntemplates to efficiently generate new graphs, exploring the potential to extend\nour approach into additional critical domains. Experimental results demonstrate\nthat reasoning graphs substantially enhance the interpretability and accuracy\nof LLM reasoning compared to traditional baselines, although significant gaps\nremain relative to expert-level reasoning performance."}
{"id": "2410.05563", "pdf": "https://arxiv.org/pdf/2410.05563.pdf", "abs": "https://arxiv.org/abs/2410.05563", "title": "Rational Metareasoning for Large Language Models", "authors": ["C. Nicolò De Sabbata", "Theodore R. Sumers", "Badr AlKhamissi", "Antoine Bosselut", "Thomas L. Griffiths"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Being prompted to engage in reasoning has emerged as a core technique for\nusing large language models (LLMs), deploying additional inference-time compute\nto improve task performance. However, as LLMs increase in both size and\nadoption, inference costs are correspondingly becoming increasingly burdensome.\nHow, then, might we optimize reasoning's cost-performance tradeoff? This work\nintroduces a novel approach based on computational models of metareasoning used\nin cognitive science, training LLMs to selectively use intermediate reasoning\nsteps only when necessary. We first develop a reward function that incorporates\nthe Value of Computation by penalizing unnecessary reasoning, then use this\nreward function with Expert Iteration to train the LLM. Compared to few-shot\nchain-of-thought prompting and STaR, our method significantly reduces inference\ncosts (20-37\\% fewer tokens generated across three models) while maintaining\ntask performance across diverse datasets."}
{"id": "2410.18469", "pdf": "https://arxiv.org/pdf/2410.18469.pdf", "abs": "https://arxiv.org/abs/2410.18469", "title": "ADVLLM: Iterative Self-Tuning LLMs for Enhanced Jailbreaking Capabilities", "authors": ["Chung-En Sun", "Xiaodong Liu", "Weiwei Yang", "Tsui-Wei Weng", "Hao Cheng", "Aidan San", "Michel Galley", "Jianfeng Gao"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to NAACL 2025 Main (oral)", "summary": "Recent research has shown that Large Language Models (LLMs) are vulnerable to\nautomated jailbreak attacks, where adversarial suffixes crafted by algorithms\nappended to harmful queries bypass safety alignment and trigger unintended\nresponses. Current methods for generating these suffixes are computationally\nexpensive and have low Attack Success Rates (ASR), especially against\nwell-aligned models like Llama2 and Llama3. To overcome these limitations, we\nintroduce ADV-LLM, an iterative self-tuning process that crafts adversarial\nLLMs with enhanced jailbreak ability. Our framework significantly reduces the\ncomputational cost of generating adversarial suffixes while achieving nearly\n100\\% ASR on various open-source LLMs. Moreover, it exhibits strong attack\ntransferability to closed-source models, achieving 99\\% ASR on GPT-3.5 and 49\\%\nASR on GPT-4, despite being optimized solely on Llama3. Beyond improving\njailbreak ability, ADV-LLM provides valuable insights for future safety\nalignment research through its ability to generate large datasets for studying\nLLM safety."}
{"id": "2410.23478", "pdf": "https://arxiv.org/pdf/2410.23478.pdf", "abs": "https://arxiv.org/abs/2410.23478", "title": "Collage: Decomposable Rapid Prototyping for Information Extraction on Scientific PDFs", "authors": ["Sireesh Gururaja", "Yueheng Zhang", "Guannan Tang", "Tianhao Zhang", "Kevin Murphy", "Yu-Tsen Yi", "Junwon Seo", "Anthony Rollett", "Emma Strubell"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Recent years in NLP have seen the continued development of domain-specific\ninformation extraction tools for scientific documents, alongside the release of\nincreasingly multimodal pretrained transformer models. While the opportunity\nfor scientists outside of NLP to evaluate and apply such systems to their own\ndomains has never been clearer, these models are difficult to compare: they\naccept different input formats, are often black-box and give little insight\ninto processing failures, and rarely handle PDF documents, the most common\nformat of scientific publication. In this work, we present Collage, a tool\ndesigned for rapid prototyping, visualization, and evaluation of different\ninformation extraction models on scientific PDFs. Collage allows the use and\nevaluation of any HuggingFace token classifier, several LLMs, and multiple\nother task-specific models out of the box, and provides extensible software\ninterfaces to accelerate experimentation with new models. Further, we enable\nboth developers and users of NLP-based tools to inspect, debug, and better\nunderstand modeling pipelines by providing granular views of intermediate\nstates of processing. We demonstrate our system in the context of information\nextraction to assist with literature review in materials science."}
{"id": "2411.10227", "pdf": "https://arxiv.org/pdf/2411.10227.pdf", "abs": "https://arxiv.org/abs/2411.10227", "title": "Entropy and type-token ratio in gigaword corpora", "authors": ["Pablo Rosillo-Rodes", "Maxi San Miguel", "David Sanchez"], "categories": ["cs.CL", "cs.IR", "physics.soc-ph"], "comment": "15 pages, 10 figures, 8 tables", "summary": "There are different ways of measuring diversity in complex systems. In\nparticular, in language, lexical diversity is characterized in terms of the\ntype-token ratio and the word entropy. We here investigate both diversity\nmetrics in six massive linguistic datasets in English, Spanish, and Turkish,\nconsisting of books, news articles, and tweets. These gigaword corpora\ncorrespond to languages with distinct morphological features and differ in\nregisters and genres, thus constituting a varied testbed for a quantitative\napproach to lexical diversity. We unveil an empirical functional relation\nbetween entropy and type-token ratio of texts of a given corpus and language,\nwhich is a consequence of the statistical laws observed in natural language.\nFurther, in the limit of large text lengths we find an analytical expression\nfor this relation relying on both Zipf and Heaps laws that agrees with our\nempirical findings."}
{"id": "2411.19832", "pdf": "https://arxiv.org/pdf/2411.19832.pdf", "abs": "https://arxiv.org/abs/2411.19832", "title": "Sensitive Content Classification in Social Media: A Holistic Resource and Evaluation", "authors": ["Dimosthenis Antypas", "Indira Sen", "Carla Perez-Almendros", "Jose Camacho-Collados", "Francesco Barbieri"], "categories": ["cs.CL", "I.2.7"], "comment": "Accepted at the 9th Workshop on Online Abuse and Harms (WOAH)", "summary": "The detection of sensitive content in large datasets is crucial for ensuring\nthat shared and analysed data is free from harmful material. However, current\nmoderation tools, such as external APIs, suffer from limitations in\ncustomisation, accuracy across diverse sensitive categories, and privacy\nconcerns. Additionally, existing datasets and open-source models focus\npredominantly on toxic language, leaving gaps in detecting other sensitive\ncategories such as substance abuse or self-harm. In this paper, we put forward\na unified dataset tailored for social media content moderation across six\nsensitive categories: conflictual language, profanity, sexually explicit\nmaterial, drug-related content, self-harm, and spam. By collecting and\nannotating data with consistent retrieval strategies and guidelines, we address\nthe shortcomings of previous focalised research. Our analysis demonstrates that\nfine-tuning large language models (LLMs) on this novel dataset yields\nsignificant improvements in detection performance compared to open\noff-the-shelf models such as LLaMA, and even proprietary OpenAI models, which\nunderperform by 10-15% overall. This limitation is even more pronounced on\npopular moderation APIs, which cannot be easily tailored to specific sensitive\ncontent categories, among others."}
{"id": "2502.12743", "pdf": "https://arxiv.org/pdf/2502.12743.pdf", "abs": "https://arxiv.org/abs/2502.12743", "title": "\"I know myself better, but not really greatly\": How Well Can LLMs Detect and Explain LLM-Generated Texts?", "authors": ["Jiazhou Ji", "Jie Guo", "Weidong Qiu", "Zheng Huang", "Yang Xu", "Xinru Lu", "Xiaoyu Jiang", "Ruizhe Li", "Shujun Li"], "categories": ["cs.CL", "cs.AI"], "comment": "Under review", "summary": "Distinguishing between human- and LLM-generated texts is crucial given the\nrisks associated with misuse of LLMs. This paper investigates detection and\nexplanation capabilities of current LLMs across two settings: binary (human vs.\nLLM-generated) and ternary classification (including an ``undecided'' class).\nWe evaluate 6 close- and open-source LLMs of varying sizes and find that\nself-detection (LLMs identifying their own outputs) consistently outperforms\ncross-detection (identifying outputs from other LLMs), though both remain\nsuboptimal. Introducing a ternary classification framework improves both\ndetection accuracy and explanation quality across all models. Through\ncomprehensive quantitative and qualitative analyses using our human-annotated\ndataset, we identify key explanation failures, primarily reliance on inaccurate\nfeatures, hallucinations, and flawed reasoning. Our findings underscore the\nlimitations of current LLMs in self-detection and self-explanation,\nhighlighting the need for further research to address overfitting and enhance\ngeneralizability."}
{"id": "2502.17036", "pdf": "https://arxiv.org/pdf/2502.17036.pdf", "abs": "https://arxiv.org/abs/2502.17036", "title": "Language Model Re-rankers are Fooled by Lexical Similarities", "authors": ["Lovisa Hagström", "Ercong Nie", "Ruben Halifa", "Helmut Schmid", "Richard Johansson", "Alexander Junge"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to FEVER 2025", "summary": "Language model (LM) re-rankers are used to refine retrieval results for\nretrieval-augmented generation (RAG). They are more expensive than lexical\nmatching methods like BM25 but assumed to better process semantic information\nand the relations between the query and the retrieved answers. To understand\nwhether LM re-rankers always live up to this assumption, we evaluate 6\ndifferent LM re-rankers on the NQ, LitQA2 and DRUID datasets. Our results show\nthat LM re-rankers struggle to outperform a simple BM25 baseline on DRUID.\nLeveraging a novel separation metric based on BM25 scores, we explain and\nidentify re-ranker errors stemming from lexical dissimilarities. We also\ninvestigate different methods to improve LM re-ranker performance and find\nthese methods mainly useful for NQ. Taken together, our work identifies and\nexplains weaknesses of LM re-rankers and points to the need for more\nadversarial and realistic datasets for their evaluation."}
{"id": "2503.16553", "pdf": "https://arxiv.org/pdf/2503.16553.pdf", "abs": "https://arxiv.org/abs/2503.16553", "title": "A Foundational individual Mobility Prediction Model based on Open-Source Large Language Models", "authors": ["Zhenlin Qin", "Leizhen Wang", "Francisco Camara Pereira", "Zhenliang Ma"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are widely applied to domain-specific tasks due\nto their massive general knowledge and remarkable inference capacities. Current\nstudies on LLMs have shown immense potential in applying LLMs to model\nindividual mobility prediction problems. However, most LLM-based mobility\nprediction models only train on specific datasets or use single well-designed\nprompts, leading to difficulty in adapting to different cities and users with\ndiverse contexts. To fill these gaps, this paper proposes a unified fine-tuning\nframework to train a foundational open source LLM-based mobility prediction\nmodel. We conducted extensive experiments on six real-world mobility datasets\nto validate the proposed model. The results showed that the proposed model\nachieved the best performance in prediction accuracy and transferability over\nstate-of-the-art models based on deep learning and LLMs."}
{"id": "2504.08697", "pdf": "https://arxiv.org/pdf/2504.08697.pdf", "abs": "https://arxiv.org/abs/2504.08697", "title": "Large Language Models as Span Annotators", "authors": ["Zdeněk Kasner", "Vilém Zouhar", "Patrícia Schmidtová", "Ivan Kartáč", "Kristýna Onderková", "Ondřej Plátek", "Dimitra Gkatzia", "Saad Mahamood", "Ondřej Dušek", "Simone Balloccu"], "categories": ["cs.CL"], "comment": null, "summary": "Span annotation is the task of localizing and classifying text spans\naccording to custom guidelines. Annotated spans can be used to analyze and\nevaluate high-quality texts for which single-score metrics fail to provide\nactionable feedback. Until recently, span annotation was limited to human\nannotators or fine-tuned models. In this study, we show that large language\nmodels (LLMs) can serve as flexible and cost-effective span annotation\nbackbones. To demonstrate their utility, we compare LLMs to skilled human\nannotators on three diverse span annotation tasks: evaluating data-to-text\ngeneration, identifying translation errors, and detecting propaganda\ntechniques. We demonstrate that LLMs achieve inter-annotator agreement (IAA)\ncomparable to human annotators at a fraction of a cost per output annotation.\nWe also manually analyze model outputs, finding that LLMs make errors at a\nsimilar rate to human annotators. We release the dataset of more than 40k model\nand human annotations for further research."}
{"id": "2504.13816", "pdf": "https://arxiv.org/pdf/2504.13816.pdf", "abs": "https://arxiv.org/abs/2504.13816", "title": "Analyzing LLMs' Knowledge Boundary Cognition Across Languages Through the Lens of Internal Representations", "authors": ["Chenghao Xiao", "Hou Pong Chan", "Hao Zhang", "Mahani Aljunied", "Lidong Bing", "Noura Al Moubayed", "Yu Rong"], "categories": ["cs.CL"], "comment": "ACL 2025 main; camera ready", "summary": "While understanding the knowledge boundaries of LLMs is crucial to prevent\nhallucination, research on the knowledge boundaries of LLMs has predominantly\nfocused on English. In this work, we present the first study to analyze how\nLLMs recognize knowledge boundaries across different languages by probing their\ninternal representations when processing known and unknown questions in\nmultiple languages. Our empirical studies reveal three key findings: 1) LLMs'\nperceptions of knowledge boundaries are encoded in the middle to middle-upper\nlayers across different languages. 2) Language differences in knowledge\nboundary perception follow a linear structure, which motivates our proposal of\na training-free alignment method that effectively transfers knowledge boundary\nperception ability across languages, thereby helping reduce hallucination risk\nin low-resource languages; 3) Fine-tuning on bilingual question pair\ntranslation further enhances LLMs' recognition of knowledge boundaries across\nlanguages. Given the absence of standard testbeds for cross-lingual knowledge\nboundary analysis, we construct a multilingual evaluation suite comprising\nthree representative types of knowledge boundary data. Our code and datasets\nare publicly available at\nhttps://github.com/DAMO-NLP-SG/LLM-Multilingual-Knowledge-Boundaries."}
{"id": "2505.11462", "pdf": "https://arxiv.org/pdf/2505.11462.pdf", "abs": "https://arxiv.org/abs/2505.11462", "title": "Disentangling Reasoning and Knowledge in Medical Large Language Models", "authors": ["Rahul Thapa", "Qingyang Wu", "Kevin Wu", "Harrison Zhang", "Angela Zhang", "Eric Wu", "Haotian Ye", "Suhana Bedi", "Nevin Aresh", "Joseph Boen", "Shriya Reddy", "Ben Athiwaratkun", "Shuaiwen Leon Song", "James Zou"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Medical reasoning in large language models (LLMs) aims to emulate clinicians'\ndiagnostic thinking, but current benchmarks such as MedQA-USMLE, MedMCQA, and\nPubMedQA often mix reasoning with factual recall. We address this by separating\n11 biomedical QA benchmarks into reasoning- and knowledge-focused subsets using\na PubMedBERT classifier that reaches 81 percent accuracy, comparable to human\nperformance. Our analysis shows that only 32.8 percent of questions require\ncomplex reasoning. We evaluate biomedical models (HuatuoGPT-o1, MedReason, m1)\nand general-domain models (DeepSeek-R1, o4-mini, Qwen3), finding consistent\ngaps between knowledge and reasoning performance. For example, HuatuoGPT-o1\nscores 56.9 on knowledge but only 44.8 on reasoning. In adversarial tests where\nmodels are misled with incorrect initial reasoning, biomedical models degrade\nsharply, while larger or RL-trained general models show more robustness. To\naddress this, we train BioMed-R1 using fine-tuning and reinforcement learning\non reasoning-heavy examples. It achieves the strongest performance among\nsimilarly sized models. Further gains may come from incorporating clinical case\nreports and training with adversarial and backtracking scenarios."}
{"id": "2505.16078", "pdf": "https://arxiv.org/pdf/2505.16078.pdf", "abs": "https://arxiv.org/abs/2505.16078", "title": "Small Language Models in the Real World: Insights from Industrial Text Classification", "authors": ["Lujun Li", "Lama Sleem", "Niccolo' Gentile", "Geoffrey Nichil", "Radu State"], "categories": ["cs.CL"], "comment": "This paper has been accepted as a conference paper in the Industry\n  Track of the 63rd Annual Meeting of the Association for Computational\n  Linguistics (ACL)", "summary": "With the emergence of ChatGPT, Transformer models have significantly advanced\ntext classification and related tasks. Decoder-only models such as Llama\nexhibit strong performance and flexibility, yet they suffer from inefficiency\non inference due to token-by-token generation, and their effectiveness in text\nclassification tasks heavily depends on prompt quality. Moreover, their\nsubstantial GPU resource requirements often limit widespread adoption. Thus,\nthe question of whether smaller language models are capable of effectively\nhandling text classification tasks emerges as a topic of significant interest.\nHowever, the selection of appropriate models and methodologies remains largely\nunderexplored. In this paper, we conduct a comprehensive evaluation of prompt\nengineering and supervised fine-tuning methods for transformer-based text\nclassification. Specifically, we focus on practical industrial scenarios,\nincluding email classification, legal document categorization, and the\nclassification of extremely long academic texts. We examine the strengths and\nlimitations of smaller models, with particular attention to both their\nperformance and their efficiency in Video Random-Access Memory (VRAM)\nutilization, thereby providing valuable insights for the local deployment and\napplication of compact models in industrial settings."}
{"id": "2505.23966", "pdf": "https://arxiv.org/pdf/2505.23966.pdf", "abs": "https://arxiv.org/abs/2505.23966", "title": "FLAT-LLM: Fine-grained Low-rank Activation Space Transformation for Large Language Model Compression", "authors": ["Jiayi Tian", "Ryan Solgi", "Jinming Lu", "Yifan Yang", "Hai Li", "Zheng Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have enabled remarkable progress in natural\nlanguage processing, yet their high computational and memory demands pose\nchallenges for deployment in resource-constrained environments. Although recent\nlow-rank decomposition methods offer a promising path for structural\ncompression, they often suffer from accuracy degradation, expensive calibration\nprocedures, and result in inefficient model architectures that hinder\nreal-world inference speedups. In this paper, we propose FLAT-LLM, a fast and\naccurate, training-free structural compression method based on fine-grained\nlow-rank transformations in the activation space. Specifically, we reduce the\nhidden dimension by transforming the weights using truncated eigenvectors\ncomputed via head-wise Principal Component Analysis (PCA), and employ an\nimportance-based metric to adaptively allocate ranks across decoders. FLAT-LLM\nachieves efficient and effective weight compression without recovery\nfine-tuning, which could complete the calibration within a few minutes.\nEvaluated across 4 models and 11 datasets, FLAT-LLM outperforms structural\npruning baselines in generalization and downstream performance, while\ndelivering inference speedups over decomposition-based methods."}
{"id": "2506.06609", "pdf": "https://arxiv.org/pdf/2506.06609.pdf", "abs": "https://arxiv.org/abs/2506.06609", "title": "Transferring Features Across Language Models With Model Stitching", "authors": ["Alan Chen", "Jack Merullo", "Alessandro Stolfo", "Ellie Pavlick"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "In this work, we demonstrate that affine mappings between residual streams of\nlanguage models is a cheap way to effectively transfer represented features\nbetween models. We apply this technique to transfer the weights of Sparse\nAutoencoders (SAEs) between models of different sizes to compare their\nrepresentations. We find that small and large models learn similar\nrepresentation spaces, which motivates training expensive components like SAEs\non a smaller model and transferring to a larger model at a FLOPs savings. In\nparticular, using a small-to-large transferred SAE as initialization can lead\nto 50% cheaper training runs when training SAEs on larger models. Next, we show\nthat transferred probes and steering vectors can effectively recover ground\ntruth performance. Finally, we dive deeper into feature-level transferability,\nfinding that semantic and structural features transfer noticeably differently\nwhile specific classes of functional features have their roles faithfully\nmapped. Overall, our findings illustrate similarities and differences in the\nlinear representation spaces of small and large models and demonstrate a method\nfor improving the training efficiency of SAEs."}
{"id": "2506.06877", "pdf": "https://arxiv.org/pdf/2506.06877.pdf", "abs": "https://arxiv.org/abs/2506.06877", "title": "Right Is Not Enough: The Pitfalls of Outcome Supervision in Training LLMs for Math Reasoning", "authors": ["Jiaxing Guo", "Wenjie Yang", "Shengzhong Zhang", "Tongshan Xu", "Lun Du", "Da Zheng", "Zengfeng Huang"], "categories": ["cs.CL"], "comment": null, "summary": "Outcome-rewarded Large Language Models (LLMs) have demonstrated remarkable\nsuccess in mathematical problem-solving. However, this success often masks a\ncritical issue: models frequently achieve correct answers through fundamentally\nunsound reasoning processes, a phenomenon indicative of reward hacking. We\nintroduce MathOlympiadEval, a new dataset with fine-grained annotations, which\nreveals a significant gap between LLMs' answer correctness and their low\nprocess correctness. Existing automated methods like LLM-as-a-judge struggle to\nreliably detect these reasoning flaws. To address this, we propose\nParaStepVerifier, a novel methodology for meticulous, step-by-step verification\nof mathematical solutions. ParaStepVerifier identifies incorrect reasoning\nsteps. Empirical results demonstrate that ParaStepVerifier substantially\nimproves the accuracy of identifying flawed solutions compared to baselines,\nespecially for complex, multi-step problems. This offers a more robust path\ntowards evaluating and training LLMs with genuine mathematical reasoning."}
{"id": "2506.11903", "pdf": "https://arxiv.org/pdf/2506.11903.pdf", "abs": "https://arxiv.org/abs/2506.11903", "title": "GeistBERT: Breathing Life into German NLP", "authors": ["Raphael Scheible-Schmitt", "Johann Frei"], "categories": ["cs.CL"], "comment": null, "summary": "Advances in transformer-based language models have highlighted the benefits\nof language-specific pre-training on high-quality corpora. In this context,\nGerman NLP stands to gain from updated architectures and modern datasets\ntailored to the linguistic characteristics of the German language. GeistBERT\nseeks to improve German language processing by incrementally training on a\ndiverse corpus and optimizing model performance across various NLP tasks. It\nwas pre-trained using fairseq with standard hyperparameters, initialized from\nGottBERT weights, and trained on a large-scale German corpus using Whole Word\nMasking (WWM). Based on the pre-trained model, we derived extended-input\nvariants using Nystr\\\"omformer and Longformer architectures with support for\nsequences up to 8k tokens. While these long-context models were not evaluated\non dedicated long-context benchmarks, they are included in our release. We\nassessed all models on NER (CoNLL 2003, GermEval 2014) and text classification\n(GermEval 2018 fine/coarse, 10kGNAD) using $F_1$ score and accuracy. The\nGeistBERT models achieved strong performance, leading all tasks among the base\nmodels and setting a new state-of-the-art (SOTA). Notably, the base models\noutperformed larger models in several tasks. To support the German NLP research\ncommunity, we are releasing GeistBERT under the MIT license."}
{"id": "2506.16640", "pdf": "https://arxiv.org/pdf/2506.16640.pdf", "abs": "https://arxiv.org/abs/2506.16640", "title": "Long-Context Generalization with Sparse Attention", "authors": ["Pavlo Vasylenko", "Marcos Treviso", "André F. T. Martins"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Transformer-based architectures traditionally employ softmax to compute\nattention weights, which produces dense distributions over all tokens in a\nsequence. While effective in many settings, this density has been shown to be\ndetrimental for tasks that demand precise focus on fixed-size patterns: as\nsequence length increases, non-informative tokens accumulate attention\nprobability mass, leading to dispersion and representational collapse. We show\nin this paper that sparse attention mechanisms using $\\alpha$-entmax can avoid\nthese issues, due to their ability to assign exact zeros to irrelevant tokens.\nFurthermore, we introduce Adaptive-Scalable Entmax (ASEntmax), which endows\n$\\alpha$-entmax with a learnable temperature parameter, allowing the attention\ndistribution to interpolate between sparse (pattern-focused) and dense\n(softmax-like) regimes. Finally, we show that the ability to locate and\ngeneralize fixed-size patterns can be further improved through a careful design\nof position encodings, which impacts both dense and sparse attention methods.\nBy integrating ASEntmax into standard transformer layers alongside proper\npositional encodings, we show that our models greatly outperform softmax,\nscalable softmax, and fixed-temperature $\\alpha$-entmax baselines on\nlong-context generalization."}
{"id": "2506.17728", "pdf": "https://arxiv.org/pdf/2506.17728.pdf", "abs": "https://arxiv.org/abs/2506.17728", "title": "KAG-Thinker: Interactive Thinking and Deep Reasoning in LLMs via Knowledge-Augmented Generation", "authors": ["Dalong Zhang", "Jun Xu", "Jun Zhou", "Lei Liang", "Lin Yuan", "Ling Zhong", "Mengshu Sun", "Peilong Zhao", "QiWei Wang", "Xiaorui Wang", "Xinkai Du", "YangYang Hou", "Yu Ao", "ZhaoYang Wang", "Zhengke Gui", "ZhiYing Yi", "Zhongpu Bo"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this paper, we introduce KAG-Thinker, which upgrade KAG to a multi-turn\ninteractive thinking and deep reasoning framework powered by a dedicated\nparameter-light large language model (LLM). Our approach constructs a\nstructured thinking process for solving complex problems, enhancing the the\nlogical coherence and contextual consistency of the reasoning process in\nquestion-answering (Q&A) tasks on domain-specific knowledge bases (KBs) within\nLLMs. Following the \\textbf{Logical Form} guided retrieval and reasoning\ntechnology route of KAG, this framework first decomposes complex questions into\nindependently solvable sub-problems (which are also referred to as logical\nforms) through \\textbf{breadth decomposition}. Each such logical form is\nrepresented in two equivalent forms-natural language and logical function-and\nsubsequently classified as either a Knowledge Retrieval or Reasoning Analysis\ntask. Dependencies and parameter passing between these tasks are explicitly\nmodeled via logical function interfaces. In the solving process, the Retrieval\nfunction performs retrieval tasks. It retrieves one-hop structured and\nunstructured information of specified knowledge unit. While the Math and Deduce\nfunctions are used to perform reasoning analysis tasks. Secondly, it is worth\nnoting that, in the Knowledge Retrieval sub-problem tasks, LLMs and external\nknowledge sources are regarded as equivalent KBs. We use the \\textbf{knowledge\nboundary} module to determine the optimal source using self-regulatory\nmechanisms such as confidence calibration and reflective reasoning, and use the\n\\textbf{depth solving} module to enhance the comprehensiveness of knowledge\nacquisition..."}
{"id": "2506.17789", "pdf": "https://arxiv.org/pdf/2506.17789.pdf", "abs": "https://arxiv.org/abs/2506.17789", "title": "Multilingual Tokenization through the Lens of Indian Languages: Challenges and Insights", "authors": ["N J Karthika", "Maharaj Brahma", "Rohit Saluja", "Ganesh Ramakrishnan", "Maunendra Sankar Desarkar"], "categories": ["cs.CL"], "comment": null, "summary": "Tokenization plays a pivotal role in multilingual NLP. However, existing\ntokenizers are often skewed towards high-resource languages, limiting their\neffectiveness for linguistically diverse and morphologically rich languages\nsuch as those in the Indian subcontinent. This paper presents a comprehensive\nintrinsic evaluation of tokenization strategies across 17 Indian languages. We\nquantify the trade-offs between bottom-up and top-down tokenizer algorithms\n(BPE and Unigram LM), effects of vocabulary sizes, and compare strategies of\nmultilingual vocabulary construction such as joint and cluster-based training.\nWe also show that extremely low-resource languages can benefit from tokenizers\ntrained on related high-resource languages. Our study provides practical\ninsights for building more fair, efficient, and linguistically informed\ntokenizers for multilingual NLP."}
{"id": "2506.18082", "pdf": "https://arxiv.org/pdf/2506.18082.pdf", "abs": "https://arxiv.org/abs/2506.18082", "title": "Statistical Multicriteria Evaluation of LLM-Generated Text", "authors": ["Esteban Garces Arias", "Hannah Blocher", "Julian Rodemann", "Matthias Aßenmacher", "Christoph Jansen"], "categories": ["cs.CL", "stat.AP"], "comment": null, "summary": "Assessing the quality of LLM-generated text remains a fundamental challenge\nin natural language processing. Current evaluation approaches often rely on\nisolated metrics or simplistic aggregations that fail to capture the nuanced\ntrade-offs between coherence, diversity, fluency, and other relevant indicators\nof text quality. In this work, we adapt a recently proposed framework for\nstatistical inference based on Generalized Stochastic Dominance (GSD) that\naddresses three critical limitations in existing benchmarking methodologies:\nthe inadequacy of single-metric evaluation, the incompatibility between\ncardinal automatic metrics and ordinal human judgments, and the lack of\ninferential statistical guarantees. The GSD-front approach enables simultaneous\nevaluation across multiple quality dimensions while respecting their different\nmeasurement scales, building upon partial orders of decoding strategies, thus\navoiding arbitrary weighting of the involved metrics. By applying this\nframework to evaluate common decoding strategies against human-generated text,\nwe demonstrate its ability to identify statistically significant performance\ndifferences while accounting for potential deviations from the i.i.d.\nassumption of the sampling design."}
{"id": "2506.18710", "pdf": "https://arxiv.org/pdf/2506.18710.pdf", "abs": "https://arxiv.org/abs/2506.18710", "title": "Benchmarking the Pedagogical Knowledge of Large Language Models", "authors": ["Maxime Lelièvre", "Amy Waldock", "Meng Liu", "Natalia Valdés Aspillaga", "Alasdair Mackintosh", "María José Ogando Portela", "Jared Lee", "Paul Atherton", "Robin A. A. Ince", "Oliver G. B. Garrod"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Benchmarks like Massive Multitask Language Understanding (MMLU) have played a\npivotal role in evaluating AI's knowledge and abilities across diverse domains.\nHowever, existing benchmarks predominantly focus on content knowledge, leaving\na critical gap in assessing models' understanding of pedagogy - the method and\npractice of teaching. This paper introduces The Pedagogy Benchmark, a novel\ndataset designed to evaluate large language models on their Cross-Domain\nPedagogical Knowledge (CDPK) and Special Education Needs and Disability (SEND)\npedagogical knowledge. These benchmarks are built on a carefully curated set of\nquestions sourced from professional development exams for teachers, which cover\na range of pedagogical subdomains such as teaching strategies and assessment\nmethods. Here we outline the methodology and development of these benchmarks.\nWe report results for 97 models, with accuracies spanning a range from 28% to\n89% on the pedagogical knowledge questions. We consider the relationship\nbetween cost and accuracy and chart the progression of the Pareto value\nfrontier over time. We provide online leaderboards at\nhttps://rebrand.ly/pedagogy which are updated with new models and allow\ninteractive exploration and filtering based on various model properties, such\nas cost per token and open-vs-closed weights, as well as looking at performance\nin different subjects. LLMs and generative AI have tremendous potential to\ninfluence education and help to address the global learning crisis.\nEducation-focused benchmarks are crucial to measure models' capacities to\nunderstand pedagogical concepts, respond appropriately to learners' needs, and\nsupport effective teaching practices across diverse contexts. They are needed\nfor informing the responsible and evidence-based deployment of LLMs and\nLLM-based tools in educational settings, and for guiding both development and\npolicy decisions."}
{"id": "2405.10213", "pdf": "https://arxiv.org/pdf/2405.10213.pdf", "abs": "https://arxiv.org/abs/2405.10213", "title": "Words as Trigger Points in Social Media Discussions: A Large-Scale Case Study about UK Politics on Reddit", "authors": ["Dimosthenis Antypas", "Christian Arnold", "Jose Camacho-Collados", "Nedjma Ousidhoum", "Carla Perez Almendros"], "categories": ["cs.SI", "cs.CL", "cs.CY"], "comment": null, "summary": "Political debates on social media sometimes flare up. From that moment on,\nusers engage much more with one another; their communication is also more\nemotional and polarised. While it has been difficult to grasp such moments with\ncomputational methods, we suggest that trigger points are a useful concept to\nunderstand and ultimately model such behaviour. Established in qualitative\nfocus group interviews to understand political polarisation (Mau, Lux, and\nWestheuser 2023), trigger points represent moments when individuals feel that\ntheir understanding of what is fair, normal, or appropriate in society is\nquestioned. In the original studies, individuals show strong and negative\nemotional responses when certain triggering words or topics are mentioned. Our\npaper finds that these trigger points also exist in online debates. We examine\nonline deliberations on Reddit between 2020 and 2022 and collect >100 million\ncomments from subreddits related to a set of words identified as trigger points\nin UK politics. Analysing the comments, we find that trigger words increase\nuser engagement and animosity, i.e., more negativity, hate speech, and\ncontroversial comments. Introducing trigger points to computational studies of\nonline communication, our findings are relevant to researchers interested in\naffective computing, online deliberation, and how citizens debate politics and\nsociety in light of affective polarisation."}
{"id": "2406.05410", "pdf": "https://arxiv.org/pdf/2406.05410.pdf", "abs": "https://arxiv.org/abs/2406.05410", "title": "ChatSR: Multimodal Large Language Models for Scientific Formula Discovery", "authors": ["Yanjie Li", "Lina Yu", "Weijun Li", "Min Wu", "Jingyi Liu", "Wenqiang Li", "Shu Wei", "Yusong Deng"], "categories": ["cs.AI", "cs.CL"], "comment": "23 pages,", "summary": "Formulas are the language of communication between humans and nature. The\ndiscovery of formulas to describe natural laws from observational data is the\npurpose of scientific research. It is also an important research topic in\nartificial intelligence, which is called a symbolic regression problem. Most of\nthe existing symbolic regression methods generate expressions directly from\nobserved data. Although in some methods, we can inject some prior knowledge\ninto the model by adding constraints or introducing some special character\nhints. However, these methods can only introduce a limited amount of prior\nknowledge specified in advance. Not to mention understanding natural language\ninstructions. In this article, based on the powerful knowledge reserve and\nlanguage understanding ability of multi-modal large language models, we present\nChatSR, which acts like a knowledgeable human scientist, and we can tell it any\nprior knowledge through natural language to guide it in formula generation. By\ntesting on 13 datasets, ChatSR not only shows state-of-the-art performance on\ntraditional symbolic regression tasks. More notably, ChatSR can well understand\nthe prior knowledge contained in natural language prompts and improve the\nquality of generated expressions. In addition, it is exciting that ChatSR has a\ngood zero-shot capability to understand prior knowledge that is not present in\nthe training data."}
{"id": "2501.10326", "pdf": "https://arxiv.org/pdf/2501.10326.pdf", "abs": "https://arxiv.org/abs/2501.10326", "title": "Large language models for automated scholarly paper review: A survey", "authors": ["Zhenzhen Zhuang", "Jiandong Chen", "Hongfeng Xu", "Yuwen Jiang", "Jialiang Lin"], "categories": ["cs.AI", "cs.CL", "cs.DL"], "comment": "Please cite the version of Information Fusion", "summary": "Large language models (LLMs) have significantly impacted human society,\ninfluencing various domains. Among them, academia is not simply a domain\naffected by LLMs, but it is also the pivotal force in the development of LLMs.\nIn academic publication, this phenomenon is represented during the\nincorporation of LLMs into the peer review mechanism for reviewing manuscripts.\nLLMs hold transformative potential for the full-scale implementation of\nautomated scholarly paper review (ASPR), but they also pose new issues and\nchallenges that need to be addressed. In this survey paper, we aim to provide a\nholistic view of ASPR in the era of LLMs. We begin with a survey to find out\nwhich LLMs are used to conduct ASPR. Then, we review what ASPR-related\ntechnological bottlenecks have been solved with the incorporation of LLM\ntechnology. After that, we move on to explore new methods, new datasets, new\nsource code, and new online systems that come with LLMs for ASPR. Furthermore,\nwe summarize the performance and issues of LLMs in ASPR, and investigate the\nattitudes and reactions of publishers and academia to ASPR. Lastly, we discuss\nthe challenges and future directions associated with the development of LLMs\nfor ASPR. This survey serves as an inspirational reference for the researchers\nand can promote the progress of ASPR for its actual implementation."}
{"id": "2502.00258", "pdf": "https://arxiv.org/pdf/2502.00258.pdf", "abs": "https://arxiv.org/abs/2502.00258", "title": "ProxSparse: Regularized Learning of Semi-Structured Sparsity Masks for Pretrained LLMs", "authors": ["Hongyi Liu", "Rajarshi Saha", "Zhen Jia", "Youngsuk Park", "Jiaji Huang", "Shoham Sabach", "Yu-Xiang Wang", "George Karypis"], "categories": ["cs.LG", "cs.CL"], "comment": "ICML25", "summary": "Large Language Models (LLMs) have demonstrated exceptional performance in\nnatural language processing tasks, yet their massive size makes serving them\ninefficient and costly. Semi-structured pruning has emerged as an effective\nmethod for model acceleration, but existing approaches are suboptimal because\nthey focus on local, layer-wise optimizations using heuristic rules, failing to\nleverage global feedback. We present ProxSparse, a learning-based framework for\nmask selection enabled by regularized optimization. ProxSparse transforms the\nrigid, non-differentiable mask selection process into a smoother optimization\nprocedure, allowing gradual mask exploration with flexibility. ProxSparse does\nnot involve additional weight updates once the mask is determined. Our\nextensive evaluations on 7 widely used models show that ProxSparse consistently\noutperforms previously proposed semi-structured mask selection methods with\nsignificant improvement, demonstrating the effectiveness of our learned\napproach towards semi-structured pruning."}
{"id": "2503.09730", "pdf": "https://arxiv.org/pdf/2503.09730.pdf", "abs": "https://arxiv.org/abs/2503.09730", "title": "Local Look-Ahead Guidance via Verifier-in-the-Loop for Automated Theorem Proving", "authors": ["Sara Rajaee", "Kumar Pratik", "Gabriele Cesa", "Arash Behboodi"], "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.LO"], "comment": "Accepted at the Findings of ACL 2025, Accepted at ICLR 2025 Workshop\n  on Reasoning and Planning for Large Language Models", "summary": "The most promising recent methods for AI reasoning require applying variants\nof reinforcement learning (RL) either on rolled out trajectories from the LLMs,\neven for the step-wise rewards, or large quantities of human-annotated\ntrajectory data. The reliance on the rolled-out trajectory renders the compute\ncost and time prohibitively high. In particular, the correctness of a reasoning\ntrajectory can typically only be judged at its completion, leading to sparse\nrewards in RL or requiring expensive synthetic data generation in expert\niteration-like methods. In this work, we focus on the Automatic Theorem Proving\n(ATP) task and propose a novel verifier-in-the-loop design, which, unlike\nexisting approaches that leverage feedback on the entire reasoning trajectory,\nemploys an automated verifier to give intermediate feedback at each step of the\nreasoning process. Using Lean as the verifier, we empirically show that the\nstep-by-step local verification produces a global improvement in the model's\nreasoning accuracy and efficiency."}
{"id": "2504.16828", "pdf": "https://arxiv.org/pdf/2504.16828.pdf", "abs": "https://arxiv.org/abs/2504.16828", "title": "Process Reward Models That Think", "authors": ["Muhammad Khalifa", "Rishabh Agarwal", "Lajanugen Logeswaran", "Jaekyeom Kim", "Hao Peng", "Moontae Lee", "Honglak Lee", "Lu Wang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Step-by-step verifiers -- also known as process reward models (PRMs) -- are a\nkey ingredient for test-time scaling. PRMs require step-level supervision,\nmaking them expensive to train. This work aims to build data-efficient PRMs as\nverbalized step-wise reward models that verify every step in the solution by\ngenerating a verification chain-of-thought (CoT). We propose ThinkPRM, a long\nCoT verifier fine-tuned on orders of magnitude fewer process labels than those\nrequired by discriminative PRMs. Our approach capitalizes on the inherent\nreasoning abilities of long CoT models, and outperforms LLM-as-a-Judge and\ndiscriminative verifiers -- using only 1% of the process labels in PRM800K --\nacross several challenging benchmarks. Specifically, ThinkPRM beats the\nbaselines on ProcessBench, MATH-500, and AIME '24 under best-of-N selection and\nreward-guided search. In an out-of-domain evaluation on a subset of\nGPQA-Diamond and LiveCodeBench, our PRM surpasses discriminative verifiers\ntrained on the full PRM800K by 8% and 4.5%, respectively. Lastly, under the\nsame token budget, ThinkPRM scales up verification compute more effectively\ncompared to LLM-as-a-Judge, outperforming it by 7.2% on a subset of\nProcessBench. Our work highlights the value of generative, long CoT PRMs that\ncan scale test-time compute for verification while requiring minimal\nsupervision for training. Our code, data, and models will be released at\nhttps://github.com/mukhal/thinkprm."}
{"id": "2505.08638", "pdf": "https://arxiv.org/pdf/2505.08638.pdf", "abs": "https://arxiv.org/abs/2505.08638", "title": "TRAIL: Trace Reasoning and Agentic Issue Localization", "authors": ["Darshan Deshpande", "Varun Gangal", "Hersh Mehta", "Jitin Krishnan", "Anand Kannappan", "Rebecca Qian"], "categories": ["cs.AI", "cs.CL"], "comment": "Dataset: https://huggingface.co/datasets/PatronusAI/TRAIL", "summary": "The increasing adoption of agentic workflows across diverse domains brings a\ncritical need to scalably and systematically evaluate the complex traces these\nsystems generate. Current evaluation methods depend on manual, domain-specific\nhuman analysis of lengthy workflow traces - an approach that does not scale\nwith the growing complexity and volume of agentic outputs. Error analysis in\nthese settings is further complicated by the interplay of external tool outputs\nand language model reasoning, making it more challenging than traditional\nsoftware debugging. In this work, we (1) articulate the need for robust and\ndynamic evaluation methods for agentic workflow traces, (2) introduce a formal\ntaxonomy of error types encountered in agentic systems, and (3) present a set\nof 148 large human-annotated traces (TRAIL) constructed using this taxonomy and\ngrounded in established agentic benchmarks. To ensure ecological validity, we\ncurate traces from both single and multi-agent systems, focusing on real-world\napplications such as software engineering and open-world information retrieval.\nOur evaluations reveal that modern long context LLMs perform poorly at trace\ndebugging, with the best Gemini-2.5-pro model scoring a mere 11% on TRAIL. Our\ndataset and code are made publicly available to support and accelerate future\nresearch in scalable evaluation for agentic workflows."}
{"id": "2506.10412", "pdf": "https://arxiv.org/pdf/2506.10412.pdf", "abs": "https://arxiv.org/abs/2506.10412", "title": "Time-IMM: A Dataset and Benchmark for Irregular Multimodal Multivariate Time Series", "authors": ["Ching Chang", "Jeehyun Hwang", "Yidan Shi", "Haixin Wang", "Wen-Chih Peng", "Tien-Fu Chen", "Wei Wang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "This paper is currently under review", "summary": "Time series data in real-world applications such as healthcare, climate\nmodeling, and finance are often irregular, multimodal, and messy, with varying\nsampling rates, asynchronous modalities, and pervasive missingness. However,\nexisting benchmarks typically assume clean, regularly sampled, unimodal data,\ncreating a significant gap between research and real-world deployment. We\nintroduce Time-IMM, a dataset specifically designed to capture cause-driven\nirregularity in multimodal multivariate time series. Time-IMM represents nine\ndistinct types of time series irregularity, categorized into trigger-based,\nconstraint-based, and artifact-based mechanisms. Complementing the dataset, we\nintroduce IMM-TSF, a benchmark library for forecasting on irregular multimodal\ntime series, enabling asynchronous integration and realistic evaluation.\nIMM-TSF includes specialized fusion modules, including a timestamp-to-text\nfusion module and a multimodality fusion module, which support both\nrecency-aware averaging and attention-based integration strategies. Empirical\nresults demonstrate that explicitly modeling multimodality on irregular time\nseries data leads to substantial gains in forecasting performance. Time-IMM and\nIMM-TSF provide a foundation for advancing time series analysis under\nreal-world conditions. The dataset is publicly available at\nhttps://www.kaggle.com/datasets/blacksnail789521/time-imm/data, and the\nbenchmark library can be accessed at\nhttps://anonymous.4open.science/r/IMMTSF_NeurIPS2025."}
{"id": "2506.11555", "pdf": "https://arxiv.org/pdf/2506.11555.pdf", "abs": "https://arxiv.org/abs/2506.11555", "title": "RAG+: Enhancing Retrieval-Augmented Generation with Application-Aware Reasoning", "authors": ["Yu Wang", "Shiwan Zhao", "Zhihu Wang", "Yubo Zhang", "Xicheng Zhang", "Zhengfan Wang", "Heyuan Huang", "Ming Fan", "Ting Liu"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "The integration of external knowledge through Retrieval-Augmented Generation\n(RAG) has become foundational in enhancing large language models (LLMs) for\nknowledge-intensive tasks. However, existing RAG paradigms often overlook the\ncognitive step of applying knowledge, leaving a gap between retrieved facts and\ntask-specific reasoning. In this work, we introduce RAG+, a principled and\nmodular extension that explicitly incorporates application-aware reasoning into\nthe RAG pipeline. RAG+ constructs a dual corpus consisting of knowledge and\naligned application examples, created either manually or automatically, and\nretrieves both jointly during inference. This design enables LLMs not only to\naccess relevant information but also to apply it within structured,\ngoal-oriented reasoning processes. Experiments across mathematical, legal, and\nmedical domains, conducted on multiple models, demonstrate that RAG+\nconsistently outperforms standard RAG variants, achieving average improvements\nof 3-5%, and peak gains up to 7.5% in complex scenarios. By bridging retrieval\nwith actionable application, RAG+ advances a more cognitively grounded\nframework for knowledge integration, representing a step toward more\ninterpretable and capable LLMs."}
{"id": "2506.11558", "pdf": "https://arxiv.org/pdf/2506.11558.pdf", "abs": "https://arxiv.org/abs/2506.11558", "title": "DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning with Video LLMs", "authors": ["Bo-Cheng Chiu", "Jen-Jee Chen", "Yu-Chee Tseng", "Feng-Chi Chen"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "I would like to request the withdrawal of this submission because the\n  current version contains significant errors and incomplete results. I intend\n  to revise the manuscript thoroughly before resubmitting. I apologize for the\n  oversight and appreciate your understanding", "summary": "Large Language Models (LLMs) have recently been extended to the video domain,\nenabling sophisticated video-language understanding. However, existing Video\nLLMs often exhibit limitations in fine-grained temporal reasoning, restricting\ntheir ability to precisely attribute responses to specific video moments,\nespecially under constrained supervision. We introduce DaMO, a data-efficient\nVideo LLM explicitly designed for accurate temporal reasoning and multimodal\nunderstanding. At its core, the proposed Temporal-aware Fuseformer employs a\nhierarchical dual-stream architecture that progressively captures temporal\ndynamics within each modality and effectively fuses complementary visual and\naudio information. To further enhance computational efficiency, DaMO integrates\na global residual that reduces spatial redundancy while preserving essential\nsemantic details. We train DaMO via a structured four-stage progressive\ntraining paradigm, incrementally equipping the model with multimodal alignment,\nsemantic grounding, and temporal reasoning capabilities. This work also\ncontributes multiple datasets augmented from existing ones with GPT-generated\ntemporally grounded QA pairs for tasks requiring temporal supervision.\nComprehensive experiments on temporal grounding and video QA benchmarks\ndemonstrate that DaMO consistently surpasses prior methods, particularly in\ntasks demanding precise temporal alignment and reasoning. Our work establishes\na promising direction for data-efficient video-language modeling."}
{"id": "2506.18631", "pdf": "https://arxiv.org/pdf/2506.18631.pdf", "abs": "https://arxiv.org/abs/2506.18631", "title": "ReDit: Reward Dithering for Improved LLM Policy Optimization", "authors": ["Chenxing Wei", "Jiarui Yu", "Ying Tiffany He", "Hande Dong", "Yao Shu", "Fei Yu"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "10 pages, 15 figures", "summary": "DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning\ncapabilities through its rule-based reward system. While it's a ''perfect''\nreward system that effectively mitigates reward hacking, such reward functions\nare often discrete. Our experimental observations suggest that discrete rewards\ncan lead to gradient anomaly, unstable optimization, and slow convergence. To\naddress this issue, we propose ReDit (Reward Dithering), a method that dithers\nthe discrete reward signal by adding simple random noise. With this perturbed\nreward, exploratory gradients are continuously provided throughout the learning\nprocess, enabling smoother gradient updates and accelerating convergence. The\ninjected noise also introduces stochasticity into flat reward regions,\nencouraging the model to explore novel policies and escape local optima.\nExperiments across diverse tasks demonstrate the effectiveness and efficiency\nof ReDit. On average, ReDit achieves performance comparable to vanilla GRPO\nwith only approximately 10% the training steps, and furthermore, still exhibits\na 4% performance improvement over vanilla GRPO when trained for a similar\nduration. Visualizations confirm significant mitigation of gradient issues with\nReDit. Moreover, theoretical analyses are provided to further validate these\nadvantages."}
{"id": "2506.18810", "pdf": "https://arxiv.org/pdf/2506.18810.pdf", "abs": "https://arxiv.org/abs/2506.18810", "title": "ConciseHint: Boosting Efficient Reasoning via Continuous Concise Hints during Generation", "authors": ["Siao Tang", "Xinyin Ma", "Gongfan Fang", "Xinchao Wang"], "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "Codes are available at https://github.com/tsa18/ConciseHint", "summary": "Recent advancements in large reasoning models (LRMs) like DeepSeek-R1 and\nOpenAI o1 series have achieved notable performance enhancements on complex\nreasoning tasks by scaling up the generation length by Chain-of-Thought (CoT).\nHowever, an emerging issue is their inclination to produce excessively verbose\nreasoning processes, leading to the inefficiency problem. Existing literature\non improving efficiency mainly adheres to the before-reasoning paradigms such\nas prompting and reasoning or fine-tuning and reasoning, but ignores the\npromising direction of directly encouraging the model to speak concisely by\nintervening during the generation of reasoning. In order to fill the blank, we\npropose a framework dubbed ConciseHint, which continuously encourages the\nreasoning model to speak concisely by injecting the textual hint (manually\ndesigned or trained on the concise data) during the token generation of the\nreasoning process. Besides, ConciseHint is adaptive to the complexity of the\nquery by adaptively adjusting the hint intensity, which ensures it will not\nundermine model performance. Experiments on the state-of-the-art LRMs,\nincluding DeepSeek-R1 and Qwen-3 series, demonstrate that our method can\neffectively produce concise reasoning processes while maintaining performance\nwell. For instance, we achieve a reduction ratio of 65\\% for the reasoning\nlength on GSM8K benchmark with Qwen-3 4B with nearly no accuracy loss."}
{"id": "2506.18902", "pdf": "https://arxiv.org/pdf/2506.18902.pdf", "abs": "https://arxiv.org/abs/2506.18902", "title": "jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval", "authors": ["Michael Günther", "Saba Sturua", "Mohammad Kalim Akram", "Isabelle Mohr", "Andrei Ungureanu", "Bo Wang", "Sedigheh Eslami", "Scott Martens", "Maximilian Werk", "Nan Wang", "Han Xiao"], "categories": ["cs.AI", "cs.CL", "cs.IR", "68T50", "I.2.7"], "comment": "22 pages, 1-10 main, 14-22 experimental results, benchmark tables", "summary": "We introduce jina-embeddings-v4, a 3.8 billion parameter multimodal embedding\nmodel that unifies text and image representations through a novel architecture\nsupporting both single-vector and multi-vector embeddings in the late\ninteraction style. The model incorporates task-specific Low-Rank Adaptation\n(LoRA) adapters to optimize performance across diverse retrieval scenarios,\nincluding query-document retrieval, semantic text similarity, and code search.\nComprehensive evaluations demonstrate that jina-embeddings-v4 achieves\nstate-of-the-art performance on both single-modal and cross-modal retrieval\ntasks, with particular strength in processing visually rich content such as\ntables, charts, diagrams, and mixed-media formats. To facilitate evaluation of\nthis capability, we also introduce Jina-VDR, a novel benchmark specifically\ndesigned for visually rich image retrieval."}
