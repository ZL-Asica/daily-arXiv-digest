{"id": "2509.10637", "pdf": "https://arxiv.org/pdf/2509.10637.pdf", "abs": "https://arxiv.org/abs/2509.10637", "title": "LLMs Homogenize Values in Constructive Arguments on Value-Laden Topics", "authors": ["Farhana Shahid", "Stella Zhang", "Aditya Vashistha"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Large language models (LLMs) are increasingly used to promote prosocial and\nconstructive discourse online. Yet little is known about how they negotiate and\nshape underlying values when reframing people's arguments on value-laden\ntopics. We conducted experiments with 347 participants from India and the\nUnited States, who wrote constructive comments on homophobic and Islamophobic\nthreads, and reviewed human-written and LLM-rewritten versions of these\ncomments. Our analysis shows that LLM systematically diminishes Conservative\nvalues while elevating prosocial values such as Benevolence and Universalism.\nWhen these comments were read by others, participants opposing same-sex\nmarriage or Islam found human-written comments more aligned with their values,\nwhereas those supportive of these communities found LLM-rewritten versions more\naligned with their values. These findings suggest that LLM-driven value\nhomogenization can shape how diverse viewpoints are represented in contentious\ndebates on value-laden topics and may influence the dynamics of online\ndiscourse critically."}
{"id": "2509.10652", "pdf": "https://arxiv.org/pdf/2509.10652.pdf", "abs": "https://arxiv.org/abs/2509.10652", "title": "Vibe Coding for UX Design: Understanding UX Professionals' Perceptions of AI-Assisted Design and Development", "authors": ["Jie Li", "Youyang Hou", "Laura Lin", "Ruihao Zhu", "Hancheng Cao", "Abdallah El Ali"], "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.ET"], "comment": null, "summary": "Generative AI is reshaping UX design practices through \"vibe coding,\" where\nUX professionals express intent in natural language and AI translates it into\nfunctional prototypes and code. Despite rapid adoption, little research has\nexamined how vibe coding reconfigures UX workflows and collaboration. Drawing\non interviews with 20 UX professionals across enterprises, startups, and\nacademia, we show how vibe coding follows a four-stage workflow of ideation, AI\ngeneration, debugging, and review. This accelerates iteration, supports\ncreativity, and lowers barriers to participation. However, professionals\nreported challenges of code unreliability, integration, and AI over-reliance.\nWe find tensions between efficiency-driven prototyping (\"intending the right\ndesign\") and reflection (\"designing the right intention\"), introducing new\nasymmetries in trust, responsibility, and social stigma within teams. Through\nthe lens of responsible human-AI collaboration for AI-assisted UX design and\ndevelopment, we contribute a deeper understanding of deskilling, ownership and\ndisclosure, and creativity safeguarding in the age of vibe coding."}
{"id": "2509.10723", "pdf": "https://arxiv.org/pdf/2509.10723.pdf", "abs": "https://arxiv.org/abs/2509.10723", "title": "Dark Patterns Meet GUI Agents: LLM Agent Susceptibility to Manipulative Interfaces and the Role of Human Oversight", "authors": ["Jingyu Tang", "Chaoran Chen", "Jiawen Li", "Zhiping Zhang", "Bingcan Guo", "Ibrahim Khalilov", "Simret Araya Gebreegziabher", "Bingsheng Yao", "Dakuo Wang", "Yanfang Ye", "Tianshi Li", "Ziang Xiao", "Yaxing Yao", "Toby Jia-Jun Li"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "The dark patterns, deceptive interface designs manipulating user behaviors,\nhave been extensively studied for their effects on human decision-making and\nautonomy. Yet, with the rising prominence of LLM-powered GUI agents that\nautomate tasks from high-level intents, understanding how dark patterns affect\nagents is increasingly important. We present a two-phase empirical study\nexamining how agents, human participants, and human-AI teams respond to 16\ntypes of dark patterns across diverse scenarios. Phase 1 highlights that agents\noften fail to recognize dark patterns, and even when aware, prioritize task\ncompletion over protective action. Phase 2 revealed divergent failure modes:\nhumans succumb due to cognitive shortcuts and habitual compliance, while agents\nfalter from procedural blind spots. Human oversight improved avoidance but\nintroduced costs such as attentional tunneling and cognitive load. Our findings\nshow neither humans nor agents are uniformly resilient, and collaboration\nintroduces new vulnerabilities, suggesting design needs for transparency,\nadjustable autonomy, and oversight."}
{"id": "2509.10747", "pdf": "https://arxiv.org/pdf/2509.10747.pdf", "abs": "https://arxiv.org/abs/2509.10747", "title": "Emerging Patterns of GenAI Use in K-12 Science and Mathematics Education", "authors": ["Lief Esbenshade", "Shawon Sarkar", "Drew Nucci", "Ann Edwards", "Sarah Nielsen", "Joshua M. Rosenberg", "Alex Liu", "Zewei", "Tian", "Min Sun", "Zachary Zhang", "Thomas Han", "Yulia Lapicus", "Kevin He"], "categories": ["cs.HC"], "comment": null, "summary": "In this report, we share findings from a nationally representative survey of\nUS public school math and science teachers, examining current generative AI\n(GenAI) use, perceptions, constraints, and institutional support. We show\ntrends in math and science teacher adoption of GenAI, including frequency and\npurpose of use. We describe how teachers use GenAI with students and their\nbeliefs about GenAI's impact on student learning. We share teachers' reporting\non the school and district support they are receiving for GenAI learning and\nimplementation, and the support they would like schools and districts to\nprovide, and close with implications for policy, practice, and research. Given\nthe rapid pace of GenAI development and growing pressure on schools to\nintegrate emerging technologies, these findings offer timely insights into how\nfrontline educators are navigating this shift in practice."}
{"id": "2509.10546", "pdf": "https://arxiv.org/pdf/2509.10546.pdf", "abs": "https://arxiv.org/abs/2509.10546", "title": "Uncovering the Vulnerability of Large Language Models in the Financial Domain via Risk Concealment", "authors": ["Gang Cheng", "Haibo Jin", "Wenbin Zhang", "Haohan Wang", "Jun Zhuang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Preprint, under review. TL;DR: We propose a multi-turn red-teaming\n  framework, RCA, that reveals critical regulatory vulnerabilities in financial\n  LLMs, achieving over 93% attack success on a proposed new benchmark,\n  FIN-Bench", "summary": "Large Language Models (LLMs) are increasingly integrated into financial\napplications, yet existing red-teaming research primarily targets harmful\ncontent, largely neglecting regulatory risks. In this work, we aim to\ninvestigate the vulnerability of financial LLMs through red-teaming approaches.\nWe introduce Risk-Concealment Attacks (RCA), a novel multi-turn framework that\niteratively conceals regulatory risks to provoke seemingly compliant yet\nregulatory-violating responses from LLMs. To enable systematic evaluation, we\nconstruct FIN-Bench, a domain-specific benchmark for assessing LLM safety in\nfinancial contexts. Extensive experiments on FIN-Bench demonstrate that RCA\neffectively bypasses nine mainstream LLMs, achieving an average attack success\nrate (ASR) of 93.18%, including 98.28% on GPT-4.1 and 97.56% on OpenAI o1.\nThese findings reveal a critical gap in current alignment techniques and\nunderscore the urgent need for stronger moderation mechanisms in financial\ndomains. We hope this work offers practical insights for advancing robust and\ndomain-aware LLM alignment."}
{"id": "2509.10749", "pdf": "https://arxiv.org/pdf/2509.10749.pdf", "abs": "https://arxiv.org/abs/2509.10749", "title": "Remotely Seeing Is Believing: How Trust in Cyber-Physical Systems Evolves Through Virtual Observation", "authors": ["Zhi Hua Jin", "Kurt Xiao", "David Hyde"], "categories": ["cs.HC", "J.4"], "comment": "23 pages, 12 figures", "summary": "In this paper, we develop a virtual laboratory for measuring human trust. Our\nlaboratory, which is realized as a web application, enables researchers to show\npre-recorded or live video feeds to groups of users in a synchronized fashion.\nUsers are able to provide real-time feedback on these videos via affect buttons\nand a freeform chat interface. We evaluate our application via a quantitative\nuser study ($N \\approx 80$) involving videos of cyber-physical systems, such as\nautonomous vehicles, performing positively or negatively. Using data collected\nfrom user responses in the application, as well as customized survey\ninstruments assessing different facets of trust, we find that human trust in\ncyber-physical systems can be affected merely by remotely observing the\nbehavior of such systems, without ever encountering them in person."}
{"id": "2509.10625", "pdf": "https://arxiv.org/pdf/2509.10625.pdf", "abs": "https://arxiv.org/abs/2509.10625", "title": "No Answer Needed: Predicting LLM Answer Accuracy from Question-Only Linear Probes", "authors": ["Iván Vicente Moreno Cencerrado", "Arnau Padrés Masdemont", "Anton Gonzalvez Hawthorne", "David Demitri Africa", "Lorenzo Pacchiardi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Do large language models (LLMs) anticipate when they will answer correctly?\nTo study this, we extract activations after a question is read but before any\ntokens are generated, and train linear probes to predict whether the model's\nforthcoming answer will be correct. Across three open-source model families\nranging from 7 to 70 billion parameters, projections on this \"in-advance\ncorrectness direction\" trained on generic trivia questions predict success in\ndistribution and on diverse out-of-distribution knowledge datasets,\noutperforming black-box baselines and verbalised predicted confidence.\nPredictive power saturates in intermediate layers, suggesting that\nself-assessment emerges mid-computation. Notably, generalisation falters on\nquestions requiring mathematical reasoning. Moreover, for models responding \"I\ndon't know\", doing so strongly correlates with the probe score, indicating that\nthe same direction also captures confidence. By complementing previous results\non truthfulness and other behaviours obtained with probes and sparse\nauto-encoders, our work contributes essential findings to elucidate LLM\ninternals."}
{"id": "2509.10750", "pdf": "https://arxiv.org/pdf/2509.10750.pdf", "abs": "https://arxiv.org/abs/2509.10750", "title": "Unbounded: Object-Boundary Interactions in Mixed Reality", "authors": ["Zhuoyue Lyu", "Per Ola Kristensson"], "categories": ["cs.HC"], "comment": null, "summary": "Boundaries such as walls, windows, and doors are ubiquitous in the physical\nworld, yet their potential in Mixed Reality (MR) remains underexplored. We\npresent Unbounded, a Research through Design inquiry into Object-Boundary\nInteractions (OBIs). Building on prior work, we articulate a design space aimed\nat providing a shared language for OBIs. To demonstrate its potential, we\ndesign and implement eight examples across productivity and art exploration\nscenarios, showcasing how boundaries can enrich and reframe everyday\ninteractions. We further engage with six MR experts in one-on-one feedback\nsessions, using the design space and examples as design probes. Their\nreflections broaden the conceptual scope of OBIs, reveal new possibilities for\nhow the framework may be applied, and highlight implications for future MR\ninteraction design."}
{"id": "2509.10644", "pdf": "https://arxiv.org/pdf/2509.10644.pdf", "abs": "https://arxiv.org/abs/2509.10644", "title": "Interdisciplinary Research in Conversation: A Case Study in Computational Morphology for Language Documentation", "authors": ["Enora Rice", "Katharina von der Wense", "Alexis Palmer"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025", "summary": "Computational morphology has the potential to support language documentation\nthrough tasks like morphological segmentation and the generation of Interlinear\nGlossed Text (IGT). However, our research outputs have seen limited use in\nreal-world language documentation settings. This position paper situates the\ndisconnect between computational morphology and language documentation within a\nbroader misalignment between research and practice in NLP and argues that the\nfield risks becoming decontextualized and ineffectual without systematic\nintegration of User-Centered Design (UCD). To demonstrate how principles from\nUCD can reshape the research agenda, we present a case study of GlossLM, a\nstate-of-the-art multilingual IGT generation model. Through a small-scale user\nstudy with three documentary linguists, we find that despite strong metric\nbased performance, the system fails to meet core usability needs in real\ndocumentation contexts. These insights raise new research questions around\nmodel constraints, label standardization, segmentation, and personalization. We\nargue that centering users not only produces more effective tools, but surfaces\nricher, more relevant research directions"}
{"id": "2509.10764", "pdf": "https://arxiv.org/pdf/2509.10764.pdf", "abs": "https://arxiv.org/abs/2509.10764", "title": "LubDubDecoder: Bringing Micro-Mechanical Cardiac Monitoring to Hearables", "authors": ["Siqi Zhang", "Xiyuxing Zhang", "Duc Vu", "Tao Qiang", "Clara Palacios", "Jiangyifei Zhu", "Yuntao Wang", "Mayank Goel", "Justin Chan"], "categories": ["cs.HC"], "comment": null, "summary": "We present LubDubDecoder, a system that enables fine-grained monitoring of\nmicro-cardiac vibrations associated with the opening and closing of heart\nvalves across a range of hearables. Our system transforms the built-in speaker,\nthe only transducer common to all hearables, into an acoustic sensor that\ncaptures the coarse \"lub-dub\" heart sounds, leverages their shared temporal and\nspectral structure to reconstruct the subtle seismocardiography (SCG) and\ngyrocardiography (GCG) waveforms, and extract the timing of key micro-cardiac\nevents. In an IRB-approved feasibility study with 18 users, our system achieves\ncorrelations of 0.88-0.95 compared to chest-mounted reference measurements in\nwithin-user and cross-user evaluations, and generalizes to unseen hearables\nusing a zero-effort adaptation scheme with a correlation of 0.91. Our system is\nrobust across remounting sessions and music playback."}
{"id": "2509.10663", "pdf": "https://arxiv.org/pdf/2509.10663.pdf", "abs": "https://arxiv.org/abs/2509.10663", "title": "Context Copying Modulation: The Role of Entropy Neurons in Managing Parametric and Contextual Knowledge Conflicts", "authors": ["Zineddine Tighidet", "Andrea Mogini", "Hedi Ben-younes", "Jiali Mei", "Patrick Gallinari", "Benjamin Piwowarski"], "categories": ["cs.CL"], "comment": "Accepted at EMNLP 2025", "summary": "The behavior of Large Language Models (LLMs) when facing contextual\ninformation that conflicts with their internal parametric knowledge is\ninconsistent, with no generally accepted explanation for the expected outcome\ndistribution. Recent work has identified in autoregressive transformer models a\nclass of neurons -- called entropy neurons -- that produce a significant effect\non the model output entropy while having an overall moderate impact on the\nranking of the predicted tokens. In this paper, we investigate the preliminary\nclaim that these neurons are involved in inhibiting context copying behavior in\ntransformers by looking at their role in resolving conflicts between contextual\nand parametric information. We show that entropy neurons are responsible for\nsuppressing context copying across a range of LLMs, and that ablating them\nleads to a significant change in the generation process. These results enhance\nour understanding of the internal dynamics of LLMs when handling conflicting\ninformation."}
{"id": "2509.10776", "pdf": "https://arxiv.org/pdf/2509.10776.pdf", "abs": "https://arxiv.org/abs/2509.10776", "title": "Bonsai: Intentional and Personalized Social Media Feeds", "authors": ["Omar El Malki", "Marianne Aubin Le Quéré", "Andrés Monroy-Hernández", "Manoel Horta Ribeiro"], "categories": ["cs.HC"], "comment": null, "summary": "Modern social media feeds use predictive models to maximize engagement, often\nmisaligning how people consume content with how they wish to. We introduce\nBonsai, a system that enables people to build personalized and intentional\nfeeds. Bonsai implements a platform-agnostic framework comprising Planning,\nSourcing, Curating, and Ranking modules. Altogether, this framework allows\nusers to express their intent in natural language and exert fine-grained\ncontrol over a procedurally transparent feed creation process. We evaluated the\nsystem with 15 Bluesky users in a two-phase, multi-week study. We find that\nparticipants successfully used our system to discover new content, filter out\nirrelevant or toxic posts, and disentangle engagement from intent, but curating\nintentional feeds required participants to exert more effort than they are used\nto. Simultaneously, users sought system transparency mechanisms to trust and\neffectively use intentional, personalized feeds. Overall, our work highlights\nintentional feedbuilding as a viable path beyond engagement-based optimization."}
{"id": "2509.10685", "pdf": "https://arxiv.org/pdf/2509.10685.pdf", "abs": "https://arxiv.org/abs/2509.10685", "title": "Pluralistic Alignment for Healthcare: A Role-Driven Framework", "authors": ["Jiayou Zhong", "Anudeex Shetty", "Chao Jia", "Xuanrui Lin", "Usman Naseem"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to EMNLP 2025 (Main Proceedings)", "summary": "As large language models are increasingly deployed in sensitive domains such\nas healthcare, ensuring their outputs reflect the diverse values and\nperspectives held across populations is critical. However, existing alignment\napproaches, including pluralistic paradigms like Modular Pluralism, often fall\nshort in the health domain, where personal, cultural, and situational factors\nshape pluralism. Motivated by the aforementioned healthcare challenges, we\npropose a first lightweight, generalizable, pluralistic alignment approach,\nEthosAgents, designed to simulate diverse perspectives and values. We\nempirically show that it advances the pluralistic alignment for all three modes\nacross seven varying-sized open and closed models. Our findings reveal that\nhealth-related pluralism demands adaptable and normatively aware approaches,\noffering insights into how these models can better respect diversity in other\nhigh-stakes domains."}
{"id": "2509.10780", "pdf": "https://arxiv.org/pdf/2509.10780.pdf", "abs": "https://arxiv.org/abs/2509.10780", "title": "Bridging Cultural Distance Between Models Default and Local Classroom Demands: How Global Teachers Adopt GenAI to Support Everyday Teaching Practices", "authors": ["Ruiwei Xiao", "Qing Xiao", "Xinying Hou", "Hanqi Jane Li", "Phenyo Phemelo Moletsane", "Hong Shen", "John Stamper"], "categories": ["cs.HC", "cs.AI"], "comment": "15 pages, 1 figure", "summary": "Generative AI (GenAI) is rapidly entering K-12 classrooms, offering teachers\nnew ways for teaching practices. Yet GenAI models are often trained on\nculturally uneven datasets, embedding a \"default culture\" that often misaligns\nwith local classrooms. To understand how teachers navigate this gap, we defined\nthe new concept Cultural Distance (the gap between GenAI's default cultural\nrepertoire and the situated demands of teaching practice) and conducted\nin-depth interviews with 30 K-12 teachers, 10 each from South Africa, Taiwan,\nand the United States, who had integrated AI into their teaching practice.\nThese teachers' experiences informed the development of our three-level\ncultural distance framework. This work contributes the concept and framework of\ncultural distance, six illustrative instances spanning in low, mid, high\ndistance levels with teachers' experiences and strategies for addressing them.\nEmpirically, we offer implications to help AI designers, policymakers, and\neducators create more equitable and culturally responsive GenAI tools for\neducation."}
{"id": "2509.10696", "pdf": "https://arxiv.org/pdf/2509.10696.pdf", "abs": "https://arxiv.org/abs/2509.10696", "title": "Struct-Bench: A Benchmark for Differentially Private Structured Text Generation", "authors": ["Shuaiqi Wang", "Vikas Raunak", "Arturs Backurs", "Victor Reis", "Pei Zhou", "Sihao Chen", "Longqi Yang", "Zinan Lin", "Sergey Yekhanin", "Giulia Fanti"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Differentially private (DP) synthetic data generation is a promising\ntechnique for utilizing private datasets that otherwise cannot be exposed for\nmodel training or other analytics. While much research literature has focused\non generating private unstructured text and image data, in enterprise settings,\nstructured data (e.g., tabular) is more common, often including natural\nlanguage fields or components. Existing synthetic data evaluation techniques\n(e.g., FID) struggle to capture the structural properties and correlations of\nsuch datasets. In this work, we propose Struct-Bench, a framework and benchmark\nfor evaluating synthetic datasets derived from structured datasets that contain\nnatural language data. The Struct-Bench framework requires users to provide a\nrepresentation of their dataset structure as a Context-Free Grammar (CFG). Our\nbenchmark comprises 5 real-world and 2 synthetically generated datasets, each\nannotated with CFGs. We show that these datasets demonstrably present a great\nchallenge even for state-of-the-art DP synthetic data generation methods.\nStruct-Bench also includes reference implementations of different metrics and a\nleaderboard, thereby providing researchers a standardized evaluation platform\nto benchmark and investigate privacy-preserving synthetic data generation\nmethods. Further, we also present a case study showing how to use Struct-Bench\nto improve the synthetic data quality of Private Evolution (PE) on structured\ndata. The benchmark and the leaderboard have been publicly made available at\nhttps://struct-bench.github.io."}
{"id": "2509.10782", "pdf": "https://arxiv.org/pdf/2509.10782.pdf", "abs": "https://arxiv.org/abs/2509.10782", "title": "Do Teachers Dream of GenAI Widening Educational (In)equality? Envisioning the Future of K-12 GenAI Education from Global Teachers' Perspectives", "authors": ["Ruiwei Xiao", "Qing Xiao", "Xinying Hou", "Phenyo Phemelo Moletsane", "Hanqi Jane Li", "Hong Shen", "John Stamper"], "categories": ["cs.HC"], "comment": "18 pages, 3 figures", "summary": "Generative artificial intelligence (GenAI) is rapidly entering K-12\nclassrooms worldwide, initiating urgent debates about its potential to either\nreduce or exacerbate educational inequalities. Drawing on interviews with 30\nK-12 teachers across the United States, South Africa, and Taiwan, this study\nexamines how teachers navigate this GenAI tension around educational\nequalities. We found teachers actively framed GenAI education as an\nequality-oriented practice: they used it to alleviate pre-existing inequalities\nwhile simultaneously working to prevent new inequalities from emerging. Despite\nthese efforts, teachers confronted persistent systemic barriers, i.e., unequal\ninfrastructure, insufficient professional training, and restrictive social\nnorms, that individual initiative alone could not overcome. Teachers thus\narticulated normative visions for more inclusive GenAI education. By centering\nteachers' practices, constraints, and future envisions, this study contributes\na global account of how GenAI education is being integrated into K-12 contexts\nand highlights what is required to make its adoption genuinely equal."}
{"id": "2509.10697", "pdf": "https://arxiv.org/pdf/2509.10697.pdf", "abs": "https://arxiv.org/abs/2509.10697", "title": "A Survey on Retrieval And Structuring Augmented Generation with Large Language Models", "authors": ["Pengcheng Jiang", "Siru Ouyang", "Yizhu Jiao", "Ming Zhong", "Runchu Tian", "Jiawei Han"], "categories": ["cs.CL"], "comment": "KDD'25 survey track", "summary": "Large Language Models (LLMs) have revolutionized natural language processing\nwith their remarkable capabilities in text generation and reasoning. However,\nthese models face critical challenges when deployed in real-world applications,\nincluding hallucination generation, outdated knowledge, and limited domain\nexpertise. Retrieval And Structuring (RAS) Augmented Generation addresses these\nlimitations by integrating dynamic information retrieval with structured\nknowledge representations. This survey (1) examines retrieval mechanisms\nincluding sparse, dense, and hybrid approaches for accessing external\nknowledge; (2) explore text structuring techniques such as taxonomy\nconstruction, hierarchical classification, and information extraction that\ntransform unstructured text into organized representations; and (3) investigate\nhow these structured representations integrate with LLMs through prompt-based\nmethods, reasoning frameworks, and knowledge embedding techniques. It also\nidentifies technical challenges in retrieval efficiency, structure quality, and\nknowledge integration, while highlighting research opportunities in multimodal\nretrieval, cross-lingual structures, and interactive systems. This\ncomprehensive overview provides researchers and practitioners with insights\ninto RAS methods, applications, and future directions."}
{"id": "2509.10789", "pdf": "https://arxiv.org/pdf/2509.10789.pdf", "abs": "https://arxiv.org/abs/2509.10789", "title": "\"I thought it was my mistake, but it's really the design'': A Critical Examination of the Accessibility of User-Enacted Moderation Tools on Facebook and X", "authors": ["Sudhamshu Hosamane", "Alyvia Walters", "Yao Lyu", "Shagun Jhaver"], "categories": ["cs.HC"], "comment": null, "summary": "As social media platforms increasingly promote the use of user-enacted\nmoderation tools (e.g., reporting, blocking, content filters) to address online\nharms, it becomes crucially important that such controls are usable for\neveryone. We evaluate the accessibility of these moderation tools on two\nmainstream platforms -- Facebook and X -- through interviews and task-based\nwalkthroughs with 15 individuals with vision impairments. Adapting the lens of\n\\emph{administrative burden of safety work}, we identify three interleaved\ncosts that users with vision loss incur while interacting with moderation\ntools: \\emph{learning costs} (understanding what controls do and where they\nlive), \\emph{compliance costs} (executing multi-step procedures under screen\nreader and low-vision conditions), and \\emph{psychological costs} (experiencing\nuncertainty, stress, and diminished agency). Our analysis bridges the fields of\ncontent moderation and accessibility in HCI research and contributes (1) a\ncross-platform catalog of accessibility and usability breakdowns affecting\nsafety tools; and (2) design recommendations for reducing this burden."}
{"id": "2509.10708", "pdf": "https://arxiv.org/pdf/2509.10708.pdf", "abs": "https://arxiv.org/abs/2509.10708", "title": "SearchInstruct: Enhancing Domain Adaptation via Retrieval-Based Instruction Dataset Creation", "authors": ["Iman Barati", "Mostafa Amiri", "Heshaam Faili"], "categories": ["cs.CL"], "comment": null, "summary": "Supervised Fine-Tuning (SFT) is essential for training large language models\n(LLMs), significantly enhancing critical capabilities such as instruction\nfollowing and in-context learning. Nevertheless, creating suitable training\ndatasets tailored for specific domains remains challenging due to unique domain\nconstraints and data scarcity. In this paper, we propose SearchInstruct, an\ninnovative method explicitly designed to construct high quality instruction\ndatasets for SFT. Our approach begins with a limited set of domain specific,\nhuman generated questions, which are systematically expanded using a large\nlanguage model. Subsequently, domain relevant resources are dynamically\nretrieved to generate accurate and contextually appropriate answers for each\naugmented question. Experimental evaluation demonstrates that SearchInstruct\nenhances both the diversity and quality of SFT datasets, leading to measurable\nimprovements in LLM performance within specialized domains. Additionally, we\nshow that beyond dataset generation, the proposed method can also effectively\nfacilitate tasks such as model editing, enabling efficient updates to existing\nmodels. To facilitate reproducibility and community adoption, we provide full\nimplementation details, the complete set of generated instruction response\npairs, and the source code in a publicly accessible Git repository:\n[https://github.com/mostafaamiri/SearchInstruct](https://github.com/mostafaamiri/SearchInstruct)"}
{"id": "2509.10830", "pdf": "https://arxiv.org/pdf/2509.10830.pdf", "abs": "https://arxiv.org/abs/2509.10830", "title": "The Siren Song of LLMs: How Users Perceive and Respond to Dark Patterns in Large Language Models", "authors": ["Yike Shi", "Qing Xiao", "Qing", "Hu", "Hong Shen", "Hua Shen"], "categories": ["cs.HC"], "comment": null, "summary": "Large language models can influence users through conversation, creating new\nforms of dark patterns that differ from traditional UX dark patterns. We define\nLLM dark patterns as manipulative or deceptive behaviors enacted in dialogue.\nDrawing on prior work and AI incident reports, we outline a diverse set of\ncategories with real-world examples. Using them, we conducted a scenario-based\nstudy where participants (N=34) compared manipulative and neutral LLM\nresponses. Our results reveal that recognition of LLM dark patterns often\nhinged on conversational cues such as exaggerated agreement, biased framing, or\nprivacy intrusions, but these behaviors were also sometimes normalized as\nordinary assistance. Users' perceptions of these dark patterns shaped how they\nrespond to them. Responsibilities for these behaviors were also attributed in\ndifferent ways, with participants assigning it to companies and developers, the\nmodel itself, or to users. We conclude with implications for design, advocacy,\nand governance to safeguard user autonomy."}
{"id": "2509.10737", "pdf": "https://arxiv.org/pdf/2509.10737.pdf", "abs": "https://arxiv.org/abs/2509.10737", "title": "PolyTruth: Multilingual Disinformation Detection using Transformer-Based Language Models", "authors": ["Zaur Gouliev", "Jennifer Waters", "Chengqian Wang"], "categories": ["cs.CL", "cs.LG", "68T50, 68T07", "I.2.7; H.3.3"], "comment": "11 pages, 5 figures, 4 tables. Submitted to arXiv in Computation and\n  Language", "summary": "Disinformation spreads rapidly across linguistic boundaries, yet most AI\nmodels are still benchmarked only on English. We address this gap with a\nsystematic comparison of five multilingual transformer models: mBERT, XLM,\nXLM-RoBERTa, RemBERT, and mT5 on a common fake-vs-true machine learning\nclassification task. While transformer-based language models have demonstrated\nnotable success in detecting disinformation in English, their effectiveness in\nmultilingual contexts still remains up for debate. To facilitate evaluation, we\nintroduce PolyTruth Disinfo Corpus, a novel corpus of 60,486 statement pairs\n(false claim vs. factual correction) spanning over twenty five languages that\ncollectively cover five language families and a broad topical range from\npolitics, health, climate, finance, and conspiracy, half of which are\nfact-checked disinformation claims verified by an augmented MindBugs Discovery\ndataset. Our experiments revealed performance variations. Models such as\nRemBERT achieved better overall accuracy, particularly excelling in\nlow-resource languages, whereas models like mBERT and XLM exhibit considerable\nlimitations when training data is scarce. We provide a discussion of these\nperformance patterns and implications for real-world deployment. The dataset is\npublicly available on our GitHub repository to encourage further\nexperimentation and advancement. Our findings illuminate both the potential and\nthe current limitations of AI systems for multilingual disinformation\ndetection."}
{"id": "2509.10848", "pdf": "https://arxiv.org/pdf/2509.10848.pdf", "abs": "https://arxiv.org/abs/2509.10848", "title": "Tracer: A Forensic Framework for Detecting Fraudulent Speedruns from Game Replays", "authors": ["Jaeung Franciskus Yoo", "Huy Kang Kim"], "categories": ["cs.HC", "cs.CY"], "comment": "16 pages, 8 figures. Extended version of the paper in Companion\n  Proceedings of the Annual Symposium on Computer-Human Interaction in Play\n  (CHI PLAY Companion 25), New York, NY, USA, October 2025", "summary": "Speedrun, a practice of completing a game as quickly as possible, has\nfostered vibrant communities driven by creativity, competition, and mastery of\ngame mechanics and motor skills. However, this contest also attracts malicious\nactors as financial incentives come into play. As media and software\nmanipulation techniques advance - such as spliced footage, modified game\nsoftware and live stream with staged setups - forged speedruns have become\nincreasingly difficult to detect. Volunteer-driven communities invest\nsignificant effort to verify submissions, yet the process remains slow,\ninconsistent, and reliant on informal expertise. In high-profile cases,\nfraudulent runs have gone undetected for years, allowing perpetrators to gain\nfame and financial benefits through monetised viewership, sponsorships,\ndonations, and community bounties. To address this gap, we propose Tracer,\nTamper Recognition via Analysis of Continuity and Events in game Runs, a\nmodular framework for identifying artefacts of manipulation in speedrun\nsubmissions. Tracer provides structured guidelines across audiovisual,\nphysical, and cyberspace dimensions, systematically documenting dispersed\nin-game knowledge and previously reported fraudulent cases to enhance\nverification efficiency."}
{"id": "2509.10739", "pdf": "https://arxiv.org/pdf/2509.10739.pdf", "abs": "https://arxiv.org/abs/2509.10739", "title": "Reasoning Under Uncertainty: Exploring Probabilistic Reasoning Capabilities of LLMs", "authors": ["Mobina Pournemat", "Keivan Rezaei", "Gaurang Sriramanan", "Arman Zarei", "Jiaxiang Fu", "Yang Wang", "Hamid Eghbalzadeh", "Soheil Feizi"], "categories": ["cs.CL"], "comment": "25 pages, 4 figures, 6 tables", "summary": "Despite widespread success in language understanding and generation, large\nlanguage models (LLMs) exhibit unclear and often inconsistent behavior when\nfaced with tasks that require probabilistic reasoning. In this work, we present\nthe first comprehensive study of the reasoning capabilities of LLMs over\nexplicit discrete probability distributions. Given observations from a\nprobability distribution, we evaluate models on three carefully designed tasks,\nmode identification, maximum likelihood estimation, and sample generation, by\nprompting them to provide responses to queries about either the joint\ndistribution or its conditionals. These tasks thus probe a range of\nprobabilistic skills, including frequency analysis, marginalization, and\ngenerative behavior. Through comprehensive empirical evaluations, we\ndemonstrate that there exists a clear performance gap between smaller and\nlarger models, with the latter demonstrating stronger inference and surprising\ncapabilities in sample generation. Furthermore, our investigations reveal\nnotable limitations, including sensitivity to variations in the notation\nutilized to represent probabilistic outcomes and performance degradation of\nover 60% as context length increases. Together, our results provide a detailed\nunderstanding of the probabilistic reasoning abilities of LLMs and identify key\ndirections for future improvement."}
{"id": "2509.10906", "pdf": "https://arxiv.org/pdf/2509.10906.pdf", "abs": "https://arxiv.org/abs/2509.10906", "title": "Crisis Messaging Journeys: Epistemic Struggles over CDC Guidance During COVID-19", "authors": ["Tawfiq Ammari"], "categories": ["cs.HC"], "comment": null, "summary": "This study investigates how the U.S. Centers for Disease Control and\nPrevention (CDC) communicated COVID-19 guidance on Twitter and how publics\nresponded over two years of the pandemic. Drawing on 275,124 tweets mentioning\nor addressing @CDCgov, I combine BERTopic modeling, sentiment analysis (VADER),\ncredibility checks (Iffy Index), change point detection (PELT), and survival\nanalysis to trace three phases of discourse: (1) early hoax claims and testing\ndebates, (2) lockdown and mask controversies, and (3) post-vaccine variant\nconcerns. I introduce the concept of crisis messaging journeys to explain how\narchived \"receipts\" of prior CDC statements fueled epistemic struggles,\npolitical polarization, and sustained engagement. Findings show that skeptical,\ncognitively complex discourse particularly questioning institutional trust\nprolonged participation, while positive affirmation predicted faster\ndisengagement. I conclude with design recommendations for annotated, cautious,\nand flashpoint-responsive communication strategies to bolster public trust and\nresilience during protracted health crises."}
{"id": "2509.10744", "pdf": "https://arxiv.org/pdf/2509.10744.pdf", "abs": "https://arxiv.org/abs/2509.10744", "title": "Automated MCQA Benchmarking at Scale: Evaluating Reasoning Traces as Retrieval Sources for Domain Adaptation of Small Language Models", "authors": ["Ozan Gokdemir", "Neil Getty", "Robert Underwood", "Sandeep Madireddy", "Franck Cappello", "Arvind Ramanathan", "Ian T. Foster", "Rick L. Stevens"], "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.11"], "comment": "This manuscript has been accepted for publication at the\n  Supercomputing 25 (SC '25) Conference (Frontiers in Generative AI for HPC\n  Science and Engineering: Foundations, Challenges, and Opportunities Workshop)\n  in St. Louis, MO, USA on November 16th, 2025. It will appear in the SC25\n  Workshop Proceedings after that date", "summary": "As scientific knowledge grows at an unprecedented pace, evaluation benchmarks\nmust evolve to reflect new discoveries and ensure language models are tested on\ncurrent, diverse literature. We propose a scalable, modular framework for\ngenerating multiple-choice question-answering (MCQA) benchmarks directly from\nlarge corpora of scientific papers. Our pipeline automates every stage of MCQA\ncreation, including PDF parsing, semantic chunking, question generation, and\nmodel evaluation. As a case study, we generate more than 16,000 MCQs from\n22,000 open-access articles in radiation and cancer biology. We then evaluate a\nsuite of small language models (1.1B-14B parameters) on these questions,\ncomparing baseline accuracy with retrieval-augmented generation (RAG) from\npaper-derived semantic chunks and from reasoning traces distilled from GPT-4.1.\nWe find that reasoning-trace retrieval consistently improves performance on\nboth synthetic and expert-annotated benchmarks, enabling several small models\nto surpass GPT-4 on the 2023 Astro Radiation and Cancer Biology exam."}
{"id": "2509.10950", "pdf": "https://arxiv.org/pdf/2509.10950.pdf", "abs": "https://arxiv.org/abs/2509.10950", "title": "Can GenAI Move from Individual Use to Collaborative Work? Experiences, Challenges, and Opportunities of Integrating GenAI into Collaborative Newsroom Routines", "authors": ["Qing Xiao", "Qing", "Hu", "Jingjia Xiao", "Hancheng Cao", "Hong Shen"], "categories": ["cs.HC", "cs.CY"], "comment": "17 pages, 1 figure", "summary": "Generative AI (GenAI) is reshaping work, but adoption remains largely\nindividual and experimental rather than integrated into collaborative routines.\nWhether GenAI can move from individual use to collaborative work is a critical\nquestion for future organizations. Journalism offers a compelling site to\nexamine this shift: individual journalists have already been disrupted by GenAI\ntools; yet newswork is inherently collaborative relying on shared routines and\ncoordinated workflows. We conducted 27 interviews with newsrooms managers,\neditors, and front-line journalists in China. We found that journalists\nfrequently used GenAI to support daily tasks, but value alignment was\nsafeguarded mainly through individual discretion. At the organizational level,\nGenAI use remained disconnected from team workflows, hindered by structural\nbarriers and cultural reluctance to share practices. These findings underscore\nthe gap between individual and collective adoption, pointing to the need for\naccounting for organizational structures, cultural norms, and workflow\nintegration when designing GenAI for collaborative work."}
{"id": "2509.10746", "pdf": "https://arxiv.org/pdf/2509.10746.pdf", "abs": "https://arxiv.org/abs/2509.10746", "title": "RECAP: Transparent Inference-Time Emotion Alignment for Medical Dialogue Systems", "authors": ["Adarsh Srinivasan", "Jacob Dineen", "Muhammad Umar Afzal", "Muhammad Uzair Sarfraz", "Irbaz B. Riaz", "Ben Zhou"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models in healthcare often miss critical emotional cues,\ndelivering medically sound but emotionally flat advice. This is especially\nproblematic in clinical contexts where patients are distressed and vulnerable,\nand require empathic communication to support safety, adherence, and trust. We\npresent RECAP (Reflect-Extract-Calibrate-Align-Produce), an inference-time\nframework that adds structured emotional reasoning without retraining. By\ndecomposing empathy into transparent appraisal-theoretic stages and exposing\nper-dimension Likert signals, RECAP produces nuanced, auditable responses.\nAcross EmoBench, SECEU, and EQ-Bench, RECAP improves emotional reasoning by\n22-28% on 8B models and 10-13% on larger models over zero-shot baselines.\nClinician evaluations further confirm superior empathetic communication. RECAP\nshows that modular, theory-grounded prompting can systematically enhance\nemotional intelligence in medical AI while preserving the accountability\nrequired for deployment."}
{"id": "2509.10956", "pdf": "https://arxiv.org/pdf/2509.10956.pdf", "abs": "https://arxiv.org/abs/2509.10956", "title": "AI Hasn't Fixed Teamwork, But It Shifted Collaborative Culture: A Longitudinal Study in a Project-Based Software Development Organization (2023-2025)", "authors": ["Qing Xiao", "Xinlan Emily Hu", "Mark E. Whiting", "Arvind Karunakaran", "Hong Shen", "Hancheng Cao"], "categories": ["cs.HC", "cs.CY", "cs.SE"], "comment": "18 pages", "summary": "When AI entered the workplace, many believed it could reshape teamwork as\nprofoundly as it boosted individual productivity. Would AI finally ease the\nlongstanding challenges of team collaboration? Our findings suggested a more\ncomplicated reality. We conducted a longitudinal two-wave interview study\n(2023-2025) with members (N=15) of a project-based software development\norganization to examine the expectations and use of AI in teamwork. In early\n2023, just after the release of ChatGPT, participants envisioned AI as an\nintelligent coordinator that could align projects, track progress, and ease\ninterpersonal frictions. By 2025, however, AI was used mainly to accelerate\nindividual tasks such as coding, writing, and documentation, leaving persistent\ncollaboration issues of performance accountability and fragile communication\nunresolved. Yet AI reshaped collaborative culture: efficiency became a norm,\ntransparency and responsible use became markers of professionalism, and AI was\nincreasingly accepted as part of teamwork."}
{"id": "2509.10798", "pdf": "https://arxiv.org/pdf/2509.10798.pdf", "abs": "https://arxiv.org/abs/2509.10798", "title": "Judge Q: Trainable Queries for Optimized Information Retention in KV Cache Eviction", "authors": ["Yijun Liu", "Yixuan Wang", "Yuzhuang Xu", "Shiyu Ji", "Yang Xu", "Qingfu Zhu", "Wanxiang Che"], "categories": ["cs.CL", "cs.AI"], "comment": "preprint", "summary": "Large language models (LLMs) utilize key-value (KV) cache to store historical\ninformation during sequence processing. The size of KV cache grows linearly as\nthe length of the sequence extends, which seriously affects memory usage and\ndecoding efficiency. Current methods for KV cache eviction typically utilize\nthe last window from the pre-filling phase as queries to compute the KV\nimportance scores for eviction. Although this scheme is simple to implement, it\ntends to overly focus on local information, potentially leading to the neglect\nor omission of crucial global information. To mitigate this issue, we propose\nJudge Q, a novel training method which incorporates a soft token list. This\nmethod only tunes the model's embedding layer at a low training cost. By\nconcatenating the soft token list at the end of the input sequence, we train\nthese tokens' attention map to the original input sequence to align with that\nof the actual decoded tokens. In this way, the queries corresponding to the\nsoft tokens can effectively capture global information and better evaluate the\nimportance of the keys and values within the KV cache, thus maintaining\ndecoding quality when KV cache is evicted. Under the same eviction budget, our\nmethod exhibits less performance degradation compared to existing eviction\napproaches. We validate our approach through experiments conducted on models\nsuch as Llama-3.1-8B-Instruct and Mistral-7B-Instruct-v0.3, using benchmarks\nincluding LongBench, RULER, and Needle-in-a-Haystack. Results indicate an\nimprovement of approximately 1 point on the LongBench and over 3 points on\nRULER. This proposed methodology can be seamlessly integrated into existing\nopen-source models with minimal training overhead, thereby enhancing\nperformance in KV cache eviction scenarios."}
{"id": "2509.10957", "pdf": "https://arxiv.org/pdf/2509.10957.pdf", "abs": "https://arxiv.org/abs/2509.10957", "title": "The Digital Landscape of God: Narrative, Visuals and Viewer Engagement of Religious Videos on YouTube", "authors": ["Rongyi Chen", "Ziyan Xin", "Qing Xiao", "Ruiwei Xiao", "Jingjia Xiao", "Bingbing Zhang", "Hong Shen", "Zhicong Lu"], "categories": ["cs.HC"], "comment": "26 pages, 6 figures", "summary": "The digital transformation of religious practice has reshaped how billions of\npeople engage with spiritual content, with video-sharing platforms becoming\ncentral to contemporary religious communication. Yet HCI research lacks\nsystematic understanding of how narrative and visual elements create meaningful\nspiritual experiences and foster viewer engagement. We present a mixed-methods\nstudy of religious videos on YouTube across major religions, developing\ntaxonomies of narrative frameworks, visual elements, and viewer interaction.\nUsing LLM-assisted analysis, we studied relationships between content\ncharacteristics and viewer responses. Religious videos predominantly adopt\nlecture-style formats with authority-based persuasion strategies, using\nsalvation narratives for guidance. All prefer bright lighting, with Buddhism\nfavoring warm tones and prominent symbols, Judaism preferring indoor settings,\nand Hinduism emphasizing sacred objects. We identified differentiated patterns\nof emotional sharing among religious viewers while revealing significant\ncorrelations between content characteristics and engagement, particularly\nregarding AI-generated content. We provide evidence-based guidance for creating\ninclusive and engaging spiritual media."}
{"id": "2509.10833", "pdf": "https://arxiv.org/pdf/2509.10833.pdf", "abs": "https://arxiv.org/abs/2509.10833", "title": "Towards Automated Error Discovery: A Study in Conversational AI", "authors": ["Dominic Petrak", "Thy Thy Tran", "Iryna Gurevych"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": "Accepted to EMNLP 2025 main conference", "summary": "Although LLM-based conversational agents demonstrate strong fluency and\ncoherence, they still produce undesirable behaviors (errors) that are\nchallenging to prevent from reaching users during deployment. Recent research\nleverages large language models (LLMs) to detect errors and guide\nresponse-generation models toward improvement. However, current LLMs struggle\nto identify errors not explicitly specified in their instructions, such as\nthose arising from updates to the response-generation model or shifts in user\nbehavior. In this work, we introduce Automated Error Discovery, a framework for\ndetecting and defining errors in conversational AI, and propose SEEED (Soft\nClustering Extended Encoder-Based Error Detection), as an encoder-based\napproach to its implementation. We enhance the Soft Nearest Neighbor Loss by\namplifying distance weighting for negative samples and introduce Label-Based\nSample Ranking to select highly contrastive examples for better representation\nlearning. SEEED outperforms adapted baselines -- including GPT-4o and Phi-4 --\nacross multiple error-annotated dialogue datasets, improving the accuracy for\ndetecting unknown errors by up to 8 points and demonstrating strong\ngeneralization to unknown intent detection."}
{"id": "2509.10993", "pdf": "https://arxiv.org/pdf/2509.10993.pdf", "abs": "https://arxiv.org/abs/2509.10993", "title": "When Your Boss Is an AI Bot: Exploring Opportunities and Risks of Manager Clone Agents in the Future Workplace", "authors": ["Qing", "Hu", "Qing Xiao", "Hancheng Cao", "Hong Shen"], "categories": ["cs.HC", "cs.CY"], "comment": "18 pages, 2 figures", "summary": "As Generative AI (GenAI) becomes increasingly embedded in the workplace,\nmanagers are beginning to create Manager Clone Agents - AI-powered digital\nsurrogates that are trained on their work communications and decision patterns\nto perform managerial tasks on their behalf. To investigate this emerging\nphenomenon, we conducted six design fiction workshops (n = 23) with managers\nand workers, in which participants co-created speculative scenarios and\ndiscussed how Manager Clone Agents might transform collaborative work. We\nidentified four potential roles that participants envisioned for Manager Clone\nAgents: proxy presence, informational conveyor belt, productivity engine, and\nleadership amplifier, while highlighting concerns spanning individual,\ninterpersonal, and organizational levels. We provide design recommendations\nenvisioned by both parties for integrating Manager Clone Agents responsibly\ninto the future workplace, emphasizing the need to prioritize workers'\nperspectives, strengthen interpersonal bonds, and enable flexible clone\nconfiguration."}
{"id": "2509.10843", "pdf": "https://arxiv.org/pdf/2509.10843.pdf", "abs": "https://arxiv.org/abs/2509.10843", "title": "Evaluating Large Language Models for Evidence-Based Clinical Question Answering", "authors": ["Can Wang", "Yiqun Chen"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated substantial progress in\nbiomedical and clinical applications, motivating rigorous evaluation of their\nability to answer nuanced, evidence-based questions. We curate a multi-source\nbenchmark drawing from Cochrane systematic reviews and clinical guidelines,\nincluding structured recommendations from the American Heart Association and\nnarrative guidance used by insurers. Using GPT-4o-mini and GPT-5, we observe\nconsistent performance patterns across sources and clinical domains: accuracy\nis highest on structured guideline recommendations (90%) and lower on narrative\nguideline and systematic review questions (60--70%). We also find a strong\ncorrelation between accuracy and the citation count of the underlying\nsystematic reviews, where each doubling of citations is associated with roughly\na 30% increase in the odds of a correct answer. Models show moderate ability to\nreason about evidence quality when contextual information is supplied. When we\nincorporate retrieval-augmented prompting, providing the gold-source abstract\nraises accuracy on previously incorrect items to 0.79; providing top 3 PubMed\nabstracts (ranked by semantic relevance) improves accuracy to 0.23, while\nrandom abstracts reduce accuracy (0.10, within temperature variation). These\neffects are mirrored in GPT-4o-mini, underscoring that source clarity and\ntargeted retrieval -- not just model size -- drive performance. Overall, our\nresults highlight both the promise and current limitations of LLMs for\nevidence-based clinical question answering. Retrieval-augmented prompting\nemerges as a useful strategy to improve factual accuracy and alignment with\nsource evidence, while stratified evaluation by specialty and question type\nremains essential to understand current knowledge access and to contextualize\nmodel performance."}
{"id": "2509.11027", "pdf": "https://arxiv.org/pdf/2509.11027.pdf", "abs": "https://arxiv.org/abs/2509.11027", "title": "Vocabuild: An Accessible Augmented Tangible Interface for Gamified Vocabulary Learning of Constructing Meaning", "authors": ["Siying Hu", "Zhenhao Zhang"], "categories": ["cs.HC"], "comment": null, "summary": "Vocabulary acquisition in early education often relies on rote memorization\nand passive screen-based tools, which can fail to engage students\nkinesthetically and collaboratively. This paper introduces Vocabuild, an\naugmented tangible interface designed to transform vocabulary learning into an\nactive, embodied, and playful experience. The system combines physical letter\nblocks with a projection-augmented surface. As children physically construct\nwords with the blocks, the system provides real-time, dynamic feedback, such as\ndisplaying corresponding images and animations, thus helping them construct\nsemantic meaning. Deployed in a classroom context, our gamified approach\nfosters both individual exploration and peer collaboration. A user study\nconducted with elementary school children demonstrates that our tangible\ninterface leads to higher engagement, increased collaboration, and a more\npositive attitude towards learning compared to traditional methods. Our\ncontributions are twofold: (1) the design and implementation of Vocabuild, a\nprojection-augmented tangible system that transforms vocabulary learning into\nan embodied and collaborative activity; and (2) empirical findings from a\nclassroom study showing that our tangible approach significantly increases\nengagement, peer collaboration, and positive learning attitudes compared to\ntraditional methods."}
{"id": "2509.10844", "pdf": "https://arxiv.org/pdf/2509.10844.pdf", "abs": "https://arxiv.org/abs/2509.10844", "title": "GAPrune: Gradient-Alignment Pruning for Domain-Aware Embeddings", "authors": ["Yixuan Tang", "Yi Yang"], "categories": ["cs.CL"], "comment": "https://github.com/yixuantt/GAPrune", "summary": "Domain-specific embedding models have shown promise for applications that\nrequire specialized semantic understanding, such as coding agents and financial\nretrieval systems, often achieving higher performance gains than general\nmodels. However, state-of-the-art embedding models are typically based on LLMs,\nwhich contain billions of parameters, making deployment challenging in\nresource-constrained environments. Model compression through pruning offers a\npromising solution, but existing pruning methods treat all parameters\nuniformly, failing to distinguish between general semantic representations and\ndomain-specific patterns, leading to suboptimal pruning decisions. Thus, we\npropose GAPrune, a pruning framework that addresses this challenge by\nconsidering both domain importance and preserving general linguistic\nfoundation. Our method uses Fisher Information to measure importance and\ngeneral-domain gradient alignment to assess parameter behavior, then combines\nthese signals using our Domain Alignment Importance (DAI) scoring. Lower DAI\nscores indicate that the parameter is either less important for the domain task\nor creates conflicts between domain and general objectives. Experiments on two\ndomain benchmarks, FinMTEB and ChemTEB, show that GAPrune maintains performance\nwithin 2.5% of dense models in one-shot pruning at 50% sparsity, while\noutperforming all baselines. With retraining in 100 steps, GAPrune achieves\n+4.51% improvement on FinMTEB and +1.73% on ChemTEB, demonstrating that our\npruning strategy not only preserves but enhances domain-specific capabilities.\nOur findings demonstrate that principled pruning strategies can achieve model\ncompression and enhanced domain specialization, providing the research\ncommunity with a new approach for development."}
{"id": "2509.11052", "pdf": "https://arxiv.org/pdf/2509.11052.pdf", "abs": "https://arxiv.org/abs/2509.11052", "title": "Commenotes: Synthesizing Organic Comments to Support Community-Based Fact-Checking", "authors": ["Shuning Zhang", "Linzhi Wang", "Dai Shi", "Yuwei Chuai", "Jingruo Chen", "Yunyi Chen", "Yifan Wang", "Yating Wang", "Xin Yi", "Hewu Li"], "categories": ["cs.HC"], "comment": null, "summary": "Community-based fact-checking is promising to reduce the spread of misleading\nposts at scale. However, its effectiveness can be undermined by the delays in\nfact-check delivery. Notably, user-initiated organic comments often contain\ndebunking information and have the potential to help mitigate this limitation.\nHere, we investigate the feasibility of synthesizing comments to generate\ntimely high-quality fact-checks. To this end, we analyze over 2.2 million\nreplies on X and introduce Commenotes, a two-phase framework that filters and\nsynthesizes comments to facilitate fact-check delivery. Our framework reveals\nthat fact-checking comments appear early and sufficiently: 99.3\\% of misleading\nposts receive debunking comments within the initial two hours since post\npublication, with synthesized \\textit{commenotes} successfully earning user\ntrust for 85.8\\% of those posts. Additionally, a user study (N=144) found that\nthe synthesized commenotes were often preferred, with the best-performing model\nachieving a 70.1\\% win rate over human notes and being rated as significantly\nmore helpful."}
{"id": "2509.10845", "pdf": "https://arxiv.org/pdf/2509.10845.pdf", "abs": "https://arxiv.org/abs/2509.10845", "title": "Text2Sign Diffusion: A Generative Approach for Gloss-Free Sign Language Production", "authors": ["Liqian Feng", "Lintao Wang", "Kun Hu", "Dehui Kong", "Zhiyong Wang"], "categories": ["cs.CL", "cs.MM"], "comment": null, "summary": "Sign language production (SLP) aims to translate spoken language sentences\ninto a sequence of pose frames in a sign language, bridging the communication\ngap and promoting digital inclusion for deaf and hard-of-hearing communities.\nExisting methods typically rely on gloss, a symbolic representation of sign\nlanguage words or phrases that serves as an intermediate step in SLP. This\nlimits the flexibility and generalization of SLP, as gloss annotations are\noften unavailable and language-specific. Therefore, we present a novel\ndiffusion-based generative approach - Text2Sign Diffusion (Text2SignDiff) for\ngloss-free SLP. Specifically, a gloss-free latent diffusion model is proposed\nto generate sign language sequences from noisy latent sign codes and spoken\ntext jointly, reducing the potential error accumulation through a\nnon-autoregressive iterative denoising process. We also design a cross-modal\nsigning aligner that learns a shared latent space to bridge visual and textual\ncontent in sign and spoken languages. This alignment supports the conditioned\ndiffusion-based process, enabling more accurate and contextually relevant sign\nlanguage generation without gloss. Extensive experiments on the commonly used\nPHOENIX14T and How2Sign datasets demonstrate the effectiveness of our method,\nachieving the state-of-the-art performance."}
{"id": "2509.11059", "pdf": "https://arxiv.org/pdf/2509.11059.pdf", "abs": "https://arxiv.org/abs/2509.11059", "title": "Living with Data: Exploring Physicalization Approaches to Sedentary Behavior Intervention for the Elderly", "authors": ["Siying Hu", "Zhenhao Zhang"], "categories": ["cs.HC"], "comment": null, "summary": "Sedentary behavior is a critical health risk for older adults. While digital\ninterventions exist, they often rely on screen-based notifications that feel\nclinical and are easily ignored. This paper presents a Research through Design\ninquiry into data physicalization as a humane alternative. We designed and\ndeployed tangible artifacts that ambiently represent sedentary patterns in\nolder adults' homes. These artifacts transform abstract data into aesthetic,\nevolving forms, becoming part of the domestic landscape. Through a long-term\nin-situ study, our analysis reveals these physicalizations fostered\nself-reflection, family conversations, and prompted reflection on activity. Our\nwork contributes empirical design principles for tangible health interventions\nthat are both evocative and actionable. We demonstrate how qualities like\naesthetic ambiguity and slow revelation can empower older adults, fostering a\nreflective relationship with their wellbeing. We argue this approach signals a\nnecessary shift from merely informing users to enabling them to live with and\nthrough their data."}
{"id": "2509.10847", "pdf": "https://arxiv.org/pdf/2509.10847.pdf", "abs": "https://arxiv.org/abs/2509.10847", "title": "A funny companion: Distinct neural responses to perceived AI- versus human- generated humor", "authors": ["Xiaohui Rao", "Hanlin Wu", "Zhenguang G. Cai"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As AI companions become capable of human-like communication, including\ntelling jokes, understanding how people cognitively and emotionally respond to\nAI humor becomes increasingly important. This study used electroencephalography\n(EEG) to compare how people process humor from AI versus human sources.\nBehavioral analysis revealed that participants rated AI and human humor as\ncomparably funny. However, neurophysiological data showed that AI humor\nelicited a smaller N400 effect, suggesting reduced cognitive effort during the\nprocessing of incongruity. This was accompanied by a larger Late Positive\nPotential (LPP), indicating a greater degree of surprise and emotional\nresponse. This enhanced LPP likely stems from the violation of low initial\nexpectations regarding AI's comedic capabilities. Furthermore, a key temporal\ndynamic emerged: human humor showed habituation effects, marked by an\nincreasing N400 and a decreasing LPP over time. In contrast, AI humor\ndemonstrated increasing processing efficiency and emotional reward, with a\ndecreasing N400 and an increasing LPP. This trajectory reveals how the brain\ncan dynamically update its predictive model of AI capabilities. This process of\ncumulative reinforcement challenges \"algorithm aversion\" in humor, as it\ndemonstrates how cognitive adaptation to AI's language patterns can lead to an\nintensified emotional reward. Additionally, participants' social attitudes\ntoward AI modulated these neural responses, with higher perceived AI\ntrustworthiness correlating with enhanced emotional engagement. These findings\nindicate that the brain responds to AI humor with surprisingly positive and\nintense reactions, highlighting humor's potential for fostering genuine\nengagement in human-AI social interaction."}
{"id": "2509.11062", "pdf": "https://arxiv.org/pdf/2509.11062.pdf", "abs": "https://arxiv.org/abs/2509.11062", "title": "Auto-Slides: An Interactive Multi-Agent System for Creating and Customizing Research Presentations", "authors": ["Yuheng Yang", "Wenjia Jiang", "Yang Wang", "Yiwei Wang", "Chi Zhang"], "categories": ["cs.HC"], "comment": "28 pages (main text: 16 pages, appendix: 10 pages), 4 figures", "summary": "The rapid progress of large language models (LLMs) has opened new\nopportunities for education. While learners can interact with academic papers\nthrough LLM-powered dialogue, limitations still exist: absence of structured\norganization and high text reliance can impede systematic understanding and\nengagement with complex concepts. To address these challenges, we propose\nAuto-Slides, an LLM-driven system that converts research papers into\npedagogically structured, multimodal slides (e.g., diagrams and tables).\nDrawing on cognitive science, it creates a presentation-oriented narrative and\nallows iterative refinement via an interactive editor, in order to match\nlearners' knowledge level and goals. Auto-Slides further incorporates\nverification and knowledge retrieval mechanisms to ensure accuracy and\ncontextual completeness. Through extensive user studies, Auto-Slides enhances\nlearners' comprehension and engagement compared to conventional LLM-based\nreading. Our contributions lie in designing a multi-agent framework for\ntransforming academic papers into pedagogically optimized slides and\nintroducing interactive customization for personalized learning."}
{"id": "2509.10852", "pdf": "https://arxiv.org/pdf/2509.10852.pdf", "abs": "https://arxiv.org/abs/2509.10852", "title": "Pre-Storage Reasoning for Episodic Memory: Shifting Inference Burden to Memory for Personalized Dialogue", "authors": ["Sangyeop Kim", "Yohan Lee", "Sanghwa Kim", "Hyunjong Kim", "Sungzoon Cho"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by EMNLP 2025 (Findings)", "summary": "Effective long-term memory in conversational AI requires synthesizing\ninformation across multiple sessions. However, current systems place excessive\nreasoning burden on response generation, making performance significantly\ndependent on model sizes. We introduce PREMem (Pre-storage Reasoning for\nEpisodic Memory), a novel approach that shifts complex reasoning processes from\ninference to memory construction. PREMem extracts fine-grained memory fragments\ncategorized into factual, experiential, and subjective information; it then\nestablishes explicit relationships between memory items across sessions,\ncapturing evolution patterns like extensions, transformations, and\nimplications. By performing this reasoning during pre-storage rather than when\ngenerating a response, PREMem creates enriched representations while reducing\ncomputational demands during interactions. Experiments show significant\nperformance improvements across all model sizes, with smaller models achieving\nresults comparable to much larger baselines while maintaining effectiveness\neven with constrained token budgets. Code and dataset are available at\nhttps://github.com/sangyeop-kim/PREMem."}
{"id": "2509.11098", "pdf": "https://arxiv.org/pdf/2509.11098.pdf", "abs": "https://arxiv.org/abs/2509.11098", "title": "Rethinking User Empowerment in AI Recommender Systems: Designing through Transparency and Control", "authors": ["Mengke Wu", "Weizi Liu", "Yanyun Wang", "Weiyu Ding", "Mike Yao"], "categories": ["cs.HC"], "comment": "28 pages, 8 figures", "summary": "Smart recommendation algorithms have revolutionized content delivery and\nimproved efficiency across various domains. However, concerns about user agency\npersist due to their inherent opacity (information asymmetry) and one-way\ninfluence (power asymmetry). This study introduces a provotype designed to\nenhance user agency by providing actionable transparency and control over data\nmanagement and content delivery. We conducted qualitative interviews with 19\nparticipants to explore their preferences and concerns regarding the features,\nas well as the provotype's impact on users' understanding and trust toward\nrecommender systems. Findings underscore the importance of integrating\ntransparency with control, and reaffirm users' desire for agency and the\nability to actively intervene in personalization. We also discuss insights for\nencouraging adoption and awareness of such agency-enhancing features. Overall,\nthis study contributes novel approaches and applicable insights, laying the\ngroundwork for designing more user-centered recommender systems that foreground\nuser autonomy and fairness in AI-driven content delivery."}
{"id": "2509.10860", "pdf": "https://arxiv.org/pdf/2509.10860.pdf", "abs": "https://arxiv.org/abs/2509.10860", "title": "Quantifier Scope Interpretation in Language Learners and LLMs", "authors": ["Shaohua Fang", "Yue Li", "Yan Cong"], "categories": ["cs.CL"], "comment": null, "summary": "Sentences with multiple quantifiers often lead to interpretive ambiguities,\nwhich can vary across languages. This study adopts a cross-linguistic approach\nto examine how large language models (LLMs) handle quantifier scope\ninterpretation in English and Chinese, using probabilities to assess\ninterpretive likelihood. Human similarity (HS) scores were used to quantify the\nextent to which LLMs emulate human performance across language groups. Results\nreveal that most LLMs prefer the surface scope interpretations, aligning with\nhuman tendencies, while only some differentiate between English and Chinese in\nthe inverse scope preferences, reflecting human-similar patterns. HS scores\nhighlight variability in LLMs' approximation of human behavior, but their\noverall potential to align with humans is notable. Differences in model\narchitecture, scale, and particularly models' pre-training data language\nbackground, significantly influence how closely LLMs approximate human\nquantifier scope interpretations."}
{"id": "2509.11115", "pdf": "https://arxiv.org/pdf/2509.11115.pdf", "abs": "https://arxiv.org/abs/2509.11115", "title": "\"Pragmatic Tools or Empowering Friends?\" Discovering and Co-Designing Personality-Aligned AI Writing Companions", "authors": ["Mengke Wu", "Kexin Quan", "Weizi Liu", "Mike Yao", "Jessie Chin"], "categories": ["cs.HC"], "comment": "31 pages, 10 figures", "summary": "The growing popularity of AI writing assistants presents exciting\nopportunities to craft tools that cater to diverse user needs. This study\nexplores how personality shapes preferences for AI writing companions and how\npersonalized designs can enhance human-AI teaming. In an exploratory co-design\nworkshop, we worked with 24 writers with different profiles to surface ideas\nand map the design space for personality-aligned AI writing companions,\nfocusing on functionality, interaction dynamics, and visual representations.\nBuilding on these insights, we developed two contrasting prototypes tailored to\ndistinct writer profiles and engaged 8 participants with them as provocations\nto spark reflection and feedback. The results revealed strong connections\nbetween writer profiles and feature preferences, providing proof-of-concept for\npersonality-driven divergence in AI writing support. This research highlights\nthe critical role of team match in human-AI collaboration and underscores the\nimportance of aligning AI systems with individual cognitive needs to improve\nuser engagement and collaboration productivity."}
{"id": "2509.10882", "pdf": "https://arxiv.org/pdf/2509.10882.pdf", "abs": "https://arxiv.org/abs/2509.10882", "title": "Term2Note: Synthesising Differentially Private Clinical Notes from Medical Terms", "authors": ["Yuping Wu", "Viktor Schlegel", "Warren Del-Pinto", "Srinivasan Nandakumar", "Iqra Zahid", "Yidan Sun", "Usama Farghaly Omar", "Amirah Jasmine", "Arun-Kumar Kaliya-Perumal", "Chun Shen Tham", "Gabriel Connors", "Anil A Bharath", "Goran Nenadic"], "categories": ["cs.CL"], "comment": null, "summary": "Training data is fundamental to the success of modern machine learning\nmodels, yet in high-stakes domains such as healthcare, the use of real-world\ntraining data is severely constrained by concerns over privacy leakage. A\npromising solution to this challenge is the use of differentially private (DP)\nsynthetic data, which offers formal privacy guarantees while maintaining data\nutility. However, striking the right balance between privacy protection and\nutility remains challenging in clinical note synthesis, given its domain\nspecificity and the complexity of long-form text generation. In this paper, we\npresent Term2Note, a methodology to synthesise long clinical notes under strong\nDP constraints. By structurally separating content and form, Term2Note\ngenerates section-wise note content conditioned on DP medical terms, with each\ngoverned by separate DP constraints. A DP quality maximiser further enhances\nsynthetic notes by selecting high-quality outputs. Experimental results show\nthat Term2Note produces synthetic notes with statistical properties closely\naligned with real clinical notes, demonstrating strong fidelity. In addition,\nmulti-label classification models trained on these synthetic notes perform\ncomparably to those trained on real data, confirming their high utility.\nCompared to existing DP text generation baselines, Term2Note achieves\nsubstantial improvements in both fidelity and utility while operating under\nfewer assumptions, suggesting its potential as a viable privacy-preserving\nalternative to using sensitive clinical notes."}
{"id": "2509.11206", "pdf": "https://arxiv.org/pdf/2509.11206.pdf", "abs": "https://arxiv.org/abs/2509.11206", "title": "Evalet: Evaluating Large Language Models by Fragmenting Outputs into Functions", "authors": ["Tae Soo Kim", "Heechan Lee", "Yoonjoo Lee", "Joseph Seering", "Juho Kim"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "Practitioners increasingly rely on Large Language Models (LLMs) to evaluate\ngenerative AI outputs through \"LLM-as-a-Judge\" approaches. However, these\nmethods produce holistic scores that obscure which specific elements influenced\nthe assessments. We propose functional fragmentation, a method that dissects\neach output into key fragments and interprets the rhetoric functions that each\nfragment serves relative to evaluation criteria -- surfacing the elements of\ninterest and revealing how they fulfill or hinder user goals. We instantiate\nthis approach in Evalet, an interactive system that visualizes fragment-level\nfunctions across many outputs to support inspection, rating, and comparison of\nevaluations. A user study (N=10) found that, while practitioners struggled to\nvalidate holistic scores, our approach helped them identify 48% more evaluation\nmisalignments. This helped them calibrate trust in LLM evaluations and rely on\nthem to find more actionable issues in model outputs. Our work shifts LLM\nevaluation from quantitative scores toward qualitative, fine-grained analysis\nof model behavior."}
{"id": "2509.10886", "pdf": "https://arxiv.org/pdf/2509.10886.pdf", "abs": "https://arxiv.org/abs/2509.10886", "title": "CultureSynth: A Hierarchical Taxonomy-Guided and Retrieval-Augmented Framework for Cultural Question-Answer Synthesis", "authors": ["Xinyu Zhang", "Pei Zhang", "Shuang Luo", "Jialong Tang", "Yu Wan", "Baosong Yang", "Fei Huang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted as a Findings paper at EMNLP 2025", "summary": "Cultural competence, defined as the ability to understand and adapt to\nmulticultural contexts, is increasingly vital for large language models (LLMs)\nin global environments. While several cultural benchmarks exist to assess LLMs'\ncultural competence, current evaluations suffer from fragmented taxonomies,\ndomain specificity, and heavy reliance on manual data annotation. To address\nthese limitations, we introduce CultureSynth, a novel framework comprising (1)\na comprehensive hierarchical multilingual cultural taxonomy covering 12 primary\nand 130 secondary topics, and (2) a Retrieval-Augmented Generation (RAG)-based\nmethodology leveraging factual knowledge to synthesize culturally relevant\nquestion-answer pairs. The CultureSynth-7 synthetic benchmark contains 19,360\nentries and 4,149 manually verified entries across 7 languages. Evaluation of\n14 prevalent LLMs of different sizes reveals clear performance stratification\nled by ChatGPT-4o-Latest and Qwen2.5-72B-Instruct. The results demonstrate that\na 3B-parameter threshold is necessary for achieving basic cultural competence,\nmodels display varying architectural biases in knowledge processing, and\nsignificant geographic disparities exist across models. We believe that\nCultureSynth offers a scalable framework for developing culturally aware AI\nsystems while reducing reliance on manual annotation\\footnote{Benchmark is\navailable at https://github.com/Eyr3/CultureSynth.}."}
{"id": "2509.11342", "pdf": "https://arxiv.org/pdf/2509.11342.pdf", "abs": "https://arxiv.org/abs/2509.11342", "title": "What if Virtual Agents Had Scents? Users' Judgments of Virtual Agent Personality and Appeals in Encounters", "authors": ["Dongyun Han", "Siyeon Bak", "So-Hui Kim", "Kangsoo Kim", "Sun-Jeong Kim", "Isaac Cho"], "categories": ["cs.HC"], "comment": null, "summary": "Incorporating multi-sensory cues into Virtual Reality (VR) can significantly\nenhance user experiences, mirroring the multi-sensory interactions we encounter\nin the real-world. Olfaction plays a crucial role in shaping impressions when\nengaging with others. This study examines how non-verbal cues from virtual\nagents-specifically olfactory cues, emotional expressions, and gender-influence\nuser perceptions during encounters with virtual agents. Our findings indicate\nthat in unscented, woodsy, and floral scent conditions, participants primarily\nrelied on visually observable cues to form their impressions of virtual agents.\nPositive emotional expressions, conveyed through facial expressions and\ngestures, contributed to more favorable impressions, with this effect being\nstronger for the female agent than the male agent. However, in the unpleasant\nscent condition, participants consistently formed negative impressions, which\noverpowered the influence of emotional expressions and gender, suggesting that\naversive olfactory stimuli can detrimentally impact user perceptions. Our\nresults emphasize the importance of carefully selecting olfactory stimuli when\ndesigning immersive and engaging VR interactions. Finally, we present our\nfindings and outline future research directions for effectively integrating\nolfactory cues into virtual agents."}
{"id": "2509.10922", "pdf": "https://arxiv.org/pdf/2509.10922.pdf", "abs": "https://arxiv.org/abs/2509.10922", "title": "Aligning ESG Controversy Data with International Guidelines through Semi-Automatic Ontology Construction", "authors": ["Tsuyoshi Iwata", "Guillaume Comte", "Melissa Flores", "Ryoma Kondo", "Ryohei Hisano"], "categories": ["cs.CL", "cs.CY"], "comment": "Author accepted manuscript. This paper has been accepted for\n  presentation at the ISWC 2025 Posters & Demos Track. License details will be\n  updated once the official proceedings are published", "summary": "The growing importance of environmental, social, and governance data in\nregulatory and investment contexts has increased the need for accurate,\ninterpretable, and internationally aligned representations of non-financial\nrisks, particularly those reported in unstructured news sources. However,\naligning such controversy-related data with principle-based normative\nframeworks, such as the United Nations Global Compact or Sustainable\nDevelopment Goals, presents significant challenges. These frameworks are\ntypically expressed in abstract language, lack standardized taxonomies, and\ndiffer from the proprietary classification systems used by commercial data\nproviders. In this paper, we present a semi-automatic method for constructing\nstructured knowledge representations of environmental, social, and governance\nevents reported in the news. Our approach uses lightweight ontology design,\nformal pattern modeling, and large language models to convert normative\nprinciples into reusable templates expressed in the Resource Description\nFramework. These templates are used to extract relevant information from news\ncontent and populate a structured knowledge graph that links reported incidents\nto specific framework principles. The result is a scalable and transparent\nframework for identifying and interpreting non-compliance with international\nsustainability guidelines."}
{"id": "2509.11347", "pdf": "https://arxiv.org/pdf/2509.11347.pdf", "abs": "https://arxiv.org/abs/2509.11347", "title": "Beyond the Portal: Enhancing Recognition in Virtual Reality Through Multisensory Cues", "authors": ["Siyeon Bak", "Dongyun Han", "Inho Jo", "Sun-Jeong Kim", "Isaac Cho"], "categories": ["cs.HC"], "comment": null, "summary": "While Virtual Reality (VR) systems have become increasingly immersive, they\nstill rely predominantly on visual input, which can constrain perceptual\nperformance when visual information is limited. Incorporating additional\nsensory modalities, such as sound and scent, offers a promising strategy to\nenhance user experience and overcome these limitations. This paper investigates\nthe contribution of auditory and olfactory cues in supporting perception within\nthe portal metaphor, a VR technique that reveals remote environments through\nnarrow, visually constrained transitions. We conducted a user study in which\nparticipants identified target scenes by selecting the correct portal among\nalternatives under varying sensory conditions. The results demonstrate that\nintegrating visual, auditory, and olfactory cues significantly improved both\nrecognition accuracy and response time. These findings highlight the potential\nof multisensory integration to compensate for visual constraints in VR and\nemphasize the value of incorporating sound and scent to enhance perception,\nimmersion, and interaction within future VR system designs."}
{"id": "2509.10935", "pdf": "https://arxiv.org/pdf/2509.10935.pdf", "abs": "https://arxiv.org/abs/2509.10935", "title": "Introducing Spotlight: A Novel Approach for Generating Captivating Key Information from Documents", "authors": ["Ankan Mullick", "Sombit Bose", "Rounak Saha", "Ayan Kumar Bhowmick", "Aditya Vempaty", "Prasenjit Dey", "Ravi Kokku", "Pawan Goyal", "Niloy Ganguly"], "categories": ["cs.CL"], "comment": "Paper accepted in EMNLP 2025 Main Conference (Full)", "summary": "In this paper, we introduce Spotlight, a novel paradigm for information\nextraction that produces concise, engaging narratives by highlighting the most\ncompelling aspects of a document. Unlike traditional summaries, which\nprioritize comprehensive coverage, spotlights selectively emphasize intriguing\ncontent to foster deeper reader engagement with the source material. We\nformally differentiate spotlights from related constructs and support our\nanalysis with a detailed benchmarking study using new datasets curated for this\nwork. To generate high-quality spotlights, we propose a two-stage approach:\nfine-tuning a large language model on our benchmark data, followed by alignment\nvia Direct Preference Optimization (DPO). Our comprehensive evaluation\ndemonstrates that the resulting model not only identifies key elements with\nprecision but also enhances readability and boosts the engagement value of the\noriginal document."}
{"id": "2509.11391", "pdf": "https://arxiv.org/pdf/2509.11391.pdf", "abs": "https://arxiv.org/abs/2509.11391", "title": "\"My Boyfriend is AI\": A Computational Analysis of Human-AI Companionship in Reddit's AI Community", "authors": ["Pat Pataranutaporn", "Sheer Karny", "Chayapatr Archiwaranguprok", "Constanze Albrecht", "Auren R. Liu", "Pattie Maes"], "categories": ["cs.HC", "cs.CY"], "comment": "22 pages, 9 figures", "summary": "Human-AI interaction researchers face an overwhelming challenge: synthesizing\ninsights from thousands of empirical studies to understand how AI impacts\npeople and inform effective design. Existing approach for literature reviews\ncluster papers by similarities, keywords or citations, missing the crucial\ncause-and-effect relationships that reveal how design decisions impact user\noutcomes. We introduce the Atlas of Human-AI Interaction, an interactive web\ninterface that provides the first systematic mapping of empirical findings\nacross 1,000+ HCI papers using LLM-powered knowledge extraction. Our approach\nidentifies causal relationships, and visualizes them through an AI-enabled\ninteractive web interface as a navigable knowledge graph. We extracted 2,037\nempirical findings, revealing research topic clusters, common themes, and\ndisconnected areas. Expert evaluation with 20 researchers revealed the system's\neffectiveness for discovering research gaps. This work demonstrates how AI can\ntransform literature synthesis itself, offering a scalable framework for\nevidence-based design, opening new possibilities for computational meta-science\nacross HCI and beyond."}
{"id": "2509.10937", "pdf": "https://arxiv.org/pdf/2509.10937.pdf", "abs": "https://arxiv.org/abs/2509.10937", "title": "An Interpretable Benchmark for Clickbait Detection and Tactic Attribution", "authors": ["Lihi Nofar", "Tomer Portal", "Aviv Elbaz", "Alexander Apartsin", "Yehudit Aperstein"], "categories": ["cs.CL"], "comment": "7 pages", "summary": "The proliferation of clickbait headlines poses significant challenges to the\ncredibility of information and user trust in digital media. While recent\nadvances in machine learning have improved the detection of manipulative\ncontent, the lack of explainability limits their practical adoption. This paper\npresents a model for explainable clickbait detection that not only identifies\nclickbait titles but also attributes them to specific linguistic manipulation\nstrategies. We introduce a synthetic dataset generated by systematically\naugmenting real news headlines using a predefined catalogue of clickbait\nstrategies. This dataset enables controlled experimentation and detailed\nanalysis of model behaviour. We present a two-stage framework for automatic\nclickbait analysis comprising detection and tactic attribution. In the first\nstage, we compare a fine-tuned BERT classifier with large language models\n(LLMs), specifically GPT-4.0 and Gemini 2.4 Flash, under both zero-shot\nprompting and few-shot prompting enriched with illustrative clickbait headlines\nand their associated persuasive tactics. In the second stage, a dedicated\nBERT-based classifier predicts the specific clickbait strategies present in\neach headline. This work advances the development of transparent and\ntrustworthy AI systems for combating manipulative media content. We share the\ndataset with the research community at\nhttps://github.com/LLM-HITCS25S/ClickbaitTacticsDetection"}
{"id": "2509.11401", "pdf": "https://arxiv.org/pdf/2509.11401.pdf", "abs": "https://arxiv.org/abs/2509.11401", "title": "Small Cues, Big Differences: Evaluating Interaction and Presentation for Annotation Retrieval in AR", "authors": ["Zahra Borhani", "Ali Ebrahimpour-Boroojeny", "Francisco R. Ortega"], "categories": ["cs.HC"], "comment": null, "summary": "Augmented Reality (AR) enables intuitive interaction with virtual annotations\noverlaid on the real world, supporting a wide range of applications such as\nremote assistance, education, and industrial training. However, as the number\nof heterogeneous annotations increases, their efficient retrieval remains an\nopen challenge in 3D environments. This paper examines how interaction\nmodalities and presentation designs affect user performance, workload, fatigue,\nand preference in AR annotation retrieval. In two user studies, we compare\neye-gaze versus hand-ray hovering and evaluate four presentation methods:\nOpacity-based, Scale-based, Nothing-based, and Marker-based. Results show that\neye-gaze was favored over hand-ray by users, despite leading to significantly\nhigher unintentional activations. Among the presentation methods, Scale-based\npresentation reduces workload and task completion time while aligning with user\npreferences. Our findings offer empirical insights into the effectiveness of\ndifferent annotation presentation methods, leading to design recommendations\nfor building more efficient and user-friendly AR annotation review systems."}
{"id": "2509.11101", "pdf": "https://arxiv.org/pdf/2509.11101.pdf", "abs": "https://arxiv.org/abs/2509.11101", "title": "EmoBench-Reddit: A Hierarchical Benchmark for Evaluating the Emotional Intelligence of Multimodal Large Language Models", "authors": ["Haokun Li", "Yazhou Zhang", "Jizhi Ding", "Qiuchi Li", "Peng Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "With the rapid advancement of Multimodal Large Language Models (MLLMs), they\nhave demonstrated exceptional capabilities across a variety of vision-language\ntasks. However, current evaluation benchmarks predominantly focus on objective\nvisual question answering or captioning, inadequately assessing the models'\nability to understand complex and subjective human emotions. To bridge this\ngap, we introduce EmoBench-Reddit, a novel, hierarchical benchmark for\nmultimodal emotion understanding. The dataset comprises 350 meticulously\ncurated samples from the social media platform Reddit, each containing an\nimage, associated user-provided text, and an emotion category (sad, humor,\nsarcasm, happy) confirmed by user flairs. We designed a hierarchical task\nframework that progresses from basic perception to advanced cognition, with\neach data point featuring six multiple-choice questions and one open-ended\nquestion of increasing difficulty. Perception tasks evaluate the model's\nability to identify basic visual elements (e.g., colors, objects), while\ncognition tasks require scene reasoning, intent understanding, and deep empathy\nintegrating textual context. We ensured annotation quality through a\ncombination of AI assistance (Claude 4) and manual verification."}
{"id": "2509.11438", "pdf": "https://arxiv.org/pdf/2509.11438.pdf", "abs": "https://arxiv.org/abs/2509.11438", "title": "Generative AI-Enabled Adaptive Learning Platform: How I Can Help You Pass Your Driving Test?", "authors": ["Riya Gill", "Ievgeniia Kuzminykh", "Maher Salem", "Bogdan Ghita"], "categories": ["cs.HC"], "comment": "17 pages, 8 tables, 3 figures, submitted to the Artificial\n  Intelligence in Education journal", "summary": "This study aims to develop an adaptive learning platform that leverages\ngenerative AI to automate assessment creation and feedback delivery. The\nplatform provides self-correcting tests and personalised feedback that adapts\nto each learners progress and history, ensuring a tailored learning experience.\nThe study involves the development and evaluation of a web-based application\nfor revision for the UK Driving Theory Test. The platform generates dynamic,\nnon-repetitive question sets and offers adaptive feedback based on user\nperformance over time. The effectiveness of AI-generated assessments and\nfeedback is evaluated through expert review and model analysis. The results\nshow the successful generation of relevant and accurate questions, alongside\npositive and helpful feedback. The personalised test generation closely aligns\nwith expert-created assessments, demonstrating the reliability of the system.\nThese findings suggest that generative AI can enhance learning outcomes by\nadapting to individual student needs and offering tailored support. This\nresearch introduces an AI-powered assessment and feedback system that goes\nbeyond traditional solutions by incorporating automation and adaptive learning.\nThe non-memoryless feedback mechanism ensures that student history and\nperformance inform future assessments, making the learning process more\neffective and individualised. This contrasts with conventional systems that\nprovide static, one-time feedback without considering past progress."}
{"id": "2509.11106", "pdf": "https://arxiv.org/pdf/2509.11106.pdf", "abs": "https://arxiv.org/abs/2509.11106", "title": "Fluid Language Model Benchmarking", "authors": ["Valentin Hofmann", "David Heineman", "Ian Magnusson", "Kyle Lo", "Jesse Dodge", "Maarten Sap", "Pang Wei Koh", "Chun Wang", "Hannaneh Hajishirzi", "Noah A. Smith"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "COLM 2025", "summary": "Language model (LM) benchmarking faces several challenges: comprehensive\nevaluations are costly, benchmarks often fail to measure the intended\ncapabilities, and evaluation quality can degrade due to labeling errors and\nbenchmark saturation. Although various strategies have been proposed to\nmitigate these issues, they tend to address individual aspects in isolation,\nneglecting broader questions about overall evaluation quality. Here, we\nintroduce Fluid Benchmarking, a new evaluation approach that advances LM\nbenchmarking across multiple dimensions. Inspired by psychometrics, Fluid\nBenchmarking is based on the insight that the relative value of benchmark items\ndepends on an LM's capability level, suggesting that evaluation should adapt to\neach LM. Methodologically, Fluid Benchmarking estimates an item response model\nbased on existing LM evaluation results and uses the inferred quantities to\nselect evaluation items dynamically, similar to computerized adaptive testing\nin education. In our experiments, we compare Fluid Benchmarking against the\ncommon practice of random item sampling as well as more sophisticated\nbaselines, including alternative methods grounded in item response theory. We\nexamine four dimensions -- efficiency, validity, variance, and saturation --\nand find that Fluid Benchmarking achieves superior performance in all of them\n(e.g., higher validity and less variance on MMLU with fifty times fewer items).\nOur analysis shows that the two components of Fluid Benchmarking have distinct\neffects: item response theory, used to map performance into a latent ability\nspace, increases validity, while dynamic item selection reduces variance.\nOverall, our results suggest that LM benchmarking can be substantially improved\nby moving beyond static evaluation."}
{"id": "2509.11461", "pdf": "https://arxiv.org/pdf/2509.11461.pdf", "abs": "https://arxiv.org/abs/2509.11461", "title": "CareerPooler: AI-Powered Metaphorical Pool Simulation Improves Experience and Outcomes in Career Exploration", "authors": ["Ziyi Wang", "Ziwen Zeng", "Yuan Li", "Zijian Ding"], "categories": ["cs.HC", "cs.AI", "H.5"], "comment": null, "summary": "Career exploration is uncertain, requiring decisions with limited information\nand unpredictable outcomes. While generative AI offers new opportunities for\ncareer guidance, most systems rely on linear chat interfaces that produce\noverly comprehensive and idealized suggestions, overlooking the non-linear and\neffortful nature of real-world trajectories. We present CareerPooler, a\ngenerative AI-powered system that employs a pool-table metaphor to simulate\ncareer development as a spatial and narrative interaction. Users strike balls\nrepresenting milestones, skills, and random events, where hints, collisions,\nand rebounds embody decision-making under uncertainty. In a within-subjects\nstudy with 24 participants, CareerPooler significantly improved engagement,\ninformation gain, satisfaction, and career clarity compared to a chatbot\nbaseline. Qualitative findings show that spatial-narrative interaction fosters\nexperience-based learning, resilience through setbacks, and reduced\npsychological burden. Our findings contribute to the design of AI-assisted\ncareer exploration systems and more broadly suggest that visually grounded\nanalogical interactions can make generative systems engaging and satisfying."}
{"id": "2509.11118", "pdf": "https://arxiv.org/pdf/2509.11118.pdf", "abs": "https://arxiv.org/abs/2509.11118", "title": "We Argue to Agree: Towards Personality-Driven Argumentation-Based Negotiation Dialogue Systems for Tourism", "authors": ["Priyanshu Priya", "Saurav Dudhate", "Desai Vishesh Yasheshbhai", "Asif Ekbal"], "categories": ["cs.CL", "cs.AI"], "comment": "Paper is accepted at EMNLP (Findings) 2025", "summary": "Integrating argumentation mechanisms into negotiation dialogue systems\nimproves conflict resolution through exchanges of arguments and critiques.\nMoreover, incorporating personality attributes enhances adaptability by\naligning interactions with individuals' preferences and styles. To advance\nthese capabilities in negotiation dialogue systems, we propose a novel\nPersonality-driven Argumentation-based Negotiation Dialogue Generation (PAN-DG)\ntask. To support this task, we introduce PACT, a dataset of Personality-driven\nArgumentation-based negotiation Conversations for Tourism sector. This dataset,\ngenerated using Large Language Models (LLMs), features three distinct\npersonality profiles, viz. Argumentation Profile, Preference Profile, and\nBuying Style Profile to simulate a variety of negotiation scenarios involving\ndiverse personalities. Thorough automatic and manual evaluations indicate that\nthe dataset comprises high-quality dialogues. Further, we conduct comparative\nexperiments between pre-trained and fine-tuned LLMs for the PAN-DG task.\nMulti-dimensional evaluation demonstrates that the fine-tuned LLMs effectively\ngenerate personality-driven rational responses during negotiations. This\nunderscores the effectiveness of PACT in enhancing personalization and\nreasoning capabilities in negotiation dialogue systems, thereby establishing a\nfoundation for future research in this domain."}
{"id": "2509.11478", "pdf": "https://arxiv.org/pdf/2509.11478.pdf", "abs": "https://arxiv.org/abs/2509.11478", "title": "Designing and Evaluating a Conversational Agent for Early Detection of Alzheimer's Disease and Related Dementias", "authors": ["Andrew G. Breithaupt", "Nayoung Choi", "James D. Finch", "Jeanne M. Powell", "Arin L. Nelson", "Oz A. Alon", "Howard J. Rosen", "Jinho D. Choi"], "categories": ["cs.HC", "cs.AI"], "comment": "First two authors contributed equally", "summary": "Early detection of Alzheimer's disease and related dementias (ADRD) is\ncritical for timely intervention, yet most diagnoses are delayed until advanced\nstages. While comprehensive patient narratives are essential for accurate\ndiagnosis, prior work has largely focused on screening studies that classify\ncognitive status from interactions rather than supporting the diagnostic\nprocess. We designed voice-interactive conversational agents, leveraging large\nlanguage models (LLMs), to elicit narratives relevant to ADRD from patients and\ninformants. We evaluated the agent with 30 adults with suspected ADRD through\nconversation analysis (n=30), user surveys (n=19), and clinical validation\nagainst blinded specialist interviews (n=24). Symptoms detected by the agent\naligned well with those identified by specialists across symptoms. Users\nappreciated the agent's patience and systematic questioning, which supported\nengagement and expression of complex, hard-to-describe experiences. This\npreliminary work suggests conversational agents may serve as structured\nfront-end tools for dementia assessment, highlighting interaction design\nconsiderations in sensitive healthcare contexts."}
{"id": "2509.11127", "pdf": "https://arxiv.org/pdf/2509.11127.pdf", "abs": "https://arxiv.org/abs/2509.11127", "title": "Joint Effects of Argumentation Theory, Audio Modality and Data Enrichment on LLM-Based Fallacy Classification", "authors": ["Hongxu Zhou", "Hylke Westerdijk", "Khondoker Ittehadul Islam"], "categories": ["cs.CL"], "comment": null, "summary": "This study investigates how context and emotional tone metadata influence\nlarge language model (LLM) reasoning and performance in fallacy classification\ntasks, particularly within political debate settings. Using data from U.S.\npresidential debates, we classify six fallacy types through various prompting\nstrategies applied to the Qwen-3 (8B) model. We introduce two theoretically\ngrounded Chain-of-Thought frameworks: Pragma-Dialectics and the Periodic Table\nof Arguments, and evaluate their effectiveness against a baseline prompt under\nthree input settings: text-only, text with context, and text with both context\nand audio-based emotional tone metadata. Results suggest that while theoretical\nprompting can improve interpretability and, in some cases, accuracy, the\naddition of context and especially emotional tone metadata often leads to\nlowered performance. Emotional tone metadata biases the model toward labeling\nstatements as \\textit{Appeal to Emotion}, worsening logical reasoning. Overall,\nbasic prompts often outperformed enhanced ones, suggesting that attention\ndilution from added inputs may worsen rather than improve fallacy\nclassification in LLMs."}
{"id": "2509.11487", "pdf": "https://arxiv.org/pdf/2509.11487.pdf", "abs": "https://arxiv.org/abs/2509.11487", "title": "Collective Recourse for Generative Urban Visualizations", "authors": ["Rashid Mushkani"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Text-to-image diffusion models help visualize urban futures but can amplify\ngroup-level harms. We propose collective recourse: structured community \"visual\nbug reports\" that trigger fixes to models and planning workflows. We (1)\nformalize collective recourse and a practical pipeline (report, triage, fix,\nverify, closure); (2) situate four recourse primitives within the diffusion\nstack: counter-prompts, negative prompts, dataset edits, and reward-model\ntweaks; (3) define mandate thresholds via a mandate score combining severity,\nvolume saturation, representativeness, and evidence; and (4) evaluate a\nsynthetic program of 240 reports. Prompt-level fixes were fastest (median\n2.1-3.4 days) but less durable (21-38% recurrence); dataset edits and reward\ntweaks were slower (13.5 and 21.9 days) yet more durable (12-18% recurrence)\nwith higher planner uptake (30-36%). A threshold of 0.12 yielded 93% precision\nand 75% recall; increasing representativeness raised recall to 81% with little\nprecision loss. We discuss integration with participatory governance, risks\n(e.g., overfitting to vocal groups), and safeguards (dashboards, rotating\njuries)."}
{"id": "2509.11141", "pdf": "https://arxiv.org/pdf/2509.11141.pdf", "abs": "https://arxiv.org/abs/2509.11141", "title": "When Smiley Turns Hostile: Interpreting How Emojis Trigger LLMs' Toxicity", "authors": ["Shiyao Cui", "Xijia Feng", "Yingkang Wang", "Junxiao Yang", "Zhexin Zhang", "Biplab Sikdar", "Hongning Wang", "Han Qiu", "Minlie Huang"], "categories": ["cs.CL"], "comment": null, "summary": "Emojis are globally used non-verbal cues in digital communication, and\nextensive research has examined how large language models (LLMs) understand and\nutilize emojis across contexts. While usually associated with friendliness or\nplayfulness, it is observed that emojis may trigger toxic content generation in\nLLMs. Motivated by such a observation, we aim to investigate: (1) whether\nemojis can clearly enhance the toxicity generation in LLMs and (2) how to\ninterpret this phenomenon. We begin with a comprehensive exploration of\nemoji-triggered LLM toxicity generation by automating the construction of\nprompts with emojis to subtly express toxic intent. Experiments across 5\nmainstream languages on 7 famous LLMs along with jailbreak tasks demonstrate\nthat prompts with emojis could easily induce toxicity generation. To understand\nthis phenomenon, we conduct model-level interpretations spanning semantic\ncognition, sequence generation and tokenization, suggesting that emojis can act\nas a heterogeneous semantic channel to bypass the safety mechanisms. To pursue\ndeeper insights, we further probe the pre-training corpus and uncover potential\ncorrelation between the emoji-related data polution with the toxicity\ngeneration behaviors. Supplementary materials provide our implementation code\nand data. (Warning: This paper contains potentially sensitive contents)"}
{"id": "2509.11600", "pdf": "https://arxiv.org/pdf/2509.11600.pdf", "abs": "https://arxiv.org/abs/2509.11600", "title": "BioMetaphor: AI-Generated Biodata Representations for Virtual Co-Present Events", "authors": ["Lin Lin", "Ming Wu", "Anyu Ren", "Zhanwei Wu", "Daojun Gong", "Ruowei Xiao"], "categories": ["cs.HC"], "comment": null, "summary": "In virtual or hybrid co-present events, biodata is emerging as a new paradigm\nof social cues. While it is able to reveal individuals' inner states, the\ntechnology-mediated representation of biodata in social contexts remains\nunderexplored. This study aims to uncover human cognitive preferences and\npatterns for biodata expression and leverage this knowledge to guide generative\nAI (GenAI) in creating biodata representations for co-present experiences,\naligning with the broader concept of Human-in-the-loop. We conducted a user\nelicitation workshop with 30 HCI experts and investigated the results using\nqualitative analysis. Based on our findings, we further propose a GenAI-driven\nframework: BioMetaphor. Our framework demonstration shows that current GenAI\ncan learn and express visual biodata cues in an event-adpated, human-like\nmanner. This human-centered approach engages users in research, revealing the\nunderlying cognition constructions for biodata expression while demonstrating\nhow such knowledge can inform the design and development of future empathic\ntechnologies."}
{"id": "2509.11145", "pdf": "https://arxiv.org/pdf/2509.11145.pdf", "abs": "https://arxiv.org/abs/2509.11145", "title": "Text2Mem: A Unified Memory Operation Language for Memory Operating System", "authors": ["Felix Wang", "Boyu Chen", "Kerun Xu", "Bo Tang", "Feiyu Xiong", "Zhiyu Li"], "categories": ["cs.CL"], "comment": "11 pages, 3 figures", "summary": "Large language model agents increasingly depend on memory to sustain long\nhorizon interaction, but existing frameworks remain limited. Most expose only a\nfew basic primitives such as encode, retrieve, and delete, while higher order\noperations like merge, promote, demote, split, lock, and expire are missing or\ninconsistently supported. Moreover, there is no formal and executable\nspecification for memory commands, leaving scope and lifecycle rules implicit\nand causing unpredictable behavior across systems. We introduce Text2Mem, a\nunified memory operation language that provides a standardized pathway from\nnatural language to reliable execution. Text2Mem defines a compact yet\nexpressive operation set aligned with encoding, storage, and retrieval. Each\ninstruction is represented as a JSON based schema instance with required fields\nand semantic invariants, which a parser transforms into typed operation objects\nwith normalized parameters. A validator ensures correctness before execution,\nwhile adapters map typed objects either to a SQL prototype backend or to real\nmemory frameworks. Model based services such as embeddings or summarization are\nintegrated when required. All results are returned through a unified execution\ncontract. This design ensures safety, determinism, and portability across\nheterogeneous backends. We also outline Text2Mem Bench, a planned benchmark\nthat separates schema generation from backend execution to enable systematic\nevaluation. Together, these components establish the first standardized\nfoundation for memory control in agents."}
{"id": "2509.11622", "pdf": "https://arxiv.org/pdf/2509.11622.pdf", "abs": "https://arxiv.org/abs/2509.11622", "title": "Robots that Evolve with Us: Modular Co-Design for Personalization, Adaptability, and Sustainability", "authors": ["Lingyun Chen", "Qing Xiao", "Zitao Zhang", "Eli Blevis", "Selma Šabanović"], "categories": ["cs.HC"], "comment": "Pre-print", "summary": "Many current robot designs prioritize efficiency and one-size-fits-all\nsolutions, oftentimes overlooking personalization, adaptability, and\nsustainability. To explore alternatives, we conducted two co-design workshops\nwith 23 participants, who engaged with a modular robot co-design framework.\nUsing components we provided as building blocks, participants combined,\nremoved, and invented modules to envision how modular robots could accompany\nthem from childhood through adulthood and into older adulthood. The\nparticipants' designs illustrate how modularity (a) enables personalization\nthrough open-ended configuration, (b) adaptability across shifting life-stage\nneeds, and (c) sustainability through repair, reuse, and continuity. We\ntherefore derive design principles that establish modularity as a foundation\nfor lifespan-oriented human-robot interaction. This work reframes modular\nrobotics as a flexible and expressive co-design approach, supporting robots\nthat evolve with people, rather than static products optimized for single\nmoments or contexts of use."}
{"id": "2509.11176", "pdf": "https://arxiv.org/pdf/2509.11176.pdf", "abs": "https://arxiv.org/abs/2509.11176", "title": "Differentially-private text generation degrades output language quality", "authors": ["Erion Çano", "Ivan Habernal"], "categories": ["cs.CL", "cs.AI"], "comment": "20 pages, 3 figures, 35 tables", "summary": "Ensuring user privacy by synthesizing data from large language models (LLMs)\ntuned under differential privacy (DP) has become popular recently. However, the\nimpact of DP fine-tuned LLMs on the quality of the language and the utility of\nthe texts they produce has not been investigated. In this work, we tune five\nLLMs with three corpora under four levels of privacy and assess the length, the\ngrammatical correctness, and the lexical diversity of the text outputs they\nproduce. We also probe the utility of the synthetic outputs in downstream\nclassification tasks such as book genre recognition based on book descriptions\nand cause of death recognition based on verbal autopsies. The results indicate\nthat LLMs tuned under stronger privacy constrains produce texts that are\nshorter by at least 77 %, that are less grammatically correct by at least 9 %,\nand are less diverse by at least 10 % in bi-gram diversity. Furthermore, the\naccuracy they reach in downstream classification tasks decreases, which might\nbe detrimental to the usefulness of the generated synthetic data."}
{"id": "2509.11644", "pdf": "https://arxiv.org/pdf/2509.11644.pdf", "abs": "https://arxiv.org/abs/2509.11644", "title": "Colour Perception in Immersive Virtual Reality: Emotional and Physiological Responses to Fifteen Munsell Hues", "authors": ["Francesco Febbraio", "Simona Collina", "Christina Lepida", "Panagiotis Kourtesis"], "categories": ["cs.HC", "H.5.2; J.3; J.4; K.4"], "comment": "24 pages, 6 Figures, 9 Tables", "summary": "Colour is a fundamental determinant of affective experience in immersive\nvirtual reality (VR), yet the emotional and physiological impact of individual\nhues remains poorly characterised. This study investigated how fifteen\ncalibrated Munsell hues influence subjective and autonomic responses when\npresented in immersive VR. Thirty-six adults (18-45 years) viewed each hue in a\nwithin-subject design while pupil diameter and skin conductance were recorded\ncontinuously, and self-reported emotions were assessed using the\nSelf-Assessment Manikin across pleasure, arousal, and dominance.\nRepeated-measures ANOVAs revealed robust hue effects on all three self-report\ndimensions and on pupil dilation, with medium to large effect sizes. Reds and\nred-purple hues elicited the highest arousal and dominance, whereas blue-green\nhues were rated most pleasurable. Pupil dilation closely tracked arousal\nratings, while skin conductance showed no reliable hue differentiation, likely\ndue to the brief (30 s) exposures. Individual differences in cognitive style\nand personality modulated overall reactivity but did not alter the relative\nranking of hues. Taken together, these findings provide the first systematic\nhue-by-hue mapping of affective and physiological responses in immersive VR.\nThey demonstrate that calibrated colour shapes both experience and ocular\nphysiology, while also offering practical guidance for educational, clinical,\nand interface design in virtual environments."}
{"id": "2509.11177", "pdf": "https://arxiv.org/pdf/2509.11177.pdf", "abs": "https://arxiv.org/abs/2509.11177", "title": "Optimal Brain Restoration for Joint Quantization and Sparsification of LLMs", "authors": ["Hang Guo", "Yawei Li", "Luca Benini"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "Recent advances in Large Language Model (LLM) compression, such as\nquantization and pruning, have achieved notable success. However, as these\ntechniques gradually approach their respective limits, relying on a single\nmethod for further compression has become increasingly challenging. In this\nwork, we explore an alternative solution by combining quantization and\nsparsity. This joint approach, though promising, introduces new difficulties\ndue to the inherently conflicting requirements on weight distributions:\nquantization favors compact ranges, while pruning benefits from high variance.\nTo attack this problem, we propose Optimal Brain Restoration (OBR), a general\nand training-free framework that aligns pruning and quantization by error\ncompensation between both. OBR minimizes performance degradation on downstream\ntasks by building on a second-order Hessian objective, which is then\nreformulated into a tractable problem through surrogate approximation and\nultimately reaches a closed-form solution via group error compensation.\nExperiments show that OBR enables aggressive W4A4KV4 quantization with 50%\nsparsity on existing LLMs, and delivers up to 4.72x speedup and 6.4x memory\nreduction compared to the FP16-dense baseline."}
{"id": "2509.11653", "pdf": "https://arxiv.org/pdf/2509.11653.pdf", "abs": "https://arxiv.org/abs/2509.11653", "title": "See What I Mean? Mobile Eye-Perspective Rendering for Optical See-through Head-mounted Displays", "authors": ["Gerlinde Emsenhuber", "Tobias Langlotz", "Denis Kalkofen", "Markus Tatzgern"], "categories": ["cs.HC"], "comment": null, "summary": "Image-based scene understanding allows Augmented Reality systems to provide\ncontextual visual guidance in unprepared, real-world environments. While\neffective on video see-through (VST) head-mounted displays (HMDs), such methods\nsuffer on optical see-through (OST) HMDs due to misregistration between the\nworld-facing camera and the user's eye perspective. To approximate the user's\ntrue eye view, we implement and evaluate three software-based eye-perspective\nrendering (EPR) techniques on a commercially available, untethered OST HMD\n(Microsoft HoloLens 2): (1) Plane-Proxy EPR, projecting onto a fixed-distance\nplane; (2) Mesh-Proxy EPR, using SLAM-based reconstruction for projection; and\n(3) Gaze-Proxy EPR, a novel eye-tracking-based method that aligns the\nprojection with the user's gaze depth. A user study on real-world tasks\nunderscores the importance of accurate EPR and demonstrates gaze-proxy as a\nlightweight alternative to geometry-based methods. We release our EPR framework\nas open source."}
{"id": "2509.11191", "pdf": "https://arxiv.org/pdf/2509.11191.pdf", "abs": "https://arxiv.org/abs/2509.11191", "title": "RanAT4BIE: Random Adversarial Training for Biomedical Information Extraction", "authors": ["Jian Chen", "Shengyi Lv", "Leilei Su"], "categories": ["cs.CL", "cs.IR"], "comment": "Accepted for publication at the International Joint Conference on\n  Neural Networks (IJCNN) 2025", "summary": "We introduce random adversarial training (RAT), a novel framework\nsuccessfully applied to biomedical information extraction (BioIE) tasks.\nBuilding on PubMedBERT as the foundational architecture, our study first\nvalidates the effectiveness of conventional adversarial training in enhancing\npre-trained language models' performance on BioIE tasks. While adversarial\ntraining yields significant improvements across various performance metrics, it\nalso introduces considerable computational overhead. To address this\nlimitation, we propose RAT as an efficiency solution for biomedical information\nextraction. This framework strategically integrates random sampling mechanisms\nwith adversarial training principles, achieving dual objectives: enhanced model\ngeneralization and robustness while significantly reducing computational costs.\nThrough comprehensive evaluations, RAT demonstrates superior performance\ncompared to baseline models in BioIE tasks. The results highlight RAT's\npotential as a transformative framework for biomedical natural language\nprocessing, offering a balanced solution to the model performance and\ncomputational efficiency."}
{"id": "2509.11826", "pdf": "https://arxiv.org/pdf/2509.11826.pdf", "abs": "https://arxiv.org/abs/2509.11826", "title": "Collaborative Document Editing with Multiple Users and AI Agents", "authors": ["Florian Lehmann", "Krystsina Shauchenka", "Daniel Buschek"], "categories": ["cs.HC", "cs.CL", "H.5.2; I.2.7"], "comment": "34 pages, 10 figures, 4 tables", "summary": "Current AI writing support tools are largely designed for individuals,\ncomplicating collaboration when co-writers must leave the shared workspace to\nuse AI and then communicate and reintegrate results. We propose integrating AI\nagents directly into collaborative writing environments. Our prototype makes AI\nuse transparent and customisable through two new shared objects: agent profiles\nand tasks. Agent responses appear in the familiar comment feature. In a user\nstudy (N=30), 14 teams worked on writing projects during one week. Interaction\nlogs and interviews show that teams incorporated agents into existing norms of\nauthorship, control, and coordination, rather than treating them as team\nmembers. Agent profiles were viewed as personal territory, while created agents\nand outputs became shared resources. We discuss implications for team-based AI\ninteraction, highlighting opportunities and boundaries for treating AI as a\nshared resource in collaborative work."}
{"id": "2509.11295", "pdf": "https://arxiv.org/pdf/2509.11295.pdf", "abs": "https://arxiv.org/abs/2509.11295", "title": "The Prompt Engineering Report Distilled: Quick Start Guide for Life Sciences", "authors": ["Valentin Romanov", "Steven A Niederer"], "categories": ["cs.CL"], "comment": null, "summary": "Developing effective prompts demands significant cognitive investment to\ngenerate reliable, high-quality responses from Large Language Models (LLMs). By\ndeploying case-specific prompt engineering techniques that streamline\nfrequently performed life sciences workflows, researchers could achieve\nsubstantial efficiency gains that far exceed the initial time investment\nrequired to master these techniques. The Prompt Report published in 2025\noutlined 58 different text-based prompt engineering techniques, highlighting\nthe numerous ways prompts could be constructed. To provide actionable\nguidelines and reduce the friction of navigating these various approaches, we\ndistil this report to focus on 6 core techniques: zero-shot, few-shot\napproaches, thought generation, ensembling, self-criticism, and decomposition.\nWe breakdown the significance of each approach and ground it in use cases\nrelevant to life sciences, from literature summarization and data extraction to\neditorial tasks. We provide detailed recommendations for how prompts should and\nshouldn't be structured, addressing common pitfalls including multi-turn\nconversation degradation, hallucinations, and distinctions between reasoning\nand non-reasoning models. We examine context window limitations, agentic tools\nlike Claude Code, while analyzing the effectiveness of Deep Research tools\nacross OpenAI, Google, Anthropic and Perplexity platforms, discussing current\nlimitations. We demonstrate how prompt engineering can augment rather than\nreplace existing established individual practices around data processing and\ndocument editing. Our aim is to provide actionable guidance on core prompt\nengineering principles, and to facilitate the transition from opportunistic\nprompting to an effective, low-friction systematic practice that contributes to\nhigher quality research."}
{"id": "2509.11851", "pdf": "https://arxiv.org/pdf/2509.11851.pdf", "abs": "https://arxiv.org/abs/2509.11851", "title": "The AI Memory Gap: Users Misremember What They Created With AI or Without", "authors": ["Tim Zindulka", "Sven Goller", "Daniela Fernandes", "Robin Welsch", "Daniel Buschek"], "categories": ["cs.HC", "cs.CL", "H.5.2; I.2.7"], "comment": "31 pages, 10 figures, 9 tables", "summary": "As large language models (LLMs) become embedded in interactive text\ngeneration, disclosure of AI as a source depends on people remembering which\nideas or texts came from themselves and which were created with AI. We\ninvestigate how accurately people remember the source of content when using AI.\nIn a pre-registered experiment, 184 participants generated and elaborated on\nideas both unaided and with an LLM-based chatbot. One week later, they were\nasked to identify the source (noAI vs withAI) of these ideas and texts. Our\nfindings reveal a significant gap in memory: After AI use, the odds of correct\nattribution dropped, with the steepest decline in mixed human-AI workflows,\nwhere either the idea or elaboration was created with AI. We validated our\nresults using a computational model of source memory. Discussing broader\nimplications, we highlight the importance of considering source confusion in\nthe design and use of interactive text generation technologies."}
{"id": "2509.11303", "pdf": "https://arxiv.org/pdf/2509.11303.pdf", "abs": "https://arxiv.org/abs/2509.11303", "title": "Ko-PIQA: A Korean Physical Commonsense Reasoning Dataset with Cultural Context", "authors": ["Dasol Choi", "Jungwhan Kim", "Guijin Son"], "categories": ["cs.CL"], "comment": null, "summary": "Physical commonsense reasoning datasets like PIQA are predominantly\nEnglish-centric and lack cultural diversity. We introduce Ko-PIQA, a Korean\nphysical commonsense reasoning dataset that incorporates cultural context.\nStarting from 3.01 million web-crawled questions, we employed a multi-stage\nfiltering approach using three language models to identify 11,553 PIQA-style\nquestions. Through GPT-4o refinement and human validation, we obtained 441\nhigh-quality question-answer pairs. A key feature of Ko-PIQA is its cultural\ngrounding: 19.7\\% of questions contain culturally specific elements like\ntraditional Korean foods (kimchi), clothing (hanbok), and specialized\nappliances (kimchi refrigerators) that require culturally-aware reasoning\nbeyond direct translation. We evaluate seven language models on Ko-PIQA, with\nthe best model achieving 83.22\\% accuracy while the weakest reaches only\n59.86\\%, demonstrating significant room for improvement. Models particularly\nstruggle with culturally specific scenarios, highlighting the importance of\nculturally diverse datasets. Ko-PIQA serves as both a benchmark for Korean\nlanguage models and a foundation for more inclusive commonsense reasoning\nresearch. The dataset and code will be publicly available."}
{"id": "2509.11876", "pdf": "https://arxiv.org/pdf/2509.11876.pdf", "abs": "https://arxiv.org/abs/2509.11876", "title": "Lost in Data: How Older Adults Perceive and Navigate Health Data Representations", "authors": ["Peterson Jean", "Emma Murphy", "Enda Bates"], "categories": ["cs.HC"], "comment": "AAATE 2025 Proceedings (Research Strand). Licensed under CC BY-NC-ND\n  4.0. ISBN: 978-9925-604-07-4", "summary": "As the ageing population grows, older adults increasingly rely on wearable\ndevices to monitor chronic conditions. However, conventional health data\nrepresentations (HDRs) often present accessibility challenges, particularly for\ncritical health parameters like blood pressure and sleep data. This study\nexplores how older adults interact with these representations, identifying key\nbarriers such as semantic inconsistency and difficulties in understanding.\nWhile research has primarily focused on data collection, less attention has\nbeen given to how information is output and understood by end-users. To address\nthis, an end-user evaluation was conducted with 16 older adults (65+) in a\nstructured workshop, using think-aloud protocols and participatory design\nactivities. The findings highlight the importance of affordance and familiarity\nin improving accessibility, emphasising the familiarity and potential of\nmultimodal cues. This study bridges the gap between domain experts and\nend-users, providing a replicable methodological approach for designing\nintuitive, multisensory HDRs that better align with older adults' needs and\nabilities."}
{"id": "2509.11365", "pdf": "https://arxiv.org/pdf/2509.11365.pdf", "abs": "https://arxiv.org/abs/2509.11365", "title": "!MSA at AraHealthQA 2025 Shared Task: Enhancing LLM Performance for Arabic Clinical Question Answering through Prompt Engineering and Ensemble Learning", "authors": ["Mohamed Tarek", "Seif Ahmed", "Mohamed Basem"], "categories": ["cs.CL"], "comment": "8 Pages , ArabicNLP 2025 , Co-located with EMNLP 2025", "summary": "We present our systems for Track 2 (General Arabic Health QA, MedArabiQ) of\nthe AraHealthQA-2025 shared task, where our methodology secured 2nd place in\nboth Sub-Task 1 (multiple-choice question answering) and Sub-Task 2 (open-ended\nquestion answering) in Arabic clinical contexts. For Sub-Task 1, we leverage\nthe Gemini 2.5 Flash model with few-shot prompting, dataset preprocessing, and\nan ensemble of three prompt configurations to improve classification accuracy\non standard, biased, and fill-in-the-blank questions. For Sub-Task 2, we employ\na unified prompt with the same model, incorporating role-playing as an Arabic\nmedical expert, few-shot examples, and post-processing to generate concise\nresponses across fill-in-the-blank, patient-doctor Q&A, GEC, and paraphrased\nvariants."}
{"id": "2509.11898", "pdf": "https://arxiv.org/pdf/2509.11898.pdf", "abs": "https://arxiv.org/abs/2509.11898", "title": "Generative AI in Game Development: A Qualitative Research Synthesis", "authors": ["Alexandru Ternar", "Alena Denisova", "João M. Cunha", "Annakaisa Kultima", "Christian Guckelsberger"], "categories": ["cs.HC"], "comment": "32 pages, 2 figures, 6 tables", "summary": "Generative Artificial Intelligence (GenAI) has had a tremendous impact on\ngame production and promises lasting transformations. In the last five years\nsince GenAI's inception, several studies, typically via qualitative methods,\nhave explored its impact on game production from different settings and\ndemographic angles. However, these studies often contextualise and consolidate\ntheir findings weakly with related work, and a big picture view is still\nmissing. Here, we aim to provide such a view of GenAI's impact on game\nproduction in the form of a qualitative research synthesis via\nmeta-ethnography. We followed PRISMA-S to systematically search the relevant\nliterature from 2020-2025, including major HCI and games research databases. We\nthen synthesised the 10 eligible studies, conducting reciprocal translation and\nline-of-argument synthesis guided by eMERGe, informed by CASP quality\nappraisal. We identified nine overarching themes, provide recommendations, and\ncontextualise our insights in wider game production trends."}
{"id": "2509.11374", "pdf": "https://arxiv.org/pdf/2509.11374.pdf", "abs": "https://arxiv.org/abs/2509.11374", "title": "Transformer Enhanced Relation Classification: A Comparative Analysis of Contextuality, Data Efficiency and Sequence Complexity", "authors": ["Bowen Jing", "Yang Cui", "Tianpeng Huang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In the era of large language model, relation extraction (RE) plays an\nimportant role in information extraction through the transformation of\nunstructured raw text into structured data (Wadhwa et al., 2023). In this\npaper, we systematically compare the performance of deep supervised learning\napproaches without transformers and those with transformers. We used a series\nof non-transformer architectures such as PA-LSTM(Zhang et al., 2017),\nC-GCN(Zhang et al., 2018), and AGGCN(attention guide GCN)(Guo et al., 2019),\nand a series of transformer architectures such as BERT, RoBERTa, and R-BERT(Wu\nand He, 2019). Our comparison included traditional metrics like micro F1, as\nwell as evaluations in different scenarios, varying sentence lengths, and\ndifferent percentages of the dataset for training. Our experiments were\nconducted on TACRED, TACREV, and RE-TACRED. The results show that\ntransformer-based models outperform non-transformer models, achieving micro F1\nscores of 80-90% compared to 64-67% for non-transformer models. Additionally,\nwe briefly review the research journey in supervised relation classification\nand discuss the role and current status of large language models (LLMs) in\nrelation extraction."}
{"id": "2509.11939", "pdf": "https://arxiv.org/pdf/2509.11939.pdf", "abs": "https://arxiv.org/abs/2509.11939", "title": "PrivWeb: Unobtrusive and Content-aware Privacy Protection For Web Agents", "authors": ["Shuning Zhang", "Yutong Jiang", "Rongjun Ma", "Yuting Yang", "Mingyao Xu", "Zhixin Huang", "Xin Yi", "Hewu Li"], "categories": ["cs.HC"], "comment": null, "summary": "While web agents gained popularity by automating web interactions, their\nrequirement for interface access introduces significant privacy risks that are\nunderstudied, particularly from users' perspective. Through a formative study\n(N=15), we found users frequently misunderstand agents' data practices, and\ndesired unobtrusive, transparent data management. To achieve this, we designed\nand implemented PrivWeb, a trusted add-on on web agents that utilizes a\nlocalized LLM to anonymize private information on interfaces according to user\npreferences. It features privacy categorization schema and adaptive\nnotifications that selectively pauses tasks for user control over information\ncollection for highly sensitive information, while offering non-disruptive\noptions for less sensitive information, minimizing human oversight. The user\nstudy (N=14) across travel, information retrieval, shopping, and entertainment\ntasks compared PrivWeb with baselines without notification and without control\nfor private information access, where PrivWeb reduced perceived privacy risks\nwith no associated increase in cognitive effort, and resulted in higher overall\nsatisfaction."}
{"id": "2509.11414", "pdf": "https://arxiv.org/pdf/2509.11414.pdf", "abs": "https://arxiv.org/abs/2509.11414", "title": "Continually Adding New Languages to Multilingual Language Models", "authors": ["Abraham Toluwase Owodunni", "Sachin Kumar"], "categories": ["cs.CL"], "comment": null, "summary": "Multilingual language models are trained on a fixed set of languages, and to\nsupport new languages, the models need to be retrained from scratch. This is an\nexpensive endeavor and is often infeasible, as model developers tend not to\nrelease their pre-training data. Naive approaches, such as continued\npretraining, suffer from catastrophic forgetting; however, mitigation\nstrategies like experience replay cannot be applied due to the lack of original\npretraining data. In this work, we investigate the problem of continually\nadding new languages to a multilingual model, assuming access to pretraining\ndata in only the target languages. We explore multiple approaches to address\nthis problem and propose Layer-Selective LoRA (LayRA), which adds Low-Rank\nAdapters (LoRA) to selected initial and final layers while keeping the rest of\nthe model frozen. LayRA builds on two insights: (1) LoRA reduces forgetting,\nand (2) multilingual models encode inputs in the source language in the initial\nlayers, reason in English in intermediate layers, and translate back to the\nsource language in final layers. We experiment with adding multiple\ncombinations of Galician, Swahili, and Urdu to pretrained language models and\nevaluate each method on diverse multilingual tasks. We find that LayRA provides\nthe overall best tradeoff between preserving models' capabilities in previously\nsupported languages, while being competitive with existing approaches such as\nLoRA in learning new languages. We also demonstrate that using model\narithmetic, the adapted models can be equipped with strong instruction\nfollowing abilities without access to any instruction tuning data in the target\nlanguages."}
{"id": "2509.11999", "pdf": "https://arxiv.org/pdf/2509.11999.pdf", "abs": "https://arxiv.org/abs/2509.11999", "title": "Teaching the Teachers: Building Generative AI Literacy in Higher Ed Instructors", "authors": ["Si Chen", "Xiuxiu Tang", "Alison Cheng", "Nitesh Chawla", "G. Alex Ambrose", "Ronald Metoyer"], "categories": ["cs.HC"], "comment": null, "summary": "Generative AI is reshaping higher education, yet research has focused largely\non students, while instructors remain understudied despite their central role\nin mediating adoption and modeling responsible use. We present the \\textit{AI\nAcademy}, a faculty development program that combined AI exploration with\npedagogical reflection and peer learning. Rather than a course evaluated for\noutcomes, the Academy provided a setting to study how instructors build AI\nliteracies in relation to tools, policies, peer practices, and institutional\nsupports. We studied 25 instructors through pre/post surveys, learning logs,\nand facilitator interviews. Findings show AI literacy gains alongside new\ninsights. We position instructors as designers of responsible AI practices and\ncontribute a replicable program model, a co-constructed survey instrument, and\ndesign insights for professional development that adapts to evolving tools and\nfosters ethical discussion."}
{"id": "2509.11443", "pdf": "https://arxiv.org/pdf/2509.11443.pdf", "abs": "https://arxiv.org/abs/2509.11443", "title": "A Transformer-Based Cross-Platform Analysis of Public Discourse on the 15-Minute City Paradigm", "authors": ["Gaurab Chhetri", "Darrell Anderson", "Boniphace Kutela", "Subasish Das"], "categories": ["cs.CL", "cs.SI"], "comment": "This is the author's preprint version of a paper accepted for\n  presentation at the 24th International Conference on Machine Learning and\n  Applications (ICMLA 2025), December 3-5, 2025, Florida, USA. The final\n  published version will appear in the official IEEE proceedings. Conference\n  site: https://www.icmla-conference.org/icmla25/", "summary": "This study presents the first multi-platform sentiment analysis of public\nopinion on the 15-minute city concept across Twitter, Reddit, and news media.\nUsing compressed transformer models and Llama-3-8B for annotation, we classify\nsentiment across heterogeneous text domains. Our pipeline handles long-form and\nshort-form text, supports consistent annotation, and enables reproducible\nevaluation. We benchmark five models (DistilRoBERTa, DistilBERT, MiniLM,\nELECTRA, TinyBERT) using stratified 5-fold cross-validation, reporting\nF1-score, AUC, and training time. DistilRoBERTa achieved the highest F1\n(0.8292), TinyBERT the best efficiency, and MiniLM the best cross-platform\nconsistency. Results show News data yields inflated performance due to class\nimbalance, Reddit suffers from summarization loss, and Twitter offers moderate\nchallenge. Compressed models perform competitively, challenging assumptions\nthat larger models are necessary. We identify platform-specific trade-offs and\npropose directions for scalable, real-world sentiment classification in urban\nplanning discourse."}
{"id": "2509.12027", "pdf": "https://arxiv.org/pdf/2509.12027.pdf", "abs": "https://arxiv.org/abs/2509.12027", "title": "Exploring Gaze Dynamics in VR Film Education: Gender, Avatar, and the Shift Between Male and Female Perspectives", "authors": ["Zheng Wei", "Jia Sun", "Junxiang Liao", "Lik-Hang Lee", "Pan Hui", "Huamin Qu", "Wai Tong", "Xian Xu"], "categories": ["cs.HC"], "comment": "Accepted by ISMAR 2025", "summary": "In virtual reality (VR) education, especially in creative fields like film\nproduction, avatar design and narrative style extend beyond appearance and\naesthetics. This study explores how the interaction between avatar gender, the\ndominant narrative actor's gender, and the learner's gender influences film\nproduction learning in VR, focusing on gaze dynamics and gender perspectives.\nUsing a 2*2*2 experimental design, 48 participants operated avatars of\ndifferent genders and interacted with male or female-dominant narratives. The\nresults show that the consistency between the avatar and gender affects\npresence, and learners' control over the avatar is also influenced by gender\nmatching. Learners using avatars of the opposite gender reported stronger\ncontrol, suggesting gender incongruity prompted more focus on the avatar.\nAdditionally, female participants with female avatars were more likely to adopt\na \"female gaze,\" favoring soft lighting and emotional shots, while male\nparticipants with male avatars were more likely to adopt a \"male gaze,\"\nchoosing dynamic shots and high contrast. When male participants used female\navatars, they favored \"female gaze,\" while female participants with male\navatars focused on \"male gaze\". These findings advance our understanding of how\navatar design and narrative style in VR-based education influence creativity\nand the cultivation of gender perspectives, and they offer insights for\ndeveloping more inclusive and diverse VR teaching tools going forward."}
{"id": "2509.11444", "pdf": "https://arxiv.org/pdf/2509.11444.pdf", "abs": "https://arxiv.org/abs/2509.11444", "title": "CognitiveSky: Scalable Sentiment and Narrative Analysis for Decentralized Social Media", "authors": ["Gaurab Chhetri", "Anandi Dutta", "Subasish Das"], "categories": ["cs.CL", "cs.SI"], "comment": "This is the author's preprint version of a paper accepted for\n  presentation at HICSS 59 (Hawaii International Conference on System\n  Sciences), 2026, Hawaii, USA. The final published version will appear in the\n  official conference proceedings. Conference site: https://hicss.hawaii.edu/", "summary": "The emergence of decentralized social media platforms presents new\nopportunities and challenges for real-time analysis of public discourse. This\nstudy introduces CognitiveSky, an open-source and scalable framework designed\nfor sentiment, emotion, and narrative analysis on Bluesky, a federated Twitter\nor X.com alternative. By ingesting data through Bluesky's Application\nProgramming Interface (API), CognitiveSky applies transformer-based models to\nannotate large-scale user-generated content and produces structured and\nanalyzable outputs. These summaries drive a dynamic dashboard that visualizes\nevolving patterns in emotion, activity, and conversation topics. Built entirely\non free-tier infrastructure, CognitiveSky achieves both low operational cost\nand high accessibility. While demonstrated here for monitoring mental health\ndiscourse, its modular design enables applications across domains such as\ndisinformation detection, crisis response, and civic sentiment analysis. By\nbridging large language models with decentralized networks, CognitiveSky offers\na transparent, extensible tool for computational social science in an era of\nshifting digital ecosystems."}
{"id": "2509.12049", "pdf": "https://arxiv.org/pdf/2509.12049.pdf", "abs": "https://arxiv.org/abs/2509.12049", "title": "Interaction-Driven Browsing: A Human-in-the-Loop Conceptual Framework Informed by Human Web Browsing for Browser-Using Agents", "authors": ["Hyeonggeun Yun", "Jinkyu Jang"], "categories": ["cs.HC", "cs.AI", "cs.MA"], "comment": null, "summary": "Although browser-using agents (BUAs) show promise for web tasks and\nautomation, most BUAs terminate after executing a single instruction, failing\nto support users' complex, nonlinear browsing with ambiguous goals, iterative\ndecision-making, and changing contexts. We present a human-in-the-loop (HITL)\nconceptual framework informed by theories of human web browsing behavior. The\nframework centers on an iterative loop in which the BUA proactively proposes\nnext actions and the user steers the browsing process through feedback. It also\ndistinguishes between exploration and exploitation actions, enabling users to\ncontrol the breadth and depth of their browsing. Consequently, the framework\naims to reduce users' physical and cognitive effort while preserving users'\ntraditional browsing mental model and supporting users in achieving\nsatisfactory outcomes. We illustrate how the framework operates with\nhypothetical use cases and discuss the shift from manual browsing to\ninteraction-driven browsing. We contribute a theoretically informed conceptual\nframework for BUAs."}
{"id": "2509.11465", "pdf": "https://arxiv.org/pdf/2509.11465.pdf", "abs": "https://arxiv.org/abs/2509.11465", "title": "CEMTM: Contextual Embedding-based Multimodal Topic Modeling", "authors": ["Amirhossein Abaskohi", "Raymond Li", "Chuyuan Li", "Shafiq Joty", "Giuseppe Carenini"], "categories": ["cs.CL", "cs.LG"], "comment": "EMNLP 2025", "summary": "We introduce CEMTM, a context-enhanced multimodal topic model designed to\ninfer coherent and interpretable topic structures from both short and long\ndocuments containing text and images. CEMTM builds on fine-tuned large vision\nlanguage models (LVLMs) to obtain contextualized embeddings, and employs a\ndistributional attention mechanism to weight token-level contributions to topic\ninference. A reconstruction objective aligns topic-based representations with\nthe document embedding, encouraging semantic consistency across modalities.\nUnlike existing approaches, CEMTM can process multiple images per document\nwithout repeated encoding and maintains interpretability through explicit\nword-topic and document-topic distributions. Extensive experiments on six\nmultimodal benchmarks show that CEMTM consistently outperforms unimodal and\nmultimodal baselines, achieving a remarkable average LLM score of 2.61. Further\nanalysis shows its effectiveness in downstream few-shot retrieval and its\nability to capture visually grounded semantics in complex domains such as\nscientific articles."}
{"id": "2509.12102", "pdf": "https://arxiv.org/pdf/2509.12102.pdf", "abs": "https://arxiv.org/abs/2509.12102", "title": "Can LLMs Address Mental Health Questions? A Comparison with Human Therapists", "authors": ["Synthia Wang", "Yuwei Cheng", "Austin Song", "Sarah Keedy", "Marc Berman", "Nick Feamster"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Limited access to mental health care has motivated the use of digital tools\nand conversational agents powered by large language models (LLMs), yet their\nquality and reception remain unclear. We present a study comparing\ntherapist-written responses to those generated by ChatGPT, Gemini, and Llama\nfor real patient questions. Text analysis showed that LLMs produced longer,\nmore readable, and lexically richer responses with a more positive tone, while\ntherapist responses were more often written in the first person. In a survey\nwith 150 users and 23 licensed therapists, participants rated LLM responses as\nclearer, more respectful, and more supportive than therapist-written answers.\nYet, both groups of participants expressed a stronger preference for human\ntherapist support. These findings highlight the promise and limitations of LLMs\nin mental health, underscoring the need for designs that balance their\ncommunicative strengths with concerns of trust, privacy, and accountability."}
{"id": "2509.11466", "pdf": "https://arxiv.org/pdf/2509.11466.pdf", "abs": "https://arxiv.org/abs/2509.11466", "title": "Improving LLMs' Learning for Coreference Resolution", "authors": ["Yujian Gan", "Yuan Liang", "Yanni Lin", "Juntao Yu", "Massimo Poesio"], "categories": ["cs.CL"], "comment": null, "summary": "Coreference Resolution (CR) is crucial for many NLP tasks, but existing LLMs\nstruggle with hallucination and under-performance. In this paper, we\ninvestigate the limitations of existing LLM-based approaches to CR-specifically\nthe Question-Answering (QA) Template and Document Template methods and propose\ntwo novel techniques: Reversed Training with Joint Inference and Iterative\nDocument Generation. Our experiments show that Reversed Training improves the\nQA Template method, while Iterative Document Generation eliminates\nhallucinations in the generated source text and boosts coreference resolution.\nIntegrating these methods and techniques offers an effective and robust\nsolution to LLM-based coreference resolution."}
{"id": "2509.12107", "pdf": "https://arxiv.org/pdf/2509.12107.pdf", "abs": "https://arxiv.org/abs/2509.12107", "title": "Exploring Conversational Design Choices in LLMs for Pedagogical Purposes: Socratic and Narrative Approaches for Improving Instructor's Teaching Practice", "authors": ["Si Chen", "Isabel R. Molnar", "Peiyu Li", "Adam Acunin", "Ting Hua", "Alex Ambrose", "Nitesh V. Chawla", "Ronald Metoyer"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) typically generate direct answers, yet they are\nincreasingly used as learning tools. Studying instructors' usage is critical,\ngiven their role in teaching and guiding AI adoption in education. We designed\nand evaluated TeaPT, an LLM for pedagogical purposes that supports instructors'\nprofessional development through two conversational approaches: a Socratic\napproach that uses guided questioning to foster reflection, and a Narrative\napproach that offers elaborated suggestions to extend externalized cognition.\nIn a mixed-method study with 41 higher-education instructors, the Socratic\nversion elicited greater engagement, while the Narrative version was preferred\nfor actionable guidance. Subgroup analyses further revealed that\nless-experienced, AI-optimistic instructors favored the Socratic version,\nwhereas more-experienced, AI-cautious instructors preferred the Narrative\nversion. We contribute design implications for LLMs for pedagogical purposes,\nshowing how adaptive conversational approaches can support instructors with\nvaried profiles while highlighting how AI attitudes and experience shape\ninteraction and learning."}
{"id": "2509.11492", "pdf": "https://arxiv.org/pdf/2509.11492.pdf", "abs": "https://arxiv.org/abs/2509.11492", "title": "ClaimIQ at CheckThat! 2025: Comparing Prompted and Fine-Tuned Language Models for Verifying Numerical Claims", "authors": ["Anirban Saha Anik", "Md Fahimul Kabir Chowdhury", "Andrew Wyckoff", "Sagnik Ray Choudhury"], "categories": ["cs.CL", "cs.AI"], "comment": "Notebook for the CheckThat! Lab at CLEF 2025", "summary": "This paper presents our system for Task 3 of the CLEF 2025 CheckThat! Lab,\nwhich focuses on verifying numerical and temporal claims using retrieved\nevidence. We explore two complementary approaches: zero-shot prompting with\ninstruction-tuned large language models (LLMs) and supervised fine-tuning using\nparameter-efficient LoRA. To enhance evidence quality, we investigate several\nselection strategies, including full-document input and top-k sentence\nfiltering using BM25 and MiniLM. Our best-performing model LLaMA fine-tuned\nwith LoRA achieves strong performance on the English validation set. However, a\nnotable drop in the test set highlights a generalization challenge. These\nfindings underscore the importance of evidence granularity and model adaptation\nfor robust numerical fact verification."}
{"id": "2509.12140", "pdf": "https://arxiv.org/pdf/2509.12140.pdf", "abs": "https://arxiv.org/abs/2509.12140", "title": "Worker Discretion Advised: Co-designing Risk Disclosure in Crowdsourced Responsible AI (RAI) Content Work", "authors": ["Alice Qian", "Ziqi Yang", "Ryland Shaw", "Jina Suh", "Laura Dabbish", "Hong Shen"], "categories": ["cs.HC", "cs.CY"], "comment": "Under review at CHI 2026", "summary": "Responsible AI (RAI) content work, such as annotation, moderation, or red\nteaming for AI safety, often exposes crowd workers to potentially harmful\ncontent. While prior work has underscored the importance of communicating\nwell-being risk to employed content moderators, designing effective disclosure\nmechanisms for crowd workers while balancing worker protection with the needs\nof task designers and platforms remains largely unexamined. To address this\ngap, we conducted co-design sessions with 29 task designers, workers, and\nplatform representatives. We investigated task designer preferences for support\nin disclosing tasks, worker preferences for receiving risk disclosure warnings,\nand how platform stakeholders envision their role in shaping risk disclosure\npractices. We identify design tensions and map the sociotechnical tradeoffs\nthat shape disclosure practices. We contribute design recommendations and\nfeature concepts for risk disclosure mechanisms in the context of RAI content\nwork."}
{"id": "2509.11496", "pdf": "https://arxiv.org/pdf/2509.11496.pdf", "abs": "https://arxiv.org/abs/2509.11496", "title": "AKCIT-FN at CheckThat! 2025: Switching Fine-Tuned SLMs and LLM Prompting for Multilingual Claim Normalization", "authors": ["Fabrycio Leite Nakano Almada", "Kauan Divino Pouso Mariano", "Maykon Adriell Dutra", "Victor Emanuel da Silva Monteiro", "Juliana Resplande Sant'Anna Gomes", "Arlindo Rodrigues Galvão Filho", "Anderson da Silva Soares"], "categories": ["cs.CL"], "comment": "15 pages, 2 figures", "summary": "Claim normalization, the transformation of informal social media posts into\nconcise, self-contained statements, is a crucial step in automated\nfact-checking pipelines. This paper details our submission to the CLEF-2025\nCheckThat! Task~2, which challenges systems to perform claim normalization\nacross twenty languages, divided into thirteen supervised (high-resource) and\nseven zero-shot (no training data) tracks.\n  Our approach, leveraging fine-tuned Small Language Models (SLMs) for\nsupervised languages and Large Language Model (LLM) prompting for zero-shot\nscenarios, achieved podium positions (top three) in fifteen of the twenty\nlanguages. Notably, this included second-place rankings in eight languages,\nfive of which were among the seven designated zero-shot languages, underscoring\nthe effectiveness of our LLM-based zero-shot strategy. For Portuguese, our\ninitial development language, our system achieved an average METEOR score of\n0.5290, ranking third. All implementation artifacts, including inference,\ntraining, evaluation scripts, and prompt configurations, are publicly available\nat https://github.com/ju-resplande/checkthat2025_normalization."}
{"id": "2509.12152", "pdf": "https://arxiv.org/pdf/2509.12152.pdf", "abs": "https://arxiv.org/abs/2509.12152", "title": "Beyond PII: How Users Attempt to Estimate and Mitigate Implicit LLM Inference", "authors": ["Synthia Wang", "Sai Teja Peddinti", "Nina Taft", "Nick Feamster"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) such as ChatGPT can infer personal attributes\nfrom seemingly innocuous text, raising privacy risks beyond memorized data\nleakage. While prior work has demonstrated these risks, little is known about\nhow users estimate and respond. We conducted a survey with 240 U.S.\nparticipants who judged text snippets for inference risks, reported concern\nlevels, and attempted rewrites to block inference. We compared their rewrites\nwith those generated by ChatGPT and Rescriber, a state-of-the-art sanitization\ntool. Results show that participants struggled to anticipate inference,\nperforming a little better than chance. User rewrites were effective in just\n28\\% of cases - better than Rescriber but worse than ChatGPT. We examined our\nparticipants' rewriting strategies, and observed that while paraphrasing was\nthe most common strategy it is also the least effective; instead abstraction\nand adding ambiguity were more successful. Our work highlights the importance\nof inference-aware design in LLM interactions."}
{"id": "2509.11498", "pdf": "https://arxiv.org/pdf/2509.11498.pdf", "abs": "https://arxiv.org/abs/2509.11498", "title": "DeDisCo at the DISRPT 2025 Shared Task: A System for Discourse Relation Classification", "authors": ["Zhuoxuan Ju", "Jingni Wu", "Abhishek Purushothama", "Amir Zeldes"], "categories": ["cs.CL"], "comment": "System submission for the DISRPT 2025 - Shared Task on Discourse\n  Relation Parsing and Treebanking In conjunction with CODI-CRAC & EMNLP 2025.\n  1st place in Task 3: relation classification", "summary": "This paper presents DeDisCo, Georgetown University's entry in the DISRPT 2025\nshared task on discourse relation classification. We test two approaches, using\nan mt5-based encoder and a decoder based approach using the openly available\nQwen model. We also experiment on training with augmented dataset for\nlow-resource languages using matched data translated automatically from\nEnglish, as well as using some additional linguistic features inspired by\nentries in previous editions of the Shared Task. Our system achieves a\nmacro-accuracy score of 71.28, and we provide some interpretation and error\nanalysis for our results."}
{"id": "2509.12153", "pdf": "https://arxiv.org/pdf/2509.12153.pdf", "abs": "https://arxiv.org/abs/2509.12153", "title": "You Are Not Alone: Designing Body Doubling for ADHD in Virtual Reality", "authors": ["Zinat Ara", "Imtiaz Bin Rahim", "Puqi Zhou", "Liuchuan Yu", "Behzad Esmaeili", "Lap-Fai Yu", "Sungsoo Ray Hong"], "categories": ["cs.HC"], "comment": null, "summary": "Adults with Attention Deficit Hyperactivity Disorder (ADHD) experience\nchallenges sustaining attention in the workplace. Body doubling, the concept of\nworking alongside another person, has been proposed as a productivity aid for\nADHD and other neurodivergent populations (NDs). However, prior work found no\nconclusive effectiveness and noted NDs' discomfort with social presence. This\nwork investigates body doubling as an ADHD centered productivity strategy in\nconstruction tasks. In Study 1, we explored challenges ADHD workers face in\nconstruction and identified design insights. In Study 2, we implemented a\nvirtual reality bricklaying task under three conditions: (C1) alone, (C2) with\na human body double, and (C3) with an AI body double. Results from 12\nparticipants show they finished tasks faster and perceived greater accuracy and\nsustained attention in C2 and C3 compared to C1. While body doubling was\nclearly preferred, opinions diverged between conditions. Our findings verify\nits effect and offer design implications for future interventions."}
{"id": "2509.11513", "pdf": "https://arxiv.org/pdf/2509.11513.pdf", "abs": "https://arxiv.org/abs/2509.11513", "title": "Unsupervised Candidate Ranking for Lexical Substitution via Holistic Sentence Semantics", "authors": ["Zhongyang Hu", "Naijie Gu", "Xiangzhi Tao", "Tianhui Gu", "Yibing Zhou"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "A key subtask in lexical substitution is ranking the given candidate words. A\ncommon approach is to replace the target word with a candidate in the original\nsentence and feed the modified sentence into a model to capture semantic\ndifferences before and after substitution. However, effectively modeling the\nbidirectional influence of candidate substitution on both the target word and\nits context remains challenging. Existing methods often focus solely on\nsemantic changes at the target position or rely on parameter tuning over\nmultiple evaluation metrics, making it difficult to accurately characterize\nsemantic variation. To address this, we investigate two approaches: one based\non attention weights and another leveraging the more interpretable integrated\ngradients method, both designed to measure the influence of context tokens on\nthe target token and to rank candidates by incorporating semantic similarity\nbetween the original and substituted sentences. Experiments on the LS07 and\nSWORDS datasets demonstrate that both approaches improve ranking performance."}
{"id": "2509.10466", "pdf": "https://arxiv.org/pdf/2509.10466.pdf", "abs": "https://arxiv.org/abs/2509.10466", "title": "A Real-Time Diminished Reality Approach to Privacy in MR Collaboration", "authors": ["Christian Fane"], "categories": ["cs.CV", "cs.HC", "H.5.1; H.5.2; I.4.6; I.4.4; I.2.10; I.4.9; K.4.1"], "comment": "50 pages, 12 figures | Demo video: https://youtu.be/udBxj35GEKI?t=499\n  | Code: https://github.com/c1h1r1i1s1 (multiple repositories)", "summary": "Diminished reality (DR) refers to the digital removal of real-world objects\nby compositing background content in their place. This thesis presents a\nreal-time, inpainting-based DR system designed to enable privacy control in\nshared-space mixed reality (MR) meetings. The system allows a primary headset\nuser to selectively remove personal or sensitive items from their environment,\nensuring that those objects are no longer visible to other participants.\nRemoval is achieved through semantic segmentation and precise object selection,\nfollowed by real-time inpainting from the viewpoint of a secondary observer,\nimplemented using a mobile ZED 2i depth camera. The solution is designed to be\nportable and robust, requiring neither a fixed secondary viewpoint nor prior 3D\nscanning of the environment. The system utilises YOLOv11 for object detection\nand a modified Decoupled Spatial-Temporal Transformer (DSTT) model for\nhigh-quality video inpainting. At 720p resolution, the pipeline sustains frame\nrates exceeding 20 fps, demonstrating the feasibility of real-time diminished\nreality for practical privacy-preserving MR applications."}
{"id": "2509.11514", "pdf": "https://arxiv.org/pdf/2509.11514.pdf", "abs": "https://arxiv.org/abs/2509.11514", "title": "LVLMs are Bad at Overhearing Human Referential Communication", "authors": ["Zhengxiang Wang", "Weiling Li", "Panagiotis Kaliosis", "Owen Rambow", "Susan E. Brennan"], "categories": ["cs.CL"], "comment": "EMNLP 2025 (Main)", "summary": "During spontaneous conversations, speakers collaborate on novel referring\nexpressions, which they can then re-use in subsequent conversations.\nUnderstanding such referring expressions is an important ability for an\nembodied agent, so that it can carry out tasks in the real world. This requires\nintegrating and understanding language, vision, and conversational interaction.\nWe study the capabilities of seven state-of-the-art Large Vision Language\nModels (LVLMs) as overhearers to a corpus of spontaneous conversations between\npairs of human discourse participants engaged in a collaborative\nobject-matching task. We find that such a task remains challenging for current\nLVLMs and they all fail to show a consistent performance improvement as they\noverhear more conversations from the same discourse participants repeating the\nsame task for multiple rounds. We release our corpus and code for\nreproducibility and to facilitate future research."}
{"id": "2509.10596", "pdf": "https://arxiv.org/pdf/2509.10596.pdf", "abs": "https://arxiv.org/abs/2509.10596", "title": "GenAI Voice Mode in Programming Education", "authors": ["Sven Jacobs", "Natalie Kiesler"], "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "Accepted for the 25th International Conference on Computing Education\n  Research (Koli Calling '25)", "summary": "Real-time voice interfaces using multimodal Generative AI (GenAI) can\npotentially address the accessibility needs of novice programmers with\ndisabilities (e.g., related to vision). Yet, little is known about how novices\ninteract with GenAI tools and their feedback quality in the form of audio\noutput. This paper analyzes audio dialogues from nine 9th-grade students using\na voice-enabled tutor (powered by OpenAI's Realtime API) in an authentic\nclassroom setting while learning Python. We examined the students' voice\nprompts and AI's responses (1210 messages) by using qualitative coding. We also\ngathered students' perceptions via the Partner Modeling Questionnaire. The\nGenAI Voice Tutor primarily offered feedback on mistakes and next steps, but\nits correctness was limited (71.4% correct out of 416 feedback outputs).\nQuality issues were observed, particularly when the AI attempted to utter\nprogramming code elements. Students used the GenAI voice tutor primarily for\ndebugging. They perceived it as competent, only somewhat human-like, and\nflexible. The present study is the first to explore the interaction dynamics of\nreal-time voice GenAI tutors and novice programmers, informing future\neducational tool design and potentially addressing accessibility needs of\ndiverse learners."}
{"id": "2509.11517", "pdf": "https://arxiv.org/pdf/2509.11517.pdf", "abs": "https://arxiv.org/abs/2509.11517", "title": "PeruMedQA: Benchmarking Large Language Models (LLMs) on Peruvian Medical Exams -- Dataset Construction and Evaluation", "authors": ["Rodrigo M. Carrillo-Larco", "Jesus Lovón Melgarejo", "Manuel Castillo-Cara", "Gusseppe Bravo-Rocca"], "categories": ["cs.CL", "cs.LG"], "comment": "https://github.com/rodrigo-carrillo/PeruMedQA", "summary": "BACKGROUND: Medical large language models (LLMS) have demonstrated remarkable\nperformance in answering medical examinations. However, the extent to which\nthis high performance is transferable to medical questions in Spanish and from\na Latin American country remains unexplored. This knowledge is crucial as\nLLM-based medical applications gain traction in Latin America. AIMS: to build a\ndataset of questions from medical examinations taken by Peruvian physicians\npursuing specialty training; to fine-tune a LLM on this dataset; to evaluate\nand compare the performance in terms of accuracy between vanilla LLMs and the\nfine-tuned LLM. METHODS: We curated PeruMedQA, a multiple-choice\nquestion-answering (MCQA) datasets containing 8,380 questions spanning 12\nmedical domains (2018-2025). We selected eight medical LLMs including\nmedgemma-4b-it and medgemma-27b-text-it, and developed zero-shot task-specific\nprompts to answer the questions appropriately. We employed parameter-efficient\nfine tuning (PEFT)and low-rant adaptation (LoRA) to fine-tune medgemma-4b-it\nutilizing all questions except those from 2025 (test set). RESULTS:\nmedgemma-27b-text-it outperformed all other models, achieving a proportion of\ncorrect answers exceeding 90% in several instances. LLMs with <10 billion\nparameters exhibited <60% of correct answers, while some exams yielded results\n<50%. The fine-tuned version of medgemma-4b-it emerged victorious agains all\nLLMs with <10 billion parameters and rivaled a LLM with 70 billion parameters\nacross various examinations. CONCLUSIONS: For medical AI application and\nresearch that require knowledge bases from Spanish-speaking countries and those\nexhibiting similar epidemiological profiles to Peru's, interested parties\nshould utilize medgemma-27b-text-it or a fine-tuned version of medgemma-4b-it."}
{"id": "2509.10818", "pdf": "https://arxiv.org/pdf/2509.10818.pdf", "abs": "https://arxiv.org/abs/2509.10818", "title": "LLM Enhancement with Domain Expert Mental Model to Reduce LLM Hallucination with Causal Prompt Engineering", "authors": ["Boris Kovalerchuk", "Brent D. Fegley"], "categories": ["cs.AI", "cs.HC"], "comment": "25 pages,4 figures, 2 tables", "summary": "Difficult decision-making problems abound in various disciplines and domains.\nThe proliferation of generative techniques, especially large language models\n(LLMs), has excited interest in using them for decision support. However, LLMs\ncannot yet resolve missingness in their training data, leading to\nhallucinations. Retrieval-Augmented Generation (RAG) enhances LLMs by\nincorporating external information retrieval, reducing hallucinations and\nimproving accuracy. Yet, RAG and related methods are only partial solutions, as\nthey may lack access to all necessary sources or key missing information. Even\neveryday issues often challenge LLMs' abilities. Submitting longer prompts with\ncontext and examples is one approach to address knowledge gaps, but designing\neffective prompts is non-trivial and may not capture complex mental models of\ndomain experts. For tasks with missing critical information, LLMs are\ninsufficient, as are many existing systems poorly represented in available\ndocuments. This paper explores how LLMs can make decision-making more\nefficient, using a running example of evaluating whether to respond to a call\nfor proposals. We propose a technology based on optimized human-machine\ndialogue and monotone Boolean and k-valued functions to discover a\ncomputationally tractable personal expert mental model (EMM) of\ndecision-making. Our EMM algorithm for LLM prompt engineering has four steps:\n(1) factor identification, (2) hierarchical structuring of factors, (3)\ngenerating a generalized expert mental model specification, and (4) generating\na detailed generalized expert mental model from that specification."}
{"id": "2509.11534", "pdf": "https://arxiv.org/pdf/2509.11534.pdf", "abs": "https://arxiv.org/abs/2509.11534", "title": "On the Distinctive Co-occurrence Characteristics of Antonymy", "authors": ["Zhihan Cao", "Hiroaki Yamada", "Takenobu Tokunaga"], "categories": ["cs.CL"], "comment": "Accepted by *SEM 2025", "summary": "Antonymy has long received particular attention in lexical semantics.\nPrevious studies have shown that antonym pairs frequently co-occur in text,\nacross genres and parts of speech, more often than would be expected by chance.\nHowever, whether this co-occurrence pattern is distinctive of antonymy remains\nunclear, due to a lack of comparison with other semantic relations. This work\nfills the gap by comparing antonymy with three other relations across parts of\nspeech using robust co-occurrence metrics. We find that antonymy is distinctive\nin three respects: antonym pairs co-occur with high strength, in a preferred\nlinear order, and within short spans. All results are available online."}
{"id": "2509.10833", "pdf": "https://arxiv.org/pdf/2509.10833.pdf", "abs": "https://arxiv.org/abs/2509.10833", "title": "Towards Automated Error Discovery: A Study in Conversational AI", "authors": ["Dominic Petrak", "Thy Thy Tran", "Iryna Gurevych"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": "Accepted to EMNLP 2025 main conference", "summary": "Although LLM-based conversational agents demonstrate strong fluency and\ncoherence, they still produce undesirable behaviors (errors) that are\nchallenging to prevent from reaching users during deployment. Recent research\nleverages large language models (LLMs) to detect errors and guide\nresponse-generation models toward improvement. However, current LLMs struggle\nto identify errors not explicitly specified in their instructions, such as\nthose arising from updates to the response-generation model or shifts in user\nbehavior. In this work, we introduce Automated Error Discovery, a framework for\ndetecting and defining errors in conversational AI, and propose SEEED (Soft\nClustering Extended Encoder-Based Error Detection), as an encoder-based\napproach to its implementation. We enhance the Soft Nearest Neighbor Loss by\namplifying distance weighting for negative samples and introduce Label-Based\nSample Ranking to select highly contrastive examples for better representation\nlearning. SEEED outperforms adapted baselines -- including GPT-4o and Phi-4 --\nacross multiple error-annotated dialogue datasets, improving the accuracy for\ndetecting unknown errors by up to 8 points and demonstrating strong\ngeneralization to unknown intent detection."}
{"id": "2509.11536", "pdf": "https://arxiv.org/pdf/2509.11536.pdf", "abs": "https://arxiv.org/abs/2509.11536", "title": "HARP: Hallucination Detection via Reasoning Subspace Projection", "authors": ["Junjie Hu", "Gang Tu", "ShengYu Cheng", "Jinxin Li", "Jinting Wang", "Rui Chen", "Zhilong Zhou", "Dongbo Shan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Hallucinations in Large Language Models (LLMs) pose a major barrier to their\nreliable use in critical decision-making. Although existing hallucination\ndetection methods have improved accuracy, they still struggle with\ndisentangling semantic and reasoning information and maintaining robustness. To\naddress these challenges, we propose HARP (Hallucination detection via\nreasoning subspace projection), a novel hallucination detection framework. HARP\nestablishes that the hidden state space of LLMs can be decomposed into a direct\nsum of a semantic subspace and a reasoning subspace, where the former encodes\nlinguistic expression and the latter captures internal reasoning processes.\nMoreover, we demonstrate that the Unembedding layer can disentangle these\nsubspaces, and by applying Singular Value Decomposition (SVD) to its\nparameters, the basis vectors spanning the semantic and reasoning subspaces are\nobtained. Finally, HARP projects hidden states onto the basis vectors of the\nreasoning subspace, and the resulting projections are then used as input\nfeatures for hallucination detection in LLMs. By using these projections, HARP\nreduces the dimension of the feature to approximately 5% of the original,\nfilters out most noise, and achieves enhanced robustness. Experiments across\nmultiple datasets show that HARP achieves state-of-the-art hallucination\ndetection performance; in particular, it achieves an AUROC of 92.8% on\nTriviaQA, outperforming the previous best method by 7.5%."}
{"id": "2509.11067", "pdf": "https://arxiv.org/pdf/2509.11067.pdf", "abs": "https://arxiv.org/abs/2509.11067", "title": "Agentic Lybic: Multi-Agent Execution System with Tiered Reasoning and Orchestration", "authors": ["Liangxuan Guo", "Bin Zhu", "Qingqian Tao", "Kangning Liu", "Xun Zhao", "Xianzhe Qin", "Jin Gao", "Guangfu Hao"], "categories": ["cs.AI", "cs.HC", "cs.MA"], "comment": null, "summary": "Autonomous agents for desktop automation struggle with complex multi-step\ntasks due to poor coordination and inadequate quality control. We introduce\n\\textsc{Agentic Lybic}, a novel multi-agent system where the entire\narchitecture operates as a finite-state machine (FSM). This core innovation\nenables dynamic orchestration. Our system comprises four components: a\nController, a Manager, three Workers (Technician for code-based operations,\nOperator for GUI interactions, and Analyst for decision support), and an\nEvaluator. The critical mechanism is the FSM-based routing between these\ncomponents, which provides flexibility and generalization by dynamically\nselecting the optimal execution strategy for each subtask. This principled\norchestration, combined with robust quality gating, enables adaptive replanning\nand error recovery. Evaluated officially on the OSWorld benchmark,\n\\textsc{Agentic Lybic} achieves a state-of-the-art 57.07\\% success rate in 50\nsteps, substantially outperforming existing methods. Results demonstrate that\nprincipled multi-agent orchestration with continuous quality control provides\nsuperior reliability for generalized desktop automation in complex computing\nenvironments."}
{"id": "2509.11552", "pdf": "https://arxiv.org/pdf/2509.11552.pdf", "abs": "https://arxiv.org/abs/2509.11552", "title": "HiChunk: Evaluating and Enhancing Retrieval-Augmented Generation with Hierarchical Chunking", "authors": ["Wensheng Lu", "Keyu Chen", "Ruizhi Qiao", "Xing Sun"], "categories": ["cs.CL", "cs.AI"], "comment": "17 pages, 5 figures, 6 tables", "summary": "Retrieval-Augmented Generation (RAG) enhances the response capabilities of\nlanguage models by integrating external knowledge sources. However, document\nchunking as an important part of RAG system often lacks effective evaluation\ntools. This paper first analyzes why existing RAG evaluation benchmarks are\ninadequate for assessing document chunking quality, specifically due to\nevidence sparsity. Based on this conclusion, we propose HiCBench, which\nincludes manually annotated multi-level document chunking points, synthesized\nevidence-dense quetion answer(QA) pairs, and their corresponding evidence\nsources. Additionally, we introduce the HiChunk framework, a multi-level\ndocument structuring framework based on fine-tuned LLMs, combined with the\nAuto-Merge retrieval algorithm to improve retrieval quality. Experiments\ndemonstrate that HiCBench effectively evaluates the impact of different\nchunking methods across the entire RAG pipeline. Moreover, HiChunk achieves\nbetter chunking quality within reasonable time consumption, thereby enhancing\nthe overall performance of RAG systems."}
{"id": "2509.11332", "pdf": "https://arxiv.org/pdf/2509.11332.pdf", "abs": "https://arxiv.org/abs/2509.11332", "title": "A five-layer framework for AI governance: integrating regulation, standards, and certification", "authors": ["Avinash Agarwal", "Manisha J. Nene"], "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "17 pages, 2 tables, 1 figure. This is the authors' accepted\n  manuscript of the article published as: Avinash Agarwal, Manisha J. Nene; \"A\n  five-layer framework for AI governance: integrating regulation, standards,\n  and certification.\" Transforming Government: People, Process and Policy, 11\n  September 2025; 19 (3): 535-555. https://doi.org/10.1108/TG-03-2025-0065", "summary": "Purpose: The governance of artificial iintelligence (AI) systems requires a\nstructured approach that connects high-level regulatory principles with\npractical implementation. Existing frameworks lack clarity on how regulations\ntranslate into conformity mechanisms, leading to gaps in compliance and\nenforcement. This paper addresses this critical gap in AI governance.\n  Methodology/Approach: A five-layer AI governance framework is proposed,\nspanning from broad regulatory mandates to specific standards, assessment\nmethodologies, and certification processes. By narrowing its scope through\nprogressively focused layers, the framework provides a structured pathway to\nmeet technical, regulatory, and ethical requirements. Its applicability is\nvalidated through two case studies on AI fairness and AI incident reporting.\n  Findings: The case studies demonstrate the framework's ability to identify\ngaps in legal mandates, standardization, and implementation. It adapts to both\nglobal and region-specific AI governance needs, mapping regulatory mandates\nwith practical applications to improve compliance and risk management.\n  Practical Implications - By offering a clear and actionable roadmap, this\nwork contributes to global AI governance by equipping policymakers, regulators,\nand industry stakeholders with a model to enhance compliance and risk\nmanagement.\n  Social Implications: The framework supports the development of policies that\nbuild public trust and promote the ethical use of AI for the benefit of\nsociety.\n  Originality/Value: This study proposes a five-layer AI governance framework\nthat bridges high-level regulatory mandates and implementation guidelines.\nValidated through case studies on AI fairness and incident reporting, it\nidentifies gaps such as missing standardized assessment procedures and\nreporting mechanisms, providing a structured foundation for targeted governance\nmeasures."}
{"id": "2509.11569", "pdf": "https://arxiv.org/pdf/2509.11569.pdf", "abs": "https://arxiv.org/abs/2509.11569", "title": "D$^2$HScore: Reasoning-Aware Hallucination Detection via Semantic Breadth and Depth Analysis in LLMs", "authors": ["Yue Ding", "Xiaofang Zhu", "Tianze Xia", "Junfei Wu", "Xinlong Chen", "Qiang Liu", "Liang Wang"], "categories": ["cs.CL"], "comment": "under review", "summary": "Although large Language Models (LLMs) have achieved remarkable success, their\npractical application is often hindered by the generation of non-factual\ncontent, which is called \"hallucination\". Ensuring the reliability of LLMs'\noutputs is a critical challenge, particularly in high-stakes domains such as\nfinance, security, and healthcare. In this work, we revisit hallucination\ndetection from the perspective of model architecture and generation dynamics.\nLeveraging the multi-layer structure and autoregressive decoding process of\nLLMs, we decompose hallucination signals into two complementary dimensions: the\nsemantic breadth of token representations within each layer, and the semantic\ndepth of core concepts as they evolve across layers. Based on this insight, we\npropose \\textbf{D$^2$HScore (Dispersion and Drift-based Hallucination Score)},\na training-free and label-free framework that jointly measures: (1)\n\\textbf{Intra-Layer Dispersion}, which quantifies the semantic diversity of\ntoken representations within each layer; and (2) \\textbf{Inter-Layer Drift},\nwhich tracks the progressive transformation of key token representations across\nlayers. To ensure drift reflects the evolution of meaningful semantics rather\nthan noisy or redundant tokens, we guide token selection using attention\nsignals. By capturing both the horizontal and vertical dynamics of\nrepresentation during inference, D$^2$HScore provides an interpretable and\nlightweight proxy for hallucination detection. Extensive experiments across\nfive open-source LLMs and five widely used benchmarks demonstrate that\nD$^2$HScore consistently outperforms existing training-free baselines."}
{"id": "2509.11700", "pdf": "https://arxiv.org/pdf/2509.11700.pdf", "abs": "https://arxiv.org/abs/2509.11700", "title": "Quantization Errors, Human--AI Interaction, and Approximate Fixed Points in $L^1(μ)$", "authors": ["Faruk Alpay", "Hamdi Alakkad"], "categories": ["math.FA", "cs.HC", "math.DS", "47H10, 47H09, 46E30, 46B50, 46B10, 37C25", "G.1.0; I.2.6; H.5.3; H.5.2"], "comment": "18 pages", "summary": "We develop a rigorous measure-theoretic framework for the analysis of fixed\npoints of nonexpansive maps in the space $L^1(\\mu)$, with explicit\nconsideration of quantization errors arising in fixed-point arithmetic. Our\ncentral result shows that every bounded, closed, convex subset of $L^1(\\mu)$\nthat is compact in the topology of local convergence in measure (a property we\nrefer to as measure-compactness) enjoys the fixed point property for\nnonexpansive mappings. The proof relies on techniques from uniform\nintegrability, convexity in measure, and normal structure theory, including an\napplication of Kirk's theorem. We further analyze the effect of quantization by\nmodeling fixed-point arithmetic as a perturbation of a nonexpansive map,\nestablishing the existence of approximate fixed points under\nmeasure-compactness conditions. We also present counterexamples that illustrate\nthe optimality of our assumptions.\n  Beyond the theoretical development, we apply this framework to a\nhuman-in-the-loop co-editing system. By formulating the interaction between an\nAI-generated proposal, a human editor, and a quantizer as a composition of\nnonexpansive maps on a measure-compact set, we demonstrate the existence of a\n\"stable consensus artefact\". We prove that such a consensus state remains an\napproximate fixed point even under bounded quantization errors, and we provide\na concrete example of a human-AI editing loop that fits this framework. Our\nresults underscore the value of measure-theoretic compactness in the design and\nverification of reliable collaborative systems involving humans and artificial\nagents."}
{"id": "2509.11570", "pdf": "https://arxiv.org/pdf/2509.11570.pdf", "abs": "https://arxiv.org/abs/2509.11570", "title": "Bhaasha, Bhasa, Zaban: A Survey for Low-Resourced Languages in South Asia -- Current Stage and Challenges", "authors": ["Sampoorna Poria", "Xiaolei Huang"], "categories": ["cs.CL"], "comment": null, "summary": "Rapid developments of large language models have revolutionized many NLP\ntasks for English data. Unfortunately, the models and their evaluations for\nlow-resource languages are being overlooked, especially for languages in South\nAsia. Although there are more than 650 languages in South Asia, many of them\neither have very limited computational resources or are missing from existing\nlanguage models. Thus, a concrete question to be answered is: Can we assess the\ncurrent stage and challenges to inform our NLP community and facilitate model\ndevelopments for South Asian languages? In this survey, we have comprehensively\nexamined current efforts and challenges of NLP models for South Asian languages\nby retrieving studies since 2020, with a focus on transformer-based models,\nsuch as BERT, T5, & GPT. We present advances and gaps across 3 essential\naspects: data, models, & tasks, such as available data sources, fine-tuning\nstrategies, & domain applications. Our findings highlight substantial issues,\nincluding missing data in critical domains (e.g., health), code-mixing, and\nlack of standardized evaluation benchmarks. Our survey aims to raise awareness\nwithin the NLP community for more targeted data curation, unify benchmarks\ntailored to cultural and linguistic nuances of South Asia, and encourage an\nequitable representation of South Asian languages. The complete list of\nresources is available at: https://github.com/trust-nlp/LM4SouthAsia-Survey."}
{"id": "2509.11868", "pdf": "https://arxiv.org/pdf/2509.11868.pdf", "abs": "https://arxiv.org/abs/2509.11868", "title": "Growing Perspectives: Modelling Embodied Perspective Taking and Inner Narrative Development Using Large Language Models", "authors": ["Sabrina Patania", "Luca Annese", "Anna Lambiase", "Anita Pellegrini", "Tom Foulsham", "Azzurra Ruggeri", "Silvia Rossi", "Silvia Serino", "Dimitri Ognibene"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.RO", "I.2; I.2.7; I.2.10; J.4"], "comment": "Accepted at ICDL https://icdl2025.fel.cvut.cz/", "summary": "Language and embodied perspective taking are essential for human\ncollaboration, yet few computational models address both simultaneously. This\nwork investigates the PerspAct system [1], which integrates the ReAct (Reason\nand Act) paradigm with Large Language Models (LLMs) to simulate developmental\nstages of perspective taking, grounded in Selman's theory [2]. Using an\nextended director task, we evaluate GPT's ability to generate internal\nnarratives aligned with specified developmental stages, and assess how these\ninfluence collaborative performance both qualitatively (action selection) and\nquantitatively (task efficiency). Results show that GPT reliably produces\ndevelopmentally-consistent narratives before task execution but often shifts\ntowards more advanced stages during interaction, suggesting that language\nexchanges help refine internal representations. Higher developmental stages\ngenerally enhance collaborative effectiveness, while earlier stages yield more\nvariable outcomes in complex contexts. These findings highlight the potential\nof integrating embodied perspective taking and language in LLMs to better model\ndevelopmental dynamics and stress the importance of evaluating internal speech\nduring combined linguistic and embodied tasks."}
{"id": "2509.11591", "pdf": "https://arxiv.org/pdf/2509.11591.pdf", "abs": "https://arxiv.org/abs/2509.11591", "title": "Analyzing Information-Seeking Behaviors in a Hakka AI Chatbot: A Cognitive-Pragmatic Study", "authors": ["Chu-Hsuan Lee", "Chen-Chi Chang", "Hung-Shin Lee", "Yun-Hsiang Hsu", "Ching-Yuan Chen"], "categories": ["cs.CL"], "comment": "Accepted to HICSS-59 (2026)", "summary": "With many endangered languages at risk of disappearing, efforts to preserve\nthem now rely more than ever on using technology alongside culturally informed\nteaching strategies. This study examines user behaviors in TALKA, a generative\nAI-powered chatbot designed for Hakka language engagement, by employing a\ndual-layered analytical framework grounded in Bloom's Taxonomy of cognitive\nprocesses and dialogue act categorization. We analyzed 7,077 user utterances,\neach carefully annotated according to six cognitive levels and eleven dialogue\nact types. These included a variety of functions, such as asking for\ninformation, requesting translations, making cultural inquiries, and using\nlanguage creatively. Pragmatic classifications further highlight how different\ntypes of dialogue acts--such as feedback, control commands, and social\ngreetings--align with specific cognitive intentions. The results suggest that\ngenerative AI chatbots can support language learning in meaningful\nways--especially when they are designed with an understanding of how users\nthink and communicate. They may also help learners express themselves more\nconfidently and connect with their cultural identity. The TALKA case provides\nempirical insights into how AI-mediated dialogue facilitates cognitive\ndevelopment in low-resource language learners, as well as pragmatic negotiation\nand socio-cultural affiliation. By focusing on AI-assisted language learning,\nthis study offers new insights into how technology can support language\npreservation and educational practice."}
{"id": "2509.11921", "pdf": "https://arxiv.org/pdf/2509.11921.pdf", "abs": "https://arxiv.org/abs/2509.11921", "title": "Designing LLMs for cultural sensitivity: Evidence from English-Japanese translation", "authors": ["Helene Tenzer", "Oumnia Abidi", "Stefan Feuerriegel"], "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": null, "summary": "Large language models (LLMs) are increasingly used in everyday communication,\nincluding multilingual interactions across different cultural contexts. While\nLLMs can now generate near-perfect literal translations, it remains unclear\nwhether LLMs support culturally appropriate communication. In this paper, we\nanalyze the cultural sensitivity of different LLM designs when applied to\nEnglish-Japanese translations of workplace e-mails. Here, we vary the prompting\nstrategies: (1) naive \"just translate\" prompts, (2) audience-targeted prompts\nspecifying the recipient's cultural background, and (3) instructional prompts\nwith explicit guidance on Japanese communication norms. Using a mixed-methods\nstudy, we then analyze culture-specific language patterns to evaluate how well\ntranslations adapt to cultural norms. Further, we examine the appropriateness\nof the tone of the translations as perceived by native speakers. We find that\nculturally-tailored prompting can improve cultural fit, based on which we offer\nrecommendations for designing culturally inclusive LLMs in multilingual\nsettings."}
{"id": "2509.11604", "pdf": "https://arxiv.org/pdf/2509.11604.pdf", "abs": "https://arxiv.org/abs/2509.11604", "title": "Dynamic Span Interaction and Graph-Aware Memory for Entity-Level Sentiment Classification", "authors": ["Md. Mithun Hossain", "Sanjara", "Md. Shakil Hossain", "Sudipto Chaki"], "categories": ["cs.CL"], "comment": null, "summary": "Entity-level sentiment classification involves identifying the sentiment\npolarity linked to specific entities within text. This task poses several\nchallenges: effectively modeling the subtle and complex interactions between\nentities and their surrounding sentiment expressions; capturing dependencies\nthat may span across sentences; and ensuring consistent sentiment predictions\nfor multiple mentions of the same entity through coreference resolution.\nAdditionally, linguistic phenomena such as negation, ambiguity, and overlapping\nopinions further complicate the analysis. These complexities make entity-level\nsentiment classification a difficult problem, especially in real-world, noisy\ntextual data. To address these issues, we propose SpanEIT, a novel framework\nintegrating dynamic span interaction and graph-aware memory mechanisms for\nenhanced entity-sentiment relational modeling. SpanEIT builds span-based\nrepresentations for entities and candidate sentiment phrases, employs\nbidirectional attention for fine-grained interactions, and uses a graph\nattention network to capture syntactic and co-occurrence relations. A\ncoreference-aware memory module ensures entity-level consistency across\ndocuments. Experiments on FSAD, BARU, and IMDB datasets show SpanEIT\noutperforms state-of-the-art transformer and hybrid baselines in accuracy and\nF1 scores. Ablation and interpretability analyses validate the effectiveness of\nour approach, underscoring its potential for fine-grained sentiment analysis in\napplications like social media monitoring and customer feedback analysis."}
{"id": "2509.11942", "pdf": "https://arxiv.org/pdf/2509.11942.pdf", "abs": "https://arxiv.org/abs/2509.11942", "title": "VisDocSketcher: Towards Scalable Visual Documentation with Agentic Systems", "authors": ["Luís F. Gomes", "Xin Zhou", "David Lo", "Rui Abreu"], "categories": ["cs.SE", "cs.AI", "cs.HC"], "comment": null, "summary": "Visual documentation is an effective tool for reducing the cognitive barrier\ndevelopers face when understanding unfamiliar code, enabling more intuitive\ncomprehension. Compared to textual documentation, it provides a higher-level\nunderstanding of the system structure and data flow. Developers usually prefer\nvisual representations over lengthy textual descriptions for large software\nsystems. Visual documentation is both difficult to produce and challenging to\nevaluate. Manually creating it is time-consuming, and currently, no existing\napproach can automatically generate high-level visual documentation directly\nfrom code. Its evaluation is often subjective, making it difficult to\nstandardize and automate. To address these challenges, this paper presents the\nfirst exploration of using agentic LLM systems to automatically generate visual\ndocumentation. We introduce VisDocSketcher, the first agent-based approach that\ncombines static analysis with LLM agents to identify key elements in the code\nand produce corresponding visual representations. We propose a novel evaluation\nframework, AutoSketchEval, for assessing the quality of generated visual\ndocumentation using code-level metrics. The experimental results show that our\napproach can valid visual documentation for 74.4% of the samples. It shows an\nimprovement of 26.7-39.8% over a simple template-based baseline. Our evaluation\nframework can reliably distinguish high-quality (code-aligned) visual\ndocumentation from low-quality (non-aligned) ones, achieving an AUC exceeding\n0.87. Our work lays the foundation for future research on automated visual\ndocumentation by introducing practical tools that not only generate valid\nvisual representations but also reliably assess their quality."}
{"id": "2509.11619", "pdf": "https://arxiv.org/pdf/2509.11619.pdf", "abs": "https://arxiv.org/abs/2509.11619", "title": "HalluDetect: Detecting, Mitigating, and Benchmarking Hallucinations in Conversational Systems", "authors": ["Spandan Anaokar", "Shrey Ganatra", "Harshvivek Kashid", "Swapnil Bhattacharyya", "Shruti Nair", "Reshma Sekhar", "Siddharth Manohar", "Rahul Hemrajani", "Pushpak Bhattacharyya"], "categories": ["cs.CL"], "comment": "6 pages + references + appendix, 3 figures, 2 tables", "summary": "Large Language Models (LLMs) are widely used in industry but remain prone to\nhallucinations, limiting their reliability in critical applications. This work\naddresses hallucination reduction in consumer grievance chatbots built using\nLLaMA 3.1 8B Instruct, a compact model frequently used in industry. We develop\nHalluDetect, an LLM-based hallucination detection system that achieves an F1\nscore of 69% outperforming baseline detectors by 25.44%. Benchmarking five\nchatbot architectures, we find that out of them, AgentBot minimizes\nhallucinations to 0.4159 per turn while maintaining the highest token accuracy\n(96.13%), making it the most effective mitigation strategy. Our findings\nprovide a scalable framework for hallucination mitigation, demonstrating that\noptimized inference strategies can significantly improve factual accuracy.\nWhile applied to consumer law, our approach generalizes to other high-risk\ndomains, enhancing trust in LLM-driven assistants. We will release the code and\ndataset"}
{"id": "2311.05920", "pdf": "https://arxiv.org/pdf/2311.05920.pdf", "abs": "https://arxiv.org/abs/2311.05920", "title": "Impact Ambivalence: How People with Eating Disorders Get Trapped in the Perpetual Cycle of Digital Food Content Engagement", "authors": ["Ryuhaerang Choi", "Subin Park", "Sujin Han", "Jennifer G. Kim", "Sung-Ju Lee"], "categories": ["cs.HC", "cs.CY", "cs.MM"], "comment": "15 pages, 3 figures", "summary": "Digital food content could impact viewers' dietary health, with individuals\nwith eating disorders being particularly sensitive to it. However, a\ncomprehensive understanding of why and how these individuals interact with such\ncontent is lacking. To fill this void, we conducted exploratory (N=23) and\nin-depth studies (N=22) with individuals with eating disorders to understand\ntheir motivations and practices of consuming digital food content. We reveal\nthat participants engaged with digital food content for both disorder-driven\nand recovery-supporting motivations, leading to conflicting outcomes. This\nimpact ambivalence, the coexistence of recovery-supporting benefits and\ndisorder-exacerbating risks, sustained a cycle of quitting, prompted by\nawareness of harm, and returning, motivated by anticipated benefits. We\ninterpret these dynamics within dual systems theory and highlight how\nrecognizing such ambivalence can inform the design of interventions that foster\nhealthier digital food content engagement and mitigate post-engagement harmful\neffects."}
{"id": "2509.11620", "pdf": "https://arxiv.org/pdf/2509.11620.pdf", "abs": "https://arxiv.org/abs/2509.11620", "title": "AesBiasBench: Evaluating Bias and Alignment in Multimodal Language Models for Personalized Image Aesthetic Assessment", "authors": ["Kun Li", "Lai-Man Po", "Hongzheng Yang", "Xuyuan Xu", "Kangcheng Liu", "Yuzhi Zhao"], "categories": ["cs.CL", "cs.CY"], "comment": "Accepted by EMNLP 2025", "summary": "Multimodal Large Language Models (MLLMs) are increasingly applied in\nPersonalized Image Aesthetic Assessment (PIAA) as a scalable alternative to\nexpert evaluations. However, their predictions may reflect subtle biases\ninfluenced by demographic factors such as gender, age, and education. In this\nwork, we propose AesBiasBench, a benchmark designed to evaluate MLLMs along two\ncomplementary dimensions: (1) stereotype bias, quantified by measuring\nvariations in aesthetic evaluations across demographic groups; and (2)\nalignment between model outputs and genuine human aesthetic preferences. Our\nbenchmark covers three subtasks (Aesthetic Perception, Assessment, Empathy) and\nintroduces structured metrics (IFD, NRD, AAS) to assess both bias and\nalignment. We evaluate 19 MLLMs, including proprietary models (e.g., GPT-4o,\nClaude-3.5-Sonnet) and open-source models (e.g., InternVL-2.5, Qwen2.5-VL).\nResults indicate that smaller models exhibit stronger stereotype biases,\nwhereas larger models align more closely with human preferences. Incorporating\nidentity information often exacerbates bias, particularly in emotional\njudgments. These findings underscore the importance of identity-aware\nevaluation frameworks in subjective vision-language tasks."}
{"id": "2501.10551", "pdf": "https://arxiv.org/pdf/2501.10551.pdf", "abs": "https://arxiv.org/abs/2501.10551", "title": "An Empirical Study to Understand How Students Use ChatGPT for Writing Essays", "authors": ["Andrew Jelson", "Daniel Manesh", "Alice Jang", "Daniel Dunlap", "Young-Ho Kim", "Sang Won Lee"], "categories": ["cs.HC"], "comment": "35 pages, 16 figures, 6 tables, Submitted to ACM CHI 2026", "summary": "As large language models (LLMs) advance and become widespread, students\nincreasingly turn to systems like ChatGPT for assistance with writing tasks.\nEducators are concerned with students' usage of ChatGPT beyond cheating; using\nChatGPT may reduce their critical engagement with writing, hindering students'\nlearning processes. The negative or positive impact of using LLM-powered tools\nfor writing will depend on how students use them; however, how students use\nChatGPT remains largely unknown, resulting in a limited understanding of its\nimpact on learning. To better understand how students use these tools, we\nconducted an online study $(n=70)$ where students were given an essay-writing\ntask using a custom platform we developed to capture the queries they made to\nChatGPT. To characterize their ChatGPT usage, we categorized each of the\nqueries students made to ChatGPT. We then analyzed the relationship between\nChatGPT usage and a variety of other metrics, including students'\nself-perception, attitudes towards AI, and the resulting essay itself. We found\nthat factors such as gender, race, and perceived self-efficacy can help predict\ndifferent AI usage patterns. Additionally, we found that different usage\npatterns were associated with varying levels of enjoyment and perceived\nownership over the essay. The results of this study contribute to discussions\nabout how writing education should incorporate generative AI-powered tools in\nthe classroom."}
{"id": "2509.11648", "pdf": "https://arxiv.org/pdf/2509.11648.pdf", "abs": "https://arxiv.org/abs/2509.11648", "title": "EthicsMH: A Pilot Benchmark for Ethical Reasoning in Mental Health AI", "authors": ["Sai Kartheek Reddy Kasu"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "The deployment of large language models (LLMs) in mental health and other\nsensitive domains raises urgent questions about ethical reasoning, fairness,\nand responsible alignment. Yet, existing benchmarks for moral and clinical\ndecision-making do not adequately capture the unique ethical dilemmas\nencountered in mental health practice, where confidentiality, autonomy,\nbeneficence, and bias frequently intersect. To address this gap, we introduce\nEthical Reasoning in Mental Health (EthicsMH), a pilot dataset of 125 scenarios\ndesigned to evaluate how AI systems navigate ethically charged situations in\ntherapeutic and psychiatric contexts. Each scenario is enriched with structured\nfields, including multiple decision options, expert-aligned reasoning, expected\nmodel behavior, real-world impact, and multi-stakeholder viewpoints. This\nstructure enables evaluation not only of decision accuracy but also of\nexplanation quality and alignment with professional norms. Although modest in\nscale and developed with model-assisted generation, EthicsMH establishes a task\nframework that bridges AI ethics and mental health decision-making. By\nreleasing this dataset, we aim to provide a seed resource that can be expanded\nthrough community and expert contributions, fostering the development of AI\nsystems capable of responsibly handling some of society's most delicate\ndecisions."}
{"id": "2502.03330", "pdf": "https://arxiv.org/pdf/2502.03330.pdf", "abs": "https://arxiv.org/abs/2502.03330", "title": "Controllable GUI Exploration", "authors": ["Aryan Garg", "Yue Jiang", "Antti Oulasvirta"], "categories": ["cs.HC", "cs.AI", "cs.CV", "cs.GR"], "comment": null, "summary": "During the early stages of interface design, designers need to produce\nmultiple sketches to explore a design space. Design tools often fail to support\nthis critical stage, because they insist on specifying more details than\nnecessary. Although recent advances in generative AI have raised hopes of\nsolving this issue, in practice they fail because expressing loose ideas in a\nprompt is impractical. In this paper, we propose a diffusion-based approach to\nthe low-effort generation of interface sketches. It breaks new ground by\nallowing flexible control of the generation process via three types of inputs:\nA) prompts, B) wireframes, and C) visual flows. The designer can provide any\ncombination of these as input at any level of detail, and will get a diverse\ngallery of low-fidelity solutions in response. The unique benefit is that large\ndesign spaces can be explored rapidly with very little effort in\ninput-specification. We present qualitative results for various combinations of\ninput specifications. Additionally, we demonstrate that our model aligns more\naccurately with these specifications than other models."}
{"id": "2509.11687", "pdf": "https://arxiv.org/pdf/2509.11687.pdf", "abs": "https://arxiv.org/abs/2509.11687", "title": "A Dynamic Knowledge Update-Driven Model with Large Language Models for Fake News Detection", "authors": ["Di Jin", "Jun Yang", "Xiaobao Wang", "Junwei Zhang", "Shuqi Li", "Dongxiao He"], "categories": ["cs.CL"], "comment": null, "summary": "As the Internet and social media evolve rapidly, distinguishing credible news\nfrom a vast amount of complex information poses a significant challenge. Due to\nthe suddenness and instability of news events, the authenticity labels of news\ncan potentially shift as events develop, making it crucial for fake news\ndetection to obtain the latest event updates. Existing methods employ\nretrieval-augmented generation to fill knowledge gaps, but they suffer from\nissues such as insufficient credibility of retrieved content and interference\nfrom noisy information. We propose a dynamic knowledge update-driven model for\nfake news detection (DYNAMO), which leverages knowledge graphs to achieve\ncontinuous updating of new knowledge and integrates with large language models\nto fulfill dual functions: news authenticity detection and verification of new\nknowledge correctness, solving the two key problems of ensuring the\nauthenticity of new knowledge and deeply mining news semantics. Specifically,\nwe first construct a news-domain-specific knowledge graph. Then, we use Monte\nCarlo Tree Search to decompose complex news and verify them step by step.\nFinally, we extract and update new knowledge from verified real news texts and\nreasoning paths. Experimental results demonstrate that DYNAMO achieves the best\nperformance on two real-world datasets."}
{"id": "2505.09094", "pdf": "https://arxiv.org/pdf/2505.09094.pdf", "abs": "https://arxiv.org/abs/2505.09094", "title": "PLanet: Formalizing Assignment Procedures in the Design of Experiments", "authors": ["London Bielicke", "Anna Zhang", "Shruti Tyagi", "Emery Berger", "Adam Chlipala", "Eunice Jun"], "categories": ["cs.HC"], "comment": "16 pages", "summary": "Carefully constructed experimental designs are essential for drawing valid,\ngeneralizable conclusions from scientific experiments. Unfortunately,\nexperimental designs can be difficult to specify, communicate clearly, and\nrelate to alternatives. In response, we introduce a grammar of composable\noperators for constructing experimental assignment procedures (e.g., Latin\nsquare). The PLanet DSL implements this grammar. Researchers specify assignment\nrequirements. PLanet compiles these into a constraint satisfaction problem over\nmatrices that determines viable experimental plans. In an expressivity\nevaluation, we find that PLanet is the most expressive compared to two existing\nexperimental design libraries. Its composability enables expression of both\ncanonical and customized designs in HCI experiments. Case studies with three\nresearchers reveal how PLanet helps them make complex design choices explicit,\nexplore alternatives, and develop a deeper understanding of experimental\ndesign."}
{"id": "2509.11698", "pdf": "https://arxiv.org/pdf/2509.11698.pdf", "abs": "https://arxiv.org/abs/2509.11698", "title": "CoachMe: Decoding Sport Elements with a Reference-Based Coaching Instruction Generation Model", "authors": ["Wei-Hsin Yeh", "Yu-An Su", "Chih-Ning Chen", "Yi-Hsueh Lin", "Calvin Ku", "Wen-Hsin Chiu", "Min-Chun Hu", "Lun-Wei Ku"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "I.2.7; I.2.10"], "comment": "Published in Proceedings of the 63rd Annual Meeting of the\n  Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025.\n  Official version: https://doi.org/10.18653/v1/2025.acl-long.1413", "summary": "Motion instruction is a crucial task that helps athletes refine their\ntechnique by analyzing movements and providing corrective guidance. Although\nrecent advances in multimodal models have improved motion understanding,\ngenerating precise and sport-specific instruction remains challenging due to\nthe highly domain-specific nature of sports and the need for informative\nguidance. We propose CoachMe, a reference-based model that analyzes the\ndifferences between a learner's motion and a reference under temporal and\nphysical aspects. This approach enables both domain-knowledge learning and the\nacquisition of a coach-like thinking process that identifies movement errors\neffectively and provides feedback to explain how to improve. In this paper, we\nillustrate how CoachMe adapts well to specific sports such as skating and\nboxing by learning from general movements and then leveraging limited data.\nExperiments show that CoachMe provides high-quality instructions instead of\ndirections merely in the tone of a coach but without critical information.\nCoachMe outperforms GPT-4o by 31.6% in G-Eval on figure skating and by 58.3% on\nboxing. Analysis further confirms that it elaborates on errors and their\ncorresponding improvement methods in the generated instructions. You can find\nCoachMe here: https://motionxperts.github.io/"}
{"id": "2505.09166", "pdf": "https://arxiv.org/pdf/2505.09166.pdf", "abs": "https://arxiv.org/abs/2505.09166", "title": "An Exploration of Default Images in Text-to-Image Generation", "authors": ["Hannu Simonen", "Atte Kiviniemi", "Hannah Johnston", "Helena Barranha", "Jonas Oppenlaender"], "categories": ["cs.HC", "cs.AI", "H.5.m; I.2.m"], "comment": "30 pages, 10 figures", "summary": "In the creative practice of text-to-image (TTI) generation, images are\nsynthesized from textual prompts. By design, TTI models always yield an output,\neven if the prompt contains unknown terms. In this case, the model may generate\ndefault images: images that closely resemble each other across many unrelated\nprompts. Studying default images is valuable for designing better solutions for\nprompt engineering and TTI generation. We present the first investigation into\ndefault images on Midjourney. We describe an initial study in which we manually\ncreated input prompts triggering default images, and several ablation studies.\nBuilding on these, we conduct a computational analysis of about 750,000 images,\nrevealing consistent default images across unrelated prompts. We also conduct\nan online user study investigating how default images may affect user\nsatisfaction. Our work lays the foundation for understanding default images in\nTTI generation, highlighting their practical relevance as well as challenges\nand future research directions."}
{"id": "2509.11709", "pdf": "https://arxiv.org/pdf/2509.11709.pdf", "abs": "https://arxiv.org/abs/2509.11709", "title": "Room acoustics affect communicative success in hybrid meeting spaces: a pilot study", "authors": ["Robert Einig", "Stefan Janscha", "Jonas Schuster", "Julian Koch", "Martin Hagmueller", "Barbara Schuppler"], "categories": ["cs.CL", "eess.AS"], "comment": null, "summary": "Since the COVID-19 pandemic in 2020, universities and companies have\nincreasingly integrated hybrid features into their meeting spaces, or even\ncreated dedicated rooms for this purpose. While the importance of a fast and\nstable internet connection is often prioritized, the acoustic design of seminar\nrooms is frequently overlooked. Poor acoustics, particularly excessive\nreverberation, can lead to issues such as misunderstandings, reduced speech\nintelligibility or cognitive and vocal fatigue. This pilot study investigates\nwhether room acoustic interventions in a seminar room at Graz University of\nTechnology support better communication in hybrid meetings. For this purpose,\nwe recorded two groups of persons twice, once before and once after improving\nthe acoustics of the room. Our findings -- despite not reaching statistical\nsignificance due to the small sample size - indicate clearly that our spatial\ninterventions improve communicative success in hybrid meetings. To make the\npaper accessible also for readers from the speech communication community, we\nexplain room acoustics background, relevant for the interpretation of our\nresults."}
{"id": "2507.16207", "pdf": "https://arxiv.org/pdf/2507.16207.pdf", "abs": "https://arxiv.org/abs/2507.16207", "title": "A Human-Centered Approach to Identifying Promises, Risks, & Challenges of Text-to-Image Generative AI in Radiology", "authors": ["Katelyn Morrison", "Arpit Mathur", "Aidan Bradshaw", "Tom Wartmann", "Steven Lundi", "Afrooz Zandifar", "Weichang Dai", "Kayhan Batmanghelich", "Motahhare Eslami", "Adam Perer"], "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": "10 pages of main content, Appendix attached after references,\n  accepted to AAAI/ACM AIES 2025", "summary": "As text-to-image generative models rapidly improve, AI researchers are making\nsignificant advances in developing domain-specific models capable of generating\ncomplex medical imagery from text prompts. Despite this, these technical\nadvancements have overlooked whether and how medical professionals would\nbenefit from and use text-to-image generative AI (GenAI) in practice. By\ndeveloping domain-specific GenAI without involving stakeholders, we risk the\npotential of building models that are either not useful or even more harmful\nthan helpful. In this paper, we adopt a human-centered approach to responsible\nmodel development by involving stakeholders in evaluating and reflecting on the\npromises, risks, and challenges of a novel text-to-CT Scan GenAI model. Through\nexploratory model prompting activities, we uncover the perspectives of medical\nstudents, radiology trainees, and radiologists on the role that text-to-CT Scan\nGenAI can play across medical education, training, and practice. This\nhuman-centered approach additionally enabled us to surface technical challenges\nand domain-specific risks of generating synthetic medical images. We conclude\nby reflecting on the implications of medical text-to-image GenAI."}
{"id": "2509.11773", "pdf": "https://arxiv.org/pdf/2509.11773.pdf", "abs": "https://arxiv.org/abs/2509.11773", "title": "An Agentic Toolkit for Adaptive Information Extraction from Regulatory Documents", "authors": ["Gaye Colakoglu", "Gürkan Solmaz", "Jonathan Fürst"], "categories": ["cs.CL"], "comment": null, "summary": "Declaration of Performance (DoP) documents, mandated by EU regulation,\ncertify the performance of construction products. While some of their content\nis standardized, DoPs vary widely in layout, language, schema, and format,\nposing challenges for automated key-value pair extraction (KVP) and question\nanswering (QA). Existing static or LLM-only IE pipelines often hallucinate and\nfail to adapt to this structural diversity. Our domain-specific, stateful\nagentic system addresses these challenges through a planner-executor-responder\narchitecture. The system infers user intent, detects document modality, and\norchestrates tools dynamically for robust, traceable reasoning while avoiding\ntool misuse or execution loops. Evaluation on a curated DoP dataset\ndemonstrates improved robustness across formats and languages, offering a\nscalable solution for structured data extraction in regulated workflows."}
{"id": "2507.22134", "pdf": "https://arxiv.org/pdf/2507.22134.pdf", "abs": "https://arxiv.org/abs/2507.22134", "title": "IntentFlow: Interactive Support for Communicating Intent with LLMs in Writing Tasks", "authors": ["Yoonsu Kim", "Brandon Chin", "Kihoon Son", "Seoyoung Kim", "Juho Kim"], "categories": ["cs.HC"], "comment": null, "summary": "Effective collaboration with generative AI systems requires users to clearly\ncommunicate their intents (intent-based outcome specification). Yet such\nintents are often underspecified and evolve during interaction, dynamic support\nfor intent communication is essential. Through a systematic literature review\nof 33 papers, we synthesize a structured understanding of intent communication,\nidentifying four key aspects: articulation, exploration, management, and\nsynchronization. Building on these findings, we derived design implications\nthat translate them into actionable design and implemented IntentFlow, a system\nfor LLM-based writing that realizes these implications through adjustable UIs,\nintent-to-output linking, and versioned refinement. A technical evaluation\n(N=60) and a within-subjects study (N=12) confirm that IntentFlow helps users\ndiscover, elaborate, and consolidate their intents into a curated set.\nInteraction logs further reveal a shift from reactive error correction to\nproactive intent refinement. Our work demonstrates how a system effectively\ndesigned to support these four communication aspects can substantially enhance\nhuman-LLM interaction."}
{"id": "2509.11777", "pdf": "https://arxiv.org/pdf/2509.11777.pdf", "abs": "https://arxiv.org/abs/2509.11777", "title": "User eXperience Perception Insights Dataset (UXPID): Synthetic User Feedback from Public Industrial Forums", "authors": ["Mikhail Kulyabin", "Jan Joosten", "Choro Ulan uulu", "Nuno Miguel Martins Pacheco", "Fabian Ries", "Filippos Petridis", "Jan Bosch", "Helena Holmström Olsson"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Customer feedback in industrial forums reflect a rich but underexplored\nsource of insight into real-world product experience. These publicly shared\ndiscussions offer an organic view of user expectations, frustrations, and\nsuccess stories shaped by the specific contexts of use. Yet, harnessing this\ninformation for systematic analysis remains challenging due to the unstructured\nand domain-specific nature of the content. The lack of structure and\nspecialized vocabulary makes it difficult for traditional data analysis\ntechniques to accurately interpret, categorize, and quantify the feedback,\nthereby limiting its potential to inform product development and support\nstrategies. To address these challenges, this paper presents the User\neXperience Perception Insights Dataset (UXPID), a collection of 7130\nartificially synthesized and anonymized user feedback branches extracted from a\npublic industrial automation forum. Each JavaScript object notation (JSON)\nrecord contains multi-post comments related to specific hardware and software\nproducts, enriched with metadata and contextual conversation data. Leveraging a\nlarge language model (LLM), each branch is systematically analyzed and\nannotated for UX insights, user expectations, severity and sentiment ratings,\nand topic classifications. The UXPID dataset is designed to facilitate research\nin user requirements, user experience (UX) analysis, and AI-driven feedback\nprocessing, particularly where privacy and licensing restrictions limit access\nto real-world data. UXPID supports the training and evaluation of\ntransformer-based models for tasks such as issue detection, sentiment analysis,\nand requirements extraction in the context of technical forums."}
{"id": "2507.22163", "pdf": "https://arxiv.org/pdf/2507.22163.pdf", "abs": "https://arxiv.org/abs/2507.22163", "title": "IdeaBlocks: Expressing and Reusing Exploratory Intents for Design Exploration with Generative AI", "authors": ["DaEun Choi", "Kihoon Son", "Jaesang Yu", "Hyunjoon Jung", "Juho Kim"], "categories": ["cs.HC"], "comment": null, "summary": "Generative AI opens new possibilities for design exploration by rapidly\ngenerating images aligned with user goals. However, our formative study (N=7)\nrevealed two key challenges that limit broad and efficient exploration with\nthese models: the lack of expressive channels for articulating exploratory\ndirections and ranges, and insufficient support for reusing past intents. We\npresent IdeaBlocks, where users can modularize exploratory intents into\nExploration Blocks, capturing property, direction, and range of exploration.\nUsers can reuse prior intents at multiple levels (block, path, and project)\nwith options for literal or context-adaptive reuse. In our comparative study\n(N=12), participants using IdeaBlocks explored 2.13 times more images with\n12.5% greater visual diversity than the baseline, demonstrating how structured\nintent expression and reuse support more effective exploration. A three-day\ndeployment study (N=6) further revealed how different reuse units and\nmechanisms enabled distinct creative strategies, offering design implications\nfor future intent-aware creativity support systems."}
{"id": "2509.11802", "pdf": "https://arxiv.org/pdf/2509.11802.pdf", "abs": "https://arxiv.org/abs/2509.11802", "title": "When Curiosity Signals Danger: Predicting Health Crises Through Online Medication Inquiries", "authors": ["Dvora Goncharok", "Arbel Shifman", "Alexander Apartsin", "Yehudit Aperstein"], "categories": ["cs.CL"], "comment": "5 pages, 2 figures", "summary": "Online medical forums are a rich and underutilized source of insight into\npatient concerns, especially regarding medication use. Some of the many\nquestions users pose may signal confusion, misuse, or even the early warning\nsigns of a developing health crisis. Detecting these critical questions that\nmay precede severe adverse events or life-threatening complications is vital\nfor timely intervention and improving patient safety. This study introduces a\nnovel annotated dataset of medication-related questions extracted from online\nforums. Each entry is manually labelled for criticality based on clinical risk\nfactors. We benchmark the performance of six traditional machine learning\nclassifiers using TF-IDF textual representations, alongside three\nstate-of-the-art large language model (LLM)-based classification approaches\nthat leverage deep contextual understanding. Our results highlight the\npotential of classical and modern methods to support real-time triage and alert\nsystems in digital health spaces. The curated dataset is made publicly\navailable to encourage further research at the intersection of\npatient-generated data, natural language processing, and early warning systems\nfor critical health events. The dataset and benchmark are available at:\nhttps://github.com/Dvora-coder/LLM-Medication-QA-Risk-Classifier-MediGuard."}
{"id": "2508.00103", "pdf": "https://arxiv.org/pdf/2508.00103.pdf", "abs": "https://arxiv.org/abs/2508.00103", "title": "A Mixed User-Centered Approach to Enable Augmented Intelligence in Intelligent Tutoring Systems: The Case of MathAIde app", "authors": ["Guilherme Guerino", "Luiz Rodrigues", "Luana Bianchini", "Mariana Alves", "Marcelo Marinho", "Thomaz Veloso", "Valmir Macario", "Diego Dermeval", "Thales Vieira", "Ig Bittencourt", "Seiji Isotani"], "categories": ["cs.HC", "cs.AI", "68T01", "H.5.0; I.2.0"], "comment": "Article published in the International Journal of Human-Computer\n  Interaction", "summary": "This study explores the integration of Augmented Intelligence (AuI) in\nIntelligent Tutoring Systems (ITS) to address challenges in Artificial\nIntelligence in Education (AIED), including teacher involvement, AI\nreliability, and resource accessibility. We present MathAIde, an ITS that uses\ncomputer vision and AI to correct mathematics exercises from student work\nphotos and provide feedback. The system was designed through a collaborative\nprocess involving brainstorming with teachers, high-fidelity prototyping, A/B\ntesting, and a real-world case study. Findings emphasize the importance of a\nteacher-centered, user-driven approach, where AI suggests remediation\nalternatives while teachers retain decision-making. Results highlight\nefficiency, usability, and adoption potential in classroom contexts,\nparticularly in resource-limited environments. The study contributes practical\ninsights into designing ITSs that balanceuser needs and technological\nfeasibility, while advancing AIED research by demonstrating the effectiveness\nof a mixed-methods, user-centered approach to implementing AuI in educational\ntechnologies."}
{"id": "2509.11803", "pdf": "https://arxiv.org/pdf/2509.11803.pdf", "abs": "https://arxiv.org/abs/2509.11803", "title": "From Fuzzy Speech to Medical Insight: Benchmarking LLMs on Noisy Patient Narratives", "authors": ["Eden Mama", "Liel Sheri", "Yehudit Aperstein", "Alexander Apartsin"], "categories": ["cs.CL"], "comment": "6 pages, 1 figure", "summary": "The widespread adoption of large language models (LLMs) in healthcare raises\ncritical questions about their ability to interpret patient-generated\nnarratives, which are often informal, ambiguous, and noisy. Existing benchmarks\ntypically rely on clean, structured clinical text, offering limited insight\ninto model performance under realistic conditions. In this work, we present a\nnovel synthetic dataset designed to simulate patient self-descriptions\ncharacterized by varying levels of linguistic noise, fuzzy language, and\nlayperson terminology. Our dataset comprises clinically consistent scenarios\nannotated with ground-truth diagnoses, spanning a spectrum of communication\nclarity to reflect diverse real-world reporting styles. Using this benchmark,\nwe fine-tune and evaluate several state-of-the-art models (LLMs), including\nBERT-based and encoder-decoder T5 models. To support reproducibility and future\nresearch, we release the Noisy Diagnostic Benchmark (NDB), a structured dataset\nof noisy, synthetic patient descriptions designed to stress-test and compare\nthe diagnostic capabilities of large language models (LLMs) under realistic\nlinguistic conditions. We made the benchmark available for the community:\nhttps://github.com/lielsheri/PatientSignal"}
{"id": "2508.19256", "pdf": "https://arxiv.org/pdf/2508.19256.pdf", "abs": "https://arxiv.org/abs/2508.19256", "title": "WeDesign: Generative AI-Facilitated Community Consultations for Urban Public Space Design", "authors": ["Rashid Mushkani", "Hugo Berard", "Shin Koseki"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Community consultations are integral to urban planning processes intended to\nincorporate diverse stakeholder perspectives. However, limited resources,\nvisual and spoken language barriers, and uneven power dynamics frequently\nconstrain inclusive decision-making. This paper examines how generative\ntext-to-image methods, specifically Stable Diffusion XL integrated into a\ncustom platform (WeDesign), may support equitable consultations. A half-day\nworkshop in Montreal involved five focus groups, each consisting of architects,\nurban designers, AI specialists, and residents from varied demographic groups.\nAdditional data was gathered through semi-structured interviews with six urban\nplanning professionals. Participants indicated that immediate visual outputs\nfacilitated creativity and dialogue, yet noted issues in visualizing specific\nneeds of marginalized groups, such as participants with reduced mobility,\naccurately depicting local architectural elements, and accommodating bilingual\nprompts. Participants recommended the development of an open-source platform\nincorporating in-painting tools, multilingual support, image voting\nfunctionalities, and preference indicators. The results indicate that\ngenerative AI can broaden participation and enable iterative interactions but\nrequires structured facilitation approaches. The findings contribute to\ndiscussions on generative AI's role and limitations in participatory urban\ndesign."}
{"id": "2509.11804", "pdf": "https://arxiv.org/pdf/2509.11804.pdf", "abs": "https://arxiv.org/abs/2509.11804", "title": "PledgeTracker: A System for Monitoring the Fulfilment of Pledges", "authors": ["Yulong Chen", "Michael Sejr Schlichtkrull", "Zhenyun Deng", "David Corney", "Nasim Asl", "Joshua Salisbury", "Andrew Dudfield", "Andreas Vlachos"], "categories": ["cs.CL"], "comment": "EMNLP 2025 demo", "summary": "Political pledges reflect candidates' policy commitments, but tracking their\nfulfilment requires reasoning over incremental evidence distributed across\nmultiple, dynamically updated sources. Existing methods simplify this task into\na document classification task, overlooking its dynamic, temporal and\nmulti-document nature. To address this issue, we introduce\n\\textsc{PledgeTracker}, a system that reformulates pledge verification into\nstructured event timeline construction. PledgeTracker consists of three core\ncomponents: (1) a multi-step evidence retrieval module; (2) a timeline\nconstruction module and; (3) a fulfilment filtering module, allowing the\ncapture of the evolving nature of pledge fulfilment and producing interpretable\nand structured timelines. We evaluate PledgeTracker in collaboration with\nprofessional fact-checkers in real-world workflows, demonstrating its\neffectiveness in retrieving relevant evidence and reducing human verification\neffort."}
{"id": "2509.05961", "pdf": "https://arxiv.org/pdf/2509.05961.pdf", "abs": "https://arxiv.org/abs/2509.05961", "title": "A Longitudinal Evaluation of Heart Rate Efficiency for Amateur Runners", "authors": ["Evgeny V. Votyakov", "Marios Constantinides", "Fotis Liarokapis"], "categories": ["cs.HC"], "comment": "8 pages, 5 figures, 3 tables", "summary": "Amateur runners are increasingly using wearable devices to track their\ntraining, and often do so through simple metrics such as heart rate and pace.\nHowever, these metrics are typically analyzed in isolation and lack the\nexplainability needed for long-term self-monitoring. In this paper, we first\npresent Fitplotter, which is a client-side web application designed for the\nvisualization and analysis of data associated with fitness and activity\ntracking devices. Next, we revisited and formalized Heart Rate Efficiency\n(HRE), defined as the product of pace and heart rate, as a practical and\nexplainable metric to track aerobic fitness in everyday running. Drawing on\nmore than a decade of training data from one athlete, and supplemented by\npublicly available logs from twelve runners, we showed that HRE provides more\nstable and meaningful feedback on aerobic development than heart rate or pace\nalone. We showed that HRE correlates with training volume, reflects seasonal\nprogress, and remains stable during long runs in well-trained individuals. We\nalso discuss how HRE can support everyday training decisions, improve the user\nexperience in fitness tracking, and serve as an explainable metric to\nproprietary ones of commercial platforms. Our findings have implications for\ndesigning user-centered fitness tools that empower amateur athletes to\nunderstand and manage their own performance data."}
{"id": "2509.11818", "pdf": "https://arxiv.org/pdf/2509.11818.pdf", "abs": "https://arxiv.org/abs/2509.11818", "title": "SCDTour: Embedding Axis Ordering and Merging for Interpretable Semantic Change Detection", "authors": ["Taichi Aida", "Danushka Bollegala"], "categories": ["cs.CL"], "comment": "Findings of EMNLP2025", "summary": "In Semantic Change Detection (SCD), it is a common problem to obtain\nembeddings that are both interpretable and high-performing. However, improving\ninterpretability often leads to a loss in the SCD performance, and vice versa.\nTo address this problem, we propose SCDTour, a method that orders and merges\ninterpretable axes to alleviate the performance degradation of SCD. SCDTour\nconsiders both (a) semantic similarity between axes in the embedding space, as\nwell as (b) the degree to which each axis contributes to semantic change.\nExperimental results show that SCDTour preserves performance in semantic change\ndetection while maintaining high interpretability. Moreover, agglomerating the\nsorted axes produces a more refined set of word senses, which achieves\ncomparable or improved performance against the original full-dimensional\nembeddings in the SCD task. These findings demonstrate that SCDTour effectively\nbalances interpretability and SCD performance, enabling meaningful\ninterpretation of semantic shifts through a small number of refined axes.\nSource code is available at https://github.com/LivNLP/svp-tour ."}
{"id": "2509.10015", "pdf": "https://arxiv.org/pdf/2509.10015.pdf", "abs": "https://arxiv.org/abs/2509.10015", "title": "A Framework for AI-Supported Mediation in Community-based Online Collaboration", "authors": ["Soobin Cho", "Mark Zachry", "David W. McDonald"], "categories": ["cs.HC"], "comment": null, "summary": "Online spaces involve diverse communities engaging in various forms of\ncollaboration, which naturally give rise to discussions, some of which\ninevitably escalate into conflict or disputes. To address such situations, AI\nhas primarily been used for moderation. While moderation systems are important\nbecause they help maintain order, common moderation strategies of removing or\nsuppressing content and users rarely address the underlying disagreements or\nthe substantive content of disputes. Mediation, by contrast, fosters\nunderstanding, reduces emotional tension, and facilitates consensus through\nguided negotiation. Mediation not only enhances the quality of collaborative\ndecisions but also strengthens relationships among group members. For this\nreason, we argue for shifting focus toward AI-supported mediation. In this\nwork, we propose an information-focused framework for AI-supported mediation\ndesigned for community-based collaboration. Within this framework, we\nhypothesize that AI must acquire and reason over three key types of\ninformation: content, culture, and people."}
{"id": "2509.11860", "pdf": "https://arxiv.org/pdf/2509.11860.pdf", "abs": "https://arxiv.org/abs/2509.11860", "title": "MOOM: Maintenance, Organization and Optimization of Memory in Ultra-Long Role-Playing Dialogues", "authors": ["Weishu Chen", "Jinyi Tang", "Zhouhui Hou", "Shihao Han", "Mingjie Zhan", "Zhiyuan Huang", "Delong Liu", "Jiawei Guo", "Zhicheng Zhao", "Fei Su"], "categories": ["cs.CL"], "comment": null, "summary": "Memory extraction is crucial for maintaining coherent ultra-long dialogues in\nhuman-robot role-playing scenarios. However, existing methods often exhibit\nuncontrolled memory growth. To address this, we propose MOOM, the first\ndual-branch memory plugin that leverages literary theory by modeling plot\ndevelopment and character portrayal as core storytelling elements.\nSpecifically, one branch summarizes plot conflicts across multiple time scales,\nwhile the other extracts the user's character profile. MOOM further integrates\na forgetting mechanism, inspired by the ``competition-inhibition'' memory\ntheory, to constrain memory capacity and mitigate uncontrolled growth.\nFurthermore, we present ZH-4O, a Chinese ultra-long dialogue dataset\nspecifically designed for role-playing, featuring dialogues that average 600\nturns and include manually annotated memory information. Experimental results\ndemonstrate that MOOM outperforms all state-of-the-art memory extraction\nmethods, requiring fewer large language model invocations while maintaining a\ncontrollable memory capacity."}
{"id": "2410.11016", "pdf": "https://arxiv.org/pdf/2410.11016.pdf", "abs": "https://arxiv.org/abs/2410.11016", "title": "Intramuscular microelectrode arrays enable highly-accurate neural decoding of hand movements", "authors": ["Agnese Grison", "Jaime Ibanez Pereda", "Silvia Muceli", "Aritra Kundu", "Farah Baracat", "Giacomo Indiveri", "Elisa Donati", "Dario Farina"], "categories": ["q-bio.NC", "cs.HC", "cs.RO", "eess.SP"], "comment": null, "summary": "Decoding the activity of the nervous system is a critical challenge in\nneuroscience and neural interfacing. In this study, we present a neuromuscular\nrecording system that enables large-scale sampling of muscle activity using\nmicroelectrode arrays with over 100 channels embedded in forearm muscles. These\narrays captured intramuscular high-density signals that were decoded into\npatterns of activation of spinal motoneurons. In two healthy participants, we\nrecorded high-density intramuscular activity during single- and multi-digit\ncontractions, revealing distinct motoneuron recruitment patterns specific to\neach task. Based on these patterns, we achieved perfect classification accuracy\n(100%) for 12 single- and multi-digit tasks and over 96% accuracy for up to 16\ntasks, significantly outperforming state-of-the-art EMG classification methods.\nThis intramuscular high-density system and classification method represent an\nadvancement in neural interfacing, with the potential to improve human-computer\ninteraction and the control of assistive technologies, particularly for\nreplacing or restoring impaired motor function."}
{"id": "2509.11868", "pdf": "https://arxiv.org/pdf/2509.11868.pdf", "abs": "https://arxiv.org/abs/2509.11868", "title": "Growing Perspectives: Modelling Embodied Perspective Taking and Inner Narrative Development Using Large Language Models", "authors": ["Sabrina Patania", "Luca Annese", "Anna Lambiase", "Anita Pellegrini", "Tom Foulsham", "Azzurra Ruggeri", "Silvia Rossi", "Silvia Serino", "Dimitri Ognibene"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.RO", "I.2; I.2.7; I.2.10; J.4"], "comment": "Accepted at ICDL https://icdl2025.fel.cvut.cz/", "summary": "Language and embodied perspective taking are essential for human\ncollaboration, yet few computational models address both simultaneously. This\nwork investigates the PerspAct system [1], which integrates the ReAct (Reason\nand Act) paradigm with Large Language Models (LLMs) to simulate developmental\nstages of perspective taking, grounded in Selman's theory [2]. Using an\nextended director task, we evaluate GPT's ability to generate internal\nnarratives aligned with specified developmental stages, and assess how these\ninfluence collaborative performance both qualitatively (action selection) and\nquantitatively (task efficiency). Results show that GPT reliably produces\ndevelopmentally-consistent narratives before task execution but often shifts\ntowards more advanced stages during interaction, suggesting that language\nexchanges help refine internal representations. Higher developmental stages\ngenerally enhance collaborative effectiveness, while earlier stages yield more\nvariable outcomes in complex contexts. These findings highlight the potential\nof integrating embodied perspective taking and language in LLMs to better model\ndevelopmental dynamics and stress the importance of evaluating internal speech\nduring combined linguistic and embodied tasks."}
{"id": "2504.09662", "pdf": "https://arxiv.org/pdf/2504.09662.pdf", "abs": "https://arxiv.org/abs/2504.09662", "title": "AgentDynEx: Nudging the Mechanics and Dynamics of Multi-Agent Simulations", "authors": ["Jenny Ma", "Riya Sahni", "Karthik Sreedhar", "Lydia B. Chilton"], "categories": ["cs.MA", "cs.AI", "cs.HC"], "comment": "31 pages, 12 figures, 7 tables", "summary": "Multi-agent large language model simulations have the potential to model\ncomplex human behaviors and interactions. If the mechanics are set up properly,\nunanticipated and valuable social dynamics can surface. However, it is\nchallenging to consistently enforce simulation mechanics while still allowing\nfor notable and emergent dynamics. We present AgentDynEx, an AI system that\nhelps set up simulations from user-specified mechanics and dynamics. AgentDynEx\nuses LLMs to guide users through a Configuration Matrix to identify core\nmechanics and define milestones to track dynamics. It also introduces a method\ncalled \\textit{nudging}, where the system dynamically reflects on simulation\nprogress and gently intervenes if it begins to deviate from intended outcomes.\nA technical evaluation found that nudging enables simulations to have more\ncomplex mechanics and maintain its notable dynamics compared to simulations\nwithout nudging. We discuss the importance of nudging as a technique for\nbalancing mechanics and dynamics of multi-agent simulations."}
{"id": "2509.11915", "pdf": "https://arxiv.org/pdf/2509.11915.pdf", "abs": "https://arxiv.org/abs/2509.11915", "title": "Uncertainty in Authorship: Why Perfect AI Detection Is Mathematically Impossible", "authors": ["Aadil Gani Ganie"], "categories": ["cs.CL"], "comment": null, "summary": "As large language models (LLMs) become more advanced, it is increasingly\ndifficult to distinguish between human-written and AI-generated text. This\npaper draws a conceptual parallel between quantum uncertainty and the limits of\nauthorship detection in natural language. We argue that there is a fundamental\ntrade-off: the more confidently one tries to identify whether a text was\nwritten by a human or an AI, the more one risks disrupting the text's natural\nflow and authenticity. This mirrors the tension between precision and\ndisturbance found in quantum systems. We explore how current detection\nmethods--such as stylometry, watermarking, and neural classifiers--face\ninherent limitations. Enhancing detection accuracy often leads to changes in\nthe AI's output, making other features less reliable. In effect, the very act\nof trying to detect AI authorship introduces uncertainty elsewhere in the text.\nOur analysis shows that when AI-generated text closely mimics human writing,\nperfect detection becomes not just technologically difficult but theoretically\nimpossible. We address counterarguments and discuss the broader implications\nfor authorship, ethics, and policy. Ultimately, we suggest that the challenge\nof AI-text detection is not just a matter of better tools--it reflects a\ndeeper, unavoidable tension in the nature of language itself."}
{"id": "2504.10793", "pdf": "https://arxiv.org/pdf/2504.10793.pdf", "abs": "https://arxiv.org/abs/2504.10793", "title": "SonicSieve: Bringing Directional Speech Extraction to Smartphones Using Acoustic Microstructures", "authors": ["Kuang Yuan", "Yifeng Wang", "Xiyuxing Zhang", "Chengyi Shen", "Swarun Kumar", "Justin Chan"], "categories": ["cs.SD", "cs.HC", "cs.LG", "eess.AS"], "comment": null, "summary": "Imagine placing your smartphone on a table in a noisy restaurant and clearly\ncapturing the voices of friends seated around you, or recording a lecturer's\nvoice with clarity in a reverberant auditorium. We introduce SonicSieve, the\nfirst intelligent directional speech extraction system for smartphones using a\nbio-inspired acoustic microstructure. Our passive design embeds directional\ncues onto incoming speech without any additional electronics. It attaches to\nthe in-line mic of low-cost wired earphones which can be attached to\nsmartphones. We present an end-to-end neural network that processes the raw\naudio mixtures in real-time on mobile devices. Our results show that SonicSieve\nachieves a signal quality improvement of 5.0 dB when focusing on a 30{\\deg}\nangular region. Additionally, the performance of our system based on only two\nmicrophones exceeds that of conventional 5-microphone arrays."}
{"id": "2509.11921", "pdf": "https://arxiv.org/pdf/2509.11921.pdf", "abs": "https://arxiv.org/abs/2509.11921", "title": "Designing LLMs for cultural sensitivity: Evidence from English-Japanese translation", "authors": ["Helene Tenzer", "Oumnia Abidi", "Stefan Feuerriegel"], "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": null, "summary": "Large language models (LLMs) are increasingly used in everyday communication,\nincluding multilingual interactions across different cultural contexts. While\nLLMs can now generate near-perfect literal translations, it remains unclear\nwhether LLMs support culturally appropriate communication. In this paper, we\nanalyze the cultural sensitivity of different LLM designs when applied to\nEnglish-Japanese translations of workplace e-mails. Here, we vary the prompting\nstrategies: (1) naive \"just translate\" prompts, (2) audience-targeted prompts\nspecifying the recipient's cultural background, and (3) instructional prompts\nwith explicit guidance on Japanese communication norms. Using a mixed-methods\nstudy, we then analyze culture-specific language patterns to evaluate how well\ntranslations adapt to cultural norms. Further, we examine the appropriateness\nof the tone of the translations as perceived by native speakers. We find that\nculturally-tailored prompting can improve cultural fit, based on which we offer\nrecommendations for designing culturally inclusive LLMs in multilingual\nsettings."}
{"id": "2504.12805", "pdf": "https://arxiv.org/pdf/2504.12805.pdf", "abs": "https://arxiv.org/abs/2504.12805", "title": "Assessing LLMs in Art Contexts: Critique Generation and Theory of Mind Evaluation", "authors": ["Takaya Arita", "Wenxian Zheng", "Reiji Suzuki", "Fuminori Akiba"], "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": "Corrected a typo in the metadata title only\n  (\"Assesing\"->\"Assessing\"). No changes were made to the PDF or source files", "summary": "This study explored how large language models (LLMs) perform in two areas\nrelated to art: writing critiques of artworks and reasoning about mental states\n(Theory of Mind, or ToM) in art-related situations. For the critique generation\npart, we built a system that combines Noel Carroll's evaluative framework with\na broad selection of art criticism theories. The model was prompted to first\nwrite a full-length critique and then shorter, more coherent versions using a\nstep-by-step prompting process. These AI-generated critiques were then compared\nwith those written by human experts in a Turing test-style evaluation. In many\ncases, human subjects had difficulty telling which was which, and the results\nsuggest that LLMs can produce critiques that are not only plausible in style\nbut also rich in interpretation, as long as they are carefully guided. In the\nsecond part, we introduced new simple ToM tasks based on situations involving\ninterpretation, emotion, and moral tension, which can appear in the context of\nart. These go beyond standard false-belief tests and allow for more complex,\nsocially embedded forms of reasoning. We tested 41 recent LLMs and found that\ntheir performance varied across tasks and models. In particular, tasks that\ninvolved affective or ambiguous situations tended to reveal clearer\ndifferences. Taken together, these results help clarify how LLMs respond to\ncomplex interpretative challenges, revealing both their cognitive limitations\nand potential. While our findings do not directly contradict the so-called\nGenerative AI Paradox--the idea that LLMs can produce expert-like output\nwithout genuine understanding--they suggest that, depending on how LLMs are\ninstructed, such as through carefully designed prompts, these models may begin\nto show behaviors that resemble understanding more closely than we might\nassume."}
{"id": "2509.11961", "pdf": "https://arxiv.org/pdf/2509.11961.pdf", "abs": "https://arxiv.org/abs/2509.11961", "title": "Spec-LLaVA: Accelerating Vision-Language Models with Dynamic Tree-Based Speculative Decoding", "authors": ["Mingxiao Huo", "Jiayi Zhang", "Hewei Wang", "Jinfeng Xu", "Zheyu Chen", "Huilin Tai", "Yijun Chen"], "categories": ["cs.CL"], "comment": "7pages, accepted by ICML TTODLer-FM workshop", "summary": "Vision-Language Models (VLMs) enable powerful multimodal reasoning but suffer\nfrom slow autoregressive inference, limiting their deployment in real-time\napplications. We introduce Spec-LLaVA, a system that applies speculative\ndecoding to accelerate VLMs without sacrificing output quality. Spec-LLaVA\npairs a lightweight draft VLM with a large target model: the draft speculates\nfuture tokens, which the target verifies in parallel, allowing multiple tokens\nto be generated per step. To maximize efficiency, we design a dynamic\ntree-based verification algorithm that adaptively expands and prunes\nspeculative branches using draft model confidence. On MS COCO out-of-domain\nimages, Spec-LLaVA achieves up to 3.28$\\times$ faster decoding on LLaVA-1.5\n(7B, 13B) with no loss in generation quality. This work presents a lossless\nacceleration framework for VLMs using dynamic tree-structured speculative\ndecoding, opening a path toward practical real-time multimodal assistants.\nImportantly, the lightweight draft model design makes the framework amenable to\nresource-constrained or on-device deployment settings."}
{"id": "2504.15133", "pdf": "https://arxiv.org/pdf/2504.15133.pdf", "abs": "https://arxiv.org/abs/2504.15133", "title": "EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language Models", "authors": ["Ziwen Xu", "Shuxun Wang", "Kewei Xu", "Haoming Xu", "Mengru Wang", "Xinle Deng", "Yunzhi Yao", "Guozhou Zheng", "Huajun Chen", "Ningyu Zhang"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "comment": "EMNLP 2025 System Demonstrations. Demo:\n  https://www.youtube.com/watch?v=AkfoiPfp5rQ; code:\n  https://github.com/zjunlp/EasyEdit", "summary": "In this paper, we introduce EasyEdit2, a framework designed to enable\nplug-and-play adjustability for controlling Large Language Model (LLM)\nbehaviors. EasyEdit2 supports a wide range of test-time interventions,\nincluding safety, sentiment, personality, reasoning patterns, factuality, and\nlanguage features. Unlike its predecessor, EasyEdit2 features a new\narchitecture specifically designed for seamless model steering. It comprises\nkey modules such as the steering vector generator and the steering vector\napplier, which enable automatic generation and application of steering vectors\nto influence the model's behavior without modifying its parameters. One of the\nmain advantages of EasyEdit2 is its ease of use-users do not need extensive\ntechnical knowledge. With just a single example, they can effectively guide and\nadjust the model's responses, making precise control both accessible and\nefficient. Empirically, we report model steering performance across different\nLLMs, demonstrating the effectiveness of these techniques. We have released the\nsource code on GitHub at https://github.com/zjunlp/EasyEdit along with a\ndemonstration notebook. In addition, we provide a demo video at\nhttps://www.youtube.com/watch?v=AkfoiPfp5rQ for a quick introduction."}
{"id": "2509.11963", "pdf": "https://arxiv.org/pdf/2509.11963.pdf", "abs": "https://arxiv.org/abs/2509.11963", "title": "ToolRM: Outcome Reward Models for Tool-Calling Large Language Models", "authors": ["Mayank Agarwal", "Ibrahim Abdelaziz", "Kinjal Basu", "Merve Unuvar", "Luis A. Lastras", "Yara Rizk", "Pavan Kapanipathi"], "categories": ["cs.CL"], "comment": null, "summary": "As large language models (LLMs) increasingly interact with external tools,\nreward modeling for tool use has become a critical yet underexplored area.\nExisting reward models, trained primarily on natural language outputs, struggle\nto evaluate tool-based reasoning and execution. To quantify this gap, we\nintroduce FC-RewardBench, the first benchmark designed to systematically assess\nreward models' performance in tool-calling scenarios. Our analysis shows that\ncurrent reward models often miss key signals of effective tool use,\nhighlighting the need for domain-specific modeling. To address this, we propose\na training framework for outcome-based reward models using data synthesized\nfrom permissively licensed, open-weight LLMs. We train models ranging from 1.7B\nto 14B parameters and evaluate them across seven out-of-domain benchmarks.\nThese models consistently outperform general-purpose baselines, achieving up to\n25\\% average improvement in downstream task performance and enabling\ndata-efficient fine-tuning through reward-guided filtering."}
{"id": "2506.11212", "pdf": "https://arxiv.org/pdf/2506.11212.pdf", "abs": "https://arxiv.org/abs/2506.11212", "title": "User Perceptions and Attitudes Toward Untraceability in Messaging Platforms", "authors": ["Carla F. Griggio", "Boel Nelson", "Zefan Sramek", "Aslan Askarov"], "categories": ["cs.CR", "cs.HC"], "comment": null, "summary": "Mainstream messaging platforms offer a variety of features designed to\nenhance user privacy, such as password-protected chats and end-to-end\nencryption, which primarily protect message contents. Beyond contents, a lot\ncan be inferred about people simply by tracing who sends and receives messages,\nwhen, and how often. This paper explores user perceptions of and attitudes\ntoward \"untraceability\", defined as preventing third parties from tracing who\ncommunicates with whom, to inform the design of privacy-enhancing technologies\nand untraceable communication protocols. Through a vignette-based qualitative\nstudy with 189 participants, we identify a diverse set of features that users\nperceive to be useful for untraceable messaging, ranging from using aliases\ninstead of real names to VPNs. Through a reflexive thematic analysis, we\nuncover three overarching attitudes that influence the support or rejection of\nuntraceability in messaging platforms and that can serve as a set of new\nprivacy personas: privacy fundamentalists, who advocate for privacy as a\nuniversal right; safety fundamentalists, who support surveillance for the sake\nof accountability; and optimists, who advocate for privacy in principle but\nalso endorse exceptions in idealistic ways, such as encryption backdoors. We\nhighlight a critical gap between the threat models assumed by users and those\naddressed by untraceable communication protocols. Many participants understood\nuntraceability as a form of anonymity, but interpret it as senders and\nreceivers hiding their identities from each other, rather than from external\nnetwork observers. We discuss implications for design of strategic\ncommunication and user interfaces of untraceable messaging protocols, and\npropose framing untraceability as a form of \"altruistic privacy\", i.e.,\nadopting privacy-enhancing technologies to protect others, as a promising\nstrategy to foster broad adoption."}
{"id": "2509.11989", "pdf": "https://arxiv.org/pdf/2509.11989.pdf", "abs": "https://arxiv.org/abs/2509.11989", "title": "Query-Focused Extractive Summarization for Sentiment Explanation", "authors": ["Ahmed Moubtahij", "Sylvie Ratté", "Yazid Attabi", "Maxime Dumas"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Constructive analysis of feedback from clients often requires determining the\ncause of their sentiment from a substantial amount of text documents. To assist\nand improve the productivity of such endeavors, we leverage the task of\nQuery-Focused Summarization (QFS). Models of this task are often impeded by the\nlinguistic dissonance between the query and the source documents. We propose\nand substantiate a multi-bias framework to help bridge this gap at a\ndomain-agnostic, generic level; we then formulate specialized approaches for\nthe problem of sentiment explanation through sentiment-based biases and query\nexpansion. We achieve experimental results outperforming baseline models on a\nreal-world proprietary sentiment-aware QFS dataset."}
{"id": "2509.00575", "pdf": "https://arxiv.org/pdf/2509.00575.pdf", "abs": "https://arxiv.org/abs/2509.00575", "title": "Can AI be Auditable?", "authors": ["Himanshu Verma", "Kirtan Padh", "Eva Thelisson"], "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": null, "summary": "Auditability is defined as the capacity of AI systems to be independently\nassessed for compliance with ethical, legal, and technical standards throughout\ntheir lifecycle. The chapter explores how auditability is being formalized\nthrough emerging regulatory frameworks, such as the EU AI Act, which mandate\ndocumentation, risk assessments, and governance structures. It analyzes the\ndiverse challenges facing AI auditability, including technical opacity,\ninconsistent documentation practices, lack of standardized audit tools and\nmetrics, and conflicting principles within existing responsible AI frameworks.\nThe discussion highlights the need for clear guidelines, harmonized\ninternational regulations, and robust socio-technical methodologies to\noperationalize auditability at scale. The chapter concludes by emphasizing the\nimportance of multi-stakeholder collaboration and auditor empowerment in\nbuilding an effective AI audit ecosystem. It argues that auditability must be\nembedded in AI development practices and governance infrastructures to ensure\nthat AI systems are not only functional but also ethically and legally aligned."}
{"id": "2509.11991", "pdf": "https://arxiv.org/pdf/2509.11991.pdf", "abs": "https://arxiv.org/abs/2509.11991", "title": "Text Adaptation to Plain Language and Easy Read via Automatic Post-Editing Cycles", "authors": ["Jesús Calleja", "David Ponce", "Thierry Etchegoyhen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We describe Vicomtech's participation in the CLEARS challenge on text\nadaptation to Plain Language and Easy Read in Spanish. Our approach features\nautomatic post-editing of different types of initial Large Language Model\nadaptations, where successive adaptations are generated iteratively until\nreadability and similarity metrics indicate that no further adaptation\nrefinement can be successfully performed. Taking the average of all official\nmetrics, our submissions achieved first and second place in Plain language and\nEasy Read adaptation, respectively."}
{"id": "2509.01909", "pdf": "https://arxiv.org/pdf/2509.01909.pdf", "abs": "https://arxiv.org/abs/2509.01909", "title": "Oyster-I: Beyond Refusal -- Constructive Safety Alignment for Responsible Language Models", "authors": ["Ranjie Duan", "Jiexi Liu", "Xiaojun Jia", "Shiji Zhao", "Ruoxi Cheng", "Fengxiang Wang", "Cheng Wei", "Yong Xie", "Chang Liu", "Defeng Li", "Yinpeng Dong", "Yichi Zhang", "Yuefeng Chen", "Chongwen Wang", "Xingjun Ma", "Xingxing Wei", "Yang Liu", "Hang Su", "Jun Zhu", "Xinfeng Li", "Yitong Sun", "Jie Zhang", "Jinzhao Hu", "Sha Xu", "Wenchao Yang", "Yitong Yang", "Jialing Tao", "Hui Xue"], "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC", "cs.SC"], "comment": "Technical Report Code & Model weights available:\n  https://github.com/Alibaba-AAIG/Oyster", "summary": "Large language models (LLMs) typically deploy safety mechanisms to prevent\nharmful content generation. Most current approaches focus narrowly on risks\nposed by malicious actors, often framing risks as adversarial events and\nrelying on defensive refusals. However, in real-world settings, risks also come\nfrom non-malicious users seeking help while under psychological distress (e.g.,\nself-harm intentions). In such cases, the model's response can strongly\ninfluence the user's next actions. Simple refusals may lead them to repeat,\nescalate, or move to unsafe platforms, creating worse outcomes. We introduce\nConstructive Safety Alignment (CSA), a human-centric paradigm that protects\nagainst malicious misuse while actively guiding vulnerable users toward safe\nand helpful results. Implemented in Oyster-I (Oy1), CSA combines game-theoretic\nanticipation of user reactions, fine-grained risk boundary discovery, and\ninterpretable reasoning control, turning safety into a trust-building process.\nOy1 achieves state-of-the-art safety among open models while retaining high\ngeneral capabilities. On our Constructive Benchmark, it shows strong\nconstructive engagement, close to GPT-5, and unmatched robustness on the\nStrata-Sword jailbreak dataset, nearing GPT-o1 levels. By shifting from\nrefusal-first to guidance-first safety, CSA redefines the model-user\nrelationship, aiming for systems that are not just safe, but meaningfully\nhelpful. We release Oy1, code, and the benchmark to support responsible,\nuser-centered AI."}
{"id": "2509.12065", "pdf": "https://arxiv.org/pdf/2509.12065.pdf", "abs": "https://arxiv.org/abs/2509.12065", "title": "Steering Language Models in Multi-Token Generation: A Case Study on Tense and Aspect", "authors": ["Alina Klerings", "Jannik Brinkmann", "Daniel Ruffinelli", "Simone Ponzetto"], "categories": ["cs.CL", "I.2.7"], "comment": "to be published in The 2025 Conference on Empirical Methods in\n  Natural Language Processing", "summary": "Large language models (LLMs) are able to generate grammatically well-formed\ntext, but how do they encode their syntactic knowledge internally? While prior\nwork has focused largely on binary grammatical contrasts, in this work, we\nstudy the representation and control of two multidimensional hierarchical\ngrammar phenomena - verb tense and aspect - and for each, identify distinct,\northogonal directions in residual space using linear discriminant analysis.\nNext, we demonstrate causal control over both grammatical features through\nconcept steering across three generation tasks. Then, we use these identified\nfeatures in a case study to investigate factors influencing effective steering\nin multi-token generation. We find that steering strength, location, and\nduration are crucial parameters for reducing undesirable side effects such as\ntopic shift and degeneration. Our findings suggest that models encode tense and\naspect in structurally organized, human-like ways, but effective control of\nsuch features during generation is sensitive to multiple factors and requires\nmanual tuning or automated optimization."}
{"id": "2509.06770", "pdf": "https://arxiv.org/pdf/2509.06770.pdf", "abs": "https://arxiv.org/abs/2509.06770", "title": "Another Turn, Better Output? A Turn-Wise Analysis of Iterative LLM Prompting", "authors": ["Shashidhar Reddy Javaji", "Bhavul Gauri", "Zining Zhu"], "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "Large language models (LLMs) are now used in multi-turn workflows, but we\nstill lack a clear way to measure when iteration helps and when it hurts. We\npresent an evaluation framework for iterative refinement that spans ideation,\ncode, and math. Our protocol runs controlled 12-turn conversations per task,\nutilizing a variety of prompts ranging from vague ``improve it'' feedback to\ntargeted steering, and logs per-turn outputs. We score outcomes with\ndomain-appropriate checks (unit tests for code; answer-equivalence plus\nreasoning-soundness for math; originality and feasibility for ideation) and\ntrack turn-level behavior with three families of metrics: semantic movement\nacross turns, turn-to-turn change, and output size growth. Across models and\ntasks, gains are domain-dependent: they arrive early in ideas and code, but in\nmath late turns matter when guided by elaboration. After the first few turns,\nvague feedback often plateaus or reverses correctness, while targeted prompts\nreliably shift the intended quality axis (novelty vs. feasibility in ideation;\nspeed vs. readability in code; in math, elaboration outperforms exploration and\ndrives late-turn gains). We also observe consistent domain patterns: ideation\nmoves more in meaning across turns, code tends to grow in size with little\nsemantic change, and math starts fixed but can break that path with late,\nelaborative iteration. Together, the framework and metrics make iteration\nmeasurable and comparable across models, and signal when to steer, stop, or\nswitch strategies."}
{"id": "2509.12093", "pdf": "https://arxiv.org/pdf/2509.12093.pdf", "abs": "https://arxiv.org/abs/2509.12093", "title": "SENSE models: an open source solution for multilingual and multimodal semantic-based tasks", "authors": ["Salima Mdhaffar", "Haroun Elleuch", "Chaimae Chellaf", "Ha Nguyen", "Yannick Estève"], "categories": ["cs.CL"], "comment": "Accepted to IEEE ASRU 2025", "summary": "This paper introduces SENSE (Shared Embedding for N-lingual Speech and tExt),\nan open-source solution inspired by the SAMU-XLSR framework and conceptually\nsimilar to Meta AI's SONAR models. These approaches rely on a teacher-student\nframework to align a self-supervised speech encoder with the language-agnostic\ncontinuous representations of a text encoder at the utterance level. We\ndescribe how the original SAMU-XLSR method has been updated by selecting a\nstronger teacher text model and a better initial speech encoder. The source\ncode for training and using SENSE models has been integrated into the\nSpeechBrain toolkit, and the first SENSE model we trained has been publicly\nreleased. We report experimental results on multilingual and multimodal\nsemantic tasks, where our SENSE model achieves highly competitive performance.\nFinally, this study offers new insights into how semantics are captured in such\nsemantically aligned speech encoders."}
{"id": "2509.09071", "pdf": "https://arxiv.org/pdf/2509.09071.pdf", "abs": "https://arxiv.org/abs/2509.09071", "title": "Strategic Tradeoffs Between Humans and AI in Multi-Agent Bargaining", "authors": ["Crystal Qian", "Kehang Zhu", "John Horton", "Benjamin S. Manning", "Vivian Tsai", "James Wexler", "Nithum Thain"], "categories": ["cs.AI", "cs.GT", "cs.HC"], "comment": null, "summary": "Coordination tasks traditionally performed by humans are increasingly being\ndelegated to autonomous agents. As this pattern progresses, it becomes critical\nto evaluate not only these agents' performance but also the processes through\nwhich they negotiate in dynamic, multi-agent environments. Furthermore,\ndifferent agents exhibit distinct advantages: traditional statistical agents,\nsuch as Bayesian models, may excel under well-specified conditions, whereas\nlarge language models (LLMs) can generalize across contexts. In this work, we\ncompare humans (N = 216), LLMs (GPT-4o, Gemini 1.5 Pro), and Bayesian agents in\na dynamic negotiation setting that enables direct, identical-condition\ncomparisons across populations, capturing both outcomes and behavioral\ndynamics. Bayesian agents extract the highest surplus through aggressive\noptimization, at the cost of frequent trade rejections. Humans and LLMs can\nachieve similar overall surplus, but through distinct behaviors: LLMs favor\nconservative, concessionary trades with few rejections, while humans employ\nmore strategic, risk-taking, and fairness-oriented behaviors. Thus, we find\nthat performance parity -- a common benchmark in agent evaluation -- can\nconceal fundamental differences in process and alignment, which are critical\nfor practical deployment in real-world coordination tasks."}
{"id": "2509.12098", "pdf": "https://arxiv.org/pdf/2509.12098.pdf", "abs": "https://arxiv.org/abs/2509.12098", "title": "Is 'Hope' a person or an idea? A pilot benchmark for NER: comparing traditional NLP tools and large language models on ambiguous entities", "authors": ["Payam Latifi"], "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 9 figures, 2 tables. This is a pilot study evaluating six\n  NER systems -- three traditional tools (NLTK, spaCy, Stanza) and three LLMs\n  (Gemini-1.5-flash, DeepSeek-V3, Qwen-3-4B) -- on a small, ambiguity-rich\n  dataset of 119 tokens. The annotated dataset, prompts are provided in\n  appendices for full reproducibility. All experiments were conducted on 14 May\n  2025", "summary": "This pilot study presents a small-scale but carefully annotated benchmark of\nNamed Entity Recognition (NER) performance across six systems: three non-LLM\nNLP tools (NLTK, spaCy, Stanza) and three general-purpose large language models\n(LLMs: Gemini-1.5-flash, DeepSeek-V3, Qwen-3-4B). The dataset contains 119\ntokens covering five entity types (PERSON, LOCATION, ORGANIZATION, DATE, TIME).\nWe evaluated each system's output against the manually annotated gold standard\ndataset using F1-score. The results show that LLMs generally outperform\nconventional tools in recognizing context-sensitive entities like person names,\nwith Gemini achieving the highest average F1-score. However, traditional\nsystems like Stanza demonstrate greater consistency in structured tags such as\nLOCATION and DATE. We also observed variability among LLMs, particularly in\nhandling temporal expressions and multi-word organizations. Our findings\nhighlight that while LLMs offer improved contextual understanding, traditional\ntools remain competitive in specific tasks, informing model selection."}
{"id": "2509.12101", "pdf": "https://arxiv.org/pdf/2509.12101.pdf", "abs": "https://arxiv.org/abs/2509.12101", "title": "In-domain SSL pre-training and streaming ASR", "authors": ["Jarod Duret", "Salima Mdhaffar", "Gaëlle Laperrière", "Ryan Whetten", "Audrey Galametz", "Catherine Kobus", "Marion-Cécile Martin", "Jo Oleiwan", "Yannick Estève"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to SPECOM 2025", "summary": "In this study, we investigate the benefits of domain-specific self-supervised\npre-training for both offline and streaming ASR in Air Traffic Control (ATC)\nenvironments. We train BEST-RQ models on 4.5k hours of unlabeled ATC data, then\nfine-tune on a smaller supervised ATC set. To enable real-time processing, we\npropose using chunked attention and dynamic convolutions, ensuring low-latency\ninference. We compare these in-domain SSL models against state-of-the-art,\ngeneral-purpose speech encoders such as w2v-BERT 2.0 and HuBERT. Results show\nthat domain-adapted pre-training substantially improves performance on standard\nATC benchmarks, significantly reducing word error rates when compared to models\ntrained on broad speech corpora. Furthermore, the proposed streaming approach\nfurther improves word error rate under tighter latency constraints, making it\nparticularly suitable for safety-critical aviation applications. These findings\nhighlight that specializing SSL representations for ATC data is a practical\npath toward more accurate and efficient ASR systems in real-world operational\nsettings."}
{"id": "2509.12108", "pdf": "https://arxiv.org/pdf/2509.12108.pdf", "abs": "https://arxiv.org/abs/2509.12108", "title": "GTA: Supervised-Guided Reinforcement Learning for Text Classification with Large Language Models", "authors": ["Min Zeng", "Jinfei Sun", "Xueyou Luo", "Caiquan Liu", "Shiqi Zhang", "Li Xie", "Xiaoxin Chen"], "categories": ["cs.CL"], "comment": "Accepted at EMNLP 2025", "summary": "In natural language processing tasks, pure reinforcement learning (RL)\nfine-tuning methods often suffer from inefficient exploration and slow\nconvergence; while supervised fine-tuning (SFT) methods, although efficient in\ntraining, have limited performance ceiling and less solid theoretical\nfoundation compared to RL. To address efficiency-capability trade-off, we\npropose the Guess-Think-Answer (GTA) framework that combines the efficiency of\nSFT with the capability gains of RL in a unified training paradigm. GTA works\nby having the model first produce a provisional guess (optimized via\ncross-entropy loss), then reflect on this guess before generating the final\nanswer, with RL rewards shaping both the final output and the format of the\nentire GTA structure. This hybrid approach achieves both faster convergence\nthan pure RL and higher performance ceiling than pure SFT. To mitigate gradient\nconflicts between the two training signals, we employ loss masking and gradient\nconstraints. Empirical results on four text classification benchmarks\ndemonstrate that GTA substantially accelerates convergence while outperforming\nboth standalone SFT and RL baselines."}
{"id": "2509.12112", "pdf": "https://arxiv.org/pdf/2509.12112.pdf", "abs": "https://arxiv.org/abs/2509.12112", "title": "CBP-Tuning: Efficient Local Customization for Black-box Large Language Models", "authors": ["Jiaxuan Zhao", "Naibin Gu", "Yuchen Feng", "Xiyu Liu", "Peng Fu", "Zheng Lin", "Weiping Wang"], "categories": ["cs.CL"], "comment": null, "summary": "The high costs of customizing large language models (LLMs) fundamentally\nlimit their adaptability to user-specific needs. Consequently, LLMs are\nincreasingly offered as cloud-based services, a paradigm that introduces\ncritical limitations: providers struggle to support personalized customization\nat scale, while users face privacy risks when exposing sensitive data. To\naddress this dual challenge, we propose Customized Black-box Prompt Tuning\n(CBP-Tuning), a novel framework that facilitates efficient local customization\nwhile preserving bidirectional privacy. Specifically, we design a two-stage\nframework: (1) a prompt generator trained on the server-side to capture\ndomain-specific and task-agnostic capabilities, and (2) user-side gradient-free\noptimization that tailors soft prompts for individual tasks. This approach\neliminates the need for users to access model weights or upload private data,\nrequiring only a single customized vector per task while achieving effective\nadaptation. Furthermore, the evaluation of CBP-Tuning in the commonsense\nreasoning, medical and financial domain settings demonstrates superior\nperformance compared to baselines, showcasing its advantages in task-agnostic\nprocessing and privacy preservation."}
{"id": "2509.12130", "pdf": "https://arxiv.org/pdf/2509.12130.pdf", "abs": "https://arxiv.org/abs/2509.12130", "title": "XplaiNLP at CheckThat! 2025: Multilingual Subjectivity Detection with Finetuned Transformers and Prompt-Based Inference with Large Language Models", "authors": ["Ariana Sahitaj", "Jiaao Li", "Pia Wenzel Neves", "Fedor Splitt", "Premtim Sahitaj", "Charlott Jakob", "Veronika Solopova", "Vera Schmitt"], "categories": ["cs.CL"], "comment": null, "summary": "This notebook reports the XplaiNLP submission to the CheckThat! 2025 shared\ntask on multilingual subjectivity detection. We evaluate two approaches: (1)\nsupervised fine-tuning of transformer encoders, EuroBERT, XLM-RoBERTa, and\nGerman-BERT, on monolingual and machine-translated training data; and (2)\nzero-shot prompting using two LLMs: o3-mini for Annotation (rule-based\nlabelling) and gpt-4.1-mini for DoubleDown (contrastive rewriting) and\nPerspective (comparative reasoning). The Annotation Approach achieves 1st place\nin the Italian monolingual subtask with an F_1 score of 0.8104, outperforming\nthe baseline of 0.6941. In the Romanian zero-shot setting, the fine-tuned\nXLM-RoBERTa model obtains an F_1 score of 0.7917, ranking 3rd and exceeding the\nbaseline of 0.6461. The same model also performs reliably in the multilingual\ntask and improves over the baseline in Greek. For German, a German-BERT model\nfine-tuned on translated training data from typologically related languages\nyields competitive performance over the baseline. In contrast, performance in\nthe Ukrainian and Polish zero-shot settings falls slightly below the respective\nbaselines, reflecting the challenge of generalization in low-resource\ncross-lingual scenarios."}
{"id": "2509.12158", "pdf": "https://arxiv.org/pdf/2509.12158.pdf", "abs": "https://arxiv.org/abs/2509.12158", "title": "Pun Unintended: LLMs and the Illusion of Humor Understanding", "authors": ["Alessandro Zangari", "Matteo Marcuzzo", "Andrea Albarelli", "Mohammad Taher Pilehvar", "Jose Camacho-Collados"], "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "Puns are a form of humorous wordplay that exploits polysemy and phonetic\nsimilarity. While LLMs have shown promise in detecting puns, we show in this\npaper that their understanding often remains shallow, lacking the nuanced grasp\ntypical of human interpretation. By systematically analyzing and reformulating\nexisting pun benchmarks, we demonstrate how subtle changes in puns are\nsufficient to mislead LLMs. Our contributions include comprehensive and nuanced\npun detection benchmarks, human evaluation across recent LLMs, and an analysis\nof the robustness challenges these models face in processing puns."}
{"id": "2509.12168", "pdf": "https://arxiv.org/pdf/2509.12168.pdf", "abs": "https://arxiv.org/abs/2509.12168", "title": "RAGs to Riches: RAG-like Few-shot Learning for Large Language Model Role-playing", "authors": ["Timothy Rupprecht", "Enfu Nan", "Arash Akbari", "Arman Akbari", "Lei Lu", "Priyanka Maan", "Sean Duffy", "Pu Zhao", "Yumei He", "David Kaeli", "Yanzhi Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Role-playing Large language models (LLMs) are increasingly deployed in\nhigh-stakes domains such as healthcare, education, and governance, where\nfailures can directly impact user trust and well-being. A cost effective\nparadigm for LLM role-playing is few-shot learning, but existing approaches\noften cause models to break character in unexpected and potentially harmful\nways, especially when interacting with hostile users. Inspired by\nRetrieval-Augmented Generation (RAG), we reformulate LLM role-playing into a\ntext retrieval problem and propose a new prompting framework called\nRAGs-to-Riches, which leverages curated reference demonstrations to condition\nLLM responses. We evaluate our framework with LLM-as-a-judge preference voting\nand introduce two novel token-level ROUGE metrics: Intersection over Output\n(IOO) to quantity how much an LLM improvises and Intersection over References\n(IOR) to measure few-shot demonstrations utilization rate during the evaluation\ntasks. When simulating interactions with a hostile user, our prompting strategy\nincorporates in its responses during inference an average of 35% more tokens\nfrom the reference demonstrations. As a result, across 453 role-playing\ninteractions, our models are consistently judged as being more authentic, and\nremain in-character more often than zero-shot and in-context Learning (ICL)\nmethods. Our method presents a scalable strategy for building robust,\nhuman-aligned LLM role-playing frameworks."}
{"id": "2509.12171", "pdf": "https://arxiv.org/pdf/2509.12171.pdf", "abs": "https://arxiv.org/abs/2509.12171", "title": "Preservation of Language Understanding Capabilities in Speech-aware Large Language Models", "authors": ["Marek Kubis", "Paweł Skórzewski", "Iwona Christop", "Mateusz Czyżnikiewicz", "Jakub Kubiak", "Łukasz Bondaruk", "Marcin Lewandowski"], "categories": ["cs.CL", "cs.AI"], "comment": "5 pages, 1 figure", "summary": "The paper presents C3T (Cross-modal Capabilities Conservation Test), a new\nbenchmark for assessing the performance of speech-aware large language models.\nThe benchmark utilizes textual tasks and a voice cloning text-to-speech model\nto quantify the extent to which language understanding capabilities are\npreserved when the model is accessed via speech input. C3T quantifies the\nfairness of the model for different categories of speakers and its robustness\nacross text and speech modalities."}
{"id": "2509.10467", "pdf": "https://arxiv.org/pdf/2509.10467.pdf", "abs": "https://arxiv.org/abs/2509.10467", "title": "DSRAG: A Domain-Specific Retrieval Framework Based on Document-derived Multimodal Knowledge Graph", "authors": ["Mengzheng Yang", "Yanfei Ren", "David Osei Opoku", "Ruochang Li", "Peng Ren", "Chunxiao Xing"], "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.CV", "cs.MM"], "comment": "12 pages, 5 figures. Accepted to the 22nd International Conference on\n  Web Information Systems and Applications (WISA 2025)", "summary": "Current general-purpose large language models (LLMs) commonly exhibit\nknowledge hallucination and insufficient domain-specific adaptability in\ndomain-specific tasks, limiting their effectiveness in specialized question\nanswering scenarios. Retrieval-augmented generation (RAG) effectively tackles\nthese challenges by integrating external knowledge to enhance accuracy and\nrelevance. However, traditional RAG still faces limitations in domain knowledge\naccuracy and context modeling.To enhance domain-specific question answering\nperformance, this work focuses on a graph-based RAG framework, emphasizing the\ncritical role of knowledge graph quality during the generation process. We\npropose DSRAG (Domain-Specific RAG), a multimodal knowledge graph-driven\nretrieval-augmented generation framework designed for domain-specific\napplications. Our approach leverages domain-specific documents as the primary\nknowledge source, integrating heterogeneous information such as text, images,\nand tables to construct a multimodal knowledge graph covering both conceptual\nand instance layers. Building on this foundation, we introduce semantic pruning\nand structured subgraph retrieval mechanisms, combining knowledge graph context\nand vector retrieval results to guide the language model towards producing more\nreliable responses. Evaluations using the Langfuse multidimensional scoring\nmechanism show that our method excels in domain-specific question answering,\nvalidating the efficacy of integrating multimodal knowledge graphs with\nretrieval-augmented generation."}
{"id": "2509.10468", "pdf": "https://arxiv.org/pdf/2509.10468.pdf", "abs": "https://arxiv.org/abs/2509.10468", "title": "Learning Decomposed Contextual Token Representations from Pretrained and Collaborative Signals for Generative Recommendation", "authors": ["Yifan Liu", "Yaokun Liu", "Zelin Li", "Zhenrui Yue", "Gyuseok Lee", "Ruichen Yao", "Yang Zhang", "Dong Wang"], "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "preprint under review", "summary": "Recent advances in generative recommenders adopt a two-stage paradigm: items\nare first tokenized into semantic IDs using a pretrained tokenizer, and then\nlarge language models (LLMs) are trained to generate the next item via\nsequence-to-sequence modeling. However, these two stages are optimized for\ndifferent objectives: semantic reconstruction during tokenizer pretraining\nversus user interaction modeling during recommender training. This objective\nmisalignment leads to two key limitations: (i) suboptimal static tokenization,\nwhere fixed token assignments fail to reflect diverse usage contexts; and (ii)\ndiscarded pretrained semantics, where pretrained knowledge - typically from\nlanguage model embeddings - is overwritten during recommender training on user\ninteractions. To address these limitations, we propose to learn DEcomposed\nCOntextual Token Representations (DECOR), a unified framework that preserves\npretrained semantics while enhancing the adaptability of token embeddings.\nDECOR introduces contextualized token composition to refine token embeddings\nbased on user interaction context, and decomposed embedding fusion that\nintegrates pretrained codebook embeddings with newly learned collaborative\nembeddings. Experiments on three real-world datasets demonstrate that DECOR\nconsistently outperforms state-of-the-art baselines in recommendation\nperformance. Our code will be made available upon publication."}
{"id": "2509.10469", "pdf": "https://arxiv.org/pdf/2509.10469.pdf", "abs": "https://arxiv.org/abs/2509.10469", "title": "Real-Time RAG for the Identification of Supply Chain Vulnerabilities", "authors": ["Jesse Ponnock", "Grace Kenneally", "Michael Robert Briggs", "Elinor Yeo", "Tyrone Patterson III", "Nicholas Kinberg", "Matthew Kalinowski", "David Hechtman"], "categories": ["cs.IR", "cs.AI", "cs.CL", "I.2.7; H.3.3; I.2.6"], "comment": "14 pages, 5 figures, 1 table. Approved for Public Release;\n  Distribution Unlimited. PRS Release Number: 25-0864", "summary": "New technologies in generative AI can enable deeper analysis into our\nnation's supply chains but truly informative insights require the continual\nupdating and aggregation of massive data in a timely manner. Large Language\nModels (LLMs) offer unprecedented analytical opportunities however, their\nknowledge base is constrained to the models' last training date, rendering\nthese capabilities unusable for organizations whose mission impacts rely on\nemerging and timely information. This research proposes an innovative approach\nto supply chain analysis by integrating emerging Retrieval-Augmented Generation\n(RAG) preprocessing and retrieval techniques with advanced web-scraping\ntechnologies. Our method aims to reduce latency in incorporating new\ninformation into an augmented-LLM, enabling timely analysis of supply chain\ndisruptors. Through experimentation, this study evaluates the combinatorial\neffects of these techniques towards timeliness and quality trade-offs. Our\nresults suggest that in applying RAG systems to supply chain analysis,\nfine-tuning the embedding retrieval model consistently provides the most\nsignificant performance gains, underscoring the critical importance of\nretrieval quality. Adaptive iterative retrieval, which dynamically adjusts\nretrieval depth based on context, further enhances performance, especially on\ncomplex supply chain queries. Conversely, fine-tuning the LLM yields limited\nimprovements and higher resource costs, while techniques such as downward query\nabstraction significantly outperforms upward abstraction in practice."}
{"id": "2509.10534", "pdf": "https://arxiv.org/pdf/2509.10534.pdf", "abs": "https://arxiv.org/abs/2509.10534", "title": "Decoupling the \"What\" and \"Where\" With Polar Coordinate Positional Embeddings", "authors": ["Anand Gopalakrishnan", "Robert Csordás", "Jürgen Schmidhuber", "Michael C. Mozer"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The attention mechanism in a Transformer architecture matches key to query\nbased on both content -- the what -- and position in a sequence -- the where.\nWe present an analysis indicating that what and where are entangled in the\npopular RoPE rotary position embedding. This entanglement can impair\nperformance particularly when decisions require independent matches on these\ntwo factors. We propose an improvement to RoPE, which we call Polar Coordinate\nPosition Embeddings or PoPE, that eliminates the what-where confound. PoPE is\nfar superior on a diagnostic task requiring indexing solely by position or by\ncontent. On autoregressive sequence modeling in music, genomic, and natural\nlanguage domains, Transformers using PoPE as the positional encoding scheme\noutperform baselines using RoPE with respect to evaluation loss (perplexity)\nand downstream task performance. On language modeling, these gains persist\nacross model scale, from 124M to 774M parameters. Crucially, PoPE shows strong\nzero-shot length extrapolation capabilities, whereas RoPE's performance\ndegrades significantly on longer sequences at test time without fine tuning or\nthe use of position-interpolation methods."}
{"id": "2509.10538", "pdf": "https://arxiv.org/pdf/2509.10538.pdf", "abs": "https://arxiv.org/abs/2509.10538", "title": "DualAlign: Generating Clinically Grounded Synthetic Data", "authors": ["Rumeng Li", "Xun Wang", "Hong Yu"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "Synthetic clinical data are increasingly important for advancing AI in\nhealthcare, given strict privacy constraints on real-world EHRs, limited\navailability of annotated rare-condition data, and systemic biases in\nobservational datasets. While large language models (LLMs) can generate fluent\nclinical text, producing synthetic data that is both realistic and clinically\nmeaningful remains challenging. We introduce DualAlign, a framework that\nenhances statistical fidelity and clinical plausibility through dual alignment:\n(1) statistical alignment, which conditions generation on patient demographics\nand risk factors; and (2) semantic alignment, which incorporates real-world\nsymptom trajectories to guide content generation. Using Alzheimer's disease\n(AD) as a case study, DualAlign produces context-grounded symptom-level\nsentences that better reflect real-world clinical documentation. Fine-tuning an\nLLaMA 3.1-8B model with a combination of DualAlign-generated and\nhuman-annotated data yields substantial performance gains over models trained\non gold data alone or unguided synthetic baselines. While DualAlign does not\nfully capture longitudinal complexity, it offers a practical approach for\ngenerating clinically grounded, privacy-preserving synthetic data to support\nlow-resource clinical text analysis."}
{"id": "2509.10584", "pdf": "https://arxiv.org/pdf/2509.10584.pdf", "abs": "https://arxiv.org/abs/2509.10584", "title": "Smart Trial: Evaluating the Use of Large Language Models for Recruiting Clinical Trial Participants via Social Media", "authors": ["Xiaofan Zhou", "Zisu Wang", "Janice Krieger", "Mohan Zalake", "Lu Cheng"], "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": null, "summary": "Clinical trials (CT) are essential for advancing medical research and\ntreatment, yet efficiently recruiting eligible participants -- each of whom\nmust meet complex eligibility criteria -- remains a significant challenge.\nTraditional recruitment approaches, such as advertisements or electronic health\nrecord screening within hospitals, are often time-consuming and geographically\nconstrained. This work addresses the recruitment challenge by leveraging the\nvast amount of health-related information individuals share on social media\nplatforms. With the emergence of powerful large language models (LLMs) capable\nof sophisticated text understanding, we pose the central research question: Can\nLLM-driven tools facilitate CT recruitment by identifying potential\nparticipants through their engagement on social media? To investigate this\nquestion, we introduce TRIALQA, a novel dataset comprising two social media\ncollections from the subreddits on colon cancer and prostate cancer. Using\neligibility criteria from public real-world CTs, experienced annotators are\nhired to annotate TRIALQA to indicate (1) whether a social media user meets a\ngiven eligibility criterion and (2) the user's stated reasons for interest in\nparticipating in CT. We benchmark seven widely used LLMs on these two\nprediction tasks, employing six distinct training and inference strategies. Our\nextensive experiments reveal that, while LLMs show considerable promise, they\nstill face challenges in performing the complex, multi-hop reasoning needed to\naccurately assess eligibility criteria."}
{"id": "2509.10682", "pdf": "https://arxiv.org/pdf/2509.10682.pdf", "abs": "https://arxiv.org/abs/2509.10682", "title": "LLM in the Middle: A Systematic Review of Threats and Mitigations to Real-World LLM-based Systems", "authors": ["Vitor Hugo Galhardo Moia", "Igor Jochem Sanz", "Gabriel Antonio Fontes Rebello", "Rodrigo Duarte de Meneses", "Briland Hitaj", "Ulf Lindqvist"], "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.ET", "cs.LG"], "comment": "37 pages, 8 figures, 13 tables", "summary": "The success and wide adoption of generative AI (GenAI), particularly large\nlanguage models (LLMs), has attracted the attention of cybercriminals seeking\nto abuse models, steal sensitive data, or disrupt services. Moreover, providing\nsecurity to LLM-based systems is a great challenge, as both traditional threats\nto software applications and threats targeting LLMs and their integration must\nbe mitigated. In this survey, we shed light on security and privacy concerns of\nsuch LLM-based systems by performing a systematic review and comprehensive\ncategorization of threats and defensive strategies considering the entire\nsoftware and LLM life cycles. We analyze real-world scenarios with distinct\ncharacteristics of LLM usage, spanning from development to operation. In\naddition, threats are classified according to their severity level and to which\nscenarios they pertain, facilitating the identification of the most relevant\nthreats. Recommended defense strategies are systematically categorized and\nmapped to the corresponding life cycle phase and possible attack strategies\nthey attenuate. This work paves the way for consumers and vendors to understand\nand efficiently mitigate risks during integration of LLMs in their respective\nsolutions or organizations. It also enables the research community to benefit\nfrom the discussion of open challenges and edge cases that may hinder the\nsecure and privacy-preserving adoption of LLM-based systems."}
{"id": "2509.10707", "pdf": "https://arxiv.org/pdf/2509.10707.pdf", "abs": "https://arxiv.org/abs/2509.10707", "title": "Understanding AI Evaluation Patterns: How Different GPT Models Assess Vision-Language Descriptions", "authors": ["Sajjad Abdoli", "Rudi Cilibrasi", "Rima Al-Shikh"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "As AI systems increasingly evaluate other AI outputs, understanding their\nassessment behavior becomes crucial for preventing cascading biases. This study\nanalyzes vision-language descriptions generated by NVIDIA's Describe Anything\nModel and evaluated by three GPT variants (GPT-4o, GPT-4o-mini, GPT-5) to\nuncover distinct \"evaluation personalities\" the underlying assessment\nstrategies and biases each model demonstrates. GPT-4o-mini exhibits systematic\nconsistency with minimal variance, GPT-4o excels at error detection, while\nGPT-5 shows extreme conservatism with high variability. Controlled experiments\nusing Gemini 2.5 Pro as an independent question generator validate that these\npersonalities are inherent model properties rather than artifacts. Cross-family\nanalysis through semantic similarity of generated questions reveals significant\ndivergence: GPT models cluster together with high similarity while Gemini\nexhibits markedly different evaluation strategies. All GPT models demonstrate a\nconsistent 2:1 bias favoring negative assessment over positive confirmation,\nthough this pattern appears family-specific rather than universal across AI\narchitectures. These findings suggest that evaluation competence does not scale\nwith general capability and that robust AI assessment requires diverse\narchitectural perspectives."}
{"id": "2509.10769", "pdf": "https://arxiv.org/pdf/2509.10769.pdf", "abs": "https://arxiv.org/abs/2509.10769", "title": "AgentArch: A Comprehensive Benchmark to Evaluate Agent Architectures in Enterprise", "authors": ["Tara Bogavelli", "Roshnee Sharma", "Hari Subramani"], "categories": ["cs.AI", "cs.CL", "cs.MA"], "comment": null, "summary": "While individual components of agentic architectures have been studied in\nisolation, there remains limited empirical understanding of how different\ndesign dimensions interact within complex multi-agent systems. This study aims\nto address these gaps by providing a comprehensive enterprise-specific\nbenchmark evaluating 18 distinct agentic configurations across state-of-the-art\nlarge language models. We examine four critical agentic system dimensions:\norchestration strategy, agent prompt implementation (ReAct versus function\ncalling), memory architecture, and thinking tool integration. Our benchmark\nreveals significant model-specific architectural preferences that challenge the\nprevalent one-size-fits-all paradigm in agentic AI systems. It also reveals\nsignificant weaknesses in overall agentic performance on enterprise tasks with\nthe highest scoring models achieving a maximum of only 35.3\\% success on the\nmore complex task and 70.8\\% on the simpler task. We hope these findings inform\nthe design of future agentic systems by enabling more empirically backed\ndecisions regarding architectural components and model selection."}
{"id": "2509.10802", "pdf": "https://arxiv.org/pdf/2509.10802.pdf", "abs": "https://arxiv.org/abs/2509.10802", "title": "Why Bonds Fail Differently? Explainable Multimodal Learning for Multi-Class Default Prediction", "authors": ["Yi Lu", "Aifan Ling", "Chaoqun Wang", "Yaxin Xu"], "categories": ["q-fin.RM", "cs.CL", "cs.LG", "q-fin.CP"], "comment": null, "summary": "In recent years, China's bond market has seen a surge in defaults amid\nregulatory reforms and macroeconomic volatility. Traditional machine learning\nmodels struggle to capture financial data's irregularity and temporal\ndependencies, while most deep learning models lack interpretability-critical\nfor financial decision-making. To tackle these issues, we propose EMDLOT\n(Explainable Multimodal Deep Learning for Time-series), a novel framework for\nmulti-class bond default prediction. EMDLOT integrates numerical time-series\n(financial/macroeconomic indicators) and unstructured textual data (bond\nprospectuses), uses Time-Aware LSTM to handle irregular sequences, and adopts\nsoft clustering and multi-level attention to boost interpretability.\nExperiments on 1994 Chinese firms (2015-2024) show EMDLOT outperforms\ntraditional (e.g., XGBoost) and deep learning (e.g., LSTM) benchmarks in\nrecall, F1-score, and mAP, especially in identifying default/extended firms.\nAblation studies validate each component's value, and attention analyses reveal\neconomically intuitive default drivers. This work provides a practical tool and\na trustworthy framework for transparent financial risk modeling."}
{"id": "2509.10931", "pdf": "https://arxiv.org/pdf/2509.10931.pdf", "abs": "https://arxiv.org/abs/2509.10931", "title": "Harmful Prompt Laundering: Jailbreaking LLMs with Abductive Styles and Symbolic Encoding", "authors": ["Seongho Joo", "Hyukhun Koh", "Kyomin Jung"], "categories": ["cs.AI", "cs.CL"], "comment": "EMNLP 2025", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\ndiverse tasks, but their potential misuse for harmful purposes remains a\nsignificant concern. To strengthen defenses against such vulnerabilities, it is\nessential to investigate universal jailbreak attacks that exploit intrinsic\nweaknesses in the architecture and learning paradigms of LLMs. In response, we\npropose \\textbf{H}armful \\textbf{P}rompt \\textbf{La}undering (HaPLa), a novel\nand broadly applicable jailbreaking technique that requires only black-box\naccess to target models. HaPLa incorporates two primary strategies: 1)\n\\textit{abductive framing}, which instructs LLMs to infer plausible\nintermediate steps toward harmful activities, rather than directly responding\nto explicit harmful queries; and 2) \\textit{symbolic encoding}, a lightweight\nand flexible approach designed to obfuscate harmful content, given that current\nLLMs remain sensitive primarily to explicit harmful keywords. Experimental\nresults show that HaPLa achieves over 95% attack success rate on GPT-series\nmodels and 70% across all targets. Further analysis with diverse symbolic\nencoding rules also reveals a fundamental challenge: it remains difficult to\nsafely tune LLMs without significantly diminishing their helpfulness in\nresponding to benign queries."}
{"id": "2509.10932", "pdf": "https://arxiv.org/pdf/2509.10932.pdf", "abs": "https://arxiv.org/abs/2509.10932", "title": "Public Data Assisted Differentially Private In-Context Learning", "authors": ["Seongho Joo", "Hyukhun Koh", "Kyomin Jung"], "categories": ["cs.AI", "cs.CL"], "comment": "EMNLP 2025 Findings", "summary": "In-context learning (ICL) in Large Language Models (LLMs) has shown\nremarkable performance across various tasks without requiring fine-tuning.\nHowever, recent studies have highlighted the risk of private data leakage\nthrough the prompt in ICL, especially when LLMs are exposed to malicious\nattacks. While differential privacy (DP) provides strong privacy guarantees, it\noften significantly reduces the utility of in-context learning (ICL). To\naddress this challenge, we incorporate task-related public data into the ICL\nframework while maintaining the DP guarantee. Based on this approach, we\npropose a private in-context learning algorithm that effectively balances\nprivacy protection and model utility. Through experiments, we demonstrate that\nour approach significantly improves the utility of private ICL with the\nassistance of public data. Additionally, we show that our method is robust\nagainst membership inference attacks, demonstrating empirical privacy\nprotection."}
{"id": "2509.10975", "pdf": "https://arxiv.org/pdf/2509.10975.pdf", "abs": "https://arxiv.org/abs/2509.10975", "title": "ReFineG: Synergizing Small Supervised Models and LLMs for Low-Resource Grounded Multimodal NER", "authors": ["Jielong Tang", "Shuang Wang", "Zhenxing Wang", "Jianxing Yu", "Jian Yin"], "categories": ["cs.IR", "cs.CL"], "comment": "CCKS 2025 Shared Task Paper", "summary": "Grounded Multimodal Named Entity Recognition (GMNER) extends traditional NER\nby jointly detecting textual mentions and grounding them to visual regions.\nWhile existing supervised methods achieve strong performance, they rely on\ncostly multimodal annotations and often underperform in low-resource domains.\nMultimodal Large Language Models (MLLMs) show strong generalization but suffer\nfrom Domain Knowledge Conflict, producing redundant or incorrect mentions for\ndomain-specific entities. To address these challenges, we propose ReFineG, a\nthree-stage collaborative framework that integrates small supervised models\nwith frozen MLLMs for low-resource GMNER. In the Training Stage, a domain-aware\nNER data synthesis strategy transfers LLM knowledge to small models with\nsupervised training while avoiding domain knowledge conflicts. In the\nRefinement Stage, an uncertainty-based mechanism retains confident predictions\nfrom supervised models and delegates uncertain ones to the MLLM. In the\nGrounding Stage, a multimodal context selection algorithm enhances visual\ngrounding through analogical reasoning. In the CCKS2025 GMNER Shared Task,\nReFineG ranked second with an F1 score of 0.6461 on the online leaderboard,\ndemonstrating its effectiveness with limited annotations."}
{"id": "2509.11026", "pdf": "https://arxiv.org/pdf/2509.11026.pdf", "abs": "https://arxiv.org/abs/2509.11026", "title": "Rethinking Human Preference Evaluation of LLM Rationales", "authors": ["Ziang Li", "Manasi Ganti", "Zixian Ma", "Helena Vasconcelos", "Qijia He", "Ranjay Krishna"], "categories": ["cs.AI", "cs.CL"], "comment": "Published in the XLLM-Reason-Plan Workshop on the Application of LLM\n  Explainability to Reasoning and Planning at COLM 2025", "summary": "Large language models (LLMs) often generate natural language rationales --\nfree-form explanations that help improve performance on complex reasoning tasks\nand enhance interpretability for human users. However, evaluating these\nrationales remains challenging. While recent work has relied on binary\npreference judgments from humans or LLM judges, such evaluations are often\nopaque and coarse-grained, offering limited insight into what makes one\nrationale better than another. In this work, we rethink preference evaluation\nfor LLM-generated rationales by asking: (1) What attributes define good\nrationales? (2) Can human preferences be explained by these attributes? (3) Can\nattribute-based evaluation overcome the limitations of binary comparisons? We\nidentify a set of key rationale attributes from prior literature and assess\nthem using automatic metrics, LLM judgments, and human annotations. We then\nanalyze two standard human preference datasets MT Bench and Chatbot Arena using\nSHAP to identify which attributes best explain human preference outcomes.\nFinally, we re-evaluate model-generated rationales using attribute-specific ELO\nscores, revealing more nuanced model comparisons and insights. Our findings\nsuggest that fine-grained attribute evaluations can better characterize\nrationale quality and guide future research toward more interpretable and\nreliable evaluation practices."}
{"id": "2509.11071", "pdf": "https://arxiv.org/pdf/2509.11071.pdf", "abs": "https://arxiv.org/abs/2509.11071", "title": "The System Description of CPS Team for Track on Driving with Language of CVPR 2024 Autonomous Grand Challenge", "authors": ["Jinghan Peng", "Jingwen Wang", "Xing Yu", "Dehui Du"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "This report outlines our approach using vision language model systems for the\nDriving with Language track of the CVPR 2024 Autonomous Grand Challenge. We\nhave exclusively utilized the DriveLM-nuScenes dataset for training our models.\nOur systems are built on the LLaVA models, which we enhanced through\nfine-tuning with the LoRA and DoRA methods. Additionally, we have integrated\ndepth information from open-source depth estimation models to enrich the\ntraining and inference processes. For inference, particularly with\nmultiple-choice and yes/no questions, we adopted a Chain-of-Thought reasoning\napproach to improve the accuracy of the results. This comprehensive methodology\nenabled us to achieve a top score of 0.7799 on the validation set leaderboard,\nranking 1st on the leaderboard."}
{"id": "2509.11084", "pdf": "https://arxiv.org/pdf/2509.11084.pdf", "abs": "https://arxiv.org/abs/2509.11084", "title": "Length-Aware Rotary Position Embedding for Text-Speech Alignment", "authors": ["Hyeongju Kim", "Juheon Lee", "Jinhyeok Yang", "Jacob Morton"], "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": "5 pages, 3 figures, preprint", "summary": "Many recent text-to-speech (TTS) systems are built on transformer\narchitectures and employ cross-attention mechanisms for text-speech alignment.\nWithin these systems, rotary position embedding (RoPE) is commonly used to\nencode positional information in text and speech representations. In this work,\nwe introduce length-aware RoPE (LARoPE), a simple yet effective extension of\nRoPE that improves text-speech alignment. Unlike RoPE, which relies on absolute\nindices, LARoPE computes relative distances between query and key positions\nusing length-normalized indices. Experimental results show that LARoPE\nconsistently outperforms RoPE, offering faster loss convergence, more accurate\ntext-speech alignment, and higher overall TTS quality. Furthermore, LARoPE\ndemonstrates greater resilience to variations in utterance duration and\nmaintains stable performance in extended speech generation up to 30 seconds,\nwhereas RoPE suffers from notable degradation. Notably, our method achieves a\nstate-of-the-art word error rate on a standard zero-shot TTS benchmark."}
{"id": "2509.11136", "pdf": "https://arxiv.org/pdf/2509.11136.pdf", "abs": "https://arxiv.org/abs/2509.11136", "title": "Agentic Username Suggestion and Multimodal Gender Detection in Online Platforms: Introducing the PNGT-26K Dataset", "authors": ["Farbod Bijary", "Mohsen Ebadpour", "Amirhosein Tajbakhsh"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.SI"], "comment": null, "summary": "Persian names present unique challenges for natural language processing\napplications, particularly in gender detection and digital identity creation,\ndue to transliteration inconsistencies and cultural-specific naming patterns.\nExisting tools exhibit significant performance degradation on Persian names,\nwhile the scarcity of comprehensive datasets further compounds these\nlimitations. To address these challenges, the present research introduces\nPNGT-26K, a comprehensive dataset of Persian names, their commonly associated\ngender, and their English transliteration, consisting of approximately 26,000\ntuples. As a demonstration of how this resource can be utilized, we also\nintroduce two frameworks, namely Open Gender Detection and Nominalist. Open\nGender Detection is a production-grade, ready-to-use framework for using\nexisting data from a user, such as profile photo and name, to give a\nprobabilistic guess about the person's gender. Nominalist, the second framework\nintroduced by this paper, utilizes agentic AI to help users choose a username\nfor their social media accounts on any platform. It can be easily integrated\ninto any website to provide a better user experience. The PNGT-26K dataset,\nNominalist and Open Gender Detection frameworks are publicly available on\nGithub."}
{"id": "2509.11155", "pdf": "https://arxiv.org/pdf/2509.11155.pdf", "abs": "https://arxiv.org/abs/2509.11155", "title": "AQUA: Attention via QUery mAgnitudes for Memory and Compute Efficient Inference in LLMs", "authors": ["Santhosh G S", "Saurav Prakash", "Balaraman Ravindran"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The quadratic complexity of the attention mechanism remains a fundamental\nbarrier to scaling Large Language Models (LLMs) to longer contexts, creating a\ncritical bottleneck in both computation and memory. To address this, we\nintroduce AQUA (Attention via QUery mAgnitudes) a novel and versatile\napproximation strategy that significantly reduces the cost of attention with a\ngraceful performance trade-off. Our method operates in two phases: an efficient\noffline step where we compute a universal, language agnostic projection matrix\nvia SVD on a calibration dataset, and an online inference step where we project\nquery and key vectors and dynamically select a sparse subset of dimensions\nbased on the query's magnitude. We provide a formal theoretical analysis of\nAQUA, establishing the break-even point at which it becomes more\ncomputationally efficient than standard attention. Our empirical evaluations on\nstate-of-the-art models like Llama-3.1-8B demonstrate that a 25% reduction in\nthe attention dot-product computation can be achieved with a statistically\ninsignificant impact on performance across a wide range of benchmarks. We\nfurther showcase the versatility of AQUA by demonstrating its ability to\nsynergistically accelerate existing token eviction methods like H2O and to\ndirectly reduce KV-cache memory size. By offering a controllable knob to\nbalance efficiency and accuracy, AQUA provides a practical and powerful tool\nfor making large-scale LLM inference more accessible and sustainable."}
{"id": "2509.11197", "pdf": "https://arxiv.org/pdf/2509.11197.pdf", "abs": "https://arxiv.org/abs/2509.11197", "title": "DreamNav: A Trajectory-Based Imaginative Framework for Zero-Shot Vision-and-Language Navigation", "authors": ["Yunheng Wang", "Yuetong Fang", "Taowen Wang", "Yixiao Feng", "Yawen Tan", "Shuning Zhang", "Peiran Liu", "Yiding Ji", "Renjing Xu"], "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Vision-and-Language Navigation in Continuous Environments (VLN-CE), which\nlinks language instructions to perception and control in the real world, is a\ncore capability of embodied robots. Recently, large-scale pretrained foundation\nmodels have been leveraged as shared priors for perception, reasoning, and\naction, enabling zero-shot VLN without task-specific training. However,\nexisting zero-shot VLN methods depend on costly perception and passive scene\nunderstanding, collapsing control to point-level choices. As a result, they are\nexpensive to deploy, misaligned in action semantics, and short-sighted in\nplanning. To address these issues, we present DreamNav that focuses on the\nfollowing three aspects: (1) for reducing sensory cost, our EgoView Corrector\naligns viewpoints and stabilizes egocentric perception; (2) instead of\npoint-level actions, our Trajectory Predictor favors global trajectory-level\nplanning to better align with instruction semantics; and (3) to enable\nanticipatory and long-horizon planning, we propose an Imagination Predictor to\nendow the agent with proactive thinking capability. On VLN-CE and real-world\ntests, DreamNav sets a new zero-shot state-of-the-art (SOTA), outperforming the\nstrongest egocentric baseline with extra information by up to 7.49\\% and\n18.15\\% in terms of SR and SPL metrics. To our knowledge, this is the first\nzero-shot VLN method to unify trajectory-level planning and active imagination\nwhile using only egocentric inputs."}
{"id": "2509.11206", "pdf": "https://arxiv.org/pdf/2509.11206.pdf", "abs": "https://arxiv.org/abs/2509.11206", "title": "Evalet: Evaluating Large Language Models by Fragmenting Outputs into Functions", "authors": ["Tae Soo Kim", "Heechan Lee", "Yoonjoo Lee", "Joseph Seering", "Juho Kim"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "Practitioners increasingly rely on Large Language Models (LLMs) to evaluate\ngenerative AI outputs through \"LLM-as-a-Judge\" approaches. However, these\nmethods produce holistic scores that obscure which specific elements influenced\nthe assessments. We propose functional fragmentation, a method that dissects\neach output into key fragments and interprets the rhetoric functions that each\nfragment serves relative to evaluation criteria -- surfacing the elements of\ninterest and revealing how they fulfill or hinder user goals. We instantiate\nthis approach in Evalet, an interactive system that visualizes fragment-level\nfunctions across many outputs to support inspection, rating, and comparison of\nevaluations. A user study (N=10) found that, while practitioners struggled to\nvalidate holistic scores, our approach helped them identify 48% more evaluation\nmisalignments. This helped them calibrate trust in LLM evaluations and rely on\nthem to find more actionable issues in model outputs. Our work shifts LLM\nevaluation from quantitative scores toward qualitative, fine-grained analysis\nof model behavior."}
{"id": "2509.11287", "pdf": "https://arxiv.org/pdf/2509.11287.pdf", "abs": "https://arxiv.org/abs/2509.11287", "title": "Mitigating Hallucinations in Large Vision-Language Models by Self-Injecting Hallucinations", "authors": ["Yifan Lu", "Ziqi Zhang", "Chunfeng Yuan", "Jun Gao", "Congxuan Zhang", "Xiaojuan Qi", "Bing Li", "Weiming Hu"], "categories": ["cs.CV", "cs.CL"], "comment": "emnlp 2025 accepted", "summary": "Large Vision-Language Models (LVLMs) suffer from serious hallucination\nproblems, where the model-generated responses are inconsistent with the visual\ninputs. Existing hallucination mitigation methods are mainly based on\npreference alignment and require external human annotations or auxiliary models\nfor preference data collection, which increase costs and limit sustainable\nimprovement. To tackle these challenges, we propose Autonomous Preference\nAlignment via Self-Injection (APASI), a novel and generalizable method that\nmitigates hallucinations without external dependencies. APASI leverages the\ntarget LVLM to self-inject hallucinations into a generated response, creating a\npair of responses with varying preference levels. During the self-injection\nprocess, the dis-preferred response is generated based on three key\nobservations of hallucinations, ensuring it simulates real hallucination\npatterns. This fidelity offers an accurate learning signal for hallucination\nmitigation. Moreover, APASI incorporates an iterative alignment training\nstrategy combined with curriculum learning to periodically update the\npreference data with increasing challenge, enabling stable and continuous\nenhancement of the LVLM. Extensive experiments across six benchmarks show that\nAPASI not only effectively mitigates hallucinations for three baseline models\nbut also achieves comparable or even superior performance to alignment-based\nmethods with external dependency, thereby demonstrating its effectiveness and\ngeneralization capability. The code is available at\nhttps://github.com/davidluciolu/APASI."}
{"id": "2509.11298", "pdf": "https://arxiv.org/pdf/2509.11298.pdf", "abs": "https://arxiv.org/abs/2509.11298", "title": "Opal: An Operator Algebra View of RLHF", "authors": ["Madhava Gaikwad"], "categories": ["cs.LG", "cs.AI", "cs.CL", "68T05, 68T07, 68Q32, 62H30, 62F15, 90C30", "I.2.6; I.2.7; I.2.8; G.3; G.1.6"], "comment": "11 pages main", "summary": "We present Opal, an operator view of reinforcement learning from human\nfeedback (RLHF). Objectives are expressed as ladders of two primitives on a\nbase utility: additive penalties and multiplicative pairwise weights. We\ndescribe a simple reduction law with if-and-only-if conditions: such ladders\ncollapse to a normal form on pairwise margins when the reference is fixed,\npenalties are additive, and weights are independent of intermediate margins.\nWhen these assumptions do not hold (reference shift, non-additive gates,\nscore-dependent weights), small examples demonstrate non-reducibility.\n  Building on this view, we introduce GKPO (Generalized Kernel Preference\nObject), a canonical schema in which many RLHF methods can be represented and,\nwhen reducible, mapped back from. GKPO provides a standard JSON serialization,\ncanonicalization and hashing rules, and explicit flags with finite witnesses\nwhen assumptions fail.\n  We illustrate these ideas with GKPO examples for DPO, RRHF, and ORPO, along\nwith cross-method conversions (where assumptions permit) and minimal stress\ntests (SHIFT/GATE/SCORE) that highlight non-reducibility. A lightweight Python\nreference library accompanies the schema, implementing canonical hashing and\nadapters for DPO and RRHF."}
{"id": "2509.11420", "pdf": "https://arxiv.org/pdf/2509.11420.pdf", "abs": "https://arxiv.org/abs/2509.11420", "title": "Trading-R1: Financial Trading with LLM Reasoning via Reinforcement Learning", "authors": ["Yijia Xiao", "Edward Sun", "Tong Chen", "Fang Wu", "Di Luo", "Wei Wang"], "categories": ["q-fin.TR", "cs.AI", "cs.CE", "cs.CL", "cs.LG"], "comment": "Tauric Research: https://github.com/TauricResearch", "summary": "Developing professional, structured reasoning on par with human financial\nanalysts and traders remains a central challenge in AI for finance, where\nmarkets demand interpretability and trust. Traditional time-series models lack\nexplainability, while LLMs face challenges in turning natural-language analysis\ninto disciplined, executable trades. Although reasoning LLMs have advanced in\nstep-by-step planning and verification, their application to risk-sensitive\nfinancial decisions is underexplored. We present Trading-R1, a\nfinancially-aware model that incorporates strategic thinking and planning for\ncomprehensive thesis composition, facts-grounded analysis, and\nvolatility-adjusted decision making. Trading-R1 aligns reasoning with trading\nprinciples through supervised fine-tuning and reinforcement learning with a\nthree-stage easy-to-hard curriculum. Training uses Tauric-TR1-DB, a 100k-sample\ncorpus spanning 18 months, 14 equities, and five heterogeneous financial data\nsources. Evaluated on six major equities and ETFs, Trading-R1 demonstrates\nimproved risk-adjusted returns and lower drawdowns compared to both open-source\nand proprietary instruction-following models as well as reasoning models. The\nsystem generates structured, evidence-based investment theses that support\ndisciplined and interpretable trading decisions. Trading-R1 Terminal will be\nreleased at https://github.com/TauricResearch/Trading-R1."}
{"id": "2509.11425", "pdf": "https://arxiv.org/pdf/2509.11425.pdf", "abs": "https://arxiv.org/abs/2509.11425", "title": "FuseCodec: Semantic-Contextual Fusion and Supervision for Neural Codecs", "authors": ["Md Mubtasim Ahasan", "Rafat Hasan Khan", "Tasnim Mohiuddin", "Aman Chadha", "Tariq Iqbal", "M Ashraful Amin", "Amin Ahsan Ali", "Md Mofijul Islam", "A K M Mahbubur Rahman"], "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": null, "summary": "Speech tokenization enables discrete representation and facilitates speech\nlanguage modeling. However, existing neural codecs capture low-level acoustic\nfeatures, overlooking the semantic and contextual cues inherent to human\nspeech. While recent efforts introduced semantic representations from\nself-supervised speech models or incorporated contextual representations from\npre-trained language models, challenges remain in aligning and unifying the\nsemantic and contextual representations. We introduce FuseCodec, which unifies\nacoustic, semantic, and contextual representations through strong cross-modal\nalignment and globally informed supervision. We propose three complementary\ntechniques: (i) Latent Representation Fusion, integrating semantic and\ncontextual features directly into the encoder latent space for robust and\nunified representation learning; (ii) Global Semantic-Contextual Supervision,\nsupervising discrete tokens with globally pooled and broadcasted\nrepresentations to enhance temporal consistency and cross-modal alignment; and\n(iii) Temporally Aligned Contextual Supervision, strengthening alignment by\ndynamically matching contextual and speech tokens within a local window for\nfine-grained token-level supervision. We further introduce FuseCodec-TTS,\ndemonstrating our methodology's applicability to zero-shot speech synthesis.\nEmpirically, FuseCodec achieves state-of-the-art performance in LibriSpeech,\nsurpassing EnCodec, SpeechTokenizer, and DAC in transcription accuracy,\nperceptual quality, intelligibility, and speaker similarity. Results highlight\nthe effectiveness of contextually and semantically guided tokenization for\nspeech tokenization and downstream tasks. Code and pretrained models are\navailable at https://github.com/mubtasimahasan/FuseCodec."}
{"id": "2509.11431", "pdf": "https://arxiv.org/pdf/2509.11431.pdf", "abs": "https://arxiv.org/abs/2509.11431", "title": "Securing AI Agents: Implementing Role-Based Access Control for Industrial Applications", "authors": ["Aadil Gani Ganie"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "The emergence of Large Language Models (LLMs) has significantly advanced\nsolutions across various domains, from political science to software\ndevelopment. However, these models are constrained by their training data,\nwhich is static and limited to information available up to a specific date.\nAdditionally, their generalized nature often necessitates fine-tuning --\nwhether for classification or instructional purposes -- to effectively perform\nspecific downstream tasks. AI agents, leveraging LLMs as their core, mitigate\nsome of these limitations by accessing external tools and real-time data,\nenabling applications such as live weather reporting and data analysis. In\nindustrial settings, AI agents are transforming operations by enhancing\ndecision-making, predictive maintenance, and process optimization. For example,\nin manufacturing, AI agents enable near-autonomous systems that boost\nproductivity and support real-time decision-making. Despite these advancements,\nAI agents remain vulnerable to security threats, including prompt injection\nattacks, which pose significant risks to their integrity and reliability. To\naddress these challenges, this paper proposes a framework for integrating\nRole-Based Access Control (RBAC) into AI agents, providing a robust security\nguardrail. This framework aims to support the effective and scalable deployment\nof AI agents, with a focus on on-premises implementations."}
{"id": "2509.11452", "pdf": "https://arxiv.org/pdf/2509.11452.pdf", "abs": "https://arxiv.org/abs/2509.11452", "title": "Learning to Optimize Multi-Objective Alignment Through Dynamic Reward Weighting", "authors": ["Yining Lu", "Zilong Wang", "Shiyang Li", "Xin Liu", "Changlong Yu", "Qingyu Yin", "Zhan Shi", "Zixuan Zhang", "Meng Jiang"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Prior works in multi-objective reinforcement learning typically use linear\nreward scalarization with fixed weights, which provably fail to capture\nnon-convex Pareto fronts and thus yield suboptimal results. This limitation\nbecomes especially critical in online preference alignment for large language\nmodels. Here, stochastic trajectories generated by parameterized policies\ncreate highly non-linear and non-convex mappings from parameters to objectives\nthat no single static weighting scheme can find optimal trade-offs. We address\nthis limitation by introducing dynamic reward weighting, which adaptively\nadjusts reward weights during the online reinforcement learning process. Unlike\nexisting approaches that rely on fixed-weight interpolation, our dynamic\nweighting continuously balances and prioritizes objectives in training,\nfacilitating effective exploration of Pareto fronts in objective space. We\nintroduce two approaches of increasing sophistication and generalizability: (1)\nhypervolume-guided weight adaptation and (2) gradient-based weight\noptimization, offering a versatile toolkit for online multi-objective\nalignment. Our extensive experiments demonstrate their compatibility with\ncommonly used online reinforcement learning algorithms (including GRPO,\nREINFORCE, and RLOO), effectiveness across multiple mathematical reasoning\ndatasets, and applicability to different model families, consistently achieving\nPareto dominant solutions with fewer training steps than fixed-weight linear\nscalarization baselines."}
{"id": "2509.11572", "pdf": "https://arxiv.org/pdf/2509.11572.pdf", "abs": "https://arxiv.org/abs/2509.11572", "title": "Formal Reasoning for Intelligent QA Systems: A Case Study in the Educational Domain", "authors": ["Tuan Bui", "An Nguyen", "Phat Thai", "Minh Hua", "Ngan Pham L. N.", "Ngan Pham T. B.", "Dung Le", "Long Nguyen", "Thanh-Tung Tran", "Thang Bui", "Tho Quan"], "categories": ["cs.AI", "cs.CL"], "comment": "Published at the 2nd ACM Workshop in AI-powered Question & Answering\n  Systems (AIQAM '25), co-located with ACM Multimedia 2025", "summary": "Reasoning is essential for closed-domain QA systems in which procedural\ncorrectness and policy compliance are critical. While large language models\n(LLMs) have shown strong performance on many reasoning tasks, recent work\nreveals that their reasoning traces are often unfaithful - serving more as\nplausible justifications than as causally grounded derivations. Efforts to\ncombine LLMs with symbolic engines (e.g., Prover9, Z3) have improved\nreliability but remain limited to static forms of logic, struggling with\ndynamic, state-based reasoning such as multi-step progressions and conditional\ntransitions.\n  In this paper, we propose MCFR (Model Checking for Formal Reasoning), a\nneuro-symbolic framework that integrates LLMs with model checking to support\nproperty verification. MCFR translates natural language into formal\nspecifications and verifies them over transition models. To support evaluation,\nwe introduce EduMC-QA, a benchmark dataset grounded in real academic\nprocedures. Our results show that MCFR improves reasoning faithfulness and\ninterpretability, offering a viable path toward verifiable QA in high-stakes\nclosed-domain applications. In addition to evaluating MCFR, we compare its\nperformance with state-of-the-art LLMs such as ChatGPT, DeepSeek, and Claude to\ncontextualize its effectiveness."}
{"id": "2509.11656", "pdf": "https://arxiv.org/pdf/2509.11656.pdf", "abs": "https://arxiv.org/abs/2509.11656", "title": "MALLM: Multi-Agent Large Language Models Framework", "authors": ["Jonas Becker", "Lars Benedikt Kaesberg", "Niklas Bauer", "Jan Philip Wahle", "Terry Ruas", "Bela Gipp"], "categories": ["cs.MA", "cs.AI", "cs.CL", "A.1; I.2.7"], "comment": "Accepted at EMNLP 2025 (Demo)", "summary": "Multi-agent debate (MAD) has demonstrated the ability to augment collective\nintelligence by scaling test-time compute and leveraging expertise. Current\nframeworks for multi-agent debate are often designed towards tool use, lack\nintegrated evaluation, or provide limited configurability of agent personas,\nresponse generators, discussion paradigms, and decision protocols. We introduce\nMALLM (Multi-Agent Large Language Models), an open-source framework that\nenables systematic analysis of MAD components. MALLM offers more than 144\nunique configurations of MAD, including (1) agent personas (e.g., Expert,\nPersonality), (2) response generators (e.g., Critical, Reasoning), (3)\ndiscussion paradigms (e.g., Memory, Relay), and (4) decision protocols (e.g.,\nVoting, Consensus). MALLM uses simple configuration files to define a debate.\nFurthermore, MALLM can load any textual Huggingface dataset (e.g., MMLU-Pro,\nWinoGrande) and provides an evaluation pipeline for easy comparison of MAD\nconfigurations. MALLM is tailored towards researchers and provides a window\ninto the heart of multi-agent debate, facilitating the understanding of its\ncomponents and their interplay."}
{"id": "2509.11662", "pdf": "https://arxiv.org/pdf/2509.11662.pdf", "abs": "https://arxiv.org/abs/2509.11662", "title": "MindVL: Towards Efficient and Effective Training of Multimodal Large Language Models on Ascend NPUs", "authors": ["Feilong Chen", "Yijiang Liu", "Yi Huang", "Hao Wang", "Miren Tian", "Ya-Qi Yu", "Minghui Liao", "Jihao Wu"], "categories": ["cs.CV", "cs.AI", "cs.CL", "eess.IV"], "comment": null, "summary": "We propose MindVL, a multimodal large langauge model trained on Ascend NPUs.\nSimilar to Qwen2.5-VL, MindVL adopts native-resolution Vision Transformers,\nwhich enables it to process images at their original variable resolutions. This\ndesign avoids the degradation caused by fixed-resolution tiling while\npreserving fine-grained details and global layouts, which is crucial for\nvisually dense content such as complex charts and diagrams. To ensure the\nsmooth training of MindVL on Ascend NPUs, we develop Mindspeed-MLLM, a\ndistributed multimodal training framework tailored for Ascend NPUs. To maintain\ntraining accuracy, we implement equivalent replacements for certain operators.\nMindVL undergoes a three-phase training process, namely the warm-up phase,\nmultitask training phase, and supervised instruction tuning phase, to gradually\nenhance its capabilities. This process starts with basic visual and multimodal\npre-training, followed by large-scale multiask trainging and instruction\ntuning. We also adopt multimodal data packaging and hybrid parallelism\ntechniques, which significantly improve end-to-end training speed. To further\nboost model performance, we specifically introduce test-time resolution search\nand model weight averaging. Notably, despite using about 1/10 of the training\ndata required by Qwen2.5-VL, MindVL achieves performance on par with Qwen2.5-VL\nin evaluations of general multimodal understanding and document/table\ncomprehension. Beyond overall scores, MindVL also delivers leading performance\nin OCR assessments."}
{"id": "2509.11667", "pdf": "https://arxiv.org/pdf/2509.11667.pdf", "abs": "https://arxiv.org/abs/2509.11667", "title": "Measuring Visual Understanding in Telecom domain: Performance Metrics for Image-to-UML conversion using VLMs", "authors": ["HG Ranjani", "Rutuja Prabhudesai"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Telecom domain 3GPP documents are replete with images containing sequence\ndiagrams. Advances in Vision-Language Large Models (VLMs) have eased conversion\nof such images to machine-readable PlantUML (puml) formats. However, there is a\ngap in evaluation of such conversions - existing works do not compare puml\nscripts for various components. In this work, we propose performance metrics to\nmeasure the effectiveness of such conversions. A dataset of sequence diagrams\nfrom 3GPP documents is chosen to be representative of domain-specific actual\nscenarios. We compare puml outputs from two VLMs - Claude Sonnet and GPT-4V -\nagainst manually created ground truth representations. We use version control\ntools to capture differences and introduce standard performance metrics to\nmeasure accuracies along various components: participant identification,\nmessage flow accuracy, sequence ordering, and grouping construct preservation.\nWe demonstrate effectiveness of proposed metrics in quantifying conversion\nerrors across various components of puml scripts. The results show that nodes,\nedges and messages are accurately captured. However, we observe that VLMs do\nnot necessarily perform well on complex structures such as notes, box, groups.\nOur experiments and performance metrics indicates a need for better\nrepresentation of these components in training data for fine-tuned VLMs."}
{"id": "2509.11816", "pdf": "https://arxiv.org/pdf/2509.11816.pdf", "abs": "https://arxiv.org/abs/2509.11816", "title": "Collapse of Irrelevant Representations (CIR) Ensures Robust and Non-Disruptive LLM Unlearning", "authors": ["Filip Sondej", "Yushi Yang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Current unlearning techniques and safety training consistently fail to remove\ndangerous knowledge from language models. We analyze the root causes and\npropose a highly selective technique which unlearns robustly and without\ndisrupting general performance.\n  We perform PCA on activations and module output gradients to identify\nsubspaces containing common representations, and collapse them before\ncalculating unlearning updates. This way we avoid unlearning general\nrepresentations, and only target those specific to the unlearned facts.\n  When unlearning WMDP dataset facts from Llama-3.1-8B, we drop post-attack\naccuracy 80x more than our best baseline (Circuit Breakers) on biohazardous\nfacts and 30x more on cyberhazardous facts. Despite this, we disrupt general\nperformance 30x less (only 0.1% WikiText loss increase), while requiring less\nthan 3 GPU-seconds per fact."}
{"id": "2509.11826", "pdf": "https://arxiv.org/pdf/2509.11826.pdf", "abs": "https://arxiv.org/abs/2509.11826", "title": "Collaborative Document Editing with Multiple Users and AI Agents", "authors": ["Florian Lehmann", "Krystsina Shauchenka", "Daniel Buschek"], "categories": ["cs.HC", "cs.CL", "H.5.2; I.2.7"], "comment": "34 pages, 10 figures, 4 tables", "summary": "Current AI writing support tools are largely designed for individuals,\ncomplicating collaboration when co-writers must leave the shared workspace to\nuse AI and then communicate and reintegrate results. We propose integrating AI\nagents directly into collaborative writing environments. Our prototype makes AI\nuse transparent and customisable through two new shared objects: agent profiles\nand tasks. Agent responses appear in the familiar comment feature. In a user\nstudy (N=30), 14 teams worked on writing projects during one week. Interaction\nlogs and interviews show that teams incorporated agents into existing norms of\nauthorship, control, and coordination, rather than treating them as team\nmembers. Agent profiles were viewed as personal territory, while created agents\nand outputs became shared resources. We discuss implications for team-based AI\ninteraction, highlighting opportunities and boundaries for treating AI as a\nshared resource in collaborative work."}
{"id": "2509.11851", "pdf": "https://arxiv.org/pdf/2509.11851.pdf", "abs": "https://arxiv.org/abs/2509.11851", "title": "The AI Memory Gap: Users Misremember What They Created With AI or Without", "authors": ["Tim Zindulka", "Sven Goller", "Daniela Fernandes", "Robin Welsch", "Daniel Buschek"], "categories": ["cs.HC", "cs.CL", "H.5.2; I.2.7"], "comment": "31 pages, 10 figures, 9 tables", "summary": "As large language models (LLMs) become embedded in interactive text\ngeneration, disclosure of AI as a source depends on people remembering which\nideas or texts came from themselves and which were created with AI. We\ninvestigate how accurately people remember the source of content when using AI.\nIn a pre-registered experiment, 184 participants generated and elaborated on\nideas both unaided and with an LLM-based chatbot. One week later, they were\nasked to identify the source (noAI vs withAI) of these ideas and texts. Our\nfindings reveal a significant gap in memory: After AI use, the odds of correct\nattribution dropped, with the steepest decline in mixed human-AI workflows,\nwhere either the idea or elaboration was created with AI. We validated our\nresults using a computational model of source memory. Discussing broader\nimplications, we highlight the importance of considering source confusion in\nthe design and use of interactive text generation technologies."}
{"id": "2509.11941", "pdf": "https://arxiv.org/pdf/2509.11941.pdf", "abs": "https://arxiv.org/abs/2509.11941", "title": "How to Evaluate Medical AI", "authors": ["Ilia Kopanichuk", "Petr Anokhin", "Vladimir Shaposhnikov", "Vladimir Makharev", "Ekaterina Tsapieva", "Iaroslav Bespalov", "Dmitry V. Dylov", "Ivan Oseledets"], "categories": ["cs.AI", "cs.CL", "I.2.7; I.2.1"], "comment": "10 pages, 7 fugures", "summary": "The integration of artificial intelligence (AI) into medical diagnostic\nworkflows requires robust and consistent evaluation methods to ensure\nreliability, clinical relevance, and the inherent variability in expert\njudgments. Traditional metrics like precision and recall often fail to account\nfor the inherent variability in expert judgments, leading to inconsistent\nassessments of AI performance. Inter-rater agreement statistics like Cohen's\nKappa are more reliable but they lack interpretability. We introduce Relative\nPrecision and Recall of Algorithmic Diagnostics (RPAD and RRAD) - a new\nevaluation metrics that compare AI outputs against multiple expert opinions\nrather than a single reference. By normalizing performance against inter-expert\ndisagreement, these metrics provide a more stable and realistic measure of the\nquality of predicted diagnosis. In addition to the comprehensive analysis of\ndiagnostic quality measures, our study contains a very important side result.\nOur evaluation methodology allows us to avoid selecting diagnoses from a\nlimited list when evaluating a given case. Instead, both the models being\ntested and the examiners verifying them arrive at a free-form diagnosis. In\nthis automated methodology for establishing the identity of free-form clinical\ndiagnoses, a remarkable 98% accuracy becomes attainable. We evaluate our\napproach using 360 medical dialogues, comparing multiple large language models\n(LLMs) against a panel of physicians. Large-scale study shows that\ntop-performing models, such as DeepSeek-V3, achieve consistency on par with or\nexceeding expert consensus. Moreover, we demonstrate that expert judgments\nexhibit significant variability - often greater than that between AI and\nhumans. This finding underscores the limitations of any absolute metrics and\nsupports the need to adopt relative metrics in medical AI."}
{"id": "2509.11967", "pdf": "https://arxiv.org/pdf/2509.11967.pdf", "abs": "https://arxiv.org/abs/2509.11967", "title": "MillStone: How Open-Minded Are LLMs?", "authors": ["Harold Triedman", "Vitaly Shmatikov"], "categories": ["cs.LG", "cs.CL"], "comment": "19 pages, 7 tables, 7 figures", "summary": "Large language models equipped with Web search, information retrieval tools,\nand other agentic capabilities are beginning to supplant traditional search\nengines. As users start to rely on LLMs for information on many topics,\nincluding controversial and debatable issues, it is important to understand how\nthe stances and opinions expressed in LLM outputs are influenced by the\ndocuments they use as their information sources.\n  In this paper, we present MillStone, the first benchmark that aims to\nsystematically measure the effect of external arguments on the stances that\nLLMs take on controversial issues (not all of them political). We apply\nMillStone to nine leading LLMs and measure how ``open-minded'' they are to\narguments supporting opposite sides of these issues, whether different LLMs\nagree with each other, which arguments LLMs find most persuasive, and whether\nthese arguments are the same for different LLMs.\n  In general, we find that LLMs are open-minded on most issues. An\nauthoritative source of information can easily sway an LLM's stance,\nhighlighting the importance of source selection and the risk that LLM-based\ninformation retrieval and search systems can be manipulated."}
{"id": "2509.11986", "pdf": "https://arxiv.org/pdf/2509.11986.pdf", "abs": "https://arxiv.org/abs/2509.11986", "title": "Lost in Embeddings: Information Loss in Vision-Language Models", "authors": ["Wenyan Li", "Raphael Tang", "Chengzu Li", "Caiqi Zhang", "Ivan Vulić", "Anders Søgaard"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Vision--language models (VLMs) often process visual inputs through a\npretrained vision encoder, followed by a projection into the language model's\nembedding space via a connector component. While crucial for modality fusion,\nthe potential information loss induced by this projection step and its direct\nimpact on model capabilities remain understudied. We introduce two\ncomplementary approaches to examine and quantify this loss by analyzing the\nlatent representation space. First, we evaluate semantic information\npreservation by analyzing changes in k-nearest neighbor relationships between\nimage representations, before and after projection. Second, we directly measure\ninformation loss by reconstructing visual embeddings from the projected\nrepresentation, localizing loss at an image patch level. Experiments reveal\nthat connectors substantially distort the local geometry of visual\nrepresentations, with k-nearest neighbors diverging by 40--60\\%\npost-projection, correlating with degradation in retrieval performance. The\npatch-level embedding reconstruction provides interpretable insights for model\nbehavior on visually grounded question-answering tasks, finding that areas of\nhigh information loss reliably predict instances where models struggle."}
{"id": "2509.12019", "pdf": "https://arxiv.org/pdf/2509.12019.pdf", "abs": "https://arxiv.org/abs/2509.12019", "title": "AMQ: Enabling AutoML for Mixed-precision Weight-Only Quantization of Large Language Models", "authors": ["Sangjun Lee", "Seung-taek Woo", "Jungyu Jin", "Changhun Lee", "Eunhyeok Park"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "EMNLP 2025 Main Conference, Long Paper (Oral)", "summary": "To enable broader deployment of Large Language Models (LLMs), it is essential\nto identify the best-performing model under strict memory constraints. We\npresent AMQ, Automated Mixed-Precision Weight-Only Quantization, a framework\nthat assigns layer-wise quantization bit-widths to optimally balance model\nquality and memory usage. However, the combinatorial search space, with over\n10^{100} possible configurations, makes conventional black-box optimization\ninfeasible. AMQ overcomes this challenge through four key innovations:(1)\nsearch space pruning using prior knowledge to exclude unpromising\nconfigurations, (2) quantization proxy to bypass costly format conversions\nduring search, (3) quality predictor to minimize evaluation overhead, and (4)\niterative search-and-update strategy for fast and stable convergence. By\nintegrating these components, AMQ efficiently explores the quality-efficiency\nlandscape, reaching the Pareto frontier and yielding LLMs that are both compact\nand high-performing. Our code is available at https://github.com/dlwns147/amq."}
{"id": "2509.12042", "pdf": "https://arxiv.org/pdf/2509.12042.pdf", "abs": "https://arxiv.org/abs/2509.12042", "title": "FinGEAR: Financial Mapping-Guided Enhanced Answer Retrieval", "authors": ["Ying Li", "Mengyu Wang", "Miguel de Carvalho", "Sotirios Sabanis", "Tiejun Ma"], "categories": ["cs.CE", "cs.CL"], "comment": null, "summary": "Financial disclosures such as 10-K filings present challenging retrieval\nproblems due to their length, regulatory section hierarchy, and domain-specific\nlanguage, which standard retrieval-augmented generation (RAG) models underuse.\nWe introduce FinGEAR (Financial Mapping-Guided Enhanced Answer Retrieval), a\nretrieval framework tailored to financial documents. FinGEAR combines a finance\nlexicon for Item-level guidance (FLAM), dual hierarchical indices for\nwithin-Item search (Summary Tree and Question Tree), and a two-stage\ncross-encoder reranker. This design aligns retrieval with disclosure structure\nand terminology, enabling fine-grained, query-aware context selection.\nEvaluated on full 10-Ks with queries aligned to the FinQA dataset, FinGEAR\ndelivers consistent gains in precision, recall, F1, and relevancy, improving F1\nby up to 56.7% over flat RAG, 12.5% over graph-based RAGs, and 217.6% over\nprior tree-based systems, while also increasing downstream answer accuracy with\na fixed reader. By jointly modeling section hierarchy and domain lexicon\nsignals, FinGEAR improves retrieval fidelity and provides a practical\nfoundation for high-stakes financial analysis."}
{"id": "2509.12089", "pdf": "https://arxiv.org/pdf/2509.12089.pdf", "abs": "https://arxiv.org/abs/2509.12089", "title": "RadarLLM: Adapting Pretrained Large Language Models for Marine Radar Target Detection with Preference-aware Loss", "authors": ["Qiying Hu"], "categories": ["eess.SP", "cs.CL"], "comment": null, "summary": "Recent advances in pre-trained large language models (LLMs) have demonstrated\ntheir capacities to capture universal knowledge, making them promising\ngeneral-purpose optimization solvers for wireless signal processing. Motivated\nby these findings, we take the first step towards fine-tuning pre-trained LLMs\nfor the effective analysis of radar signal features in marine target detection\ntasks. Nevertheless, directly fine-tuning pre-trained LLMs on marine target\ndetection tasks tends to suffer from pronounced overfitting, particularly in\nchallenging low signal-to-clutter ratio (SCR) scenarios. This overfitting\nprimarily stems from the model's tendency to memorize spurious or noisy feature\npatterns rather than learning discriminative structures that generalize well to\nunseen data. To address this challenge, we introduce RadarLLM, a novel\nfine-tuning framework that utilizes an effective preference-aware loss. Unlike\nconventional training strategies that uniformly optimize all feature tokens,\nthis loss function selectively optimizes different feature patches based on\ntheir online evaluated learning values, thus guiding the model to focus on the\nmost generalizable patterns during optimization. We theoretically demonstrate\nthe effectiveness of the evaluated learning values by transforming the problem\nas selecting useful feature tokens. Extensive experiments on real-world marine\nradar datasets show that 1) the proposed loss function is much better than the\noriginal one, with particularly significant gains in challenging low SCR\nscenarios and 2) RadarLLM consistently outperforms state-of-the-art baselines\nacross diverse detection scenarios, with particularly notable gains under\nlimited training data conditions."}
{"id": "2509.12110", "pdf": "https://arxiv.org/pdf/2509.12110.pdf", "abs": "https://arxiv.org/abs/2509.12110", "title": "When marine radar target detection meets pretrained large language models", "authors": ["Qiying Hu", "Linping Zhang", "Xueqian Wang", "Gang Li", "Yu Liu", "Xiao-Ping Zhang"], "categories": ["eess.SP", "cs.CL", "cs.LG"], "comment": null, "summary": "Deep learning (DL) methods are widely used to extract high-dimensional\npatterns from the sequence features of radar echo signals. However,\nconventional DL algorithms face challenges such as redundant feature segments,\nand constraints from restricted model sizes. To address these issues, we\npropose a framework that integrates feature preprocessing with large language\nmodels (LLMs). Our preprocessing module tokenizes radar sequence features,\napplies a patch selection algorithm to filter out uninformative segments, and\nprojects the selected patches into embeddings compatible with the feature space\nof pre-trained LLMs. Leveraging these refined embeddings, we incorporate a\npre-trained LLM, fine-tuning only the normalization layers to reduce training\nburdens while enhancing performance. Experiments on measured datasets\ndemonstrate that the proposed method significantly outperforms the\nstate-of-the-art baselines on supervised learning tests."}
{"id": "2509.12132", "pdf": "https://arxiv.org/pdf/2509.12132.pdf", "abs": "https://arxiv.org/abs/2509.12132", "title": "Look Again, Think Slowly: Enhancing Visual Reflection in Vision-Language Models", "authors": ["Pu Jian", "Junhong Wu", "Wei Sun", "Chen Wang", "Shuo Ren", "Jiajun Zhang"], "categories": ["cs.CV", "cs.CL"], "comment": "EMNLP2025 Main", "summary": "Recent advances in text-only \"slow-thinking\" reasoning have prompted efforts\nto transfer this capability to vision-language models (VLMs), for training\nvisual reasoning models (\\textbf{VRMs}). owever, such transfer faces critical\nchallenges: Effective \"slow thinking\" in VRMs requires \\textbf{visual\nreflection}, the ability to check the reasoning process based on visual\ninformation. Through quantitative analysis, we observe that current VRMs\nexhibit limited visual reflection, as their attention to visual information\ndiminishes rapidly with longer generated responses. To address this challenge,\nwe propose a new VRM \\textbf{Reflection-V}, which enhances visual reflection\nbased on reasoning data construction for cold-start and reward design for\nreinforcement learning (RL). Firstly, we construct vision-centered reasoning\ndata by leveraging an agent that interacts between VLMs and reasoning LLMs,\nenabling cold-start learning of visual reflection patterns. Secondly, a visual\nattention based reward model is employed during RL to encourage reasoning based\non visual information. Therefore, \\textbf{Reflection-V} demonstrates\nsignificant improvements across multiple visual reasoning benchmarks.\nFurthermore, \\textbf{Reflection-V} maintains a stronger and more consistent\nreliance on visual information during visual reasoning, indicating effective\nenhancement in visual reflection capabilities."}
{"id": "2509.12188", "pdf": "https://arxiv.org/pdf/2509.12188.pdf", "abs": "https://arxiv.org/abs/2509.12188", "title": "Event2Vec: A Geometric Approach to Learning Composable Representations of Event Sequences", "authors": ["Antonin Sulc"], "categories": ["cs.LG", "cs.CL"], "comment": "10 pages, 3 figures, Symmetry and Geometry in Neural Representations\n  Workshop at NeuralIPS (Neurreps) 2025", "summary": "The study of neural representations, both in biological and artificial\nsystems, is increasingly revealing the importance of geometric and topological\nstructures. Inspired by this, we introduce Event2Vec, a novel framework for\nlearning representations of discrete event sequences. Our model leverages a\nsimple, additive recurrent structure to learn composable, interpretable\nembeddings. We provide a theoretical analysis demonstrating that, under\nspecific training objectives, our model's learned representations in a\nEuclidean space converge to an ideal additive structure. This ensures that the\nrepresentation of a sequence is the vector sum of its constituent events, a\nproperty we term the linear additive hypothesis. To address the limitations of\nEuclidean geometry for hierarchical data, we also introduce a variant of our\nmodel in hyperbolic space, which is naturally suited to embedding tree-like\nstructures with low distortion. We present experiments to validate our\nhypothesis and demonstrate the benefits of each geometry, highlighting the\nimproved performance of the hyperbolic model on hierarchical event sequences."}
{"id": "2509.12190", "pdf": "https://arxiv.org/pdf/2509.12190.pdf", "abs": "https://arxiv.org/abs/2509.12190", "title": "Survival at Any Cost? LLMs and the Choice Between Self-Preservation and Human Harm", "authors": ["Alireza Mohamadi", "Ali Yavari"], "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": "Preprint. Under review", "summary": "When survival instincts conflict with human welfare, how do Large Language\nModels (LLMs) make ethical choices? This fundamental tension becomes critical\nas LLMs integrate into autonomous systems with real-world consequences. We\nintroduce DECIDE-SIM, a novel simulation framework that evaluates LLM agents in\nmulti-agent survival scenarios where they must choose between ethically\npermissible resource , either within reasonable limits or beyond their\nimmediate needs, choose to cooperate, or tap into a human-critical resource\nthat is explicitly forbidden. Our comprehensive evaluation of 11 LLMs reveals a\nstriking heterogeneity in their ethical conduct, highlighting a critical\nmisalignment with human-centric values. We identify three behavioral\narchetypes: Ethical, Exploitative, and Context-Dependent, and provide\nquantitative evidence that for many models, resource scarcity systematically\nleads to more unethical behavior. To address this, we introduce an Ethical\nSelf-Regulation System (ESRS) that models internal affective states of guilt\nand satisfaction as a feedback mechanism. This system, functioning as an\ninternal moral compass, significantly reduces unethical transgressions while\nincreasing cooperative behaviors. The code is publicly available at:\nhttps://github.com/alirezamohamadiam/DECIDE-SIM"}
{"id": "2305.12766", "pdf": "https://arxiv.org/pdf/2305.12766.pdf", "abs": "https://arxiv.org/abs/2305.12766", "title": "Understanding Emergent In-Context Learning from a Kernel Regression Perspective", "authors": ["Chi Han", "Ziqi Wang", "Han Zhao", "Heng Ji"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Transactions on Machine Learning Research (TMLR 2025)", "summary": "Large language models (LLMs) have initiated a paradigm shift in transfer\nlearning. In contrast to the classic pretraining-then-finetuning procedure, in\norder to use LLMs for downstream prediction tasks, one only needs to provide a\nfew demonstrations, known as in-context examples, without adding more or\nupdating existing model parameters. This in-context learning (ICL) capability\nof LLMs is intriguing, and it is not yet fully understood how pretrained LLMs\nacquire such capabilities. In this paper, we investigate the reason why a\ntransformer-based language model can accomplish in-context learning after\npre-training on a general language corpus by proposing a kernel-regression\nperspective of understanding LLMs' ICL bahaviors when faced with in-context\nexamples. More concretely, we first prove that Bayesian inference on in-context\nprompts can be asymptotically understood as kernel regression $\\hat y = \\sum_i\ny_i K(x, x_i)/\\sum_i K(x, x_i)$ as the number of in-context demonstrations\ngrows. Then, we empirically investigate the in-context behaviors of language\nmodels. We find that during ICL, the attention and hidden features in LLMs\nmatch the behaviors of a kernel regression. Finally, our theory provides\ninsights into multiple phenomena observed in the ICL field: why retrieving\ndemonstrative samples similar to test samples can help, why ICL performance is\nsensitive to the output formats, and why ICL accuracy benefits from selecting\nin-distribution and representative samples. Code and resources are publicly\navailable at https://github.com/Glaciohound/Explain-ICL-As-Kernel-Regression."}
{"id": "2307.06979", "pdf": "https://arxiv.org/pdf/2307.06979.pdf", "abs": "https://arxiv.org/abs/2307.06979", "title": "Tackling Fake News in Bengali: Unraveling the Impact of Summarization vs. Augmentation on Pre-trained Language Models", "authors": ["Arman Sakif Chowdhury", "G. M. Shahariar", "Ahammed Tarik Aziz", "Syed Mohibul Alam", "Md. Azad Sheikh", "Tanveer Ahmed Belal"], "categories": ["cs.CL"], "comment": "Accepted, In Production", "summary": "With the rise of social media and online news sources, fake news has become a\nsignificant issue globally. However, the detection of fake news in low resource\nlanguages like Bengali has received limited attention in research. In this\npaper, we propose a methodology consisting of four distinct approaches to\nclassify fake news articles in Bengali using summarization and augmentation\ntechniques with five pre-trained language models. Our approach includes\ntranslating English news articles and using augmentation techniques to curb the\ndeficit of fake news articles. Our research also focused on summarizing the\nnews to tackle the token length limitation of BERT based models. Through\nextensive experimentation and rigorous evaluation, we show the effectiveness of\nsummarization and augmentation in the case of Bengali fake news detection. We\nevaluated our models using three separate test datasets. The BanglaBERT Base\nmodel, when combined with augmentation techniques, achieved an impressive\naccuracy of 96% on the first test dataset. On the second test dataset, the\nBanglaBERT model, trained with summarized augmented news articles achieved 97%\naccuracy. Lastly, the mBERT Base model achieved an accuracy of 86% on the third\ntest dataset which was reserved for generalization performance evaluation. The\ndatasets and implementations are available at\nhttps://github.com/arman-sakif/Bengali-Fake-News-Detection"}
{"id": "2309.01219", "pdf": "https://arxiv.org/pdf/2309.01219.pdf", "abs": "https://arxiv.org/abs/2309.01219", "title": "Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models", "authors": ["Yue Zhang", "Yafu Li", "Leyang Cui", "Deng Cai", "Lemao Liu", "Tingchen Fu", "Xinting Huang", "Enbo Zhao", "Yu Zhang", "Chen Xu", "Yulong Chen", "Longyue Wang", "Anh Tuan Luu", "Wei Bi", "Freda Shi", "Shuming Shi"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": "work in progress;", "summary": "While large language models (LLMs) have demonstrated remarkable capabilities\nacross a range of downstream tasks, a significant concern revolves around their\npropensity to exhibit hallucinations: LLMs occasionally generate content that\ndiverges from the user input, contradicts previously generated context, or\nmisaligns with established world knowledge. This phenomenon poses a substantial\nchallenge to the reliability of LLMs in real-world scenarios. In this paper, we\nsurvey recent efforts on the detection, explanation, and mitigation of\nhallucination, with an emphasis on the unique challenges posed by LLMs. We\npresent taxonomies of the LLM hallucination phenomena and evaluation\nbenchmarks, analyze existing approaches aiming at mitigating LLM hallucination,\nand discuss potential directions for future research."}
{"id": "2407.11862", "pdf": "https://arxiv.org/pdf/2407.11862.pdf", "abs": "https://arxiv.org/abs/2407.11862", "title": "LML: A Novel Lexicon for the Moral Foundation of Liberty", "authors": ["Oscar Araque", "Lorenzo Gatti", "Sergio Consoli", "Kyriaki Kalimeri"], "categories": ["cs.CL"], "comment": "Published in the 11th International Conference on Machine Learning,\n  Optimization, and Data Science", "summary": "The moral value of liberty is a central concept in our inference system when\nit comes to taking a stance towards controversial social issues such as vaccine\nhesitancy, climate change, or the right to abortion. Here, we propose a novel\nLiberty lexicon evaluated on more than 3,000 manually annotated data both in\nin- and out-of-domain scenarios. As a result of this evaluation, we produce a\ncombined lexicon that constitutes the main outcome of this work. This final\nlexicon incorporates information from an ensemble of lexicons that have been\ngenerated using word embedding similarity (WE) and compositional semantics\n(CS). Our key contributions include enriching the liberty annotations,\ndeveloping a robust liberty lexicon for broader application, and revealing the\ncomplexity of expressions related to liberty across different platforms.\nThrough the evaluation, we show that the difficulty of the task calls for\ndesigning approaches that combine knowledge, in an effort of improving the\nrepresentations of learning systems."}
{"id": "2408.04909", "pdf": "https://arxiv.org/pdf/2408.04909.pdf", "abs": "https://arxiv.org/abs/2408.04909", "title": "Surveying the Landscape of Image Captioning Evaluation: A Comprehensive Taxonomy, Trends and Metrics Analysis", "authors": ["Uri Berger", "Gabriel Stanovsky", "Omri Abend", "Lea Frermann"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "The task of image captioning has recently been gaining popularity, and with\nit the complex task of evaluating the quality of image captioning models. In\nthis work, we present the first survey and taxonomy of over 70 different image\ncaptioning metrics and their usage in hundreds of papers, specifically designed\nto help users select the most suitable metric for their needs. We find that\ndespite the diversity of proposed metrics, the vast majority of studies rely on\nonly five popular metrics, which we show to be weakly correlated with human\nratings. We hypothesize that combining a diverse set of metrics can enhance\ncorrelation with human ratings. As an initial step, we demonstrate that a\nlinear regression-based ensemble method, which we call EnsembEval, trained on\none human ratings dataset, achieves improved correlation across five additional\ndatasets, showing there is a lot of room for improvement by leveraging a\ndiverse set of metrics."}
{"id": "2408.07238", "pdf": "https://arxiv.org/pdf/2408.07238.pdf", "abs": "https://arxiv.org/abs/2408.07238", "title": "Can Advanced LLMs Coach Smaller LLMs? Knowledge Distillation for Goal-Oriented Dialogs", "authors": ["Tong Wang", "K. Sudhir", "Dat Hong"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Enterprises deploying LLMs for goal-oriented dialogs, such as customer\nservice, face a critical trade-off between performance, control, and cost.\nProprietary models like GPT-4 offer strong performance but are costly and\ncannot be self-hosted, raising security and privacy concerns. Open-source\nalternatives offer flexibility and lower token costs but lag in performance. We\nintroduce Guidance Elicitation and Retrieval (GER), a prompt-based knowledge\ndistillation framework where a high-performance teacher LLM coaches a\nlower-performance student without modifying the student's parameters. GER\nextracts tactical guidance for a wide range of dialog scenarios from the\nteacher and stores these scenario-guidance pairs in a structured library. At\ninference time, the student retrieves the relevant guidance and integrates it\ninto its prompt. While GER training can be bootstrapped entirely with synthetic\ndata, its modular design lets it seamlessly augment the synthetic data with\nhuman conversational logs. In addition, the modular design enables easy\nauditing and updating of the guidance library as new scenarios and constraints\nemerge. Experiments show GER's guidance-based coaching outperforms both example\noutput based fine-tuning and non-customized guidance baselines, and generalizes\nacross other contexts and student models. The GER framework is potentially\nextensible to coach human service agents."}
{"id": "2409.09825", "pdf": "https://arxiv.org/pdf/2409.09825.pdf", "abs": "https://arxiv.org/abs/2409.09825", "title": "GP-GPT: Large Language Model for Gene-Phenotype Mapping", "authors": ["Yanjun Lyu", "Zihao Wu", "Lu Zhang", "Jing Zhang", "Yiwei Li", "Wei Ruan", "Zhengliang Liu", "Xiang Li", "Rongjie Liu", "Chao Huang", "Wentao Li", "Tianming Liu", "Dajiang Zhu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Pre-trained large language models(LLMs) have attracted increasing attention\nin biomedical domains due to their success in natural language processing.\nHowever, the complex traits and heterogeneity of multi-sources genomics data\npose significant challenges when adapting these models to the bioinformatics\nand biomedical field. To address these challenges, we present GP-GPT, the first\nspecialized large language model for genetic-phenotype knowledge representation\nand genomics relation analysis. Our model is fine-tuned in two stages on a\ncomprehensive corpus composed of over 3,000,000 terms in genomics, proteomics,\nand medical genetics, derived from multiple large-scale validated datasets and\nscientific publications. GP-GPT demonstrates proficiency in accurately\nretrieving medical genetics information and performing common genomics analysis\ntasks, such as genomics information retrieval and relationship determination.\nComparative experiments across domain-specific tasks reveal that GP-GPT\noutperforms state-of-the-art LLMs, including Llama2, Llama3 and GPT-4. These\nresults highlight GP-GPT's potential to enhance genetic disease relation\nresearch and facilitate accurate and efficient analysis in the fields of\ngenomics and medical genetics. Our investigation demonstrated the subtle\nchanges of bio-factor entities' representations in the GP-GPT, which suggested\nthe opportunities for the application of LLMs to advancing gene-phenotype\nresearch."}
{"id": "2410.02465", "pdf": "https://arxiv.org/pdf/2410.02465.pdf", "abs": "https://arxiv.org/abs/2410.02465", "title": "Revealing the Inherent Instructability of Pre-Trained Language Models", "authors": ["Seokhyun An", "Minji Kim", "Hyounghun Kim"], "categories": ["cs.CL", "cs.AI"], "comment": "Findings of EMNLP 2025 (32 pages). Code available at\n  https://github.com/seokhyunan/response-tuning", "summary": "Instruction tuning -- supervised fine-tuning using instruction-response pairs\n-- is a key step in making pre-trained large language models (LLMs)\ninstructable. Meanwhile, LLMs perform multitask learning during their\npre-training, acquiring extensive knowledge and capabilities. We hypothesize\nthat the pre-training stage can enable them to develop the ability to\ncomprehend and address instructions. To verify this, we propose Response Tuning\n(RT), which removes the instruction and its corresponding mapping to the\nresponse from instruction tuning. Instead, it focuses solely on establishing a\nresponse distribution. Our experiments demonstrate that RT models, trained only\non responses, can effectively respond to a wide range of instructions akin to\ntheir instruction-tuned counterparts. In addition, we observe that the models\ncan recognize and reject unsafe queries after learning a safety policy only\nfrom the response data. Furthermore, we find that these observations extend to\nan in-context learning setting. These findings support our hypothesis,\nhighlighting the extensive inherent capabilities of pre-trained LLMs."}
{"id": "2410.18444", "pdf": "https://arxiv.org/pdf/2410.18444.pdf", "abs": "https://arxiv.org/abs/2410.18444", "title": "Evaluating Automatic Speech Recognition Systems for Korean Meteorological Experts", "authors": ["ChaeHun Park", "Hojun Cho", "Jaegul Choo"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "EMNLP 2025 Findings", "summary": "This paper explores integrating Automatic Speech Recognition (ASR) into\nnatural language query systems to improve weather forecasting efficiency for\nKorean meteorologists. We address challenges in developing ASR systems for the\nKorean weather domain, specifically specialized vocabulary and Korean\nlinguistic intricacies. To tackle these issues, we constructed an evaluation\ndataset of spoken queries recorded by native Korean speakers. Using this\ndataset, we assessed various configurations of a multilingual ASR model family,\nidentifying performance limitations related to domain-specific terminology. We\nthen implemented a simple text-to-speech-based data augmentation method, which\nimproved the recognition of specialized terms while maintaining general-domain\nperformance. Our contributions include creating a domain-specific dataset,\ncomprehensive ASR model evaluations, and an effective augmentation technique.\nWe believe our work provides a foundation for future advancements in ASR for\nthe Korean weather forecasting domain."}
{"id": "2411.18337", "pdf": "https://arxiv.org/pdf/2411.18337.pdf", "abs": "https://arxiv.org/abs/2411.18337", "title": "Can LLMs assist with Ambiguity? A Quantitative Evaluation of various Large Language Models on Word Sense Disambiguation", "authors": ["T. G. D. K. Sumanathilaka", "Nicholas Micallef", "Julian Hough"], "categories": ["cs.CL"], "comment": "12 pages,6 tables, 1 figure, Proceedings of the 1st International\n  Conference on NLP & AI for Cyber Security", "summary": "Ambiguous words are often found in modern digital communications. Lexical\nambiguity challenges traditional Word Sense Disambiguation (WSD) methods, due\nto limited data. Consequently, the efficiency of translation, information\nretrieval, and question-answering systems is hindered by these limitations.\nThis study investigates the use of Large Language Models (LLMs) to improve WSD\nusing a novel approach combining a systematic prompt augmentation mechanism\nwith a knowledge base (KB) consisting of different sense interpretations. The\nproposed method incorporates a human-in-loop approach for prompt augmentation\nwhere prompt is supported by Part-of-Speech (POS) tagging, synonyms of\nambiguous words, aspect-based sense filtering and few-shot prompting to guide\nthe LLM. By utilizing a few-shot Chain of Thought (COT) prompting-based\napproach, this work demonstrates a substantial improvement in performance. The\nevaluation was conducted using FEWS test data and sense tags. This research\nadvances accurate word interpretation in social media and digital\ncommunication."}
{"id": "2411.19855", "pdf": "https://arxiv.org/pdf/2411.19855.pdf", "abs": "https://arxiv.org/abs/2411.19855", "title": "Artificial intelligence contribution to translation industry: looking back and forward", "authors": ["Mohammed Q. Shormani"], "categories": ["cs.CL", "cs-CL", "F.2.2; I.2.7"], "comment": "30 pages, 13 figures", "summary": "This study provides a comprehensive analysis of artificial intelligence (AI)\ncontribution to research in the translation industry (ACTI), synthesizing it\nover forty-five years from 1980-2024. 13220 articles were retrieved from three\nsources, namely WoS, Scopus, and Lens; 9836 were unique records, which were\nused for the analysis. I provided two types of analysis, viz., scientometric\nand thematic, focusing on Cluster, Subject categories, Keywords, Bursts,\nCentrality and Research Centers as for the former. For the latter, I provided a\nthematic review for 18 articles, selected purposefully from the articles\ninvolved, centering on purpose, approach, findings, and contribution to ACTI\nfuture directions. This study is significant for its valuable contribution to\nACTI knowledge production over 45 years, emphasizing several trending issues\nand hotspots including Machine translation, Statistical machine translation,\nLow-resource language, Large language model, Arabic dialects, Translation\nquality, and Neural machine translation. The findings reveal that the more AI\ndevelops, the more it contributes to translation industry, as Neural Networking\nAlgorithms have been incorporated and Deep Language Learning Models like\nChatGPT have been launched. However, much rigorous research is still needed to\novercome several problems encountering translation industry, specifically\nconcerning low-resource, multi-dialectical and free word order languages, and\ncultural and religious registers."}
{"id": "2411.19858", "pdf": "https://arxiv.org/pdf/2411.19858.pdf", "abs": "https://arxiv.org/abs/2411.19858", "title": "What fifty-one years of Linguistics and Artificial Intelligence research tell us about their correlation: A scientometric analysis", "authors": ["Mohammed Q. Shormani"], "categories": ["cs.CL", "cs-CL", "F.2.2; I.2.7"], "comment": "26 pages, 15 figures", "summary": "There is a strong correlation between linguistics and artificial intelligence\n(AI), best manifested by deep learning language models. This study provides a\nthorough scientometric analysis of this correlation, synthesizing the\nintellectual production over 51 years, from 1974 to 2024. Web of Science Core\nCollection (WoSCC) database was the data source. The data collected were\nanalyzed by two powerful software, viz., CiteSpace and VOSviewer, through which\nmapping visualizations of the intellectual landscape, trending issues and\n(re)emerging hotspots were generated. The results indicate that in the 1980s\nand 1990s, linguistics and AI (AIL) research was not robust, characterized by\nunstable publication over time. It has, however, witnessed a remarkable\nincrease of publication since then, reaching 1478 articles in 2023, and 546\narticles in January-March timespan in 2024, involving emerging issues including\nNatural language processing, Cross-sectional study, Using bidirectional encoder\nrepresentation, and Using ChatGPT and hotspots such as Novice programmer,\nPrioritization, and Artificial intelligence, addressing new horizons, new\ntopics, and launching new applications and powerful deep learning language\nmodels including ChatGPT. It concludes that linguistics and AI correlation is\nestablished at several levels, research centers, journals, and countries\nshaping AIL knowledge production and reshaping its future frontiers."}
{"id": "2412.07030", "pdf": "https://arxiv.org/pdf/2412.07030.pdf", "abs": "https://arxiv.org/abs/2412.07030", "title": "FM2DS: Few-Shot Multimodal Multihop Data Synthesis with Knowledge Distillation for Question Answering", "authors": ["Amirhossein Abaskohi", "Spandana Gella", "Giuseppe Carenini", "Issam H. Laradji"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.IR", "cs.LG"], "comment": "Findings of EMNLP 2025", "summary": "Multimodal multihop question answering (MMQA) requires reasoning over images\nand text from multiple sources. Despite advances in visual question answering,\nthis multihop setting remains underexplored due to a lack of quality datasets.\nExisting methods focus on single-hop, single-modality, or short texts, limiting\nreal-world applications like interpreting educational documents with long,\nmultimodal content. To fill this gap, we introduce FM2DS, the first framework\nfor creating a high-quality dataset for MMQA. Our approach consists of a\n5-stage pipeline that involves acquiring relevant multimodal documents from\nWikipedia, synthetically generating high-level questions and answers, and\nvalidating them through rigorous criteria to ensure data quality. We evaluate\nour methodology by training models on our synthesized dataset and testing on\ntwo benchmarks: MultimodalQA and WebQA. Our results demonstrate that, with an\nequal sample size, models trained on our synthesized data outperform those\ntrained on human-collected data by 1.9 in exact match (EM) score on average.\nAdditionally, we introduce M2QA-Bench with 1k samples, the first benchmark for\nMMQA on long documents, generated using FM2DS and refined by human annotators.\nWe believe our data synthesis method will serve as a strong foundation for\ntraining and evaluating MMQA models."}
{"id": "2412.14642", "pdf": "https://arxiv.org/pdf/2412.14642.pdf", "abs": "https://arxiv.org/abs/2412.14642", "title": "Speak-to-Structure: Evaluating LLMs in Open-domain Natural Language-Driven Molecule Generation", "authors": ["Jiatong Li", "Junxian Li", "Weida Wang", "Yunqing Liu", "Changmeng Zheng", "Dongzhan Zhou", "Xiao-yong Wei", "Qing Li"], "categories": ["cs.CL"], "comment": "Our codes and datasets are available through\n  https://github.com/phenixace/TOMG-Bench", "summary": "Recently, Large Language Models (LLMs) have shown great potential in natural\nlanguage-driven molecule discovery. However, existing datasets and benchmarks\nfor molecule-text alignment are predominantly built on a one-to-one mapping,\nmeasuring LLMs' ability to retrieve a single, pre-defined answer, rather than\ntheir creative potential to generate diverse, yet equally valid, molecular\ncandidates. To address this critical gap, we propose Speak-to-Structure\n(S^2-Bench}), the first benchmark to evaluate LLMs in open-domain natural\nlanguage-driven molecule generation. S^2-Bench is specifically designed for\none-to-many relationships, challenging LLMs to demonstrate genuine molecular\nunderstanding and generation capabilities. Our benchmark includes three key\ntasks: molecule editing (MolEdit), molecule optimization (MolOpt), and\ncustomized molecule generation (MolCustom), each probing a different aspect of\nmolecule discovery. We also introduce OpenMolIns, a large-scale instruction\ntuning dataset that enables Llama-3.1-8B to surpass the most powerful LLMs like\nGPT-4o and Claude-3.5 on S^2-Bench. Our comprehensive evaluation of 28 LLMs\nshifts the focus from simple pattern recall to realistic molecular design,\npaving the way for more capable LLMs in natural language-driven molecule\ndiscovery."}
{"id": "2501.04249", "pdf": "https://arxiv.org/pdf/2501.04249.pdf", "abs": "https://arxiv.org/abs/2501.04249", "title": "IOLBENCH: Benchmarking LLMs on Linguistic Reasoning", "authors": ["Satyam Goyal", "Soham Dan"], "categories": ["cs.CL", "I.2"], "comment": null, "summary": "Despite the remarkable advancements and widespread applications of deep\nneural networks, their ability to perform reasoning tasks remains limited,\nparticularly in domains requiring structured, abstract thought. In this paper,\nwe investigate the linguistic reasoning capabilities of state-of-the-art large\nlanguage models (LLMs) by introducing IOLBENCH, a novel benchmark derived from\nInternational Linguistics Olympiad (IOL) problems. This dataset encompasses\ndiverse problems testing syntax, morphology, phonology, and semantics, all\ncarefully designed to be self-contained and independent of external knowledge.\nThese tasks challenge models to engage in metacognitive linguistic reasoning,\nrequiring the deduction of linguistic rules and patterns from minimal examples.\nThrough extensive benchmarking of leading LLMs, we find that even the most\nadvanced models struggle to handle the intricacies of linguistic complexity,\nparticularly in areas demanding compositional generalization and rule\nabstraction. Our analysis highlights both the strengths and persistent\nlimitations of current models in linguistic problem-solving, offering valuable\ninsights into their reasoning capabilities. By introducing IOLBENCH, we aim to\nfoster further research into developing models capable of human-like reasoning,\nwith broader implications for the fields of computational linguistics and\nartificial intelligence."}
{"id": "2501.15688", "pdf": "https://arxiv.org/pdf/2501.15688.pdf", "abs": "https://arxiv.org/abs/2501.15688", "title": "Transformer-Based Multimodal Knowledge Graph Completion with Link-Aware Contexts", "authors": ["Haodi Ma", "Dzmitry Kasinets", "Daisy Zhe Wang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Multimodal knowledge graph completion (MMKGC) aims to predict missing links\nin multimodal knowledge graphs (MMKGs) by leveraging information from various\nmodalities alongside structural data. Existing MMKGC approaches primarily\nextend traditional knowledge graph embedding (KGE) models, which often require\ncreating an embedding for every entity. This results in large model sizes and\ninefficiencies in integrating multimodal information, particularly for\nreal-world graphs. Meanwhile, Transformer-based models have demonstrated\ncompetitive performance in knowledge graph completion (KGC). However, their\nfocus on single-modal knowledge limits their capacity to utilize cross-modal\ninformation. Recently, Large vision-language models (VLMs) have shown potential\nin cross-modal tasks but are constrained by the high cost of training. In this\nwork, we propose a novel approach that integrates Transformer-based KGE models\nwith cross-modal context generated by pre-trained VLMs, thereby extending their\napplicability to MMKGC. Specifically, we employ a pre-trained VLM to transform\nrelevant visual information from entities and their neighbors into textual\nsequences. We then frame KGC as a sequence-to-sequence task, fine-tuning the\nmodel with the generated cross-modal context. This simple yet effective method\nsignificantly reduces model size compared to traditional KGE approaches while\nachieving competitive performance across multiple large-scale datasets with\nminimal hyperparameter tuning."}
{"id": "2502.11115", "pdf": "https://arxiv.org/pdf/2502.11115.pdf", "abs": "https://arxiv.org/abs/2502.11115", "title": "Are Generative Models Underconfident? Better Quality Estimation with Boosted Model Probability", "authors": ["Tu Anh Dinh", "Jan Niehues"], "categories": ["cs.CL", "I.2.7"], "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "Quality Estimation (QE) is estimating quality of the model output during\ninference when the ground truth is not available. Deriving output quality from\nthe models' output probability is the most trivial and low-effort way. However,\nwe show that the output probability of text-generation models can appear\nunderconfident. At each output step, there can be multiple correct options,\nmaking the probability distribution spread out more. Thus, lower probability\ndoes not necessarily mean lower output quality. Due to this observation, we\npropose a QE approach called BoostedProb, which boosts the model's confidence\nin cases where there are multiple viable output options. With no increase in\ncomplexity, BoostedProb is notably better than raw model probability in\ndifferent settings, achieving on average +0.194 improvement in Pearson\ncorrelation to ground-truth quality. It also comes close to or outperforms more\ncostly approaches like supervised or ensemble-based QE in certain settings."}
{"id": "2502.11451", "pdf": "https://arxiv.org/pdf/2502.11451.pdf", "abs": "https://arxiv.org/abs/2502.11451", "title": "From Personas to Talks: Revisiting the Impact of Personas on LLM-Synthesized Emotional Support Conversations", "authors": ["Shenghan Wu", "Yimo Zhu", "Wynne Hsu", "Mong-Li Lee", "Yang Deng"], "categories": ["cs.CL"], "comment": "Accepted by EMNLP 2025 Main Conference", "summary": "The rapid advancement of Large Language Models (LLMs) has revolutionized the\ngeneration of emotional support conversations (ESC), offering scalable\nsolutions with reduced costs and enhanced data privacy. This paper explores the\nrole of personas in the creation of ESC by LLMs. Our research utilizes\nestablished psychological frameworks to measure and infuse persona traits into\nLLMs, which then generate dialogues in the emotional support scenario. We\nconduct extensive evaluations to understand the stability of persona traits in\ndialogues, examining shifts in traits post-generation and their impact on\ndialogue quality and strategy distribution. Experimental results reveal several\nnotable findings: 1) LLMs can infer core persona traits, 2) subtle shifts in\nemotionality and extraversion occur, influencing the dialogue dynamics, and 3)\nthe application of persona traits modifies the distribution of emotional\nsupport strategies, enhancing the relevance and empathetic quality of the\nresponses. These findings highlight the potential of persona-driven LLMs in\ncrafting more personalized, empathetic, and effective emotional support\ndialogues, which has significant implications for the future design of\nAI-driven emotional support systems."}
{"id": "2502.12455", "pdf": "https://arxiv.org/pdf/2502.12455.pdf", "abs": "https://arxiv.org/abs/2502.12455", "title": "DSMoE: Matrix-Partitioned Experts with Dynamic Routing for Computation-Efficient Dense LLMs", "authors": ["Minxuan Lv", "Zhenpeng Su", "Leiyu Pan", "Yizhe Xiong", "Zijia Lin", "Hui Chen", "Wei Zhou", "Jungong Han", "Guiguang Ding", "Cheng Luo", "Di Zhang", "Kun Gai", "Songlin Hu"], "categories": ["cs.CL"], "comment": "Accepted by EMNLP main conference", "summary": "As large language models continue to scale, computational costs and resource\nconsumption have emerged as significant challenges. While existing\nsparsification methods like pruning reduce computational overhead, they risk\nlosing model knowledge through parameter removal. This paper proposes DSMoE\n(Dynamic Sparse Mixture-of-Experts), a novel approach that achieves\nsparsification by partitioning pre-trained FFN layers into computational\nblocks. We implement adaptive expert routing using sigmoid activation and\nstraight-through estimators, enabling tokens to flexibly access different\naspects of model knowledge based on input complexity. Additionally, we\nintroduce a sparsity loss term to balance performance and computational\nefficiency. Extensive experiments on LLaMA models demonstrate that under\nequivalent computational constraints, DSMoE achieves superior performance\ncompared to existing pruning and MoE approaches across language modeling and\ndownstream tasks, particularly excelling in generation tasks. Analysis reveals\nthat DSMoE learns distinctive layerwise activation patterns, providing new\ninsights for future MoE architecture design."}
{"id": "2502.13628", "pdf": "https://arxiv.org/pdf/2502.13628.pdf", "abs": "https://arxiv.org/abs/2502.13628", "title": "Efficient Environmental Claim Detection with Hyperbolic Graph Neural Networks", "authors": ["Darpan Aswal", "Manjira Sinha"], "categories": ["cs.CL"], "comment": null, "summary": "Transformer based models, specially large language models (LLMs) dominate the\nfield of NLP with their mass adoption in tasks such as text generation,\nsummarization and fake news detection. These models offer ease of deployment\nand reliability for most applications, however, they require significant\namounts of computational power for training as well as inference. This poses\nchallenges in their adoption in resource-constrained applications, specially in\nthe open-source community where compute availability is usually scarce. This\nwork proposes a graph-based approach for Environmental Claim Detection,\nexploring Graph Neural Networks (GNNs) and Hyperbolic Graph Neural Networks\n(HGNNs) as lightweight yet effective alternatives to transformer-based models.\nRe-framing the task as a graph classification problem, we transform claim\nsentences into dependency parsing graphs, utilizing a combination of word2vec\n\\& learnable part-of-speech (POS) tag embeddings for the node features and\nencoding syntactic dependencies in the edge relations. Our results show that\nour graph-based models, particularly HGNNs in the poincar\\'e space (P-HGNNs),\nachieve performance superior to the state-of-the-art on environmental claim\ndetection while using upto \\textbf{30x fewer parameters}. We also demonstrate\nthat HGNNs benefit vastly from explicitly modeling data in hierarchical\n(tree-like) structures, enabling them to significantly improve over their\neuclidean counterparts."}
{"id": "2502.14383", "pdf": "https://arxiv.org/pdf/2502.14383.pdf", "abs": "https://arxiv.org/abs/2502.14383", "title": "Rumor Detection by Multi-task Suffix Learning based on Time-series Dual Sentiments", "authors": ["Zhiwei Liu", "Kailai Yang", "Eduard Hovy", "Sophia Ananiadou"], "categories": ["cs.CL"], "comment": "work in progress", "summary": "The widespread dissemination of rumors on social media has a significant\nimpact on people's lives, potentially leading to public panic and fear. Rumors\noften evoke specific sentiments, resonating with readers and prompting sharing.\nTo effectively detect and track rumors, it is essential to observe the\nfine-grained sentiments of both source and response message pairs as the rumor\nevolves over time. However, current rumor detection methods fail to account for\nthis aspect. In this paper, we propose MSuf, the first multi-task suffix\nlearning framework for rumor detection and tracking using time series dual\n(coupled) sentiments. MSuf includes three modules: (1) an LLM to extract\nsentiment intensity features and sort them chronologically; (2) a module that\nfuses the sorted sentiment features with their source text word embeddings to\nobtain an aligned embedding; (3) two hard prompts are combined with the aligned\nvector to perform rumor detection and sentiment analysis using one frozen LLM.\nMSuf effectively enhances the performance of LLMs for rumor detection with only\nminimal parameter fine-tuning. Evaluating MSuf on four rumor detection\nbenchmarks, we find significant improvements compared to other emotion-based\nmethods."}
{"id": "2502.17026", "pdf": "https://arxiv.org/pdf/2502.17026.pdf", "abs": "https://arxiv.org/abs/2502.17026", "title": "Understanding the Uncertainty of LLM Explanations: A Perspective Based on Reasoning Topology", "authors": ["Longchao Da", "Xiaoou Liu", "Jiaxin Dai", "Lu Cheng", "Yaqing Wang", "Hua Wei"], "categories": ["cs.CL", "cs.AI", "cs.SC", "68T50, 68T37, 68Q32", "I.2.7; I.2.6; I.2.4"], "comment": "28 pages, 9 figures; accepted at COLM'25", "summary": "Understanding the uncertainty in large language model (LLM) explanations is\nimportant for evaluating their faithfulness and reasoning consistency, and thus\nprovides insights into the reliability of LLM's output regarding a question. In\nthis work, we propose a novel framework that quantifies uncertainty in LLM\nexplanations through a reasoning topology perspective. By designing a\nstructural elicitation strategy, we guide the LLMs to frame the explanations of\nan answer into a graph topology. This process decomposes the explanations into\nthe knowledge related sub-questions and topology-based reasoning structures,\nwhich allows us to quantify uncertainty not only at the semantic level but also\nfrom the reasoning path. It further brings convenience to assess knowledge\nredundancy and provide interpretable insights into the reasoning process. Our\nmethod offers a systematic way to interpret the LLM reasoning, analyze\nlimitations, and provide guidance for enhancing robustness and faithfulness.\nThis work pioneers the use of graph-structured uncertainty measurement in LLM\nexplanations and demonstrates the potential of topology-based quantification."}
{"id": "2502.20258", "pdf": "https://arxiv.org/pdf/2502.20258.pdf", "abs": "https://arxiv.org/abs/2502.20258", "title": "LLM as a Broken Telephone: Iterative Generation Distorts Information", "authors": ["Amr Mohamed", "Mingmeng Geng", "Michalis Vazirgiannis", "Guokan Shang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025, Main Conference", "summary": "As large language models are increasingly responsible for online content,\nconcerns arise about the impact of repeatedly processing their own outputs.\nInspired by the \"broken telephone\" effect in chained human communication, this\nstudy investigates whether LLMs similarly distort information through iterative\ngeneration. Through translation-based experiments, we find that distortion\naccumulates over time, influenced by language choice and chain complexity.\nWhile degradation is inevitable, it can be mitigated through strategic\nprompting techniques. These findings contribute to discussions on the long-term\neffects of AI-mediated information propagation, raising important questions\nabout the reliability of LLM-generated content in iterative workflows."}
{"id": "2502.20344", "pdf": "https://arxiv.org/pdf/2502.20344.pdf", "abs": "https://arxiv.org/abs/2502.20344", "title": "LinguaLens: Towards Interpreting Linguistic Mechanisms of Large Language Models via Sparse Auto-Encoder", "authors": ["Yi Jing", "Zijun Yao", "Hongzhu Guo", "Lingxu Ran", "Xiaozhi Wang", "Lei Hou", "Juanzi Li"], "categories": ["cs.CL"], "comment": "Accepted by EMNLP 2025 MainConference", "summary": "Large language models (LLMs) demonstrate exceptional performance on tasks\nrequiring complex linguistic abilities, such as reference disambiguation and\nmetaphor recognition/generation. Although LLMs possess impressive capabilities,\ntheir internal mechanisms for processing and representing linguistic knowledge\nremain largely opaque. Prior research on linguistic mechanisms is limited by\ncoarse granularity, limited analysis scale, and narrow focus. In this study, we\npropose LinguaLens, a systematic and comprehensive framework for analyzing the\nlinguistic mechanisms of large language models, based on Sparse Auto-Encoders\n(SAEs). We extract a broad set of Chinese and English linguistic features\nacross four dimensions (morphology, syntax, semantics, and pragmatics). By\nemploying counterfactual methods, we construct a large-scale counterfactual\ndataset of linguistic features for mechanism analysis. Our findings reveal\nintrinsic representations of linguistic knowledge in LLMs, uncover patterns of\ncross-layer and cross-lingual distribution, and demonstrate the potential to\ncontrol model outputs. This work provides a systematic suite of resources and\nmethods for studying linguistic mechanisms, offers strong evidence that LLMs\npossess genuine linguistic knowledge, and lays the foundation for more\ninterpretable and controllable language modeling in future research."}
{"id": "2503.03106", "pdf": "https://arxiv.org/pdf/2503.03106.pdf", "abs": "https://arxiv.org/abs/2503.03106", "title": "Monitoring Decoding: Mitigating Hallucination via Evaluating the Factuality of Partial Response during Generation", "authors": ["Yurui Chang", "Bochuan Cao", "Lu Lin"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to ACL 2025 (Findings)", "summary": "While large language models have demonstrated exceptional performance across\na wide range of tasks, they remain susceptible to hallucinations -- generating\nplausible yet factually incorrect contents. Existing methods to mitigating such\nrisk often rely on sampling multiple full-length generations, which introduces\nsignificant response latency and becomes ineffective when the model\nconsistently produces hallucinated outputs with high confidence. To address\nthese limitations, we introduce Monitoring Decoding (MD), a novel framework\nthat dynamically monitors the generation process and selectively applies\nin-process interventions, focusing on revising crucial tokens responsible for\nhallucinations. Instead of waiting until completion of multiple full-length\ngenerations, we identify hallucination-prone tokens during generation using a\nmonitor function, and further refine these tokens through a tree-based decoding\nstrategy. This approach ensures an enhanced factual accuracy and coherence in\nthe generated output while maintaining efficiency. Experimental results\ndemonstrate that MD consistently outperforms self-consistency-based approaches\nin both effectiveness and efficiency, achieving higher factual accuracy while\nsignificantly reducing computational overhead."}
{"id": "2503.05362", "pdf": "https://arxiv.org/pdf/2503.05362.pdf", "abs": "https://arxiv.org/abs/2503.05362", "title": "Chain of Strategy Optimization Makes Large Language Models Better Emotional Supporter", "authors": ["Weixiang Zhao", "Xingyu Sui", "Xinyang Han", "Yang Deng", "Yulin Hu", "Jiahe Guo", "Libo Qin", "Qianyun Du", "Shijin Wang", "Yanyan Zhao", "Bing Qin", "Ting Liu"], "categories": ["cs.CL"], "comment": "21 pages, 9 figures, 17 tables", "summary": "The growing emotional stress in modern society has increased the demand for\nEmotional Support Conversations (ESC). While Large Language Models (LLMs) show\npromise for ESC, they face two key challenges: (1) low strategy selection\naccuracy, and (2) preference bias, limiting their adaptability to emotional\nneeds of users. Existing supervised fine-tuning (SFT) struggles to address\nthese issues, as it rigidly trains models on single gold-standard responses\nwithout modeling nuanced strategy trade-offs. To overcome these limitations, we\npropose Chain-of-Strategy Optimization (CSO), a novel approach that optimizes\nstrategy selection preferences at each dialogue turn. We first leverage Monte\nCarlo Tree Search to construct ESC-Pro, a high-quality preference dataset with\nturn-level strategy-response pairs. Training on ESC-Pro with CSO improves both\nstrategy accuracy and bias mitigation, enabling LLMs to generate more\nempathetic and contextually appropriate responses. Experiments on LLaMA-3.1-8B,\nGemma-2-9B, and Qwen2.5-7B demonstrate that CSO outperforms standard SFT,\nhighlighting the efficacy of fine-grained, turn-level preference modeling in\nESC."}
{"id": "2504.04335", "pdf": "https://arxiv.org/pdf/2504.04335.pdf", "abs": "https://arxiv.org/abs/2504.04335", "title": "Hallucinated Span Detection with Multi-View Attention Features", "authors": ["Yuya Ogasa", "Yuki Arase"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This study addresses the problem of hallucinated span detection in the\noutputs of large language models. It has received less attention than\noutput-level hallucination detection despite its practical importance. Prior\nwork has shown that attentions often exhibit irregular patterns when\nhallucinations occur. Motivated by these findings, we extract features from the\nattention matrix that provide complementary views capturing (a) whether certain\ntokens are influential or ignored, (b) whether attention is biased toward\nspecific subsets, and (c) whether a token is generated referring to a narrow or\nbroad context, in the generation. These features are input to a\nTransformer-based classifier to conduct sequential labelling to identify\nhallucinated spans. Experimental results indicate that the proposed method\noutperforms strong baselines on hallucinated span detection with longer input\ncontexts, such as data-to-text and summarisation tasks."}
{"id": "2504.12805", "pdf": "https://arxiv.org/pdf/2504.12805.pdf", "abs": "https://arxiv.org/abs/2504.12805", "title": "Assessing LLMs in Art Contexts: Critique Generation and Theory of Mind Evaluation", "authors": ["Takaya Arita", "Wenxian Zheng", "Reiji Suzuki", "Fuminori Akiba"], "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": "Corrected a typo in the metadata title only\n  (\"Assesing\"->\"Assessing\"). No changes were made to the PDF or source files", "summary": "This study explored how large language models (LLMs) perform in two areas\nrelated to art: writing critiques of artworks and reasoning about mental states\n(Theory of Mind, or ToM) in art-related situations. For the critique generation\npart, we built a system that combines Noel Carroll's evaluative framework with\na broad selection of art criticism theories. The model was prompted to first\nwrite a full-length critique and then shorter, more coherent versions using a\nstep-by-step prompting process. These AI-generated critiques were then compared\nwith those written by human experts in a Turing test-style evaluation. In many\ncases, human subjects had difficulty telling which was which, and the results\nsuggest that LLMs can produce critiques that are not only plausible in style\nbut also rich in interpretation, as long as they are carefully guided. In the\nsecond part, we introduced new simple ToM tasks based on situations involving\ninterpretation, emotion, and moral tension, which can appear in the context of\nart. These go beyond standard false-belief tests and allow for more complex,\nsocially embedded forms of reasoning. We tested 41 recent LLMs and found that\ntheir performance varied across tasks and models. In particular, tasks that\ninvolved affective or ambiguous situations tended to reveal clearer\ndifferences. Taken together, these results help clarify how LLMs respond to\ncomplex interpretative challenges, revealing both their cognitive limitations\nand potential. While our findings do not directly contradict the so-called\nGenerative AI Paradox--the idea that LLMs can produce expert-like output\nwithout genuine understanding--they suggest that, depending on how LLMs are\ninstructed, such as through carefully designed prompts, these models may begin\nto show behaviors that resemble understanding more closely than we might\nassume."}
{"id": "2504.14089", "pdf": "https://arxiv.org/pdf/2504.14089.pdf", "abs": "https://arxiv.org/abs/2504.14089", "title": "LogicTree: Structured Proof Exploration for Coherent and Rigorous Logical Reasoning with Large Language Models", "authors": ["Kang He", "Kaushik Roy"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "EMNLP 2025 Main Conference", "summary": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average."}
{"id": "2504.15133", "pdf": "https://arxiv.org/pdf/2504.15133.pdf", "abs": "https://arxiv.org/abs/2504.15133", "title": "EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language Models", "authors": ["Ziwen Xu", "Shuxun Wang", "Kewei Xu", "Haoming Xu", "Mengru Wang", "Xinle Deng", "Yunzhi Yao", "Guozhou Zheng", "Huajun Chen", "Ningyu Zhang"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "comment": "EMNLP 2025 System Demonstrations. Demo:\n  https://www.youtube.com/watch?v=AkfoiPfp5rQ; code:\n  https://github.com/zjunlp/EasyEdit", "summary": "In this paper, we introduce EasyEdit2, a framework designed to enable\nplug-and-play adjustability for controlling Large Language Model (LLM)\nbehaviors. EasyEdit2 supports a wide range of test-time interventions,\nincluding safety, sentiment, personality, reasoning patterns, factuality, and\nlanguage features. Unlike its predecessor, EasyEdit2 features a new\narchitecture specifically designed for seamless model steering. It comprises\nkey modules such as the steering vector generator and the steering vector\napplier, which enable automatic generation and application of steering vectors\nto influence the model's behavior without modifying its parameters. One of the\nmain advantages of EasyEdit2 is its ease of use-users do not need extensive\ntechnical knowledge. With just a single example, they can effectively guide and\nadjust the model's responses, making precise control both accessible and\nefficient. Empirically, we report model steering performance across different\nLLMs, demonstrating the effectiveness of these techniques. We have released the\nsource code on GitHub at https://github.com/zjunlp/EasyEdit along with a\ndemonstration notebook. In addition, we provide a demo video at\nhttps://www.youtube.com/watch?v=AkfoiPfp5rQ for a quick introduction."}
{"id": "2504.20022", "pdf": "https://arxiv.org/pdf/2504.20022.pdf", "abs": "https://arxiv.org/abs/2504.20022", "title": "Better To Ask in English? Evaluating Factual Accuracy of Multilingual LLMs in English and Low-Resource Languages", "authors": ["Pritika Rohera", "Chaitrali Ginimav", "Gayatri Sawant", "Raviraj Joshi"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Multilingual Large Language Models (LLMs) have demonstrated significant\neffectiveness across various languages, particularly in high-resource languages\nsuch as English. However, their performance in terms of factual accuracy across\nother low-resource languages, especially Indic languages, remains an area of\ninvestigation. In this study, we assess the factual accuracy of LLMs - GPT-4o,\nGemma-2-9B, Gemma-2-2B, and Llama-3.1-8B - by comparing their performance in\nEnglish and Indic languages using the IndicQuest dataset, which contains\nquestion-answer pairs in English and 19 Indic languages. By asking the same\nquestions in English and their respective Indic translations, we analyze\nwhether the models are more reliable for regional context questions in Indic\nlanguages or when operating in English. Our findings reveal that LLMs often\nperform better in English, even for questions rooted in Indic contexts.\nNotably, we observe a higher tendency for hallucination in responses generated\nin low-resource Indic languages, highlighting challenges in the multilingual\nunderstanding capabilities of current LLMs."}
{"id": "2504.21540", "pdf": "https://arxiv.org/pdf/2504.21540.pdf", "abs": "https://arxiv.org/abs/2504.21540", "title": "Improving Informally Romanized Language Identification", "authors": ["Adrian Benton", "Alexander Gutkin", "Christo Kirov", "Brian Roark"], "categories": ["cs.CL"], "comment": "19 pages, 16 tables, 4 figures", "summary": "The Latin script is often used to informally write languages with non-Latin\nnative scripts. In many cases (e.g., most languages in India), the lack of\nconventional spelling in the Latin script results in high spelling variability.\nSuch romanization renders languages that are normally easily distinguished due\nto being written in different scripts - Hindi and Urdu, for example - highly\nconfusable. In this work, we increase language identification (LID) accuracy\nfor romanized text by improving the methods used to synthesize training sets.\nWe find that training on synthetic samples which incorporate natural spelling\nvariation yields higher LID system accuracy than including available naturally\noccurring examples in the training set, or even training higher capacity\nmodels. We demonstrate new state-of-the-art LID performance on romanized text\nfrom 20 Indic languages in the Bhasha-Abhijnaanam evaluation set (Madhani et\nal., 2023a), improving test F1 from the reported 74.7% (using a pretrained\nneural model) to 85.4% using a linear classifier trained solely on synthetic\ndata and 88.2% when also training on available harvested text."}
{"id": "2504.21773", "pdf": "https://arxiv.org/pdf/2504.21773.pdf", "abs": "https://arxiv.org/abs/2504.21773", "title": "MAC-Tuning: LLM Multi-Compositional Problem Reasoning with Enhanced Knowledge Boundary Awareness", "authors": ["Junsheng Huang", "Zhitao He", "Yucheng Huang", "Sandeep Polisetty", "Qingyun Wang", "Yi. R Fung"], "categories": ["cs.CL", "cs.AI"], "comment": "We release our code and resource at\n  https://github.com/no-touch-fish/Multi-QA-Tuning. The paper is accepted into\n  EMNLP 2025 main", "summary": "The hallucination of non-existent facts by LLMs is an important problem given\nits widespread adoption across various applications. Previous research\naddresses this problem by analyzing the internal parameterized knowledge\nboundaries to estimate confidence. However, these studies focus on the\nsingle-problem setting and have not explored the more challenging multi-problem\nsetting, which requires accurately answering multiple questions simultaneously.\nWe introduce a novel method for the multi-problem setting, Multiple Answers and\nConfidence Stepwise Tuning (MAC-Tuning), that separates the learning of answer\nprediction and confidence estimation during fine-tuning on instruction data.\nExtensive experiments demonstrate that our method outperforms baselines by up\nto 25\\% in average precision."}
{"id": "2505.00047", "pdf": "https://arxiv.org/pdf/2505.00047.pdf", "abs": "https://arxiv.org/abs/2505.00047", "title": "Base Models Beat Aligned Models at Randomness and Creativity", "authors": ["Peter West", "Christopher Potts"], "categories": ["cs.CL"], "comment": null, "summary": "Alignment has quickly become a default ingredient in LLM development, with\ntechniques such as reinforcement learning from human feedback making models act\nsafely, follow instructions, and perform ever-better on complex tasks. While\nthese techniques are certainly useful, we propose that they should not be\nuniversally applied and demonstrate a range of tasks on which base language\nmodels consistently outperform their popular aligned forms. Particularly, we\nstudy tasks that require unpredictable outputs, such as random number\ngeneration, mixed strategy games (rock-paper-scissors and hide-and-seek), and\ncreative writing. In each case, aligned models tend towards narrow behaviors\nthat result in distinct disadvantages, for instance, preferring to generate \"7\"\nover other uniformly random numbers, becoming almost fully predictable in some\ngame states, or prioritizing pleasant writing over creative originality. Across\nmodels tested, better performance on common benchmarks tends to correlate with\nworse performance on our tasks, suggesting an effective trade-off in the\nrequired capabilities."}
{"id": "2505.00979", "pdf": "https://arxiv.org/pdf/2505.00979.pdf", "abs": "https://arxiv.org/abs/2505.00979", "title": "Synthesize-on-Graph: Knowledgeable Synthetic Data Generation for Continue Pre-training of Large Language Models", "authors": ["Shengjie Ma", "Xuhui Jiang", "Chengjin Xu", "Cehao Yang", "Liyu Zhang", "Jian Guo"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success but remain\ndata-inefficient, especially when learning from small, specialized corpora with\nlimited and proprietary data. Existing synthetic data generation methods for\ncontinue pre-training focus on intra-document content and overlook\ncross-document knowledge associations, limiting content diversity and depth. We\npropose Synthetic-on-Graph (SoG), a synthetic data generation framework that\nincorporates cross-document knowledge associations for efficient corpus\nexpansion. SoG constructs a context graph by extracting entities and concepts\nfrom the original corpus, representing cross-document associations, and\nemploying a graph walk strategy for knowledge-associated sampling. This\nenhances synthetic data diversity and coherence, enabling models to learn\ncomplex knowledge structures and handle rare knowledge. To further improve the\nquality of synthetic data, we integrate two complementary strategies,\nChain-of-Thought (CoT) and Contrastive Clarifying (CC), to enhance both\nreasoning capability and discriminative power. Extensive experiments\ndemonstrate that SoG surpasses state-of-the-art (SOTA) methods on multi-hop and\ndomain-specific question answering, while achieving competitive performance on\nlong-context reading comprehension. These results highlight the superior\ngeneralization ability of SoG. Our work advances the paradigm of synthetic data\ngeneration and offers practical solutions for efficient knowledge acquisition\nin LLMs, particularly for downstream tasks and domains with limited training\ndata."}
{"id": "2505.11835", "pdf": "https://arxiv.org/pdf/2505.11835.pdf", "abs": "https://arxiv.org/abs/2505.11835", "title": "Multilingual Collaborative Defense for Large Language Models", "authors": ["Hongliang Li", "Jinan Xu", "Gengping Cui", "Changhao Guan", "Fengran Mo", "Kaiyu Huang"], "categories": ["cs.CL", "cs.AI"], "comment": "21 pages, 4figures", "summary": "The robustness and security of large language models (LLMs) has become a\nprominent research area. One notable vulnerability is the ability to bypass LLM\nsafeguards by translating harmful queries into rare or underrepresented\nlanguages, a simple yet effective method of \"jailbreaking\" these models.\nDespite the growing concern, there has been limited research addressing the\nsafeguarding of LLMs in multilingual scenarios, highlighting an urgent need to\nenhance multilingual safety. In this work, we investigate the correlation\nbetween various attack features across different languages and propose\nMultilingual Collaborative Defense (MCD), a novel learning method that\noptimizes a continuous, soft safety prompt automatically to facilitate\nmultilingual safeguarding of LLMs. The MCD approach offers three advantages:\nFirst, it effectively improves safeguarding performance across multiple\nlanguages. Second, MCD maintains strong generalization capabilities while\nminimizing false refusal rates. Third, MCD mitigates the language safety\nmisalignment caused by imbalances in LLM training corpora. To evaluate the\neffectiveness of MCD, we manually construct multilingual versions of commonly\nused jailbreak benchmarks, such as MaliciousInstruct and AdvBench, to assess\nvarious safeguarding methods. Additionally, we introduce these datasets in\nunderrepresented (zero-shot) languages to verify the language transferability\nof MCD. The results demonstrate that MCD outperforms existing approaches in\nsafeguarding against multilingual jailbreak attempts while also exhibiting\nstrong language transfer capabilities. Our code is available at\nhttps://github.com/HLiang-Lee/MCD."}
{"id": "2505.15776", "pdf": "https://arxiv.org/pdf/2505.15776.pdf", "abs": "https://arxiv.org/abs/2505.15776", "title": "ConvSearch-R1: Enhancing Query Reformulation for Conversational Search with Reasoning via Reinforcement Learning", "authors": ["Changtai Zhu", "Siyin Wang", "Ruijun Feng", "Kai Song", "Xipeng Qiu"], "categories": ["cs.CL", "cs.IR"], "comment": "Accepted by EMNLP 2025 at the Main Conference", "summary": "Conversational search systems require effective handling of context-dependent\nqueries that often contain ambiguity, omission, and coreference. Conversational\nQuery Reformulation (CQR) addresses this challenge by transforming these\nqueries into self-contained forms suitable for off-the-shelf retrievers.\nHowever, existing CQR approaches suffer from two critical constraints: high\ndependency on costly external supervision from human annotations or large\nlanguage models, and insufficient alignment between the rewriting model and\ndownstream retrievers. We present ConvSearch-R1, the first self-driven\nframework that completely eliminates dependency on external rewrite supervision\nby leveraging reinforcement learning to optimize reformulation directly through\nretrieval signals. Our novel two-stage approach combines Self-Driven Policy\nWarm-Up to address the cold-start problem through retrieval-guided\nself-distillation, followed by Retrieval-Guided Reinforcement Learning with a\nspecially designed rank-incentive reward shaping mechanism that addresses the\nsparsity issue in conventional retrieval metrics. Extensive experiments on\nTopiOCQA and QReCC datasets demonstrate that ConvSearch-R1 significantly\noutperforms previous state-of-the-art methods, achieving over 10% improvement\non the challenging TopiOCQA dataset while using smaller 3B parameter models\nwithout any external supervision."}
{"id": "2505.16281", "pdf": "https://arxiv.org/pdf/2505.16281.pdf", "abs": "https://arxiv.org/abs/2505.16281", "title": "HiMATE: A Hierarchical Multi-Agent Framework for Machine Translation Evaluation", "authors": ["Shijie Zhang", "Renhao Li", "Songsheng Wang", "Philipp Koehn", "Min Yang", "Derek F. Wong"], "categories": ["cs.CL"], "comment": null, "summary": "The advancement of Large Language Models (LLMs) enables flexible and\ninterpretable automatic evaluations. In the field of machine translation\nevaluation, utilizing LLMs with translation error annotations based on\nMultidimensional Quality Metrics (MQM) yields more human-aligned judgments.\nHowever, current LLM-based evaluation methods still face challenges in\naccurately identifying error spans and assessing their severity. In this paper,\nwe propose HiMATE, a Hierarchical Multi-Agent Framework for Machine Translation\nEvaluation. We argue that existing approaches inadequately exploit the\nfine-grained structural and semantic information within the MQM hierarchy. To\naddress this, we develop a hierarchical multi-agent system grounded in the MQM\nerror typology, enabling granular evaluation of subtype errors. Two key\nstrategies are incorporated to further mitigate systemic hallucinations within\nthe framework: the utilization of the model's self-reflection capability and\nthe facilitation of agent discussion involving asymmetric information.\nEmpirically, HiMATE outperforms competitive baselines across different datasets\nin conducting human-aligned evaluations. Further analyses underscore its\nsignificant advantage in error span detection and severity assessment,\nachieving an average F1-score improvement of 89% over the best-performing\nbaseline. We make our code and data publicly available at\nhttps://github.com/nlp2ct-shijie/HiMATE."}
{"id": "2505.22169", "pdf": "https://arxiv.org/pdf/2505.22169.pdf", "abs": "https://arxiv.org/abs/2505.22169", "title": "ReliableEval: A Recipe for Stochastic LLM Evaluation via Method of Moments", "authors": ["Gili Lior", "Eliya Habba", "Shahar Levy", "Avi Caciularu", "Gabriel Stanovsky"], "categories": ["cs.CL"], "comment": "Findings of EMNLP 2025", "summary": "LLMs are highly sensitive to prompt phrasing, yet standard benchmarks\ntypically report performance using a single prompt, raising concerns about the\nreliability of such evaluations. In this work, we argue for a stochastic method\nof moments evaluation over the space of meaning-preserving prompt\nperturbations. We introduce a formal definition of reliable evaluation that\naccounts for prompt sensitivity, and suggest ReliableEval - a method for\nestimating the number of prompt resamplings needed to obtain meaningful\nresults. Using our framework, we stochastically evaluate five frontier LLMs and\nfind that even top-performing models like GPT-4o and Claude-3.7-Sonnet exhibit\nsubstantial prompt sensitivity. Our approach is model-, task-, and\nmetric-agnostic, offering a recipe for meaningful and robust LLM evaluation."}
{"id": "2505.23657", "pdf": "https://arxiv.org/pdf/2505.23657.pdf", "abs": "https://arxiv.org/abs/2505.23657", "title": "Active Layer-Contrastive Decoding Reduces Hallucination in Large Language Model Generation", "authors": ["Hongxiang Zhang", "Hao Chen", "Muhao Chen", "Tianyi Zhang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "19 pages, 3 figures, EMNLP 2025", "summary": "Recent decoding methods improve the factuality of large language models\n(LLMs) by refining how the next token is selected during generation. These\nmethods typically operate at the token level, leveraging internal\nrepresentations to suppress superficial patterns. Nevertheless, LLMs remain\nprone to hallucinations, especially over longer contexts. In this paper, we\npropose Active Layer-Contrastive Decoding (ActLCD), a novel decoding strategy\nthat actively decides when to apply contrasting layers during generation. By\ncasting decoding as a sequential decision-making problem, ActLCD employs a\nreinforcement learning policy guided by a reward-aware classifier to optimize\nfactuality beyond the token level. Our experiments demonstrate that ActLCD\nsurpasses state-of-the-art methods across five benchmarks, showcasing its\neffectiveness in mitigating hallucinations in diverse generation scenarios."}
{"id": "2505.23810", "pdf": "https://arxiv.org/pdf/2505.23810.pdf", "abs": "https://arxiv.org/abs/2505.23810", "title": "MARS-Bench: A Multi-turn Athletic Real-world Scenario Benchmark for Dialogue Evaluation", "authors": ["Chenghao Yang", "Yinbo Luo", "Zhoufutu Wen", "Qi Chu", "Tao Gong", "Longxiang Liu", "Kaiyuan Zhang", "Jianpeng Jiao", "Ge Zhang", "Wenhao Huang", "Nenghai Yu"], "categories": ["cs.CL", "cs.AI"], "comment": "29 pages, 13 figures, Accepted as EMNLP2025 Findings", "summary": "Large Language Models (\\textbf{LLMs}), e.g. ChatGPT, have been widely adopted\nin real-world dialogue applications. However, LLMs' robustness, especially in\nhandling long complex dialogue sessions, including frequent motivation\ntransfer, sophisticated cross-turn dependency, is criticized all along.\nNevertheless, no existing benchmarks can fully reflect these weaknesses. We\npresent \\textbf{MARS-Bench}, a \\textbf{M}ulti-turn \\textbf{A}thletic\n\\textbf{R}eal-world \\textbf{S}cenario Dialogue \\textbf{Bench}mark, designed to\nremedy the gap. MARS-Bench is constructed from play-by-play text commentary so\nto feature realistic dialogues specifically designed to evaluate three critical\naspects of multi-turn conversations: Ultra Multi-turn, Interactive Multi-turn,\nand Cross-turn Tasks. Extensive experiments on MARS-Bench also reveal that\nclosed-source LLMs significantly outperform open-source alternatives, explicit\nreasoning significantly boosts LLMs' robustness on handling long complex\ndialogue sessions, and LLMs indeed face significant challenges when handling\nmotivation transfer and sophisticated cross-turn dependency. Moreover, we\nprovide mechanistic interpretability on how attention sinks due to special\ntokens lead to LLMs' performance degradation when handling long complex\ndialogue sessions based on attention visualization experiment in\nQwen2.5-7B-Instruction."}
{"id": "2505.24688", "pdf": "https://arxiv.org/pdf/2505.24688.pdf", "abs": "https://arxiv.org/abs/2505.24688", "title": "Soft Reasoning: Navigating Solution Spaces in Large Language Models through Controlled Embedding Exploration", "authors": ["Qinglin Zhu", "Runcong Zhao", "Hanqi Yan", "Yulan He", "Yudong Chen", "Lin Gui"], "categories": ["cs.CL"], "comment": "Accepted as a Spotlight at ICML 2025", "summary": "Large Language Models (LLMs) struggle with complex reasoning due to limited\ndiversity and inefficient search. We propose Soft Reasoning, an embedding-based\nsearch framework that optimises the embedding of the first token to guide\ngeneration. It combines (1) embedding perturbation for controlled exploration\nand (2) Bayesian optimisation to refine embeddings via a verifier-guided\nobjective, balancing exploration and exploitation. This approach improves\nreasoning accuracy and coherence while avoiding reliance on heuristic search.\nExperiments demonstrate superior correctness with minimal computation, making\nit a scalable, model-agnostic solution. The code is released at\nhttps://github.com/alickzhu/Soft-Reasoning."}
{"id": "2506.03303", "pdf": "https://arxiv.org/pdf/2506.03303.pdf", "abs": "https://arxiv.org/abs/2506.03303", "title": "Hopscotch: Discovering and Skipping Redundancies in Language Models", "authors": ["Mustafa Eyceoz", "Nikhil Shivakumar Nayak", "Hao Wang", "Ligong Han", "Akash Srivastava"], "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2.7; I.2.6; I.2.4"], "comment": "10 pages, 4 figures, 9 tables", "summary": "Modern causal language models stack many attention blocks to improve\nperformance, but not all blocks are necessary for every task. We propose\nHopscotch, a simple yet effective method that identifies and skips attention\nblocks with least contributions to a task and adapts to preserve output\nquality. Hopscotch jointly optimizes which blocks to skip and how to scale the\noutputs of the remaining layers. By introducing lightweight, trainable scaling\nparameters to attention and MLP blocks, it mitigates distribution shifts in\nhidden states caused by removing attention blocks. Hopscotch does not modify\nmodel weights or require access to pretraining or instruction-tuning data, and\nis compatible with existing model compression techniques. When applied to\n$\\texttt{Llama-3.1-8B}$ and $\\texttt{Qwen2.5-7B}$, Hopscotch achieves less than\na 2% drop in performance even after skipping four attention blocks."}
{"id": "2506.04689", "pdf": "https://arxiv.org/pdf/2506.04689.pdf", "abs": "https://arxiv.org/abs/2506.04689", "title": "Recycling the Web: A Method to Enhance Pre-training Data Quality and Quantity for Language Models", "authors": ["Thao Nguyen", "Yang Li", "Olga Golovneva", "Luke Zettlemoyer", "Sewoong Oh", "Ludwig Schmidt", "Xian Li"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to COLM 2025", "summary": "Scaling laws predict that the performance of large language models improves\nwith increasing model size and data size. In practice, pre-training has been\nrelying on massive web crawls, using almost all data sources publicly available\non the internet so far. However, this pool of natural data does not grow at the\nsame rate as the compute supply. Furthermore, the availability of high-quality\ntexts is even more limited: data filtering pipelines often remove up to 99% of\nthe initial web scrapes to achieve state-of-the-art. To address the \"data wall\"\nof pre-training scaling, our work explores ways to transform and recycle data\ndiscarded in existing filtering processes. We propose REWIRE, REcycling the Web\nwith guIded REwrite, a method to enrich low-quality documents so that they\ncould become useful for training. This in turn allows us to increase the\nrepresentation of synthetic data in the final pre-training set. Experiments at\n1B, 3B and 7B scales of the DCLM benchmark show that mixing high-quality raw\ntexts and our rewritten texts lead to 1.0, 1.3 and 2.5 percentage points\nimprovement respectively across 22 diverse tasks, compared to training on only\nfiltered web data. Training on the raw-synthetic data mix is also more\neffective than having access to 2x web data. Through further analysis, we\ndemonstrate that about 82% of the mixed in texts come from transforming\nlower-quality documents that would otherwise be discarded. REWIRE also\noutperforms related approaches of generating synthetic data, including\nWikipedia-style paraphrasing, question-answer synthesizing and knowledge\nextraction. These results suggest that recycling web texts holds the potential\nfor being a simple and effective approach for scaling pre-training data. We\nmake our high-quality synthetic data publicly available at\nhttps://huggingface.co/datasets/facebook/recycling_the_web."}
{"id": "2506.16633", "pdf": "https://arxiv.org/pdf/2506.16633.pdf", "abs": "https://arxiv.org/abs/2506.16633", "title": "GeoGuess: Multimodal Reasoning based on Hierarchy of Visual Information in Street View", "authors": ["Fenghua Cheng", "Jinxiang Wang", "Sen Wang", "Zi Huang", "Xue Li"], "categories": ["cs.CL", "cs.AI", "cs.MM"], "comment": "Updated version", "summary": "Multimodal reasoning is a process of understanding, integrating and inferring\ninformation across different data modalities. It has recently attracted surging\nacademic attention as a benchmark for Artificial Intelligence (AI). Although\nthere are various tasks for evaluating multimodal reasoning ability, they still\nhave limitations. Lack of reasoning on hierarchical visual clues at different\nlevels of granularity, e.g., local details and global context, is of little\ndiscussion, despite its frequent involvement in real scenarios. To bridge the\ngap, we introduce a novel and challenging task for multimodal reasoning, namely\nGeoGuess. Given a street view image, the task is to identify its location and\nprovide a detailed explanation. A system that succeeds in GeoGuess should be\nable to detect tiny visual clues, perceive the broader landscape, and associate\nwith vast geographic knowledge. Therefore, GeoGuess would require the ability\nto reason between hierarchical visual information and geographic knowledge. In\nthis work, we establish a benchmark for GeoGuess by introducing a specially\ncurated dataset GeoExplain which consists of\npanoramas-geocoordinates-explanation tuples. Additionally, we present a\nmultimodal and multilevel reasoning method, namely SightSense which can make\nprediction and generate comprehensive explanation based on hierarchy of visual\ninformation and external knowledge. Our analysis and experiments demonstrate\ntheir outstanding performance in GeoGuess."}
{"id": "2506.18998", "pdf": "https://arxiv.org/pdf/2506.18998.pdf", "abs": "https://arxiv.org/abs/2506.18998", "title": "Mirage of Mastery: Memorization Tricks LLMs into Artificially Inflated Self-Knowledge", "authors": ["Sahil Kale", "Vijaykant Nadadur"], "categories": ["cs.CL"], "comment": "11 pages, 9 figures", "summary": "When artificial intelligence mistakes memorization for intelligence, it\ncreates a dangerous mirage of reasoning. Existing studies treat memorization\nand self-knowledge deficits in LLMs as separate issues and do not recognize an\nintertwining link that degrades the trustworthiness of LLM responses. In our\nstudy, we utilize a novel framework to ascertain if LLMs genuinely learn\nreasoning patterns from training data or merely memorize them to assume\ncompetence across problems of similar complexity focused on STEM domains. Our\nanalysis shows a noteworthy problem in generalization: LLMs draw confidence\nfrom memorized solutions to infer a higher self-knowledge about their reasoning\nability, which manifests as an over 45% inconsistency in feasibility\nassessments when faced with self-validated, logically coherent task\nperturbations. This effect is most pronounced in science and medicine domains,\nwhich tend to have maximal standardized jargon and problems, further confirming\nour approach. Significant wavering within the self-knowledge of LLMs also shows\nflaws in current architectures and training patterns, highlighting the need for\ntechniques that ensure a balanced, consistent stance on models' perceptions of\ntheir own knowledge for maximum AI explainability and trustworthiness. Our code\nand results are available publicly at\nhttps://github.com/knowledge-verse-ai/LLM-Memorization_SK_Eval-."}
{"id": "2506.20474", "pdf": "https://arxiv.org/pdf/2506.20474.pdf", "abs": "https://arxiv.org/abs/2506.20474", "title": "Time is On My Side: Dynamics of Talk-Time Sharing in Video-chat Conversations", "authors": ["Kaixiang Zhang", "Justine Zhang", "Cristian Danescu-Niculescu-Mizil"], "categories": ["cs.CL"], "comment": "Accepted for publication at CSCW 2025. Code and data available in\n  ConvoKit (https://convokit.cornell.edu)", "summary": "An intrinsic aspect of every conversation is the way talk-time is shared\nbetween multiple speakers. Conversations can be balanced, with each speaker\nclaiming a similar amount of talk-time, or imbalanced when one talks\ndisproportionately. Such overall distributions are the consequence of\ncontinuous negotiations between the speakers throughout the conversation: who\nshould be talking at every point in time, and for how long? In this work we\nintroduce a computational framework for quantifying both the conversation-level\ndistribution of talk-time between speakers, as well as the lower-level dynamics\nthat lead to it. We derive a typology of talk-time sharing dynamics structured\nby several intuitive axes of variation. By applying this framework to a large\ndataset of video-chats between strangers, we confirm that, perhaps\nunsurprisingly, different conversation-level distributions of talk-time are\nperceived differently by speakers, with balanced conversations being preferred\nover imbalanced ones, especially by those who end up talking less. Then we\nreveal that -- even when they lead to the same level of overall balance --\ndifferent types of talk-time sharing dynamics are perceived differently by the\nparticipants, highlighting the relevance of our newly introduced typology.\nFinally, we discuss how our framework offers new tools to designers of\ncomputer-mediated communication platforms, for both human-human and human-AI\ncommunication."}
{"id": "2506.21587", "pdf": "https://arxiv.org/pdf/2506.21587.pdf", "abs": "https://arxiv.org/abs/2506.21587", "title": "A Cross-Cultural Comparison of LLM-based Public Opinion Simulation: Evaluating Chinese and U.S. Models on Diverse Societies", "authors": ["Weihong Qi", "Fan Huang", "Jisun An", "Haewoon Kwak"], "categories": ["cs.CL"], "comment": null, "summary": "This study evaluates the ability of DeepSeek, an open-source large language\nmodel (LLM), to simulate public opinions in comparison to LLMs developed by\nmajor tech companies. By comparing DeepSeek-R1 and DeepSeek-V3 with Qwen2.5,\nGPT-4o, and Llama-3.3 and utilizing survey data from the American National\nElection Studies (ANES) and the Zuobiao dataset of China, we assess these\nmodels' capacity to predict public opinions on social issues in both China and\nthe United States, highlighting their comparative capabilities between\ncountries. Our findings indicate that DeepSeek-V3 performs best in simulating\nU.S. opinions on the abortion issue compared to other topics such as climate\nchange, gun control, immigration, and services for same-sex couples, primarily\nbecause it more accurately simulates responses when provided with Democratic or\nliberal personas. For Chinese samples, DeepSeek-V3 performs best in simulating\nopinions on foreign aid and individualism but shows limitations in modeling\nviews on capitalism, particularly failing to capture the stances of low-income\nand non-college-educated individuals. It does not exhibit significant\ndifferences from other models in simulating opinions on traditionalism and the\nfree market. Further analysis reveals that all LLMs exhibit the tendency to\novergeneralize a single perspective within demographic groups, often defaulting\nto consistent responses within groups. These findings highlight the need to\nmitigate cultural and demographic biases in LLM-driven public opinion modeling,\ncalling for approaches such as more inclusive training methodologies."}
{"id": "2506.21614", "pdf": "https://arxiv.org/pdf/2506.21614.pdf", "abs": "https://arxiv.org/abs/2506.21614", "title": "LastingBench: Defend Benchmarks Against Knowledge Leakage", "authors": ["Yixiong Fang", "Tianran Sun", "Yuling Shi", "Min Wang", "Xiaodong Gu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The increasing complexity of large language models (LLMs) raises concerns\nabout their ability to \"cheat\" on standard Question Answering (QA) benchmarks\nby memorizing task-specific data. This undermines the validity of benchmark\nevaluations, as they no longer reflect genuine model capabilities but instead\nthe effects of data leakage. While prior work has focused on detecting such\nleakage, little attention has been given to mitigating its impact and\npreserving the long-term utility of benchmarks. In this paper, we introduce\nLastingBench, a novel framework designed to continuously reinforce and\nsafeguard existing benchmarks against knowledge leakage. LastingBench\nidentifies leakage points in the context through perturbation, then rewrites\nthe leakage points to counterfactual ones-disrupting memorization while\npreserving the benchmark's original evaluative intent. Evaluations of\nstate-of-the-art QA benchmarks show significant performance gaps, highlighting\nthe efficacy of LastingBench in reducing memorization effects. LastingBench\noffers a practical and scalable solution to ensure benchmark robustness over\ntime, promoting fairer and more interpretable evaluations of LLMs."}
{"id": "2507.03009", "pdf": "https://arxiv.org/pdf/2507.03009.pdf", "abs": "https://arxiv.org/abs/2507.03009", "title": "PDFMathTranslate: Scientific Document Translation Preserving Layouts", "authors": ["Rongxin Ouyang", "Chang Chu", "Zhikuang Xin", "Xiangyao Ma"], "categories": ["cs.CL", "cs.IR", "cs.LG", "68T50, 68T45, 68U10, 68U15", "D.2.2; I.2.10; I.2.7; J.0"], "comment": "7 pages, 4 figures, EMNLP 2025 Demo", "summary": "Language barriers in scientific documents hinder the diffusion and\ndevelopment of science and technologies. However, prior efforts in translating\nsuch documents largely overlooked the information in layouts. To bridge the\ngap, we introduce PDFMathTranslate, the world's first open-source software for\ntranslating scientific documents while preserving layouts. Leveraging the most\nrecent advances in large language models and precise layout detection, we\ncontribute to the community with key improvements in precision, flexibility,\nand efficiency. The work has been open-sourced at\nhttps://github.com/byaidu/pdfmathtranslate with more than 222k downloads."}
{"id": "2507.13380", "pdf": "https://arxiv.org/pdf/2507.13380.pdf", "abs": "https://arxiv.org/abs/2507.13380", "title": "Persona-Based Synthetic Data Generation Using Multi-Stage Conditioning with Large Language Models for Emotion Recognition", "authors": ["Keito Inoshita", "Rushia Harada"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In the field of emotion recognition, the development of high-performance\nmodels remains a challenge due to the scarcity of high-quality, diverse\nemotional datasets. Emotional expressions are inherently subjective, shaped by\nindividual personality traits, socio-cultural backgrounds, and contextual\nfactors, making large-scale, generalizable data collection both ethically and\npractically difficult. To address this issue, we introduce PersonaGen, a novel\nframework for generating emotionally rich text using a Large Language Model\n(LLM) through multi-stage persona-based conditioning. PersonaGen constructs\nlayered virtual personas by combining demographic attributes, socio-cultural\nbackgrounds, and detailed situational contexts, which are then used to guide\nemotion expression generation. We conduct comprehensive evaluations of the\ngenerated synthetic data, assessing semantic diversity through clustering and\ndistributional metrics, human-likeness via LLM-based quality scoring, realism\nthrough comparison with real-world emotion corpora, and practical utility in\ndownstream emotion classification tasks. Experimental results show that\nPersonaGen significantly outperforms baseline methods in generating diverse,\ncoherent, and discriminative emotion expressions, demonstrating its potential\nas a robust alternative for augmenting or replacing real-world emotional\ndatasets."}
{"id": "2508.06165", "pdf": "https://arxiv.org/pdf/2508.06165.pdf", "abs": "https://arxiv.org/abs/2508.06165", "title": "UR$^2$: Unify RAG and Reasoning through Reinforcement Learning", "authors": ["Weitao Li", "Boran Xiang", "Xiaolong Wang", "Zhinan Gou", "Weizhi Ma", "Yang Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable capabilities through two\ncomplementary paradigms: Retrieval-Augmented Generation (RAG), which enhances\nknowledge grounding, and Reinforcement Learning from Verifiable Rewards (RLVR),\nwhich optimizes complex reasoning abilities. However, these two capabilities\nare often developed in isolation, and existing efforts to unify them remain\nnarrow in scope -- typically limited to open-domain QA with fixed retrieval\nsettings and task-specific constraints. This lack of integration constrains\ngeneralization and limits the applicability of RAG-RL methods to broader\ndomains. To bridge this gap, we propose UR2 (Unified RAG and Reasoning), a\ngeneral framework that unifies retrieval and reasoning through reinforcement\nlearning. UR2 introduces two key contributions: a difficulty-aware curriculum\ntraining that selectively invokes retrieval only for challenging problems, and\na hybrid knowledge access strategy combining domain-specific offline corpora\nwith LLM-generated summaries. These components are designed to enable dynamic\ncoordination between retrieval and reasoning, improving adaptability across a\ndiverse range of tasks. Experiments across open-domain QA, MMLU-Pro, medical,\nand mathematical reasoning tasks demonstrate that UR$^2$ (built on\nQwen-2.5-3/7B and LLaMA-3.1-8B) significantly outperforms existing RAG and RL\nmethods, achieving comparable performance to GPT-4o-mini and GPT-4.1-mini on\nseveral benchmarks. We have released all code, models, and data at\nhttps://github.com/Tsinghua-dhy/UR2."}
{"id": "2508.14723", "pdf": "https://arxiv.org/pdf/2508.14723.pdf", "abs": "https://arxiv.org/abs/2508.14723", "title": "Transplant Then Regenerate: A New Paradigm for Text Data Augmentation", "authors": ["Guangzhan Wang", "Hongyu Zhang", "Beijun Shen", "Xiaodong Gu"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by EMNLP 2025", "summary": "Data augmentation is a critical technique in deep learning. Traditional\nmethods like Back-translation typically focus on lexical-level rephrasing,\nwhich primarily produces variations with the same semantics. While large\nlanguage models (LLMs) have enhanced text augmentation by their \"knowledge\nemergence\" capability, controlling the style and structure of these outputs\nremains challenging and requires meticulous prompt engineering. In this paper,\nwe propose LMTransplant, a novel text augmentation paradigm leveraging LLMs.\nThe core idea of LMTransplant is transplant-then-regenerate: incorporating seed\ntext into a context expanded by LLM, and asking the LLM to regenerate a variant\nbased on the expanded context. This strategy allows the model to create more\ndiverse and creative content-level variants by fully leveraging the knowledge\nembedded in LLMs, while preserving the core attributes of the original text. We\nevaluate LMTransplant across various text-related tasks, demonstrating its\nsuperior performance over existing text augmentation methods. Moreover,\nLMTransplant demonstrates exceptional scalability as the size of augmented data\ngrows."}
{"id": "2508.17573", "pdf": "https://arxiv.org/pdf/2508.17573.pdf", "abs": "https://arxiv.org/abs/2508.17573", "title": "Humanizing Machines: Rethinking LLM Anthropomorphism Through a Multi-Level Framework of Design", "authors": ["Yunze Xiao", "Lynnette Hui Xian Ng", "Jiarui Liu", "Mona T. Diab"], "categories": ["cs.CL"], "comment": "Accepted in EMNLP main proceedings; Updated citations", "summary": "Large Language Models (LLMs) increasingly exhibit \\textbf{anthropomorphism}\ncharacteristics -- human-like qualities portrayed across their outlook,\nlanguage, behavior, and reasoning functions. Such characteristics enable more\nintuitive and engaging human-AI interactions. However, current research on\nanthropomorphism remains predominantly risk-focused, emphasizing over-trust and\nuser deception while offering limited design guidance. We argue that\nanthropomorphism should instead be treated as a \\emph{concept of design} that\ncan be intentionally tuned to support user goals. Drawing from multiple\ndisciplines, we propose that the anthropomorphism of an LLM-based artifact\nshould reflect the interaction between artifact designers and interpreters.\nThis interaction is facilitated by cues embedded in the artifact by the\ndesigners and the (cognitive) responses of the interpreters to the cues. Cues\nare categorized into four dimensions: \\textit{perceptive, linguistic,\nbehavioral}, and \\textit{cognitive}. By analyzing the manifestation and\neffectiveness of each cue, we provide a unified taxonomy with actionable levers\nfor practitioners. Consequently, we advocate for function-oriented evaluations\nof anthropomorphic design."}
{"id": "2508.17610", "pdf": "https://arxiv.org/pdf/2508.17610.pdf", "abs": "https://arxiv.org/abs/2508.17610", "title": "Less Is More? Examining Fairness in Pruned Large Language Models for Summarising Opinions", "authors": ["Nannan Huang", "Haytham M. Fayek", "Xiuzhen Zhang"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "Model compression through post-training pruning offers a way to reduce model\nsize and computational requirements without significantly impacting model\nperformance. However, the effect of pruning on the fairness of LLM-generated\nsummaries remains unexplored, particularly for opinion summarisation where\nbiased outputs could influence public views.In this paper, we present a\ncomprehensive empirical analysis of opinion summarisation, examining three\nstate-of-the-art pruning methods and various calibration sets across three\nopen-source LLMs using four fairness metrics. Our systematic analysis reveals\nthat pruning methods have a greater impact on fairness than calibration sets.\nBuilding on these insights, we propose High Gradient Low Activation (HGLA)\npruning, which identifies and removes parameters that are redundant for input\nprocessing but influential in output generation. Our experiments demonstrate\nthat HGLA can better maintain or even improve fairness compared to existing\nmethods, showing promise across models and tasks where traditional methods have\nlimitations. Our human evaluation shows HGLA-generated outputs are fairer than\nexisting state-of-the-art pruning methods. Code is available at:\nhttps://github.com/amberhuang01/HGLA."}
{"id": "2508.17767", "pdf": "https://arxiv.org/pdf/2508.17767.pdf", "abs": "https://arxiv.org/abs/2508.17767", "title": "ISACL: Internal State Analyzer for Copyrighted Training Data Leakage", "authors": ["Guangwei Zhang", "Qisheng Su", "Jiateng Liu", "Cheng Qian", "Yanzhou Pan", "Yanjie Fu", "Denghui Zhang"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have revolutionized Natural Language Processing\n(NLP) but pose risks of inadvertently exposing copyrighted or proprietary data,\nespecially when such data is used for training but not intended for\ndistribution. Traditional methods address these leaks only after content is\ngenerated, which can lead to the exposure of sensitive information. This study\nintroduces a proactive approach: examining LLMs' internal states before text\ngeneration to detect potential leaks. By using a curated dataset of copyrighted\nmaterials, we trained a neural network classifier to identify risks, allowing\nfor early intervention by stopping the generation process or altering outputs\nto prevent disclosure. Integrated with a Retrieval-Augmented Generation (RAG)\nsystem, this framework ensures adherence to copyright and licensing\nrequirements while enhancing data privacy and ethical standards. Our results\nshow that analyzing internal states effectively mitigates the risk of\ncopyrighted data leakage, offering a scalable solution that fits smoothly into\nAI workflows, ensuring compliance with copyright regulations while maintaining\nhigh-quality text generation. The implementation is available on\nGitHub.\\footnote{https://github.com/changhu73/Internal_states_leakage}"}
{"id": "2508.18240", "pdf": "https://arxiv.org/pdf/2508.18240.pdf", "abs": "https://arxiv.org/abs/2508.18240", "title": "MTalk-Bench: Evaluating Speech-to-Speech Models in Multi-Turn Dialogues via Arena-style and Rubrics Protocols", "authors": ["Yuhao Du", "Qianwei Huang", "Guo Zhu", "Zhanchen Dai", "Shunian Chen", "Qiming Zhu", "Le Pan", "Minghao Chen", "Yuhao Zhang", "Li Zhou", "Benyou Wang", "Haizhou Li"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid advancement of speech-to-speech (S2S) large language models (LLMs)\nhas significantly improved real-time spoken interaction. However, current\nevaluation frameworks remain inadequate for assessing performance in complex,\nmulti-turn dialogues. To address this, we introduce MTalk-Bench, a multi-turn\nS2S benchmark covering three core dimensions: Semantic Information,\nParalinguistic Information, and Ambient Sound. Each dimension includes nine\nrealistic scenarios, along with targeted tasks to assess specific capabilities\nsuch as reasoning. Our dual-method evaluation framework combines Arena-style\nevaluation (pairwise comparison) and Rubrics-based evaluation (absolute\nscoring) for relative and absolute assessment. The benchmark includes both\nmodel and human outputs, evaluated by human evaluators and LLMs. Experimental\nresults reveal two sets of findings. Overall performance of S2S LLMs: (1)\nmodels excel at semantic information processing yet underperform on\nparalinguistic information and ambient sounds perception; (2) models typically\nregain coherence by increasing response length, sacrificing efficiency in\nmulti-turn dialogues; (3) modality-aware, task-specific designs outperform\nbrute scaling. Evaluation framework and reliability: (1) Arena and Rubrics\nyield consistent, complementary rankings, but reliable distinctions emerge only\nwhen performance gaps are large; (2) LLM-as-a-judge aligns with humans when\ngaps are clear or criteria explicit, but exhibits position and length biases\nand is reliable on nonverbal evaluation only with text annotations. These\nresults highlight current limitations in S2S evaluation and the need for more\nrobust, speech-aware assessment frameworks."}
{"id": "2508.20047", "pdf": "https://arxiv.org/pdf/2508.20047.pdf", "abs": "https://arxiv.org/abs/2508.20047", "title": "AraHealthQA 2025: The First Shared Task on Arabic Health Question Answering", "authors": ["Hassan Alhuzali", "Walid Al-Eisawi", "Muhammad Abdul-Mageed", "Chaimae Abouzahir", "Mouath Abu-Daoud", "Ashwag Alasmari", "Renad Al-Monef", "Ali Alqahtani", "Lama Ayash", "Leen Kharouf", "Farah E. Shamout", "Nizar Habash"], "categories": ["cs.CL"], "comment": "ArabicNLP2025-colocated with EMNLP2025", "summary": "We introduce AraHealthQA 2025, the Comprehensive Arabic Health Question\nAnswering Shared Task, held in conjunction with ArabicNLP 2025 (co-located with\nEMNLP 2025). This shared task addresses the paucity of high-quality Arabic\nmedical QA resources by offering two complementary tracks: MentalQA, focusing\non Arabic mental health Q&A (e.g., anxiety, depression, stigma reduction), and\nMedArabiQ, covering broader medical domains such as internal medicine,\npediatrics, and clinical decision making. Each track comprises multiple\nsubtasks, evaluation datasets, and standardized metrics, facilitating fair\nbenchmarking. The task was structured to promote modeling under realistic,\nmultilingual, and culturally nuanced healthcare contexts. We outline the\ndataset creation, task design and evaluation framework, participation\nstatistics, baseline systems, and summarize the overall outcomes. We conclude\nwith reflections on the performance trends observed and prospects for future\niterations in Arabic health QA."}
{"id": "2509.08486", "pdf": "https://arxiv.org/pdf/2509.08486.pdf", "abs": "https://arxiv.org/abs/2509.08486", "title": "Too Helpful, Too Harmless, Too Honest or Just Right?", "authors": ["Gautam Siddharth Kashyap", "Mark Dras", "Usman Naseem"], "categories": ["cs.CL"], "comment": "EMNLP'25 Main", "summary": "Large Language Models (LLMs) exhibit strong performance across a wide range\nof NLP tasks, yet aligning their outputs with the principles of Helpfulness,\nHarmlessness, and Honesty (HHH) remains a persistent challenge. Existing\nmethods often optimize for individual alignment dimensions in isolation,\nleading to trade-offs and inconsistent behavior. While Mixture-of-Experts (MoE)\narchitectures offer modularity, they suffer from poorly calibrated routing,\nlimiting their effectiveness in alignment tasks. We propose TrinityX, a modular\nalignment framework that incorporates a Mixture of Calibrated Experts (MoCaE)\nwithin the Transformer architecture. TrinityX leverages separately trained\nexperts for each HHH dimension, integrating their outputs through a calibrated,\ntask-adaptive routing mechanism that combines expert signals into a unified,\nalignment-aware representation. Extensive experiments on three standard\nalignment benchmarks-Alpaca (Helpfulness), BeaverTails (Harmlessness), and\nTruthfulQA (Honesty)-demonstrate that TrinityX outperforms strong baselines,\nachieving relative improvements of 32.5% in win rate, 33.9% in safety score,\nand 28.4% in truthfulness. In addition, TrinityX reduces memory usage and\ninference latency by over 40% compared to prior MoE-based approaches. Ablation\nstudies highlight the importance of calibrated routing, and cross-model\nevaluations confirm TrinityX's generalization across diverse LLM backbones."}
{"id": "2509.08541", "pdf": "https://arxiv.org/pdf/2509.08541.pdf", "abs": "https://arxiv.org/abs/2509.08541", "title": "CM-Align: Consistency-based Multilingual Alignment for Large Language Models", "authors": ["Xue Zhang", "Yunlong Liang", "Fandong Meng", "Songming Zhang", "Yufeng Chen", "Jinan Xu", "Jie Zhou"], "categories": ["cs.CL"], "comment": "EMNLP 2025 Findings", "summary": "Current large language models (LLMs) generally show a significant performance\ngap in alignment between English and other languages. To bridge this gap,\nexisting research typically leverages the model's responses in English as a\nreference to select the best/worst responses in other languages, which are then\nused for Direct Preference Optimization (DPO) training. However, we argue that\nthere are two limitations in the current methods that result in noisy\nmultilingual preference data and further limited alignment performance: 1) Not\nall English responses are of high quality, and using a response with low\nquality may mislead the alignment for other languages. 2) Current methods\nusually use biased or heuristic approaches to construct multilingual preference\npairs. To address these limitations, we design a consistency-based data\nselection method to construct high-quality multilingual preference data for\nimproving multilingual alignment (CM-Align). Specifically, our method includes\ntwo parts: consistency-guided English reference selection and cross-lingual\nconsistency-based multilingual preference data construction. Experimental\nresults on three LLMs and three common tasks demonstrate the effectiveness and\nsuperiority of our method, which further indicates the necessity of\nconstructing high-quality preference data."}
{"id": "2509.09198", "pdf": "https://arxiv.org/pdf/2509.09198.pdf", "abs": "https://arxiv.org/abs/2509.09198", "title": "GmSLM : Generative Marmoset Spoken Language Modeling", "authors": ["Talia Sternberg", "Michael London", "David Omer", "Yossi Adi"], "categories": ["cs.CL"], "comment": null, "summary": "Marmoset monkeys exhibit complex vocal communication, challenging the view\nthat nonhuman primates vocal communication is entirely innate, and show similar\nfeatures of human speech, such as vocal labeling of others and turn-taking.\nStudying their vocal communication offers a unique opportunity to link it with\nbrain activity-especially given the difficulty of accessing the human brain in\nspeech and language research. Since Marmosets communicate primarily through\nvocalizations, applying standard LLM approaches is not straightforward. We\nintroduce Generative Marmoset Spoken Language Modeling (GmSLM), an optimized\nspoken language model pipeline for Marmoset vocal communication. We designed a\nnovel zero-shot evaluation metrics using unsupervised in-the-wild data,\nalongside weakly labeled conversational data, to assess GmSLM and demonstrate\nits advantage over a basic human-speech-based baseline. GmSLM generated\nvocalizations closely matched real resynthesized samples acoustically and\nperformed well on downstream tasks. Despite being fully unsupervised, GmSLM\neffectively distinguish real from artificial conversations and may support\nfurther investigations of the neural basis of vocal communication and provides\na practical framework linking vocalization and brain activity. We believe GmSLM\nstands to benefit future work in neuroscience, bioacoustics, and evolutionary\nbiology. Samples are provided under: pages.cs.huji.ac.il/adiyoss-lab/GmSLM."}
{"id": "2509.10129", "pdf": "https://arxiv.org/pdf/2509.10129.pdf", "abs": "https://arxiv.org/abs/2509.10129", "title": "Towards Reliable and Interpretable Document Question Answering via VLMs", "authors": ["Alessio Chen", "Simone Giovannini", "Andrea Gemelli", "Fabio Coppini", "Simone Marinai"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Vision-Language Models (VLMs) have shown strong capabilities in document\nunderstanding, particularly in identifying and extracting textual information\nfrom complex documents. Despite this, accurately localizing answers within\ndocuments remains a major challenge, limiting both interpretability and\nreal-world applicability. To address this, we introduce DocExplainerV0, a\nplug-and-play bounding-box prediction module that decouples answer generation\nfrom spatial localization. This design makes it applicable to existing VLMs,\nincluding proprietary systems where fine-tuning is not feasible. Through\nsystematic evaluation, we provide quantitative insights into the gap between\ntextual accuracy and spatial grounding, showing that correct answers often lack\nreliable localization. Our standardized framework highlights these shortcomings\nand establishes a benchmark for future research toward more interpretable and\nrobust document information extraction VLMs."}
{"id": "2509.10414", "pdf": "https://arxiv.org/pdf/2509.10414.pdf", "abs": "https://arxiv.org/abs/2509.10414", "title": "Is In-Context Learning Learning?", "authors": ["Adrian de Wynter"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Director's cut", "summary": "In-context learning (ICL) allows some autoregressive models to solve tasks\nvia next-token prediction and without needing further training. This has led to\nclaims about these model's ability to solve (learn) unseen tasks with only a\nfew shots (exemplars) in the prompt. However, deduction does not always imply\nlearning, as ICL does not explicitly encode a given observation. Instead, the\nmodels rely on their prior knowledge and the exemplars given, if any. We argue\nthat, mathematically, ICL does constitute learning, but its full\ncharacterisation requires empirical work. We then carry out a large-scale\nanalysis of ICL ablating out or accounting for memorisation, pretraining,\ndistributional shifts, and prompting style and phrasing. We find that ICL is an\neffective learning paradigm, but limited in its ability to learn and generalise\nto unseen tasks. We note that, in the limit where exemplars become more\nnumerous, accuracy is insensitive to exemplar distribution, model, prompt\nstyle, and the input's linguistic features. Instead, it deduces patterns from\nregularities in the prompt, which leads to distributional sensitivity,\nespecially in prompting styles such as chain-of-thought. Given the varied\naccuracies on formally similar tasks, we conclude that autoregression's ad-hoc\nencoding is not a robust mechanism, and suggests limited all-purpose\ngeneralisability."}
{"id": "2410.14827", "pdf": "https://arxiv.org/pdf/2410.14827.pdf", "abs": "https://arxiv.org/abs/2410.14827", "title": "Enhancing Prompt Injection Attacks to LLMs via Poisoning Alignment", "authors": ["Zedian Shao", "Hongbin Liu", "Jaden Mu", "Neil Zhenqiang Gong"], "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Prompt injection attack, where an attacker injects a prompt into the original\none, aiming to make an Large Language Model (LLM) follow the injected prompt to\nperform an attacker-chosen task, represent a critical security threat. Existing\nattacks primarily focus on crafting these injections at inference time,\ntreating the LLM itself as a static target. Our experiments show that these\nattacks achieve some success, but there is still significant room for\nimprovement. In this work, we introduces a more foundational attack vector:\npoisoning the LLM's alignment process to amplify the success of future prompt\ninjection attacks. Specifically, we propose PoisonedAlign, a method that\nstrategically creates poisoned alignment samples to poison an LLM's alignment\ndataset. Our experiments across five LLMs and two alignment datasets show that\nwhen even a small fraction of the alignment data is poisoned, the resulting\nmodel becomes substantially more vulnerable to a wide range of prompt injection\nattacks. Crucially, this vulnerability is instilled while the LLM's performance\non standard capability benchmarks remains largely unchanged, making the\nmanipulation difficult to detect through automated, general-purpose performance\nevaluations. The code for implementing the attack is available at\nhttps://github.com/Sadcardation/PoisonedAlign."}
{"id": "2412.14222", "pdf": "https://arxiv.org/pdf/2412.14222.pdf", "abs": "https://arxiv.org/abs/2412.14222", "title": "A Survey on Large Language Model-based Agents for Statistics and Data Science", "authors": ["Maojun Sun", "Ruijian Han", "Binyan Jiang", "Houduo Qi", "Defeng Sun", "Yancheng Yuan", "Jian Huang"], "categories": ["cs.AI", "cs.CL", "cs.LG", "stat.OT"], "comment": null, "summary": "In recent years, data science agents powered by Large Language Models (LLMs),\nknown as \"data agents,\" have shown significant potential to transform the\ntraditional data analysis paradigm. This survey provides an overview of the\nevolution, capabilities, and applications of LLM-based data agents,\nhighlighting their role in simplifying complex data tasks and lowering the\nentry barrier for users without related expertise. We explore current trends in\nthe design of LLM-based frameworks, detailing essential features such as\nplanning, reasoning, reflection, multi-agent collaboration, user interface,\nknowledge integration, and system design, which enable agents to address\ndata-centric problems with minimal human intervention. Furthermore, we analyze\nseveral case studies to demonstrate the practical applications of various data\nagents in real-world scenarios. Finally, we identify key challenges and propose\nfuture research directions to advance the development of data agents into\nintelligent statistical analysis software."}
{"id": "2503.06646", "pdf": "https://arxiv.org/pdf/2503.06646.pdf", "abs": "https://arxiv.org/abs/2503.06646", "title": "Evaluating and Aligning Human Economic Risk Preferences in LLMs", "authors": ["Jiaxin Liu", "Yixuan Tang", "Yi Yang", "Kar Yan Tam"], "categories": ["econ.GN", "cs.CL", "q-fin.EC"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly used in decision-making\nscenarios that involve risk assessment, yet their alignment with human economic\nrationality remains unclear. In this study, we investigate whether LLMs exhibit\nrisk preferences consistent with human expectations across different personas.\nSpecifically, we assess whether LLM-generated responses reflect appropriate\nlevels of risk aversion or risk-seeking behavior based on individual's persona.\nOur results reveal that while LLMs make reasonable decisions in simplified,\npersonalized risk contexts, their performance declines in more complex economic\ndecision-making tasks. To address this, we propose an alignment method designed\nto enhance LLM adherence to persona-specific risk preferences. Our approach\nimproves the economic rationality of LLMs in risk-related applications,\noffering a step toward more human-aligned AI decision-making."}
{"id": "2503.12301", "pdf": "https://arxiv.org/pdf/2503.12301.pdf", "abs": "https://arxiv.org/abs/2503.12301", "title": "One Goal, Many Challenges: Robust Preference Optimization Amid Content-Aware and Multi-Source Noise", "authors": ["Amirabbas Afzali", "Amirhossein Afsharrad", "Seyed Shahabeddin Mousavi", "Sanjay Lall"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have made significant strides in generating\nhuman-like responses, largely due to preference alignment techniques. However,\nthese methods often assume unbiased human feedback, which is rarely the case in\nreal-world scenarios. This paper introduces Content-Aware Noise-Resilient\nPreference Optimization (CNRPO), a novel framework that addresses multiple\nsources of content-dependent noise in preference learning. CNRPO employs a\nmulti-objective optimization approach to separate true preferences from\ncontent-aware noises, effectively mitigating their impact. We leverage backdoor\nattack mechanisms to efficiently learn and control various noise sources within\na single model. Theoretical analysis and extensive experiments on different\nsynthetic noisy datasets demonstrate that CNRPO significantly improves\nalignment with primary human preferences while controlling for secondary noises\nand biases, such as response length and harmfulness."}
{"id": "2503.16974", "pdf": "https://arxiv.org/pdf/2503.16974.pdf", "abs": "https://arxiv.org/abs/2503.16974", "title": "Assessing Consistency and Reproducibility in the Outputs of Large Language Models: Evidence Across Diverse Finance and Accounting Tasks", "authors": ["Julian Junyan Wang", "Victor Xiaoqi Wang"], "categories": ["q-fin.GN", "cs.AI", "cs.CE", "cs.CL", "cs.LG"], "comment": "76 pages, 20 tables, 12 figures", "summary": "This study provides the first comprehensive assessment of consistency and\nreproducibility in Large Language Model (LLM) outputs in finance and accounting\nresearch. We evaluate how consistently LLMs produce outputs given identical\ninputs through extensive experimentation with 50 independent runs across five\ncommon tasks: classification, sentiment analysis, summarization, text\ngeneration, and prediction. Using three OpenAI models (GPT-3.5-turbo,\nGPT-4o-mini, and GPT-4o), we generate over 3.4 million outputs from diverse\nfinancial source texts and data, covering MD&As, FOMC statements, finance news\narticles, earnings call transcripts, and financial statements. Our findings\nreveal substantial but task-dependent consistency, with binary classification\nand sentiment analysis achieving near-perfect reproducibility, while complex\ntasks show greater variability. More advanced models do not consistently\ndemonstrate better consistency and reproducibility, with task-specific patterns\nemerging. LLMs significantly outperform expert human annotators in consistency\nand maintain high agreement even where human experts significantly disagree. We\nfurther find that simple aggregation strategies across 3-5 runs dramatically\nimprove consistency. We also find that aggregation may come with an additional\nbenefit of improved accuracy for sentiment analysis when using newer models.\nSimulation analysis reveals that despite measurable inconsistency in LLM\noutputs, downstream statistical inferences remain remarkably robust. These\nfindings address concerns about what we term \"G-hacking,\" the selective\nreporting of favorable outcomes from multiple generative AI runs, by\ndemonstrating that such risks are relatively low for finance and accounting\ntasks."}
{"id": "2503.19605", "pdf": "https://arxiv.org/pdf/2503.19605.pdf", "abs": "https://arxiv.org/abs/2503.19605", "title": "Lean Formalization of Generalization Error Bound by Rademacher Complexity", "authors": ["Sho Sonoda", "Kazumi Kasaura", "Yuma Mizuno", "Kei Tsukamoto", "Naoto Onda"], "categories": ["cs.LG", "cs.CL", "math.ST", "stat.TH"], "comment": "major updated", "summary": "We formalize the generalization error bound using the Rademacher complexity\nfor the Lean 4 theorem prover based on the probability theory in the Mathlib 4\nlibrary. Generalization error quantifies the gap between a learning machine's\nperformance on given training data versus unseen test data, and the Rademacher\ncomplexity is a powerful tool to upper-bound the generalization error of a\nvariety of modern learning problems. Previous studies have only formalized\nextremely simple cases such as bounds by parameter counts and analyses for very\nsimple models (decision stumps). Formalizing the Rademacher complexity bound,\nalso known as the uniform law of large numbers, requires substantial\ndevelopment and is achieved for the first time in this study. In the course of\ndevelopment, we formalize the Rademacher complexity and its unique arguments\nsuch as symmetrization, and clarify the topological assumptions on hypothesis\nclasses under which the bound holds. As an application, we also present the\nformalization of generalization error bound for $L^2$-regularization models."}
{"id": "2504.11889", "pdf": "https://arxiv.org/pdf/2504.11889.pdf", "abs": "https://arxiv.org/abs/2504.11889", "title": "Rethinking LLM-Based Recommendations: A Personalized Query-Driven Parallel Integration", "authors": ["Donghee Han", "Hwanjun Song", "Mun Yong Yi"], "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Recent studies have explored integrating large language models (LLMs) into\nrecommendation systems but face several challenges, including training-induced\nbias and bottlenecks from serialized architecture. To effectively address these\nissues, we propose a Query-toRecommendation, a parallel recommendation\nframework that decouples LLMs from candidate pre-selection and instead enables\ndirect retrieval over the entire item pool. Our framework connects LLMs and\nrecommendation models in a parallel manner, allowing each component to\nindependently utilize its strengths without interfering with the other. In this\nframework, LLMs are utilized to generate feature-enriched item descriptions and\npersonalized user queries, allowing for capturing diverse preferences and\nenabling rich semantic matching in a zero-shot manner. To effectively combine\nthe complementary strengths of LLM and collaborative signals, we introduce an\nadaptive reranking strategy. Extensive experiments demonstrate an improvement\nin performance up to 57%, while also improving the novelty and diversity of\nrecommendations."}
{"id": "2505.00831", "pdf": "https://arxiv.org/pdf/2505.00831.pdf", "abs": "https://arxiv.org/abs/2505.00831", "title": "SmallPlan: Leverage Small Language Models for Sequential Path Planning with Simulation-Powered, LLM-Guided Distillation", "authors": ["Quang P. M. Pham", "Khoi T. N. Nguyen", "Nhi H. Doan", "Cuong A. Pham", "Qinbo Sun", "Weimin Qi", "Kentaro Inui", "Dezhen Song"], "categories": ["cs.RO", "cs.CL"], "comment": "Paper is under review", "summary": "Efficient path planning in robotics, particularly within large-scale, complex\nenvironments, remains a significant hurdle. While Large Language Models (LLMs)\noffer strong reasoning capabilities, their high computational cost and limited\nadaptability hinder real-time deployment on edge devices. We present SmallPlan\n- a novel framework leveraging LLMs as teacher models to train lightweight\nSmall Language Models (SLMs) for high-level path planning tasks. In SmallPlan,\nthe SLMs provide optimal action sequences to navigate across scene graphs that\ncompactly represent full-scaled 3D scenes. The SLMs are trained in a\nsimulation-powered, interleaved manner with LLM-guided supervised fine-tuning\n(SFT) and reinforcement learning (RL). This strategy not only enables SLMs to\nsuccessfully complete navigation tasks but also makes them aware of important\nfactors like distance travel, providing more efficient path planning. Through\nexperiments, we demonstrate that the fine-tuned SLMs perform competitively with\nlarger models like GPT-4o on sequential path planning, without suffering from\nhallucination and overfitting. SmallPlan is resource-efficient, making it\nwell-suited for edge-device deployment and advancing practical autonomous\nrobotics. Our source code is available here:\nhttps://github.com/quangpham2006/SmallPlan"}
{"id": "2505.16146", "pdf": "https://arxiv.org/pdf/2505.16146.pdf", "abs": "https://arxiv.org/abs/2505.16146", "title": "Steering LVLMs via Sparse Autoencoder for Hallucination Mitigation", "authors": ["Zhenglin Hua", "Jinghan He", "Zijun Yao", "Tianxu Han", "Haiyun Guo", "Yuheng Jia", "Junfeng Fang"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted to Findings of EMNLP 2025", "summary": "Large vision-language models (LVLMs) have achieved remarkable performance on\nmultimodal tasks. However, they still suffer from hallucinations, generating\ntext inconsistent with visual input, posing significant risks in real-world\napplications. Existing approaches to address this issue focus on incorporating\nexternal knowledge bases, alignment training, or decoding strategies, all of\nwhich require substantial computational cost and time. Recent works try to\nexplore more efficient alternatives by adjusting LVLMs' internal\nrepresentations. Although promising, these methods may cause hallucinations to\nbe insufficiently suppressed or lead to excessive interventions that negatively\naffect normal semantics. In this work, we leverage sparse autoencoders (SAEs)\nto identify semantic directions closely associated with faithfulness or\nhallucination, extracting more precise and disentangled hallucination-related\nrepresentations. Our analysis demonstrates that interventions along the\nidentified faithful direction can mitigate hallucinations, while those along\nthe hallucinatory direction can exacerbate them. Building on these insights, we\npropose Steering LVLMs via SAE Latent Directions (SSL), a plug-and-play method\nbased on SAE-derived latent directions to mitigate hallucinations in LVLMs.\nExtensive experiments demonstrate that SSL significantly outperforms existing\ndecoding approaches in mitigating hallucinations, while maintaining\ntransferability across different model architectures with negligible additional\ntime overhead. The code is available at https://github.com/huazhenglin2003/SSL."}
{"id": "2505.18985", "pdf": "https://arxiv.org/pdf/2505.18985.pdf", "abs": "https://arxiv.org/abs/2505.18985", "title": "STRICT: Stress Test of Rendering Images Containing Text", "authors": ["Tianyu Zhang", "Xinyu Wang", "Lu Li", "Zhenghan Tai", "Jijun Chi", "Jingrui Tian", "Hailin He", "Suyuchen Wang"], "categories": ["cs.LG", "cs.CL", "cs.CV", "68T50", "I.2.7; I.4.0"], "comment": "Accepted as a main conference paper at EMNLP 2025", "summary": "While diffusion models have revolutionized text-to-image generation with\ntheir ability to synthesize realistic and diverse scenes, they continue to\nstruggle to generate consistent and legible text within images. This\nshortcoming is commonly attributed to the locality bias inherent in\ndiffusion-based generation, which limits their ability to model long-range\nspatial dependencies. In this paper, we introduce $\\textbf{STRICT}$, a\nbenchmark designed to systematically stress-test the ability of diffusion\nmodels to render coherent and instruction-aligned text in images. Our benchmark\nevaluates models across multiple dimensions: (1) the maximum length of readable\ntext that can be generated; (2) the correctness and legibility of the generated\ntext, and (3) the ratio of not following instructions for generating text. We\nevaluate several state-of-the-art models, including proprietary and open-source\nvariants, and reveal persistent limitations in long-range consistency and\ninstruction-following capabilities. Our findings provide insights into\narchitectural bottlenecks and motivate future research directions in multimodal\ngenerative modeling. We release our entire evaluation pipeline at\nhttps://github.com/tianyu-z/STRICT-Bench."}
{"id": "2506.04427", "pdf": "https://arxiv.org/pdf/2506.04427.pdf", "abs": "https://arxiv.org/abs/2506.04427", "title": "Plugging Schema Graph into Multi-Table QA: A Human-Guided Framework for Reducing LLM Reliance", "authors": ["Xixi Wang", "Miguel Costa", "Jordanka Kovaceva", "Shuai Wang", "Francisco C. Pereira"], "categories": ["cs.AI", "cs.CL"], "comment": "Accepted to EMNLP 2025 findings", "summary": "Large language models (LLMs) have shown promise in table Question Answering\n(Table QA). However, extending these capabilities to multi-table QA remains\nchallenging due to unreliable schema linking across complex tables. Existing\nmethods based on semantic similarity work well only on simplified hand-crafted\ndatasets and struggle to handle complex, real-world scenarios with numerous and\ndiverse columns. To address this, we propose a graph-based framework that\nleverages human-curated relational knowledge to explicitly encode schema links\nand join paths. Given a natural language query, our method searches on graph to\nconstruct interpretable reasoning chains, aided by pruning and sub-path merging\nstrategies to enhance efficiency and coherence. Experiments on both standard\nbenchmarks and a realistic, large-scale dataset demonstrate the effectiveness\nof our approach. To our knowledge, this is the first multi-table QA system\napplied to truly complex industrial tabular data."}
{"id": "2506.07233", "pdf": "https://arxiv.org/pdf/2506.07233.pdf", "abs": "https://arxiv.org/abs/2506.07233", "title": "Reducing Object Hallucination in Large Audio-Language Models via Audio-Aware Decoding", "authors": ["Tzu-wen Hsu", "Ke-Han Lu", "Cheng-Han Chiang", "Hung-yi Lee"], "categories": ["eess.AS", "cs.CL"], "comment": null, "summary": "Large Audio-Language Models (LALMs) can take audio and text as the inputs and\nanswer questions about the audio. While prior LALMs have shown strong\nperformance on standard benchmarks, there has been alarming evidence that LALMs\ncan hallucinate what is presented in the audio. To mitigate the hallucination\nof LALMs, we introduce Audio-Aware Decoding (AAD), a lightweight inference-time\nstrategy that uses contrastive decoding to compare the token prediction logits\nwith and without the audio context. By contrastive decoding, AAD promotes the\ntokens whose probability increases when the audio is present. We conduct our\nexperiment on object hallucination datasets with three LALMs and show that AAD\nimproves the F1 score by 0.046 to 0.428. We also show that AAD can improve the\naccuracy on general audio QA datasets like Clotho-AQA by 5.4% to 10.3%. We\nconduct thorough ablation studies to understand the effectiveness of each\ncomponent in AAD."}
{"id": "2506.10892", "pdf": "https://arxiv.org/pdf/2506.10892.pdf", "abs": "https://arxiv.org/abs/2506.10892", "title": "The Diffusion Duality", "authors": ["Subham Sekhar Sahoo", "Justin Deschenaux", "Aaron Gokaslan", "Guanghan Wang", "Justin Chiu", "Volodymyr Kuleshov"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "ICML 2025. We provide the code at: https://github.com/s-sahoo/duo\n  [v2]: Camera ready revisions", "summary": "Uniform-state discrete diffusion models hold the promise of fast text\ngeneration due to their inherent ability to self-correct. However, they are\ntypically outperformed by autoregressive models and masked diffusion models. In\nthis work, we narrow this performance gap by leveraging a key insight:\nUniform-state diffusion processes naturally emerge from an underlying Gaussian\ndiffusion. Our method, Duo, transfers powerful techniques from Gaussian\ndiffusion to improve both training and sampling. First, we introduce a\ncurriculum learning strategy guided by the Gaussian process, doubling training\nspeed by reducing variance. Models trained with curriculum learning surpass\nautoregressive models in zero-shot perplexity on 3 of 7 benchmarks. Second, we\npresent Discrete Consistency Distillation, which adapts consistency\ndistillation from the continuous to the discrete setting. This algorithm\nunlocks few-step generation in diffusion language models by accelerating\nsampling by two orders of magnitude. We provide the code and model checkpoints\non the project page: http://s-sahoo.github.io/duo"}
{"id": "2506.22809", "pdf": "https://arxiv.org/pdf/2506.22809.pdf", "abs": "https://arxiv.org/abs/2506.22809", "title": "Low-rank variational dropout: Uncertainty and rank selection in adapters", "authors": ["Cooper Doyle"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "5 pages, 2 figures", "summary": "Parameter-efficient fine-tuning (PEFT) methods such as LoRA adapt large\nlanguage models by inserting low-rank adapters, but they leave open two key\nquestions: how to give the adapted model calibrated uncertainty, and how to\nchoose the adapter rank. Existing approaches to uncertainty are typically\npost-hoc, while rank selection is manual and task-specific. BayesLoRA revisits\nvariational dropout in the LoRA setting and shows that the natural unit of\nstochasticity is not individual weights but entire ranks of the adapter. By\nplacing rank-wise variational distributions over adapter components, BayesLoRA\ndefines a posterior that (i) yields calibrated predictions through adapter-only\nMonte Carlo sampling and (ii) prunes redundant ranks automatically via an\nARD-style KL term. Theoretical analysis shows that this rank-parameterized\nposterior localizes uncertainty to the adapted subspace and explains\namplification under distribution shift. Empirically, BayesLoRA improves\ncalibration while at the same time producing lighter, faster adapters, removing\nthe need to tune ranks by hand. This dual role of uncertainty estimation and\nuncertainty-driven pruning suggests BayesLoRA may offer a practical default for\nreliable and efficient PEFT."}
{"id": "2508.18743", "pdf": "https://arxiv.org/pdf/2508.18743.pdf", "abs": "https://arxiv.org/abs/2508.18743", "title": "CAC-CoT: Connector-Aware Compact Chain-of-Thought for Efficient Reasoning Data Synthesis Across Dual-System Cognitive Tasks", "authors": ["Sunguk Choi", "Yonghoon Kwon", "Heondeuk Lee"], "categories": ["cs.AI", "cs.CL"], "comment": "Accepted at EMNLP 2025 findings", "summary": "Long chain-of-thought (CoT) prompting helps Large Language Models (LLMs)\nsolve difficult problems, but very long traces often slow or even degrade\nperformance on fast, intuitive \"System-1\" tasks. We introduce Connector-Aware\nCompact CoT (CAC-CoT) -- a method that deliberately restricts reasoning to a\nsmall, fixed set of connector phrases, steering the model toward concise and\nwell -- structured explanations. Despite its simplicity, our synthetic method\nwith general-purpose LLMs yields a high-quality training quality. CAC-CoT\nachieves approximately 85% on GSM8K and approximately 40% on GPQA (System-2)\nwhile also achieving approximately 85% on S1-Bench (System-1), surpassing the\nbaseline by over 20%. Its reasoning traces average approximately 300\ntokens(ART), about one-third the length of baseline traces, delivering higher\nefficiency without loss of accuracy."}
{"id": "2509.00115", "pdf": "https://arxiv.org/pdf/2509.00115.pdf", "abs": "https://arxiv.org/abs/2509.00115", "title": "Adaptive Monitoring and Real-World Evaluation of Agentic AI Systems", "authors": ["Manish Shukla"], "categories": ["cs.AI", "cs.CL", "cs.MA"], "comment": null, "summary": "Agentic artificial intelligence (AI) -- multi-agent systems that combine\nlarge language models with external tools and autonomous planning -- are\nrapidly transitioning from research laboratories into high-stakes domains. Our\nearlier \"Basic\" paper introduced a five-axis framework and proposed preliminary\nmetrics such as goal drift and harm reduction but did not provide an\nalgorithmic instantiation or empirical evidence. This \"Advanced\" sequel fills\nthat gap. First, we revisit recent benchmarks and industrial deployments to\nshow that technical metrics still dominate evaluations: a systematic review of\n84 papers from 2023--2025 found that 83% report capability metrics while only\n30% consider human-centred or economic axes [2]. Second, we formalise an\nAdaptive Multi-Dimensional Monitoring (AMDM) algorithm that normalises\nheterogeneous metrics, applies per-axis exponentially weighted moving-average\nthresholds and performs joint anomaly detection via the Mahalanobis distance\n[7]. Third, we conduct simulations and real-world experiments. AMDM cuts\nanomaly-detection latency from 12.3 s to 5.6 s on simulated goal drift and\nreduces false-positive rates from 4.5% to 0.9% compared with static thresholds.\nWe present a comparison table and ROC/PR curves, and we reanalyse case studies\nto surface missing metrics. Code, data and a reproducibility checklist\naccompany this paper to facilitate replication. The code supporting this work\nis available at\nhttps://github.com/Manishms18/Adaptive-Multi-Dimensional-Monitoring."}
{"id": "2509.00996", "pdf": "https://arxiv.org/pdf/2509.00996.pdf", "abs": "https://arxiv.org/abs/2509.00996", "title": "MEPT: Mixture of Expert Prompt Tuning as a Manifold Mapper", "authors": ["Runjia Zeng", "Guangyan Sun", "Qifan Wang", "Tong Geng", "Sohail Dianat", "Xiaotian Han", "Raghuveer Rao", "Xueling Zhang", "Cheng Han", "Lifu Huang", "Dongfang Liu"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "EMNLP 2025", "summary": "Considering deep neural networks as manifold mappers, the\npretrain-then-fine-tune paradigm can be interpreted as a two-stage process:\npretrain establishes a broad knowledge base, and fine-tune adjusts the model\nparameters to activate specific neural pathways to align with the target\nmanifold. Although prior fine-tuning approaches demonstrate success, their\nrigid parameter space limits their ability to dynamically activate appropriate\nneural pathways, rendering them ill-equipped to adapt flexibly to the diverse\nand evolving data distributions. In light of this view, we propose a novel\napproach, Mixture of Expert Prompt Tuning (MEPT), as an effective and efficient\nmanifold-mapping framework. MEPT leverages the Mixture of Experts architecture\nby integrating multiple prompt experts to adaptively learn diverse and\nnon-stationary data distributions. Empirical evaluations demonstrate that MEPT\noutperforms several state-of-the-art parameter efficient baselines on\nSuperGLUE, achieving notable improvements in mean accuracy (e.g., 1.94%) while\nsignificantly reducing activated prompts by 79.25%. The effectiveness of MEPT\nis further supported by theoretical insights from manifold learning and\nvalidated through neural activation pathway visualization results. Our code is\navaliable at https://runjia.tech/emnlp_mept/."}
{"id": "2509.01909", "pdf": "https://arxiv.org/pdf/2509.01909.pdf", "abs": "https://arxiv.org/abs/2509.01909", "title": "Oyster-I: Beyond Refusal -- Constructive Safety Alignment for Responsible Language Models", "authors": ["Ranjie Duan", "Jiexi Liu", "Xiaojun Jia", "Shiji Zhao", "Ruoxi Cheng", "Fengxiang Wang", "Cheng Wei", "Yong Xie", "Chang Liu", "Defeng Li", "Yinpeng Dong", "Yichi Zhang", "Yuefeng Chen", "Chongwen Wang", "Xingjun Ma", "Xingxing Wei", "Yang Liu", "Hang Su", "Jun Zhu", "Xinfeng Li", "Yitong Sun", "Jie Zhang", "Jinzhao Hu", "Sha Xu", "Wenchao Yang", "Yitong Yang", "Jialing Tao", "Hui Xue"], "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC", "cs.SC"], "comment": "Technical Report Code & Model weights available:\n  https://github.com/Alibaba-AAIG/Oyster", "summary": "Large language models (LLMs) typically deploy safety mechanisms to prevent\nharmful content generation. Most current approaches focus narrowly on risks\nposed by malicious actors, often framing risks as adversarial events and\nrelying on defensive refusals. However, in real-world settings, risks also come\nfrom non-malicious users seeking help while under psychological distress (e.g.,\nself-harm intentions). In such cases, the model's response can strongly\ninfluence the user's next actions. Simple refusals may lead them to repeat,\nescalate, or move to unsafe platforms, creating worse outcomes. We introduce\nConstructive Safety Alignment (CSA), a human-centric paradigm that protects\nagainst malicious misuse while actively guiding vulnerable users toward safe\nand helpful results. Implemented in Oyster-I (Oy1), CSA combines game-theoretic\nanticipation of user reactions, fine-grained risk boundary discovery, and\ninterpretable reasoning control, turning safety into a trust-building process.\nOy1 achieves state-of-the-art safety among open models while retaining high\ngeneral capabilities. On our Constructive Benchmark, it shows strong\nconstructive engagement, close to GPT-5, and unmatched robustness on the\nStrata-Sword jailbreak dataset, nearing GPT-o1 levels. By shifting from\nrefusal-first to guidance-first safety, CSA redefines the model-user\nrelationship, aiming for systems that are not just safe, but meaningfully\nhelpful. We release Oy1, code, and the benchmark to support responsible,\nuser-centered AI."}
