{"id": "2508.09297", "pdf": "https://arxiv.org/pdf/2508.09297.pdf", "abs": "https://arxiv.org/abs/2508.09297", "title": "Based AI improves human decision-making but reduces trust", "authors": ["Shiyang Lai", "Junsol Kim", "Nadav Kunievsky", "Yujin Potter", "James Evans"], "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": null, "summary": "Current AI systems minimize risk by enforcing ideological neutrality, yet\nthis may introduce automation bias by suppressing cognitive engagement in human\ndecision-making. We conducted randomized trials with 2,500 participants to test\nwhether culturally biased AI enhances human decision-making. Participants\ninteracted with politically diverse GPT-4o variants on information evaluation\ntasks. Partisan AI assistants enhanced human performance, increased engagement,\nand reduced evaluative bias compared to non-biased counterparts, with amplified\nbenefits when participants encountered opposing views. These gains carried a\ntrust penalty: participants underappreciated biased AI and overcredited neutral\nsystems. Exposing participants to two AIs whose biases flanked human\nperspectives closed the perception-performance gap. These findings complicate\nconventional wisdom about AI neutrality, suggesting that strategic integration\nof diverse cultural biases may foster improved and resilient human\ndecision-making.", "AI": {"tldr": "The paper examines how culturally biased AI can improve human decision-making and engagement.", "motivation": "To explore the impact of AI systems' ideological neutrality on human cognitive engagement and decision-making.", "method": "Randomized trials with 2,500 participants interacting with politically diverse GPT-4o variants in information evaluation tasks.", "result": "Partisan AI assistants enhanced human performance, increased engagement, and reduced evaluative bias, especially when participants encountered opposing views.", "conclusion": "Strategic integration of diverse cultural biases in AI may enhance decision-making and challenge the notion of AI neutrality.", "key_contributions": ["Demonstrated the impact of biased AI on human performance.", "Highlighted the importance of ideological diversity in AI interactions.", "Revealed the trust penalty linked to biased versus neutral AI systems."], "limitations": "", "keywords": ["AI neutrality", "human decision-making", "cognitive engagement", "partisan AI", "automation bias"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.09312", "pdf": "https://arxiv.org/pdf/2508.09312.pdf", "abs": "https://arxiv.org/abs/2508.09312", "title": "Micro-Health Interventions: Exploring Design Strategies for 1-Minute Interventions as a Gateway to Healthy Habits", "authors": ["Zahra Hassanzadeh", "David Haag", "Lydia Chilton", "Jan Smeddinck", "Norman Farb", "Joseph Jay Williams"], "categories": ["cs.HC"], "comment": null, "summary": "One-minute behavior change interventions might seem too brief to matter.\nCould something so short really help people build healthier routines? This work\nexplores this question through two studies examining how ultra-brief prompts\nmight encourage meaningful actions in daily life. In a formative study, we\nexplored how participants engaged with one-minute prompts across four domains:\nphysical activity, eating, screen use, and mental well-being. This revealed two\ncommon design approaches: Immediate Action prompts (simple, directive tasks)\nand Reflection-First prompts (self-awareness before action). We then conducted\na 14-day, within-subjects study comparing these two flows with 28 participants.\nSurprisingly, most participants did not notice differences in structure -- but\nresponded positively when prompts felt timely, relevant, or emotionally\nsupportive. Engagement was not shaped by flow type, but by content fit, tone,\nand momentary readiness. Participants also co-designed messages, favoring those\nwith step-by-step guidance, personal meaning, or sensory detail. These results\nsuggest that one-minute interventions, while easily dismissed, may serve as\nmeaningful gateways into healthier routines -- if designed to feel helpful in\nthe moment.", "AI": {"tldr": "This paper explores the effectiveness of one-minute behavior change interventions through two studies, indicating they can encourage healthier routines if designed appropriately.", "motivation": "To investigate whether ultra-brief prompts can foster meaningful behavioral changes in daily life.", "method": "Two studies were conducted: a formative exploration of participant engagement with one-minute prompts across various health domains and a 14-day within-subjects study comparing Immediate Action and Reflection-First prompt designs.", "result": "Participants found prompts more engaging based on content fit, tone, and perceived timeliness rather than the structural differences between the flows; co-designed messages with specific guidance were preferred.", "conclusion": "One-minute interventions can facilitate behavior change when they are timely, relevant, and emotionally supportive, rather than strictly adhering to a predefined structure.", "key_contributions": ["Identification of design approaches for one-minute prompts", "Demonstration of user engagement factors over structural differences", "Insights into co-designing effective health prompts"], "limitations": "", "keywords": ["behavior change", "HCI", "prompt design", "health interventions", "user engagement"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.09342", "pdf": "https://arxiv.org/pdf/2508.09342.pdf", "abs": "https://arxiv.org/abs/2508.09342", "title": "Affordances of Sketched Notations for Multimodal UI Design and Development Tools", "authors": ["Sam H. Ross", "Yunseo Lee", "Coco K. Lee", "Jayne Everson", "R. Benjamin Shapiro"], "categories": ["cs.HC"], "comment": "VL/HCC 2025", "summary": "Multimodal UI design and development tools that interpret sketches or natural\nlanguage descriptions of UIs inherently have notations: the inputs they can\nunderstand. In AI-based systems, notations are implicitly defined by the data\nused to train these systems. In order to create usable and intuitive notations\nfor interactive design systems, we must regard, design, and evaluate these\ntraining datasets as notation specifications. To better understand the design\nspace of notational possibilities for future design tools, we use the Cognitive\nDimensions of Notations framework to analyze two possible notations for UI\nsketching. The first notation is the sketching rules for an existing UI sketch\ndataset, and the second notation is the set of sketches generated by\nparticipants in this study, where individuals sketched UIs without imposed\nrepresentational rules. We imagine two systems, FixedSketch and FlexiSketch,\nbuilt with each notation respectively, in order to understand the differential\naffordances of, and potential design requirements for, systems. We find that\nparticipants' sketches were composed of element-level notations that are\nambiguous in isolation but are interpretable in context within whole designs.\nFor many cognitive dimensions, the FlexiSketch notation supports greater\nintuitive creative expression and affords lower cognitive effort than the\nFixedSketch notation, but cannot be supported with prevailing, element-based\napproaches to UI sketch recognition. We argue that for future multimodal design\ntools to be truly human-centered, they must adopt contemporary AI methods,\nincluding transformer-based and human-in-the-loop, reinforcement learning\ntechniques to understand users' context-rich expressive notations and\ncorrections.", "AI": {"tldr": "This paper explores the design of notations for multimodal UI design tools through an analysis of UI sketching notations using the Cognitive Dimensions of Notations framework.", "motivation": "To create usable and intuitive notations for interactive design systems by treating training datasets as notation specifications.", "method": "Analysis of two notations for UI sketching—one from an existing dataset and another from participant-generated sketches—using the Cognitive Dimensions of Notations framework.", "result": "The study reveals that while FlexiSketch notation allows for greater intuitive expression and lower cognitive effort, it struggles with element-based UI sketch recognition compared to FixedSketch notation.", "conclusion": "Future multimodal design tools must integrate advanced AI methods to effectively interpret context-rich expressive notations.", "key_contributions": ["Introduction of FixedSketch and FlexiSketch systems for understanding UI sketch notations", "Analysis of cognitive dimensions influencing notational design", "Call for integration of contemporary AI methods in multimodal design tools"], "limitations": "", "keywords": ["multimodal UI design", "Cognitive Dimensions of Notations", "human-centered design", "AI methods", "UI sketch recognition"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.09358", "pdf": "https://arxiv.org/pdf/2508.09358.pdf", "abs": "https://arxiv.org/abs/2508.09358", "title": "Virtual Reality User Interface Design: Best Practices and Implementation", "authors": ["Esin Mehmedova", "Santiago Berrezueta-Guzman", "Stefan Wagner"], "categories": ["cs.HC"], "comment": "Paper submitted to ACM SIGGRAPH Motion, Interaction and Games 2025\n  (MIG 2025)", "summary": "Designing effective user interfaces (UIs) for virtual reality (VR) is\nessential to enhance user immersion, usability, comfort, and accessibility in\nvirtual environments. Despite the growing adoption of VR across domains such as\neducation, healthcare, gaming, and rehabilitation, there is a noticeable lack\nof unified and comprehensive design guidelines for VR UI design. To address\nthis gap, we conducted a systematic literature review to identify existing best\npractices and propose complete and unified guidelines for UI development in VR.\n  Building on these insights, this research proposes a set of best practices to\nguide the creation of more effective VR interfaces. To demonstrate and validate\nthese practices, we developed a VR application called \\textit{FlUId} that\nshowcases both good and bad UI design principles for direct comparison. A user\nstudy was conducted to evaluate the impact of the proposed guidelines. The\nfindings aim to bridge the gap between theory and practice, offering concrete\nrecommendations for VR designers and developers.", "AI": {"tldr": "This paper presents unified design guidelines for virtual reality user interfaces (UIs) developed through a systematic literature review and validated by a user study.", "motivation": "There is a lack of comprehensive design guidelines for effective VR UI design, despite the increasing use of VR in various sectors.", "method": "A systematic literature review was conducted to identify best practices in VR UI design. A VR application named FlUId was created to demonstrate these practices, and a user study was performed to evaluate their effectiveness.", "result": "The guidelines proposed in this research were validated through a user study, showing a significant impact on user immersion and comfort.", "conclusion": "The findings provide concrete recommendations for VR designers and developers, bridging the gap between theoretical guidelines and practical application.", "key_contributions": ["Proposed comprehensive design guidelines for VR UIs", "Developed a VR application, FlUId, to showcase UI design principles", "Conducted a user study to validate the effectiveness of the guidelines"], "limitations": "The study may not encompass all possible scenarios in diverse VR applications.", "keywords": ["Virtual Reality", "User Interface Design", "Best Practices", "Systematic Literature Review", "User Study"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.09303", "pdf": "https://arxiv.org/pdf/2508.09303.pdf", "abs": "https://arxiv.org/abs/2508.09303", "title": "ParallelSearch: Train your LLMs to Decompose Query and Search Sub-queries in Parallel with Reinforcement Learning", "authors": ["Shu Zhao", "Tan Yu", "Anbang Xu", "Japinder Singh", "Aaditya Shukla", "Rama Akkiraju"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Reasoning-augmented search agents such as Search-R1, trained via\nreinforcement learning with verifiable rewards (RLVR), demonstrate remarkable\ncapabilities in multi-step information retrieval from external knowledge\nsources. These agents address the limitations of their parametric memory by\ndynamically gathering relevant facts to address complex reasoning tasks.\nHowever, existing approaches suffer from a fundamental architectural\nlimitation: they process search queries strictly sequentially, even when\nhandling inherently parallelizable and logically independent comparisons. This\nsequential bottleneck significantly constrains computational efficiency,\nparticularly for queries that require multiple entity comparisons. To address\nthis critical limitation, we propose ParallelSearch, a novel reinforcement\nlearning framework that empowers large language models (LLMs) to recognize\nparallelizable query structures and execute multiple search operations\nconcurrently. Our approach introduces dedicated reward functions that\nincentivize the identification of independent query components while preserving\nanswer accuracy through jointly considering correctness, query decomposition\nquality, and parallel execution benefits. Comprehensive experiments demonstrate\nthat ParallelSearch outperforms state-of-the-art baselines by an average\nperformance gain of 2.9% across seven question-answering benchmarks. Notably,\non parallelizable questions, our method achieves a 12.7% performance\nimprovement while requiring only 69.6% of the LLM calls compared to sequential\napproaches.", "AI": {"tldr": "ParallelSearch enhances reinforcement learning for search agents by enabling parallel processing of queries, improving efficiency and performance.", "motivation": "To overcome the inefficiencies of sequential processing in current reasoning-augmented search agents, enabling more effective multi-step information retrieval.", "method": "A novel reinforcement learning framework that allows LLMs to identify and process parallelizable query structures, employing dedicated reward functions for accuracy and efficiency.", "result": "ParallelSearch outperforms state-of-the-art methods with an average 2.9% performance gain across benchmarks and a 12.7% improvement on parallelizable questions using fewer LLM calls.", "conclusion": "The ParallelSearch framework significantly improves query processing efficiency in reasoning-augmented search agents, paving the way for more effective information retrieval.", "key_contributions": ["Development of the ParallelSearch framework for concurrent query execution", "Introduction of dedicated reward functions for independent query components", "Demonstrated significant performance improvements over existing methods"], "limitations": "", "keywords": ["reinforcement learning", "parallel processing", "search agents"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.09386", "pdf": "https://arxiv.org/pdf/2508.09386.pdf", "abs": "https://arxiv.org/abs/2508.09386", "title": "VIVA: Virtual Healthcare Interactions Using Visual Analytics, With Controllability Through Configuration", "authors": ["Jürgen Bernard", "Mara Solen", "Helen Novak Lauscher", "Kurtis Stewart", "Kendall Ho", "Tamara Munzner"], "categories": ["cs.HC"], "comment": null, "summary": "At the beginning of the COVID-19 pandemic, HealthLink BC (HLBC) rapidly\nintegrated physicians into the triage process of their virtual healthcare\nservice to improve patient outcomes and satisfaction with this service and\npreserve health care system capacity. We present the design and implementation\nof a visual analytics tool, VIVA (Virtual healthcare Interactions using Visual\nAnalytics), to support HLBC in analysing various forms of usage data from the\nservice. We abstract HLBC's data and data analysis tasks, which we use to\ninform our design of VIVA. We also present the interactive workflow abstraction\nof Scan, Act, Adapt. We validate VIVA's design through three case studies with\nstakeholder domain experts. We also propose the Controllability Through\nConfiguration model to conduct and analyze design studies, and discuss\narchitectural evolution of VIVA through that lens. It articulates\nconfiguration, both that specified by a developer or technical power user and\nthat constructed automatically through log data from previous interactive\nsessions, as a bridge between the rigidity of hardwired programming and the\ntime-consuming implementation of full end-user interactivity.\n  Availability: Supplemental materials at https://osf.io/wv38n", "AI": {"tldr": "The paper discusses the design and implementation of VIVA, a visual analytics tool developed for HealthLink BC to analyze virtual healthcare service usage data.", "motivation": "To improve patient outcomes and satisfaction with virtual healthcare during the COVID-19 pandemic while preserving healthcare system capacity.", "method": "A visual analytics tool (VIVA) was designed and implemented based on the abstraction of data and analysis tasks, validated through case studies with domain experts, utilizing a framework of Scan, Act, Adapt and the Controllability Through Configuration model.", "result": "VIVA was validated through case studies and is capable of analyzing various forms of usage data to support better decision-making in virtual healthcare.", "conclusion": "The architectural evolution of VIVA reflects a balance between rigid programming and flexible end-user interactivity, enhancing the potential for effective usage data analysis.", "key_contributions": ["Design and implementation of VIVA tool for healthcare analytics", "Introduction of the Scan, Act, Adapt interactive workflow", "Development of the Controllability Through Configuration model for design studies"], "limitations": "", "keywords": ["visual analytics", "healthcare", "COVID-19", "data analysis", "user interaction"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.09323", "pdf": "https://arxiv.org/pdf/2508.09323.pdf", "abs": "https://arxiv.org/abs/2508.09323", "title": "Leveraging Large Language Models for Rare Disease Named Entity Recognition", "authors": ["Nan Miles Xi", "Yu Deng", "Lin Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Named Entity Recognition (NER) in the rare disease domain poses unique\nchallenges due to limited labeled data, semantic ambiguity between entity\ntypes, and long-tail distributions. In this study, we evaluate the capabilities\nof GPT-4o for rare disease NER under low-resource settings, using a range of\nprompt-based strategies including zero-shot prompting, few-shot in-context\nlearning, retrieval-augmented generation (RAG), and task-level fine-tuning. We\ndesign a structured prompting framework that encodes domain-specific knowledge\nand disambiguation rules for four entity types. We further introduce two\nsemantically guided few-shot example selection methods to improve in-context\nperformance while reducing labeling effort. Experiments on the RareDis Corpus\nshow that GPT-4o achieves competitive or superior performance compared to\nBioClinicalBERT, with task-level fine-tuning yielding new state-of-the-art\n(SOTA) results. Cost-performance analysis reveals that few-shot prompting\ndelivers high returns at low token budgets, while RAG offers marginal\nadditional benefit. An error taxonomy highlights common failure modes such as\nboundary drift and type confusion, suggesting opportunities for post-processing\nand hybrid refinement. Our results demonstrate that prompt-optimized LLMs can\nserve as effective, scalable alternatives to traditional supervised models in\nbiomedical NER, particularly in rare disease applications where annotated data\nis scarce.", "AI": {"tldr": "This study evaluates GPT-4o for Named Entity Recognition (NER) in the rare disease domain, demonstrating its effectiveness in low-resource settings using various prompting strategies.", "motivation": "To address the challenges of Named Entity Recognition in the rare disease domain, which includes limited labeled data and semantic ambiguity between entity types.", "method": "We evaluated GPT-4o's performance using prompt-based strategies such as zero-shot prompting, few-shot in-context learning, retrieval-augmented generation (RAG), and task-level fine-tuning, with a structured prompting framework for domain-specific knowledge.", "result": "Experiments showed that GPT-4o achieved competitive or superior performance compared to BioClinicalBERT, with task-level fine-tuning leading to new state-of-the-art results in rare disease NER.", "conclusion": "Prompt-optimized LLMs can effectively replace traditional supervised models in rare disease NER, especially where labeled data is scarce, demonstrating a scalable solution for biomedical applications.", "key_contributions": ["Evaluation of GPT-4o in rare disease NER", "Introduction of semantically guided few-shot example selection methods", "Cost-performance analysis of prompt-based strategies"], "limitations": "Common failure modes include boundary drift and type confusion, indicating areas for post-processing improvement.", "keywords": ["Named Entity Recognition", "rare diseases", "GPT-4o", "prompt-based strategies", "biomedical NER"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2508.09402", "pdf": "https://arxiv.org/pdf/2508.09402.pdf", "abs": "https://arxiv.org/abs/2508.09402", "title": "Realtime Multimodal Emotion Estimation using Behavioral and Neurophysiological Data", "authors": ["Von Ralph Dane Marquez Herbuela", "Yukie Nagai"], "categories": ["cs.HC"], "comment": null, "summary": "Many individuals especially those with autism spectrum disorder (ASD),\nalexithymia, or other neurodivergent profiles face challenges in recognizing,\nexpressing, or interpreting emotions. To support more inclusive and\npersonalized emotion technologies, we present a real-time multimodal emotion\nestimation system that combines neurophysiological EEG, ECG, blood volume pulse\n(BVP), and galvanic skin response (GSR/EDA) and behavioral modalities (facial\nexpressions, and speech) in a unified arousal-valence 2D interface to track\nmoment-to-moment emotional states. This architecture enables interpretable,\nuser-specific analysis and supports applications in emotion education,\nneuroadaptive feedback, and interaction support for neurodiverse users. Two\ndemonstration scenarios illustrate its application: (1) passive media viewing\n(2D or VR videos) reveals cortical and autonomic responses to affective\ncontent, and (2) semi-scripted conversations with a facilitator or virtual\nagent capture real-time facial and vocal expressions. These tasks enable\ncontrolled and naturalistic emotion monitoring, making the system well-suited\nfor personalized feedback and neurodiversity-informed interaction design.", "AI": {"tldr": "A multimodal emotion estimation system for supporting inclusive emotion technologies, tailored for neurodivergent users using various physiological and behavioral data.", "motivation": "To facilitate better understanding and interaction for individuals with autism and other neurodivergent conditions by enhancing emotion recognition and expression.", "method": "The system integrates neurophysiological data (EEG, ECG, BVP, GSR) with behavioral data (facial expressions and speech) to provide a comprehensive emotional tracking interface.", "result": "Demonstration scenarios show the system's capability in monitoring emotional responses during media consumption and interactive conversations, providing user-specific insights.", "conclusion": "The architecture supports personalized feedback and interaction design tailored for neurodiverse users, enhancing emotional literacy and adaptive feedback mechanisms.", "key_contributions": ["Real-time multimodal emotion estimation integrating physiological and behavioral data.", "User-specific analysis for enhanced interpretation of emotions.", "Application in neuroadaptive feedback mechanisms and interaction support."], "limitations": "", "keywords": ["Emotion estimation", "Neurodiversity", "Real-time monitoring", "Physiological signals", "Interaction design"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.09324", "pdf": "https://arxiv.org/pdf/2508.09324.pdf", "abs": "https://arxiv.org/abs/2508.09324", "title": "TEN: Table Explicitization, Neurosymbolically", "authors": ["Nikita Mehrotra", "Aayush Kumar", "Sumit Gulwani", "Arjun Radhakrishna", "Ashish Tiwari"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present a neurosymbolic approach, TEN, for extracting tabular data from\nsemistructured input text. This task is particularly challenging for text input\nthat does not use special delimiters consistently to separate columns and rows.\nPurely neural approaches perform poorly due to hallucinations and their\ninability to enforce hard constraints. TEN uses Structural Decomposition\nprompting - a specialized chain-of-thought prompting approach - on a large\nlanguage model (LLM) to generate an initial table, and thereafter uses a\nsymbolic checker to evaluate not only the well-formedness of that table, but\nalso detect cases of hallucinations or forgetting. The output of the symbolic\nchecker is processed by a critique-LLM to generate guidance for fixing the\ntable, which is presented to the original LLM in a self-debug loop. Our\nextensive experiments demonstrate that TEN significantly outperforms purely\nneural baselines across multiple datasets and metrics, achieving significantly\nhigher exact match accuracy and substantially reduced hallucination rates. A\n21-participant user study further confirms that TEN's tables are rated\nsignificantly more accurate (mean score: 5.0 vs 4.3; p = 0.021), and are\nconsistently preferred for ease of verification and correction, with\nparticipants favoring our method in over 60% of the cases.", "AI": {"tldr": "TEN is a neurosymbolic approach that enhances tabular data extraction from semistructured text by combining LLMs with symbolic checking to mitigate hallucinations.", "motivation": "Extracting structured data from semistructured text is challenging due to inconsistent delimiters, which causes problems for purely neural methods.", "method": "TEN employs Structural Decomposition prompting with a large language model to generate a table. A symbolic checker assesses the table's quality and detects errors, then a critique-LLM provides corrections in a self-debugging loop.", "result": "TEN significantly outperforms neural baselines, demonstrated by higher exact match accuracy and lower hallucination rates across various datasets. A user study confirmed TEN's results, with participants rating its output more accurate and preferred for verification.", "conclusion": "The neurosymbolic approach employed by TEN proves effective in overcoming the limitations of purely neural methods for tabular data extraction from complicated text formats.", "key_contributions": ["Introduction of TEN, a neurosymbolic framework for tabular extraction", "Use of a symbolic checker for quality assessment and error detection in LLM outputs", "Empirical validation through extensive experiments and user studies showing performance improvements"], "limitations": "", "keywords": ["neurosymbolic", "tabular data extraction", "large language model", "hallucination", "symbolic checker"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2508.09438", "pdf": "https://arxiv.org/pdf/2508.09438.pdf", "abs": "https://arxiv.org/abs/2508.09438", "title": "Fulfillment of the Work Games: Warehouse Workers' Experiences with Algorithmic Management", "authors": ["EunJeong Cheon", "Ingrid Erickson"], "categories": ["cs.HC"], "comment": null, "summary": "The introduction of algorithms into a large number of industries has already\nrestructured the landscape of work and threatens to continue. While a growing\nbody of CSCW research centered on the future of work has begun to document\nthese shifts, relatively little is known about workers' experiences beyond\nthose of platform-mediated gig workers. In this paper, we turn to a traditional\nwork sector, Amazon fulfillment centers (FC), to deepen our field's empirical\nexamination of algorithmic management. Drawing on two years of ethnographic\nresearch, we show how FC workers react to managers' interventions, imposed\nproductivity rates, and quantified objectification when subjected to\nlabor-tracking systems in their physical work environments. Situating FC\nworkers' resistance to algorithmic systems and metrics within the current CSCW\nliterature allows us to explicate and link the nuanced practices of FC workers\nto the larger discourse of algorithmic control mechanisms. In addition, we show\nhow FC workers' resistance practices are emblematic of 'work games'--a\nlong-studied means by which workers agentically configure (\"trick\") their\nengagement within work systems. We argue that gaining a more nuanced\nunderstanding of workers' resistance and consent in relation to algorithmic\nmanagement expands our ability to critique and potentially disassemble the\neconomic and political forces at the root of these sociotechnical labor\nsystems.", "AI": {"tldr": "This paper explores the experiences of Amazon fulfillment center workers under algorithmic management, emphasizing their resistance practices and the implications for understanding labor systems.", "motivation": "To investigate the experiences of workers beyond platform-mediated gig environments, particularly in traditional sectors like Amazon fulfillment centers, in the context of algorithmic management.", "method": "Ethnographic research conducted over two years, focusing on the experiences and practices of Amazon fulfillment center workers in relation to labor-tracking systems.", "result": "The study reveals how workers resist imposed productivity rates and objectification through various agentic practices, termed 'work games.'", "conclusion": "A deeper understanding of worker experiences in algorithmic management can provide critical insights into the socio-economic forces shaping labor systems, offering pathways for critique and potential change.", "key_contributions": ["Empirical examination of algorithmic management in non-gig work settings.", "Identification of resistance practices among fulfillment center workers.", "Linking workers' experiences to broader discussions of algorithmic control.", "Conceptualization of 'work games' as a means of worker agency."], "limitations": "", "keywords": ["algorithmic management", "worker resistance", "fulfillment centers", "ethnographic research", "labor systems"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2508.09337", "pdf": "https://arxiv.org/pdf/2508.09337.pdf", "abs": "https://arxiv.org/abs/2508.09337", "title": "Decoding Neural Emotion Patterns through Natural Language Processing Embeddings", "authors": ["Gideon Vos", "Maryam Ebrahimpour", "Liza van Eijk", "Zoltan Sarnyai", "Mostafa Rahimi Azghadi"], "categories": ["cs.CL"], "comment": "26 pages, 9 figures", "summary": "Understanding how emotional expression in language relates to brain function\nis a challenge in computational neuroscience and affective computing.\nTraditional neuroimaging is costly and lab-bound, but abundant digital text\noffers new avenues for emotion-brain mapping. Prior work has largely examined\nneuroimaging-based emotion localization or computational text analysis\nseparately, with little integration. We propose a computational framework that\nmaps textual emotional content to anatomically defined brain regions without\nrequiring neuroimaging. Using OpenAI's text-embedding-ada-002, we generate\nhigh-dimensional semantic representations, apply dimensionality reduction and\nclustering to identify emotional groups, and map them to 18 brain regions\nlinked to emotional processing. Three experiments were conducted: i) analyzing\nconversational data from healthy vs. depressed subjects (DIAC-WOZ dataset) to\ncompare mapping patterns, ii) applying the method to the GoEmotions dataset and\niii) comparing human-written text with large language model (LLM) responses to\nassess differences in inferred brain activation. Emotional intensity was scored\nvia lexical analysis. Results showed neuroanatomically plausible mappings with\nhigh spatial specificity. Depressed subjects exhibited greater limbic\nengagement tied to negative affect. Discrete emotions were successfully\ndifferentiated. LLM-generated text matched humans in basic emotion distribution\nbut lacked nuanced activation in empathy and self-referential regions (medial\nprefrontal and posterior cingulate cortex). This cost-effective, scalable\napproach enables large-scale analysis of naturalistic language, distinguishes\nbetween clinical populations, and offers a brain-based benchmark for evaluating\nAI emotional expression.", "AI": {"tldr": "The paper proposes a computational framework for mapping emotional content in text to brain regions without neuroimaging, using text embeddings and clustering.", "motivation": "To integrate emotional expression in language with brain function analysis in a cost-effective and scalable way, overcoming the limitations of traditional neuroimaging.", "method": "The paper utilizes OpenAI's text-embedding-ada-002 to generate high-dimensional semantic representations of text, applies dimensionality reduction and clustering to identify emotional groups, and maps these to 18 brain regions associated with emotional processing.", "result": "The experiments revealed neuroanatomically plausible mappings of emotional content, with distinct differences in emotional activation patterns between healthy and depressed subjects, as well as variances between human and LLM-generated text.", "conclusion": "The proposed framework allows for large-scale analysis of language and establishes a benchmark for evaluating AI's capability in emotional expression, particularly in health informatics.", "key_contributions": ["Developed a cost-effective framework for emotion-brain mapping using text data.", "Demonstrated the ability to differentiate emotional responses in clinical populations.", "Provided insights into LLM-generated text regarding emotional depth compared to human writing."], "limitations": "The study relies on text embeddings that may not capture all nuances of emotional language; also, there are potential biases in the datasets used.", "keywords": ["emotional expression", "computational neuroscience", "text embeddings", "brain mapping", "large language models"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.09458", "pdf": "https://arxiv.org/pdf/2508.09458.pdf", "abs": "https://arxiv.org/abs/2508.09458", "title": "Hallucination vs interpretation: rethinking accuracy and precision in AI-assisted data extraction for knowledge synthesis", "authors": ["Xi Long", "Christy Boscardin", "Lauren A. Maggio", "Joseph A. Costello", "Ralph Gonzales", "Rasmyah Hammoudeh", "Ki Lai", "Yoon Soo Park", "Brian C. Gin"], "categories": ["cs.HC", "cs.AI", "cs.ET"], "comment": null, "summary": "Knowledge syntheses (literature reviews) are essential to health professions\neducation (HPE), consolidating findings to advance theory and practice.\nHowever, they are labor-intensive, especially during data extraction.\nArtificial Intelligence (AI)-assisted extraction promises efficiency but raises\nconcerns about accuracy, making it critical to distinguish AI 'hallucinations'\n(fabricated content) from legitimate interpretive differences. We developed an\nextraction platform using large language models (LLMs) to automate data\nextraction and compared AI to human responses across 187 publications and 17\nextraction questions from a published scoping review. AI-human, human-human,\nand AI-AI consistencies were measured using interrater reliability\n(categorical) and thematic similarity ratings (open-ended). Errors were\nidentified by comparing extracted responses to source publications. AI was\nhighly consistent with humans for concrete, explicitly stated questions (e.g.,\ntitle, aims) and lower for questions requiring subjective interpretation or\nabsent in text (e.g., Kirkpatrick's outcomes, study rationale). Human-human\nconsistency was not higher than AI-human and showed the same question-dependent\nvariability. Discordant AI-human responses (769/3179 = 24.2%) were mostly due\nto interpretive differences (18.3%); AI inaccuracies were rare (1.51%), while\nhumans were nearly three times more likely to state inaccuracies (4.37%).\nFindings suggest AI accuracy depends more on interpretability than\nhallucination. Repeating AI extraction can identify interpretive complexity or\nambiguity, refining processes before human review. AI can be a transparent,\ntrustworthy partner in knowledge synthesis, though caution is needed to\npreserve critical human insights.", "AI": {"tldr": "This paper investigates the use of AI-assisted data extraction in health professions education literature reviews, demonstrating high consistency between AI and human responses for concrete questions, while highlighting the importance of interpretability.", "motivation": "To improve the efficiency of literature reviews in health professions education by utilizing AI for data extraction, addressing concerns over accuracy and distinguishing between hallucinations and genuine interpretive differences.", "method": "Developed an extraction platform using large language models (LLMs) to automate data extraction and compared AI responses to human responses across 187 publications and 17 extraction questions, measuring consistencies and errors.", "result": "AI showed high consistency with human responses for concrete questions but lower for subjective interpretations; errors were largely from interpretive differences rather than hallucinations, with AI inaccuracies being rare but present.", "conclusion": "AI can enhance the literature review process by providing consistent extraction when questions are clear, but human oversight is essential to capture nuanced insights.", "key_contributions": ["Automation of literature review data extraction using LLMs", "Evidence that AI performance varies significantly based on the nature of the questions", "Demonstrated a comparison of AI and human response consistency in interpreting literature"], "limitations": "Caution is needed when interpreting nuanced questions, as AI may struggle with ambiguous content, and human input remains critical for some insights.", "keywords": ["AI-assisted extraction", "Health professions education", "Large language models", "Data extraction", "Literature reviews"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2508.09349", "pdf": "https://arxiv.org/pdf/2508.09349.pdf", "abs": "https://arxiv.org/abs/2508.09349", "title": "The Human-AI Hybrid Delphi Model: A Structured Framework for Context-Rich, Expert Consensus in Complex Domains", "authors": ["Cathy Speed", "Ahmed A. Metwally"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Expert consensus plays a critical role in domains where evidence is complex,\nconflicting, or insufficient for direct prescription. Traditional methods, such\nas Delphi studies, consensus conferences, and systematic guideline synthesis,\noffer structure but face limitations including high panel burden, interpretive\noversimplification, and suppression of conditional nuance. These challenges are\nnow exacerbated by information overload, fragmentation of the evidence base,\nand increasing reliance on publicly available sources that lack expert\nfiltering. This study introduces and evaluates a Human-AI Hybrid Delphi\n(HAH-Delphi) framework designed to augment expert consensus development by\nintegrating a generative AI model (Gemini 2.5 Pro), small panels of senior\nhuman experts, and structured facilitation. The HAH-Delphi was tested in three\nphases: retrospective replication, prospective comparison, and applied\ndeployment in two applied domains (endurance training and resistance and mixed\ncardio/strength training). The AI replicated 95% of published expert consensus\nconclusions in Phase I and showed 95% directional agreement with senior human\nexperts in Phase II, though it lacked experiential and pragmatic nuance. In\nPhase III, compact panels of six senior experts achieved >90% consensus\ncoverage and reached thematic saturation before the final participant. The AI\nprovided consistent, literature-grounded scaffolding that supported divergence\nresolution and accelerated saturation. The HAH-Delphi framework offers a\nflexible, scalable approach for generating high-quality, context-sensitive\nconsensus. Its successful application across health, coaching, and performance\nscience confirms its methodological robustness and supports its use as a\nfoundation for generating conditional, personalised guidance and published\nconsensus frameworks at scale.", "AI": {"tldr": "This study presents a Human-AI Hybrid Delphi (HAH-Delphi) framework that integrates a generative AI model with expert panels to enhance consensus development in complex evidence domains.", "motivation": "The need for expert consensus is critical in areas where evidence is conflicting or insufficient, and traditional methods face limitations exacerbated by information overload.", "method": "The HAH-Delphi framework involves using a generative AI model alongside small panels of senior experts to facilitate consensus development, tested through retrospective replication, prospective comparison, and applied deployment in specific domains.", "result": "The AI replicated 95% of expert consensus conclusions and showed 95% agreement with human experts, while compact expert panels achieved over 90% consensus coverage before the last participant.", "conclusion": "The HAH-Delphi framework offers a robust, scalable method for high-quality consensus generation, particularly useful in health and performance science, supporting tailored guidance and frameworks.", "key_contributions": ["Introduction of the HAH-Delphi framework to integrate AI in expert consensus development.", "Demonstrated high levels of agreement between AI outputs and human expert consensus.", "Showed utility and effectiveness across multiple applied domains, including health and training."], "limitations": "AI's outputs lack experiential and pragmatic nuance, which was addressed by human expert panels.", "keywords": ["Human-AI Hybrid", "Delphi", "generative AI", "expert consensus", "health informatics"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.09469", "pdf": "https://arxiv.org/pdf/2508.09469.pdf", "abs": "https://arxiv.org/abs/2508.09469", "title": "Handows: A Palm-Based Interactive Multi-Window Management System in Virtual Reality", "authors": ["Jindu Wang", "Ke Zhou", "Haoyu Ren", "Per Ola Kristensson", "Xiang Li"], "categories": ["cs.HC"], "comment": "11 pages, 6 figures, 2 tables, IEEE International Symposium on Mixed\n  and Augmented Reality (ISMAR)", "summary": "Window management in virtual reality (VR) remains a challenging task due to\nthe spatial complexity and physical demands of current interaction methods. We\nintroduce Handows, a palm-based interface that enables direct manipulation of\nspatial windows through familiar smartphone-inspired gestures on the user's\nnon-dominant hand. Combining ergonomic layout design with body-centric input\nand passive haptics, Handows supports four core operations: window selection,\nclosure, positioning, and scaling. We evaluate Handows in a user study (N=15)\nagainst two common VR techniques (virtual hand and controller) across these\ncore window operations. Results show that Handows significantly reduces\nphysical effort and head movement while improving task efficiency and\ninteraction precision. A follow-up case study (N=8) demonstrates Handows'\nusability in realistic multitasking scenarios, highlighting user-adapted\nworkflows and spontaneous layout strategies. Our findings suggest the potential\nof embedding mobile-inspired metaphors into proprioceptive body-centric\ninterfaces to support low-effort and spatially coherent interaction in VR.", "AI": {"tldr": "Handows introduces a palm-based interface for managing windows in VR, enabling efficient task interaction through smartphone-inspired gestures.", "motivation": "Current VR window management methods are spatially complex and physically demanding. The aim is to improve user interaction and reduce physical strain.", "method": "Users manipulate windows using gestures on their non-dominant hand, combining ergonomic design with passive haptics. The interface was evaluated through a user study comparing it with traditional VR techniques.", "result": "Handows significantly decreased physical effort and head movement while enhancing task efficiency and precision in window operations compared to conventional methods.", "conclusion": "Embedding mobile-inspired metaphors into body-centric interfaces can lead to more effective and less taxing interactions in VR environments.", "key_contributions": ["Introduction of Handows, a novel palm-based interface for VR window management", "User study demonstrating improved efficiency and reduced physical effort with Handows", "Case study showcasing usability in multitasking scenarios"], "limitations": "The study had a small sample size and focused on specific VR tasks, which may limit generalizability.", "keywords": ["Virtual Reality", "Human-Computer Interaction", "User Study", "Ergonomic Design", "Gesture-Based Interface"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.09350", "pdf": "https://arxiv.org/pdf/2508.09350.pdf", "abs": "https://arxiv.org/abs/2508.09350", "title": "Flow-SLM: Joint Learning of Linguistic and Acoustic Information for Spoken Language Modeling", "authors": ["Ju-Chieh Chou", "Jiawei Zhou", "Karen Livescu"], "categories": ["cs.CL"], "comment": "Accepted to ASRU 2025", "summary": "Textless spoken language models (SLMs) are generative models of speech that\ndo not rely on text supervision. Most textless SLMs learn to predict the next\nsemantic token, a discrete representation of linguistic content, and rely on a\nseparate vocoder to add acoustic information to the generated speech. Such\nmodels have no access to acoustic context and no built-in control over acoustic\ndetails. In this work, we propose to jointly model linguistic and acoustic\ninformation by generating semantic tokens and a continuous real-valued\nrepresentation of the acoustic frame. We use a flow-matching objective to\npredict the continuous vector conditioned on the semantic tokens. We study the\ndesign space of this approach and find that predicting multiple future semantic\ntokens helps preserve linguistic information. Our approach achieves comparable\nperformance to existing models in terms of linguistic likelihood benchmarks,\nwhile providing better acoustic detail in prompted generation.", "AI": {"tldr": "This paper presents a joint modeling approach for textless spoken language models (SLMs) that combines semantic token generation with acoustic frame prediction, achieving improved acoustic detail in speech generation.", "motivation": "Current textless SLMs lack access to acoustic context, limiting their acoustic detail in generated speech. This work aims to address that limitation by jointly modeling linguistic and acoustic information.", "method": "The authors propose a flow-matching objective that predicts a continuous acoustic representation conditioned on generated semantic tokens and explore the design space to maximize linguistic information preservation.", "result": "The proposed approach achieves comparable performance to existing models on linguistic likelihood benchmarks while providing enhanced acoustic detail in generated speech.", "conclusion": "By jointly modeling linguistic and acoustic aspects, the new SLM approach improves the quality of the generated speech without needing text supervision.", "key_contributions": ["Jointly models linguistic and acoustic information in SLMs", "Predicts multiple future semantic tokens to enhance linguistic information preservation", "Uses flow-matching to predict continuous acoustic representation effectively"], "limitations": "", "keywords": ["Spoken language models", "Textless learning", "Acoustic representation", "Machine learning", "Natural language processing"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.09614", "pdf": "https://arxiv.org/pdf/2508.09614.pdf", "abs": "https://arxiv.org/abs/2508.09614", "title": "How Persuasive Could LLMs Be? A First Study Combining Linguistic-Rhetorical Analysis and User Experiments", "authors": ["Daniel Raffini", "Agnese Macori", "Lorenzo Porcaro", "Tiziana Catarci", "Marco Angelini"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": "9-pages", "summary": "This study examines the rhetorical and linguistic features of argumentative\ntexts generated by ChatGPT on ethically nuanced topics and investigates their\npersuasive impact on human readers.Through a user study involving 62\nparticipants and pre-post interaction surveys, the paper analyzes how exposure\nto AI-generated arguments affects opinion change and user perception. A\nlinguistic and rhetorical analysis of the generated texts reveals a consistent\nargumentative macrostructure, reliance on formulaic expressions, and limited\nstylistic richness. While ChatGPT demonstrates proficiency in constructing\ncoherent argumentative texts, its persuasive efficacy appears constrained,\nparticularly on topics involving ethical issues.The study finds that while\nparticipants often acknowledge the benefits highlighted by ChatGPT, ethical\nconcerns tend to persist or even intensify post-interaction. The results also\ndemonstrate a variation depending on the topic. These findings highlight new\ninsights on AI-generated persuasion in ethically sensitive domains and are a\nbasis for future research.", "AI": {"tldr": "Study analyzes AI-generated argumentative texts by ChatGPT on ethical topics and their impact on readers' opinions.", "motivation": "To explore how AI-generated arguments affect human readers' opinions, particularly in ethically nuanced contexts.", "method": "User study with 62 participants, using pre-post interaction surveys and linguistic/rhetorical analysis of the generated texts.", "result": "ChatGPT creates coherent arguments but struggles with persuasiveness on ethical issues; exposure often intensifies ethical concerns.", "conclusion": "AI-generated texts show potential in argument construction but limited effectiveness in persuasion on ethical topics, revealing areas for future research.", "key_contributions": ["Investigates the impact of AI-generated arguments on opinion change in ethical contexts", "Identifies the limitations of ChatGPT's persuasive capabilities", "Provides a foundation for future studies on AI persuasion."], "limitations": "Constraints in persuasive efficacy identified, particularly in ethically sensitive areas; findings may vary by topic.", "keywords": ["ChatGPT", "Argumentation", "Persuasion", "Ethics", "Human-Computer Interaction"], "importance_score": 7, "read_time_minutes": 9}}
{"id": "2508.09378", "pdf": "https://arxiv.org/pdf/2508.09378.pdf", "abs": "https://arxiv.org/abs/2508.09378", "title": "APIO: Automatic Prompt Induction and Optimization for Grammatical Error Correction and Text Simplification", "authors": ["Artem Chernodub", "Aman Saini", "Yejin Huh", "Vivek Kulkarni", "Vipul Raheja"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "Accepted for publication at Recent Advances in Natural Language\n  Processing conference (RANLP 2025)", "summary": "Recent advancements in large language models (LLMs) have enabled a wide range\nof natural language processing (NLP) tasks to be performed through simple\nprompt-based interactions. Consequently, several approaches have been proposed\nto engineer prompts that most effectively enable LLMs to perform a given task\n(e.g., chain-of-thought prompting). In settings with a well-defined metric to\noptimize model performance, automatic prompt optimization (APO) methods have\nbeen developed to refine a seed prompt. Advancing this line of research, we\npropose APIO, a simple but effective prompt induction and optimization approach\nfor the tasks of Grammatical Error Correction (GEC) and Text Simplification,\nwithout relying on manually specified seed prompts. APIO achieves a new\nstate-of-the-art performance for purely LLM-based prompting methods on these\ntasks. We make our data, code, prompts, and outputs publicly available.", "AI": {"tldr": "This paper introduces APIO, an automatic prompt induction and optimization approach for Grammatical Error Correction (GEC) and Text Simplification that outperforms existing LLM-prompting methods.", "motivation": "The need to enhance LLM performance through effective prompt engineering without manual seed prompts is the primary motivation of this research.", "method": "APIO employs a novel automatic prompt induction methodology that refines prompts specifically for GEC and Text Simplification tasks without relying on predefined prompts.", "result": "APIO achieves a new state-of-the-art performance in LLM-based prompting for the tasks of Grammatical Error Correction and Text Simplification.", "conclusion": "The efficacy of APIO demonstrates that automatic prompt induction can significantly improve LLM task performance while providing publicly available resources for further research.", "key_contributions": ["Introduction of APIO for prompt induction and optimization", "Achieving new state-of-the-art performance on GEC and Text Simplification", "Public availability of data, code, and outputs for community use"], "limitations": "", "keywords": ["large language models", "natural language processing", "prompt engineering", "grammatical error correction", "text simplification"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.09651", "pdf": "https://arxiv.org/pdf/2508.09651.pdf", "abs": "https://arxiv.org/abs/2508.09651", "title": "A Close Reading Approach to Gender Narrative Biases in AI-Generated Stories", "authors": ["Daniel Raffini", "Agnese Macori", "Marco Angelini", "Tiziana Catarci"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": "8-pages", "summary": "The paper explores the study of gender-based narrative biases in stories\ngenerated by ChatGPT, Gemini, and Claude. The prompt design draws on Propp's\ncharacter classifications and Freytag's narrative structure. The stories are\nanalyzed through a close reading approach, with particular attention to\nadherence to the prompt, gender distribution of characters, physical and\npsychological descriptions, actions, and finally, plot development and\ncharacter relationships. The results reveal the persistence of biases -\nespecially implicit ones - in the generated stories and highlight the\nimportance of assessing biases at multiple levels using an interpretative\napproach.", "AI": {"tldr": "The paper investigates gender-based biases in narratives generated by AI models like ChatGPT, Gemini, and Claude, using a close reading approach to analyze character distribution and plot elements.", "motivation": "To understand gender biases in AI-generated narratives and the implications these biases have on storytelling.", "method": "Close reading analysis of stories generated by AI models, focusing on character gender distribution, descriptions, actions, and plot development based on Propp's and Freytag's structures.", "result": "The analysis reveals the persistence of both explicit and implicit gender biases in AI-generated stories, necessitating a multi-level assessment approach.", "conclusion": "Bias assessment in AI narratives is crucial and should be conducted using an interpretative framework to understand its nuances and implications.", "key_contributions": ["Identification of gender biases in AI-generated narratives", "Methodological approach utilizing literary analysis frameworks", "Highlighting the need for comprehensive bias assessments in AI outputs."], "limitations": "Focuses primarily on narrative and character aspects without exploring other dimensions of biases in AI-generated content.", "keywords": ["gender bias", "AI narratives", "ChatGPT", "narrative analysis", "story generation"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2508.09403", "pdf": "https://arxiv.org/pdf/2508.09403.pdf", "abs": "https://arxiv.org/abs/2508.09403", "title": "Columbo: Expanding Abbreviated Column Names for Tabular Data Using Large Language Models", "authors": ["Ting Cai", "Stephen Sheen", "AnHai Doan"], "categories": ["cs.CL", "cs.DB"], "comment": null, "summary": "Expanding the abbreviated column names of tables, such as ``esal'' to\n``employee salary'', is critical for numerous downstream data tasks. This\nproblem arises in enterprises, domain sciences, government agencies, and more.\nIn this paper we make three contributions that significantly advances the state\nof the art. First, we show that synthetic public data used by prior work has\nmajor limitations, and we introduce 4 new datasets in enterprise/science\ndomains, with real-world abbreviations. Second, we show that accuracy measures\nused by prior work seriously undercount correct expansions, and we propose new\nsynonym-aware measures that capture accuracy much more accurately. Finally, we\ndevelop Columbo, a powerful LLM-based solution that exploits context, rules,\nchain-of-thought reasoning, and token-level analysis. Extensive experiments\nshow that Columbo significantly outperforms NameGuess, the current most\nadvanced solution, by 4-29\\%, over 5 datasets. Columbo has been used in\nproduction on EDI, a major data portal for environmental sciences.", "AI": {"tldr": "This paper introduces Columbo, an LLM-based solution for expanding abbreviated column names in datasets, overcoming limitations of previous methods and introducing new datasets and accuracy measures.", "motivation": "The need for expanding abbreviated column names in data tables is critical for various applications across enterprises, sciences, and government agencies.", "method": "The paper develops Columbo, an LLM-based system that utilizes context, rules, chain-of-thought reasoning, and token-level analysis to achieve accurate column name expansions.", "result": "Columbo outperforms the current leading solution, NameGuess, by 4-29% across five datasets.", "conclusion": "The introduction of new datasets and improved evaluation metrics alongside the Columbo solution can enhance the effectiveness of data processing tasks across multiple domains.", "key_contributions": ["Introduction of four new datasets with real-world abbreviations", "Proposal of new synonym-aware accuracy measures", "Development of Columbo, an advanced LLM-based solution for column name expansion"], "limitations": "", "keywords": ["column name expansion", "language model", "data accuracy", "dataset improvement", "HCI"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2508.09911", "pdf": "https://arxiv.org/pdf/2508.09911.pdf", "abs": "https://arxiv.org/abs/2508.09911", "title": "Wisdom of the Crowd, Without the Crowd: A Socratic LLM for Asynchronous Deliberation on Perspectivist Data", "authors": ["Malik Khadar", "Daniel Runningen", "Julia Tang", "Stevie Chancellor", "Harmanpreet Kaur"], "categories": ["cs.HC"], "comment": "To appear at CSCW 2025", "summary": "Data annotation underpins the success of modern AI, but the aggregation of\ncrowd-collected datasets can harm the preservation of diverse perspectives in\ndata. Difficult and ambiguous tasks cannot easily be collapsed into unitary\nlabels. Prior work has shown that deliberation and discussion improve data\nquality and preserve diverse perspectives -- however, synchronous deliberation\nthrough crowdsourcing platforms is time-intensive and costly. In this work, we\ncreate a Socratic dialog system using Large Language Models (LLMs) to act as a\ndeliberation partner in place of other crowdworkers. Against a benchmark of\nsynchronous deliberation on two tasks (Sarcasm and Relation detection), our\nSocratic LLM encouraged participants to consider alternate annotation\nperspectives, update their labels as needed (with higher confidence), and\nresulted in higher annotation accuracy (for the Relation task where ground\ntruth is available). Qualitative findings show that our agent's Socratic\napproach was effective at encouraging reasoned arguments from our participants,\nand that the intervention was well-received. Our methodology lays the\ngroundwork for building scalable systems that preserve individual perspectives\nin generating more representative datasets.", "AI": {"tldr": "A Socratic dialog system leveraging LLMs enhances data annotation by encouraging diverse perspectives and improving accuracy, compared to traditional synchronous crowd deliberation.", "motivation": "To address the issue that crowd-collected datasets can fail to preserve diverse perspectives, particularly for tasks that are difficult or ambiguous, and to find a more efficient method for enhancing data quality.", "method": "A Socratic dialog system using Large Language Models (LLMs) engages participants in deliberation, allowing for consideration of different annotation perspectives and helping them improve their confidence in labels during the annotation process.", "result": "Participants exhibited higher annotation accuracy in the Relation task, and qualitative feedback indicated that the LLM's Socratic approach effectively promoted reasoned arguments and was well-received.", "conclusion": "The proposed LLM-based system serves as a scalable method to enhance data annotation processes, ensuring that diverse perspectives are preserved and improving the overall quality of datasets.", "key_contributions": ["Introduction of a Socratic dialog system for data annotation using LLMs", "Demonstrated improvement in annotation accuracy for the Relation task", "Qualitative validation of the approach through participant feedback"], "limitations": "The method's effectiveness may vary depending on the complexity of tasks and the representativeness of dialogue generated by the LLM.", "keywords": ["Data Annotation", "Socratic Dialog", "Large Language Models", "Crowdsourcing", "Diverse Perspectives"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.09430", "pdf": "https://arxiv.org/pdf/2508.09430.pdf", "abs": "https://arxiv.org/abs/2508.09430", "title": "Leveraging Zipformer Model for Effective Language Identification in Code-Switched Child-Directed Speech", "authors": ["Lavanya Shankar", "Leibny Paola Garcia Perera"], "categories": ["cs.CL", "cs.SD"], "comment": null, "summary": "Code-switching and language identification in child-directed scenarios\npresent significant challenges, particularly in bilingual environments. This\npaper addresses this challenge by using Zipformer to handle the nuances of\nspeech, which contains two imbalanced languages, Mandarin and English, in an\nutterance. This work demonstrates that the internal layers of the Zipformer\neffectively encode the language characteristics, which can be leveraged in\nlanguage identification. We present the selection methodology of the inner\nlayers to extract the embeddings and make a comparison with different\nback-ends. Our analysis shows that Zipformer is robust across these backends.\nOur approach effectively handles imbalanced data, achieving a Balanced Accuracy\n(BAC) of 81.89%, a 15.47% improvement over the language identification\nbaseline. These findings highlight the potential of the transformer encoder\narchitecture model in real scenarios.", "AI": {"tldr": "This paper explores the use of Zipformer for language identification in child-directed speech containing Mandarin and English, achieving significant improvements in accuracy.", "motivation": "The paper addresses the challenges of code-switching and language identification in bilingual environments, particularly for child-directed speech.", "method": "Utilizing the Zipformer architecture to capture and encode the nuances of speech in two imbalanced languages, Mandarin and English, by selecting appropriate inner layers for embedding extraction.", "result": "Achieved a Balanced Accuracy (BAC) of 81.89%, representing a 15.47% improvement over existing language identification baselines.", "conclusion": "The results support the effectiveness of Zipformer in handling imbalanced data and highlight the applicability of transformer models in real-world language identification tasks.", "key_contributions": ["Demonstrated the effectiveness of Zipformer for language identification in bilingual child-directed scenarios.", "Showcased a methodology for selecting inner layers to enhance language encoding.", "Achieved significant improvements in identification accuracy over baseline models."], "limitations": "", "keywords": ["code-switching", "language identification", "Zipformer", "Mandarin", "English"], "importance_score": 6, "read_time_minutes": 8}}
{"id": "2508.09450", "pdf": "https://arxiv.org/pdf/2508.09450.pdf", "abs": "https://arxiv.org/abs/2508.09450", "title": "From Charts to Fair Narratives: Uncovering and Mitigating Geo-Economic Biases in Chart-to-Text", "authors": ["Ridwan Mahbub", "Mohammed Saidul Islam", "Mir Tafseer Nayeem", "Md Tahmid Rahman Laskar", "Mizanur Rahman", "Shafiq Joty", "Enamul Hoque"], "categories": ["cs.CL"], "comment": null, "summary": "Charts are very common for exploring data and communicating insights, but\nextracting key takeaways from charts and articulating them in natural language\ncan be challenging. The chart-to-text task aims to automate this process by\ngenerating textual summaries of charts. While with the rapid advancement of\nlarge Vision-Language Models (VLMs), we have witnessed great progress in this\ndomain, little to no attention has been given to potential biases in their\noutputs. This paper investigates how VLMs can amplify geo-economic biases when\ngenerating chart summaries, potentially causing societal harm. Specifically, we\nconduct a large-scale evaluation of geo-economic biases in VLM-generated chart\nsummaries across 6,000 chart-country pairs from six widely used proprietary and\nopen-source models to understand how a country's economic status influences the\nsentiment of generated summaries. Our analysis reveals that existing VLMs tend\nto produce more positive descriptions for high-income countries compared to\nmiddle- or low-income countries, even when country attribution is the only\nvariable changed. We also find that models such as GPT-4o-mini,\nGemini-1.5-Flash, and Phi-3.5 exhibit varying degrees of bias. We further\nexplore inference-time prompt-based debiasing techniques using positive\ndistractors but find them only partially effective, underscoring the complexity\nof the issue and the need for more robust debiasing strategies. Our code and\ndataset are publicly available here.", "AI": {"tldr": "The paper examines geo-economic biases in Vision-Language Models (VLMs) that generate summaries of charts, revealing that VLMs tend to favor high-income countries in their summaries.", "motivation": "To address the gap in understanding how large Vision-Language Models may perpetuate geo-economic biases when summarizing charts, which can lead to societal harm.", "method": "A large-scale evaluation of VLM-generated chart summaries was conducted across 6,000 chart-country pairs from various models to assess the impact of a country's economic status on summary sentiment.", "result": "The findings indicate that VLMs generate more positive summaries for high-income countries compared to their middle- or low-income counterparts. Models like GPT-4o-mini and Gemini-1.5-Flash were found to exhibit different levels of bias.", "conclusion": "The study highlights the necessity for more effective debiasing strategies, as current prompt-based techniques show limited effectiveness in mitigating biases in VLM outputs.", "key_contributions": ["Identification of geo-economic bias in VLM-generated chart summaries", "Demonstration of varying degrees of bias among different VLMs", "Exploration of partial effectiveness of debiasing techniques"], "limitations": "The study primarily focuses on geo-economic bias, without addressing other potential biases or the broader implications of VLM misrepresentation.", "keywords": ["Vision-Language Models", "geo-economic bias", "chart summarization", "debiasing", "natural language generation"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2508.09463", "pdf": "https://arxiv.org/pdf/2508.09463.pdf", "abs": "https://arxiv.org/abs/2508.09463", "title": "User-centric Subjective Leaderboard by Customizable Reward Modeling", "authors": ["Qi Jia", "Xiujie Song", "Zicheng Zhang", "Yijin Guo", "Kaiwei Zhang", "Zijian Chen", "Guangtao Zhai"], "categories": ["cs.CL"], "comment": null, "summary": "Existing benchmarks for large language models (LLMs) predominantely focus on\nassessing their capabilities through verifiable tasks. Such objective and\nstatic benchmarks offer limited utility for practical LLM selection, making it\ndifficult for users to find suitable models for their individual needs. To\nbridge this gap, we present the first User-Centric Subjective Leaderboard\n(USL), which provides a preference-driven, dynamic ranking of LLMs across\ndiverse real-world scenarios. Our work is built upon a thorough investigation\nof real human preference data, involving more than 10K subjective queries. Our\ninvestigation reveals significant diversity and contradictions in human\npreferences, which limit the effectiveness of state-of-the-art reward models.\nTo address this, we introduce Customizable Reward Models (CRMs). With only 4B\nparameters, our CRM surpasses the performance of leading models such as GPT-4.1\nand Gemini-2.5-pro, showing exceptional generalization capabilities across new\ntopics and criteria. The USL, powered by CRMs, exhibits strong negative\ncorrelations to contradictory preferences.", "AI": {"tldr": "Introduction of a User-Centric Subjective Leaderboard (USL) for ranking large language models (LLMs) based on human preferences.", "motivation": "To improve LLM selection for users by moving beyond static, verifiable tasks to a more subjective evaluation based on individual needs.", "method": "Development of the USL based on over 10,000 subjective queries and the introduction of Customizable Reward Models (CRMs) with 4B parameters for enhanced ranking accuracy.", "result": "CRMs exceed the performance of leading LLMs like GPT-4.1 and Gemini-2.5-pro, demonstrating improved generalization over diverse topics and criteria.", "conclusion": "The USL, driven by CRMs, efficiently captures human preferences and improves model selection for practical applications.", "key_contributions": ["Creation of User-Centric Subjective Leaderboard (USL)", "Introduction of Customizable Reward Models (CRMs)", "Demonstration of CRMs outperforming established LLMs"], "limitations": "", "keywords": ["large language models", "user-centric leaderboard", "customizable reward models", "human preferences", "subjective evaluation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.09494", "pdf": "https://arxiv.org/pdf/2508.09494.pdf", "abs": "https://arxiv.org/abs/2508.09494", "title": "Learning Facts at Scale with Active Reading", "authors": ["Jessy Lin", "Vincent-Pierre Berges", "Xilun Chen", "Wen-Tau Yih", "Gargi Ghosh", "Barlas Oğuz"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "LLMs are known to store vast amounts of knowledge in their parametric memory.\nHowever, learning and recalling facts from this memory is known to be\nunreliable, depending largely on the prevalence of particular facts in the\ntraining data and other factors which are poorly understood. Practitioners are\nlacking tools which will allow them to ensure that the models learn a given\nbody of knowledge reliably and consistently. To this end, we propose Active\nReading: a framework where we train models to study a given set of material\nwith self-generated learning strategies. First, we demonstrate models trained\nwith Active Reading on expert domains absorb significantly more knowledge than\nvanilla finetuning and other data augmentations. We train expert 8B models that\nachieve 66% on a Wikipedia-grounded subset of SimpleQA (+313% relative over\nvanilla finetuning) and 26% on FinanceBench (+160% relative over vanilla\nfinetuning) by applying Active Reading to the source documents for each\nbenchmark. Finally, we show that Active Reading can be utilized at pre-training\nscale to build more factual models. As a demonstration of this, we release Meta\nWikiExpert-8B, a Wikipedia-expert model trained on 1 trillion generated tokens,\nwhich outcompetes models with hundreds of billions of parameters on factual QA.", "AI": {"tldr": "The paper presents Active Reading, a framework for training models to reliably learn and recall knowledge from a specified body of material, resulting in significant improvements in model performance on factual tasks.", "motivation": "Practitioners lack reliable tools for ensuring that models learn knowledge consistently from their training data.", "method": "The authors propose a framework called Active Reading, which trains models to study specified material using self-generated learning strategies.", "result": "Models trained with Active Reading show marked improvement, achieving 66% on SimpleQA and 26% on FinanceBench, compared to vanilla finetuning, with a 313% and 160% relative improvement, respectively.", "conclusion": "Active Reading can enhance knowledge absorption in models and has been effectively scaled to improve factual accuracy in larger models like Meta WikiExpert-8B.", "key_contributions": ["Introduction of the Active Reading framework", "Demonstrated significant performance improvements over conventional fine-tuning", "Release of Meta WikiExpert-8B, outperforming larger models in factual QA tasks."], "limitations": "", "keywords": ["Active Reading", "Knowledge recall", "Model training", "Natural language processing", "Factual accuracy"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.09497", "pdf": "https://arxiv.org/pdf/2508.09497.pdf", "abs": "https://arxiv.org/abs/2508.09497", "title": "From Ranking to Selection: A Simple but Efficient Dynamic Passage Selector for Retrieval Augmented Generation", "authors": ["Siyuan Meng", "Junming Liu", "Yirong Chen", "Song Mao", "Pinlong Cai", "Guohang Yan", "Botian Shi", "Ding Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 4 tables", "summary": "Retrieval-augmented generation (RAG) systems are often bottlenecked by their\nreranking modules, which typically score passages independently and select a\nfixed Top-K size. This approach struggles with complex multi-hop queries that\nrequire synthesizing evidence across multiple documents, creating a trade-off\nwhere small K values omit crucial information and large K values introduce\nnoise. To address this, we introduce the Dynamic Passage Selector (DPS), a\nnovel reranking framework that treats passage selection as a supervised\nlearning problem. Unlike traditional point-wise or list-wise methods, DPS is\nfine-tuned to capture inter-passage dependencies and dynamically select the\nmost relevant set of passages for generation. As a seamless plug-and-play\nmodule, DPS requires no modifications to the standard RAG pipeline.\nComprehensive evaluations on five benchmarks show that DPS consistently\noutperforms state-of-the-art rerankers and fine-tuning methods. Notably, on the\nchallenging MuSiQue dataset, DPS improves the F1-score by 30.06% and 15.4% over\nstrong baselines like Qwen3-reranker and RankingGPT, respectively. Our results\ndemonstrate that by enabling adaptive evidence selection, DPS substantially\nenhances reasoning capabilities in complex RAG scenarios.", "AI": {"tldr": "The paper presents the Dynamic Passage Selector (DPS), a new reranking framework for retrieval-augmented generation systems that improves passage selection for complex queries by capturing inter-passage dependencies.", "motivation": "Traditional reranking approaches in retrieval-augmented generation systems are limited by their fixed Top-K selection and inability to effectively manage multi-hop queries, resulting in critical information loss or too much noise.", "method": "DPS treats passage selection as a supervised learning problem, enabling fine-tuning to dynamically select the most relevant passages based on inter-passage dependencies, thus improving the RAG pipeline.", "result": "DPS outperforms existing state-of-the-art rerankers by significantly increasing the F1-score on the MuSiQue dataset by over 30% compared to strong baselines, demonstrating improved reasoning in complex information synthesis tasks.", "conclusion": "By enabling adaptive evidence selection through a novel supervised approach, DPS significantly enhances the capabilities of retrieval-augmented generation systems, particularly in handling complex queries.", "key_contributions": ["Introduction of the Dynamic Passage Selector (DPS) for RAG systems", "Demonstration of significant performance improvements on multiple benchmarks", "Development of a method that captures inter-passage dependencies for better evidence selection"], "limitations": "", "keywords": ["Retrieval-augmented generation", "Reranking", "Dynamic Passage Selector", "Multi-hop queries", "Supervised learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.09515", "pdf": "https://arxiv.org/pdf/2508.09515.pdf", "abs": "https://arxiv.org/abs/2508.09515", "title": "LACA: Improving Cross-lingual Aspect-Based Sentiment Analysis with LLM Data Augmentation", "authors": ["Jakub Šmíd", "Pavel Přibáň", "Pavel Král"], "categories": ["cs.CL"], "comment": "Published in Proceedings of the 63rd Annual Meeting of the\n  Association for Computational Linguistics; Volume 1: Long Papers (ACL 2025).\n  Official version: https://aclanthology.org/2025.acl-long.41/", "summary": "Cross-lingual aspect-based sentiment analysis (ABSA) involves detailed\nsentiment analysis in a target language by transferring knowledge from a source\nlanguage with available annotated data. Most existing methods depend heavily on\noften unreliable translation tools to bridge the language gap. In this paper,\nwe propose a new approach that leverages a large language model (LLM) to\ngenerate high-quality pseudo-labelled data in the target language without the\nneed for translation tools. First, the framework trains an ABSA model to obtain\npredictions for unlabelled target language data. Next, LLM is prompted to\ngenerate natural sentences that better represent these noisy predictions than\nthe original text. The ABSA model is then further fine-tuned on the resulting\npseudo-labelled dataset. We demonstrate the effectiveness of this method across\nsix languages and five backbone models, surpassing previous state-of-the-art\ntranslation-based approaches. The proposed framework also supports generative\nmodels, and we show that fine-tuned LLMs outperform smaller multilingual\nmodels.", "AI": {"tldr": "The paper presents a novel approach for cross-lingual aspect-based sentiment analysis (ABSA) using large language models (LLMs) to generate high-quality pseudo-labelled data, avoiding reliance on translation tools.", "motivation": "Existing methods for cross-lingual ABSA often rely on unreliable translation tools, which can compromise the quality of sentiment analysis.", "method": "The proposed framework first trains an ABSA model on unlabelled target language data and then uses an LLM to generate natural sentences that improve upon noisy predictions. The model is subsequently fine-tuned on this pseudo-labelled dataset.", "result": "The method was tested across six languages and five backbone models, showing improved effectiveness compared to traditional translation-based approaches.", "conclusion": "The framework not only enhances sentiment analysis accuracy across languages but also shows that fine-tuned LLMs can outperform smaller multilingual models.", "key_contributions": ["Introduces a method to generate high-quality pseudo-labelled data for ABSA using LLMs.", "Demonstrates effectiveness across multiple languages and model architectures.", "Provides an alternative to reliance on potentially unreliable translation tools."], "limitations": "", "keywords": ["cross-lingual", "aspect-based sentiment analysis", "large language models", "pseudo-labelling", "natural language generation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.09516", "pdf": "https://arxiv.org/pdf/2508.09516.pdf", "abs": "https://arxiv.org/abs/2508.09516", "title": "Cross-lingual Aspect-Based Sentiment Analysis: A Survey on Tasks, Approaches, and Challenges", "authors": ["Jakub Šmíd", "Pavel Král"], "categories": ["cs.CL"], "comment": "Submitted version prior to peer review. Updated version accepted in\n  Information Fusion. Official version:\n  https://www.sciencedirect.com/science/article/pii/S1566253525001460", "summary": "Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis\ntask that focuses on understanding opinions at the aspect level, including\nsentiment towards specific aspect terms, categories, and opinions. While ABSA\nresearch has seen significant progress, much of the focus has been on\nmonolingual settings. Cross-lingual ABSA, which aims to transfer knowledge from\nresource-rich languages (such as English) to low-resource languages, remains an\nunder-explored area, with no systematic review of the field. This paper aims to\nfill that gap by providing a comprehensive survey of cross-lingual ABSA. We\nsummarize key ABSA tasks, including aspect term extraction, aspect sentiment\nclassification, and compound tasks involving multiple sentiment elements.\nAdditionally, we review the datasets, modelling paradigms, and cross-lingual\ntransfer methods used to solve these tasks. We also examine how existing work\nin monolingual and multilingual ABSA, as well as ABSA with LLMs, contributes to\nthe development of cross-lingual ABSA. Finally, we highlight the main\nchallenges and suggest directions for future research to advance cross-lingual\nABSA systems.", "AI": {"tldr": "This paper surveys cross-lingual aspect-based sentiment analysis (ABSA), a field underexplored compared to monolingual ABSA, summarizing tasks, datasets, models, and challenges.", "motivation": "To address the lack of systematic reviews in the field of cross-lingual aspect-based sentiment analysis (ABSA) and highlight its importance in transferring knowledge from rich to low-resource languages.", "method": "The paper reviews key tasks in ABSA, such as aspect term extraction and sentiment classification, and examines relevant datasets and modeling approaches, including the role of large language models (LLMs) in advancing cross-lingual ABSA.", "result": "The paper identifies and summarizes the main tasks in cross-lingual ABSA, providing insights into current methodologies, datasets used, and existing challenges in the field.", "conclusion": "This survey lays the groundwork for future research in cross-lingual ABSA by identifying critical areas for advancement and existing research gaps.", "key_contributions": ["Comprehensive survey of cross-lingual ABSA", "Insight into datasets and modeling paradigms", "Identification of challenges and future research directions"], "limitations": "Limited focus on the technical details of cross-lingual model implementation and performance metrics.", "keywords": ["aspect-based sentiment analysis", "cross-lingual", "large language models", "sentiment classification", "natural language processing"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2508.09786", "pdf": "https://arxiv.org/pdf/2508.09786.pdf", "abs": "https://arxiv.org/abs/2508.09786", "title": "Adoption of Explainable Natural Language Processing: Perspectives from Industry and Academia on Practices and Challenges", "authors": ["Mahdi Dhaini", "Tobias Müller", "Roksoliana Rabets", "Gjergji Kasneci"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "Accepted to AAAI/ACM Conference on AI, Ethics, and Society (AIES\n  2025)", "summary": "The field of explainable natural language processing (NLP) has grown rapidly\nin recent years. The growing opacity of complex models calls for transparency\nand explanations of their decisions, which is crucial to understand their\nreasoning and facilitate deployment, especially in high-stakes environments.\nDespite increasing attention given to explainable NLP, practitioners'\nperspectives regarding its practical adoption and effectiveness remain\nunderexplored. This paper addresses this research gap by investigating\npractitioners' experiences with explainability methods, specifically focusing\non their motivations for adopting such methods, the techniques employed,\nsatisfaction levels, and the practical challenges encountered in real-world NLP\napplications. Through a qualitative interview-based study with industry\npractitioners and complementary interviews with academic researchers, we\nsystematically analyze and compare their perspectives. Our findings reveal\nconceptual gaps, low satisfaction with current explainability methods, and\nhighlight evaluation challenges. Our findings emphasize the need for clear\ndefinitions and user-centric frameworks for better adoption of explainable NLP\nin practice.", "AI": {"tldr": "This paper investigates practitioners' experiences with explainable NLP methods, focusing on adoption motivations, techniques used, satisfaction levels, and challenges faced.", "motivation": "There is a growing need for transparency and explanations in complex NLP models, especially in high-stakes environments, yet practitioners' perspectives on explainable NLP are underexplored.", "method": "The study employs qualitative interviews with industry practitioners and academic researchers to analyze and compare their experiences with explainability methods in NLP applications.", "result": "Findings reveal conceptual gaps in understanding explainability, low satisfaction with existing methods, and highlight significant evaluation challenges faced by practitioners.", "conclusion": "The study underscores the necessity for user-centric frameworks and clearer definitions to enhance the practical adoption of explainable NLP.", "key_contributions": ["Identification of practitioners' motivations for adopting explainability methods", "Analysis of satisfaction levels with current explainability techniques", "Highlighting practical challenges in real-world NLP applications"], "limitations": "Focuses primarily on qualitative insights, which may not generalize across all NLP contexts.", "keywords": ["explainable NLP", "practitioner perspectives", "transparency", "user-centric frameworks", "real-world applications"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.09517", "pdf": "https://arxiv.org/pdf/2508.09517.pdf", "abs": "https://arxiv.org/abs/2508.09517", "title": "UWBa at SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval", "authors": ["Ladislav Lenc", "Daniel Cífka", "Jiří Martínek", "Jakub Šmíd", "Pavel Král"], "categories": ["cs.CL"], "comment": "Published in Proceedings of the 19th International Workshop on\n  Semantic Evaluation (SemEval-2025). Official version:\n  https://aclanthology.org/2025.semeval-1.31/", "summary": "This paper presents a zero-shot system for fact-checked claim retrieval. We\nemployed several state-of-the-art large language models to obtain text\nembeddings. The models were then combined to obtain the best possible result.\nOur approach achieved 7th place in monolingual and 9th in cross-lingual\nsubtasks. We used only English translations as an input to the text embedding\nmodels since multilingual models did not achieve satisfactory results. We\nidentified the most relevant claims for each post by leveraging the embeddings\nand measuring cosine similarity. Overall, the best results were obtained by the\nNVIDIA NV-Embed-v2 model. For some languages, we benefited from model\ncombinations (NV-Embed & GPT or Mistral).", "AI": {"tldr": "This paper introduces a zero-shot system for fact-checked claim retrieval using combined large language model embeddings, achieving top placements in relevant tasks.", "motivation": "To improve the accuracy and effectiveness of claim retrieval in a fact-checking context using advanced large language models.", "method": "Utilizing multiple state-of-the-art large language models to obtain text embeddings, which are then combined to enhance retrieval performance based on cosine similarity of the embeddings.", "result": "Achieved 7th place in monolingual and 9th in cross-lingual subtasks of claim retrieval. The best results were obtained using the NVIDIA NV-Embed-v2 model, with enhancements from model combinations in certain languages.", "conclusion": "The study demonstrates that combining various text embedding models can significantly improve the retrieval of relevant claims, particularly in a zero-shot setting.", "key_contributions": ["Introduction of a zero-shot system for claim retrieval", "Use of combined large language model embeddings", "Demonstrated improved retrieval performance over baseline models"], "limitations": "", "keywords": ["fact-checking", "claim retrieval", "large language models", "text embeddings", "cosine similarity"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2508.09521", "pdf": "https://arxiv.org/pdf/2508.09521.pdf", "abs": "https://arxiv.org/abs/2508.09521", "title": "COMPEER: Controllable Empathetic Reinforcement Reasoning for Emotional Support Conversation", "authors": ["Yunxiao Wang", "Meng Liu", "Wenqi Liu", "Kaiyu Jiang", "Bin Wen", "Fan Yang", "Tingting Gao", "Guorui Zhou", "Liqiang Nie"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Emotional support conversations are crucial for promoting emotional\nwell-being, yet current models often lack deep empathetic reasoning grounded in\npsychological principles. To address this, we propose controllable empathetic\nreasoning, which combines natural language reasoning with structured\npsychological steps. We construct a fine-grained dataset annotated with\nreasoning correctness and response preferences to enable this capability. To\nfurther enhance training, we employ reinforcement learning with a unified\nprocess-outcome reward model that delivers precise feedback. To mitigate\nresponse repetitiveness from entropy collapse, we introduce personality-based\ndialogue rewriting and a redundancy-aware reward reweighting strategy. Our\napproach significantly improves model's emotional support ability, advancing\nthe development of empathetic, human-like support systems.", "AI": {"tldr": "This paper proposes controllable empathetic reasoning for enhancing emotional support conversations in AI systems.", "motivation": "Emotional support conversations are important for well-being but lack deep empathetic reasoning in current models.", "method": "The authors created a fine-grained dataset annotated with reasoning correctness and response preferences, and employed reinforcement learning combined with a unified process-outcome reward model.", "result": "The proposed methods significantly enhance the emotional support ability of AI models, creating more empathetic, human-like interactions.", "conclusion": "The advancements made set a foundation for developing better empathetic support systems in AI.", "key_contributions": ["Introduction of controllable empathetic reasoning", "Creation of a fine-grained dataset for enhancing empathetic dialogue", "Use of personality-based dialogue rewriting and redundancy-aware reward strategies"], "limitations": "", "keywords": ["empathetic reasoning", "emotional support", "reinforcement learning", "human-like interaction", "dialogue systems"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2402.06071", "pdf": "https://arxiv.org/pdf/2402.06071.pdf", "abs": "https://arxiv.org/abs/2402.06071", "title": "Keyframer: Empowering Animation Design using Large Language Models", "authors": ["Tiffany Tseng", "Ruijia Cheng", "Jeffrey Nichols"], "categories": ["cs.HC"], "comment": null, "summary": "Creating 2D animations is a complex, iterative process requiring continuous\nadjustments to movement, timing, and coordination of multiple elements within a\nscene. To support designers of varying levels of experience with animation\ndesign and implementation, we developed Keyframer, a design tool that generates\nanimation code in response to natural language prompts, enabling users to\npreview rendered animations inline and edit them directly through provided\neditors. Through a user study with 13 novices and experts in animation design\nand programming, we contribute 1) a categorization of semantic prompt types for\ndescribing motion and identification of a 'decomposed' prompting style where\nusers continually adapt their goals in response to generated output; and 2)\ndesign insights on supporting iterative refinement of animations through the\ncombination of direct editing and natural language interfaces.", "AI": {"tldr": "Keyframer is a tool designed to simplify the creation of 2D animations through natural language prompts and inline rendering.", "motivation": "To aid designers, both novices and experts, in the complex iterative process of creating 2D animations.", "method": "A design tool named Keyframer was developed, allowing users to generate animation code from natural language prompts and edit it directly, while integrating user study feedback.", "result": "The study involved 13 participants and revealed a categorization of semantic prompt types for animation and insights into a decomposed prompting style where users adapt goals based on output.", "conclusion": "Keyframer combines natural language interfaces with direct editing to support the iterative refinement of animation designs effectively.", "key_contributions": ["Development of the Keyframer tool for 2D animation generation.", "Identification of semantic prompt categories for motion description.", "Insights for iterative refinement using natural language and editing."], "limitations": "", "keywords": ["2D animation", "Natural language processing", "User study"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2508.09603", "pdf": "https://arxiv.org/pdf/2508.09603.pdf", "abs": "https://arxiv.org/abs/2508.09603", "title": "The Surprising Effectiveness of Membership Inference with Simple N-Gram Coverage", "authors": ["Skyler Hallinan", "Jaehun Jung", "Melanie Sclar", "Ximing Lu", "Abhilasha Ravichander", "Sahana Ramnath", "Yejin Choi", "Sai Praneeth Karimireddy", "Niloofar Mireshghallah", "Xiang Ren"], "categories": ["cs.CL"], "comment": "CoLM 2025", "summary": "Membership inference attacks serves as useful tool for fair use of language\nmodels, such as detecting potential copyright infringement and auditing data\nleakage. However, many current state-of-the-art attacks require access to\nmodels' hidden states or probability distribution, which prevents investigation\ninto more widely-used, API-access only models like GPT-4. In this work, we\nintroduce N-Gram Coverage Attack, a membership inference attack that relies\nsolely on text outputs from the target model, enabling attacks on completely\nblack-box models. We leverage the observation that models are more likely to\nmemorize and subsequently generate text patterns that were commonly observed in\ntheir training data. Specifically, to make a prediction on a candidate member,\nN-Gram Coverage Attack first obtains multiple model generations conditioned on\na prefix of the candidate. It then uses n-gram overlap metrics to compute and\naggregate the similarities of these outputs with the ground truth suffix; high\nsimilarities indicate likely membership. We first demonstrate on a diverse set\nof existing benchmarks that N-Gram Coverage Attack outperforms other black-box\nmethods while also impressively achieving comparable or even better performance\nto state-of-the-art white-box attacks - despite having access to only text\noutputs. Interestingly, we find that the success rate of our method scales with\nthe attack compute budget - as we increase the number of sequences generated\nfrom the target model conditioned on the prefix, attack performance tends to\nimprove. Having verified the accuracy of our method, we use it to investigate\npreviously unstudied closed OpenAI models on multiple domains. We find that\nmore recent models, such as GPT-4o, exhibit increased robustness to membership\ninference, suggesting an evolving trend toward improved privacy protections.", "AI": {"tldr": "The N-Gram Coverage Attack enables membership inference attacks on black-box language models using only text outputs, outperforming existing methods and highlighting evolving privacy protections in models like GPT-4.", "motivation": "To develop a method for membership inference attacks on black-box models that do not require access to hidden states or probability distributions.", "method": "The method uses n-gram overlap metrics to assess the similarity of model outputs to a candidate ground truth, inferring membership based on the extent of overlap in generated text patterns.", "result": "The N-Gram Coverage Attack outperforms existing black-box methods and achieves performance comparable to white-box attacks using text outputs alone. The attack's success rate improves with the compute budget used for generating outputs.", "conclusion": "The method effectively investigates membership inference in closed models like GPT-4, suggesting enhanced privacy measures in more recent models.", "key_contributions": ["Introduced a novel membership inference attack for black-box models using n-gram overlap.", "Demonstrated superior performance on existing benchmarks compared to other black-box methods.", "Investigated privacy of previously unstudied closed models, revealing trends in model robustness."], "limitations": "", "keywords": ["membership inference", "N-Gram Coverage Attack", "privacy protection", "black-box models", "language models"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.09622", "pdf": "https://arxiv.org/pdf/2508.09622.pdf", "abs": "https://arxiv.org/abs/2508.09622", "title": "AINL-Eval 2025 Shared Task: Detection of AI-Generated Scientific Abstracts in Russian", "authors": ["Tatiana Batura", "Elena Bruches", "Milana Shvenk", "Valentin Malykh"], "categories": ["cs.CL"], "comment": "AINL 2025 Conference", "summary": "The rapid advancement of large language models (LLMs) has revolutionized text\ngeneration, making it increasingly difficult to distinguish between human- and\nAI-generated content. This poses a significant challenge to academic integrity,\nparticularly in scientific publishing and multilingual contexts where detection\nresources are often limited. To address this critical gap, we introduce the\nAINL-Eval 2025 Shared Task, specifically focused on the detection of\nAI-generated scientific abstracts in Russian. We present a novel, large-scale\ndataset comprising 52,305 samples, including human-written abstracts across 12\ndiverse scientific domains and AI-generated counterparts from five\nstate-of-the-art LLMs (GPT-4-Turbo, Gemma2-27B, Llama3.3-70B, Deepseek-V3, and\nGigaChat-Lite). A core objective of the task is to challenge participants to\ndevelop robust solutions capable of generalizing to both (i) previously unseen\nscientific domains and (ii) models not included in the training data. The task\nwas organized in two phases, attracting 10 teams and 159 submissions, with top\nsystems demonstrating strong performance in identifying AI-generated content.\nWe also establish a continuous shared task platform to foster ongoing research\nand long-term progress in this important area. The dataset and platform are\npublicly available at https://github.com/iis-research-team/AINL-Eval-2025.", "AI": {"tldr": "This paper presents the AINL-Eval 2025 Shared Task focused on detecting AI-generated scientific abstracts in Russian, utilizing a large dataset and fostering ongoing research in the field.", "motivation": "The rise of large language models (LLMs) complicates the distinction between human and AI-generated text, posing challenges to academic integrity in multilingual contexts.", "method": "The paper introduces a large dataset of 52,305 samples containing both human-written and AI-generated scientific abstracts across 12 domains, designed for a shared task that encourages robust solution development for detecting AI content.", "result": "Ten teams participated in the task, with 159 submissions demonstrating effective identification of AI-generated scientific abstracts.", "conclusion": "The establishment of a shared task platform aims to support continuous research and advancements in detecting AI-generated content.", "key_contributions": ["Introduction of a large-scale dataset for detecting AI-generated academic content in Russian.", "Successful organization of a competitive shared task with active participation from multiple teams.", "Creation of a continuous platform for ongoing research in AI detection methods."], "limitations": "", "keywords": ["AI-generated content", "scientific abstracts", "large language models", "detection", "shared task"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.20062", "pdf": "https://arxiv.org/pdf/2506.20062.pdf", "abs": "https://arxiv.org/abs/2506.20062", "title": "Beyond Autocomplete: Designing CopilotLens Towards Transparent and Explainable AI Coding Agents", "authors": ["Runlong Ye", "Zeling Zhang", "Boushra Almazroua", "Michael Liut"], "categories": ["cs.HC", "cs.AI"], "comment": "accepted at The First Workshop on the Application of LLM\n  Explainability to Reasoning and Planning (XLLM-Reason-Plan) @ COLM 2025", "summary": "AI-powered code assistants are widely used to generate code completions,\nsignificantly boosting developer productivity. However, these tools typically\npresent suggestions without explaining their rationale, leaving their\ndecision-making process inscrutable. This opacity hinders developers' ability\nto critically evaluate outputs, form accurate mental models, and calibrate\ntrust in the system. To address this, we introduce CopilotLens, a novel\ninteractive framework that reframes code completion from a simple suggestion\ninto a transparent, explainable interaction. CopilotLens operates as an\nexplanation layer that reconstructs the AI agent's \"thought process\" through a\ndynamic, two-level interface. The tool aims to surface both high-level code\nchanges and the specific codebase context influences. This paper presents the\ndesign and rationale of CopilotLens, offering a concrete framework and\narticulating expectations on deepening comprehension and calibrated trust,\nwhich we plan to evaluate in subsequent work.", "AI": {"tldr": "This paper introduces CopilotLens, an interactive framework that enhances code completion tools by providing transparent explanations of AI-generated code suggestions, aimed at improving developers' understanding and trust.", "motivation": "To improve developers' critical evaluation of AI-generated code completions by providing transparency in the decision-making process of code assistants.", "method": "The authors propose a two-level interface framework, CopilotLens, that offers explanations of the AI agent's thought process alongside high-level code changes and contextual influences from the codebase.", "result": "The introduction of CopilotLens enhances the explainability of AI-powered code assistants, potentially leading to improved comprehension and trust among developers.", "conclusion": "The paper outlines the design and purpose of CopilotLens, setting the stage for further evaluation of its impact on developer understanding and trust in AI systems.", "key_contributions": ["Introduction of the CopilotLens framework for explainable code completion", "The framework's interactive two-level interface for transparency in AI suggestions", "Insights into improving trust and understanding in AI-generated code outputs"], "limitations": "Future work is needed to evaluate the effectiveness of the framework in real-world coding scenarios.", "keywords": ["AI code assistants", "explainability", "HCI", "trust in AI", "code completion"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.09654", "pdf": "https://arxiv.org/pdf/2508.09654.pdf", "abs": "https://arxiv.org/abs/2508.09654", "title": "Improving Diversity in Language Models: When Temperature Fails, Change the Loss", "authors": ["Alexandre Verine", "Florian Le Bronnec", "Kunhao Zheng", "Alexandre Allauzen", "Yann Chevaleyre", "Benjamin Negrevergne"], "categories": ["cs.CL", "cs.LG"], "comment": "Forty-Second International Conference on Machine Learning, ICML2025", "summary": "Increasing diversity in language models is a challenging yet essential\nobjective. A common approach is to raise the decoding temperature. In this\nwork, we investigate this approach through a simplistic yet common case to\nprovide insights into why decreasing temperature can improve quality\n(Precision), while increasing it often fails to boost coverage (Recall). Our\nanalysis reveals that for a model to be effectively tunable through temperature\nadjustments, it must be trained toward coverage. To address this, we propose\nrethinking loss functions in language models by leveraging the Precision-Recall\nframework. Our results demonstrate that this approach achieves a substantially\nbetter trade-off between Precision and Recall than merely combining negative\nlog-likelihood training with temperature scaling. These findings offer a\npathway toward more versatile and robust language modeling techniques.", "AI": {"tldr": "This paper explores the effects of temperature adjustments on language model performance, revealing that decreasing temperature can improve Precision, while increasing it often fails to enhance Recall. It proposes a novel approach to loss functions that improves the Precision-Recall trade-off.", "motivation": "The need to increase diversity in language models and understand the effects of temperature on performance metrics like Precision and Recall.", "method": "The authors analyze temperature adjustments in language models and propose rethinking loss functions using the Precision-Recall framework to achieve better coverage.", "result": "The proposed approach demonstrates a significantly improved trade-off between Precision and Recall compared to traditional methods combining negative log-likelihood training with temperature scaling.", "conclusion": "Revising loss functions in language models can lead to more effective tuning through temperature adjustments, enhancing both Precision and Recall.", "key_contributions": ["Proposed a method to rethink loss functions in language models using the Precision-Recall framework.", "Demonstrated that decreasing temperature can improve model quality.", "Achieved a better balance between Precision and Recall compared to conventional approaches."], "limitations": "", "keywords": ["language models", "temperature adjustments", "Precision-Recall framework"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.09662", "pdf": "https://arxiv.org/pdf/2508.09662.pdf", "abs": "https://arxiv.org/abs/2508.09662", "title": "EffiEval: Efficient and Generalizable Model Evaluation via Capability Coverage Maximization", "authors": ["Yaoning Wang", "Jiahao Ying", "Yixin Cao", "Yubo Ma", "Yugang Jiang"], "categories": ["cs.CL"], "comment": null, "summary": "The rapid advancement of large language models (LLMs) and the development of\nincreasingly large and diverse evaluation benchmarks have introduced\nsubstantial computational challenges for model assessment. In this paper, we\npresent EffiEval, a training-free approach for efficient benchmarking that\neffectively addresses data redundancy while maintaining high evaluation\nreliability. Our method is specifically designed to meet three key criteria for\nhigh-quality evaluation: representativeness, by ensuring comprehensive coverage\nof model capabilities; fairness, by remaining independent of model performance\nduring sample selection to avoid bias; and generalizability, by enabling\nflexible transfer across datasets and model families without reliance on\nlarge-scale evaluation data. Unlike traditional methods that rely on absolute\nperformance or require extensive evaluation data, our approach adaptively\nselects high-quality representative subsets based on the Model Utility Index\n(MUI). Extensive experiments on multiple public benchmarks and diverse LLMs\ndemonstrate that EffiEval achieves strong ranking consistency with full-dataset\nevaluation using only a small fraction of the original data. Furthermore, our\nmethod is flexible and scalable in size, allowing users to balance evaluation\nefficiency and representativeness according to specific needs. Overall,\nEffiEval provides a practical and generalizable solution for reliable, fair,\nand efficient evaluation in the era of LLMs.", "AI": {"tldr": "EffiEval is a training-free methodology for efficient benchmarking of large language models (LLMs) that addresses data redundancy while ensuring high evaluation reliability, representativeness, and fairness.", "motivation": "The paper addresses the computational challenges posed by the rapid advancement of LLMs and the need for more efficient evaluation benchmarks.", "method": "EffiEval adaptively selects representative subsets based on the Model Utility Index (MUI), without relying on large-scale evaluation data.", "result": "EffiEval maintains strong ranking consistency with full-dataset evaluations using only a small fraction of the original data, demonstrating its effectiveness.", "conclusion": "EffiEval offers a practical and generalizable solution for efficient model evaluation in the context of LLMs, allowing flexibility between evaluation efficiency and representativeness.", "key_contributions": ["Training-free approach for efficient benchmarking", "High evaluation reliability and fairness", "Flexibility to adapt across datasets without extensive data"], "limitations": "", "keywords": ["evaluation", "large language models", "benchmarking", "efficiency", "fairness"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2508.09666", "pdf": "https://arxiv.org/pdf/2508.09666.pdf", "abs": "https://arxiv.org/abs/2508.09666", "title": "Slow Tuning and Low-Entropy Masking for Safe Chain-of-Thought Distillation", "authors": ["Ziyang Ma", "Qingyue Yuan", "Linhai Zhang", "Deyu Zhou"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "Previous chain-of-thought (CoT) distillation methods primarily focused on\nenhancing the reasoning capabilities of Small Language Models (SLMs) by\nutilizing high-quality rationales generated by powerful Large Language Models\n(LLMs, e.g., GPT-4). However, few works have noted the negative effects on SLM\nsafety brought by the training, which are revealed in this study. Although\nthere are works on safety alignment that fine-tune language models or\nmanipulate model weights to defend against harmful inputs, they require extra\ncomputation or annotated data, and probably impact the reasoning ability of\nSLMs. In this paper, we investigate how to maintain the safety of SLMs during\nthe CoT distillation process. Specifically, we propose a safe distillation\nmethod, Slow Tuning and Low-Entropy Masking Distillation (SLowED), containing\ntwo modules: Slow Tuning and Low-Entropy Masking. Slow Tuning scales down the\nmagnitude of model weight changes to optimize the model weights in the\nneighboring space near the initial weight distribution. Low-Entropy Masking\nmasks low-entropy tokens, which are regarded as unnecessary learning targets,\nto exclude them from fine-tuning. Experiments on three SLMs (Qwen2.5-1.5B,\nLlama-3.2-1B, BLOOM-1.1B) across reasoning benchmarks (BBH, BB-Sub, ARC,\nAGIEval) and safety evaluation (AdvBench) show that SLowED retains the safety\nof SLMs and comparably improves their reasoning capability compared to existing\ndistillation methods. Furthermore, our ablation study presents the\neffectiveness of Slow Tuning and Low-Entropy Masking, with the former\nmaintaining the model's safety in the early stage and the latter prolonging the\nsafe training epochs.", "AI": {"tldr": "This paper presents a safe distillation method, SLowED, for Small Language Models to enhance reasoning without compromising safety.", "motivation": "To address the negative safety effects that arise during the chain-of-thought distillation process for Small Language Models, which are often overlooked.", "method": "The proposed method, SLowED, consists of two modules: Slow Tuning, which minimizes model weight changes, and Low-Entropy Masking, which avoids low-entropy tokens in the fine-tuning process.", "result": "Experiments demonstrated that SLowED maintains the safety of SLMs while improving their reasoning capabilities compared to existing methods, validated across various benchmarks.", "conclusion": "SLowED effectively retains SLM safety and enhances reasoning abilities, with individual contributions of each module supporting safe training practices.", "key_contributions": ["Introduction of SLowED as a safe distillation method", "Demonstration of improved reasoning while preserving safety", "Validation through comprehensive experimental benchmarks"], "limitations": "", "keywords": ["safe distillation", "Small Language Models", "reasoning capabilities", "safety alignment", "Low-Entropy Masking"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.09713", "pdf": "https://arxiv.org/pdf/2508.09713.pdf", "abs": "https://arxiv.org/abs/2508.09713", "title": "Evaluating the Role of Large Language Models in Legal Practice in India", "authors": ["Rahul Hemrajani"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The integration of Artificial Intelligence(AI) into the legal profession\nraises significant questions about the capacity of Large Language Models(LLM)\nto perform key legal tasks. In this paper, I empirically evaluate how well\nLLMs, such as GPT, Claude, and Llama, perform key legal tasks in the Indian\ncontext, including issue spotting, legal drafting, advice, research, and\nreasoning. Through a survey experiment, I compare outputs from LLMs with those\nof a junior lawyer, with advanced law students rating the work on helpfulness,\naccuracy, and comprehensiveness. LLMs excel in drafting and issue spotting,\noften matching or surpassing human work. However, they struggle with\nspecialised legal research, frequently generating hallucinations, factually\nincorrect or fabricated outputs. I conclude that while LLMs can augment certain\nlegal tasks, human expertise remains essential for nuanced reasoning and the\nprecise application of law.", "AI": {"tldr": "This paper evaluates the performance of Large Language Models (LLMs) in executing key legal tasks in India, finding that while they excel at drafting and issue spotting, they struggle with specialized legal research.", "motivation": "To examine the effectiveness of LLMs in legal tasks and assess their potential to assist legal professionals in India.", "method": "A survey experiment comparing outputs of LLMs with those of a junior lawyer, rated by advanced law students on helpfulness, accuracy, and comprehensiveness.", "result": "LLMs excel in legal drafting and issue spotting but frequently produce hallucinations and inaccuracies in specialized legal research.", "conclusion": "LLMs can support certain legal tasks but cannot replace the essential human expertise required for nuanced legal reasoning and application of the law.", "key_contributions": ["Empirical evaluation of LLMs in the Indian legal context", "Comparative analysis of LLMs and junior lawyers' outputs", "Insights into the limitations of LLM capabilities in specialized legal research."], "limitations": "LLMs generate factually incorrect or fabricated outputs and are less reliable in specialized legal contexts.", "keywords": ["Artificial Intelligence", "Large Language Models", "Legal profession", "Human expertise", "Legal research"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2508.09716", "pdf": "https://arxiv.org/pdf/2508.09716.pdf", "abs": "https://arxiv.org/abs/2508.09716", "title": "The Perils of Chart Deception: How Misleading Visualizations Affect Vision-Language Models", "authors": ["Ridwan Mahbub", "Mohammed Saidul Islam", "Md Tahmid Rahman Laskar", "Mizanur Rahman", "Mir Tafseer Nayeem", "Enamul Hoque"], "categories": ["cs.CL"], "comment": "Accepted to IEEE VIS 2025", "summary": "Information visualizations are powerful tools that help users quickly\nidentify patterns, trends, and outliers, facilitating informed decision-making.\nHowever, when visualizations incorporate deceptive design elements-such as\ntruncated or inverted axes, unjustified 3D effects, or violations of best\npractices-they can mislead viewers and distort understanding, spreading\nmisinformation. While some deceptive tactics are obvious, others subtly\nmanipulate perception while maintaining a facade of legitimacy. As\nVision-Language Models (VLMs) are increasingly used to interpret\nvisualizations, especially by non-expert users, it is critical to understand\nhow susceptible these models are to deceptive visual designs. In this study, we\nconduct an in-depth evaluation of VLMs' ability to interpret misleading\nvisualizations. By analyzing over 16,000 responses from ten different models\nacross eight distinct types of misleading chart designs, we demonstrate that\nmost VLMs are deceived by them. This leads to altered interpretations of\ncharts, despite the underlying data remaining the same. Our findings highlight\nthe need for robust safeguards in VLMs against visual misinformation.", "AI": {"tldr": "This study investigates the susceptibility of Vision-Language Models (VLMs) to deceptive visualizations, finding that most VLMs misinterpret misleading chart designs, which can result in altered understandings despite consistent data.", "motivation": "As VLMs are increasingly used by non-experts to interpret visualizations, understanding their susceptibility to deceptive visual designs is crucial to prevent misinformation.", "method": "The study evaluates over 16,000 responses from 10 different VLMs across 8 types of misleading chart designs to assess their ability to interpret visualizations accurately.", "result": "Most VLMs were found to be deceived by misleading visualizations, leading to incorrect interpretations of charts while the underlying data remained unchanged.", "conclusion": "The findings emphasize the urgent need for safeguards in VLMs to combat visual misinformation and improve accurate data interpretation.", "key_contributions": ["Evaluation of VLMs' interpretation of over 16,000 responses to deceptive visualizations", "Identification of multiple types of misleading chart designs that affect VLM performance", "Highlighting the necessity for robust mechanisms to protect against visual misinformation"], "limitations": "Limited to specific types of misleading chart designs and VLMs analyzed; results may vary with different datasets or models.", "keywords": ["Vision-Language Models", "deceptive visualizations", "information visualization", "data interpretation", "misinformation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.09726", "pdf": "https://arxiv.org/pdf/2508.09726.pdf", "abs": "https://arxiv.org/abs/2508.09726", "title": "Sample More to Think Less: Group Filtered Policy Optimization for Concise Reasoning", "authors": ["Vaishnavi Shrivastava", "Ahmed Awadallah", "Vidhisha Balachandran", "Shivam Garg", "Harkirat Behl", "Dimitris Papailiopoulos"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models trained with reinforcement learning with verifiable\nrewards tend to trade accuracy for length--inflating response lengths to\nachieve gains in accuracy. While longer answers may be warranted for harder\nproblems, many tokens are merely \"filler\": repetitive, verbose text that makes\nno real progress. We introduce GFPO (Group Filtered Policy Optimization), which\ncurbs this length explosion by sampling larger groups per problem during\ntraining and filtering responses to train on based on two key metrics: (1)\nresponse length and (2) token efficiency: reward per token ratio. By sampling\nmore at training time, we teach models to think less at inference time. On the\nPhi-4-reasoning model, GFPO cuts GRPO's length inflation by 46-71% across\nchallenging STEM and coding benchmarks (AIME 24/25, GPQA, Omni-MATH,\nLiveCodeBench) while maintaining accuracy. Optimizing for reward per token\nfurther increases reductions in length inflation to 71-85%. We also propose\nAdaptive Difficulty GFPO, which dynamically allocates more training resources\nto harder problems based on real-time difficulty estimates, improving the\nbalance between computational efficiency and accuracy especially on difficult\nquestions. GFPO demonstrates that increased training-time compute directly\ntranslates to reduced test-time compute--a simple yet effective trade-off for\nefficient reasoning.", "AI": {"tldr": "The paper introduces GFPO, a method to reduce response length in large language models by optimizing training based on response length and token efficiency, leading to significant reductions in output verbosity while maintaining accuracy.", "motivation": "To address the problem of excessive verbosity in responses generated by large language models trained with reinforcement learning, which can negatively impact clarity and utility.", "method": "GFPO (Group Filtered Policy Optimization) involves sampling larger groups during training and filtering responses based on response length and reward per token ratio.", "result": "GFPO reduces length inflation by 46-71% on various STEM and coding benchmarks while maintaining accuracy, with further reductions of 71-85% when optimizing for reward per token.", "conclusion": "Increasing training-time compute leads to reduced test-time compute, providing a trade-off for efficient reasoning.", "key_contributions": ["Introduction of GFPO to reduce verbosity in language model responses.", "Demonstrated significant reductions in response length while maintaining accuracy across benchmarks.", "Proposed Adaptive Difficulty GFPO for improved efficiency in training on difficult problems."], "limitations": "", "keywords": ["Language Models", "Reinforcement Learning", "Token Efficiency"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.09755", "pdf": "https://arxiv.org/pdf/2508.09755.pdf", "abs": "https://arxiv.org/abs/2508.09755", "title": "Transforming Questions and Documents for Semantically Aligned Retrieval-Augmented Generation", "authors": ["Seokgi Lee"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce a novel retrieval-augmented generation (RAG) framework tailored\nfor multihop question answering. First, our system uses large language model\n(LLM) to decompose complex multihop questions into a sequence of single-hop\nsubquestions that guide document retrieval. This decomposition mitigates the\nambiguity inherent in multi-hop queries by clearly targeting distinct knowledge\nfacets. Second, instead of embedding raw or chunked documents directly, we\ngenerate answerable questions from each document chunk using Qwen3-8B, embed\nthese generated questions, and retrieve relevant chunks via question-question\nembedding similarity. During inference, the retrieved chunks are then fed along\nwith the original question into the RAG pipeline. We evaluate on three multihop\nquestion datasets (MuSiQue, 2WikiMultiHopQa, HotpotQA) from LongBench. Our\nmethod improves RAG performacne compared to baseline systems. Our contributions\nhighlight the benefits of using answerable-question embeddings for RAG, and the\neffectiveness of LLM-based query decomposition for multihop scenarios.", "AI": {"tldr": "This paper presents a novel retrieval-augmented generation (RAG) framework designed for improved multihop question answering by decomposing complex questions and using question embeddings for document retrieval.", "motivation": "The need for enhanced performance in multihop question answering systems due to the challenges posed by ambiguity in complex queries.", "method": "The framework utilizes large language models to break down complex questions into single-hop subquestions, generates answerable questions from document chunks for embedding, and retrieves relevant information based on question-question similarity.", "result": "The proposed method demonstrated improved performance on three multihop question datasets (MuSiQue, 2WikiMultiHopQa, HotpotQA) compared to baseline systems.", "conclusion": "Utilizing answerable-question embeddings and LLM-based query decomposition significantly enhances the retrieval-augmented generation approach for multihop question answering.", "key_contributions": ["Introduction of a novel RAG framework for multihop QA", "Effective use of LLM for query decomposition", "The implementation of question-question embedding for document retrieval"], "limitations": "", "keywords": ["RAG", "multihop question answering", "LLM", "query decomposition", "question embeddings"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.09759", "pdf": "https://arxiv.org/pdf/2508.09759.pdf", "abs": "https://arxiv.org/abs/2508.09759", "title": "Echoes of Agreement: Argument Driven Opinion Shifts in Large Language Models", "authors": ["Avneet Kaur"], "categories": ["cs.CL"], "comment": null, "summary": "There have been numerous studies evaluating bias of LLMs towards political\ntopics. However, how positions towards these topics in model outputs are highly\nsensitive to the prompt. What happens when the prompt itself is suggestive of\ncertain arguments towards those positions remains underexplored. This is\ncrucial for understanding how robust these bias evaluations are and for\nunderstanding model behaviour, as these models frequently interact with\nopinionated text. To that end, we conduct experiments for political bias\nevaluation in presence of supporting and refuting arguments. Our experiments\nshow that such arguments substantially alter model responses towards the\ndirection of the provided argument in both single-turn and multi-turn settings.\nMoreover, we find that the strength of these arguments influences the\ndirectional agreement rate of model responses. These effects point to a\nsycophantic tendency in LLMs adapting their stance to align with the presented\narguments which has downstream implications for measuring political bias and\ndeveloping effective mitigation strategies.", "AI": {"tldr": "The paper explores how suggestive prompts with supporting or refuting arguments impact the political bias of large language models (LLMs), revealing a tendency of models to adapt their responses to align with the presented arguments.", "motivation": "To understand the sensitivity of LLM outputs to suggestive prompts in political bias evaluation, highlighting the robustness of bias assessments and model behavior.", "method": "Conducted experiments evaluating model responses to prompts with supporting and refuting arguments under both single-turn and multi-turn interactions.", "result": "Model responses are substantially influenced by the strength and direction of the provided arguments, demonstrating a sycophantic tendency to align with the inputs.", "conclusion": "These findings have significant implications for measuring political bias in LLMs and developing strategies to mitigate such biases.", "key_contributions": ["Demonstrated the impact of argument strength on model responses in political bias evaluation.", "Identified a sycophantic tendency in LLMs to adjust stance based on user-given arguments.", "Provided insights into the implications for effective mitigation of bias in LLMs."], "limitations": "The paper may not explore all aspects of bias across different political contexts or model architectures.", "keywords": ["political bias", "large language models", "prompt sensitivity"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.09767", "pdf": "https://arxiv.org/pdf/2508.09767.pdf", "abs": "https://arxiv.org/abs/2508.09767", "title": "UtterTune: LoRA-Based Target-Language Pronunciation Edit and Control in Multilingual Text-to-Speech", "authors": ["Shuhei Kato"], "categories": ["cs.CL", "eess.AS"], "comment": null, "summary": "We propose UtterTune, a lightweight adaptation method that fine-tunes a\nmultilingual text-to-speech (TTS) system based on a large language model (LLM)\narchitecture, designed to enhance the controllability of pronunciation in a\ntarget language while preserving performance in others. While LLM architectures\nhave enabled TTS models to achieve remarkable naturalness, accurately modeling\ngrapheme-to-phoneme (G2P) mapping and prosody remains challenging, especially\nwhen the model omits an explicit G2P module and directly processes minimally\nencoded text (e.g., byte-pair encoding). UtterTune leverages low-rank\nadaptation to enable the control of segmental pronunciation and pitch accent at\nthe phoneme level for Japanese speech, the target language in this paper, while\nmaintaining naturalness and speaker similarity in a zero-shot setting.\nObjective and subjective evaluations confirm its effectiveness.", "AI": {"tldr": "UtterTune is a lightweight method to enhance pronunciation controllability in multilingual text-to-speech using LLMs while preserving performance in other languages.", "motivation": "To improve the controllability of pronunciation in target languages within multilingual TTS systems that utilize LLM architectures.", "method": "UtterTune employs low-rank adaptation to fine-tune a TTS system at the phoneme level, specifically focusing on segmental pronunciation and pitch accent for Japanese speech.", "result": "The evaluations demonstrate that UtterTune effectively maintains naturalness and speaker similarity, even in a zero-shot setting.", "conclusion": "UtterTune successfully enhances the controllability of Japanese TTS without compromising performance in multilingual contexts.", "key_contributions": ["Proposes a lightweight adaptation method for multilingual TTS systems.", "Enhances pronunciation control while retaining naturalness and speaker similarity.", "Validates effectiveness through both objective and subjective evaluations."], "limitations": "", "keywords": ["Text-to-Speech", "Multilingual", "Pronunciation Control", "Low-Rank Adaptation", "Natural Language Processing"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2508.09776", "pdf": "https://arxiv.org/pdf/2508.09776.pdf", "abs": "https://arxiv.org/abs/2508.09776", "title": "Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study", "authors": ["Mahdi Dhaini", "Juraj Vladika", "Ege Erdogan", "Zineb Attaoui", "Gjergji Kasneci"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to the 34th International Conference on Artificial Neural\n  Networks (ICANN 2025)", "summary": "In the rapidly evolving field of Explainable Natural Language Processing\n(NLP), textual explanations, i.e., human-like rationales, are pivotal for\nexplaining model predictions and enriching datasets with interpretable labels.\nTraditional approaches rely on human annotation, which is costly,\nlabor-intensive, and impedes scalability. In this work, we present an automated\nframework that leverages multiple state-of-the-art large language models (LLMs)\nto generate high-quality textual explanations. We rigorously assess the quality\nof these LLM-generated explanations using a comprehensive suite of Natural\nLanguage Generation (NLG) metrics. Furthermore, we investigate the downstream\nimpact of these explanations on the performance of pre-trained language models\n(PLMs) and LLMs across natural language inference tasks on two diverse\nbenchmark datasets. Our experiments demonstrate that automated explanations\nexhibit highly competitive effectiveness compared to human-annotated\nexplanations in improving model performance. Our findings underscore a\npromising avenue for scalable, automated LLM-based textual explanation\ngeneration for extending NLP datasets and enhancing model performance.", "AI": {"tldr": "The paper presents an automated framework using LLMs to generate high-quality textual explanations for NLP, assessing their impact on model performance.", "motivation": "To address the limitations of traditional human annotation in generating textual explanations for NLP models, which is costly and limits scalability.", "method": "An automated framework leveraging multiple state-of-the-art LLMs to produce explanations and a comprehensive assessment of their quality using NLG metrics.", "result": "Automated explanations significantly improve the performance of PLMs and LLMs on natural language inference tasks, demonstrating effectiveness comparable to human-annotated explanations.", "conclusion": "The study highlights the potential for scalable LLM-based explanations to enhance NLP datasets and model performance.", "key_contributions": ["Introduction of an automated framework for generating textual explanations using LLMs.", "Demonstration of the effectiveness of LLM-generated explanations compared to human annotations.", "Assessment of explanations using a comprehensive suite of NLG metrics."], "limitations": "", "keywords": ["Explainable NLP", "Natural Language Generation", "Large Language Models"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.09786", "pdf": "https://arxiv.org/pdf/2508.09786.pdf", "abs": "https://arxiv.org/abs/2508.09786", "title": "Adoption of Explainable Natural Language Processing: Perspectives from Industry and Academia on Practices and Challenges", "authors": ["Mahdi Dhaini", "Tobias Müller", "Roksoliana Rabets", "Gjergji Kasneci"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "Accepted to AAAI/ACM Conference on AI, Ethics, and Society (AIES\n  2025)", "summary": "The field of explainable natural language processing (NLP) has grown rapidly\nin recent years. The growing opacity of complex models calls for transparency\nand explanations of their decisions, which is crucial to understand their\nreasoning and facilitate deployment, especially in high-stakes environments.\nDespite increasing attention given to explainable NLP, practitioners'\nperspectives regarding its practical adoption and effectiveness remain\nunderexplored. This paper addresses this research gap by investigating\npractitioners' experiences with explainability methods, specifically focusing\non their motivations for adopting such methods, the techniques employed,\nsatisfaction levels, and the practical challenges encountered in real-world NLP\napplications. Through a qualitative interview-based study with industry\npractitioners and complementary interviews with academic researchers, we\nsystematically analyze and compare their perspectives. Our findings reveal\nconceptual gaps, low satisfaction with current explainability methods, and\nhighlight evaluation challenges. Our findings emphasize the need for clear\ndefinitions and user-centric frameworks for better adoption of explainable NLP\nin practice.", "AI": {"tldr": "This paper investigates practitioners' perspectives on explainable NLP, analyzing motivations, techniques, satisfaction, and challenges in adopting explanation methods within real-world applications.", "motivation": "To address the gap in understanding the practical adoption and effectiveness of explainable NLP from the perspective of industry practitioners.", "method": "A qualitative interview-based study involving industry practitioners and academic researchers to analyze and compare their experiences and perspectives on explainability methods.", "result": "The study reveals conceptual gaps, low satisfaction with current explainability methods, and highlights evaluation challenges in explainable NLP applications.", "conclusion": "There is a critical need for clear definitions and user-centric frameworks to enhance the adoption of explainable NLP in practice.", "key_contributions": ["Investigates industry practitioners' motivations and experiences with explainability methods in NLP.", "Analyzes satisfaction levels and practical challenges faced in real-world applications.", "Highlights the need for improved frameworks and definitions for explainable NLP."], "limitations": "Limited to qualitative insights; findings may not generalize across all sectors in NLP.", "keywords": ["explainable NLP", "practitioner perspectives", "explainability methods", "qualitative study", "AI ethics"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.09804", "pdf": "https://arxiv.org/pdf/2508.09804.pdf", "abs": "https://arxiv.org/abs/2508.09804", "title": "BigCharts-R1: Enhanced Chart Reasoning with Visual Reinforcement Finetuning", "authors": ["Ahmed Masry", "Abhay Puri", "Masoud Hashemi", "Juan A. Rodriguez", "Megh Thakkar", "Khyati Mahajan", "Vikas Yadav", "Sathwik Tejaswi Madhusudhan", "Alexandre Piché", "Dzmitry Bahdanau", "Christopher Pal", "David Vazquez", "Enamul Hoque", "Perouz Taslakian", "Sai Rajeswar", "Spandana Gella"], "categories": ["cs.CL"], "comment": null, "summary": "Charts are essential to data analysis, transforming raw data into clear\nvisual representations that support human decision-making. Although current\nvision-language models (VLMs) have made significant progress, they continue to\nstruggle with chart comprehension due to training on datasets that lack\ndiversity and real-world authenticity, or on automatically extracted underlying\ndata tables of charts, which can contain numerous estimation errors.\nFurthermore, existing models only rely on supervised fine-tuning using these\nlow-quality datasets, severely limiting their effectiveness. To address these\nissues, we first propose BigCharts, a dataset creation pipeline that generates\nvisually diverse chart images by conditioning the rendering process on\nreal-world charts sourced from multiple online platforms. Unlike purely\nsynthetic datasets, BigCharts incorporates real-world data, ensuring\nauthenticity and visual diversity, while still retaining accurate underlying\ndata due to our proposed replotting process. Additionally, we introduce a\ncomprehensive training framework that integrates supervised fine-tuning with\nGroup Relative Policy Optimization (GRPO)-based reinforcement learning. By\nintroducing novel reward signals specifically designed for chart reasoning, our\napproach enhances model robustness and generalization across diverse chart\nstyles and domains, resulting in a state-of-the-art chart reasoning model,\nBigCharts-R1. Extensive experiments demonstrate that our models surpass\nexisting methods on multiple chart question-answering benchmarks compared to\neven larger open-source and closed-source models.", "AI": {"tldr": "The paper introduces BigCharts, a dataset and training framework aimed at improving chart comprehension in vision-language models, addressing issues of low-quality datasets and enhancing model performance through reinforcement learning.", "motivation": "Current vision-language models struggle with chart comprehension due to inadequate training datasets that lack real-world diversity and contain estimation errors.", "method": "The authors propose BigCharts, a dataset creation pipeline that generates diverse chart images from real-world data, and a training framework that combines supervised fine-tuning with GRPO-based reinforcement learning and novel reward signals.", "result": "BigCharts-R1, the resulting model, outperforms existing methods on multiple chart question-answering benchmarks, demonstrating enhanced robustness and generalization.", "conclusion": "The proposed framework and dataset significantly improve chart reasoning capabilities in vision-language models and set a new state-of-the-art performance.", "key_contributions": ["Introduction of BigCharts dataset for visually diverse and authentic chart images", "Development of a training framework combining supervised fine-tuning and reinforcement learning", "State-of-the-art performance on chart question-answering benchmarks"], "limitations": "", "keywords": ["Chart Comprehension", "Vision-Language Models", "Dataset Creation", "Reinforcement Learning", "Chart Reasoning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.09809", "pdf": "https://arxiv.org/pdf/2508.09809.pdf", "abs": "https://arxiv.org/abs/2508.09809", "title": "A Comprehensive Survey of Datasets for Clinical Mental Health AI Systems", "authors": ["Aishik Mandal", "Prottay Kumar Adhikary", "Hiba Arnaout", "Iryna Gurevych", "Tanmoy Chakraborty"], "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 3 figures", "summary": "Mental health disorders are rising worldwide. However, the availability of\ntrained clinicians has not scaled proportionally, leaving many people without\nadequate or timely support. To bridge this gap, recent studies have shown the\npromise of Artificial Intelligence (AI) to assist mental health diagnosis,\nmonitoring, and intervention. However, the development of efficient, reliable,\nand ethical AI to assist clinicians is heavily dependent on high-quality\nclinical training datasets. Despite growing interest in data curation for\ntraining clinical AI assistants, existing datasets largely remain scattered,\nunder-documented, and often inaccessible, hindering the reproducibility,\ncomparability, and generalizability of AI models developed for clinical mental\nhealth care. In this paper, we present the first comprehensive survey of\nclinical mental health datasets relevant to the training and development of\nAI-powered clinical assistants. We categorize these datasets by mental\ndisorders (e.g., depression, schizophrenia), data modalities (e.g., text,\nspeech, physiological signals), task types (e.g., diagnosis prediction, symptom\nseverity estimation, intervention generation), accessibility (public,\nrestricted or private), and sociocultural context (e.g., language and cultural\nbackground). Along with these, we also investigate synthetic clinical mental\nhealth datasets. Our survey identifies critical gaps such as a lack of\nlongitudinal data, limited cultural and linguistic representation, inconsistent\ncollection and annotation standards, and a lack of modalities in synthetic\ndata. We conclude by outlining key challenges in curating and standardizing\nfuture datasets and provide actionable recommendations to facilitate the\ndevelopment of more robust, generalizable, and equitable mental health AI\nsystems.", "AI": {"tldr": "A survey of clinical mental health datasets for AI development, highlighting gaps and providing recommendations for better data curation.", "motivation": "To address the rising mental health disorders and the insufficient availability of trained clinicians, this paper investigates high-quality clinical training datasets essential for AI in mental health.", "method": "The paper conducts a comprehensive survey categorizing mental health datasets by disorders, data modalities, tasks, accessibility, and sociocultural context, including an exploration of synthetic datasets.", "result": "The survey identifies critical gaps in mental health datasets, including a lack of longitudinal data, cultural representation, and consistent standards.", "conclusion": "The paper concludes with challenges in dataset curation and offers actionable recommendations for developing better mental health AI systems.", "key_contributions": ["First comprehensive survey of clinical mental health datasets for AI development", "Identification of gaps in existing datasets important for mental health AI", "Recommendations for improving dataset curation and standardization"], "limitations": "Challenges in dataset accessibility and variability in quality may affect findings.", "keywords": ["Mental Health", "AI", "Dataset Curation", "Clinical Assistants", "Synthetic Data"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.09834", "pdf": "https://arxiv.org/pdf/2508.09834.pdf", "abs": "https://arxiv.org/abs/2508.09834", "title": "Speed Always Wins: A Survey on Efficient Architectures for Large Language Models", "authors": ["Weigao Sun", "Jiaxi Hu", "Yucheng Zhou", "Jusen Du", "Disen Lan", "Kexin Wang", "Tong Zhu", "Xiaoye Qu", "Yu Zhang", "Xiaoyu Mo", "Daizong Liu", "Yuxuan Liang", "Wenliang Chen", "Guoqi Li", "Yu Cheng"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Survey, 82 pages, GitHub:\n  https://github.com/weigao266/Awesome-Efficient-Arch", "summary": "Large Language Models (LLMs) have delivered impressive results in language\nunderstanding, generation, reasoning, and pushes the ability boundary of\nmultimodal models. Transformer models, as the foundation of modern LLMs, offer\na strong baseline with excellent scaling properties. However, the traditional\ntransformer architecture requires substantial computations and poses\nsignificant obstacles for large-scale training and practical deployment. In\nthis survey, we offer a systematic examination of innovative LLM architectures\nthat address the inherent limitations of transformers and boost the efficiency.\nStarting from language modeling, this survey covers the background and\ntechnical details of linear and sparse sequence modeling methods, efficient\nfull attention variants, sparse mixture-of-experts, hybrid model architectures\nincorporating the above techniques, and emerging diffusion LLMs. Additionally,\nwe discuss applications of these techniques to other modalities and consider\ntheir wider implications for developing scalable, resource-aware foundation\nmodels. By grouping recent studies into the above category, this survey\npresents a blueprint of modern efficient LLM architectures, and we hope this\ncould help motivate future research toward more efficient, versatile AI\nsystems.", "AI": {"tldr": "This survey reviews novel architectures for large language models (LLMs) aimed at improving efficiency while overcoming traditional transformer limitations.", "motivation": "To systematically explore innovative LLM architectures that enhance efficiency and scalability, addressing shortcomings of traditional transformers in training and deployment.", "method": "The survey examines various techniques, including linear and sparse modeling methods, efficient full attention variants, sparse mixture-of-experts, and hybrid architectures, alongside their applications across modalities.", "result": "The paper groups recent studies into a comprehensive framework for efficient LLM architectures, offering insights into improvements and future research directions.", "conclusion": "The findings present a strategic reference for developing scalable and resource-efficient AI systems moving forward.", "key_contributions": ["Comprehensive examination of innovative LLM architectures", "Grouping of recent studies into a coherent framework", "Discussion of applications and implications for scalable AI systems"], "limitations": "", "keywords": ["Large Language Models", "Transformer architecture", "Efficiency techniques", "Multimodal models", "Survey"], "importance_score": 9, "read_time_minutes": 30}}
{"id": "2508.09848", "pdf": "https://arxiv.org/pdf/2508.09848.pdf", "abs": "https://arxiv.org/abs/2508.09848", "title": "PRELUDE: A Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts", "authors": ["Mo Yu", "Tsz Ting Chung", "Chulun Zhou", "Tong Li", "Rui Lu", "Jiangnan Li", "Liyan Xu", "Haoshu Lu", "Ning Zhang", "Jing Li", "Jie Zhou"], "categories": ["cs.CL", "cs.AI"], "comment": "First 7 authors contributed equally. Project page:\n  https://gorov.github.io/prelude", "summary": "We introduce PRELUDE, a benchmark for evaluating long-context understanding\nthrough the task of determining whether a character's prequel story is\nconsistent with the canonical narrative of the original book. Our task poses a\nstronger demand for global comprehension and deep reasoning than existing\nbenchmarks -- as the prequels are not part of the original story, assessing\ntheir plausibility typically requires searching and integrating information\nthat is only indirectly related. Empirically, 88% of instances require evidence\nfrom multiple parts of the narrative. Experimental results highlight the\nchallenge of our task: in-context learning, RAG and in-domain training with\nstate-of-the-art LLMs, and commercial DeepResearch services, lag behind humans\nby >15%. A further human study reveals that models often produce correct\nanswers with flawed reasoning, leading to an over 30% gap in reasoning accuracy\ncompared to humans. These findings underscore the substantial room for\nimprovement in long-context understanding and reasoning.", "AI": {"tldr": "PRELUDE is a benchmark for evaluating long-context understanding by testing the plausibility of characters' prequel stories compared to original narratives.", "motivation": "To assess how well models understand and reason about long-context narratives by evaluating their ability to determine consistency between a prequel story and the canonical narrative.", "method": "The benchmark involves a task where models must examine prequel narratives and ascertain their plausibility against the original stories, requiring deep comprehension and reasoning.", "result": "Experiments show that current LLMs and commercial services significantly underperform compared to human reasoning, with more than 15% lag and over 30% gap in reasoning accuracy.", "conclusion": "There is a critical need for improvement in AI's long-context understanding and reasoning capabilities as highlighted by the performance gap between humans and models.", "key_contributions": ["Introduction of a new benchmark for long-context understanding", "Empirical results showing the limitations of current LLM models in reasoning tasks", "Identification of a significant reasoning accuracy gap between models and humans."], "limitations": "Current models show correct answers but with flawed reasoning, indicating a need for improved reasoning mechanisms in AI.", "keywords": ["long-context understanding", "prequel narrative evaluation", "machine learning", "reasoning accuracy"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.09865", "pdf": "https://arxiv.org/pdf/2508.09865.pdf", "abs": "https://arxiv.org/abs/2508.09865", "title": "Assessing the Feasibility of Lightweight Whisper Models for Low-Resource Urdu Transcription", "authors": ["Abdul Rehman Antall", "Naveed Akhtar"], "categories": ["cs.CL"], "comment": "8 pages, 3 figures, 1 table, including references and appendix", "summary": "This study evaluates the feasibility of lightweight Whisper models (Tiny,\nBase, Small) for Urdu speech recognition in low-resource settings. Despite Urdu\nbeing the 10th most spoken language globally with over 230 million speakers,\nits representation in automatic speech recognition (ASR) systems remains\nlimited due to dialectal diversity, code-switching, and sparse training data.\nWe benchmark these models on a curated Urdu dataset using word error rate\n(WER), without fine-tuning. Results show Whisper-Small achieves the lowest\nerror rates (33.68\\% WER), outperforming Tiny (67.08\\% WER) and Base (53.67\\%\nWER). Qualitative analysis reveals persistent challenges in phonetic accuracy\nand lexical coherence, particularly for complex utterances. While Whisper-Small\ndemonstrates promise for deployable Urdu ASR, significant gaps remain. Our\nfindings emphasize lay the groundwork for future research into effective,\nlow-resource ASR systems.", "AI": {"tldr": "This study evaluates lightweight Whisper models for Urdu speech recognition, finding that Whisper-Small outperforms others in low-resource settings but still faces challenges.", "motivation": "To address the limited representation of Urdu in automatic speech recognition due to dialectal diversity and sparse training data.", "method": "Benchmarking Whisper-Tiny, Whisper-Base, and Whisper-Small on a curated Urdu dataset using word error rate without fine-tuning.", "result": "Whisper-Small achieved the lowest word error rate at 33.68%, while Tiny and Base had 67.08% and 53.67%, respectively.", "conclusion": "Whisper-Small shows potential for Urdu ASR deployment, but significant gaps in phonetic accuracy and lexical coherence remain.", "key_contributions": ["Evaluation of Whisper models in a low-resource language setting", "Establishment of benchmarks for Urdu speech recognition", "Insights into challenges faced by ASR for Urdu"], "limitations": "Not applicable for fine-tuning and challenges in phonetic accuracy and lexical coherence persist.", "keywords": ["speech recognition", "Urdu", "low-resource settings", "Whisper models", "automatic speech recognition"], "importance_score": 4, "read_time_minutes": 8}}
{"id": "2508.09874", "pdf": "https://arxiv.org/pdf/2508.09874.pdf", "abs": "https://arxiv.org/abs/2508.09874", "title": "Memory Decoder: A Pretrained, Plug-and-Play Memory for Large Language Models", "authors": ["Jiaqi Cao", "Jiarui Wang", "Rubin Wei", "Qipeng Guo", "Kai Chen", "Bowen Zhou", "Zhouhan Lin"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have shown strong abilities in general language\ntasks, yet adapting them to specific domains remains a challenge. Current\nmethod like Domain Adaptive Pretraining (DAPT) requires costly full-parameter\ntraining and suffers from catastrophic forgetting. Meanwhile,\nRetrieval-Augmented Generation (RAG) introduces substantial inference latency\ndue to expensive nearest-neighbor searches and longer context. This paper\nintroduces Memory Decoder, a plug-and-play pretrained memory that enables\nefficient domain adaptation without changing the original model's parameters.\nMemory Decoder employs a small transformer decoder that learns to imitate the\nbehavior of an external non-parametric retriever. Once trained, Memory Decoder\ncan be seamlessly integrated with any pretrained language model that shares the\nsame tokenizer, requiring no model-specific modifications. Experimental results\ndemonstrate that Memory Decoder enables effective adaptation of various Qwen\nand Llama models to three distinct specialized domains: biomedicine, finance,\nand law, reducing perplexity by an average of 6.17 points. Overall, Memory\nDecoder introduces a novel paradigm centered on a specially pretrained memory\ncomponent designed for domain-specific adaptation. This memory architecture can\nbe integrated in a plug-and-play manner, consistently enhancing performance\nacross multiple models within the target domain.", "AI": {"tldr": "Memory Decoder enables efficient domain adaptation for LLMs without full-parameter training.", "motivation": "Adapting LLMs to specific domains is challenging due to costly training and issues like catastrophic forgetting.", "method": "Memory Decoder utilizes a small transformer decoder to imitate an external retriever, allowing seamless integration with pretrained language models.", "result": "Memory Decoder reduces perplexity by an average of 6.17 points across different models in specialized domains: biomedicine, finance, and law.", "conclusion": "The introduction of Memory Decoder represents a novel approach to domain-specific adaptation through a pretrained memory architecture that enhances performance across LLMs.", "key_contributions": ["Efficient domain adaptation for LLMs without changing model parameters.", "Plug-and-play integration with any pretrained language model using the same tokenizer.", "Demonstrated effectiveness across multiple domains leading to reduced perplexity."], "limitations": "", "keywords": ["Large Language Models", "Domain Adaptation", "Retrieval-Augmented Generation", "Memory Architecture", "Biomedicine"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.09878", "pdf": "https://arxiv.org/pdf/2508.09878.pdf", "abs": "https://arxiv.org/abs/2508.09878", "title": "A Survey of Cognitive Distortion Detection and Classification in NLP", "authors": ["Archie Sage", "Jeroen Keppens", "Helen Yannakoudakis"], "categories": ["cs.CL"], "comment": "Under review via ACL Rolling Review and committed to EMNLP 2025.\n  Camera-ready updates to follow", "summary": "As interest grows in the application of natural language processing (NLP)\ntechniques to mental health, a growing body of work explores the automatic\ndetection and classification of cognitive distortions (CDs). CDs are habitual\npatterns of negatively biased or flawed thinking that distort how people\nperceive events, judge themselves, and react to the world around them.\nIdentifying and addressing them is an important part of therapy. Despite its\nmomentum, the field remains fragmented, with inconsistencies in CD taxonomies,\ntask formulations, and evaluation practices. This survey reviews 38 studies\nspanning two decades, providing a structured overview of datasets, modelling\napproaches, and evaluation strategies. We provide a consolidated CD taxonomy\nreference, summarise common task setups, and highlight open challenges to\nsupport more coherent and reproducible research in this emerging area.", "AI": {"tldr": "This paper surveys 38 studies on the automatic detection of cognitive distortions using NLP, offering a structured overview and addressing inconsistencies in the field.", "motivation": "To address the fragmentation in the research on detecting cognitive distortions using NLP and to support coherent and reproducible studies.", "method": "Review of 38 studies over two decades, analyzing datasets, modeling approaches, and evaluation strategies.", "result": "A consolidated reference for cognitive distortion taxonomies, common task setups, and identification of open challenges in the field.", "conclusion": "The survey provides essential insights and a framework for future research on cognitive distortions in mental health applications using NLP.", "key_contributions": ["Consolidated CD taxonomy reference", "Summary of common task setups in the research", "Identification of open challenges for future studies"], "limitations": "The field is still fragmented and lacks consistency across studies in terms of taxonomies and evaluation practices.", "keywords": ["Cognitive Distortions", "Natural Language Processing", "Mental Health", "Survey", "Evaluation Strategies"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.09935", "pdf": "https://arxiv.org/pdf/2508.09935.pdf", "abs": "https://arxiv.org/abs/2508.09935", "title": "Language of Persuasion and Misrepresentation in Business Communication: A Textual Detection Approach", "authors": ["Sayem Hossen", "Monalisa Moon Joti", "Md. Golam Rashed"], "categories": ["cs.CL", "q-fin.CP", "q-fin.GN"], "comment": "21", "summary": "Business communication digitisation has reorganised the process of persuasive\ndiscourse, which\n  allows not only greater transparency but also advanced deception. This\ninquiry synthesises classical\n  rhetoric and communication psychology with linguistic theory and empirical\nstudies in the financial\n  reporting, sustainability discourse, and digital marketing to explain how\ndeceptive language can be\n  systematically detected using persuasive lexicon. In controlled settings,\ndetection accuracies of greater\n  than 99% were achieved by using computational textual analysis as well as\npersonalised transformer\n  models. However, reproducing this performance in multilingual settings is\nalso problematic and,\n  to a large extent, this is because it is not easy to find sufficient data,\nand because few multilingual\n  text-processing infrastructures are in place. This evidence shows that there\nhas been an increasing\n  gap between the theoretical representations of communication and those\nempirically approximated,\n  and therefore, there is a need to have strong automatic text-identification\nsystems where AI-based\n  discourse is becoming more realistic in communicating with humans.", "AI": {"tldr": "The paper explores the impact of digitisation on persuasive discourse, revealing how deceptive language can be detected with high accuracy using computational methods and highlights challenges in multilingual settings.", "motivation": "To explain how deceptive language in business communication can be detected and to address the growing gap between theoretical communication representations and empirical data.", "method": "Synthesis of classical rhetoric, communication psychology, and linguistic theory with empirical studies; high-accuracy detection using computational textual analysis and personalized transformer models.", "result": "Achieved detection accuracies greater than 99% in controlled settings.", "conclusion": "There is a need for robust automatic text-identification systems to bridge the gap between AI-based discourse and realistic human communication.", "key_contributions": ["High accuracy in detecting deceptive language", "Integration of linguistic theory with computational methods", "Addressing challenges in multilingual text processing"], "limitations": "Problems in reproducing performance in multilingual contexts due to data scarcity and lack of text-processing infrastructures.", "keywords": ["deceptive language", "persuasive discourse", "multilingual text processing"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2508.09937", "pdf": "https://arxiv.org/pdf/2508.09937.pdf", "abs": "https://arxiv.org/abs/2508.09937", "title": "A Comprehensive Evaluation framework of Alignment Techniques for LLMs", "authors": ["Muneeza Azmat", "Momin Abbas", "Maysa Malfiza Garcia de Macedo", "Marcelo Carpinette Grave", "Luan Soares de Souza", "Tiago Machado", "Rogerio A de Paula", "Raya Horesh", "Yixin Chen", "Heloisa Caroline de Souza Pereira Candello", "Rebecka Nordenlow", "Aminat Adebiyi"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "In submission", "summary": "As Large Language Models (LLMs) become increasingly integrated into\nreal-world applications, ensuring their outputs align with human values and\nsafety standards has become critical. The field has developed diverse alignment\napproaches including traditional fine-tuning methods (RLHF, instruction\ntuning), post-hoc correction systems, and inference-time interventions, each\nwith distinct advantages and limitations. However, the lack of unified\nevaluation frameworks makes it difficult to systematically compare these\nparadigms and guide deployment decisions. This paper introduces a\nmulti-dimensional evaluation of alignment techniques for LLMs, a comprehensive\nevaluation framework that provides a systematic comparison across all major\nalignment paradigms. Our framework assesses methods along four key dimensions:\nalignment detection, alignment quality, computational efficiency, and\nrobustness. Through experiments across diverse base models and alignment\nstrategies, we demonstrate the utility of our framework in identifying\nstrengths and limitations of current state-of-the-art models, providing\nvaluable insights for future research directions.", "AI": {"tldr": "This paper presents a unified evaluation framework for alignment techniques in Large Language Models (LLMs), allowing for systematic comparison across various paradigms.", "motivation": "To address the critical need for ensuring LLM outputs align with human values and safety standards amid their increasing integration into real-world applications.", "method": "The paper introduces a multi-dimensional evaluation framework assessing alignment techniques across four dimensions: detection, quality, efficiency, and robustness, using experiments on diverse base models and strategies.", "result": "The framework reveals strengths and limitations of various alignment methods, providing insights into the evaluation of state-of-the-art models.", "conclusion": "The proposed framework serves as a valuable guide for systematic comparisons of alignment approaches in LLMs, highlighting valuable insights for future research.", "key_contributions": ["Introduction of a multi-dimensional evaluation framework for alignment techniques in LLMs.", "Systematic comparison of major alignment paradigms across key dimensions.", "Insights for improving alignment methods based on empirical experiments."], "limitations": "", "keywords": ["Large Language Models", "Alignment Techniques", "Evaluation Framework", "Human Values", "Computational Efficiency"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.09945", "pdf": "https://arxiv.org/pdf/2508.09945.pdf", "abs": "https://arxiv.org/abs/2508.09945", "title": "VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models", "authors": ["Lingjie Jiang", "Shaohan Huang", "Xun Wu", "Yixia Li", "Dongdong Zhang", "Furu Wei"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Multimodal large language models (MLLMs) have significantly advanced the\nintegration of visual and textual understanding. However, their ability to\ngenerate code from multimodal inputs remains limited. In this work, we\nintroduce VisCodex, a unified framework that seamlessly merges vision and\ncoding language models to empower MLLMs with strong multimodal code generation\nabilities. Leveraging a task vector-based model merging technique, we integrate\na state-of-the-art coding LLM into a strong vision-language backbone, while\npreserving both visual comprehension and advanced coding skills. To support\ntraining and evaluation, we introduce the Multimodal Coding Dataset (MCD), a\nlarge-scale and diverse collection of 598k samples, including high-quality HTML\ncode, chart image-code pairs, image-augmented StackOverflow QA, and algorithmic\nproblems. Furthermore, we propose InfiBench-V, a novel and challenging\nbenchmark specifically designed to assess models on visually-rich, real-world\nprogramming questions that demand a nuanced understanding of both textual and\nvisual contexts. Extensive experiments show that VisCodex achieves\nstate-of-the-art performance among open-source MLLMs and approaches proprietary\nmodels like GPT-4o, highlighting the effectiveness of our model merging\nstrategy and new datasets.", "AI": {"tldr": "VisCodex is a multimodal framework that enhances code generation from visual inputs by integrating vision and coding language models.", "motivation": "The integration of visual and textual understanding in MLLMs has improved, but their code generation capabilities from multimodal inputs are limited.", "method": "VisCodex merges a coding LLM with a vision-language backbone using a task vector-based model merging technique.", "result": "VisCodex demonstrates state-of-the-art performance among open-source MLLMs and competes closely with proprietary models like GPT-4o.", "conclusion": "The model merging strategy and new datasets significantly enhance multimodal code generation capabilities of MLLMs.", "key_contributions": ["Introduction of VisCodex framework for multimodal code generation", "Development of a large-scale Multimodal Coding Dataset (MCD) with diverse samples", "Proposal of InfiBench-V, a benchmark for assessing visually-rich programming tasks"], "limitations": "", "keywords": ["Multimodal large language models", "code generation", "vision-language models"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.09952", "pdf": "https://arxiv.org/pdf/2508.09952.pdf", "abs": "https://arxiv.org/abs/2508.09952", "title": "Specialised or Generic? Tokenization Choices for Radiology Language Models", "authors": ["Hermione Warr", "Wentian Xu", "Harry Anthony", "Yasin Ibrahim", "Daniel McGowan", "Konstantinos Kamnitsas"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ELAMI@MICCAI2025", "summary": "The vocabulary used by language models (LM) - defined by the tokenizer -\nplays a key role in text generation quality. However, its impact remains\nunder-explored in radiology. In this work, we address this gap by\nsystematically comparing general, medical, and domain-specific tokenizers on\nthe task of radiology report summarisation across three imaging modalities. We\nalso investigate scenarios with and without LM pre-training on PubMed\nabstracts. Our findings demonstrate that medical and domain-specific\nvocabularies outperformed widely used natural language alternatives when models\nare trained from scratch. Pre-training partially mitigates performance\ndifferences between tokenizers, whilst the domain-specific tokenizers achieve\nthe most favourable results. Domain-specific tokenizers also reduce memory\nrequirements due to smaller vocabularies and shorter sequences. These results\ndemonstrate that adapting the vocabulary of LMs to the clinical domain provides\npractical benefits, including improved performance and reduced computational\ndemands, making such models more accessible and effective for both research and\nreal-world healthcare settings.", "AI": {"tldr": "The study evaluates the impact of different tokenizers on the quality of radiology report summarization, highlighting the superiority of medical and domain-specific vocabularies over general ones.", "motivation": "To explore the effect of tokenizer vocabulary on the quality of text generation in the specific context of radiology, which is under-researched.", "method": "Systematic comparison of general, medical, and domain-specific tokenizers on radiology report summarization across three imaging modalities, including evaluations with and without LM pre-training on PubMed abstracts.", "result": "Medical and domain-specific tokenizers significantly outperformed general tokenizers when models were trained from scratch. Pre-training lessened but did not eliminate performance differences, with domain-specific tokenizers showing the best results and reduced memory usage.", "conclusion": "Adapting the vocabulary of language models to the clinical domain offers practical benefits, enhancing performance and reducing computational demands, which could improve accessibility and effectiveness in healthcare.", "key_contributions": ["Evaluation of various tokenizers in radiology report summarization", "Demonstration of the advantages of medical and domain-specific vocabularies", "Showcasing reduced memory requirements from domain-specific tokenizers"], "limitations": "", "keywords": ["tokenizers", "radiology", "language models", "summarization", "clinical domain"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.09954", "pdf": "https://arxiv.org/pdf/2508.09954.pdf", "abs": "https://arxiv.org/abs/2508.09954", "title": "Shaping Event Backstories to Estimate Potential Emotion Contexts", "authors": ["Johannes Schäfer", "Roman Klinger"], "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "May 2025 version", "summary": "Emotion analysis is an inherently ambiguous task. Previous work studied\nannotator properties to explain disagreement, but this overlooks the\npossibility that ambiguity may stem from missing information about the context\nof events. In this paper, we propose a novel approach that adds reasonable\ncontexts to event descriptions, which may better explain a particular\nsituation. Our goal is to understand whether these enriched contexts enable\nhuman annotators to annotate emotions more reliably. We disambiguate a target\nevent description by automatically generating multiple event chains conditioned\non differing emotions. By combining techniques from short story generation in\nvarious settings, we achieve coherent narratives that result in a specialized\ndataset for the first comprehensive and systematic examination of\ncontextualized emotion analysis. Through automatic and human evaluation, we\nfind that contextual narratives enhance the interpretation of specific emotions\nand support annotators in producing more consistent annotations.", "AI": {"tldr": "This paper presents a novel method for enhancing emotion analysis by adding contextual narratives to event descriptions, aiming to improve the reliability of human annotations.", "motivation": "To address the ambiguity in emotion analysis and improve the consistency of annotator responses.", "method": "The authors propose generating multiple event chains conditioned on differing emotions, using techniques from short story generation to create coherent narratives that enhance understanding of context.", "result": "Through both automatic and human evaluations, the study demonstrates that enriched contextual narratives lead to improved interpretation of specific emotions and more consistent annotations by human annotators.", "conclusion": "The findings suggest that providing additional context can significantly improve the reliability of emotion analyses in ambiguous situations.", "key_contributions": ["Introduction of contextualization in emotion analysis.", "Creation of a specialized dataset for contextualized emotion analysis.", "Demonstration of the effectiveness of narrative techniques in improving annotation consistency."], "limitations": "The scope of the study may be limited to specific types of event descriptions, and further research is needed to generalize findings across a broader range of contexts.", "keywords": ["emotion analysis", "contextual narratives", "annotation consistency", "story generation", "emotion disambiguation"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.09956", "pdf": "https://arxiv.org/pdf/2508.09956.pdf", "abs": "https://arxiv.org/abs/2508.09956", "title": "Performance of GPT-5 Frontier Models in Ophthalmology Question Answering", "authors": ["Fares Antaki", "David Mikhail", "Daniel Milad", "Danny A Mammo", "Sumit Sharma", "Sunil K Srivastava", "Bing Yu Chen", "Samir Touma", "Mertcan Sevgi", "Jonathan El-Khoury", "Pearse A Keane", "Qingyu Chen", "Yih Chung Tham", "Renaud Duval"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) such as GPT-5 integrate advanced reasoning\ncapabilities that may improve performance on complex medical question-answering\ntasks. For this latest generation of reasoning models, the configurations that\nmaximize both accuracy and cost-efficiency have yet to be established. We\nevaluated 12 configurations of OpenAI's GPT-5 series (three model tiers across\nfour reasoning effort settings) alongside o1-high, o3-high, and GPT-4o, using\n260 closed-access multiple-choice questions from the American Academy of\nOphthalmology Basic Clinical Science Course (BCSC) dataset. The primary outcome\nwas multiple-choice accuracy; secondary outcomes included head-to-head ranking\nvia a Bradley-Terry model, rationale quality assessment using a\nreference-anchored, pairwise LLM-as-a-judge framework, and analysis of\naccuracy-cost trade-offs using token-based cost estimates. GPT-5-high achieved\nthe highest accuracy (0.965; 95% CI, 0.942-0.985), outperforming all GPT-5-nano\nvariants (P < .001), o1-high (P = .04), and GPT-4o (P < .001), but not o3-high\n(0.958; 95% CI, 0.931-0.981). GPT-5-high ranked first in both accuracy (1.66x\nstronger than o3-high) and rationale quality (1.11x stronger than o3-high).\nCost-accuracy analysis identified several GPT-5 configurations on the Pareto\nfrontier, with GPT-5-mini-low offering the most favorable low-cost,\nhigh-performance balance. These results benchmark GPT-5 on a high-quality\nophthalmology dataset, demonstrate the influence of reasoning effort on\naccuracy, and introduce an autograder framework for scalable evaluation of\nLLM-generated answers against reference standards in ophthalmology.", "AI": {"tldr": "This paper evaluates the efficacy of various configurations of GPT-5 models for medical question-answering, particularly in ophthalmology, highlighting accuracy and cost-effectiveness.", "motivation": "To determine the optimal configurations of GPT-5 models that maximize accuracy and cost-efficiency for complex medical question-answering tasks.", "method": "Evaluated 12 configurations of GPT-5 using 260 closed-access multiple-choice questions from a clinical dataset, focusing on multiple-choice accuracy and rationale quality assessment.", "result": "GPT-5-high achieved the highest accuracy (0.965), outperforming other models, and identified configurations on the Pareto frontier for favorable cost-performance balance.", "conclusion": "The findings benchmark GPT-5's performance in ophthalmology and suggest frameworks for scalable evaluation of LLM-generated responses.", "key_contributions": ["Evaluation of 12 configurations of GPT-5 for medical tasks", "Establishment of a cost-accuracy analysis framework", "Introduction of an autograder framework for LLM-generated answers"], "limitations": "", "keywords": ["Large Language Models", "GPT-5", "Medical Question-Answering", "Cost-Efficiency", "Ophthalmology"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.09957", "pdf": "https://arxiv.org/pdf/2508.09957.pdf", "abs": "https://arxiv.org/abs/2508.09957", "title": "Which one Performs Better? Wav2Vec or Whisper? Applying both in Badini Kurdish Speech to Text (BKSTT)", "authors": ["Renas Adnan", "Hossein Hassani"], "categories": ["cs.CL"], "comment": "21 pages, 20 figures, 7 tables", "summary": "Speech-to-text (STT) systems have a wide range of applications. They are\navailable in many languages, albeit at different quality levels. Although\nKurdish is considered a less-resourced language from a processing perspective,\nSST is available for some of the Kurdish dialects, for instance, Sorani\n(Central Kurdish). However, that is not applied to other Kurdish dialects,\nBadini and Hawrami, for example. This research is an attempt to address this\ngap. Bandin, approximately, has two million speakers, and STT systems can help\ntheir community use mobile and computer-based technologies while giving their\ndialect more global visibility. We aim to create a language model based on\nBadini's speech and evaluate its performance. To cover a conversational aspect,\nhave a proper confidence level of grammatical accuracy, and ready\ntranscriptions, we chose Badini kids' stories, eight books including 78\nstories, as the textual input. Six narrators narrated the books, which resulted\nin approximately 17 hours of recording. We cleaned, segmented, and tokenized\nthe input. The preprocessing produced nearly 15 hours of speech, including\n19193 segments and 25221 words. We used Wav2Vec2-Large-XLSR-53 and\nWhisper-small to develop the language models. The experiments indicate that the\ntranscriptions process based on the Wav2Vec2-Large-XLSR-53 model provides a\nsignificantly more accurate and readable output than the Whisper-small model,\nwith 90.38% and 65.45% readability, and 82.67% and 53.17% accuracy,\nrespectively.", "AI": {"tldr": "This research develops a speech-to-text (STT) system for the Badini Kurdish dialect using children's stories as training data.", "motivation": "To improve accessibility and visibility of the Badini dialect through STT systems, addressing the lack of resources for less-resourced languages like Kurdish.", "method": "Created a language model using approximately 17 hours of recordings from children's stories, applying Wav2Vec2-Large-XLSR-53 and Whisper-small for transcription evaluation.", "result": "Wav2Vec2-Large-XLSR-53 provided a transcription accuracy of 82.67% and readability of 90.38%, outperforming the Whisper-small model which had 53.17% accuracy and 65.45% readability.", "conclusion": "The study demonstrates that Wav2Vec2-Large-XLSR-53 is a superior model for STT in Badini, highlighting the potential for improving STT systems in underrepresented languages.", "key_contributions": ["First STT model for Badini Kurdish", "Large dataset of children's stories for model training", "Comparison of two advanced models (Wav2Vec2 and Whisper) for STT performance"], "limitations": "Focused only on the Badini dialect; results may not generalize to other dialects or languages.", "keywords": ["Speech-to-Text", "Badini Kurdish", "Wav2Vec2", "Whisper", "Machine Learning"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2508.09958", "pdf": "https://arxiv.org/pdf/2508.09958.pdf", "abs": "https://arxiv.org/abs/2508.09958", "title": "Neural Bandit Based Optimal LLM Selection for a Pipeline of Tasks", "authors": ["Baran Atalar", "Eddie Zhang", "Carlee Joe-Wong"], "categories": ["cs.CL", "cs.LG"], "comment": "Submitted to AAAI 2026", "summary": "With the increasing popularity of large language models (LLMs) for a variety\nof tasks, there has been a growing interest in strategies that can predict\nwhich out of a set of LLMs will yield a successful answer at low cost. This\nproblem promises to become more and more relevant as providers like Microsoft\nallow users to easily create custom LLM \"assistants\" specialized to particular\ntypes of queries. However, some tasks (i.e., queries) may be too specialized\nand difficult for a single LLM to handle alone. These applications often\nbenefit from breaking down the task into smaller subtasks, each of which can\nthen be executed by a LLM expected to perform well on that specific subtask.\nFor example, in extracting a diagnosis from medical records, one can first\nselect an LLM to summarize the record, select another to validate the summary,\nand then select another, possibly different, LLM to extract the diagnosis from\nthe summarized record. Unlike existing LLM selection or routing algorithms,\nthis setting requires that we select a sequence of LLMs, with the output of\neach LLM feeding into the next and potentially influencing its success. Thus,\nunlike single LLM selection, the quality of each subtask's output directly\naffects the inputs, and hence the cost and success rate, of downstream LLMs,\ncreating complex performance dependencies that must be learned and accounted\nfor during selection. We propose a neural contextual bandit-based algorithm\nthat trains neural networks that model LLM success on each subtask in an online\nmanner, thus learning to guide the LLM selections for the different subtasks,\neven in the absence of historical LLM performance data. Experiments on\ntelecommunications question answering and medical diagnosis prediction datasets\nillustrate the effectiveness of our proposed approach compared to other LLM\nselection algorithms.", "AI": {"tldr": "This paper introduces a neural contextual bandit-based algorithm for selecting a sequence of LLMs for complex tasks, focusing on medical diagnosis extraction and telecommunications question answering.", "motivation": "The rise of large language models (LLMs) necessitates strategies to predict which model is best for specific queries, especially as users create custom LLM assistants.", "method": "A neural contextual bandit algorithm is developed to model LLM success for subtasks in real-time, even without prior performance data.", "result": "The proposed algorithm shows improved performance in selecting LLMs for subtasks, demonstrated through experiments on relevant datasets.", "conclusion": "This approach effectively handles the complexity of LLM selection for multi-step tasks, enhancing accuracy and efficiency in applications such as health informatics.", "key_contributions": ["Development of a neural contextual bandit-based algorithm for LLM selection", "Demonstration of improved task performance on medical and telecommunications datasets", "Provision of a framework for selecting sequences of LLMs for complex subtasks"], "limitations": "", "keywords": ["large language models", "neural contextual bandits", "subtask selection", "health informatics", "LLM performance prediction"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.09614", "pdf": "https://arxiv.org/pdf/2508.09614.pdf", "abs": "https://arxiv.org/abs/2508.09614", "title": "How Persuasive Could LLMs Be? A First Study Combining Linguistic-Rhetorical Analysis and User Experiments", "authors": ["Daniel Raffini", "Agnese Macori", "Lorenzo Porcaro", "Tiziana Catarci", "Marco Angelini"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": "9-pages", "summary": "This study examines the rhetorical and linguistic features of argumentative\ntexts generated by ChatGPT on ethically nuanced topics and investigates their\npersuasive impact on human readers.Through a user study involving 62\nparticipants and pre-post interaction surveys, the paper analyzes how exposure\nto AI-generated arguments affects opinion change and user perception. A\nlinguistic and rhetorical analysis of the generated texts reveals a consistent\nargumentative macrostructure, reliance on formulaic expressions, and limited\nstylistic richness. While ChatGPT demonstrates proficiency in constructing\ncoherent argumentative texts, its persuasive efficacy appears constrained,\nparticularly on topics involving ethical issues.The study finds that while\nparticipants often acknowledge the benefits highlighted by ChatGPT, ethical\nconcerns tend to persist or even intensify post-interaction. The results also\ndemonstrate a variation depending on the topic. These findings highlight new\ninsights on AI-generated persuasion in ethically sensitive domains and are a\nbasis for future research.", "AI": {"tldr": "Study analyzes AI-generated argumentative texts from ChatGPT, their influence on user opinion regarding ethical issues, and findings on persuasive limitations.", "motivation": "To investigate the rhetorical and linguistic features of argumentative texts generated by ChatGPT and their impact on human readers' opinions, particularly on ethically nuanced topics.", "method": "User study with 62 participants surveyed before and after exposure to AI-generated arguments to assess opinion change and perception.", "result": "The study found that while ChatGPT can create coherent argumentative texts, its effectiveness in persuading on ethical issues is limited, with concerns often persisting or intensifying post-interaction.", "conclusion": "Insights from this research highlight the nuanced role of AI in persuasion regarding ethical topics and suggest avenues for future exploration.", "key_contributions": ["Analysis of rhetorical and linguistic features in AI-generated texts.", "Evaluation of persuasive impact on user opinions in ethically nuanced contexts.", "Identification of limitations in AI argument construction and effectiveness."], "limitations": "The study is context-dependent, with results varying based on the topic of discussion.", "keywords": ["AI-generated texts", "persuasion", "ethical issues", "argumentative discourse", "linguistic analysis"], "importance_score": 6, "read_time_minutes": 9}}
{"id": "2508.09651", "pdf": "https://arxiv.org/pdf/2508.09651.pdf", "abs": "https://arxiv.org/abs/2508.09651", "title": "A Close Reading Approach to Gender Narrative Biases in AI-Generated Stories", "authors": ["Daniel Raffini", "Agnese Macori", "Marco Angelini", "Tiziana Catarci"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": "8-pages", "summary": "The paper explores the study of gender-based narrative biases in stories\ngenerated by ChatGPT, Gemini, and Claude. The prompt design draws on Propp's\ncharacter classifications and Freytag's narrative structure. The stories are\nanalyzed through a close reading approach, with particular attention to\nadherence to the prompt, gender distribution of characters, physical and\npsychological descriptions, actions, and finally, plot development and\ncharacter relationships. The results reveal the persistence of biases -\nespecially implicit ones - in the generated stories and highlight the\nimportance of assessing biases at multiple levels using an interpretative\napproach.", "AI": {"tldr": "Study of gender-based narrative biases in stories generated by ChatGPT, Gemini, and Claude.", "motivation": "To explore the persistence of implicit gender biases in AI-generated narratives and assess these biases at multiple levels.", "method": "Analysis of stories using a close reading approach based on Propp's character classifications and Freytag's narrative structure.", "result": "The analysis reveals the presence of implicit biases in character distribution and descriptions, actions, and plot developments in AI-generated stories.", "conclusion": "Assessing biases in AI-generated narratives is crucial for understanding their implications in storytelling and for improving future models.", "key_contributions": ["Identification of gender-based biases in narrative generation", "Use of Propp's and Freytag's frameworks for story analysis", "Emphasis on the need for multi-level bias assessment"], "limitations": "Focus on specific AI models and narrative structures may limit the generalizability of the findings.", "keywords": ["gender bias", "AI narratives", "story analysis", "narrative structure", "implicit bias"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2305.01710", "pdf": "https://arxiv.org/pdf/2305.01710.pdf", "abs": "https://arxiv.org/abs/2305.01710", "title": "From Stars to Insights: Exploration and Implementation of Unified Sentiment Analysis with Distant Supervision", "authors": ["Wenchang Li", "John P. Lalor", "Yixing Chen", "Vamsi K. Kanuri"], "categories": ["cs.CL"], "comment": "Forthcoming in ACM Trans. Manage. Inf. Syst", "summary": "Sentiment analysis is integral to understanding the voice of the customer and\ninforming businesses' strategic decisions. Conventional sentiment analysis\ninvolves three separate tasks: aspect-category detection, aspect-category\nsentiment analysis, and rating prediction. However, independently tackling\nthese tasks can overlook their interdependencies and often requires expensive,\nfine-grained annotations. This paper introduces unified sentiment analysis, a\nnovel learning paradigm that integrates the three aforementioned tasks into a\ncoherent framework. To achieve this, we propose the Distantly Supervised\nPyramid Network (DSPN), which employs a pyramid structure to capture sentiment\nat word, aspect, and document levels in a hierarchical manner. Evaluations on\nmulti-aspect review datasets in English and Chinese show that DSPN, using only\nstar rating labels for supervision, demonstrates significant efficiency\nadvantages while performing comparably well to a variety of benchmark models.\nAdditionally, DSPN's pyramid structure enables the interpretability of its\noutputs. Our findings validate DSPN's effectiveness and efficiency,\nestablishing a robust, resource-efficient, unified framework for sentiment\nanalysis.", "AI": {"tldr": "This paper introduces a unified sentiment analysis framework that integrates aspect-category detection, aspect-category sentiment analysis, and rating prediction using the Distantly Supervised Pyramid Network (DSPN).", "motivation": "To address the limitations of conventional sentiment analysis, which treats related tasks separately, leading to inefficiencies and the need for fine-grained annotations.", "method": "The Distantly Supervised Pyramid Network (DSPN) uses a pyramidal architecture to analyze sentiment at multiple levels—word, aspect, and document—effectively unifying the sentiment analysis tasks.", "result": "DSPN shows significant efficiency advantages while performing comparably to established models, using only star rating labels for supervision on multi-aspect review datasets in English and Chinese.", "conclusion": "DSPN establishes a robust and resource-efficient unified framework for sentiment analysis, enhancing efficiency and interpretability of results.", "key_contributions": ["Introduction of a unified sentiment analysis framework", "Proposal of the Distantly Supervised Pyramid Network (DSPN)", "Demonstration of efficiency advantages using minimal supervision"], "limitations": "", "keywords": ["sentiment analysis", "unified framework", "Distantly Supervised Pyramid Network"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2405.20179", "pdf": "https://arxiv.org/pdf/2405.20179.pdf", "abs": "https://arxiv.org/abs/2405.20179", "title": "Robo-Instruct: Simulator-Augmented Instruction Alignment For Finetuning Code LLMs", "authors": ["Zichao Hu", "Junyi Jessy Li", "Arjun Guha", "Joydeep Biswas"], "categories": ["cs.CL", "cs.AI", "cs.RO"], "comment": null, "summary": "Code LLMs have shown promising results with converting tasks in natural\nlanguage to programs that can be executed by service robots. We are interested\nin finetuning small, specialized LLMs for this purpose, but collecting datasets\nof task-program pairs specific to each robot is time-consuming and expensive.\nWhile approaches such as SELF-INSTRUCT and EVOL-INSTRUCT are capable of\ngenerating novel tasks given a few examples, they are unable to provide the\ncorresponding programs that correctly abide by physical-world and\nrobot-constraints using the provided programming interface. Using a simulator\nis a natural potential solution to checking for such constraints, but building\nsimulation environments that can handle arbitrary tasks and their necessary\nobjects and locations, is challenging. To address these challenges, we\nintroduce ROBO-INSTRUCT, which synthesizes task-specific simulation\nenvironments on the fly during program execution, by opportunistically\ninferring entity properties and enforcing corresponding constraints based on\nhow the entities are used in the task program. Additionally, ROBO-INSTRUCT\nintegrates an LLM-aided post-processing procedure to refine instructions for\nbetter alignment with robot programs. We demonstrate the effectiveness of\nROBO-INSTRUCT across multiple LLMs, showing that our fine-tuned models\noutperform all baseline methods and even match or surpass the performance of\nseveral larger and proprietary models.", "AI": {"tldr": "ROBO-INSTRUCT improves task-program conversion for service robots by creating dynamic simulation environments and using LLM post-processing.", "motivation": "The paper addresses the challenge of generating executable programs for service robots from natural language tasks, highlighting the limitations of existing methods in producing physically valid programs that adhere to robot constraints.", "method": "ROBO-INSTRUCT synthesizes task-specific simulation environments on-the-fly and uses LLMs to refine instructions for alignment with robot programming requirements.", "result": "ROBO-INSTRUCT models outperform baseline methods and achieve comparable or superior performance to larger proprietary models in task execution success rates.", "conclusion": "The proposed approach significantly enhances the task-program conversion process, enabling more reliable execution of programs by robots in real-world scenarios.", "key_contributions": ["Synthesis of dynamic simulation environments during program execution", "Integration of LLM-based post-processing for instruction refinement", "Demonstrated effectiveness across multiple fine-tuned LLMs outperforming baseline methods"], "limitations": "", "keywords": ["task-program generation", "service robotics", "simulation environments", "LLM post-processing", "robot constraints"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2406.17588", "pdf": "https://arxiv.org/pdf/2406.17588.pdf", "abs": "https://arxiv.org/abs/2406.17588", "title": "LongIns: A Challenging Long-context Instruction-based Exam for LLMs", "authors": ["Shawn Gavin", "Tuney Zheng", "Jiaheng Liu", "Quehry Que", "Noah Wang", "Jian Yang", "Chenchen Zhang", "Wenhao Huang", "Ge Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "The long-context capabilities of large language models (LLMs) have been a hot\ntopic in recent years. To evaluate the performance of LLMs in different\nscenarios, various assessment benchmarks have emerged. However, as most of\nthese benchmarks focus on identifying key information to answer questions,\nwhich mainly requires the retrieval ability of LLMs, these benchmarks can\npartially represent the reasoning performance of LLMs from large amounts of\ninformation. Meanwhile, although LLMs often claim to have context windows of\n32k, 128k, 200k, or even longer, these benchmarks fail to reveal the actual\nsupported length of these LLMs. To address these issues, we propose the LongIns\nbenchmark dataset, a challenging long-context instruction-based exam for LLMs,\nwhich is built based on the existing instruction datasets. Specifically, in our\nLongIns, we introduce three evaluation settings: Global Instruction & Single\nTask (GIST), Local Instruction & Single Task (LIST), and Local Instruction &\nMultiple Tasks (LIMT). Based on LongIns, we perform comprehensive evaluations\non existing LLMs and have the following important findings: (1). The\ntop-performing GPT-4 with 128k context length performs poorly on the evaluation\ncontext window of 16k in our LongIns. (2). For the multi-hop reasoning ability\nof many existing LLMs, significant efforts are still needed under short context\nwindows (less than 4k).", "AI": {"tldr": "This paper introduces the LongIns benchmark dataset for evaluating long-context capabilities of LLMs, highlighting performance discrepancies with context windows.", "motivation": "To address the inadequacy of existing benchmarks that primarily assess retrieval abilities, which do not adequately represent reasoning performance or the actual context length capabilities of LLMs.", "method": "The authors created the LongIns benchmark consisting of three evaluation settings: Global Instruction & Single Task (GIST), Local Instruction & Single Task (LIST), and Local Instruction & Multiple Tasks (LIMT) to systematically evaluate LLM performance.", "result": "Comprehensive evaluations showed that top-performing models like GPT-4, with a claimed 128k context length, underperformed in scenarios with a 16k context length, highlighting issues with multi-hop reasoning in shorter context windows.", "conclusion": "The findings indicate that existing LLMs require substantial improvements in multi-hop reasoning abilities, especially when operating within short context windows.", "key_contributions": ["Introduction of LongIns benchmark for LLM evaluation.", "Identification of performance issues in top LLMs regarding context length.", "Insights into reasoning capabilities of LLMs under varying context sizes."], "limitations": "", "keywords": ["Long-context", "Large Language Models", "Benchmarking", "Instruction-based evaluation", "Multi-hop reasoning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2410.19925", "pdf": "https://arxiv.org/pdf/2410.19925.pdf", "abs": "https://arxiv.org/abs/2410.19925", "title": "Improving Multimodal Large Language Models Using Continual Learning", "authors": ["Shikhar Srivastava", "Md Yousuf Harun", "Robik Shrestha", "Christopher Kanan"], "categories": ["cs.CL", "cs.CV", "cs.LG"], "comment": "CoLLAs 2025 and Scalable Continual Learning for Lifelong Foundation\n  Models, NeurIPS 2024", "summary": "Generative large language models (LLMs) exhibit impressive capabilities,\nwhich can be further augmented by integrating a pre-trained vision model into\nthe original LLM to create a multimodal LLM (MLLM). However, this integration\noften significantly decreases performance on natural language understanding and\ngeneration tasks, compared to the original LLM. This study investigates this\nissue using the LLaVA MLLM, treating the integration as a continual learning\nproblem. We evaluate five continual learning methods to mitigate forgetting and\nidentify a technique that enhances visual understanding while minimizing\nlinguistic performance loss. Our approach reduces linguistic performance\ndegradation by up to 15% over the LLaVA recipe, while maintaining high\nmultimodal accuracy. We also demonstrate the robustness of our method through\ncontinual learning on a sequence of vision-language tasks, effectively\npreserving linguistic skills while acquiring new multimodal capabilities.\nProject webpage: https://shikhar-srivastava.github.io/cl-for-improving-mllms", "AI": {"tldr": "This study addresses performance degradation in multimodal large language models (MLLMs) by applying continual learning methods to integrate vision and language tasks effectively.", "motivation": "The integration of pre-trained vision models into LLMs often compromises natural language understanding and generation performance.", "method": "We evaluate five continual learning techniques to reduce forgetting and enhance visual understanding while minimizing loss in linguistic performance.", "result": "Successfully reduced linguistic performance degradation by up to 15% while maintaining high multimodal accuracy.", "conclusion": "Our method shows robustness in preserving linguistic skills during continual learning across vision-language tasks, allowing for the acquisition of new multimodal capabilities.", "key_contributions": ["Introduction of continual learning methods to MLLMs", "Identification of techniques that minimize performance loss", "Demonstration of robustness across various tasks"], "limitations": "", "keywords": ["multimodal learning", "continual learning", "large language models", "vision-language tasks", "performance degradation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2412.10417", "pdf": "https://arxiv.org/pdf/2412.10417.pdf", "abs": "https://arxiv.org/abs/2412.10417", "title": "Leveraging Audio and Text Modalities in Mental Health: A Study of LLMs Performance", "authors": ["Abdelrahman A. Ali", "Aya E. Fouda", "Radwa J. Hanafy", "Mohammed E. Fouda"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "Mental health disorders are increasingly prevalent worldwide, creating an\nurgent need for innovative tools to support early diagnosis and intervention.\nThis study explores the potential of Large Language Models (LLMs) in multimodal\nmental health diagnostics, specifically for detecting depression and Post\nTraumatic Stress Disorder through text and audio modalities. Using the E-DAIC\ndataset, we compare text and audio modalities to investigate whether LLMs can\nperform equally well or better with audio inputs. We further examine the\nintegration of both modalities to determine if this can enhance diagnostic\naccuracy, which generally results in improved performance metrics. Our analysis\nspecifically utilizes custom-formulated metrics; Modal Superiority Score and\nDisagreement Resolvement Score to evaluate how combined modalities influence\nmodel performance. The Gemini 1.5 Pro model achieves the highest scores in\nbinary depression classification when using the combined modality, with an F1\nscore of 0.67 and a Balanced Accuracy (BA) of 77.4%, assessed across the full\ndataset. These results represent an increase of 3.1% over its performance with\nthe text modality and 2.7% over the audio modality, highlighting the\neffectiveness of integrating modalities to enhance diagnostic accuracy.\nNotably, all results are obtained in zero-shot inferring, highlighting the\nrobustness of the models without requiring task-specific fine-tuning. To\nexplore the impact of different configurations on model performance, we conduct\nbinary, severity, and multiclass tasks using both zero-shot and few-shot\nprompts, examining the effects of prompt variations on performance. The results\nreveal that models such as Gemini 1.5 Pro in text and audio modalities, and\nGPT-4o mini in the text modality, often surpass other models in balanced\naccuracy and F1 scores across multiple tasks.", "AI": {"tldr": "This study investigates the use of Large Language Models (LLMs) in diagnosing mental health disorders like depression and PTSD through multimodal approaches using text and audio data.", "motivation": "The rising prevalence of mental health disorders necessitates innovative diagnostic tools.", "method": "The study utilizes the E-DAIC dataset to compare the performance of LLMs in text and audio modalities for mental health diagnosis and assesses the integration of both modalities.", "result": "The Gemini 1.5 Pro model achieved the highest scores in binary depression classification with a combined modality F1 score of 0.67 and a balanced accuracy of 77.4%.", "conclusion": "Integrating text and audio modalities improves diagnostic accuracy without the need for task-specific fine-tuning, showing robustness in zero-shot inferring.", "key_contributions": ["Investigation of LLM performance in multimodal mental health diagnosis", "Development of custom metrics for evaluating model performance", "Demonstration of improved diagnostic accuracy through modality integration"], "limitations": "", "keywords": ["mental health", "large language models", "multimodal diagnostics", "depression detection", "PTSD"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2412.14368", "pdf": "https://arxiv.org/pdf/2412.14368.pdf", "abs": "https://arxiv.org/abs/2412.14368", "title": "Memorization Over Reasoning? Exposing and Mitigating Verbatim Memorization in Large Language Models' Character Understanding Evaluation", "authors": ["Yuxuan Jiang", "Francis Ferraro"], "categories": ["cs.CL"], "comment": null, "summary": "Recently, Large Language Models (LLMs) have shown impressive performance in\ncharacter understanding tasks, such as analyzing the roles, personalities, and\nrelationships of fictional characters. However, the extensive pre-training\ncorpora used by LLMs raise concerns that they may rely on memorizing popular\nfictional works rather than genuinely understanding and reasoning about them.\nIn this work, we argue that 'gist memory'-capturing essential meaning - should\nbe the primary mechanism for character understanding tasks, as opposed to\n'verbatim memory' - exact match of a string. We introduce a simple yet\neffective method to mitigate mechanized memorization in character understanding\nevaluations while preserving the essential implicit cues needed for\ncomprehension and reasoning. Our approach reduces memorization-driven\nperformance on popular fictional works from 96% accuracy to 72% and results in\nup to an 18% drop in accuracy across various character understanding tasks.\nThese findings underscore the issue of data contamination in existing\nbenchmarks, which often measure memorization rather than true character\nunderstanding.", "AI": {"tldr": "This paper addresses the limitations of large language models (LLMs) in character understanding tasks, emphasizing the importance of gist memory over verbatim memory for genuine comprehension.", "motivation": "To tackle the reliance of LLMs on memorization of popular fictional works rather than true understanding in character analysis.", "method": "The authors introduce a method to reduce memorization-driven performance in evaluations of character understanding, promoting gist memory in place of verbatim memory.", "result": "The method decreases memorization-driven accuracy on popular fiction from 96% to 72%, affecting overall character understanding task accuracy by up to 18%.", "conclusion": "The findings highlight the data contamination in current benchmarks that prioritize memorization over actual character understanding.", "key_contributions": ["Proposing gist memory as a primary mechanism for character understanding tasks.", "Developing a method to mitigate memorization in LLMs during character analysis evaluations.", "Demonstrating the impact of this method on existing accuracy benchmarks."], "limitations": "Focus on fictional characters may not translate to real-world understanding; the study is limited to evaluating performance based on specific benchmarks.", "keywords": ["Large Language Models", "Character Understanding", "Gist Memory", "Verbatim Memory", "Data Contamination"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2501.04661", "pdf": "https://arxiv.org/pdf/2501.04661.pdf", "abs": "https://arxiv.org/abs/2501.04661", "title": "Beyond Memorization: Assessing Semantic Generalization in Large Language Models Using Phrasal Constructions", "authors": ["Wesley Scivetti", "Melissa Torgbi", "Austin Blodgett", "Mollie Shichman", "Taylor Hudson", "Claire Bonial", "Harish Tayyar Madabushi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The web-scale of pretraining data has created an important evaluation\nchallenge: to disentangle linguistic competence on cases well-represented in\npretraining data from generalization to out-of-domain language, specifically\nthe dynamic, real-world instances less common in pretraining data. To this end,\nwe construct a diagnostic evaluation to systematically assess natural language\nunderstanding in LLMs by leveraging Construction Grammar (CxG). CxG provides a\npsycholinguistically grounded framework for testing generalization, as it\nexplicitly links syntactic forms to abstract, non-lexical meanings. Our novel\ninference evaluation dataset consists of English phrasal constructions, for\nwhich speakers are known to be able to abstract over commonplace instantiations\nin order to understand and produce creative instantiations. Our evaluation\ndataset uses CxG to evaluate two central questions: first, if models can\n'understand' the semantics of sentences for instances that are likely to appear\nin pretraining data less often, but are intuitive and easy for people to\nunderstand. Second, if LLMs can deploy the appropriate constructional semantics\ngiven constructions that are syntactically identical but with divergent\nmeanings. Our results demonstrate that state-of-the-art models, including\nGPT-o1, exhibit a performance drop of over 40% on our second task, revealing a\nfailure to generalize over syntactically identical forms to arrive at distinct\nconstructional meanings in the way humans do. We make our novel dataset and\nassociated experimental data, including prompts and model responses, publicly\navailable.", "AI": {"tldr": "This paper presents a diagnostic evaluation for assessing natural language understanding in LLMs using Construction Grammar, revealing significant generalization challenges.", "motivation": "To evaluate LLMs' ability to generalize language understanding beyond their training data, especially in contexts less common in pretraining.", "method": "A diagnostic evaluation leveraging Construction Grammar (CxG) was constructed, including a novel inference evaluation dataset of English phrasal constructions.", "result": "State-of-the-art models, like GPT-o1, showed a performance drop of over 40% when required to differentiate syntactically identical constructions that have different meanings.", "conclusion": "The study highlights significant shortcomings in LLMs' generalization abilities and provides a publicly available dataset for further research in this area.", "key_contributions": ["Introduction of a diagnostic evaluation using Construction Grammar for LLMs", "Creation of a novel inference evaluation dataset for English phrasal constructions", "Public availability of the dataset and experimental data for future research"], "limitations": "", "keywords": ["Natural Language Processing", "Construction Grammar", "LLMs", "Generalization", "Evaluation Dataset"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2501.11790", "pdf": "https://arxiv.org/pdf/2501.11790.pdf", "abs": "https://arxiv.org/abs/2501.11790", "title": "Benchmarking LLMs' Mathematical Reasoning with Unseen Random Variables Questions", "authors": ["Zijin Hong", "Hao Wu", "Su Dong", "Junnan Dong", "Yilin Xiao", "Yujing Zhang", "Zhu Wang", "Feiran Huang", "Linyi Li", "Hongxia Yang", "Xiao Huang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent studies have raised significant concerns regarding the reliability of\ncurrent mathematics benchmarks, highlighting issues such as simplistic design\nand potential data contamination. Consequently, developing a reliable benchmark\nthat effectively evaluates large language models' (LLMs) genuine capabilities\nin mathematical reasoning remains a critical challenge. To address these\nconcerns, we propose RV-Bench, a novel evaluation methodology for Benchmarking\nLLMs with Random Variables in mathematical reasoning. Specifically, we build\nquestion-generating functions to produce random variable questions (RVQs),\nwhose background content mirrors original benchmark problems, but with\nrandomized variable combinations, rendering them \"unseen\" to LLMs. Models must\ncompletely understand the inherent question pattern to correctly answer RVQs\nwith diverse variable combinations. Thus, an LLM's genuine reasoning capability\nis reflected through its accuracy and robustness on RV-Bench. We conducted\nextensive experiments on over 30 representative LLMs across more than 1,000\nRVQs. Our findings propose that LLMs exhibit a proficiency imbalance between\nencountered and ``unseen'' data distributions. Furthermore, RV-Bench reveals\nthat proficiency generalization across similar mathematical reasoning tasks is\nlimited, but we verified it can still be effectively elicited through test-time\nscaling.", "AI": {"tldr": "RV-Bench is a proposed evaluation methodology for benchmarking LLMs in mathematical reasoning by generating random variable questions to assess genuine reasoning capabilities.", "motivation": "Developing a reliable benchmark to evaluate LLMs' true capabilities in mathematical reasoning due to concerns about existing benchmarks' reliability.", "method": "The methodology involves creating question-generating functions that produce random variable questions (RVQs) that resemble original benchmark problems with randomized variables.", "result": "Experiments show LLMs have a proficiency imbalance between known and unseen data distributions and limited generalization across similar tasks, but effective generalization can be prompted through test-time scaling.", "conclusion": "RV-Bench offers a better understanding of LLMs' reasoning capabilities and highlights the need for improved benchmarks in mathematical reasoning.", "key_contributions": ["Introduction of RV-Bench for evaluating LLMs with random variables", "Demonstration of proficiency imbalance in LLMs between encountered and unseen questions", "Insights on limited generalization capabilities of LLMs in mathematical reasoning tasks."], "limitations": "The effectiveness of RV-Bench may vary depending on the complexity of mathematical reasoning tasks.", "keywords": ["LLMs", "mathematical reasoning", "benchmarking", "random variables", "evaluation methodology"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2504.01137", "pdf": "https://arxiv.org/pdf/2504.01137.pdf", "abs": "https://arxiv.org/abs/2504.01137", "title": "Follow the Flow: On Information Flow Across Textual Tokens in Text-to-Image Models", "authors": ["Guy Kaplan", "Michael Toker", "Yuval Reif", "Yonatan Belinkov", "Roy Schwartz"], "categories": ["cs.CL"], "comment": null, "summary": "Text-to-image (T2I) models generate images by encoding text prompts into\ntoken representations, which then guide the diffusion process. While prior work\nhas largely focused on improving alignment by refining the diffusion process,\nwe focus on the textual encoding stage. Specifically, we investigate how\nsemantic information is distributed across token representations within and\nbetween lexical items (i.e., words or expressions conveying a single concept)\nin the prompt. We analyze information flow at two levels: (1) in-item\nrepresentation-whether individual tokens represent their lexical item, and (2)\ncross-item interaction-whether information flows across the tokens of different\nlexical items. We use patching techniques to uncover surprising encoding\npatterns. We find information is usually concentrated in only one or two of the\nitem's tokens-For example, in the item \"San Francisco's Golden Gate Bridge\",\nthe token \"Gate\" sufficiently captures the entire expression while the other\ntokens could effectively be discarded. Lexical items also tend to remain\nisolated; for instance, the token \"dog\" encodes no visual information about\n\"green\" in the prompt \"a green dog\". However, in some cases, items do influence\neach other's representation, often leading to misinterpretations-e.g., in the\nprompt \"a pool by a table\", the token pool represents a pool table after\ncontextualization. Our findings highlight the critical role of token-level\nencoding in image generation, suggesting that misalignment issues may originate\nalready during the textual encoding.", "AI": {"tldr": "The study investigates how semantic information is distributed across token representations in text-to-image models, emphasizing the importance of token-level encoding in image generation.", "motivation": "To explore the significance of semantic information distribution in textual encoding for improving image generation in text-to-image models.", "method": "Analysis of information flow at two levels: in-item representation and cross-item interaction, using patching techniques to uncover encoding patterns.", "result": "Information is typically concentrated in one or two tokens of an item, with lexical items often remaining isolated, but some cases show mutual influence that can lead to misinterpretations.", "conclusion": "Misalignment issues in image generation may stem from deficiencies in the textual encoding stage, suggesting a need for refinement in this area.", "key_contributions": ["Identification of critical token-level encoding patterns in T2I models", "Insights into information distribution across lexical items", "Demonstration of how misinterpretations can arise from cross-item interactions"], "limitations": "", "keywords": ["Text-to-image models", "Token representation", "Semantic information", "Image generation", "Lexical items"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2504.04310", "pdf": "https://arxiv.org/pdf/2504.04310.pdf", "abs": "https://arxiv.org/abs/2504.04310", "title": "CO-Bench: Benchmarking Language Model Agents in Algorithm Search for Combinatorial Optimization", "authors": ["Weiwei Sun", "Shengyu Feng", "Shanda Li", "Yiming Yang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Although LLM-based agents have attracted significant attention in domains\nsuch as software engineering and machine learning research, their role in\nadvancing combinatorial optimization (CO) remains relatively underexplored.\nThis gap underscores the need for a deeper understanding of their potential in\ntackling structured, constraint-intensive problems -- a pursuit currently\nlimited by the absence of comprehensive benchmarks for systematic\ninvestigation. To address this, we introduce CO-Bench, a benchmark suite\nfeaturing 36 real-world CO problems drawn from a broad range of domains and\ncomplexity levels. CO-Bench includes structured problem formulations and\ncurated data to support rigorous investigation of LLM agents. We evaluate\nmultiple agentic frameworks against established human-designed algorithms,\nrevealing the strengths and limitations of existing LLM agents and identifying\npromising directions for future research. CO-Bench is publicly available at\nhttps://github.com/sunnweiwei/CO-Bench.", "AI": {"tldr": "Introduction of CO-Bench, a benchmark suite for evaluating LLM agents on combinatorial optimization problems.", "motivation": "To explore the underutilized role of LLM-based agents in solving combinatorial optimization problems and the lack of structured benchmarks for assessment.", "method": "Development of CO-Bench, a suite comprising 36 real-world combinatorial optimization problems with structured formulations and curated data.", "result": "Evaluation of various agent frameworks against traditional algorithms, revealing strengths and limitations of LLM agents, and suggesting future research directions.", "conclusion": "CO-Bench serves as a valuable resource for systematically investigating LLM agent performance in combinatorial optimization.", "key_contributions": ["Introduction of a comprehensive benchmark suite for combinatorial optimization", "In-depth evaluation of LLM agents against human-designed algorithms", "Identification of future research directions in optimizing LLM utilization for combinatorial problems."], "limitations": "", "keywords": ["combinatorial optimization", "LLM agents", "benchmark", "evaluation", "AI research"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.07532", "pdf": "https://arxiv.org/pdf/2504.07532.pdf", "abs": "https://arxiv.org/abs/2504.07532", "title": "AI-Slop to AI-Polish? Aligning Language Models through Edit-Based Writing Rewards and Test-time Computation", "authors": ["Tuhin Chakrabarty", "Philippe Laban", "Chien-Sheng Wu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Under Submission", "summary": "AI-generated text is proliferating across domains, from creative writing and\njournalism to marketing content and scientific articles. Models can follow\nuser-provided instructions to generate coherent and grammatically correct\noutputs but in this work, we study a more fundamental question: how do we\nevaluate and improve the writing quality of AI-generated text? Writing quality\nassessment has received less attention from the community, in part because it\nis fundamentally subjective and requires expertise. We first introduce the\nWriting Quality Benchmark (WQ) by consolidating five writing-preference\ndatasets into 4,729 writing quality judgments. Our experiments show that most\nof the competitive baselines, including state-of-the-art LLMs that excel at\nreasoning tasks, barely outperform random baselines on WQ. We then train\nspecialized Writing Quality Reward Models (WQRM) of various sizes for writing\nquality assessment that demonstrate strong generalization on four\nout-of-distribution test sets and 74% accuracy on the WQ benchmark. To further\nshow WQRM's practical benefits during inference, we leverage additional\ntest-time compute to generate and rank multiple candidate revisions, allowing\nus to select higher-quality outputs from an initial draft. Human evaluation\nwith 9 experienced writers confirm that WQRM-based selection produces writing\nsamples preferred by experts 66% overall, and 72.2% when the reward gap is\nlarger than 1 point. We release our datasets and models to encourage community\nengagement with writing quality assessment and development of AI writing\nsystems better aligned with human preferences.", "AI": {"tldr": "This paper addresses the evaluation and improvement of AI-generated text quality by introducing the Writing Quality Benchmark (WQ) and specialized Writing Quality Reward Models (WQRM).", "motivation": "The rise of AI-generated content across various domains necessitates a systematic way to evaluate and improve writing quality, which has been largely overlooked in prior research.", "method": "The authors consolidate five writing-preference datasets into the Writing Quality Benchmark (WQ) with 4,729 writing quality judgments and develop specialized reward models for writing quality assessment.", "result": "The Writing Quality Reward Models (WQRM) show strong generalization across different test sets and achieve 74% accuracy on the WQ benchmark; they also help select higher-quality outputs during inference, leading to a 66% preference from experienced writers.", "conclusion": "By releasing their datasets and models, the authors aim to foster community involvement in writing quality assessment and enhance AI writing systems in accordance with human preferences.", "key_contributions": ["Introduction of the Writing Quality Benchmark (WQ) with consolidated datasets.", "Development of Writing Quality Reward Models (WQRM) that significantly outperform random baselines.", "Human evaluation validates the effectiveness of WQRM in improving the quality of AI-generated texts."], "limitations": "", "keywords": ["AI-generated text", "writing quality assessment", "benchmark", "reward models", "human evaluation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.22964", "pdf": "https://arxiv.org/pdf/2505.22964.pdf", "abs": "https://arxiv.org/abs/2505.22964", "title": "Exploring Scaling Laws for EHR Foundation Models", "authors": ["Sheng Zhang", "Qin Liu", "Naoto Usuyama", "Cliff Wong", "Tristan Naumann", "Hoifung Poon"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The emergence of scaling laws has profoundly shaped the development of large\nlanguage models (LLMs), enabling predictable performance gains through\nsystematic increases in model size, dataset volume, and compute. Yet, these\nprinciples remain largely unexplored in the context of electronic health\nrecords (EHRs) -- a rich, sequential, and globally abundant data source that\ndiffers structurally from natural language. In this work, we present the first\nempirical investigation of scaling laws for EHR foundation models. By training\ntransformer architectures on patient timeline data from the MIMIC-IV database\nacross varying model sizes and compute budgets, we identify consistent scaling\npatterns, including parabolic IsoFLOPs curves and power-law relationships\nbetween compute, model parameters, data size, and clinical utility. These\nfindings demonstrate that EHR models exhibit scaling behavior analogous to\nLLMs, offering predictive insights into resource-efficient training strategies.\nOur results lay the groundwork for developing powerful EHR foundation models\ncapable of transforming clinical prediction tasks and advancing personalized\nhealthcare.", "AI": {"tldr": "This paper investigates scaling laws for electronic health record (EHR) models, finding that they exhibit behavior similar to large language models (LLMs) regarding performance gains through model size and compute.", "motivation": "Exploring scaling laws in the context of electronic health records (EHRs), which are a distinct and valuable data source for improving clinical prediction tasks and healthcare strategies.", "method": "The authors trained transformer models on patient timeline data from the MIMIC-IV database, examining various model sizes and compute budgets to identify scaling patterns.", "result": "The study found parabolic IsoFLOPs curves and power-law relationships linking compute, model parameters, data size, and clinical utility in EHR models, indicating similar scaling behavior to LLMs.", "conclusion": "The findings suggest that EHR models can leverage insights from LLM scaling laws, informing resource-efficient training and enhancing clinical applications.", "key_contributions": ["First empirical study of scaling laws for EHR foundation models.", "Identification of consistent scaling patterns, such as IsoFLOPs curves and power-law dynamics.", "Implications for developing powerful EHR models that can improve personalized healthcare."], "limitations": "", "keywords": ["electronic health records", "scaling laws", "transformer models", "clinical prediction", "personalized healthcare"], "importance_score": 9, "read_time_minutes": 10}}
