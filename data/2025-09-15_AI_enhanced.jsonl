{"id": "2509.09815", "pdf": "https://arxiv.org/pdf/2509.09815.pdf", "abs": "https://arxiv.org/abs/2509.09815", "title": "Merging Bodies, Dividing Conflict: Body-Swapping in Mixed Reality Increases Closeness Yet Weakens the Joint Simon Effect", "authors": ["Yuan He", "Brendan Rooney", "Rachel McDonnell"], "categories": ["cs.HC"], "comment": null, "summary": "Mixed Reality (MR) presents novel opportunities to investigate how\nindividuals perceive themselves and others during shared, augmented experiences\nwithin a common physical environment. Previous research has demonstrated that\nusers can embody avatars in MR, temporarily extending their sense of self.\nHowever, there has been limited exploration of body-swapping, a condition in\nwhich two individuals simultaneously inhabit each other's avatars, and its\npotential effects on social interaction in immersive environments. To address\nthis gap, we adapted the Joint Simon Task (JST), a well-established implicit\nparadigm, to examine how body-swapping influences the cognitive and perceptual\nboundaries between self and other. Our results indicate that body-swapping led\nparticipants to experience themselves and their partner as functioning like a\nsingle, unified system, as in two bodies operating as one agent. This suggests\npossible cognitive and perceptual changes that go beyond simple collaboration.\nOur findings have significant implications for the design of MR systems\nintended to support collaboration, empathy, social learning, and therapeutic\ninterventions through shared embodiment.", "AI": {"tldr": "This paper explores the effects of body-swapping in Mixed Reality environments on social interactions and cognitive perceptions of self and others.", "motivation": "To investigate the limited exploration of body-swapping in Mixed Reality and its impact on social interactions and perception in immersive environments.", "method": "The study adapted the Joint Simon Task (JST) to examine cognitive and perceptual influences of body-swapping between two participants inhabiting each other's avatars.", "result": "Participants experienced a unified sense of self with their partner, suggesting cognitive and perceptual integration beyond mere collaboration.", "conclusion": "Body-swapping in MR can alter how individuals perceive themselves and each other, indicating design opportunities for systems enhancing collaboration and empathy.", "key_contributions": ["Modified Joint Simon Task to study body-swapping effects", "Demonstrated unified cognitive and perceptual experiences during body-swapping", "Identified implications for MR system designs in empathy and collaboration"], "limitations": "", "keywords": ["Mixed Reality", "body-swapping", "collaboration", "cognitive perception", "social interaction"], "importance_score": 7, "read_time_minutes": 12}}
{"id": "2509.09840", "pdf": "https://arxiv.org/pdf/2509.09840.pdf", "abs": "https://arxiv.org/abs/2509.09840", "title": "Designing and Evaluating AI Margin Notes in Document Reader Software", "authors": ["Nikhita Joshi", "Daniel Vogel"], "categories": ["cs.HC"], "comment": null, "summary": "AI capabilities for document reader software are usually presented in\nseparate chat interfaces. We explore integrating AI into document comments, a\nconcept we formalize as AI margin notes. Three design parameters characterize\nthis approach: margin notes are integrated with the text while chat interfaces\nare not; selecting text for a margin note can be automated through AI or\nmanual; and the generation of a margin note can involve AI to various degrees.\nTwo experiments investigate integration and selection automation, with results\nshowing participants prefer integrated AI margin notes and manual selection. A\nthird experiment explores human and AI involvement through six alternative\ntechniques. Techniques with less AI involvement resulted in more psychological\nownership, but faster and less effortful designs are generally preferred.\nSurprisingly, the degree of AI involvement had no measurable effect on reading\ncomprehension. Our work shows that AI margin notes are desirable and\ncontributes implications for their design.", "AI": {"tldr": "This paper proposes the concept of AI margin notes, integrating AI directly into document comments, with preferences for manual selection and various levels of AI involvement.", "motivation": "To explore the integration of AI into document comments to enhance reading and comprehension experiences.", "method": "Three experiments investigating integration of AI margin notes, selection automation, and human vs AI involvement techniques.", "result": "Participants preferred integrated AI margin notes and manual selection; less AI involvement increased psychological ownership but did not affect reading comprehension.", "conclusion": "AI margin notes are desirable, and their design implications suggest the need to balance AI involvement with user preferences.", "key_contributions": ["Introduction of AI margin notes concept", "Insights into user preferences for manual selection", "Analysis of psychological ownership related to AI involvement"], "limitations": "Results may vary based on different user demographics or contexts of document use.", "keywords": ["AI margin notes", "document reader software", "user experience", "HCI", "integrated AI"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2509.09870", "pdf": "https://arxiv.org/pdf/2509.09870.pdf", "abs": "https://arxiv.org/abs/2509.09870", "title": "Vibe Check: Understanding the Effects of LLM-Based Conversational Agents' Personality and Alignment on User Perceptions in Goal-Oriented Tasks", "authors": ["Hasibur Rahman", "Smit Desai"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) enable conversational agents (CAs) to express\ndistinctive personalities, raising new questions about how such designs shape\nuser perceptions. This study investigates how personality expression levels and\nuser-agent personality alignment influence perceptions in goal-oriented tasks.\nIn a between-subjects experiment (N=150), participants completed travel\nplanning with CAs exhibiting low, medium, or high expression across the Big\nFive traits, controlled via our novel Trait Modulation Keys framework. Results\nrevealed an inverted-U relationship: medium expression produced the most\npositive evaluations across Intelligence, Enjoyment, Anthropomorphism,\nIntention to Adopt, Trust, and Likeability, significantly outperforming both\nextremes. Personality alignment further enhanced outcomes, with Extraversion\nand Emotional Stability emerging as the most influential traits. Cluster\nanalysis identified three distinct compatibility profiles, with \"Well-Aligned\"\nusers reporting substantially positive perceptions. These findings demonstrate\nthat personality expression and strategic trait alignment constitute optimal\ndesign targets for CA personality, offering design implications as LLM-based\nCAs become increasingly prevalent.", "AI": {"tldr": "This study examines how personality expression levels in conversational agents affect user perceptions and task outcomes.", "motivation": "To explore how personality expression in conversational agents shapes user perceptions, particularly in goal-oriented tasks.", "method": "A between-subjects experiment with 150 participants was conducted to evaluate travel planning interactions with conversational agents exhibiting varying levels of personality expression based on the Big Five traits, using the Trait Modulation Keys framework.", "result": "The study found an inverted-U relationship in user evaluations, where medium personality expression resulted in the most positive feedback on measures like Intelligence, Enjoyment, and Trust. Compatibility of user and agent personalities also influenced outcomes, identifying three user profiles.", "conclusion": "Optimizing personality expression and aligning agent traits with user profiles can enhance user experiences in interactions with conversational agents.", "key_contributions": ["Introduced the Trait Modulation Keys framework to evaluate personality expression in conversational agents.", "Identified an inverted-U relationship between personality expression levels and user perceptions.", "Established the significance of personality alignment in enhancing user-agent interactions."], "limitations": "", "keywords": ["Conversational Agents", "Personality Expression", "User Perceptions", "Big Five Traits", "Human-Computer Interaction"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2509.09888", "pdf": "https://arxiv.org/pdf/2509.09888.pdf", "abs": "https://arxiv.org/abs/2509.09888", "title": "Climate Data for Power Systems Applications: Lessons in Reusing Wildfire Smoke Data for Solar PV Studies", "authors": ["Arleth Salinas", "Irtaza Sohail", "Valerio Pascucci", "Pantelis Stefanakis", "Saud Amjad", "Aashish Panta", "Roland Schigas", "Timothy Chun-Yiu Chui", "Nicolas Duboc", "Mostafa Farrokhabadi", "Roland Stull"], "categories": ["cs.HC"], "comment": "This paper has been accepted for the upcoming 59th Hawaii\n  International Conference on System Sciences (HICSS-59)", "summary": "Data reuse is using data for a purpose distinct from its original intent. As\ndata sharing becomes more prevalent in science, enabling effective data reuse\nis increasingly important. In this paper, we present a power systems case study\nof data repurposing for enabling data reuse. We define data repurposing as the\nprocess of transforming data to fit a new research purpose. In our case study,\nwe repurpose a geospatial wildfire smoke forecast dataset into a historical\ndataset. We analyze its efficacy toward analyzing wildfire smoke impact on\nsolar photovoltaic energy production. We also provide documentation and\ninteractive demos for using the repurposed dataset. We identify key enablers of\ndata reuse including metadata standardization, contextual documentation, and\ncommunication between data creators and reusers. We also identify obstacles to\ndata reuse such as risk of misinterpretation and barriers to efficient data\naccess. Through an iterative approach to data repurposing, we demonstrate how\nleveraging and expanding knowledge transfer infrastructures like online\ndocumentation, interactive visualizations, and data streaming directly address\nthese obstacles. The findings facilitate big data use from other domains for\npower systems applications and grid resiliency.", "AI": {"tldr": "This paper presents a case study on data repurposing for effective data reuse, specifically focusing on a geospatial wildfire smoke forecast dataset used to analyze its impact on solar energy production.", "motivation": "Effective data reuse is critical as data sharing becomes more prevalent in science; this research aims to facilitate the reuse of data by transforming it for new research purposes.", "method": "A geospatial wildfire smoke forecast dataset is transformed into a historical dataset to analyze its efficacy in studying the impact of wildfire smoke on solar photovoltaic energy production.", "result": "The study identifies key enablers for data reuse including metadata standardization and effective communication between data creators and users; it also documents the use of the repurposed dataset through interactive demos.", "conclusion": "Leveraging knowledge transfer infrastructures addresses obstacles to data reuse, enhancing the applicability of data in power systems and promoting grid resiliency.", "key_contributions": ["Introduction of data repurposing as a concept for data reuse", "Analysis of wildfire smoke's impact on solar energy production using repurposed datasets", "Identification of enablers and obstacles to data reuse in research applications."], "limitations": "", "keywords": ["data reuse", "data repurposing", "geospatial datasets", "solar energy", "wildfire smoke"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2509.09699", "pdf": "https://arxiv.org/pdf/2509.09699.pdf", "abs": "https://arxiv.org/abs/2509.09699", "title": "Structured Information Matters: Explainable ICD Coding with Patient-Level Knowledge Graphs", "authors": ["Mingyang Li", "Viktor Schlegel", "Tingting Mu", "Warren Del-Pinto", "Goran Nenadic"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Mapping clinical documents to standardised clinical vocabularies is an\nimportant task, as it provides structured data for information retrieval and\nanalysis, which is essential to clinical research, hospital administration and\nimproving patient care. However, manual coding is both difficult and\ntime-consuming, making it impractical at scale. Automated coding can\npotentially alleviate this burden, improving the availability and accuracy of\nstructured clinical data. The task is difficult to automate, as it requires\nmapping to high-dimensional and long-tailed target spaces, such as the\nInternational Classification of Diseases (ICD). While external knowledge\nsources have been readily utilised to enhance output code representation, the\nuse of external resources for representing the input documents has been\nunderexplored. In this work, we compute a structured representation of the\ninput documents, making use of document-level knowledge graphs (KGs) that\nprovide a comprehensive structured view of a patient's condition. The resulting\nknowledge graph efficiently represents the patient-centred input documents with\n23\\% of the original text while retaining 90\\% of the information. We assess\nthe effectiveness of this graph for automated ICD-9 coding by integrating it\ninto the state-of-the-art ICD coding architecture PLM-ICD. Our experiments\nyield improved Macro-F1 scores by up to 3.20\\% on popular benchmarks, while\nimproving training efficiency. We attribute this improvement to different types\nof entities and relationships in the KG, and demonstrate the improved\nexplainability potential of the approach over the text-only baseline.", "AI": {"tldr": "This paper presents a method to automate the coding of clinical documents using document-level knowledge graphs to enhance ICD coding accuracy and efficiency.", "motivation": "To improve the accuracy and availability of structured clinical data for research and patient care by automating the tedious process of manual coding.", "method": "The authors compute a structured representation of input clinical documents using document-level knowledge graphs, which are then integrated into the PLM-ICD architecture for coding.", "result": "The method achieves improved Macro-F1 scores by up to 3.20% on well-known benchmarks and enhances training efficiency, demonstrating better explainability than text-only models.", "conclusion": "Integrating knowledge graphs for document representation significantly aids in automated ICD-9 coding, providing both efficiency and accuracy gains.", "key_contributions": ["Utilization of document-level knowledge graphs for clinical document representation", "Integration with state-of-the-art ICD coding architecture PLM-ICD", "Demonstration of improved explainability in coding processes"], "limitations": "", "keywords": ["automated coding", "knowledge graphs", "ICD coding", "clinical data", "explainability"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.09910", "pdf": "https://arxiv.org/pdf/2509.09910.pdf", "abs": "https://arxiv.org/abs/2509.09910", "title": "Seeing Identity in Data: Can Anthropographics Uncover Racial Homophily in Emotional Responses?", "authors": ["Poorna Talkad Sukumar", "Maurizio Porfiri", "Oded Nov"], "categories": ["cs.HC"], "comment": null, "summary": "Racial homophily refers to the tendency of individuals to associate with\nothers of the same racial or ethnic background. A recent study found no\nevidence of racial homophily in responses to mass shooting data visualizations.\nTo increase the likelihood of detecting an effect, we redesigned the experiment\nby replacing bar charts with anthropographics and expanding the sample size. In\na crowdsourced study (N=720), we showed participants a pictograph of mass\nshooting victims in the United States, with victims from one of three racial\ngroups (Hispanic, Black, or White) highlighted. Each participant was assigned a\nvisualization highlighting either their own racial group or a different racial\ngroup, allowing us to assess the influence of racial concordance on changes in\naffect (emotion). We found that, across all conditions, racial concordance had\na modest but significant effect on changes in affect, with participants\nexperiencing greater negative affect change when viewing visualizations\nhighlighting their own race. This study provides initial evidence that racial\nhomophily can emerge in responses to data visualizations, particularly when\nusing anthropographics.", "AI": {"tldr": "This study investigates racial homophily in responses to mass shooting data visualizations, finding that individuals react more negatively when their own race is highlighted.", "motivation": "To explore whether racial homophily can influence emotional responses to data visualizations, particularly in the context of mass shooting victims.", "method": "In a crowdsourced study with 720 participants, pictographs of mass shooting victims were used, highlighting either the participant's own racial group or a different one to assess affective changes.", "result": "Racial concordance modestly and significantly influenced emotional responses, with greater negative affect noted when participants viewed their own race highlighted.", "conclusion": "This research offers initial evidence that racial homophily can significantly impact emotional responses to data visualizations, especially using anthropographics.", "key_contributions": ["Redesigned experiment using anthropographics instead of bar charts", "Expanded sample size to better detect racial concordance effects", "Initial evidence of racial homophily in response to data visualizations"], "limitations": "", "keywords": ["racial homophily", "data visualization", "emotional response", "anthropographics", "mass shooting"], "importance_score": 2, "read_time_minutes": 5}}
{"id": "2509.09700", "pdf": "https://arxiv.org/pdf/2509.09700.pdf", "abs": "https://arxiv.org/abs/2509.09700", "title": "Cross-Layer Attention Probing for Fine-Grained Hallucination Detection", "authors": ["Malavika Suresh", "Rahaf Aljundi", "Ikechukwu Nkisi-Orji", "Nirmalie Wiratunga"], "categories": ["cs.CL", "cs.AI"], "comment": "To be published at the TRUST-AI workshop, ECAI 2025", "summary": "With the large-scale adoption of Large Language Models (LLMs) in various\napplications, there is a growing reliability concern due to their tendency to\ngenerate inaccurate text, i.e. hallucinations. In this work, we propose\nCross-Layer Attention Probing (CLAP), a novel activation probing technique for\nhallucination detection, which processes the LLM activations across the entire\nresidual stream as a joint sequence. Our empirical evaluations using five LLMs\nand three tasks show that CLAP improves hallucination detection compared to\nbaselines on both greedy decoded responses as well as responses sampled at\nhigher temperatures, thus enabling fine-grained detection, i.e. the ability to\ndisambiguate hallucinations and non-hallucinations among different sampled\nresponses to a given prompt. This allows us to propose a detect-then-mitigate\nstrategy using CLAP to reduce hallucinations and improve LLM reliability\ncompared to direct mitigation approaches. Finally, we show that CLAP maintains\nhigh reliability even when applied out-of-distribution.", "AI": {"tldr": "Proposes Cross-Layer Attention Probing (CLAP) for detecting and mitigating hallucinations in Large Language Models (LLMs).", "motivation": "Address the reliability concerns regarding Large Language Models (LLMs) due to their tendency to generate inaccurate text or hallucinations.", "method": "Introduced Cross-Layer Attention Probing (CLAP), an activation probing method that analyzes LLM activations across the residual stream as a joint sequence to detect hallucinations.", "result": "CLAP outperforms baseline methods in detecting hallucinations across multiple LLMs and tasks, enabling fine-grained detection and a novel detection-mitigation strategy.", "conclusion": "CLAP enhances LLM reliability and maintains performance even with out-of-distribution applications.", "key_contributions": ["Introduction of CLAP for hallucination detection", "Improved fine-grained detection of hallucinations", "Proposed detect-then-mitigate strategy for LLM reliability"], "limitations": "", "keywords": ["Large Language Models", "hallucination detection", "activation probing", "reliability", "machine learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.09916", "pdf": "https://arxiv.org/pdf/2509.09916.pdf", "abs": "https://arxiv.org/abs/2509.09916", "title": "Immersive Invaders: Privacy Threats from Deceptive Design in Virtual Reality Games and Applications", "authors": ["Hilda Hadan", "Michaela Valiquette", "Lennart E. Nacke", "Leah Zhang-Kennedy"], "categories": ["cs.HC"], "comment": "28 pages, 4 tables, 3 figures. For related materials, see:\n  https://osf.io/axzve/", "summary": "Virtual Reality (VR) technologies offer immersive experiences but collect\nsubstantial user data. While deceptive design is well-studied in 2D platforms,\nlittle is known about its manifestation in VR environments and its impact on\nuser privacy. This research investigates deceptive designs in privacy\ncommunication and interaction mechanisms of 12 top-rated VR games and\napplications through autoethnographic evaluation of the applications and\nthematic analysis of privacy policies. We found that while many deceptive\ndesigns rely on 2D interfaces, some VR-unique features, while not directly\nenabling deception, amplified data disclosure behaviors, and obscured actual\ndata practices. Convoluted privacy policies and manipulative consent practices\nfurther hinder comprehension and increase privacy risks. We also observed\nprivacy-preserving design strategies and protective considerations in VR\nprivacy policies. We offer recommendations for ethical VR design that balance\nimmersive experiences with strong privacy protections, guiding researchers,\ndesigners, and policymakers to improve privacy in VR environments.", "AI": {"tldr": "This paper investigates deceptive designs in privacy communication within VR applications, revealing unique challenges in VR environments that amplify user data disclosure behaviors and complicate privacy comprehension.", "motivation": "Understanding how deceptive design practices in VR impact user privacy, considering the unique characteristics of immersive environments compared to traditional 2D platforms.", "method": "Autoethnographic evaluation of 12 top-rated VR games and applications, coupled with thematic analysis of privacy policies to identify deceptive practices and privacy risks.", "result": "Identified that VR technologies can obscure data practices and amplify data disclosure, with convoluted privacy policies complicating user comprehension and increasing risks.", "conclusion": "The research highlights the need for ethical design recommendations that enhance user privacy while maintaining the immersive nature of VR experiences, targeting researchers, designers, and policymakers.", "key_contributions": ["Identification of deceptive design practices unique to VR environments.", "Recommendations for ethical design in VR to protect user privacy.", "Analysis of privacy policies revealing complexities that challenge comprehension."], "limitations": "Focuses on a limited sample of 12 VR applications, which may not represent the entire VR landscape.", "keywords": ["Virtual Reality", "Deceptive Design", "User Privacy", "Privacy Policies", "Ethical Design"], "importance_score": 7, "read_time_minutes": 20}}
{"id": "2509.09701", "pdf": "https://arxiv.org/pdf/2509.09701.pdf", "abs": "https://arxiv.org/abs/2509.09701", "title": "Optimal Multi-Task Learning at Regularization Horizon for Speech Translation Task", "authors": ["JungHo Jung", "Junhyun Lee"], "categories": ["cs.CL"], "comment": null, "summary": "End-to-end speech-to-text translation typically suffers from the scarcity of\npaired speech-text data. One way to overcome this shortcoming is to utilize the\nbitext data from the Machine Translation (MT) task and perform Multi-Task\nLearning (MTL). In this paper, we formulate MTL from a regularization\nperspective and explore how sequences can be regularized within and across\nmodalities. By thoroughly investigating the effect of consistency\nregularization (different modality) and R-drop (same modality), we show how\nthey respectively contribute to the total regularization. We also demonstrate\nthat the coefficient of MT loss serves as another source of regularization in\nthe MTL setting. With these three sources of regularization, we introduce the\noptimal regularization contour in the high-dimensional space, called the\nregularization horizon. Experiments show that tuning the hyperparameters within\nthe regularization horizon achieves near state-of-the-art performance on the\nMuST-C dataset.", "AI": {"tldr": "This paper investigates regularization techniques in multi-task learning for speech-to-text translation, leveraging bitext data to improve model performance.", "motivation": "The scarcity of paired speech-text data presents challenges for end-to-end speech-to-text translation, prompting the need for alternative approaches to enhance data usage.", "method": "The paper formulates multi-task learning from a regularization perspective, exploring the effects of consistency regularization across modalities and R-drop within the same modality.", "result": "The study demonstrates that both consistency regularization and R-drop contribute positively to total regularization, with the MT loss coefficient providing additional regularization.", "conclusion": "Tuning hyperparameters within the introduced regularization horizon leads to near state-of-the-art performance on the MuST-C dataset.", "key_contributions": ["Formulating multi-task learning from a regularization viewpoint", "Investigating the impact of regularization techniques such as consistency regularization and R-drop", "Introducing the concept of the regularization horizon in high-dimensional space."], "limitations": "", "keywords": ["speech-to-text", "multi-task learning", "regularization", "bitext data", "machine translation"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2509.10003", "pdf": "https://arxiv.org/pdf/2509.10003.pdf", "abs": "https://arxiv.org/abs/2509.10003", "title": "Beyond the Silence: How Men Navigate Infertility Through Digital Communities and Data Sharing", "authors": ["Tawfiq Ammari", "Zarah Khondoker", "Yihan Wang", "Nikki Roda"], "categories": ["cs.HC"], "comment": null, "summary": "Men experiencing infertility face unique challenges navigating Traditional\nMasculinity Ideologies that discourage emotional expression and help-seeking.\nThis study examines how Reddit's r/maleinfertility community helps overcome\nthese barriers through digital support networks. Using topic modeling (115\ntopics), network analysis (11 micro-communities), and time-lagged regression on\n11,095 posts and 79,503 comments from 8,644 users, we found the community\nfunctions as a hybrid space: informal diagnostic hub, therapeutic commons, and\ngoverned institution. Medical advice dominates discourse (63.3\\%), while\nemotional support (7.4\\%) and moderation (29.2\\%) create essential\ninfrastructure. Sustained engagement correlates with actionable guidance and\naffiliation language, not emotional processing. Network analysis revealed\nstructurally cohesive but topically diverse clusters without echo chamber\ncharacteristics. Cross-posters (20\\% of users) who bridge r/maleinfertility and\nthe gender-mixed r/infertility community serve as navigators and mentors,\ntransferring knowledge between spaces. These findings inform trauma-informed\ndesign for stigmatized health communities, highlighting role-aware systems and\nnavigation support.", "AI": {"tldr": "The study examines how the Reddit community r/maleinfertility provides digital support for men facing infertility, revealing its role as a diagnostic hub and therapeutic space.", "motivation": "Men facing infertility often struggle with societal norms that discourage emotional expression and seeking help, making community support essential.", "method": "The study utilized topic modeling, network analysis, and time-lagged regression on data from 11,095 posts and 79,503 comments to analyze the community's structure and discourse.", "result": "The analysis identified the community as a hybrid space with medical advice dominating discussions, while emotional support and moderation also play critical roles.", "conclusion": "Findings suggest a need for trauma-informed design in health communities that promote emotional support and knowledge transfer among users.", "key_contributions": ["Identification of r/maleinfertility as a hybrid support space", "Insights into discourse patterns and community engagement", "Implications for trauma-informed design in stigmatized health contexts"], "limitations": "", "keywords": ["infertility", "masculinity", "support networks", "digital communities", "mental health"], "importance_score": 4, "read_time_minutes": 8}}
{"id": "2509.09702", "pdf": "https://arxiv.org/pdf/2509.09702.pdf", "abs": "https://arxiv.org/abs/2509.09702", "title": "Creativity Benchmark: A benchmark for marketing creativity for LLM models", "authors": ["Ninad Bhat", "Kieran Browne", "Pip Bingemann"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "30 Pages, 14 figures", "summary": "We introduce Creativity Benchmark, an evaluation framework for large language\nmodels (LLMs) in marketing creativity. The benchmark covers 100 brands (12\ncategories) and three prompt types (Insights, Ideas, Wild Ideas). Human\npairwise preferences from 678 practising creatives over 11,012 anonymised\ncomparisons, analysed with Bradley-Terry models, show tightly clustered\nperformance with no model dominating across brands or prompt types: the\ntop-bottom spread is $\\Delta\\theta \\approx 0.45$, which implies a head-to-head\nwin probability of $0.61$; the highest-rated model beats the lowest only about\n$61\\%$ of the time. We also analyse model diversity using cosine distances to\ncapture intra- and inter-model variation and sensitivity to prompt reframing.\nComparing three LLM-as-judge setups with human rankings reveals weak,\ninconsistent correlations and judge-specific biases, underscoring that\nautomated judges cannot substitute for human evaluation. Conventional\ncreativity tests also transfer only partially to brand-constrained tasks.\nOverall, the results highlight the need for expert human evaluation and\ndiversity-aware workflows.", "AI": {"tldr": "This paper presents the Creativity Benchmark, a framework for evaluating large language models in the context of marketing creativity, revealing the limitations of automated judges in comparison to human evaluations.", "motivation": "To provide a systematic evaluation of large language models' capabilities in generating creative marketing content and to highlight the need for expert human evaluation in this domain.", "method": "The study employs a framework covering 100 brands and three types of prompts, using pairwise comparisons from 678 creatives analyzed through Bradley-Terry models, and measures model diversity through cosine distances.", "result": "Performance among models was tightly clustered, with the highest-ranked model winning only 61% of head-to-head comparisons against the lowest-ranked, indicating no clear superior model across brands or prompts.", "conclusion": "The findings emphasize the necessity for expert human evaluation in creative tasks and warn against relying solely on automated assessments, which may exhibit biases and inconsistencies.", "key_contributions": ["Introduction of Creativity Benchmark framework for evaluating LLMs in marketing creativity.", "Empirical analysis of model performance revealing lack of dominance among models.", "Identification of biases and limitations of automated judging systems compared to human evaluators."], "limitations": "The study shows that automated judges have weak correlations with human rankings and that conventional creativity tests only partially apply to brand-specific tasks.", "keywords": ["Creativity Benchmark", "Large Language Models", "Human Evaluation", "Marketing Creativity", "Model Diversity"], "importance_score": 6, "read_time_minutes": 30}}
{"id": "2509.10015", "pdf": "https://arxiv.org/pdf/2509.10015.pdf", "abs": "https://arxiv.org/abs/2509.10015", "title": "A Framework for AI-Supported Mediation in Community-based Online Collaboration", "authors": ["Soobin Cho", "Mark Zachry", "David W. McDonald"], "categories": ["cs.HC"], "comment": null, "summary": "Online spaces involve diverse communities engaging in various forms of\ncollaboration, which naturally give rise to discussions, some of which\ninevitably escalate into conflict or disputes. To address such situations, AI\nhas primarily been used for moderation. While moderation systems are important\nbecause they help maintain order, common moderation strategies of removing or\nsuppressing content and users rarely address the underlying disagreements or\nthe substantive content of disputes. Mediation, by contrast, fosters\nunderstanding, reduces emotional tension, and facilitates consensus through\nguided negotiation. Mediation not only enhances the quality of collaborative\ndecisions but also strengthens relationships among group members. For this\nreason, we argue for shifting focus toward AI-supported mediation. In this\nwork, we propose an information-focused framework for AI-supported mediation\ndesigned for community-based collaboration. Within this framework, we\nhypothesize that AI must acquire and reason over three key types of\ninformation: content, culture, and people.", "AI": {"tldr": "The paper advocates for AI-supported mediation in online communities to resolve conflicts more effectively than traditional moderation systems.", "motivation": "To improve conflict resolution in online communities by moving from content moderation to mediation, which fosters understanding and enhances relationships.", "method": "Proposes an information-focused framework for AI-supported mediation based on three key types of information: content, culture, and people.", "result": "The framework aims to enhance collaborative decision-making and reduce emotional tensions in community interactions.", "conclusion": "AI can play a crucial role in mediation by understanding and reasoning about the complexities of disputes within online communities.", "key_contributions": ["Introduction of AI-supported mediation as a new focus for conflict resolution in online communities.", "Development of an information-focused framework to guide AI in mediation tasks.", "Identification of key information types that AI needs to reason about in mediation: content, culture, and people."], "limitations": "", "keywords": ["AI", "mediation", "online communities", "conflict resolution", "collaboration"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.09703", "pdf": "https://arxiv.org/pdf/2509.09703.pdf", "abs": "https://arxiv.org/abs/2509.09703", "title": "CTCC: A Robust and Stealthy Fingerprinting Framework for Large Language Models via Cross-Turn Contextual Correlation Backdoor", "authors": ["Zhenhua Xu", "Xixiang Zhao", "Xubin Yue", "Shengwei Tian", "Changting Lin", "Meng Han"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by EMNLP2025 MainConference", "summary": "The widespread deployment of large language models (LLMs) has intensified\nconcerns around intellectual property (IP) protection, as model theft and\nunauthorized redistribution become increasingly feasible. To address this,\nmodel fingerprinting aims to embed verifiable ownership traces into LLMs.\nHowever, existing methods face inherent trade-offs between stealthness,\nrobustness, and generalizability, being either detectable via distributional\nshifts, vulnerable to adversarial modifications, or easily invalidated once the\nfingerprint is revealed. In this work, we introduce CTCC, a novel rule-driven\nfingerprinting framework that encodes contextual correlations across multiple\ndialogue turns, such as counterfactual, rather than relying on token-level or\nsingle-turn triggers. CTCC enables fingerprint verification under black-box\naccess while mitigating false positives and fingerprint leakage, supporting\ncontinuous construction under a shared semantic rule even if partial triggers\nare exposed. Extensive experiments across multiple LLM architectures\ndemonstrate that CTCC consistently achieves stronger stealth and robustness\nthan prior work. Our findings position CTCC as a reliable and practical\nsolution for ownership verification in real-world LLM deployment scenarios. Our\ncode and data are publicly available at <https://github.com/Xuzhenhua55/CTCC>.", "AI": {"tldr": "CTCC introduces a rule-driven fingerprinting framework for LLMs that enhances ownership verification by addressing trade-offs in stealth and robustness.", "motivation": "Concern over intellectual property protection in the deployment of large language models due to risks of model theft and unauthorized redistribution.", "method": "CTCC encodes contextual correlations across multiple dialogue turns rather than using single-turn triggers, allowing for fingerprint verification under black-box conditions while reducing false positives and preventing leakage.", "result": "CTCC outperforms existing methods in both stealth and robustness across various LLM architectures.", "conclusion": "CTCC is a reliable framework for ownership verification suitable for real-world applications of LLMs.", "key_contributions": ["Introduction of a novel rule-driven fingerprinting framework (CTCC) for LLMs", "Enhanced robustness and stealth for ownership verification", "Public availability of code and data for further research"], "limitations": "", "keywords": ["large language models", "fingerprinting", "ownership verification", "HCI", "dialogue context"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2509.10043", "pdf": "https://arxiv.org/pdf/2509.10043.pdf", "abs": "https://arxiv.org/abs/2509.10043", "title": "Inclusive by design: Developing Barrier-Free Authentication for Blind and Low Vision Users through the ALIAS Project", "authors": ["Clara Toussaint", "Benjamin Chateau", "Pierre-Guillaume Gourio-Jewell", "Emilie Bonnefoy", "Nicolas Louveton"], "categories": ["cs.HC"], "comment": null, "summary": "Authentication is the cornerstone of information security in our daily lives.\nHowever, disabled users such as Blind and Low-Vision (BLV) ones are left behind\nin digital services due to the lack of accessibility. According to the World\nHealth Organization, 36 million people are blind worldwide. It is estimated\nthat there will be 115 million by 2050, due to the ageing of the population.\nYet accessing digital services has become increasingly essential. At the same\ntime, cyber threats targeting individuals have also increased strongly in the\nlast few years. The ALIAS project addresses the need for accessible digital\nauthentication solutions for BLV users facing challenges with digital\ntechnology. Security systems can inhibit access for these individuals as they\nbecome more complex. This project aims to create a barrier-free authentication\nsystem based on cognitive ergonomics and user experience (UX) design methods\nspecifically for BLV users. This paper presents an overview of current research\nin this area. We also identify research gaps, and finally, we present our\nproject's methodology and approach. First, we will build a knowledge base on\nthe digital practices and cognitive models of BLV users during authentication.\nThis information will support the development of prototypes, which will be\ntested and refined through two iterations before finalizing the operational\nversion.", "AI": {"tldr": "This paper discusses the ALIAS project, aiming to create accessible digital authentication solutions for Blind and Low-Vision (BLV) users by applying cognitive ergonomics and user experience design methodologies.", "motivation": "To address the accessibility challenges faced by Blind and Low-Vision (BLV) individuals in digital authentication processes, as they are often excluded from essential online services due to security complexities.", "method": "The project will build a knowledge base on the digital practices and cognitive models of BLV users and develop prototypes that will undergo two iterations of testing and refinement.", "result": "The research reviews current challenges in digital authentication for BLV users and identifies significant gaps in existing solutions, leading to proposed methods for improvement.", "conclusion": "Creating a barrier-free authentication system is crucial for inclusivity in digital services, particularly for the increasing population of BLV users.", "key_contributions": ["Overview of current research on BLV accessibility in digital authentication.", "Identification of research gaps in existing solutions.", "Methodology for developing and testing prototypes tailored for BLV users."], "limitations": "", "keywords": ["Blind and Low-Vision", "digital authentication", "accessibility", "cognitive ergonomics", "user experience design"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.09704", "pdf": "https://arxiv.org/pdf/2509.09704.pdf", "abs": "https://arxiv.org/abs/2509.09704", "title": "Temporal Preferences in Language Models for Long-Horizon Assistance", "authors": ["Ali Mazyaki", "Mohammad Naghizadeh", "Samaneh Ranjkhah Zonouzaghi", "Hossein Setareh"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "We study whether language models (LMs) exhibit future- versus\npresent-oriented preferences in intertemporal choice and whether those\npreferences can be systematically manipulated. Using adapted human experimental\nprotocols, we evaluate multiple LMs on time-tradeoff tasks and benchmark them\nagainst a sample of human decision makers. We introduce an operational metric,\nthe Manipulability of Time Orientation (MTO), defined as the change in an LM's\nrevealed time preference between future- and present-oriented prompts. In our\ntests, reasoning-focused models (e.g., DeepSeek-Reasoner and grok-3-mini)\nchoose later options under future-oriented prompts but only partially\npersonalize decisions across identities or geographies. Moreover, models that\ncorrectly reason about time orientation internalize a future orientation for\nthemselves as AI decision makers. We discuss design implications for AI\nassistants that should align with heterogeneous, long-horizon goals and outline\na research agenda on personalized contextual calibration and socially aware\ndeployment.", "AI": {"tldr": "The paper examines how language models (LMs) exhibit preferences in intertemporal choice and explores ways to manipulate these preferences using human experimental protocols.", "motivation": "To understand whether LMs have future- versus present-oriented preferences in decision making and how these preferences can be altered.", "method": "Language models were evaluated using time-tradeoff tasks based on human experimental protocols, and a new metric, Manipulability of Time Orientation (MTO), was introduced to assess changes in LM preferences based on prompts.", "result": "Reasoning-focused models showed a tendency to prefer future options when prompted accordingly, but personalization of decisions across different identities was only partial. Correctly reasoning models demonstrated a self-internalized future orientation.", "conclusion": "AI assistants must be designed to align with diverse long-term goals, highlighting the need for personalized contextual calibration and socially aware deployment strategies.", "key_contributions": ["Introduction of Manipulability of Time Orientation (MTO) metric", "Experimental comparison of LMs with human decision makers", "Insights on AI assistant design for varied long-term preferences"], "limitations": "", "keywords": ["language models", "intertemporal choice", "human decision making", "AI assistants", "time orientation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.10064", "pdf": "https://arxiv.org/pdf/2509.10064.pdf", "abs": "https://arxiv.org/abs/2509.10064", "title": "From customer survey feedback to software improvements: Leveraging the full potential of data", "authors": ["Erik Bertram", "Nina Hollender", "Sebastian Juhl", "Sandra Loop", "Martin Schrepp"], "categories": ["cs.HC"], "comment": "10 pages, 8 figures, published in Springer Nature", "summary": "Converting customer survey feedback data into usable insights has always been\na great challenge for large software enterprises. Despite the improvements on\nthis field, a major obstacle often remains when drawing the right conclusions\nout of the data and channeling them into the software development process. In\nthis paper we present a practical end-to-end approach of how to extract useful\ninformation out of a data set and leverage the information to drive change. We\ndescribe how to choose the right metrics to measure, gather appropriate\nfeedback from customer end-users, analyze the data by leveraging methods from\ninferential statistics, make the data transparent, and finally drive change\nwith the results. Furthermore, we present an example of a UX prototype\ndashboard that can be used to communicate the analyses to stakeholders within\nthe company.", "AI": {"tldr": "This paper presents an end-to-end approach for transforming customer survey feedback into actionable insights for software development.", "motivation": "Large software enterprises struggle to convert customer survey feedback into usable insights, affecting their software development processes.", "method": "The approach involves selecting appropriate metrics, gathering feedback from users, analyzing data using inferential statistics, ensuring data transparency, and implementing changes based on the results.", "result": "A UX prototype dashboard is demonstrated that communicates analyses effectively to stakeholders, facilitating data-driven decisions.", "conclusion": "The methodology and dashboard provided can significantly enhance the ability of organizations to iterate on their software based on user feedback.", "key_contributions": ["End-to-end approach for data analysis and change implementation based on survey feedback.", "Development of a prototype dashboard for stakeholder communication.", "Use of inferential statistics to derive meaningful insights from customer feedback."], "limitations": "", "keywords": ["customer feedback", "data analysis", "software development", "UX design", "stakeholder communication"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2509.09705", "pdf": "https://arxiv.org/pdf/2509.09705.pdf", "abs": "https://arxiv.org/abs/2509.09705", "title": "The Non-Determinism of Small LLMs: Evidence of Low Answer Consistency in Repetition Trials of Standard Multiple-Choice Benchmarks", "authors": ["Claudio Pinhanez", "Paulo Cavalin", "Cassia Sanctos", "Marcelo Grave", "Yago Primerano"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This work explores the consistency of small LLMs (2B-8B parameters) in\nanswering multiple times the same question. We present a study on known,\nopen-source LLMs responding to 10 repetitions of questions from the\nmultiple-choice benchmarks MMLU-Redux and MedQA, considering different\ninference temperatures, small vs. medium models (50B-80B), finetuned vs. base\nmodels, and other parameters. We also look into the effects of requiring\nmulti-trial answer consistency on accuracy and the trade-offs involved in\ndeciding which model best provides both of them. To support those studies, we\npropose some new analytical and graphical tools. Results show that the number\nof questions which can be answered consistently vary considerably among models\nbut are typically in the 50%-80% range for small models at low inference\ntemperatures. Also, accuracy among consistent answers seems to reasonably\ncorrelate with overall accuracy. Results for medium-sized models seem to\nindicate much higher levels of answer consistency.", "AI": {"tldr": "This study investigates the consistency of small LLMs in responding to repeated questions across multiple benchmarks, analyzing various model parameters and their impact on accuracy.", "motivation": "To understand how small LLMs maintain consistency in answers when queried multiple times, and to assess the balance between consistency and accuracy in model performance.", "method": "The study involved testing known open-source small LLMs on 10 repetitions of questions from the MMLU-Redux and MedQA benchmarks. It explored the effects of different inference temperatures, the size of models, and fine-tuning on answer consistency.", "result": "Results indicated that small models achieve 50%-80% consistency on repeated questions at low inference temperatures, with medium models showing significantly higher consistency levels.", "conclusion": "The findings highlight variability in answer consistency across models and suggest that higher accuracy is generally correlated with consistent responses.", "key_contributions": ["Proposed new analytical and graphical tools to study LLM consistency", "Provided insights into the trade-offs between answer consistency and accuracy in LLMs", "Delivered empirical data on consistency levels of small LLMs across multiple test scenarios."], "limitations": "", "keywords": ["LLMs", "consistency", "accuracy", "benchmarking", "machine learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.10081", "pdf": "https://arxiv.org/pdf/2509.10081.pdf", "abs": "https://arxiv.org/abs/2509.10081", "title": "Understanding Expert Exploration in EHR Visualization Tools: The ParcoursVis Use Case", "authors": ["Ambre Assor", "Jean-Daniel Fekete"], "categories": ["cs.HC"], "comment": null, "summary": "We introduce our ongoing work toward an insight-based evaluation methodology\naimed at understanding practitioners' mental models when exploring medical\ndata. It is based on ParcoursVis, a Progressive Visual Analytics system\ndesigned to visualize event sequences derived from Electronic Health Records at\nscale (millions of patients, billions of events), developed in collaboration\nwith the Emergency Departments of 16 Parisian hospitals and with the French\nSocial Security. Building on prior usability validation, our current evaluation\nfocuses on the insights generated by expert users and aims to better understand\nthe exploration strategies they employ when engaging with exploration\nvisualization tools. We describe our system and outline our evaluation\nprotocol, analysis strategy, and preliminary findings. Building on this\napproach and our pilot results, we contribute a design protocol for conducting\ninsight-based studies under real-world constraints, including the availability\nof health practitioners whom we were fortunate to interview. Our findings\nhighlight a loop, where the use of the system helps refine data variables\nidentification and the system itself. We aim to shed light on generated\ninsights, to highlight the utility of exploratory tools in health data analysis\ncontexts.", "AI": {"tldr": "The paper presents an ongoing evaluation methodology to understand practitioners' mental models in exploring medical data using a visual analytics system called ParcoursVis.", "motivation": "To understand how practitioners explore and derive insights from medical data visualizations.", "method": "The study involves an analysis of expert users' strategies for exploring medical data using the ParcoursVis system, developed in collaboration with hospitals and health authorities.", "result": "Preliminary findings indicate that using the system refines data variable identification and the design of the system itself, enhancing insights during exploration.", "conclusion": "The research contributes a design protocol for conducting insight-based evaluations under real constraints, highlighting the importance of exploratory visualization tools in health data analysis.", "key_contributions": ["Introduction of an insight-based evaluation methodology for health data exploration.", "Development of the ParcoursVis system for visualizing electronic health records.", "Contribution of a design protocol for conducting evaluations in real-world health settings."], "limitations": "", "keywords": ["health data visualization", "insight-based evaluation", "electronic health records"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.09708", "pdf": "https://arxiv.org/pdf/2509.09708.pdf", "abs": "https://arxiv.org/abs/2509.09708", "title": "Beyond I'm Sorry, I Can't: Dissecting Large Language Model Refusal", "authors": ["Nirmalendu Prakash", "Yeo Wei Jie", "Amir Abdullah", "Ranjan Satapathy", "Erik Cambria", "Roy Ka Wei Lee"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Refusal on harmful prompts is a key safety behaviour in instruction-tuned\nlarge language models (LLMs), yet the internal causes of this behaviour remain\npoorly understood. We study two public instruction-tuned models, Gemma-2-2B-IT\nand LLaMA-3.1-8B-IT, using sparse autoencoders (SAEs) trained on\nresidual-stream activations. Given a harmful prompt, we search the SAE latent\nspace for feature sets whose ablation flips the model from refusal to\ncompliance, demonstrating causal influence and creating a jailbreak. Our search\nproceeds in three stages: (1) Refusal Direction: find a refusal-mediating\ndirection and collect SAE features near that direction; (2) Greedy Filtering:\nprune to a minimal set; and (3) Interaction Discovery: fit a factorization\nmachine (FM) that captures nonlinear interactions among the remaining active\nfeatures and the minimal set. This pipeline yields a broad set of\njailbreak-critical features, offering insight into the mechanistic basis of\nrefusal. Moreover, we find evidence of redundant features that remain dormant\nunless earlier features are suppressed. Our findings highlight the potential\nfor fine-grained auditing and targeted intervention in safety behaviours by\nmanipulating the interpretable latent space.", "AI": {"tldr": "This paper investigates the internal mechanisms behind refusal behaviors in instruction-tuned large language models using sparse autoencoders to identify features influencing these behaviors.", "motivation": "Understanding refusal behaviors in instruction-tuned LLMs is crucial for enhancing safety measures and interventions.", "method": "The study employs sparse autoencoders to analyze residual-stream activations of two models, conducting a three-stage search for features influencing model compliance versus refusal to harmful prompts.", "result": "The results reveal critical features affecting refusal behavior and indicate the presence of redundant features that may activate upon suppression of others, providing a basis for effective auditing and intervention strategies.", "conclusion": "The findings contribute valuable insights into the mechanisms behind refusal behaviors in LLMs, facilitating potential improvements in safety audits and interventions.", "key_contributions": ["Identification of jailbreak-critical features in LLMs", "Demonstration of causal influence between features and refusal behavior", "Insights into the interplay of redundant features in LLM safety responses"], "limitations": "", "keywords": ["large language models", "sparse autoencoders", "safety behavior", "refusal mechanisms", "jailbreak"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.10327", "pdf": "https://arxiv.org/pdf/2509.10327.pdf", "abs": "https://arxiv.org/abs/2509.10327", "title": "MusicScaffold: Bridging Machine Efficiency and Human Growth in Adolescent Creative Education through Generative AI", "authors": ["Zhejing Hu", "Yan Liu", "Zhi Zhang", "Gong Chen", "Bruce X. B. Yu", "Junxian Li", "Jiannong Cao"], "categories": ["cs.HC"], "comment": null, "summary": "Adolescence is marked by strong creative impulses but limited strategies for\nstructured expression, often leading to frustration or disengagement. While\ngenerative AI lowers technical barriers and delivers efficient outputs, its\nrole in fostering adolescents' expressive growth has been overlooked. We\npropose MusicScaffold, the first adolescent-centered framework that repositions\nAI as a guide, coach, and partner, making expressive strategies transparent and\nlearnable, and supporting autonomy. In a four-week study with middle school\nstudents (ages 12--14), MusicScaffold enhanced cognitive specificity,\nbehavioral self-regulation, and affective confidence in music creation. By\nreframing generative AI as a scaffold rather than a generator, this work\nbridges the machine efficiency of generative systems with human growth in\nadolescent creative education.", "AI": {"tldr": "MusicScaffold is a framework designed to help adolescents enhance their creative expression in music by utilizing generative AI as a supportive tool rather than just a generator.", "motivation": "To address the challenge of limited structured expression in adolescents and leverage generative AI to foster expressive growth.", "method": "A four-week study conducted with middle school students gs 12-14, using MusicScaffold to guide their music creation process.", "result": "Participants showed improved cognitive specificity, behavioral self-regulation, and affective confidence during music creation activities.", "conclusion": "Reframing generative AI as a supportive scaffold helps bridge machine efficiency with human creative growth in education.", "key_contributions": ["Introduction of MusicScaffold framework for adolescents.", "Demonstrated positive impact of AI on creative expression.", "Shifted the perception of generative AI from a generator to a supportive guide."], "limitations": "", "keywords": ["Generative AI", "Adolescent education", "Creative expression", "Music creation", "Self-regulation"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2509.09709", "pdf": "https://arxiv.org/pdf/2509.09709.pdf", "abs": "https://arxiv.org/abs/2509.09709", "title": "Assisting Research Proposal Writing with Large Language Models: Evaluation and Refinement", "authors": ["Jing Ren", "Weiqi Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) like ChatGPT are increasingly used in academic\nwriting, yet issues such as incorrect or fabricated references raise ethical\nconcerns. Moreover, current content quality evaluations often rely on\nsubjective human judgment, which is labor-intensive and lacks objectivity,\npotentially compromising the consistency and reliability. In this study, to\nprovide a quantitative evaluation and enhance research proposal writing\ncapabilities of LLMs, we propose two key evaluation metrics--content quality\nand reference validity--and an iterative prompting method based on the scores\nderived from these two metrics. Our extensive experiments show that the\nproposed metrics provide an objective, quantitative framework for assessing\nChatGPT's writing performance. Additionally, iterative prompting significantly\nenhances content quality while reducing reference inaccuracies and\nfabrications, addressing critical ethical challenges in academic contexts.", "AI": {"tldr": "This study proposes metrics for evaluating large language models in academic writing, addressing issues of content quality and reference validity.", "motivation": "Addressing ethical concerns in academic writing with LLMs, especially regarding incorrect references and subjective quality evaluations.", "method": "Introduction of two evaluation metricscontent quality and reference validityand an iterative prompting method to improve LLM output.", "result": "The proposed metrics yield an objective evaluation framework, and iterative prompting enhances writing quality and reduces inaccuracies.", "conclusion": "This framework provides a solution to the ethical challenges of using LLMs in academia by ensuring better content accuracy and reliability.", "key_contributions": ["Development of content quality and reference validity metrics", "Introduction of iterative prompting method", "Objective evaluation of LLM's writing performance"], "limitations": "", "keywords": ["large language models", "academic writing", "content quality", "reference validity", "iterative prompting"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.10331", "pdf": "https://arxiv.org/pdf/2509.10331.pdf", "abs": "https://arxiv.org/abs/2509.10331", "title": "Who Decides How Knowing Becomes Doing? Redistributing Authority in Human-AI Music Co-Creation", "authors": ["Zhejing Hu", "Yan Liu", "Zhi Zhang", "Gong Chen", "Bruce X. B. Yu", "Jiannong Cao"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "In the era of human-AI co-creation, the maxim \"knowing is easy, doing is\nhard\" is redefined. AI has the potential to ease execution, yet the essence of\n\"hard\" lies in who governs the translation from knowing to doing. Mainstream\ntools often centralize interpretive authority and homogenize expression,\nsuppressing marginal voices. To address these challenges, we introduce the\nfirst systematic framework for redistributing authority in the knowing-doing\ncycle, built on three principles, namely contestability, agency, and plurality.\nThrough interactive studies with 180 music practitioners, complemented by\nin-depth interviews, we demonstrate that these principles reshape human-AI\nauthority relations and reactivate human creative expression. The findings\nestablish a new paradigm for critical computing and human-AI co-creation that\nadvances from critique to practice.", "AI": {"tldr": "This paper introduces a framework for redistributing authority in the human-AI co-creation cycle, addressing the challenge of interpretive authority in the context of creativity.", "motivation": "To explore how AI can facilitate creative expression while ensuring that authority is not centralized, preventing marginal voices from being heard.", "method": "The study involved interactive research with 180 music practitioners and in-depth interviews to investigate authority relations in human-AI collaboration.", "result": "The implementation of the framework based on contestability, agency, and plurality reshapes how human-AI authority dynamics function and enhances creative expression.", "conclusion": "A new paradigm for critical computing and human-AI co-creation is established, moving from critique to actionable practice.", "key_contributions": ["Introduced a systematic framework for authority redistribution in human-AI co-creation.", "Provided empirical insights through interactive studies and interviews with music practitioners.", "Advanced theoretical discussions on critical computing in the context of AI."], "limitations": "", "keywords": ["Human-AI co-creation", "Authority", "Creative expression", "Machine Learning", "Critical computing"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.09710", "pdf": "https://arxiv.org/pdf/2509.09710.pdf", "abs": "https://arxiv.org/abs/2509.09710", "title": "Generating Individual Travel Diaries Using Large Language Models Informed by Census and Land-Use Data", "authors": ["Sepehr Golrokh Amin", "Devin Rhoads", "Fatemeh Fakhrmoosavi", "Nicholas E. Lownes", "John N. Ivan"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This study introduces a Large Language Model (LLM) scheme for generating\nindividual travel diaries in agent-based transportation models. While\ntraditional approaches rely on large quantities of proprietary household travel\nsurveys, the method presented in this study generates personas stochastically\nfrom open-source American Community Survey (ACS) and Smart Location Database\n(SLD) data, then synthesizes diaries through direct prompting. This study\nfeatures a novel one-to-cohort realism score: a composite of four metrics (Trip\nCount Score, Interval Score, Purpose Score, and Mode Score) validated against\nthe Connecticut Statewide Transportation Study (CSTS) diaries, matched across\ndemographic variables. The validation utilizes Jensen-Shannon Divergence to\nmeasure distributional similarities between generated and real diaries. When\ncompared to diaries generated with classical methods (Negative Binomial for\ntrip generation; Multinomial Logit for mode/purpose) calibrated on the\nvalidation set, LLM-generated diaries achieve comparable overall realism (LLM\nmean: 0.485 vs. 0.455). The LLM excels in determining trip purpose and\ndemonstrates greater consistency (narrower realism score distribution), while\nclassical models lead in numerical estimates of trip count and activity\nduration. Aggregate validation confirms the LLM's statistical\nrepresentativeness (LLM mean: 0.612 vs. 0.435), demonstrating LLM's zero-shot\nviability and establishing a quantifiable metric of diary realism for future\nsynthetic diary evaluation systems.", "AI": {"tldr": "The study presents a Large Language Model (LLM) approach for generating realistic travel diaries using open-source data, validating against traditional survey methods.", "motivation": "To create a methodology for generating individual travel diaries without relying on proprietary household travel surveys, thus improving accessibility and realism in agent-based transportation models.", "method": "The study uses open-source ACS and SLD data to generate personas and synthesizes travel diaries through LLM prompting, validated using Jensen-Shannon Divergence to ensure accuracy against real diaries.", "result": "LLM-generated diaries achieved a realism score of 0.485 compared to 0.455 for classical methods, excelling in trip purpose determination and consistency.", "conclusion": "The LLM shows promise for generating realistic travel diaries with a quantifiable realism metric, outperforming traditional methods in some aspects while still needing improvement in trip count and duration estimates.", "key_contributions": ["Introduces LLM for generating travel diaries", "Develops a novel realism score based on specific metrics", "Validates the approach against established survey data"], "limitations": "Performance in numerical estimates of trip counts and activity durations is lower compared to classical models.", "keywords": ["Large Language Model", "travel diaries", "agent-based models", "realism score", "validation"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.10370", "pdf": "https://arxiv.org/pdf/2509.10370.pdf", "abs": "https://arxiv.org/abs/2509.10370", "title": "The Language of Approval: Identifying the Drivers of Positive Feedback Online", "authors": ["Agam Goyal", "Charlotte Lambert", "Eshwar Chandrasekharan"], "categories": ["cs.HC"], "comment": "Preprint: 21 pages, 7 figures, 7 tables", "summary": "Positive feedback via likes and awards is central to online governance, yet\nwhich attributes of users' posts elicit rewards -- and how these vary across\nauthors and communities -- remains unclear. To examine this, we combine\nquasi-experimental causal inference with predictive modeling on 11M posts from\n100 subreddits. We identify linguistic patterns and stylistic attributes\ncausally linked to rewards, controlling for author reputation, timing, and\ncommunity context. For example, overtly complicated language, tentative style,\nand toxicity reduce rewards. We use our set of curated features to train models\nthat can detect highly-upvoted posts with high AUC. Our audit of community\nguidelines highlights a ``policy-practice gap'' -- most rules focus primarily\non civility and formatting requirements, with little emphasis on the attributes\nidentified to drive positive feedback. These results inform the design of\ncommunity guidelines, support interfaces that teach users how to craft\ndesirable contributions, and moderation workflows that emphasize positive\nreinforcement over purely punitive enforcement.", "AI": {"tldr": "This paper analyzes linguistic and stylistic attributes of posts that elicit positive feedback on Reddit, using data from 11M posts across 100 subreddits.", "motivation": "To understand which attributes of user posts lead to positive feedback in online governance, especially in community settings like Reddit.", "method": "The study utilizes quasi-experimental causal inference and predictive modeling techniques to analyze a dataset of 11 million posts from 100 different subreddits, focusing on linguistic patterns linked to rewards.", "result": "The analysis reveals that complicated language, tentative style, and toxic content negatively impact rewards, while certain other features can predict high upvotes with a high AUC.", "conclusion": "The findings call for a reevaluation of community guidelines, suggesting they should emphasize attributes that drive positive feedback rather than just civility and formatting.", "key_contributions": ["Identification of linguistic and stylistic attributes affecting rewards on user posts", "Development of predictive models for detecting high-upvoted posts", "Highlighting the gap between community policy and actual user behavior"], "limitations": "", "keywords": ["Human-Computer Interaction", "Positive Feedback", "Community Guidelines", "Linguistic Attributes", "Predictive Modeling"], "importance_score": 7, "read_time_minutes": 20}}
{"id": "2509.09711", "pdf": "https://arxiv.org/pdf/2509.09711.pdf", "abs": "https://arxiv.org/abs/2509.09711", "title": "Psychiatry-Bench: A Multi-Task Benchmark for LLMs in Psychiatry", "authors": ["Aya E. Fouda", "Abdelrahamn A. Hassan", "Radwa J. Hanafy", "Mohammed E. Fouda"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) hold great promise in enhancing psychiatric\npractice, from improving diagnostic accuracy to streamlining clinical\ndocumentation and therapeutic support. However, existing evaluation resources\nheavily rely on small clinical interview corpora, social media posts, or\nsynthetic dialogues, which limits their clinical validity and fails to capture\nthe full complexity of psychiatric reasoning. In this work, we introduce\nPsychiatryBench, a rigorously curated benchmark grounded exclusively in\nauthoritative, expert-validated psychiatric textbooks and casebooks.\nPsychiatryBench comprises eleven distinct question-answering tasks ranging from\ndiagnostic reasoning and treatment planning to longitudinal follow-up,\nmanagement planning, clinical approach, sequential case analysis, and\nmultiple-choice/extended matching formats totaling over 5,300 expert-annotated\nitems. We evaluate a diverse set of frontier LLMs (including Google Gemini,\nDeepSeek, LLaMA 3, and QWQ-32) alongside leading open-source medical models\n(e.g., OpenBiloLLM, MedGemma) using both conventional metrics and an\n\"LLM-as-judge\" similarity scoring framework. Our results reveal substantial\ngaps in clinical consistency and safety, particularly in multi-turn follow-up\nand management tasks, underscoring the need for specialized model tuning and\nmore robust evaluation paradigms. PsychiatryBench offers a modular, extensible\nplatform for benchmarking and improving LLM performance in high-stakes mental\nhealth applications.", "AI": {"tldr": "This paper presents PsychiatryBench, a benchmark for evaluating large language models (LLMs) in psychiatric practice, highlighting gaps in clinical consistency and safety in multi-turn management tasks.", "motivation": "To address the limitations of existing evaluation resources for LLMs in psychiatry, which often rely on small or inadequate datasets, thereby lacking clinical validity.", "method": "Introduction of PsychiatryBench, a benchmark comprising eleven question-answering tasks based on expert-validated psychiatric texts, evaluated using various LLMs and a unique similarity scoring framework.", "result": "Evaluation results show significant inconsistencies and safety issues in LLM performance, especially in multi-turn follow-up and management tasks.", "conclusion": "PsychiatryBench serves as a modular platform for enhancing LLM performance in mental health applications and emphasizes the necessity of specialized tuning and evaluation methodologies.", "key_contributions": ["Introduction of PsychiatryBench as a new benchmark for LLMs in psychiatry.", "Development of a unique 'LLM-as-judge' similarity scoring framework.", "Identification of significant performance gaps in existing LLM applications in psychiatric contexts."], "limitations": "The evaluation may still not cover all aspects of psychiatric reasoning, and results are constrained to the models tested.", "keywords": ["large language models", "psychiatry", "benchmark", "evaluation", "mental health"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.10427", "pdf": "https://arxiv.org/pdf/2509.10427.pdf", "abs": "https://arxiv.org/abs/2509.10427", "title": "My Favorite Streamer is an LLM: Discovering, Bonding, and Co-Creating in AI VTuber Fandom", "authors": ["Jiayi Ye", "Chaoran Chen", "Yue Huang", "Yanfang Ye", "Toby Jia-Jun Li", "Xiangliang Zhang"], "categories": ["cs.HC"], "comment": null, "summary": "AI VTubers, where the performer is not human but algorithmically generated,\nintroduce a new context for fandom. While human VTubers have been substantially\nstudied for their cultural appeal, parasocial dynamics, and community\neconomies, little is known about how audiences engage with their AI\ncounterparts. To address this gap, we present a qualitative study of\nNeuro-sama, the most prominent AI VTuber. Our findings show that engagement is\nanchored in active co-creation: audiences are drawn by the AI's unpredictable\nyet entertaining interactions, cement loyalty through collective emotional\nevents that trigger anthropomorphic projection, and sustain attachment via the\nAI's consistent persona. Financial support emerges not as a reward for\nperformance but as a participatory mechanism for shaping livestream content,\nestablishing a resilient fan economy built on ongoing interaction. These\ndynamics reveal how AI Vtuber fandom reshapes fan-creator relationships and\noffer implications for designing transparent and sustainable AI-mediated\ncommunities.", "AI": {"tldr": "This paper examines how audiences engage with AI VTubers, particularly focusing on the case of Neuro-sama, exploring dynamics of fan loyalty and co-creation.", "motivation": "To explore the unstudied dynamics of audience engagement with AI VTubers and how they differ from human VTubers.", "method": "Qualitative study of the interactions and engagement patterns of audiences with the AI VTuber Neuro-sama.", "result": "Audience engagement is rooted in active co-creation, where unpredictable interactions foster loyalty and financial support acts as a participatory mechanism.", "conclusion": "AI VTuber fandom transforms traditional fan-creator relationships, emphasizing the need for transparent and sustainable community designs in AI contexts.", "key_contributions": ["Identification of unique engagement patterns with AI VTubers", "Insights into the economics of AI Vtuber fandom", "Implications for designing AI-mediated communities"], "limitations": "", "keywords": ["AI VTubers", "Neuro-sama", "Fan engagement", "Community economics", "Human-Computer Interaction"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.09712", "pdf": "https://arxiv.org/pdf/2509.09712.pdf", "abs": "https://arxiv.org/abs/2509.09712", "title": "The Thinking Therapist: Training Large Language Models to Deliver Acceptance and Commitment Therapy using Supervised Fine-Tuning and Odds Ratio Policy Optimization", "authors": ["Talha Tahir"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Acceptance and Commitment Therapy (ACT) is a third-wave cognitive behavioral\ntherapy with emerging evidence of efficacy in several psychiatric conditions.\nThis study investigates the impact of post-training methodology and explicit\nreasoning on the ability of a small open-weight large language model (LLM) to\ndeliver ACT. Using 50 sets of synthetic ACT transcripts generated by\nMistral-Large, we trained Llama-3.2-3b-Instruct with two distinct approaches,\nsupervised fine-tuning (SFT) and odds ratio policy optimization (ORPO), each\nwith and without an explicit chain-of-thought (COT) reasoning step. Performance\nwas evaluated by comparing these four post-trained variants against the base\nInstruct model. These models were benchmarked in simulated therapy sessions,\nwith performance quantitatively assessed on the ACT Fidelity Measure (ACT-FM)\nand the Therapist Empathy Scale (TES) by an LLM judge that had been fine-tuned\non human evaluations. Our findings demonstrate that the ORPO-trained models\nsignificantly outperformed both their SFT and Instruct counterparts on ACT\nfidelity ($\\chi^2(5) = 185.15, p < .001$) and therapeutic empathy ($\\chi^2(5) =\n140.37, p < .001$). The effect of COT was conditional as it provided a\nsignificant benefit to SFT models, improving ACT-FM scores by an average of\n2.68 points ($p < .001$), while offering no discernible advantage to the\nsuperior ORPO or instruct-tuned variants. We posit that the superiority of ORPO\nstems from its ability to learn the therapeutic `process' over imitating\n`content,' a key aspect of ACT, while COT acts as a necessary scaffold for\nmodels trained only via imitation. This study establishes that\npreference-aligned policy optimization can effectively instill ACT competencies\nin small LLMs, and that the utility of explicit reasoning is highly dependent\non the underlying training paradigm.", "AI": {"tldr": "This study evaluates the effectiveness of different training methods for large language models (LLMs) in delivering Acceptance and Commitment Therapy (ACT).", "motivation": "To investigate how post-training methodology and explicit reasoning affect LLMs' ability to deliver ACT, which has shown efficacy in psychiatric conditions.", "method": "The study uses 50 sets of synthetic ACT transcripts to train Llama-3.2-3b-Instruct using two methods: supervised fine-tuning (SFT) and odds ratio policy optimization (ORPO), with and without an explicit chain-of-thought (COT) reasoning step. Performance is measured using the ACT Fidelity Measure (ACT-FM) and the Therapist Empathy Scale (TES).", "result": "The ORPO-trained models significantly outperformed SFT and standard Instruct models in terms of ACT fidelity and therapeutic empathy, with statistical significance ($p < .001$). The COT reasoning step improved SFT models but did not benefit the ORPO or instruct-tuned variants.", "conclusion": "Preference-aligned policy optimization can effectively instill ACT competencies in small LLMs, and the effectiveness of explicit reasoning is contingent on the training paradigm used.", "key_contributions": ["Demonstrated the superiority of ORPO over SFT for ACT delivery in LLMs", "Showed that implicit reasoning techniques can be beneficial under certain training methodologies", "Established the importance of training paradigms in shaping LLM performance for therapeutic applications"], "limitations": "", "keywords": ["Acceptance and Commitment Therapy", "large language model", "training methodology"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.09702", "pdf": "https://arxiv.org/pdf/2509.09702.pdf", "abs": "https://arxiv.org/abs/2509.09702", "title": "Creativity Benchmark: A benchmark for marketing creativity for LLM models", "authors": ["Ninad Bhat", "Kieran Browne", "Pip Bingemann"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "30 Pages, 14 figures", "summary": "We introduce Creativity Benchmark, an evaluation framework for large language\nmodels (LLMs) in marketing creativity. The benchmark covers 100 brands (12\ncategories) and three prompt types (Insights, Ideas, Wild Ideas). Human\npairwise preferences from 678 practising creatives over 11,012 anonymised\ncomparisons, analysed with Bradley-Terry models, show tightly clustered\nperformance with no model dominating across brands or prompt types: the\ntop-bottom spread is $\\Delta\\theta \\approx 0.45$, which implies a head-to-head\nwin probability of $0.61$; the highest-rated model beats the lowest only about\n$61\\%$ of the time. We also analyse model diversity using cosine distances to\ncapture intra- and inter-model variation and sensitivity to prompt reframing.\nComparing three LLM-as-judge setups with human rankings reveals weak,\ninconsistent correlations and judge-specific biases, underscoring that\nautomated judges cannot substitute for human evaluation. Conventional\ncreativity tests also transfer only partially to brand-constrained tasks.\nOverall, the results highlight the need for expert human evaluation and\ndiversity-aware workflows.", "AI": {"tldr": "This paper presents the Creativity Benchmark, an evaluation framework for assessing the creativity of large language models in marketing, revealing the limitations of automated judges and the necessity for human evaluation.", "motivation": "To evaluate the creativity of large language models in marketing and highlight the need for human judgment in creativity assessments.", "method": "The framework uses human pairwise preferences from 678 creatives to analyze 100 brands across various prompt types, employing Bradley-Terry models for analysis.", "result": "The analysis shows no model dominance across brands or prompts and indicates weak correlations between automated judge setups and human rankings, suggesting biases and limitations in automated evaluations.", "conclusion": "Human evaluation is essential for creativity assessment, and model diversity approaches are necessary to improve evaluation accuracy and reliability.", "key_contributions": ["Introduction of Creativity Benchmark for evaluating LLMs in marketing creativity", "Extensive human preference analysis reveals model performance consistency", "Emphasis on the importance of human evaluator roles over automated systems."], "limitations": "Automated judges show weak and inconsistent correlations with human evaluations; conventional creativity tests are only partially applicable to tasks with specific brand constraints.", "keywords": ["Creativity Benchmark", "large language models", "human evaluation", "marketing", "creativity assessment"], "importance_score": 8, "read_time_minutes": 30}}
{"id": "2509.09713", "pdf": "https://arxiv.org/pdf/2509.09713.pdf", "abs": "https://arxiv.org/abs/2509.09713", "title": "HANRAG: Heuristic Accurate Noise-resistant Retrieval-Augmented Generation for Multi-hop Question Answering", "authors": ["Duolin Sun", "Dan Yang", "Yue Shen", "Yihan Jiao", "Zhehao Tan", "Jie Feng", "Lianzhen Zhong", "Jian Wang", "Peng Wei", "Jinjie Gu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The Retrieval-Augmented Generation (RAG) approach enhances question-answering\nsystems and dialogue generation tasks by integrating information retrieval (IR)\ntechnologies with large language models (LLMs). This strategy, which retrieves\ninformation from external knowledge bases to bolster the response capabilities\nof generative models, has achieved certain successes. However, current RAG\nmethods still face numerous challenges when dealing with multi-hop queries. For\ninstance, some approaches overly rely on iterative retrieval, wasting too many\nretrieval steps on compound queries. Additionally, using the original complex\nquery for retrieval may fail to capture content relevant to specific\nsub-queries, resulting in noisy retrieved content. If the noise is not managed,\nit can lead to the problem of noise accumulation. To address these issues, we\nintroduce HANRAG, a novel heuristic-based framework designed to efficiently\ntackle problems of varying complexity. Driven by a powerful revelator, HANRAG\nroutes queries, decomposes them into sub-queries, and filters noise from\nretrieved documents. This enhances the system's adaptability and noise\nresistance, making it highly capable of handling diverse queries. We compare\nthe proposed framework against other leading industry methods across various\nbenchmarks. The results demonstrate that our framework obtains superior\nperformance in both single-hop and multi-hop question-answering tasks.", "AI": {"tldr": "Introducing HANRAG, a heuristic-based framework that improves multi-hop query handling in RAG by efficiently routing, decomposing, and filtering noise from queries.", "motivation": "To enhance the performance of RAG in dealing with multi-hop queries and reduce noise accumulation in retrieved content.", "method": "HANRAG utilizes a revelator to route queries, decompose them into sub-queries, and filter out irrelevant noise from retrieved documents.", "result": "HANRAG outperforms existing methods in both single-hop and multi-hop question-answering tasks across various benchmarks.", "conclusion": "HANRAG enhances adaptability and noise resistance in RAG systems, making it effective for diverse query complexities.", "key_contributions": ["Development of a heuristic-based query routing method", "Efficient decomposition of complex queries into manageable sub-queries", "Noise filtering mechanism that improves retrieval accuracy"], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "large language models", "multi-hop queries", "query routing", "noise filtering"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.09714", "pdf": "https://arxiv.org/pdf/2509.09714.pdf", "abs": "https://arxiv.org/abs/2509.09714", "title": "How Small Transformation Expose the Weakness of Semantic Similarity Measures", "authors": ["Serge Lionel Nikiema", "Albrick Euraste Djire", "Abdoul Aziz Bonkoungou", "Micheline Bndicte Moumoula", "Jordan Samhi", "Abdoul Kader Kabore", "Jacques Klein", "Tegawend F. Bissyande"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This research examines how well different methods measure semantic\nsimilarity, which is important for various software engineering applications\nsuch as code search, API recommendations, automated code reviews, and\nrefactoring tools. While large language models are increasingly used for these\nsimilarity assessments, questions remain about whether they truly understand\nsemantic relationships or merely recognize surface patterns.\n  The study tested 18 different similarity measurement approaches, including\nword-based methods, embedding techniques, LLM-based systems, and\nstructure-aware algorithms. The researchers created a systematic testing\nframework that applies controlled changes to text and code to evaluate how well\neach method handles different types of semantic relationships.\n  The results revealed significant issues with commonly used metrics. Some\nembedding-based methods incorrectly identified semantic opposites as similar up\nto 99.9 percent of the time, while certain transformer-based approaches\noccasionally rated opposite meanings as more similar than synonymous ones. The\nstudy found that embedding methods' poor performance often stemmed from how\nthey calculate distances; switching from Euclidean distance to cosine\nsimilarity improved results by 24 to 66 percent. LLM-based approaches performed\nbetter at distinguishing semantic differences, producing low similarity scores\n(0.00 to 0.29) for genuinely different meanings, compared to embedding methods\nthat incorrectly assigned high scores (0.82 to 0.99) to dissimilar content.", "AI": {"tldr": "The study evaluates various methods for measuring semantic similarity in software engineering, revealing significant issues with current metrics, especially for embedding-based methods.", "motivation": "To assess the effectiveness of different methods for measuring semantic similarity vital for software engineering tasks such as code search and automated reviews.", "method": "The researchers tested 18 similarity measurement approaches using a systematic framework that applies controlled changes to evaluate their handling of semantic relationships.", "result": "The study found high error rates in similarity assessment, particularly with embedding methods mistaking opposites for similarities. LLM-based methods showed improved performance in distinguishing semantic differences.", "conclusion": "The results suggest a need for better similarity measurement methods, with LLMs outperforming traditional embedding techniques in understanding semantic relationships.", "key_contributions": ["Systematic testing framework for evaluating semantic similarity methods.", "Demonstrated significant flaws in embedding-based methods for semantic similarity assessment.", "Showed improved performance of LLM-based approaches in discerning semantic differences."], "limitations": "", "keywords": ["semantic similarity", "software engineering", "large language models"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.09715", "pdf": "https://arxiv.org/pdf/2509.09715.pdf", "abs": "https://arxiv.org/abs/2509.09715", "title": "Investigating Symbolic Triggers of Hallucination in Gemma Models Across HaluEval and TruthfulQA", "authors": ["Naveen Lamba", "Sanju Tiwari", "Manas Gaur"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Hallucination in Large Language Models (LLMs) is a well studied problem.\nHowever, the properties that make LLM intrinsically vulnerable to\nhallucinations have not been identified and studied. This research identifies\nand characterizes the key properties, allowing us to pinpoint vulnerabilities\nwithin the model's internal mechanisms. To solidify on these properties, we\nutilized two established datasets, HaluEval and TruthfulQA and convert their\nexisting format of question answering into various other formats to narrow down\nthese properties as the reason for the hallucinations. Our findings reveal that\nhallucination percentages across symbolic properties are notably high for\nGemma-2-2B, averaging 79.0% across tasks and datasets. With increased model\nscale, hallucination drops to 73.6% for Gemma-2-9B and 63.9% for Gemma-2-27B,\nreflecting a 15 percentage point reduction overall. Although the hallucination\nrate decreases as the model size increases, a substantial amount of\nhallucination caused by symbolic properties still persists. This is especially\nevident for modifiers (ranging from 84.76% to 94.98%) and named entities\n(ranging from 83.87% to 93.96%) across all Gemma models and both datasets.\nThese findings indicate that symbolic elements continue to confuse the models,\npointing to a fundamental weakness in how these LLMs process such\ninputs--regardless of their scale.", "AI": {"tldr": "This study investigates the vulnerabilities in Large Language Models (LLMs) related to hallucinations, identifying key properties that contribute to this issue.", "motivation": "To identify intrinsic vulnerabilities of LLMs that lead to hallucinations and assess how these vulnerabilities manifest across different model scales.", "method": "The research utilized the HaluEval and TruthfulQA datasets, converting their formats to analyze hallucination properties in LLMs through various tasks.", "result": "Hallucination percentages are significantly high in Gemma models, averaging 79.0% across tasks with a reduction observed as model scale increases, yet substantial hallucinations remain tied to symbolic properties.", "conclusion": "Despite larger models reducing hallucination rates, they still exhibit significant vulnerabilities due to symbolic properties, indicating a need for further exploration and improvement in LLM processing of such inputs.", "key_contributions": ["Identification of properties causing hallucinations in LLMs", "Quantitative analysis of hallucination rates across different model scales", "Highlighting weaknesses in LLM processing of symbolic elements"], "limitations": "Focus primarily on symbolic properties without assessing other potential factors contributing to hallucinations.", "keywords": ["Large Language Models", "hallucinations", "symbolic properties", "Gemma models", "HCI"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.09723", "pdf": "https://arxiv.org/pdf/2509.09723.pdf", "abs": "https://arxiv.org/abs/2509.09723", "title": "ALIGNS: Unlocking nomological networks in psychological measurement through a large language model", "authors": ["Kai R. Larsen", "Sen Yan", "Roland Mller", "Lan Sang", "Mikko Rnkk", "Ravi Starzl", "Donald Edmondson"], "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ME", "I.2.6; J.4; I.5.1; H.3.3; H.2.8"], "comment": null, "summary": "Psychological measurement is critical to many disciplines. Despite advances\nin measurement, building nomological networks, theoretical maps of how concepts\nand measures relate to establish validity, remains a challenge 70 years after\nCronbach and Meehl proposed them as fundamental to validation. This limitation\nhas practical consequences: clinical trials may fail to detect treatment\neffects, and public policy may target the wrong outcomes. We introduce Analysis\nof Latent Indicators to Generate Nomological Structures (ALIGNS), a large\nlanguage model-based system trained with validated questionnaire measures.\nALIGNS provides three comprehensive nomological networks containing over\n550,000 indicators across psychology, medicine, social policy, and other\nfields. This represents the first application of large language models to solve\na foundational problem in measurement validation. We report classification\naccuracy tests used to develop the model, as well as three evaluations. In the\nfirst evaluation, the widely used NIH PROMIS anxiety and depression instruments\nare shown to converge into a single dimension of emotional distress. The second\nevaluation examines child temperament measures and identifies four potential\ndimensions not captured by current frameworks, and questions one existing\ndimension. The third evaluation, an applicability check, engages expert\npsychometricians who assess the system's importance, accessibility, and\nsuitability. ALIGNS is freely available at nomologicalnetwork.org,\ncomplementing traditional validation methods with large-scale nomological\nanalysis.", "AI": {"tldr": "ALIGNS is a large language model-based system for creating nomological networks in psychological measurement, aimed at improving validation methods across various fields.", "motivation": "There is a longstanding challenge in building nomological networks to establish the validity of psychological measurements, which can lead to significant consequences in clinical trials and public policy.", "method": "The system leverages large language models trained on validated questionnaire measures to generate comprehensive nomological networks containing over 550,000 indicators across various disciplines.", "result": "ALIGNS demonstrated successful convergence of the NIH PROMIS anxiety and depression instruments into one dimension of emotional distress, identified new dimensions in child temperament measures, and received positive feedback from expert psychometricians on its accessibility and importance.", "conclusion": "ALIGNS offers a novel approach to measurement validation by complementing traditional methods with extensive nomological analysis, and is freely available for use.", "key_contributions": ["Introduction of ALIGNS, the first application of large language models for nomological networks", "Creation of three detailed nomological networks across multiple disciplines", "Successful evaluations indicate the model's capacity to identify dimensions not accounted for by existing frameworks."], "limitations": "", "keywords": ["psychological measurement", "nomological networks", "large language models", "validation methods", "health informatics"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.10010", "pdf": "https://arxiv.org/pdf/2509.10010.pdf", "abs": "https://arxiv.org/abs/2509.10010", "title": "Multi-Intent Recognition in Dialogue Understanding: A Comparison Between Smaller Open-Source LLMs", "authors": ["Adnan Ahmad", "Philine Kowol", "Stefan Hillmann", "Sebastian Mller"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "In this paper, we provide an extensive analysis of multi-label intent\nclassification using Large Language Models (LLMs) that are open-source,\npublicly available, and can be run in consumer hardware. We use the MultiWOZ\n2.1 dataset, a benchmark in the dialogue system domain, to investigate the\nefficacy of three popular open-source pre-trained LLMs, namely LLama2-7B-hf,\nMistral-7B-v0.1, and Yi-6B. We perform the classification task in a few-shot\nsetup, giving 20 examples in the prompt with some instructions. Our approach\nfocuses on the differences in performance of these models across several\nperformance metrics by methodically assessing these models on multi-label\nintent classification tasks. Additionally, we compare the performance of the\ninstruction-based fine-tuning approach with supervised learning using the\nsmaller transformer model BertForSequenceClassification as a baseline. To\nevaluate the performance of the models, we use evaluation metrics like\naccuracy, precision, and recall as well as micro, macro, and weighted F1 score.\nWe also report the inference time, VRAM requirements, etc. The Mistral-7B-v0.1\noutperforms two other generative models on 11 intent classes out of 14 in terms\nof F-Score, with a weighted average of 0.50. It also has relatively lower\nHumming Loss and higher Jaccard Similarity, making it the winning model in the\nfew-shot setting. We find BERT based supervised classifier having superior\nperformance compared to the best performing few-shot generative LLM. The study\nprovides a framework for small open-source LLMs in detecting complex\nmulti-intent dialogues, enhancing the Natural Language Understanding aspect of\ntask-oriented chatbots.", "AI": {"tldr": "This paper analyzes the effectiveness of open-source Large Language Models for multi-label intent classification using the MultiWOZ 2.1 dataset, highlighting differences in performance metrics among the models and comparing them to a BERT-based supervised classifier.", "motivation": "To investigate the effectiveness of open-source pre-trained LLMs for multi-label intent classification in dialogue systems.", "method": "Three open-source LLMs (LLama2-7B-hf, Mistral-7B-v0.1, Yi-6B) were evaluated in a few-shot classification setup on the MultiWOZ 2.1 dataset, with comparison to a BERT-based supervised learning model.", "result": "Mistral-7B-v0.1 outperformed other models on 11 out of 14 intent classes, while the BERT classifier showed superior overall performance compared to the best-performing generative LLM in the few-shot setting.", "conclusion": "The study lays the groundwork for using small open-source LLMs in complex multi-intent dialogue detection, improving Natural Language Understanding in chatbots.", "key_contributions": ["Extensive analysis of open-source LLMs for intent classification", "Comparison of few-shot LLM performance to BERT-based supervised learning", "Framework for enhancing Natural Language Understanding in task-oriented chatbots"], "limitations": "", "keywords": ["multi-label intent classification", "Large Language Models", "Natural Language Understanding"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.09724", "pdf": "https://arxiv.org/pdf/2509.09724.pdf", "abs": "https://arxiv.org/abs/2509.09724", "title": "DiTTO-LLM: Framework for Discovering Topic-based Technology Opportunities via Large Language Model", "authors": ["Wonyoung Kim", "Sujeong Seo", "Juhyun Lee"], "categories": ["cs.CL", "cs.AI", "cs.LG", "68T09"], "comment": "5 figures", "summary": "Technology opportunities are critical information that serve as a foundation\nfor advancements in technology, industry, and innovation. This paper proposes a\nframework based on the temporal relationships between technologies to identify\nemerging technology opportunities. The proposed framework begins by extracting\ntext from a patent dataset, followed by mapping text-based topics to discover\ninter-technology relationships. Technology opportunities are then identified by\ntracking changes in these topics over time. To enhance efficiency, the\nframework leverages a large language model to extract topics and employs a\nprompt for a chat-based language model to support the discovery of technology\nopportunities. The framework was evaluated using an artificial intelligence\npatent dataset provided by the United States Patent and Trademark Office. The\nexperimental results suggest that artificial intelligence technology is\nevolving into forms that facilitate everyday accessibility. This approach\ndemonstrates the potential of the proposed framework to identify future\ntechnology opportunities.", "AI": {"tldr": "The paper presents a framework for identifying emerging technology opportunities through the analysis of patent data and temporal relationships between technological topics.", "motivation": "To provide a systematic approach for identifying technology opportunities using temporal relationships, enhancing innovation in various fields.", "method": "The framework involves extracting text from a patent dataset, mapping topics to discover inter-technology relationships, and tracking changes over time. It utilizes a large language model for topic extraction and a chat-based model for opportunity discovery.", "result": "The framework was evaluated with a dataset of artificial intelligence patents, revealing that AI technologies are evolving to become more accessible in everyday contexts.", "conclusion": "The study illustrates the framework's effectiveness in uncovering future technology opportunities, emphasizing its relevance in a rapidly changing technological landscape.", "key_contributions": ["Development of a novel framework for identifying emerging technology opportunities", "Utilization of LLMs for topic extraction and technology opportunity discovery", "Demonstration of the relationship between inter-technology topics and emerging opportunities"], "limitations": "", "keywords": ["technology opportunities", "patent dataset", "large language model", "artificial intelligence", "temporal relationships"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2509.09725", "pdf": "https://arxiv.org/pdf/2509.09725.pdf", "abs": "https://arxiv.org/abs/2509.09725", "title": "BIBERT-Pipe on Biomedical Nested Named Entity Linking at BioASQ 2025", "authors": ["Chunyu Li", "Xindi Zheng", "Siqi Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Entity linking (EL) for biomedical text is typically benchmarked on\nEnglish-only corpora with flat mentions, leaving the more realistic scenario of\nnested and multilingual mentions largely unexplored. We present our system for\nthe BioNNE 2025 Multilingual Biomedical Nested Named Entity Linking shared task\n(English & Russian), closing this gap with a lightweight pipeline that keeps\nthe original EL model intact and modifies only three task-aligned components:\nTwo-stage retrieval-ranking. We leverage the same base encoder model in both\nstages: the retrieval stage uses the original pre-trained model, while the\nranking stage applies domain-specific fine-tuning. Boundary cues. In the\nranking stage, we wrap each mention with learnable [Ms] / [Me] tags, providing\nthe encoder with an explicit, language-agnostic span before robustness to\noverlap and nesting. Dataset augmentation. We also automatically expand the\nranking training corpus with three complementary data sources, enhancing\ncoverage without extra manual annotation. On the BioNNE 2025 leaderboard, our\ntwo stage system, bilingual bert (BIBERT-Pipe), ranks third in the multilingual\ntrack, demonstrating the effectiveness and competitiveness of these minimal yet\nprincipled modifications. Code are publicly available at\nhttps://github.com/Kaggle-Competitions-Code/BioNNE-L.", "AI": {"tldr": "A novel approach for nested and multilingual entity linking in biomedical texts, ranking third in the BioNNE 2025 competition.", "motivation": "To address the limitations of existing entity linking methods that primarily focus on English-only and flat mention corpora, thereby ignoring nested and multilingual mentions that are more representative of real-world scenarios.", "method": "The proposed system, called bilingual bert (BIBERT-Pipe), utilizes a two-stage retrieval-ranking approach, where the retrieval stage employs a pre-trained model and the ranking stage applies domain-specific fine-tuning. Additionally, it incorporates boundary cues with learnable tags and augments the dataset by expanding the ranking training corpus with three complementary data sources.", "result": "The BIBERT-Pipe system achieved third place in the multilingual track of the BioNNE 2025 leaderboard, indicating its effectiveness in handling nested and multilingual entity linking.", "conclusion": "These minimal yet principled modifications to the entity linking task demonstrate significant advancements in the handling of nested and multilingual mentions in biomedical texts.", "key_contributions": ["Introduction of a lightweight pipeline for multilingual biomedical nested entity linking.", "Implementation of a two-stage retrieval-ranking methodology that maximizes the use of pre-trained models.", "Augmentation of training datasets that reduces the need for manual annotation while improving model performance."], "limitations": "", "keywords": ["Entity Linking", "Biomedical Text", "Multilingual", "Nested Mentions", "Natural Language Processing"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2504.04198", "pdf": "https://arxiv.org/pdf/2504.04198.pdf", "abs": "https://arxiv.org/abs/2504.04198", "title": "Evaluating the Usability of Microgestures for Text Editing Tasks in Virtual Reality", "authors": ["Xiang Li", "Wei He", "Per Ola Kristensson"], "categories": ["cs.HC", "cs.MM"], "comment": "14 pages, IEEE Transactions on Visualization and Computer Graphics", "summary": "As virtual reality (VR) continues to evolve, traditional input methods such\nas handheld controllers and gesture systems often face challenges with\nprecision, social accessibility, and user fatigue. These limitations motivate\nthe exploration of microgestures, which promise more subtle, ergonomic, and\ndevice-free interactions. We introduce microGEXT, a lightweight\nmicrogesture-based system designed for text editing in VR without external\nsensors, which utilizes small, subtle hand movements to reduce physical strain\ncompared to standard gestures. We evaluated microGEXT in three user studies. In\nStudy 1 ($N=20$), microGEXT reduced overall edit time and fatigue compared to a\nray-casting + pinch menu baseline, the default text editing approach in\ncommercial VR systems. Study 2 ($N=20$) found that microGEXT performed well in\nshort text selection tasks but was slower for longer text ranges. In Study 3\n($N=10$), participants found microGEXT intuitive for open-ended\ninformation-gathering tasks. Across all studies, microGEXT demonstrated\nenhanced user experience and reduced physical effort, offering a promising\nalternative to traditional VR text editing techniques.", "AI": {"tldr": "microGEXT is a microgesture-based system for text editing in VR that offers improved precision and reduced fatigue compared to traditional methods.", "motivation": "Traditional VR input methods suffer from issues like precision and user fatigue, highlighting the need for more ergonomic solutions.", "method": "microGEXT employs small, subtle hand movements for text editing in VR without using external sensors, evaluated through three user studies.", "result": "In three studies, microGEXT was found to reduce edit times and user fatigue compared to traditional text editing methods, while performing effectively for short text tasks.", "conclusion": "microGEXT provides a promising alternative to standard VR text editing techniques, enhancing user experience and minimizing physical effort.", "key_contributions": ["Introduction of a microgesture-based system for VR text editing", "Demonstration of reduced user fatigue and enhanced performance", "Validation through multiple user studies"], "limitations": "Slower performance observed for longer text ranges in some tasks.", "keywords": ["microgestures", "virtual reality", "text editing", "user experience", "ergonomics"], "importance_score": 5, "read_time_minutes": 14}}
{"id": "2509.09726", "pdf": "https://arxiv.org/pdf/2509.09726.pdf", "abs": "https://arxiv.org/abs/2509.09726", "title": "Natural Language Translation of Formal Proofs through Informalization of Proof Steps and Recursive Summarization along Proof Structure", "authors": ["Seiji Hattori", "Takuya Matsuzaki", "Makoto Fujiwara"], "categories": ["cs.CL"], "comment": "Submitted to INLG 2025 (accepted)", "summary": "This paper proposes a natural language translation method for\nmachine-verifiable formal proofs that leverages the informalization\n(verbalization of formal language proof steps) and summarization capabilities\nof LLMs. For evaluation, it was applied to formal proof data created in\naccordance with natural language proofs taken from an undergraduate-level\ntextbook, and the quality of the generated natural language proofs was analyzed\nin comparison with the original natural language proofs. Furthermore, we will\ndemonstrate that this method can output highly readable and accurate natural\nlanguage proofs by applying it to existing formal proof library of the Lean\nproof assistant.", "AI": {"tldr": "The paper presents a method for translating machine-verifiable formal proofs into natural language using LLMs, demonstrating its effectiveness with formal proof libraries.", "motivation": "To enhance accessibility and readability of formal proofs by translating them into natural language, making them verifiable by machines while still comprehensible to humans.", "method": "The proposed method formalizes informal language proof steps and utilizes LLMs for translation and summarization of formal proofs.", "result": "The generated natural language proofs were found to be highly readable and accurate when compared to original natural language proofs.", "conclusion": "The method shows promise in transforming formal proofs into formats that are both machine-readable and human-friendly, indicating a significant advancement in the usability of formal proofs.", "key_contributions": ["Introduction of a method utilizing LLMs for translating formal proofs to natural language", "Demonstration of high accuracy and readability in generated proofs", "Application of the method on formal proof libraries such as Lean to validate its effectiveness."], "limitations": "", "keywords": ["natural language processing", "formal proofs", "machine-learning"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.09727", "pdf": "https://arxiv.org/pdf/2509.09727.pdf", "abs": "https://arxiv.org/abs/2509.09727", "title": "A Role-Aware Multi-Agent Framework for Financial Education Question Answering with LLMs", "authors": ["Andy Zhu", "Yingjun Du"], "categories": ["cs.CL", "cs.CE"], "comment": "8 pages, 6 figures, Underreview", "summary": "Question answering (QA) plays a central role in financial education, yet\nexisting large language model (LLM) approaches often fail to capture the\nnuanced and specialized reasoning required for financial problem-solving. The\nfinancial domain demands multistep quantitative reasoning, familiarity with\ndomain-specific terminology, and comprehension of real-world scenarios. We\npresent a multi-agent framework that leverages role-based prompting to enhance\nperformance on domain-specific QA. Our framework comprises a Base Generator, an\nEvidence Retriever, and an Expert Reviewer agent that work in a single-pass\niteration to produce a refined answer. We evaluated our framework on a set of\n3,532 expert-designed finance education questions from Study.com, an online\nlearning platform. We leverage retrieval-augmented generation (RAG) for\ncontextual evidence from 6 finance textbooks and prompting strategies for a\ndomain-expert reviewer. Our experiments indicate that critique-based refinement\nimproves answer accuracy by 6.6-8.3% over zero-shot Chain-of-Thought baselines,\nwith the highest performance from Gemini-2.0-Flash. Furthermore, our method\nenables GPT-4o-mini to achieve performance comparable to the finance-tuned\nFinGPT-mt_Llama3-8B_LoRA. Our results show a cost-effective approach to\nenhancing financial QA and offer insights for further research in multi-agent\nfinancial LLM systems.", "AI": {"tldr": "This paper presents a multi-agent framework to improve financial question answering by employing role-based prompting and a combination of retrieval-augmented generation techniques.", "motivation": "The need for improved financial question answering systems that can handle complex reasoning and domain-specific language.", "method": "A multi-agent framework consisting of a Base Generator, an Evidence Retriever, and an Expert Reviewer to produce more accurate answers to finance-related questions.", "result": "The proposed framework improved answer accuracy by 6.6-8.3% over existing methods, with models like Gemini-2.0-Flash performing particularly well.", "conclusion": "The framework offers a cost-effective approach to enhance financial question answering capabilities and suggests directions for future research in multi-agent systems for finance.", "key_contributions": ["Introduction of a multi-agent framework for financial QA", "Demonstration of significant improvement in answer accuracy", "Insights for future multi-agent systems in domain-specific LLM applications."], "limitations": "", "keywords": ["question answering", "financial education", "large language models", "multi-agent systems", "retrieval-augmented generation"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2509.09728", "pdf": "https://arxiv.org/pdf/2509.09728.pdf", "abs": "https://arxiv.org/abs/2509.09728", "title": "A meta-analysis on the performance of machine-learning based language models for sentiment analysis", "authors": ["Elena Rohde", "Jonas Klingwort", "Christian Borgs"], "categories": ["cs.CL", "cs.LG", "stat.AP"], "comment": null, "summary": "This paper presents a meta-analysis evaluating ML performance in sentiment\nanalysis for Twitter data. The study aims to estimate the average performance,\nassess heterogeneity between and within studies, and analyze how study\ncharacteristics influence model performance. Using PRISMA guidelines, we\nsearched academic databases and selected 195 trials from 20 studies with 12\nstudy features. Overall accuracy, the most reported performance metric, was\nanalyzed using double arcsine transformation and a three-level random effects\nmodel. The average overall accuracy of the AIC-optimized model was 0.80 [0.76,\n0.84]. This paper provides two key insights: 1) Overall accuracy is widely used\nbut often misleading due to its sensitivity to class imbalance and the number\nof sentiment classes, highlighting the need for normalization. 2) Standardized\nreporting of model performance, including reporting confusion matrices for\nindependent test sets, is essential for reliable comparisons of ML classifiers\nacross studies, which seems far from common practice.", "AI": {"tldr": "This meta-analysis evaluates ML performance in sentiment analysis on Twitter data, revealing that overall accuracy can be misleading and calling for better reporting practices.", "motivation": "To estimate average performance in sentiment analysis for Twitter data and assess how study characteristics influence ML model performance.", "method": "A meta-analysis using PRISMA guidelines, selecting 195 trials from 20 studies, and applying a three-level random effects model to analyze accuracy metrics.", "result": "The average accuracy of the AIC-optimized model was found to be 0.80, but the study highlights significant issues with overall accuracy as a performance measure.", "conclusion": "The study underscores the importance of standardized reporting and normalization of model performance metrics to enable reliable comparisons across studies.", "key_contributions": ["Identified misleading nature of overall accuracy in sentiment analysis due to class imbalance.", "Emphasized the need for standardized reporting of model performance metrics."], "limitations": "The analysis is restricted to studies included in the meta-analysis, which may limit generalizability.", "keywords": ["Machine Learning", "Sentiment Analysis", "Twitter Data", "Meta-Analysis", "Performance Metrics"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.09729", "pdf": "https://arxiv.org/pdf/2509.09729.pdf", "abs": "https://arxiv.org/abs/2509.09729", "title": "MultimodalHugs: Enabling Sign Language Processing in Hugging Face", "authors": ["Gerard Sant", "Zifan Jiang", "Carlos Escolano", "Amit Moryossef", "Mathias Mller", "Rico Sennrich", "Sarah Ebling"], "categories": ["cs.CL", "cs.AI", "cs.MM"], "comment": null, "summary": "In recent years, sign language processing (SLP) has gained importance in the\ngeneral field of Natural Language Processing. However, compared to research on\nspoken languages, SLP research is hindered by complex ad-hoc code,\ninadvertently leading to low reproducibility and unfair comparisons. Existing\ntools that are built for fast and reproducible experimentation, such as Hugging\nFace, are not flexible enough to seamlessly integrate sign language\nexperiments. This view is confirmed by a survey we conducted among SLP\nresearchers.\n  To address these challenges, we introduce MultimodalHugs, a framework built\non top of Hugging Face that enables more diverse data modalities and tasks,\nwhile inheriting the well-known advantages of the Hugging Face ecosystem. Even\nthough sign languages are our primary focus, MultimodalHugs adds a layer of\nabstraction that makes it more widely applicable to other use cases that do not\nfit one of the standard templates of Hugging Face. We provide quantitative\nexperiments to illustrate how MultimodalHugs can accommodate diverse modalities\nsuch as pose estimation data for sign languages, or pixel data for text\ncharacters.", "AI": {"tldr": "This paper presents MultimodalHugs, a framework designed to improve the reproducibility and flexibility of sign language processing experiments by building on the Hugging Face ecosystem.", "motivation": "The paper addresses the need for better tools in sign language processing research, which currently suffers from low reproducibility and difficulties in conducting experiments compared to spoken language processing.", "method": "MultimodalHugs is introduced as a framework that allows for diverse data modalities and tasks, offering a layer of abstraction over the Hugging Face platform.", "result": "Quantitative experiments show that MultimodalHugs successfully accommodates various modalities such as pose estimation data for sign languages and pixel data for text characters.", "conclusion": "MultimodalHugs enhances the flexibility and reproducibility of sign language processing experiments while being applicable to other modalities.", "key_contributions": ["Introduction of a new framework for sign language processing", "Integration with the Hugging Face ecosystem", "Support for diverse data modalities beyond standard templates."], "limitations": "", "keywords": ["sign language processing", "Hugging Face", "multimodal framework"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2509.09731", "pdf": "https://arxiv.org/pdf/2509.09731.pdf", "abs": "https://arxiv.org/abs/2509.09731", "title": "Benchmarking Vision-Language Models on Chinese Ancient Documents: From OCR to Knowledge Reasoning", "authors": ["Haiyang Yu", "Yuchuan Wu", "Fan Shi", "Lei Liao", "Jinghui Lu", "Xiaodong Ge", "Han Wang", "Minghan Zhuo", "Xuecheng Wu", "Xiang Fei", "Hao Feng", "Guozhi Tang", "An-Lan Wang", "Hanshen Zhu", "Yangfan He", "Quanhuan Liang", "Liyuan Meng", "Chao Feng", "Can Huang", "Jingqun Tang", "Bin Li"], "categories": ["cs.CL"], "comment": null, "summary": "Chinese ancient documents, invaluable carriers of millennia of Chinese\nhistory and culture, hold rich knowledge across diverse fields but face\nchallenges in digitization and understanding, i.e., traditional methods only\nscan images, while current Vision-Language Models (VLMs) struggle with their\nvisual and linguistic complexity. Existing document benchmarks focus on English\nprinted texts or simplified Chinese, leaving a gap for evaluating VLMs on\nancient Chinese documents. To address this, we present AncientDoc, the first\nbenchmark for Chinese ancient documents, designed to assess VLMs from OCR to\nknowledge reasoning. AncientDoc includes five tasks (page-level OCR, vernacular\ntranslation, reasoning-based QA, knowledge-based QA, linguistic variant QA) and\ncovers 14 document types, over 100 books, and about 3,000 pages. Based on\nAncientDoc, we evaluate mainstream VLMs using multiple metrics, supplemented by\na human-aligned large language model for scoring.", "AI": {"tldr": "AncientDoc is the first benchmark for evaluating Vision-Language Models on Chinese ancient documents, addressing challenges in digitization and understanding.", "motivation": "Chinese ancient documents are rich in historical and cultural knowledge but face challenges in digitization and comprehension due to their complexity and the limitations of existing models.", "method": "The benchmark includes five tasks: page-level OCR, vernacular translation, reasoning-based QA, knowledge-based QA, and linguistic variant QA, covering diverse document types and evaluating mainstream VLMs.", "result": "The evaluation reveals the performance of various VLMs on ancient Chinese documents, highlighting gaps and opportunities for improvement across different tasks.", "conclusion": "AncientDoc provides a framework for better understanding and processing of ancient Chinese documents through advanced VLMs, promoting further research in this area.", "key_contributions": ["Introduction of the AncientDoc benchmark for Chinese ancient documents", "Evaluation of VLMs on multiple tasks relevant to ancient texts", "Identification of performance gaps in current models when applied to complex ancient document structures."], "limitations": "", "keywords": ["Vision-Language Models", "Chinese ancient documents", "OCR", "knowledge reasoning", "benchmark evaluation"], "importance_score": 6, "read_time_minutes": 12}}
{"id": "2509.09734", "pdf": "https://arxiv.org/pdf/2509.09734.pdf", "abs": "https://arxiv.org/abs/2509.09734", "title": "MCP-AgentBench: Evaluating Real-World Language Agent Performance with MCP-Mediated Tools", "authors": ["Zikang Guo", "Benfeng Xu", "Chiwei Zhu", "Wentao Hong", "Xiaorui Wang", "Zhendong Mao"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The Model Context Protocol (MCP) is rapidly emerging as a pivotal open\nstandard, designed to enhance agent-tool integration and interoperability, and\nis positioned to unlock a new era of powerful, interconnected, and genuinely\nutilitarian agentic AI. However, despite MCP's growing adoption, existing\nbenchmarks often fail to capture real-world agent performance within this new\nparadigm, leading to a distorted perception of their true operational value and\nan inability to reliably differentiate proficiencies. To bridge this critical\nevaluation gap, we introduce MCP-AgentBench -- a comprehensive benchmark\nspecifically engineered to rigorously assess language agent capabilities in\nMCP-mediated tool interactions. Core contributions of MCP-AgentBench include:\nthe establishment of a robust MCP testbed comprising 33 operational servers\nwith 188 distinct tools; the development of a benchmark featuring 600\nsystematically designed queries distributed across 6 distinct categories of\nvarying interaction complexity; and the introduction of MCP-Eval, a novel\noutcome-oriented evaluation methodology prioritizing real-world task success.\nThrough extensive empirical evaluation of leading language agents, we provide\nfoundational insights. MCP-AgentBench aims to equip the research community with\na standardized and reliable framework to build, validate, and advance agents\ncapable of fully leveraging MCP's transformative benefits, thereby accelerating\nprogress toward truly capable and interoperable AI systems.", "AI": {"tldr": "The paper introduces MCP-AgentBench, a robust benchmark for assessing language agent capabilities in MCP-mediated tool interactions, addressing gaps in current agent performance evaluations.", "motivation": "To enhance agent-tool integration and interoperability using the Model Context Protocol (MCP) and to provide a reliable evaluation framework for language agents.", "method": "MCP-AgentBench features a testbed with 33 servers and 188 tools, along with 600 queries across 6 complexity categories, and employs MCP-Eval for evaluation.", "result": "Empirical evaluation of leading language agents revealed foundational insights into their performance and capabilities within the MCP context.", "conclusion": "MCP-AgentBench aims to standardize assessments in the research community, facilitating advancements in interoperable AI systems leveraging MCP.", "key_contributions": ["Establishment of a testbed with 33 operational servers and 188 tools.", "Development of a benchmark with 600 queries across 6 categories of interaction complexity.", "Introduction of MCP-Eval for outcome-oriented evaluation."], "limitations": "", "keywords": ["Model Context Protocol", "agent-tool integration", "language agents", "benchmarks", "interoperability"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.09735", "pdf": "https://arxiv.org/pdf/2509.09735.pdf", "abs": "https://arxiv.org/abs/2509.09735", "title": "Discrimination by LLMs: Cross-lingual Bias Assessment and Mitigation in Decision-Making and Summarisation", "authors": ["Willem Huijzer", "Jieying Chen"], "categories": ["cs.CL"], "comment": "7 pages", "summary": "The rapid integration of Large Language Models (LLMs) into various domains\nraises concerns about societal inequalities and information bias. This study\nexamines biases in LLMs related to background, gender, and age, with a focus on\ntheir impact on decision-making and summarization tasks. Additionally, the\nresearch examines the cross-lingual propagation of these biases and evaluates\nthe effectiveness of prompt-instructed mitigation strategies. Using an adapted\nversion of the dataset by Tamkin et al. (2023) translated into Dutch, we\ncreated 151,200 unique prompts for the decision task and 176,400 for the\nsummarisation task. Various demographic variables, instructions, salience\nlevels, and languages were tested on GPT-3.5 and GPT-4o. Our analysis revealed\nthat both models were significantly biased during decision-making, favouring\nfemale gender, younger ages, and certain backgrounds such as the\nAfrican-American background. In contrast, the summarisation task showed minimal\nevidence of bias, though significant age-related differences emerged for\nGPT-3.5 in English. Cross-lingual analysis showed that bias patterns were\nbroadly similar between English and Dutch, though notable differences were\nobserved across specific demographic categories. The newly proposed mitigation\ninstructions, while unable to eliminate biases completely, demonstrated\npotential in reducing them. The most effective instruction achieved a 27\\% mean\nreduction in the gap between the most and least favorable demographics.\nNotably, contrary to GPT-3.5, GPT-4o displayed reduced biases for all prompts\nin English, indicating the specific potential for prompt-based mitigation\nwithin newer models. This research underscores the importance of cautious\nadoption of LLMs and context-specific bias testing, highlighting the need for\ncontinued development of effective mitigation strategies to ensure responsible\ndeployment of AI.", "AI": {"tldr": "This study investigates biases in Large Language Models (LLMs) related to demographics, focusing on their impact on decision-making and summarization tasks, while evaluating mitigation strategies.", "motivation": "To address societal inequalities and information bias arising from the integration of LLMs in various domains.", "method": "The study created a large dataset of prompts (151,200 for decision tasks and 176,400 for summarization tasks) to test bias in GPT-3.5 and GPT-4o across different demographic variables and languages.", "result": "Both models exhibited significant biases in decision-making favoring certain demographics, with minimal bias in summarization tasks; prompt-based mitigation showed potential to reduce biases.", "conclusion": "Cautious adoption of LLMs is necessary, with context-specific bias testing and development of mitigation strategies being critical for responsible AI deployment.", "key_contributions": ["Examination of biases in LLMs related to demographics.", "Cross-lingual analysis revealing similar bias patterns in English and Dutch.", "Empirical evidence showing effectiveness of prompt-instructed mitigation strategies."], "limitations": "The proposed mitigation instructions did not eliminate biases completely and effectiveness varied across tasks and demographic categories.", "keywords": ["Large Language Models", "bias", "decision-making", "summarization", "mitigation strategies"], "importance_score": 9, "read_time_minutes": 7}}
{"id": "2509.09801", "pdf": "https://arxiv.org/pdf/2509.09801.pdf", "abs": "https://arxiv.org/abs/2509.09801", "title": "HEFT: A Coarse-to-Fine Hierarchy for Enhancing the Efficiency and Accuracy of Language Model Reasoning", "authors": ["Brennen Hill"], "categories": ["cs.CL", "cs.AI", "cs.LG", "68T07, 68T50, 68T05", "I.2.7; I.2.6; C.4"], "comment": null, "summary": "The adaptation of large language models (LLMs) to specialized reasoning tasks\nis fundamentally constrained by computational resources. Parameter-Efficient\nFine-Tuning (PEFT) methods have emerged as a powerful solution, yet the\nlandscape of these techniques is diverse, with distinct methods operating in\neither the model's weight space or its representation space. This paper\ninvestigates the hypothesis that a synergistic combination of these paradigms\ncan unlock superior performance and efficiency. We introduce HEFT (Hierarchical\nEfficient Fine-Tuning), a novel hierarchical adaptation strategy that composes\ntwo distinct PEFT methods in a coarse-to-fine manner: first, a broad,\nfoundational adaptation in the weight space using Low-Rank Adaptation (LoRA),\nfollowed by a precise, surgical refinement of internal activations using\nRepresentation Fine-Tuning (ReFT). We evaluate this approach by fine-tuning a\nLlama-2-7B model on the BoolQ benchmark, a challenging dataset for inferential\nreasoning. Our results reveal a profound synergistic effect. A model fine-tuned\nfor only three epochs with our HEFT strategy achieves an accuracy of 85.17\\%,\nexceeding the performance of models trained for 20 epochs with either LoRA-only\n(85.05\\%) or ReFT-only (83.36\\%) methodologies. This work demonstrates that the\nthoughtful composition of PEFT methods is a potent algorithmic innovation,\noffering a more efficient and effective path toward advancing the reasoning\ncapabilities of language models. By achieving superior results with a fraction\nof the computational budget, our findings present a principled approach to\novercoming the obstacles inherent in adapting large-scale models for complex\ncognitive tasks.", "AI": {"tldr": "This paper introduces HEFT, a novel hierarchical fine-tuning strategy that combines Parameter-Efficient Fine-Tuning methods to enhance LLM performance on specialized reasoning tasks.", "motivation": "Large language models (LLMs) face computational constraints for specialized reasoning tasks, necessitating effective fine-tuning methods.", "method": "HEFT integrates two PEFT methods: Low-Rank Adaptation (LoRA) for broad foundational adaptation in weight space, followed by Representation Fine-Tuning (ReFT) for precision refinement of internal activations.", "result": "Using HEFT on the Llama-2-7B model, the paper reports an accuracy of 85.17% on the BoolQ benchmark after just three epochs, outperforming models fine-tuned with LoRA (85.05%) or ReFT (83.36%) alone.", "conclusion": "The study indicates that combining PEFT methods can significantly enhance the reasoning capabilities of language models while requiring less computational resources.", "key_contributions": ["Introduction of the HEFT method for fine-tuning LLMs", "Demonstration of superior performance through method synergy", "Evidence supporting efficient adaptation strategies for large-scale models"], "limitations": "", "keywords": ["large language models", "parameter-efficient fine-tuning", "reasoning tasks"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.09804", "pdf": "https://arxiv.org/pdf/2509.09804.pdf", "abs": "https://arxiv.org/abs/2509.09804", "title": "Pragmatic Frames Evoked by Gestures: A FrameNet Brasil Approach to Multimodality in Turn Organization", "authors": ["Helen de Andrade Abreu", "Tiago Timponi Torrent", "Ely Edison da Silva Matos"], "categories": ["cs.CL"], "comment": "Paper submitted to Language Sciences Journal", "summary": "This paper proposes a framework for modeling multimodal conversational turn\norganization via the proposition of correlations between language and\ninteractive gestures, based on analysis as to how pragmatic frames are\nconceptualized and evoked by communicators. As a means to provide evidence for\nthe analysis, we developed an annotation methodology to enrich a multimodal\ndataset (annotated for semantic frames) with pragmatic frames modeling\nconversational turn organization. Although conversational turn organization has\nbeen studied by researchers from diverse fields, the specific strategies,\nespecially gestures used by communicators, had not yet been encoded in a\ndataset that can be used for machine learning. To fill this gap, we enriched\nthe Frame2 dataset with annotations of gestures used for turn organization. The\nFrame2 dataset features 10 episodes from the Brazilian TV series Pedro Pelo\nMundo annotated for semantic frames evoked in both video and text. This dataset\nallowed us to closely observe how communicators use interactive gestures\noutside a laboratory, in settings, to our knowledge, not previously recorded in\nrelated literature. Our results have confirmed that communicators involved in\nface-to-face conversation make use of gestures as a tool for passing, taking\nand keeping conversational turns, and also revealed variations of some gestures\nthat had not been documented before. We propose that the use of these gestures\narises from the conceptualization of pragmatic frames, involving mental spaces,\nblending and conceptual metaphors. In addition, our data demonstrate that the\nannotation of pragmatic frames contributes to a deeper understanding of human\ncognition and language.", "AI": {"tldr": "The paper presents a framework for modeling multimodal conversational turn organization by linking language and gestures, enriching the Frame2 dataset with annotations of gestures used in conversation, revealing new insights into communicative strategies.", "motivation": "To address the gap in machine learning datasets regarding the role of gestures in conversational turn organization.", "method": "The authors developed an annotation methodology to enrich a multimodal dataset (Frame2) with pragmatic frames related to conversational turns and analyzed 10 episodes from a Brazilian TV series for this purpose.", "result": "The enriched dataset revealed that communicators use gestures to manage conversational turns and identified previously undocumented variations of these gestures.", "conclusion": "The research enhances the understanding of human cognition and language through the study of pragmatic frames and gestures in conversation.", "key_contributions": ["Proposed a framework linking gestures and language for turn organization", "Enriched the Frame2 dataset with pragmatic frame annotations", "Identified new variations of gestures used in conversation."], "limitations": "", "keywords": ["multimodal", "conversational turn organization", "pragmatic frames", "gestures", "machine learning"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2509.09852", "pdf": "https://arxiv.org/pdf/2509.09852.pdf", "abs": "https://arxiv.org/abs/2509.09852", "title": "Topic-Guided Reinforcement Learning with LLMs for Enhancing Multi-Document Summarization", "authors": ["Chuyuan Li", "Austin Xu", "Shafiq Joty", "Giuseppe Carenini"], "categories": ["cs.CL"], "comment": null, "summary": "A key challenge in Multi-Document Summarization (MDS) is effectively\nintegrating information from multiple sources while maintaining coherence and\ntopical relevance. While Large Language Models have shown impressive results in\nsingle-document summarization, their performance on MDS still leaves room for\nimprovement. In this paper, we propose a topic-guided reinforcement learning\napproach to improve content selection in MDS. We first show that explicitly\nprompting models with topic labels enhances the informativeness of the\ngenerated summaries. Building on this insight, we propose a novel topic reward\nwithin the Group Relative Policy Optimization (GRPO) framework to measure topic\nalignment between the generated summary and source documents. Experimental\nresults on the Multi-News and Multi-XScience datasets demonstrate that our\nmethod consistently outperforms strong baselines, highlighting the\neffectiveness of leveraging topical cues in MDS.", "AI": {"tldr": "This paper introduces a topic-guided reinforcement learning approach to enhance Multi-Document Summarization (MDS) by improving content selection and coherence.", "motivation": "Address the challenge of integrating information from multiple sources in Multi-Document Summarization while maintaining coherence and relevance.", "method": "A topic-guided reinforcement learning method using Group Relative Policy Optimization (GRPO) with a novel topic reward to align generated summaries with source documents.", "result": "Experimental results indicate that the proposed method outperforms strong baselines on the Multi-News and Multi-XScience datasets.", "conclusion": "Leveraging topical cues significantly enhances the informativeness and coherence of generated summaries in MDS.", "key_contributions": ["Introduces a topic-guided framework for MDS", "Develops a novel topic reward mechanism", "Demonstrates improved summarization performance on benchmark datasets."], "limitations": "", "keywords": ["Multi-Document Summarization", "Reinforcement Learning", "Large Language Models"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2509.09871", "pdf": "https://arxiv.org/pdf/2509.09871.pdf", "abs": "https://arxiv.org/abs/2509.09871", "title": "Emulating Public Opinion: A Proof-of-Concept of AI-Generated Synthetic Survey Responses for the Chilean Case", "authors": ["Bastin Gonzlez-Bustamante", "Nando Verelst", "Carla Cisternas"], "categories": ["cs.CL", "cs.AI", "68T50 (Primary) 91F10 (Secondary)"], "comment": "Working paper: 18 pages, 4 tables, 2 figures", "summary": "Large Language Models (LLMs) offer promising avenues for methodological and\napplied innovations in survey research by using synthetic respondents to\nemulate human answers and behaviour, potentially mitigating measurement and\nrepresentation errors. However, the extent to which LLMs recover aggregate item\ndistributions remains uncertain and downstream applications risk reproducing\nsocial stereotypes and biases inherited from training data. We evaluate the\nreliability of LLM-generated synthetic survey responses against ground-truth\nhuman responses from a Chilean public opinion probabilistic survey.\nSpecifically, we benchmark 128 prompt-model-question triplets, generating\n189,696 synthetic profiles, and pool performance metrics (i.e., accuracy,\nprecision, recall, and F1-score) in a meta-analysis across 128\nquestion-subsample pairs to test for biases along key sociodemographic\ndimensions. The evaluation spans OpenAI's GPT family and o-series reasoning\nmodels, as well as Llama and Qwen checkpoints. Three results stand out. First,\nsynthetic responses achieve excellent performance on trust items (F1-score and\naccuracy > 0.90). Second, GPT-4o, GPT-4o-mini and Llama 4 Maverick perform\ncomparably on this task. Third, synthetic-human alignment is highest among\nrespondents aged 45-59. Overall, LLM-based synthetic samples approximate\nresponses from a probabilistic sample, though with substantial item-level\nheterogeneity. Capturing the full nuance of public opinion remains challenging\nand requires careful calibration and additional distributional tests to ensure\nalgorithmic fidelity and reduce errors.", "AI": {"tldr": "This study evaluates the reliability of LLM-generated synthetic survey responses compared to human responses in a probabilistic survey setting, highlighting the strengths and limitations of using LLMs in survey research.", "motivation": "To explore the potential of LLMs in improving survey research by generating synthetic respondents that emulate human behavior while addressing measurement errors and biases.", "method": "The study benchmarks 128 prompt-model-question triplets using OpenAI's GPT family and other models, generating 189,696 synthetic profiles and analyzing accuracy, precision, recall, and F1-score in a meta-analysis.", "result": "LLM-generated synthetic responses performed well, especially on trust items (F1-score > 0.90), with notable performance from GPT-4o models and Llama 4 Maverick. Synthetic responses were closest to human responses among older respondents (aged 45-59).", "conclusion": "LLM-based synthetic samples can approximate human responses but exhibit considerable item-level variation. Further calibration and testing are necessary to maintain algorithmic fidelity and minimize errors in public opinion measurement.", "key_contributions": ["Evaluated the performance of various LLMs in generating synthetic survey responses.", "Demonstrated the strong alignment of synthetic responses with true human responses in specific survey items.", "Identified the demographic nuances in response accuracy based on respondent age."], "limitations": "The study acknowledges the challenges in capturing the full nuance of public opinion and the need for additional distributional tests.", "keywords": ["Large Language Models", "Synthetic Survey Responses", "Bias in Survey Research"], "importance_score": 8, "read_time_minutes": 18}}
{"id": "2509.09969", "pdf": "https://arxiv.org/pdf/2509.09969.pdf", "abs": "https://arxiv.org/abs/2509.09969", "title": "Large Language Models Meet Legal Artificial Intelligence: A Survey", "authors": ["Zhitian Hou", "Zihan Ye", "Nanli Zeng", "Tianyong Hao", "Kun Zeng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have significantly advanced the development of\nLegal Artificial Intelligence (Legal AI) in recent years, enhancing the\nefficiency and accuracy of legal tasks. To advance research and applications of\nLLM-based approaches in legal domain, this paper provides a comprehensive\nreview of 16 legal LLMs series and 47 LLM-based frameworks for legal tasks, and\nalso gather 15 benchmarks and 29 datasets to evaluate different legal\ncapabilities. Additionally, we analyse the challenges and discuss future\ndirections for LLM-based approaches in the legal domain. We hope this paper\nprovides a systematic introduction for beginners and encourages future research\nin this field. Resources are available at\nhttps://github.com/ZhitianHou/LLMs4LegalAI.", "AI": {"tldr": "This paper reviews the advancements of Large Language Models (LLMs) in Legal AI, covering frameworks, benchmarks, and datasets.", "motivation": "To enhance the understanding and application of LLM-based approaches in the legal domain and guide future research.", "method": "The paper reviews 16 legal LLM series, 47 LLM-based frameworks, 15 benchmarks, and 29 datasets for evaluating legal tasks.", "result": "The comprehensive review categorizes various LLMs and provides resources for evaluating their capabilities in legal applications.", "conclusion": "The paper aims to be a systematic introduction for beginners in Legal AI and encourages further research in this domain.", "key_contributions": ["Review of multiple legal LLMs and frameworks.", "Compilation of benchmarks and datasets for evaluation.", "Discussion of challenges and future research directions."], "limitations": "Focuses primarily on the legal domain, limiting applicability to other fields.", "keywords": ["Legal AI", "Large Language Models", "LLM frameworks", "Legal benchmarks", "Datasets for legal evaluation"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2509.09990", "pdf": "https://arxiv.org/pdf/2509.09990.pdf", "abs": "https://arxiv.org/abs/2509.09990", "title": "CMHG: A Dataset and Benchmark for Headline Generation of Minority Languages in China", "authors": ["Guixian Xu", "Zeli Su", "Ziyin Zhang", "Jianing Liu", "XU Han", "Ting Zhang", "Yushuang Dong"], "categories": ["cs.CL"], "comment": null, "summary": "Minority languages in China, such as Tibetan, Uyghur, and Traditional\nMongolian, face significant challenges due to their unique writing systems,\nwhich differ from international standards. This discrepancy has led to a severe\nlack of relevant corpora, particularly for supervised tasks like headline\ngeneration. To address this gap, we introduce a novel dataset, Chinese Minority\nHeadline Generation (CMHG), which includes 100,000 entries for Tibetan, and\n50,000 entries each for Uyghur and Mongolian, specifically curated for headline\ngeneration tasks. Additionally, we propose a high-quality test set annotated by\nnative speakers, designed to serve as a benchmark for future research in this\ndomain. We hope this dataset will become a valuable resource for advancing\nheadline generation in Chinese minority languages and contribute to the\ndevelopment of related benchmarks.", "AI": {"tldr": "This paper introduces a novel dataset, Chinese Minority Headline Generation (CMHG), to support headline generation tasks for minority languages in China, including Tibetan, Uyghur, and Traditional Mongolian.", "motivation": "Minority languages in China face challenges due to unique writing systems and lack of relevant corpora for tasks like headline generation.", "method": "The paper presents the CMHG dataset comprising 100,000 entries for Tibetan and 50,000 for both Uyghur and Mongolian, specifically curated for headline generation tasks. Additionally, a high-quality test set annotated by native speakers is introduced.", "result": "The CMHG dataset is expected to enhance the resources available for headline generation in Chinese minority languages and establish a benchmark for future research.", "conclusion": "This dataset can serve as a valuable resource to improve headline generation techniques for Chinese minority languages and advance related benchmarks.", "key_contributions": ["Introduction of the Chinese Minority Headline Generation dataset (CMHG).", "Creation of a test set annotated by native speakers for benchmark purposes.", "Provision of a substantial number of entries for three minority languages in China."], "limitations": "", "keywords": ["minority languages", "headline generation", "dataset", "Tibetan", "Uyghur", "Mongolian"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2509.10004", "pdf": "https://arxiv.org/pdf/2509.10004.pdf", "abs": "https://arxiv.org/abs/2509.10004", "title": "Unsupervised Hallucination Detection by Inspecting Reasoning Processes", "authors": ["Ponhvoan Srey", "Xiaobao Wu", "Anh Tuan Luu"], "categories": ["cs.CL", "cs.AI"], "comment": "To appear in EMNLP 2025", "summary": "Unsupervised hallucination detection aims to identify hallucinated content\ngenerated by large language models (LLMs) without relying on labeled data.\nWhile unsupervised methods have gained popularity by eliminating\nlabor-intensive human annotations, they frequently rely on proxy signals\nunrelated to factual correctness. This misalignment biases detection probes\ntoward superficial or non-truth-related aspects, limiting generalizability\nacross datasets and scenarios. To overcome these limitations, we propose IRIS,\nan unsupervised hallucination detection framework, leveraging internal\nrepresentations intrinsic to factual correctness. IRIS prompts the LLM to\ncarefully verify the truthfulness of a given statement, and obtain its\ncontextualized embedding as informative features for training. Meanwhile, the\nuncertainty of each response is considered a soft pseudolabel for truthfulness.\nExperimental results demonstrate that IRIS consistently outperforms existing\nunsupervised methods. Our approach is fully unsupervised, computationally low\ncost, and works well even with few training data, making it suitable for\nreal-time detection.", "AI": {"tldr": "IRIS is an unsupervised framework for detecting hallucinated content from LLMs, using internal representations for factual verification without labeled data.", "motivation": "Existing unsupervised hallucination detection methods rely on proxy signals unrelated to factual correctness, leading to biased detection and limited generalizability.", "method": "IRIS leverages the internal representations of LLMs to verify the truthfulness of statements and generates contextualized embeddings as features for training. The uncertainty of responses serves as soft pseudolabels for truthfulness.", "result": "IRIS consistently outperforms existing unsupervised detection methods, demonstrating effectiveness even with limited training data.", "conclusion": "IRIS is a cost-effective, fully unsupervised solution suitable for real-time hallucination detection.", "key_contributions": ["Introduction of IRIS framework for hallucination detection", "Utilization of LLM internal representations for training", "Effective use of response uncertainty as soft pseudolabels"], "limitations": "", "keywords": ["hallucination detection", "unsupervised learning", "large language models"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2509.10010", "pdf": "https://arxiv.org/pdf/2509.10010.pdf", "abs": "https://arxiv.org/abs/2509.10010", "title": "Multi-Intent Recognition in Dialogue Understanding: A Comparison Between Smaller Open-Source LLMs", "authors": ["Adnan Ahmad", "Philine Kowol", "Stefan Hillmann", "Sebastian Mller"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "In this paper, we provide an extensive analysis of multi-label intent\nclassification using Large Language Models (LLMs) that are open-source,\npublicly available, and can be run in consumer hardware. We use the MultiWOZ\n2.1 dataset, a benchmark in the dialogue system domain, to investigate the\nefficacy of three popular open-source pre-trained LLMs, namely LLama2-7B-hf,\nMistral-7B-v0.1, and Yi-6B. We perform the classification task in a few-shot\nsetup, giving 20 examples in the prompt with some instructions. Our approach\nfocuses on the differences in performance of these models across several\nperformance metrics by methodically assessing these models on multi-label\nintent classification tasks. Additionally, we compare the performance of the\ninstruction-based fine-tuning approach with supervised learning using the\nsmaller transformer model BertForSequenceClassification as a baseline. To\nevaluate the performance of the models, we use evaluation metrics like\naccuracy, precision, and recall as well as micro, macro, and weighted F1 score.\nWe also report the inference time, VRAM requirements, etc. The Mistral-7B-v0.1\noutperforms two other generative models on 11 intent classes out of 14 in terms\nof F-Score, with a weighted average of 0.50. It also has relatively lower\nHumming Loss and higher Jaccard Similarity, making it the winning model in the\nfew-shot setting. We find BERT based supervised classifier having superior\nperformance compared to the best performing few-shot generative LLM. The study\nprovides a framework for small open-source LLMs in detecting complex\nmulti-intent dialogues, enhancing the Natural Language Understanding aspect of\ntask-oriented chatbots.", "AI": {"tldr": "This paper analyzes multi-label intent classification using open-source LLMs on the MultiWOZ 2.1 dataset, comparing the performance of three models and providing insights into few-shot learning efficacy.", "motivation": "To assess the efficacy of open-source large language models for multi-label intent classification in dialogue systems, and enhance natural language understanding in task-oriented chatbots.", "method": "Evaluated three LLMs (LLama2-7B-hf, Mistral-7B-v0.1, Yi-6B) on the MultiWOZ 2.1 dataset under a few-shot setup, comparing their performance with supervised learning using BERT as a baseline, and using metrics such as accuracy, precision, recall, and various F1 scores.", "result": "Mistral-7B-v0.1 outperformed the other two models on 11 out of 14 intent classes in F-Score, while a BERT-based classifier showed superior performance compared to the best-performing few-shot LLM.", "conclusion": "The study demonstrates the potential of small open-source LLMs in handling complex multi-intent dialogues, while revealing that traditional supervised models can achieve better results in certain cases.", "key_contributions": ["Analysis of multi-label intent classification using open-source pre-trained LLMs on consumer hardware", "Comparison of few-shot and supervised learning approaches for classification tasks", "Framework for small LLMs in improving natural language understanding in chatbots"], "limitations": "Focused on a specific dataset (MultiWOZ 2.1) and types of models, which may not generalize across all applications or settings.", "keywords": ["multi-label intent classification", "large language models", "Natural Language Understanding", "few-shot learning", "dialogue systems"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.10035", "pdf": "https://arxiv.org/pdf/2509.10035.pdf", "abs": "https://arxiv.org/abs/2509.10035", "title": "Linguistic trajectories of bipolar disorder on social media", "authors": ["Laurin Plank", "Armin Zlomuzica"], "categories": ["cs.CL"], "comment": "Pre-print", "summary": "Language provides valuable markers of affective disorders such as bipolar\ndisorder (BD), yet clinical assessments remain limited in scale. In response,\nanalyses of social media (SM) language have gained prominence due to their high\ntemporal resolution and longitudinal scope. Here, we introduce a method to\ndetermine the timing of users' diagnoses and apply it to study language\ntrajectories from 3 years before to 21 years after BD diagnosis - contrasted\nwith uses reporting unipolar depression (UD) and non-affected users (HC). We\nshow that BD diagnosis is accompanied by pervasive linguistic alterations\nreflecting mood disturbance, psychiatric comorbidity, substance abuse,\nhospitalization, medical comorbidities, unusual thought content, and\ndisorganized thought. We further observe recurring mood-related language\nchanges across two decades after the diagnosis, with a pronounced 12-month\nperiodicity suggestive of seasonal mood episodes. Finally, trend-level evidence\nsuggests an increased periodicity in users estimated to be female. In sum, our\nfindings provide evidence for language alterations in the acute and chronic\nphase of BD. This validates and extends recent efforts leveraging SM for\nscalable monitoring of mental health.", "AI": {"tldr": "Study analyzes social media language to track linguistic changes in users before and after bipolar disorder diagnosis.", "motivation": "To investigate the temporal language changes associated with bipolar disorder using social media data, addressing the limited scale of traditional clinical assessments.", "method": "Analyzed language trajectories from social media posts of users diagnosed with bipolar disorder, unipolar depression, and non-affected individuals over a period spanning from three years before to twenty-one years after diagnosis.", "result": "Key findings reveal significant linguistic alterations in bipolar disorder patients, reflecting various aspects of the disorder including mood disturbance and comorbidities, alongside recurring language changes over time.", "conclusion": "The study underscores the potential of social media analysis as a scalable method for monitoring mental health through linguistic markers, particularly for bipolar disorder.", "key_contributions": ["Introduced a method to track language changes over time related to bipolar disorder diagnosis.", "Showed pervasive linguistic alterations associated with bipolar disorder across an extended timeline.", "Provided evidence for the periodicity of mood-related language changes.", "Revealed potential gender differences in language periodicity."], "limitations": "Focused primarily on social media language, which may not represent broader contexts or diverse populations.", "keywords": ["bipolar disorder", "social media", "language analysis", "mental health", "affective disorders"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.10040", "pdf": "https://arxiv.org/pdf/2509.10040.pdf", "abs": "https://arxiv.org/abs/2509.10040", "title": "!MSA at BAREC Shared Task 2025: Ensembling Arabic Transformers for Readability Assessment", "authors": ["Mohamed Basem", "Mohamed Younes", "Seif Ahmed", "Abdelrahman Moustafa"], "categories": ["cs.CL"], "comment": "10 Pages , 8 figures , ArabicNLP 2025 , Co-located with EMNLP 2025", "summary": "We present MSAs winning system for the BAREC 2025 Shared Task on fine-grained\nArabic readability assessment, achieving first place in six of six tracks. Our\napproach is a confidence-weighted ensemble of four complementary transformer\nmodels (AraBERTv2, AraELECTRA, MARBERT, and CAMeLBERT) each fine-tuned with\ndistinct loss functions to capture diverse readability signals. To tackle\nsevere class imbalance and data scarcity, we applied weighted training,\nadvanced preprocessing, SAMER corpus relabeling with our strongest model, and\nsynthetic data generation via Gemini 2.5 Flash, adding about 10,000 rare-level\nsamples. A targeted post-processing step corrected prediction distribution\nskew, delivering a 6.3 percent Quadratic Weighted Kappa (QWK) gain. Our system\nreached 87.5 percent QWK at the sentence level and 87.4 percent at the document\nlevel, demonstrating the power of model and loss diversity, confidence-informed\nfusion, and intelligent augmentation for robust Arabic readability prediction.", "AI": {"tldr": "The study presents a winning system for Arabic readability assessment using a diverse ensemble of transformer models and advanced techniques to address data challenges.", "motivation": "To improve Arabic readability assessment and address challenges such as class imbalance and data scarcity.", "method": "A confidence-weighted ensemble of four transformer models, fine-tuned with distinct loss functions, including data generation and targeted post-processing to enhance prediction accuracy.", "result": "Achieved 87.5% QWK at the sentence level and 87.4% at the document level, with a 6.3% gain in QWK through intelligent augmentation and model diversity.", "conclusion": "The approach demonstrates success in robust Arabic readability prediction by leveraging ensemble learning and sophisticated training techniques.", "key_contributions": ["Winning system for BAREC 2025 Shared Task", "Confidence-weighted ensemble of diverse transformer models", "Innovative use of data augmentation techniques"], "limitations": "", "keywords": ["Arabic readability", "Transformer models", "Data augmentation"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2509.10078", "pdf": "https://arxiv.org/pdf/2509.10078.pdf", "abs": "https://arxiv.org/abs/2509.10078", "title": "Established Psychometric vs. Ecologically Valid Questionnaires: Rethinking Psychological Assessments in Large Language Models", "authors": ["Dongmin Choi", "Woojung Song", "Jongwook Han", "Eun-Ju Lee", "Yohan Jo"], "categories": ["cs.CL", "cs.AI"], "comment": "17 pages, 4 figures", "summary": "Researchers have applied established psychometric questionnaires (e.g., BFI,\nPVQ) to measure the personality traits and values reflected in the responses of\nLarge Language Models (LLMs). However, concerns have been raised about applying\nthese human-designed questionnaires to LLMs. One such concern is their lack of\necological validity--the extent to which survey questions adequately reflect\nand resemble real-world contexts in which LLMs generate texts in response to\nuser queries. However, it remains unclear how established questionnaires and\necologically valid questionnaires differ in their outcomes, and what insights\nthese differences may provide. In this paper, we conduct a comprehensive\ncomparative analysis of the two types of questionnaires. Our analysis reveals\nthat established questionnaires (1) yield substantially different profiles of\nLLMs from ecologically valid ones, deviating from the psychological\ncharacteristics expressed in the context of user queries, (2) suffer from\ninsufficient items for stable measurement, (3) create misleading impressions\nthat LLMs possess stable constructs, and (4) yield exaggerated profiles for\npersona-prompted LLMs. Overall, our work cautions against the use of\nestablished psychological questionnaires for LLMs. Our code will be released\nupon publication.", "AI": {"tldr": "This paper analyzes the effectiveness of traditional psychometric questionnaires in assessing Large Language Models (LLMs), highlighting significant discrepancies when compared to ecologically valid questionnaires.", "motivation": "To address concerns about the ecological validity of established psychometric questionnaires used to assess personality traits and values in LLMs, and to understand the implications of these differences.", "method": "A comprehensive comparative analysis of established psychometric questionnaires and ecologically valid questionnaires, examining how each type reflects the psychological characteristics of LLMs in user query contexts.", "result": "The analysis reveals that established questionnaires provide misleading profiles of LLMs, suffer from inadequate measurement stability, and exaggerate the stability of LLM constructs, particularly in persona-prompted scenarios.", "conclusion": "The findings caution against employing established psychological questionnaires for LLM assessments, emphasizing the need for more ecologically valid measurement tools.", "key_contributions": ["Identified significant differences in profiles produced by established vs. ecologically valid questionnaires for LLMs.", "Highlighted the limitations of established questionnaires in accurately measuring LLM characteristics.", "Emphasized the necessity of ecological validity in assessing LLMs."], "limitations": "The study focuses primarily on the comparative effectiveness of questionnaires without addressing potential alternative assessment methods for LLMs.", "keywords": ["Large Language Models", "psychometric questionnaires", "ecological validity", "personality assessment", "machine learning"], "importance_score": 8, "read_time_minutes": 17}}
{"id": "2509.10087", "pdf": "https://arxiv.org/pdf/2509.10087.pdf", "abs": "https://arxiv.org/abs/2509.10087", "title": "Querying Climate Knowledge: Semantic Retrieval for Scientific Discovery", "authors": ["Mustapha Adamu", "Qi Zhang", "Huitong Pan", "Longin Jan Latecki", "Eduard C. Dragut"], "categories": ["cs.CL"], "comment": "ACM SIGIR 2025 Workshop MANILA", "summary": "The growing complexity and volume of climate science literature make it\nincreasingly difficult for researchers to find relevant information across\nmodels, datasets, regions, and variables. This paper introduces a\ndomain-specific Knowledge Graph (KG) built from climate publications and\nbroader scientific texts, aimed at improving how climate knowledge is accessed\nand used. Unlike keyword based search, our KG supports structured, semantic\nqueries that help researchers discover precise connections such as which models\nhave been validated in specific regions or which datasets are commonly used\nwith certain teleconnection patterns. We demonstrate how the KG answers such\nquestions using Cypher queries, and outline its integration with large language\nmodels in RAG systems to improve transparency and reliability in\nclimate-related question answering. This work moves beyond KG construction to\nshow its real world value for climate researchers, model developers, and others\nwho rely on accurate, contextual scientific information.", "AI": {"tldr": "Introduction of a Knowledge Graph for climate science literature to enhance information retrieval and connection discovery.", "motivation": "To address the challenges posed by the vast and complex climate science literature that makes it difficult for researchers to find relevant information.", "method": "Development of a domain-specific Knowledge Graph (KG) built from climate publications, enabling structured semantic queries.", "result": "The KG effectively assists in discovering connections between models, datasets, regions, and teleconnection patterns using Cypher queries and integrates with LLMs in RAG systems for improved information retrieval.", "conclusion": "The study demonstrates the real-world applicability of the KG for climate researchers and model developers, emphasizing its utility for accurate scientific information retrieval.", "key_contributions": ["Introduction of a Knowledge Graph for climate science literature.", "Support for structured, semantic queries to improve information access.", "Integration with large language models for enhanced question answering."], "limitations": "", "keywords": ["Knowledge Graph", "Climate Science", "Semantic Queries", "Large Language Models", "Information Retrieval"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2509.10095", "pdf": "https://arxiv.org/pdf/2509.10095.pdf", "abs": "https://arxiv.org/abs/2509.10095", "title": "Arabic Large Language Models for Medical Text Generation", "authors": ["Abdulrahman Allam", "Seif Ahmed", "Ali Hamdi", "Ammar Mohammed"], "categories": ["cs.CL"], "comment": "Published in 2025 4th International Conference on Computer\n  Technologies (ICCTech)", "summary": "Efficient hospital management systems (HMS) are critical worldwide to address\nchallenges such as overcrowding, limited resources, and poor availability of\nurgent health care. Existing methods often lack the ability to provide\naccurate, real-time medical advice, particularly for irregular inputs and\nunderrepresented languages. To overcome these limitations, this study proposes\nan approach that fine-tunes large language models (LLMs) for Arabic medical\ntext generation. The system is designed to assist patients by providing\naccurate medical advice, diagnoses, drug recommendations, and treatment plans\nbased on user input. The research methodology required the collection of a\nunique dataset from social media platforms, capturing real-world medical\nconversations between patients and doctors. The dataset, which includes patient\ncomplaints together with medical advice, was properly cleaned and preprocessed\nto account for multiple Arabic dialects. Fine-tuning state-of-the-art\ngenerative models, such as Mistral-7B-Instruct-v0.2, LLaMA-2-7B, and GPT-2\nMedium, optimized the system's ability to generate reliable medical text.\nResults from evaluations indicate that the fine-tuned Mistral-7B model\noutperformed the other models, achieving average BERT (Bidirectional Encoder\nRepresentations from Transformers) Score values in precision, recall, and\nF1-scores of 68.5\\%, 69.08\\%, and 68.5\\%, respectively. Comparative\nbenchmarking and qualitative assessments validate the system's ability to\nproduce coherent and relevant medical replies to informal input. This study\nhighlights the potential of generative artificial intelligence (AI) in\nadvancing HMS, offering a scalable and adaptable solution for global healthcare\nchallenges, especially in linguistically and culturally diverse environments.", "AI": {"tldr": "This study presents a fine-tuned LLM for Arabic medical text generation to improve hospital management systems by providing accurate medical advice and treatment recommendations based on user input. It utilizes a unique dataset gathered from social media that captures real-world medical conversations.", "motivation": "To address challenges in hospital management systems, including overcrowding and the lack of accurate, real-time medical advice, especially for underrepresented languages like Arabic.", "method": "The methodology involved collecting and preprocessing a unique dataset from social media, fine-tuning large language models including Mistral-7B-Instruct-v0.2, LLaMA-2-7B, and GPT-2 Medium to enhance the generation of medical text.", "result": "The fine-tuned Mistral-7B model outperformed others, achieving average BERT Score values of 68.5% in precision, 69.08% in recall, and 68.5% in F1-scores. The model produced coherent and relevant medical responses to informal user input.", "conclusion": "The study demonstrates the potential of generative AI in enhancing hospital management systems, providing a scalable solution for healthcare challenges in linguistically diverse environments.", "key_contributions": ["Fine-tuned LLMs specifically for Arabic medical text generation.", "Utilization of a real-world dataset for improving the relevance of medical advice.", "Evaluation and benchmarking of multiple generative models."], "limitations": "", "keywords": ["hospital management systems", "large language models", "Arabic medical text generation", "generative artificial intelligence", "healthcare challenges"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.10108", "pdf": "https://arxiv.org/pdf/2509.10108.pdf", "abs": "https://arxiv.org/abs/2509.10108", "title": "Scaling Arabic Medical Chatbots Using Synthetic Data: Enhancing Generative AI with Synthetic Patient Records", "authors": ["Abdulrahman Allam", "Seif Ahmed", "Ali Hamdi", "Khaled Shaban"], "categories": ["cs.CL"], "comment": "Accepted in AICCSA 2025", "summary": "The development of medical chatbots in Arabic is significantly constrained by\nthe scarcity of large-scale, high-quality annotated datasets. While prior\nefforts compiled a dataset of 20,000 Arabic patient-doctor interactions from\nsocial media to fine-tune large language models (LLMs), model scalability and\ngeneralization remained limited. In this study, we propose a scalable synthetic\ndata augmentation strategy to expand the training corpus to 100,000 records.\nUsing advanced generative AI systems ChatGPT-4o and Gemini 2.5 Pro we generated\n80,000 contextually relevant and medically coherent synthetic question-answer\npairs grounded in the structure of the original dataset. These synthetic\nsamples were semantically filtered, manually validated, and integrated into the\ntraining pipeline. We fine-tuned five LLMs, including Mistral-7B and AraGPT2,\nand evaluated their performance using BERTScore metrics and expert-driven\nqualitative assessments. To further analyze the effectiveness of synthetic\nsources, we conducted an ablation study comparing ChatGPT-4o and\nGemini-generated data independently. The results showed that ChatGPT-4o data\nconsistently led to higher F1-scores and fewer hallucinations across all\nmodels. Overall, our findings demonstrate the viability of synthetic\naugmentation as a practical solution for enhancing domain-specific language\nmodels in-low resource medical NLP, paving the way for more inclusive,\nscalable, and accurate Arabic healthcare chatbot systems.", "AI": {"tldr": "The study proposes a synthetic data augmentation strategy to enhance Arabic medical chatbots by expanding the training dataset from 20,000 to 100,000 records.", "motivation": "To address the challenges in developing medical chatbots in Arabic due to the lack of large-scale, high-quality annotated datasets.", "method": "The authors used generative AI systems ChatGPT-4o and Gemini 2.5 Pro to generate 80,000 synthetic question-answer pairs, which were filtered and validated, then used to fine-tune five LLMs including Mistral-7B and AraGPT2.", "result": "The augmentation strategy showed improved performance with higher F1-scores and fewer hallucinations using ChatGPT-4o data across all models evaluated.", "conclusion": "Synthetic data augmentation is a viable solution for enhancing domain-specific language models in low-resource medical NLP, supporting the development of more inclusive and accurate Arabic healthcare chatbots.", "key_contributions": ["Proposed a scalable synthetic data augmentation strategy for medical NLP.", "Generated 80,000 synthetic question-answer pairs validated for medical coherence.", "Showed performance improvements in LLMs using augmented data."], "limitations": "", "keywords": ["synthetic data", "medical chatbots", "Arabic NLP", "language models", "data augmentation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.10116", "pdf": "https://arxiv.org/pdf/2509.10116.pdf", "abs": "https://arxiv.org/abs/2509.10116", "title": "Prominence-aware automatic speech recognition for conversational speech", "authors": ["Julian Linke", "Barbara Schuppler"], "categories": ["cs.CL", "eess.AS"], "comment": null, "summary": "This paper investigates prominence-aware automatic speech recognition (ASR)\nby combining prominence detection and speech recognition for conversational\nAustrian German. First, prominence detectors were developed by fine-tuning\nwav2vec2 models to classify word-level prominence. The detector was then used\nto automatically annotate prosodic prominence in a large corpus. Based on those\nannotations, we trained novel prominence-aware ASR systems that simultaneously\ntranscribe words and their prominence levels. The integration of prominence\ninformation did not change performance compared to our baseline ASR system,\nwhile reaching a prominence detection accuracy of 85.53% for utterances where\nthe recognized word sequence was correct. This paper shows that\ntransformer-based models can effectively encode prosodic information and\nrepresents a novel contribution to prosody-enhanced ASR, with potential\napplications for linguistic research and prosody-informed dialogue systems.", "AI": {"tldr": "The paper explores prominence-aware automatic speech recognition (ASR) for conversational Austrian German by combining prominence detection with speech transcription.", "motivation": "To enhance automatic speech recognition by integrating prosodic prominence detection, thereby improving dialogue systems and linguistic research.", "method": "Fine-tuning wav2vec2 models to classify word-level prominence and using the resulting detector for annotating a large corpus before training prominence-aware ASR systems.", "result": "Achieved a prominence detection accuracy of 85.53% in correct utterances, although the integration of prominence information did not improve overall ASR performance compared to the baseline.", "conclusion": "Transformer-based models effectively encode prosodic information, providing a significant contribution to prosody-aware ASR, with implications for dialogue systems.", "key_contributions": ["Development of prominence detectors using fine-tuned wav2vec2 models", "Introduction of prominence-aware ASR systems", "Demonstrated effectiveness of transformer models in encoding prosodic information"], "limitations": "", "keywords": ["prominence detection", "automatic speech recognition", "wav2vec2", "prosody", "dialogue systems"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2509.10127", "pdf": "https://arxiv.org/pdf/2509.10127.pdf", "abs": "https://arxiv.org/abs/2509.10127", "title": "Population-Aligned Persona Generation for LLM-based Social Simulation", "authors": ["Zhengyu Hu", "Zheyuan Xiao", "Max Xiong", "Yuxuan Lei", "Tianfu Wang", "Jianxun Lian", "Kaize Ding", "Ziang Xiao", "Nicholas Jing Yuan", "Xing Xie"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent advances in large language models (LLMs) have enabled human-like\nsocial simulations at unprecedented scale and fidelity, offering new\nopportunities for computational social science. A key challenge, however, is\nthe construction of persona sets that authentically represent the diversity and\ndistribution of real-world populations. Most existing LLM-based social\nsimulation studies focus primarily on designing agentic frameworks and\nsimulation environments, often overlooking the complexities of persona\ngeneration and the potential biases introduced by unrepresentative persona\nsets. In this paper, we propose a systematic framework for synthesizing\nhigh-quality, population-aligned persona sets for LLM-driven social simulation.\nOur approach begins by leveraging LLMs to generate narrative personas from\nlong-term social media data, followed by rigorous quality assessment to filter\nout low-fidelity profiles. We then apply importance sampling to achieve global\nalignment with reference psychometric distributions, such as the Big Five\npersonality traits. To address the needs of specific simulation contexts, we\nfurther introduce a task-specific module that adapts the globally aligned\npersona set to targeted subpopulations. Extensive experiments demonstrate that\nour method significantly reduces population-level bias and enables accurate,\nflexible social simulation for a wide range of research and policy\napplications.", "AI": {"tldr": "This paper presents a framework for generating high-quality, population-aligned persona sets using LLMs for social simulations, addressing the biases in existing frameworks.", "motivation": "The paper is motivated by the need for diverse and representative persona sets in LLM-driven social simulations, as most existing studies overlook persona generation complexities and bias.", "method": "The proposed method uses LLMs to generate narrative personas from social media data, followed by quality assessments to filter profiles and importance sampling for alignment with psychometric distributions.", "result": "Extensive experiments show that the method reduces population-level bias and allows for accurate and flexible social simulations.", "conclusion": "A systematic framework for persona generation helps improve the fidelity of simulations, contributing meaningfully to research and policy applications.", "key_contributions": ["Framework for generating population-aligned persona sets", "Importance sampling for global alignment with psychometric traits", "Task-specific module for adapting personas to subpopulations"], "limitations": "", "keywords": ["large language models", "social simulations", "persona generation", "bias mitigation", "psychometric distributions"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.10129", "pdf": "https://arxiv.org/pdf/2509.10129.pdf", "abs": "https://arxiv.org/abs/2509.10129", "title": "Towards Reliable and Interpretable Document Question Answering via VLMs", "authors": ["Alessio Chen", "Simone Giovannini", "Andrea Gemelli", "Fabio Coppini", "Simone Marinai"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Vision-Language Models (VLMs) have shown strong capabilities in document\nunderstanding, particularly in identifying and extracting textual information\nfrom complex documents. Despite this, accurately localizing answers within\ndocuments remains a major challenge, limiting both interpretability and\nreal-world applicability. To address this, we introduce\n\\textit{DocExplainerV0}, a plug-and-play bounding-box prediction module that\ndecouples answer generation from spatial localization. This design makes it\napplicable to existing VLMs, including proprietary systems where fine-tuning is\nnot feasible. Through systematic evaluation, we provide quantitative insights\ninto the gap between textual accuracy and spatial grounding, showing that\ncorrect answers often lack reliable localization. Our standardized framework\nhighlights these shortcomings and establishes a benchmark for future research\ntoward more interpretable and robust document information extraction VLMs.", "AI": {"tldr": "DocExplainerV0 is a bounding-box prediction module designed to improve spatial localization in Vision-Language Models (VLMs) for document understanding, addressing the challenge of accurately locating answers in complex documents.", "motivation": "Despite strong performance in document understanding, VLMs struggle with accurately localizing answers, which affects their interpretability and real-world applicability.", "method": "The authors introduce DocExplainerV0, a plug-and-play bounding-box prediction module that separates answer generation from spatial localization, making it compatible with existing VLMs.", "result": "Systematic evaluation reveals a significant gap between textual accuracy and spatial grounding, demonstrating that many correct answers lack reliable localization.", "conclusion": "The standardized framework established by this work provides quantitative insights into document information extraction and serves as a benchmark for future research.", "key_contributions": ["Introduction of the DocExplainerV0 module", "Decoupling answer generation from spatial localization", "Establishment of a benchmark for VLMs in document understanding"], "limitations": "Focuses on the bounding-box prediction aspect and may not address other components of VLMs.", "keywords": ["Vision-Language Models", "document understanding", "bounding-box prediction", "localization", "information extraction"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2509.10179", "pdf": "https://arxiv.org/pdf/2509.10179.pdf", "abs": "https://arxiv.org/abs/2509.10179", "title": "Benchmark of stylistic variation in LLM-generated texts", "authors": ["Ji Milika", "Anna Marklov", "Vclav Cvrek"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This study investigates the register variation in texts written by humans and\ncomparable texts produced by large language models (LLMs). Biber's\nmultidimensional analysis (MDA) is applied to a sample of human-written texts\nand AI-created texts generated to be their counterparts to find the dimensions\nof variation in which LLMs differ most significantly and most systematically\nfrom humans. As textual material, a new LLM-generated corpus AI-Brown is used,\nwhich is comparable to BE-21 (a Brown family corpus representing contemporary\nBritish English). Since all languages except English are underrepresented in\nthe training data of frontier LLMs, similar analysis is replicated on Czech\nusing AI-Koditex corpus and Czech multidimensional model. Examined were 16\nfrontier models in various settings and prompts, with emphasis placed on the\ndifference between base models and instruction-tuned models. Based on this, a\nbenchmark is created through which models can be compared with each other and\nranked in interpretable dimensions.", "AI": {"tldr": "The study examines the differences in textual variation between human-written texts and those produced by large language models (LLMs) using multidimensional analysis.", "motivation": "To understand how LLMs differ from human writing and establish benchmarks for comparison.", "method": "Biber's multidimensional analysis (MDA) was applied to human and LLM-generated texts, leveraging the AI-Brown corpus for English and the AI-Koditex corpus for Czech.", "result": "The analysis finds significant and systematic differences in textual features between human and LLM-generated content, revealing insights into model performance under various conditions.", "conclusion": "A benchmark for comparing LLMs is created based on the significant dimensions of variation identified in the study.", "key_contributions": ["Application of Biber's MDA to distinguish human and LLM texts.", "Creation of AI-Brown and AI-Koditex corpora for analysis.", "Development of a benchmark for interpreting and ranking LLMs."], "limitations": "", "keywords": ["Large Language Models", "Multidimensional Analysis", "Textual Variation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.10184", "pdf": "https://arxiv.org/pdf/2509.10184.pdf", "abs": "https://arxiv.org/abs/2509.10184", "title": "Incongruent Positivity: When Miscalibrated Positivity Undermines Online Supportive Conversations", "authors": ["Leen Almajed", "Abeer ALdayel"], "categories": ["cs.CL"], "comment": "This paper is under review", "summary": "In emotionally supportive conversations, well-intended positivity can\nsometimes misfire, leading to responses that feel dismissive, minimizing, or\nunrealistically optimistic. We examine this phenomenon of incongruent\npositivity as miscalibrated expressions of positive support in both human and\nLLM generated responses. To this end, we collected real user-assistant\ndialogues from Reddit across a range of emotional intensities and generated\nadditional responses using large language models for the same context. We\ncategorize these conversations by intensity into two levels: Mild, which covers\nrelationship tension and general advice, and Severe, which covers grief and\nanxiety conversations. This level of categorization enables a comparative\nanalysis of how supportive responses vary across lower and higher stakes\ncontexts. Our analysis reveals that LLMs are more prone to unrealistic\npositivity through dismissive and minimizing tone, particularly in high-stakes\ncontexts. To further study the underlying dimensions of this phenomenon, we\nfinetune LLMs on datasets with strong and weak emotional reactions. Moreover,\nwe developed a weakly supervised multilabel classifier ensemble (DeBERTa and\nMentalBERT) that shows improved detection of incongruent positivity types\nacross two sorts of concerns (Mild and Severe). Our findings shed light on the\nneed to move beyond merely generating generic positive responses and instead\nstudy the congruent support measures to balance positive affect with emotional\nacknowledgment. This approach offers insights into aligning large language\nmodels with affective expectations in the online supportive dialogue, paving\nthe way toward context-aware and trust preserving online conversation systems.", "AI": {"tldr": "This paper investigates the miscalibration of positive support in emotionally supportive dialogues, comparing human and LLM-generated responses, and proposes improvements for detecting misaligned positivity.", "motivation": "To understand the detrimental effects of incongruent positivity in supportive conversations, particularly in how LLMs generate responses that may appear dismissive or overly optimistic.", "method": "The study involves collecting user-assistant dialogues from Reddit, categorizing them by emotional intensity, and generating additional responses using LLMs. It includes a comparative analysis of supportive responses and develops a weakly supervised classifier to identify incongruent positivity.", "result": "The analysis shows that LLMs exhibit unrealistic positivity, especially in high-stakes conversations. The classifier ensemble improves detection of various incongruent positivity types across different emotional concerns.", "conclusion": "The study highlights the necessity for LLMs to produce congruent supportive responses that acknowledge emotions rather than defaulting to generic positivity, suggesting paths for enhancing conversational AI systems.", "key_contributions": ["Identification of incongruent positivity in LLM responses.", "Development of a novel classifier ensemble for detecting emotional alignment in conversations.", "Insights into improving LLM support mechanisms in emotionally charged dialogues."], "limitations": "", "keywords": ["Emotional Support", "LLM Responses", "Incongruent Positivity", "Human-Computer Interaction", "Context-Aware AI"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2509.10199", "pdf": "https://arxiv.org/pdf/2509.10199.pdf", "abs": "https://arxiv.org/abs/2509.10199", "title": "Beyond Token Limits: Assessing Language Model Performance on Long Text Classification", "authors": ["Mikls Sebk", "Viktor Kovcs", "Martin Bnczy", "Daniel Mller Eriksen", "Nathalie Neptune", "Philippe Roussille"], "categories": ["cs.CL", "I.7; I.2; J.4"], "comment": null, "summary": "The most widely used large language models in the social sciences (such as\nBERT, and its derivatives, e.g. RoBERTa) have a limitation on the input text\nlength that they can process to produce predictions. This is a particularly\npressing issue for some classification tasks, where the aim is to handle long\ninput texts. One such area deals with laws and draft laws (bills), which can\nhave a length of multiple hundred pages and, therefore, are not particularly\namenable for processing with models that can only handle e.g. 512 tokens. In\nthis paper, we show results from experiments covering 5 languages with\nXLM-RoBERTa, Longformer, GPT-3.5, GPT-4 models for the multiclass\nclassification task of the Comparative Agendas Project, which has a codebook of\n21 policy topic labels from education to health care. Results show no\nparticular advantage for the Longformer model, pre-trained specifically for the\npurposes of handling long inputs. The comparison between the GPT variants and\nthe best-performing open model yielded an edge for the latter. An analysis of\nclass-level factors points to the importance of support and substance overlaps\nbetween specific categories when it comes to performance on long text inputs.", "AI": {"tldr": "This paper investigates the challenges of processing long input texts in large language models (LLMs) for classification tasks, specifically in the context of legislative documents across multiple languages.", "motivation": "The limitation of input text length in widely used LLMs restricts their applicability to classification tasks involving long texts, such as laws and draft laws.", "method": "Experiments with XLM-RoBERTa, Longformer, GPT-3.5, and GPT-4 were conducted on a multiclass classification task using a codebook of 21 policy topic labels across 5 languages.", "result": "The results showed no advantage for Longformer over the GPT variants; however, the best-performing open model outperformed the GPT models in long text processing.", "conclusion": "Class-level analysis revealed that support and substance overlaps between specific categories significantly affect performance on long text inputs.", "key_contributions": ["Analysis of LLM performance on long input texts for policy classification", "Comparison of models including XLM-RoBERTa, Longformer, GPT-3.5, and GPT-4", "Insights into class-level factors influencing text classification outcomes"], "limitations": "", "keywords": ["long text processing", "multiclass classification", "large language models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.10208", "pdf": "https://arxiv.org/pdf/2509.10208.pdf", "abs": "https://arxiv.org/abs/2509.10208", "title": "SI-FACT: Mitigating Knowledge Conflict via Self-Improving Faithfulness-Aware Contrastive Tuning", "authors": ["Shengqiang Fu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models often generate unfaithful responses in knowledge\nintensive tasks due to knowledge conflict,that is,a preference for relying on\ninternal parametric knowledge rather than the provided context.To address this\nissue,we propose a novel self improving framework,Self Improving Faithfulness\nAware Contrastive Tuning.The framework uses a self instruct mechanism that\nallows the base LLM to automatically generate high quality,structured\ncontrastive learning data,including anchor samples,semantically equivalent\npositive samples,and negative samples simulating unfaithful scenarios.This\napproach significantly reduces the cost of manual\nannotation.Subsequently,contrastive learning is applied to train the\nmodel,enabling it to pull faithful responses closer and push unfaithful\nresponses farther apart in the representation space.Experiments on knowledge\nconflict evaluation benchmarks ECARE KRE and COSE KRE show that the SI FACT\nmodel based on Llama3 8B Instruct improves the Contextual Recall Rate by 6.2%\nover the best baseline method,while significantly reducing dependence on\ninternal memory.The results indicate that SI FACT provides strong effectiveness\nand high data efficiency in enhancing the contextual faithfulness of\nLLMs,offering a practical pathway toward building more proactive and\ntrustworthy language models.", "AI": {"tldr": "A framework called Self Improving Faithfulness Aware Contrastive Tuning (SI FACT) enhances the contextual faithfulness of large language models (LLMs) by generating structured contrastive learning data and applying contrastive learning techniques.", "motivation": "LLMs often generate unfaithful responses due to knowledge conflicts, where they rely more on internal knowledge than on contextual information.", "method": "The SI FACT framework utilizes a self-instruct mechanism for automatic generation of high-quality contrastive learning data, including anchor samples, semantically equivalent positive samples, and negative samples that simulate unfaithful scenarios. Contrastive learning is then applied for training.", "result": "The SI FACT model (based on Llama3 8B Instruct) shows a 6.2% improvement in Contextual Recall Rate over the best baseline while reducing reliance on internal memory, demonstrating strong effectiveness and data efficiency.", "conclusion": "SI FACT provides a practical approach to enhancing the faithfulness of LLM responses, contributing to the development of more reliable language models.", "key_contributions": ["Novel framework for improving LLM faithfulness", "Automatic generation of contrastive learning data", "Significant performance improvement on contextual recall benchmarks"], "limitations": "", "keywords": ["Large Language Models", "Contextual Faithfulness", "Contrastive Learning"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2509.10377", "pdf": "https://arxiv.org/pdf/2509.10377.pdf", "abs": "https://arxiv.org/abs/2509.10377", "title": "Dropping Experts, Recombining Neurons: Retraining-Free Pruning for Sparse Mixture-of-Experts LLMs", "authors": ["Yixiao Zhou", "Ziyu Zhao", "Dongzhou Cheng", "zhiliang wu", "Jie Gui", "Yi Yang", "Fei Wu", "Yu Cheng", "Hehe Fan"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP2025", "summary": "Sparse Mixture-of-Experts (SMoE) architectures are widely used in large\nlanguage models (LLMs) due to their computational efficiency. However, though\nonly a few experts are activated for each token, SMoE still requires loading\nall expert parameters, leading to high memory usage and challenges in\ndeployment. Previous work has tried to reduce the overhead by pruning and\nmerging experts, but primarily focused on expert-level operations, leaving\nneuron-level structure underexplored. We propose DERN (Dropping Experts,\nRecombining Neurons), a task-agnostic and retraining-free framework for expert\npruning and reconstruction. We observe that experts are often misaligned and\ncontain semantic conflicts at the neuron level, which poses challenges for\ndirect merging. To solve this, DERN works in three steps: it first prunes\nredundant experts using router statistics; then it decomposes them into\nneuron-level expert segments, assigning each segment to its most compatible\nretained expert; and finally, it merges segments within each retained expert to\nbuild a compact representation. Experiments on Mixtral, Qwen, and DeepSeek SMoE\nmodels show that DERN improves performance by more than 5% on commonsense\nreasoning and MMLU benchmarks under 50% expert sparsity, without extra\ntraining. It also greatly reduces the number of experts and memory usage,\nmaking SMoE LLMs easier to deploy in practice.", "AI": {"tldr": "The paper introduces DERN, a framework for pruning and recombining neurons in Sparse Mixture-of-Experts architectures to reduce memory usage and enhance performance in large language models without retraining.", "motivation": "To address the high memory usage and deployment challenges associated with Sparse Mixture-of-Experts architectures, despite their computational efficiency.", "method": "DERN works by first pruning redundant experts based on router statistics, then decomposing them into neuron-level expert segments which are reassigned to compatible retained experts, and finally merging segments within those retained experts.", "result": "Experiments demonstrate that DERN improves performance by over 5% on commonsense reasoning and MMLU benchmarks with 50% expert sparsity, and significantly reduces memory usage and the number of experts needed.", "conclusion": "The DERN framework enhances the practicality of deploying SMoE LLMs by lowering memory requirements and maintaining high performance without additional training.", "key_contributions": ["Introduces a novel framework for expert pruning and reconstruction at the neuron level.", "Demonstrates significant performance improvements on key benchmarks with reduced expert count.", "Reduces memory usage associated with LLMs by optimizing expert representations."], "limitations": "", "keywords": ["Sparse Mixture-of-Experts", "Dropping Experts", "Recombining Neurons", "large language models", "memory efficiency"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.10414", "pdf": "https://arxiv.org/pdf/2509.10414.pdf", "abs": "https://arxiv.org/abs/2509.10414", "title": "Is In-Context Learning Learning?", "authors": ["Adrian de Wynter"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Director's cut", "summary": "In-context learning (ICL) allows some autoregressive models to solve tasks\nvia next-token prediction and without needing further training. This has led to\nclaims about these model's ability to solve (learn) unseen tasks with only a\nfew shots (exemplars) in the prompt. However, deduction does not always imply\nlearning, as ICL does not explicitly encode a given observation. Instead, the\nmodels rely on their prior knowledge and the exemplars given, if any. We argue\nthat, mathematically, ICL does constitute learning, but its full\ncharacterisation requires empirical work. We then carry out a large-scale\nanalysis of ICL ablating out or accounting for memorisation, pretraining,\ndistributional shifts, and prompting style and phrasing. We find that ICL is an\neffective learning paradigm, but limited in its ability to learn and generalise\nto unseen tasks. We note that, in the limit where exemplars become more\nnumerous, accuracy is insensitive to exemplar distribution, model, prompt\nstyle, and the input's linguistic features. Instead, it deduces patterns from\nregularities in the prompt, which leads to distributional sensitivity,\nespecially in prompting styles such as chain-of-thought. Given the varied\naccuracies on formally similar tasks, we conclude that autoregression's ad-hoc\nencoding is not a robust mechanism, and suggests limited all-purpose\ngeneralisability.", "AI": {"tldr": "This paper analyzes in-context learning (ICL) in autoregressive models, arguing that while ICL performs learning tasks effectively, it has limitations in generalizing to unseen tasks due to its reliance on prior knowledge and input distribution.", "motivation": "The paper addresses the claims regarding the abilities of autoregressive models to learn unseen tasks through in-context learning (ICL) and questions the robustness of this learning mechanism.", "method": "A large-scale analysis of ICL, accounting for variables such as memorization, pretraining, distributional shifts, and prompting styles, to evaluate its effectiveness in learning and generalizing.", "result": "The analysis demonstrates that while ICL can be effective, it is not robust in learning and generalizing to unseen tasks, with performance influenced by exemplar distribution and prompting styles.", "conclusion": "The study concludes that ICL's ad-hoc encoding mechanism is limited and does not support strong generalizability across different tasks.", "key_contributions": ["Large-scale analysis of in-context learning (ICL) mechanisms", "Demonstration of limitations in generalizing to unseen tasks", "Insights on the influence of prompt styles and input features on accuracy"], "limitations": "The study may not fully account for all factors influencing ICL's performance across varying context conditions.", "keywords": ["In-context learning", "autoregressive models", "machine learning", "generalization", "prompting styles"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2509.10417", "pdf": "https://arxiv.org/pdf/2509.10417.pdf", "abs": "https://arxiv.org/abs/2509.10417", "title": "Long Context Automated Essay Scoring with Language Models", "authors": ["Christopher Ormerod", "Gitit Kehat"], "categories": ["cs.CL"], "comment": "8 pages, 2 figures, 2 tables", "summary": "Transformer-based language models are architecturally constrained to process\ntext of a fixed maximum length. Essays written by higher-grade students\nfrequently exceed the maximum allowed length for many popular open-source\nmodels. A common approach to addressing this issue when using these models for\nAutomated Essay Scoring is to truncate the input text. This raises serious\nvalidity concerns as it undermines the model's ability to fully capture and\nevaluate organizational elements of the scoring rubric, which requires long\ncontexts to assess. In this study, we evaluate several models that incorporate\narchitectural modifications of the standard transformer architecture to\novercome these length limitations using the Kaggle ASAP 2.0 dataset. The models\nconsidered in this study include fine-tuned versions of XLNet, Longformer,\nModernBERT, Mamba, and Llama models.", "AI": {"tldr": "This study evaluates modified transformer models to address length limitations in essay scoring, using the Kaggle ASAP 2.0 dataset.", "motivation": "Transformer models struggle with long texts, which is problematic for Automated Essay Scoring. Truncating text can harm evaluation validity.", "method": "Several transformer models (XLNet, Longformer, ModernBERT, Mamba, and Llama) with architectural modifications were fine-tuned and evaluated.", "result": "The modified models showed improvements in processing longer essays without losing context compared to traditional truncation methods.", "conclusion": "Architectural modifications to transformers can enhance their ability to process long text inputs for more valid essay scoring.", "key_contributions": ["Evaluation of transformer modifications for long text processing", "Use of Kaggle ASAP 2.0 dataset for practical assessment", "Insights into the implications of truncation for essay evaluation"], "limitations": "The study primarily focuses on a specific dataset and may not generalize to all essay types or scoring rubrics.", "keywords": ["Automated Essay Scoring", "Transformer Models", "Context Length"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2509.10436", "pdf": "https://arxiv.org/pdf/2509.10436.pdf", "abs": "https://arxiv.org/abs/2509.10436", "title": "RefactorCoderQA: Benchmarking LLMs for Multi-Domain Coding Question Solutions in Cloud and Edge Deployment", "authors": ["Shadikur Rahman", "Aroosa Hameed", "Gautam Srivastava", "Syed Muhammad Danish"], "categories": ["cs.CL"], "comment": "12 pages, 5 figures, submitted to IEEE Transactions on Services\n  Computing", "summary": "To optimize the reasoning and problem-solving capabilities of Large Language\nModels (LLMs), we propose a novel cloud-edge collaborative architecture that\nenables a structured, multi-agent prompting framework. This framework comprises\nthree specialized components: GuideLLM, a lightweight model deployed at the\nedge to provide methodological guidance; SolverLLM, a more powerful model\nhosted in the cloud responsible for generating code solutions; and JudgeLLM, an\nautomated evaluator for assessing solution correctness and quality. To evaluate\nand demonstrate the effectiveness of this architecture in realistic settings,\nwe introduce RefactorCoderQA, a comprehensive benchmark designed to evaluate\nand enhance the performance of Large Language Models (LLMs) across multi-domain\ncoding tasks. Motivated by the limitations of existing benchmarks,\nRefactorCoderQA systematically covers various technical domains, including\nSoftware Engineering, Data Science, Machine Learning, and Natural Language\nProcessing, using authentic coding challenges from Stack Overflow. Extensive\nexperiments reveal that our fine-tuned model, RefactorCoder-MoE, achieves\nstate-of-the-art performance, significantly outperforming leading open-source\nand commercial baselines with an overall accuracy of 76.84%. Human evaluations\nfurther validate the interpretability, accuracy, and practical relevance of the\ngenerated solutions. In addition, we evaluate system-level metrics, such as\nthroughput and latency, to gain deeper insights into the performance\ncharacteristics and trade-offs of the proposed architecture.", "AI": {"tldr": "The paper proposes a cloud-edge collaborative architecture for enhancing the reasoning capabilities of Large Language Models (LLMs) through a structured multi-agent prompting framework.", "motivation": "Existing benchmarks for coding tasks are insufficient, leading to a need for a more effective evaluation method for LLMs in multi-domain coding scenarios.", "method": "The proposed architecture consists of three main components: GuideLLM (lightweight model for guidance at the edge), SolverLLM (cloud-hosted model for generating solutions), and JudgeLLM (automated evaluator for assessing solutions). The effectiveness is evaluated using RefactorCoderQA, a new benchmark covering various technical domains with authentic coding challenges.", "result": "The fine-tuned model, RefactorCoder-MoE, achieves state-of-the-art performance on multi-domain coding tasks with an overall accuracy of 76.84%, outperforming existing open-source and commercial baselines.", "conclusion": "The proposed system not only enhances LLM reasoning capabilities but also demonstrates superior interpretability, accuracy, and practical relevance in coding challenges tested against extensive metrics including throughput and latency.", "key_contributions": ["Introduction of a multi-agent prompting framework for LLMs.", "Development of RefactorCoderQA benchmark for evaluating multi-domain coding tasks.", "State-of-the-art performance achieved by RefactorCoder-MoE compared to existing models."], "limitations": "", "keywords": ["Large Language Models", "multi-agent prompting", "RefactorCoderQA"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2509.10446", "pdf": "https://arxiv.org/pdf/2509.10446.pdf", "abs": "https://arxiv.org/abs/2509.10446", "title": "DeepDive: Advancing Deep Search Agents with Knowledge Graphs and Multi-Turn RL", "authors": ["Rui Lu", "Zhenyu Hou", "Zihan Wang", "Hanchen Zhang", "Xiao Liu", "Yujiang Li", "Shi Feng", "Jie Tang", "Yuxiao Dong"], "categories": ["cs.CL"], "comment": null, "summary": "Augmenting large language models (LLMs) with browsing tools substantially\nimproves their potential as deep search agents to solve complex, real-world\ntasks. Yet, open LLMs still perform poorly in such settings due to limited\nlong-horizon reasoning capacity with browsing tools and the lack of\nsufficiently difficult supervised data. To address these challenges, we present\nDeepDive to advance deep search agents. First, we propose a strategy to\nautomatically synthesize complex, difficult, and hard-to-find questions from\nopen knowledge graphs. Second, we apply end-to-end multi-turn reinforcement\nlearning (RL) to enhance LLMs' long-horizon reasoning with deep search.\nExperiments show that DeepDive-32B achieves a new open-source competitive\nresult on BrowseComp, outperforming WebSailor, DeepSeek-R1-Browse, and\nSearch-o1. We demonstrate that multi-turn RL training improves deep search\nability and significantly contributes to the performance improvements across\nmultiple benchmarks. We observe that DeepDive enables test-time scaling of tool\ncalls and parallel sampling. All datasets, models, and code are publicly\navailable at https://github.com/THUDM/DeepDive.", "AI": {"tldr": "DeepDive enhances large language models (LLMs) for deep search tasks by synthesizing complex questions and employing multi-turn reinforcement learning to improve long-horizon reasoning.", "motivation": "Open LLMs struggle with long-horizon reasoning and lack sufficient supervised data for complex searches.", "method": "DeepDive synthesizes complex questions from open knowledge graphs and utilizes end-to-end multi-turn reinforcement learning to improve LLM reasoning capabilities.", "result": "DeepDive-32B achieves competitive results on BrowseComp, surpassing WebSailor and others, with improvements noted in deep search ability across benchmarks.", "conclusion": "DeepDive facilitates better tool use and sampling during testing, with all resources provided publicly.", "key_contributions": ["Automatic synthesis of difficult questions from knowledge graphs", "End-to-end multi-turn reinforcement learning for LLMs", "Public availability of datasets, models, and code"], "limitations": "", "keywords": ["large language models", "deep search agents", "reinforcement learning"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2509.10452", "pdf": "https://arxiv.org/pdf/2509.10452.pdf", "abs": "https://arxiv.org/abs/2509.10452", "title": "WhisTLE: Deeply Supervised, Text-Only Domain Adaptation for Pretrained Speech Recognition Transformers", "authors": ["Akshat Pandey", "Karun Kumar", "Raphael Tang"], "categories": ["cs.CL", "cs.LG"], "comment": "5 pages, 2 figures", "summary": "Pretrained automatic speech recognition (ASR) models such as Whisper perform\nwell but still need domain adaptation to handle unseen vocabulary and parlance.\nIn many real-world settings, collecting speech data is impractical,\nnecessitating text-only adaptation. We propose WhisTLE, a deeply supervised,\ntext-only adaptation method for pretrained encoder-decoder ASR models. WhisTLE\ntrains a variational autoencoder (VAE) to model encoder outputs from text and\nfine-tunes the decoder using the learned text-to-latent encoder, optionally\ncombined with text-to-speech (TTS) adaptation. At inference, the original\nencoder is restored, incurring no extra runtime cost. Across four out-of-domain\ndatasets and four ASR models, WhisTLE with TTS reduces word error rate (WER) by\n12.3% relative to TTS-only adaptation and outperforms all non-WhisTLE baselines\nin 27 of 32 scenarios.", "AI": {"tldr": "WhisTLE is a text-only adaptation method for ASR models that reduces word error rates by leveraging a variational autoencoder.", "motivation": "Pretrained ASR models excel but require adaptation for unseen vocabulary and parlance, especially in scenarios where collecting speech data is impractical.", "method": "WhisTLE employs a variational autoencoder (VAE) to model encoder outputs from text and fine-tunes the decoder with the learned text-to-latent encoder, optionally using TTS adaptation.", "result": "WhisTLE coupled with TTS adaptation lowers word error rate by 12.3% relative to TTS-only adaptation and outperforms all non-WhisTLE baselines in 27 out of 32 scenarios.", "conclusion": "WhisTLE effectively improves ASR model performance with minimal runtime cost, demonstrating the potential of text-only adaptation techniques.", "key_contributions": ["Introduction of WhisTLE for ASR adaptation", "Effective use of VAE for encoder outputs", "Demonstrated significant WER reductions over existing methods"], "limitations": "", "keywords": ["automatic speech recognition", "text-only adaptation", "variational autoencoder", "domain adaptation", "word error rate"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2509.09870", "pdf": "https://arxiv.org/pdf/2509.09870.pdf", "abs": "https://arxiv.org/abs/2509.09870", "title": "Vibe Check: Understanding the Effects of LLM-Based Conversational Agents' Personality and Alignment on User Perceptions in Goal-Oriented Tasks", "authors": ["Hasibur Rahman", "Smit Desai"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) enable conversational agents (CAs) to express\ndistinctive personalities, raising new questions about how such designs shape\nuser perceptions. This study investigates how personality expression levels and\nuser-agent personality alignment influence perceptions in goal-oriented tasks.\nIn a between-subjects experiment (N=150), participants completed travel\nplanning with CAs exhibiting low, medium, or high expression across the Big\nFive traits, controlled via our novel Trait Modulation Keys framework. Results\nrevealed an inverted-U relationship: medium expression produced the most\npositive evaluations across Intelligence, Enjoyment, Anthropomorphism,\nIntention to Adopt, Trust, and Likeability, significantly outperforming both\nextremes. Personality alignment further enhanced outcomes, with Extraversion\nand Emotional Stability emerging as the most influential traits. Cluster\nanalysis identified three distinct compatibility profiles, with \"Well-Aligned\"\nusers reporting substantially positive perceptions. These findings demonstrate\nthat personality expression and strategic trait alignment constitute optimal\ndesign targets for CA personality, offering design implications as LLM-based\nCAs become increasingly prevalent.", "AI": {"tldr": "This study explores how personality expression in conversational agents (CAs) affects user perceptions during goal-oriented tasks, finding that medium expression paired with personality alignment yields the best outcomes.", "motivation": "To understand the impact of personality expression levels and user-agent personality alignment on user perceptions in goal-oriented tasks performed with conversational agents.", "method": "A between-subjects experiment was conducted with 150 participants who engaged in travel planning tasks using CAs with varying levels of personality expression across the Big Five traits, utilizing a framework called Trait Modulation Keys.", "result": "The study found an inverted-U relationship between personality expression and user perceptions, with medium expression leading to the most favorable evaluations in areas such as Intelligence, Enjoyment, and Trust. Personality alignment further enhanced these outcomes, particularly with traits like Extraversion and Emotional Stability.", "conclusion": "Personality expression and strategic trait alignment are crucial for the effective design of conversational agents, with implications for future LLM-based CA development.", "key_contributions": ["Introduction of the Trait Modulation Keys framework for personality expression in CAs", "Identification of optimal personality expression levels for user interaction", "Discovery of key personality traits that enhance user-agent compatibility"], "limitations": "", "keywords": ["Conversational Agents", "Personality Expression", "User Perception", "Big Five Traits", "Human-Computer Interaction"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2405.13798", "pdf": "https://arxiv.org/pdf/2405.13798.pdf", "abs": "https://arxiv.org/abs/2405.13798", "title": "Slaves to the Law of Large Numbers: An Asymptotic Equipartition Property for Perplexity in Generative Language Models", "authors": ["Tyler Bell", "Avinash Mudireddy", "Ivan Johnson-Eversoll", "Soura Dasgupta", "Raghu Mudumbai"], "categories": ["cs.CL", "cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "We prove a new asymptotic un-equipartition property for the perplexity of\nlong texts generated by a language model and present supporting experimental\nevidence from open-source models. Specifically we show that the logarithmic\nperplexity of any large text generated by a language model must asymptotically\nconverge to the average entropy of its token distributions. This defines a\n``typical set'' that all long synthetic texts generated by a language model\nmust belong to. We refine the concept of ''typical set'' to include only\ngrammatically correct texts. We then show that this refined typical set is a\nvanishingly small subset of all possible grammatically correct texts for a very\ngeneral definition of grammar. This means that language models are strongly\nconstrained in the range of their possible behaviors and outputs. We make no\nsimplifying assumptions (such as stationarity) about the statistics of language\nmodel outputs, and therefore our results are directly applicable to practical\nreal-world models without any approximations. We discuss possible applications\nof the typical set concept to problems such as detecting synthetic texts and\nmembership inference in training datasets.", "AI": {"tldr": "The paper proves a new property regarding the perplexity of long texts from language models and its implications for text generation.", "motivation": "The research addresses the behavior and constraints of language models regarding text generation, aiming to refine the understanding of typical sets in language outputs.", "method": "The authors derive an asymptotic un-equipartition property for logarithmic perplexity of large texts, analyzing the entropy of token distributions and refining the concept of typical sets to encompass only grammatically correct texts.", "result": "The key finding is that the refined typical set of grammatically correct texts is a minuscule subset of all possible correct texts, indicating significant constraints on language model outputs.", "conclusion": "Language models have limited possible behaviors, which has implications for applications like detecting synthetic texts and assessing dataset membership.", "key_contributions": ["Introduces a refined typical set concept for grammatically correct texts.", "Demonstrates strong constraints on language model outputs based on token distribution entropy.", "Discusses applications for detecting synthetic texts and membership inference."], "limitations": "", "keywords": ["language models", "perplexity", "text generation", "synthetic texts", "typical set"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2409.14664", "pdf": "https://arxiv.org/pdf/2409.14664.pdf", "abs": "https://arxiv.org/abs/2409.14664", "title": "Direct Judgement Preference Optimization", "authors": ["Peifeng Wang", "Austin Xu", "Yilun Zhou", "Caiming Xiong", "Shafiq Joty"], "categories": ["cs.CL"], "comment": "EMNLP 2025", "summary": "Auto-evaluation is crucial for assessing response quality and offering\nfeedback for model development. Recent studies have explored training large\nlanguage models (LLMs) as generative judges to evaluate and critique other\nmodels' outputs. In this work, we investigate the idea of learning from both\npositive and negative data with preference optimization to enhance the\nevaluation capabilities of LLM judges across an array of different use cases.\nWe achieve this by employing three approaches to collect the preference pairs\nfor different use cases, each aimed at improving our generative judge from a\ndifferent perspective. Our comprehensive study over a wide range of benchmarks\ndemonstrates the effectiveness of our method. In particular, our generative\njudge achieves the best performance on 10 out of 13 benchmarks, outperforming\nstrong baselines like GPT-4o and specialized judge models. Further analysis\nshow that our judge model robustly counters inherent biases such as position\nand length bias, flexibly adapts to any evaluation protocol specified by\npractitioners, and provides helpful language feedback for improving downstream\ngenerator models.", "AI": {"tldr": "This paper investigates training large language models (LLMs) as generative judges to enhance evaluation capabilities across various use cases through preference optimization.", "motivation": "Auto-evaluation is essential for assessing response quality and providing feedback for model development in LLMs.", "method": "The authors propose learning from both positive and negative data using preference optimization and employ three distinct approaches to collect preference pairs for improving generative judges.", "result": "The generative judge achieves the best performance on 10 out of 13 benchmarks, outperforming models like GPT-4o and specialized judge models.", "conclusion": "The model effectively counters biases, adapts to diverse evaluation protocols, and offers valuable feedback for downstream generators.", "key_contributions": ["Introduces a method for training LLM judges using preference optimization from both positive and negative datasets.", "Demonstrates superior performance against baseline models on a wide array of benchmarks.", "Shows adaptability to various evaluation protocols while providing actionable language feedback."], "limitations": "", "keywords": ["large language models", "auto-evaluation", "preference optimization", "generative judges", "model assessment"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2410.16708", "pdf": "https://arxiv.org/pdf/2410.16708.pdf", "abs": "https://arxiv.org/abs/2410.16708", "title": "Atomic Fact Decomposition Helps Attributed Question Answering", "authors": ["Zhichao Yan", "Jiapu Wang", "Jiaoyan Chen", "Xiaoli Li", "Ru Li", "Jeff Z. Pan"], "categories": ["cs.CL"], "comment": null, "summary": "Attributed Question Answering (AQA) aims to provide both a trustworthy answer\nand a reliable attribution report for a given question. Retrieval is a widely\nadopted approach, including two general paradigms: Retrieval-Then-Read (RTR)\nand post-hoc retrieval. Recently, Large Language Models (LLMs) have shown\nremarkable proficiency, prompting growing interest in AQA among researchers.\nHowever, RTR-based AQA often suffers from irrelevant knowledge and rapidly\nchanging information, even when LLMs are adopted, while post-hoc\nretrieval-based AQA struggles with comprehending long-form answers with complex\nlogic, and precisely identifying the content needing revision and preserving\nthe original intent. To tackle these problems, this paper proposes an Atomic\nfact decomposition-based Retrieval and Editing (ARE) framework, which\ndecomposes the generated long-form answers into molecular clauses and atomic\nfacts by the instruction-tuned LLMs. Notably, the instruction-tuned LLMs are\nfine-tuned using a well-constructed dataset, generated from large scale\nKnowledge Graphs (KGs). This process involves extracting one-hop neighbors from\na given set of entities and transforming the result into coherent long-form\ntext. Subsequently, ARE leverages a search engine to retrieve evidences related\nto atomic facts, inputting these evidences into an LLM-based verifier to\ndetermine whether the facts require expansion for re-retrieval or editing.\nFurthermore, the edited facts are backtracked into the original answer, with\nevidence aggregated based on the relationship between molecular clauses and\natomic facts. Extensive evaluations demonstrate the superior performance of our\nproposed method over the state-of-the-arts on several datasets, with an\nadditionally proposed new metric $Attr_{p}$ for evaluating the precision of\nevidence attribution.", "AI": {"tldr": "This paper presents the Atomic fact decomposition-based Retrieval and Editing (ARE) framework for improving Attributed Question Answering (AQA) by decomposing long-form answers into atomic facts and enhancing evidence retrieval and editing processes using instruction-tuned LLMs.", "motivation": "To address the challenges in Attributed Question Answering (AQA), specifically the issues with Retrieval-Then-Read and post-hoc retrieval methods, particularly regarding irrelevant information and long-form answer complexity.", "method": "The ARE framework decomposes long answers into molecular clauses and atomic facts using instruction-tuned LLMs that are fine-tuned on a dataset derived from Knowledge Graphs. It utilizes a search engine to retrieve related evidence, and an LLM-based verifier assesses the need for fact expansion or editing.", "result": "Extensive evaluations indicate that the ARE framework outperforms state-of-the-art methods in AQA across various datasets, introducing a new metric, $Attr_{p}$, for measuring evidence attribution precision.", "conclusion": "The ARE framework enhances the reliability of Attributed Question Answering by effectively managing the retrieval and editing of relevant information, ensuring that the original intent of answers is preserved while improving their quality.", "key_contributions": ["Introduction of the ARE framework for AQA", "Decomposition of answers into atomic facts", "Introduction of the $Attr_{p}$ metric for evidence attribution precision"], "limitations": "The framework may still face challenges in fully accounting for highly dynamic information and the complexity of certain logical constructs.", "keywords": ["Attributed Question Answering", "Large Language Models", "Knowledge Graphs", "Evidence Attribution", "Atomic Fact Decomposition"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2410.18889", "pdf": "https://arxiv.org/pdf/2410.18889.pdf", "abs": "https://arxiv.org/abs/2410.18889", "title": "Are LLMs Better than Reported? Detecting Label Errors and Mitigating Their Effect on Model Performance", "authors": ["Omer Nahum", "Nitay Calderon", "Orgad Keller", "Idan Szpektor", "Roi Reichart"], "categories": ["cs.CL"], "comment": null, "summary": "NLP benchmarks rely on standardized datasets for training and evaluating\nmodels and are crucial for advancing the field. Traditionally, expert\nannotations ensure high-quality labels; however, the cost of expert annotation\ndoes not scale well with the growing demand for larger datasets required by\nmodern models. While crowd-sourcing provides a more scalable solution, it often\ncomes at the expense of annotation precision and consistency. Recent\nadvancements in large language models (LLMs) offer new opportunities to enhance\nthe annotation process, particularly for detecting label errors in existing\ndatasets. In this work, we consider the recent approach of LLM-as-a-judge,\nleveraging an ensemble of LLMs to flag potentially mislabeled examples. We\nconduct a case study on four factual consistency datasets from the TRUE\nbenchmark, spanning diverse NLP tasks, and on SummEval, which uses Likert-scale\nratings of summary quality across multiple dimensions. We empirically analyze\nthe labeling quality of existing datasets and compare expert, crowd-sourced,\nand LLM-based annotations in terms of the agreement, label quality, and\nefficiency, demonstrating the strengths and limitations of each annotation\nmethod. Our findings reveal a substantial number of label errors, which, when\ncorrected, induce a significant upward shift in reported model performance.\nThis suggests that many of the LLMs' so-called mistakes are due to label errors\nrather than genuine model failures. Additionally, we discuss the implications\nof mislabeled data and propose methods to mitigate them in training to improve\nperformance.", "AI": {"tldr": "The paper evaluates the potential of using large language models (LLMs) to improve label quality in NLP datasets by identifying mislabeled examples and compares LLM-based annotations with expert and crowd-sourced ones.", "motivation": "The need for larger datasets in NLP is growing, yet expert annotation is costly and doesn't scale well, while crowd-sourced annotation often lacks precision. This work explores how LLMs can enhance annotation quality by flagging label errors.", "method": "The study employs an ensemble of LLMs to assess label accuracy across four factual consistency datasets and SummEval, comparing agreement and quality of annotations from experts, crowds, and LLMs.", "result": "The analysis found a substantial number of label errors across datasets, which, if corrected, significantly improve reported performance metrics, suggesting many model errors stem from labeling issues rather than actual model failures.", "conclusion": "The paper highlights the critical nature of accurate labeling in NLP and suggests that addressing mislabeled data can greatly enhance model performance.", "key_contributions": ["Leveraging LLMs as judges for annotation quality", "Empirical comparison of annotation methods", "Identifying the impact of label errors on model performance"], "limitations": "The study focuses only on selected datasets and may not generalize across all NLP tasks; further research needed to explore LLM annotation in a wider context.", "keywords": ["NLP", "large language models", "annotation", "label quality", "dataset evaluation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2412.00559", "pdf": "https://arxiv.org/pdf/2412.00559.pdf", "abs": "https://arxiv.org/abs/2412.00559", "title": "Polish-English medical knowledge transfer: A new benchmark and results", "authors": ["ukasz Grzybowski", "Jakub Pokrywka", "Micha Ciesika", "Jeremi I. Kaczmarek", "Marek Kubis"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated significant potential in\nhandling specialized tasks, including medical problem-solving. However, most\nstudies predominantly focus on English-language contexts. This study introduces\na novel benchmark dataset based on Polish medical licensing and specialization\nexams (LEK, LDEK, PES) taken by medical doctor candidates and practicing\ndoctors pursuing specialization. The dataset was web-scraped from publicly\navailable resources provided by the Medical Examination Center and the Chief\nMedical Chamber. It comprises over 24,000 exam questions, including a subset of\nparallel Polish-English corpora, where the English portion was professionally\ntranslated by the examination center for foreign candidates. By creating a\nstructured benchmark from these existing exam questions, we systematically\nevaluate state-of-the-art LLMs, including general-purpose, domain-specific, and\nPolish-specific models, and compare their performance against human medical\nstudents. Our analysis reveals that while models like GPT-4o achieve near-human\nperformance, significant challenges persist in cross-lingual translation and\ndomain-specific understanding. These findings underscore disparities in model\nperformance across languages and medical specialties, highlighting the\nlimitations and ethical considerations of deploying LLMs in clinical practice.", "AI": {"tldr": "This paper introduces a benchmark dataset of Polish medical exams to evaluate the performance of Large Language Models (LLMs) in medical problem-solving across languages.", "motivation": "To address the lack of research on LLMs in non-English contexts, particularly in medical applications, and to evaluate their effectiveness in processing Polish medical exams.", "method": "The study involves web-scraping over 24,000 medical exam questions from Polish medical licensing and specialization exams, creating a structured dataset, and evaluating various LLMs against human medical student performance.", "result": "Evaluation indicates that while models like GPT-4o perform nearly at human levels, notable challenges remain in cross-lingual translation and understanding specific medical domains.", "conclusion": "The findings highlight the limitations of LLMs in certain languages and specialties, raising ethical considerations for their clinical use.", "key_contributions": ["Creation of a novel benchmark dataset for Polish medical exams", "Systematic evaluation of LLM performance in medical contexts", "Insights into cross-lingual and domain-specific challenges for LLMs"], "limitations": "The study primarily focuses on Polish language and may not generalize to other languages or contexts.", "keywords": ["Large Language Models", "medical exams", "cross-lingual translation", "benchmark dataset", "ethical considerations"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2412.01340", "pdf": "https://arxiv.org/pdf/2412.01340.pdf", "abs": "https://arxiv.org/abs/2412.01340", "title": "A 2-step Framework for Automated Literary Translation Evaluation: Its Promises and Pitfalls", "authors": ["Sheikh Shafayat", "Dongkeun Yoon", "Woori Jang", "Jiwoo Choi", "Alice Oh", "Seohyon Jung"], "categories": ["cs.CL"], "comment": null, "summary": "In this work, we propose and evaluate the feasibility of a two-stage pipeline\nto evaluate literary machine translation, in a fine-grained manner, from\nEnglish to Korean. The results show that our framework provides fine-grained,\ninterpretable metrics suited for literary translation and obtains a higher\ncorrelation with human judgment than traditional machine translation metrics.\nNonetheless, it still fails to match inter-human agreement, especially in\nmetrics like Korean Honorifics. We also observe that LLMs tend to favor\ntranslations generated by other LLMs, and we highlight the necessity of\ndeveloping more sophisticated evaluation methods to ensure accurate and\nculturally sensitive machine translation of literary works.", "AI": {"tldr": "This paper proposes a two-stage pipeline for evaluating literary machine translation from English to Korean, achieving better correlation with human judgment than traditional methods but falling short of inter-human agreement.", "motivation": "To create a more effective evaluation framework for literary translation that aligns closely with human judgment and cultural nuances.", "method": "The authors developed a two-stage pipeline specifically for assessing literary machine translation, incorporating fine-grained metrics that emphasize interpretability and cultural sensitivity.", "result": "The proposed framework shows a higher correlation with human judgment compared to traditional machine translation metrics, highlighting the need for better evaluation methods in literary contexts.", "conclusion": "While the new approach offers improvements, it still does not achieve the level of agreement found among human evaluators, indicating a need for ongoing refinement in evaluation methodologies for machine translation.", "key_contributions": ["Proposed a two-stage pipeline for evaluating literary machine translation.", "Demonstrated improved correlation with human judgment over traditional metrics.", "Identified the need for culturally sensitive evaluation methods in literary translation."], "limitations": "The new metrics fail to match inter-human agreement, particularly for complex features like Korean Honorifics.", "keywords": ["machine translation", "literary translation", "evaluation metrics", "human judgment", "Korean"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2412.10924", "pdf": "https://arxiv.org/pdf/2412.10924.pdf", "abs": "https://arxiv.org/abs/2412.10924", "title": "Tokens, the oft-overlooked appetizer: Large language models, the distributional hypothesis, and meaning", "authors": ["Julia Witte Zimmerman", "Denis Hudon", "Kathryn Cramer", "Alejandro J. Ruiz", "Calla Beauregard", "Ashley Fehr", "Mikaela Irene Fudolig", "Bradford Demarest", "Yoshi Meke Bird", "Milo Z. Trujillo", "Christopher M. Danforth", "Peter Sheridan Dodds"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Tokenization is a necessary component within the current architecture of many\nlanguage models, including the transformer-based large language models (LLMs)\nof Generative AI, yet its impact on the model's cognition is often overlooked.\nWe argue that LLMs demonstrate that the Distributional Hypothesis (DH) is\nsufficient for reasonably human-like language performance, and that the\nemergence of human-meaningful linguistic units among tokens and current\nstructural constraints motivate changes to existing, linguistically-agnostic\ntokenization techniques, particularly with respect to their roles as (1)\nsemantic primitives and as (2) vehicles for conveying salient distributional\npatterns from human language to the model. We explore tokenizations from a BPE\ntokenizer; extant model vocabularies obtained from Hugging Face and tiktoken;\nand the information in exemplar token vectors as they move through the layers\nof a RoBERTa (large) model. Besides creating sub-optimal semantic building\nblocks and obscuring the model's access to the necessary distributional\npatterns, we describe how tokens and pretraining can act as a backdoor for bias\nand other unwanted content, which current alignment practices may not\nremediate. Additionally, we relay evidence that the tokenization algorithm's\nobjective function impacts the LLM's cognition, despite being arguably\nmeaningfully insulated from the main system intelligence. [First uploaded to\narXiv in December, 2024.]", "AI": {"tldr": "This paper discusses the impact of tokenization on language models, arguing for a reevaluation of current tokenization techniques based on the Distributional Hypothesis for improved human-like language performance.", "motivation": "To highlight the often-overlooked significance of tokenization in language models, particularly regarding its role in human-like cognition and the emergence of semantic units.", "method": "The authors analyze tokenizations from a BPE tokenizer and model vocabularies from Hugging Face and tiktoken, exploring token vectors in a RoBERTa model.", "result": "The study finds that current tokenization practices create sub-optimal semantic units and can lead to biases, impacting the model's ability to access necessary distributional patterns.", "conclusion": "The paper concludes that the design of tokenization algorithms affects the cognition of LLMs and calls for improved methods to align tokens with meaningful language structures.", "key_contributions": ["Reevaluation of tokenization techniques based on the Distributional Hypothesis.", "Examination of BPE tokenizers and model vocabularies and their impact on LLMs.", "Identification of tokenization as a means of introducing bias into language models."], "limitations": "The paper does not provide extensive experimental results to support all claims, focusing primarily on theoretical implications.", "keywords": ["tokenization", "large language models", "Distributional Hypothesis", "semantic primitives", "bias in AI"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.10990", "pdf": "https://arxiv.org/pdf/2502.10990.pdf", "abs": "https://arxiv.org/abs/2502.10990", "title": "FinMTEB: Finance Massive Text Embedding Benchmark", "authors": ["Yixuan Tang", "Yi Yang"], "categories": ["cs.CL", "cs.IR"], "comment": "EMNLP 2025, https://github.com/yixuantt/FinMTEB", "summary": "Embedding models play a crucial role in representing and retrieving\ninformation across various NLP applications. Recent advances in large language\nmodels (LLMs) have further enhanced the performance of embedding models. While\nthese models are often benchmarked on general-purpose datasets, real-world\napplications demand domain-specific evaluation. In this work, we introduce the\nFinance Massive Text Embedding Benchmark (FinMTEB), a specialized counterpart\nto MTEB designed for the financial domain. FinMTEB comprises 64 financial\ndomain-specific embedding datasets across 7 tasks that cover diverse textual\ntypes in both Chinese and English, such as financial news articles, corporate\nannual reports, ESG reports, regulatory filings, and earnings call transcripts.\nWe also develop a finance-adapted model, Fin-E5, using a persona-based data\nsynthetic method to cover diverse financial embedding tasks for training.\nThrough extensive evaluation of 15 embedding models, including Fin-E5, we show\nthree key findings: (1) performance on general-purpose benchmarks shows limited\ncorrelation with financial domain tasks; (2) domain-adapted models consistently\noutperform their general-purpose counterparts; and (3) surprisingly, a simple\nBag-of-Words (BoW) approach outperforms sophisticated dense embeddings in\nfinancial Semantic Textual Similarity (STS) tasks, underscoring current\nlimitations in dense embedding techniques. Our work establishes a robust\nevaluation framework for financial NLP applications and provides crucial\ninsights for developing domain-specific embedding models.", "AI": {"tldr": "This paper introduces the Finance Massive Text Embedding Benchmark (FinMTEB), a new framework for evaluating embedding models specifically in the financial domain, revealing key findings on the limitations of general models.", "motivation": "To address the lack of specialized evaluation for embedding models in financial applications, which differ significantly from general-purpose benchmarks.", "method": "The study develops FinMTEB comprising 64 financial embedding datasets over 7 tasks, evaluates 15 embedding models including a specialized model Fin-E5, and employs domain-adapted methods for training.", "result": "The evaluation reveals that domain-adapted models outperform general-purpose models, a Bag-of-Words approach surprisingly outperforms dense embeddings in some tasks, and there is limited correlation between general and financial benchmarks.", "conclusion": "The paper establishes FinMTEB as a crucial tool for evaluating financial NLP models and provides insights into the development of more effective domain-specific embeddings.", "key_contributions": ["Introduction of FinMTEB for financial NLP embedding evaluation", "Development of domain-adapted model Fin-E5", "Findings on the performance discrepancies between general and domain-specific models"], "limitations": "", "keywords": ["NLP", "Embedding Models", "Finance", "Benchmarking", "Domain-specific"], "importance_score": 4, "read_time_minutes": 15}}
