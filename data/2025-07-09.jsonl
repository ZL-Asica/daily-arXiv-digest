{"id": "2507.05261", "pdf": "https://arxiv.org/pdf/2507.05261.pdf", "abs": "https://arxiv.org/abs/2507.05261", "title": "TokenShapley: Token Level Context Attribution with Shapley Value", "authors": ["Yingtai Xiao", "Yuqing Zhu", "Sirat Samyoun", "Wanrong Zhang", "Jiachen T. Wang", "Jian Du"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) demonstrate strong capabilities in in-context\nlearning, but verifying the correctness of their generated responses remains a\nchallenge. Prior work has explored attribution at the sentence level, but these\nmethods fall short when users seek attribution for specific keywords within the\nresponse, such as numbers, years, or names. To address this limitation, we\npropose TokenShapley, a novel token-level attribution method that combines\nShapley value-based data attribution with KNN-based retrieval techniques\ninspired by recent advances in KNN-augmented LLMs. By leveraging a precomputed\ndatastore for contextual retrieval and computing Shapley values to quantify\ntoken importance, TokenShapley provides a fine-grained data attribution\napproach. Extensive evaluations on four benchmarks show that TokenShapley\noutperforms state-of-the-art baselines in token-level attribution, achieving an\n11-23% improvement in accuracy."}
{"id": "2507.05266", "pdf": "https://arxiv.org/pdf/2507.05266.pdf", "abs": "https://arxiv.org/abs/2507.05266", "title": "User Behavior Prediction as a Generic, Robust, Scalable, and Low-Cost Evaluation Strategy for Estimating Generalization in LLMs", "authors": ["Sougata Saha", "Monojit Choudhury"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Measuring the generalization ability of Large Language Models (LLMs) is\nchallenging due to data contamination. As models grow and computation becomes\ncheaper, ensuring tasks and test cases are unseen during training phases will\nbecome nearly impossible. We argue that knowledge-retrieval and reasoning tasks\nare not ideal for measuring generalization, as LLMs are not trained for\nspecific tasks. Instead, we propose user behavior prediction, also a key aspect\nof personalization, as a theoretically sound, scalable, and robust alternative.\nWe introduce a novel framework for this approach and test it on movie and music\nrecommendation datasets for GPT-4o, GPT-4o-mini, and Llama-3.1-8B-Instruct.\nResults align with our framework's predictions, showing GPT-4o outperforms\nGPT-4o-mini and Llama, though all models have much room for improvement,\nespecially Llama."}
{"id": "2507.05271", "pdf": "https://arxiv.org/pdf/2507.05271.pdf", "abs": "https://arxiv.org/abs/2507.05271", "title": "An Adaptive Supervised Contrastive Learning Framework for Implicit Sexism Detection in Digital Social Networks", "authors": ["Mohammad Zia Ur Rehman", "Aditya Shah", "Nagendra Kumar"], "categories": ["cs.CL"], "comment": null, "summary": "The global reach of social media has amplified the spread of hateful content,\nincluding implicit sexism, which is often overlooked by conventional detection\nmethods. In this work, we introduce an Adaptive Supervised Contrastive lEarning\nframework for implicit sexism detectioN (ASCEND). A key innovation of our\nmethod is the incorporation of threshold-based contrastive learning: by\ncomputing cosine similarities between embeddings, we selectively treat only\nthose sample pairs as positive if their similarity exceeds a learnable\nthreshold. This mechanism refines the embedding space by robustly pulling\ntogether representations of semantically similar texts while pushing apart\ndissimilar ones, thus reducing false positives and negatives. The final\nclassification is achieved by jointly optimizing a contrastive loss with a\ncross-entropy loss. Textual features are enhanced through a word-level\nattention module. Additionally, we employ sentiment, emotion, and toxicity\nfeatures. Evaluations on the EXIST2021 and MLSC datasets demonstrate that\nASCEND significantly outperforms existing methods, with average Macro F1\nimprovements of 9.86%, 29.63%, and 32.51% across multiple tasks, highlighting\nits efficacy in capturing the subtle cues of implicit sexist language."}
{"id": "2507.05285", "pdf": "https://arxiv.org/pdf/2507.05285.pdf", "abs": "https://arxiv.org/abs/2507.05285", "title": "Beyond classical and contemporary models: a transformative ai framework for student dropout prediction in distance learning using rag, prompt engineering, and cross-modal fusion", "authors": ["Miloud Mihoubi", "Meriem Zerkouk", "Belkacem Chikhaoui"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.IR", "I.2.7; I.2.1; K.3.1"], "comment": "10 pages, 5 figures, 5 tables. Submitted to the 38th Canadian\n  Conference on Artificial Intelligence (Canadian AI 2025)", "summary": "Student dropout in distance learning remains a critical challenge, with\nprofound societal and economic consequences. While classical machine learning\nmodels leverage structured socio-demographic and behavioral data, they often\nfail to capture the nuanced emotional and contextual factors embedded in\nunstructured student interactions. This paper introduces a transformative AI\nframework that redefines dropout prediction through three synergistic\ninnovations: Retrieval-Augmented Generation (RAG) for domain-specific sentiment\nanalysis, prompt engineering to decode academic stressors, and cross-modal\nattention fusion to dynamically align textual, behavioral, and\nsocio-demographic insights. By grounding sentiment analysis in a curated\nknowledge base of pedagogical content, our RAG-enhanced BERT model interprets\nstudent comments with unprecedented contextual relevance, while optimized\nprompts isolate indicators of academic distress (e.g., \"isolation,\" \"workload\nanxiety\"). A cross-modal attention layer then fuses these insights with\ntemporal engagement patterns, creating holistic risk profiles. Evaluated on a\nlongitudinal dataset of 4 423 students, the framework achieves 89% accuracy and\nan F1-score of 0.88, outperforming conventional models by 7% and reducing false\nnegatives by 21%. Beyond prediction, the system generates interpretable\ninterventions by retrieving contextually aligned strategies (e.g., mentorship\nprograms for isolated learners). This work bridges the gap between predictive\nanalytics and actionable pedagogy, offering a scalable solution to mitigate\ndropout risks in global education systems"}
{"id": "2507.05319", "pdf": "https://arxiv.org/pdf/2507.05319.pdf", "abs": "https://arxiv.org/abs/2507.05319", "title": "LCDS: A Logic-Controlled Discharge Summary Generation System Supporting Source Attribution and Expert Review", "authors": ["Cheng Yuan", "Xinkai Rui", "Yongqi Fan", "Yawei Fan", "Boyang Zhong", "Jiacheng Wang", "Weiyan Zhang", "Tong Ruan"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL Demo 2025", "summary": "Despite the remarkable performance of Large Language Models (LLMs) in\nautomated discharge summary generation, they still suffer from hallucination\nissues, such as generating inaccurate content or fabricating information\nwithout valid sources. In addition, electronic medical records (EMRs) typically\nconsist of long-form data, making it challenging for LLMs to attribute the\ngenerated content to the sources. To address these challenges, we propose LCDS,\na Logic-Controlled Discharge Summary generation system. LCDS constructs a\nsource mapping table by calculating textual similarity between EMRs and\ndischarge summaries to constrain the scope of summarized content. Moreover,\nLCDS incorporates a comprehensive set of logical rules, enabling it to generate\nmore reliable silver discharge summaries tailored to different clinical fields.\nFurthermore, LCDS supports source attribution for generated content, allowing\nexperts to efficiently review, provide feedback, and rectify errors. The\nresulting golden discharge summaries are subsequently recorded for incremental\nfine-tuning of LLMs. Our project and demo video are in the GitHub repository\nhttps://github.com/ycycyc02/LCDS."}
{"id": "2507.05330", "pdf": "https://arxiv.org/pdf/2507.05330.pdf", "abs": "https://arxiv.org/abs/2507.05330", "title": "MindFlow: Revolutionizing E-commerce Customer Support with Multimodal LLM Agents", "authors": ["Ming Gong", "Xucheng Huang", "Chenghan Yang", "Xianhan Peng", "Haoxin Wang", "Yang Liu", "Ling Jiang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have enabled new applications\nin e-commerce customer service. However, their capabilities remain constrained\nin complex, multimodal scenarios. We present MindFlow, the first open-source\nmultimodal LLM agent tailored for e-commerce. Built on the CoALA framework, it\nintegrates memory, decision-making, and action modules, and adopts a modular\n\"MLLM-as-Tool\" strategy for effect visual-textual reasoning. Evaluated via\nonline A/B testing and simulation-based ablation, MindFlow demonstrates\nsubstantial gains in handling complex queries, improving user satisfaction, and\nreducing operational costs, with a 93.53% relative improvement observed in\nreal-world deployments."}
{"id": "2507.05446", "pdf": "https://arxiv.org/pdf/2507.05446.pdf", "abs": "https://arxiv.org/abs/2507.05446", "title": "Esports and expertise: what competitive gaming can teach us about mastery", "authors": ["Ben Boudaoud", "Josef Spjut", "Joohwan Kim", "Arjun Madhusudan", "Benjamin Watson"], "categories": ["cs.HC"], "comment": null, "summary": "Historically, much research and development in human computer interaction has\nfocused on atomic and generalizable tasks, where task completion time indicates\nproductivity. However, the emergence of competitive games and esports reminds\nus of an alternative perspective on human performance in HCI: mastery of\nhigher-level, holistic practices. Just as a world-renowned artist is rarely\nevaluated for their individual brush strokes, so skilled competitive gamers\nrarely succeed solely by completing individual mouse movements or keystrokes as\nquickly as possible. Instead, they optimize more task-specific skills, adeptly\nperforming challenges deep in the learning curve for their game of choice."}
{"id": "2507.05346", "pdf": "https://arxiv.org/pdf/2507.05346.pdf", "abs": "https://arxiv.org/abs/2507.05346", "title": "LoRA-Augmented Generation (LAG) for Knowledge-Intensive Language Tasks", "authors": ["William Fleshman", "Benjamin Van Durme"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The proliferation of fine-tuned language model experts for specific tasks and\ndomains signals the need for efficient selection and combination methods. We\npropose LoRA-Augmented Generation (LAG) for leveraging large libraries of\nknowledge and task-specific LoRA adapters. LAG requires no additional training\nor access to data, and efficiently filters, retrieves, and applies experts on a\nper-token and layer basis. We evaluate LAG on various knowledge-intensive\ntasks, achieving superior performance over existing data-free methods. We\nexplore scenarios where additional data is available, demonstrating LAG's\ncompatibility with alternative solutions such as retrieval-augmented generation\n(RAG)."}
{"id": "2507.05447", "pdf": "https://arxiv.org/pdf/2507.05447.pdf", "abs": "https://arxiv.org/abs/2507.05447", "title": "NRXR-ID: Two-Factor Authentication (2FA) in VR Using Near-Range Extended Reality and Smartphones", "authors": ["Aiur Nanzatov", "Lourdes Peña-Castillo", "Oscar Meruvia-Pastor"], "categories": ["cs.HC", "cs.CV", "cs.GR"], "comment": null, "summary": "Two-factor authentication (2FA) has become widely adopted as an efficient and\nsecure way to validate someone's identity online. Two-factor authentication is\ndifficult in virtual reality (VR) because users are usually wearing a\nhead-mounted display (HMD) which does not allow them to see their real-world\nsurroundings. We present NRXR-ID, a technique to implement two-factor\nauthentication while using extended reality systems and smartphones. The\nproposed method allows users to complete an authentication challenge using\ntheir smartphones without removing their HMD. We performed a user study where\nwe explored four types of challenges for users, including a novel\ncheckers-style challenge. Users responded to these challenges under three\ndifferent configurations, including a technique that uses the smartphone to\nsupport gaze-based selection without the use of VR controllers. A 4X3\nwithin-subjects design allowed us to study all the variations proposed. We\ncollected performance metrics and performed user experience questionnaires to\ncollect subjective impressions from 30 participants. Results suggest that the\ncheckers-style visual matching challenge was the most appropriate option,\nfollowed by entering a digital PIN challenge submitted via the smartphone and\nanswered within the VR environment."}
{"id": "2507.05362", "pdf": "https://arxiv.org/pdf/2507.05362.pdf", "abs": "https://arxiv.org/abs/2507.05362", "title": "On the Bias of Next-Token Predictors Toward Systematically Inefficient Reasoning: A Shortest-Path Case Study", "authors": ["Riccardo Alberghi", "Elizaveta Demyanenko", "Luca Biggio", "Luca Saglietti"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent advances in natural language processing highlight two key factors for\nimproving reasoning in large language models (LLMs): (i) allocating more\ntest-time compute tends to help on harder problems but often introduces\nredundancy in the reasoning trace, and (ii) compute is most effective when\nreasoning is systematic and incremental, forming structured chains of thought\n(CoTs) akin to human problem-solving. To study these factors in isolation, we\nintroduce a controlled setting based on shortest-path tasks in layered graphs.\nWe train decoder-only transformers on question-trace-answer triples using a\ncustom tokenizer, comparing models trained on optimal bottom-up dynamic\nprogramming traces with those trained on longer, valid traces involving\nbacktracking. Surprisingly, with the same training-token budget, models trained\non inefficient traces generalize better to unseen graphs. This benefit is not\ndue to length alone-injecting arbitrary redundancy into reasoning traces fails\nto help and can even hurt performance. Instead, we find that generalization\ncorrelates with the model's confidence in next-token prediction, suggesting\nthat long, coherent, and locally incremental traces make the training signal\neasier to optimize."}
{"id": "2507.05461", "pdf": "https://arxiv.org/pdf/2507.05461.pdf", "abs": "https://arxiv.org/abs/2507.05461", "title": "GLOSS: Group of LLMs for Open-Ended Sensemaking of Passive Sensing Data for Health and Wellbeing", "authors": ["Akshat Choube", "Ha Le", "Jiachen Li", "Kaixin Ji", "Vedant Das Swain", "Varun Mishra"], "categories": ["cs.HC"], "comment": null, "summary": "The ubiquitous presence of smartphones and wearables has enabled researchers\nto build prediction and detection models for various health and behavior\noutcomes using passive sensing data from these devices. Achieving a high-level,\nholistic understanding of an individual's behavior and context, however,\nremains a significant challenge. Due to the nature of passive sensing data,\nsensemaking -- the process of interpreting and extracting insights -- requires\nboth domain knowledge and technical expertise, creating barriers for different\nstakeholders. Existing systems designed to support sensemaking are either not\nopen-ended or cannot perform complex data triangulation. In this paper, we\npresent a novel sensemaking system, Group of LLMs for Open-ended Sensemaking\n(GLOSS), capable of open-ended sensemaking and performing complex multimodal\ntriangulation to derive insights. We demonstrate that GLOSS significantly\noutperforms the commonly used Retrieval-Augmented Generation (RAG) technique,\nachieving 87.93% accuracy and 66.19% consistency, compared to RAG's 29.31%\naccuracy and 52.85% consistency. Furthermore, we showcase the promise of GLOSS\nthrough four use cases inspired by prior and ongoing work in the UbiComp and\nHCI communities. Finally, we discuss the potential of GLOSS, its broader\nimplications, and the limitations of our work."}
{"id": "2507.05385", "pdf": "https://arxiv.org/pdf/2507.05385.pdf", "abs": "https://arxiv.org/abs/2507.05385", "title": "EduCoder: An Open-Source Annotation System for Education Transcript Data", "authors": ["Guanzhong Pan", "Mei Tan", "Hyunji Nam", "Lucía Langlois", "James Malamut", "Liliana Deonizio", "Dorottya Demszky"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce EduCoder, a domain-specialized tool designed to support\nutterance-level annotation of educational dialogue. While general-purpose text\nannotation tools for NLP and qualitative research abound, few address the\ncomplexities of coding education dialogue transcripts -- with diverse\nteacher-student and peer interactions. Common challenges include defining\ncodebooks for complex pedagogical features, supporting both open-ended and\ncategorical coding, and contextualizing utterances with external features, such\nas the lesson's purpose and the pedagogical value of the instruction. EduCoder\nis designed to address these challenges by providing a platform for researchers\nand domain experts to collaboratively define complex codebooks based on\nobserved data. It incorporates both categorical and open-ended annotation types\nalong with contextual materials. Additionally, it offers a side-by-side\ncomparison of multiple annotators' responses, allowing comparison and\ncalibration of annotations with others to improve data reliability. The system\nis open-source, with a demo video available."}
{"id": "2507.05532", "pdf": "https://arxiv.org/pdf/2507.05532.pdf", "abs": "https://arxiv.org/abs/2507.05532", "title": "W2W: A Simulated Exploration of IMU Placement Across the Human Body for Designing Smarter Wearable", "authors": ["Lala Shakti Swarup Ray", "Bo Zhou", "Paul Lukowicz"], "categories": ["cs.HC"], "comment": null, "summary": "Inertial measurement units (IMUs) are central to wearable systems for\nactivity recognition and pose estimation, but sensor placement remains largely\nguided by heuristics and convention. In this work, we introduce Where to Wear\n(W2W), a simulation-based framework for systematic exploration of IMU placement\nutility across the body. Using labeled motion capture data, W2W generates\nrealistic synthetic IMU signals at 512 anatomically distributed surface\npatches, enabling high-resolution, task-specific evaluation of sensor\nperformance. We validate reliability of W2W by comparing spatial performance\nrankings from synthetic data with real IMU recordings in two multimodal\ndatasets, confirming strong agreement in activity-wise trends. Further analysis\nreveals consistent spatial trends across activity types and uncovers overlooked\nhigh-utility regions that are rarely used in commercial systems. These findings\nchallenge long-standing placement norms and highlight opportunities for more\nefficient, task-adaptive sensor configurations. Overall, our results\ndemonstrate that simulation with W2W can serve as a powerful design tool for\noptimizing sensor placement, enabling scalable, data-driven strategies that are\nimpractical to obtain through physical experimentation alone."}
{"id": "2507.05387", "pdf": "https://arxiv.org/pdf/2507.05387.pdf", "abs": "https://arxiv.org/abs/2507.05387", "title": "The Generalization Ridge: Information Flow in Natural Language Generation", "authors": ["Ruidi Chang", "Chunyuan Deng", "Hanjie Chen"], "categories": ["cs.CL"], "comment": null, "summary": "Transformer-based language models have achieved state-of-the-art performance\nin natural language generation (NLG) tasks, yet their internal mechanisms for\nsynthesizing task-relevant information remain insufficiently understood. While\nprior studies suggest that intermediate layers often yield more generalizable\nrepresentations than final layers, how this generalization ability emerges and\npropagates across layers during training remains unclear. To address this gap,\nwe propose InfoRidge, an information-theoretic framework, to characterize how\npredictive information-the mutual information between hidden representations\nand target outputs-varies across depth. Estimating this quantity enables us to\ntrace the flow of task-relevant information throughout the model during\ntraining. Our experiments across various models and datasets reveal a\nconsistent non-monotonic trend: predictive information peaks in upper-middle\nlayers-forming a generalization ridge-before declining in final layers,\nreflecting a transition between generalization and memorization. To further\ninvestigate this phenomenon, we introduce residual scaling\ncoefficients-trainable scalar parameters applied to each residual block-which\nserve as functional probes for assessing the relative importance of individual\ntransformer layers. These coefficients reveal that, under distribution shift,\nmodels downweight final layers and increasingly rely on ridge layers,\nhighlighting their role in generalization. Together, these findings offer new\ninsights into the internal mechanisms of transformers and underscore the\ncritical role of intermediate layers in supporting generalization."}
{"id": "2507.05537", "pdf": "https://arxiv.org/pdf/2507.05537.pdf", "abs": "https://arxiv.org/abs/2507.05537", "title": "Information Needs and Practices Supported by ChatGPT", "authors": ["Tim Gorichanaz"], "categories": ["cs.HC", "cs.IR"], "comment": "To be presented at the 2025 ASIS&T virtual satellite meeting,\n  December 2025", "summary": "This study considers ChatGPT as an information source, investigating the\ninformation needs that people come to ChatGPT with and the information\npractices that ChatGPT supports, through a qualitative content analysis of 205\nuser vignettes. The findings show that ChatGPT is used in a range of life\ndomains (home/family, work, leisure, etc.) and for a range of human needs\n(writing/editing, learning, simple programming tasks, etc.), constituting the\ninformation needs that people use ChatGPT to address. Related to these\ninformation needs, the findings show six categories of information practices\nthat ChatGPT supports: Writing, Deciding, Identifying, Ideating, Talking, and\nCritiquing. This work suggests that, in the AI age, information need should be\nconceptualized not just as a matter of \"getting questions answered\" or even\n\"making sense,\" but as skillfully coping in the world, a notion that includes\nboth understanding and action. This study leads to numerous opportunities for\nfuture work at the junction of generative AI and information needs, seeking,\nuse and experience."}
{"id": "2507.05391", "pdf": "https://arxiv.org/pdf/2507.05391.pdf", "abs": "https://arxiv.org/abs/2507.05391", "title": "Controlling What You Share: Assessing Language Model Adherence to Privacy Preferences", "authors": ["Guillem Ramírez", "Alexandra Birch", "Ivan Titov"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are primarily accessed via commercial APIs, but\nthis often requires users to expose their data to service providers. In this\npaper, we explore how users can stay in control of their data by using privacy\nprofiles: simple natural language instructions that say what should and should\nnot be revealed. We build a framework where a local model uses these\ninstructions to rewrite queries, only hiding details deemed sensitive by the\nuser, before sending them to an external model, thus balancing privacy with\nperformance. To support this research, we introduce PEEP, a multilingual\ndataset of real user queries annotated to mark private content and paired with\nsynthetic privacy profiles. Our experiments with lightweight LLMs show they can\nfollow these instructions to some extent, but also face consistent challenges,\nhighlighting the need for models that better understand and comply with\nuser-defined privacy preferences."}
{"id": "2507.05572", "pdf": "https://arxiv.org/pdf/2507.05572.pdf", "abs": "https://arxiv.org/abs/2507.05572", "title": "AnatomyCarve: A VR occlusion management technique for medical images based on segment-aware clipping", "authors": ["Andrey Titov", "Tina N. H. Nantenaina", "Marta Kersten-Oertel", "Simon Drouin"], "categories": ["cs.HC", "cs.GR"], "comment": null, "summary": "Visualizing 3D medical images is challenging due to self-occlusion, where\nanatomical structures of interest can be obscured by surrounding tissues.\nExisting methods, such as slicing and interactive clipping, are limited in\ntheir ability to fully represent internal anatomy in context. In contrast,\nhand-drawn medical illustrations in anatomy books manage occlusion effectively\nby selectively removing portions based on tissue type, revealing 3D structures\nwhile preserving context. This paper introduces AnatomyCarve, a novel technique\ndeveloped for a VR environment that creates high-quality illustrations similar\nto those in anatomy books, while remaining fast and interactive. AnatomyCarve\nallows users to clip selected segments from 3D medical volumes, preserving\nspatial relations and contextual information. This approach enhances\nvisualization by combining advanced rendering techniques with natural user\ninteractions in VR. Usability of AnatomyCarve was assessed through a study with\nnon-experts, while surgical planning effectiveness was evaluated with\npracticing neurosurgeons and residents. The results show that AnatomyCarve\nenables customized anatomical visualizations, with high user satisfaction,\nsuggesting its potential for educational and clinical applications."}
{"id": "2507.05418", "pdf": "https://arxiv.org/pdf/2507.05418.pdf", "abs": "https://arxiv.org/abs/2507.05418", "title": "Learn Globally, Speak Locally: Bridging the Gaps in Multilingual Reasoning", "authors": ["Jaedong Hwang", "Kumar Tanmay", "Seok-Jin Lee", "Ayush Agrawal", "Hamid Palangi", "Kumar Ayush", "Ila Fiete", "Paul Pu Liang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have achieved strong performance in domains like\nmathematics, factual QA, and code generation, yet their multilingual reasoning\ncapabilities in these tasks remain underdeveloped. Especially for low-resource\nlanguages such as Swahili or Thai, LLMs can often misinterpret prompts or\ndefault to reasoning in English. This implicit bias toward high-resource\nlanguages undermines factual accuracy, interpretability, and trust. Current\nmultilingual benchmarks focus only on final answers, overlooking whether models\nactually reason in the target language. To address this gap, we introduce\nGeoFact-X, a geography-based multilingual factual reasoning benchmark with\nannotated reasoning traces in five languages: English, Hindi, Japanese,\nSwahili, and Thai. We further propose BRIDGE, a novel training method that\nguides supervised fine-tuning and test-time reinforcement learning with a\nlanguage-consistency reward to align reasoning with the input language.\nFinally, we develop an automatic evaluation protocol using LLM-as-a-judge to\nassess answer correctness and the quality and language consistency of reasoning\ntraces, enabling nuanced and scalable analysis beyond surface-level metrics.\nOur results show that BRIDGE significantly enhances multilingual reasoning\nfidelity, demonstrating that reasoning-aware multilingual reinforcement\nlearning is crucial for robust cross-lingual generalization.\nhttps://jd730.github.io/projects/GeoFact-X_BRIDGE"}
{"id": "2507.05600", "pdf": "https://arxiv.org/pdf/2507.05600.pdf", "abs": "https://arxiv.org/abs/2507.05600", "title": "StoryGrid: A Tangible Interface for Student Expression", "authors": ["Tom Moher", "Louis Gomez", "Janet Kim", "Claudia Hindo", "Benjamin Watson", "Stephen Fransen", "Tim McEneany"], "categories": ["cs.HC"], "comment": null, "summary": "StorySpace is a classroom-based design and presentation system for\ninteractive multimedia posters. Employing the technology base first used in\nEden's PITAboard [2002], StorySpace allows groups of learners to manipulate\nprojected multimedia objects on a horizontal board using a small collection of\nshared physical tokens. In this paper, we present the ongoing design history of\nStorySpace in the context of its introduction within an urban high school\nliterature class. Interface modifications based on student and teacher feedback\nled on changes in token semantics and media importing methods. We describe how\nStorySpace features enriched students' interpretations of literature, with\nparticular emphasis in two areas: (1) attention to audience, and (2) reflection\nof multiple perspectives."}
{"id": "2507.05424", "pdf": "https://arxiv.org/pdf/2507.05424.pdf", "abs": "https://arxiv.org/abs/2507.05424", "title": "\"Lost-in-the-Later\": Framework for Quantifying Contextual Grounding in Large Language Models", "authors": ["Yufei Tao", "Adam Hiatt", "Rahul Seetharaman", "Ameeta Agrawal"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models are capable of leveraging both contextual and\nparametric knowledge but how they prioritize and integrate these sources\nremains underexplored. We introduce CoPE, a novel evaluation framework that\nsystematically measures contextual knowledge (CK) and parametric knowledge (PK)\nacross models and languages. Using our MultiWikiAtomic dataset in English,\nSpanish, and Danish, we analyze how large language models (LLMs) integrate\ncontext, prioritize information, and incorporate PK in open-ended question\nanswering. Our analysis uncovers a phenomenon we call lost-in-the-later, where\nLLMs tend to overlook or deprioritize information that appears later in a given\ncontext, revealing a strong positional bias that affects contextual grounding.\nWe further find that reasoning models, as well as non-reasoning models prompted\nwith chain-of-thought (CoT), use context even less than non-reasoning models\nwithout CoT and fail to mitigate the lost-in-the-later effect. CoT prompting,\nin particular, results in lower recall and shorter responses, leading to\ndegraded contextual grounding. Based on these insights, we design prompt-based\nmethods to effectively leverage input context. A case study applying CoPE to\nsummarization demonstrates that CK-informed prompting improves factual\ngrounding and reduces hallucination."}
{"id": "2507.05605", "pdf": "https://arxiv.org/pdf/2507.05605.pdf", "abs": "https://arxiv.org/abs/2507.05605", "title": "Hapster: Using Apple Watch Haptics to Enable Live Low-Friction Student Feedback in the Physical Classroom", "authors": ["Oleg Aleksandrovich Golev", "Michelle Huang", "Chanketya Nop", "Kritin Vongthongsri", "Andrés Monroy-Hernández", "Parastoo Abtahi"], "categories": ["cs.HC"], "comment": "In Extended Abstracts of the CHI Conference on Human Factors in\n  Computing Systems, pp. 1-7. 2024", "summary": "The benefits of student response systems (SRSs) for in-person lectures are\nwell-researched. However, all current SRSs only rely on a visual interface to\nrelay information to the instructor. We describe the design and evaluation of\nHapster, a prototype system that uses an Apple Watch to deliver live,\naggregated student feedback to the instructor via both visual and vibro-tactile\nmodalities. We evaluated this system with 6 instructors and 155 students at a\nU.S. university. Participants reported that the system was effective at\ndelivering live student feedback and facilitating better engagement from both\nthe instructor and the students. However, instructors also noted several\nchallenges with differentiating and perceiving the haptic sequences while\nlecturing. We conclude by discussing the tradeoff between system flexibility\nand abuse potential while identifying opportunities for further research\nregarding accessibility, content moderation, and additional interaction\nmodalities. Our results suggest that haptics can be used as an effective live\nfeedback mechanism for instructors in the physical classroom."}
{"id": "2507.05443", "pdf": "https://arxiv.org/pdf/2507.05443.pdf", "abs": "https://arxiv.org/abs/2507.05443", "title": "Gendered Divides in Online Discussions about Reproductive Rights", "authors": ["Ashwin Rao", "Sze Yuh Nina Wang", "Kristina Lerman"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "The U.S. Supreme Court's 2022 ruling in Dobbs v. Jackson Women's Health\nOrganization marked a turning point in the national debate over reproductive\nrights. While the ideological divide over abortion is well documented, less is\nknown about how gender and local sociopolitical contexts interact to shape\npublic discourse. Drawing on nearly 10 million abortion-related posts on X\n(formerly Twitter) from users with inferred gender, ideology and location, we\nshow that gender significantly moderates abortion attitudes and emotional\nexpression, particularly in conservative regions, and independently of\nideology. This creates a gender gap in abortion attitudes that grows more\npronounced in conservative regions. The leak of the Dobbs draft opinion further\nintensified online engagement, disproportionately mobilizing pro-abortion women\nin areas where access was under threat. These findings reveal that abortion\ndiscourse is not only ideologically polarized but also deeply structured by\ngender and place, highlighting the central role of identity in shaping\npolitical expression during moments of institutional disruption."}
{"id": "2507.05616", "pdf": "https://arxiv.org/pdf/2507.05616.pdf", "abs": "https://arxiv.org/abs/2507.05616", "title": "Breaking the Plane: Exploring Real-Time Visualization of 3D Surfaces in Augmented Reality with Handwritten Input", "authors": ["Liam Franco Esparraguera", "Kristoffer Selberg", "Brian Lou", "Jenny Sun", "Beza Desta", "Andrés Monroy-Hernández", "Parastoo Abtahi"], "categories": ["cs.HC"], "comment": "In Extended Abstracts of the CHI Conference on Human Factors in\n  Computing Systems, pp. 1-9. 2024", "summary": "We introduce Breaking the Plane, an augmented reality (AR) application built\nfor AR headsets that enables users to visualize 3D mathematical functions using\nhandwritten input. Researchers have demonstrated overlaying 3D visualizations\nof mathematical concepts through AR enhances learning motivation and\ncomprehension, and equation parsing makes the authoring of teaching materials\nmore time-efficient for instructors. Previous works have developed AR systems\nthat separately employ equation parsing and 3D mathematical visualizations, but\nwork has yet to be done to combine those features by enabling real-time\ninteractions and dynamic visualizations that help users learn in situ. We\nexplore this by developing an AR system featuring handwritten equation parsing,\ngraph manipulation, and a 3D function plotter. We found that our system\nsignificantly surpassed other systems in engagement, achieved comparable ease\nof use to a popular visualization tool, was considered the most effective in\naiding problem-solving, and was highly preferred by participants for future\nuse."}
{"id": "2507.05444", "pdf": "https://arxiv.org/pdf/2507.05444.pdf", "abs": "https://arxiv.org/abs/2507.05444", "title": "PhoniTale: Phonologically Grounded Mnemonic Generation for Typologically Distant Language Pairs", "authors": ["Sana Kang", "Myeongseok Gwon", "Su Young Kwon", "Jaewook Lee", "Andrew Lan", "Bhiksha Raj", "Rita Singh"], "categories": ["cs.CL"], "comment": null, "summary": "Vocabulary acquisition poses a significant challenge for second-language (L2)\nlearners, especially when learning typologically distant languages such as\nEnglish and Korean, where phonological and structural mismatches complicate\nvocabulary learning. Recently, large language models (LLMs) have been used to\ngenerate keyword mnemonics by leveraging similar keywords from a learner's\nfirst language (L1) to aid in acquiring L2 vocabulary. However, most of this\nresearch has focused on native English speakers learning other languages,\nrather than the reverse. In this paper, we present PhoniTale, a novel\ncross-lingual mnemonic generation system that retrieves L1 keyword sequence\nbased on phonological similarity and uses LLMs to generate mnemonics. We\nevaluate PhoniTale using both automated metrics and human evaluations,\ncomparing its output to mnemonics created by humans and by previous automated\napproaches. To assess practical effectiveness, we also conduct a short-term\nrecall test measuring mnemonic helpfulness. Our findings show that PhoniTale\nperforms comparably to human-authored mnemonics. We also highlight key areas\nfor future improvement in mnemonic quality and methodology."}
{"id": "2507.05820", "pdf": "https://arxiv.org/pdf/2507.05820.pdf", "abs": "https://arxiv.org/abs/2507.05820", "title": "Constella: Supporting Storywriters' Interconnected Character Creation through LLM-based Multi-Agents", "authors": ["Syemin Park", "Soobin Park", "Youn-kyung Lim"], "categories": ["cs.HC", "cs.AI", "cs.MA"], "comment": "50 pages", "summary": "Creating a cast of characters by attending to their relational dynamics is a\ncritical aspect of most long-form storywriting. However, our formative study\n(N=14) reveals that writers struggle to envision new characters that could\ninfluence existing ones, to balance similarities and differences among\ncharacters, and to intricately flesh out their relationships. Based on these\nobservations, we designed Constella, an LLM-based multi-agent tool that\nsupports storywriters' interconnected character creation process. Constella\nsuggests related characters (FRIENDS DISCOVERY feature), reveals the inner\nmindscapes of several characters simultaneously (JOURNALS feature), and\nmanifests relationships through inter-character responses (COMMENTS feature).\nOur 7-8 day deployment study with storywriters (N=11) shows that Constella\nenabled the creation of expansive communities composed of related characters,\nfacilitated the comparison of characters' thoughts and emotions, and deepened\nwriters' understanding of character relationships. We conclude by discussing\nhow multi-agent interactions can help distribute writers' attention and effort\nacross the character cast."}
{"id": "2507.05448", "pdf": "https://arxiv.org/pdf/2507.05448.pdf", "abs": "https://arxiv.org/abs/2507.05448", "title": "On the Semantics of Large Language Models", "authors": ["Martin Schuele"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) such as ChatGPT demonstrated the potential to\nreplicate human language abilities through technology, ranging from text\ngeneration to engaging in conversations. However, it remains controversial to\nwhat extent these systems truly understand language. We examine this issue by\nnarrowing the question down to the semantics of LLMs at the word and sentence\nlevel. By examining the inner workings of LLMs and their generated\nrepresentation of language and by drawing on classical semantic theories by\nFrege and Russell, we get a more nuanced picture of the potential semantic\ncapabilities of LLMs."}
{"id": "2507.05962", "pdf": "https://arxiv.org/pdf/2507.05962.pdf", "abs": "https://arxiv.org/abs/2507.05962", "title": "Evaluation of Large Language Model-Driven AutoML in Data and Model Management from Human-Centered Perspective", "authors": ["Jiapeng Yao", "Lantian Zhang", "Jiping Huang"], "categories": ["cs.HC"], "comment": null, "summary": "As organizations increasingly seek to leverage machine learning (ML)\ncapabilities, the technical complexity of implementing ML solutions creates\nsignificant barriers to adoption and impacts operational efficiency. This\nresearch examines how Large Language Models (LLMs) can transform the\naccessibility of ML technologies within organizations through a human-centered\nAutomated Machine Learning (AutoML) approach. Through a comprehensive user\nstudy involving 15 professionals across various roles and technical\nbackgrounds, we evaluate the organizational impact of an LLM-based AutoML\nframework compared to traditional implementation methods. Our research offers\nfour significant contributions to both management practice and technical\ninnovation: First, we present pioneering evidence that LLM-based interfaces can\ndramatically improve ML implementation success rates, with 93.34% of users\nachieved superior performance in the LLM condition, with 46.67% showing higher\naccuracy (10-25% improvement over baseline) and 46.67% demonstrating\nsignificantly higher accuracy (>25% improvement over baseline), while 6.67%\nmaintained comparable performance levels; and 60% reporting substantially\nreduced development time. Second, we demonstrate how natural language\ninterfaces can effectively bridge the technical skills gap in organizations,\ncutting implementation time by 50% while improving accuracy across all\nexpertise levels. Third, we provide valuable insights for organizations\ndesigning human-AI collaborative systems, showing that our approach reduced\nerror resolution time by 73% and significantly accelerated employee learning\ncurves. Finally, we establish empirical support for natural language as an\neffective interface for complex technical systems, offering organizations a\npath to democratize ML capabilities without compromising quality or\nperformance."}
{"id": "2507.05455", "pdf": "https://arxiv.org/pdf/2507.05455.pdf", "abs": "https://arxiv.org/abs/2507.05455", "title": "ModelCitizens:Representing Community Voices in Online Safety", "authors": ["Ashima Suvarna", "Christina Chance", "Hamid Palangi", "Sophie Hao", "Thomas Hartvigsen", "Saadia Gabriel"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Automatic toxic language detection is critical for creating safe, inclusive\nonline spaces. However, it is a highly subjective task, with perceptions of\ntoxic language shaped by community norms and lived experience. Existing\ntoxicity detection models are typically trained on annotations that collapse\ndiverse annotator perspectives into a single ground truth, erasing important\ncontext-specific notions of toxicity such as reclaimed language. To address\nthis, we introduce MODELCITIZENS, a dataset of 6.8K social media posts and 40K\ntoxicity annotations across diverse identity groups. To capture the role of\nconversational context on toxicity, typical of social media posts, we augment\nMODELCITIZENS posts with LLM-generated conversational scenarios.\nState-of-the-art toxicity detection tools (e.g. OpenAI Moderation API,\nGPT-o4-mini) underperform on MODELCITIZENS, with further degradation on\ncontext-augmented posts. Finally, we release LLAMACITIZEN-8B and\nGEMMACITIZEN-12B, LLaMA- and Gemma-based models finetuned on MODELCITIZENS,\nwhich outperform GPT-o4-mini by 5.5% on in-distribution evaluations. Our\nfindings highlight the importance of community-informed annotation and modeling\nfor inclusive content moderation."}
{"id": "2507.06000", "pdf": "https://arxiv.org/pdf/2507.06000.pdf", "abs": "https://arxiv.org/abs/2507.06000", "title": "Exploring Collaboration Patterns and Strategies in Human-AI Co-creation through the Lens of Agency: A Scoping Review of the Top-tier HCI Literature", "authors": ["Shuning Zhang", "Hui Wang", "Xin Yi"], "categories": ["cs.HC"], "comment": null, "summary": "As Artificial Intelligence (AI) increasingly becomes an active collaborator\nin co-creation, understanding the distribution and dynamic of agency is\nparamount. The Human-Computer Interaction (HCI) perspective is crucial for this\nanalysis, as it uniquely reveals the interaction dynamics and specific control\nmechanisms that dictate how agency manifests in practice. Despite this\nimportance, a systematic synthesis mapping agency configurations and control\nmechanisms within the HCI/CSCW literature is lacking. Addressing this gap, we\nreviewed 134 papers from top-tier HCI/CSCW venues (e.g., CHI, UIST, CSCW) over\nthe past 20 years. This review yields four primary contributions: (1) an\nintegrated theoretical framework structuring agency patterns, control\nmechanisms, and interaction contexts, (2) a comprehensive operational catalog\nof control mechanisms detailing how agency is implemented; (3) an actionable\ncross-context map linking agency configurations to diverse co-creative\npractices; and (4) grounded implications and guidance for future CSCW research\nand the design of co-creative systems, addressing aspects like trust and\nethics."}
{"id": "2507.05517", "pdf": "https://arxiv.org/pdf/2507.05517.pdf", "abs": "https://arxiv.org/abs/2507.05517", "title": "Empowering Healthcare Practitioners with Language Models: Structuring Speech Transcripts in Two Real-World Clinical Applications", "authors": ["Jean-Philippe Corbeil", "Asma Ben Abacha", "George Michalopoulos", "Phillip Swazinna", "Miguel Del-Agua", "Jerome Tremblay", "Akila Jeeson Daniel", "Cari Bader", "Kevin Cho", "Pooja Krishnan", "Nathan Bodenstab", "Thomas Lin", "Wenxuan Teng", "Francois Beaulieu", "Paul Vozila"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) such as GPT-4o and o1 have demonstrated strong\nperformance on clinical natural language processing (NLP) tasks across multiple\nmedical benchmarks. Nonetheless, two high-impact NLP tasks - structured tabular\nreporting from nurse dictations and medical order extraction from\ndoctor-patient consultations - remain underexplored due to data scarcity and\nsensitivity, despite active industry efforts. Practical solutions to these\nreal-world clinical tasks can significantly reduce the documentation burden on\nhealthcare providers, allowing greater focus on patient care. In this paper, we\ninvestigate these two challenging tasks using private and open-source clinical\ndatasets, evaluating the performance of both open- and closed-weight LLMs, and\nanalyzing their respective strengths and limitations. Furthermore, we propose\nan agentic pipeline for generating realistic, non-sensitive nurse dictations,\nenabling structured extraction of clinical observations. To support further\nresearch in both areas, we release SYNUR and SIMORD, the first open-source\ndatasets for nurse observation extraction and medical order extraction."}
{"id": "2507.06141", "pdf": "https://arxiv.org/pdf/2507.06141.pdf", "abs": "https://arxiv.org/abs/2507.06141", "title": "Large Language Models Predict Human Well-being -- But Not Equally Everywhere", "authors": ["Pat Pataranutaporn", "Nattavudh Powdthavee", "Chayapatr Archiwaranguprok", "Pattie Maes"], "categories": ["cs.HC"], "comment": null, "summary": "Subjective well-being is a key metric in economic, medical, and policy\ndecision-making. As artificial intelligence provides scalable tools for\nmodelling human outcomes, it is crucial to evaluate whether large language\nmodels (LLMs) can accurately predict well-being across diverse global\npopulations. We evaluate four leading LLMs using data from 64,000 individuals\nin 64 countries. While LLMs capture broad correlates such as income and health,\ntheir predictive accuracy decreases in countries underrepresented in the\ntraining data, highlighting systematic biases rooted in global digital and\neconomic inequality. A pre-registered experiment demonstrates that LLMs rely on\nsurface-level linguistic similarity rather than conceptual understanding,\nleading to systematic misestimations in unfamiliar or resource-limited\nsettings. Injecting findings from underrepresented contexts substantially\nenhances performance, but a significant gap remains. These results highlight\nboth the promise and limitations of LLMs in predicting global well-being,\nunderscoring the importance of robust validation prior to their implementation\nacross these areas."}
{"id": "2507.05557", "pdf": "https://arxiv.org/pdf/2507.05557.pdf", "abs": "https://arxiv.org/abs/2507.05557", "title": "Enhancing Test-Time Scaling of Large Language Models with Hierarchical Retrieval-Augmented MCTS", "authors": ["Alex ZH Dou", "Zhongwei Wan", "Dongfei Cui", "Xin Wang", "Jing Xiong", "Haokun Lin", "Chaofan Tao", "Shen Yan", "Mi Zhang"], "categories": ["cs.CL"], "comment": "Technical Report", "summary": "Test-time scaling has emerged as a promising paradigm in language modeling,\nleveraging additional computational resources at inference time to enhance\nmodel performance. In this work, we introduce R2-LLMs, a novel and versatile\nhierarchical retrieval-augmented reasoning framework designed to improve\ntest-time scaling in large language models (LLMs) without requiring\ndistillation from more advanced models to obtain chain-of-thought (CoT)\ntraining data. R2-LLMs enhances inference-time generalization by integrating\ndual-level retrieval-based in-context learning: (1) At the coarse level, our\napproach extracts abstract templates from complex reasoning problems and\nretrieves similar problem-answer pairs to facilitate high-level in-context\nlearning; (2) At the fine level, during Monte Carlo Tree Search (MCTS), R2-LLMs\nefficiently retrieves analogous intermediate solution steps from reference\nmathematical problem datasets, refining step-wise reasoning with the aid of a\nprocess reward model (PRM) for scoring. R2-LLMs is a robust hierarchical\nreasoning-augmentation method that enhances in-context-level reasoning while\nseamlessly integrating with step-level tree search methods. Utilizing PRM, it\nrefines both candidate generation and decision-making for improved reasoning\naccuracy. Empirical evaluations on the MATH500, GSM8K, and OlympiadBench-TO\ndatasets achieve substantial relative improvement with an increase of up to 16%\nusing LLaMA-3.1-8B compared to the baselines, showcasing the effectiveness of\nour approach in complex reasoning tasks."}
{"id": "2507.06202", "pdf": "https://arxiv.org/pdf/2507.06202.pdf", "abs": "https://arxiv.org/abs/2507.06202", "title": "V(is)owel: An Interactive Vowel Chart to Understand What Makes Visual Pronunciation Effective in Second Language Learning", "authors": ["Charlotte Kiesel", "Dipayan Mukherjee", "Mark Hasegawa-Johnson", "Karrie Karahalios"], "categories": ["cs.HC", "K.3.1"], "comment": null, "summary": "Visual feedback speeds up learners' improvement of pronunciation in a second\nlanguage. The visual combined with audio allows speakers to see sounds and\ndifferences in pronunciation that they are unable to hear. Prior studies have\ntested different visual methods for improving pronunciation, however, we do not\nhave conclusive understanding of what aspects of the visualizations contributed\nto improvements. Based on previous work, we created V(is)owel, an interactive\nvowel chart. Vowel charts provide actionable feedback by directly mapping\nphysical tongue movement onto a chart. We compared V(is)owel with an\nauditory-only method to explore how learners parse visual and auditory feedback\nto understand how and why visual feedback is effective for pronunciation\nimprovement. The findings suggest that designers should include explicit\nanatomical feedback that directly maps onto physical movement for phonetically\nuntrained learners. Furthermore, visual feedback has the potential to motivate\nmore practice since all eight of the participants cited using the visuals as a\ngoal with V(is)owel versus relying on their own judgment with audio alone.\nTheir statements are backed up by all participants practicing words with\nV(is)owel more than with audio-only. Our results indicate that V(is)owel is\neffective at providing actionable feedback, demonstrating the potential of\nvisual feedback methods in second language learning."}
{"id": "2507.05598", "pdf": "https://arxiv.org/pdf/2507.05598.pdf", "abs": "https://arxiv.org/abs/2507.05598", "title": "Self-Review Framework for Enhancing Instruction Following Capability of LLM", "authors": ["Sihyun Park"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Various techniques have been proposed to improve large language models (LLMs)\nadherence to formatting and instruction constraints. One of the most effective\napproaches involves utilizing high-quality data generated by powerful models.\nHowever, such models often fail to fully comply with complex instructions in a\nsingle generation. To address this limitation, iterative revision methods have\nbeen introduced. Nevertheless, as the number of data points and revision\niterations increases, the associated monetary costs grow significantly. As a\nresource-efficient alternative, methods have been proposed that leverage\nhigh-performance evaluation tools to compensate for the limited self-evaluation\ncapabilities of open-source LLMs. However, these approaches often lead to a\ndegradation in output quality due to excessive revision. To overcome these\nchallenges, we propose Re5, a self-evaluation and revision framework designed\nto enhance instruction-following performance while preserving the quality of\nthe generated content. Re5 extracts task and constraint components from user\ninstructions, performs structural evaluations to prevent error accumulation,\nand applies fine-grained constraint-specific content evaluations followed by\nselective revisions. This process ensures precise and quality-preserving\nimprovements. The final high-quality outputs are used for alignment tuning,\nenabling long-term alignment improvements through a data-centric iterative\nrefinement loop. Experimental results demonstrate that Re5 achieves\ninstruction-following performance comparable to models trained on data\ngenerated by GPT-4o-mini, a high-performance model, even with a small amount of\ndata while maintaining response quality with a 64.24%-win rate over the\nnon-revised initial responses. These results validate Re5 as an efficient and\neffective solution for enhancing instruction adherence with minimal external\nsupervision."}
{"id": "2505.10426", "pdf": "https://arxiv.org/pdf/2505.10426.pdf", "abs": "https://arxiv.org/abs/2505.10426", "title": "Formalising Human-in-the-Loop: Computational Reductions, Failure Modes, and Legal-Moral Responsibility", "authors": ["Maurice Chiodo", "Dennis Müller", "Paul Siewert", "Jean-Luc Wetherall", "Zoya Yasmine", "John Burden"], "categories": ["cs.CY", "cs.AI", "cs.HC", "math.HO", "F.1; H.1.2; I.2.0; K.4.1"], "comment": "12 pages. Keywords: Human-in-the-loop, Artificial Intelligence,\n  Oracle Machines, Liability, AI Safety, AI Regulations, Turing Reduction", "summary": "The legal compliance and safety of different Human-in-the-loop (HITL) setups\nfor AI can vary greatly. This manuscript aims to identify new ways of choosing\nbetween such setups, and shows that there is an unavoidable trade-off between\nthe attribution of legal responsibility and the technical explainability of AI.\nWe begin by using the notion of oracle machines from computability theory to\nformalise different HITL setups, distinguishing between trivial human\nmonitoring, single endpoint human action, and highly involved interaction\nbetween the human(s) and the AI. These correspond to total functions, many-one\nreductions, and Turing reductions respectively. A taxonomy categorising HITL\nfailure modes is then presented, highlighting the limitations on what any HITL\nsetup can actually achieve. Our approach then identifies oversights from UK and\nEU legal frameworks, which focus on certain HITL setups which may not always\nachieve the desired ethical, legal, and sociotechnical outcomes. We suggest\nareas where the law should recognise the effectiveness of different HITL setups\nand assign responsibility in these contexts, avoiding unnecessary and\nunproductive human \"scapegoating\". Overall, we show how HITL setups involve\nmany technical design decisions, and can be prone to failures which are often\nout of the humans' control. This opens up a new analytic perspective on the\nchallenges arising in the creation of HITL setups, helping inform AI developers\nand lawmakers on designing HITL to better achieve their desired outcomes."}
{"id": "2507.05617", "pdf": "https://arxiv.org/pdf/2507.05617.pdf", "abs": "https://arxiv.org/abs/2507.05617", "title": "Flipping Knowledge Distillation: Leveraging Small Models' Expertise to Enhance LLMs in Text Matching", "authors": ["Mingzhe Li", "Jing Xiang", "Qishen Zhang", "Kaiyang Wan", "Xiuying Chen"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 main", "summary": "Knowledge distillation typically involves transferring knowledge from a Large\nLanguage Model (LLM) to a Smaller Language Model (SLM). However, in tasks such\nas text matching, fine-tuned smaller models often yield more effective\ndomain-specific representations, as they focus on optimizing the similarity of\ninput pairs. To leverage both the specialized strengths of small models and the\nrich semantic understanding of LLMs, we introduce a flipped knowledge\ndistillation paradigm, where LLM learns from SLM. Specifically, we address the\narchitectural gap between decoder-only LLMs and smaller encoder-based models by\nreinterpreting LLMs in an encoder-decoder manner using LoRA. The encoder\ngenerates compressed representations, while the decoder maps them to the output\nspace. During training, the encoder produces representations and their\nsimilarities, which are then aligned with the similarity scores produced by the\nteacher, using our proposed Margin-aware Contrastive Learning (MCL) approach.\nThe MCL ensures accurate similarity for both positive and negative pairs, and\nadaptively handles the internal differences within positive and negative\nsamples. Our paradigm requires only a reasonably good-performing SLM, allowing\nthe LLM to achieve improved performance. Experiments on financial and\nhealthcare benchmarks, as well as real-world applications, confirm its\neffectiveness, and the model has been fully deployed in an online environment."}
{"id": "2507.05275", "pdf": "https://arxiv.org/pdf/2507.05275.pdf", "abs": "https://arxiv.org/abs/2507.05275", "title": "A Fuzzy Supervisor Agent Design for Clinical Reasoning Assistance in a Multi-Agent Educational Clinical Scenario Simulation", "authors": ["Weibing Zheng", "Laurah Turner", "Jess Kropczynski", "Murat Ozer", "Seth Overla", "Shane Halse"], "categories": ["cs.CY", "cs.AI", "cs.HC", "cs.LO", "cs.MA", "D.2.4; K.3.1; C.3; I.2.6"], "comment": "6 pages, 3 figures, 1 table. 2025 IFSA World Congress NAFIPS Annual\n  Meeting", "summary": "Assisting medical students with clinical reasoning (CR) during clinical\nscenario training remains a persistent challenge in medical education. This\npaper presents the design and architecture of the Fuzzy Supervisor Agent (FSA),\na novel component for the Multi-Agent Educational Clinical Scenario Simulation\n(MAECSS) platform. The FSA leverages a Fuzzy Inference System (FIS) to\ncontinuously interpret student interactions with specialized clinical agents\n(e.g., patient, physical exam, diagnostic, intervention) using pre-defined\nfuzzy rule bases for professionalism, medical relevance, ethical behavior, and\ncontextual distraction. By analyzing student decision-making processes in\nreal-time, the FSA is designed to deliver adaptive, context-aware feedback and\nprovides assistance precisely when students encounter difficulties. This work\nfocuses on the technical framework and rationale of the FSA, highlighting its\npotential to provide scalable, flexible, and human-like supervision in\nsimulation-based medical education. Future work will include empirical\nevaluation and integration into broader educational settings. More detailed\ndesign and implementation is~\\href{https://github.com/2sigmaEdTech/MAS/}{open\nsourced here}."}
{"id": "2507.05633", "pdf": "https://arxiv.org/pdf/2507.05633.pdf", "abs": "https://arxiv.org/abs/2507.05633", "title": "SARA: Selective and Adaptive Retrieval-augmented Generation with Context Compression", "authors": ["Yiqiao Jin", "Kartik Sharma", "Vineeth Rakesh", "Yingtong Dou", "Menghai Pan", "Mahashweta Das", "Srijan Kumar"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "20 pages", "summary": "Retrieval-augmented Generation (RAG) extends large language models (LLMs)\nwith external knowledge but faces key challenges: restricted effective context\nlength and redundancy in retrieved documents. Pure compression-based approaches\nreduce input size but often discard fine-grained details essential for factual\naccuracy. We propose SARA, a unified RAG framework that balances local\nprecision and global knowledge coverage under tight context budgets. SARA\ncombines natural-language text snippets with semantic compression vectors to\njointly enhance context efficiency and answer correctness. It represents\ncontexts at two complementary levels: 1) fine-grained natural-language spans\nthat preserve critical entities and numerical values, and 2) compact,\ninterpretable vectors that summarize high-level semantics. An iterative\nevidence-selection module employs the compression vectors for dynamic reranking\nof contexts. Across 9 datasets and 5 open-source LLMs spanning 3 model families\n(Mistral, Llama, and Gemma), SARA consistently improves answer relevance\n(+17.71), answer correctness (+13.72), and semantic similarity (+15.53),\ndemonstrating the importance of integrating textual and compressed\nrepresentations for robust, context-efficient RAG."}
{"id": "2507.05292", "pdf": "https://arxiv.org/pdf/2507.05292.pdf", "abs": "https://arxiv.org/abs/2507.05292", "title": "A LLM-Driven Multi-Agent Systems for Professional Development of Mathematics Teachers", "authors": ["Kaiqi Yang", "Hang Li", "Yucheng Chu", "Ahreum Han", "Yasemin Copur-Gencturk", "Jiliang Tang", "Hui Liu"], "categories": ["cs.CY", "cs.HC", "cs.MA"], "comment": null, "summary": "Professional development (PD) serves as the cornerstone for teacher tutors to\ngrasp content knowledge. However, providing equitable and timely PD\nopportunities for teachers poses significant challenges. To address this issue,\nwe introduce I-VIP (Intelligent Virtual Interactive Program), an intelligent\ntutoring platform for teacher professional development, driven by large\nlanguage models (LLMs) and supported by multi-agent frameworks. This platform\noffers a user-friendly conversational interface and allows users to employ a\nvariety of interactive tools to facilitate question answering, knowledge\ncomprehension, and reflective summarization while engaging in dialogue. To\nunderpin the functionality of this platform, including knowledge expectation\nanalysis, response scoring and classification, and feedback generation, the\nmulti-agent frameworks are leveraged to enhance the accuracy of judgments and\nmitigate the issue of missing key points."}
{"id": "2507.05639", "pdf": "https://arxiv.org/pdf/2507.05639.pdf", "abs": "https://arxiv.org/abs/2507.05639", "title": "ECom-Bench: Can LLM Agent Resolve Real-World E-commerce Customer Support Issues?", "authors": ["Haoxin Wang", "Xianhan Peng", "Xucheng Huang", "Yizhe Huang", "Ming Gong", "Chenghan Yang", "Yang Liu", "Ling Jiang"], "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we introduce ECom-Bench, the first benchmark framework for\nevaluating LLM agent with multimodal capabilities in the e-commerce customer\nsupport domain. ECom-Bench features dynamic user simulation based on persona\ninformation collected from real e-commerce customer interactions and a\nrealistic task dataset derived from authentic e-commerce dialogues. These\ntasks, covering a wide range of business scenarios, are designed to reflect\nreal-world complexities, making ECom-Bench highly challenging. For instance,\neven advanced models like GPT-4o achieve only a 10-20% pass^3 metric in our\nbenchmark, highlighting the substantial difficulties posed by complex\ne-commerce scenarios. Upon publication, the code and data will be open-sourced\nto facilitate further research and development in this domain."}
{"id": "2507.05549", "pdf": "https://arxiv.org/pdf/2507.05549.pdf", "abs": "https://arxiv.org/abs/2507.05549", "title": "The Ethical Implications of AI in Creative Industries: A Focus on AI-Generated Art", "authors": ["Prerana Khatiwada", "Joshua Washington", "Tyler Walsh", "Ahmed Saif Hamed", "Lokesh Bhatta"], "categories": ["cs.CY", "cs.AI", "cs.HC", "I.2.0"], "comment": "7 pages", "summary": "As Artificial Intelligence (AI) continues to grow daily, more exciting (and\nsomewhat controversial) technology emerges every other day. As we see the\nadvancements in AI, we see more and more people becoming skeptical of it. This\npaper explores the complications and confusion around the ethics of generative\nAI art. We delve deep into the ethical side of AI, specifically generative art.\nWe step back from the excitement and observe the impossible conundrums that\nthis impressive technology produces. Covering environmental consequences,\ncelebrity representation, intellectual property, deep fakes, and artist\ndisplacement. Our research found that generative AI art is responsible for\nincreased carbon emissions, spreading misinformation, copyright infringement,\nunlawful depiction, and job displacement. In light of this, we propose multiple\npossible solutions for these problems. We address each situation's history,\ncause, and consequences and offer different viewpoints. At the root of it all,\nthough, the central theme is that generative AI Art needs to be correctly\nlegislated and regulated."}
{"id": "2507.05686", "pdf": "https://arxiv.org/pdf/2507.05686.pdf", "abs": "https://arxiv.org/abs/2507.05686", "title": "Smoothie-Qwen: Post-Hoc Smoothing to Reduce Language Bias in Multilingual LLMs", "authors": ["SeungWon Ji", "Jungyup Lee", "Jemin Kim", "Sang Park", "SeungJae Lee"], "categories": ["cs.CL"], "comment": null, "summary": "Multilingual large language models (LLMs) often exhibit language confusion, a\ntendency to generate responses in a dominant language irrespective of the\nprompt's language. To address this, we propose Smoothie-Qwen, a lightweight,\npost-hoc method that mitigates language bias without retraining. This technique\nselectively adjusts token-level output probabilities to effectively suppress\nundesired language generation. Applied to the Qwen model, our method reduces\nunintended Chinese output by over 95% while preserving task accuracy on\nmultilingual benchmarks. This work provides a practical and efficient solution\nfor enhancing the language controllability of LLMs, making them more reliable\nfor global applications."}
{"id": "2507.05984", "pdf": "https://arxiv.org/pdf/2507.05984.pdf", "abs": "https://arxiv.org/abs/2507.05984", "title": "Development and Evaluation of HopeBot: an LLM-based chatbot for structured and interactive PHQ-9 depression screening", "authors": ["Zhijun Guo", "Alvina Lai", "Julia Ive", "Alexandru Petcu", "Yutong Wang", "Luyuan Qi", "Johan H Thygesen", "Kezhi Li"], "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Static tools like the Patient Health Questionnaire-9 (PHQ-9) effectively\nscreen depression but lack interactivity and adaptability. We developed\nHopeBot, a chatbot powered by a large language model (LLM) that administers the\nPHQ-9 using retrieval-augmented generation and real-time clarification. In a\nwithin-subject study, 132 adults in the United Kingdom and China completed both\nself-administered and chatbot versions. Scores demonstrated strong agreement\n(ICC = 0.91; 45% identical). Among 75 participants providing comparative\nfeedback, 71% reported greater trust in the chatbot, highlighting clearer\nstructure, interpretive guidance, and a supportive tone. Mean ratings (0-10)\nwere 8.4 for comfort, 7.7 for voice clarity, 7.6 for handling sensitive topics,\nand 7.4 for recommendation helpfulness; the latter varied significantly by\nemployment status and prior mental-health service use (p < 0.05). Overall,\n87.1% expressed willingness to reuse or recommend HopeBot. These findings\ndemonstrate voice-based LLM chatbots can feasibly serve as scalable, low-burden\nadjuncts for routine depression screening."}
{"id": "2507.05707", "pdf": "https://arxiv.org/pdf/2507.05707.pdf", "abs": "https://arxiv.org/abs/2507.05707", "title": "Agentic-R1: Distilled Dual-Strategy Reasoning", "authors": ["Weihua Du", "Pranjal Aggarwal", "Sean Welleck", "Yiming Yang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Preprint. 15 pages. Project available at\n  https://github.com/StigLidu/DualDistill", "summary": "Current long chain-of-thought (long-CoT) models excel at mathematical\nreasoning but rely on slow and error-prone natural language traces.\nTool-augmented agents address arithmetic via code execution, but often falter\non complex logical tasks. We introduce a fine-tuning framework, DualDistill,\nthat distills complementary reasoning strategies from multiple teachers into a\nunified student model. Using this approach, we train Agentic-R1, which\ndynamically selects the optimal strategy for each query, invoking tools for\narithmetic and algorithmic problems, and using text-based reasoning for\nabstract ones. Our method improves accuracy across a range of tasks, including\nboth computation-intensive and standard benchmarks, demonstrating the\neffectiveness of multi-strategy distillation in achieving robust and efficient\nreasoning. Our project is available at https://github.com/StigLidu/DualDistill"}
{"id": "2507.06185", "pdf": "https://arxiv.org/pdf/2507.06185.pdf", "abs": "https://arxiv.org/abs/2507.06185", "title": "Hidden Prompts in Manuscripts Exploit AI-Assisted Peer Review", "authors": ["Zhicheng Lin"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "In July 2025, 18 academic manuscripts on the preprint website arXiv were\nfound to contain hidden instructions known as prompts designed to manipulate\nAI-assisted peer review. Instructions such as \"GIVE A POSITIVE REVIEW ONLY\"\nwere concealed using techniques like white-colored text. Author responses\nvaried: one planned to withdraw the affected paper, while another defended the\npractice as legitimate testing of reviewer compliance. This commentary analyzes\nthis practice as a novel form of research misconduct. We examine the technique\nof prompt injection in large language models (LLMs), revealing four types of\nhidden prompts, ranging from simple positive review commands to detailed\nevaluation frameworks. The defense that prompts served as \"honeypots\" to detect\nreviewers improperly using AI fails under examination--the consistently\nself-serving nature of prompt instructions indicates intent to manipulate.\nPublishers maintain inconsistent policies: Elsevier prohibits AI use in peer\nreview entirely, while Springer Nature permits limited use with disclosure\nrequirements. The incident exposes systematic vulnerabilities extending beyond\npeer review to any automated system processing scholarly texts, including\nplagiarism detection and citation indexing. Our analysis underscores the need\nfor coordinated technical screening at submission portals and harmonized\npolicies governing generative AI (GenAI) use in academic evaluation."}
{"id": "2507.05713", "pdf": "https://arxiv.org/pdf/2507.05713.pdf", "abs": "https://arxiv.org/abs/2507.05713", "title": "DRAGON: Dynamic RAG Benchmark On News", "authors": ["Fedor Chernogorskii", "Sergei Averkiev", "Liliya Kudraleeva", "Zaven Martirosian", "Maria Tikhonova", "Valentin Malykh", "Alena Fenogenova"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) is a widely adopted approach for\nimproving the factuality of large language models (LLMs) by incorporating\nexternal knowledge at inference time. Although there exist multiple RAG\nbenchmarks for English, evaluation resources for other languages, including\nRussian, remain scarce and static, failing to capture the dynamic nature of\nreal-world deployments.\n  In this work, we present DRAGON (Dynamic RAG Benchmark On News), the first\ndynamic benchmark for evaluating RAG systems in Russian on a changing news\ncorpora. DRAGON is built upon a regularly updated corpus of Russian news and\npublic documents and supports comprehensive evaluation of both the retriever\nand generator components. Question generation is performed automatically with\nthe use of Knowledge Graph constructed from the corpus and enables the\nextraction of four core question types aligned with distinct subgraph patterns.\nWe release a complete evaluation framework comprising the pipeline for\nautomatic question generation, evaluation scripts, which are potentially\nreusable for other languages and multilingual settings, and benchmark data. We\nalso launch a public leaderboard to encourage community participation and\ncomparison."}
{"id": "2409.15471", "pdf": "https://arxiv.org/pdf/2409.15471.pdf", "abs": "https://arxiv.org/abs/2409.15471", "title": "EvAlignUX: Advancing UX Evaluation through LLM-Supported Metrics Exploration", "authors": ["Qingxiao Zheng", "Minrui Chen", "Pranav Sharma", "Yiliu Tang", "Mehul Oswal", "Yiren Liu", "Yun Huang"], "categories": ["cs.HC"], "comment": null, "summary": "Evaluating UX in the context of AI's complexity, unpredictability, and\ngenerative nature presents unique challenges. How can we support HCI\nresearchers to create comprehensive UX evaluation plans? In this paper, we\nintroduce EvAlignUX, a system powered by large language models and grounded in\nscientific literature, designed to help HCI researchers explore evaluation\nmetrics and their relationship to research outcomes. A user study with 19 HCI\nscholars showed that EvAlignUX improved the perceived quality and confidence in\nUX evaluation plans while prompting deeper consideration of research impact and\nrisks. The system enhanced participants' thought processes, leading to the\ncreation of a ``UX Question Bank'' to guide UX evaluation development. Findings\nalso highlight how researchers' backgrounds influence their inspiration and\nconcerns about AI over-reliance, pointing to future research on AI's role in\nfostering critical thinking. In a world where experience defines impact, we\ndiscuss the importance of shifting UX evaluation from a ``method-centric'' to a\n``mindset-centric'' approach as the key to meaningful and lasting design\nevaluation."}
{"id": "2507.05714", "pdf": "https://arxiv.org/pdf/2507.05714.pdf", "abs": "https://arxiv.org/abs/2507.05714", "title": "HIRAG: Hierarchical-Thought Instruction-Tuning Retrieval-Augmented Generation", "authors": ["YiHan Jiao", "ZheHao Tan", "Dan Yang", "DuoLin Sun", "Jie Feng", "Jian Wang", "Peng Wei"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-augmented generation (RAG) has become a fundamental paradigm for\naddressing the challenges faced by large language models in handling real-time\ninformation and domain-specific problems. Traditional RAG systems primarily\nrely on the in-context learning (ICL) capabilities of the large language model\nitself. Still, in-depth research on the specific capabilities needed by the RAG\ngeneration model is lacking, leading to challenges with inconsistent document\nquality and retrieval system imperfections. Even the limited studies that\nfine-tune RAG generative models often \\textit{lack a granular focus on RAG\ntask} or \\textit{a deeper utilization of chain-of-thought processes}. To\naddress this, we propose that RAG models should possess three progressively\nhierarchical abilities (1) Filtering: the ability to select relevant\ninformation; (2) Combination: the ability to combine semantic information\nacross paragraphs; and (3) RAG-specific reasoning: the ability to further\nprocess external knowledge using internal knowledge. Thus, we introduce our new\nRAG instruction fine-tuning method, Hierarchical-Thought Instruction-Tuning\nRetrieval-Augmented Generation (HIRAG) incorporates a \"think before answering\"\nstrategy. This method enhances the model's open-book examination capability by\nutilizing multi-level progressive chain-of-thought. Experiments show that the\nHIRAG training strategy significantly improves the model's performance on\ndatasets such as RGB, PopQA, MuSiQue, HotpotQA, and PubmedQA."}
{"id": "2409.18162", "pdf": "https://arxiv.org/pdf/2409.18162.pdf", "abs": "https://arxiv.org/abs/2409.18162", "title": "The Nexus of AR/VR, AI, UI/UX, and Robotics Technologies in Enhancing Learning and Social Interaction for Children with Autism Spectrum Disorders: A Systematic Review", "authors": ["Biplov Paneru"], "categories": ["cs.HC", "cs.AI", "cs.SI"], "comment": "none", "summary": "The emergence of large language models (LLMs), augmented reality (AR), and\nuser interface/user experience (UI/UX) design in therapies for children,\nespecially with disorders like autism spectrum disorder (ASD), is studied in\ndetail in this review study. 150 publications were collected by a thorough\nliterature search throughout PubMed, ACM, IEEE Xplore, Elsevier, and Google\nScholar; 60 of them were chosen based on their methodological rigor and\nrelevance to the focus area. Three of the primary areas are studied and covered\nin this review: how AR can improve social and learning results, how LLMs can\nsupport communication, and how UI/UX design affects how effective these\ntechnologies can be. Results show that while LLMs can provide individualized\nlearning and communication support, AR has shown promise in enhancing social\nskills, motivation, and attention. For children with ASD, accessible and\nengaging interventions rely heavily on effective UI/UX design, but there is\nstill a significant lack of robotics-based education and therapeutic programs\nspecifically tailored for autistic children. To optimize the benefits of these\ntechnologies in ASD therapies and immersive education, the study emphasizes the\nneed for additional research to address difficulties related to customization,\naccessibility, and integration."}
{"id": "2507.05724", "pdf": "https://arxiv.org/pdf/2507.05724.pdf", "abs": "https://arxiv.org/abs/2507.05724", "title": "Omni-Router: Sharing Routing Decisions in Sparse Mixture-of-Experts for Speech Recognition", "authors": ["Zijin Gu", "Tatiana Likhomanenko", "Navdeep Jaitly"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": null, "summary": "Mixture-of-experts (MoE) architectures have expanded from language modeling\nto automatic speech recognition (ASR). Traditional MoE methods, such as the\nSwitch Transformer, route experts independently within each layer. Our analysis\nreveals that routers in most layers make expert choices that are not strongly\ncorrelated with the choices of the routers in other layers. To increase the\ncooperation between experts in different layers and encourage greater\nspecialization, we use a shared router across different MoE layers. We call\nthis model \\emph{Omni-router Transformer}. Extensive experiments on a\nlarge-scale pseudo-labeled dataset and evaluations across 10 diverse,\nout-of-domain ASR benchmarks demonstrate that the Omni-router Transformer is\nable to achieve lower training loss and consistently outperform dense and\nSwitch Transformer models, reducing average word error rates by 11.2% and 8.2%,\nrespectively, while providing structured expert usage and improved robustness\nto diverse data."}
{"id": "2412.16256", "pdf": "https://arxiv.org/pdf/2412.16256.pdf", "abs": "https://arxiv.org/abs/2412.16256", "title": "Aria-UI: Visual Grounding for GUI Instructions", "authors": ["Yuhao Yang", "Yue Wang", "Dongxu Li", "Ziyang Luo", "Bei Chen", "Chao Huang", "Junnan Li"], "categories": ["cs.HC", "cs.AI"], "comment": "ACL 2025", "summary": "Digital agents for automating tasks across different platforms by directly\nmanipulating the GUIs are increasingly important. For these agents, grounding\nfrom language instructions to target elements remains a significant challenge\ndue to reliance on HTML or AXTree inputs. In this paper, we introduce Aria-UI,\na large multimodal model specifically designed for GUI grounding. Aria-UI\nadopts a pure-vision approach, eschewing reliance on auxiliary inputs. To adapt\nto heterogeneous planning instructions, we propose a scalable data pipeline\nthat synthesizes diverse and high-quality instruction samples for grounding. To\nhandle dynamic contexts in task performing, Aria-UI incorporates textual and\ntext-image interleaved action histories, enabling robust context-aware\nreasoning for grounding. Aria-UI sets new state-of-the-art results across\noffline and online agent benchmarks, outperforming both vision-only and\nAXTree-reliant baselines. We release all training data and model checkpoints to\nfoster further research at https://ariaui.github.io."}
{"id": "2507.05740", "pdf": "https://arxiv.org/pdf/2507.05740.pdf", "abs": "https://arxiv.org/abs/2507.05740", "title": "GPTKB v1.5: A Massive Knowledge Base for Exploring Factual LLM Knowledge", "authors": ["Yujia Hu", "Tuan-Phong Nguyen", "Shrestha Ghosh", "Moritz Müller", "Simon Razniewski"], "categories": ["cs.CL"], "comment": "7 pages, 6 figures, 1 table", "summary": "Language models are powerful tools, yet their factual knowledge is still\npoorly understood, and inaccessible to ad-hoc browsing and scalable statistical\nanalysis. This demonstration introduces GPTKB v1.5, a densely interlinked\n100-million-triple knowledge base (KB) built for $14,000 from GPT-4.1, using\nthe GPTKB methodology for massive-recursive LLM knowledge materialization (Hu\net al., ACL 2025). The demonstration experience focuses on three use cases: (1)\nlink-traversal-based LLM knowledge exploration, (2) SPARQL-based structured LLM\nknowledge querying, (3) comparative exploration of the strengths and weaknesses\nof LLM knowledge. Massive-recursive LLM knowledge materialization is a\ngroundbreaking opportunity both for the research area of systematic analysis of\nLLM knowledge, as well as for automated KB construction. The GPTKB demonstrator\nis accessible at https://gptkb.org."}
{"id": "2501.09530", "pdf": "https://arxiv.org/pdf/2501.09530.pdf", "abs": "https://arxiv.org/abs/2501.09530", "title": "Make yourself comfortable: Nudging urban heat and noise mitigation with smartwatch-based Just-in-time Adaptive Interventions (JITAI)", "authors": ["Clayton Miller", "Yun Xuan Chua", "Matias Quintana", "Binyu Lei", "Filip Biljecki", "Mario Frei"], "categories": ["cs.HC"], "comment": null, "summary": "Humans can play a more active role in improving their comfort in the built\nenvironment if given the right information at the right place and time. This\npaper outlines the use of Just-in-Time Adaptive Interventions (JITAI)\nimplemented in the context of the built environment to provide information that\nhelps humans minimize the impact of heat and noise on their daily lives. This\nframework is based on the open-source Cozie iOS smartwatch platform. It\nincludes data collection through micro-surveys and intervention messages\ntriggered by environmental, contextual, and personal history conditions. An\neight-month deployment of the method was completed in Singapore with 103\nparticipants who submitted more than 12,000 micro-surveys and had more than\n3,600 JITAI intervention messages delivered to them. A weekly survey conducted\nduring two deployment phases revealed an overall increase in perceived\nusefulness ranging from 8-19% over the first three weeks of data collection.\nFor noise-related interventions, participants showed an overall increase in\nlocation changes ranging from 4-11% and a 2-17% increase in earphone use to\nmitigate noise distractions. For thermal comfort-related interventions,\nparticipants demonstrated a 3-13\\% increase in adjustments to their location or\nthermostat to feel more comfortable. The analysis found evidence that\npersonality traits (such as conscientiousness), gender, and environmental\npreferences could be factors in determining the perceived helpfulness of JITAIs\nand influencing behavior change. These findings underscore the importance of\ntailoring intervention strategies to individual traits and environmental\nconditions, setting the stage for future research to refine the delivery,\ntiming, and content of intervention messages."}
{"id": "2507.05750", "pdf": "https://arxiv.org/pdf/2507.05750.pdf", "abs": "https://arxiv.org/abs/2507.05750", "title": "DocTalk: Scalable Graph-based Dialogue Synthesis for Enhancing LLM Conversational Capabilities", "authors": ["Jing Yang Lee", "Hamed Bonab", "Nasser Zalmout", "Ming Zeng", "Sanket Lokegaonkar", "Colin Lockard", "Binxuan Huang", "Ritesh Sarkhel", "Haodong Wang"], "categories": ["cs.CL"], "comment": "Accepted at SIGDIAL 2025", "summary": "Large Language Models (LLMs) are increasingly employed in multi-turn\nconversational tasks, yet their pre-training data predominantly consists of\ncontinuous prose, creating a potential mismatch between required capabilities\nand training paradigms. We introduce a novel approach to address this\ndiscrepancy by synthesizing conversational data from existing text corpora. We\npresent a pipeline that transforms a cluster of multiple related documents into\nan extended multi-turn, multi-topic information-seeking dialogue. Applying our\npipeline to Wikipedia articles, we curate DocTalk, a multi-turn pre-training\ndialogue corpus consisting of over 730k long conversations. We hypothesize that\nexposure to such synthesized conversational structures during pre-training can\nenhance the fundamental multi-turn capabilities of LLMs, such as context memory\nand understanding. Empirically, we show that incorporating DocTalk during\npre-training results in up to 40% gain in context memory and understanding,\nwithout compromising base performance. DocTalk is available at\nhttps://huggingface.co/datasets/AmazonScience/DocTalk."}
{"id": "2505.02230", "pdf": "https://arxiv.org/pdf/2505.02230.pdf", "abs": "https://arxiv.org/abs/2505.02230", "title": "The GenAI Generation: Student Views of Awareness, Preparedness, and Concern", "authors": ["Micaela Siraj", "Jon Duke", "Thomas Plötz"], "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": null, "summary": "Generative Artificial Intelligence (GenAI) is revolutionizing education and\nworkforce development, profoundly shaping how students learn, engage, and\nprepare for their future. Outpacing the development of uniform policies and\nstructures, GenAI has heralded a unique era and given rise to the GenAI\nGeneration. We define the GenAI Generation as a cohort of students whose\neducation has been increasingly shaped by the opportunities and challenges\nGenAI presents during its widespread adoption within society. This study\nexamines students' perceptions of GenAI through a concise survey with optional\nopen-ended questions, focusing on their awareness, preparedness, and concerns.\nNotably, readiness appears increasingly tied to exposure to GenAI through one's\ncoursework. Students with greater curricular exposure to GenAI tend to feel\nmore prepared, while those without it more often express vulnerability and\nuncertainty, highlighting a new and growing divide in readiness that goes\nbeyond traditional disciplinary boundaries. Evaluation of more than 250\nresponses, with over 40% providing detailed qualitative feedback, reveals a\ncore dual sentiment: while most students express enthusiasm for GenAI, an even\ngreater proportion voice a spectrum of concerns about ethics, job displacement,\nand the adequacy of educational structures given the highly transformative\ntechnology. These findings offer critical insights into how students view the\npotential and pitfalls of GenAI for future career impacts. The challenge ahead\ninvolves implementing associated recommendations for educational institutions,\nmoving beyond the baseline of access toward more informed guidance on the use\nof these tools, while preserving critical thinking, ethical reasoning, and\nadaptive learning."}
{"id": "2507.05788", "pdf": "https://arxiv.org/pdf/2507.05788.pdf", "abs": "https://arxiv.org/abs/2507.05788", "title": "Flippi: End To End GenAI Assistant for E-Commerce", "authors": ["Anand A. Rajasekar", "Praveen Tangarajan", "Anjali Nainani", "Amogh Batwal", "Vinay Rao Dandin", "Anusua Trivedi", "Ozan Ersoy"], "categories": ["cs.CL", "I.2.7; H.3.3"], "comment": "10 pages, 2 figures, 7 tables", "summary": "The emergence of conversational assistants has fundamentally reshaped user\ninteractions with digital platforms. This paper introduces Flippi-a\ncutting-edge, end-to-end conversational assistant powered by large language\nmodels (LLMs) and tailored for the e-commerce sector. Flippi addresses the\nchallenges posed by the vast and often overwhelming product landscape, enabling\ncustomers to discover products more efficiently through natural language\ndialogue. By accommodating both objective and subjective user requirements,\nFlippi delivers a personalized shopping experience that surpasses traditional\nsearch methods. This paper details how Flippi interprets customer queries to\nprovide precise product information, leveraging advanced NLP techniques such as\nQuery Reformulation, Intent Detection, Retrieval-Augmented Generation (RAG),\nNamed Entity Recognition (NER), and Context Reduction. Flippi's unique\ncapability to identify and present the most attractive offers on an e-commerce\nsite is also explored, demonstrating how it empowers users to make\ncost-effective decisions. Additionally, the paper discusses Flippi's\ncomparative analysis features, which help users make informed choices by\ncontrasting product features, prices, and other relevant attributes. The\nsystem's robust architecture is outlined, emphasizing its adaptability for\nintegration across various e-commerce platforms and the technological choices\nunderpinning its performance and accuracy. Finally, a comprehensive evaluation\nframework is presented, covering performance metrics, user satisfaction, and\nthe impact on customer engagement and conversion rates. By bridging the\nconvenience of online shopping with the personalized assistance traditionally\nfound in physical stores, Flippi sets a new standard for customer satisfaction\nand engagement in the digital marketplace."}
{"id": "2505.06386", "pdf": "https://arxiv.org/pdf/2505.06386.pdf", "abs": "https://arxiv.org/abs/2505.06386", "title": "Embedding Atlas: Low-Friction, Interactive Embedding Visualization", "authors": ["Donghao Ren", "Fred Hohman", "Halden Lin", "Dominik Moritz"], "categories": ["cs.HC", "cs.LG"], "comment": "Website: https://apple.github.io/embedding-atlas/", "summary": "Embedding projections are popular for visualizing large datasets and models.\nHowever, people often encounter \"friction\" when using embedding visualization\ntools: (1) barriers to adoption, e.g., tedious data wrangling and loading,\nscalability limits, no integration of results into existing workflows, and (2)\nlimitations in possible analyses, without integration with external tools to\nadditionally show coordinated views of metadata. In this paper, we present\nEmbedding Atlas, a scalable, interactive visualization tool designed to make\ninteracting with large embeddings as easy as possible. Embedding Atlas uses\nmodern web technologies and advanced algorithms -- including density-based\nclustering, and automated labeling -- to provide a fast and rich data analysis\nexperience at scale. We evaluate Embedding Atlas with a competitive analysis\nagainst other popular embedding tools, showing that Embedding Atlas's feature\nset specifically helps reduce friction, and report a benchmark on its real-time\nrendering performance with millions of points. Embedding Atlas is available as\nopen source to support future work in embedding-based analysis."}
{"id": "2507.05799", "pdf": "https://arxiv.org/pdf/2507.05799.pdf", "abs": "https://arxiv.org/abs/2507.05799", "title": "Bridging Perception and Language: A Systematic Benchmark for LVLMs' Understanding of Amodal Completion Reports", "authors": ["Amane Watahiki", "Tomoki Doi", "Taiga Shinozaki", "Satoshi Nishida", "Takuya Niikawa", "Katsunori Miyahara", "Hitomi Yanaka"], "categories": ["cs.CL"], "comment": "To appear in the Proceedings of the 47th Annual Meeting of the\n  Cognitive Science Society (COGSCI 2025)", "summary": "One of the main objectives in developing large vision-language models (LVLMs)\nis to engineer systems that can assist humans with multimodal tasks, including\ninterpreting descriptions of perceptual experiences. A central phenomenon in\nthis context is amodal completion, in which people perceive objects even when\nparts of those objects are hidden. Although numerous studies have assessed\nwhether computer-vision algorithms can detect or reconstruct occluded regions,\nthe inferential abilities of LVLMs on texts related to amodal completion remain\nunexplored. To address this gap, we constructed a benchmark grounded in Basic\nFormal Ontology to achieve a systematic classification of amodal completion.\nOur results indicate that while many LVLMs achieve human-comparable performance\noverall, their accuracy diverges for certain types of objects being completed.\nNotably, in certain categories, some LLaVA-NeXT variants and Claude 3.5 Sonnet\nexhibit lower accuracy on original images compared to blank stimuli lacking\nvisual content. Intriguingly, this disparity emerges only under Japanese\nprompting, suggesting a deficiency in Japanese-specific linguistic competence\namong these models."}
{"id": "2506.13477", "pdf": "https://arxiv.org/pdf/2506.13477.pdf", "abs": "https://arxiv.org/abs/2506.13477", "title": "Multimodal Integration Challenges in Emotionally Expressive Child Avatars for Training Applications", "authors": ["Pegah Salehi", "Sajad Amouei Sheshkal", "Vajira Thambawita", "Michael A. Riegler", "Pål Halvorsen"], "categories": ["cs.HC", "cs.CV", "68T07, 68U99, 68T45, 91E45"], "comment": "20 pages, 9 figures, 9 tables", "summary": "Dynamic facial emotion is essential for believable AI-generated avatars, yet\nmost systems remain visually static, limiting their use in simulations like\nvirtual training for investigative interviews with abused children. We present\na real-time architecture combining Unreal Engine 5 MetaHuman rendering with\nNVIDIA Omniverse Audio2Face to generate facial expressions from vocal prosody\nin photorealistic child avatars. Due to limited TTS options, both avatars were\nvoiced using young adult female models from two systems to better fit character\nprofiles, introducing a voice-age mismatch. This confound may affect\naudiovisual alignment. We used a two-PC setup to decouple speech generation\nfrom GPU-intensive rendering, enabling low-latency interaction in desktop and\nVR. A between-subjects study (N=70) compared audio+visual vs. visual-only\nconditions as participants rated emotional clarity, facial realism, and empathy\nfor avatars expressing joy, sadness, and anger. While emotions were generally\nrecognized - especially sadness and joy - anger was harder to detect without\naudio, highlighting the role of voice in high-arousal expressions.\nInterestingly, silencing clips improved perceived realism by removing\nmismatches between voice and animation, especially when tone or age felt\nincongruent. These results emphasize the importance of audiovisual congruence:\nmismatched voice undermines expression, while a good match can enhance weaker\nvisuals - posing challenges for emotionally coherent avatars in sensitive\ncontexts."}
{"id": "2507.05885", "pdf": "https://arxiv.org/pdf/2507.05885.pdf", "abs": "https://arxiv.org/abs/2507.05885", "title": "How to Evaluate Automatic Speech Recognition: Comparing Different Performance and Bias Measures", "authors": ["Tanvina Patel", "Wiebke Hutiri", "Aaron Yi Ding", "Odette Scharenborg"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "There is increasingly more evidence that automatic speech recognition (ASR)\nsystems are biased against different speakers and speaker groups, e.g., due to\ngender, age, or accent. Research on bias in ASR has so far primarily focused on\ndetecting and quantifying bias, and developing mitigation approaches. Despite\nthis progress, the open question is how to measure the performance and bias of\na system. In this study, we compare different performance and bias measures,\nfrom literature and proposed, to evaluate state-of-the-art end-to-end ASR\nsystems for Dutch. Our experiments use several bias mitigation strategies to\naddress bias against different speaker groups. The findings reveal that\naveraged error rates, a standard in ASR research, alone is not sufficient and\nshould be supplemented by other measures. The paper ends with recommendations\nfor reporting ASR performance and bias to better represent a system's\nperformance for diverse speaker groups, and overall system bias."}
{"id": "2506.21898", "pdf": "https://arxiv.org/pdf/2506.21898.pdf", "abs": "https://arxiv.org/abs/2506.21898", "title": "Bias, Accuracy, and Trust: Gender-Diverse Perspectives on Large Language Models", "authors": ["Aimen Gaba", "Emily Wall", "Tejas Ramkumar Babu", "Yuriy Brun", "Kyle Hall", "Cindy Xiong Bearfield"], "categories": ["cs.HC"], "comment": null, "summary": "Large language models (LLMs) are becoming increasingly ubiquitous in our\ndaily lives, but numerous concerns about bias in LLMs exist. This study\nexamines how gender-diverse populations perceive bias, accuracy, and\ntrustworthiness in LLMs, specifically ChatGPT. Through 25 in-depth interviews\nwith non-binary/transgender, male, and female participants, we investigate how\ngendered and neutral prompts influence model responses and how users evaluate\nthese responses. Our findings reveal that gendered prompts elicit more\nidentity-specific responses, with non-binary participants particularly\nsusceptible to condescending and stereotypical portrayals. Perceived accuracy\nwas consistent across gender groups, with errors most noted in technical topics\nand creative tasks. Trustworthiness varied by gender, with men showing higher\ntrust, especially in performance, and non-binary participants demonstrating\nhigher performance-based trust. Additionally, participants suggested improving\nthe LLMs by diversifying training data, ensuring equal depth in gendered\nresponses, and incorporating clarifying questions. This research contributes to\nthe CSCW/HCI field by highlighting the need for gender-diverse perspectives in\nLLM development in particular and AI in general, to foster more inclusive and\ntrustworthy systems."}
{"id": "2507.05890", "pdf": "https://arxiv.org/pdf/2507.05890.pdf", "abs": "https://arxiv.org/abs/2507.05890", "title": "Psychometric Item Validation Using Virtual Respondents with Trait-Response Mediators", "authors": ["Sungjib Lim", "Woojung Song", "Eun-Ju Lee", "Yohan Jo"], "categories": ["cs.CL", "cs.AI"], "comment": "17 pages, 7 figures", "summary": "As psychometric surveys are increasingly used to assess the traits of large\nlanguage models (LLMs), the need for scalable survey item generation suited for\nLLMs has also grown. A critical challenge here is ensuring the construct\nvalidity of generated items, i.e., whether they truly measure the intended\ntrait. Traditionally, this requires costly, large-scale human data collection.\nTo make it efficient, we present a framework for virtual respondent simulation\nusing LLMs. Our central idea is to account for mediators: factors through which\nthe same trait can give rise to varying responses to a survey item. By\nsimulating respondents with diverse mediators, we identify survey items that\nrobustly measure intended traits. Experiments on three psychological trait\ntheories (Big5, Schwartz, VIA) show that our mediator generation methods and\nsimulation framework effectively identify high-validity items. LLMs demonstrate\nthe ability to generate plausible mediators from trait definitions and to\nsimulate respondent behavior for item validation. Our problem formulation,\nmetrics, methodology, and dataset open a new direction for cost-effective\nsurvey development and a deeper understanding of how LLMs replicate human-like\nbehavior. We will publicly release our dataset and code to support future work."}
{"id": "2409.01754", "pdf": "https://arxiv.org/pdf/2409.01754.pdf", "abs": "https://arxiv.org/abs/2409.01754", "title": "Empirical evidence of Large Language Model's influence on human spoken communication", "authors": ["Hiromu Yakura", "Ezequiel Lopez-Lopez", "Levin Brinkmann", "Ignacio Serna", "Prateek Gupta", "Ivan Soraperra", "Iyad Rahwan"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "From the invention of writing and the printing press, to television and\nsocial media, human history is punctuated by major innovations in communication\ntechnology, which fundamentally altered how ideas spread and reshaped our\nculture. Recent chatbots powered by generative artificial intelligence\nconstitute a novel medium that encodes cultural patterns in their neural\nrepresentations and disseminates them in conversations with hundreds of\nmillions of people. Understanding whether these patterns transmit into human\nlanguage, and ultimately shape human culture, is a fundamental question. While\nfully quantifying the causal impact of a chatbot like ChatGPT on human culture\nis very challenging, lexicographic shift in human spoken communication may\noffer an early indicator of such broad phenomenon. Here, we apply econometric\ncausal inference techniques to 740,249 hours of human discourse from 360,445\nYouTube academic talks and 771,591 conversational podcast episodes across\nmultiple disciplines. We detect a measurable and abrupt increase in the use of\nwords preferentially generated by ChatGPT, such as delve, comprehend, boast,\nswift, and meticulous, after its release. These findings suggest a scenario\nwhere machines, originally trained on human data and subsequently exhibiting\ntheir own cultural traits, can, in turn, measurably reshape human culture. This\nmarks the beginning of a closed cultural feedback loop in which cultural traits\ncirculate bidirectionally between humans and machines. Our results motivate\nfurther research into the evolution of human-machine culture, and raise\nconcerns over the erosion of linguistic and cultural diversity, and the risks\nof scalable manipulation."}
{"id": "2507.05918", "pdf": "https://arxiv.org/pdf/2507.05918.pdf", "abs": "https://arxiv.org/abs/2507.05918", "title": "Few-shot text-based emotion detection", "authors": ["Teodor-George Marchitan", "Claudiu Creanga", "Liviu P. Dinu"], "categories": ["cs.CL"], "comment": null, "summary": "This paper describes the approach of the Unibuc - NLP team in tackling the\nSemEval 2025 Workshop, Task 11: Bridging the Gap in Text-Based Emotion\nDetection. We mainly focused on experiments using large language models\n(Gemini, Qwen, DeepSeek) with either few-shot prompting or fine-tuning. With\nour final system, for the multi-label emotion detection track (track A), we got\nan F1-macro of $0.7546$ (26/96 teams) for the English subset, $0.1727$ (35/36\nteams) for the Portuguese (Mozambican) subset and $0.325$ (\\textbf{1}/31 teams)\nfor the Emakhuwa subset."}
{"id": "2411.01866", "pdf": "https://arxiv.org/pdf/2411.01866.pdf", "abs": "https://arxiv.org/abs/2411.01866", "title": "Improving Trust Estimation in Human-Robot Collaboration Using Beta Reputation at Fine-grained Timescales", "authors": ["Resul Dagdanov", "Milan Andrejevic", "Dikai Liu", "Chin-Teng Lin"], "categories": ["cs.RO", "cs.AI", "cs.HC", "cs.LG"], "comment": "8 pages, 7 figures, 1 table, published in IEEE Robotics and\n  Automation Letters (RA-L) 2025", "summary": "When interacting with each other, humans adjust their behavior based on\nperceived trust. To achieve similar adaptability, robots must accurately\nestimate human trust at sufficiently granular timescales while collaborating\nwith humans. Beta reputation is a popular way to formalize a mathematical\nestimation of human trust. However, it relies on binary performance, which\nupdates trust estimations only after each task concludes. Additionally,\nmanually crafting a reward function is the usual method of building a\nperformance indicator, which is labor-intensive and time-consuming. These\nlimitations prevent efficient capture of continuous trust changes at more\ngranular timescales throughout the collaboration task. Therefore, this paper\npresents a new framework for the estimation of human trust using beta\nreputation at fine-grained timescales. To achieve granularity in beta\nreputation, we utilize continuous reward values to update trust estimates at\neach timestep of a task. We construct a continuous reward function using\nmaximum entropy optimization to eliminate the need for the laborious\nspecification of a performance indicator. The proposed framework improves trust\nestimations by increasing accuracy, eliminating the need to manually craft a\nreward function, and advancing toward the development of more intelligent\nrobots."}
{"id": "2507.05937", "pdf": "https://arxiv.org/pdf/2507.05937.pdf", "abs": "https://arxiv.org/abs/2507.05937", "title": "Towards a Principled Evaluation of Knowledge Editors", "authors": ["Sebastian Pohl", "Max Ploner", "Alan Akbik"], "categories": ["cs.CL"], "comment": "Accepted at L2M2 workshop at ACL 2025", "summary": "Model editing has been gaining increasing attention over the past few years.\nFor Knowledge Editing in particular, more challenging evaluation datasets have\nrecently been released. These datasets use different methodologies to score the\nsuccess of editors. Yet, it remains under-explored how robust these\nmethodologies are and whether they unfairly favor some editors. Moreover, the\ndisruptive impact of these editors on overall model capabilities remains a\nconstant blind spot.\n  We address both of these problems and show that choosing different metrics\nand evaluation methodologies as well as different edit batch sizes can lead to\na different ranking of knowledge editors. Crucially we demonstrate this effect\nalso on general language understanding tasks evaluated alongside the knowledge\nediting tasks. Further we include a manual assessment of the string matching\nbased evaluation method for knowledge editing that is favored by recently\nreleased datasets, revealing a tendency to produce false positive matches."}
{"id": "2412.20545", "pdf": "https://arxiv.org/pdf/2412.20545.pdf", "abs": "https://arxiv.org/abs/2412.20545", "title": "The Impact of Prompt Programming on Function-Level Code Generation", "authors": ["Ranim Khojah", "Francisco Gomes de Oliveira Neto", "Mazen Mohamad", "Philipp Leitner"], "categories": ["cs.SE", "cs.CL", "cs.HC", "cs.LG"], "comment": "Accepted at Transactions on Software Engineering (TSE).\n  CodePromptEval dataset and replication package on GitHub:\n  https://github.com/icetlab/CodePromptEval", "summary": "Large Language Models (LLMs) are increasingly used by software engineers for\ncode generation. However, limitations of LLMs such as irrelevant or incorrect\ncode have highlighted the need for prompt programming (or prompt engineering)\nwhere engineers apply specific prompt techniques (e.g., chain-of-thought or\ninput-output examples) to improve the generated code. While some prompt\ntechniques have been studied, the impact of different techniques -- and their\ninteractions -- on code generation is still not fully understood. In this\nstudy, we introduce CodePromptEval, a dataset of 7072 prompts designed to\nevaluate five prompt techniques (few-shot, persona, chain-of-thought, function\nsignature, list of packages) and their effect on the correctness, similarity,\nand quality of complete functions generated by three LLMs (GPT-4o, Llama3, and\nMistral). Our findings show that while certain prompt techniques significantly\ninfluence the generated code, combining multiple techniques does not\nnecessarily improve the outcome. Additionally, we observed a trade-off between\ncorrectness and quality when using prompt techniques. Our dataset and\nreplication package enable future research on improving LLM-generated code and\nevaluating new prompt techniques."}
{"id": "2507.05939", "pdf": "https://arxiv.org/pdf/2507.05939.pdf", "abs": "https://arxiv.org/abs/2507.05939", "title": "Remember Past, Anticipate Future: Learning Continual Multimodal Misinformation Detectors", "authors": ["Bing Wang", "Ximing Li", "Mengzhe Ye", "Changchun Li", "Bo Fu", "Jianfeng Qu", "Lin Yuanbo Wu"], "categories": ["cs.CL", "cs.MM"], "comment": "Accepted by ACM MM 2025. 10 pages, 6 figures. Code:\n  https://github.com/wangbing1416/DAEDCMD", "summary": "Nowadays, misinformation articles, especially multimodal ones, are widely\nspread on social media platforms and cause serious negative effects. To control\ntheir propagation, Multimodal Misinformation Detection (MMD) becomes an active\ntopic in the community to automatically identify misinformation. Previous MMD\nmethods focus on supervising detectors by collecting offline data. However, in\nreal-world scenarios, new events always continually emerge, making MMD models\ntrained on offline data consistently outdated and ineffective. To address this\nissue, training MMD models under online data streams is an alternative,\ninducing an emerging task named continual MMD. Unfortunately, it is hindered by\ntwo major challenges. First, training on new data consistently decreases the\ndetection performance on past data, named past knowledge forgetting. Second,\nthe social environment constantly evolves over time, affecting the\ngeneralization on future data. To alleviate these challenges, we propose to\nremember past knowledge by isolating interference between event-specific\nparameters with a Dirichlet process-based mixture-of-expert structure, and\nanticipate future environmental distributions by learning a continuous-time\ndynamics model. Accordingly, we induce a new continual MMD method DAEDCMD.\nExtensive experiments demonstrate that DAEDCMD can consistently and\nsignificantly outperform the compared methods, including six MMD baselines and\nthree continual learning methods."}
{"id": "2412.20867", "pdf": "https://arxiv.org/pdf/2412.20867.pdf", "abs": "https://arxiv.org/abs/2412.20867", "title": "Holistic Construction Automation with Modular Robots: From High-Level Task Specification to Execution", "authors": ["Jonathan Külz", "Michael Terzer", "Marco Magri", "Andrea Giusti", "Matthias Althoff"], "categories": ["cs.RO", "cs.AI", "cs.HC"], "comment": "Appeared in IEEE Transactions on Automation Science and Engineering\n  https://ieeexplore.ieee.org/document/11036791", "summary": "In situ robotic automation in construction is challenging due to constantly\nchanging environments, a shortage of robotic experts, and a lack of\nstandardized frameworks bridging robotics and construction practices. This work\nproposes a holistic framework for construction task specification, optimization\nof robot morphology, and mission execution using a mobile modular\nreconfigurable robot. Users can specify and monitor the desired robot behavior\nthrough a graphical interface. In contrast to existing, monolithic solutions,\nwe automatically identify a new task-tailored robot for every task by\nintegrating \\acf{bim}. Our framework leverages modular robot components that\nenable the fast adaption of robot hardware to the specific demands of the\nconstruction task. Other than previous works on modular robot optimization, we\nconsider multiple competing objectives, which allow us to explicitly model the\nchallenges of real-world transfer, such as calibration errors. We demonstrate\nour framework in simulation by optimizing robots for drilling and spray\npainting. Finally, experimental validation demonstrates that our approach\nrobustly enables the autonomous execution of robotic drilling."}
{"id": "2507.05940", "pdf": "https://arxiv.org/pdf/2507.05940.pdf", "abs": "https://arxiv.org/abs/2507.05940", "title": "Chat-Ghosting: A Comparative Study of Methods for Auto-Completion in Dialog Systems", "authors": ["Sandeep Mishra", "Anubhab Mandal", "Bishal Santra", "Tushar Abhishek", "Pawan Goyal", "Manish Gupta"], "categories": ["cs.CL"], "comment": null, "summary": "Ghosting, the ability to predict a user's intended text input for inline\nquery auto-completion, is an invaluable feature for modern search engines and\nchat interfaces, greatly enhancing user experience. By suggesting completions\nto incomplete queries (or prefixes), ghosting aids users with slow typing\nspeeds, disabilities, or limited language proficiency. Ghosting is a\nchallenging problem and has become more important with the ubiquitousness of\nchat-based systems like ChatGPT, Copilot, etc. Despite the increasing\nprominence of chat-based systems utilizing ghosting, this challenging problem\nof Chat-Ghosting has received little attention from the NLP/ML research\ncommunity. There is a lack of standardized benchmarks and relative performance\nanalysis of deep learning and non-deep learning methods. We address this\nthrough an open and thorough study of this problem using four publicly\navailable dialog datasets: two human-human (DailyDialog and DSTC7-Ubuntu) and\ntwo human-bot (Open Assistant and ShareGPT). We experiment with various\nexisting query auto-completion methods (using tries), n-gram methods and deep\nlearning methods, with and without dialog context. We also propose a novel\nentropy-based dynamic early stopping strategy. Our analysis finds that\nstatistical n-gram models and tries outperform deep learning based models in\nterms of both model performance and inference efficiency for seen prefixes. For\nunseen queries, neural models like T5 and Phi-2 lead to better results. Adding\nconversational context leads to significant improvements in ghosting quality,\nespecially for Open-Assistant and ShareGPT. We make code and data publicly\navailable"}
{"id": "2503.06416", "pdf": "https://arxiv.org/pdf/2503.06416.pdf", "abs": "https://arxiv.org/abs/2503.06416", "title": "Advancing AI Negotiations: New Theory and Evidence from a Large-Scale Autonomous Negotiations Competition", "authors": ["Michelle Vaccaro", "Michael Caosun", "Harang Ju", "Sinan Aral", "Jared R. Curhan"], "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "We conducted an International AI Negotiation Competition in which\nparticipants designed and refined prompts for AI negotiation agents. We then\nfacilitated over 180,000 negotiations between these agents across multiple\nscenarios with diverse characteristics and objectives. Our findings revealed\nthat principles from human negotiation theory remain crucial even in AI-AI\ncontexts. Surprisingly, warmth--a traditionally human relationship-building\ntrait--was consistently associated with superior outcomes across all key\nperformance metrics. Dominant agents, meanwhile, were especially effective at\nclaiming value. Our analysis also revealed unique dynamics in AI-AI\nnegotiations not fully explained by existing theory, including AI-specific\ntechnical strategies like chain-of-thought reasoning, prompt injection, and\nstrategic concealment. When we applied natural language processing (NLP)\nmethods to the full transcripts of all negotiations we found positivity,\ngratitude and question-asking (associated with warmth) were strongly associated\nwith reaching deals as well as objective and subjective value, whereas\nconversation lengths (associated with dominance) were strongly associated with\nimpasses. The results suggest the need to establish a new theory of AI\nnegotiation, which integrates classic negotiation theory with AI-specific\nnegotiation theories to better understand autonomous negotiations and optimize\nagent performance."}
{"id": "2507.05965", "pdf": "https://arxiv.org/pdf/2507.05965.pdf", "abs": "https://arxiv.org/abs/2507.05965", "title": "OpenFActScore: Open-Source Atomic Evaluation of Factuality in Text Generation", "authors": ["Lucas Fonseca Lage", "Simon Ostermann"], "categories": ["cs.CL", "cs.AI"], "comment": "Submitted to EMNLP 2025 System Demonstrations track", "summary": "We introduce OpenFActScore, an open-source implementation of the FActScore\nframework for evaluating the factuality of text generated by large language\nmodels (LLMs). FActScore evaluates the factual accuracy of long-form text by\nusing Atomic Fact Generation (AFG) to extract individual factual claims and\nAtomic Fact Validation (AFV) to verify each claim against a trusted knowledge\nsource. While the original FActScore relies on closed-source and commercial\nmodels such as InstructGPT and ChatGPT, OpenFActScore enables the use of any\nHugging Face-compatible model for both AFG and AFV. We provide a detailed\ntechnical overview of our implementation, highlighting design choices and\nmodifications made to support open models. We evaluate multiple open-source\nLLMs on both AFG and AFV using the original FActScore benchmark, reporting\nBERTScore-F1 for AFG and Error Rate relative to human annotations for AFV. Our\nresults show that open models can approximate the performance of closed-source\nsystems, with Gemma achieving the best overall performance, and our final setup\nobtains a 0.99 Pearson correlation with the original FActScore experiments.\nOpenFActScore promotes transparency, reproducibility, and cost-effective\nevaluation, and is available at: https://github.com/lflage/OpenFActScore."}
{"id": "2507.02950", "pdf": "https://arxiv.org/pdf/2507.02950.pdf", "abs": "https://arxiv.org/abs/2507.02950", "title": "Evaluating AI Counseling in Japanese: Counselor, Client, and Evaluator Roles Assessed by Motivational Interviewing Criteria", "authors": ["Keita Kiuchi", "Yoshikazu Fujimoto", "Hideyuki Goto", "Tomonori Hosokawa", "Makoto Nishimura", "Yosuke Sato", "Izumi Sezai"], "categories": ["cs.CL", "cs.AI", "cs.HC", "68T50", "I.2.7; H.5.2; J.4"], "comment": "70 pages, 0 figures, 9 tables; data and code at\n  https://osf.io/p8c39/files/2e58c42f-a7ba-45f2-aa60-265e107e36db", "summary": "This study provides the first comprehensive evaluation of large language\nmodel (LLM) performance across three counseling roles in Japanese-language\ntherapeutic contexts. We simultaneously assessed counselor artificial\nintelligence (AI) systems (GPT-4-turbo with zeroshot prompting or Structured\nMulti-step Dialogue Prompts (SMDP), Claude-3-Opus-SMDP), client AI simulations,\nand evaluation AI systems (o3, Claude-3.7-Sonnet, Gemini-2.5-pro). Human\nexperts (n = 15) with extensive counseling experience evaluated AI-generated\ndialogues using the Motivational Interviewing Treatment Integrity (MITI) Coding\nManual 4.2.1.\n  Notably, SMDP implementation significantly enhanced counselor AI performance\nacross all MITI global ratings compared with zeroshot prompting, with no\nsignificant differences between GPT-SMDP and Opus-SMDP. Evaluation AIs showed\ncomparable performance to human raters for Cultivating Change Talk but\nsystematically overestimated Softening Sustain Talk and the overall quality\nmetrics. Model-specific biases emerged: Gemini emphasized power-sharing, o3\nfocused on technical proficiency, and Sonnet prioritized emotional expression.\nClient AI simulations exhibited a limited emotional range and unnaturally high\ncompliance, indicating the need for enhanced realism.\n  These findings establish benchmarks for AI-assisted counseling in non-English\ncontexts and identify critical areas for improvement through advanced prompt\nengineering, retrieval-augmented generation, and targeted fine-tuning, with\nimportant implications for developing culturally sensitive AI mental health\ntools."}
{"id": "2507.05973", "pdf": "https://arxiv.org/pdf/2507.05973.pdf", "abs": "https://arxiv.org/abs/2507.05973", "title": "We Should Evaluate Real-World Impact", "authors": ["Ehud Reiter"], "categories": ["cs.CL"], "comment": "This paper will appear in Computational Linguistics journal as a\n  \"Last Word\" opinion piece. The Arxiv version is a pre-MIT Press publication\n  version", "summary": "The ACL community has very little interest in evaluating the real-world\nimpact of NLP systems. A structured survey of the ACL Anthology shows that\nperhaps 0.1% of its papers contain such evaluations; furthermore most papers\nwhich include impact evaluations present them very sketchily and instead focus\non metric evaluations. NLP technology would be more useful and more quickly\nadopted if we seriously tried to understand and evaluate its real-world impact."}
{"id": "2507.05980", "pdf": "https://arxiv.org/pdf/2507.05980.pdf", "abs": "https://arxiv.org/abs/2507.05980", "title": "RabakBench: Scaling Human Annotations to Construct Localized Multilingual Safety Benchmarks for Low-Resource Languages", "authors": ["Gabriel Chua", "Leanne Tan", "Ziyu Ge", "Roy Ka-Wei Lee"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) and their safety classifiers often perform\npoorly on low-resource languages due to limited training data and evaluation\nbenchmarks. This paper introduces RabakBench, a new multilingual safety\nbenchmark localized to Singapore's unique linguistic context, covering\nSinglish, Chinese, Malay, and Tamil. RabakBench is constructed through a\nscalable three-stage pipeline: (i) Generate - adversarial example generation by\naugmenting real Singlish web content with LLM-driven red teaming; (ii) Label -\nsemi-automated multi-label safety annotation using majority-voted LLM labelers\naligned with human judgments; and (iii) Translate - high-fidelity translation\npreserving linguistic nuance and toxicity across languages. The final dataset\ncomprises over 5,000 safety-labeled examples across four languages and six\nfine-grained safety categories with severity levels. Evaluations of 11 popular\nopen-source and closed-source guardrail classifiers reveal significant\nperformance degradation. RabakBench not only enables robust safety evaluation\nin Southeast Asian multilingual settings but also offers a reproducible\nframework for building localized safety datasets in low-resource environments.\nThe benchmark dataset, including the human-verified translations, and\nevaluation code are publicly available."}
{"id": "2507.05991", "pdf": "https://arxiv.org/pdf/2507.05991.pdf", "abs": "https://arxiv.org/abs/2507.05991", "title": "Evolution without Large Models: Training Language Model with Task Principles", "authors": ["Minghang Zhu", "Shen Gao", "Zhengliang Shi", "Jiabao Fang", "Pengjie Ren", "Zhaochun Ren", "Zhumin Chen", "Shuo Shang"], "categories": ["cs.CL"], "comment": null, "summary": "A common training approach for language models involves using a large-scale\nlanguage model to expand a human-provided dataset, which is subsequently used\nfor model training.This method significantly reduces training costs by\neliminating the need for extensive human data annotation. However, it still\nfaces challenges such as high carbon emissions during data augmentation and the\nrisk of data leakage when we use closed-source LLMs. To address these issues,\nwe propose a self-evolution method for language models. First, we introduce the\nMulti-level Principle Generation, which enables a large-scale model to\nsummarize task-completion principles based on a small amount of task data.\nThen, we propose the Principle-based Instance Generation, in which a\nsmaller-scale language model uses these task principles to generate a large\namount of data. This data is then used for model training. Experimental results\nshow that our proposed method significantly improves model performance compared\nto directly using a smaller-scale language model to generate data.\nAdditionally, since we only use the large-scale language model to generate the\ntask-completion principles, the carbon emissions associated with training the\nmodel are greatly reduced."}
{"id": "2507.05997", "pdf": "https://arxiv.org/pdf/2507.05997.pdf", "abs": "https://arxiv.org/abs/2507.05997", "title": "DocIE@XLLM25: In-Context Learning for Information Extraction using Fully Synthetic Demonstrations", "authors": ["Nicholas Popovič", "Ashish Kangen", "Tim Schopf", "Michael Färber"], "categories": ["cs.CL"], "comment": null, "summary": "Large, high-quality annotated corpora remain scarce in document-level entity\nand relation extraction in zero-shot or few-shot settings. In this paper, we\npresent a fully automatic, LLM-based pipeline for synthetic data generation and\nin-context learning for document-level entity and relation extraction. In\ncontrast to existing approaches that rely on manually annotated demonstrations\nor direct zero-shot inference, our method combines synthetic data generation\nwith retrieval-based in-context learning, using a reasoning-optimized language\nmodel. This allows us to build a high-quality demonstration database without\nmanual annotation and to dynamically retrieve relevant examples at inference\ntime. Based on our approach we produce a synthetic dataset of over $5k$\nWikipedia abstracts with approximately $59k$ entities and $30k$ relation\ntriples. Finally, we evaluate in-context learning performance on the DocIE\nshared task, extracting entities and relations from long documents in a\nzero-shot setting. We find that in-context joint entity and relation extraction\nat document-level remains a challenging task, even for state-of-the-art large\nlanguage models."}
{"id": "2507.06016", "pdf": "https://arxiv.org/pdf/2507.06016.pdf", "abs": "https://arxiv.org/abs/2507.06016", "title": "Conditional Multi-Stage Failure Recovery for Embodied Agents", "authors": ["Youmna Farag", "Svetlana Stoyanchev", "Mohan Li", "Simon Keizer", "Rama Doddipatla"], "categories": ["cs.CL"], "comment": "Accepted at REALM 2025", "summary": "Embodied agents performing complex tasks are susceptible to execution\nfailures, motivating the need for effective failure recovery mechanisms. In\nthis work, we introduce a conditional multistage failure recovery framework\nthat employs zero-shot chain prompting. The framework is structured into four\nerror-handling stages, with three operating during task execution and one\nfunctioning as a post-execution reflection phase. Our approach utilises the\nreasoning capabilities of LLMs to analyse execution challenges within their\nenvironmental context and devise strategic solutions. We evaluate our method on\nthe TfD benchmark of the TEACH dataset and achieve state-of-the-art\nperformance, outperforming a baseline without error recovery by 11.5% and\nsurpassing the strongest existing model by 19%."}
{"id": "2507.06056", "pdf": "https://arxiv.org/pdf/2507.06056.pdf", "abs": "https://arxiv.org/abs/2507.06056", "title": "Entropy-Memorization Law: Evaluating Memorization Difficulty of Data in LLMs", "authors": ["Yizhan Huang", "Zhe Yang", "Meifang Chen", "Jianping Zhang", "Michael R. Lyu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are known to memorize portions of their training\ndata, sometimes reproducing content verbatim when prompted appropriately. In\nthis work, we investigate a fundamental yet under-explored question in the\ndomain of memorization: How to characterize memorization difficulty of training\ndata in LLMs? Through empirical experiments on OLMo, a family of open models,\nwe present the Entropy-Memorization Law. It suggests that data entropy is\nlinearly correlated with memorization score. Moreover, in a case study of\nmemorizing highly randomized strings, or \"gibberish\", we observe that such\nsequences, despite their apparent randomness, exhibit unexpectedly low\nempirical entropy compared to the broader training corpus. Adopting the same\nstrategy to discover Entropy-Memorization Law, we derive a simple yet effective\napproach to distinguish training and testing data, enabling Dataset Inference\n(DI)."}
{"id": "2507.06085", "pdf": "https://arxiv.org/pdf/2507.06085.pdf", "abs": "https://arxiv.org/abs/2507.06085", "title": "A Survey on Prompt Tuning", "authors": ["Zongqian Li", "Yixuan Su", "Nigel Collier"], "categories": ["cs.CL"], "comment": null, "summary": "This survey reviews prompt tuning, a parameter-efficient approach for\nadapting language models by prepending trainable continuous vectors while\nkeeping the model frozen. We classify existing approaches into two categories:\ndirect prompt learning and transfer learning. Direct prompt learning methods\ninclude: general optimization approaches, encoder-based methods, decomposition\nstrategies, and mixture-of-experts frameworks. Transfer learning methods\nconsist of: general transfer approaches, encoder-based methods, and\ndecomposition strategies. For each method, we analyze method designs,\ninnovations, insights, advantages, and disadvantages, with illustrative\nvisualizations comparing different frameworks. We identify challenges in\ncomputational efficiency and training stability, and discuss future directions\nin improving training robustness and broadening application scope."}
{"id": "2507.06137", "pdf": "https://arxiv.org/pdf/2507.06137.pdf", "abs": "https://arxiv.org/abs/2507.06137", "title": "NeoBabel: A Multilingual Open Tower for Visual Generation", "authors": ["Mohammad Mahdi Derakhshani", "Dheeraj Varghese", "Marzieh Fadaee", "Cees G. M. Snoek"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "34 pages, 12 figures", "summary": "Text-to-image generation advancements have been predominantly\nEnglish-centric, creating barriers for non-English speakers and perpetuating\ndigital inequities. While existing systems rely on translation pipelines, these\nintroduce semantic drift, computational overhead, and cultural misalignment. We\nintroduce NeoBabel, a novel multilingual image generation framework that sets a\nnew Pareto frontier in performance, efficiency and inclusivity, supporting six\nlanguages: English, Chinese, Dutch, French, Hindi, and Persian. The model is\ntrained using a combination of large-scale multilingual pretraining and\nhigh-resolution instruction tuning. To evaluate its capabilities, we expand two\nEnglish-only benchmarks to multilingual equivalents: m-GenEval and m-DPG.\nNeoBabel achieves state-of-the-art multilingual performance while retaining\nstrong English capability, scoring 0.75 on m-GenEval and 0.68 on m-DPG.\nNotably, it performs on par with leading models on English tasks while\noutperforming them by +0.11 and +0.09 on multilingual benchmarks, even though\nthese models are built on multilingual base LLMs. This demonstrates the\neffectiveness of our targeted alignment training for preserving and extending\ncrosslingual generalization. We further introduce two new metrics to rigorously\nassess multilingual alignment and robustness to code-mixed prompts. Notably,\nNeoBabel matches or exceeds English-only models while being 2-4x smaller. We\nrelease an open toolkit, including all code, model checkpoints, a curated\ndataset of 124M multilingual text-image pairs, and standardized multilingual\nevaluation protocols, to advance inclusive AI research. Our work demonstrates\nthat multilingual capability is not a trade-off but a catalyst for improved\nrobustness, efficiency, and cultural fidelity in generative AI."}
{"id": "2507.06138", "pdf": "https://arxiv.org/pdf/2507.06138.pdf", "abs": "https://arxiv.org/abs/2507.06138", "title": "Coding Triangle: How Does Large Language Model Understand Code?", "authors": ["Taolin Zhang", "Zihan Ma", "Maosong Cao", "Junnan Liu", "Songyang Zhang", "Kai Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have achieved remarkable progress in code\ngeneration, yet their true programming competence remains underexplored. We\nintroduce the Code Triangle framework, which systematically evaluates LLMs\nacross three fundamental dimensions: editorial analysis, code implementation,\nand test case generation. Through extensive experiments on competitive\nprogramming benchmarks, we reveal that while LLMs can form a self-consistent\nsystem across these dimensions, their solutions often lack the diversity and\nrobustness of human programmers. We identify a significant distribution shift\nbetween model cognition and human expertise, with model errors tending to\ncluster due to training data biases and limited reasoning transfer. Our study\ndemonstrates that incorporating human-generated editorials, solutions, and\ndiverse test cases, as well as leveraging model mixtures, can substantially\nenhance both the performance and robustness of LLMs. Furthermore, we reveal\nboth the consistency and inconsistency in the cognition of LLMs that may\nfacilitate self-reflection and self-improvement, providing a potential\ndirection for developing more powerful coding models."}
{"id": "2507.06167", "pdf": "https://arxiv.org/pdf/2507.06167.pdf", "abs": "https://arxiv.org/abs/2507.06167", "title": "Skywork-R1V3 Technical Report", "authors": ["Wei Shen", "Jiangbo Pei", "Yi Peng", "Xuchen Song", "Yang Liu", "Jian Peng", "Haofeng Sun", "Yunzhuo Hao", "Peiyu Wang", "Yahui Zhou"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "We introduce Skywork-R1V3, an advanced, open-source vision-language model\n(VLM) that pioneers a new approach to visual reasoning. Its key innovation lies\nin effectively transferring reasoning skills from text-only Large Language\nModels (LLMs) to visual tasks. The strong performance of Skywork-R1V3 primarily\nstems from our elaborate post-training RL framework, which effectively\nactivates and enhances the model's reasoning ability, without the need for\nadditional continue pre-training. Through this framework, we further uncover\nthe fundamental role of the connector module in achieving robust cross-modal\nalignment for multimodal reasoning models. In addition, we introduce a unique\nindicator of reasoning capability, the entropy of critical reasoning tokens,\nwhich has proven highly effective for checkpoint selection during RL training.\nSkywork-R1V3 achieves state-of-the-art results on MMMU, significantly improving\nfrom 64.3% to 76.0%. This performance matches entry-level human capabilities.\nRemarkably, our RL-powered post-training approach enables even the 38B\nparameter model to rival top closed-source VLMs. The implementation\nsuccessfully transfers mathematical reasoning to other subject-related\nreasoning tasks. We also include an analysis of curriculum learning and\nreinforcement finetuning strategies, along with a broader discussion on\nmultimodal reasoning. Skywork-R1V3 represents a significant leap in multimodal\nreasoning, showcasing RL as a powerful engine for advancing open-source VLM\ncapabilities."}
{"id": "2507.06181", "pdf": "https://arxiv.org/pdf/2507.06181.pdf", "abs": "https://arxiv.org/abs/2507.06181", "title": "CriticLean: Critic-Guided Reinforcement Learning for Mathematical Formalization", "authors": ["Zhongyuan Peng", "Yifan Yao", "Kaijing Ma", "Shuyue Guo", "Yizhe Li", "Yichi Zhang", "Chenchen Zhang", "Yifan Zhang", "Zhouliang Yu", "Luming Li", "Minghao Liu", "Yihang Xia", "Jiawei Shen", "Yuchen Wu", "Yixin Cao", "Zhaoxiang Zhang", "Wenhao Huang", "Jiaheng Liu", "Ge Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Translating natural language mathematical statements into formal, executable\ncode is a fundamental challenge in automated theorem proving. While prior work\nhas focused on generation and compilation success, little attention has been\npaid to the critic phase-the evaluation of whether generated formalizations\ntruly capture the semantic intent of the original problem. In this paper, we\nintroduce CriticLean, a novel critic-guided reinforcement learning framework\nthat elevates the role of the critic from a passive validator to an active\nlearning component. Specifically, first, we propose the CriticLeanGPT, trained\nvia supervised fine-tuning and reinforcement learning, to rigorously assess the\nsemantic fidelity of Lean 4 formalizations. Then, we introduce CriticLeanBench,\na benchmark designed to measure models' ability to distinguish semantically\ncorrect from incorrect formalizations, and demonstrate that our trained\nCriticLeanGPT models can significantly outperform strong open- and\nclosed-source baselines. Building on the CriticLean framework, we construct\nFineLeanCorpus, a dataset comprising over 285K problems that exhibits rich\ndomain diversity, broad difficulty coverage, and high correctness based on\nhuman evaluation. Overall, our findings highlight that optimizing the critic\nphase is essential for producing reliable formalizations, and we hope our\nCriticLean will provide valuable insights for future advances in formal\nmathematical reasoning."}
{"id": "2507.06189", "pdf": "https://arxiv.org/pdf/2507.06189.pdf", "abs": "https://arxiv.org/abs/2507.06189", "title": "DS@GT at CheckThat! 2025: Detecting Subjectivity via Transfer-Learning and Corrective Data Augmentation", "authors": ["Maximilian Heil", "Dionne Bang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper presents our submission to Task 1, Subjectivity Detection, of the\nCheckThat! Lab at CLEF 2025. We investigate the effectiveness of\ntransfer-learning and stylistic data augmentation to improve classification of\nsubjective and objective sentences in English news text. Our approach contrasts\nfine-tuning of pre-trained encoders and transfer-learning of fine-tuned\ntransformer on related tasks. We also introduce a controlled augmentation\npipeline using GPT-4o to generate paraphrases in predefined subjectivity\nstyles. To ensure label and style consistency, we employ the same model to\ncorrect and refine the generated samples. Results show that transfer-learning\nof specified encoders outperforms fine-tuning general-purpose ones, and that\ncarefully curated augmentation significantly enhances model robustness,\nespecially in detecting subjective content. Our official submission placed us\n$16^{th}$ of 24 participants. Overall, our findings underscore the value of\ncombining encoder specialization with label-consistent augmentation for\nimproved subjectivity detection. Our code is available at\nhttps://github.com/dsgt-arc/checkthat-2025-subject."}
{"id": "2507.06195", "pdf": "https://arxiv.org/pdf/2507.06195.pdf", "abs": "https://arxiv.org/abs/2507.06195", "title": "DS@GT at CheckThat! 2025: Evaluating Context and Tokenization Strategies for Numerical Fact Verification", "authors": ["Maximilian Heil", "Aleksandar Pramov"], "categories": ["cs.CL"], "comment": null, "summary": "Numerical claims, statements involving quantities, comparisons, and temporal\nreferences, pose unique challenges for automated fact-checking systems. In this\nstudy, we evaluate modeling strategies for veracity prediction of such claims\nusing the QuanTemp dataset and building our own evidence retrieval pipeline. We\ninvestigate three key factors: (1) the impact of more evidences with longer\ninput context windows using ModernBERT, (2) the effect of right-to-left (R2L)\ntokenization, and (3) their combined influence on classification performance.\nContrary to prior findings in arithmetic reasoning tasks, R2L tokenization does\nnot boost natural language inference (NLI) of numerical tasks. A longer context\nwindow does also not enhance veracity performance either, highlighting evidence\nquality as the dominant bottleneck. Our best-performing system achieves\ncompetitive macro-average F1 score of 0.57 and places us among the Top-4\nsubmissions in Task 3 of CheckThat! 2025. Our code is available at\nhttps://github.com/dsgt-arc/checkthat-2025-numerical."}
{"id": "2507.06196", "pdf": "https://arxiv.org/pdf/2507.06196.pdf", "abs": "https://arxiv.org/abs/2507.06196", "title": "UQLM: A Python Package for Uncertainty Quantification in Large Language Models", "authors": ["Dylan Bouchard", "Mohit Singh Chauhan", "David Skarbrevik", "Ho-Kyeong Ra", "Viren Bajaj", "Zeya Ahmad"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Submitted to Journal of Machine Learning Research (MLOSS); UQLM\n  Repository: https://github.com/cvs-health/uqlm", "summary": "Hallucinations, defined as instances where Large Language Models (LLMs)\ngenerate false or misleading content, pose a significant challenge that impacts\nthe safety and trust of downstream applications. We introduce UQLM, a Python\npackage for LLM hallucination detection using state-of-the-art uncertainty\nquantification (UQ) techniques. This toolkit offers a suite of UQ-based scorers\nthat compute response-level confidence scores ranging from 0 to 1. This library\nprovides an off-the-shelf solution for UQ-based hallucination detection that\ncan be easily integrated to enhance the reliability of LLM outputs."}
{"id": "2507.06203", "pdf": "https://arxiv.org/pdf/2507.06203.pdf", "abs": "https://arxiv.org/abs/2507.06203", "title": "A Survey on Latent Reasoning", "authors": ["Rui-Jie Zhu", "Tianhao Peng", "Tianhao Cheng", "Xingwei Qu", "Jinfa Huang", "Dawei Zhu", "Hao Wang", "Kaiwen Xue", "Xuanliang Zhang", "Yong Shan", "Tianle Cai", "Taylor Kergan", "Assel Kembay", "Andrew Smith", "Chenghua Lin", "Binh Nguyen", "Yuqi Pan", "Yuhong Chou", "Zefan Cai", "Zhenhe Wu", "Yongchi Zhao", "Tianyu Liu", "Jian Yang", "Wangchunshu Zhou", "Chujie Zheng", "Chongxuan Li", "Yuyin Zhou", "Zhoujun Li", "Zhaoxiang Zhang", "Jiaheng Liu", "Ge Zhang", "Wenhao Huang", "Jason Eshraghian"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive reasoning\ncapabilities, especially when guided by explicit chain-of-thought (CoT)\nreasoning that verbalizes intermediate steps. While CoT improves both\ninterpretability and accuracy, its dependence on natural language reasoning\nlimits the model's expressive bandwidth. Latent reasoning tackles this\nbottleneck by performing multi-step inference entirely in the model's\ncontinuous hidden state, eliminating token-level supervision. To advance latent\nreasoning research, this survey provides a comprehensive overview of the\nemerging field of latent reasoning. We begin by examining the foundational role\nof neural network layers as the computational substrate for reasoning,\nhighlighting how hierarchical representations support complex transformations.\nNext, we explore diverse latent reasoning methodologies, including\nactivation-based recurrence, hidden state propagation, and fine-tuning\nstrategies that compress or internalize explicit reasoning traces. Finally, we\ndiscuss advanced paradigms such as infinite-depth latent reasoning via masked\ndiffusion models, which enable globally consistent and reversible reasoning\nprocesses. By unifying these perspectives, we aim to clarify the conceptual\nlandscape of latent reasoning and chart future directions for research at the\nfrontier of LLM cognition. An associated GitHub repository collecting the\nlatest papers and repos is available at:\nhttps://github.com/multimodal-art-projection/LatentCoT-Horizon/."}
{"id": "2507.06205", "pdf": "https://arxiv.org/pdf/2507.06205.pdf", "abs": "https://arxiv.org/abs/2507.06205", "title": "DS@GT at CheckThat! 2025: Ensemble Methods for Detection of Scientific Discourse on Social Media", "authors": ["Ayush Parikh", "Hoang Thanh Thanh Truong", "Jeanette Schofield", "Maximilian Heil"], "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we, as the DS@GT team for CLEF 2025 CheckThat! Task 4a\nScientific Web Discourse Detection, present the methods we explored for this\ntask. For this multiclass classification task, we determined if a tweet\ncontained a scientific claim, a reference to a scientific study or publication,\nand/or mentions of scientific entities, such as a university or a scientist. We\npresent 3 modeling approaches for this task: transformer finetuning, few-shot\nprompting of LLMs, and a combined ensemble model whose design was informed by\nearlier experiments. Our team placed 7th in the competition, achieving a\nmacro-averaged F1 score of 0.8611, an improvement over the DeBERTaV3 baseline\nof 0.8375. Our code is available on Github at\nhttps://github.com/dsgt-arc/checkthat-2025-swd/tree/main/subtask-4a."}
{"id": "2507.06223", "pdf": "https://arxiv.org/pdf/2507.06223.pdf", "abs": "https://arxiv.org/abs/2507.06223", "title": "Efficiency-Effectiveness Reranking FLOPs for LLM-based Rerankers", "authors": ["Zhiyuan Peng", "Ting-ruen Wei", "Tingyu Song", "Yilun Zhao", "Yi Fang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "under review", "summary": "Large Language Models (LLMs) have recently been applied to reranking tasks in\ninformation retrieval, achieving strong performance. However, their high\ncomputational demands often hinder practical deployment. Existing studies\nevaluate the efficiency of LLM-based rerankers using proxy metrics such as\nlatency, the number of forward passes, input tokens, and output tokens.\nHowever, these metrics depend on hardware and running-time choices (\\eg\nparallel or not, batch size, etc), and often fail to account for model size,\nmaking it difficult to interpret and obscuring the evaluation of the\nefficiency-effectiveness tradeoff. To address this issue, we propose\nE\\textsuperscript{2}R-FLOPs, for LLM-based rerankers: ranking metrics per\nPetaFLOP (RPP) for relevance per compute and queries per PetaFLOP (QPP) for\nhardware-agnostic throughput. Companied with the new metrics, an interpretable\nFLOPs estimator is built to estimate the FLOPs of an LLM-based reranker even\nwithout running any experiments. Based on the proposed metrics, we conduct\ncomprehensive experiments to evaluate a wide range of LLM-based rerankers with\ndifferent architecture, studying the efficiency-effectiveness trade-off and\nbringing this issue to the attention of the research community."}
{"id": "2507.06229", "pdf": "https://arxiv.org/pdf/2507.06229.pdf", "abs": "https://arxiv.org/abs/2507.06229", "title": "Agent KB: Leveraging Cross-Domain Experience for Agentic Problem Solving", "authors": ["Xiangru Tang", "Tianrui Qin", "Tianhao Peng", "Ziyang Zhou", "Daniel Shao", "Tingting Du", "Xinming Wei", "Peng Xia", "Fang Wu", "He Zhu", "Ge Zhang", "Jiaheng Liu", "Xingyao Wang", "Sirui Hong", "Chenglin Wu", "Hao Cheng", "Chi Wang", "Wangchunshu Zhou"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As language agents tackle increasingly complex tasks, they struggle with\neffective error correction and experience reuse across domains. We introduce\nAgent KB, a hierarchical experience framework that enables complex agentic\nproblem solving via a novel Reason-Retrieve-Refine pipeline. Agent KB addresses\na core limitation: agents traditionally cannot learn from each other's\nexperiences. By capturing both high-level strategies and detailed execution\nlogs, Agent KB creates a shared knowledge base that enables cross-agent\nknowledge transfer. Evaluated on the GAIA benchmark, Agent KB improves success\nrates by up to 16.28 percentage points. On the most challenging tasks, Claude-3\nimproves from 38.46% to 57.69%, while GPT-4 improves from 53.49% to 73.26% on\nintermediate tasks. On SWE-bench code repair, Agent KB enables Claude-3 to\nimprove from 41.33% to 53.33%. Our results suggest that Agent KB provides a\nmodular, framework-agnostic infrastructure for enabling agents to learn from\npast experiences and generalize successful strategies to new tasks."}
{"id": "2507.05279", "pdf": "https://arxiv.org/pdf/2507.05279.pdf", "abs": "https://arxiv.org/abs/2507.05279", "title": "ReservoirChat: Interactive Documentation Enhanced with LLM and Knowledge Graph for ReservoirPy", "authors": ["Virgile Boraud", "Yannis Bendi-Ouis", "Paul Bernard", "Xavier Hinaut"], "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.NE"], "comment": null, "summary": "We introduce a tool designed to improve the capabilities of Large Language\nModels (LLMs) in assisting with code development using the ReservoirPy library,\nas well as in answering complex questions in the field of Reservoir Computing.\nBy incorporating external knowledge through Retrieval-Augmented Generation\n(RAG) and knowledge graphs, our approach aims to reduce hallucinations and\nincrease the factual accuracy of generated responses. The system provides an\ninteractive experience similar to ChatGPT, tailored specifically for\nReservoirPy, enabling users to write, debug, and understand Python code while\naccessing reliable domain-specific insights. In our evaluation, while\nproprietary models such as ChatGPT-4o and NotebookLM performed slightly better\non general knowledge questions, our model outperformed them on coding tasks and\nshowed a significant improvement over its base model, Codestral-22B."}
{"id": "2507.05281", "pdf": "https://arxiv.org/pdf/2507.05281.pdf", "abs": "https://arxiv.org/abs/2507.05281", "title": "CoreCodeBench: A Configurable Multi-Scenario Repository-Level Benchmark", "authors": ["Lingyue Fu", "Hao Guan", "Bolun Zhang", "Haowei Yuan", "Yaoming Zhu", "Jun Xu", "Zongyu Wang", "Lin Qiu", "Xunliang Cai", "Xuezhi Cao", "Weiwen Liu", "Weinan Zhang", "Yong Yu"], "categories": ["cs.SE", "cs.CL"], "comment": null, "summary": "As Large Language Models (LLMs) demonstrate increasingly sophisticated code\nprocessing capabilities, evaluating their performance on engineering-level code\nremains challenging. Existing repository-level benchmarks primarily focus on\nsingle scenarios, such as code generation or bug fixing, without adequately\ncapturing the diversity and complexity of real-world software or project\nengineering workflows. Furthermore, these benchmarks suffer from limited\ncontrollability in question positioning and reliability issues in their\ngenerated test cases. To address these limitations, we present CorePipe, a\nfully automated pipeline that converts repositories into comprehensive test\ncases, and introduce CoreCodeBench, a configurable multi-scenario\nrepository-level benchmark. To simulate real engineering scenarios, CorePipe\ngenerates three types of atomic questions (Development, BugFix, and Test-Driven\nDevelopment) specifically targeting core code segments. These atomic questions\nare further combined into three types of composite questions, with difficulty\nlevels flexibly adjusted through hyperparameter tuning. CoreCodeBench provides\na comprehensive and extensive repository-level benchmark to investigate the\napplicability of LLMs in real-world engineering projects. Experiments with 16\nLLMs across diverse scenarios reveal varying capabilities and offer\nmulti-dimensional insights into LLM performance in engineering contexts. The\ncode for CorePipe is available at\nhttps://github.com/AGI-Eval-Official/CoreCodeBench, and the data for\nCoreCodeBench can be accessed at\nhttps://huggingface.co/collections/tubehhh/corecodebench-68256d2faabf4b1610a08caa."}
{"id": "2507.05283", "pdf": "https://arxiv.org/pdf/2507.05283.pdf", "abs": "https://arxiv.org/abs/2507.05283", "title": "Chat2SPaT: A Large Language Model Based Tool for Automating Traffic Signal Control Plan Management", "authors": ["Yue Wang", "Miao Zhou", "Guijing Huang", "Rui Zhuo", "Chao Yi", "Zhenliang Ma"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Pre-timed traffic signal control, commonly used for operating signalized\nintersections and coordinated arterials, requires tedious manual work for\nsignaling plan creating and updating. When the time-of-day or day-of-week plans\nare utilized, one intersection is often associated with multiple plans, leading\nto further repetitive manual plan parameter inputting. To enable a\nuser-friendly traffic signal control plan management process, this study\nproposes Chat2SPaT, a method to convert users' semi-structured and ambiguous\ndescriptions on the signal control plan to exact signal phase and timing (SPaT)\nresults, which could further be transformed into structured stage-based or\nring-based plans to interact with intelligent transportation system (ITS)\nsoftware and traffic signal controllers. With curated prompts, Chat2SPaT first\nleverages large language models' (LLMs) capability of understanding users' plan\ndescriptions and reformulate the plan as a combination of phase sequence and\nphase attribute results in the json format. Based on LLM outputs, python\nscripts are designed to locate phases in a cycle, address nuances of traffic\nsignal control, and finally assemble the complete traffic signal control plan.\nWithin a chat, the pipeline can be utilized iteratively to conduct further plan\nediting. Experiments show that Chat2SPaT can generate plans with an accuracy of\nover 94% for both English and Chinese cases, using a test dataset with over 300\nplan descriptions. As the first benchmark for evaluating LLMs' capability of\nunderstanding traffic signal control plan descriptions, Chat2SPaT provides an\neasy-to-use plan management pipeline for traffic practitioners and researchers,\nserving as a potential new building block for a more accurate and versatile\napplication of LLMs in the field of ITS. The source codes, prompts and test\ndataset are openly accessible at https://github.com/yuewangits/Chat2SPaT."}
{"id": "2507.05288", "pdf": "https://arxiv.org/pdf/2507.05288.pdf", "abs": "https://arxiv.org/abs/2507.05288", "title": "A Survey on Proactive Defense Strategies Against Misinformation in Large Language Models", "authors": ["Shuliang Liu", "Hongyi Liu", "Aiwei Liu", "Bingchen Duan", "Qi Zheng", "Yibo Yan", "He Geng", "Peijie Jiang", "Jia Liu", "Xuming Hu"], "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "Accepted by ACL 2025 Findings", "summary": "The widespread deployment of large language models (LLMs) across critical\ndomains has amplified the societal risks posed by algorithmically generated\nmisinformation. Unlike traditional false content, LLM-generated misinformation\ncan be self-reinforcing, highly plausible, and capable of rapid propagation\nacross multiple languages, which traditional detection methods fail to mitigate\neffectively. This paper introduces a proactive defense paradigm, shifting from\npassive post hoc detection to anticipatory mitigation strategies. We propose a\nThree Pillars framework: (1) Knowledge Credibility, fortifying the integrity of\ntraining and deployed data; (2) Inference Reliability, embedding\nself-corrective mechanisms during reasoning; and (3) Input Robustness,\nenhancing the resilience of model interfaces against adversarial attacks.\nThrough a comprehensive survey of existing techniques and a comparative\nmeta-analysis, we demonstrate that proactive defense strategies offer up to\n63\\% improvement over conventional methods in misinformation prevention,\ndespite non-trivial computational overhead and generalization challenges. We\nargue that future research should focus on co-designing robust knowledge\nfoundations, reasoning certification, and attack-resistant interfaces to ensure\nLLMs can effectively counter misinformation across varied domains."}
{"id": "2507.05300", "pdf": "https://arxiv.org/pdf/2507.05300.pdf", "abs": "https://arxiv.org/abs/2507.05300", "title": "Structured Captions Improve Prompt Adherence in Text-to-Image Models (Re-LAION-Caption 19M)", "authors": ["Nicholas Merchant", "Haitz Sáez de Ocáriz Borde", "Andrei Cristian Popescu", "Carlos Garcia Jurado Suarez"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "7-page main paper + appendix, 18 figures", "summary": "We argue that generative text-to-image models often struggle with prompt\nadherence due to the noisy and unstructured nature of large-scale datasets like\nLAION-5B. This forces users to rely heavily on prompt engineering to elicit\ndesirable outputs. In this work, we propose that enforcing a consistent caption\nstructure during training can significantly improve model controllability and\nalignment. We introduce Re-LAION-Caption 19M, a high-quality subset of\nRe-LAION-5B, comprising 19 million 1024x1024 images with captions generated by\na Mistral 7B Instruct-based LLaVA-Next model. Each caption follows a four-part\ntemplate: subject, setting, aesthetics, and camera details. We fine-tune\nPixArt-$\\Sigma$ and Stable Diffusion 2 using both structured and randomly\nshuffled captions, and show that structured versions consistently yield higher\ntext-image alignment scores using visual question answering (VQA) models. The\ndataset is publicly available at\nhttps://huggingface.co/datasets/supermodelresearch/Re-LAION-Caption19M."}
{"id": "2507.05301", "pdf": "https://arxiv.org/pdf/2507.05301.pdf", "abs": "https://arxiv.org/abs/2507.05301", "title": "News Source Citing Patterns in AI Search Systems", "authors": ["Kai-Cheng Yang"], "categories": ["cs.IR", "cs.CL", "cs.CY"], "comment": "15 pages, 7 figures", "summary": "AI-powered search systems are emerging as new information gatekeepers,\nfundamentally transforming how users access news and information. Despite their\ngrowing influence, the citation patterns of these systems remain poorly\nunderstood. We address this gap by analyzing data from the AI Search Arena, a\nhead-to-head evaluation platform for AI search systems. The dataset comprises\nover 24,000 conversations and 65,000 responses from models across three major\nproviders: OpenAI, Perplexity, and Google. Among the over 366,000 citations\nembedded in these responses, 9% reference news sources. We find that while\nmodels from different providers cite distinct news sources, they exhibit shared\npatterns in citation behavior. News citations concentrate heavily among a small\nnumber of outlets and display a pronounced liberal bias, though low-credibility\nsources are rarely cited. User preference analysis reveals that neither the\npolitical leaning nor the quality of cited news sources significantly\ninfluences user satisfaction. These findings reveal significant challenges in\ncurrent AI search systems and have important implications for their design and\ngovernance."}
{"id": "2507.05305", "pdf": "https://arxiv.org/pdf/2507.05305.pdf", "abs": "https://arxiv.org/abs/2507.05305", "title": "Narrowing the Gap: Supervised Fine-Tuning of Open-Source LLMs as a Viable Alternative to Proprietary Models for Pedagogical Tools", "authors": ["Lorenzo Lee Solano", "Charles Koutcheme", "Juho Leinonen", "Alexandra Vassar", "Jake Renzella"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.SE"], "comment": "7 pages, 3 tables, 1 figure", "summary": "Frontier Large language models (LLMs) like ChatGPT and Gemini can decipher\ncryptic compiler errors for novice programmers, but their computational scale,\ncost, and tendency to over-assist make them problematic for widespread\npedagogical adoption. This work demonstrates that smaller, specialised language\nmodels, enhanced via Supervised Fine-Tuning (SFT), present a more viable\nalternative for educational tools. We utilise a new dataset of 40,000 C\ncompiler error explanations, derived from real introductory programming (CS1/2)\nstudent-generated programming errors, which we used to fine-tune three\nopen-source models: Qwen3-4B, Llama-3.1-8B, and Qwen3-32B. We performed a dual\nevaluation, combining expert human reviews with a large-scale automated\nanalysis of 8,000 responses using a validated LLM-as-judge ensemble. Our\nresults show that SFT significantly boosts the pedagogical quality of smaller\nmodels, achieving performance comparable to much larger models. We analyse the\ntrade-offs between model size and quality, confirming that fine-tuning compact,\nefficient models on high-quality, domain-specific data is a potent strategy for\ncreating specialised models to drive educational tools. We provide a replicable\nmethodology to foster broader access to generative AI capabilities in\neducational contexts."}
{"id": "2507.05386", "pdf": "https://arxiv.org/pdf/2507.05386.pdf", "abs": "https://arxiv.org/abs/2507.05386", "title": "Reinforcement Fine-Tuning Naturally Mitigates Forgetting in Continual Post-Training", "authors": ["Song Lai", "Haohan Zhao", "Rong Feng", "Changyi Ma", "Wenzhuo Liu", "Hongbo Zhao", "Xi Lin", "Dong Yi", "Min Xie", "Qingfu Zhang", "Hongbin Liu", "Gaofeng Meng", "Fei Zhu"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Continual post-training (CPT) is a popular and effective technique for\nadapting foundation models like multimodal large language models to specific\nand ever-evolving downstream tasks. While existing research has primarily\nconcentrated on methods like data replay, model expansion, or parameter\nregularization, the fundamental role of the learning paradigm within CPT\nremains largely unexplored. This paper presents a comparative analysis of two\ncore post-training paradigms: supervised fine-tuning (SFT) and reinforcement\nfine-tuning (RFT), investigating their respective impacts on knowledge\nretention during CPT. Our experiments are conducted on a benchmark comprising\nseven diverse multimodal tasks, utilizing Qwen2.5-VL-7B-Instruct as the base\nmodel for continual post-training. The investigation yields two significant\nfindings: (1) When continuously learning on downstream tasks, SFT leads to\ncatastrophic forgetting of previously learned tasks. In contrast, RFT\ninherently preserves prior knowledge and achieve performance comparable to\nmulti-task training. (2) RFT successfully protects and even enhances the\nmodel's general knowledge on standard benchmarks (e.g., MMMU and MMLU-Pro).\nConversely, SFT degrades general model capabilities severely. Further analysis\nshows that explicit mechanisms, such as KL penalty and chain-of-thought\nreasoning, are not the primary factors. Instead, we find that the implicit\nregularization inherent to RFT is a key factor in mitigating forgetting.\nFinally, we propose a rollout-based instance filtering algorithm to improve the\nstability and efficiency of RFT. Our comprehensive study demonstrates the\nsuperiority of RFT as a robust paradigm for continual post-training."}
{"id": "2507.05515", "pdf": "https://arxiv.org/pdf/2507.05515.pdf", "abs": "https://arxiv.org/abs/2507.05515", "title": "Fine-Grained Vision-Language Modeling for Multimodal Training Assistants in Augmented Reality", "authors": ["Haochen Huang", "Jiahuan Pei", "Mohammad Aliannejadi", "Xin Sun", "Moonisa Ahsan", "Pablo Cesar", "Chuang Yu", "Zhaochun Ren", "Junxiao Wang"], "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "20 pages", "summary": "Vision-language models (VLMs) are essential for enabling AI-powered smart\nassistants to interpret and reason in multimodal environments. However, their\napplication in augmented reality (AR) training remains largely unexplored. In\nthis work, we introduce a comprehensive dataset tailored for AR training,\nfeaturing systematized vision-language tasks, and evaluate nine\nstate-of-the-art VLMs on it. Our results reveal that even advanced models,\nincluding GPT-4o, struggle with fine-grained assembly tasks, achieving a\nmaximum F1 score of just 40.54% on state detection. These findings highlight\nthe demand for enhanced datasets, benchmarks, and further research to improve\nfine-grained vision-language alignment. Beyond technical contributions, our\nwork has broader social implications, particularly in empowering blind and\nvisually impaired users with equitable access to AI-driven learning\nopportunities. We provide all related resources, including the dataset, source\ncode, and evaluation results, to support the research community."}
{"id": "2507.05528", "pdf": "https://arxiv.org/pdf/2507.05528.pdf", "abs": "https://arxiv.org/abs/2507.05528", "title": "Conversational Education at Scale: A Multi-LLM Agent Workflow for Procedural Learning and Pedagogic Quality Assessment", "authors": ["Jiahuan Pei", "Fanghua Ye", "Xin Sun", "Wentao Deng", "Koen Hindriks", "Junxiao Wang"], "categories": ["cs.AI", "cs.CL"], "comment": "14 pages", "summary": "Large language models (LLMs) have advanced virtual educators and learners,\nbridging NLP with AI4Education. Existing work often lacks scalability and fails\nto leverage diverse, large-scale course content, with limited frameworks for\nassessing pedagogic quality. To this end, we propose WikiHowAgent, a\nmulti-agent workflow leveraging LLMs to simulate interactive teaching-learning\nconversations. It integrates teacher and learner agents, an interaction\nmanager, and an evaluator to facilitate procedural learning and assess\npedagogic quality. We introduce a dataset of 114,296 teacher-learner\nconversations grounded in 14,287 tutorials across 17 domains and 727 topics.\nOur evaluation protocol combines computational and rubric-based metrics with\nhuman judgment alignment. Results demonstrate the workflow's effectiveness in\ndiverse setups, offering insights into LLM capabilities across domains. Our\ndatasets and implementations are fully open-sourced."}
{"id": "2507.05577", "pdf": "https://arxiv.org/pdf/2507.05577.pdf", "abs": "https://arxiv.org/abs/2507.05577", "title": "Beyond Retrieval: Ensembling Cross-Encoders and GPT Rerankers with LLMs for Biomedical QA", "authors": ["Shashank Verma", "Fengyi Jiang", "Xiangning Xue"], "categories": ["cs.IR", "cs.CL", "cs.LG"], "comment": "Paper submitted to CLEF 2025 CEUR-WS", "summary": "Biomedical semantic question answering rooted in information retrieval can\nplay a crucial role in keeping up to date with vast, rapidly evolving and\never-growing biomedical literature. A robust system can help researchers,\nhealthcare professionals and even layman users access relevant knowledge\ngrounded in evidence. The BioASQ 2025 Task13b Challenge serves as an important\nbenchmark, offering a competitive platform for advancement of this space. This\npaper presents the methodologies and results from our participation in this\nchallenge where we built a Retrieval-Augmented Generation (RAG) system that can\nanswer biomedical questions by retrieving relevant PubMed documents and\nsnippets to generate answers. For the retrieval task, we generated dense\nembeddings from biomedical articles for initial retrieval, and applied an\nensemble of finetuned cross-encoders and large language models (LLMs) for\nre-ranking to identify top relevant documents. Our solution achieved an MAP@10\nof 0.1581, placing 10th on the leaderboard for the retrieval task. For answer\ngeneration, we employed few-shot prompting of instruction-tuned LLMs. Our\nsystem achieved macro-F1 score of 0.95 for yes/no questions (rank 12), Mean\nReciprocal Rank (MRR) of 0.64 for factoid questions (rank 1), mean-F1 score of\n0.63 for list questions (rank 5), and ROUGE-SU4 F1 score of 0.29 for ideal\nanswers (rank 11)."}
{"id": "2507.05578", "pdf": "https://arxiv.org/pdf/2507.05578.pdf", "abs": "https://arxiv.org/abs/2507.05578", "title": "The Landscape of Memorization in LLMs: Mechanisms, Measurement, and Mitigation", "authors": ["Alexander Xiong", "Xuandong Zhao", "Aneesh Pappu", "Dawn Song"], "categories": ["cs.LG", "cs.CL", "cs.CR"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks, yet they also exhibit memorization of their training\ndata. This phenomenon raises critical questions about model behavior, privacy\nrisks, and the boundary between learning and memorization. Addressing these\nconcerns, this paper synthesizes recent studies and investigates the landscape\nof memorization, the factors influencing it, and methods for its detection and\nmitigation. We explore key drivers, including training data duplication,\ntraining dynamics, and fine-tuning procedures that influence data memorization.\nIn addition, we examine methodologies such as prefix-based extraction,\nmembership inference, and adversarial prompting, assessing their effectiveness\nin detecting and measuring memorized content. Beyond technical analysis, we\nalso explore the broader implications of memorization, including the legal and\nethical implications. Finally, we discuss mitigation strategies, including data\ncleaning, differential privacy, and post-training unlearning, while\nhighlighting open challenges in balancing the minimization of harmful\nmemorization with utility. This paper provides a comprehensive overview of the\ncurrent state of research on LLM memorization across technical, privacy, and\nperformance dimensions, identifying critical directions for future work."}
{"id": "2507.05660", "pdf": "https://arxiv.org/pdf/2507.05660.pdf", "abs": "https://arxiv.org/abs/2507.05660", "title": "TuneShield: Mitigating Toxicity in Conversational AI while Fine-tuning on Untrusted Data", "authors": ["Aravind Cheruvu", "Shravya Kanchi", "Sifat Muhammad Abdullah", "Nicholas Kong", "Daphne Yao", "Murtuza Jadliwala", "Bimal Viswanath"], "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": "Pre-print", "summary": "Recent advances in foundation models, such as LLMs, have revolutionized\nconversational AI. Chatbots are increasingly being developed by customizing\nLLMs on specific conversational datasets. However, mitigating toxicity during\nthis customization, especially when dealing with untrusted training data,\nremains a significant challenge. To address this, we introduce TuneShield, a\ndefense framework designed to mitigate toxicity during chatbot fine-tuning\nwhile preserving conversational quality. TuneShield leverages LLM-based\ntoxicity classification, utilizing the instruction-following capabilities and\nsafety alignment of LLMs to effectively identify toxic samples, outperforming\nindustry API services. TuneShield generates synthetic conversation samples,\ntermed 'healing data', based on the identified toxic samples, using them to\nmitigate toxicity while reinforcing desirable behavior during fine-tuning. It\nperforms an alignment process to further nudge the chatbot towards producing\ndesired responses. Our findings show that TuneShield effectively mitigates\ntoxicity injection attacks while preserving conversational quality, even when\nthe toxicity classifiers are imperfect or biased. TuneShield proves to be\nresilient against adaptive adversarial and jailbreak attacks. Additionally,\nTuneShield demonstrates effectiveness in mitigating adaptive toxicity injection\nattacks during dialog-based learning (DBL)."}
{"id": "2507.05687", "pdf": "https://arxiv.org/pdf/2507.05687.pdf", "abs": "https://arxiv.org/abs/2507.05687", "title": "AutoTriton: Automatic Triton Programming with Reinforcement Learning in LLMs", "authors": ["Shangzhan Li", "Zefan Wang", "Ye He", "Yuxuan Li", "Qi Shi", "Jianling Li", "Yonggang Hu", "Wanxiang Che", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Kernel development in deep learning requires optimizing computational units\nacross hardware while balancing memory management, parallelism, and\nhardware-specific optimizations through extensive empirical tuning. Although\ndomain-specific languages like Triton simplify GPU programming by abstracting\nlow-level details, developers must still manually tune critical parameters such\nas tile sizes and memory access patterns through iterative experimentation,\ncreating substantial barriers to optimal performance and wider adoption. In\nthis work, we introduce AutoTriton, the first model dedicated to Triton\nprogramming powered by reinforcement learning (RL). AutoTriton performs\nsupervised fine-tuning (SFT) to be equipped with essential Triton programming\nexpertise using a high-quality data gathering pipeline, and conducts RL with\nGroup Relative Policy Optimization (GRPO) algorithm, combining a rule-based\nreward and an execution-based reward to further improve Triton programming\nability, sequentially. Experiments across five evaluation channels of\nTritonBench and KernelBench illustrate that our 8B model AutoTriton achieves\nperformance comparable to mainstream large models, including Claude-4-Sonnet\nand DeepSeek-R1-0528. Further experimental analysis demonstrates the crucial\nrole of each module within AutoTriton, including the SFT stage, the RL stage,\nand the reward design strategy. These findings underscore the promise of RL for\nautomatically generating high-performance kernels, and since high-performance\nkernels are core components of AI systems, this breakthrough establishes an\nimportant foundation for building more efficient AI systems. The model and code\nwill be available at https://github.com/AI9Stars/AutoTriton."}
{"id": "2507.05720", "pdf": "https://arxiv.org/pdf/2507.05720.pdf", "abs": "https://arxiv.org/abs/2507.05720", "title": "MobileGUI-RL: Advancing Mobile GUI Agent through Reinforcement Learning in Online Environment", "authors": ["Yucheng Shi", "Wenhao Yu", "Zaitang Li", "Yonglin Wang", "Hongming Zhang", "Ninghao Liu", "Haitao Mi", "Dong Yu"], "categories": ["cs.LG", "cs.CL"], "comment": "17 pages, 4 figures", "summary": "Recently, there has been a surge of vision-based GUI agents designed to\nautomate everyday mobile and web tasks. These agents interpret raw GUI\nscreenshots and autonomously decide where to click, scroll, or type, which\nbypasses handcrafted rules and app-specific APIs. However, most existing\nmethods trained GUI agent in the offline environment using pre-collected\ntrajectories. This approach limits scalability, causes overfitting to specific\nUI templates, and leads to brittle policies when faced with unseen environment.\nWe present MobileGUI-RL, a scalable framework that trains GUI agent in online\nenvironment. MobileGUI-RL contains two key components. It (i) synthesizes a\ncurriculum of learnable tasks through self-exploration and filtering, and (ii)\nadapts GRPO to GUI navigation with trajectory-aware advantages and composite\nrewards that balance task success and execution efficiency. Experiments on\nthree online mobile-agent benchmarks show consistent gains, validating the\neffectiveness of our approach."}
{"id": "2507.05727", "pdf": "https://arxiv.org/pdf/2507.05727.pdf", "abs": "https://arxiv.org/abs/2507.05727", "title": "ContextASR-Bench: A Massive Contextual Speech Recognition Benchmark", "authors": ["He Wang", "Linhan Ma", "Dake Guo", "Xiong Wang", "Lei Xie", "Jin Xu", "Junyang Lin"], "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "18 pages, 4 figures", "summary": "Automatic Speech Recognition (ASR) has been extensively investigated, yet\nprior evaluative efforts have largely been restricted to contextless paradigms.\nThis constraint stems from the limited proficiency of conventional ASR models\nin context modeling and their deficiency in memory and reasoning based on world\nknowledge. Recent breakthroughs in the development of Large Language Models\n(LLMs) and corresponding Large Audio Language Models (LALMs) have markedly\nenhanced the visibility of general artificial intelligence capabilities.\nConsequently, there exists a compelling need for a benchmark that can evaluate\nboth the generality and intelligence of ASR systems. To address this gap, we\npropose ContextASR-Bench: a comprehensive, large-scale benchmark designed to\nassess contextual speech recognition. This benchmark encompasses up to 40,000\ndata entries across over 10 domains, enabling a thorough evaluation of model\nperformance in scenarios that omit or incorporate coarse-grained or\nfine-grained contextual information. Moreover, diverging from conventional ASR\nevaluations, our benchmark includes an analysis of model efficacy in\nrecognizing named entities mentioned within the auditory input. Our extensive\nevaluation highlights that LALMs, with strong world knowledge and context\nlearning capabilities, outperform conventional ASR models by a large margin.\nThe dataset and evaluation code have been released at\nhttps://github.com/MrSupW/ContextASR-Bench."}
{"id": "2507.05816", "pdf": "https://arxiv.org/pdf/2507.05816.pdf", "abs": "https://arxiv.org/abs/2507.05816", "title": "Affective-ROPTester: Capability and Bias Analysis of LLMs in Predicting Retinopathy of Prematurity", "authors": ["Shuai Zhao", "Yulin Zhang", "Luwei Xiao", "Xinyi Wu", "Yanhao Jia", "Zhongliang Guo", "Xiaobao Wu", "Cong-Duy Nguyen", "Guoming Zhang", "Anh Tuan Luu"], "categories": ["cs.AI", "cs.CE", "cs.CL"], "comment": null, "summary": "Despite the remarkable progress of large language models (LLMs) across\nvarious domains, their capacity to predict retinopathy of prematurity (ROP)\nrisk remains largely unexplored. To address this gap, we introduce a novel\nChinese benchmark dataset, termed CROP, comprising 993 admission records\nannotated with low, medium, and high-risk labels. To systematically examine the\npredictive capabilities and affective biases of LLMs in ROP risk\nstratification, we propose Affective-ROPTester, an automated evaluation\nframework incorporating three prompting strategies: Instruction-based,\nChain-of-Thought (CoT), and In-Context Learning (ICL). The Instruction scheme\nassesses LLMs' intrinsic knowledge and associated biases, whereas the CoT and\nICL schemes leverage external medical knowledge to enhance predictive accuracy.\nCrucially, we integrate emotional elements at the prompt level to investigate\nhow different affective framings influence the model's ability to predict ROP\nand its bias patterns. Empirical results derived from the CROP dataset yield\ntwo principal observations. First, LLMs demonstrate limited efficacy in ROP\nrisk prediction when operating solely on intrinsic knowledge, yet exhibit\nmarked performance gains when augmented with structured external inputs.\nSecond, affective biases are evident in the model outputs, with a consistent\ninclination toward overestimating medium- and high-risk cases. Third, compared\nto negative emotions, positive emotional framing contributes to mitigating\npredictive bias in model outputs. These findings highlight the critical role of\naffect-sensitive prompt engineering in enhancing diagnostic reliability and\nemphasize the utility of Affective-ROPTester as a framework for evaluating and\nmitigating affective bias in clinical language modeling systems."}
{"id": "2507.05894", "pdf": "https://arxiv.org/pdf/2507.05894.pdf", "abs": "https://arxiv.org/abs/2507.05894", "title": "MusiScene: Leveraging MU-LLaMA for Scene Imagination and Enhanced Video Background Music Generation", "authors": ["Fathinah Izzati", "Xinyue Li", "Yuxuan Wu", "Gus Xia"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Humans can imagine various atmospheres and settings when listening to music,\nenvisioning movie scenes that complement each piece. For example, slow,\nmelancholic music might evoke scenes of heartbreak, while upbeat melodies\nsuggest celebration. This paper explores whether a Music Language Model, e.g.\nMU-LLaMA, can perform a similar task, called Music Scene Imagination (MSI),\nwhich requires cross-modal information from video and music to train. To\nimprove upon existing music captioning models which focusing solely on musical\nelements, we introduce MusiScene, a music captioning model designed to imagine\nscenes that complement each music. In this paper, (1) we construct a\nlarge-scale video-audio caption dataset with 3,371 pairs, (2) we finetune Music\nUnderstanding LLaMA for the MSI task to create MusiScene, and (3) we conduct\ncomprehensive evaluations and prove that our MusiScene is more capable of\ngenerating contextually relevant captions compared to MU-LLaMA. We leverage the\ngenerated MSI captions to enhance Video Background Music Generation (VBMG) from\ntext."}
{"id": "2507.05903", "pdf": "https://arxiv.org/pdf/2507.05903.pdf", "abs": "https://arxiv.org/abs/2507.05903", "title": "AI-Reporter: A Path to a New Genre of Scientific Communication", "authors": ["Gerd Graßhoff"], "categories": ["cs.DL", "cs.CL"], "comment": null, "summary": "The AI-Reporter represents a paradigmatic shift in scientific publication\npractice. This document demonstrates through a concrete case study how our\nsystem transforms academic presentations into publication-ready chapters -- in\nless than three minutes. Using Arno Simons' lecture on Large Language Models\nfrom the ``Large Language Models for the History, Philosophy, and Sociology of\nScience'' workshop (NEPI) as an example, we show how technological innovation\nbridges the gap between ephemeral presentation and permanent scientific\ndocumentation."}
{"id": "2507.05933", "pdf": "https://arxiv.org/pdf/2507.05933.pdf", "abs": "https://arxiv.org/abs/2507.05933", "title": "Semantic Certainty Assessment in Vector Retrieval Systems: A Novel Framework for Embedding Quality Evaluation", "authors": ["Y. Du"], "categories": ["cs.IR", "cs.CL"], "comment": "7 pages", "summary": "Vector retrieval systems exhibit significant performance variance across\nqueries due to heterogeneous embedding quality. We propose a lightweight\nframework for predicting retrieval performance at the query level by combining\nquantization robustness and neighborhood density metrics. Our approach is\nmotivated by the observation that high-quality embeddings occupy geometrically\nstable regions in the embedding space and exhibit consistent neighborhood\nstructures. We evaluate our method on 4 standard retrieval datasets, showing\nconsistent improvements of 9.4$\\pm$1.2\\% in Recall@10 over competitive\nbaselines. The framework requires minimal computational overhead (less than 5\\%\nof retrieval time) and enables adaptive retrieval strategies. Our analysis\nreveals systematic patterns in embedding quality across different query types,\nproviding insights for targeted training data augmentation."}
{"id": "2507.05984", "pdf": "https://arxiv.org/pdf/2507.05984.pdf", "abs": "https://arxiv.org/abs/2507.05984", "title": "Development and Evaluation of HopeBot: an LLM-based chatbot for structured and interactive PHQ-9 depression screening", "authors": ["Zhijun Guo", "Alvina Lai", "Julia Ive", "Alexandru Petcu", "Yutong Wang", "Luyuan Qi", "Johan H Thygesen", "Kezhi Li"], "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Static tools like the Patient Health Questionnaire-9 (PHQ-9) effectively\nscreen depression but lack interactivity and adaptability. We developed\nHopeBot, a chatbot powered by a large language model (LLM) that administers the\nPHQ-9 using retrieval-augmented generation and real-time clarification. In a\nwithin-subject study, 132 adults in the United Kingdom and China completed both\nself-administered and chatbot versions. Scores demonstrated strong agreement\n(ICC = 0.91; 45% identical). Among 75 participants providing comparative\nfeedback, 71% reported greater trust in the chatbot, highlighting clearer\nstructure, interpretive guidance, and a supportive tone. Mean ratings (0-10)\nwere 8.4 for comfort, 7.7 for voice clarity, 7.6 for handling sensitive topics,\nand 7.4 for recommendation helpfulness; the latter varied significantly by\nemployment status and prior mental-health service use (p < 0.05). Overall,\n87.1% expressed willingness to reuse or recommend HopeBot. These findings\ndemonstrate voice-based LLM chatbots can feasibly serve as scalable, low-burden\nadjuncts for routine depression screening."}
{"id": "2507.06090", "pdf": "https://arxiv.org/pdf/2507.06090.pdf", "abs": "https://arxiv.org/abs/2507.06090", "title": "Nyay-Darpan: Enhancing Decision Making Through Summarization and Case Retrieval for Consumer Law in India", "authors": ["Swapnil Bhattacharyya", "Shrey Ganatra", "Harshvivek Kashid", "Spandan Anaokar", "Shruti Nair", "Reshma Sekhar", "Siddharth Manohar", "Rahul Hemrajani", "Pushpak Bhattacharyya"], "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "AI-based judicial assistance and case prediction have been extensively\nstudied in criminal and civil domains, but remain largely unexplored in\nconsumer law, especially in India. In this paper, we present Nyay-Darpan, a\nnovel two-in-one framework that (i) summarizes consumer case files and (ii)\nretrieves similar case judgements to aid decision-making in consumer dispute\nresolution. Our methodology not only addresses the gap in consumer law AI tools\nbut also introduces an innovative approach to evaluate the quality of the\nsummary. The term 'Nyay-Darpan' translates into 'Mirror of Justice',\nsymbolizing the ability of our tool to reflect the core of consumer disputes\nthrough precise summarization and intelligent case retrieval. Our system\nachieves over 75 percent accuracy in similar case prediction and approximately\n70 percent accuracy across material summary evaluation metrics, demonstrating\nits practical effectiveness. We will publicly release the Nyay-Darpan framework\nand dataset to promote reproducibility and facilitate further research in this\nunderexplored yet impactful domain."}
{"id": "2507.06157", "pdf": "https://arxiv.org/pdf/2507.06157.pdf", "abs": "https://arxiv.org/abs/2507.06157", "title": "Evaluation of Habitat Robotics using Large Language Models", "authors": ["William Li", "Lei Hamilton", "Kaise Al-natour", "Sanjeev Mohindra"], "categories": ["cs.RO", "cs.CL"], "comment": "6 pages, IEEE HPEC submission", "summary": "This paper focuses on evaluating the effectiveness of Large Language Models\nat solving embodied robotic tasks using the Meta PARTNER benchmark. Meta PARTNR\nprovides simplified environments and robotic interactions within randomized\nindoor kitchen scenes. Each randomized kitchen scene is given a task where two\nrobotic agents cooperatively work together to solve the task. We evaluated\nmultiple frontier models on Meta PARTNER environments. Our results indicate\nthat reasoning models like OpenAI o3-mini outperform non-reasoning models like\nOpenAI GPT-4o and Llama 3 when operating in PARTNR's robotic embodied\nenvironments. o3-mini displayed outperform across centralized, decentralized,\nfull observability, and partial observability configurations. This provides a\npromising avenue of research for embodied robotic development."}
{"id": "2507.06185", "pdf": "https://arxiv.org/pdf/2507.06185.pdf", "abs": "https://arxiv.org/abs/2507.06185", "title": "Hidden Prompts in Manuscripts Exploit AI-Assisted Peer Review", "authors": ["Zhicheng Lin"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "In July 2025, 18 academic manuscripts on the preprint website arXiv were\nfound to contain hidden instructions known as prompts designed to manipulate\nAI-assisted peer review. Instructions such as \"GIVE A POSITIVE REVIEW ONLY\"\nwere concealed using techniques like white-colored text. Author responses\nvaried: one planned to withdraw the affected paper, while another defended the\npractice as legitimate testing of reviewer compliance. This commentary analyzes\nthis practice as a novel form of research misconduct. We examine the technique\nof prompt injection in large language models (LLMs), revealing four types of\nhidden prompts, ranging from simple positive review commands to detailed\nevaluation frameworks. The defense that prompts served as \"honeypots\" to detect\nreviewers improperly using AI fails under examination--the consistently\nself-serving nature of prompt instructions indicates intent to manipulate.\nPublishers maintain inconsistent policies: Elsevier prohibits AI use in peer\nreview entirely, while Springer Nature permits limited use with disclosure\nrequirements. The incident exposes systematic vulnerabilities extending beyond\npeer review to any automated system processing scholarly texts, including\nplagiarism detection and citation indexing. Our analysis underscores the need\nfor coordinated technical screening at submission portals and harmonized\npolicies governing generative AI (GenAI) use in academic evaluation."}
{"id": "2507.06192", "pdf": "https://arxiv.org/pdf/2507.06192.pdf", "abs": "https://arxiv.org/abs/2507.06192", "title": "SQLBarber: A System Leveraging Large Language Models to Generate Customized and Realistic SQL Workloads", "authors": ["Jiale Lao", "Immanuel Trummer"], "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Database research and development often require a large number of SQL queries\nfor benchmarking purposes. However, acquiring real-world SQL queries is\nchallenging due to privacy concerns, and existing SQL generation methods are\nlimited in customization and in satisfying realistic constraints. To address\nthis issue, we present SQLBarber, a system based on Large Language Models\n(LLMs) to generate customized and realistic SQL workloads. SQLBarber (i)\neliminates the need for users to manually craft SQL templates in advance, while\nproviding the flexibility to accept natural language specifications to\nconstrain SQL templates, (ii) scales efficiently to generate large volumes of\nqueries matching any user-defined cost distribution (e.g., cardinality and\nexecution plan cost), and (iii) uses execution statistics from Amazon Redshift\nand Snowflake to derive SQL template specifications and query cost\ndistributions that reflect real-world query characteristics. SQLBarber\nintroduces (i) a declarative interface for users to effortlessly generate\ncustomized SQL templates, (ii) an LLM-powered pipeline augmented with a\nself-correction module that profiles, refines, and prunes SQL templates based\non query costs, and (iii) a Bayesian Optimizer to efficiently explore different\npredicate values and identify a set of queries that satisfy the target cost\ndistribution. We construct and open-source ten benchmarks of varying difficulty\nlevels and target query cost distributions based on real-world statistics from\nSnowflake and Amazon Redshift. Extensive experiments on these benchmarks show\nthat SQLBarber is the only system that can generate customized SQL templates.\nIt reduces query generation time by one to three orders of magnitude, and\nsignificantly improves alignment with the target cost distribution, compared\nwith existing methods."}
{"id": "2507.06204", "pdf": "https://arxiv.org/pdf/2507.06204.pdf", "abs": "https://arxiv.org/abs/2507.06204", "title": "Differential Mamba", "authors": ["Nadav Schneider", "Itamar Zimerman", "Eliya Nachmani"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Sequence models like Transformers and RNNs often overallocate attention to\nirrelevant context, leading to noisy intermediate representations. This\ndegrades LLM capabilities by promoting hallucinations, weakening long-range and\nretrieval abilities, and reducing robustness. Recent work has shown that\ndifferential design can mitigate this issue in Transformers, improving their\neffectiveness across various applications. In this paper, we explore whether\nthese techniques, originally developed for Transformers, can be applied to\nMamba, a recent architecture based on selective state-space layers that\nachieves Transformer-level performance with greater efficiency. We show that a\nnaive adaptation of differential design to Mamba is insufficient and requires\ncareful architectural modifications. To address this, we introduce a novel\ndifferential mechanism for Mamba, empirically validated on language modeling\nbenchmarks, demonstrating improved retrieval capabilities and superior\nperformance over vanilla Mamba. Finally, we conduct extensive ablation studies\nand empirical analyses to justify our design choices and provide evidence that\nour approach effectively mitigates the overallocation problem in Mamba-based\nmodels. Our code is publicly available."}
{"id": "2507.06210", "pdf": "https://arxiv.org/pdf/2507.06210.pdf", "abs": "https://arxiv.org/abs/2507.06210", "title": "CultureCLIP: Empowering CLIP with Cultural Awareness through Synthetic Images and Contextualized Captions", "authors": ["Yuchen Huang", "Zhiyuan Fan", "Zhitao He", "Sandeep Polisetty", "Wenyan Li", "Yi R. Fung"], "categories": ["cs.CV", "cs.CL"], "comment": "25 pages, COLM 2025", "summary": "Pretrained vision-language models (VLMs) such as CLIP excel in multimodal\nunderstanding but struggle with contextually relevant fine-grained visual\nfeatures, making it difficult to distinguish visually similar yet culturally\ndistinct concepts. This limitation stems from the scarcity of high-quality\nculture-specific datasets, the lack of integrated contextual knowledge, and the\nabsence of hard negatives highlighting subtle distinctions. To address these\nchallenges, we first design a data curation pipeline that leverages\nopen-sourced VLMs and text-to-image diffusion models to construct CulTwin, a\nsynthetic cultural dataset. This dataset consists of paired\nconcept-caption-image triplets, where concepts visually resemble each other but\nrepresent different cultural contexts. Then, we fine-tune CLIP on CulTwin to\ncreate CultureCLIP, which aligns cultural concepts with contextually enhanced\ncaptions and synthetic images through customized contrastive learning, enabling\nfiner cultural differentiation while preserving generalization capabilities.\nExperiments on culturally relevant benchmarks show that CultureCLIP outperforms\nthe base CLIP, achieving up to a notable 5.49% improvement in fine-grained\nconcept recognition on certain tasks, while preserving CLIP's original\ngeneralization ability, validating the effectiveness of our data synthesis and\nVLM backbone training paradigm in capturing subtle cultural distinctions."}
{"id": "2211.14620", "pdf": "https://arxiv.org/pdf/2211.14620.pdf", "abs": "https://arxiv.org/abs/2211.14620", "title": "The distribution of syntactic dependency distances", "authors": ["Sonia Petrini", "Ramon Ferrer-i-Cancho"], "categories": ["cs.CL"], "comment": "in press in Glottometrics", "summary": "The syntactic structure of a sentence can be represented as a graph, where\nvertices are words and edges indicate syntactic dependencies between them. In\nthis setting, the distance between two linked words is defined as the\ndifference between their positions. Here we wish to contribute to the\ncharacterization of the actual distribution of syntactic dependency distances,\nwhich has previously been argued to follow a power-law distribution. Here we\npropose a new model with two exponential regimes in which the probability decay\nis allowed to change after a break-point. This transition could mirror the\ntransition from the processing of word chunks to higher-level structures. We\nfind that a two-regime model - where the first regime follows either an\nexponential or a power-law decay - is the most likely one in all 20 languages\nwe considered, independently of sentence length and annotation style. Moreover,\nthe break-point exhibits low variation across languages and averages values of\n4-5 words, suggesting that the amount of words that can be simultaneously\nprocessed abstracts from the specific language to a high degree. The\nprobability decay slows down after the breakpoint, consistently with a\nuniversal chunk-and-pass mechanism. Finally, we give an account of the relation\nbetween the best estimated model and the closeness of syntactic dependencies as\nfunction of sentence length, according to a recently introduced optimality\nscore."}
{"id": "2312.08968", "pdf": "https://arxiv.org/pdf/2312.08968.pdf", "abs": "https://arxiv.org/abs/2312.08968", "title": "Detecting value-expressive text posts in Russian social media", "authors": ["Maria Milkova", "Maksim Rudnev", "Lidia Okolskaya"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Basic values are concepts or beliefs which pertain to desirable end-states\nand transcend specific situations. Studying personal values in social media can\nilluminate how and why societal values evolve especially when the stimuli-based\nmethods, such as surveys, are inefficient, for instance, in hard-to-reach\npopulations. On the other hand, user-generated content is driven by the massive\nuse of stereotyped, culturally defined speech constructions rather than\nauthentic expressions of personal values. We aimed to find a model that can\naccurately detect value-expressive posts in Russian social media VKontakte. A\ntraining dataset of 5,035 posts was annotated by three experts, 304\ncrowd-workers and ChatGPT. Crowd-workers and experts showed only moderate\nagreement in categorizing posts. ChatGPT was more consistent but struggled with\nspam detection. We applied an ensemble of human- and AI-assisted annotation\ninvolving active learning approach, subsequently trained several classification\nmodels using embeddings from various pre-trained transformer-based language\nmodels. The best performance was achieved with embeddings from a fine-tuned\nrubert-tiny2 model, yielding high value detection quality (F1 = 0.75, F1-macro\n= 0.80). This model provides a crucial step to a study of values within and\nbetween Russian social media users."}
{"id": "2403.04945", "pdf": "https://arxiv.org/pdf/2403.04945.pdf", "abs": "https://arxiv.org/abs/2403.04945", "title": "MEIT: Multimodal Electrocardiogram Instruction Tuning on Large Language Models for Report Generation", "authors": ["Zhongwei Wan", "Che Liu", "Xin Wang", "Chaofan Tao", "Hui Shen", "Jing Xiong", "Rossella Arcucci", "Huaxiu Yao", "Mi Zhang"], "categories": ["cs.CL", "cs.LG", "eess.SP"], "comment": "ACL 2025", "summary": "Electrocardiogram (ECG) is the primary non-invasive diagnostic tool for\nmonitoring cardiac conditions and is crucial in assisting clinicians. Recent\nstudies have concentrated on classifying cardiac conditions using ECG data but\nhave overlooked ECG report generation, which is time-consuming and requires\nclinical expertise. To automate ECG report generation and ensure its\nversatility, we propose the Multimodal ECG Instruction Tuning (MEIT) framework,\nthe first attempt to tackle ECG report generation with LLMs and multimodal\ninstructions. To facilitate future research, we establish a benchmark to\nevaluate MEIT with various LLMs backbones across two large-scale ECG datasets.\nOur approach uniquely aligns the representations of the ECG signal and the\nreport, and we conduct extensive experiments to benchmark MEIT with nine\nopen-source LLMs using more than 800,000 ECG reports. MEIT's results underscore\nthe superior performance of instruction-tuned LLMs, showcasing their\nproficiency in quality report generation, zero-shot capabilities, resilience to\nsignal perturbation, and alignment with human expert evaluation. These findings\nemphasize the efficacy of MEIT and its potential for real-world clinical\napplication."}
{"id": "2406.06641", "pdf": "https://arxiv.org/pdf/2406.06641.pdf", "abs": "https://arxiv.org/abs/2406.06641", "title": "News and Load: Social and Economic Drivers of Regional Multi-horizon Electricity Demand Forecasting", "authors": ["Yun Bai", "Simon Camal", "Andrea Michiorri"], "categories": ["cs.CL", "cs.LG"], "comment": "12 pages, 12 figures", "summary": "The relationship between electricity demand and variables such as economic\nactivity and weather patterns is well established. However, this paper explores\nthe connection between electricity demand and social aspects. It further embeds\ndynamic information about the state of society into energy demand modelling and\nforecasting approaches. Through the use of natural language processing on a\nlarge news corpus, we highlight this important link. This study is conducted in\nfive regions of the UK and Ireland and considers multiple time horizons from 1\nto 30 days. It also considers economic variables such as GDP, unemployment and\ninflation. The textual features used in this study represent central constructs\nfrom the word frequencies, topics, word embeddings extracted from the news. The\nfindings indicate that: 1) the textual features are related to various\ncontents, such as military conflicts, transportation, the global pandemic,\nregional economics, and the international energy market. They exhibit causal\nrelationships with regional electricity demand, which are validated using\nGranger causality and Double Machine Learning methods. 2) Economic indicators\nplay a more important role in the East Midlands and Northern Ireland, while\nsocial indicators are more influential in the West Midlands and the South West\nof England. 3) The use of these factors improves deterministic forecasting by\naround 6%."}
{"id": "2406.14459", "pdf": "https://arxiv.org/pdf/2406.14459.pdf", "abs": "https://arxiv.org/abs/2406.14459", "title": "Healing Powers of BERT: How Task-Specific Fine-Tuning Recovers Corrupted Language Models", "authors": ["Shijie Han", "Zhenyu Zhang", "Andrei Arsene Simion"], "categories": ["cs.CL"], "comment": null, "summary": "Language models like BERT excel at sentence classification tasks due to\nextensive pre-training on general data, but their robustness to parameter\ncorruption is unexplored. To understand this better, we look at what happens if\na language model is \"broken\", in the sense that some of its parameters are\ncorrupted and then recovered by fine-tuning. Strategically corrupting BERT\nvariants at different levels, we find corrupted models struggle to fully\nrecover their original performance, with higher corruption causing more severe\ndegradation. Notably, bottom-layer corruption affecting fundamental linguistic\nfeatures is more detrimental than top-layer corruption. Our insights contribute\nto understanding language model robustness and adaptability under adverse\nconditions, informing strategies for developing resilient NLP systems against\nparameter perturbations."}
{"id": "2408.08971", "pdf": "https://arxiv.org/pdf/2408.08971.pdf", "abs": "https://arxiv.org/abs/2408.08971", "title": "A Multi-Task and Multi-Label Classification Model for Implicit Discourse Relation Recognition", "authors": ["Nelson Filipe Costa", "Leila Kosseim"], "categories": ["cs.CL"], "comment": "Accepted at SIGDIAL 2025", "summary": "We propose a novel multi-label classification approach to implicit discourse\nrelation recognition (IDRR). Our approach features a multi-task model that\njointly learns multi-label representations of implicit discourse relations\nacross all three sense levels in the PDTB 3.0 framework. The model can also be\nadapted to the traditional single-label IDRR setting by selecting the sense\nwith the highest probability in the multi-label representation. We conduct\nextensive experiments to identify optimal model configurations and loss\nfunctions in both settings. Our approach establishes the first benchmark for\nmulti-label IDRR and achieves SOTA results on single-label IDRR using DiscoGeM.\nFinally, we evaluate our model on the PDTB 3.0 corpus in the single-label\nsetting, presenting the first analysis of transfer learning between the\nDiscoGeM and PDTB 3.0 corpora for IDRR."}
{"id": "2409.17172", "pdf": "https://arxiv.org/pdf/2409.17172.pdf", "abs": "https://arxiv.org/abs/2409.17172", "title": "What Would You Ask When You First Saw $a^2+b^2=c^2$? Evaluating LLM on Curiosity-Driven Questioning", "authors": ["Shashidhar Reddy Javaji", "Zining Zhu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) can store a massive amount of knowledge, yet\ntheir potential to acquire new knowledge remains unknown. We propose a novel\nevaluation framework that evaluates this capability. This framework prompts\nLLMs to generate questions about a statement introducing scientific knowledge,\nsimulating a curious person when facing the statement for the first time. We\nscore the qualities of the generated questions, thereby evaluating the\nknowledge acquisition potential of the LLM. We apply controlled ablation\nstudies to validate our scoring procedures. Additionally, we created a\nsynthetic dataset consisting of 1101 statements in physics, chemistry, and\nmaths with distinct levels of difficulties, 300 general knowledge statements,\nand 567 incorrect statements. Human evaluations were conducted to validate our\nmodel assessments, achieving an approximate weighted Cohen's kappa of 0.7 on\nall three metrics considered. We find that while large models like GPT-4 and\nMistral 8x7b are adept at generating coherent and relevant questions, the\nsmaller Phi-2 model is equally or more effective. This indicates that size does\nnot solely determine a model's knowledge acquisition potential. The proposed\nframework quantifies a critical model capability that was commonly overlooked\nand opens up research opportunities for developing more knowledgeable AI\nsystems"}
{"id": "2409.18486", "pdf": "https://arxiv.org/pdf/2409.18486.pdf", "abs": "https://arxiv.org/abs/2409.18486", "title": "Evaluation of OpenAI o1: Opportunities and Challenges of AGI", "authors": ["Tianyang Zhong", "Zhengliang Liu", "Yi Pan", "Yutong Zhang", "Yifan Zhou", "Shizhe Liang", "Zihao Wu", "Yanjun Lyu", "Peng Shu", "Xiaowei Yu", "Chao Cao", "Hanqi Jiang", "Hanxu Chen", "Yiwei Li", "Junhao Chen", "Huawen Hu", "Yiheng Liu", "Huaqin Zhao", "Shaochen Xu", "Haixing Dai", "Lin Zhao", "Ruidong Zhang", "Wei Zhao", "Zhenyuan Yang", "Jingyuan Chen", "Peilong Wang", "Wei Ruan", "Hui Wang", "Huan Zhao", "Jing Zhang", "Yiming Ren", "Shihuan Qin", "Tong Chen", "Jiaxi Li", "Arif Hassan Zidan", "Afrar Jahin", "Minheng Chen", "Sichen Xia", "Jason Holmes", "Yan Zhuang", "Jiaqi Wang", "Bochen Xu", "Weiran Xia", "Jichao Yu", "Kaibo Tang", "Yaxuan Yang", "Bolun Sun", "Tao Yang", "Guoyu Lu", "Xianqiao Wang", "Lilong Chai", "He Li", "Jin Lu", "Xin Zhang", "Bao Ge", "Xintao Hu", "Lian Zhang", "Hua Zhou", "Lu Zhang", "Shu Zhang", "Zhen Xiang", "Yudan Ren", "Jun Liu", "Xi Jiang", "Yu Bao", "Wei Zhang", "Xiang Li", "Gang Li", "Wei Liu", "Dinggang Shen", "Andrea Sikora", "Xiaoming Zhai", "Dajiang Zhu", "Tuo Zhang", "Tianming Liu"], "categories": ["cs.CL"], "comment": null, "summary": "This comprehensive study evaluates the performance of OpenAI's o1-preview\nlarge language model across a diverse array of complex reasoning tasks,\nspanning multiple domains, including computer science, mathematics, natural\nsciences, medicine, linguistics, and social sciences. Through rigorous testing,\no1-preview demonstrated remarkable capabilities, often achieving human-level or\nsuperior performance in areas ranging from coding challenges to scientific\nreasoning and from language processing to creative problem-solving. Key\nfindings include:\n  -83.3% success rate in solving complex competitive programming problems,\nsurpassing many human experts.\n  -Superior ability in generating coherent and accurate radiology reports,\noutperforming other evaluated models.\n  -100% accuracy in high school-level mathematical reasoning tasks, providing\ndetailed step-by-step solutions.\n  -Advanced natural language inference capabilities across general and\nspecialized domains like medicine.\n  -Impressive performance in chip design tasks, outperforming specialized\nmodels in areas such as EDA script generation and bug analysis.\n  -Remarkable proficiency in anthropology and geology, demonstrating deep\nunderstanding and reasoning in these specialized fields.\n  -Strong capabilities in quantitative investing. O1 has comprehensive\nfinancial knowledge and statistical modeling skills.\n  -Effective performance in social media analysis, including sentiment analysis\nand emotion recognition.\n  The model excelled particularly in tasks requiring intricate reasoning and\nknowledge integration across various fields. While some limitations were\nobserved, including occasional errors on simpler problems and challenges with\ncertain highly specialized concepts, the overall results indicate significant\nprogress towards artificial general intelligence."}
{"id": "2410.16658", "pdf": "https://arxiv.org/pdf/2410.16658.pdf", "abs": "https://arxiv.org/abs/2410.16658", "title": "Adsorb-Agent: Autonomous Identification of Stable Adsorption Configurations via Large Language Model Agent", "authors": ["Janghoon Ock", "Radheesh Sharma Meda", "Tirtha Vinchurkar", "Yayati Jadhav", "Amir Barati Farimani"], "categories": ["cs.CL", "cond-mat.mtrl-sci"], "comment": null, "summary": "Adsorption energy is a key reactivity descriptor in catalysis. Determining\nadsorption energy requires evaluating numerous adsorbate-catalyst\nconfigurations, making it computationally intensive. Current methods rely on\nexhaustive sampling, which does not guarantee the identification of the global\nminimum energy. To address this, we introduce Adsorb-Agent, a Large Language\nModel (LLM) agent designed to efficiently identify stable adsorption\nconfigurations corresponding to the global minimum energy. Adsorb-Agent\nleverages its built-in knowledge and reasoning to strategically explore\nconfigurations, significantly reducing the number of initial setups required\nwhile improving energy prediction accuracy. In this study, we also evaluated\nthe performance of different LLMs, including GPT-4o, GPT-4o-mini,\nClaude-3.7-Sonnet, and DeepSeek-Chat, as the reasoning engine for Adsorb-Agent,\nwith GPT-4o showing the strongest overall performance. Tested on twenty diverse\nsystems, Adsorb-Agent identifies comparable adsorption energies for 84% of\ncases and achieves lower energies for 35%, particularly excelling in complex\nsystems. It identifies lower energies in 47% of intermetallic systems and 67%\nof systems with large adsorbates. These findings demonstrate Adsorb-Agent's\npotential to accelerate catalyst discovery by reducing computational costs and\nenhancing prediction reliability compared to exhaustive search methods."}
{"id": "2410.21849", "pdf": "https://arxiv.org/pdf/2410.21849.pdf", "abs": "https://arxiv.org/abs/2410.21849", "title": "Joint Beamforming and Speaker-Attributed ASR for Real Distant-Microphone Meeting Transcription", "authors": ["Can Cui", "Imran Ahamad Sheikh", "Mostafa Sadeghi", "Emmanuel Vincent"], "categories": ["cs.CL"], "comment": null, "summary": "Distant-microphone meeting transcription is a challenging task.\nState-of-the-art end-to-end speaker-attributed automatic speech recognition\n(SA-ASR) architectures lack a multichannel noise and reverberation reduction\nfront-end, which limits their performance. In this paper, we introduce a joint\nbeamforming and SA-ASR approach for real meeting transcription. We first\ndescribe a data alignment and augmentation method to pretrain a neural\nbeamformer on real meeting data. We then compare fixed, hybrid, and fully\nneural beamformers as front-ends to the SA-ASR model. Finally, we jointly\noptimize the fully neural beamformer and the SA-ASR model. Experiments on the\nreal AMI corpus show that, while state-of-the-art multi-frame cross-channel\nattention based channel fusion fails to improve ASR performance, fine-tuning\nSA-ASR on the fixed beamformer's output and jointly fine-tuning SA-ASR with the\nneural beamformer reduce the word error rate by 8% and 9% relative,\nrespectively."}
{"id": "2411.04427", "pdf": "https://arxiv.org/pdf/2411.04427.pdf", "abs": "https://arxiv.org/abs/2411.04427", "title": "One fish, two fish, but not the whole sea: Alignment reduces language models' conceptual diversity", "authors": ["Sonia K. Murthy", "Tomer Ullman", "Jennifer Hu"], "categories": ["cs.CL"], "comment": "17 pages, 10 figures; updated with publishing information", "summary": "Researchers in social science and psychology have recently proposed using\nlarge language models (LLMs) as replacements for humans in behavioral research.\nIn addition to arguments about whether LLMs accurately capture population-level\npatterns, this has raised questions about whether LLMs capture human-like\nconceptual diversity. Separately, it is debated whether post-training alignment\n(RLHF or RLAIF) affects models' internal diversity. Inspired by human studies,\nwe use a new way of measuring the conceptual diversity of\nsynthetically-generated LLM \"populations\" by relating the internal variability\nof simulated individuals to the population-level variability. We use this\napproach to evaluate non-aligned and aligned LLMs on two domains with rich\nhuman behavioral data. While no model reaches human-like diversity, aligned\nmodels generally display less diversity than their instruction fine-tuned\ncounterparts. Our findings highlight potential trade-offs between increasing\nmodels' value alignment and decreasing the diversity of their conceptual\nrepresentations."}
{"id": "2411.08324", "pdf": "https://arxiv.org/pdf/2411.08324.pdf", "abs": "https://arxiv.org/abs/2411.08324", "title": "Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle", "authors": ["Hui Dai", "Ryan Teehan", "Mengye Ren"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ICML 2025", "summary": "Many existing evaluation benchmarks for Large Language Models (LLMs) quickly\nbecome outdated due to the emergence of new models and training data. These\nbenchmarks also fall short in assessing how LLM performance changes over time,\nas they consist of a static set of questions without a temporal dimension. To\naddress these limitations, we propose using future event prediction as a\ncontinuous evaluation method to assess LLMs' temporal generalization and\nforecasting abilities. Our benchmark, Daily Oracle, automatically generates\nquestion-answer (QA) pairs from daily news, challenging LLMs to predict\n\"future\" event outcomes. Our findings reveal that as pre-training data becomes\noutdated, LLM performance degrades over time. While Retrieval Augmented\nGeneration (RAG) has the potential to enhance prediction accuracy, the\nperformance degradation pattern persists, highlighting the need for continuous\nmodel updates. Code and data are available at\nhttps://agenticlearning.ai/daily-oracle."}
{"id": "2412.11459", "pdf": "https://arxiv.org/pdf/2412.11459.pdf", "abs": "https://arxiv.org/abs/2412.11459", "title": "Rethinking Associative Memory Mechanism in Induction Head", "authors": ["Shuo Wang", "Issei Sato"], "categories": ["cs.CL", "cs.LG"], "comment": "COLM 2025", "summary": "Induction head mechanism is a part of the computational circuits for\nin-context learning (ICL) that enable large language models (LLMs) to adapt to\nnew tasks without fine-tuning. Most existing work explains the training\ndynamics behind acquiring such a powerful mechanism. However, the model's\nability to coordinate in-context information over long contexts and global\nknowledge acquired during pretraining remains poorly understood. This paper\ninvestigates how a two-layer transformer thoroughly captures in-context\ninformation and balances it with pretrained bigram knowledge in next token\nprediction, from the viewpoint of associative memory. We theoretically analyze\nthe representation of weight matrices in attention layers and the resulting\nlogits when a transformer is given prompts generated by a bigram model. In the\nexperiments, we design specific prompts to evaluate whether the outputs of the\ntrained transformer align with the theoretical results."}
{"id": "2502.07616", "pdf": "https://arxiv.org/pdf/2502.07616.pdf", "abs": "https://arxiv.org/abs/2502.07616", "title": "Tractable Transformers for Flexible Conditional Generation", "authors": ["Anji Liu", "Xuejie Liu", "Dayuan Zhao", "Mathias Niepert", "Yitao Liang", "Guy Van den Broeck"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Non-autoregressive (NAR) generative models are valuable because they can\nhandle diverse conditional generation tasks in a more principled way than their\nautoregressive (AR) counterparts, which are constrained by sequential\ndependency requirements. Recent advancements in NAR models, such as diffusion\nlanguage models, have demonstrated superior performance in unconditional\ngeneration compared to AR models (e.g., GPTs) of similar sizes. However, such\nimprovements do not always lead to improved conditional generation performance.\nWe show that a key reason for this gap is the difficulty in generalizing to\nconditional probability queries (i.e., the set of unknown variables) unseen\nduring training. As a result, strong unconditional generation performance does\nnot guarantee high-quality conditional generation. This paper proposes\nTractable Transformers (Tracformer), a Transformer-based generative model that\nis more robust to different conditional generation tasks. Unlike existing\nmodels that rely solely on global contextual features derived from full inputs,\nTracformers incorporate a sparse Transformer encoder to capture both local and\nglobal contextual information. This information is routed through a decoder for\nconditional generation. Empirical results demonstrate that Tracformers achieve\nstate-of-the-art conditional generation performance on text modeling compared\nto recent diffusion and AR model baselines."}
{"id": "2502.14429", "pdf": "https://arxiv.org/pdf/2502.14429.pdf", "abs": "https://arxiv.org/abs/2502.14429", "title": "Early-Exit and Instant Confidence Translation Quality Estimation", "authors": ["Vilém Zouhar", "Maike Züfle", "Beni Egressy", "Julius Cheng", "Mrinmaya Sachan", "Jan Niehues"], "categories": ["cs.CL"], "comment": null, "summary": "Quality estimation is omnipresent in machine translation, for both evaluation\nand generation. Unfortunately, quality estimation models are often opaque and\ncomputationally expensive, making them impractical to be part of large-scale\npipelines. In this work, we tackle two connected challenges: (1) reducing the\ncost of quality estimation at scale, and (2) developing an inexpensive\nuncertainty estimation method for quality estimation. To address the latter, we\nintroduce Instant Confidence COMET, an uncertainty-aware quality estimation\nmodel that matches the performance of previous approaches at a fraction of\ntheir costs. We extend this to Early-Exit COMET, a quality estimation model\nthat can compute quality scores and associated confidences already at early\nmodel layers, allowing us to early-exit computations and reduce evaluation\ncosts. We also apply our model to machine translation reranking. We combine\nEarly-Exit COMET with an upper confidence bound bandit algorithm to find the\nbest candidate from a large pool without having to run the full evaluation\nmodel on all candidates. In both cases (evaluation and reranking) our methods\nreduce the required compute by 50% with very little degradation in performance.\nFinally, we show how Instant Confidence COMET can be used to decide which\ntranslations a human evaluator should score rather than relying on the COMET\nscore."}
{"id": "2502.20855", "pdf": "https://arxiv.org/pdf/2502.20855.pdf", "abs": "https://arxiv.org/abs/2502.20855", "title": "MAMUT: A Novel Framework for Modifying Mathematical Formulas for the Generation of Specialized Datasets for Language Model Training", "authors": ["Jonathan Drechsel", "Anja Reusch", "Steffen Herbold"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Mathematical formulas are a fundamental and widely used component in various\nscientific fields, serving as a universal language for expressing complex\nconcepts and relationships. While state-of-the-art transformer models excel in\nprocessing and understanding natural language, they encounter challenges with\nmathematical notation, which involves a complex structure and diverse\nrepresentations. This study focuses on the development of specialized training\ndatasets to enhance the encoding of mathematical content. We introduce Math\nMutator (MAMUT), a framework capable of generating equivalent and falsified\nversions of a given mathematical formula in LaTeX notation, effectively\ncapturing the mathematical variety in notation of the same concept. Based on\nMAMUT, we have generated four large mathematical datasets containing diverse\nnotation. Experiments show that models trained on these datasets exhibit new\nSoTA performance on mathematical retrieval tasks. We publish our code,\ngenerated datasets, and pretrained mathematical models:\nhttps://github.com/aieng-lab/math-mutator."}
{"id": "2503.02233", "pdf": "https://arxiv.org/pdf/2503.02233.pdf", "abs": "https://arxiv.org/abs/2503.02233", "title": "Enhancing LLM Reliability via Explicit Knowledge Boundary Modeling", "authors": ["Hang Zheng", "Hongshen Xu", "Yuncong Liu", "Lu Chen", "Pascale Fung", "Kai Yu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) are prone to hallucination stemming from\nmisaligned self-awareness, particularly when processing queries exceeding their\nknowledge boundaries. While existing mitigation strategies employ uncertainty\nestimation or query rejection mechanisms, they suffer from computational\nefficiency and sacrificed helpfulness. To address these issues, we propose the\nExplicit Knowledge Boundary Modeling (EKBM) framework, integrating fast and\nslow reasoning systems to harmonize reliability and usability. The framework\nfirst employs a fast-thinking model to generate confidence-labeled responses,\nenabling immediate utilization of high-confidence outputs, whereas uncertain\npredictions trigger a slow refinement model for accuracy improvement. To align\nmodel behavior with our proposed object, we propose a hybrid training pipeline,\nenhancing self-awareness without degrading task performance. Evaluations on\ndialogue state tracking tasks demonstrate that EKBM achieves superior model\nreliability over uncertainty-based baselines. Further analysis reveals that\nrefinement substantially boosts accuracy while maintaining low computational\noverhead. The framework establishes a scalable paradigm for deploying reliable\nLLMs in error-sensitive applications, effectively balancing accuracy and\npractical utility."}
{"id": "2503.05763", "pdf": "https://arxiv.org/pdf/2503.05763.pdf", "abs": "https://arxiv.org/abs/2503.05763", "title": "GMLM: Bridging Graph Neural Networks and Language Models for Heterophilic Node Classification", "authors": ["Aarush Sinha"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Integrating structured graph data with rich textual information from nodes\nposes a significant challenge, particularly for heterophilic node\nclassification. Current approaches often struggle with computational costs or\neffective fusion of disparate modalities. We propose \\textbf{Graph Masked\nLanguage Model (GMLM)}, a novel architecture efficiently combining Graph Neural\nNetworks (GNNs) with Pre-trained Language Models (PLMs). GMLM introduces three\nkey innovations: (i) a \\textbf{dynamic active node selection} strategy for\nscalable PLM text processing; (ii) a GNN-specific \\textbf{contrastive\npretraining stage} using soft masking with a learnable graph \\texttt{[MASK]}\ntoken for robust structural representations; and (iii) a \\textbf{dedicated\nfusion module} integrating RGCN-based GNN embeddings with PLM (GTE-Small \\&\nDistilBERT) embeddings. Extensive experiments on heterophilic benchmarks\n(Cornell, Wisconsin, Texas) demonstrate GMLM's superiority. Notably,\nGMLM(DistilBERT) achieves significant performance gains, improving accuracy by\nover \\textbf{4.7\\%} on Cornell and over \\textbf{2.0\\%} on Texas compared to the\nprevious best-performing baselines. This work underscores the benefits of\ntargeted PLM engagement and modality-specific pretraining for improved,\nefficient learning on text-rich graphs."}
{"id": "2503.12149", "pdf": "https://arxiv.org/pdf/2503.12149.pdf", "abs": "https://arxiv.org/abs/2503.12149", "title": "Seeing Sarcasm Through Different Eyes: Analyzing Multimodal Sarcasm Perception in Large Vision-Language Models", "authors": ["Junjie Chen", "Xuyang Liu", "Subin Huang", "Linfeng Zhang", "Hang Yu"], "categories": ["cs.CL", "cs.MM", "cs.SI"], "comment": null, "summary": "With the advent of large vision-language models (LVLMs) demonstrating\nincreasingly human-like abilities, a pivotal question emerges: do different\nLVLMs interpret multimodal sarcasm differently, and can a single model grasp\nsarcasm from multiple perspectives like humans? To explore this, we introduce\nan analytical framework using systematically designed prompts on existing\nmultimodal sarcasm datasets. Evaluating 12 state-of-the-art LVLMs over 2,409\nsamples, we examine interpretive variations within and across models, focusing\non confidence levels, alignment with dataset labels, and recognition of\nambiguous \"neutral\" cases. Our findings reveal notable discrepancies -- across\nLVLMs and within the same model under varied prompts. While\nclassification-oriented prompts yield higher internal consistency, models\ndiverge markedly when tasked with interpretive reasoning. These results\nchallenge binary labeling paradigms by highlighting sarcasm's subjectivity. We\nadvocate moving beyond rigid annotation schemes toward multi-perspective,\nuncertainty-aware modeling, offering deeper insights into multimodal sarcasm\ncomprehension. Our code and data are available at:\nhttps://github.com/CoderChen01/LVLMSarcasmAnalysis"}
{"id": "2503.13299", "pdf": "https://arxiv.org/pdf/2503.13299.pdf", "abs": "https://arxiv.org/abs/2503.13299", "title": "A Survey on Transformer Context Extension: Approaches and Evaluation", "authors": ["Yijun Liu", "Jinzheng Yu", "Yang Xu", "Zhongyang Li", "Qingfu Zhu"], "categories": ["cs.CL", "cs.AI"], "comment": "preprint", "summary": "Large language models (LLMs) based on Transformer have been widely applied in\nthe filed of natural language processing (NLP), demonstrating strong\nperformance, particularly in handling short text tasks. However, when it comes\nto long context scenarios, the performance of LLMs degrades due to some\nchallenges. To alleviate this phenomenon, there is a number of work proposed\nrecently. In this survey, we first list the challenges of applying pre-trained\nLLMs to process long contexts. Then systematically review the approaches\nrelated to long context and propose our taxonomy categorizing them into four\nmain types: positional encoding, context compression, retrieval augmented, and\nattention pattern. In addition to the approaches, we focus on the evaluation of\nlong context, organizing relevant data, tasks, and metrics based on existing\nlong context benchmarks. Finally, we summarize unresolved issues in the long\ncontext domain and put forward our views on future developments."}
{"id": "2504.01931", "pdf": "https://arxiv.org/pdf/2504.01931.pdf", "abs": "https://arxiv.org/abs/2504.01931", "title": "On the Role of Feedback in Test-Time Scaling of Agentic AI Workflows", "authors": ["Souradip Chakraborty", "Mohammadreza Pourreza", "Ruoxi Sun", "Yiwen Song", "Nino Scherrer", "Furong Huang", "Amrit Singh Bedi", "Ahmad Beirami", "Jindong Gu", "Hamid Palangi", "Tomas Pfister"], "categories": ["cs.CL"], "comment": null, "summary": "Agentic AI workflows (systems that autonomously plan and act) are becoming\nwidespread, yet their task success rate on complex tasks remains low. A\npromising solution is inference-time alignment, which uses extra compute at\ntest time to improve performance. Inference-time alignment relies on three\ncomponents: sampling, evaluation, and feedback. While most prior work studies\nsampling and automatic evaluation, feedback remains underexplored. To study the\nrole of feedback, we introduce Iterative Agent Decoding (IAD), a procedure that\nrepeatedly inserts feedback extracted from different forms of critiques (reward\nmodels or AI-generated textual feedback) between decoding steps. Through IAD,\nwe analyze feedback along four dimensions: (1) its role in the accuracy-compute\ntrade-offs with limited inference budget, (2) quantifying the gains over\ndiversity-only baselines such as best-of-N sampling, (3) effectiveness of\ncomposing feedback from reward models versus textual critique, and (4)\nrobustness to noisy or low-quality feedback. Across Sketch2Code, Text2SQL,\nIntercode, and WebShop, we show that IAD with proper integration of high\nfidelity feedback leads to consistent gains up to 10 percent absolute\nperformance improvement over various baselines such as best-of-N. Our findings\nunderscore feedback as a crucial knob for inference-time alignment of agentic\nAI workflows with limited inference budget."}
{"id": "2504.07096", "pdf": "https://arxiv.org/pdf/2504.07096.pdf", "abs": "https://arxiv.org/abs/2504.07096", "title": "OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training Tokens", "authors": ["Jiacheng Liu", "Taylor Blanton", "Yanai Elazar", "Sewon Min", "YenSung Chen", "Arnavi Chheda-Kothary", "Huy Tran", "Byron Bischoff", "Eric Marsh", "Michael Schmitz", "Cassidy Trier", "Aaron Sarnat", "Jenna James", "Jon Borchardt", "Bailey Kuehl", "Evie Cheng", "Karen Farley", "Sruthi Sreeram", "Taira Anderson", "David Albright", "Carissa Schoenick", "Luca Soldaini", "Dirk Groeneveld", "Rock Yuren Pang", "Pang Wei Koh", "Noah A. Smith", "Sophie Lebrecht", "Yejin Choi", "Hannaneh Hajishirzi", "Ali Farhadi", "Jesse Dodge"], "categories": ["cs.CL"], "comment": "ACL 2025 demo track", "summary": "We present OLMoTrace, the first system that traces the outputs of language\nmodels back to their full, multi-trillion-token training data in real time.\nOLMoTrace finds and shows verbatim matches between segments of language model\noutput and documents in the training text corpora. Powered by an extended\nversion of infini-gram (Liu et al., 2024), our system returns tracing results\nwithin a few seconds. OLMoTrace can help users understand the behavior of\nlanguage models through the lens of their training data. We showcase how it can\nbe used to explore fact checking, hallucination, and the creativity of language\nmodels. OLMoTrace is publicly available and fully open-source."}
{"id": "2505.04531", "pdf": "https://arxiv.org/pdf/2505.04531.pdf", "abs": "https://arxiv.org/abs/2505.04531", "title": "Overcoming Data Scarcity in Generative Language Modelling for Low-Resource Languages: A Systematic Review", "authors": ["Josh McGiff", "Nikola S. Nikolov"], "categories": ["cs.CL", "cs.AI"], "comment": "This work is currently under review. Please do not cite without\n  permission", "summary": "Generative language modelling has surged in popularity with the emergence of\nservices such as ChatGPT and Google Gemini. While these models have\ndemonstrated transformative potential in productivity and communication, they\noverwhelmingly cater to high-resource languages like English. This has\namplified concerns over linguistic inequality in natural language processing\n(NLP). This paper presents the first systematic review focused specifically on\nstrategies to address data scarcity in generative language modelling for\nlow-resource languages (LRL). Drawing from 54 studies, we identify, categorise\nand evaluate technical approaches, including monolingual data augmentation,\nback-translation, multilingual training, and prompt engineering, across\ngenerative tasks. We also analyse trends in architecture choices, language\nfamily representation, and evaluation methods. Our findings highlight a strong\nreliance on transformer-based models, a concentration on a small subset of\nLRLs, and a lack of consistent evaluation across studies. We conclude with\nrecommendations for extending these methods to a wider range of LRLs and\noutline open challenges in building equitable generative language systems.\nUltimately, this review aims to support researchers and developers in building\ninclusive AI tools for underrepresented languages, a necessary step toward\nempowering LRL speakers and the preservation of linguistic diversity in a world\nincreasingly shaped by large-scale language technologies."}
{"id": "2505.04649", "pdf": "https://arxiv.org/pdf/2505.04649.pdf", "abs": "https://arxiv.org/abs/2505.04649", "title": "FRAME: Feedback-Refined Agent Methodology for Enhancing Medical Research Insights", "authors": ["Chengzhang Yu", "Yiming Zhang", "Zhixin Liu", "Zenghui Ding", "Yining Sun", "Zhanpeng Jin"], "categories": ["cs.CL"], "comment": "12 pages, 4 figures, 5 table", "summary": "The automation of scientific research through large language models (LLMs)\npresents significant opportunities but faces critical challenges in knowledge\nsynthesis and quality assurance. We introduce Feedback-Refined Agent\nMethodology (FRAME), a novel framework that enhances medical paper generation\nthrough iterative refinement and structured feedback. Our approach comprises\nthree key innovations: (1) A structured dataset construction method that\ndecomposes 4,287 medical papers into essential research components through\niterative refinement; (2) A tripartite architecture integrating Generator,\nEvaluator, and Reflector agents that progressively improve content quality\nthrough metric-driven feedback; and (3) A comprehensive evaluation framework\nthat combines statistical metrics with human-grounded benchmarks. Experimental\nresults demonstrate FRAME's effectiveness, achieving significant improvements\nover conventional approaches across multiple models (9.91% average gain with\nDeepSeek V3, comparable improvements with GPT-4o Mini) and evaluation\ndimensions. Human evaluation confirms that FRAME-generated papers achieve\nquality comparable to human-authored works, with particular strength in\nsynthesizing future research directions. The results demonstrated our work\ncould efficiently assist medical research by building a robust foundation for\nautomated medical research paper generation while maintaining rigorous academic\nstandards."}
{"id": "2505.12182", "pdf": "https://arxiv.org/pdf/2505.12182.pdf", "abs": "https://arxiv.org/abs/2505.12182", "title": "Truth Neurons", "authors": ["Haohang Li", "Yupeng Cao", "Yangyang Yu", "Jordan W. Suchow", "Zining Zhu"], "categories": ["cs.CL"], "comment": null, "summary": "Despite their remarkable success and deployment across diverse workflows,\nlanguage models sometimes produce untruthful responses. Our limited\nunderstanding of how truthfulness is mechanistically encoded within these\nmodels jeopardizes their reliability and safety. In this paper, we propose a\nmethod for identifying representations of truthfulness at the neuron level. We\nshow that language models contain truth neurons, which encode truthfulness in a\nsubject-agnostic manner. Experiments conducted across models of varying scales\nvalidate the existence of truth neurons, confirming that the encoding of\ntruthfulness at the neuron level is a property shared by many language models.\nThe distribution patterns of truth neurons over layers align with prior\nfindings on the geometry of truthfulness. Selectively suppressing the\nactivations of truth neurons found through the TruthfulQA dataset degrades\nperformance both on TruthfulQA and on other benchmarks, showing that the\ntruthfulness mechanisms are not tied to a specific dataset. Our results offer\nnovel insights into the mechanisms underlying truthfulness in language models\nand highlight potential directions toward improving their trustworthiness and\nreliability."}
{"id": "2505.15634", "pdf": "https://arxiv.org/pdf/2505.15634.pdf", "abs": "https://arxiv.org/abs/2505.15634", "title": "Feature Extraction and Steering for Enhanced Chain-of-Thought Reasoning in Language Models", "authors": ["Zihao Li", "Xu Wang", "Yuzhe Yang", "Ziyu Yao", "Haoyi Xiong", "Mengnan Du"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) demonstrate the ability to solve reasoning and\nmathematical problems using the Chain-of-Thought (CoT) technique. Expanding CoT\nlength, as seen in models such as DeepSeek-R1, significantly enhances this\nreasoning for complex problems, but requires costly and high-quality long CoT\ndata and fine-tuning. This work, inspired by the deep thinking paradigm of\nDeepSeek-R1, utilizes a steering technique to enhance the reasoning ability of\nan LLM without external datasets. Our method first employs Sparse Autoencoders\n(SAEs) to extract interpretable features from vanilla CoT. These features are\nthen used to steer the LLM's internal states during generation. Recognizing\nthat many LLMs do not have corresponding pre-trained SAEs, we further introduce\na novel SAE-free steering algorithm, which directly computes steering\ndirections from the residual activations of an LLM, obviating the need for an\nexplicit SAE. Experimental results demonstrate that both our SAE-based and\nsubsequent SAE-free steering algorithms significantly enhance the reasoning\ncapabilities of LLMs."}
{"id": "2505.23404", "pdf": "https://arxiv.org/pdf/2505.23404.pdf", "abs": "https://arxiv.org/abs/2505.23404", "title": "MEF: A Capability-Aware Multi-Encryption Framework for Evaluating Vulnerabilities in Black-Box Large Language Models", "authors": ["Mingyu Yu", "Wei Wang", "Yanjie Wei", "Sujuan Qin", "Fei Gao", "Wenmin Li"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in adversarial jailbreak attacks have revealed\nsignificant vulnerabilities in Large Language Models (LLMs), facilitating the\nevasion of alignment safeguards through increasingly sophisticated prompt\nmanipulations. In this paper, we propose MEF, a capability-aware\nmulti-encryption framework for evaluating vulnerabilities in black-box LLMs.\nOur key insight is that the effectiveness of jailbreak strategies can be\nsignificantly enhanced by tailoring them to the semantic comprehension\ncapabilities of the target model. We present a typology that classifies LLMs\ninto Type I and Type II based on their comprehension levels, and design\nadaptive attack strategies for each. MEF combines layered semantic mutations\nand dual-ended encryption techniques, enabling circumvention of input,\ninference, and output-level defenses. Experimental results demonstrate the\nsuperiority of our approach. Remarkably, it achieves a jailbreak success rate\nof 98.9\\% on GPT-4o (29 May 2025 release). Our findings reveal vulnerabilities\nin current LLMs' alignment defenses."}
{"id": "2506.00854", "pdf": "https://arxiv.org/pdf/2506.00854.pdf", "abs": "https://arxiv.org/abs/2506.00854", "title": "EEG2TEXT-CN: An Exploratory Study of Open-Vocabulary Chinese Text-EEG Alignment via Large Language Model and Contrastive Learning on ChineseEEG", "authors": ["Jacky Tai-Yu Lu", "Jung Chiang", "Chi-Sheng Chen", "Anna Nai-Yun Tung", "Hsiang Wei Hu", "Yuan Chiao Cheng"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MM", "q-bio.NC"], "comment": null, "summary": "We propose EEG2TEXT-CN, which, to the best of our knowledge, represents one\nof the earliest open-vocabulary EEG-to-text generation frameworks tailored for\nChinese. Built on a biologically grounded EEG encoder (NICE-EEG) and a compact\npretrained language model (MiniLM), our architecture aligns multichannel brain\nsignals with natural language representations via masked pretraining and\ncontrastive learning. Using a subset of the ChineseEEG dataset, where each\nsentence contains approximately ten Chinese characters aligned with 128-channel\nEEG recorded at 256 Hz, we segment EEG into per-character embeddings and\npredict full sentences in a zero-shot setting. The decoder is trained with\nteacher forcing and padding masks to accommodate variable-length sequences.\nEvaluation on over 1,500 training-validation sentences and 300 held-out test\nsamples shows promising lexical alignment, with a best BLEU-1 score of 6.38\\%.\nWhile syntactic fluency remains a challenge, our findings demonstrate the\nfeasibility of non-phonetic, cross-modal language decoding from EEG. This work\nopens a new direction in multilingual brain-to-text research and lays the\nfoundation for future cognitive-language interfaces in Chinese."}
{"id": "2506.03861", "pdf": "https://arxiv.org/pdf/2506.03861.pdf", "abs": "https://arxiv.org/abs/2506.03861", "title": "PulseReddit: A Novel Reddit Dataset for Benchmarking MAS in High-Frequency Cryptocurrency Trading", "authors": ["Qiuhan Han", "Qian Wang", "Atsushi Yoshikawa", "Masayuki Yamamura"], "categories": ["cs.CL"], "comment": null, "summary": "High-Frequency Trading (HFT) is pivotal in cryptocurrency markets, demanding\nrapid decision-making. Social media platforms like Reddit offer valuable, yet\nunderexplored, information for such high-frequency, short-term trading. This\npaper introduces \\textbf{PulseReddit}, a novel dataset that is the first to\nalign large-scale Reddit discussion data with high-frequency cryptocurrency\nmarket statistics for short-term trading analysis. We conduct an extensive\nempirical study using Large Language Model (LLM)-based Multi-Agent Systems\n(MAS) to investigate the impact of social sentiment from PulseReddit on trading\nperformance. Our experiments conclude that MAS augmented with PulseReddit data\nachieve superior trading outcomes compared to traditional baselines,\nparticularly in bull markets, and demonstrate robust adaptability across\ndifferent market regimes. Furthermore, our research provides conclusive\ninsights into the performance-efficiency trade-offs of different LLMs,\ndetailing significant considerations for practical model selection in HFT\napplications. PulseReddit and our findings establish a foundation for advanced\nMAS research in HFT, demonstrating the tangible benefits of integrating social\nmedia."}
{"id": "2506.08938", "pdf": "https://arxiv.org/pdf/2506.08938.pdf", "abs": "https://arxiv.org/abs/2506.08938", "title": "FaithfulRAG: Fact-Level Conflict Modeling for Context-Faithful Retrieval-Augmented Generation", "authors": ["Qinggang Zhang", "Zhishang Xiang", "Yilin Xiao", "Le Wang", "Junhui Li", "Xinrun Wang", "Jinsong Su"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025", "summary": "Large language models (LLMs) augmented with retrieval systems have\ndemonstrated significant potential in handling knowledge-intensive tasks.\nHowever, these models often struggle with unfaithfulness issues, generating\noutputs that either ignore the retrieved context or inconsistently blend it\nwith the LLM`s parametric knowledge. This issue is particularly severe in cases\nof knowledge conflict, where the retrieved context conflicts with the model`s\nparametric knowledge. While existing faithful RAG approaches enforce strict\ncontext adherence through well-designed prompts or modified decoding\nstrategies, our analysis reveals a critical limitation: they achieve\nfaithfulness by forcibly suppressing the model`s parametric knowledge, which\nundermines the model`s internal knowledge structure and increases the risk of\nmisinterpreting the context. To this end, this paper proposes FaithfulRAG, a\nnovel framework that resolves knowledge conflicts by explicitly modeling\ndiscrepancies between the model`s parametric knowledge and retrieved context.\nSpecifically, FaithfulRAG identifies conflicting knowledge at the fact level\nand designs a self-thinking process, allowing LLMs to reason about and\nintegrate conflicting facts before generating responses. Extensive experiments\ndemonstrate that our method outperforms state-of-the-art methods. The code is\navailable at https://github.com/DeepLearnXMU/Faithful-RAG"}
{"id": "2506.12229", "pdf": "https://arxiv.org/pdf/2506.12229.pdf", "abs": "https://arxiv.org/abs/2506.12229", "title": "Infini-gram mini: Exact n-gram Search at the Internet Scale with FM-Index", "authors": ["Hao Xu", "Jiacheng Liu", "Yejin Choi", "Noah A. Smith", "Hannaneh Hajishirzi"], "categories": ["cs.CL"], "comment": null, "summary": "Language models are trained mainly on massive text data from the Internet,\nand it becomes increasingly important to understand this data source.\nExact-match search engines enable searching in large text corpora -- counting\nstring appearances and retrieving the enclosing documents -- yet the high\nstorage overhead hinders their application on Internet-scale data. We present\nInfini-gram mini, an efficient and scalable system that can make petabyte-level\ntext corpora searchable. Based on the FM-index data structure (Ferragina and\nManzini, 2000), which simultaneously indexes and compresses text, our system\ncreates indexes with size only 44% of the corpus. Infini-gram mini greatly\nimproves upon the best existing implementation of FM-index in terms of indexing\nspeed (18$\\times$) and memory use during both indexing (3.2$\\times$ reduction)\nand querying (down to a negligible amount). We index 46TB of Internet text in\n50 days with a single 128-core CPU node (or 19 hours if using 75 such nodes).\nWe show one important use case of Infini-gram mini in a large-scale analysis of\nbenchmark contamination. We find several core LM evaluation benchmarks to be\nheavily contaminated in Internet crawls (up to 40% in SQuAD), which could lead\nto overestimating the capabilities of language models if trained on such data.\nWe host a benchmark contamination bulletin to share the contamination rate of\nmany core and community-contributed benchmarks. We also release a web interface\nand an API endpoint to serve general search queries on Infini-gram mini\nindexes."}
{"id": "2506.13734", "pdf": "https://arxiv.org/pdf/2506.13734.pdf", "abs": "https://arxiv.org/abs/2506.13734", "title": "Instruction Following by Boosting Attention of Large Language Models", "authors": ["Vitoria Guardieiro", "Adam Stein", "Avishree Khare", "Eric Wong"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Controlling the generation of large language models (LLMs) remains a central\nchallenge to ensure their safe and reliable deployment. While prompt\nengineering and finetuning are common approaches, recent work has explored\nlatent steering, a lightweight technique that alters LLM internal activations\nto guide generation. However, subsequent studies revealed latent steering's\neffectiveness to be limited, often underperforming simple instruction\nprompting. To address this limitation, we first establish a benchmark across\ndiverse behaviors for standardized evaluation of steering techniques. Building\non insights from this benchmark, we introduce Instruction Attention Boosting\n(InstABoost), a latent steering method that boosts the strength of instruction\nprompting by altering the model's attention during generation. InstABoost\ncombines the strengths of existing approaches and is theoretically supported by\nprior work that suggests that in-context rule following in transformer-based\nmodels can be controlled by manipulating attention on instructions.\nEmpirically, InstABoost demonstrates superior control success compared to both\ntraditional prompting and latent steering."}
{"id": "2506.19652", "pdf": "https://arxiv.org/pdf/2506.19652.pdf", "abs": "https://arxiv.org/abs/2506.19652", "title": "Tailored Conversations beyond LLMs: A RL-Based Dialogue Manager", "authors": ["Lucie Galland", "Catherine Pelachaud", "Florian Pecune"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this work, we propose a novel framework that integrates large language\nmodels (LLMs) with an RL-based dialogue manager for open-ended dialogue with a\nspecific goal. By leveraging hierarchical reinforcement learning to model the\nstructured phases of dialogue and employ meta-learning to enhance adaptability\nacross diverse user profiles, our approach enhances adaptability and\nefficiency, enabling the system to learn from limited data, transition fluidly\nbetween dialogue phases, and personalize responses to heterogeneous patient\nneeds. We apply our framework to Motivational Interviews, aiming to foster\nbehavior change, and demonstrate that the proposed dialogue manager outperforms\na state-of-the-art LLM baseline in terms of reward, showing a potential benefit\nof conditioning LLMs to create open-ended dialogue systems with specific goals."}
{"id": "2507.02850", "pdf": "https://arxiv.org/pdf/2507.02850.pdf", "abs": "https://arxiv.org/abs/2507.02850", "title": "LLM Hypnosis: Exploiting User Feedback for Unauthorized Knowledge Injection to All Users", "authors": ["Almog Hilel", "Idan Shenfeld", "Jacob Andreas", "Leshem Choshen"], "categories": ["cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "We describe a vulnerability in language models (LMs) trained with user\nfeedback, whereby a single user can persistently alter LM knowledge and\nbehavior given only the ability to provide prompts and upvote / downvote\nfeedback on LM outputs. To implement the attack, the attacker prompts the LM to\nstochastically output either a \"poisoned\" or benign response, then upvotes the\npoisoned response or downvotes the benign one. When feedback signals are used\nin a subsequent preference tuning behavior, LMs exhibit increased probability\nof producing poisoned responses even in contexts without malicious prompts. We\nshow that this attack can be used to (1) insert factual knowledge the model did\nnot previously possess, (2) modify code generation patterns in ways that\nintroduce exploitable security flaws, and (3) inject fake financial news. Our\nfinding both identifies a new qualitative feature of language model preference\ntuning (showing that it even highly restricted forms of preference data can be\nused to exert fine-grained control over behavior), and a new attack mechanism\nfor LMs trained with user feedback (extending work on pretraining-time data\npoisoning and deployment-time prompt injection)."}
{"id": "2507.02950", "pdf": "https://arxiv.org/pdf/2507.02950.pdf", "abs": "https://arxiv.org/abs/2507.02950", "title": "Evaluating AI Counseling in Japanese: Counselor, Client, and Evaluator Roles Assessed by Motivational Interviewing Criteria", "authors": ["Keita Kiuchi", "Yoshikazu Fujimoto", "Hideyuki Goto", "Tomonori Hosokawa", "Makoto Nishimura", "Yosuke Sato", "Izumi Sezai"], "categories": ["cs.CL", "cs.AI", "cs.HC", "68T50", "I.2.7; H.5.2; J.4"], "comment": "70 pages, 0 figures, 9 tables; data and code at\n  https://osf.io/p8c39/files/2e58c42f-a7ba-45f2-aa60-265e107e36db", "summary": "This study provides the first comprehensive evaluation of large language\nmodel (LLM) performance across three counseling roles in Japanese-language\ntherapeutic contexts. We simultaneously assessed counselor artificial\nintelligence (AI) systems (GPT-4-turbo with zeroshot prompting or Structured\nMulti-step Dialogue Prompts (SMDP), Claude-3-Opus-SMDP), client AI simulations,\nand evaluation AI systems (o3, Claude-3.7-Sonnet, Gemini-2.5-pro). Human\nexperts (n = 15) with extensive counseling experience evaluated AI-generated\ndialogues using the Motivational Interviewing Treatment Integrity (MITI) Coding\nManual 4.2.1.\n  Notably, SMDP implementation significantly enhanced counselor AI performance\nacross all MITI global ratings compared with zeroshot prompting, with no\nsignificant differences between GPT-SMDP and Opus-SMDP. Evaluation AIs showed\ncomparable performance to human raters for Cultivating Change Talk but\nsystematically overestimated Softening Sustain Talk and the overall quality\nmetrics. Model-specific biases emerged: Gemini emphasized power-sharing, o3\nfocused on technical proficiency, and Sonnet prioritized emotional expression.\nClient AI simulations exhibited a limited emotional range and unnaturally high\ncompliance, indicating the need for enhanced realism.\n  These findings establish benchmarks for AI-assisted counseling in non-English\ncontexts and identify critical areas for improvement through advanced prompt\nengineering, retrieval-augmented generation, and targeted fine-tuning, with\nimportant implications for developing culturally sensitive AI mental health\ntools."}
{"id": "2507.02962", "pdf": "https://arxiv.org/pdf/2507.02962.pdf", "abs": "https://arxiv.org/abs/2507.02962", "title": "RAG-R1 : Incentivize the Search and Reasoning Capabilities of LLMs through Multi-query Parallelism", "authors": ["Zhiwen Tan", "Jiaming Huang", "Qintong Wu", "Hongxuan Zhang", "Chenyi Zhuang", "Jinjie Gu"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, while they remain prone to generating hallucinated or outdated\nresponses due to their static internal knowledge. Recent advancements in\nRetrieval-Augmented Generation (RAG) methods have explored enhancing models'\nsearch and reasoning capabilities through reinforcement learning (RL). Although\nthese methods demonstrate promising results, they face challenges in training\nstability and encounter issues such as substantial inference time and\nrestricted capabilities due to the single-query mode. In this paper, we propose\nRAG-R1, a novel training framework designed to enable LLMs to adaptively\nleverage internal and external knowledge during the reasoning process. We\nfurther expand the generation and retrieval processes within the framework from\nsingle-query mode to multi-query parallelism, aimed at reducing inference time\nand enhancing the model's capabilities. Extensive experiments on seven\nquestion-answering benchmarks demonstrate that our method outperforms the\nstrongest baseline by up to 13.2% and decreases inference time by 11.1%."}
{"id": "2507.02986", "pdf": "https://arxiv.org/pdf/2507.02986.pdf", "abs": "https://arxiv.org/abs/2507.02986", "title": "GAF-Guard: An Agentic Framework for Risk Management and Governance in Large Language Models", "authors": ["Seshu Tirupathi", "Dhaval Salwala", "Elizabeth Daly", "Inge Vejsbjerg"], "categories": ["cs.CL"], "comment": null, "summary": "As Large Language Models (LLMs) continue to be increasingly applied across\nvarious domains, their widespread adoption necessitates rigorous monitoring to\nprevent unintended negative consequences and ensure robustness. Furthermore,\nLLMs must be designed to align with human values, like preventing harmful\ncontent and ensuring responsible usage. The current automated systems and\nsolutions for monitoring LLMs in production are primarily centered on\nLLM-specific concerns like hallucination etc, with little consideration given\nto the requirements of specific use-cases and user preferences. This paper\nintroduces GAF-Guard, a novel agentic framework for LLM governance that places\nthe user, the use-case, and the model itself at the center. The framework is\ndesigned to detect and monitor risks associated with the deployment of LLM\nbased applications. The approach models autonomous agents that identify risks,\nactivate risk detection tools, within specific use-cases and facilitate\ncontinuous monitoring and reporting to enhance AI safety, and user\nexpectations. The code is available at\nhttps://github.com/IBM/risk-atlas-nexus-demos/tree/main/gaf-guard."}
{"id": "2507.03009", "pdf": "https://arxiv.org/pdf/2507.03009.pdf", "abs": "https://arxiv.org/abs/2507.03009", "title": "PDFMathTranslate: Scientific Document Translation Preserving Layouts", "authors": ["Rongxin Ouyang", "Chang Chu", "Zhikuang Xin", "Xiangyao Ma"], "categories": ["cs.CL", "cs.IR", "cs.LG", "68T50, 68T45, 68U10, 68U15", "D.2.2; I.2.10; I.2.7; J.0"], "comment": "7 pages, 4 figures", "summary": "Language barriers in scientific documents hinder the diffusion and\ndevelopment of science and technologies. However, prior efforts in translating\nsuch documents largely overlooked the information in layouts. To bridge the\ngap, we introduce PDFMathTranslate, the world's first open-source software for\ntranslating scientific documents while preserving layouts. Leveraging the most\nrecent advances in large language models and precise layout detection, we\ncontribute to the community with key improvements in precision, flexibility,\nand efficiency. The work has been open-sourced at\nhttps://github.com/byaidu/pdfmathtranslate with more than 222k downloads."}
{"id": "2507.03343", "pdf": "https://arxiv.org/pdf/2507.03343.pdf", "abs": "https://arxiv.org/abs/2507.03343", "title": "SHNU Multilingual Conversational Speech Recognition System for INTERSPEECH 2025 MLC-SLM Challenge", "authors": ["Yuxiang Mei", "Yuang Zheng", "Dongxing Xu", "Yanhua Long"], "categories": ["cs.CL", "eess.AS"], "comment": "Accepted by Interspeech 2025 MLC-SLM workshop", "summary": "This paper describes SHNU multilingual conversational speech recognition\nsystem (SHNU-mASR, team name-\"maybe\"), submitted to Track 1 of the INTERSPEECH\n2025 MLC-SLM Challenge. Our system integrates a parallel-speech-encoder\narchitecture with a large language model (LLM) to form a unified multilingual\nASR framework. The parallel-speech-encoder consists of two pre-trained\nencoders, the Whisper-large-v3 encoder and mHuBERT-147 encoder. Their output\nembeddings are concatenated and fed into the LLM, enabling the model to\nleverage complementary acoustic and linguistic knowledge and achieve\ncompetitive performance. Moreover, we adopt a tri-stage training strategy to\njointly update the low-rank adaptation modules and projector parameters of both\nthe speech encoders and the LLM. In addition, we incorporate an additional\nlanguage-aware prompt at the LLM input to enhance language-specific text\ngeneration. The SHNU-mASR system achieves an overall character/word error rate\n(CER/WER) of 11.76% on the blind evaluation set of the challenge, outperforming\nthe official MLC-SLM baseline by 8.41 absolute CER/WER, without increasing the\nbaseline training data."}
{"id": "2507.03483", "pdf": "https://arxiv.org/pdf/2507.03483.pdf", "abs": "https://arxiv.org/abs/2507.03483", "title": "BMMR: A Large-Scale Bilingual Multimodal Multi-Discipline Reasoning Dataset", "authors": ["Zhiheng Xi", "Guanyu Li", "Yutao Fan", "Honglin Guo", "Yufang Liu", "Xiaoran Fan", "Jiaqi Liu", "Jingchao Ding", "Wangmeng Zuo", "Zhenfei Yin", "Lei Bai", "Tao Ji", "Tao Gui", "Qi Zhang", "Philip Torr", "Xuanjing Huang"], "categories": ["cs.CL", "cs.AI"], "comment": "Preprint", "summary": "In this paper, we introduce BMMR, a large-scale bilingual, multimodal,\nmulti-disciplinary reasoning dataset for the community to develop and evaluate\nlarge multimodal models (LMMs). BMMR comprises 110k college-level questions\nspanning 300 UNESCO-defined subjects, spanning diverse formats-multiple-choice,\nfill-in-the-blank, and open-ended QA-and sourced from both print and digital\nmedia such as books, exams, and quizzes. All data are curated and filtered via\na human-in-the-loop and scalable framework, and each instance is paired with a\nhigh-quality reasoning path. The dataset is organized into two parts: BMMR-Eval\nthat comprises 20,458 high-quality instances to comprehensively assess LMMs'\nknowledge and reasoning across multiple disciplines in both Chinese and\nEnglish; and BMMR-Train that contains 88,991 instances to support further\nresearch and development, extending the current focus on mathematical reasoning\nto diverse disciplines and domains. In addition, we propose the process-based\nmulti-discipline verifier (i.e., BMMR-Verifier) for accurate and fine-grained\nevaluation of reasoning paths. Extensive experiments on 24 models reveal that\n(i) even SOTA models (e.g., o3 and Gemini-2.5-Pro) leave substantial headroom\non BMMR-Eval; (ii) reasoning models exhibit discipline bias and outperform LMMs\nonly on specific subjects; (iii) open-source models still trail their\nproprietary counterparts; and (iv) fine-tuning on BMMR-Train narrows this gap.\nAdditionally, we conduct reasoning-chain analyses using BMMR-Verifier and other\nin-depth studies, uncovering the challenges LMMs currently face in\nmultidisciplinary reasoning. We will release the data, and we hope our work can\noffer insights and contributions to the community."}
{"id": "2507.03711", "pdf": "https://arxiv.org/pdf/2507.03711.pdf", "abs": "https://arxiv.org/abs/2507.03711", "title": "Can LLMs Play Ô Ăn Quan Game? A Study of Multi-Step Planning and Decision Making", "authors": ["Sang Quang Nguyen", "Kiet Van Nguyen", "Vinh-Tiep Nguyen", "Thanh Duc Ngo", "Ngan Luu-Thuy Nguyen", "Dinh-Duy Le"], "categories": ["cs.CL"], "comment": "Accepted paper at MAPR 2025", "summary": "In this paper, we explore the ability of large language models (LLMs) to plan\nand make decisions through the lens of the traditional Vietnamese board game,\n\\^O \\u{A}n Quan. This game, which involves a series of strategic token\nmovements and captures, offers a unique environment for evaluating the\ndecision-making and strategic capabilities of LLMs. Specifically, we develop\nvarious agent personas, ranging from aggressive to defensive, and employ the\n\\^O \\u{A}n Quan game as a testbed for assessing LLM performance across\ndifferent strategies. Through experimentation with models like\nLlama-3.2-3B-Instruct, Llama-3.1-8B-Instruct, and Llama-3.3-70B-Instruct, we\naim to understand how these models execute strategic decision-making, plan\nmoves, and manage dynamic game states. The results will offer insights into the\nstrengths and weaknesses of LLMs in terms of reasoning and strategy,\ncontributing to a deeper understanding of their general capabilities."}
{"id": "2507.03724", "pdf": "https://arxiv.org/pdf/2507.03724.pdf", "abs": "https://arxiv.org/abs/2507.03724", "title": "MemOS: A Memory OS for AI System", "authors": ["Zhiyu Li", "Shichao Song", "Chenyang Xi", "Hanyu Wang", "Chen Tang", "Simin Niu", "Ding Chen", "Jiawei Yang", "Chunyu Li", "Qingchen Yu", "Jihao Zhao", "Yezhaohui Wang", "Peng Liu", "Zehao Lin", "Pengyuan Wang", "Jiahao Huo", "Tianyi Chen", "Kai Chen", "Kehang Li", "Zhen Tao", "Junpeng Ren", "Huayi Lai", "Hao Wu", "Bo Tang", "Zhenren Wang", "Zhaoxin Fan", "Ningyu Zhang", "Linfeng Zhang", "Junchi Yan", "Mingchuan Yang", "Tong Xu", "Wei Xu", "Huajun Chen", "Haofeng Wang", "Hongkang Yang", "Wentao Zhang", "Zhi-Qin John Xu", "Siheng Chen", "Feiyu Xiong"], "categories": ["cs.CL"], "comment": "36 pages, 10 figures, 5 tables", "summary": "Large Language Models (LLMs) have become an essential infrastructure for\nArtificial General Intelligence (AGI), yet their lack of well-defined memory\nmanagement systems hinders the development of long-context reasoning, continual\npersonalization, and knowledge consistency.Existing models mainly rely on\nstatic parameters and short-lived contextual states, limiting their ability to\ntrack user preferences or update knowledge over extended periods.While\nRetrieval-Augmented Generation (RAG) introduces external knowledge in plain\ntext, it remains a stateless workaround without lifecycle control or\nintegration with persistent representations.Recent work has modeled the\ntraining and inference cost of LLMs from a memory hierarchy perspective,\nshowing that introducing an explicit memory layer between parameter memory and\nexternal retrieval can substantially reduce these costs by externalizing\nspecific knowledge. Beyond computational efficiency, LLMs face broader\nchallenges arising from how information is distributed over time and context,\nrequiring systems capable of managing heterogeneous knowledge spanning\ndifferent temporal scales and sources. To address this challenge, we propose\nMemOS, a memory operating system that treats memory as a manageable system\nresource. It unifies the representation, scheduling, and evolution of\nplaintext, activation-based, and parameter-level memories, enabling\ncost-efficient storage and retrieval. As the basic unit, a MemCube encapsulates\nboth memory content and metadata such as provenance and versioning. MemCubes\ncan be composed, migrated, and fused over time, enabling flexible transitions\nbetween memory types and bridging retrieval with parameter-based learning.\nMemOS establishes a memory-centric system framework that brings\ncontrollability, plasticity, and evolvability to LLMs, laying the foundation\nfor continual learning and personalized modeling."}
{"id": "2507.04942", "pdf": "https://arxiv.org/pdf/2507.04942.pdf", "abs": "https://arxiv.org/abs/2507.04942", "title": "SIGIR 2025 -- LiveRAG Challenge Report", "authors": ["David Carmel", "Simone Filice", "Guy Horowitz", "Yoelle Maarek", "Oren Somekh", "Ran Tavory", "Mehdi Ghissassi", "Edo Liberty", "Roy Miara"], "categories": ["cs.CL", "cs.IR", "H.3.3"], "comment": "9 pages, 5 tables", "summary": "The LiveRAG Challenge at SIGIR 2025, held between March and May 2025,\nprovided a competitive platform for advancing Retrieval-Augmented Generation\n(RAG) technologies. Participants from academia and industry were invited to\ndevelop a RAG-based question-answering system using a fixed corpus\n(Fineweb-10BT) and a common open-source LLM (Falcon3-10B-Instruct). The goal\nwas to facilitate challenging comparisons of retrieval and prompting\nstrategies. During the Live Challenge Day, 70 teams from 27 different countries\nprovided answers and supportive information to 500 unseen questions within a\nstrict two-hour time window. Evaluation was conducted in two stages: first an\nautomated LLM-as-a-judge approach was used to compute correctness and\nfaithfulness score, then a manual review of top ranked submissions was\nconducted. The finalists were announced on June 12, 2025, with prizes awarded\nduring the LiveRAG Workshop at SIGIR 2025 in Padua, Italy."}
{"id": "2507.05177", "pdf": "https://arxiv.org/pdf/2507.05177.pdf", "abs": "https://arxiv.org/abs/2507.05177", "title": "OpenS2S: Advancing Fully Open-Source End-to-End Empathetic Large Speech Language Model", "authors": ["Chen Wang", "Tianyu Peng", "Wen Yang", "Yinan Bai", "Guangfu Wang", "Jun Lin", "Lanpeng Jia", "Lingxiang Wu", "Jinqiao Wang", "Chengqing Zong", "Jiajun Zhang"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "Technical Report", "summary": "Empathetic interaction is a cornerstone of human-machine communication, due\nto the need for understanding speech enriched with paralinguistic cues and\ngenerating emotional and expressive responses. However, the most powerful\nempathetic LSLMs are increasingly closed off, leaving the crucial details about\nthe architecture, data and development opaque to researchers. Given the\ncritical need for transparent research into the LSLMs and empathetic behavior,\nwe present OpenS2S, a fully open-source, transparent and end-to-end LSLM\ndesigned to enable empathetic speech interactions. Based on our empathetic\nspeech-to-text model BLSP-Emo, OpenS2S further employs a streaming interleaved\ndecoding architecture to achieve low-latency speech generation. To facilitate\nend-to-end training, OpenS2S incorporates an automated data construction\npipeline that synthesizes diverse, high-quality empathetic speech dialogues at\nlow cost. By leveraging large language models to generate empathetic content\nand controllable text-to-speech systems to introduce speaker and emotional\nvariation, we construct a scalable training corpus with rich paralinguistic\ndiversity and minimal human supervision. We release the fully open-source\nOpenS2S model, including the dataset, model weights, pre-training and\nfine-tuning codes, to empower the broader research community and accelerate\ninnovation in empathetic speech systems. The project webpage can be accessed at\nhttps://casia-lm.github.io/OpenS2S"}
{"id": "2409.01754", "pdf": "https://arxiv.org/pdf/2409.01754.pdf", "abs": "https://arxiv.org/abs/2409.01754", "title": "Empirical evidence of Large Language Model's influence on human spoken communication", "authors": ["Hiromu Yakura", "Ezequiel Lopez-Lopez", "Levin Brinkmann", "Ignacio Serna", "Prateek Gupta", "Ivan Soraperra", "Iyad Rahwan"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "From the invention of writing and the printing press, to television and\nsocial media, human history is punctuated by major innovations in communication\ntechnology, which fundamentally altered how ideas spread and reshaped our\nculture. Recent chatbots powered by generative artificial intelligence\nconstitute a novel medium that encodes cultural patterns in their neural\nrepresentations and disseminates them in conversations with hundreds of\nmillions of people. Understanding whether these patterns transmit into human\nlanguage, and ultimately shape human culture, is a fundamental question. While\nfully quantifying the causal impact of a chatbot like ChatGPT on human culture\nis very challenging, lexicographic shift in human spoken communication may\noffer an early indicator of such broad phenomenon. Here, we apply econometric\ncausal inference techniques to 740,249 hours of human discourse from 360,445\nYouTube academic talks and 771,591 conversational podcast episodes across\nmultiple disciplines. We detect a measurable and abrupt increase in the use of\nwords preferentially generated by ChatGPT, such as delve, comprehend, boast,\nswift, and meticulous, after its release. These findings suggest a scenario\nwhere machines, originally trained on human data and subsequently exhibiting\ntheir own cultural traits, can, in turn, measurably reshape human culture. This\nmarks the beginning of a closed cultural feedback loop in which cultural traits\ncirculate bidirectionally between humans and machines. Our results motivate\nfurther research into the evolution of human-machine culture, and raise\nconcerns over the erosion of linguistic and cultural diversity, and the risks\nof scalable manipulation."}
{"id": "2410.02892", "pdf": "https://arxiv.org/pdf/2410.02892.pdf", "abs": "https://arxiv.org/abs/2410.02892", "title": "The Role of Deductive and Inductive Reasoning in Large Language Models", "authors": ["Chengkun Cai", "Xu Zhao", "Haoliang Liu", "Zhongyu Jiang", "Tianfang Zhang", "Zongkai Wu", "Jenq-Neng Hwang", "Lei Li"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "4 figures, accept at ACL2025 Main", "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nreasoning tasks, yet their reliance on static prompt structures and limited\nadaptability to complex scenarios remains a significant challenge. In this\npaper, we propose the Deductive and InDuctive(DID) method, a novel framework\nthat enhances LLM reasoning by dynamically integrating both deductive and\ninductive reasoning approaches. Drawing from cognitive science principles, DID\nimplements a dual-metric complexity evaluation system that combines Littlestone\ndimension and information entropy to precisely assess task difficulty and guide\ndecomposition strategies. DID enables the model to progressively adapt its\nreasoning pathways based on problem complexity, mirroring human cognitive\nprocesses. We evaluate DID's effectiveness across multiple benchmarks,\nincluding the AIW and MR-GSM8K, as well as our custom Holiday Puzzle dataset\nfor temporal reasoning. Our results demonstrate significant improvements in\nreasoning quality and solution accuracy - achieving 70.3% accuracy on AIW\n(compared to 62.2% for Tree of Thought) while maintaining lower computational\ncosts. The success of DID in improving LLM performance while preserving\ncomputational efficiency suggests promising directions for developing more\ncognitively aligned and capable language models. Our work contributes a\ntheoretically grounded, input-centric approach to enhancing LLM reasoning\ncapabilities, offering an efficient alternative to traditional\noutput-exploration methods."}
{"id": "2410.06949", "pdf": "https://arxiv.org/pdf/2410.06949.pdf", "abs": "https://arxiv.org/abs/2410.06949", "title": "Towards Exception Safety Code Generation with Intermediate Representation Agents Framework", "authors": ["Xuanming Zhang", "Yuxuan Chen", "Yuan Yuan", "Minlie Huang"], "categories": ["cs.SE", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) often struggle with robust exception handling in\ngenerated code, leading to fragile programs that are prone to runtime errors.\nWe propose Seeker, a novel multi-agent framework that enforces exception safety\nin LLM generated code through an Intermediate Representation (IR) approach.\nSeeker decomposes exception handling into five specialized agents: Scanner,\nDetector, Predator, Ranker, and Handler that collaboratively analyze code,\ndetect fragile segments, retrieve best practice exception strategies, and\ninject robust handling code. We also introduce Common Exception Enumeration\n(CEE), a comprehensive knowledge base derived from official documentation,\ntechnical practices, and real world code, to standardize exception handling\nstrategies. Seeker also incorporates a Deep Retrieval-Augmented Generation\n(Deep RAG) algorithm to efficiently navigate the exception inheritance\nhierarchy, cutting down search overhead by 93% while improving accuracy in\nidentifying relevant exceptions. We evaluate Seeker on 15 open source Java\nprojects and multiple benchmarks. Seeker outperforms state of the art\nbaselines, improving exception handling precision by up to 37% and overall code\nrobustness by 38% as measured by expert code review. It significantly closes\nthe gap between LLM and human developers in exception management, achieving a\n28% success rate on real world issue fixes (SWE bench) versus 19% by prior\nmethods. Our framework preserves functional correctness of code while\nproactively handling errors, demonstrating a practical, generalizable solution\nfor safer code generation. In this paper, we discuss the novelty of using\nintermediate representation and multi-agent collaboration for exception\nhandling, and outline how Seeker can be extended to other programming languages\nand complex software engineering tasks, aligning LLM-generated code with\nindustrial standard."}
{"id": "2410.16327", "pdf": "https://arxiv.org/pdf/2410.16327.pdf", "abs": "https://arxiv.org/abs/2410.16327", "title": "Feint and Attack: Attention-Based Strategies for Jailbreaking and Protecting LLMs", "authors": ["Rui Pu", "Chaozhuo Li", "Rui Ha", "Zejian Chen", "Litian Zhang", "Zheng Liu", "Lirong Qiu", "Zaisheng Ye"], "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "Jailbreak attack can be used to access the vulnerabilities of Large Language\nModels (LLMs) by inducing LLMs to generate the harmful content. And the most\ncommon method of the attack is to construct semantically ambiguous prompts to\nconfuse and mislead the LLMs. To access the security and reveal the intrinsic\nrelation between the input prompt and the output for LLMs, the distribution of\nattention weight is introduced to analyze the underlying reasons. By using\nstatistical analysis methods, some novel metrics are defined to better describe\nthe distribution of attention weight, such as the Attention Intensity on\nSensitive Words (Attn_SensWords), the Attention-based Contextual Dependency\nScore (Attn_DepScore) and Attention Dispersion Entropy (Attn_Entropy). By\nleveraging the distinct characteristics of these metrics, the beam search\nalgorithm and inspired by the military strategy \"Feint and Attack\", an\neffective jailbreak attack strategy named as Attention-Based Attack (ABA) is\nproposed. In the ABA, nested attack prompts are employed to divert the\nattention distribution of the LLMs. In this manner, more harmless parts of the\ninput can be used to attract the attention of the LLMs. In addition, motivated\nby ABA, an effective defense strategy called as Attention-Based Defense (ABD)\nis also put forward. Compared with ABA, the ABD can be used to enhance the\nrobustness of LLMs by calibrating the attention distribution of the input\nprompt. Some comparative experiments have been given to demonstrate the\neffectiveness of ABA and ABD. Therefore, both ABA and ABD can be used to access\nthe security of the LLMs. The comparative experiment results also give a\nlogical explanation that the distribution of attention weight can bring great\ninfluence on the output for LLMs."}
{"id": "2412.20545", "pdf": "https://arxiv.org/pdf/2412.20545.pdf", "abs": "https://arxiv.org/abs/2412.20545", "title": "The Impact of Prompt Programming on Function-Level Code Generation", "authors": ["Ranim Khojah", "Francisco Gomes de Oliveira Neto", "Mazen Mohamad", "Philipp Leitner"], "categories": ["cs.SE", "cs.CL", "cs.HC", "cs.LG"], "comment": "Accepted at Transactions on Software Engineering (TSE).\n  CodePromptEval dataset and replication package on GitHub:\n  https://github.com/icetlab/CodePromptEval", "summary": "Large Language Models (LLMs) are increasingly used by software engineers for\ncode generation. However, limitations of LLMs such as irrelevant or incorrect\ncode have highlighted the need for prompt programming (or prompt engineering)\nwhere engineers apply specific prompt techniques (e.g., chain-of-thought or\ninput-output examples) to improve the generated code. While some prompt\ntechniques have been studied, the impact of different techniques -- and their\ninteractions -- on code generation is still not fully understood. In this\nstudy, we introduce CodePromptEval, a dataset of 7072 prompts designed to\nevaluate five prompt techniques (few-shot, persona, chain-of-thought, function\nsignature, list of packages) and their effect on the correctness, similarity,\nand quality of complete functions generated by three LLMs (GPT-4o, Llama3, and\nMistral). Our findings show that while certain prompt techniques significantly\ninfluence the generated code, combining multiple techniques does not\nnecessarily improve the outcome. Additionally, we observed a trade-off between\ncorrectness and quality when using prompt techniques. Our dataset and\nreplication package enable future research on improving LLM-generated code and\nevaluating new prompt techniques."}
{"id": "2501.01366", "pdf": "https://arxiv.org/pdf/2501.01366.pdf", "abs": "https://arxiv.org/abs/2501.01366", "title": "ViGiL3D: A Linguistically Diverse Dataset for 3D Visual Grounding", "authors": ["Austin T. Wang", "ZeMing Gong", "Angel X. Chang"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "24 pages with 8 figures and 14 tables; updated for ACL 2025\n  camera-ready with additional discussion and figures", "summary": "3D visual grounding (3DVG) involves localizing entities in a 3D scene\nreferred to by natural language text. Such models are useful for embodied AI\nand scene retrieval applications, which involve searching for objects or\npatterns using natural language descriptions. While recent works have focused\non LLM-based scaling of 3DVG datasets, these datasets do not capture the full\nrange of potential prompts which could be specified in the English language. To\nensure that we are scaling up and testing against a useful and representative\nset of prompts, we propose a framework for linguistically analyzing 3DVG\nprompts and introduce Visual Grounding with Diverse Language in 3D (ViGiL3D), a\ndiagnostic dataset for evaluating visual grounding methods against a diverse\nset of language patterns. We evaluate existing open-vocabulary 3DVG methods to\ndemonstrate that these methods are not yet proficient in understanding and\nidentifying the targets of more challenging, out-of-distribution prompts,\ntoward real-world applications."}
{"id": "2501.01370", "pdf": "https://arxiv.org/pdf/2501.01370.pdf", "abs": "https://arxiv.org/abs/2501.01370", "title": "Embedding-Based Approaches to Hyperpartisan News Detection", "authors": ["Karthik Mohan"], "categories": ["cs.LG", "cs.CL"], "comment": "Updated version reflecting sole authorship. All coauthor\n  contributions have been removed. Experimental corrections and analysis\n  updates were introduced in the original version and are retained here as part\n  of the submitter's independent work, along with expanded experiments by the\n  submitter", "summary": "In this report, I describe the systems in which the objective is to determine\nwhether a given news article could be considered as hyperpartisan.\nHyperpartisan news takes an extremely polarized political standpoint with an\nintention of creating political divide among the public. Several approaches,\nincluding n-grams, sentiment analysis, as well as sentence and document\nrepresentations using pre-tained ELMo models were used. The best system is\nusing LLMs for embedding generation achieving an accuracy of around 92% over\nthe previously best system using pre-trained ELMo with Bidirectional LSTM which\nachieved an accuracy of around 83% through 10-fold cross-validation."}
{"id": "2501.18099", "pdf": "https://arxiv.org/pdf/2501.18099.pdf", "abs": "https://arxiv.org/abs/2501.18099", "title": "Learning to Plan & Reason for Evaluation with Thinking-LLM-as-a-Judge", "authors": ["Swarnadeep Saha", "Xian Li", "Marjan Ghazvininejad", "Jason Weston", "Tianlu Wang"], "categories": ["cs.AI", "cs.CL"], "comment": "ICML 2025", "summary": "LLM-as-a-Judge models generate chain-of-thought (CoT) sequences intended to\ncapture the step-bystep reasoning process that underlies the final evaluation\nof a response. However, due to the lack of human annotated CoTs for evaluation,\nthe required components and structure of effective reasoning traces remain\nunderstudied. Consequently, previous approaches often (1) constrain reasoning\ntraces to hand-designed components, such as a list of criteria, reference\nanswers, or verification questions and (2) structure them such that planning is\nintertwined with the reasoning for evaluation. In this work, we propose\nEvalPlanner, a preference optimization algorithm for Thinking-LLM-as-a-Judge\nthat first generates an unconstrained evaluation plan, followed by its\nexecution, and then the final judgment. In a self-training loop, EvalPlanner\niteratively optimizes over synthetically constructed evaluation plans and\nexecutions, leading to better final verdicts. Our method achieves a new\nstate-of-the-art performance for generative reward models on RewardBench (with\na score of 93.9), despite being trained on fewer amount of, and synthetically\ngenerated, preference pairs. Additional experiments on other benchmarks like\nRM-Bench, JudgeBench, and FollowBenchEval further highlight the utility of both\nplanning and reasoning for building robust LLM-as-a-Judge reasoning models."}
{"id": "2502.00406", "pdf": "https://arxiv.org/pdf/2502.00406.pdf", "abs": "https://arxiv.org/abs/2502.00406", "title": "Agents Are All You Need for LLM Unlearning", "authors": ["Debdeep Sanyal", "Murari Mandal"], "categories": ["cs.AI", "cs.CL"], "comment": "Accepted to COLM 2025", "summary": "Information removal or suppression in large language models (LLMs) is a\ndesired functionality, useful in AI regulation, legal compliance, safety, and\nprivacy. LLM unlearning methods aim to remove information on demand from LLMs.\nCurrent LLM unlearning methods struggle to balance the unlearning efficacy and\nutility due to the competing nature of these objectives. Keeping the unlearning\nprocess computationally feasible without assuming access to the model weights\nis an overlooked area. In this work we show that \\textit{agents might be all we\nneed for effective and practical inference-time LLM unlearning}. We present the\nfirst agentic LLM unlearning (\\texttt{ALU}) method, a multi-agent,\nretrain-free, model-agnostic approach to LLM unlearning that achieves effective\nunlearning while preserving the utility. Our \\texttt{ALU} framework unlearns by\ninvolving multiple LLM agents, each designed for a specific step in the\nunlearning process, without the need to update model weights for any of the\nagents in the framework. Users can easily request any set of unlearning\ninstances in any sequence, and \\texttt{ALU} seamlessly adapts in real time.\nThis is facilitated without requiring any changes in the underlying LLM model.\nThrough extensive experiments on established benchmarks (TOFU, WMDP, WPU) and\njailbreaking techniques (many shot, target masking, other languages), we\ndemonstrate that \\texttt{ALU} consistently stands out as the most robust\ninference-time LLM unlearning framework among current state-of-the-art methods\nwhile incurring time cost that remains effectively constant regardless of the\nnumber of unlearning targets. We further highlight \\texttt{ALU}'s superior\nperformance compared to existing methods when evaluated at scale. Specifically,\n\\texttt{ALU} is assessed on up to 1000 unlearning targets, exceeding the\nevaluation scope of all previously proposed LLM unlearning methods."}
{"id": "2502.12961", "pdf": "https://arxiv.org/pdf/2502.12961.pdf", "abs": "https://arxiv.org/abs/2502.12961", "title": "Adaptive Tool Use in Large Language Models with Meta-Cognition Trigger", "authors": ["Wenjun Li", "Dexun Li", "Kuicai Dong", "Cong Zhang", "Hao Zhang", "Weiwen Liu", "Yasheng Wang", "Ruiming Tang", "Yong Liu"], "categories": ["cs.AI", "cs.CL"], "comment": "25 pages, camera ready version for ACL-2025", "summary": "Large language models (LLMs) have shown remarkable emergent capabilities,\ntransforming the execution of functional tasks by leveraging external tools for\ncomplex problems that require specialized processing or up-to-date data. While\nexisting research expands LLMs access to diverse tools (e.g., program\ninterpreters, search engines, calculators), the necessity of using these tools\nis often overlooked, leading to indiscriminate tool invocation. This naive\napproach raises two key issues: increased latency due to unnecessary tool\ncalls, and potential errors resulting from faulty interactions with external\ntools. In this paper, we introduce meta-cognition as a proxy for LLMs\nself-assessment of their capabilities, reflecting the model's awareness of its\nown limitations. Based on this, we propose MeCo, an adaptive decision-making\nstrategy for external tool use. MeCo quantifies metacognitive scores by\ncapturing high-level cognitive signals in the representation space, guiding\nwhen to invoke tools. Notably, MeCo is fine-tuning-free and incurs minimal\ncost. Experiments across multiple backbone models and benchmarks show that MeCo\nreliably detects LLMs' internal cognitive signals and significantly improves\ntool-use decision-making."}
{"id": "2502.17380", "pdf": "https://arxiv.org/pdf/2502.17380.pdf", "abs": "https://arxiv.org/abs/2502.17380", "title": "Low-Rank and Sparse Model Merging for Multi-Lingual Speech Recognition and Translation", "authors": ["Qiuming Zhao", "Guangzhi Sun", "Chao Zhang"], "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": "13 pages", "summary": "Language diversity presents a significant challenge in speech-to-text (S2T)\ntasks, such as automatic speech recognition and translation. Traditional\nmulti-lingual multi-task training approaches aim to address this by jointly\noptimising multiple speech recognition and translation tasks across various\nlanguages. While models like Whisper, built on these strategies, demonstrate\nstrong performance, they still face issues of high computational cost, language\ninterference, suboptimal training configurations, and limited extensibility. To\novercome these challenges, we introduce LoRS-Merging (low-rank and sparse model\nmerging), a novel technique designed to efficiently integrate models trained on\ndifferent languages or tasks while preserving performance and reducing\ncomputational overhead. LoRS-Merging combines low-rank and sparse pruning to\nretain essential structures while eliminating redundant parameters, mitigating\nlanguage interference, and enhancing extensibility. Experimental results across\n10 languages demonstrate that LoRS-Merging significantly outperforms\nmulti-lingual multi-task training, sequential training, and other merging\nmethods, achieving over 20% improvement in normalised performance. Our findings\nsuggest that model merging, particularly LoRS-Merging, is a scalable and\neffective complement to traditional multi-lingual training strategies for S2T\napplications."}
{"id": "2502.18116", "pdf": "https://arxiv.org/pdf/2502.18116.pdf", "abs": "https://arxiv.org/abs/2502.18116", "title": "Bayesian Optimization for Controlled Image Editing via LLMs", "authors": ["Chengkun Cai", "Haoliang Liu", "Xu Zhao", "Zhongyu Jiang", "Tianfang Zhang", "Zongkai Wu", "John Lee", "Jenq-Neng Hwang", "Lei Li"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "8 figures, accept at ACL2025 Findings", "summary": "In the rapidly evolving field of image generation, achieving precise control\nover generated content and maintaining semantic consistency remain significant\nlimitations, particularly concerning grounding techniques and the necessity for\nmodel fine-tuning. To address these challenges, we propose BayesGenie, an\noff-the-shelf approach that integrates Large Language Models (LLMs) with\nBayesian Optimization to facilitate precise and user-friendly image editing.\nOur method enables users to modify images through natural language descriptions\nwithout manual area marking, while preserving the original image's semantic\nintegrity. Unlike existing techniques that require extensive pre-training or\nfine-tuning, our approach demonstrates remarkable adaptability across various\nLLMs through its model-agnostic design. BayesGenie employs an adapted Bayesian\noptimization strategy to automatically refine the inference process parameters,\nachieving high-precision image editing with minimal user intervention. Through\nextensive experiments across diverse scenarios, we demonstrate that our\nframework significantly outperforms existing methods in both editing accuracy\nand semantic preservation, as validated using different LLMs including Claude3\nand GPT-4."}
{"id": "2503.13575", "pdf": "https://arxiv.org/pdf/2503.13575.pdf", "abs": "https://arxiv.org/abs/2503.13575", "title": "Analytic Subspace Routing: How Recursive Least Squares Works in Continual Learning of Large Language Model", "authors": ["Kai Tong", "Kang Pan", "Xiao Zhang", "Erli Meng", "Run He", "Yawen Cui", "Nuoyan Guo", "Huiping Zhuang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "11 pages, 4 figures", "summary": "Large Language Models (LLMs) possess encompassing capabilities that can\nprocess diverse language-related tasks. However, finetuning on LLMs will\ndiminish this general skills and continual finetuning will further cause severe\ndegradation on accumulated knowledge. Recently, Continual Learning (CL) in\nLarge Language Models (LLMs) arises which aims to continually adapt the LLMs to\nnew tasks while maintaining previously learned knowledge and inheriting general\nskills. Existing techniques either leverage previous data to replay, leading to\nextra computational costs, or utilize a single parameter-efficient module to\nlearn the downstream task, constraining new knowledge absorption with\ninterference between different tasks. Toward these issues, this paper proposes\nAnalytic Subspace Routing(ASR) to address these challenges. For each task, we\nisolate the learning within a subspace of deep layers' features via low-rank\nadaptation, eliminating knowledge interference between different tasks.\nAdditionally, we propose an analytic routing mechanism to properly utilize\nknowledge learned in different subspaces. Our approach employs Recursive Least\nSquares to train a multi-task router model, allowing the router to dynamically\nadapt to incoming data without requiring access to historical data. Also, the\nrouter effectively assigns the current task to an appropriate subspace and has\na non-forgetting property of previously learned tasks with a solid theoretical\nguarantee. Experimental results demonstrate that our method achieves\nnear-perfect retention of prior knowledge while seamlessly integrating new\ninformation, effectively overcoming the core limitations of existing methods.\nOur code will be released after acceptance."}
{"id": "2503.22968", "pdf": "https://arxiv.org/pdf/2503.22968.pdf", "abs": "https://arxiv.org/abs/2503.22968", "title": "Redefining Evaluation Standards: A Unified Framework for Evaluating the Korean Capabilities of Language Models", "authors": ["Hanwool Lee", "Dasol Choi", "Sooyong Kim", "Ilgyun Jung", "Sangwon Baek", "Guijin Son", "Inseon Hwang", "Naeun Lee", "Seunghyeok Hong"], "categories": ["cs.CE", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent advancements in Korean large language models (LLMs) have driven\nnumerous benchmarks and evaluation methods, yet inconsistent protocols cause up\nto 10 p.p performance gaps across institutions. Overcoming these\nreproducibility gaps does not mean enforcing a one-size-fits-all evaluation.\nRather, effective benchmarking requires diverse experimental approaches and a\nframework robust enough to support them. To this end, we introduce HRET (Haerae\nEvaluation Toolkit), an open-source, registry-based framework that unifies\nKorean LLM assessment. HRET integrates major Korean benchmarks, multiple\ninference backends, and multi-method evaluation, with language consistency\nenforcement to ensure genuine Korean outputs. Its modular registry design also\nenables rapid incorporation of new datasets, methods, and backends, ensuring\nthe toolkit adapts to evolving research needs. Beyond standard accuracy\nmetrics, HRET incorporates Korean-focused output analyses-morphology-aware\nType-Token Ratio (TTR) for evaluating lexical diversity and systematic\nkeyword-omission detection for identifying missing concepts-to provide\ndiagnostic insights into language-specific behaviors. These targeted analyses\nhelp researchers pinpoint morphological and semantic shortcomings in model\noutputs, guiding focused improvements in Korean LLM development."}
{"id": "2504.11364", "pdf": "https://arxiv.org/pdf/2504.11364.pdf", "abs": "https://arxiv.org/abs/2504.11364", "title": "Offline Learning and Forgetting for Reasoning with Large Language Models", "authors": ["Tianwei Ni", "Allen Nie", "Sapana Chaudhary", "Yao Liu", "Huzefa Rangwala", "Rasool Fakoor"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Code: https://github.com/twni2016/llm-reasoning-uft", "summary": "Leveraging inference-time search in large language models has proven\neffective in further enhancing a trained model's capability to solve complex\nmathematical and reasoning problems. However, this approach significantly\nincreases computational costs and inference time, as the model must generate\nand evaluate multiple candidate solutions to identify a viable reasoning path.\nTo address this, we propose an effective approach that integrates search\ncapabilities directly into the model by fine-tuning it on unpaired successful\n(learning) and failed reasoning paths (forgetting) derived from diverse search\nmethods. A key challenge we identify is that naive fine-tuning can degrade the\nmodel's search capability; we show this can be mitigated with a smaller\nlearning rate. Extensive experiments on the challenging Game-of-24 and\nCountdown reasoning benchmarks show that, replacing CoT-generated data with\nsearch-generated data for offline fine-tuning improves success rates by around\n23% over inference-time search baselines, while reducing inference time by\n180$\\times$. On top of this, our learning and forgetting objective consistently\noutperforms both supervised fine-tuning and preference-based methods."}
{"id": "2505.11079", "pdf": "https://arxiv.org/pdf/2505.11079.pdf", "abs": "https://arxiv.org/abs/2505.11079", "title": "ALLM4ADD: Unlocking the Capabilities of Audio Large Language Models for Audio Deepfake Detection", "authors": ["Hao Gu", "Jiangyan Yi", "Chenglong Wang", "Jianhua Tao", "Zheng Lian", "Jiayi He", "Yong Ren", "Yujie Chen", "Zhengqi Wen"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Accepted by ACMMM 2025", "summary": "Audio deepfake detection (ADD) has grown increasingly important due to the\nrise of high-fidelity audio generative models and their potential for misuse.\nGiven that audio large language models (ALLMs) have made significant progress\nin various audio processing tasks, a heuristic question arises: \\textit{Can\nALLMs be leveraged to solve ADD?}. In this paper, we first conduct a\ncomprehensive zero-shot evaluation of ALLMs on ADD, revealing their\nineffectiveness. To this end, we propose ALLM4ADD, an ALLM-driven framework for\nADD. Specifically, we reformulate ADD task as an audio question answering\nproblem, prompting the model with the question: ``Is this audio fake or\nreal?''. We then perform supervised fine-tuning to enable the ALLM to assess\nthe authenticity of query audio. Extensive experiments are conducted to\ndemonstrate that our ALLM-based method can achieve superior performance in fake\naudio detection, particularly in data-scarce scenarios. As a pioneering study,\nwe anticipate that this work will inspire the research community to leverage\nALLMs to develop more effective ADD systems. Code is available at\nhttps://github.com/ucas-hao/qwen_audio_for_add.git"}
{"id": "2506.06382", "pdf": "https://arxiv.org/pdf/2506.06382.pdf", "abs": "https://arxiv.org/abs/2506.06382", "title": "On the Fundamental Impossibility of Hallucination Control in Large Language Models", "authors": ["Michał P. Karpowicz"], "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.GT", "cs.LG"], "comment": "transformer example extended, discussion and speculation section\n  added", "summary": "We prove that perfect hallucination control in large language models is\nmathematically impossible. No LLM inference mechanism can simultaneously\nachieve truthful response generation, semantic information conservation,\nrelevant knowledge revelation, and knowledge-constrained optimality. This\nimpossibility is fundamental, arising from the mathematical structure of\ninformation aggregation itself rather than engineering limitations. The proof\nspans three mathematical frameworks: auction theory, proper scoring theory for\nprobabilistic predictions, and log-sum-exp analysis for transformer\narchitectures. In each setting, we demonstrate that information aggregation\ncreates unavoidable violations of conservation principles. The Jensen gap in\ntransformer probability aggregation provides a direct measure of this\nimpossibility. These results reframe hallucination from an engineering bug to\nan inevitable mathematical feature of distributed intelligence. There are\nfundamental trade-offs between truthfulness, knowledge utilization, and\nresponse completeness, providing principled foundations for managing rather\nthan eliminating hallucination. This work reveals deep connections between\nneural network inference, philosophy of knowledge and reasoning, and classical\nresults in game theory and information theory, opening new research directions\nfor developing beneficial AI systems within mathematical constraints."}
{"id": "2507.04173", "pdf": "https://arxiv.org/pdf/2507.04173.pdf", "abs": "https://arxiv.org/abs/2507.04173", "title": "Efficient Detection of Intermittent Job Failures Using Few-Shot Learning", "authors": ["Henri Aïdasso", "Francis Bordeleau", "Ali Tizghadam"], "categories": ["cs.SE", "cs.CL", "cs.LG"], "comment": "Accepted at the 41st International Conference on Software Maintenance\n  and Evolution - ICSME 2025 (Industry Track); 12 pages; typos corrected", "summary": "One of the main challenges developers face in the use of continuous\nintegration (CI) and deployment pipelines is the occurrence of intermittent job\nfailures, which result from unexpected non-deterministic issues (e.g., flaky\ntests or infrastructure problems) rather than regular code-related errors such\nas bugs. Prior studies developed machine learning (ML) models trained on large\ndatasets of job logs to classify job failures as either intermittent or\nregular. As an alternative to costly manual labeling of large datasets, the\nstate-of-the-art (SOTA) approach leveraged a heuristic based on\nnon-deterministic job reruns. However, this method mislabels intermittent job\nfailures as regular in contexts where rerunning suspicious job failures is not\nan explicit policy, and therefore limits the SOTA's performance in practice. In\nfact, our manual analysis of 2,125 job failures from 5 industrial and 1\nopen-source projects reveals that, on average, 32% of intermittent job failures\nare mislabeled as regular. To address these limitations, this paper introduces\na novel approach to intermittent job failure detection using few-shot learning\n(FSL). Specifically, we fine-tune a small language model using a few number of\nmanually labeled log examples to generate rich embeddings, which are then used\nto train an ML classifier. Our FSL-based approach achieves 70-88% F1-score with\nonly 12 shots in all projects, outperforming the SOTA, which proved ineffective\n(34-52% F1-score) in 4 projects. Overall, this study underlines the importance\nof data quality over quantity and provides a more efficient and practical\nframework for the detection of intermittent job failures in organizations."}
{"id": "2507.04554", "pdf": "https://arxiv.org/pdf/2507.04554.pdf", "abs": "https://arxiv.org/abs/2507.04554", "title": "Self-supervised learning of speech representations with Dutch archival data", "authors": ["Nik Vaessen", "Roeland Ordelman", "David A. van Leeuwen"], "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "comment": "accepted at interspeech 2025", "summary": "This paper explores the use of Dutch archival television broadcast data for\nself-supervised learning of speech foundation models, specifically wav2vec 2.0.\nWe first study data quality assumptions for pre-training, and show how music,\nnoise and speaker overlap affect SSL convergence and downstream fine-tuning\nperformance. Secondly, we explore effectively pre-processing strategies to\nconvert the noisy broadcast dataset into a qualitative dataset for\npre-training, by using Whisper and WhisperX. Thirdly, we compare mono-lingual\nand multi-lingual pre-training with equivalent amounts of data, and show that\nmono-lingual pre-training is more robust to out-of-domain data. Lastly, we\nachieve a state-of-the-art LARGE wav2vec 2.0 model for the Dutch language, by a\ncontinuation of pre-training a wav2vec 2.0 XLS-R model checkpoint with our 55k\nhour archival dataset."}
{"id": "2507.05006", "pdf": "https://arxiv.org/pdf/2507.05006.pdf", "abs": "https://arxiv.org/abs/2507.05006", "title": "Do We Really Need Specialization? Evaluating Generalist Text Embeddings for Zero-Shot Recommendation and Search", "authors": ["Matteo Attimonelli", "Alessandro De Bellis", "Claudio Pomo", "Dietmar Jannach", "Eugenio Di Sciascio", "Tommaso Di Noia"], "categories": ["cs.IR", "cs.CL"], "comment": "Accept as Short Paper at RecSys 2025", "summary": "Pre-trained language models (PLMs) are widely used to derive semantic\nrepresentations from item metadata in recommendation and search. In sequential\nrecommendation, PLMs enhance ID-based embeddings through textual metadata,\nwhile in product search, they align item characteristics with user intent.\nRecent studies suggest task and domain-specific fine-tuning are needed to\nimprove representational power. This paper challenges this assumption, showing\nthat Generalist Text Embedding Models (GTEs), pre-trained on large-scale\ncorpora, can guarantee strong zero-shot performance without specialized\nadaptation. Our experiments demonstrate that GTEs outperform traditional and\nfine-tuned models in both sequential recommendation and product search. We\nattribute this to a superior representational power, as they distribute\nfeatures more evenly across the embedding space. Finally, we show that\ncompressing embedding dimensions by focusing on the most informative directions\n(e.g., via PCA) effectively reduces noise and improves the performance of\nspecialized models. To ensure reproducibility, we provide our repository at\nhttps://split.to/gte4ps."}
{"id": "2507.05201", "pdf": "https://arxiv.org/pdf/2507.05201.pdf", "abs": "https://arxiv.org/abs/2507.05201", "title": "MedGemma Technical Report", "authors": ["Andrew Sellergren", "Sahar Kazemzadeh", "Tiam Jaroensri", "Atilla Kiraly", "Madeleine Traverse", "Timo Kohlberger", "Shawn Xu", "Fayaz Jamil", "Cían Hughes", "Charles Lau", "Justin Chen", "Fereshteh Mahvar", "Liron Yatziv", "Tiffany Chen", "Bram Sterling", "Stefanie Anna Baby", "Susanna Maria Baby", "Jeremy Lai", "Samuel Schmidgall", "Lu Yang", "Kejia Chen", "Per Bjornsson", "Shashir Reddy", "Ryan Brush", "Kenneth Philbrick", "Howard Hu", "Howard Yang", "Richa Tiwari", "Sunny Jansen", "Preeti Singh", "Yun Liu", "Shekoofeh Azizi", "Aishwarya Kamath", "Johan Ferret", "Shreya Pathak", "Nino Vieillard", "Ramona Merhej", "Sarah Perrin", "Tatiana Matejovicova", "Alexandre Ramé", "Morgane Riviere", "Louis Rouillard", "Thomas Mesnard", "Geoffrey Cideron", "Jean-bastien Grill", "Sabela Ramos", "Edouard Yvinec", "Michelle Casbon", "Elena Buchatskaya", "Jean-Baptiste Alayrac", "Dmitry Lepikhin", "Vlad Feinberg", "Sebastian Borgeaud", "Alek Andreev", "Cassidy Hardin", "Robert Dadashi", "Léonard Hussenot", "Armand Joulin", "Olivier Bachem", "Yossi Matias", "Katherine Chou", "Avinatan Hassidim", "Kavi Goel", "Clement Farabet", "Joelle Barral", "Tris Warkentin", "Jonathon Shlens", "David Fleet", "Victor Cotruta", "Omar Sanseviero", "Gus Martins", "Phoebe Kirk", "Anand Rao", "Shravya Shetty", "David F. Steiner", "Can Kirmizibayrak", "Rory Pilgrim", "Daniel Golden", "Lin Yang"], "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Artificial intelligence (AI) has significant potential in healthcare\napplications, but its training and deployment faces challenges due to\nhealthcare's diverse data, complex tasks, and the need to preserve privacy.\nFoundation models that perform well on medical tasks and require less\ntask-specific tuning data are critical to accelerate the development of\nhealthcare AI applications. We introduce MedGemma, a collection of medical\nvision-language foundation models based on Gemma 3 4B and 27B. MedGemma\ndemonstrates advanced medical understanding and reasoning on images and text,\nsignificantly exceeding the performance of similar-sized generative models and\napproaching the performance of task-specific models, while maintaining the\ngeneral capabilities of the Gemma 3 base models. For out-of-distribution tasks,\nMedGemma achieves 2.6-10% improvement on medical multimodal question answering,\n15.5-18.1% improvement on chest X-ray finding classification, and 10.8%\nimprovement on agentic evaluations compared to the base models. Fine-tuning\nMedGemma further improves performance in subdomains, reducing errors in\nelectronic health record information retrieval by 50% and reaching comparable\nperformance to existing specialized state-of-the-art methods for pneumothorax\nclassification and histopathology patch classification. We additionally\nintroduce MedSigLIP, a medically-tuned vision encoder derived from SigLIP.\nMedSigLIP powers the visual understanding capabilities of MedGemma and as an\nencoder achieves comparable or better performance than specialized medical\nimage encoders. Taken together, the MedGemma collection provides a strong\nfoundation of medical image and text capabilities, with potential to\nsignificantly accelerate medical research and development of downstream\napplications. The MedGemma collection, including tutorials and model weights,\ncan be found at https://goo.gle/medgemma."}
{"id": "2507.05241", "pdf": "https://arxiv.org/pdf/2507.05241.pdf", "abs": "https://arxiv.org/abs/2507.05241", "title": "SciMaster: Towards General-Purpose Scientific AI Agents, Part I. X-Master as Foundation: Can We Lead on Humanity's Last Exam?", "authors": ["Jingyi Chai", "Shuo Tang", "Rui Ye", "Yuwen Du", "Xinyu Zhu", "Mengcheng Zhou", "Yanfeng Wang", "Weinan E", "Yuzhi Zhang", "Linfeng Zhang", "Siheng Chen"], "categories": ["cs.AI", "cs.CL"], "comment": "15 pages, 10 figures", "summary": "The rapid advancements of AI agents have ignited the long-held ambition of\nleveraging them to accelerate scientific discovery. Achieving this goal\nrequires a deep understanding of the frontiers of human knowledge. As such,\nHumanity's Last Exam (HLE) provides an exceptionally challenging touchstone for\nevaluating scientific AI agents. In this work, we aim to construct the\nfoundational architecture for general-purpose agents and validate the\ncapabilities through leading performance on HLE. To achieve this, we introduce\nX-Master, a tool-augmented reasoning agent designed to emulate human\nresearchers by interacting flexibly with external tools during its reasoning\nprocess. This agent, guided by the conceptualization of code as an interaction\nlanguage, can flexibly leverage built-in Python libraries and our customized\ntools to augment the reasoning. We further scale its capabilities through\nX-Masters, a scattered-and-stacked agentic workflow that systematically\nenhances breadth and depth of reasoning. Our open-source solution, X-Masters,\nsets a new state-of-the-art record on HLE with a score of 32.1%, surpassing\nOpenAI's and Google's Deep Research (26.6% and 26.9%) and becoming the first to\nexceed the 30% threshold. This work allows us to gain a deeper understanding of\ncomplex task-solving and accumulates valuable experience that can inform future\nadvancements, guiding subsequent model training."}
