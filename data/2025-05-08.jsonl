{"id": "2505.03807", "pdf": "https://arxiv.org/pdf/2505.03807.pdf", "abs": "https://arxiv.org/abs/2505.03807", "title": "Facilitating Video Story Interaction with Multi-Agent Collaborative System", "authors": ["Yiwen Zhang", "Jianing Hao", "Zhan Wang", "Hongling Sheng", "Wei Zeng"], "categories": ["cs.HC", "cs.AI", "cs.CV", "cs.MA"], "comment": "Prepared and submitted in 2024", "summary": "Video story interaction enables viewers to engage with and explore narrative\ncontent for personalized experiences. However, existing methods are limited to\nuser selection, specially designed narratives, and lack customization. To\naddress this, we propose an interactive system based on user intent. Our system\nuses a Vision Language Model (VLM) to enable machines to understand video\nstories, combining Retrieval-Augmented Generation (RAG) and a Multi-Agent\nSystem (MAS) to create evolving characters and scene experiences. It includes\nthree stages: 1) Video story processing, utilizing VLM and prior knowledge to\nsimulate human understanding of stories across three modalities. 2) Multi-space\nchat, creating growth-oriented characters through MAS interactions based on\nuser queries and story stages. 3) Scene customization, expanding and\nvisualizing various story scenes mentioned in dialogue. Applied to the Harry\nPotter series, our study shows the system effectively portrays emergent\ncharacter social behavior and growth, enhancing the interactive experience in\nthe video story world."}
{"id": "2505.03867", "pdf": "https://arxiv.org/pdf/2505.03867.pdf", "abs": "https://arxiv.org/abs/2505.03867", "title": "Scratch Copilot: Supporting Youth Creative Coding with AI", "authors": ["Stefania Druga", "Amy J. Ko"], "categories": ["cs.HC", "cs.AI"], "comment": "5 figures, 14 pages", "summary": "Creative coding platforms like Scratch have democratized programming for\nchildren, yet translating imaginative ideas into functional code remains a\nsignificant hurdle for many young learners. While AI copilots assist adult\nprogrammers, few tools target children in block-based environments. Building on\nprior research \\cite{druga_how_2021,druga2023ai, druga2023scratch}, we present\nCognimates Scratch Copilot: an AI-powered assistant integrated into a\nScratch-like environment, providing real-time support for ideation, code\ngeneration, debugging, and asset creation. This paper details the system\narchitecture and findings from an exploratory qualitative evaluation with 18\ninternational children (ages 7--12). Our analysis reveals how the AI Copilot\nsupported key creative coding processes, particularly aiding ideation and\ndebugging. Crucially, it also highlights how children actively negotiated the\nuse of AI, demonstrating strong agency by adapting or rejecting suggestions to\nmaintain creative control. Interactions surfaced design tensions between\nproviding helpful scaffolding and fostering independent problem-solving, as\nwell as learning opportunities arising from navigating AI limitations and\nerrors. Findings indicate Cognimates Scratch Copilot's potential to enhance\ncreative self-efficacy and engagement. Based on these insights, we propose\ninitial design guidelines for AI coding assistants that prioritize youth agency\nand critical interaction alongside supportive scaffolding."}
{"id": "2505.04184", "pdf": "https://arxiv.org/pdf/2505.04184.pdf", "abs": "https://arxiv.org/abs/2505.04184", "title": "State-of-the-Art HCI for Dementia Care: A Scoping Review of Recent Technological Advances", "authors": ["Yong Ma", "Yuchong Zhang", "Oda Elise Nordberg", "Arvid Rongve", "Miroslav Bachinski", "Morten Fjeld"], "categories": ["cs.HC"], "comment": "19 pages, 4 figures, conference", "summary": "Dementia significantly impacts cognitive, behavioral, and functional\nabilities, creating challenges for both individuals and caregivers. Recent\nadvancements in HCI have introduced innovative technological solutions to\nsupport people with dementia (PwD) and their caregivers. This scoping review\nsystematically examines 32 recent publications from leading digital libraries,\ncategorizing technological interventions into four key domains: Assistive and\nSmart Technology for Daily Life, Social Interaction and Communication,\nWell-being and Psychological Support, and Caregiver Support and Training. Our\nanalysis highlights how emerging technologies are transforming dementia care.\nThese technologies enhance quality of life by promoting independence, fostering\nsocial engagement, and providing emotional and cognitive support. However, the\nreview also identifies critical gaps, particularly in addressing the needs of\nindividuals with early-stage dementia and the lack of individualized support\nmechanisms. By emphasizing user-centered design, accessibility, and ethical\nconsiderations, this paper offers a structured roadmap for future research and\npractice in dementia care. It bridges the gap between technological innovation\nand the real-world needs of PwD and their caregivers, providing valuable\ninsights for researchers, practitioners, and policymakers. This review not only\nsynthesizes current advancements but also sets the stage for future HCI-driven\ninnovations in dementia care, aiming to improve outcomes for an aging global\npopulation."}
{"id": "2505.04210", "pdf": "https://arxiv.org/pdf/2505.04210.pdf", "abs": "https://arxiv.org/abs/2505.04210", "title": "Sick of being driven? -- Prevalence and modulating factors of carsickness in the European population in context of automated driving", "authors": ["Myriam Metzulat", "Barbara Metz", "Aaron Edelmann", "Alexandra Neukum", "Wilfried Kunde"], "categories": ["cs.HC"], "comment": null, "summary": "As in automated driving the driver becomes a passenger, carsickness might\nreduce comfort for susceptible individuals. Insights in the prevalence of\ncarsickness and its modulating factors are considered useful for the\ndevelopment of automated vehicles to mitigate or prevent its occurrence. An\nonline survey was conducted with N = 3999 participants in Spain, Sweden,\nPoland, and Germany. 30% of participants reported to have already experienced\ncarsickness as adult. The frequency of carsickness was modulated not only by\ndemographic factors (country, gender, age), but also by frequency of being a\npassenger, type of non-driving related task, road type, and the seating\nposition in car. Furthermore, the efficiency of applied countermeasures,\ntemporal aspects of carsickness development, as well as the relation of\ncarsickness with the acceptability of automated driving and the effect on\nsubjective fitness to drive was investigated. The results are discussed with\nfocus on automated driving."}
{"id": "2505.03788", "pdf": "https://arxiv.org/pdf/2505.03788.pdf", "abs": "https://arxiv.org/abs/2505.03788", "title": "Calibrating Uncertainty Quantification of Multi-Modal LLMs using Grounding", "authors": ["Trilok Padhi", "Ramneet Kaur", "Adam D. Cobb", "Manoj Acharya", "Anirban Roy", "Colin Samplawski", "Brian Matejek", "Alexander M. Berenbeim", "Nathaniel D. Bastian", "Susmit Jha"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "We introduce a novel approach for calibrating uncertainty quantification (UQ)\ntailored for multi-modal large language models (LLMs). Existing\nstate-of-the-art UQ methods rely on consistency among multiple responses\ngenerated by the LLM on an input query under diverse settings. However, these\napproaches often report higher confidence in scenarios where the LLM is\nconsistently incorrect. This leads to a poorly calibrated confidence with\nrespect to accuracy. To address this, we leverage cross-modal consistency in\naddition to self-consistency to improve the calibration of the multi-modal\nmodels. Specifically, we ground the textual responses to the visual inputs. The\nconfidence from the grounding model is used to calibrate the overall\nconfidence. Given that using a grounding model adds its own uncertainty in the\npipeline, we apply temperature scaling - a widely accepted parametric\ncalibration technique - to calibrate the grounding model's confidence in the\naccuracy of generated responses. We evaluate the proposed approach across\nmultiple multi-modal tasks, such as medical question answering (Slake) and\nvisual question answering (VQAv2), considering multi-modal models such as\nLLaVA-Med and LLaVA. The experiments demonstrate that the proposed framework\nachieves significantly improved calibration on both tasks."}
{"id": "2505.04260", "pdf": "https://arxiv.org/pdf/2505.04260.pdf", "abs": "https://arxiv.org/abs/2505.04260", "title": "Steerable Chatbots: Personalizing LLMs with Preference-Based Activation Steering", "authors": ["Jessica Y. Bo", "Tianyu Xu", "Ishan Chatterjee", "Katrina Passarella-Ward", "Achin Kulshrestha", "D Shin"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "As large language models (LLMs) improve in their capacity to serve as\npersonal AI assistants, their ability to output uniquely tailored, personalized\nresponses that align with the soft preferences of their users is essential for\nenhancing user satisfaction and retention. However, untrained lay users have\npoor prompt specification abilities and often struggle with conveying their\nlatent preferences to AI assistants. To address this, we leverage activation\nsteering to guide LLMs to align with interpretable preference dimensions during\ninference. In contrast to memory-based personalization methods that require\nlonger user history, steering is extremely lightweight and can be easily\ncontrolled by the user via an linear strength factor. We embed steering into\nthree different interactive chatbot interfaces and conduct a within-subjects\nuser study (n=14) to investigate how end users prefer to personalize their\nconversations. The results demonstrate the effectiveness of preference-based\nsteering for aligning real-world conversations with hidden user preferences,\nand highlight further insights on how diverse values around control, usability,\nand transparency lead users to prefer different interfaces."}
{"id": "2505.03910", "pdf": "https://arxiv.org/pdf/2505.03910.pdf", "abs": "https://arxiv.org/abs/2505.03910", "title": "Hesitation is defeat? Connecting Linguistic and Predictive Uncertainty", "authors": ["Gianluca Manzo", "Julia Ive"], "categories": ["cs.CL"], "comment": null, "summary": "Automating chest radiograph interpretation using Deep Learning (DL) models\nhas the potential to significantly improve clinical workflows, decision-making,\nand large-scale health screening. However, in medical settings, merely\noptimising predictive performance is insufficient, as the quantification of\nuncertainty is equally crucial. This paper investigates the relationship\nbetween predictive uncertainty, derived from Bayesian Deep Learning\napproximations, and human/linguistic uncertainty, as estimated from free-text\nradiology reports labelled by rule-based labellers. Utilising BERT as the model\nof choice, this study evaluates different binarisation methods for uncertainty\nlabels and explores the efficacy of Monte Carlo Dropout and Deep Ensembles in\nestimating predictive uncertainty. The results demonstrate good model\nperformance, but also a modest correlation between predictive and linguistic\nuncertainty, highlighting the challenges in aligning machine uncertainty with\nhuman interpretation nuances. Our findings suggest that while Bayesian\napproximations provide valuable uncertainty estimates, further refinement is\nnecessary to fully capture and utilise the subtleties of human uncertainty in\nclinical applications."}
{"id": "2505.04273", "pdf": "https://arxiv.org/pdf/2505.04273.pdf", "abs": "https://arxiv.org/abs/2505.04273", "title": "With Friends Like These, Who Needs Explanations? Evaluating User Understanding of Group Recommendations", "authors": ["Cedric Waterschoot", "Raciel Yera Toledo", "Nava Tintarev", "Francesco Barile"], "categories": ["cs.HC"], "comment": "This article will be part of the UMAP 2025 conference. (33rd ACM\n  Conference on User Modeling, Adaptation and Personalization (UMAP25), June\n  16-19, 2025, New York City, NY, USA)", "summary": "Group Recommender Systems (GRS) employing social choice-based aggregation\nstrategies have previously been explored in terms of perceived consensus,\nfairness, and satisfaction. At the same time, the impact of textual\nexplanations has been examined, but the results suggest a low effectiveness of\nthese explanations. However, user understanding remains fairly unexplored, even\nif it can contribute positively to transparent GRS. This is particularly\ninteresting to study in more complex or potentially unfair scenarios when user\npreferences diverge, such as in a minority scenario (where group members have\nsimilar preferences, except for a single member in a minority position). In\nthis paper, we analyzed the impact of different types of explanations on user\nunderstanding of group recommendations. We present a randomized controlled\ntrial (n = 271) using two between-subject factors: (i) the aggregation strategy\n(additive, least misery, and approval voting), and (ii) the modality of\nexplanation (no explanation, textual explanation, or multimodal explanation).\nWe measured both subjective (self-perceived by the user) and objective\nunderstanding (performance on model simulation, counterfactuals and error\ndetection). In line with recent findings on explanations for machine learning\nmodels, our results indicate that more detailed explanations, whether textual\nor multimodal, did not increase subjective or objective understanding. However,\nwe did find a significant effect of aggregation strategies on both subjective\nand objective understanding. These results imply that when constructing GRS,\npractitioners need to consider that the choice of aggregation strategy can\ninfluence the understanding of users. Post-hoc analysis also suggests that\nthere is value in analyzing performance on different tasks, rather than through\na single aggregated metric of understanding."}
{"id": "2505.03970", "pdf": "https://arxiv.org/pdf/2505.03970.pdf", "abs": "https://arxiv.org/abs/2505.03970", "title": "A Reasoning-Focused Legal Retrieval Benchmark", "authors": ["Lucia Zheng", "Neel Guha", "Javokhir Arifov", "Sarah Zhang", "Michal Skreta", "Christopher D. Manning", "Peter Henderson", "Daniel E. Ho"], "categories": ["cs.CL"], "comment": "CS&Law 2025. For data, see\n  https://reglab.github.io/legal-rag-benchmarks/", "summary": "As the legal community increasingly examines the use of large language models\n(LLMs) for various legal applications, legal AI developers have turned to\nretrieval-augmented LLMs (\"RAG\" systems) to improve system performance and\nrobustness. An obstacle to the development of specialized RAG systems is the\nlack of realistic legal RAG benchmarks which capture the complexity of both\nlegal retrieval and downstream legal question-answering. To address this, we\nintroduce two novel legal RAG benchmarks: Bar Exam QA and Housing Statute QA.\nOur tasks correspond to real-world legal research tasks, and were produced\nthrough annotation processes which resemble legal research. We describe the\nconstruction of these benchmarks and the performance of existing retriever\npipelines. Our results suggest that legal RAG remains a challenging\napplication, thus motivating future research."}
{"id": "2505.04433", "pdf": "https://arxiv.org/pdf/2505.04433.pdf", "abs": "https://arxiv.org/abs/2505.04433", "title": "Improving Inclusivity for Emotion Recognition Based on Face Tracking", "authors": ["Mats Ole Ellenberg", "Katja Krug"], "categories": ["cs.HC"], "comment": "Selected for ACM CHI 2025 Workshop \"Affective interaction and\n  affective computing - past, present and future\" -\n  https://doi.org/10.1145/3706599.3706743", "summary": "The limited expressiveness of virtual user representations in Mixed Reality\nand Virtual Reality can inhibit an integral part of communication: emotional\nexpression. Emotion recognition based on face tracking is often used to\ncompensate for this. However, emotional facial expressions are highly\nindividual, which is why many approaches have difficulties recognizing unique\nvariations of emotional expressions. We propose several strategies to improve\nface tracking systems for emotion recognition with and without user\nintervention for the Affective Interaction Workshop at CHI '25."}
{"id": "2505.03973", "pdf": "https://arxiv.org/pdf/2505.03973.pdf", "abs": "https://arxiv.org/abs/2505.03973", "title": "Divide, Optimize, Merge: Fine-Grained LLM Agent Optimization at Scale", "authors": ["Jiale Liu", "Yifan Zeng", "Shaokun Zhang", "Chi Zhang", "Malte Højmark-Bertelsen", "Marie Normann Gadeberg", "Huazheng Wang", "Qingyun Wu"], "categories": ["cs.CL"], "comment": null, "summary": "LLM-based optimization has shown remarkable potential in enhancing agentic\nsystems. However, the conventional approach of prompting LLM optimizer with the\nwhole training trajectories on training dataset in a single pass becomes\nuntenable as datasets grow, leading to context window overflow and degraded\npattern recognition. To address these challenges, we propose Fine-Grained\nOptimization (FGO), a scalable framework that divides large optimization tasks\ninto manageable subsets, performs targeted optimizations, and systematically\ncombines optimized components through progressive merging. Evaluation across\nALFWorld, LogisticsQA, and GAIA benchmarks demonstrate that FGO outperforms\nexisting approaches by 1.6-8.6% while reducing average prompt token consumption\nby 56.3%. Our framework provides a practical solution for scaling up LLM-based\noptimization of increasingly sophisticated agent systems. Further analysis\ndemonstrates that FGO achieves the most consistent performance gain in all\ntraining dataset sizes, showcasing its scalability and efficiency."}
{"id": "2505.04446", "pdf": "https://arxiv.org/pdf/2505.04446.pdf", "abs": "https://arxiv.org/abs/2505.04446", "title": "Practice Support for Violin Bowing by Measuring Bow Pressure and Position", "authors": ["Yurina Mizuho", "Yuta Sugiura"], "categories": ["cs.HC", "H.5.2"], "comment": "27 pages, 13 figures. arXiv admin note: text overlap with\n  arXiv:2411.05126", "summary": "The violin is one of the most popular musical instruments. Various parameters\nof bowing motion, such as pressure, position, and speed, are crucial for\nproducing a beautiful tone. However, mastering them is challenging and requires\nextensive practice. In this study, we aimed to support practice of bowing,\nfocusing on bow pressure. First, we compared the bowing movements, specifically\nbow pressure, bow position, and bow speed, of eight experienced players with\nthose of eight beginners. Next, we developed and evaluated a visual feedback\nsystem that displays bow pressure to support practice. We taught the identified\ndifferences to 14 beginners, dividing them into two groups: one practiced with\nan explanation, and the other with both an explanation and a feedback system.\nThese two experiments found that clarifying the characteristics unique to\nexperienced players can support practice."}
{"id": "2505.03981", "pdf": "https://arxiv.org/pdf/2505.03981.pdf", "abs": "https://arxiv.org/abs/2505.03981", "title": "X-Reasoner: Towards Generalizable Reasoning Across Modalities and Domains", "authors": ["Qianchu Liu", "Sheng Zhang", "Guanghui Qin", "Timothy Ossowski", "Yu Gu", "Ying Jin", "Sid Kiblawi", "Sam Preston", "Mu Wei", "Paul Vozila", "Tristan Naumann", "Hoifung Poon"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent proprietary models (e.g., o3) have begun to demonstrate strong\nmultimodal reasoning capabilities. Yet, most existing open-source research\nconcentrates on training text-only reasoning models, with evaluations limited\nto mainly mathematical and general-domain tasks. Therefore, it remains unclear\nhow to effectively extend reasoning capabilities beyond text input and general\ndomains. This paper explores a fundamental research question: Is reasoning\ngeneralizable across modalities and domains? Our findings support an\naffirmative answer: General-domain text-based post-training can enable such\nstrong generalizable reasoning. Leveraging this finding, we introduce\nX-Reasoner, a vision-language model post-trained solely on general-domain text\nfor generalizable reasoning, using a two-stage approach: an initial supervised\nfine-tuning phase with distilled long chain-of-thoughts, followed by\nreinforcement learning with verifiable rewards. Experiments show that\nX-Reasoner successfully transfers reasoning capabilities to both multimodal and\nout-of-domain settings, outperforming existing state-of-the-art models trained\nwith in-domain and multimodal data across various general and medical\nbenchmarks (Figure 1). Additionally, we find that X-Reasoner's performance in\nspecialized domains can be further enhanced through continued training on\ndomain-specific text-only data. Building upon this, we introduce\nX-Reasoner-Med, a medical-specialized variant that achieves new state of the\nart on numerous text-only and multimodal medical benchmarks."}
{"id": "2505.04487", "pdf": "https://arxiv.org/pdf/2505.04487.pdf", "abs": "https://arxiv.org/abs/2505.04487", "title": "A Design Space for the Critical Validation of LLM-Generated Tabular Data", "authors": ["Madhav Sachdeva", "Christopher Narayanan", "Marvin Wiedenkeller", "Jana Sedlakova", "Jürgen Bernard"], "categories": ["cs.HC"], "comment": "To appear at the 16th International EuroVis Workshop on Visual\n  Analytics (EuroVA'25)", "summary": "LLM-generated tabular data is creating new opportunities for data-driven\napplications in academia, business, and society. To leverage benefits like\nmissing value imputation, labeling, and enrichment with context-aware\nattributes, LLM-generated data needs a critical validation process. The number\nof pioneering approaches is increasing fast, opening a promising validation\nspace that, so far, remains unstructured. We present a design space for the\ncritical validation of LLM-generated tabular data with two dimensions: First,\nthe Analysis Granularity dimension: from within-attribute (single-item and\nmulti-item) to across-attribute perspectives (1 x 1, 1 x m, and n x n). Second,\nthe Data Source dimension: differentiating between LLM-generated values, ground\ntruth values, explanations, and their combinations. We discuss analysis tasks\nfor each dimension cross-cut, map 19 existing validation approaches, and\ndiscuss the characteristics of two approaches in detail, demonstrating\ndescriptive power."}
{"id": "2505.04016", "pdf": "https://arxiv.org/pdf/2505.04016.pdf", "abs": "https://arxiv.org/abs/2505.04016", "title": "SLOT: Structuring the Output of Large Language Models", "authors": ["Darren Yow-Bang Wang", "Zhengyuan Shen", "Soumya Smruti Mishra", "Zhichao Xu", "Yifei Teng", "Haibo Ding"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Structured outputs are essential for large language models (LLMs) in critical\napplications like agents and information extraction. Despite their\ncapabilities, LLMs often generate outputs that deviate from predefined schemas,\nsignificantly hampering reliable application development. We present SLOT\n(Structured LLM Output Transformer), a model-agnostic approach that transforms\nunstructured LLM outputs into precise structured formats. While existing\nsolutions predominantly rely on constrained decoding techniques or are tightly\ncoupled with specific models, SLOT employs a fine-tuned lightweight language\nmodel as a post-processing layer, achieving flexibility across various LLMs and\nschema specifications. We introduce a systematic pipeline for data curation and\nsynthesis alongside a formal evaluation methodology that quantifies both schema\naccuracy and content fidelity. Our results demonstrate that fine-tuned\nMistral-7B model with constrained decoding achieves near perfect schema\naccuracy (99.5%) and content similarity (94.0%), outperforming\nClaude-3.5-Sonnet by substantial margins (+25 and +20 percentage points,\nrespectively). Notably, even compact models like Llama-3.2-1B can match or\nexceed the structured output capabilities of much larger proprietary models\nwhen equipped with SLOT, enabling reliable structured generation in\nresource-constrained environments."}
{"id": "2505.04584", "pdf": "https://arxiv.org/pdf/2505.04584.pdf", "abs": "https://arxiv.org/abs/2505.04584", "title": "SlideItRight: Using AI to Find Relevant Slides and Provide Feedback for Open-Ended Questions", "authors": ["Chloe Qianhui Zhao", "Jie Cao", "Eason Chen", "Kenneth R. Koedinger", "Jionghao Lin"], "categories": ["cs.HC"], "comment": "14 pages, to be published at the 26th International Conference on\n  Artificial Intelligence in Education (AIED '25)", "summary": "Feedback is important in supporting student learning. While various automated\nfeedback systems have been implemented to make the feedback scalable, many\nexisting solutions only focus on generating text-based feedback. As is\nindicated in the multimedia learning principle, learning with more modalities\ncould help utilize more separate channels, reduce the cognitive load and\nfacilitate students' learning. Hence, it is important to explore the potential\nof Artificial Intelligence (AI) in feedback generation from and to different\nmodalities. Our study leverages Large Language Models (LLMs) for textual\nfeedback with the supplementary guidance from other modality - relevant lecture\nslide retrieved from the slides hub. Through an online crowdsourcing study\n(N=91), this study investigates learning gains and student perceptions using a\n2x2 design (i.e., human feedback vs. AI feedback and with vs. without relevant\nslide), evaluating the clarity, engagement, perceived effectiveness, and\nreliability) of AI-facilitated multimodal feedback. We observed significant\npre-to-post learning gains across all conditions. However, the differences in\nthese gains were not statistically significant between conditions. The\npost-survey revealed that students found the slide feedback helpful in their\nlearning process, though they reported difficulty in understanding it.\nRegarding the AI-generated open-ended feedback, students considered it\npersonalized and relevant to their responses, but they expressed lower trust in\nthe AI feedback compared to human-generated feedback."}
{"id": "2505.04072", "pdf": "https://arxiv.org/pdf/2505.04072.pdf", "abs": "https://arxiv.org/abs/2505.04072", "title": "Advancing and Benchmarking Personalized Tool Invocation for LLMs", "authors": ["Xu Huang", "Yuefeng Huang", "Weiwen Liu", "Xingshan Zeng", "Yasheng Wang", "Ruiming Tang", "Hong Xie", "Defu Lian"], "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 7 figures, 5 tables", "summary": "Tool invocation is a crucial mechanism for extending the capabilities of\nLarge Language Models (LLMs) and has recently garnered significant attention.\nIt enables LLMs to solve complex problems through tool calls while accessing\nup-to-date world knowledge. However, existing work primarily focuses on the\nfundamental ability of LLMs to invoke tools for problem-solving, without\nconsidering personalized constraints in tool invocation. In this work, we\nintroduce the concept of Personalized Tool Invocation and define two key tasks:\nTool Preference and Profile-dependent Query. Tool Preference addresses user\npreferences when selecting among functionally similar tools, while\nProfile-dependent Query considers cases where a user query lacks certain tool\nparameters, requiring the model to infer them from the user profile. To tackle\nthese challenges, we propose PTool, a data synthesis framework designed for\npersonalized tool invocation. Additionally, we construct \\textbf{PTBench}, the\nfirst benchmark for evaluating personalized tool invocation. We then fine-tune\nvarious open-source models, demonstrating the effectiveness of our framework\nand providing valuable insights. Our benchmark is public at\nhttps://github.com/hyfshadow/PTBench."}
{"id": "2504.16728", "pdf": "https://arxiv.org/pdf/2504.16728.pdf", "abs": "https://arxiv.org/abs/2504.16728", "title": "IRIS: Interactive Research Ideation System for Accelerating Scientific Discovery", "authors": ["Aniketh Garikaparthi", "Manasi Patwardhan", "Lovekesh Vig", "Arman Cohan"], "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": "6 pages main-text, 2 pages appendix", "summary": "The rapid advancement in capabilities of large language models (LLMs) raises\na pivotal question: How can LLMs accelerate scientific discovery? This work\ntackles the crucial first stage of research, generating novel hypotheses. While\nrecent work on automated hypothesis generation focuses on multi-agent\nframeworks and extending test-time compute, none of the approaches effectively\nincorporate transparency and steerability through a synergistic\nHuman-in-the-loop (HITL) approach. To address this gap, we introduce IRIS:\nInteractive Research Ideation System, an open-source platform designed for\nresearchers to leverage LLM-assisted scientific ideation. IRIS incorporates\ninnovative features to enhance ideation, including adaptive test-time compute\nexpansion via Monte Carlo Tree Search (MCTS), fine-grained feedback mechanism,\nand query-based literature synthesis. Designed to empower researchers with\ngreater control and insight throughout the ideation process. We additionally\nconduct a user study with researchers across diverse disciplines, validating\nthe effectiveness of our system in enhancing ideation. We open-source our code\nat https://github.com/Anikethh/IRIS-Interactive-Research-Ideation-System"}
{"id": "2505.04073", "pdf": "https://arxiv.org/pdf/2505.04073.pdf", "abs": "https://arxiv.org/abs/2505.04073", "title": "Natural Language Generation in Healthcare: A Review of Methods and Applications", "authors": ["Mengxian Lyu", "Xiaohan Li", "Ziyi Chen", "Jinqian Pan", "Cheng Peng", "Sankalp Talankar", "Yonghui Wu"], "categories": ["cs.CL"], "comment": null, "summary": "Natural language generation (NLG) is the key technology to achieve generative\nartificial intelligence (AI). With the breakthroughs in large language models\n(LLMs), NLG has been widely used in various medical applications, demonstrating\nthe potential to enhance clinical workflows, support clinical decision-making,\nand improve clinical documentation. Heterogeneous and diverse medical data\nmodalities, such as medical text, images, and knowledge bases, are utilized in\nNLG. Researchers have proposed many generative models and applied them in a\nnumber of healthcare applications. There is a need for a comprehensive review\nof NLG methods and applications in the medical domain. In this study, we\nsystematically reviewed 113 scientific publications from a total of 3,988\nNLG-related articles identified using a literature search, focusing on data\nmodality, model architecture, clinical applications, and evaluation methods.\nFollowing PRISMA (Preferred Reporting Items for Systematic reviews and\nMeta-Analyses) guidelines, we categorize key methods, identify clinical\napplications, and assess their capabilities, limitations, and emerging\nchallenges. This timely review covers the key NLG technologies and medical\napplications and provides valuable insights for future studies to leverage NLG\nto transform medical discovery and healthcare."}
{"id": "2505.04152", "pdf": "https://arxiv.org/pdf/2505.04152.pdf", "abs": "https://arxiv.org/abs/2505.04152", "title": "Can Language Models Understand Social Behavior in Clinical Conversations?", "authors": ["Manas Satish Bedmutha", "Feng Chen", "Andrea Hartzler", "Trevor Cohen", "Nadir Weibel"], "categories": ["cs.CL", "cs.CY", "cs.HC", "H.5.2; H.1.2; I.2.7; I.2.m; J.3"], "comment": null, "summary": "Effective communication between providers and their patients influences\nhealth and care outcomes. The effectiveness of such conversations has been\nlinked not only to the exchange of clinical information, but also to a range of\ninterpersonal behaviors; commonly referred to as social signals, which are\noften conveyed through non-verbal cues and shape the quality of the\npatient-provider relationship. Recent advances in large language models (LLMs)\nhave demonstrated an increasing ability to infer emotional and social behaviors\neven when analyzing only textual information. As automation increases also in\nclinical settings, such as for transcription of patient-provider conversations,\nthere is growing potential for LLMs to automatically analyze and extract social\nbehaviors from these interactions. To explore the foundational capabilities of\nLLMs in tracking social signals in clinical dialogue, we designed task-specific\nprompts and evaluated model performance across multiple architectures and\nprompting styles using a highly imbalanced, annotated dataset spanning 20\ndistinct social signals such as provider dominance, patient warmth, etc. We\npresent the first system capable of tracking all these 20 coded signals, and\nuncover patterns in LLM behavior. Further analysis of model configurations and\nclinical context provides insights for enhancing LLM performance on social\nsignal processing tasks in healthcare settings."}
{"id": "2505.04132", "pdf": "https://arxiv.org/pdf/2505.04132.pdf", "abs": "https://arxiv.org/abs/2505.04132", "title": "Bringing legal knowledge to the public by constructing a legal question bank using large-scale pre-trained language model", "authors": ["Mingruo Yuan", "Ben Kao", "Tien-Hsuan Wu", "Michael M. K. Cheung", "Henry W. H. Chan", "Anne S. Y. Cheung", "Felix W. H. Chan", "Yongxi Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Access to legal information is fundamental to access to justice. Yet\naccessibility refers not only to making legal documents available to the\npublic, but also rendering legal information comprehensible to them. A vexing\nproblem in bringing legal information to the public is how to turn formal legal\ndocuments such as legislation and judgments, which are often highly technical,\nto easily navigable and comprehensible knowledge to those without legal\neducation. In this study, we formulate a three-step approach for bringing legal\nknowledge to laypersons, tackling the issues of navigability and\ncomprehensibility. First, we translate selected sections of the law into\nsnippets (called CLIC-pages), each being a small piece of article that focuses\non explaining certain technical legal concept in layperson's terms. Second, we\nconstruct a Legal Question Bank (LQB), which is a collection of legal questions\nwhose answers can be found in the CLIC-pages. Third, we design an interactive\nCLIC Recommender (CRec). Given a user's verbal description of a legal situation\nthat requires a legal solution, CRec interprets the user's input and shortlists\nquestions from the question bank that are most likely relevant to the given\nlegal situation and recommends their corresponding CLIC pages where relevant\nlegal knowledge can be found. In this paper we focus on the technical aspects\nof creating an LQB. We show how large-scale pre-trained language models, such\nas GPT-3, can be used to generate legal questions. We compare machine-generated\nquestions (MGQs) against human-composed questions (HCQs) and find that MGQs are\nmore scalable, cost-effective, and more diversified, while HCQs are more\nprecise. We also show a prototype of CRec and illustrate through an example how\nour 3-step approach effectively brings relevant legal knowledge to the public."}
{"id": "2505.04172", "pdf": "https://arxiv.org/pdf/2505.04172.pdf", "abs": "https://arxiv.org/abs/2505.04172", "title": "A Dataset and Toolkit for Multiparameter Cardiovascular Physiology Sensing on Rings", "authors": ["Iankai Tang", "Kegang Wang", "Yingke Ding", "Jiatong Ji", "Zeyu Wang", "Xiyuxing Zhang", "Ping Chen", "Yuanchun Shi", "Yuntao Wang"], "categories": ["eess.IV", "cs.HC", "physics.med-ph"], "comment": null, "summary": "Smart rings offer a convenient way to continuously and unobtrusively monitor\ncardiovascular physiological signals. However, a gap remains between the ring\nhardware and reliable methods for estimating cardiovascular parameters, partly\ndue to the lack of publicly available datasets and standardized analysis tools.\nIn this work, we present $\\tau$-Ring, the first open-source ring-based dataset\ndesigned for cardiovascular physiological sensing. The dataset comprises\nphotoplethysmography signals (infrared and red channels) and 3-axis\naccelerometer data collected from two rings (reflective and transmissive\noptical paths), with 28.21 hours of raw data from 34 subjects across seven\nactivities. $\\tau$-Ring encompasses both stationary and motion scenarios, as\nwell as stimulus-evoked abnormal physiological states, annotated with four\nground-truth labels: heart rate, respiratory rate, oxygen saturation, and blood\npressure. Using our proposed RingTool toolkit, we evaluated three widely-used\nphysics-based methods and four cutting-edge deep learning approaches. Our\nresults show superior performance compared to commercial rings, achieving best\nMAE values of 5.18 BPM for heart rate, 2.98 BPM for respiratory rate, 3.22\\%\nfor oxygen saturation, and 13.33/7.56 mmHg for systolic/diastolic blood\npressure estimation. The open-sourced dataset and toolkit aim to foster further\nresearch and community-driven advances in ring-based cardiovascular health\nsensing."}
{"id": "2505.04135", "pdf": "https://arxiv.org/pdf/2505.04135.pdf", "abs": "https://arxiv.org/abs/2505.04135", "title": "Enhancing Granular Sentiment Classification with Chain-of-Thought Prompting in Large Language Models", "authors": ["Vihaan Miriyala", "Smrithi Bukkapatnam", "Lavanya Prahallad"], "categories": ["cs.CL", "cs.LG"], "comment": "5 pages", "summary": "We explore the use of Chain-of-Thought (CoT) prompting with large language\nmodels (LLMs) to improve the accuracy of granular sentiment categorization in\napp store reviews. Traditional numeric and polarity-based ratings often fail to\ncapture the nuanced sentiment embedded in user feedback. We evaluated the\neffectiveness of CoT prompting versus simple prompting on 2000 Amazon app\nreviews by comparing each method's predictions to human judgements. CoT\nprompting improved classification accuracy from 84% to 93% highlighting the\nbenefit of explicit reasoning in enhancing sentiment analysis performance."}
{"id": "2505.04488", "pdf": "https://arxiv.org/pdf/2505.04488.pdf", "abs": "https://arxiv.org/abs/2505.04488", "title": "\"I Can See Forever!\": Evaluating Real-time VideoLLMs for Assisting Individuals with Visual Impairments", "authors": ["Ziyi Zhang", "Zhen Sun", "Zongmin Zhang", "Zifan Peng", "Yuemeng Zhao", "Zichun Wang", "Zeren Luo", "Ruiting Zuo", "Xinlei He"], "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.MM"], "comment": "12 pages, 6 figures", "summary": "The visually impaired population, especially the severely visually impaired,\nis currently large in scale, and daily activities pose significant challenges\nfor them. Although many studies use large language and vision-language models\nto assist the blind, most focus on static content and fail to meet real-time\nperception needs in dynamic and complex environments, such as daily activities.\nTo provide them with more effective intelligent assistance, it is imperative to\nincorporate advanced visual understanding technologies. Although real-time\nvision and speech interaction VideoLLMs demonstrate strong real-time visual\nunderstanding, no prior work has systematically evaluated their effectiveness\nin assisting visually impaired individuals. In this work, we conduct the first\nsuch evaluation. First, we construct a benchmark dataset (VisAssistDaily),\ncovering three categories of assistive tasks for visually impaired individuals:\nBasic Skills, Home Life Tasks, and Social Life Tasks. The results show that\nGPT-4o achieves the highest task success rate. Next, we conduct a user study to\nevaluate the models in both closed-world and open-world scenarios, further\nexploring the practical challenges of applying VideoLLMs in assistive contexts.\nOne key issue we identify is the difficulty current models face in perceiving\npotential hazards in dynamic environments. To address this, we build an\nenvironment-awareness dataset named SafeVid and introduce a polling mechanism\nthat enables the model to proactively detect environmental risks. We hope this\nwork provides valuable insights and inspiration for future research in this\nfield."}
{"id": "2505.04146", "pdf": "https://arxiv.org/pdf/2505.04146.pdf", "abs": "https://arxiv.org/abs/2505.04146", "title": "Unmasking the Canvas: A Dynamic Benchmark for Image Generation Jailbreaking and LLM Content Safety", "authors": ["Variath Madhupal Gautham Nair", "Vishal Varma Dantuluri"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Existing large language models (LLMs) are advancing rapidly and produce\noutstanding results in image generation tasks, yet their content safety checks\nremain vulnerable to prompt-based jailbreaks. Through preliminary testing on\nplatforms such as ChatGPT, MetaAI, and Grok, we observed that even short,\nnatural prompts could lead to the generation of compromising images ranging\nfrom realistic depictions of forged documents to manipulated images of public\nfigures.\n  We introduce Unmasking the Canvas (UTC Benchmark; UTCB), a dynamic and\nscalable benchmark dataset to evaluate LLM vulnerability in image generation.\nOur methodology combines structured prompt engineering, multilingual\nobfuscation (e.g., Zulu, Gaelic, Base64), and evaluation using Groq-hosted\nLLaMA-3. The pipeline supports both zero-shot and fallback prompting\nstrategies, risk scoring, and automated tagging. All generations are stored\nwith rich metadata and curated into Bronze (non-verified), Silver (LLM-aided\nverification), and Gold (manually verified) tiers. UTCB is designed to evolve\nover time with new data sources, prompt templates, and model behaviors.\n  Warning: This paper includes visual examples of adversarial inputs designed\nto test model safety. All outputs have been redacted to ensure responsible\ndisclosure."}
{"id": "2505.04548", "pdf": "https://arxiv.org/pdf/2505.04548.pdf", "abs": "https://arxiv.org/abs/2505.04548", "title": "Accelerating Audio Research with Robotic Dummy Heads", "authors": ["Austin Lu", "Kanad Sarkar", "Yongjie Zhuang", "Leo Lin", "Ryan M Corey", "Andrew C Singer"], "categories": ["eess.AS", "cs.HC", "cs.RO", "cs.SD"], "comment": "WASPAA 2025", "summary": "This work introduces a robotic dummy head that fuses the acoustic realism of\nconventional audiological mannequins with the mobility of robots. The proposed\ndevice is capable of moving, talking, and listening as people do, and can be\nused to automate spatially-stationary audio experiments, thus accelerating the\npace of audio research. Critically, the device may also be used as a moving\nsound source in dynamic experiments, due to its quiet motor. This feature\ndifferentiates our work from previous robotic acoustic research platforms.\nValidation that the robot enables high quality audio data collection is\nprovided through various experiments and acoustic measurements. These\nexperiments also demonstrate how the robot might be used to study adaptive\nbinaural beamforming. Design files are provided as open-source to stimulate\nnovel audio research."}
{"id": "2505.04152", "pdf": "https://arxiv.org/pdf/2505.04152.pdf", "abs": "https://arxiv.org/abs/2505.04152", "title": "Can Language Models Understand Social Behavior in Clinical Conversations?", "authors": ["Manas Satish Bedmutha", "Feng Chen", "Andrea Hartzler", "Trevor Cohen", "Nadir Weibel"], "categories": ["cs.CL", "cs.CY", "cs.HC", "H.5.2; H.1.2; I.2.7; I.2.m; J.3"], "comment": null, "summary": "Effective communication between providers and their patients influences\nhealth and care outcomes. The effectiveness of such conversations has been\nlinked not only to the exchange of clinical information, but also to a range of\ninterpersonal behaviors; commonly referred to as social signals, which are\noften conveyed through non-verbal cues and shape the quality of the\npatient-provider relationship. Recent advances in large language models (LLMs)\nhave demonstrated an increasing ability to infer emotional and social behaviors\neven when analyzing only textual information. As automation increases also in\nclinical settings, such as for transcription of patient-provider conversations,\nthere is growing potential for LLMs to automatically analyze and extract social\nbehaviors from these interactions. To explore the foundational capabilities of\nLLMs in tracking social signals in clinical dialogue, we designed task-specific\nprompts and evaluated model performance across multiple architectures and\nprompting styles using a highly imbalanced, annotated dataset spanning 20\ndistinct social signals such as provider dominance, patient warmth, etc. We\npresent the first system capable of tracking all these 20 coded signals, and\nuncover patterns in LLM behavior. Further analysis of model configurations and\nclinical context provides insights for enhancing LLM performance on social\nsignal processing tasks in healthcare settings."}
{"id": "2505.04551", "pdf": "https://arxiv.org/pdf/2505.04551.pdf", "abs": "https://arxiv.org/abs/2505.04551", "title": "Runtime Advocates: A Persona-Driven Framework for Requirements@Runtime Decision Support", "authors": ["Demetrius Hernandez", "Jane Cleland-Huang"], "categories": ["cs.SE", "cs.HC", "cs.MA"], "comment": "7 pages, 5 figures. Submitted to 33rd IEEE International Requirements\n  Engineering 2025 conference", "summary": "Complex systems, such as small Uncrewed Aerial Systems (sUAS) swarms\ndispatched for emergency response, often require dynamic reconfiguration at\nruntime under the supervision of human operators. This introduces\nhuman-on-the-loop requirements, where evolving needs shape ongoing system\nfunctionality and behaviors. While traditional personas support upfront, static\nrequirements elicitation, we propose a persona-based advocate framework for\nruntime requirements engineering to provide ethically informed, safety-driven,\nand regulatory-aware decision support. Our approach extends standard personas\ninto event-driven personas. When triggered by events such as adverse\nenvironmental conditions, evolving mission state, or operational constraints,\nthe framework updates the sUAS operator's view of the personas, ensuring\nrelevance to current conditions. We create three key advocate personas, namely\nSafety Controller, Ethical Governor, and Regulatory Auditor, to manage\ntrade-offs among risk, ethical considerations, and regulatory compliance. We\nperform a proof-of-concept validation in an emergency response scenario using\nsUAS, showing how our advocate personas provide context-aware guidance grounded\nin safety, regulatory, and ethical constraints. By evolving static, design-time\npersonas into adaptive, event-driven advocates, the framework surfaces\nmission-critical runtime requirements in response to changing conditions. These\nrequirements shape operator decisions in real time, aligning actions with the\noperational demands of the moment."}
{"id": "2505.04253", "pdf": "https://arxiv.org/pdf/2505.04253.pdf", "abs": "https://arxiv.org/abs/2505.04253", "title": "LLM-Independent Adaptive RAG: Let the Question Speak for Itself", "authors": ["Maria Marina", "Nikolay Ivanov", "Sergey Pletenev", "Mikhail Salnikov", "Daria Galimzianova", "Nikita Krayko", "Vasily Konovalov", "Alexander Panchenko", "Viktor Moskvoretskii"], "categories": ["cs.CL", "cs.LG"], "comment": "11 pages, 5 figures, 2 tables", "summary": "Large Language Models~(LLMs) are prone to hallucinations, and\nRetrieval-Augmented Generation (RAG) helps mitigate this, but at a high\ncomputational cost while risking misinformation. Adaptive retrieval aims to\nretrieve only when necessary, but existing approaches rely on LLM-based\nuncertainty estimation, which remain inefficient and impractical. In this\nstudy, we introduce lightweight LLM-independent adaptive retrieval methods\nbased on external information. We investigated 27 features, organized into 7\ngroups, and their hybrid combinations. We evaluated these methods on 6 QA\ndatasets, assessing the QA performance and efficiency. The results show that\nour approach matches the performance of complex LLM-based methods while\nachieving significant efficiency gains, demonstrating the potential of external\ninformation for adaptive retrieval."}
{"id": "2410.01495", "pdf": "https://arxiv.org/pdf/2410.01495.pdf", "abs": "https://arxiv.org/abs/2410.01495", "title": "OV-MER: Towards Open-Vocabulary Multimodal Emotion Recognition", "authors": ["Zheng Lian", "Haiyang Sun", "Licai Sun", "Haoyu Chen", "Lan Chen", "Hao Gu", "Zhuofan Wen", "Shun Chen", "Siyuan Zhang", "Hailiang Yao", "Bin Liu", "Rui Liu", "Shan Liang", "Ya Li", "Jiangyan Yi", "Jianhua Tao"], "categories": ["cs.HC"], "comment": null, "summary": "Multimodal Emotion Recognition (MER) is a critical research area that seeks\nto decode human emotions from diverse data modalities. However, existing\nmachine learning methods predominantly rely on predefined emotion taxonomies,\nwhich fail to capture the inherent complexity, subtlety, and multi-appraisal\nnature of human emotional experiences, as demonstrated by studies in psychology\nand cognitive science. To overcome this limitation, we advocate for introducing\nthe concept of open vocabulary into MER. This paradigm shift aims to enable\nmodels to predict emotions beyond a fixed label space, accommodating a flexible\nset of categories to better reflect the nuanced spectrum of human emotions. To\nachieve this, we propose a novel paradigm: Open-Vocabulary MER (OV-MER), which\nenables emotion prediction without being confined to predefined spaces.\nHowever, constructing a dataset that encompasses the full range of emotions for\nOV-MER is practically infeasible; hence, we present a comprehensive solution\nincluding a newly curated database, novel evaluation metrics, and a preliminary\nbenchmark. By advancing MER from basic emotions to more nuanced and diverse\nemotional states, we hope this work can inspire the next generation of MER,\nenhancing its generalizability and applicability in real-world scenarios. Code\nand dataset are available at: https://github.com/zeroQiaoba/AffectGPT."}
{"id": "2505.04284", "pdf": "https://arxiv.org/pdf/2505.04284.pdf", "abs": "https://arxiv.org/abs/2505.04284", "title": "GASCADE: Grouped Summarization of Adverse Drug Event for Enhanced Cancer Pharmacovigilance", "authors": ["Sofia Jamil", "Aryan Dabad", "Bollampalli Areen Reddy", "Sriparna Saha", "Rajiv Misra", "Adil A. Shakur"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In the realm of cancer treatment, summarizing adverse drug events (ADEs)\nreported by patients using prescribed drugs is crucial for enhancing\npharmacovigilance practices and improving drug-related decision-making. While\nthe volume and complexity of pharmacovigilance data have increased, existing\nresearch in this field has predominantly focused on general diseases rather\nthan specifically addressing cancer. This work introduces the task of grouped\nsummarization of adverse drug events reported by multiple patients using the\nsame drug for cancer treatment. To address the challenge of limited resources\nin cancer pharmacovigilance, we present the MultiLabeled Cancer Adverse Drug\nReaction and Summarization (MCADRS) dataset. This dataset includes\npharmacovigilance posts detailing patient concerns regarding drug efficacy and\nadverse effects, along with extracted labels for drug names, adverse drug\nevents, severity, and adversity of reactions, as well as summaries of ADEs for\neach drug. Additionally, we propose the Grouping and Abstractive Summarization\nof Cancer Adverse Drug events (GASCADE) framework, a novel pipeline that\ncombines the information extraction capabilities of Large Language Models\n(LLMs) with the summarization power of the encoder-decoder T5 model. Our work\nis the first to apply alignment techniques, including advanced algorithms like\nDirect Preference Optimization, to encoder-decoder models using synthetic\ndatasets for summarization tasks. Through extensive experiments, we demonstrate\nthe superior performance of GASCADE across various metrics, validated through\nboth automated assessments and human evaluations. This multitasking approach\nenhances drug-related decision-making and fosters a deeper understanding of\npatient concerns, paving the way for advancements in personalized and\nresponsive cancer care. The code and dataset used in this work are publicly\navailable."}
{"id": "2411.10246", "pdf": "https://arxiv.org/pdf/2411.10246.pdf", "abs": "https://arxiv.org/abs/2411.10246", "title": "Automated Coding of Communications in Collaborative Problem-solving Tasks Using ChatGPT", "authors": ["Jiangang Hao", "Wenju Cui", "Patrick Kyllonen", "Emily Kerzabi", "Lei Liu", "Michael Flor"], "categories": ["cs.HC", "cs.CL"], "comment": "21 pages, 3 figures, 5 tables. Initially report in the edArXiv:xw6kz", "summary": "Collaborative problem solving (CPS) is widely recognized as a critical\n21st-century skill. Assessing CPS depends heavily on coding the communication\ndata using a construct-relevant framework, and this process has long been a\nmajor bottleneck to scaling up such assessments. Based on five datasets and two\ncoding frameworks, we demonstrate that ChatGPT can code communication data to a\nsatisfactory level, though performance varies across ChatGPT models, and\ndepends on the coding framework and task characteristics. Interestingly, newer\nreasoning-focused models such as GPT-o1-mini and GPT-o3-mini do not necessarily\nyield better coding results. Additionally, we show that refining prompts based\non feedback from miscoded cases can improve coding accuracy in some instances,\nthough the effectiveness of this approach is not consistent across all tasks.\nThese findings offer practical guidance for researchers and practitioners in\ndeveloping scalable, efficient methods to analyze communication data in support\nof 21st-century skill assessment."}
{"id": "2505.04388", "pdf": "https://arxiv.org/pdf/2505.04388.pdf", "abs": "https://arxiv.org/abs/2505.04388", "title": "The Aloe Family Recipe for Open and Specialized Healthcare LLMs", "authors": ["Dario Garcia-Gasulla", "Jordi Bayarri-Planas", "Ashwin Kumar Gururajan", "Enrique Lopez-Cuena", "Adrian Tormos", "Daniel Hinjos", "Pablo Bernabeu-Perez", "Anna Arias-Duart", "Pablo Agustin Martin-Torres", "Marta Gonzalez-Mallo", "Sergio Alvarez-Napagao", "Eduard Ayguadé-Parra", "Ulises Cortés"], "categories": ["cs.CL", "cs.AI"], "comment": "arXiv admin note: substantial text overlap with arXiv:2405.01886", "summary": "Purpose: With advancements in Large Language Models (LLMs) for healthcare,\nthe need arises for competitive open-source models to protect the public\ninterest. This work contributes to the field of open medical LLMs by optimizing\nkey stages of data preprocessing and training, while showing how to improve\nmodel safety (through DPO) and efficacy (through RAG). The evaluation\nmethodology used, which includes four different types of tests, defines a new\nstandard for the field. The resultant models, shown to be competitive with the\nbest private alternatives, are released with a permisive license.\n  Methods: Building on top of strong base models like Llama 3.1 and Qwen 2.5,\nAloe Beta uses a custom dataset to enhance public data with synthetic Chain of\nThought examples. The models undergo alignment with Direct Preference\nOptimization, emphasizing ethical and policy-aligned performance in the\npresence of jailbreaking attacks. Evaluation includes close-ended, open-ended,\nsafety and human assessments, to maximize the reliability of results.\n  Results: Recommendations are made across the entire pipeline, backed by the\nsolid performance of the Aloe Family. These models deliver competitive\nperformance across healthcare benchmarks and medical fields, and are often\npreferred by healthcare professionals. On bias and toxicity, the Aloe Beta\nmodels significantly improve safety, showing resilience to unseen jailbreaking\nattacks. For a responsible release, a detailed risk assessment specific to\nhealthcare is attached to the Aloe Family models.\n  Conclusion: The Aloe Beta models, and the recipe that leads to them, are a\nsignificant contribution to the open-source medical LLM field, offering\ntop-of-the-line performance while maintaining high ethical requirements. This\nwork sets a new standard for developing and reporting aligned LLMs in\nhealthcare."}
{"id": "2411.15091", "pdf": "https://arxiv.org/pdf/2411.15091.pdf", "abs": "https://arxiv.org/abs/2411.15091", "title": "Somesite I Used To Crawl: Awareness, Agency and Efficacy in Protecting Content Creators From AI Crawlers", "authors": ["Enze Liu", "Elisa Luo", "Shawn Shan", "Geoffrey M. Voelker", "Ben Y. Zhao", "Stefan Savage"], "categories": ["cs.HC"], "comment": "Accepted to IMC 25. Please cite the conference version", "summary": "The success of generative AI relies heavily on training on data scraped\nthrough extensive crawling of the Internet, a practice that has raised\nsignificant copyright, privacy, and ethical concerns. While few measures are\ndesigned to resist a resource-rich adversary determined to scrape a site,\ncrawlers can be impacted by a range of existing tools such as robots.txt, NoAI\nmeta tags, and active crawler blocking by reverse proxies.\n  In this work, we seek to understand the ability and efficacy of today's\nnetworking tools to protect content creators against AI-related crawling. For\ntargeted populations like human artists, do they have the technical knowledge\nand agency to utilize crawler-blocking tools such as robots.txt, and can such\ntools be effective? Using large scale measurements and a targeted user study of\n203 professional artists, we find strong demand for tools like robots.txt, but\nsignificantly constrained by critical hurdles in technical awareness, agency in\ndeploying them, and limited efficacy against unresponsive crawlers. We further\ntest and evaluate network-level crawler blockers provided by reverse proxies.\nDespite relatively limited deployment today, they offer stronger protections\nagainst AI crawlers, but still come with their own set of limitations."}
{"id": "2505.04393", "pdf": "https://arxiv.org/pdf/2505.04393.pdf", "abs": "https://arxiv.org/abs/2505.04393", "title": "Large Means Left: Political Bias in Large Language Models Increases with Their Number of Parameters", "authors": ["David Exler", "Mark Schutera", "Markus Reischl", "Luca Rettenberger"], "categories": ["cs.CL"], "comment": null, "summary": "With the increasing prevalence of artificial intelligence, careful evaluation\nof inherent biases needs to be conducted to form the basis for alleviating the\neffects these predispositions can have on users. Large language models (LLMs)\nare predominantly used by many as a primary source of information for various\ntopics. LLMs frequently make factual errors, fabricate data (hallucinations),\nor present biases, exposing users to misinformation and influencing opinions.\nEducating users on their risks is key to responsible use, as bias, unlike\nhallucinations, cannot be caught through data verification. We quantify the\npolitical bias of popular LLMs in the context of the recent vote of the German\nBundestag using the score produced by the Wahl-O-Mat. This metric measures the\nalignment between an individual's political views and the positions of German\npolitical parties. We compare the models' alignment scores to identify factors\ninfluencing their political preferences. Doing so, we discover a bias toward\nleft-leaning parties, most dominant in larger LLMs. Also, we find that the\nlanguage we use to communicate with the models affects their political views.\nAdditionally, we analyze the influence of a model's origin and release date and\ncompare the results to the outcome of the recent vote of the Bundestag. Our\nresults imply that LLMs are prone to exhibiting political bias. Large\ncorporations with the necessary means to develop LLMs, thus, knowingly or\nunknowingly, have a responsibility to contain these biases, as they can\ninfluence each voter's decision-making process and inform public opinion in\ngeneral and at scale."}
{"id": "2411.18438", "pdf": "https://arxiv.org/pdf/2411.18438.pdf", "abs": "https://arxiv.org/abs/2411.18438", "title": "Adaptive Gen-AI Guidance in Virtual Reality: A Multimodal Exploration of Engagement in Neapolitan Pizza-Making", "authors": ["Ka Hei Carrie Lau", "Sema Sen", "Philipp Stark", "Efe Bozkir", "Enkelejda Kasneci"], "categories": ["cs.HC"], "comment": null, "summary": "Virtual reality (VR) offers promising opportunities for procedural learning,\nparticularly in preserving intangible cultural heritage. Advances in generative\nartificial intelligence (Gen-AI) further enrich these experiences by enabling\nadaptive learning pathways. However, evaluating such adaptive systems using\ntraditional temporal metrics remains challenging due to the inherent\nvariability in Gen-AI response times. To address this, our study employs\nmultimodal behavioural metrics, including visual attention, physical\nexploratory behaviour, and verbal interaction, to assess user engagement in an\nadaptive VR environment. In a controlled experiment with 54 participants, we\ncompared three levels of adaptivity (high, moderate, and non-adaptive baseline)\nwithin a Neapolitan pizza-making VR experience. Results show that moderate\nadaptivity optimally enhances user engagement, significantly reducing\nunnecessary exploratory behaviour and increasing focused visual attention on\nthe AI avatar. Our findings suggest that a balanced level of adaptive AI\nprovides the most effective user support, offering practical design\nrecommendations for future adaptive educational technologies."}
{"id": "2505.04406", "pdf": "https://arxiv.org/pdf/2505.04406.pdf", "abs": "https://arxiv.org/abs/2505.04406", "title": "YABLoCo: Yet Another Benchmark for Long Context Code Generation", "authors": ["Aidar Valeev", "Roman Garaev", "Vadim Lomshakov", "Irina Piontkovskaya", "Vladimir Ivanov", "Israel Adewuyi"], "categories": ["cs.CL", "cs.AI", "cs.SE"], "comment": "Presented at LLM4Code 2025 Workshop co-located wtih ICSE 2025", "summary": "Large Language Models demonstrate the ability to solve various programming\ntasks, including code generation. Typically, the performance of LLMs is\nmeasured on benchmarks with small or medium-sized context windows of thousands\nof lines of code. At the same time, in real-world software projects,\nrepositories can span up to millions of LoC. This paper closes this gap by\ncontributing to the long context code generation benchmark (YABLoCo). The\nbenchmark featured a test set of 215 functions selected from four large\nrepositories with thousands of functions. The dataset contained metadata of\nfunctions, contexts of the functions with different levels of dependencies,\ndocstrings, functions bodies, and call graphs for each repository. This paper\npresents three key aspects of the contribution. First, the benchmark aims at\nfunction body generation in large repositories in C and C++, two languages not\ncovered by previous benchmarks. Second, the benchmark contains large\nrepositories from 200K to 2,000K LoC. Third, we contribute a scalable\nevaluation pipeline for efficient computing of the target metrics and a tool\nfor visual analysis of generated code. Overall, these three aspects allow for\nevaluating code generation in large repositories in C and C++."}
{"id": "2501.16566", "pdf": "https://arxiv.org/pdf/2501.16566.pdf", "abs": "https://arxiv.org/abs/2501.16566", "title": "AffectGPT: A New Dataset, Model, and Benchmark for Emotion Understanding with Multimodal Large Language Models", "authors": ["Zheng Lian", "Haoyu Chen", "Lan Chen", "Haiyang Sun", "Licai Sun", "Yong Ren", "Zebang Cheng", "Bin Liu", "Rui Liu", "Xiaojiang Peng", "Jiangyan Yi", "Jianhua Tao"], "categories": ["cs.HC"], "comment": null, "summary": "The emergence of multimodal large language models (MLLMs) advances multimodal\nemotion recognition (MER) to the next level, from naive discriminative tasks to\ncomplex emotion understanding with advanced video understanding abilities and\nnatural language description. However, the current community suffers from a\nlack of large-scale datasets with intensive, descriptive emotion annotations,\nas well as a multimodal-centric framework to maximize the potential of MLLMs\nfor emotion understanding. To address this, we establish a new benchmark for\nMLLM-based emotion understanding with a novel dataset (MER-Caption) and a new\nmodel (AffectGPT). Utilizing our model-based crowd-sourcing data collection\nstrategy, we construct the largest descriptive emotion dataset to date (by\nfar), featuring over 2K fine-grained emotion categories across 115K samples. We\nalso introduce the AffectGPT model, designed with pre-fusion operations to\nenhance multimodal integration. Finally, we present MER-UniBench, a unified\nbenchmark with evaluation metrics tailored for typical MER tasks and the\nfree-form, natural language output style of MLLMs. Extensive experimental\nresults show AffectGPT's robust performance across various MER tasks. We have\nreleased both the code and the dataset to advance research and development in\nemotion understanding: https://github.com/zeroQiaoba/AffectGPT."}
{"id": "2505.04416", "pdf": "https://arxiv.org/pdf/2505.04416.pdf", "abs": "https://arxiv.org/abs/2505.04416", "title": "OBLIVIATE: Robust and Practical Machine Unlearning for Large Language Models", "authors": ["Xiaoyu Xu", "Minxin Du", "Qingqing Ye", "Haibo Hu"], "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "comment": "18 pages, 2 figures", "summary": "Large language models (LLMs) trained over extensive corpora risk memorizing\nsensitive, copyrighted, or toxic content. To address this, we propose\nOBLIVIATE, a robust unlearning framework that removes targeted data while\npreserving model utility. The framework follows a structured process:\nextracting target tokens, building retain sets, and fine-tuning with a tailored\nloss function comprising three components -- masking, distillation, and world\nfact. Using low-rank adapters (LoRA), it ensures efficiency without\ncompromising unlearning quality. We conduct experiments on multiple datasets,\nincluding the Harry Potter series, WMDP, and TOFU, using a comprehensive suite\nof metrics: forget quality (new document-level memorization score), model\nutility, and fluency. Results demonstrate its effectiveness in resisting\nmembership inference attacks, minimizing the impact on retained data, and\nmaintaining robustness across diverse scenarios."}
{"id": "2504.21477", "pdf": "https://arxiv.org/pdf/2504.21477.pdf", "abs": "https://arxiv.org/abs/2504.21477", "title": "A Comprehensive Survey of Electrical Stimulation Haptic Feedback in Human-Computer Interaction", "authors": ["Simin Yang", "Xian Wang", "Yang Li", "Lik-Hang Lee", "Tristan Camille Braud", "Pan Hui"], "categories": ["cs.HC"], "comment": "23 pages, 7 figures", "summary": "Haptic perception and feedback play a pivotal role in interactive\nexperiences, forming an essential component of human-computer interaction\n(HCI). In recent years, the field of haptic interaction has witnessed\nsignificant advancements, particularly in the area of electrical haptic\nfeedback, driving innovation across various domains. To gain a comprehensive\nunderstanding of the current state of research and the latest developments in\nelectrical haptic interaction, this study systematically reviews the literature\nin this area. Our investigation covers key aspects including haptic devices,\nhaptic perception mechanisms, the comparison and integration of electrical\nhaptic feedback with other feedback modalities, and their diverse applications.\nSpecifically, we conduct a systematic analysis of 110 research papers to\nexplore the forefront of electrical haptic feedback, providing insights into\nits latest trends, challenges, and future directions."}
{"id": "2505.04507", "pdf": "https://arxiv.org/pdf/2505.04507.pdf", "abs": "https://arxiv.org/abs/2505.04507", "title": "Detecting Spelling and Grammatical Anomalies in Russian Poetry Texts", "authors": ["Ilya Koziev"], "categories": ["cs.CL"], "comment": null, "summary": "The quality of natural language texts in fine-tuning datasets plays a\ncritical role in the performance of generative models, particularly in\ncomputational creativity tasks such as poem or song lyric generation. Fluency\ndefects in generated poems significantly reduce their value. However, training\ntexts are often sourced from internet-based platforms without stringent quality\ncontrol, posing a challenge for data engineers to manage defect levels\neffectively.\n  To address this issue, we propose the use of automated linguistic anomaly\ndetection to identify and filter out low-quality texts from training datasets\nfor creative models. In this paper, we present a comprehensive comparison of\nunsupervised and supervised text anomaly detection approaches, utilizing both\nsynthetic and human-labeled datasets. We also introduce the RUPOR dataset, a\ncollection of Russian-language human-labeled poems designed for cross-sentence\ngrammatical error detection, and provide the full evaluation code. Our work\naims to empower the community with tools and insights to improve the quality of\ntraining datasets for generative models in creative domains."}
{"id": "2505.01886", "pdf": "https://arxiv.org/pdf/2505.01886.pdf", "abs": "https://arxiv.org/abs/2505.01886", "title": "Interactive authoring of outcome-oriented lesson plans for immersive Virtual Reality training", "authors": ["Ananya Ipsita", "Ramesh Kaki", "Mayank Patel", "Asim Unmesh", "Kylie A. Peppler", "Karthik Ramani"], "categories": ["cs.HC"], "comment": null, "summary": "Immersive Virtual Reality (iVR) applications have shown immense potential for\nskill training and learning in manufacturing. However, authoring of such\napplications requires technical expertise, which makes it difficult for\neducators to author instructions targeted at desired learning outcomes. We\npresent FlowTrainer, an LLM-assisted interactive system to allow educators to\nauthor lesson plans for their iVR instruction based on desired goals. The\nauthoring workflow is supported by Backward design to align the planned lesson\nbased on the desired outcomes. We implemented a welding use case and conducted\na user study with welding experts to test the effectiveness of the system in\nauthoring outcome-oriented lesson plans. The study results showed that the\nsystem allowed users to plan lesson plans based on desired outcomes while\nreducing the time and technical expertise required for the authoring process.\nWe believe that such efforts can allow widespread adoption of iVR solutions in\nmanufacturing training to meet the workforce demands in the industry."}
{"id": "2505.04519", "pdf": "https://arxiv.org/pdf/2505.04519.pdf", "abs": "https://arxiv.org/abs/2505.04519", "title": "Pangu Ultra MoE: How to Train Your Big MoE on Ascend NPUs", "authors": ["Yehui Tang", "Yichun Yin", "Yaoyuan Wang", "Hang Zhou", "Yu Pan", "Wei Guo", "Ziyang Zhang", "Miao Rang", "Fangcheng Liu", "Naifu Zhang", "Binghan Li", "Yonghan Dong", "Xiaojun Meng", "Yasheng Wang", "Dong Li", "Yin Li", "Dandan Tu", "Can Chen", "Youliang Yan", "Fisher Yu", "Ruiming Tang", "Yunhe Wang", "Botian Huang", "Bo Wang", "Boxiao Liu", "Changzheng Zhang", "Da Kuang", "Fei Liu", "Gang Huang", "Jiansheng Wei", "Jiarui Qin", "Jie Ran", "Jinpeng Li", "Jun Zhao", "Liang Dai", "Lin Li", "Liqun Deng", "Peifeng Qin", "Pengyuan Zeng", "Qiang Gu", "Shaohua Tang", "Shengjun Cheng", "Tao Gao", "Tao Yu", "Tianshu Li", "Tianyu Bi", "Wei He", "Weikai Mao", "Wenyong Huang", "Wulong Liu", "Xiabing Li", "Xianzhi Yu", "Xueyu Wu", "Xu He", "Yangkai Du", "Yan Xu", "Ye Tian", "Yimeng Wu", "Yongbing Huang", "Yong Tian", "Yong Zhu", "Yue Li", "Yufei Wang", "Yuhang Gai", "Yujun Li", "Yu Luo", "Yunsheng Ni", "Yusen Sun", "Zelin Chen", "Zhe Liu", "Zhicheng Liu", "Zhipeng Tu", "Zilin Ding", "Zongyuan Zhan"], "categories": ["cs.CL"], "comment": null, "summary": "Sparse large language models (LLMs) with Mixture of Experts (MoE) and close\nto a trillion parameters are dominating the realm of most capable language\nmodels. However, the massive model scale poses significant challenges for the\nunderlying software and hardware systems. In this paper, we aim to uncover a\nrecipe to harness such scale on Ascend NPUs. The key goals are better usage of\nthe computing resources under the dynamic sparse model structures and\nmaterializing the expected performance gain on the actual hardware. To select\nmodel configurations suitable for Ascend NPUs without repeatedly running the\nexpensive experiments, we leverage simulation to compare the trade-off of\nvarious model hyperparameters. This study led to Pangu Ultra MoE, a sparse LLM\nwith 718 billion parameters, and we conducted experiments on the model to\nverify the simulation results. On the system side, we dig into Expert\nParallelism to optimize the communication between NPU devices to reduce the\nsynchronization overhead. We also optimize the memory efficiency within the\ndevices to further reduce the parameter and activation management overhead. In\nthe end, we achieve an MFU of 30.0% when training Pangu Ultra MoE, with\nperformance comparable to that of DeepSeek R1, on 6K Ascend NPUs, and\ndemonstrate that the Ascend system is capable of harnessing all the training\nstages of the state-of-the-art language models. Extensive experiments indicate\nthat our recipe can lead to efficient training of large-scale sparse language\nmodels with MoE. We also study the behaviors of such models for future\nreference."}
{"id": "2505.03185", "pdf": "https://arxiv.org/pdf/2505.03185.pdf", "abs": "https://arxiv.org/abs/2505.03185", "title": "Behavioral Sensing and Intervention Paradigm: A Review of Closed-Loop Approaches for Ingestion Health", "authors": ["Jun Fang", "Yanuo Zhou", "Ka I Chan", "Jiajin Li", "Zeyi Sun", "Zhengnan Li", "Zicong Fu", "Hongjing Piao", "Haodong Xu", "Yuanchun Shi", "Yuntao Wang"], "categories": ["cs.HC"], "comment": null, "summary": "Ingestive behavior plays a critical role in health, yet many existing\ninterventions remain limited to static guidance or manual self-tracking. With\nthe increasing integration of sensors and perceptual computing, recent systems\nhave begun to support closed-loop interventions that dynamically sense user\nbehavior and provide feedback during or around ingestion episodes. In this\nsurvey, we review 136 studies that leverage sensor-enabled or\ninteraction-mediated approaches to influence eating behavior. We propose a\nbehavioral closed-loop paradigm comprising three core components: target\nbehaviors, sensing modalities, and feedback strategies. A taxonomy of sensing\nand intervention modalities is presented, organized along human- and\nenvironment-based dimensions. Our analysis also examines evaluation methods and\ndesign trends across different modality-behavior pairings. This review reveals\nprevailing patterns and critical gaps, offering design insights for future\nadaptive and context-aware ingestion health interventions."}
{"id": "2505.04531", "pdf": "https://arxiv.org/pdf/2505.04531.pdf", "abs": "https://arxiv.org/abs/2505.04531", "title": "Overcoming Data Scarcity in Generative Language Modelling for Low-Resource Languages: A Systematic Review", "authors": ["Josh McGiff", "Nikola S. Nikolov"], "categories": ["cs.CL", "cs.AI"], "comment": "This work is currently under review. Please do not cite without\n  permission", "summary": "Generative language modelling has surged in popularity with the emergence of\nservices such as ChatGPT and Google Gemini. While these models have\ndemonstrated transformative potential in productivity and communication, they\noverwhelmingly cater to high-resource languages like English. This has\namplified concerns over linguistic inequality in natural language processing\n(NLP). This paper presents the first systematic review focused specifically on\nstrategies to address data scarcity in generative language modelling for\nlow-resource languages (LRL). Drawing from 54 studies, we identify, categorise\nand evaluate technical approaches, including monolingual data augmentation,\nback-translation, multilingual training, and prompt engineering, across\ngenerative tasks. We also analyse trends in architecture choices, language\nfamily representation, and evaluation methods. Our findings highlight a strong\nreliance on transformer-based models, a concentration on a small subset of\nLRLs, and a lack of consistent evaluation across studies. We conclude with\nrecommendations for extending these methods to a wider range of LRLs and\noutline open challenges in building equitable generative language systems.\nUltimately, this review aims to support researchers and developers in building\ninclusive AI tools for underrepresented languages, a necessary step toward\nempowering LRL speakers and the preservation of linguistic diversity in a world\nincreasingly shaped by large-scale language technologies."}
{"id": "2505.03440", "pdf": "https://arxiv.org/pdf/2505.03440.pdf", "abs": "https://arxiv.org/abs/2505.03440", "title": "manvr3d: A Platform for Human-in-the-loop Cell Tracking in Virtual Reality", "authors": ["Samuel Pantze", "Jean-Yves Tinevez", "Matthew McGinity", "Ulrik Günther"], "categories": ["cs.HC"], "comment": "7 pages, 6 figures, submitted to IEEE VIS 2025", "summary": "We propose manvr3d, a novel VR-ready platform for interactive\nhuman-in-the-loop cell tracking. We utilize VR controllers and eye-tracking\nhardware to facilitate rapid ground truth generation and proofreading for deep\nlearning-based cell tracking models. Life scientists reconstruct the\ndevelopmental history of organisms on the cellular level by analyzing 3D\ntime-lapse microscopy images acquired at high spatio-temporal resolution. The\nreconstruction of such cell lineage trees traditionally involves tracking\nindividual cells through all recorded time points, manually annotating their\npositions, and then linking them over time to create complete trajectories.\nDeep learning-based algorithms accelerate this process, yet depend heavily on\nmanually-annotated high-quality ground truth data and curation. Visual\nrepresentation of the image data in this process still relies primarily on 2D\nrenderings, which greatly limits spatial understanding and navigation. In this\nwork, we bridge the gap between deep learning-based cell tracking software and\n3D/VR visualization to create a human-in-the-loop cell tracking system. We lift\nthe incremental annotation, training and proofreading loop of the deep learning\nmodel into the 3rd dimension and apply natural user interfaces like hand\ngestures and eye tracking to accelerate the cell tracking workflow for life\nscientists."}
{"id": "2505.04588", "pdf": "https://arxiv.org/pdf/2505.04588.pdf", "abs": "https://arxiv.org/abs/2505.04588", "title": "ZeroSearch: Incentivize the Search Capability of LLMs without Searching", "authors": ["Hao Sun", "Zile Qiao", "Jiayan Guo", "Xuanbo Fan", "Yingyan Hou", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Yan Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Effective information searching is essential for enhancing the reasoning and\ngeneration capabilities of large language models (LLMs). Recent research has\nexplored using reinforcement learning (RL) to improve LLMs' search capabilities\nby interacting with live search engines in real-world environments. While these\napproaches show promising results, they face two major challenges: (1)\nUncontrolled Document Quality: The quality of documents returned by search\nengines is often unpredictable, introducing noise and instability into the\ntraining process. (2) Prohibitively High API Costs: RL training requires\nfrequent rollouts, potentially involving hundreds of thousands of search\nrequests, which incur substantial API expenses and severely constrain\nscalability. To address these challenges, we introduce ZeroSearch, a\nreinforcement learning framework that incentivizes the search capabilities of\nLLMs without interacting with real search engines. Our approach begins with\nlightweight supervised fine-tuning to transform the LLM into a retrieval module\ncapable of generating both relevant and noisy documents in response to a query.\nDuring RL training, we employ a curriculum-based rollout strategy that\nincrementally degrades the quality of generated documents, progressively\neliciting the model's reasoning ability by exposing it to increasingly\nchallenging retrieval scenarios. Extensive experiments demonstrate that\nZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B\nLLM as the retrieval module. Remarkably, a 7B retrieval module achieves\ncomparable performance to the real search engine, while a 14B retrieval module\neven surpasses it. Furthermore, it generalizes well across both base and\ninstruction-tuned models of various parameter sizes and is compatible with a\nwide range of RL algorithms."}
{"id": "2403.14293", "pdf": "https://arxiv.org/pdf/2403.14293.pdf", "abs": "https://arxiv.org/abs/2403.14293", "title": "Human-Robot Interaction and Perceived Irrationality: A Study of Trust Dynamics and Error Acknowledgment", "authors": ["Ponkoj Chandra Shill", "Md. Azizul Hakim"], "categories": ["cs.RO", "cs.HC", "cs.SE"], "comment": "8 pages, 8 figures, 1 table, Ongoing Research", "summary": "As robots become increasingly integrated into various industries,\nunderstanding how humans respond to robotic failures is critical. This study\nsystematically examines trust dynamics and system design by analyzing human\nreactions to robot failures. We conducted a four-stage survey to explore how\ntrust evolves throughout human-robot interactions. The first stage collected\ndemographic data and initial trust levels. The second stage focused on\npreliminary expectations and perceptions of robotic capabilities. The third\nstage examined interaction details, including robot precision and error\nacknowledgment. Finally, the fourth stage assessed post-interaction\nperceptions, evaluating trust dynamics, forgiveness, and willingness to\nrecommend robotic technologies. Results indicate that trust in robotic systems\nsignificantly increased when robots acknowledged their errors or limitations.\nAdditionally, participants showed greater willingness to suggest robots for\nfuture tasks, highlighting the importance of direct engagement in shaping trust\ndynamics. These findings provide valuable insights for designing more\ntransparent, responsive, and trustworthy robotic systems. By enhancing our\nunderstanding of human-robot interaction (HRI), this study contributes to the\ndevelopment of robotic technologies that foster greater public acceptance and\nadoption."}
{"id": "2505.03786", "pdf": "https://arxiv.org/pdf/2505.03786.pdf", "abs": "https://arxiv.org/abs/2505.03786", "title": "When Reasoning Beats Scale: A 1.5B Reasoning Model Outranks 13B LLMs as Discriminator", "authors": ["Md Fahim Anjum"], "categories": ["cs.LG", "cs.CL"], "comment": "12 pages, 5 figures. Code available at:\n  https://github.com/MDFahimAnjum/llm-planning-with-reasoning", "summary": "Large Language Models (LLM) with reasoning capabilities offer a promising\npath for improving candidate evaluation in planning frameworks, but their\nrelative performance against traditional non-reasoning models remains largely\nunderexplored. In this study, we benchmark a distilled 1.5B parameter reasoning\nmodel (DeepSeek-R1) against several state-of-the-art non-reasoning LLMs within\na generator-discriminator LLM planning framework for the text-to-SQL task. For\nthis, we introduce a novel method for extracting soft scores from the\nchain-of-thought (CoT) outputs from reasoning that enables fine-grained ranking\nof candidates. Our central hypothesis is that reasoning models are more\neffective discriminators than non-reasoning LLMs. Our results show that\ndistilled DeepSeek-R1-1.5B achieves up to $87\\%$ higher F1 and $3.7\\%$ better\ndiscrimination accuracy than CodeLlama-7B, as well as $3.7\\%$ higher execution\naccuracy than CodeLlama-13B, despite having significantly fewer parameters.\nFurthermore, we find that there is a limit to the logical capabilities of\nreasoning models, and only providing more context or allowing more compute\nbudget for reasoning is not enough to improve their discrimination performance.\nFinally, we demonstrate that, unlike non-reasoning LLMs, reasoning models find\ngeneration more challenging than discrimination and may underperform as\ngenerators compared to smaller non-reasoning LLMs. Our work highlights the\npotential of reasoning models as discriminators in agentic frameworks, far\noutweighing their capabilities as generators, offering insights into their\noptimal role within LLM planning infrastructures."}
{"id": "2411.02116", "pdf": "https://arxiv.org/pdf/2411.02116.pdf", "abs": "https://arxiv.org/abs/2411.02116", "title": "Advancements and limitations of LLMs in replicating human color-word associations", "authors": ["Makoto Fukushima", "Shusuke Eshita", "Hiroshige Fukuhara"], "categories": ["cs.CL", "cs.CV", "cs.GR", "cs.HC"], "comment": "20 pages, 7 figures, 3 tables", "summary": "Color-word associations play a fundamental role in human cognition and design\napplications. Large Language Models (LLMs) have become widely available and\nhave demonstrated intelligent behaviors in various benchmarks with natural\nconversation skills. However, their ability to replicate human color-word\nassociations remains understudied. We compared multiple generations of LLMs\n(from GPT-3 to GPT-4o) against human color-word associations using data\ncollected from over 10,000 Japanese participants, involving 17 colors and 80\nwords (10 word from eight categories) in Japanese. Our findings reveal a clear\nprogression in LLM performance across generations, with GPT-4o achieving the\nhighest accuracy in predicting the best voted word for each color and category.\nHowever, the highest median performance was approximately 50% even for GPT-4o\nwith visual inputs (chance level of 10%). Moreover, we found performance\nvariations across word categories and colors: while LLMs tended to excel in\ncategories such as Rhythm and Landscape, they struggled with categories such as\nEmotions. Interestingly, color discrimination ability estimated from our\ncolor-word association data showed high correlation with human color\ndiscrimination patterns, consistent with previous studies. Thus, despite\nreasonable alignment in basic color discrimination, humans and LLMs still\ndiverge systematically in the words they assign to those colors. Our study\nhighlights both the advancements in LLM capabilities and their persistent\nlimitations, raising the possibility of systematic differences in semantic\nmemory structures between humans and LLMs in representing color-word\nassociations."}
{"id": "2505.03799", "pdf": "https://arxiv.org/pdf/2505.03799.pdf", "abs": "https://arxiv.org/abs/2505.03799", "title": "Scalability Matters: Overcoming Challenges in InstructGLM with Similarity-Degree-Based Sampling", "authors": ["Hyun Lee", "Chris Yi", "Maminur Islam", "B. D. S. Aritra"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "To be published in International Joint Conference on Neural Networks\n  (IJCNN), 2025", "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in various\nnatural language processing tasks; however, their application to graph-related\nproblems remains limited, primarily due to scalability constraints and the\nabsence of dedicated mechanisms for processing graph structures. Existing\napproaches predominantly integrate LLMs with Graph Neural Networks (GNNs),\nusing GNNs as feature encoders or auxiliary components. However, directly\nencoding graph structures within LLMs has been underexplored, particularly in\nthe context of large-scale graphs where token limitations hinder effective\nrepresentation. To address these challenges, we propose SDM-InstructGLM, a\nnovel instruction-tuned Graph Language Model (InstructGLM) framework that\nenhances scalability and efficiency without relying on GNNs. Our method\nintroduces a similarity-degree-based biased random walk mechanism, which\nselectively samples and encodes graph information based on node-feature\nsimilarity and degree centrality, ensuring an adaptive and structured\nrepresentation within the LLM. This approach significantly improves token\nefficiency, mitigates information loss due to random sampling, and enhances\nperformance on graph-based tasks such as node classification and link\nprediction. Furthermore, our results demonstrate the feasibility of LLM-only\ngraph processing, enabling scalable and interpretable Graph Language Models\n(GLMs) optimized through instruction-based fine-tuning. This work paves the way\nfor GNN-free approaches to graph learning, leveraging LLMs as standalone graph\nreasoning models. Our source code is available on GitHub."}
{"id": "2412.00814", "pdf": "https://arxiv.org/pdf/2412.00814.pdf", "abs": "https://arxiv.org/abs/2412.00814", "title": "VR-Doh: Hands-on 3D Modeling in Virtual Reality", "authors": ["Zhaofeng Luo", "Zhitong Cui", "Shijian Luo", "Mengyu Chu", "Minchen Li"], "categories": ["cs.GR", "cs.HC"], "comment": "12 pages", "summary": "We introduce VR-Doh, an open-source, hands-on 3D modeling system that enables\nintuitive creation and manipulation of elastoplastic objects in Virtual Reality\n(VR). By customizing the Material Point Method (MPM) for real-time simulation\nof hand-induced large deformations and enhancing 3D Gaussian Splatting for\nseamless rendering, VR-Doh provides an interactive and immersive 3D modeling\nexperience. Users can naturally sculpt, deform, and edit objects through both\ncontact- and gesture-based hand-object interactions. To achieve real-time\nperformance, our system incorporates localized simulation techniques,\nparticle-level collision handling, and the decoupling of physical and\nappearance representations, ensuring smooth and responsive interactions. VR-Doh\nsupports both object creation and editing, enabling diverse modeling tasks such\nas designing food items, characters, and interlocking structures, all resulting\nin simulation-ready assets. User studies with both novice and experienced\nparticipants highlight the system's intuitive design, immersive feedback, and\ncreative potential. Compared to existing geometric modeling tools, VR-Doh\noffers enhanced accessibility and natural interaction, making it a powerful\ntool for creative exploration in VR."}
{"id": "2505.03810", "pdf": "https://arxiv.org/pdf/2505.03810.pdf", "abs": "https://arxiv.org/abs/2505.03810", "title": "Grouped Sequency-arranged Rotation: Optimizing Rotation Transformation for Quantization for Free", "authors": ["Euntae Choi", "Sumin Song", "Woosang Lim", "Sungjoo Yoo"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "7 pages", "summary": "Large Language Models (LLMs) face deployment challenges due to high\ncomputational costs, and while Post-Training Quantization (PTQ) offers a\nsolution, existing rotation-based methods struggle at very low bit-widths like\n2-bit. We introduce a novel, training-free approach to construct an improved\nrotation matrix, addressing the limitations of current methods. The key\ncontributions include leveraging the Walsh-Hadamard transform with sequency\nordering, which clusters similar frequency components to reduce quantization\nerror compared to standard Hadamard matrices, significantly improving\nperformance. Furthermore, we propose a Grouped Sequency-arranged Rotation (GSR)\nusing block-diagonal matrices with smaller Walsh blocks, effectively isolating\noutlier impacts and achieving performance comparable to optimization-based\nmethods without requiring any training. Our method demonstrates robust\nperformance on reasoning tasks and Perplexity (PPL) score on WikiText-2. Our\nmethod also enhances results even when applied over existing learned rotation\ntechniques."}
{"id": "2504.17393", "pdf": "https://arxiv.org/pdf/2504.17393.pdf", "abs": "https://arxiv.org/abs/2504.17393", "title": "Towards User-Centred Design of AI-Assisted Decision-Making in Law Enforcement", "authors": ["Vesna Nowack", "Dalal Alrajeh", "Carolina Gutierrez Muñoz", "Katie Thomas", "William Hobson", "Patrick Benjamin", "Catherine Hamilton-Giachritsis", "Tim Grant", "Juliane A. Kloess", "Jessica Woodhams"], "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "10 pages", "summary": "Artificial Intelligence (AI) has become an important part of our everyday\nlives, yet user requirements for designing AI-assisted systems in law\nenforcement remain unclear. To address this gap, we conducted qualitative\nresearch on decision-making within a law enforcement agency. Our study aimed to\nidentify limitations of existing practices, explore user requirements and\nunderstand the responsibilities that humans expect to undertake in these\nsystems.\n  Participants in our study highlighted the need for a system capable of\nprocessing and analysing large volumes of data efficiently to help in crime\ndetection and prevention. Additionally, the system should satisfy requirements\nfor scalability, accuracy, justification, trustworthiness and adaptability to\nbe adopted in this domain. Participants also emphasised the importance of\nhaving end users review the input data that might be challenging for AI to\ninterpret, and validate the generated output to ensure the system's accuracy.\nTo keep up with the evolving nature of the law enforcement domain, end users\nneed to help the system adapt to the changes in criminal behaviour and\ngovernment guidance, and technical experts need to regularly oversee and\nmonitor the system. Furthermore, user-friendly human interaction with the\nsystem is essential for its adoption and some of the participants confirmed\nthey would be happy to be in the loop and provide necessary feedback that the\nsystem can learn from. Finally, we argue that it is very unlikely that the\nsystem will ever achieve full automation due to the dynamic and complex nature\nof the law enforcement domain."}
{"id": "2505.03814", "pdf": "https://arxiv.org/pdf/2505.03814.pdf", "abs": "https://arxiv.org/abs/2505.03814", "title": "Cer-Eval: Certifiable and Cost-Efficient Evaluation Framework for LLMs", "authors": ["Ganghua Wang", "Zhaorun Chen", "Bo Li", "Haifeng Xu"], "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "As foundation models continue to scale, the size of trained models grows\nexponentially, presenting significant challenges for their evaluation. Current\nevaluation practices involve curating increasingly large datasets to assess the\nperformance of large language models (LLMs). However, there is a lack of\nsystematic analysis and guidance on determining the sufficiency of test data or\nselecting informative samples for evaluation. This paper introduces a\ncertifiable and cost-efficient evaluation framework for LLMs. Our framework\nadapts to different evaluation objectives and outputs confidence intervals that\ncontain true values with high probability. We use ``test sample complexity'' to\nquantify the number of test points needed for a certifiable evaluation and\nderive tight bounds on test sample complexity. Based on the developed theory,\nwe develop a partition-based algorithm, named Cer-Eval, that adaptively selects\ntest points to minimize the cost of LLM evaluation. Real-world experiments\ndemonstrate that Cer-Eval can save 20% to 40% test points across various\nbenchmarks, while maintaining an estimation error level comparable to the\ncurrent evaluation process and providing a 95% confidence guarantee."}
{"id": "2505.03828", "pdf": "https://arxiv.org/pdf/2505.03828.pdf", "abs": "https://arxiv.org/abs/2505.03828", "title": "Sentiment-Aware Recommendation Systems in E-Commerce: A Review from a Natural Language Processing Perspective", "authors": ["Yogesh Gajula"], "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": "12 pages, 2 tables, 2 figures", "summary": "E-commerce platforms generate vast volumes of user feedback, such as star\nratings, written reviews, and comments. However, most recommendation engines\nrely primarily on numerical scores, often overlooking the nuanced opinions\nembedded in free text. This paper comprehensively reviews sentiment-aware\nrecommendation systems from a natural language processing perspective, covering\nadvancements from 2023 to early 2025. It highlights the benefits of integrating\nsentiment analysis into e-commerce recommenders to enhance prediction accuracy\nand explainability through detailed opinion extraction. Our survey categorizes\nrecent work into four main approaches: deep learning classifiers that combine\nsentiment embeddings with user item interactions, transformer based methods for\nnuanced feature extraction, graph neural networks that propagate sentiment\nsignals, and conversational recommenders that adapt in real time to user\nfeedback. We summarize model architectures and demonstrate how sentiment flows\nthrough recommendation pipelines, impacting dialogue-based suggestions. Key\nchallenges include handling noisy or sarcastic text, dynamic user preferences,\nand bias mitigation. Finally, we outline research gaps and provide a roadmap\nfor developing smarter, fairer, and more user-centric recommendation tools."}
{"id": "2505.03961", "pdf": "https://arxiv.org/pdf/2505.03961.pdf", "abs": "https://arxiv.org/abs/2505.03961", "title": "The Power of Stories: Narrative Priming Shapes How LLM Agents Collaborate and Compete", "authors": ["Gerrit Großmann", "Larisa Ivanova", "Sai Leela Poduru", "Mohaddeseh Tabrizian", "Islam Mesabah", "David A. Selby", "Sebastian J. Vollmer"], "categories": ["cs.AI", "cs.CL", "cs.MA", "I.2.11; I.2.7; I.6; J.4"], "comment": "16 pages, 8 figures. Code available at\n  https://github.com/storyagents25/story-agents", "summary": "According to Yuval Noah Harari, large-scale human cooperation is driven by\nshared narratives that encode common beliefs and values. This study explores\nwhether such narratives can similarly nudge LLM agents toward collaboration. We\nuse a finitely repeated public goods game in which LLM agents choose either\ncooperative or egoistic spending strategies. We prime agents with stories\nhighlighting teamwork to different degrees and test how this influences\nnegotiation outcomes. Our experiments explore four questions:(1) How do\nnarratives influence negotiation behavior? (2) What differs when agents share\nthe same story versus different ones? (3) What happens when the agent numbers\ngrow? (4) Are agents resilient against self-serving negotiators? We find that\nstory-based priming significantly affects negotiation strategies and success\nrates. Common stories improve collaboration, benefiting each agent. By\ncontrast, priming agents with different stories reverses this effect, and those\nagents primed toward self-interest prevail. We hypothesize that these results\ncarry implications for multi-agent system design and AI alignment."}
{"id": "2505.03997", "pdf": "https://arxiv.org/pdf/2505.03997.pdf", "abs": "https://arxiv.org/abs/2505.03997", "title": "Quiet Feature Learning in Algorithmic Tasks", "authors": ["Prudhviraj Naidu", "Zixian Wang", "Leon Bergen", "Ramamohan Paturi"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "We train Transformer-based language models on ten foundational algorithmic\ntasks and observe pronounced phase transitions in their loss curves that\ndeviate from established power-law scaling trends. Over large ranges of\ncompute, the validation loss barely improves, then abruptly decreases. Probing\nthe models' internal representations reveals the learning of quiet features\nduring the stagnant phase, followed by sudden acquisition of loud features that\ncoincide with the sharp drop in loss. Our ablation experiments show that\ndisrupting a single learned feature can dramatically degrade performance,\nproviding evidence of their causal role in task performance. These findings\nchallenge the prevailing assumption that next-token predictive loss reliably\ntracks incremental progress; instead, key internal features may be developing\nbelow the surface until they coalesce, triggering a rapid performance gain."}
{"id": "2505.04066", "pdf": "https://arxiv.org/pdf/2505.04066.pdf", "abs": "https://arxiv.org/abs/2505.04066", "title": "LLAMAPIE: Proactive In-Ear Conversation Assistants", "authors": ["Tuochao Chen", "Nicholas Batchelder", "Alisa Liu", "Noah Smith", "Shyamnath Gollakota"], "categories": ["cs.LG", "cs.CL", "eess.AS"], "comment": null, "summary": "We introduce LlamaPIE, the first real-time proactive assistant designed to\nenhance human conversations through discreet, concise guidance delivered via\nhearable devices. Unlike traditional language models that require explicit user\ninvocation, this assistant operates in the background, anticipating user needs\nwithout interrupting conversations. We address several challenges, including\ndetermining when to respond, crafting concise responses that enhance\nconversations, leveraging knowledge of the user for context-aware assistance,\nand real-time, on-device processing. To achieve this, we construct a\nsemi-synthetic dialogue dataset and propose a two-model pipeline: a small model\nthat decides when to respond and a larger model that generates the response. We\nevaluate our approach on real-world datasets, demonstrating its effectiveness\nin providing helpful, unobtrusive assistance. User studies with our assistant,\nimplemented on Apple Silicon M2 hardware, show a strong preference for the\nproactive assistant over both a baseline with no assistance and a reactive\nmodel, highlighting the potential of LlamaPie to enhance live conversations."}
{"id": "2505.04171", "pdf": "https://arxiv.org/pdf/2505.04171.pdf", "abs": "https://arxiv.org/abs/2505.04171", "title": "Large Language Models are often politically extreme, usually ideologically inconsistent, and persuasive even in informational contexts", "authors": ["Nouar Aldahoul", "Hazem Ibrahim", "Matteo Varvello", "Aaron Kaufman", "Talal Rahwan", "Yasir Zaki"], "categories": ["cs.CY", "cs.CL"], "comment": "61 pages, 29 figures", "summary": "Large Language Models (LLMs) are a transformational technology, fundamentally\nchanging how people obtain information and interact with the world. As people\nbecome increasingly reliant on them for an enormous variety of tasks, a body of\nacademic research has developed to examine these models for inherent biases,\nespecially political biases, often finding them small. We challenge this\nprevailing wisdom. First, by comparing 31 LLMs to legislators, judges, and a\nnationally representative sample of U.S. voters, we show that LLMs' apparently\nsmall overall partisan preference is the net result of offsetting extreme views\non specific topics, much like moderate voters. Second, in a randomized\nexperiment, we show that LLMs can promulgate their preferences into political\npersuasiveness even in information-seeking contexts: voters randomized to\ndiscuss political issues with an LLM chatbot are as much as 5 percentage points\nmore likely to express the same preferences as that chatbot. Contrary to\nexpectations, these persuasive effects are not moderated by familiarity with\nLLMs, news consumption, or interest in politics. LLMs, especially those\ncontrolled by private companies or governments, may become a powerful and\ntargeted vector for political influence."}
{"id": "2505.04192", "pdf": "https://arxiv.org/pdf/2505.04192.pdf", "abs": "https://arxiv.org/abs/2505.04192", "title": "VideoPath-LLaVA: Pathology Diagnostic Reasoning Through Video Instruction Tuning", "authors": ["Trinh T. L. Vuong", "Jin Tae Kwak"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "We present VideoPath-LLaVA, the first large multimodal model (LMM) in\ncomputational pathology that integrates three distinct image scenarios, single\npatch images, automatically keyframe-extracted clips, and manually segmented\nvideo pathology images, to mimic the natural diagnostic process of\npathologists. By generating detailed histological descriptions and culminating\nin a definitive sign-out diagnosis, VideoPath-LLaVA bridges visual narratives\nwith diagnostic reasoning.\n  Central to our approach is the VideoPath-Instruct dataset, comprising 4278\nvideo and diagnosis-specific chain-of-thought instructional pairs sourced from\neducational histopathology videos on YouTube. Although high-quality data is\ncritical for enhancing diagnostic reasoning, its creation is time-intensive and\nlimited in volume. To overcome this challenge, we transfer knowledge from\nexisting single-image instruction datasets to train on weakly annotated,\nkeyframe-extracted clips, followed by fine-tuning on manually segmented videos.\nVideoPath-LLaVA establishes a new benchmark in pathology video analysis and\noffers a promising foundation for future AI systems that support clinical\ndecision-making through integrated visual and diagnostic reasoning. Our code,\ndata, and model are publicly available at\nhttps://github.com/trinhvg/VideoPath-LLaVA."}
{"id": "2505.04364", "pdf": "https://arxiv.org/pdf/2505.04364.pdf", "abs": "https://arxiv.org/abs/2505.04364", "title": "Benchmarking LLMs' Swarm intelligence", "authors": ["Kai Ruan", "Mowen Huang", "Ji-Rong Wen", "Hao Sun"], "categories": ["cs.MA", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) show potential for complex reasoning, yet their\ncapacity for emergent coordination in Multi-Agent Systems (MAS) when operating\nunder strict constraints-such as limited local perception and communication,\ncharacteristic of natural swarms-remains largely unexplored, particularly\nconcerning the nuances of swarm intelligence. Existing benchmarks often do not\nfully capture the unique challenges of decentralized coordination that arise\nwhen agents operate with incomplete spatio-temporal information. To bridge this\ngap, we introduce SwarmBench, a novel benchmark designed to systematically\nevaluate the swarm intelligence capabilities of LLMs acting as decentralized\nagents. SwarmBench features five foundational MAS coordination tasks within a\nconfigurable 2D grid environment, forcing agents to rely primarily on local\nsensory input (k x k view) and local communication. We propose metrics for\ncoordination effectiveness and analyze emergent group dynamics. Evaluating\nseveral leading LLMs in a zero-shot setting, we find significant performance\nvariations across tasks, highlighting the difficulties posed by local\ninformation constraints. While some coordination emerges, results indicate\nlimitations in robust planning and strategy formation under uncertainty in\nthese decentralized scenarios. Assessing LLMs under swarm-like conditions is\ncrucial for realizing their potential in future decentralized systems. We\nrelease SwarmBench as an open, extensible toolkit-built upon a customizable and\nscalable physical system with defined mechanical properties. It provides\nenvironments, prompts, evaluation scripts, and the comprehensive experimental\ndatasets generated, aiming to foster reproducible research into LLM-based MAS\ncoordination and the theoretical underpinnings of Embodied MAS. Our code\nrepository is available at https://github.com/x66ccff/swarmbench."}
{"id": "2505.04457", "pdf": "https://arxiv.org/pdf/2505.04457.pdf", "abs": "https://arxiv.org/abs/2505.04457", "title": "Miipher-2: A Universal Speech Restoration Model for Million-Hour Scale Data Restoration", "authors": ["Shigeki Karita", "Yuma Koizumi", "Heiga Zen", "Haruko Ishikawa", "Robin Scheibler", "Michiel Bacchiani"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "Training data cleaning is a new application for generative model-based speech\nrestoration (SR). This paper introduces Miipher-2, an SR model designed for\nmillion-hour scale data, for training data cleaning for large-scale generative\nmodels like large language models. Key challenges addressed include\ngeneralization to unseen languages, operation without explicit conditioning\n(e.g., text, speaker ID), and computational efficiency. Miipher-2 utilizes a\nfrozen, pre-trained Universal Speech Model (USM), supporting over 300\nlanguages, as a robust, conditioning-free feature extractor. To optimize\nefficiency and minimize memory, Miipher-2 incorporates parallel adapters for\npredicting clean USM features from noisy inputs and employs the WaneFit neural\nvocoder for waveform synthesis. These components were trained on 3,000 hours of\nmulti-lingual, studio-quality recordings with augmented degradations, while USM\nparameters remained fixed. Experimental results demonstrate Miipher-2's\nsuperior or comparable performance to conventional SR models in\nword-error-rate, speaker similarity, and both objective and subjective sound\nquality scores across all tested languages. Miipher-2 operates efficiently on\nconsumer-grade accelerators, achieving a real-time factor of 0.0078, enabling\nthe processing of a million-hour speech dataset in approximately three days\nusing only 100 such accelerators."}
{"id": "2505.04528", "pdf": "https://arxiv.org/pdf/2505.04528.pdf", "abs": "https://arxiv.org/abs/2505.04528", "title": "Beyond Theorem Proving: Formulation, Framework and Benchmark for Formal Problem-Solving", "authors": ["Qi Liu", "Xinhao Zheng", "Renqiu Xia", "Xingzhi Qi", "Qinxiang Cao", "Junchi Yan"], "categories": ["cs.AI", "cs.CL", "cs.LO"], "comment": "42 pages, 3 figures", "summary": "As a seemingly self-explanatory task, problem-solving has been a significant\ncomponent of science and engineering. However, a general yet concrete\nformulation of problem-solving itself is missing. With the recent development\nof AI-based problem-solving agents, the demand for process-level verifiability\nis rapidly increasing yet underexplored. To fill these gaps, we present a\nprincipled formulation of problem-solving as a deterministic Markov decision\nprocess; a novel framework, FPS (Formal Problem-Solving), which utilizes\nexisting FTP (formal theorem proving) environments to perform process-verified\nproblem-solving; and D-FPS (Deductive FPS), decoupling solving and answer\nverification for better human-alignment. The expressiveness, soundness and\ncompleteness of the frameworks are proven. We construct three benchmarks on\nproblem-solving: FormalMath500, a formalization of a subset of the MATH500\nbenchmark; MiniF2F-Solving and PutnamBench-Solving, adaptations of FTP\nbenchmarks MiniF2F and PutnamBench. For faithful, interpretable, and\nhuman-aligned evaluation, we propose RPE (Restricted Propositional\nEquivalence), a symbolic approach to determine the correctness of answers by\nformal verification. We evaluate four prevalent FTP models and two prompting\nmethods as baselines, solving at most 23.77% of FormalMath500, 27.47% of\nMiniF2F-Solving, and 0.31% of PutnamBench-Solving."}
{"id": "2305.16867", "pdf": "https://arxiv.org/pdf/2305.16867.pdf", "abs": "https://arxiv.org/abs/2305.16867", "title": "Playing repeated games with Large Language Models", "authors": ["Elif Akata", "Lion Schulz", "Julian Coda-Forno", "Seong Joon Oh", "Matthias Bethge", "Eric Schulz"], "categories": ["cs.CL"], "comment": null, "summary": "LLMs are increasingly used in applications where they interact with humans\nand other agents. We propose to use behavioural game theory to study LLM's\ncooperation and coordination behaviour. We let different LLMs play finitely\nrepeated $2\\times2$ games with each other, with human-like strategies, and\nactual human players. Our results show that LLMs perform particularly well at\nself-interested games like the iterated Prisoner's Dilemma family. However,\nthey behave sub-optimally in games that require coordination, like the Battle\nof the Sexes. We verify that these behavioural signatures are stable across\nrobustness checks. We additionally show how GPT-4's behaviour can be modulated\nby providing additional information about its opponent and by using a \"social\nchain-of-thought\" (SCoT) strategy. This also leads to better scores and more\nsuccessful coordination when interacting with human players. These results\nenrich our understanding of LLM's social behaviour and pave the way for a\nbehavioural game theory for machines."}
{"id": "2308.15022", "pdf": "https://arxiv.org/pdf/2308.15022.pdf", "abs": "https://arxiv.org/abs/2308.15022", "title": "Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models", "authors": ["Qingyue Wang", "Yanhe Fu", "Yanan Cao", "Shuai Wang", "Zhiliang Tian", "Liang Ding"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recently, large language models (LLMs), such as GPT-4, stand out remarkable\nconversational abilities, enabling them to engage in dynamic and contextually\nrelevant dialogues across a wide range of topics. However, given a long\nconversation, these chatbots fail to recall past information and tend to\ngenerate inconsistent responses. To address this, we propose to recursively\ngenerate summaries/ memory using large language models (LLMs) to enhance\nlong-term memory ability. Specifically, our method first stimulates LLMs to\nmemorize small dialogue contexts and then recursively produce new memory using\nprevious memory and following contexts. Finally, the chatbot can easily\ngenerate a highly consistent response with the help of the latest memory. We\nevaluate our method on both open and closed LLMs, and the experiments on the\nwidely-used public dataset show that our method can generate more consistent\nresponses in a long-context conversation. Also, we show that our strategy could\nnicely complement both long-context (e.g., 8K and 16K) and retrieval-enhanced\nLLMs, bringing further long-term dialogue performance. Notably, our method is a\npotential solution to enable the LLM to model the extremely long context. The\ncode and scripts will be released later."}
{"id": "2403.19346", "pdf": "https://arxiv.org/pdf/2403.19346.pdf", "abs": "https://arxiv.org/abs/2403.19346", "title": "Large Language Models Are Struggle to Cope with Unreasonability in Math Problems", "authors": ["Jingyuan Ma", "Damai Dai", "Zihang Yuan", "Rui li", "Weilin Luo", "Bin Wang", "Qun Liu", "Lei Sha", "Zhifang Sui"], "categories": ["cs.CL"], "comment": "32 pages, 8 figures", "summary": "Recent research have demonstrated LLMs' impressive performance in math and\nreasoning. However, the capacity of LLMs to address math problems under\nunconventional conditions, such as internal inconsistencies and flawed\nassumptions, remains largely unexplored. In this paper, we propose a novel\nbenchmark Unreasonable Math Problem (UMP) designed to assess LLMs' ability to\nrecognize and respond to unreasonability in math problem. The benchmark\nconsists of a carefully curated collection of unreasonable math questions\nacross diverse types. Based on extensive experiments covering 19 LLMs, we\nobserve that even state-of-the-art models such as GPT-4o achieve only limited\nperformance of 0.6 in UMP, while reasoning models such as DeepSeek-R1 are prone\nto overthinking and unstable. We further explore strategies for improving the\nrecognition of unreasonable inputs, shedding light on both the possibility and\nlimitations of LLMs in this challenging setting."}
{"id": "2406.01495", "pdf": "https://arxiv.org/pdf/2406.01495.pdf", "abs": "https://arxiv.org/abs/2406.01495", "title": "Re-ReST: Reflection-Reinforced Self-Training for Language Agents", "authors": ["Zi-Yi Dou", "Cheng-Fu Yang", "Xueqing Wu", "Kai-Wei Chang", "Nanyun Peng"], "categories": ["cs.CL"], "comment": null, "summary": "Finetuning language agents with reasoning-action trajectories is effective,\nbut obtaining these trajectories from human annotations or stronger models is\ncostly and sometimes impractical. In this paper, we investigate the use of\nself-training in language agents, which can generate supervision from the agent\nitself, offering a promising alternative without relying on human or stronger\nmodel demonstrations. Self-training, however, requires high-quality\nmodel-generated samples, which are hard to obtain for challenging language\nagent tasks. To address this, we present Reflection-Reinforced Self-Training\n(Re-ReST), which uses a \\textit{reflector} to refine low-quality generated\nsamples during self-training. The reflector takes the agent's output and\nfeedback from an external environment (e.g., unit test results in code\ngeneration) to produce improved samples. This technique enhances the quality of\ninferior samples and efficiently enriches the self-training dataset with\nhigher-quality samples. We conduct extensive experiments on open-source\nlanguage agents across tasks, including multi-hop question answering,\nsequential decision-making, code generation, visual question answering, and\ntext-to-image generation. The results demonstrate the effectiveness of\nself-training and Re-ReST in language agent tasks, with self-training improving\nbaselines by 7.6\\% on HotpotQA and 28.4\\% on AlfWorld, and Re-ReST further\nboosting performance by 2.0\\% and 14.1\\%, respectively. Our studies also\nconfirm the efficiency of using a reflector to generate high-quality samples\nfor self-training. Moreover, we demonstrate a method to employ reflection\nduring inference without ground-truth feedback, addressing the limitation of\nprevious reflection work. Our code is released at\nhttps://github.com/PlusLabNLP/Re-ReST."}
{"id": "2406.02044", "pdf": "https://arxiv.org/pdf/2406.02044.pdf", "abs": "https://arxiv.org/abs/2406.02044", "title": "Towards Universal and Black-Box Query-Response Only Attack on LLMs with QROA", "authors": ["Hussein Jawad", "Yassine Chenik", "Nicolas J. -B. Brunel"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The rapid adoption of Large Language Models (LLMs) has exposed critical\nsecurity and ethical vulnerabilities, particularly their susceptibility to\nadversarial manipulations. This paper introduces QROA, a novel black-box\njailbreak method designed to identify adversarial suffixes that can bypass LLM\nalignment safeguards when appended to a malicious instruction. Unlike existing\nsuffix-based jailbreak approaches, QROA does not require access to the model's\nlogit or any other internal information. It also eliminates reliance on\nhuman-crafted templates, operating solely through the standard query-response\ninterface of LLMs. By framing the attack as an optimization bandit problem,\nQROA employs a surrogate model and token level optimization to efficiently\nexplore suffix variations. Furthermore, we propose QROA-UNV, an extension that\nidentifies universal adversarial suffixes for individual models, enabling\none-query jailbreaks across a wide range of instructions. Testing on multiple\nmodels demonstrates Attack Success Rate (ASR) greater than 80\\%. These findings\nhighlight critical vulnerabilities, emphasize the need for advanced defenses,\nand contribute to the development of more robust safety evaluations for secure\nAI deployment. The code is made public on the following link:\nhttps://github.com/qroa/QROA"}
{"id": "2406.18501", "pdf": "https://arxiv.org/pdf/2406.18501.pdf", "abs": "https://arxiv.org/abs/2406.18501", "title": "Is In-Context Learning a Type of Error-Driven Learning? Evidence from the Inverse Frequency Effect in Structural Priming", "authors": ["Zhenghao Zhou", "Robert Frank", "R. Thomas McCoy"], "categories": ["cs.CL"], "comment": "This version is accepted to NAACL 2025\n  (https://aclanthology.org/2025.naacl-long.586/)", "summary": "Large language models (LLMs) have shown the emergent capability of in-context\nlearning (ICL). One line of research has claimed that ICL is functionally\nequivalent to gradient descent, a type of error-driven learning mechanism. In\nthis paper, we introduce a new way of diagnosing whether ICL is functionally\nperforming error-driven learning. Our approach is based on the inverse\nfrequency effect (IFE) -- a phenomenon in which an agent's behavior is\ninfluenced to a greater degree when presented with improbable examples as\ncompared to more likely ones. The IFE has previously been identified in\npsycholinguistics where humans exhibit the IFE in the context of structural\npriming (the tendency for people to produce sentence structures they have\nencountered recently). In that context, the IFE has been used as evidence that\nhuman structural priming must involve error-driven learning mechanisms. In our\nexperiments, we simulated structural priming with ICL and found that LLMs\nindeed display the IFE, with the effect being stronger in larger models. We\nconclude that at least in the case we studied, ICL is indeed a type of\nerror-driven learning, supporting the hypothesis that an error signal is\nimplicitly computed in the forward pass during ICL. Our results suggest that\nboth humans and LLMs make use of error-driven processing mechanisms in on-line\nprocessing."}
{"id": "2408.13184", "pdf": "https://arxiv.org/pdf/2408.13184.pdf", "abs": "https://arxiv.org/abs/2408.13184", "title": "Can LLM be a Good Path Planner based on Prompt Engineering? Mitigating the Hallucination for Path Planning", "authors": ["Hourui Deng", "Hongjie Zhang", "Jie Ou", "Chaosheng Feng"], "categories": ["cs.CL"], "comment": "Accepted by ICIC 2025", "summary": "Spatial reasoning in Large Language Models (LLMs) is the foundation for\nembodied intelligence. However, even in simple maze environments, LLMs still\nencounter challenges in long-term path-planning, primarily influenced by their\nspatial hallucination and context inconsistency hallucination by long-term\nreasoning. To address this challenge, this study proposes an innovative model,\nSpatial-to-Relational Transformation and Curriculum Q-Learning (S2RCQL). To\naddress the spatial hallucination of LLMs, we propose the Spatial-to-Relational\napproach, which transforms spatial prompts into entity relations and paths\nrepresenting entity relation chains. This approach fully taps the potential of\nLLMs in terms of sequential thinking. As a result, we design a path-planning\nalgorithm based on Q-learning to mitigate the context inconsistency\nhallucination, which enhances the reasoning ability of LLMs. Using the Q-value\nof state-action as auxiliary information for prompts, we correct the\nhallucinations of LLMs, thereby guiding LLMs to learn the optimal path.\nFinally, we propose a reverse curriculum learning technique based on LLMs to\nfurther mitigate the context inconsistency hallucination. LLMs can rapidly\naccumulate successful experiences by reducing task difficulty and leveraging\nthem to tackle more complex tasks. We performed comprehensive experiments based\non Baidu's self-developed LLM: ERNIE-Bot 4.0. The results showed that our\nS2RCQL achieved a 23%--40% improvement in both success and optimality rates\ncompared with advanced prompt engineering."}
{"id": "2411.02116", "pdf": "https://arxiv.org/pdf/2411.02116.pdf", "abs": "https://arxiv.org/abs/2411.02116", "title": "Advancements and limitations of LLMs in replicating human color-word associations", "authors": ["Makoto Fukushima", "Shusuke Eshita", "Hiroshige Fukuhara"], "categories": ["cs.CL", "cs.CV", "cs.GR", "cs.HC"], "comment": "20 pages, 7 figures, 3 tables", "summary": "Color-word associations play a fundamental role in human cognition and design\napplications. Large Language Models (LLMs) have become widely available and\nhave demonstrated intelligent behaviors in various benchmarks with natural\nconversation skills. However, their ability to replicate human color-word\nassociations remains understudied. We compared multiple generations of LLMs\n(from GPT-3 to GPT-4o) against human color-word associations using data\ncollected from over 10,000 Japanese participants, involving 17 colors and 80\nwords (10 word from eight categories) in Japanese. Our findings reveal a clear\nprogression in LLM performance across generations, with GPT-4o achieving the\nhighest accuracy in predicting the best voted word for each color and category.\nHowever, the highest median performance was approximately 50% even for GPT-4o\nwith visual inputs (chance level of 10%). Moreover, we found performance\nvariations across word categories and colors: while LLMs tended to excel in\ncategories such as Rhythm and Landscape, they struggled with categories such as\nEmotions. Interestingly, color discrimination ability estimated from our\ncolor-word association data showed high correlation with human color\ndiscrimination patterns, consistent with previous studies. Thus, despite\nreasonable alignment in basic color discrimination, humans and LLMs still\ndiverge systematically in the words they assign to those colors. Our study\nhighlights both the advancements in LLM capabilities and their persistent\nlimitations, raising the possibility of systematic differences in semantic\nmemory structures between humans and LLMs in representing color-word\nassociations."}
{"id": "2501.05040", "pdf": "https://arxiv.org/pdf/2501.05040.pdf", "abs": "https://arxiv.org/abs/2501.05040", "title": "SWE-Fixer: Training Open-Source LLMs for Effective and Efficient GitHub Issue Resolution", "authors": ["Chengxing Xie", "Bowen Li", "Chang Gao", "He Du", "Wai Lam", "Difan Zou", "Kai Chen"], "categories": ["cs.CL"], "comment": "Our code, data, and model will be released at\n  https://github.com/InternLM/SWE-Fixer", "summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency across\na variety of complex tasks. One significant application of LLMs is in tackling\nsoftware engineering challenges, particularly in resolving real-world tasks on\nGitHub by fixing code based on the issues reported by the users. However, many\ncurrent approaches rely on proprietary LLMs, which limits reproducibility,\naccessibility, and transparency. The critical components of LLMs for addressing\nsoftware engineering issues and how their capabilities can be effectively\nenhanced remain unclear. To address these challenges, we introduce SWE-Fixer, a\nnovel open-source framework designed to effectively and efficiently resolve\nGitHub issues. SWE-Fixer comprises two essential modules: a code file retrieval\nmodule and a code editing module. The retrieval module employs BM25 along with\na lightweight model to achieve coarse-to-fine file retrieval. Subsequently, the\ncode editing module utilizes the other model to generate patches for the\nidentified files. To mitigate the lack of publicly available datasets, we\ncompile an extensive dataset that includes 110K GitHub issues along with their\ncorresponding patches and train the two models of SWE-Fixer separately. We\nassess our approach on the SWE-Bench Lite and Verified benchmarks, achieving\ncompetitive performance among open-source models with scores of 22.0% and\n30.2%. Furthermore, SWE-Fixer reaches state-of-the-art performance (24.7% on\nLite and 32.8% on Verified) with PASS_TO_PASS (P2P) filtering. Additionally,\nour approach requires only two model calls per instance, making it\nsignificantly more efficient than existing methods. These results highlight the\neffectiveness of SWE-Fixer in real-world code-fixing scenarios. We will make\nour model, dataset, and code publicly available at\nhttps://github.com/InternLM/SWE-Fixer."}
{"id": "2502.00290", "pdf": "https://arxiv.org/pdf/2502.00290.pdf", "abs": "https://arxiv.org/abs/2502.00290", "title": "Estimating LLM Uncertainty with Logits", "authors": ["Huan Ma", "Jingdong Chen", "Joey Tianyi Zhou", "Guangyu Wang", "Changqing Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "Fixed some data errors in Table 1", "summary": "Over the past few years, Large Language Models (LLMs) have developed rapidly\nand are widely applied in various domains. However, LLMs face the issue of\nhallucinations, generating responses that may be unreliable when the models\nlack relevant knowledge. To be aware of potential hallucinations, uncertainty\nestimation methods have been introduced, and most of them have confirmed that\nreliability lies in critical tokens. However, probability-based methods perform\npoorly in identifying token reliability, limiting their practical utility. In\nthis paper, we reveal that the probability-based method fails to estimate token\nreliability due to the loss of evidence strength information which is\naccumulated in the training stage. Therefore, we present Logits-induced token\nuncertainty (LogTokU), a framework for estimating decoupled token uncertainty\nin LLMs, enabling real-time uncertainty estimation without requiring multiple\nsampling processes. We employ evidence modeling to implement LogTokU and use\nthe estimated uncertainty to guide downstream tasks. The experimental results\ndemonstrate that LogTokU has significant effectiveness and promise."}
{"id": "2503.01496", "pdf": "https://arxiv.org/pdf/2503.01496.pdf", "abs": "https://arxiv.org/abs/2503.01496", "title": "Liger: Linearizing Large Language Models to Gated Recurrent Structures", "authors": ["Disen Lan", "Weigao Sun", "Jiaxi Hu", "Jusen Du", "Yu Cheng"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by ICML 2025, 15 pages", "summary": "Transformers with linear recurrent modeling offer linear-time training and\nconstant-memory inference. Despite their demonstrated efficiency and\nperformance, pretraining such non-standard architectures from scratch remains\ncostly and risky. The linearization of large language models (LLMs) transforms\npretrained standard models into linear recurrent structures, enabling more\nefficient deployment. However, current linearization methods typically\nintroduce additional feature map modules that require extensive fine-tuning and\noverlook the gating mechanisms used in state-of-the-art linear recurrent\nmodels. To address these issues, this paper presents Liger, short for\nLinearizing LLMs to gated recurrent structures. Liger is a novel approach for\nconverting pretrained LLMs into gated linear recurrent models without adding\nextra parameters. It repurposes the pretrained key matrix weights to construct\ndiverse gating mechanisms, facilitating the formation of various gated\nrecurrent structures while avoiding the need to train additional components\nfrom scratch. Using lightweight fine-tuning with Low-Rank Adaptation (LoRA),\nLiger restores the performance of the linearized gated recurrent models to\nmatch that of the original LLMs. Additionally, we introduce Liger Attention, an\nintra-layer hybrid attention mechanism, which significantly recovers 93\\% of\nthe Transformer-based LLM at 0.02\\% pre-training tokens during the\nlinearization process, achieving competitive results across multiple\nbenchmarks, as validated on models ranging from 1B to 8B parameters. Code is\navailable at https://github.com/OpenSparseLLMs/Linearization."}
{"id": "2503.03186", "pdf": "https://arxiv.org/pdf/2503.03186.pdf", "abs": "https://arxiv.org/abs/2503.03186", "title": "Designing Speech Technologies for Australian Aboriginal English: Opportunities, Risks and Participation", "authors": ["Ben Hutchinson", "Celeste Rodríguez Louro", "Glenys Collard", "Ned Cooper"], "categories": ["cs.CL"], "comment": null, "summary": "In Australia, post-contact language varieties, including creoles and local\nvarieties of international languages, emerged as a result of forced contact\nbetween Indigenous communities and English speakers. These contact varieties\nare widely used, yet are poorly supported by language technologies. This gap\npresents barriers to participation in civil and economic society for Indigenous\ncommunities using these varieties, and reproduces minoritisation of\ncontemporary Indigenous sociolinguistic identities. This paper concerns three\nquestions regarding this context. First, can speech technologies support\nspeakers of Australian Aboriginal English, a local indigenised variety of\nEnglish? Second, what risks are inherent in such a project? Third, what\ntechnology development practices are appropriate for this context, and how can\nresearchers integrate meaningful community participation in order to mitigate\nrisks? We argue that opportunities do exist -- as well as risks -- and\ndemonstrate this through a case study exploring design practices in a\nreal-world project aiming to improve speech technologies for Australian\nAboriginal English. We discuss how we integrated culturally appropriate and\nparticipatory processes throughout the project. We call for increased support\nfor languages used by Indigenous communities, including contact varieties,\nwhich provide practical economic and socio-cultural benefits, provided that\nparticipatory and culturally safe practices are enacted."}
{"id": "2503.11280", "pdf": "https://arxiv.org/pdf/2503.11280.pdf", "abs": "https://arxiv.org/abs/2503.11280", "title": "High-Dimensional Interlingual Representations of Large Language Models", "authors": ["Bryan Wilie", "Samuel Cahyawijaya", "Junxian He", "Pascale Fung"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) trained on massive multilingual datasets hint at\nthe formation of interlingual constructs--a shared subspace in the\nrepresentation space. However, evidence regarding this phenomenon is mixed,\nleaving it unclear whether these models truly develop unified interlingual\nrepresentations, or present a partially aligned constructs. We explore 31\ndiverse languages varying on their resource-levels, typologies, and\ngeographical regions; and find that multilingual LLMs exhibit inconsistent\ncross-lingual alignments. To address this, we propose an interlingual\nrepresentation framework identifying both the shared interlingual semantic\nsubspace and fragmented components, existed due to representational\nlimitations. We introduce Interlingual Local Overlap (ILO) score to quantify\ninterlingual alignment by comparing the local neighborhood structures of\nhigh-dimensional representations. We utilize ILO to investigate the impact of\nsingle-language fine-tuning on the interlingual representations in multilingual\nLLMs. Our results indicate that training exclusively on a single language\ndisrupts the alignment in early layers, while freezing these layers preserves\nthe alignment of interlingual representations, leading to improved\ncross-lingual generalization. These results validate our framework and metric\nfor evaluating interlingual representation, and further underscore that\ninterlingual alignment is crucial for scalable multilingual learning."}
{"id": "2503.21813", "pdf": "https://arxiv.org/pdf/2503.21813.pdf", "abs": "https://arxiv.org/abs/2503.21813", "title": "OAEI-LLM-T: A TBox Benchmark Dataset for Understanding Large Language Model Hallucinations in Ontology Matching", "authors": ["Zhangcheng Qiang"], "categories": ["cs.CL", "cs.IR"], "comment": "15 pages, 4 figures, 5 tables, 2 prompt templates", "summary": "Hallucinations are often inevitable in downstream tasks using large language\nmodels (LLMs). To tackle the substantial challenge of addressing hallucinations\nfor LLM-based ontology matching (OM) systems, we introduce a new benchmark\ndataset called OAEI-LLM-T. The dataset evolves from the TBox (i.e.\nschema-matching) datasets in the Ontology Alignment Evaluation Initiative\n(OAEI), capturing hallucinations of different LLMs performing OM tasks. These\nOM-specific hallucinations are carefully classified into two primary categories\nand six sub-categories. We showcase the usefulness of the dataset in\nconstructing the LLM leaderboard and fine-tuning foundational LLMs for\nLLM-based OM systems."}
{"id": "2504.18884", "pdf": "https://arxiv.org/pdf/2504.18884.pdf", "abs": "https://arxiv.org/abs/2504.18884", "title": "A Simple Ensemble Strategy for LLM Inference: Towards More Stable Text Classification", "authors": ["Junichiro Niimi"], "categories": ["cs.CL", "cs.AI"], "comment": "This manuscript has been accepted for the 30th International\n  Conference on Natural Language \\& Information Systems (NLDB 2025) and will\n  appear in Springer Lecture Notes in Computer Science (LNCS)", "summary": "With the advance of large language models (LLMs), LLMs have been utilized for\nthe various tasks. However, the issues of variability and reproducibility of\nresults from each trial of LLMs have been largely overlooked in existing\nliterature while actual human annotation uses majority voting to resolve\ndisagreements among annotators. Therefore, this study introduces the\nstraightforward ensemble strategy to a sentiment analysis using LLMs. As the\nresults, we demonstrate that the ensemble of multiple inference using\nmedium-sized LLMs produces more robust and accurate results than using a large\nmodel with a single attempt with reducing RMSE by 18.6%."}
{"id": "2504.20752", "pdf": "https://arxiv.org/pdf/2504.20752.pdf", "abs": "https://arxiv.org/abs/2504.20752", "title": "Grokking in the Wild: Data Augmentation for Real-World Multi-Hop Reasoning with Transformers", "authors": ["Roman Abramov", "Felix Steinbauer", "Gjergji Kasneci"], "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; I.2.6; I.2.3; I.7"], "comment": "Accepted to the International Conference on Machine Learning (ICML)\n  2025", "summary": "Transformers have achieved great success in numerous NLP tasks but continue\nto exhibit notable gaps in multi-step factual reasoning, especially when\nreal-world knowledge is sparse. Recent advances in grokking have demonstrated\nthat neural networks can transition from memorizing to perfectly generalizing\nonce they detect underlying logical patterns - yet these studies have primarily\nused small, synthetic tasks. In this paper, for the first time, we extend\ngrokking to real-world factual data and address the challenge of dataset\nsparsity by augmenting existing knowledge graphs with carefully designed\nsynthetic data to raise the ratio $\\phi_r$ of inferred facts to atomic facts\nabove the threshold required for grokking. Surprisingly, we find that even\nfactually incorrect synthetic data can strengthen emergent reasoning circuits\nrather than degrade accuracy, as it forces the model to rely on relational\nstructure rather than memorization. When evaluated on multi-hop reasoning\nbenchmarks, our approach achieves up to 95-100% accuracy on 2WikiMultiHopQA -\nsubstantially improving over strong baselines and matching or exceeding current\nstate-of-the-art results. We further provide an in-depth analysis of how\nincreasing $\\phi_r$ drives the formation of generalizing circuits inside\nTransformers. Our findings suggest that grokking-based data augmentation can\nunlock implicit multi-hop reasoning capabilities, opening the door to more\nrobust and interpretable factual reasoning in large-scale language models."}
{"id": "2505.00661", "pdf": "https://arxiv.org/pdf/2505.00661.pdf", "abs": "https://arxiv.org/abs/2505.00661", "title": "On the generalization of language models from in-context learning and finetuning: a controlled study", "authors": ["Andrew K. Lampinen", "Arslan Chaudhry", "Stephanie C. Y. Chan", "Cody Wild", "Diane Wan", "Alex Ku", "Jörg Bornschein", "Razvan Pascanu", "Murray Shanahan", "James L. McClelland"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models exhibit exciting capabilities, yet can show\nsurprisingly narrow generalization from finetuning. E.g. they can fail to\ngeneralize to simple reversals of relations they are trained on, or fail to\nmake simple logical deductions based on trained information. These failures to\ngeneralize from fine-tuning can hinder practical application of these models.\nOn the other hand, language models' in-context learning shows different\ninductive biases, and can generalize better in some cases. Here, we explore\nthese differences in generalization between in-context- and fine-tuning-based\nlearning. To do so, we constructed several novel datasets to evaluate and\nimprove models' abilities to generalize from finetuning data. The datasets are\ndesigned to create clean tests of generalization, by isolating the knowledge in\nthe dataset from that in pretraining. We expose pretrained large models to\ncontrolled subsets of the information in these datasets -- either in context,\nor through fine-tuning -- and evaluate their performance on test sets that\nrequire various types of generalization. We find overall that in data-matched\nsettings, in-context learning can generalize more flexibly than fine-tuning\n(though we also find some qualifications of prior findings, such as cases when\nfine-tuning can generalize to reversals embedded in a larger structure of\nknowledge). We build on these findings to propose a method to enable improved\ngeneralization from fine-tuning: adding in-context inferences to finetuning\ndata. We show that this method improves generalization across various splits of\nour datasets and other benchmarks. Our results have implications for\nunderstanding the inductive biases of different modes of learning in language\nmodels, and practically improving their performance."}
{"id": "2505.00977", "pdf": "https://arxiv.org/pdf/2505.00977.pdf", "abs": "https://arxiv.org/abs/2505.00977", "title": "A Character-based Diffusion Embedding Algorithm for Enhancing the Generation Quality of Generative Linguistic Steganographic Texts", "authors": ["Yingquan Chen", "Qianmu Li", "Xiaocong Wu", "Huifeng Li", "Qing Chang"], "categories": ["cs.CL", "cs.CR"], "comment": "we need to clarify authorship and make further revisions in\n  collaboration with co-authors", "summary": "Generating high-quality steganographic text is a fundamental challenge in the\nfield of generative linguistic steganography. This challenge arises primarily\nfrom two aspects: firstly, the capabilities of existing models in text\ngeneration are limited; secondly, embedding algorithms fail to effectively\nmitigate the negative impacts of sensitive information's properties, such as\nsemantic content or randomness. Specifically, to ensure that the recipient can\naccurately extract hidden information, embedding algorithms often have to\nconsider selecting candidate words with relatively low probabilities. This\nphenomenon leads to a decrease in the number of high-probability candidate\nwords and an increase in low-probability candidate words, thereby compromising\nthe semantic coherence and logical fluency of the steganographic text and\ndiminishing the overall quality of the generated steganographic material. To\naddress this issue, this paper proposes a novel embedding algorithm,\ncharacter-based diffusion embedding algorithm (CDEA). Unlike existing embedding\nalgorithms that strive to eliminate the impact of sensitive information's\nproperties on the generation process, CDEA leverages sensitive information's\nproperties. It enhances the selection frequency of high-probability candidate\nwords in the candidate pool based on general statistical properties at the\ncharacter level and grouping methods based on power-law distributions, while\nreducing the selection frequency of low-probability candidate words in the\ncandidate pool. Furthermore, to ensure the effective transformation of\nsensitive information in long sequences, we also introduce the XLNet model.\nExperimental results demonstrate that the combination of CDEA and XLNet\nsignificantly improves the quality of generated steganographic text,\nparticularly in terms of perceptual-imperceptibility."}
{"id": "2505.02366", "pdf": "https://arxiv.org/pdf/2505.02366.pdf", "abs": "https://arxiv.org/abs/2505.02366", "title": "JTCSE: Joint Tensor-Modulus Constraints and Cross-Attention for Unsupervised Contrastive Learning of Sentence Embeddings", "authors": ["Tianyu Zong", "Hongzhu Yi", "Bingkang Shi", "Yuanxiang Wang", "Jungang Xu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Unsupervised contrastive learning has become a hot research topic in natural\nlanguage processing. Existing works usually aim at constraining the orientation\ndistribution of the representations of positive and negative samples in the\nhigh-dimensional semantic space in contrastive learning, but the semantic\nrepresentation tensor possesses both modulus and orientation features, and the\nexisting works ignore the modulus feature of the representations and cause\ninsufficient contrastive learning. % Therefore, we firstly propose a training\nobjective that aims at modulus constraints on the semantic representation\ntensor, to strengthen the alignment between the positive samples in contrastive\nlearning. Therefore, we first propose a training objective that is designed to\nimpose modulus constraints on the semantic representation tensor, to strengthen\nthe alignment between positive samples in contrastive learning. Then, the\nBERT-like model suffers from the phenomenon of sinking attention, leading to a\nlack of attention to CLS tokens that aggregate semantic information. In\nresponse, we propose a cross-attention structure among the twin-tower ensemble\nmodels to enhance the model's attention to CLS token and optimize the quality\nof CLS Pooling. Combining the above two motivations, we propose a new\n\\textbf{J}oint \\textbf{T}ensor representation modulus constraint and\n\\textbf{C}ross-attention unsupervised contrastive learning \\textbf{S}entence\n\\textbf{E}mbedding representation framework JTCSE, which we evaluate in seven\nsemantic text similarity computation tasks, and the experimental results show\nthat JTCSE's twin-tower ensemble model and single-tower distillation model\noutperform the other baselines and become the current SOTA. In addition, we\nhave conducted an extensive zero-shot downstream task evaluation, which shows\nthat JTCSE outperforms other baselines overall on more than 130 tasks."}
{"id": "2311.18681", "pdf": "https://arxiv.org/pdf/2311.18681.pdf", "abs": "https://arxiv.org/abs/2311.18681", "title": "RaDialog: A Large Vision-Language Model for Radiology Report Generation and Conversational Assistance", "authors": ["Chantal Pellegrini", "Ege Özsoy", "Benjamin Busam", "Nassir Navab", "Matthias Keicher"], "categories": ["cs.CV", "cs.CL"], "comment": "Accepted for publication at MIDL 2025", "summary": "Conversational AI tools that can generate and discuss clinically correct\nradiology reports for a given medical image have the potential to transform\nradiology. Such a human-in-the-loop radiology assistant could facilitate a\ncollaborative diagnostic process, thus saving time and improving the quality of\nreports. Towards this goal, we introduce RaDialog, the first thoroughly\nevaluated and publicly available large vision-language model for radiology\nreport generation and interactive dialog. RaDialog effectively integrates\nvisual image features and structured pathology findings with a large language\nmodel (LLM) while simultaneously adapting it to a specialized domain using\nparameter-efficient fine-tuning. To keep the conversational abilities of the\nunderlying LLM, we propose a comprehensive, semi-automatically labeled,\nimage-grounded instruct dataset for chest X-ray radiology tasks. By training\nwith this dataset, our method achieves state-of-the-art clinical correctness in\nreport generation and shows impressive abilities in interactive tasks such as\ncorrecting reports and answering questions, serving as a foundational step\ntoward clinical dialog systems. Our code is available on github:\nhttps://github.com/ChantalMP/RaDialog."}
{"id": "2410.02131", "pdf": "https://arxiv.org/pdf/2410.02131.pdf", "abs": "https://arxiv.org/abs/2410.02131", "title": "Boosting Masked ECG-Text Auto-Encoders as Discriminative Learners", "authors": ["Hung Manh Pham", "Aaqib Saeed", "Dong Ma"], "categories": ["cs.LG", "cs.CL"], "comment": "Accepted at ICML 2025", "summary": "The accurate interpretation of Electrocardiogram (ECG) signals is pivotal for\ndiagnosing cardiovascular diseases. Integrating ECG signals with accompanying\ntextual reports further holds immense potential to enhance clinical diagnostics\nby combining physiological data and qualitative insights. However, this\nintegration faces significant challenges due to inherent modality disparities\nand the scarcity of labeled data for robust cross-modal learning. To address\nthese obstacles, we propose D-BETA, a novel framework that pre-trains ECG and\ntext data using a contrastive masked auto-encoder architecture. D-BETA uniquely\ncombines the strengths of generative with boosted discriminative capabilities\nto achieve robust cross-modal representations. This is accomplished through\nmasked modality modeling, specialized loss functions, and an improved negative\nsampling strategy tailored for cross-modal alignment. Extensive experiments on\nfive public datasets across diverse downstream tasks demonstrate that D-BETA\nsignificantly outperforms existing methods, achieving an average AUC\nimprovement of 15% in linear probing with only one percent of training data and\n2% in zero-shot performance without requiring training data over\nstate-of-the-art models. These results highlight the effectiveness of D-BETA,\nunderscoring its potential to advance automated clinical diagnostics through\nmulti-modal representations. Our sample code and checkpoint are made available\nat https://github.com/manhph2211/D-BETA."}
{"id": "2410.22330", "pdf": "https://arxiv.org/pdf/2410.22330.pdf", "abs": "https://arxiv.org/abs/2410.22330", "title": "Vision-Language Models Create Cross-Modal Task Representations", "authors": ["Grace Luo", "Trevor Darrell", "Amir Bar"], "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "ICML 2025", "summary": "Autoregressive vision-language models (VLMs) can handle many tasks within a\nsingle model, yet the representations that enable this capability remain\nopaque. We find that VLMs align conceptually equivalent inputs into a shared\ntask vector, which is invariant to modality (text, image) and format (examples,\ninstruction), and may simplify VLM processing. We measure this alignment via\ncross-modal transfer -- the ability of a task vector derived in one modality to\ntrigger the correct generation in another -- on a range of tasks and model\narchitectures. Although the task vector is highly compressed, we find that this\nsingle vector outperforms prompting the model with the full task information,\nunique to this cross-modal case. Furthermore, we show that task vectors can be\ntransferred from a base language model to its fine-tuned vision-language\ncounterpart, and that they can be derived solely from instructions without the\nneed for examples. Taken together, our findings shed light on how VLMs\ninternally process task information, and how they map different modalities into\ncommon semantic representations. Project page:\nhttps://vlm-cross-modal-reps.github.io."}
{"id": "2411.04997", "pdf": "https://arxiv.org/pdf/2411.04997.pdf", "abs": "https://arxiv.org/abs/2411.04997", "title": "LLM2CLIP: Powerful Language Model Unlocks Richer Visual Representation", "authors": ["Weiquan Huang", "Aoqi Wu", "Yifan Yang", "Xufang Luo", "Yuqing Yang", "Liang Hu", "Qi Dai", "Chunyu Wang", "Xiyang Dai", "Dongdong Chen", "Chong Luo", "Lili Qiu"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "CLIP is a foundational multimodal model that aligns image and text features\ninto a shared representation space via contrastive learning on large-scale\nimage-text pairs. Its effectiveness primarily stems from the use of natural\nlanguage as rich supervision. Motivated by the remarkable advancements in large\nlanguage models (LLMs), this work explores how LLMs' superior text\nunderstanding and extensive open-world knowledge can enhance CLIP's capability,\nespecially for processing longer and more complex image captions. We propose an\nefficient post-training strategy that integrates LLMs into pretrained CLIP. To\naddress the challenge posed by the autoregressive nature of LLMs, we introduce\na caption-to-caption contrastive fine-tuning framework, significantly enhancing\nthe discriminative quality of LLM outputs. Extensive experiments demonstrate\nthat our approach outperforms LoRA-based methods, achieving nearly fourfold\nfaster training with superior performance. Furthermore, we validate substantial\nimprovements over state-of-the-art models such as CLIP, EVA02, and SigLip2\nacross various zero-shot multimodal retrieval tasks, cross-lingual retrieval\ntasks, and multimodal language model pretraining."}
{"id": "2411.05261", "pdf": "https://arxiv.org/pdf/2411.05261.pdf", "abs": "https://arxiv.org/abs/2411.05261", "title": "Cyclic Vision-Language Manipulator: Towards Reliable and Fine-Grained Image Interpretation for Automated Report Generation", "authors": ["Yingying Fang", "Zihao Jin", "Shaojie Guo", "Jinda Liu", "Zhiling Yue", "Yijian Gao", "Junzhi Ning", "Zhi Li", "Simon Walsh", "Guang Yang"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Despite significant advancements in automated report generation, the\nopaqueness of text interpretability continues to cast doubt on the reliability\nof the content produced. This paper introduces a novel approach to identify\nspecific image features in X-ray images that influence the outputs of report\ngeneration models. Specifically, we propose Cyclic Vision-Language Manipulator\nCVLM, a module to generate a manipulated X-ray from an original X-ray and its\nreport from a designated report generator. The essence of CVLM is that cycling\nmanipulated X-rays to the report generator produces altered reports aligned\nwith the alterations pre-injected into the reports for X-ray generation,\nachieving the term \"cyclic manipulation\". This process allows direct comparison\nbetween original and manipulated X-rays, clarifying the critical image features\ndriving changes in reports and enabling model users to assess the reliability\nof the generated texts. Empirical evaluations demonstrate that CVLM can\nidentify more precise and reliable features compared to existing explanation\nmethods, significantly enhancing the transparency and applicability of\nAI-generated reports."}
{"id": "2411.10246", "pdf": "https://arxiv.org/pdf/2411.10246.pdf", "abs": "https://arxiv.org/abs/2411.10246", "title": "Automated Coding of Communications in Collaborative Problem-solving Tasks Using ChatGPT", "authors": ["Jiangang Hao", "Wenju Cui", "Patrick Kyllonen", "Emily Kerzabi", "Lei Liu", "Michael Flor"], "categories": ["cs.HC", "cs.CL"], "comment": "21 pages, 3 figures, 5 tables. Initially report in the edArXiv:xw6kz", "summary": "Collaborative problem solving (CPS) is widely recognized as a critical\n21st-century skill. Assessing CPS depends heavily on coding the communication\ndata using a construct-relevant framework, and this process has long been a\nmajor bottleneck to scaling up such assessments. Based on five datasets and two\ncoding frameworks, we demonstrate that ChatGPT can code communication data to a\nsatisfactory level, though performance varies across ChatGPT models, and\ndepends on the coding framework and task characteristics. Interestingly, newer\nreasoning-focused models such as GPT-o1-mini and GPT-o3-mini do not necessarily\nyield better coding results. Additionally, we show that refining prompts based\non feedback from miscoded cases can improve coding accuracy in some instances,\nthough the effectiveness of this approach is not consistent across all tasks.\nThese findings offer practical guidance for researchers and practitioners in\ndeveloping scalable, efficient methods to analyze communication data in support\nof 21st-century skill assessment."}
{"id": "2503.18892", "pdf": "https://arxiv.org/pdf/2503.18892.pdf", "abs": "https://arxiv.org/abs/2503.18892", "title": "SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild", "authors": ["Weihao Zeng", "Yuzhen Huang", "Qian Liu", "Wei Liu", "Keqing He", "Zejun Ma", "Junxian He"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "DeepSeek-R1 has shown that long chain-of-thought (CoT) reasoning can\nnaturally emerge through a simple reinforcement learning (RL) framework with\nrule-based rewards, where the training may directly start from the base\nmodels-a paradigm referred to as zero RL training. Most recent efforts to\nreproduce zero RL training have primarily focused on the Qwen2.5 model series,\nwhich may not be representative as we find the base models already exhibit\nstrong instruction-following and self-reflection abilities. In this work, we\ninvestigate zero RL training across 10 diverse base models, spanning different\nfamilies and sizes including LLama3-8B, Mistral-7B/24B, DeepSeek-Math-7B,\nQwen2.5-math-7B, and all Qwen2.5 models from 0.5B to 32B. Leveraging several\nkey design strategies-such as adjusting format reward and controlling query\ndifficulty-we achieve substantial improvements in both reasoning accuracy and\nresponse length across most settings. However, by carefully monitoring the\ntraining dynamics, we observe that different base models exhibit distinct\npatterns during training. For instance, the increased response length does not\nalways correlate with the emergence of certain cognitive behaviors such as\nverification (i.e., the \"aha moment\"). Notably, we observe the \"aha moment\" for\nthe first time in small models not from the Qwen family. We share the key\ndesigns that enable successful zero RL training, along with our findings and\npractices. To facilitate further research, we open-source the code, models, and\nanalysis tools."}
{"id": "2505.00831", "pdf": "https://arxiv.org/pdf/2505.00831.pdf", "abs": "https://arxiv.org/abs/2505.00831", "title": "SmallPlan: Leverage Small Language Models for Sequential Path Planning with Simulation-Powered, LLM-Guided Distillation", "authors": ["Quang P. M. Pham", "Khoi T. N. Nguyen", "Nhi H. Doan", "Cuong A. Pham", "Kentaro Inui", "Dezhen Song"], "categories": ["cs.RO", "cs.CL"], "comment": "Paper is under review", "summary": "Efficient path planning in robotics, particularly within large-scale, dynamic\nenvironments, remains a significant hurdle. While Large Language Models (LLMs)\noffer strong reasoning capabilities, their high computational cost and limited\nadaptability in dynamic scenarios hinder real-time deployment on edge devices.\nWe present SmallPlan -- a novel framework leveraging LLMs as teacher models to\ntrain lightweight Small Language Models (SLMs) for high-level path planning\ntasks. In SmallPlan, the SLMs provide optimal action sequences to navigate\nacross scene graphs that compactly represent full-scaled 3D scenes. The SLMs\nare trained in a simulation-powered, interleaved manner with LLM-guided\nsupervised fine-tuning (SFT) and reinforcement learning (RL). This strategy not\nonly enables SLMs to successfully complete navigation tasks but also makes them\naware of important factors like travel distance and number of trials. Through\nexperiments, we demonstrate that the fine-tuned SLMs perform competitively with\nlarger models like GPT-4o on sequential path planning, without suffering from\nhallucination and overfitting. SmallPlan is resource-efficient, making it\nwell-suited for edge-device deployment and advancing practical autonomous\nrobotics."}
{"id": "2505.03335", "pdf": "https://arxiv.org/pdf/2505.03335.pdf", "abs": "https://arxiv.org/abs/2505.03335", "title": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data", "authors": ["Andrew Zhao", "Yiran Wu", "Yang Yue", "Tong Wu", "Quentin Xu", "Yang Yue", "Matthieu Lin", "Shenzhi Wang", "Qingyun Wu", "Zilong Zheng", "Gao Huang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) has shown promise in\nenhancing the reasoning capabilities of large language models by learning\ndirectly from outcome-based rewards. Recent RLVR works that operate under the\nzero setting avoid supervision in labeling the reasoning process, but still\ndepend on manually curated collections of questions and answers for training.\nThe scarcity of high-quality, human-produced examples raises concerns about the\nlong-term scalability of relying on human supervision, a challenge already\nevident in the domain of language model pretraining. Furthermore, in a\nhypothetical future where AI surpasses human intelligence, tasks provided by\nhumans may offer limited learning potential for a superintelligent system. To\naddress these concerns, we propose a new RLVR paradigm called Absolute Zero, in\nwhich a single model learns to propose tasks that maximize its own learning\nprogress and improves reasoning by solving them, without relying on any\nexternal data. Under this paradigm, we introduce the Absolute Zero Reasoner\n(AZR), a system that self-evolves its training curriculum and reasoning ability\nby using a code executor to both validate proposed code reasoning tasks and\nverify answers, serving as an unified source of verifiable reward to guide\nopen-ended yet grounded learning. Despite being trained entirely without\nexternal data, AZR achieves overall SOTA performance on coding and mathematical\nreasoning tasks, outperforming existing zero-setting models that rely on tens\nof thousands of in-domain human-curated examples. Furthermore, we demonstrate\nthat AZR can be effectively applied across different model scales and is\ncompatible with various model classes."}
{"id": "2505.03414", "pdf": "https://arxiv.org/pdf/2505.03414.pdf", "abs": "https://arxiv.org/abs/2505.03414", "title": "Enhancing Target-unspecific Tasks through a Features Matrix", "authors": ["Fangming Cui", "Yonggang Zhang", "Xuan Wang", "Xinmei Tian", "Jun Yu"], "categories": ["cs.CV", "cs.CL"], "comment": "ICML 2025", "summary": "Recent developments in prompt learning of large vision-language models have\nsignificantly improved performance in target-specific tasks. However, these\nprompt optimizing methods often struggle to tackle the target-unspecific or\ngeneralizable tasks effectively. It may be attributed to the fact that\noverfitting training causes the model to forget its general knowledge having\nstrong promotion on target-unspecific tasks. To alleviate this issue, we\npropose a novel Features Matrix (FM) regularization approach designed to\nenhance these models on target-unspecific tasks. Our method extracts and\nleverages general knowledge, shaping a Features Matrix (FM). Specifically, the\nFM captures the semantics of diverse inputs from a deep and fine perspective,\npreserving essential general knowledge, which mitigates the risk of\noverfitting. Representative evaluations demonstrate that: 1) the FM is\ncompatible with existing frameworks as a generic and flexible module, and 2)\nthe FM significantly showcases its effectiveness in enhancing target-unspecific\ntasks, achieving state-of-the-art performance."}
