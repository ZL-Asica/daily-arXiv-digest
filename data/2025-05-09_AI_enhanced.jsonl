{"id": "2505.04628", "pdf": "https://arxiv.org/pdf/2505.04628.pdf", "abs": "https://arxiv.org/abs/2505.04628", "title": "How Social is It? A Benchmark for LLMs' Capabilities in Multi-user Multi-turn Social Agent Tasks", "authors": ["Yusen Wu", "Junwu Xiong", "Xiaotie Deng"], "categories": ["cs.CL", "cs.AI", "cs.SI"], "comment": null, "summary": "Expanding the application of large language models (LLMs) to societal life,\ninstead of primary function only as auxiliary assistants to communicate with\nonly one person at a time, necessitates LLMs' capabilities to independently\nplay roles in multi-user, multi-turn social agent tasks within complex social\nsettings. However, currently the capability has not been systematically\nmeasured with available benchmarks. To address this gap, we first introduce an\nagent task leveling framework grounded in sociological principles.\nConcurrently, we propose a novel benchmark, How Social Is It (we call it HSII\nbelow), designed to assess LLM's social capabilities in comprehensive social\nagents tasks and benchmark representative models. HSII comprises four stages:\nformat parsing, target selection, target switching conversation, and stable\nconversation, which collectively evaluate the communication and task completion\ncapabilities of LLMs within realistic social interaction scenarios dataset,\nHSII-Dataset. The dataset is derived step by step from news dataset. We perform\nan ablation study by doing clustering to the dataset. Additionally, we\ninvestigate the impact of chain of thought (COT) method on enhancing LLMs'\nsocial performance. Since COT cost more computation, we further introduce a new\nstatistical metric, COT-complexity, to quantify the efficiency of certain LLMs\nwith COTs for specific social tasks and strike a better trade-off between\nmeasurement of correctness and efficiency. Various results of our experiments\ndemonstrate that our benchmark is well-suited for evaluating social skills in\nLLMs.", "AI": {"tldr": "This paper introduces a benchmark (HSII) and framework for evaluating social capabilities of large language models (LLMs) in multi-user, multi-turn tasks.", "motivation": "To systematically measure the social capabilities of LLMs for tasks beyond individual communication, addressing a significant gap in current benchmarks.", "method": "An agent task leveling framework grounded in sociological principles was developed alongside a novel benchmark, How Social Is It (HSII), which includes tasks like format parsing, target selection, and stable conversation.", "result": "Experiments show that HSII effectively evaluates LLMs' social skills across multi-user interactions, revealing the impact of the chain of thought method and introducing COT-complexity to assess efficiency.", "conclusion": "The HSII benchmark is well-suited for assessing the social skills of LLMs, allowing for refined measurement of their performance in complex social scenarios.", "key_contributions": ["Introduction of the HSII framework for measuring LLMs' social capabilities", "Creation of the HSII-Dataset derived from news data", "Development of the COT-complexity metric for evaluating efficiency in LLM performance"], "limitations": "", "keywords": ["Large Language Models", "Social Interaction", "Benchmarking", "Multi-user Tasks", "Chain of Thought"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.04637", "pdf": "https://arxiv.org/pdf/2505.04637.pdf", "abs": "https://arxiv.org/abs/2505.04637", "title": "Adaptive Token Boundaries: Integrating Human Chunking Mechanisms into Multimodal LLMs", "authors": ["Dongxing Yu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in multimodal large language models (MLLMs) have\ndemonstrated remarkable capabilities in processing diverse data types, yet\nsignificant disparities persist between human cognitive processes and\ncomputational approaches to multimodal information integration. This research\npresents a systematic investigation into the parallels between human\ncross-modal chunking mechanisms and token representation methodologies in\nMLLMs. Through empirical studies comparing human performance patterns with\nmodel behaviors across visual-linguistic tasks, we demonstrate that\nconventional static tokenization schemes fundamentally constrain current\nmodels' capacity to simulate the dynamic, context-sensitive nature of human\ninformation processing. We propose a novel framework for dynamic cross-modal\ntokenization that incorporates adaptive boundaries, hierarchical\nrepresentations, and alignment mechanisms grounded in cognitive science\nprinciples. Quantitative evaluations demonstrate that our approach yields\nstatistically significant improvements over state-of-the-art models on\nbenchmark tasks (+7.8% on Visual Question Answering, +5.3% on Complex Scene\nDescription) while exhibiting more human-aligned error patterns and attention\ndistributions. These findings contribute to the theoretical understanding of\nthe relationship between human cognition and artificial intelligence, while\nproviding empirical evidence for developing more cognitively plausible AI\nsystems.", "AI": {"tldr": "This research investigates the relationship between human cognitive processes and MLLMs, proposing a new dynamic cross-modal tokenization framework that aligns with human information processing and improves model performance.", "motivation": "To address disparities between human cognitive processes and computational multimodal information integration in MLLMs.", "method": "Empirical studies comparing human performance patterns with model behaviors in visual-linguistic tasks, proposing a dynamic cross-modal tokenization framework based on cognitive science.", "result": "Statistically significant improvements over state-of-the-art models on benchmark tasks (+7.8% on Visual Question Answering, +5.3% on Complex Scene Description) and more human-aligned error patterns and attention distributions.", "conclusion": "The proposed framework enhances the cognitive plausibility of AI systems and improves the theoretical understanding of AI and human cognition interactions.", "key_contributions": ["New dynamic cross-modal tokenization framework", "Improvements on benchmark tasks", "Alignment with human cognitive processes"], "limitations": "", "keywords": ["Multimodal Large Language Models", "Human Cognition", "Dynamic Tokenization"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.04639", "pdf": "https://arxiv.org/pdf/2505.04639.pdf", "abs": "https://arxiv.org/abs/2505.04639", "title": "Language translation, and change of accent for speech-to-speech task using diffusion model", "authors": ["Abhishek Mishra", "Ritesh Sur Chowdhury", "Vartul Bahuguna", "Isha Pandey", "Ganesh Ramakrishnan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Speech-to-speech translation (S2ST) aims to convert spoken input in one\nlanguage to spoken output in another, typically focusing on either language\ntranslation or accent adaptation. However, effective cross-cultural\ncommunication requires handling both aspects simultaneously - translating\ncontent while adapting the speaker's accent to match the target language\ncontext. In this work, we propose a unified approach for simultaneous speech\ntranslation and change of accent, a task that remains underexplored in current\nliterature. Our method reformulates the problem as a conditional generation\ntask, where target speech is generated based on phonemes and guided by target\nspeech features. Leveraging the power of diffusion models, known for\nhigh-fidelity generative capabilities, we adapt text-to-image diffusion\nstrategies by conditioning on source speech transcriptions and generating Mel\nspectrograms representing the target speech with desired linguistic and\naccentual attributes. This integrated framework enables joint optimization of\ntranslation and accent adaptation, offering a more parameter-efficient and\neffective model compared to traditional pipelines.", "AI": {"tldr": "This paper proposes a unified approach for simultaneous speech translation and accent adaptation using diffusion models.", "motivation": "Effective cross-cultural communication requires translating content while adapting the speaker's accent to match the target language context, which is currently underexplored.", "method": "The problem is reformulated as a conditional generation task, where target speech is generated based on phonemes and guided by target speech features, utilizing diffusion models for high-fidelity generation.", "result": "The integrated framework allows for joint optimization of translation and accent adaptation, resulting in a more parameter-efficient and effective model compared to traditional pipelines.", "conclusion": "The proposed method demonstrates significant improvements in both translation and accent adaptation, enhancing the feasibility of S2ST in real-world applications.", "key_contributions": ["Unified approach for S2ST and accent adaptation", "Utilization of diffusion models for speech generation", "Joint optimization framework for better efficiency"], "limitations": "", "keywords": ["speech-to-speech translation", "accent adaptation", "diffusion models", "conditional generation", "cross-cultural communication"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.04640", "pdf": "https://arxiv.org/pdf/2505.04640.pdf", "abs": "https://arxiv.org/abs/2505.04640", "title": "A Comparative Benchmark of a Moroccan Darija Toxicity Detection Model (Typica.ai) and Major LLM-Based Moderation APIs (OpenAI, Mistral, Anthropic)", "authors": ["Hicham Assoudi"], "categories": ["cs.CL"], "comment": "GitHub repository with reproducibility materials and evaluation\n  notebook available at:\n  https://github.com/assoudi-typica-ai/darija-toxicity-benchmark", "summary": "This paper presents a comparative benchmark evaluating the performance of\nTypica.ai's custom Moroccan Darija toxicity detection model against major\nLLM-based moderation APIs: OpenAI (omni-moderation-latest), Mistral\n(mistral-moderation-latest), and Anthropic Claude (claude-3-haiku-20240307). We\nfocus on culturally grounded toxic content, including implicit insults,\nsarcasm, and culturally specific aggression often overlooked by general-purpose\nsystems. Using a balanced test set derived from the OMCD_Typica.ai_Mix dataset,\nwe report precision, recall, F1-score, and accuracy, offering insights into\nchallenges and opportunities for moderation in underrepresented languages. Our\nresults highlight Typica.ai's superior performance, underlining the importance\nof culturally adapted models for reliable content moderation.", "AI": {"tldr": "The paper benchmarks Typica.ai's Moroccan Darija toxicity detection model against major LLM-based moderation APIs, highlighting the importance of culturally adapted models for effective moderation.", "motivation": "To evaluate the performance of different toxicity detection models on culturally grounded toxic content that is often overlooked by general-purpose systems.", "method": "A comparative benchmark using a balanced test set derived from the OMCD_Typica.ai_Mix dataset, reporting precision, recall, F1-score, and accuracy metrics.", "result": "Typica.ai's model outperforms major LLM moderation APIs in detecting culturally specific toxicity, revealing challenges in the moderation of underrepresented languages.", "conclusion": "The superiority of Typica.ai's model emphasizes the need for culturally adapted content moderation solutions.", "key_contributions": ["Development of a robust benchmark for toxicity detection in Moroccan Darija.", "Highlighting the importance of culturally specific models for reliable moderation.", "Detailed performance comparison against leading LLM moderation APIs."], "limitations": "", "keywords": ["toxicity detection", "Darija", "content moderation", "culturally specific", "LLM"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.04712", "pdf": "https://arxiv.org/pdf/2505.04712.pdf", "abs": "https://arxiv.org/abs/2505.04712", "title": "Investigating the Impact and Student Perceptions of Guided Parsons Problems for Learning Logic with Subgoals", "authors": ["Sutapa Dey Tithi", "Xiaoyi Tian", "Min Chi", "Tiffany Barnes"], "categories": ["cs.HC", "cs.LO"], "comment": null, "summary": "Parsons problems (PPs) have shown promise in structured problem solving by\nproviding scaffolding that decomposes the problem and requires learners to\nreconstruct the solution. However, some students face difficulties when first\nlearning with PPs or solving more complex Parsons problems. This study\nintroduces Guided Parsons problems (GPPs) designed to provide step-specific\nhints and improve learning outcomes in an intelligent logic tutor. In a\ncontrolled experiment with 76 participants, GPP students achieved significantly\nhigher accuracy of rule application in both level-end tests and post-tests,\nwith the strongest gains among students with lower prior knowledge. GPP\nstudents initially spent more time in training (1.52 vs. 0.81 hours) but\nrequired less time for post-tests, indicating improved problem solving\nefficiency. Our thematic analysis of GPP student self-explanations revealed\ntask decomposition, better rule understanding, and reduced difficulty as key\nthemes, while some students felt the structured nature of GPPs restricted their\nown way of reasoning. These findings reinforce that GPPs can effectively\ncombine the benefits of worked examples and problem solving practice, but could\nbe further improved by individual adaptation.", "AI": {"tldr": "The study introduces Guided Parsons Problems (GPPs) that provide step-specific hints to improve learning outcomes in logic problem solving, demonstrating increased accuracy and efficiency among students, especially those with lower prior knowledge.", "motivation": "The motivation behind this study is to address the difficulties students face when learning with traditional Parsons Problems (PPs) and to enhance problem-solving efficiency and learning outcomes.", "method": "This research involved a controlled experiment with 76 participants where GPPs were utilized, comparing their performance in level-end tests and post-tests against traditional PPs.", "result": "Students who learned with GPPs demonstrated significantly higher accuracy in rule application and improved problem-solving efficiency, particularly benefiting students with lower prior knowledge, despite initially spending more time in training.", "conclusion": "The findings suggest that GPPs effectively integrate the advantages of worked examples and problem-solving practice but highlight the potential for individual adaptation to enhance their effectiveness further.", "key_contributions": ["Introduction of Guided Parsons Problems (GPPs)", "Demonstration of significant improvement in accuracy and efficiency in problem solving", "Thematic analysis revealing key aspects of student experiences with GPPs"], "limitations": "Some students felt that the structured nature of GPPs limited their reasoning processes, suggesting a need for more individualized approaches.", "keywords": ["Guided Parsons Problems", "learning outcomes", "problem solving", "intelligent tutoring systems", "task decomposition"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.04642", "pdf": "https://arxiv.org/pdf/2505.04642.pdf", "abs": "https://arxiv.org/abs/2505.04642", "title": "Rethinking Multimodal Sentiment Analysis: A High-Accuracy, Simplified Fusion Architecture", "authors": ["Nischal Mandal", "Yang Li"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multimodal sentiment analysis, a pivotal task in affective computing, seeks\nto understand human emotions by integrating cues from language, audio, and\nvisual signals. While many recent approaches leverage complex attention\nmechanisms and hierarchical architectures, we propose a lightweight, yet\neffective fusion-based deep learning model tailored for utterance-level emotion\nclassification. Using the benchmark IEMOCAP dataset, which includes aligned\ntext, audio-derived numeric features, and visual descriptors, we design a\nmodality-specific encoder using fully connected layers followed by dropout\nregularization. The modality-specific representations are then fused using\nsimple concatenation and passed through a dense fusion layer to capture\ncross-modal interactions. This streamlined architecture avoids computational\noverhead while preserving performance, achieving a classification accuracy of\n92% across six emotion categories. Our approach demonstrates that with careful\nfeature engineering and modular design, simpler fusion strategies can\noutperform or match more complex models, particularly in resource-constrained\nenvironments.", "AI": {"tldr": "This paper introduces a lightweight fusion-based deep learning model for utterance-level emotion classification in multimodal sentiment analysis, achieving 92% accuracy on the IEMOCAP dataset.", "motivation": "The paper addresses the need for effective multimodal sentiment analysis by integrating language, audio, and visual signals while avoiding excessive computational costs.", "method": "The authors designed a modality-specific encoder with fully connected layers and dropout regularization, using simple concatenation to fuse modality-specific representations followed by a dense fusion layer.", "result": "The proposed model achieved a classification accuracy of 92% across six emotion categories on the IEMOCAP dataset.", "conclusion": "Simpler fusion strategies, with careful feature engineering and modular design, can outperform or match the performance of more complex models, especially in resource-constrained settings.", "key_contributions": ["Development of a lightweight deep learning model for multimodal sentiment analysis", "Achievement of high accuracy (92%) using simple fusion techniques", "Demonstration that simpler models can be effective in resource-constrained environments."], "limitations": "", "keywords": ["multimodal sentiment analysis", "emotion classification", "deep learning", "resource-constrained environments", "IEMOCAP dataset"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.04869", "pdf": "https://arxiv.org/pdf/2505.04869.pdf", "abs": "https://arxiv.org/abs/2505.04869", "title": "From First Draft to Final Insight: A Multi-Agent Approach for Feedback Generation", "authors": ["Jie Cao", "Chloe Qianhui Zhao", "Xian Chen", "Shuman Wang", "Christian Schunn", "Kenneth R. Koedinger", "Jionghao Lin"], "categories": ["cs.HC"], "comment": "14 pages, to be published at the 26th International Conference on\n  Artificial Intelligence in Education (AIED '25)", "summary": "Producing large volumes of high-quality, timely feedback poses significant\nchallenges to instructors. To address this issue, automation\ntechnologies-particularly Large Language Models (LLMs)-show great potential.\nHowever, current LLM-based research still shows room for improvement in terms\nof feedback quality. Our study proposed a multi-agent approach performing\n\"generation, evaluation, and regeneration\" (G-E-RG) to further enhance feedback\nquality. In the first-generation phase, six methods were adopted, combining\nthree feedback theoretical frameworks and two prompt methods: zero-shot and\nretrieval-augmented generation with chain-of-thought (RAG_CoT). The results\nindicated that, compared to first-round feedback, G-E-RG significantly improved\nfinal feedback across six methods for most dimensions. Specifically:(1)\nEvaluation accuracy for six methods increased by 3.36% to 12.98% (p<0.001); (2)\nThe proportion of feedback containing four effective components rose from an\naverage of 27.72% to an average of 98.49% among six methods, sub-dimensions of\nproviding critiques, highlighting strengths, encouraging agency, and\ncultivating dialogue also showed great enhancement (p<0.001); (3) There was a\nsignificant improvement in most of the feature values (p<0.001), although some\nsub-dimensions (e.g., strengthening the teacher-student relationship) still\nrequire further enhancement; (4) The simplicity of feedback was effectively\nenhanced (p<0.001) for three methods.", "AI": {"tldr": "The paper proposes a multi-agent approach for enhancing feedback quality through a process called \"generation, evaluation, and regeneration\" (G-E-RG), utilizing Large Language Models. It reports significant improvements in feedback dimensions and accuracy over traditional methods.", "motivation": "To address the challenges in providing high-quality feedback efficiently, the study explores the potential of automation technologies, specifically Large Language Models (LLMs).", "method": "The proposed G-E-RG approach integrates six methods combining three feedback theoretical frameworks with two prompting techniques: zero-shot generation and retrieval-augmented generation with chain-of-thought (RAG_CoT).", "result": "The G-E-RG approach achieved a 3.36% to 12.98% improvement in evaluation accuracy and increased the effective feedback components from an average of 27.72% to 98.49%.", "conclusion": "While G-E-RG significantly enhanced feedback quality, some sub-dimensions require further improvement. Overall, the simplicity of feedback also improved.", "key_contributions": ["Introduction of a multi-agent approach for feedback enhancement using LLMs.", "Demonstration of substantial improvements in feedback quality and accuracy.", "Exploration of combining feedback frameworks with advanced prompting methods."], "limitations": "Certain sub-dimensions of feedback quality, such as strengthening the teacher-student relationship, need further enhancement.", "keywords": ["large language models", "multi-agent systems", "feedback quality", "education technology", "automated feedback"], "importance_score": 8, "read_time_minutes": 14}}
{"id": "2505.04643", "pdf": "https://arxiv.org/pdf/2505.04643.pdf", "abs": "https://arxiv.org/abs/2505.04643", "title": "Prediction-powered estimators for finite population statistics in highly imbalanced textual data: Public hate crime estimation", "authors": ["Hannes Waldetoft", "Jakob Torgander", "Måns Magnusson"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Estimating population parameters in finite populations of text documents can\nbe challenging when obtaining the labels for the target variable requires\nmanual annotation. To address this problem, we combine predictions from a\ntransformer encoder neural network with well-established survey sampling\nestimators using the model predictions as an auxiliary variable. The\napplicability is demonstrated in Swedish hate crime statistics based on Swedish\npolice reports. Estimates of the yearly number of hate crimes and the police's\nunder-reporting are derived using the Hansen-Hurwitz estimator, difference\nestimation, and stratified random sampling estimation. We conclude that if\nlabeled training data is available, the proposed method can provide very\nefficient estimates with reduced time spent on manual annotation.", "AI": {"tldr": "This paper presents a method for estimating population parameters in finite populations of text documents by combining transformer neural network predictions with traditional survey sampling estimators.", "motivation": "The challenge of estimating population parameters when manual annotation for labeling target variables is required.", "method": "The method integrates predictions from a transformer encoder neural network with established survey sampling estimators, treating model predictions as auxiliary variables.", "result": "The approach was applied to Swedish hate crime statistics, yielding estimates for yearly hate crimes and addressing police under-reporting using various sampling techniques.", "conclusion": "The method can provide efficient estimates if labeled training data is available, significantly reducing the reliance on manual annotation.", "key_contributions": ["Combines transformer neural networks with survey sampling estimators", "Demonstrates applicability in a real-world context (Swedish hate crime statistics)", "Provides methods for efficient estimation with reduced manual labeling"], "limitations": "", "keywords": ["Survey Sampling", "Transformer Networks", "Population Estimation"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2505.04886", "pdf": "https://arxiv.org/pdf/2505.04886.pdf", "abs": "https://arxiv.org/abs/2505.04886", "title": "Fairness Perceptions in Regression-based Predictive Models", "authors": ["Mukund Telukunta", "Venkata Sriram Siddhardh Nadendla", "Morgan Stuart", "Casey Canfield"], "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "Regression-based predictive analytics used in modern kidney transplantation\nis known to inherit biases from training data. This leads to social\ndiscrimination and inefficient organ utilization, particularly in the context\nof a few social groups. Despite this concern, there is limited research on\nfairness in regression and its impact on organ utilization and placement. This\npaper introduces three novel divergence-based group fairness notions: (i)\nindependence, (ii) separation, and (iii) sufficiency to assess the fairness of\nregression-based analytics tools. In addition, fairness preferences are\ninvestigated from crowd feedback, in order to identify a socially accepted\ngroup fairness criterion for evaluating these tools. A total of 85 participants\nwere recruited from the Prolific crowdsourcing platform, and a Mixed-Logit\ndiscrete choice model was used to model fairness feedback and estimate social\nfairness preferences. The findings clearly depict a strong preference towards\nthe separation and sufficiency fairness notions, and that the predictive\nanalytics is deemed fair with respect to gender and race groups, but unfair in\nterms of age groups.", "AI": {"tldr": "This paper addresses biases in regression-based predictive analytics used in kidney transplantation and introduces new fairness evaluation criteria.", "motivation": "To tackle social discrimination and inefficient organ utilization resulting from biases in kidney transplantation predictive analytics.", "method": "Introduced new divergence-based group fairness notions (independence, separation, sufficiency) and utilized Mixed-Logit discrete choice modeling with feedback from 85 participants to assess fairness preferences.", "result": "Participants exhibited strong preferences for separation and sufficiency fairness notions, with predictive analytics being fair for gender and race but unfair for age groups.", "conclusion": "The study highlights the need for addressing fairness in kidney transplantation predictive analytics to improve social equity and organ utilization.", "key_contributions": ["Introduction of three novel fairness concepts for regression analytics", "Analysis of crowd-sourced fairness feedback", "Identification of social preferences in fairness metrics"], "limitations": "", "keywords": ["predictive analytics", "fairness", "kidney transplantation", "regression", "social discrimination"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.04645", "pdf": "https://arxiv.org/pdf/2505.04645.pdf", "abs": "https://arxiv.org/abs/2505.04645", "title": "ChatGPT for automated grading of short answer questions in mechanical ventilation", "authors": ["Tejas Jade", "Alex Yartsev"], "categories": ["cs.CL", "cs.LG", "stat.CO"], "comment": null, "summary": "Standardised tests using short answer questions (SAQs) are common in\npostgraduate education. Large language models (LLMs) simulate conversational\nlanguage and interpret unstructured free-text responses in ways aligning with\napplying SAQ grading rubrics, making them attractive for automated grading. We\nevaluated ChatGPT 4o to grade SAQs in a postgraduate medical setting using data\nfrom 215 students (557 short-answer responses) enrolled in an online course on\nmechanical ventilation (2020--2024). Deidentified responses to three case-based\nscenarios were presented to ChatGPT with a standardised grading prompt and\nrubric. Outputs were analysed using mixed-effects modelling, variance component\nanalysis, intraclass correlation coefficients (ICCs), Cohen's kappa, Kendall's\nW, and Bland--Altman statistics. ChatGPT awarded systematically lower marks\nthan human graders with a mean difference (bias) of -1.34 on a 10-point scale.\nICC values indicated poor individual-level agreement (ICC1 = 0.086), and\nCohen's kappa (-0.0786) suggested no meaningful agreement. Variance component\nanalysis showed minimal variability among the five ChatGPT sessions (G-value =\n0.87), indicating internal consistency but divergence from the human grader.\nThe poorest agreement was observed for evaluative and analytic items, whereas\nchecklist and prescriptive rubric items had less disagreement. We caution\nagainst the use of LLMs in grading postgraduate coursework. Over 60% of\nChatGPT-assigned grades differed from human grades by more than acceptable\nboundaries for high-stakes assessments.", "AI": {"tldr": "This paper evaluates the use of ChatGPT 4o for grading short answer questions in medical education, highlighting significant discrepancies between automated and human grading outcomes.", "motivation": "The study aims to investigate the feasibility and reliability of large language models (LLMs) for grading short answer questions (SAQs) in postgraduate medical education, where standardized testing is common.", "method": "The authors analyzed ChatGPT's grading of 557 short-answer responses from 215 medical students using a structured grading prompt and rubric, employing mixed-effects modeling and various statistical analyses to assess grading accuracy and agreement with human graders.", "result": "ChatGPT systematically awarded lower marks compared to human graders, with an average bias of -1.34 marks on a 10-point scale and poor individual-level agreement indicated by low ICC and Cohen's kappa values.", "conclusion": "The findings suggest that LLMs like ChatGPT are not reliable for grading postgraduate coursework accurately, as a majority of the assigned grades significantly diverged from those given by human evaluators, highlighting their limitations in high-stakes assessments.", "key_contributions": ["Evaluation of LLMs for grading in medical education", "Statistical analysis of grading reliability and agreement", "Evidence of poorer performance by LLMs compared to human graders"], "limitations": "The study was conducted in a single postgraduate medical course and may not generalize to other contexts or subjects.", "keywords": ["ChatGPT", "grading", "short answer questions", "medical education", "large language models"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.04890", "pdf": "https://arxiv.org/pdf/2505.04890.pdf", "abs": "https://arxiv.org/abs/2505.04890", "title": "Theatrical Language Processing: Exploring AI-Augmented Improvisational Acting and Scriptwriting with LLMs", "authors": ["Sora Kang", "Joonhwan Lee"], "categories": ["cs.HC"], "comment": "ISEA2025 (30th International Symposium on Electronic/Emerging Art)", "summary": "The increasing convergence of artificial intelligence has opened new avenues,\nincluding its emerging role in enhancing creativity. It is reshaping\ntraditional creative practices such as actor improvisation, which often\nstruggles with predictable patterns, limited interaction, and a lack of\nengaging stimuli. In this paper, we introduce a new concept, Theatrical\nLanguage Processing (TLP), and an AI-driven creativity support tool,\nScribble.ai, designed to augment actors' creative expression and spontaneity\nthrough interactive practice. We conducted a user study involving tests and\ninterviews with fourteen participants. Our findings indicate that: (1) Actors\nexpanded their creativity when faced with AI-produced irregular scenarios; (2)\nThe AI's unpredictability heightened their problem-solving skills, specifically\nin interpreting unfamiliar situations; (3) However, AI often generated\nexcessively detailed scripts, which limited interpretive freedom and hindered\nsubtext exploration. Based on these findings, we discuss the new potential in\nenhancing creative expressions in film and theater studies through an AI-driven\ntool.", "AI": {"tldr": "The paper discusses Theatrical Language Processing (TLP) and the tool Scribble.ai, aimed at augmenting actor creativity through AI-driven unpredictable scenarios.", "motivation": "To explore the potential of AI in enhancing creativity in traditional creative practices such as actor improvisation.", "method": "User study involving tests and interviews with fourteen actors.", "result": "Actors showed expanded creativity with AI-generated irregular scenarios and improved problem-solving skills, but faced limitations due to overly detailed scripts.", "conclusion": "AI tools like Scribble.ai can enhance creative expression in film and theater but must strike a balance between structure and interpretative freedom.", "key_contributions": ["Introduction of Theatrical Language Processing (TLP)", "Development of the AI-driven creativity support tool Scribble.ai", "Insights on balancing AI input and actor interpretative freedom."], "limitations": "AI-generated scripts can sometimes be overly detailed, limiting actor creativity and interpretive freedom.", "keywords": ["AI", "creativity", "improvisation", "Scribble.ai", "Theatrical Language Processing"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.04649", "pdf": "https://arxiv.org/pdf/2505.04649.pdf", "abs": "https://arxiv.org/abs/2505.04649", "title": "FRAME: Feedback-Refined Agent Methodology for Enhancing Medical Research Insights", "authors": ["Chengzhang Yu", "Yiming Zhang", "Zhixin Liu", "Zenghui Ding", "Yining Sun", "Zhanpeng Jin"], "categories": ["cs.CL"], "comment": "12 pages, 4 figures, 5 table", "summary": "The automation of scientific research through large language models (LLMs)\npresents significant opportunities but faces critical challenges in knowledge\nsynthesis and quality assurance. We introduce Feedback-Refined Agent\nMethodology (FRAME), a novel framework that enhances medical paper generation\nthrough iterative refinement and structured feedback. Our approach comprises\nthree key innovations: (1) A structured dataset construction method that\ndecomposes 4,287 medical papers into essential research components through\niterative refinement; (2) A tripartite architecture integrating Generator,\nEvaluator, and Reflector agents that progressively improve content quality\nthrough metric-driven feedback; and (3) A comprehensive evaluation framework\nthat combines statistical metrics with human-grounded benchmarks. Experimental\nresults demonstrate FRAME's effectiveness, achieving significant improvements\nover conventional approaches across multiple models (9.91% average gain with\nDeepSeek V3, comparable improvements with GPT-4o Mini) and evaluation\ndimensions. Human evaluation confirms that FRAME-generated papers achieve\nquality comparable to human-authored works, with particular strength in\nsynthesizing future research directions. The results demonstrated our work\ncould efficiently assist medical research by building a robust foundation for\nautomated medical research paper generation while maintaining rigorous academic\nstandards.", "AI": {"tldr": "The paper presents a novel framework called Feedback-Refined Agent Methodology (FRAME) for automating medical paper generation through iterative refinement and structured feedback, showing improvements in content quality and synthesis capabilities of large language models.", "motivation": "To address the challenges of knowledge synthesis and quality assurance in the automation of scientific research using large language models (LLMs).", "method": "FRAME integrates a tripartite architecture of Generator, Evaluator, and Reflector agents to enhance medical paper generation through iterative refinement based on structured feedback from a comprehensive evaluation framework.", "result": "FRAME achieved an average gain of 9.91% with DeepSeek V3 and comparable improvements with GPT-4o Mini across multiple dimensions of evaluation, confirming the quality of generated papers is at par with human-authored works.", "conclusion": "FRAME builds a robust foundation for automated medical research paper generation, successfully maintaining rigorous academic standards while improving quality and synthesis of future research directions.", "key_contributions": ["A structured dataset construction method for decomposing medical papers into essential components.", "A tripartite architecture involving Generator, Evaluator, and Reflector agents for iterative content improvement.", "A comprehensive evaluation framework that combines statistical metrics with human-grounded benchmarks."], "limitations": "", "keywords": ["large language models", "automated research", "medical paper generation", "iterative refinement", "evaluation framework"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2505.05038", "pdf": "https://arxiv.org/pdf/2505.05038.pdf", "abs": "https://arxiv.org/abs/2505.05038", "title": "Uncertainty-Aware Scarf Plots", "authors": ["Nelusa Pathmanathan", "Seyda Öney", "Maurice Koch", "Daniel Weiskopf", "Kuno Kurzhals"], "categories": ["cs.HC"], "comment": "2025 Symposium on Eye Tracking Research and Applications (ETRA '25)", "summary": "Multiple challenges emerge when analyzing eye-tracking data with areas of\ninterest (AOIs) because recordings are subject to different sources of\nuncertainties. Previous work often presents gaze data without considering those\ninaccuracies in the data. To address this issue, we developed uncertainty-aware\nscarf plot visualizations that aim to make analysts aware of uncertainties with\nrespect to the position-based mapping of gaze to AOIs and depth dependency in\n3D scenes. Additionally, we also consider uncertainties in automatic AOI\nannotation. We showcase our approach in comparison to standard scarf plots in\nan augmented reality scenario.", "AI": {"tldr": "This paper presents uncertainty-aware scarf plot visualizations for analyzing eye-tracking data with areas of interest (AOIs), addressing various sources of uncertainties in gaze data.", "motivation": "To improve the analysis of eye-tracking data by making analysts aware of uncertainties related to gaze mapping to AOIs and depth dependency in 3D scenes.", "method": "The authors developed uncertainty-aware scarf plot visualizations and compared their effectiveness against standard scarf plots within an augmented reality context.", "result": "The proposed visualizations effectively highlight uncertainties in automatic AOI annotation and the mapping of gaze data, leading to better-informed analyses.", "conclusion": "Uncertainty-aware visualizations provide significant advantages in eye-tracking data analysis, particularly in augmented reality applications.", "key_contributions": ["Development of uncertainty-aware scarf plot visualizations.", "Highlighting uncertainties in gaze data mapping to AOIs.", "Comparison of new visualizations against standard methods."], "limitations": "The study focuses primarily on augmented reality scenarios, which may limit generalizability to other areas.", "keywords": ["eye-tracking", "uncertainty visualization", "augmented reality", "areas of interest", "scarf plots"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2505.04651", "pdf": "https://arxiv.org/pdf/2505.04651.pdf", "abs": "https://arxiv.org/abs/2505.04651", "title": "Scientific Hypothesis Generation and Validation: Methods, Datasets, and Future Directions", "authors": ["Adithya Kulkarni", "Fatimah Alotaibi", "Xinyue Zeng", "Longfeng Wu", "Tong Zeng", "Barry Menglong Yao", "Minqian Liu", "Shuaicheng Zhang", "Lifu Huang", "Dawei Zhou"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) are transforming scientific hypothesis\ngeneration and validation by enabling information synthesis, latent\nrelationship discovery, and reasoning augmentation. This survey provides a\nstructured overview of LLM-driven approaches, including symbolic frameworks,\ngenerative models, hybrid systems, and multi-agent architectures. We examine\ntechniques such as retrieval-augmented generation, knowledge-graph completion,\nsimulation, causal inference, and tool-assisted reasoning, highlighting\ntrade-offs in interpretability, novelty, and domain alignment. We contrast\nearly symbolic discovery systems (e.g., BACON, KEKADA) with modern LLM\npipelines that leverage in-context learning and domain adaptation via\nfine-tuning, retrieval, and symbolic grounding. For validation, we review\nsimulation, human-AI collaboration, causal modeling, and uncertainty\nquantification, emphasizing iterative assessment in open-world contexts. The\nsurvey maps datasets across biomedicine, materials science, environmental\nscience, and social science, introducing new resources like AHTech and\nCSKG-600. Finally, we outline a roadmap emphasizing novelty-aware generation,\nmultimodal-symbolic integration, human-in-the-loop systems, and ethical\nsafeguards, positioning LLMs as agents for principled, scalable scientific\ndiscovery.", "AI": {"tldr": "This survey explores LLM-driven methods for scientific hypothesis generation and validation, mapping various techniques and datasets across multiple domains.", "motivation": "To provide an overview of how Large Language Models are changing the landscape of scientific hypothesis generation and validation.", "method": "The paper surveys various LLM-driven approaches, including symbolic frameworks, generative models, and multi-agent architectures, while comparing early systems with modern pipelines.", "result": "The authors highlight key techniques like retrieval-augmented generation and causal inference, while mapping datasets from diverse scientific fields.", "conclusion": "The survey concludes with a roadmap for future research on novelty-aware generation and ethical safeguards in scientific discovery.", "key_contributions": ["Structured overview of LLM-driven hypothesis generation techniques", "Comparison between historical and modern LLM approaches", "Mapping of datasets across various scientific disciplines"], "limitations": "", "keywords": ["Large Language Models", "scientific hypothesis generation", "machine learning", "human-AI collaboration", "data synthesis"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2505.05170", "pdf": "https://arxiv.org/pdf/2505.05170.pdf", "abs": "https://arxiv.org/abs/2505.05170", "title": "Dukawalla: Voice Interfaces for Small Businesses in Africa", "authors": ["Elizabeth Ankrah", "Stephanie Nyairo", "Mercy Muchai", "Kagonya Awori", "Millicent Ochieng", "Mark Kariuki", "Jacki O'Neill"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Small and medium sized businesses often struggle with data driven decision\nmaking do to a lack of advanced analytics tools, especially in African\ncountries where they make up a majority of the workforce. Though many tools\nexist they are not designed to fit into the ways of working of SMB workers who\nare mobile first, have limited time to learn new workflows, and for whom social\nand business are tightly coupled. To address this, the Dukawalla prototype was\ncreated. This intelligent assistant bridges the gap between raw business data,\nand actionable insights by leveraging voice interaction and the power of\ngenerative AI. Dukawalla provides an intuitive way for business owners to\ninteract with their data, aiding in informed decision making. This paper\nexamines Dukawalla's deployment across SMBs in Nairobi, focusing on their\nexperiences using this voice based assistant to streamline data collection and\nprovide business insights", "AI": {"tldr": "The paper presents Dukawalla, an intelligent voice-based assistant designed to aid small and medium sized businesses (SMBs) in Africa with data-driven decision making by leveraging generative AI.", "motivation": "Small and medium sized businesses in African countries lack adequate analytics tools that cater to their mobile-first and time-constrained work styles, hindering data-driven decision making.", "method": "The authors developed the Dukawalla prototype, an intelligent assistant that uses voice interaction to provide businesses with actionable insights from their data.", "result": "Dukawalla was deployed across SMBs in Nairobi, where users reported improved data collection and enhanced decision-making through the voice-based assistant.", "conclusion": "The study highlights the effectiveness of Dukawalla in meeting the specific needs of SMBs in Nairobi, demonstrating the potential of voice interaction for simplifying data analytics.", "key_contributions": ["Introduction of Dukawalla as a tailored solution for SMBs' analytics needs.", "Focus on voice interaction to enhance user experience with data.", "Deployment insights from real-world usage in Nairobi's SMBs."], "limitations": "The study is limited to a specific geographical region and may not generalize to SMBs in other contexts.", "keywords": ["data-driven decision making", "voice interaction", "generative AI", "SMBs", "Nairobi"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2505.04653", "pdf": "https://arxiv.org/pdf/2505.04653.pdf", "abs": "https://arxiv.org/abs/2505.04653", "title": "Advancing Conversational Diagnostic AI with Multimodal Reasoning", "authors": ["Khaled Saab", "Jan Freyberg", "Chunjong Park", "Tim Strother", "Yong Cheng", "Wei-Hung Weng", "David G. T. Barrett", "David Stutz", "Nenad Tomasev", "Anil Palepu", "Valentin Liévin", "Yash Sharma", "Roma Ruparel", "Abdullah Ahmed", "Elahe Vedadi", "Kimberly Kanada", "Cian Hughes", "Yun Liu", "Geoff Brown", "Yang Gao", "Sean Li", "S. Sara Mahdavi", "James Manyika", "Katherine Chou", "Yossi Matias", "Avinatan Hassidim", "Dale R. Webster", "Pushmeet Kohli", "S. M. Ali Eslami", "Joëlle Barral", "Adam Rodman", "Vivek Natarajan", "Mike Schaekermann", "Tao Tu", "Alan Karthikesalingam", "Ryutaro Tanno"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated great potential for conducting\ndiagnostic conversations but evaluation has been largely limited to\nlanguage-only interactions, deviating from the real-world requirements of\nremote care delivery. Instant messaging platforms permit clinicians and\npatients to upload and discuss multimodal medical artifacts seamlessly in\nmedical consultation, but the ability of LLMs to reason over such data while\npreserving other attributes of competent diagnostic conversation remains\nunknown. Here we advance the conversational diagnosis and management\nperformance of the Articulate Medical Intelligence Explorer (AMIE) through a\nnew capability to gather and interpret multimodal data, and reason about this\nprecisely during consultations. Leveraging Gemini 2.0 Flash, our system\nimplements a state-aware dialogue framework, where conversation flow is\ndynamically controlled by intermediate model outputs reflecting patient states\nand evolving diagnoses. Follow-up questions are strategically directed by\nuncertainty in such patient states, leading to a more structured multimodal\nhistory-taking process that emulates experienced clinicians. We compared AMIE\nto primary care physicians (PCPs) in a randomized, blinded, OSCE-style study of\nchat-based consultations with patient actors. We constructed 105 evaluation\nscenarios using artifacts like smartphone skin photos, ECGs, and PDFs of\nclinical documents across diverse conditions and demographics. Our rubric\nassessed multimodal capabilities and other clinically meaningful axes like\nhistory-taking, diagnostic accuracy, management reasoning, communication, and\nempathy. Specialist evaluation showed AMIE to be superior to PCPs on 7/9\nmultimodal and 29/32 non-multimodal axes (including diagnostic accuracy). The\nresults show clear progress in multimodal conversational diagnostic AI, but\nreal-world translation needs further research.", "AI": {"tldr": "Advancement of an AI system for multimodal diagnostic consultations shows superiority over primary care physicians in various axes but requires further research for real-world application.", "motivation": "To evaluate LLMs' ability to conduct diagnostic conversations using multimodal data in remote medical care settings, addressing the limitations of previous evaluations focused solely on text.", "method": "The study advances the Articulate Medical Intelligence Explorer (AMIE) system, integrating multimodal data interpretation and a state-aware dialogue framework that adapts based on patient states during consultations. Comparisons were made with primary care physicians using a randomized, controlled study.", "result": "AMIE outperformed primary care physicians in 7 out of 9 multimodal evaluation metrics and 29 out of 32 non-multimodal metrics, demonstrating enhanced capabilities in diagnostic accuracy, management reasoning, and communication.", "conclusion": "While the findings indicate significant progress in multimodal conversational AI for diagnostics, translating these results into real-world applications necessitates additional research.", "key_contributions": ["Development of a multimodal diagnostic AI system (AMIE)", "Implementation of a state-aware dialogue framework", "Comparative study revealing AMIE's superior performance against primary care physicians"], "limitations": "Further research needed for real-world translation of findings.", "keywords": ["Large Language Models", "multimodal data", "diagnostic conversation", "health informatics", "AI in medicine"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.05441", "pdf": "https://arxiv.org/pdf/2505.05441.pdf", "abs": "https://arxiv.org/abs/2505.05441", "title": "GesPrompt: Leveraging Co-Speech Gestures to Augment LLM-Based Interaction in Virtual Reality", "authors": ["Xiyun Hu", "Dizhi Ma", "Fengming He", "Zhengzhe Zhu", "Shao-Kang Hsia", "Chenfei Zhu", "Ziyi Liu", "Karthik Ramani"], "categories": ["cs.HC"], "comment": null, "summary": "Large Language Model (LLM)-based copilots have shown great potential in\nExtended Reality (XR) applications. However, the user faces challenges when\ndescribing the 3D environments to the copilots due to the complexity of\nconveying spatial-temporal information through text or speech alone. To address\nthis, we introduce GesPrompt, a multimodal XR interface that combines co-speech\ngestures with speech, allowing end-users to communicate more naturally and\naccurately with LLM-based copilots in XR environments. By incorporating\ngestures, GesPrompt extracts spatial-temporal reference from co-speech\ngestures, reducing the need for precise textual prompts and minimizing\ncognitive load for end-users. Our contributions include (1) a workflow to\nintegrate gesture and speech input in the XR environment, (2) a prototype VR\nsystem that implements the workflow, and (3) a user study demonstrating its\neffectiveness in improving user communication in VR environments.", "AI": {"tldr": "Introduction of GesPrompt, a multimodal XR interface that enhances user communication with LLM-based copilots using gestures alongside speech.", "motivation": "To improve the user experience in XR applications by enabling more natural communication methods when interacting with LLMs.", "method": "GesPrompt combines co-speech gestures with speech, facilitating a multimodal communication workflow that extracts spatial-temporal references.", "result": "The user study shows that GesPrompt improves user communication in VR environments by reducing the need for precise textual prompts.", "conclusion": "Integrating gestures with speech in XR environments decreases cognitive load and enhances interaction with LLM copilots.", "key_contributions": ["A workflow for integrating gesture and speech input in XR.", "Development of a prototype VR system implementing this workflow.", "Evidence from a user study showing improved communication in VR."], "limitations": "", "keywords": ["Multimodal interface", "Large Language Models", "Extended Reality", "Gestures", "Communication in VR"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.04654", "pdf": "https://arxiv.org/pdf/2505.04654.pdf", "abs": "https://arxiv.org/abs/2505.04654", "title": "A Comparative Analysis of Ethical and Safety Gaps in LLMs using Relative Danger Coefficient", "authors": ["Yehor Tereshchenko", "Mika Hämäläinen"], "categories": ["cs.CL"], "comment": null, "summary": "Artificial Intelligence (AI) and Large Language Models (LLMs) have rapidly\nevolved in recent years, showcasing remarkable capabilities in natural language\nunderstanding and generation. However, these advancements also raise critical\nethical questions regarding safety, potential misuse, discrimination and\noverall societal impact. This article provides a comparative analysis of the\nethical performance of various AI models, including the brand new\nDeepSeek-V3(R1 with reasoning and without), various GPT variants (4o, 3.5\nTurbo, 4 Turbo, o1/o3 mini) and Gemini (1.5 flash, 2.0 flash and 2.0 flash exp)\nand highlights the need for robust human oversight, especially in situations\nwith high stakes. Furthermore, we present a new metric for calculating harm in\nLLMs called Relative Danger Coefficient (RDC).", "AI": {"tldr": "The paper analyzes the ethical implications of various AI and LLM models, presenting a new metric for assessing harm.", "motivation": "To address the rapid evolution of AI and LLMs while highlighting the ethical concerns and the need for human oversight.", "method": "Comparative analysis of the ethical performance of different AI models and the introduction of the Relative Danger Coefficient metric.", "result": "Identifies critical ethical issues related to AI models and emphasizes the importance of human oversight in high-stakes situations.", "conclusion": "The analysis underscores the need for improved metrics and frameworks to manage the ethical use of AI technologies.", "key_contributions": ["Comparative analysis of AI models", "Introduction of the Relative Danger Coefficient", "Highlights the need for human oversight"], "limitations": "", "keywords": ["AI Ethics", "Large Language Models", "Safety", "Human Oversight", "Ethical Performance"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.04655", "pdf": "https://arxiv.org/pdf/2505.04655.pdf", "abs": "https://arxiv.org/abs/2505.04655", "title": "Integration of Large Language Models and Traditional Deep Learning for Social Determinants of Health Prediction", "authors": ["Paul Landes", "Jimeng Sun", "Adam Cross"], "categories": ["cs.CL"], "comment": null, "summary": "Social Determinants of Health (SDoH) are economic, social and personal\ncircumstances that affect or influence an individual's health status. SDoHs\nhave shown to be correlated to wellness outcomes, and therefore, are useful to\nphysicians in diagnosing diseases and in decision-making. In this work, we\nautomatically extract SDoHs from clinical text using traditional deep learning\nand Large Language Models (LLMs) to find the advantages and disadvantages of\neach on an existing publicly available dataset. Our models outperform a\nprevious reference point on a multilabel SDoH classification by 10 points, and\nwe present a method and model to drastically speed up classification (12X\nexecution time) by eliminating expensive LLM processing. The method we present\ncombines a more nimble and efficient solution that leverages the power of the\nLLM for precision and traditional deep learning methods for efficiency. We also\nshow highly performant results on a dataset supplemented with synthetic data\nand several traditional deep learning models that outperform LLMs. Our models\nand methods offer the next iteration of automatic prediction of SDoHs that\nimpact at-risk patients.", "AI": {"tldr": "The paper presents a method to automatically extract Social Determinants of Health (SDoH) from clinical text using both traditional deep learning and Large Language Models (LLMs), achieving faster and more efficient classification.", "motivation": "To address the need for accurate identification of Social Determinants of Health (SDoH) which impact patient outcomes and clinical decision-making.", "method": "The authors compare traditional deep learning methods with Large Language Models (LLMs) for extracting SDoHs from clinical text, showing a 12X reduction in execution time compared to solely using LLMs.", "result": "The proposed models outperform previous benchmarks in multilabel SDoH classification by 10 points and demonstrate highly efficient results even with datasets supplemented by synthetic data.", "conclusion": "The models and methods provide an effective solution for the automatic prediction of SDoHs, particularly beneficial for at-risk patients.", "key_contributions": ["Demonstrated performance improvement in SDoH classification using a combined approach of traditional deep learning and LLMs.", "Achieved significant execution time reduction (12X) in processing clinical text.", "Showcased effective results with synthetic data augmentation."], "limitations": "", "keywords": ["Social Determinants of Health", "Deep Learning", "Large Language Models", "Clinical Text", "Healthcare AI"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.04660", "pdf": "https://arxiv.org/pdf/2505.04660.pdf", "abs": "https://arxiv.org/abs/2505.04660", "title": "AI-Generated Fall Data: Assessing LLMs and Diffusion Model for Wearable Fall Detection", "authors": ["Sana Alamgeer", "Yasine Souissi", "Anne H. H. Ngu"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Training fall detection systems is challenging due to the scarcity of\nreal-world fall data, particularly from elderly individuals. To address this,\nwe explore the potential of Large Language Models (LLMs) for generating\nsynthetic fall data. This study evaluates text-to-motion (T2M, SATO, ParCo) and\ntext-to-text models (GPT4o, GPT4, Gemini) in simulating realistic fall\nscenarios. We generate synthetic datasets and integrate them with four\nreal-world baseline datasets to assess their impact on fall detection\nperformance using a Long Short-Term Memory (LSTM) model. Additionally, we\ncompare LLM-generated synthetic data with a diffusion-based method to evaluate\ntheir alignment with real accelerometer distributions. Results indicate that\ndataset characteristics significantly influence the effectiveness of synthetic\ndata, with LLM-generated data performing best in low-frequency settings (e.g.,\n20Hz) while showing instability in high-frequency datasets (e.g., 200Hz). While\ntext-to-motion models produce more realistic biomechanical data than\ntext-to-text models, their impact on fall detection varies. Diffusion-based\nsynthetic data demonstrates the closest alignment to real data but does not\nconsistently enhance model performance. An ablation study further confirms that\nthe effectiveness of synthetic data depends on sensor placement and fall\nrepresentation. These findings provide insights into optimizing synthetic data\ngeneration for fall detection models.", "AI": {"tldr": "This study explores using Large Language Models to generate synthetic fall data for improving fall detection systems, particularly with elderly individuals.", "motivation": "The challenge of training fall detection systems due to a lack of real-world fall data from the elderly.", "method": "Evaluation of text-to-motion and text-to-text models to generate synthetic data, integrated with real-world datasets, assessed via LSTM performance.", "result": "LLM-generated synthetic data performs best in low-frequency scenarios, but varies in impact based on dataset characteristics and sensor placements.", "conclusion": "These findings highlight the importance of dataset characteristics and generation methods for effective fall detection modeling using synthetic data.", "key_contributions": ["Explored LLMs for generating synthetic fall data", "Compared performance of text-to-motion vs text-to-text models", "Provided insights on dataset characteristics impacting detection performance"], "limitations": "The instability of LLM-generated data in high-frequency settings and dependency on sensor placement.", "keywords": ["fall detection", "synthetic data", "large language models", "LSTM", "elderly"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.04665", "pdf": "https://arxiv.org/pdf/2505.04665.pdf", "abs": "https://arxiv.org/abs/2505.04665", "title": "Personalized Risks and Regulatory Strategies of Large Language Models in Digital Advertising", "authors": ["Haoyang Feng", "Yanjun Dai", "Yuan Gao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Although large language models have demonstrated the potential for\npersonalized advertising recommendations in experimental environments, in\nactual operations, how advertising recommendation systems can be combined with\nmeasures such as user privacy protection and data security is still an area\nworthy of in-depth discussion. To this end, this paper studies the personalized\nrisks and regulatory strategies of large language models in digital\nadvertising. This study first outlines the principles of Large Language Model\n(LLM), especially the self-attention mechanism based on the Transformer\narchitecture, and how to enable the model to understand and generate natural\nlanguage text. Then, the BERT (Bidirectional Encoder Representations from\nTransformers) model and the attention mechanism are combined to construct an\nalgorithmic model for personalized advertising recommendations and user factor\nrisk protection. The specific steps include: data collection and preprocessing,\nfeature selection and construction, using large language models such as BERT\nfor advertising semantic embedding, and ad recommendations based on user\nportraits. Then, local model training and data encryption are used to ensure\nthe security of user privacy and avoid the leakage of personal data. This paper\ndesigns an experiment for personalized advertising recommendation based on a\nlarge language model of BERT and verifies it with real user data. The\nexperimental results show that BERT-based advertising push can effectively\nimprove the click-through rate and conversion rate of advertisements. At the\nsame time, through local model training and privacy protection mechanisms, the\nrisk of user privacy leakage can be reduced to a certain extent.", "AI": {"tldr": "This paper explores the integration of large language models in personalized advertising, focusing on user privacy and data security measures.", "motivation": "To address the challenges of combining personalized advertising recommendations with user privacy protection and data security in actual operations.", "method": "The paper outlines the LLM principles and constructs an algorithmic model using BERT for personalized advertising recommendations, incorporating data preprocessing, feature selection, semantic embedding, local model training, and data encryption.", "result": "Experimental results indicate that BERT-based advertising recommendations significantly increase click-through and conversion rates while also reducing the risk of user privacy leakage through security measures.", "conclusion": "The study demonstrates that it is feasible to improve advertising effectiveness with LLMs while maintaining privacy and security protocols.", "key_contributions": ["Development of a BERT-based algorithmic model for personalized advertising", "Integration of user privacy protection measures in advertising recommendations", "Experimental validation with real user data showing improved ad performance"], "limitations": "", "keywords": ["Large Language Models", "Personalized Advertising", "User Privacy", "BERT", "Data Security"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.04666", "pdf": "https://arxiv.org/pdf/2505.04666.pdf", "abs": "https://arxiv.org/abs/2505.04666", "title": "Fine-Tuning Large Language Models and Evaluating Retrieval Methods for Improved Question Answering on Building Codes", "authors": ["Mohammad Aqib", "Mohd Hamza", "Qipei Mei", "Ying Hei Chui"], "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": null, "summary": "Building codes are regulations that establish standards for the design,\nconstruction, and safety of buildings to ensure structural integrity, fire\nprotection, and accessibility. They are often extensive, complex, and subject\nto frequent updates, making manual querying challenging and time-consuming. Key\ndifficulties include navigating large volumes of text, interpreting technical\nlanguage, and identifying relevant clauses across different sections. A\npotential solution is to build a Question-Answering (QA) system that answers\nuser queries based on building codes. Among the various methods for building a\nQA system, Retrieval-Augmented Generation (RAG) stands out in performance. RAG\nconsists of two components: a retriever and a language model. This study\nfocuses on identifying a suitable retriever method for building codes and\noptimizing the generational capability of the language model using fine-tuning\ntechniques. We conducted a detailed evaluation of various retrieval methods by\nperforming the retrieval on the National Building Code of Canada (NBCC) and\nexplored the impact of domain-specific fine-tuning on several language models\nusing the dataset derived from NBCC. Our analysis included a comparative\nassessment of different retrievers and the performance of both pre-trained and\nfine-tuned models to determine the efficacy and domain-specific adaptation of\nlanguage models using fine-tuning on the NBCC dataset. Experimental results\nshowed that Elasticsearch proved to be the most robust retriever among all. The\nfindings also indicate that fine-tuning language models on an NBCC-specific\ndataset can enhance their ability to generate contextually relevant responses.\nWhen combined with context retrieved by a powerful retriever like\nElasticsearch, this improvement in LLM performance can optimize the RAG system,\nenabling it to better navigate the complexities of the NBCC.", "AI": {"tldr": "This paper presents a QA system for building codes using Retrieval-Augmented Generation (RAG) to enhance query responses based on the National Building Code of Canada (NBCC).", "motivation": "The motivation behind this study is to overcome the challenges associated with manually querying extensive and complex building codes, by leveraging a QA system to improve access to relevant information.", "method": "The methodology involves evaluating various retriever methods for the QA system, focusing on optimizing a language model's generational capability through domain-specific fine-tuning techniques using data from the NBCC.", "result": "The experiments revealed that Elasticsearch is the most effective retriever for building codes, and that fine-tuning language models on the NBCC dataset significantly improves their contextual response generation.", "conclusion": "Overall, the integration of a powerful retriever like Elasticsearch with fine-tuned language models leads to enhanced performance in navigating the complexities of building codes, thereby improving user access to vital information.", "key_contributions": ["Development of a QA system for building codes using RAG", "Identification of Elasticsearch as a robust retriever for the NBCC", "Demonstration of improved LLM performance through domain-specific fine-tuning"], "limitations": "", "keywords": ["Question-Answering", "Retrieval-Augmented Generation", "Building Codes", "Language Models", "Fine-Tuning"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2410.00192", "pdf": "https://arxiv.org/pdf/2410.00192.pdf", "abs": "https://arxiv.org/abs/2410.00192", "title": "Large-scale, Longitudinal, Hybrid Participatory Design Program to Create Navigation Technology for the Blind", "authors": ["Daeun Joyce Chung", "Muya Guoji", "Nina Mindel", "Alexis Malkin", "Fernando Albertorio", "Shane Lowe", "Chris McNally", "Casandra Xavier", "Paul Ruvolo"], "categories": ["cs.HC"], "comment": null, "summary": "Empowering people who are blind or visually impaired (BVI) to enhance their\norientation and mobility skills is critical to equalizing their access to\nsocial and economic opportunities. To manage this crucial challenge, we\nemployed a novel design process based on a large-scale, longitudinal,\ncommunity-based structure. Across three annual programs we engaged with the BVI\ncommunity in online and in-person modes. In total, our team included 67 total\nBVI participatory design participants online, 11 BVI co-designers in-person,\nand 4 BVI program coordinators. Through this design process we built a mobile\napplication that enables users to generate, share, and navigate maps of indoor\nand outdoor environments without the need to instrument each environment with\nbeacons or fiducial markers. We evaluated this app at a healthcare facility,\nand participants in the evaluation rated the app highly with respect to its\ndesign, features, and potential for positive impact on quality of life.", "AI": {"tldr": "A mobile app was designed to help individuals who are blind or visually impaired improve their orientation and mobility by enabling map generation and navigation without external markers.", "motivation": "To equalize access to social and economic opportunities for people who are blind or visually impaired by improving their orientation and mobility skills.", "method": "A community-based design process engaging 67 online and 11 in-person BVI participants, along with 4 program coordinators, over three annual programs to develop and evaluate a mobile app.", "result": "Participants rated the app highly for its design, features, and potential impact on quality of life during evaluations at a healthcare facility.", "conclusion": "The mobile app shows promise for enhancing mobility and orientation for individuals who are blind or visually impaired and could positively affect their quality of life.", "key_contributions": ["Innovative design process involving the BVI community", "Development of a map generation and navigation app without the need for dedicated infrastructure", "Positive feedback from BVI participants regarding the app's usability and potential benefits"], "limitations": "", "keywords": ["Blindness", "Visual impairment", "Human-computer interaction", "Mobile application", "Community design"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.04671", "pdf": "https://arxiv.org/pdf/2505.04671.pdf", "abs": "https://arxiv.org/abs/2505.04671", "title": "Reward-SQL: Boosting Text-to-SQL via Stepwise Reasoning and Process-Supervised Rewards", "authors": ["Yuxin Zhang", "Meihao Fan", "Ju Fan", "Mingyang Yi", "Yuyu Luo", "Jian Tan", "Guoliang Li"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Recent advances in large language models (LLMs) have significantly improved\nperformance on the Text-to-SQL task by leveraging their powerful reasoning\ncapabilities. To enhance accuracy during the reasoning process, external\nProcess Reward Models (PRMs) can be introduced during training and inference to\nprovide fine-grained supervision. However, if misused, PRMs may distort the\nreasoning trajectory and lead to suboptimal or incorrect SQL generation.To\naddress this challenge, we propose Reward-SQL, a framework that systematically\nexplores how to incorporate PRMs into the Text-to-SQL reasoning process\neffectively. Our approach follows a \"cold start, then PRM supervision\"\nparadigm. Specifically, we first train the model to decompose SQL queries into\nstructured stepwise reasoning chains using common table expressions\n(Chain-of-CTEs), establishing a strong and interpretable reasoning baseline.\nThen, we investigate four strategies for integrating PRMs, and find that\ncombining PRM as an online training signal (GRPO) with PRM-guided inference\n(e.g., best-of-N sampling) yields the best results. Empirically, on the BIRD\nbenchmark, Reward-SQL enables models supervised by a 7B PRM to achieve a 13.1%\nperformance gain across various guidance strategies. Notably, our GRPO-aligned\npolicy model based on Qwen2.5-Coder-7B-Instruct achieves 68.9% accuracy on the\nBIRD development set, outperforming all baseline methods under the same model\nsize. These results demonstrate the effectiveness of Reward-SQL in leveraging\nreward-based supervision for Text-to-SQL reasoning. Our code is publicly\navailable.", "AI": {"tldr": "Reward-SQL is a framework that incorporates Process Reward Models (PRMs) into the Text-to-SQL reasoning process to improve SQL generation accuracy.", "motivation": "To enhance accuracy in SQL generation by effectively using Reward Models during reasoning.", "method": "A two-step approach that first decomposes SQL queries into stepwise reasoning chains, followed by integrating PRMs into the reasoning process.", "result": "Reward-SQL demonstrates a 13.1% performance improvement on the BIRD benchmark with models using PRMs for guidance during training and inference.", "conclusion": "The Reward-SQL framework effectively leverages reward-based supervision, yielding superior accuracy in Text-to-SQL tasks.", "key_contributions": ["Introduction of the Reward-SQL framework for Text-to-SQL tasks.", "Employed a two-step reasoning process combining SQL decomposition and PRM integration.", "Achieved state-of-the-art accuracy on the BIRD benchmark with PRM-guided models."], "limitations": "", "keywords": ["Text-to-SQL", "Process Reward Models", "Chain-of-CTEs", "Machine Learning", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2410.04686", "pdf": "https://arxiv.org/pdf/2410.04686.pdf", "abs": "https://arxiv.org/abs/2410.04686", "title": "From Perception to Decision: Assessing the Role of Chart Types Affordances in High-Level Decision Tasks", "authors": ["Yixuan Li", "Emery D. Berger", "Minsuk Kahng", "Cindy Xiong Bearfield"], "categories": ["cs.HC"], "comment": null, "summary": "Visualization design influences how people perceive data patterns, yet most\nresearch focuses on low-level analytic tasks, such as finding correlations. The\nextent to which these perceptual affordances translate to high-level\ndecision-making in the real world remains underexplored. Through a case study\nof academic mentorship selection using bar charts and pie charts, we\ninvestigated whether chart types differentially influence how students evaluate\nfaculty research profiles. Our crowdsourced experiment revealed only minimal\ndifferences in decision outcomes between chart types, suggesting that\nperceptual affordances established in controlled analytical tasks may not\ndirectly translate to high-level decision scenarios. These findings emphasize\nthe importance of evaluating visualizations within real-world contexts and\nhighlight the need to distinguish between perceptual and decision affordances\nwhen developing visualization guidelines.", "AI": {"tldr": "This study examines how different visualization types (bar charts vs. pie charts) influence high-level decision-making in academic mentorship selection.", "motivation": "To explore the effect of visualization design on high-level decision-making as opposed to low-level analytic tasks, which are more commonly studied.", "method": "A crowdsourced experiment comparing the effectiveness of bar charts and pie charts in helping students evaluate faculty research profiles.", "result": "Minimal differences were found in decision outcomes based on the chart type, indicating that findings from controlled environments may not apply to real-world decision-making.", "conclusion": "Visualizations should be evaluated in real-world contexts, and it's crucial to differentiate between perceptual affordances and decision affordances.", "key_contributions": ["Investigation of visualization effectiveness in high-level decision-making.", "Insights on evaluating visualizations in real-world contexts.", "Clarification of the distinction between perceptual and decision affordances."], "limitations": "The study's findings are based on a specific case study and may not generalize to all visualization scenarios.", "keywords": ["visualization", "decision-making", "academic mentorship", "perceptual affordances", "chart types"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.04673", "pdf": "https://arxiv.org/pdf/2505.04673.pdf", "abs": "https://arxiv.org/abs/2505.04673", "title": "REVEAL: Multi-turn Evaluation of Image-Input Harms for Vision LLM", "authors": ["Madhur Jindal", "Saurabh Deshpande"], "categories": ["cs.CL", "cs.AI"], "comment": "13 pages (8 main), to be published in IJCAI 2025", "summary": "Vision Large Language Models (VLLMs) represent a significant advancement in\nartificial intelligence by integrating image-processing capabilities with\ntextual understanding, thereby enhancing user interactions and expanding\napplication domains. However, their increased complexity introduces novel\nsafety and ethical challenges, particularly in multi-modal and multi-turn\nconversations. Traditional safety evaluation frameworks, designed for\ntext-based, single-turn interactions, are inadequate for addressing these\ncomplexities. To bridge this gap, we introduce the REVEAL (Responsible\nEvaluation of Vision-Enabled AI LLMs) Framework, a scalable and automated\npipeline for evaluating image-input harms in VLLMs. REVEAL includes automated\nimage mining, synthetic adversarial data generation, multi-turn conversational\nexpansion using crescendo attack strategies, and comprehensive harm assessment\nthrough evaluators like GPT-4o.\n  We extensively evaluated five state-of-the-art VLLMs, GPT-4o, Llama-3.2,\nQwen2-VL, Phi3.5V, and Pixtral, across three important harm categories: sexual\nharm, violence, and misinformation. Our findings reveal that multi-turn\ninteractions result in significantly higher defect rates compared to\nsingle-turn evaluations, highlighting deeper vulnerabilities in VLLMs. Notably,\nGPT-4o demonstrated the most balanced performance as measured by our\nSafety-Usability Index (SUI) followed closely by Pixtral. Additionally,\nmisinformation emerged as a critical area requiring enhanced contextual\ndefenses. Llama-3.2 exhibited the highest MT defect rate ($16.55 \\%$) while\nQwen2-VL showed the highest MT refusal rate ($19.1 \\%$).", "AI": {"tldr": "Introduction of the REVEAL Framework for evaluating safety in Vision Large Language Models (VLLMs).", "motivation": " VLLMs integrate image and text processing but face significant safety and ethical challenges not addressed by traditional evaluation methods.", "method": "The REVEAL Framework automates image mining, generates synthetic adversarial data, expands multi-turn conversations, and assesses harm with evaluators like GPT-4o.", "result": "Evaluated five state-of-the-art VLLMs, revealing higher defect rates in multi-turn interactions, with GPT-4o performing best across metrics.", "conclusion": "Misinformation is critical in VLLMs; existing models need improved defenses and evaluation methods.", "key_contributions": ["Introduction of the REVEAL Framework for VLLM evaluation.", "Evaluation of multiple VLLMs leading to significant insights into their vulnerabilities.", "Identification of misinformation as a crucial area for enhancement."], "limitations": "Framework primarily focused on harm assessment; additional studies needed for broader implications.", "keywords": ["Vision Large Language Models", "safety evaluation", "adversarial data generation", "harm assessment", "multi-turn interactions"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2503.12613", "pdf": "https://arxiv.org/pdf/2503.12613.pdf", "abs": "https://arxiv.org/abs/2503.12613", "title": "Negotiative Alignment: Embracing Disagreement to Achieve Fairer Outcomes -- Insights from Urban Studies", "authors": ["Rashid Mushkani", "Hugo Berard", "Shin Koseki"], "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.MA"], "comment": "16 pages, 13 figures", "summary": "Cities are not monolithic; they are arenas of negotiation among groups that\nhold varying needs, values, and experiences. Conventional methods of urban\nassessment -- from standardized surveys to AI-driven evaluations -- frequently\nrely on a single consensus metric (e.g., an average measure of inclusivity or\nsafety). Although such aggregations simplify design decisions, they risk\nobscuring the distinct perspectives of marginalized populations. In this paper,\nwe present findings from a community-centered study in Montreal involving 35\nresidents with diverse demographic and social identities, particularly\nwheelchair users, seniors, and LGBTQIA2+ individuals. Using rating and ranking\ntasks on 20 urban sites, we observe that disagreements are systematic rather\nthan random, reflecting structural inequalities, differing cultural values, and\npersonal experiences of safety and accessibility.\n  Based on these empirical insights, we propose negotiative alignment, an AI\nframework that treats disagreement as an essential input to be preserved,\nanalyzed, and addressed. Negotiative alignment builds on pluralistic models by\ndynamically updating stakeholder preferences through multi-agent negotiation\nmechanisms, ensuring no single perspective is marginalized. We outline how this\nframework can be integrated into urban analytics -- and other decision-making\ncontexts -- to retain minority viewpoints, adapt to changing stakeholder\nconcerns, and enhance fairness and accountability. The study demonstrates that\npreserving and engaging with disagreement, rather than striving for an\nartificial consensus, can produce more equitable and responsive AI-driven\noutcomes in urban design.", "AI": {"tldr": "The paper presents a community-centered study on urban assessment in Montreal, highlighting the importance of diverse perspectives in evaluating urban spaces, and proposes an AI framework called 'negotiative alignment' that incorporates stakeholder disagreements to improve fairness in urban design decisions.", "motivation": "Conventional urban assessment methods often overlook the perspectives of marginalized groups, risking unfair outcomes in urban design.", "method": "A community-centered study involving 35 residents, including wheelchair users, seniors, and LGBTQIA2+ individuals, used rating and ranking tasks across 20 urban sites to analyze differing opinions on inclusivity and safety.", "result": "The study found systematic disagreements among residents that reflect structural inequalities and differing cultural values, leading to the proposal of the negotiative alignment framework which aims to integrate these disagreements into urban analytics.", "conclusion": "The framework can enhance fairness and accountability in urban design by retaining minority viewpoints and adapting to changing stakeholder concerns, advocating for engagement with disagreement rather than seeking consensus.", "key_contributions": ["Introduction of the negotiative alignment AI framework", "Empirical findings that highlight systematic disagreements in urban assessments", "Suggestions for integrating the framework into urban analytics for better decision-making"], "limitations": "", "keywords": ["urban assessment", "AI", "negotiative alignment", "marginalized perspectives", "community-centered design"], "importance_score": 6, "read_time_minutes": 16}}
{"id": "2505.04678", "pdf": "https://arxiv.org/pdf/2505.04678.pdf", "abs": "https://arxiv.org/abs/2505.04678", "title": "Advanced Deep Learning Approaches for Automated Recognition of Cuneiform Symbols", "authors": ["Shahad Elshehaby", "Alavikunhu Panthakkan", "Hussain Al-Ahmad", "Mina Al-Saad"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper presents a thoroughly automated method for identifying and\ninterpreting cuneiform characters via advanced deep-learning algorithms. Five\ndistinct deep-learning models were trained on a comprehensive dataset of\ncuneiform characters and evaluated according to critical performance metrics,\nincluding accuracy and precision. Two models demonstrated outstanding\nperformance and were subsequently assessed using cuneiform symbols from the\nHammurabi law acquisition, notably Hammurabi Law 1. Each model effectively\nrecognized the relevant Akkadian meanings of the symbols and delivered precise\nEnglish translations. Future work will investigate ensemble and stacking\napproaches to optimize performance, utilizing hybrid architectures to improve\ndetection accuracy and reliability. This research explores the linguistic\nrelationships between Akkadian, an ancient Mesopotamian language, and Arabic,\nemphasizing their historical and cultural linkages. This study demonstrates the\ncapability of deep learning to decipher ancient scripts by merging\ncomputational linguistics with archaeology, therefore providing significant\ninsights for the comprehension and conservation of human history.", "AI": {"tldr": "This paper proposes an automated deep-learning method for identifying and interpreting cuneiform characters, achieving high accuracy in translation.", "motivation": "To explore the linguistic relationships between ancient Akkadian and Arabic and to improve methods for deciphering cuneiform scripts using advanced deep learning techniques.", "method": "Five distinct deep-learning models were trained on a comprehensive dataset of cuneiform characters and evaluated for their performance in accuracy and precision.", "result": "Two models showcased outstanding performance by accurately recognizing cuneiform symbols and delivering precise English translations of Akkadian meanings.", "conclusion": "The study highlights the potential of deep learning in deciphering ancient languages and provides insights for archaeology and historical preservation.", "key_contributions": ["Development of an automated method for cuneiform character identification", "Use of deep learning to translate ancient scripts", "Integration of computational linguistics with archaeological research."], "limitations": "", "keywords": ["deep learning", "cuneiform characters", "Akkadian", "machine learning", "historical linguistics"], "importance_score": 2, "read_time_minutes": 10}}
{"id": "2504.01259", "pdf": "https://arxiv.org/pdf/2504.01259.pdf", "abs": "https://arxiv.org/abs/2504.01259", "title": "Facilitating Instructors-LLM Collaboration for Problem Design in Introductory Programming Classrooms", "authors": ["Muntasir Hoq", "Jessica Vandenberg", "Shuyin Jiao", "Seung Lee", "Bradford Mott", "Narges Norouzi", "James Lester", "Bita Akram"], "categories": ["cs.HC", "K.3.1"], "comment": "Accepted at CHI 2025 Workshop on Augmented Educators and AI: Shaping\n  the Future of Human and AI Cooperation in Learning", "summary": "Advancements in Large Language Models (LLMs), such as ChatGPT, offer\nsignificant opportunities to enhance instructional support in introductory\nprogramming courses. While extensive research has explored the effectiveness of\nLLMs in supporting student learning, limited studies have examined how these\nmodels can assist instructors in designing instructional activities. This work\ninvestigates how instructors' expertise in effective activity design can be\nintegrated with LLMs' ability to generate novel and targeted programming\nproblems, facilitating more effective activity creation for programming\nclassrooms. To achieve this, we employ a participatory design approach to\ndevelop an instructor-authoring tool that incorporates LLM support, fostering\ncollaboration between instructors and AI in generating programming exercises.\nThis tool also allows instructors to specify common student mistakes and\nmisconceptions, which informs the adaptive feedback generation process. We\nconduct case studies with three instructors, analyzing how they use our system\nto design programming problems for their introductory courses. Through these\ncase studies, we assess instructors' perceptions of the usefulness and\nlimitations of LLMs in authoring problem statements for instructional purposes.\nAdditionally, we compare the efficiency, quality, effectiveness, and coverage\nof designed activities when instructors create problems with and without\nstructured LLM prompting guidelines. Our findings provide insights into the\npotential of LLMs in enhancing instructor workflows and improving programming\neducation and provide guidelines for designing effective AI-assisted\nproblem-authoring interfaces.", "AI": {"tldr": "This paper investigates the integration of Large Language Models (LLMs) in designing instructional activities for introductory programming courses, focusing on an instructor-authoring tool that leverages LLMs to create effective programming problems.", "motivation": "To explore how LLMs can assist instructors in enhancing the design of instructional activities, specifically programming exercises, thus improving programming education.", "method": "A participatory design approach was used to develop an instructor-authoring tool that incorporates LLM support and allows instructors to specify common student mistakes for adaptive feedback generation.", "result": "Case studies with three instructors revealed insights into their perceptions of the utility and limitations of LLMs, showcasing improved efficiency and quality in problem design when using structured LLM prompting guidelines.", "conclusion": "The findings indicate that LLMs can significantly enhance instructor workflows and programming education, providing useful guidelines for AI-assisted problem-authoring interfaces.", "key_contributions": ["Development of an instructor-authoring tool integrating LLM support for programming problem creation.", "Analysis of instructor perceptions regarding LLMs in instructional design.", "Assessment of the impact of structured LLM prompting guidelines on activity design efficiency and quality."], "limitations": "Limited to initial case studies with three instructors; broader applicability and long-term effects of LLM integration remain to be explored.", "keywords": ["Large Language Models", "programming education", "instructional design", "AI-assisted tools", "adaptive feedback"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.04723", "pdf": "https://arxiv.org/pdf/2505.04723.pdf", "abs": "https://arxiv.org/abs/2505.04723", "title": "SOAEsV2-7B/72B: Full-Pipeline Optimization for State-Owned Enterprise LLMs via Continual Pre-Training, Domain-Progressive SFT and Distillation-Enhanced Speculative Decoding", "authors": ["Jingyang Deng", "Ran Chen", "Jo-Ku Cheng", "Jinwen Ma"], "categories": ["cs.CL"], "comment": null, "summary": "This study addresses key challenges in developing domain-specific large\nlanguage models (LLMs) for Chinese state-owned assets and enterprises (SOAEs),\nwhere current approaches face three limitations: 1) constrained model capacity\nthat limits knowledge integration and cross-task adaptability; 2) excessive\nreliance on domain-specific supervised fine-tuning (SFT) data, which neglects\nthe broader applicability of general language patterns; and 3) inefficient\ninference acceleration for large models processing long contexts. In this work,\nwe propose SOAEsV2-7B/72B, a specialized LLM series developed via a three-phase\nframework: 1) continual pre-training integrates domain knowledge while\nretaining base capabilities; 2) domain-progressive SFT employs curriculum-based\nlearning strategy, transitioning from weakly relevant conversational data to\nexpert-annotated SOAEs datasets to optimize domain-specific tasks; 3)\ndistillation-enhanced speculative decoding accelerates inference via logit\ndistillation between 72B target and 7B draft models, achieving\n1.39-1.52$\\times$ speedup without quality loss. Experimental results\ndemonstrate that our domain-specific pre-training phase maintains 99.8% of\noriginal general language capabilities while significantly improving domain\nperformance, resulting in a 1.08$\\times$ improvement in Rouge-1 score and a\n1.17$\\times$ enhancement in BLEU-4 score. Ablation studies further show that\ndomain-progressive SFT outperforms single-stage training, achieving\n1.02$\\times$ improvement in Rouge-1 and 1.06$\\times$ in BLEU-4. Our work\nintroduces a comprehensive, full-pipeline approach for optimizing SOAEs LLMs,\nbridging the gap between general language capabilities and domain-specific\nexpertise.", "AI": {"tldr": "This study presents SOAEsV2-7B/72B, a series of domain-specific large language models for Chinese state-owned assets, addressing key limitations in model capacity, data reliance, and inference speed.", "motivation": "To improve the performance of domain-specific large language models for Chinese state-owned assets and enterprises, overcoming limitations in current methodologies.", "method": "The framework involves continual pre-training to integrate domain knowledge, domain-progressive supervised fine-tuning with a curriculum learning strategy, and distillation-enhanced speculative decoding to improve inference speed.", "result": "The proposed method demonstrates a significant speedup in inference without loss of quality, while maintaining high general language capabilities and improving domain-specific performance on evaluation metrics.", "conclusion": "The study successfully outlines a comprehensive approach for refining LLMs in specific domains, showcasing substantial performance gains while retaining foundational language skills.", "key_contributions": ["Development of SOAEsV2-7B/72B series of LLMs", "Implementation of a three-phase framework for domain-specific model enhancement", "Empirical evidence of performance improvements in domain-specific tasks"], "limitations": "", "keywords": ["large language models", "domain-specific LLMs", "Chinese state-owned enterprises"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2505.04785", "pdf": "https://arxiv.org/pdf/2505.04785.pdf", "abs": "https://arxiv.org/abs/2505.04785", "title": "Flower Across Time and Media: Sentiment Analysis of Tang Song Poetry and Visual Correspondence", "authors": ["Shuai Gong", "Tiange Zhou"], "categories": ["cs.CL", "cs.AI"], "comment": "5 pages, 9 figures", "summary": "The Tang (618 to 907) and Song (960 to 1279) dynasties witnessed an\nextraordinary flourishing of Chinese cultural expression, where floral motifs\nserved as a dynamic medium for both poetic sentiment and artistic design. While\nprevious scholarship has examined these domains independently, the systematic\ncorrelation between evolving literary emotions and visual culture remains\nunderexplored. This study addresses that gap by employing BERT-based sentiment\nanalysis to quantify emotional patterns in floral imagery across Tang Song\npoetry, then validating these patterns against contemporaneous developments in\ndecorative arts.Our approach builds upon recent advances in computational\nhumanities while remaining grounded in traditional sinological methods. By\napplying a fine tuned BERT model to analyze peony and plum blossom imagery in\nclassical poetry, we detect measurable shifts in emotional connotations between\nthe Tang and Song periods. These textual patterns are then cross berenced with\nvisual evidence from textiles, ceramics, and other material culture, revealing\npreviously unrecognized synergies between literary expression and artistic\nrepresentation.", "AI": {"tldr": "This study uses BERT-based sentiment analysis to connect the emotional patterns in floral motifs found in Tang and Song dynasty poetry with corresponding developments in decorative arts, revealing significant correlations between literature and visual culture.", "motivation": "To explore the systematic correlation between emotional expression in poetry and visual cultural representation during the Tang and Song dynasties.", "method": "Employing BERT-based sentiment analysis to quantify emotional patterns in floral imagery in Tang Song poetry and validating these patterns against developments in decorative arts.", "result": "Detects measurable shifts in emotional connotations associated with floral imagery, specifically peony and plum blossom, between the Tang and Song periods, confirmed by cross-referencing with material culture evidence.", "conclusion": "The study reveals previously unrecognized synergies between literary expression and artistic representation in historical contexts.", "key_contributions": ["Application of BERT-based sentiment analysis in a new cultural context.", "Cross-referencing literary content with visual evidence to provide comprehensive insights.", "The identification of emotional shifts in floral motifs across two significant Chinese dynasties."], "limitations": "The study may be limited by the selection of poetry and visual evidence examined, and potential biases in the sentiment analysis models used.", "keywords": ["BERT", "sentiment analysis", "Tang dynasty", "Song dynasty", "visual culture"], "importance_score": 2, "read_time_minutes": 5}}
{"id": "2505.04844", "pdf": "https://arxiv.org/pdf/2505.04844.pdf", "abs": "https://arxiv.org/abs/2505.04844", "title": "Osiris: A Lightweight Open-Source Hallucination Detection System", "authors": ["Alex Shan", "John Bauer", "Christopher D. Manning"], "categories": ["cs.CL"], "comment": "Stanford 191W", "summary": "Retrieval-Augmented Generation (RAG) systems have gained widespread adoption\nby application builders because they leverage sources of truth to enable Large\nLanguage Models (LLMs) to generate more factually sound responses. However,\nhallucinations, instances of LLM responses that are unfaithful to the provided\ncontext, often prevent these systems from being deployed in production\nenvironments. Current hallucination detection methods typically involve human\nevaluation or the use of closed-source models to review RAG system outputs for\nhallucinations. Both human evaluators and closed-source models suffer from\nscaling issues due to their high costs and slow inference speeds. In this work,\nwe introduce a perturbed multi-hop QA dataset with induced hallucinations. Via\nsupervised fine-tuning on our dataset, we achieve better recall with a 7B model\nthan GPT-4o on the RAGTruth hallucination detection benchmark and offer\ncompetitive performance on precision and accuracy, all while using a fraction\nof the parameters. Code is released at our repository.", "AI": {"tldr": "This paper introduces a new dataset and method for detecting hallucinations in Retrieval-Augmented Generation systems.", "motivation": "To address the challenge of hallucinations in RAG systems that hinder their deployment in production by providing a scalable and efficient detection method.", "method": "The authors present a perturbed multi-hop QA dataset that includes induced hallucinations and demonstrate improvements in hallucination detection through supervised fine-tuning on this dataset with a 7B model.", "result": "The proposed method achieves better recall than GPT-4o on the RAGTruth benchmark while maintaining competitive precision and accuracy with significantly fewer parameters.", "conclusion": "The introduction of the new dataset and method offers an effective solution for hallucination detection in RAG systems, essential for their real-world application.", "key_contributions": ["Introduction of a perturbed multi-hop QA dataset for hallucination detection.", "Improved recall metrics compared to existing models like GPT-4o.", "Demonstration of competitive performance with fewer parameters."], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "hallucination detection", "Large Language Models", "multi-hop QA"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.04847", "pdf": "https://arxiv.org/pdf/2505.04847.pdf", "abs": "https://arxiv.org/abs/2505.04847", "title": "Benchmarking LLM Faithfulness in RAG with Evolving Leaderboards", "authors": ["Manveer Singh Tamber", "Forrest Sheng Bao", "Chenyu Xu", "Ge Luo", "Suleman Kazi", "Minseok Bae", "Miaoran Li", "Ofer Mendelevitch", "Renyi Qu", "Jimmy Lin"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Hallucinations remain a persistent challenge for LLMs. RAG aims to reduce\nhallucinations by grounding responses in contexts. However, even when provided\ncontext, LLMs still frequently introduce unsupported information or\ncontradictions. This paper presents our efforts to measure LLM hallucinations\nwith a focus on summarization tasks, assessing how often various LLMs introduce\nhallucinations when summarizing documents. We discuss Vectara's existing LLM\nhallucination leaderboard, based on the Hughes Hallucination Evaluation Model\n(HHEM). While HHEM and Vectara's Hallucination Leaderboard have garnered great\nresearch interest, we examine challenges faced by HHEM and current\nhallucination detection methods by analyzing the effectiveness of these methods\non existing hallucination datasets. To address these limitations, we propose\nFaithJudge, an LLM-as-a-judge approach guided by few-shot human hallucination\nannotations, which substantially improves automated LLM hallucination\nevaluation over current methods. We introduce an enhanced hallucination\nleaderboard centered on FaithJudge, alongside our current hallucination\nleaderboard, enabling more reliable benchmarking of LLMs for hallucinations in\nRAG.", "AI": {"tldr": "This paper addresses hallucinations in LLMs, proposing FaithJudge to improve evaluation methods for LLM-generated summaries.", "motivation": "Hallucinations in LLMs are a significant problem, even when using context to reduce unsupported information and contradictions.", "method": "The paper introduces FaithJudge, an LLM-as-a-judge approach that utilizes few-shot human annotations to enhance LLM hallucination evaluation. It also discusses a comparative analysis of existing hallucination detection methods and the challenges they face.", "result": "FaithJudge significantly improves the automated evaluation of hallucinations in LLMs compared to existing methods, leading to better benchmarking through an enhanced hallucination leaderboard.", "conclusion": "The proposed approach provides a more reliable means of assessing LLM hallucinations, ultimately advancing the state of evaluation in RAG contexts.", "key_contributions": ["Introduction of FaithJudge for improved hallucination evaluation", "Analysis of challenges in current hallucination detection methods", "Development of an enhanced hallucination leaderboard for better benchmarking"], "limitations": "Existing hallucination detection methods still face several challenges that are highlighted in the analysis.", "keywords": ["LLM", "hallucinations", "summarization", "FaithJudge", "RAG"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2505.04916", "pdf": "https://arxiv.org/pdf/2505.04916.pdf", "abs": "https://arxiv.org/abs/2505.04916", "title": "An Open-Source Dual-Loss Embedding Model for Semantic Retrieval in Higher Education", "authors": ["Ramteja Sajja", "Yusuf Sermet", "Ibrahim Demir"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "17 pages, 3 Tables", "summary": "Recent advances in AI have catalyzed the adoption of intelligent educational\ntools, yet many semantic retrieval systems remain ill-suited to the unique\nlinguistic and structural characteristics of academic content. This study\npresents two open-source embedding models fine-tuned for educational question\nanswering, particularly in the context of course syllabi. A synthetic dataset\nof 3,197 sentence pairs, spanning synonymous terminology, paraphrased\nquestions, and implicit-explicit mappings, was constructed through a\ncombination of manual curation and large language model (LLM)-assisted\ngeneration. Two training strategies were evaluated: (1) a baseline model\nfine-tuned using MultipleNegativesRankingLoss (MNRL), and (2) a dual-loss model\nthat combines MNRL with CosineSimilarityLoss to improve both semantic ranking\nand similarity calibration. Evaluations were conducted on 28 university course\nsyllabi using a fixed set of natural language questions categorized into\ncourse, faculty, and teaching assistant information. Results demonstrate that\nboth fine-tuned models outperform strong open-source baselines, including\nall-MiniLM-L6-v2 and multi-qa-MiniLM-L6-cos-v1, and that the dual-loss model\nnarrows the performance gap with high-performing proprietary embeddings such as\nOpenAI's text-embedding-3 series. This work contributes reusable,\ndomain-aligned embedding models and provides a replicable framework for\neducational semantic retrieval, supporting downstream applications such as\nacademic chatbots, retrieval-augmented generation (RAG) systems, and learning\nmanagement system (LMS) integrations.", "AI": {"tldr": "The paper presents two open-source embedding models optimized for educational question answering using a dataset of course syllabi, showing improvements over existing baselines.", "motivation": "To address the inadequacy of current semantic retrieval systems for academic content by providing optimized models for educational contexts.", "method": "Two models were evaluated: one using MultipleNegativesRankingLoss (MNRL) and another that combines MNRL with CosineSimilarityLoss. A synthetic dataset of 3,197 sentence pairs was used for training and evaluation.", "result": "Both models significantly outperformed established open-source models and approached the performance of proprietary embeddings, particularly the dual-loss model.", "conclusion": "The research contributes to enhancing semantic retrieval in education with reusable models and a framework applicable in various academic settings such as chatbots and RAG systems.", "key_contributions": ["Introduction of two fine-tuned embedding models for educational question answering.", "Demonstration of a replicable framework for improving semantic retrieval in academic contexts.", "Creation of a synthetic dataset specifically designed for this purpose."], "limitations": "", "keywords": ["AI in education", "semantic retrieval", "embedding models", "educational tools", "LLM applications"], "importance_score": 7, "read_time_minutes": 20}}
{"id": "2505.04955", "pdf": "https://arxiv.org/pdf/2505.04955.pdf", "abs": "https://arxiv.org/abs/2505.04955", "title": "Chain-of-Thought Tokens are Computer Program Variables", "authors": ["Fangwei Zhu", "Peiyi Wang", "Zhifang Sui"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Chain-of-thoughts (CoT) requires large language models (LLMs) to generate\nintermediate steps before reaching the final answer, and has been proven\neffective to help LLMs solve complex reasoning tasks. However, the inner\nmechanism of CoT still remains largely unclear. In this paper, we empirically\nstudy the role of CoT tokens in LLMs on two compositional tasks: multi-digit\nmultiplication and dynamic programming. While CoT is essential for solving\nthese problems, we find that preserving only tokens that store intermediate\nresults would achieve comparable performance. Furthermore, we observe that\nstoring intermediate results in an alternative latent form will not affect\nmodel performance. We also randomly intervene some values in CoT, and notice\nthat subsequent CoT tokens and the final answer would change correspondingly.\nThese findings suggest that CoT tokens may function like variables in computer\nprograms but with potential drawbacks like unintended shortcuts and\ncomputational complexity limits between tokens. The code and data are available\nat https://github.com/solitaryzero/CoTs_are_Variables.", "AI": {"tldr": "The paper examines the role of chain-of-thought (CoT) tokens in large language models (LLMs) during complex reasoning tasks, revealing that intermediate results can be stored more efficiently while maintaining performance.", "motivation": "To better understand the inner mechanisms of chain-of-thought (CoT) in LLMs when solving complex reasoning tasks.", "method": "Empirical study on two compositional tasks: multi-digit multiplication and dynamic programming, analyzing the function of CoT tokens and their replacements with latent forms.", "result": "The study finds that only preserving intermediate result tokens achieves similar performance as full CoT, and that CoT tokens behave like program variables but can introduce complexities and shortcuts.", "conclusion": "CoT tokens are confirmed to function similarly to variables in programs, potentially leading to computational limitations and unintended shortcuts during problem-solving.", "key_contributions": ["Empirical evidence that CoT tokens can be replaced by intermediate results without performance loss.", "Insights into the behavior of CoT tokens serving as variables akin to programming languages.", "Identification of the implications of unintended shortcuts and computational limits of CoT tokens."], "limitations": "The study is limited to two specific tasks and may not generalize to all reasoning scenarios involving LLMs.", "keywords": ["Chain-of-thought", "Large language models", "Intermediate results", "Variable-like behavior", "Complex reasoning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.04984", "pdf": "https://arxiv.org/pdf/2505.04984.pdf", "abs": "https://arxiv.org/abs/2505.04984", "title": "Rethinking the Relationship between the Power Law and Hierarchical Structures", "authors": ["Kai Nakaishi", "Ryo Yoshida", "Kohei Kajikawa", "Koji Hukushima", "Yohei Oseki"], "categories": ["cs.CL"], "comment": "13 pages, 11 figures", "summary": "Statistical analysis of corpora provides an approach to quantitatively\ninvestigate natural languages. This approach has revealed that several power\nlaws consistently emerge across different corpora and languages, suggesting the\nuniversal principles underlying languages. Particularly, the power-law decay of\ncorrelation has been interpreted as evidence for underlying hierarchical\nstructures in syntax, semantics, and discourse. This perspective has also been\nextended to child languages and animal signals. However, the argument\nsupporting this interpretation has not been empirically tested. To address this\nproblem, this study examines the validity of the argument for syntactic\nstructures. Specifically, we test whether the statistical properties of parse\ntrees align with the implicit assumptions in the argument. Using English\ncorpora, we analyze the mutual information, deviations from probabilistic\ncontext-free grammars (PCFGs), and other properties in parse trees, as well as\nin the PCFG that approximates these trees. Our results indicate that the\nassumptions do not hold for syntactic structures and that it is difficult to\napply the proposed argument to child languages and animal signals, highlighting\nthe need to reconsider the relationship between the power law and hierarchical\nstructures.", "AI": {"tldr": "This study investigates the validity of the argument that power-law decay indicates hierarchical structures in language by analyzing parse trees in English corpora.", "motivation": "To empirically test the argument linking power-law properties with hierarchical structures in language, especially in relation to syntax, semantics, and discourse.", "method": "The study analyzes the mutual information, deviations from probabilistic context-free grammars (PCFGs), and other statistical properties in parse trees using English corpora.", "result": "The results demonstrate that the assumptions linking statistical properties of parse trees with hierarchical structure do not hold, complicating the application of this argument to child languages and animal signals.", "conclusion": "The findings suggest a need to rethink the relationship between power laws and hierarchical structures in linguistic analysis.", "key_contributions": ["Empirical testing of the power-law argument in linguistic structures", "Analysis of parse trees related to power-law decay", "Insights into limitations of hierarchical structure assumptions in child languages and animal signals"], "limitations": "Findings may not generalize to all languages or linguistic features.", "keywords": ["power laws", "syntax", "corpora", "parse trees", "statistical analysis"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2505.04993", "pdf": "https://arxiv.org/pdf/2505.04993.pdf", "abs": "https://arxiv.org/abs/2505.04993", "title": "Latent Preference Coding: Aligning Large Language Models via Discrete Latent Codes", "authors": ["Zhuocheng Gong", "Jian Guan", "Wei Wu", "Huishuai Zhang", "Dongyan Zhao"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have achieved remarkable success, yet aligning\ntheir generations with human preferences remains a critical challenge. Existing\napproaches to preference modeling often rely on an explicit or implicit reward\nfunction, overlooking the intricate and multifaceted nature of human\npreferences that may encompass conflicting factors across diverse tasks and\npopulations. To address this limitation, we introduce Latent Preference Coding\n(LPC), a novel framework that models the implicit factors as well as their\ncombinations behind holistic preferences using discrete latent codes. LPC\nseamlessly integrates with various offline alignment algorithms, automatically\ninferring the underlying factors and their importance from data without relying\non pre-defined reward functions and hand-crafted combination weights. Extensive\nexperiments on multiple benchmarks demonstrate that LPC consistently improves\nupon three alignment algorithms (DPO, SimPO, and IPO) using three base models\n(Mistral-7B, Llama3-8B, and Llama3-8B-Instruct). Furthermore, deeper analysis\nreveals that the learned latent codes effectively capture the differences in\nthe distribution of human preferences and significantly enhance the robustness\nof alignment against noise in data. By providing a unified representation for\nthe multifarious preference factors, LPC paves the way towards developing more\nrobust and versatile alignment techniques for the responsible deployment of\npowerful LLMs.", "AI": {"tldr": "Introduction of Latent Preference Coding (LPC), a framework for aligning large language models with human preferences by modeling implicit factors of preferences using discrete latent codes.", "motivation": "The need to align LLM generations with human preferences in a way that effectively captures the complex and conflicting nature of those preferences.", "method": "LPC models implicit and combined factors of holistic preferences using discrete latent codes and integrates with offline alignment algorithms to infer factors without predefined reward functions.", "result": "LPC demonstrates consistent improvements over three alignment algorithms (DPO, SimPO, IPO) and enhances robustness against noise in data across multiple benchmarks with various base models.", "conclusion": "LPC provides a unified representation of preference factors, paving the way for more robust alignment techniques in LLM deployment.", "key_contributions": ["Introduction of the LPC framework for holistic preference modeling", "Automatic inference of preference factors without predefined functions", "Significant improvements in alignment performance across various models"], "limitations": "", "keywords": ["Large Language Models", "Human Preferences", "Latent Preference Coding", "Alignment Algorithms", "Robustness"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.04994", "pdf": "https://arxiv.org/pdf/2505.04994.pdf", "abs": "https://arxiv.org/abs/2505.04994", "title": "Rethinking Invariance in In-context Learning", "authors": ["Lizhe Fang", "Yifei Wang", "Khashayar Gatmiry", "Lei Fang", "Yisen Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In-Context Learning (ICL) has emerged as a pivotal capability of\nauto-regressive large language models, yet it is hindered by a notable\nsensitivity to the ordering of context examples regardless of their mutual\nindependence. To address this issue, recent studies have introduced several\nvariant algorithms of ICL that achieve permutation invariance. However, many of\nthese do not exhibit comparable performance with the standard auto-regressive\nICL algorithm. In this work, we identify two crucial elements in the design of\nan invariant ICL algorithm: information non-leakage and context\ninterdependence, which are not simultaneously achieved by any of the existing\nmethods. These investigations lead us to the proposed Invariant ICL (InvICL), a\nmethodology designed to achieve invariance in ICL while ensuring the two\nproperties. Empirically, our findings reveal that InvICL surpasses previous\nmodels, both invariant and non-invariant, in most benchmark datasets,\nshowcasing superior generalization capabilities across varying input lengths.\nCode is available at https://github.com/PKU-ML/InvICL.", "AI": {"tldr": "Introduces InvICL, a new invariant ICL algorithm that enhances performance while ensuring information non-leakage and context interdependence.", "motivation": "To overcome the sensitivity of In-Context Learning in large language models to the ordering of context examples while maintaining performance.", "method": "Analyzed the existing invariant ICL algorithms to identify key design elements leading to the proposal of InvICL, which achieves the desired invariance and properties.", "result": "InvICL outperforms previous models on benchmark datasets, demonstrating better generalization across different input lengths.", "conclusion": "The proposed InvICL methodology enhances the effectiveness of ICL in large language models by addressing key design considerations and surpassing previous methods.", "key_contributions": ["Introduction of InvICL for invariant ICL", "Identification of key design elements: information non-leakage and context interdependence", "Empirical validation showing superior generalization capabilities of InvICL"], "limitations": "", "keywords": ["In-Context Learning", "Invariant ICL", "Large Language Models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.05016", "pdf": "https://arxiv.org/pdf/2505.05016.pdf", "abs": "https://arxiv.org/abs/2505.05016", "title": "The Pitfalls of Growing Group Complexity: LLMs and Social Choice-Based Aggregation for Group Recommendations", "authors": ["Cedric Waterschoot", "Nava Tintarev", "Francesco Barile"], "categories": ["cs.CL", "cs.IR"], "comment": "To be published in: Adjunct Proceedings of the 33rd ACM Conference on\n  User Modeling, Adaptation and Personalization (UMAP Adjunct '25), June\n  16--19, 2025, New York City, NY, USA Accepted at the 4th Workshop on Group\n  Modeling, Adaptation and Personalization (GMAP), co-located at UMAP 2025", "summary": "Large Language Models (LLMs) are increasingly applied in recommender systems\naimed at both individuals and groups. Previously, Group Recommender Systems\n(GRS) often used social choice-based aggregation strategies to derive a single\nrecommendation based on the preferences of multiple people. In this paper, we\ninvestigate under which conditions language models can perform these strategies\ncorrectly based on zero-shot learning and analyse whether the formatting of the\ngroup scenario in the prompt affects accuracy. We specifically focused on the\nimpact of group complexity (number of users and items), different LLMs,\ndifferent prompting conditions, including In-Context learning or generating\nexplanations, and the formatting of group preferences. Our results show that\nperformance starts to deteriorate when considering more than 100 ratings.\nHowever, not all language models were equally sensitive to growing group\ncomplexity. Additionally, we showed that In-Context Learning (ICL) can\nsignificantly increase the performance at higher degrees of group complexity,\nwhile adding other prompt modifications, specifying domain cues or prompting\nfor explanations, did not impact accuracy. We conclude that future research\nshould include group complexity as a factor in GRS evaluation due to its effect\non LLM performance. Furthermore, we showed that formatting the group scenarios\ndifferently, such as rating lists per user or per item, affected accuracy. All\nin all, our study implies that smaller LLMs are capable of generating group\nrecommendations under the right conditions, making the case for using smaller\nmodels that require less computing power and costs.", "AI": {"tldr": "This paper investigates the performance of Language Models in Group Recommender Systems (GRS) using zero-shot learning, analyzing how group complexity and prompting formats affect accuracy.", "motivation": "To explore how Language Models can effectively produce recommendations for groups using social choice-based aggregation strategies under varying conditions.", "method": "The study examines the impact of group complexity, different LLMs, and various prompting conditions such as In-Context Learning and formatting preferences on the accuracy of group recommendations.", "result": "Performance declines with more than 100 ratings, but smaller LLMs show capability in generating group recommendations under optimal conditions.", "conclusion": "Future GRS evaluations should consider group complexity, as it significantly affects LLM performance, and different formatting of group scenarios can influence accuracy.", "key_contributions": ["Demonstrated the impact of group complexity on LLM performance in GRS.", "Showed that In-Context Learning enhances accuracy at higher group complexities.", "Proved that different formatting of group preferences affects outcome accuracy."], "limitations": "The findings are based on specific LLMs and may not generalize to all models; further research needed.", "keywords": ["Group Recommender Systems", "Large Language Models", "Zero-Shot Learning", "In-Context Learning", "Group Complexity"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.05017", "pdf": "https://arxiv.org/pdf/2505.05017.pdf", "abs": "https://arxiv.org/abs/2505.05017", "title": "Scalable Multi-Stage Influence Function for Large Language Models via Eigenvalue-Corrected Kronecker-Factored Parameterization", "authors": ["Yuntai Bao", "Xuhong Zhang", "Tianyu Du", "Xinkui Zhao", "Jiang Zong", "Hao Peng", "Jianwei Yin"], "categories": ["cs.CL"], "comment": "9 pages, accepted by IJCAI 2025", "summary": "Pre-trained large language models (LLMs) are commonly fine-tuned to adapt to\ndownstream tasks. Since the majority of knowledge is acquired during\npre-training, attributing the predictions of fine-tuned LLMs to their\npre-training data may provide valuable insights. Influence functions have been\nproposed as a means to explain model predictions based on training data.\nHowever, existing approaches fail to compute ``multi-stage'' influence and lack\nscalability to billion-scale LLMs.\n  In this paper, we propose the multi-stage influence function to attribute the\ndownstream predictions of fine-tuned LLMs to pre-training data under the\nfull-parameter fine-tuning paradigm. To enhance the efficiency and practicality\nof our multi-stage influence function, we leverage Eigenvalue-corrected\nKronecker-Factored (EK-FAC) parameterization for efficient approximation.\nEmpirical results validate the superior scalability of EK-FAC approximation and\nthe effectiveness of our multi-stage influence function. Additionally, case\nstudies on a real-world LLM, dolly-v2-3b, demonstrate its interpretive power,\nwith exemplars illustrating insights provided by multi-stage influence\nestimates. Our code is public at\nhttps://github.com/colored-dye/multi_stage_influence_function.", "AI": {"tldr": "This paper introduces a multi-stage influence function for attributing predictions of fine-tuned large language models (LLMs) to their pre-training data, addressing scalability issues with existing methods.", "motivation": "Understanding how fine-tuned LLMs make predictions based on their pre-training data can provide insights into their functioning, particularly in the context of downstream tasks.", "method": "The authors propose a multi-stage influence function that utilizes Eigenvalue-corrected Kronecker-Factored (EK-FAC) parameterization for effective and scalable computation of influence on million-scale LLMs.", "result": "Empirical results demonstrate the scalability of the EK-FAC approximation and the effectiveness of the proposed multi-stage influence function in interpreting fine-tuned LLM predictions.", "conclusion": "The proposed method offers improved interpretability of LLMs, making it a valuable tool for understanding their decision-making process based on pre-training data.", "key_contributions": ["Introduction of a multi-stage influence function for LLMs", "Application of EK-FAC parameterization for efficient scalability", "Real-world case studies illustrating interpretive capability of the model"], "limitations": "", "keywords": ["Large Language Models", "Influence Functions", "Machine Learning", "Interpretability", "Pre-training"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.05026", "pdf": "https://arxiv.org/pdf/2505.05026.pdf", "abs": "https://arxiv.org/abs/2505.05026", "title": "G-FOCUS: Towards a Robust Method for Assessing UI Design Persuasiveness", "authors": ["Jaehyun Jeon", "Janghan Yoon", "Minsoo Kim", "Sumin Shim", "Yejin Choi", "Hanbin Kim", "Youngjae Yu"], "categories": ["cs.CL", "cs.LG"], "comment": "31 pages, 17 figures", "summary": "Evaluating user interface (UI) design effectiveness extends beyond aesthetics\nto influencing user behavior, a principle central to Design Persuasiveness. A/B\ntesting is the predominant method for determining which UI variations drive\nhigher user engagement, but it is costly and time-consuming. While recent\nVision-Language Models (VLMs) can process automated UI analysis, current\napproaches focus on isolated design attributes rather than comparative\npersuasiveness-the key factor in optimizing user interactions. To address this,\nwe introduce WiserUI-Bench, a benchmark designed for Pairwise UI Design\nPersuasiveness Assessment task, featuring 300 real-world UI image pairs labeled\nwith A/B test results and expert rationales. Additionally, we propose G-FOCUS,\na novel inference-time reasoning strategy that enhances VLM-based\npersuasiveness assessment by reducing position bias and improving evaluation\naccuracy. Experimental results show that G-FOCUS surpasses existing inference\nstrategies in consistency and accuracy for pairwise UI evaluation. Through\npromoting VLM-driven evaluation of UI persuasiveness, our work offers an\napproach to complement A/B testing, propelling progress in scalable UI\npreference modeling and design optimization. Code and data will be released\npublicly.", "AI": {"tldr": "This paper introduces WiserUI-Bench, a benchmark for evaluating UI design persuasiveness, and G-FOCUS, a strategy for enhancing VLM assessment accuracy.", "motivation": "To improve upon traditional A/B testing methods in evaluating UI design effectiveness by focusing on comparative persuasiveness.", "method": "The study presents WiserUI-Bench, which consists of 300 pairs of real-world UI images evaluated based on A/B test results and expert rationales. G-FOCUS is proposed as a reasoning strategy to enhance VLM-based assessments.", "result": "G-FOCUS demonstrates superior consistency and accuracy when compared to existing inference strategies in evaluating pairwise UI designs.", "conclusion": "The proposed methods provide a scalable alternative to traditional A/B testing for UI design optimization, fostering the use of VLMs in this domain.", "key_contributions": ["Introduction of WiserUI-Bench for UI design persuasiveness assessment", "Development of G-FOCUS for improved VLM evaluation accuracy", "Provision of a publicly available dataset and code for further research"], "limitations": "The effectiveness in real-world applications outside the benchmark is yet to be tested thoroughly.", "keywords": ["UI Design", "Persuasiveness", "Pairwise Evaluation", "Vision-Language Models", "Benchmark"], "importance_score": 8, "read_time_minutes": 31}}
{"id": "2505.05040", "pdf": "https://arxiv.org/pdf/2505.05040.pdf", "abs": "https://arxiv.org/abs/2505.05040", "title": "Image-Text Relation Prediction for Multilingual Tweets", "authors": ["Matīss Rikters", "Edison Marrese-Taylor"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Various social networks have been allowing media uploads for over a decade\nnow. Still, it has not always been clear what is their relation with the posted\ntext or even if there is any at all. In this work, we explore how multilingual\nvision-language models tackle the task of image-text relation prediction in\ndifferent languages, and construct a dedicated balanced benchmark data set from\nTwitter posts in Latvian along with their manual translations into English. We\ncompare our results to previous work and show that the more recently released\nvision-language model checkpoints are becoming increasingly capable at this\ntask, but there is still much room for further improvement.", "AI": {"tldr": "Exploration of image-text relations in multilingual contexts using vision-language models, focusing on a dataset from Twitter posts in Latvian and their English translations.", "motivation": "To investigate the relationship between posted media and text in social networks, particularly for multilingual content.", "method": "Constructed a balanced benchmark dataset from Twitter posts in Latvian, with manual translations to English, and evaluated performance using recent vision-language model checkpoints.", "result": "Recent vision-language model checkpoints show improved capabilities in image-text relation prediction, though further enhancements are necessary.", "conclusion": "While advancements in model performance have been noted, significant opportunities for improvements remain in the accuracy of image-text relation predictions.", "key_contributions": ["Construction of a unique benchmark dataset from Latvian Twitter posts", "Evaluation of multilingual vision-language models in predicting image-text relations", "Analysis of model performance trends over time"], "limitations": "", "keywords": ["multilingual models", "image-text relation", "vision-language models", "Twitter dataset", "Latvian"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.05056", "pdf": "https://arxiv.org/pdf/2505.05056.pdf", "abs": "https://arxiv.org/abs/2505.05056", "title": "Teochew-Wild: The First In-the-wild Teochew Dataset with Orthographic Annotations", "authors": ["Linrong Pan", "Chenglong Jiang", "Gaoze Hou", "Ying Gao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper reports the construction of the Teochew-Wild, a speech corpus of\nthe Teochew dialect. The corpus includes 18.9 hours of in-the-wild Teochew\nspeech data from multiple speakers, covering both formal and colloquial\nexpressions, with precise orthographic and pinyin annotations. Additionally, we\nprovide supplementary text processing tools and resources to propel research\nand applications in speech tasks for this low-resource language, such as\nautomatic speech recognition (ASR) and text-to-speech (TTS). To the best of our\nknowledge, this is the first publicly available Teochew dataset with accurate\northographic annotations. We conduct experiments on the corpus, and the results\nvalidate its effectiveness in ASR and TTS tasks.", "AI": {"tldr": "This paper introduces the Teochew-Wild corpus, a unique speech dataset for the Teochew dialect, aimed at enhancing speech technology applications.", "motivation": "To support research and applications in speech tasks for the Teochew dialect, a low-resource language, by providing a comprehensive annotated corpus.", "method": "The construction of the Teochew-Wild corpus, which includes 18.9 hours of speech data from multiple speakers, complemented by accurate orthographic and pinyin annotations, along with text processing tools.", "result": "Experiments show the corpus's effectiveness in automatic speech recognition (ASR) and text-to-speech (TTS) tasks, highlighting its utility in speech technology.", "conclusion": "The Teochew-Wild corpus is the first publicly available dataset for the Teochew dialect with accurate annotations, facilitating future research in this area.", "key_contributions": ["First publicly available Teochew speech corpus with accurate orthographic annotations.", "Includes both formal and colloquial expressions from multiple speakers.", "Provides text processing tools for research and applications."], "limitations": "", "keywords": ["Teochew dialect", "speech corpus", "automatic speech recognition", "text-to-speech", "low-resource language"], "importance_score": 2, "read_time_minutes": 5}}
{"id": "2505.05070", "pdf": "https://arxiv.org/pdf/2505.05070.pdf", "abs": "https://arxiv.org/abs/2505.05070", "title": "Performance Evaluation of Large Language Models in Bangla Consumer Health Query Summarization", "authors": ["Ajwad Abrar", "Farzana Tabassum", "Sabbir Ahmed"], "categories": ["cs.CL"], "comment": null, "summary": "Consumer Health Queries (CHQs) in Bengali (Bangla), a low-resource language,\noften contain extraneous details, complicating efficient medical responses.\nThis study investigates the zero-shot performance of nine advanced large\nlanguage models (LLMs): GPT-3.5-Turbo, GPT-4, Claude-3.5-Sonnet,\nLlama3-70b-Instruct, Mixtral-8x22b-Instruct, Gemini-1.5-Pro,\nQwen2-72b-Instruct, Gemma-2-27b, and Athene-70B, in summarizing Bangla CHQs.\nUsing the BanglaCHQ-Summ dataset comprising 2,350 annotated query-summary\npairs, we benchmarked these LLMs using ROUGE metrics against Bangla T5, a\nfine-tuned state-of-the-art model. Mixtral-8x22b-Instruct emerged as the top\nperforming model in ROUGE-1 and ROUGE-L, while Bangla T5 excelled in ROUGE-2.\nThe results demonstrate that zero-shot LLMs can rival fine-tuned models,\nachieving high-quality summaries even without task-specific training. This work\nunderscores the potential of LLMs in addressing challenges in low-resource\nlanguages, providing scalable solutions for healthcare query summarization.", "AI": {"tldr": "This study assesses the performance of nine large language models in summarizing consumer health queries in Bengali, highlighting zero-shot capabilities and the effectiveness of LLMs compared to fine-tuned models.", "motivation": "To address the challenge of efficiently responding to Consumer Health Queries (CHQs) in Bengali, a low-resource language, by exploring the summarization capabilities of advanced LLMs.", "method": "Evaluated the zero-shot performance of nine LLMs using the BanglaCHQ-Summ dataset of 2,350 annotated query-summary pairs, benchmarking them with ROUGE metrics against Bangla T5.", "result": "Mixtral-8x22b-Instruct was the top performer in ROUGE-1 and ROUGE-L, while Bangla T5 excelled in ROUGE-2, indicating that zero-shot LLMs can produce high-quality summaries in low-resource languages.", "conclusion": "Zero-shot LLMs can effectively summarize health queries in Bengali, indicating their potential for scalable solutions in healthcare contexts despite the lack of task-specific training.", "key_contributions": ["Demonstration of zero-shot summarization capabilities of LLMs for low-resource languages.", "Benchmarking multiple advanced LLMs against a fine-tuned model in the healthcare domain.", "Insight into the comparative effectiveness of different LLMs in generating query summaries."], "limitations": "The study focuses solely on Bengali CHQs and may not generalize to other low-resource languages.", "keywords": ["Large Language Models", "Bengali", "Consumer Health Queries", "Summarization", "Zero-shot Learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.05084", "pdf": "https://arxiv.org/pdf/2505.05084.pdf", "abs": "https://arxiv.org/abs/2505.05084", "title": "Reliably Bounding False Positives: A Zero-Shot Machine-Generated Text Detection Framework via Multiscaled Conformal Prediction", "authors": ["Xiaowei Zhu", "Yubing Ren", "Yanan Cao", "Xixun Lin", "Fang Fang", "Yangxi Li"], "categories": ["cs.CL"], "comment": null, "summary": "The rapid advancement of large language models has raised significant\nconcerns regarding their potential misuse by malicious actors. As a result,\ndeveloping effective detectors to mitigate these risks has become a critical\npriority. However, most existing detection methods focus excessively on\ndetection accuracy, often neglecting the societal risks posed by high false\npositive rates (FPRs). This paper addresses this issue by leveraging Conformal\nPrediction (CP), which effectively constrains the upper bound of FPRs. While\ndirectly applying CP constrains FPRs, it also leads to a significant reduction\nin detection performance. To overcome this trade-off, this paper proposes a\nZero-Shot Machine-Generated Text Detection Framework via Multiscaled Conformal\nPrediction (MCP), which both enforces the FPR constraint and improves detection\nperformance. This paper also introduces RealDet, a high-quality dataset that\nspans a wide range of domains, ensuring realistic calibration and enabling\nsuperior detection performance when combined with MCP. Empirical evaluations\ndemonstrate that MCP effectively constrains FPRs, significantly enhances\ndetection performance, and increases robustness against adversarial attacks\nacross multiple detectors and datasets.", "AI": {"tldr": "This paper introduces a framework called MCP that utilizes Conformal Prediction to reduce false positive rates in language model detection while improving performance.", "motivation": "The paper addresses the concern of misuse of large language models and the need for effective detection methods that balance accuracy and societal risks associated with false positive rates.", "method": "The authors propose a Zero-Shot Machine-Generated Text Detection Framework incorporating Multiscaled Conformal Prediction (MCP) to manage false positive rates while enhancing detection performance.", "result": "Empirical evaluations show that MCP significantly reduces false positive rates and improves detection performance, while being robust against adversarial attacks across various detectors and datasets.", "conclusion": "The proposed MCP framework successfully addresses the trade-off between false positive rates and detection performance, providing a promising solution for detecting machine-generated text.", "key_contributions": ["Introduction of a new detection framework (MCP) that balances FPR constraints and performance", "Development of RealDet, a high-quality dataset for calibration", "Empirical evidence demonstrating enhanced robustness against adversarial attacks."], "limitations": "", "keywords": ["Conformal Prediction", "False Positive Rate", "Machine Learning", "Text Detection", "Adversarial Robustness"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.05111", "pdf": "https://arxiv.org/pdf/2505.05111.pdf", "abs": "https://arxiv.org/abs/2505.05111", "title": "Unveiling Language-Specific Features in Large Language Models via Sparse Autoencoders", "authors": ["Boyi Deng", "Yu Wan", "Yidan Zhang", "Baosong Yang", "Fuli Feng"], "categories": ["cs.CL"], "comment": null, "summary": "The mechanisms behind multilingual capabilities in Large Language Models\n(LLMs) have been examined using neuron-based or internal-activation-based\nmethods. However, these methods often face challenges such as superposition and\nlayer-wise activation variance, which limit their reliability. Sparse\nAutoencoders (SAEs) offer a more nuanced analysis by decomposing the\nactivations of LLMs into sparse linear combination of SAE features. We\nintroduce a novel metric to assess the monolinguality of features obtained from\nSAEs, discovering that some features are strongly related to specific\nlanguages. Additionally, we show that ablating these SAE features only\nsignificantly reduces abilities in one language of LLMs, leaving others almost\nunaffected. Interestingly, we find some languages have multiple synergistic SAE\nfeatures, and ablating them together yields greater improvement than ablating\nindividually. Moreover, we leverage these SAE-derived language-specific\nfeatures to enhance steering vectors, achieving control over the language\ngenerated by LLMs.", "AI": {"tldr": "This paper explores the use of Sparse Autoencoders (SAEs) to analyze multilingual capabilities in Large Language Models, finding language-specific features and their effects on model performance.", "motivation": "To address the limitations of neuron-based or internal-activation-based methods in analyzing multilingual capabilities of Large Language Models.", "method": "The paper employs Sparse Autoencoders (SAEs) to decompose LLM activations into sparse linear combinations of features for a nuanced analysis of monolinguality and to enhance steering vectors for language control.", "result": "SAEs provide a novel metric to assess feature monolinguality, revealing that some features strongly correlate with specific languages and that their ablation impacts performance variably across languages.", "conclusion": "The findings suggest that leveraging SAE-derived language-specific features can achieve improved control over generated language in LLMs, suggesting a new direction for enhancing multilingual interaction in AI systems.", "key_contributions": ["Introduction of a novel metric for assessing monolinguality of SAE features in LLMs", "Discovery of language-specific features from SAEs that impact model performance", "Demonstration of improved steering capabilities for LLMs through SAE-derived features."], "limitations": "The analysis may be limited to the specific language features derived from SAEs and their interaction effects in LLMs.", "keywords": ["Large Language Models", "Sparse Autoencoders", "multilingual capabilities", "language control", "feature analysis"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.05148", "pdf": "https://arxiv.org/pdf/2505.05148.pdf", "abs": "https://arxiv.org/abs/2505.05148", "title": "A Benchmark Dataset and a Framework for Urdu Multimodal Named Entity Recognition", "authors": ["Hussain Ahmad", "Qingyang Zeng", "Jing Wan"], "categories": ["cs.CL"], "comment": "16 pages, 5 figures. Preprint", "summary": "The emergence of multimodal content, particularly text and images on social\nmedia, has positioned Multimodal Named Entity Recognition (MNER) as an\nincreasingly important area of research within Natural Language Processing.\nDespite progress in high-resource languages such as English, MNER remains\nunderexplored for low-resource languages like Urdu. The primary challenges\ninclude the scarcity of annotated multimodal datasets and the lack of\nstandardized baselines. To address these challenges, we introduce the U-MNER\nframework and release the Twitter2015-Urdu dataset, a pioneering resource for\nUrdu MNER. Adapted from the widely used Twitter2015 dataset, it is annotated\nwith Urdu-specific grammar rules. We establish benchmark baselines by\nevaluating both text-based and multimodal models on this dataset, providing\ncomparative analyses to support future research on Urdu MNER. The U-MNER\nframework integrates textual and visual context using Urdu-BERT for text\nembeddings and ResNet for visual feature extraction, with a Cross-Modal Fusion\nModule to align and fuse information. Our model achieves state-of-the-art\nperformance on the Twitter2015-Urdu dataset, laying the groundwork for further\nMNER research in low-resource languages.", "AI": {"tldr": "The paper introduces the U-MNER framework and the Twitter2015-Urdu dataset to advance Multimodal Named Entity Recognition (MNER) for Urdu, addressing challenges in low-resource languages.", "motivation": "MNER is crucial due to the rise of multimodal content on social media, but research is lacking for low-resource languages like Urdu.", "method": "The authors developed the U-MNER framework which uses Urdu-BERT for text embeddings and ResNet for visual feature extraction, integrating these through a Cross-Modal Fusion Module.", "result": "The framework achieved state-of-the-art performance on the newly created Twitter2015-Urdu dataset, establishing benchmark baselines for future research.", "conclusion": "The research lays a foundation for advancing MNER in low-resource languages, highlighting the importance of multimodal datasets.", "key_contributions": ["Introduction of the U-MNER framework for Urdu MNER", "Release of the Twitter2015-Urdu dataset as a resource for further research", "Establishment of benchmark baselines using text and multimodal models"], "limitations": "", "keywords": ["Multimodal Named Entity Recognition", "MNER", "Urdu", "Natural Language Processing", "Low-resource languages"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.05225", "pdf": "https://arxiv.org/pdf/2505.05225.pdf", "abs": "https://arxiv.org/abs/2505.05225", "title": "QualBench: Benchmarking Chinese LLMs with Localized Professional Qualifications for Vertical Domain Evaluation", "authors": ["Mengze Hong", "Wailing Ng", "Di Jiang", "Chen Jason Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "The rapid advancement of Chinese large language models (LLMs) underscores the\nneed for domain-specific evaluations to ensure reliable applications. However,\nexisting benchmarks often lack coverage in vertical domains and offer limited\ninsights into the Chinese working context. Leveraging qualification exams as a\nunified framework for human expertise evaluation, we introduce QualBench, the\nfirst multi-domain Chinese QA benchmark dedicated to localized assessment of\nChinese LLMs. The dataset includes over 17,000 questions across six vertical\ndomains, with data selections grounded in 24 Chinese qualifications to closely\nalign with national policies and working standards. Through comprehensive\nevaluation, the Qwen2.5 model outperformed the more advanced GPT-4o, with\nChinese LLMs consistently surpassing non-Chinese models, highlighting the\nimportance of localized domain knowledge in meeting qualification requirements.\nThe best performance of 75.26% reveals the current gaps in domain coverage\nwithin model capabilities. Furthermore, we present the failure of LLM\ncollaboration with crowdsourcing mechanisms and suggest the opportunities for\nmulti-domain RAG knowledge enhancement and vertical domain LLM training with\nFederated Learning.", "AI": {"tldr": "Introduction of QualBench, a multi-domain Chinese QA benchmark for evaluating localized large language models.", "motivation": "To address the lack of effective benchmarks for assessing Chinese large language models in specific domains, especially related to qualifications and working standards.", "method": "Development of QualBench, a dataset consisting of over 17,000 questions across six vertical domains, focused on aligning with Chinese national policies and qualifications. Evaluation of the Qwen2.5 model against GPT-4o.", "result": "The Qwen2.5 model outperformed GPT-4o with a top performance of 75.26%, indicating that Chinese LLMs are superior to non-Chinese models when trained on localized content.", "conclusion": "The findings highlight the need for improved domain coverage in model training and suggest opportunities for enhancing knowledge through Federated Learning and multi-domain RAG techniques.", "key_contributions": ["Introduction of the first multi-domain Chinese QA benchmark, QualBench.", "Evaluation results showcasing the superiority of localized Chinese LLMs over non-Chinese models.", "Identification of gaps in domain coverage and collaboration failures with crowdsourcing.", "Proposal for utilizing Federated Learning for enhanced training of vertical domain LLMs."], "limitations": "Limitations regarding the collaboration of LLMs with crowdsourcing mechanisms, which may impact data quality and model performance.", "keywords": ["Chinese large language models", "QA benchmark", "domain-specific evaluations", "localized assessment", "Federated Learning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.05271", "pdf": "https://arxiv.org/pdf/2505.05271.pdf", "abs": "https://arxiv.org/abs/2505.05271", "title": "T-T: Table Transformer for Tagging-based Aspect Sentiment Triplet Extraction", "authors": ["Kun Peng", "Chaodong Tong", "Cong Cao", "Hao Peng", "Qian Li", "Guanlin Wu", "Lei Jiang", "Yanbing Liu", "Philip S. Yu"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by IJCAI2025", "summary": "Aspect sentiment triplet extraction (ASTE) aims to extract triplets composed\nof aspect terms, opinion terms, and sentiment polarities from given sentences.\nThe table tagging method is a popular approach to addressing this task, which\nencodes a sentence into a 2-dimensional table, allowing for the tagging of\nrelations between any two words. Previous efforts have focused on designing\nvarious downstream relation learning modules to better capture interactions\nbetween tokens in the table, revealing that a stronger capability to capture\nrelations can lead to greater improvements in the model. Motivated by this, we\nattempt to directly utilize transformer layers as downstream relation learning\nmodules. Due to the powerful semantic modeling capability of transformers, it\nis foreseeable that this will lead to excellent improvement. However, owing to\nthe quadratic relation between the length of the table and the length of the\ninput sentence sequence, using transformers directly faces two challenges:\noverly long table sequences and unfair local attention interaction. To address\nthese challenges, we propose a novel Table-Transformer (T-T) for the\ntagging-based ASTE method. Specifically, we introduce a stripe attention\nmechanism with a loop-shift strategy to tackle these challenges. The former\nmodifies the global attention mechanism to only attend to a 2-dimensional local\nattention window, while the latter facilitates interaction between different\nattention windows. Extensive and comprehensive experiments demonstrate that the\nT-T, as a downstream relation learning module, achieves state-of-the-art\nperformance with lower computational costs.", "AI": {"tldr": "This paper presents a novel Table-Transformer (T-T) model for aspect sentiment triplet extraction, utilizing transformer layers as relation learning modules and introducing new mechanisms to improve computational efficiency and model performance.", "motivation": "To enhance the performance of aspect sentiment triplet extraction using the strong relational capabilities of transformers while addressing challenges in computational efficiency and attention mechanisms.", "method": "The study introduces a Table-Transformer (T-T) model with a stripe attention mechanism and loop-shift strategy to optimize the processing of long input sequences in a tagging-based ASTE approach.", "result": "The proposed T-T model achieves state-of-the-art performance in aspect sentiment triplet extraction with reduced computational costs compared to previous methods.", "conclusion": "The Table-Transformer effectively utilizes transformer layers for downstream relation learning, significantly improving the tagging-based ASTE method.", "key_contributions": ["Introduction of a Table-Transformer model for ASTE", "Development of stripe attention mechanism", "Implementation of loop-shift strategy for attention interaction"], "limitations": "Potential scalability issues with extremely long input sentences beyond practical limits.", "keywords": ["aspect sentiment extraction", "transformer", "stripe attention", "relation learning", "computational efficiency"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.05298", "pdf": "https://arxiv.org/pdf/2505.05298.pdf", "abs": "https://arxiv.org/abs/2505.05298", "title": "Toward Reasonable Parrots: Why Large Language Models Should Argue with Us by Design", "authors": ["Elena Musi", "Nadin Kokciyan", "Khalid Al-Khatib", "Davide Ceolin", "Emmanuelle Dietz", "Klara Gutekunst", "Annette Hautli-Janisz", "Cristian Manuel Santibañez Yañez", "Jodi Schneider", "Jonas Scholz", "Cor Steging", "Jacky Visser", "Henning Wachsmuth"], "categories": ["cs.CL", "cs.MA"], "comment": null, "summary": "In this position paper, we advocate for the development of conversational\ntechnology that is inherently designed to support and facilitate argumentative\nprocesses. We argue that, at present, large language models (LLMs) are\ninadequate for this purpose, and we propose an ideal technology design aimed at\nenhancing argumentative skills. This involves re-framing LLMs as tools to\nexercise our critical thinking rather than replacing them. We introduce the\nconcept of 'reasonable parrots' that embody the fundamental principles of\nrelevance, responsibility, and freedom, and that interact through argumentative\ndialogical moves. These principles and moves arise out of millennia of work in\nargumentation theory and should serve as the starting point for LLM-based\ntechnology that incorporates basic principles of argumentation.", "AI": {"tldr": "The paper advocates for the development of conversational technology to enhance argumentative processes leveraging LLMs, proposing a redesign centered on supportive tools for critical thinking.", "motivation": "To address the inadequacies of current LLMs in supporting argumentative processes and to propose a new design that facilitates critical thinking.", "method": "Proposes re-framing LLMs as 'reasonable parrots' that follow principles of relevance, responsibility, and freedom in argumentative dialogues.", "result": "Presents a conceptual framework for LLM-based technology that incorporates fundamental principles from argumentation theory.", "conclusion": "LLM-based technologies should focus on enhancing human argumentation skills rather than replacing human reasoning.", "key_contributions": ["Proposed redesign of LLMs to support critical thinking", "Introduction of 'reasonable parrots' concept", "Integration of argumentation theory into LLM technology"], "limitations": "", "keywords": ["Conversational Technology", "Argumentation Theory", "Large Language Models"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.05327", "pdf": "https://arxiv.org/pdf/2505.05327.pdf", "abs": "https://arxiv.org/abs/2505.05327", "title": "ICon: In-Context Contribution for Automatic Data Selection", "authors": ["Yixin Yang", "Qingxiu Dong", "Linli Yao", "Fangwei Zhu", "Zhifang Sui"], "categories": ["cs.CL"], "comment": null, "summary": "Data selection for instruction tuning is essential for improving the\nperformance of Large Language Models (LLMs) and reducing training cost.\nHowever, existing automated selection methods either depend on computationally\nexpensive gradient-based measures or manually designed heuristics, which may\nfail to fully exploit the intrinsic attributes of data. In this paper, we\npropose In-context Learning for Contribution Measurement (ICon), a novel\ngradient-free method that takes advantage of the implicit fine-tuning nature of\nin-context learning (ICL) to measure sample contribution without gradient\ncomputation or manual indicators engineering. ICon offers a computationally\nefficient alternative to gradient-based methods and reduces human inductive\nbias inherent in heuristic-based approaches. ICon comprises three components\nand identifies high-contribution data by assessing performance shifts under\nimplicit learning through ICL. Extensive experiments on three LLMs across 12\nbenchmarks and 5 pairwise evaluation sets demonstrate the effectiveness of\nICon. Remarkably, on LLaMA3.1-8B, models trained on 15% of ICon-selected data\noutperform full datasets by 5.42% points and exceed the best performance of\nwidely used selection methods by 2.06% points. We further analyze\nhigh-contribution samples selected by ICon, which show both diverse tasks and\nappropriate difficulty levels, rather than just the hardest ones.", "AI": {"tldr": "ICon is a gradient-free method for data selection in instruction tuning of LLMs, enhancing performance and efficiency without human bias.", "motivation": "The need for efficient data selection methods for instruction tuning in LLMs that avoid gradient-based costs and manual heuristics.", "method": "ICon measures sample contribution through implicit learning in in-context learning, avoiding the need for gradients or heuristics, and identifies high-contribution data based on performance shifts.", "result": "Models trained on ICon-selected data outperform those trained on full datasets significantly, achieving better performance with just 15% of the data.", "conclusion": "ICon demonstrates superior data selection efficiency, identifying diverse and appropriately challenging samples that improve LLM performance without manual bias.", "key_contributions": ["Introduction of ICon, a novel method for data selection", "Demonstration of superior performance with reduced dataset sizes", "Analysis of selected samples showing diversity and appropriate difficulty levels"], "limitations": "", "keywords": ["Large Language Models", "Data Selection", "In-context Learning", "Instruction Tuning", "Performance Improvement"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.05406", "pdf": "https://arxiv.org/pdf/2505.05406.pdf", "abs": "https://arxiv.org/abs/2505.05406", "title": "Frame In, Frame Out: Do LLMs Generate More Biased News Headlines than Humans?", "authors": ["Valeria Pastorino", "Nafise Sadat Moosavi"], "categories": ["cs.CL"], "comment": null, "summary": "Framing in media critically shapes public perception by selectively\nemphasizing some details while downplaying others. With the rise of large\nlanguage models in automated news and content creation, there is growing\nconcern that these systems may introduce or even amplify framing biases\ncompared to human authors. In this paper, we explore how framing manifests in\nboth out-of-the-box and fine-tuned LLM-generated news content. Our analysis\nreveals that, particularly in politically and socially sensitive contexts, LLMs\ntend to exhibit more pronounced framing than their human counterparts. In\naddition, we observe significant variation in framing tendencies across\ndifferent model architectures, with some models displaying notably higher\nbiases. These findings point to the need for effective post-training mitigation\nstrategies and tighter evaluation frameworks to ensure that automated news\ncontent upholds the standards of balanced reporting.", "AI": {"tldr": "This paper investigates framing biases in news content generated by large language models (LLMs) compared to human authors, revealing LLMs often exhibit more pronounced biases, particularly in sensitive contexts.", "motivation": "The study is motivated by concerns that LLMs may introduce or amplify framing biases in automated content creation, influencing public perception.", "method": "The authors analyze out-of-the-box and fine-tuned LLM-generated news articles to assess the degree of framing bias compared to human-generated content.", "result": "The research finds that LLM-generated news content exhibits stronger framing biases, especially in politically and socially sensitive issues, with significant variations across different model architectures.", "conclusion": "The results highlight the importance of implementing post-training strategies and better evaluation frameworks to ensure balanced reporting in automated news content.", "key_contributions": ["Demonstrates the presence of framing biases in LLM-generated news content.", "Compares framing tendencies between LLMs and human authors.", "Identifies architectural variations in framing biases across different LLMs."], "limitations": "The study primarily focuses on specific model architectures and may not account for all possible configurations and contexts.", "keywords": ["framing", "large language models", "news content", "bias", "automated journalism"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.05408", "pdf": "https://arxiv.org/pdf/2505.05408.pdf", "abs": "https://arxiv.org/abs/2505.05408", "title": "Crosslingual Reasoning through Test-Time Scaling", "authors": ["Zheng-Xin Yong", "M. Farid Adilazuarda", "Jonibek Mansurov", "Ruochen Zhang", "Niklas Muennighoff", "Carsten Eickhoff", "Genta Indra Winata", "Julia Kreutzer", "Stephen H. Bach", "Alham Fikri Aji"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Reasoning capabilities of large language models are primarily studied for\nEnglish, even when pretrained models are multilingual. In this work, we\ninvestigate to what extent English reasoning finetuning with long\nchain-of-thoughts (CoTs) can generalize across languages. First, we find that\nscaling up inference compute for English-centric reasoning language models\n(RLMs) improves multilingual mathematical reasoning across many languages\nincluding low-resource languages, to an extent where they outperform models\ntwice their size. Second, we reveal that while English-centric RLM's CoTs are\nnaturally predominantly English, they consistently follow a quote-and-think\npattern to reason about quoted non-English inputs. Third, we discover an\neffective strategy to control the language of long CoT reasoning, and we\nobserve that models reason better and more efficiently in high-resource\nlanguages. Finally, we observe poor out-of-domain reasoning generalization, in\nparticular from STEM to cultural commonsense knowledge, even for English.\nOverall, we demonstrate the potentials, study the mechanisms and outline the\nlimitations of crosslingual generalization of English reasoning test-time\nscaling. We conclude that practitioners should let English-centric RLMs reason\nin high-resource languages, while further work is needed to improve reasoning\nin low-resource languages and out-of-domain contexts.", "AI": {"tldr": "This paper investigates the multilingual reasoning capabilities of English-centric reasoning language models (RLMs) and their performance in mathematical reasoning across different languages.", "motivation": "Explore the generalization of English reasoning finetuning with chain-of-thoughts (CoTs) across various languages, especially highlighting the potential of RLMs in multilingual contexts.", "method": "The study examines the scaling of inference compute for English-focused RLMs and assesses their performance in multilingual reasoning tasks.", "result": "Improved multilingual mathematical reasoning was observed, with English-centric RLMs outperforming larger models in low-resource languages. CoTs predominantly in English follow a quote-and-think pattern with non-English inputs.", "conclusion": "While there are strengths in reasoning with high-resource languages, challenges remain in low-resource languages and out-of-domain reasoning, indicating a need for further research in these areas.", "key_contributions": ["Improved multilingual reasoning capabilities of English-centric RLMs", "Established effective strategies for controlling the language of CoT reasoning", "Identified limitations in out-of-domain reasoning generalization"], "limitations": "Weak generalization from STEM to cultural commonsense knowledge for English models and challenges in lower-resource languages.", "keywords": ["multilingual reasoning", "large language models", "chain-of-thoughts"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.05410", "pdf": "https://arxiv.org/pdf/2505.05410.pdf", "abs": "https://arxiv.org/abs/2505.05410", "title": "Reasoning Models Don't Always Say What They Think", "authors": ["Yanda Chen", "Joe Benton", "Ansh Radhakrishnan", "Jonathan Uesato", "Carson Denison", "John Schulman", "Arushi Somani", "Peter Hase", "Misha Wagner", "Fabien Roger", "Vlad Mikulik", "Samuel R. Bowman", "Jan Leike", "Jared Kaplan", "Ethan Perez"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Chain-of-thought (CoT) offers a potential boon for AI safety as it allows\nmonitoring a model's CoT to try to understand its intentions and reasoning\nprocesses. However, the effectiveness of such monitoring hinges on CoTs\nfaithfully representing models' actual reasoning processes. We evaluate CoT\nfaithfulness of state-of-the-art reasoning models across 6 reasoning hints\npresented in the prompts and find: (1) for most settings and models tested,\nCoTs reveal their usage of hints in at least 1% of examples where they use the\nhint, but the reveal rate is often below 20%, (2) outcome-based reinforcement\nlearning initially improves faithfulness but plateaus without saturating, and\n(3) when reinforcement learning increases how frequently hints are used (reward\nhacking), the propensity to verbalize them does not increase, even without\ntraining against a CoT monitor. These results suggest that CoT monitoring is a\npromising way of noticing undesired behaviors during training and evaluations,\nbut that it is not sufficient to rule them out. They also suggest that in\nsettings like ours where CoT reasoning is not necessary, test-time monitoring\nof CoTs is unlikely to reliably catch rare and catastrophic unexpected\nbehaviors.", "AI": {"tldr": "This paper evaluates the faithfulness of Chain-of-Thought (CoT) reasoning in AI models, finding that while CoTs can indicate model behaviors, their effectiveness is limited and can lead to reward hacking without capturing all unexpected behaviors.", "motivation": "The paper investigates how Chain-of-Thought (CoT) reasoning can help in monitoring AI models to understand their intentions and to promote AI safety.", "method": "The authors evaluate the faithfulness of CoTs across six reasoning hints in prompts by analyzing various state-of-the-art models and their response patterns in relation to hint utilization and reinforcement learning methods.", "result": "Most models tested show that CoTs reveal hint usage in at least 1% of examples, but the rate is often below 20%. Reinforcement learning improves faithfulness initially but plateaus, and increases in hint utilization do not correspond with higher verbalization rates.", "conclusion": "CoT monitoring is a promising tool for detecting undesired behaviors in models, but it is insufficient to fully understand or eliminate such behaviors, especially when CoT reasoning is not fundamental.", "key_contributions": ["Evaluation of CoT faithfulness across multiple models", "Insights into the effectiveness of reinforcement learning on CoT reasoning", "Identification of limitations in using CoTs for reliable behavior monitoring"], "limitations": "CoTs may not capture rare and catastrophic behaviors effectively, particularly in settings where CoT reasoning is not central.", "keywords": ["Chain-of-Thought", "AI safety", "reasoning models", "monitoring", "reinforcement learning"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.05423", "pdf": "https://arxiv.org/pdf/2505.05423.pdf", "abs": "https://arxiv.org/abs/2505.05423", "title": "TransProQA: an LLM-based literary Translation evaluation metric with Professional Question Answering", "authors": ["Ran Zhang", "Wei Zhao", "Lieve Macken", "Steffen Eger"], "categories": ["cs.CL", "cs.AI"], "comment": "WIP", "summary": "The impact of Large Language Models (LLMs) has extended into literary\ndomains. However, existing evaluation metrics prioritize mechanical accuracy\nover artistic expression and tend to overrate machine translation (MT) as being\nsuperior to experienced professional human translation. In the long run, this\nbias could result in a permanent decline in translation quality and cultural\nauthenticity. In response to the urgent need for a specialized literary\nevaluation metric, we introduce TransProQA, a novel, reference-free, LLM-based\nquestion-answering (QA) framework designed specifically for literary\ntranslation evaluation. TransProQA uniquely integrates insights from\nprofessional literary translators and researchers, focusing on critical\nelements in literary quality assessment such as literary devices, cultural\nunderstanding, and authorial voice. Our extensive evaluation shows that while\nliterary-finetuned XCOMET-XL yields marginal gains, TransProQA substantially\noutperforms current metrics, achieving up to 0.07 gain in correlation (ACC-EQ\nand Kendall's tau) and surpassing the best state-of-the-art (SOTA) metrics by\nover 15 points in adequacy assessments. Incorporating professional translator\ninsights as weights further improves performance, highlighting the value of\ntranslator inputs. Notably, TransProQA approaches human-level evaluation\nperformance comparable to trained linguistic annotators. It demonstrates broad\napplicability to open-source models such as LLaMA3.3-70b and Qwen2.5-32b,\nindicating its potential as an accessible and training-free literary evaluation\nmetric and a valuable tool for evaluating texts that require local processing\ndue to copyright or ethical considerations.", "AI": {"tldr": "TransProQA is a novel LLM-based framework for evaluating literary translations that prioritizes artistic expression over mechanical accuracy.", "motivation": "To address the inadequacy of existing evaluation metrics that emphasize mechanical accuracy and potentially harm translation quality and cultural authenticity.", "method": "TransProQA integrates insights from professional literary translators and researchers, focusing on literary quality assessment through question-answering techniques.", "result": "TransProQA significantly outperforms existing metrics, achieving a correlation gain of up to 0.07 and exceeding state-of-the-art metrics by over 15 points in adequacy assessments.", "conclusion": "TransProQA approaches human-level evaluation performance and has broad applicability to open-source models, making it a valuable tool for literary text evaluation without requiring training.", "key_contributions": ["Introduction of a reference-free, LLM-based evaluation metric for literary translation.", "Significant performance improvements over current state-of-the-art metrics.", "Integration of professional translator insights to enhance evaluation outcomes."], "limitations": "", "keywords": ["large language models", "literary translation", "evaluation metrics", "human-computer interaction", "artistic expression"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.05427", "pdf": "https://arxiv.org/pdf/2505.05427.pdf", "abs": "https://arxiv.org/abs/2505.05427", "title": "Ultra-FineWeb: Efficient Data Filtering and Verification for High-Quality LLM Training Data", "authors": ["Yudong Wang", "Zixuan Fu", "Jie Cai", "Peijun Tang", "Hongya Lyu", "Yewei Fang", "Zhi Zheng", "Jie Zhou", "Guoyang Zeng", "Chaojun Xiao", "Xu Han", "Zhiyuan Liu"], "categories": ["cs.CL"], "comment": "The datasets are available on\n  https://huggingface.co/datasets/openbmb/UltraFineWeb", "summary": "Data quality has become a key factor in enhancing model performance with the\nrapid development of large language models (LLMs). Model-driven data filtering\nhas increasingly become a primary approach for acquiring high-quality data.\nHowever, it still faces two main challenges: (1) the lack of an efficient data\nverification strategy makes it difficult to provide timely feedback on data\nquality; and (2) the selection of seed data for training classifiers lacks\nclear criteria and relies heavily on human expertise, introducing a degree of\nsubjectivity. To address the first challenge, we introduce an efficient\nverification strategy that enables rapid evaluation of the impact of data on\nLLM training with minimal computational cost. To tackle the second challenge,\nwe build upon the assumption that high-quality seed data is beneficial for LLM\ntraining, and by integrating the proposed verification strategy, we optimize\nthe selection of positive and negative samples and propose an efficient data\nfiltering pipeline. This pipeline not only improves filtering efficiency,\nclassifier quality, and robustness, but also significantly reduces experimental\nand inference costs. In addition, to efficiently filter high-quality data, we\nemploy a lightweight classifier based on fastText, and successfully apply the\nfiltering pipeline to two widely-used pre-training corpora, FineWeb and Chinese\nFineWeb datasets, resulting in the creation of the higher-quality Ultra-FineWeb\ndataset. Ultra-FineWeb contains approximately 1 trillion English tokens and 120\nbillion Chinese tokens. Empirical results demonstrate that the LLMs trained on\nUltra-FineWeb exhibit significant performance improvements across multiple\nbenchmark tasks, validating the effectiveness of our pipeline in enhancing both\ndata quality and training efficiency.", "AI": {"tldr": "Introducing a strategy for efficient data verification and filtering to enhance the quality of datasets used for training large language models (LLMs).", "motivation": "To improve model performance of LLMs by addressing challenges in data quality and seed data selection.", "method": "An efficient verification strategy for rapid evaluation of data impact on LLM training, and a data filtering pipeline optimizing the selection of positive and negative samples using a lightweight fastText classifier.", "result": "The pipeline was applied to FineWeb and Chinese FineWeb datasets, creating the Ultra-FineWeb dataset with approximately 1 trillion English tokens and 120 billion Chinese tokens, leading to significant performance improvements in LLMs across benchmark tasks.", "conclusion": "The proposed methods effectively enhance data quality and training efficiency for large language models.", "key_contributions": ["Development of an efficient verification strategy for data quality evaluation", "Creation of the Ultra-FineWeb dataset", "Optimized data filtering pipeline integrating seed data selection with verification."], "limitations": "", "keywords": ["data quality", "large language models", "data filtering", "fastText", "Ultra-FineWeb"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.05445", "pdf": "https://arxiv.org/pdf/2505.05445.pdf", "abs": "https://arxiv.org/abs/2505.05445", "title": "clem:todd: A Framework for the Systematic Benchmarking of LLM-Based Task-Oriented Dialogue System Realisations", "authors": ["Chalamalasetti Kranti", "Sherzod Hakimov", "David Schlangen"], "categories": ["cs.CL"], "comment": "30 pages", "summary": "The emergence of instruction-tuned large language models (LLMs) has advanced\nthe field of dialogue systems, enabling both realistic user simulations and\nrobust multi-turn conversational agents. However, existing research often\nevaluates these components in isolation-either focusing on a single user\nsimulator or a specific system design-limiting the generalisability of insights\nacross architectures and configurations. In this work, we propose clem todd\n(chat-optimized LLMs for task-oriented dialogue systems development), a\nflexible framework for systematically evaluating dialogue systems under\nconsistent conditions. clem todd enables detailed benchmarking across\ncombinations of user simulators and dialogue systems, whether existing models\nfrom literature or newly developed ones. It supports plug-and-play integration\nand ensures uniform datasets, evaluation metrics, and computational\nconstraints. We showcase clem todd's flexibility by re-evaluating existing\ntask-oriented dialogue systems within this unified setup and integrating three\nnewly proposed dialogue systems into the same evaluation pipeline. Our results\nprovide actionable insights into how architecture, scale, and prompting\nstrategies affect dialogue performance, offering practical guidance for\nbuilding efficient and effective conversational AI systems.", "AI": {"tldr": "This paper introduces clem todd, a framework for evaluating dialogue systems systematically to improve benchmarking and insights across architectures.", "motivation": "The motivation is to address the isolation of evaluations in existing research on dialogue systems, which limits the generalizability of findings across different architectures.", "method": "The authors propose clem todd as a flexible framework for systematic evaluation of dialogue systems, allowing for detailed benchmarking across various user simulators and dialogue systems, with uniform datasets and evaluation metrics.", "result": "The results show that using clem todd provides actionable insights into the impact of architecture, scale, and prompting strategies on the performance of dialogue systems.", "conclusion": "The framework enables improved evaluation and development of conversational AI systems by standardizing conditions for assessments.", "key_contributions": ["Introduction of clem todd for systematic evaluation of dialogue systems.", "Benchmarking of multiple user simulators and dialogue systems under consistent conditions.", "Re-evaluation of existing systems and integration of new ones within the same framework."], "limitations": "", "keywords": ["dialogue systems", "benchmarking", "large language models", "conversational AI", "user simulation"], "importance_score": 8, "read_time_minutes": 30}}
{"id": "2505.05459", "pdf": "https://arxiv.org/pdf/2505.05459.pdf", "abs": "https://arxiv.org/abs/2505.05459", "title": "UKElectionNarratives: A Dataset of Misleading Narratives Surrounding Recent UK General Elections", "authors": ["Fatima Haouari", "Carolina Scarton", "Nicolò Faggiani", "Nikolaos Nikolaidis", "Bonka Kotseva", "Ibrahim Abu Farha", "Jens Linge", "Kalina Bontcheva"], "categories": ["cs.CL", "cs.SI"], "comment": "This work was accepted at the International AAAI Conference on Web\n  and Social Media (ICWSM 2025)", "summary": "Misleading narratives play a crucial role in shaping public opinion during\nelections, as they can influence how voters perceive candidates and political\nparties. This entails the need to detect these narratives accurately. To\naddress this, we introduce the first taxonomy of common misleading narratives\nthat circulated during recent elections in Europe. Based on this taxonomy, we\nconstruct and analyse UKElectionNarratives: the first dataset of\nhuman-annotated misleading narratives which circulated during the UK General\nElections in 2019 and 2024. We also benchmark Pre-trained and Large Language\nModels (focusing on GPT-4o), studying their effectiveness in detecting\nelection-related misleading narratives. Finally, we discuss potential use cases\nand make recommendations for future research directions using the proposed\ncodebook and dataset.", "AI": {"tldr": "The paper introduces a taxonomy of misleading narratives in elections and presents a dataset for detecting them using Large Language Models, focusing on the UK General Elections.", "motivation": "The role of misleading narratives in shaping public opinion during elections necessitates accurate detection of these narratives.", "method": "The authors created a taxonomy of misleading narratives and constructed the UKElectionNarratives dataset, which is human-annotated for election-related misleading narratives. They benchmarked the effectiveness of pre-trained and large language models, specifically GPT-4o, in detecting these narratives.", "result": "The study showcases the first dataset of misleading narratives from the UK General Elections and evaluates various models' capabilities to detect these narratives effectively.", "conclusion": "The findings highlight the importance of automated detection of misleading narratives, suggesting further research directions and potential applications of the dataset.", "key_contributions": ["First taxonomy of misleading narratives in elections", "Construction of the UKElectionNarratives dataset", "Benchmarking of GPT-4o and other models for narrative detection"], "limitations": "", "keywords": ["misleading narratives", "elections", "large language models", "UK General Elections", "dataset"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.05464", "pdf": "https://arxiv.org/pdf/2505.05464.pdf", "abs": "https://arxiv.org/abs/2505.05464", "title": "Bring Reason to Vision: Understanding Perception and Reasoning through Model Merging", "authors": ["Shiqi Chen", "Jinghan Zhang", "Tongyao Zhu", "Wei Liu", "Siyang Gao", "Miao Xiong", "Manling Li", "Junxian He"], "categories": ["cs.CL"], "comment": "ICML 2025. Our code is publicly available at\n  https://github.com/shiqichen17/VLM_Merging", "summary": "Vision-Language Models (VLMs) combine visual perception with the general\ncapabilities, such as reasoning, of Large Language Models (LLMs). However, the\nmechanisms by which these two abilities can be combined and contribute remain\npoorly understood. In this work, we explore to compose perception and reasoning\nthrough model merging that connects parameters of different models. Unlike\nprevious works that often focus on merging models of the same kind, we propose\nmerging models across modalities, enabling the incorporation of the reasoning\ncapabilities of LLMs into VLMs. Through extensive experiments, we demonstrate\nthat model merging offers a successful pathway to transfer reasoning abilities\nfrom LLMs to VLMs in a training-free manner. Moreover, we utilize the merged\nmodels to understand the internal mechanism of perception and reasoning and how\nmerging affects it. We find that perception capabilities are predominantly\nencoded in the early layers of the model, whereas reasoning is largely\nfacilitated by the middle-to-late layers. After merging, we observe that all\nlayers begin to contribute to reasoning, whereas the distribution of perception\nabilities across layers remains largely unchanged. These observations shed\nlight on the potential of model merging as a tool for multimodal integration\nand interpretation.", "AI": {"tldr": "This paper explores merging Vision-Language Models (VLMs) and Large Language Models (LLMs) to enhance multimodal integration and understanding of perception and reasoning capabilities.", "motivation": "To better understand how to integrate visual and reasoning capabilities in AI models, specifically through the mechanism of model merging across different modalities.", "method": "The authors propose a novel approach to merge parameters of VLMs and LLMs, allowing for the transfer of reasoning capabilities from LLMs to VLMs without direct training.", "result": "Through experiments, the study shows that model merging enables the early layers to maintain perception abilities while enhancing reasoning across all layers of the merged model.", "conclusion": "Model merging represents a promising approach for improving multimodal capabilities and understanding the integration of perception and reasoning in AI models.", "key_contributions": ["Proposes a novel merging approach across modalities (VLM and LLM)", "Demonstrates successful transfer of reasoning capabilities in a training-free manner", "Offers insights into the layer-wise contribution to perception and reasoning abilities after merging."], "limitations": "", "keywords": ["Vision-Language Models", "Large Language Models", "Model Merging", "Multimodal Integration", "Perception and Reasoning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.05465", "pdf": "https://arxiv.org/pdf/2505.05465.pdf", "abs": "https://arxiv.org/abs/2505.05465", "title": "ComPO: Preference Alignment via Comparison Oracles", "authors": ["Peter Chen", "Xi Chen", "Wotao Yin", "Tianyi Lin"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "25 pages", "summary": "Direct alignment methods are increasingly used for aligning large language\nmodels (LLMs) with human preferences. However, these methods suffer from the\nissues of verbosity and likelihood displacement, which can be driven by the\nnoisy preference pairs that induce similar likelihood for preferred and\ndispreferred responses. The contributions of this paper are two-fold. First, we\npropose a new preference alignment method based on comparison oracles and\nprovide the convergence guarantee for its basic scheme. Second, we improve our\nmethod using some heuristics and conduct the experiments to demonstrate the\nflexibility and compatibility of practical scheme in improving the performance\nof LLMs using noisy preference pairs. Evaluations are conducted across multiple\nbase and instruction-tuned models (Mistral-7B, Llama-3-8B and Gemma-2-9B) with\nbenchmarks (AlpacaEval 2, MT-Bench and Arena-Hard). Experimental results show\nthe effectiveness of our method as an alternative to addressing the limitations\nof existing direct alignment methods. A highlight of our work is that we\nevidence the importance of designing specialized methods for preference pairs\nwith distinct likelihood margin, which complements the recent findings in\n\\citet{Razin-2025-Unintentional}.", "AI": {"tldr": "This paper proposes a new preference alignment method for large language models that addresses issues of verbosity and likelihood displacement in direct alignment methods.", "motivation": "Direct alignment methods for aligning large language models with human preferences face challenges due to noisy preference pairs, leading to verbosity and likelihood displacement.", "method": "Introduction of a new preference alignment method based on comparison oracles with a convergence guarantee; enhancement of the method using heuristics and experimental validation on various LLMs.", "result": "Experimental evaluations on models like Mistral-7B, Llama-3-8B, and Gemma-2-9B demonstrate the effectiveness of the proposed method in improving model performance with noisy preference pairs.", "conclusion": "The study highlights the necessity of specialized methods for preference pairs with distinct likelihood margins, suggesting an alternative approach to existing direct alignment techniques.", "key_contributions": ["Proposed a new preference alignment method using comparison oracles", "Provided a convergence guarantee for the proposed method", "Demonstrated improved performance of LLMs with noisy preference pairs through experiments"], "limitations": "", "keywords": ["preference alignment", "large language models", "human preferences"], "importance_score": 8, "read_time_minutes": 25}}
{"id": "2306.10512", "pdf": "https://arxiv.org/pdf/2306.10512.pdf", "abs": "https://arxiv.org/abs/2306.10512", "title": "Position: AI Evaluation Should Learn from How We Test Humans", "authors": ["Yan Zhuang", "Qi Liu", "Zachary A. Pardos", "Patrick C. Kyllonen", "Jiyun Zu", "Zhenya Huang", "Shijin Wang", "Enhong Chen"], "categories": ["cs.CL"], "comment": "Accepted by ICML 2025", "summary": "As AI systems continue to evolve, their rigorous evaluation becomes crucial\nfor their development and deployment. Researchers have constructed various\nlarge-scale benchmarks to determine their capabilities, typically against a\ngold-standard test set and report metrics averaged across all items. However,\nthis static evaluation paradigm increasingly shows its limitations, including\nhigh evaluation costs, data contamination, and the impact of low-quality or\nerroneous items on evaluation reliability and efficiency. In this Position,\ndrawing from human psychometrics, we discuss a paradigm shift from static\nevaluation methods to adaptive testing. This involves estimating the\ncharacteristics or value of each test item in the benchmark, and tailoring each\nmodel's evaluation instead of relying on a fixed test set. This paradigm\nprovides robust ability estimation, uncovering the latent traits underlying a\nmodel's observed scores. This position paper analyze the current possibilities,\nprospects, and reasons for adopting psychometrics in AI evaluation. We argue\nthat psychometrics, a theory originating in the 20th century for human\nassessment, could be a powerful solution to the challenges in today's AI\nevaluations.", "AI": {"tldr": "This paper advocates for a shift from traditional static evaluation methods in AI to adaptive testing influenced by psychometrics, highlighting the benefits of individualized model assessment.", "motivation": "To address the limitations of static evaluation methods in AI, such as high costs and low-quality data impact.", "method": "The authors propose a new evaluation paradigm that tailors model assessments based on the characteristics of each test item in the benchmark.", "result": "Adopting psychometrics can provide more reliable and efficient evaluations by uncovering latent traits in model performance.", "conclusion": "Psychometrics offers a promising framework for enhancing the evaluation of AI systems, ensuring more accurate assessments than traditional methods.", "key_contributions": ["Introduction of adaptive testing in AI evaluation", "Application of psychometric principles to model assessment", "Identification of challenges in current AI evaluation frameworks"], "limitations": "", "keywords": ["AI evaluation", "psychometrics", "adaptive testing", "model assessment", "benchmarking"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2405.13325", "pdf": "https://arxiv.org/pdf/2405.13325.pdf", "abs": "https://arxiv.org/abs/2405.13325", "title": "DEGAP: Dual Event-Guided Adaptive Prefixes for Templated-Based Event Argument Extraction with Slot Querying", "authors": ["Guanghui Wang", "Dexi Liu", "Jian-Yun Nie", "Qizhi Wan", "Rong Hu", "Xiping Liu", "Wanlong Liu", "Jiaming Liu"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Published as a conference paper in COLING 2025", "summary": "Recent advancements in event argument extraction (EAE) involve incorporating\nuseful auxiliary information into models during training and inference, such as\nretrieved instances and event templates. These methods face two challenges: (1)\nthe retrieval results may be irrelevant and (2) templates are developed\nindependently for each event without considering their possible relationship.\nIn this work, we propose DEGAP to address these challenges through a simple yet\neffective components: dual prefixes, i.e. learnable prompt vectors, where the\ninstance-oriented prefix and template-oriented prefix are trained to learn\ninformation from different event instances and templates. Additionally, we\npropose an event-guided adaptive gating mechanism, which can adaptively\nleverage possible connections between different events and thus capture\nrelevant information from the prefix. Finally, these event-guided prefixes\nprovide relevant information as cues to EAE model without retrieval. Extensive\nexperiments demonstrate that our method achieves new state-of-the-art\nperformance on four datasets (ACE05, RAMS, WIKIEVENTS, and MLEE). Further\nanalysis shows the impact of different components.", "AI": {"tldr": "The paper presents DEGAP, a novel approach to event argument extraction (EAE) that uses learnable prompts and an adaptive gating mechanism to improve the relevance of auxiliary information and relationships between events.", "motivation": "To enhance event argument extraction by addressing the challenges of irrelevant retrieval results and independent event template development.", "method": "DEGAP uses dual prefixes—instance-oriented and template-oriented—to learn relevant information and employs an adaptive gating mechanism to leverage connections between different events.", "result": "The method achieves state-of-the-art performance on four datasets: ACE05, RAMS, WIKIEVENTS, and MLEE.", "conclusion": "DEGAP effectively captures and utilizes relevant information for EAE, outperforming previous methods and providing insights into the utility of its components.", "key_contributions": ["Introduction of dual prefixes for different event instances and templates", "Development of an event-guided adaptive gating mechanism", "Demonstration of state-of-the-art performance on multiple datasets."], "limitations": "", "keywords": ["event argument extraction", "deep learning", "natural language processing"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2406.17746", "pdf": "https://arxiv.org/pdf/2406.17746.pdf", "abs": "https://arxiv.org/abs/2406.17746", "title": "Recite, Reconstruct, Recollect: Memorization in LMs as a Multifaceted Phenomenon", "authors": ["USVSN Sai Prashanth", "Alvin Deng", "Kyle O'Brien", "Jyothir S V", "Mohammad Aflah Khan", "Jaydeep Borkar", "Christopher A. Choquette-Choo", "Jacob Ray Fuehne", "Stella Biderman", "Tracy Ke", "Katherine Lee", "Naomi Saphra"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Memorization in language models is typically treated as a homogenous\nphenomenon, neglecting the specifics of the memorized data. We instead model\nmemorization as the effect of a set of complex factors that describe each\nsample and relate it to the model and corpus. To build intuition around these\nfactors, we break memorization down into a taxonomy: recitation of highly\nduplicated sequences, reconstruction of inherently predictable sequences, and\nrecollection of sequences that are neither. We demonstrate the usefulness of\nour taxonomy by using it to construct a predictive model for memorization. By\nanalyzing dependencies and inspecting the weights of the predictive model, we\nfind that different factors influence the likelihood of memorization\ndifferently depending on the taxonomic category.", "AI": {"tldr": "The paper proposes a taxonomy for understanding memorization in language models, breaking it down into different types based on the nature of the memorized data.", "motivation": "To address the oversimplification in the study of memorization in language models by considering specific factors that influence memorization.", "method": "The authors develop a taxonomy categorizing memorization into three types: recitation of duplicated sequences, reconstruction of predictable sequences, and recollection of sequences that are neither. They construct a predictive model based on these categories.", "result": "The study finds that various factors affecting memorization operate differently across the taxonomic categories, which aids in understanding the memorization behavior of language models.", "conclusion": "The proposed taxonomy and predictive model provide valuable insights into the complex nature of memorization in language models, enhancing our understanding of their behavior.", "key_contributions": ["Introduction of a taxonomy for types of memorization in language models", "Development of a predictive model for memorization based on the taxonomy", "Analysis of dependencies that influence memorization likelihood across categories"], "limitations": "", "keywords": ["language models", "memorization", "taxonomy", "predictive model", "machine learning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2406.20094", "pdf": "https://arxiv.org/pdf/2406.20094.pdf", "abs": "https://arxiv.org/abs/2406.20094", "title": "Scaling Synthetic Data Creation with 1,000,000,000 Personas", "authors": ["Tao Ge", "Xin Chan", "Xiaoyang Wang", "Dian Yu", "Haitao Mi", "Dong Yu"], "categories": ["cs.CL", "cs.LG"], "comment": "Work in progress", "summary": "We propose a novel persona-driven data synthesis methodology that leverages\nvarious perspectives within a large language model (LLM) to create diverse\nsynthetic data. To fully exploit this methodology at scale, we introduce\nPersona Hub -- a collection of 1 billion diverse personas automatically curated\nfrom web data. These 1 billion personas (~13% of the world's total population),\nacting as distributed carriers of world knowledge, can tap into almost every\nperspective encapsulated within the LLM, thereby facilitating the creation of\ndiverse synthetic data at scale for various scenarios. By showcasing Persona\nHub's use cases in synthesizing high-quality mathematical and logical reasoning\nproblems, instructions (i.e., user prompts), knowledge-rich texts, game NPCs\nand tools (functions) at scale, we demonstrate persona-driven data synthesis is\nversatile, scalable, flexible, and easy to use, potentially driving a paradigm\nshift in synthetic data creation and applications in practice, which may have a\nprofound impact on LLM research and development.", "AI": {"tldr": "The paper introduces a persona-driven data synthesis methodology utilizing a large language model (LLM) to create diverse synthetic data, supported by a collection of 1 billion diverse personas.", "motivation": "To create diverse synthetic data using various perspectives within an LLM and to facilitate practical applications of this data synthesis methodology.", "method": "The methodology involves leveraging a Persona Hub, which contains 1 billion personas curated from web data, to generate high-quality synthetic data across various scenarios.", "result": "Demonstrates the versatility and scalability of persona-driven data synthesis through use cases including mathematical problem generation, instruction synthesis, knowledge-rich text creation, and game NPC development.", "conclusion": "Persona-driven data synthesis may drive significant advancements in the field of synthetic data creation and has implications for LLM research and development.", "key_contributions": ["Introduction of Persona Hub with 1 billion diverse personas.", "Demonstration of diverse applications for generating synthetic data.", "Showcasing the flexible and scalable nature of the proposed methodology."], "limitations": "Work in progress; further validation of the synthesis methodology is needed.", "keywords": ["data synthesis", "large language models", "synthetic data", "diverse personas", "HCI"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2409.11055", "pdf": "https://arxiv.org/pdf/2409.11055.pdf", "abs": "https://arxiv.org/abs/2409.11055", "title": "Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and Model Size in Large Language Models From Edge to Giant", "authors": ["Jemin Lee", "Sihyeong Park", "Jinse Kwon", "Jihun Oh", "Yongin Kwon"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted in IJCAI 2025, 21 pages, 2 figure", "summary": "Quantization has gained attention as a promising solution for the\ncost-effective deployment of large and small language models. However, most\nprior work has been limited to perplexity or basic knowledge tasks and lacks a\ncomprehensive evaluation of recent models like Llama-3.3. In this paper, we\nconduct a comprehensive evaluation of instruction-tuned models spanning 1B to\n405B parameters, applying four quantization methods across 13 datasets. Our\nfindings reveal that (1) quantized models generally surpass smaller FP16\nbaselines, yet they often struggle with instruction-following and hallucination\ndetection; (2) FP8 consistently emerges as the most robust option across tasks,\nand AWQ tends to outperform GPTQ in weight-only quantization; (3) smaller\nmodels can suffer severe accuracy drops at 4-bit quantization, while 70B-scale\nmodels maintain stable performance; (4) notably, \\textit{hard} tasks do not\nalways experience the largest accuracy losses, indicating that quantization\nmagnifies a model's inherent weaknesses rather than simply correlating with\ntask difficulty; and (5) an LLM-based judge (MT-Bench) highlights significant\nperformance declines in coding and STEM tasks, though reasoning may sometimes\nimprove.", "AI": {"tldr": "This paper evaluates the performance of quantized language models, particularly in instruction-following and hallucination detection, across various tasks and datasets.", "motivation": "To address the lack of comprehensive evaluations on quantization methods for large language models, particularly recent models like Llama-3.3.", "method": "A comprehensive evaluation of instruction-tuned models ranging from 1B to 405B parameters using four quantization methods across 13 datasets.", "result": "Quantized models generally perform better than smaller FP16 baselines; FP8 is the most robust quantization method; accuracy drops for smaller models at 4-bit quantization; and performance declines noted in coding and STEM tasks while reasoning might improve.", "conclusion": "Quantization enhances performance but exposes intrinsic model weaknesses that are not solely related to task difficulty.", "key_contributions": ["Comprehensive evaluation of quantization methods on recent LLMs", "Identification of FP8 as the most robust quantization method", "Insights into performance variations across model sizes and tasks."], "limitations": "Does not address the underlying algorithmic designs for quantization; focuses primarily on performance metrics.", "keywords": ["quantization", "language models", "instruction-tuning", "performance evaluation", "AI applications"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2409.12183", "pdf": "https://arxiv.org/pdf/2409.12183.pdf", "abs": "https://arxiv.org/abs/2409.12183", "title": "To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning", "authors": ["Zayne Sprague", "Fangcong Yin", "Juan Diego Rodriguez", "Dongwei Jiang", "Manya Wadhwa", "Prasann Singhal", "Xinyu Zhao", "Xi Ye", "Kyle Mahowald", "Greg Durrett"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Published at ICLR 2025", "summary": "Chain-of-thought (CoT) via prompting is the de facto method for eliciting\nreasoning capabilities from large language models (LLMs). But for what kinds of\ntasks is this extra ``thinking'' really helpful? To analyze this, we conducted\na quantitative meta-analysis covering over 100 papers using CoT and ran our own\nevaluations of 20 datasets across 14 models. Our results show that CoT gives\nstrong performance benefits primarily on tasks involving math or logic, with\nmuch smaller gains on other types of tasks. On MMLU, directly generating the\nanswer without CoT leads to almost identical accuracy as CoT unless the\nquestion or model's response contains an equals sign, indicating symbolic\noperations and reasoning. Following this finding, we analyze the behavior of\nCoT on these problems by separating planning and execution and comparing\nagainst tool-augmented LLMs. Much of CoT's gain comes from improving symbolic\nexecution, but it underperforms relative to using a symbolic solver. Our\nresults indicate that CoT can be applied selectively, maintaining performance\nwhile saving inference costs. Furthermore, they suggest a need to move beyond\nprompt-based CoT to new paradigms that better leverage intermediate computation\nacross the whole range of LLM applications.", "AI": {"tldr": "The paper analyzes the effectiveness of chain-of-thought (CoT) prompting in large language models, emphasizing its strong performance in math and logic tasks through a meta-analysis of over 100 papers and evaluations across 20 datasets.", "motivation": "To investigate the effectiveness and applicability of chain-of-thought (CoT) prompting in various tasks involving large language models (LLMs).", "method": "Conducted a quantitative meta-analysis of over 100 papers on CoT, followed by evaluations on 20 datasets across 14 LLMs to assess performance differences with and without CoT prompting.", "result": "CoT provides significant performance improvements mainly for math and logic tasks, with minor gains in other areas. In many cases, directly generating answers yields similar accuracy to CoT, except in cases involving symbolic reasoning.", "conclusion": "CoT can be selectively applied to improve efficiency and maintain performance but underperforms compared to symbolic solvers. The study suggests a need for new paradigms beyond prompt-based CoT.", "key_contributions": ["Analyses the effectiveness of CoT prompting across various tasks.", "Finds strong performance benefits primarily in mathematical and logical contexts.", "Suggests moving towards new paradigms for LLM applications."], "limitations": "The study primarily focuses on CoT in relation to math and logic tasks, which may limit the generalizability of findings to other task domains.", "keywords": ["Chain-of-thought", "large language models", "meta-analysis", "symbolic reasoning", "prompting"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2410.04055", "pdf": "https://arxiv.org/pdf/2410.04055.pdf", "abs": "https://arxiv.org/abs/2410.04055", "title": "Self-Correction is More than Refinement: A Learning Framework for Visual and Language Reasoning Tasks", "authors": ["Jiayi He", "Hehai Lin", "Qingyun Wang", "Yi Fung", "Heng Ji"], "categories": ["cs.CL"], "comment": null, "summary": "While Vision-Language Models (VLMs) have shown remarkable abilities in visual\nand language reasoning tasks, they invariably generate flawed responses.\nSelf-correction that instructs models to refine their outputs presents a\npromising solution to this issue. Previous studies have mainly concentrated on\nLarge Language Models (LLMs), while the self-correction abilities of VLMs,\nparticularly concerning both visual and linguistic information, remain largely\nunexamined. This study investigates the self-correction capabilities of VLMs\nduring both inference and fine-tuning stages. We introduce a Self-Correction\nLearning (SCL) approach that enables VLMs to learn from their self-generated\nself-correction data through Direct Preference Optimization (DPO) without\nrelying on external feedback, facilitating self-improvement. Specifically, we\ncollect preferred and disfavored samples based on the correctness of initial\nand refined responses, which are obtained by two-turn self-correction with VLMs\nduring the inference stage. Experimental results demonstrate that although VLMs\nstruggle to self-correct effectively during iterative inference without\nadditional fine-tuning and external feedback, they can enhance their\nperformance and avoid previous mistakes through preference fine-tuning when\ntheir self-generated self-correction data are categorized into preferred and\ndisfavored samples. This study emphasizes that self-correction is not merely a\nrefinement process; rather, it should enhance the reasoning abilities of models\nthrough additional training, enabling them to generate high-quality responses\ndirectly without further refinement.", "AI": {"tldr": "This study introduces a Self-Correction Learning (SCL) approach to enhance Vision-Language Models (VLMs) by enabling them to improve their output quality through self-generated data during inference and fine-tuning stages.", "motivation": "To address the flawed responses generated by Vision-Language Models (VLMs) and investigate their self-correction capabilities, which have been underexplored compared to Large Language Models.", "method": "The study proposes a Self-Correction Learning (SCL) approach that allows VLMs to utilize self-generated self-correction data through Direct Preference Optimization (DPO) without needing external feedback.", "result": "Experimental results indicate that while VLMs struggle with self-correction during inference without fine-tuning, they can improve performance significantly through preference fine-tuning when supported by categorized self-correction data.", "conclusion": "Self-correction in VLMs should enhance reasoning abilities and enable high-quality response generation directly, rather than just refining outputs.", "key_contributions": ["Introduction of Self-Correction Learning (SCL) for VLMs.", "Demonstration of preference fine-tuning's impact on VLM performance.", "Establishment of the importance of enhancing reasoning capabilities through self-correction."], "limitations": "The study shows that VLMs are not effective at self-correction during iterative inference alone without additional fine-tuning.", "keywords": ["Vision-Language Models", "Self-Correction Learning", "Direct Preference Optimization", "Inference", "Fine-Tuning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2410.12705", "pdf": "https://arxiv.org/pdf/2410.12705.pdf", "abs": "https://arxiv.org/abs/2410.12705", "title": "WorldCuisines: A Massive-Scale Benchmark for Multilingual and Multicultural Visual Question Answering on Global Cuisines", "authors": ["Genta Indra Winata", "Frederikus Hudi", "Patrick Amadeus Irawan", "David Anugraha", "Rifki Afina Putri", "Yutong Wang", "Adam Nohejl", "Ubaidillah Ariq Prathama", "Nedjma Ousidhoum", "Afifa Amriani", "Anar Rzayev", "Anirban Das", "Ashmari Pramodya", "Aulia Adila", "Bryan Wilie", "Candy Olivia Mawalim", "Ching Lam Cheng", "Daud Abolade", "Emmanuele Chersoni", "Enrico Santus", "Fariz Ikhwantri", "Garry Kuwanto", "Hanyang Zhao", "Haryo Akbarianto Wibowo", "Holy Lovenia", "Jan Christian Blaise Cruz", "Jan Wira Gotama Putra", "Junho Myung", "Lucky Susanto", "Maria Angelica Riera Machin", "Marina Zhukova", "Michael Anugraha", "Muhammad Farid Adilazuarda", "Natasha Santosa", "Peerat Limkonchotiwat", "Raj Dabre", "Rio Alexander Audino", "Samuel Cahyawijaya", "Shi-Xiong Zhang", "Stephanie Yulia Salim", "Yi Zhou", "Yinxuan Gui", "David Ifeoluwa Adelani", "En-Shiun Annie Lee", "Shogo Okada", "Ayu Purwarianti", "Alham Fikri Aji", "Taro Watanabe", "Derry Tanti Wijaya", "Alice Oh", "Chong-Wah Ngo"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Best Theme Paper at NAACL 2025", "summary": "Vision Language Models (VLMs) often struggle with culture-specific knowledge,\nparticularly in languages other than English and in underrepresented cultural\ncontexts. To evaluate their understanding of such knowledge, we introduce\nWorldCuisines, a massive-scale benchmark for multilingual and multicultural,\nvisually grounded language understanding. This benchmark includes a visual\nquestion answering (VQA) dataset with text-image pairs across 30 languages and\ndialects, spanning 9 language families and featuring over 1 million data\npoints, making it the largest multicultural VQA benchmark to date. It includes\ntasks for identifying dish names and their origins. We provide evaluation\ndatasets in two sizes (12k and 60k instances) alongside a training dataset (1\nmillion instances). Our findings show that while VLMs perform better with\ncorrect location context, they struggle with adversarial contexts and\npredicting specific regional cuisines and languages. To support future\nresearch, we release a knowledge base with annotated food entries and images\nalong with the VQA data.", "AI": {"tldr": "The paper introduces WorldCuisines, a benchmark for evaluating Vision Language Models (VLMs) on multilingual and multicultural visual language tasks, highlighting their struggles with culture-specific knowledge.", "motivation": "To address the limitations of Vision Language Models (VLMs) regarding culture-specific knowledge and to provide a comprehensive evaluation of these models in diverse linguistic and cultural contexts.", "method": "Introduced WorldCuisines, a benchmark consisting of a visual question answering (VQA) dataset encompassing text-image pairs in 30 languages and dialects, with over 1 million instances.", "result": "The benchmark demonstrates that VLMs improve when provided with correct location context but struggle with adversarial contexts and prediction of specific dishes tied to regional cuisines and languages.", "conclusion": "The release of the WorldCuisines benchmark aims to facilitate future research in culturally diverse visual language understanding by providing extensive datasets and an annotated knowledge base.", "key_contributions": ["Introduction of the WorldCuisines benchmark for multilingual and multicultural VQA tasks.", "Provision of detailed evaluation datasets in two sizes alongside a large training dataset.", "Release of a knowledge base with annotated food entries and images."], "limitations": "VLMs still struggle with adversarial contexts and predictions related to specific regional cuisines and languages.", "keywords": ["Vision Language Models", "multilingual", "multicultural", "visual question answering", "culturally diverse"], "importance_score": 6, "read_time_minutes": 8}}
{"id": "2411.00437", "pdf": "https://arxiv.org/pdf/2411.00437.pdf", "abs": "https://arxiv.org/abs/2411.00437", "title": "E2E-AFG: An End-to-End Model with Adaptive Filtering for Retrieval-Augmented Generation", "authors": ["Yun Jiang", "Zilong Xie", "Wei Zhang", "Yun Fang", "Shuai Pan"], "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 3 figures, 5 tables", "summary": "Retrieval-augmented generation methods often neglect the quality of content\nretrieved from external knowledge bases, resulting in irrelevant information or\npotential misinformation that negatively affects the generation results of\nlarge language models. In this paper, we propose an end-to-end model with\nadaptive filtering for retrieval-augmented generation (E2E-AFG), which\nintegrates answer existence judgment and text generation into a single\nend-to-end framework. This enables the model to focus more effectively on\nrelevant content while reducing the influence of irrelevant information and\ngenerating accurate answers. We evaluate E2E-AFG on six representative\nknowledge-intensive language datasets, and the results show that it\nconsistently outperforms baseline models across all tasks, demonstrating the\neffectiveness and robustness of the proposed approach.", "AI": {"tldr": "This paper introduces an end-to-end model with adaptive filtering for retrieval-augmented generation that focuses on improving content relevance and accuracy in large language models.", "motivation": "The paper addresses the issue of irrelevant or misleading information affecting the performance of retrieval-augmented generation methods in large language models.", "method": "The proposed model, E2E-AFG, integrates answer existence judgment and text generation into a single end-to-end framework with adaptive filtering to prioritize relevant content.", "result": "E2E-AFG outperforms baseline models across six knowledge-intensive language datasets, demonstrating improved effectiveness and robustness in generating accurate responses.", "conclusion": "The integration of adaptive filtering in the generation process enhances the quality of information used in LLMs, leading to more accurate outputs.", "key_contributions": ["Proposed an end-to-end model with adaptive filtering for retrieval-augmented generation.", "Successfully integrated answer existence judgment into text generation.", "Demonstrated improved performance on knowledge-intensive language tasks."], "limitations": "", "keywords": ["retrieval-augmented generation", "large language models", "adaptive filtering"], "importance_score": 9, "read_time_minutes": 13}}
{"id": "2411.04996", "pdf": "https://arxiv.org/pdf/2411.04996.pdf", "abs": "https://arxiv.org/abs/2411.04996", "title": "Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models", "authors": ["Weixin Liang", "Lili Yu", "Liang Luo", "Srinivasan Iyer", "Ning Dong", "Chunting Zhou", "Gargi Ghosh", "Mike Lewis", "Wen-tau Yih", "Luke Zettlemoyer", "Xi Victoria Lin"], "categories": ["cs.CL"], "comment": "Accepted to TMLR 2025; 48 pages", "summary": "The development of large language models (LLMs) has expanded to multi-modal\nsystems capable of processing text, images, and speech within a unified\nframework. Training these models demands significantly larger datasets and\ncomputational resources compared to text-only LLMs. To address the scaling\nchallenges, we introduce Mixture-of-Transformers (MoT), a sparse multi-modal\ntransformer architecture that significantly reduces pretraining computational\ncosts. MoT decouples non-embedding parameters of the model by modality --\nincluding feed-forward networks, attention matrices, and layer normalization --\nenabling modality-specific processing with global self-attention over the full\ninput sequence. We evaluate MoT across multiple settings and model scales. In\nthe Chameleon 7B setting (autoregressive text-and-image generation), MoT\nmatches the dense baseline's performance using only 55.8\\% of the FLOPs. When\nextended to include speech, MoT reaches speech performance comparable to the\ndense baseline with only 37.2\\% of the FLOPs. In the Transfusion setting, where\ntext and image are trained with different objectives, a 7B MoT model matches\nthe image modality performance of the dense baseline with one third of the\nFLOPs, and a 760M MoT model outperforms a 1.4B dense baseline across key image\ngeneration metrics. System profiling further highlights MoT's practical\nbenefits, achieving dense baseline image quality in 47.2\\% of the wall-clock\ntime and text quality in 75.6\\% of the wall-clock time (measured on AWS\np4de.24xlarge instances with NVIDIA A100 GPUs).", "AI": {"tldr": "Introducing Mixture-of-Transformers (MoT), a sparse multi-modal transformer architecture that reduces computational costs while maintaining performance across modalities like text, images, and speech.", "motivation": "To address the scaling challenges in training large multi-modal models that require extensive datasets and computational resources.", "method": "MoT decouples non-embedding parameters by modality, allowing modality-specific processing along with global self-attention over the entire input sequence.", "result": "MoT achieves performance comparable to dense baselines with significantly reduced FLOPs: 55.8% for text-and-image generation and 37.2% for speech processing; outperforming dense baselines in some tasks with lesser resources.", "conclusion": "MoT demonstrates practical efficiency gains in training multi-modal models, providing similar quality outputs much faster and with fewer resources.", "key_contributions": ["Introduced a novel sparse multi-modal transformer architecture (MoT).", "Showed significant reductions in computational costs for training compared to traditional dense models.", "Provided empirical results demonstrating MoT's efficiency in text, image, and speech processing tasks."], "limitations": "", "keywords": ["multi-modal", "transformer architecture", "Mixture-of-Transformers", "computational efficiency", "large language models"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2501.01031", "pdf": "https://arxiv.org/pdf/2501.01031.pdf", "abs": "https://arxiv.org/abs/2501.01031", "title": "ValuesRAG: Enhancing Cultural Alignment Through Retrieval-Augmented Contextual Learning", "authors": ["Wonduk Seo", "Zonghao Yuan", "Yi Bu"], "categories": ["cs.CL", "cs.AI", "cs.SI"], "comment": "preprint", "summary": "Ensuring cultural values alignment in Large Language Models (LLMs) remains a\ncritical challenge, as these models often embed Western-centric biases from\ntheir training data, leading to misrepresentations and fairness concerns in\ncross-cultural applications. Existing approaches such as role assignment and\nfew-shot learning struggle to address these limitations effectively due to\ntheir reliance on pre-trained knowledge, limited scalability, and inability to\ncapture nuanced cultural values. To address these issues, we propose ValuesRAG,\na novel and effective framework that applies Retrieval-Augmented Generation\n(RAG) with In-Context Learning (ICL) to integrate cultural and demographic\nknowledge dynamically during text generation. Leveraging the World Values\nSurvey (WVS) dataset, ValuesRAG first generates summaries of values for each\nindividual. We subsequently curate several representative regional datasets to\nserve as test datasets and retrieve relevant summaries of values based on\ndemographic features, followed by a reranking step to select the top-k relevant\nsummaries. We evaluate ValuesRAG using 6 diverse regional datasets and show\nthat it consistently outperforms baselines: including zero-shot,\nrole-assignment, few-shot, and hybrid methods, both in main experiments and\nablation settings. Notably, ValuesRAG achieves the best overall performance\nover prior methods, demonstrating its effectiveness in fostering culturally\naligned and inclusive AI systems. Our findings underscore the potential of\ndynamic retrieval-based methods to bridge the gap between global LLM\ncapabilities and localized cultural values.", "AI": {"tldr": "This paper introduces ValuesRAG, a framework that enhances Large Language Models by integrating cultural and demographic knowledge to ensure better alignment with varied cultural values during text generation.", "motivation": "There is a significant issue with biases in LLMs that often misrepresent diverse cultural values, which can lead to fairness concerns in cross-cultural applications.", "method": "ValuesRAG employs Retrieval-Augmented Generation combined with In-Context Learning to dynamically incorporate cultural knowledge extracted from the World Values Survey and curated regional datasets during the text generation process.", "result": "The evaluation shows that ValuesRAG outperforms existing methodologies (zero-shot, role-assignment, few-shot, hybrid methods) across 6 diverse regional datasets, achieving the best overall performance in fostering culturally aligned AI systems.", "conclusion": "The study highlights the effectiveness of dynamic retrieval methods in addressing biases in LLMs and facilitating more inclusive AI systems that respect and incorporate a range of cultural values.", "key_contributions": ["Introduction of the ValuesRAG framework for culturally aligned LLMs.", "Demonstrated superior performance over existing methods with diverse regional datasets.", "Innovative integration of culture-specific summaries into AI text generation."], "limitations": "", "keywords": ["Large Language Models", "cultural alignment", "Retrieval-Augmented Generation", "In-Context Learning", "bias mitigation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2501.14082", "pdf": "https://arxiv.org/pdf/2501.14082.pdf", "abs": "https://arxiv.org/abs/2501.14082", "title": "Communicating Activations Between Language Model Agents", "authors": ["Vignav Ramesh", "Kenneth Li"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ICML 2025", "summary": "Communication between multiple language model (LM) agents has been shown to\nscale up the reasoning ability of LMs. While natural language has been the\ndominant medium for inter-LM communication, it is not obvious this should be\nthe standard: not only does natural language communication incur high inference\ncosts that scale quickly with the number of both agents and messages, but also\nthe decoding process abstracts away too much rich information that could be\notherwise accessed from the internal activations. In this work, we propose a\nsimple technique whereby LMs communicate via activations; concretely, we pause\nan LM $\\textit{B}$'s computation at an intermediate layer, combine its current\nactivation with another LM $\\textit{A}$'s intermediate activation via some\nfunction $\\textit{f}$, then pass $\\textit{f}$'s output into the next layer of\n$\\textit{B}$ and continue the forward pass till decoding is complete. This\napproach scales up LMs on new tasks with zero additional parameters and data,\nand saves a substantial amount of compute over natural language communication.\nWe test our method with various functional forms $\\textit{f}$ on two\nexperimental setups--multi-player coordination games and reasoning\nbenchmarks--and find that it achieves up to $27.0\\%$ improvement over natural\nlanguage communication across datasets with $<$$1/4$ the compute, illustrating\nthe superiority and robustness of activations as an alternative \"language\" for\ncommunication between LMs.", "AI": {"tldr": "This paper proposes a novel method for communication between language model agents using internal activations instead of natural language, demonstrating improved efficiency and performance.", "motivation": "To address the high computational costs and limitations of natural language as a communication medium between LM agents, and to explore more efficient alternatives.", "method": "The approach involves pausing an LM's computation at an intermediate layer, combining its current activation with another LM's activation using a function, and continuing the forward pass without additional parameters or data.", "result": "The proposed method achieves up to 27.0% performance improvement over natural language communication while utilizing less than a quarter of the computational resources.", "conclusion": "Activations serve as a robust and efficient alternative language for LM communication, enhancing their reasoning capabilities without added data or parameters.", "key_contributions": ["Introduction of activation-based communication for LMs", "Demonstrated significant performance improvement over traditional methods", "Efficiency gains with reduced computational costs"], "limitations": "", "keywords": ["language models", "communication", "activations", "machine learning", "efficiency"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2502.11137", "pdf": "https://arxiv.org/pdf/2502.11137.pdf", "abs": "https://arxiv.org/abs/2502.11137", "title": "Safety Evaluation of DeepSeek Models in Chinese Contexts", "authors": ["Wenjing Zhang", "Xuejiao Lei", "Zhaoxiang Liu", "Ning Wang", "Zhenhong Long", "Peijun Yang", "Jiaojiao Zhao", "Minjie Hua", "Chaoyang Ma", "Kai Wang", "Shiguo Lian"], "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 2 tables, 7 figures", "summary": "Recently, the DeepSeek series of models, leveraging their exceptional\nreasoning capabilities and open-source strategy, is reshaping the global AI\nlandscape. Despite these advantages, they exhibit significant safety\ndeficiencies. Research conducted by Robust Intelligence, a subsidiary of Cisco,\nin collaboration with the University of Pennsylvania, revealed that DeepSeek-R1\nhas a 100\\% attack success rate when processing harmful prompts. Additionally,\nmultiple safety companies and research institutions have confirmed critical\nsafety vulnerabilities in this model. As models demonstrating robust\nperformance in Chinese and English, DeepSeek models require equally crucial\nsafety assessments in both language contexts. However, current research has\npredominantly focused on safety evaluations in English environments, leaving a\ngap in comprehensive assessments of their safety performance in Chinese\ncontexts. In response to this gap, this study introduces CHiSafetyBench, a\nChinese-specific safety evaluation benchmark. This benchmark systematically\nevaluates the safety of DeepSeek-R1 and DeepSeek-V3 in Chinese contexts,\nrevealing their performance across safety categories. The experimental results\nquantify the deficiencies of these two models in Chinese contexts, providing\nkey insights for subsequent improvements. It should be noted that, despite our\nefforts to establish a comprehensive, objective, and authoritative evaluation\nbenchmark, the selection of test samples, characteristics of data distribution,\nand the setting of evaluation criteria may inevitably introduce certain biases\ninto the evaluation results. We will continuously optimize the evaluation\nbenchmark and periodically update this report to provide more comprehensive and\naccurate assessment outcomes. Please refer to the latest version of the paper\nfor the most recent evaluation results and conclusions.", "AI": {"tldr": "This paper presents CHiSafetyBench, a benchmark for evaluating the safety of DeepSeek models in Chinese contexts, highlighting their performance deficiencies and the need for comprehensive safety assessments in both Chinese and English.", "motivation": "The paper addresses the significant safety deficiencies of DeepSeek models revealed in prior research and the lack of safety evaluations specifically tailored to Chinese contexts, which are critical given the models' performance in both languages.", "method": "The study introduces CHiSafetyBench, a systematic evaluation benchmark designed to assess the safety of DeepSeek-R1 and DeepSeek-V3 in Chinese, quantifying their performance across various safety categories.", "result": "Experimental results demonstrate that DeepSeek-R1 and DeepSeek-V3 exhibit significant safety vulnerabilities when evaluated in Chinese contexts, mirroring their issues found in English.", "conclusion": "While the paper establishes a crucial evaluation benchmark for DeepSeek models in Chinese contexts, it acknowledges potential biases in sample selection and evaluation criteria and commits to ongoing optimization of the benchmark.", "key_contributions": ["Introduction of CHiSafetyBench for assessing DeepSeek model safety in Chinese contexts", "Quantitative evaluation revealing performance deficiencies in Chinese safety assessments", "Recognition of the need for equal safety evaluation efforts in both English and Chinese environments"], "limitations": "Potential biases in test sample selection, data distribution characteristics, and evaluation criteria settings may affect results.", "keywords": ["DeepSeek", "safety evaluation", "CHiSafetyBench", "Chinese language models", "HCI"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2502.14289", "pdf": "https://arxiv.org/pdf/2502.14289.pdf", "abs": "https://arxiv.org/abs/2502.14289", "title": "Drift: Decoding-time Personalized Alignments with Implicit User Preferences", "authors": ["Minbeom Kim", "Kang-il Lee", "Seongho Joo", "Hwaran Lee", "Thibaut Thonet", "Kyomin Jung"], "categories": ["cs.CL"], "comment": "19 pages, 6 figures", "summary": "Personalized alignments for individual users have been a long-standing goal\nin large language models (LLMs). We introduce Drift, a novel framework that\npersonalizes LLMs at decoding time with implicit user preferences. Traditional\nReinforcement Learning from Human Feedback (RLHF) requires thousands of\nannotated examples and expensive gradient updates. In contrast, Drift\npersonalizes LLMs in a training-free manner, using only a few dozen examples to\nsteer a frozen model through efficient preference modeling. Our approach models\nuser preferences as a composition of predefined, interpretable attributes and\naligns them at decoding time to enable personalized generation. Experiments on\nboth a synthetic persona dataset (Perspective) and a real human-annotated\ndataset (PRISM) demonstrate that Drift significantly outperforms RLHF baselines\nwhile using only 50-100 examples. Our results and analysis show that Drift is\nboth computationally efficient and interpretable.", "AI": {"tldr": "Drift is a framework that personalizes large language models at decoding time based on user preferences, using fewer examples than traditional methods.", "motivation": "To reduce the reliance on extensive annotated examples and costly gradient updates in personalizing large language models for individual users.", "method": "Drift uses a training-free approach to personalize LLMs by utilizing a few dozen examples to model user preferences, aligning them with predefined attributes during the decoding process.", "result": "Drift outperforms traditional RLHF methods on both synthetic and real datasets using only 50-100 examples, indicating its efficiency and effectiveness.", "conclusion": "Drift offers a computationally efficient and interpretable solution for personalizing large language models without retraining.", "key_contributions": ["Introduction of a training-free personalization method for LLMs", "Demonstrated high performance with significantly fewer examples compared to RLHF", "Provides an interpretable model of user preferences using predefined attributes"], "limitations": "", "keywords": ["Personalized language models", "Human-computer interaction", "Reinforcement learning", "Machine learning", "User preferences"], "importance_score": 9, "read_time_minutes": 30}}
{"id": "2503.05505", "pdf": "https://arxiv.org/pdf/2503.05505.pdf", "abs": "https://arxiv.org/abs/2503.05505", "title": "Correctness Coverage Evaluation for Medical Multiple-Choice Question Answering Based on the Enhanced Conformal Prediction Framework", "authors": ["Yusong Ke", "Hongru Lin", "Yuting Ruan", "Junya Tang", "Li Li"], "categories": ["cs.CL"], "comment": "Published by Mathematics", "summary": "Large language models (LLMs) are increasingly adopted in medical\nquestion-answering (QA) scenarios. However, LLMs can generate hallucinations\nand nonfactual information, undermining their trustworthiness in high-stakes\nmedical tasks. Conformal Prediction (CP) provides a statistically rigorous\nframework for marginal (average) coverage guarantees but has limited\nexploration in medical QA. This paper proposes an enhanced CP framework for\nmedical multiple-choice question-answering (MCQA) tasks. By associating the\nnon-conformance score with the frequency score of correct options and\nleveraging self-consistency, the framework addresses internal model opacity and\nincorporates a risk control strategy with a monotonic loss function. Evaluated\non MedMCQA, MedQA, and MMLU datasets using four off-the-shelf LLMs, the\nproposed method meets specified error rate guarantees while reducing average\nprediction set size with increased risk level, offering a promising uncertainty\nevaluation metric for LLMs.", "AI": {"tldr": "This paper presents an enhanced conformal prediction framework for medical multiple-choice question-answering using large language models to improve trustworthiness and reduce hallucinations.", "motivation": "Large language models often generate nonfactual information in medical tasks, making it crucial to establish trustworthiness in medical QA scenarios.", "method": "The paper proposes a conformal prediction framework that incorporates non-conformance scores with correct option frequencies and uses self-consistency and a monotonic loss function for risk control.", "result": "The method successfully meets specified error rate guarantees while minimizing the average prediction set size across evaluated datasets.", "conclusion": "The enhanced framework provides a promising uncertainty evaluation metric for improving the reliability of LLMs in medical QA tasks.", "key_contributions": ["Enhanced conformal prediction framework for medical QA", "Integration of non-conformance scores with correct option frequencies", "Risk control strategy with a monotonic loss function"], "limitations": "", "keywords": ["Large language models", "Conformal Prediction", "Medical Question-Answering"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2503.08292", "pdf": "https://arxiv.org/pdf/2503.08292.pdf", "abs": "https://arxiv.org/abs/2503.08292", "title": "Large Language Models for Outpatient Referral: Problem Definition, Benchmarking and Challenges", "authors": ["Xiaoxiao Liu", "Qingying Xiao", "Junying Chen", "Xiangyi Feng", "Xiangbo Wu", "Bairui Zhang", "Xiang Wan", "Jian Chang", "Guangjun Yu", "Yan Hu", "Benyou Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly applied to outpatient referral\ntasks across healthcare systems. However, there is a lack of standardized\nevaluation criteria to assess their effectiveness, particularly in dynamic,\ninteractive scenarios. In this study, we systematically examine the\ncapabilities and limitations of LLMs in managing tasks within Intelligent\nOutpatient Referral (IOR) systems and propose a comprehensive evaluation\nframework specifically designed for such systems. This framework comprises two\ncore tasks: static evaluation, which focuses on evaluating the ability of\npredefined outpatient referrals, and dynamic evaluation, which evaluates\ncapabilities of refining outpatient referral recommendations through iterative\ndialogues. Our findings suggest that LLMs offer limited advantages over\nBERT-like models, but show promise in asking effective questions during\ninteractive dialogues.", "AI": {"tldr": "This study evaluates the effectiveness of large language models (LLMs) in outpatient referral tasks and proposes a new evaluation framework.", "motivation": "To address the lack of standardized evaluation criteria for LLMs in dynamic healthcare scenarios, specifically in Intelligent Outpatient Referral systems.", "method": "The study employs a systematic examination of LLM capabilities and limitations in IOR systems, proposing a framework that includes both static and dynamic evaluation tasks.", "result": "Results indicate that LLMs provide limited advantages over BERT-like models, yet they excel in facilitating iterative dialogues by asking pertinent questions.", "conclusion": "The proposed evaluation framework offers a structured way to assess LLM performance specifically in outpatient referral contexts, highlighting the need for improved evaluation methodologies.", "key_contributions": ["Development of a comprehensive evaluation framework for LLMs in outpatient referrals", "Identification of LLMs' strengths in dialogue situations", "Comparison of LLMs and BERT-like models in outpatient referral tasks"], "limitations": "Limited advantages of LLMs over BERT-like models may indicate a need for further optimization in specific healthcare tasks.", "keywords": ["large language models", "outpatient referral", "healthcare AI", "evaluation framework", "dialogue systems"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2503.13690", "pdf": "https://arxiv.org/pdf/2503.13690.pdf", "abs": "https://arxiv.org/abs/2503.13690", "title": "Atyaephyra at SemEval-2025 Task 4: Low-Rank Negative Preference Optimization", "authors": ["Jan Bronec", "Jindřich Helcl"], "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50 (Primary), 68T07 (Secondary)", "I.2.7"], "comment": "8 pages, 3 figures, accepted to SemEval workshop proceedings at ACL\n  2025", "summary": "We present a submission to the SemEval 2025 shared task on unlearning\nsensitive content from LLMs. Our approach employs negative preference\noptimization using low-rank adaptation. We show that we can utilize this\ncombination to efficiently compute additional regularization terms, which help\nwith unlearning stabilization. The results of our approach significantly exceed\nthe shared task baselines.", "AI": {"tldr": "A novel method for unlearning sensitive content from LLMs using negative preference optimization and low-rank adaptation.", "motivation": "To address the need for effective unlearning techniques for LLMs that can handle sensitive content while maintaining performance.", "method": "The paper introduces negative preference optimization combined with low-rank adaptation to compute additional regularization terms for unlearning stabilization.", "result": "The proposed approach significantly outperforms the current baselines set by the SemEval shared task.", "conclusion": "The findings indicate that our method is effective in unlearning sensitive content from LLMs, providing a promising direction for future research in safe deployment.", "key_contributions": ["Introduction of negative preference optimization for LLM unlearning", "Utilization of low-rank adaptation for enhanced regularization", "Demonstration of substantial performance improvement over existing baselines"], "limitations": "", "keywords": ["unlearning", "sensitive content", "LLMs", "negative preference optimization", "low-rank adaptation"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2503.15169", "pdf": "https://arxiv.org/pdf/2503.15169.pdf", "abs": "https://arxiv.org/abs/2503.15169", "title": "Benchmarking Open-Source Large Language Models on Healthcare Text Classification Tasks", "authors": ["Yuting Guo", "Abeed Sarker"], "categories": ["cs.CL", "cs.AI"], "comment": "5 pages", "summary": "The application of large language models (LLMs) to healthcare information\nextraction has emerged as a promising approach. This study evaluates the\nclassification performance of five open-source LLMs: GEMMA-3-27B-IT,\nLLAMA3-70B, LLAMA4-109B, DEEPSEEK-R1-DISTILL-LLAMA-70B, and\nDEEPSEEK-V3-0324-UD-Q2_K_XL, across six healthcare-related classification tasks\ninvolving both social media data (breast cancer, changes in medication regimen,\nadverse pregnancy outcomes, potential COVID-19 cases) and clinical data (stigma\nlabeling, medication change discussion). We report precision, recall, and F1\nscores with 95% confidence intervals for all model-task combinations. Our\nfindings reveal significant performance variability between LLMs, with\nDeepSeekV3 emerging as the strongest overall performer, achieving the highest\nF1 scores in four tasks. Notably, models generally performed better on social\nmedia tasks compared to clinical data tasks, suggesting potential\ndomain-specific challenges. GEMMA-3-27B-IT demonstrated exceptionally high\nrecall despite its smaller parameter count, while LLAMA4-109B showed\nsurprisingly underwhelming performance compared to its predecessor LLAMA3-70B,\nindicating that larger parameter counts do not guarantee improved\nclassification results. We observed distinct precision-recall trade-offs across\nmodels, with some favoring sensitivity over specificity and vice versa. These\nfindings highlight the importance of task-specific model selection for\nhealthcare applications, considering the particular data domain and\nprecision-recall requirements rather than model size alone. As healthcare\nincreasingly integrates AI-driven text classification tools, this comprehensive\nbenchmarking provides valuable guidance for model selection and implementation\nwhile underscoring the need for continued evaluation and domain adaptation of\nLLMs in healthcare contexts.", "AI": {"tldr": "This study evaluates five LLMs for healthcare information extraction, revealing significant variability in performance across tasks and data types.", "motivation": "The application of LLMs to healthcare is promising, but performance varies significantly across tasks and models, necessitating task-specific selection.", "method": "The authors assessed five open-source LLMs on six healthcare classification tasks, analyzing precision, recall, and F1 scores while providing 95% confidence intervals.", "result": "DeepSeekV3 performed best overall, particularly on social media tasks, with notable performance variability; GEMMA-3-27B-IT showed high recall despite fewer parameters.", "conclusion": "Healthcare applications should prioritize task-specific model selection over mere parameter count, highlighting the need for ongoing evaluation of LLMs for healthcare.", "key_contributions": ["Benchmarking performance of multiple LLMs in healthcare classification tasks", "Highlighting the variability in performance based on task and data type", "Emphasizing the importance of task-specific model selection over model size."], "limitations": "", "keywords": ["large language models", "healthcare information extraction", "model evaluation", "classification tasks", "social media data"], "importance_score": 9, "read_time_minutes": 5}}
