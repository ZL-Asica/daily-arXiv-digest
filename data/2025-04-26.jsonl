{"id": "2504.17018", "pdf": "https://arxiv.org/pdf/2504.17018.pdf", "abs": "https://arxiv.org/abs/2504.17018", "title": "LLM impact on BLV programming", "authors": ["Prashant Chandrasekar", "Mariel Couvillion", "Ayshwarya Saktheeswaran", "Jessica Zeitz"], "categories": ["cs.HC"], "comment": "Submitted to ASSETS 2025", "summary": "Large Language Models (LLMs) are rapidly becoming integral to a wide range of\ntools, tasks, and problem-solving processes, especially in software\ndevelopment. Originally designed for natural language processing tasks such as\ntext generation, LLMs are increasingly being used to assist both professionals\nand students in writing code. This growing reliance on LLM-based tools is\nreshaping programming workflows and task execution. In this study, we explore\nthe impact of these technologies on blind and low-vision (BLV) developers. Our\nreview of existing literature indicates that while LLMs help mitigate some of\nthe challenges faced by BLV programmers, they also introduce new forms of\ninaccessibility. We conducted an evaluation of five popular LLM-powered\nintegrated development environments (IDEs), assessing their performance across\na comprehensive set of programming tasks. Our findings highlight several\nunsupported scenarios, instances of incorrect model output, and notable\nlimitations in interaction support for specific tasks. Through observing BLV\ndevelopers as they engaged in coding activities, we uncovered key interaction\nbarriers that go beyond model accuracy or code generation quality. This paper\noutlines the challenges and corresponding opportunities for improving\naccessibility in the context of generative AI-assisted programming. Addressing\nthese issues can meaningfully enhance the programming experience for BLV\ndevelopers. As the generative AI revolution continues to unfold, it must also\naddress the unique burdens faced by this community."}
{"id": "2504.17023", "pdf": "https://arxiv.org/pdf/2504.17023.pdf", "abs": "https://arxiv.org/abs/2504.17023", "title": "What Makes for a Good Saliency Map? Comparing Strategies for Evaluating Saliency Maps in Explainable AI (XAI)", "authors": ["Felix Kares", "Timo Speith", "Hanwei Zhang", "Markus Langer"], "categories": ["cs.HC", "cs.AI"], "comment": "27 pages, 7 figures, 4 tables", "summary": "Saliency maps are a popular approach for explaining classifications of\n(convolutional) neural networks. However, it remains an open question as to how\nbest to evaluate salience maps, with three families of evaluation methods\ncommonly being used: subjective user measures, objective user measures, and\nmathematical metrics. We examine three of the most popular saliency map\napproaches (viz., LIME, Grad-CAM, and Guided Backpropagation) in a between\nsubject study (N=166) across these families of evaluation methods. We test 1)\nfor subjective measures, if the maps differ with respect to user trust and\nsatisfaction; 2) for objective measures, if the maps increase users' abilities\nand thus understanding of a model; 3) for mathematical metrics, which map\nachieves the best ratings across metrics; and 4) whether the mathematical\nmetrics can be associated with objective user measures. To our knowledge, our\nstudy is the first to compare several salience maps across all these evaluation\nmethods$-$with the finding that they do not agree in their assessment (i.e.,\nthere was no difference concerning trust and satisfaction, Grad-CAM improved\nusers' abilities best, and Guided Backpropagation had the most favorable\nmathematical metrics). Additionally, we show that some mathematical metrics\nwere associated with user understanding, although this relationship was often\ncounterintuitive. We discuss these findings in light of general debates\nconcerning the complementary use of user studies and mathematical metrics in\nthe evaluation of explainable AI (XAI) approaches."}
{"id": "2504.17055", "pdf": "https://arxiv.org/pdf/2504.17055.pdf", "abs": "https://arxiv.org/abs/2504.17055", "title": "Psychological Effect of AI driven marketing tools for beauty/facial feature enhancement", "authors": ["Ayushi Agrawal", "Aditya Kondai", "Kavita Vemuri"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "AI-powered facial assessment tools are reshaping how individuals evaluate\nappearance and internalize social judgments. This study examines the\npsychological impact of such tools on self-objectification, self-esteem, and\nemotional responses, with attention to gender differences. Two samples used\ndistinct versions of a facial analysis tool: one overtly critical (N=75; M=22.9\nyears), and another more neutral (N=51; M=19.9 years). Participants completed\nvalidated self-objectification and self-esteem scales and custom items\nmeasuring emotion, digital/physical appearance enhancement (DAE, PAEE), and\nperceived social emotion (PSE). Results revealed consistent links between high\nself-objectification, low self-esteem, and increased appearance enhancement\nbehaviors across both versions. Despite softer framing, the newer tool still\nevoked negative emotional responses (U=1466.5, p=0.013), indicating implicit\nfeedback may reinforce appearance-related insecurities. Gender differences\nemerged in DAE (p=0.025) and PSE (p<0.001), with females more prone to digital\nenhancement and less likely to perceive emotional impact in others. These\nfindings reveal how AI tools may unintentionally reinforce and amplify existing\nsocial biases and underscore the critical need for responsible AI design and\ndevelopment. Future research will investigate how human ideologies embedded in\nthe training data of such tools shape their evaluative outputs, and how these,\nin turn, influence user attitudes and decisions."}
{"id": "2504.17150", "pdf": "https://arxiv.org/pdf/2504.17150.pdf", "abs": "https://arxiv.org/abs/2504.17150", "title": "DashGuide: Authoring Interactive Dashboard Tours for Guiding Dashboard Users", "authors": ["Naimul Hoque", "Nicole Sultanum"], "categories": ["cs.HC"], "comment": null, "summary": "Dashboard guidance helps dashboard users better navigate interactive\nfeatures, understand the underlying data, and assess insights they can\npotentially extract from dashboards. However, authoring dashboard guidance is a\ntime consuming task, and embedding guidance into dashboards for effective\ndelivery is difficult to realize. In this work, we contribute DashGuide, a\nframework and system to support the creation of interactive dashboard guidance\nwith minimal authoring input. Given a dashboard and a communication goal,\nDashGuide captures a sequence of author-performed interactions to generate\nguidance materials delivered as playable step-by-step overlays, a.k.a.,\ndashboard tours. Authors can further edit and refine individual tour steps\nwhile receiving generative assistance. We also contribute findings from a\nformative assessment with 9 dashboard creators, which helped inform the design\nof DashGuide; and findings from an evaluation of DashGuide with 12 dashboard\ncreators, suggesting it provides an improved authoring experience that balances\nefficiency, expressiveness, and creative freedom."}
{"id": "2504.16956", "pdf": "https://arxiv.org/pdf/2504.16956.pdf", "abs": "https://arxiv.org/abs/2504.16956", "title": "Bidirectional Mamba for Single-Cell Data: Efficient Context Learning with Biological Fidelity", "authors": ["Cong Qi", "Hanzhang Fang", "Tianxing Hu", "Siqi Jiang", "Wei Zhi"], "categories": ["cs.CL", "cs.LG", "q-bio.GN"], "comment": null, "summary": "Single-cell RNA sequencing (scRNA-seq) enables high-resolution analysis of\ncellular heterogeneity, but its complexity, which is marked by high\ndimensionality, sparsity, and batch effects, which poses major computational\nchallenges. Transformer-based models have made significant advances in this\ndomain but are often limited by their quadratic complexity and suboptimal\nhandling of long-range dependencies. In this work, we introduce GeneMamba, a\nscalable and efficient foundation model for single-cell transcriptomics built\non state space modeling. Leveraging the Bi-Mamba architecture, GeneMamba\ncaptures bidirectional gene context with linear-time complexity, offering\nsubstantial computational gains over transformer baselines. The model is\npretrained on nearly 30 million cells and incorporates biologically informed\nobjectives, including pathway-aware contrastive loss and rank-based gene\nencoding. We evaluate GeneMamba across diverse tasks, including multi-batch\nintegration, cell type annotation, and gene-gene correlation, demonstrating\nstrong performance, interpretability, and robustness. These results position\nGeneMamba as a practical and powerful alternative to transformer-based methods,\nadvancing the development of biologically grounded, scalable tools for\nlarge-scale single-cell data analysis."}
{"id": "2504.17170", "pdf": "https://arxiv.org/pdf/2504.17170.pdf", "abs": "https://arxiv.org/abs/2504.17170", "title": "Improving Human-Autonomous Vehicle Interaction in Complex Systems", "authors": ["Robert Kaufman"], "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": "PhD Dissertation from University of California, San Diego; 175 pages", "summary": "Unresolved questions about how autonomous vehicles (AVs) should meet the\ninformational needs of riders hinder real-world adoption. Complicating our\nability to satisfy rider needs is that different people, goals, and driving\ncontexts have different criteria for what constitutes interaction success.\nUnfortunately, most human-AV research and design today treats all people and\nsituations uniformly. It is crucial to understand how an AV should communicate\nto meet rider needs, and how communications should change when the human-AV\ncomplex system changes. I argue that understanding the relationships between\ndifferent aspects of the human-AV system can help us build improved and\nadaptable AV communications. I support this argument using three empirical\nstudies. First, I identify optimal communication strategies that enhance\ndriving performance, confidence, and trust for learning in extreme driving\nenvironments. Findings highlight the need for task-sensitive,\nmodality-appropriate communications tuned to learner cognitive limits and\ngoals. Next, I highlight the consequences of deploying faulty communication\nsystems and demonstrate the need for context-sensitive communications. Third, I\nuse machine learning (ML) to illuminate personal factors predicting trust in\nAVs, emphasizing the importance of tailoring designs to individual traits and\nconcerns. Together, this dissertation supports the necessity of transparent,\nadaptable, and personalized AV systems that cater to individual needs, goals,\nand contextual demands. By considering the complex system within which human-AV\ninteractions occur, we can deliver valuable insights for designers,\nresearchers, and policymakers. This dissertation also provides a concrete\ndomain to study theories of human-machine joint action and situational\nawareness, and can be used to guide future human-AI interaction research.\n[shortened for arxiv]"}
{"id": "2504.16977", "pdf": "https://arxiv.org/pdf/2504.16977.pdf", "abs": "https://arxiv.org/abs/2504.16977", "title": "Tokenization Matters: Improving Zero-Shot NER for Indic Languages", "authors": ["Priyaranjan Pattnayak", "Hitesh Laxmichand Patel", "Amit Agarwal"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Tokenization is a critical component of Natural Language Processing (NLP),\nespecially for low resource languages, where subword segmentation influences\nvocabulary structure and downstream task accuracy. Although Byte Pair Encoding\n(BPE) is a standard tokenization method in multilingual language models, its\nsuitability for Named Entity Recognition (NER) in low resource Indic languages\nremains underexplored due to its limitations in handling morphological\ncomplexity. In this work, we systematically compare BPE, SentencePiece, and\nCharacter Level tokenization strategies using IndicBERT for NER tasks in low\nresource Indic languages like Assamese, Bengali, Marathi, and Odia, as well as\nextremely low resource Indic languages like Santali, Manipuri, and Sindhi. We\nassess both intrinsic linguistic properties tokenization efficiency, out of\nvocabulary (OOV) rates, and morphological preservation as well as extrinsic\ndownstream performance, including fine tuning and zero shot cross lingual\ntransfer.\n  Our experiments show that SentencePiece is a consistently better performing\napproach than BPE for NER in low resource Indic Languages, particularly in zero\nshot cross lingual settings, as it better preserves entity consistency. While\nBPE provides the most compact tokenization form, it is not capable of\ngeneralization because it misclassifies or even fails to recognize entity\nlabels when tested on unseen languages. In contrast, SentencePiece constitutes\na better linguistic structural preservation model, benefiting extremely low\nresource and morphologically rich Indic languages, such as Santali and\nManipuri, for superior entity recognition, as well as high generalization\nacross scripts, such as Sindhi, written in Arabic. The results point to\nSentencePiece as the more effective tokenization strategy for NER within\nmultilingual and low resource Indic NLP applications."}
{"id": "2504.17171", "pdf": "https://arxiv.org/pdf/2504.17171.pdf", "abs": "https://arxiv.org/abs/2504.17171", "title": "Augmenting Captions with Emotional Cues: An AR Interface for Real-Time Accessible Communication", "authors": ["Sunday David Ubur"], "categories": ["cs.HC"], "comment": null, "summary": "This paper introduces an augmented reality (AR) captioning framework designed\nto support Deaf and Hard of Hearing (DHH) learners in STEM classrooms by\nintegrating non-verbal emotional cues into live transcriptions. Unlike\nconventional captioning systems that offer only plain text, our system fuses\nreal-time speech recognition with affective and visual signal interpretation,\nincluding facial movements, gestures, and vocal tone, to produce emotionally\nenriched captions. These enhanced captions are rendered in an AR interface\ndeveloped with Unity and provide contextual annotations such as speaker tone\nmarkers (e.g., \"concerned\") and gesture indicators (e.g., \"nods\"). The system\nleverages live camera and microphone input, processed through AI models to\ndetect multimodal cues. Findings from preliminary evaluations suggest that this\nAR-based captioning approach significantly enhances comprehension and reduces\ncognitive effort compared to standard captions. Our work emphasizes the\npotential of immersive environments for inclusive, emotion-aware educational\naccessibility."}
{"id": "2504.17025", "pdf": "https://arxiv.org/pdf/2504.17025.pdf", "abs": "https://arxiv.org/abs/2504.17025", "title": "Optimizing LLMs for Italian: Reducing Token Fertility and Enhancing Efficiency Through Vocabulary Adaptation", "authors": ["Luca Moroni", "Giovanni Puccetti", "Pere-Lluis Huguet Cabot", "Andrei Stefan Bejgu", "Edoardo Barba", "Alessio Miaschi", "Felice Dell'Orletta", "Andrea Esuli", "Roberto Navigli"], "categories": ["cs.CL"], "comment": null, "summary": "The number of pretrained Large Language Models (LLMs) is increasing steadily,\nthough the majority are designed predominantly for the English language. While\nstate-of-the-art LLMs can handle other languages, due to language contamination\nor some degree of multilingual pretraining data, they are not optimized for\nnon-English languages, leading to inefficient encoding (high token \"fertility\")\nand slower inference speed. In this work, we thoroughly compare a variety of\nvocabulary adaptation techniques for optimizing English LLMs for the Italian\nlanguage, and put forward Semantic Alignment Vocabulary Adaptation (SAVA), a\nnovel method that leverages neural mapping for vocabulary substitution. SAVA\nachieves competitive performance across multiple downstream tasks, enhancing\ngrounded alignment strategies. We adapt two LLMs: Mistral-7b-v0.1, reducing\ntoken fertility by 25\\%, and Llama-3.1-8B, optimizing the vocabulary and\nreducing the number of parameters by 1 billion. We show that, following the\nadaptation of the vocabulary, these models can recover their performance with a\nrelatively limited stage of continual training on the target language. Finally,\nwe test the capabilities of the adapted models on various multi-choice and\ngenerative tasks."}
{"id": "2504.17173", "pdf": "https://arxiv.org/pdf/2504.17173.pdf", "abs": "https://arxiv.org/abs/2504.17173", "title": "Lessons from Deploying Learning-based CSI Localization on a Large-Scale ISAC Platform", "authors": ["Tianyu Zhang", "Dongheng Zhang", "Ruixu Geng", "Xuecheng Xie", "Shuai Yang", "Yan Chen"], "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "In recent years, Channel State Information (CSI), recognized for its\nfine-grained spatial characteristics, has attracted increasing attention in\nWiFi-based indoor localization. However, despite its potential, CSI-based\napproaches have yet to achieve the same level of deployment scale and\ncommercialization as those based on Received Signal Strength Indicator (RSSI).\nA key limitation lies in the fact that most existing CSI-based systems are\ndeveloped and evaluated in controlled, small-scale environments, limiting their\ngeneralizability. To bridge this gap, we explore the deployment of a\nlarge-scale CSI-based localization system involving over 400 Access Points\n(APs) in a real-world building under the Integrated Sensing and Communication\n(ISAC) paradigm. We highlight two critical yet often overlooked factors: the\nunderutilization of unlabeled data and the inherent heterogeneity of CSI\nmeasurements. To address these challenges, we propose a novel CSI-based\nlearning framework for WiFi localization, tailored for large-scale ISAC\ndeployments on the server side. Specifically, we employ a novel graph-based\nstructure to model heterogeneous CSI data and reduce redundancy. We further\ndesign a pretext pretraining task that incorporates spatial and temporal priors\nto effectively leverage large-scale unlabeled CSI data. Complementarily, we\nintroduce a confidence-aware fine-tuning strategy to enhance the robustness of\nlocalization results. In a leave-one-smartphone-out experiment spanning five\nfloors and 25, 600 m2, we achieve a median localization error of 2.17 meters\nand a floor accuracy of 99.49%. This performance corresponds to an 18.7%\nreduction in mean absolute error (MAE) compared to the best-performing\nbaseline."}
{"id": "2504.17052", "pdf": "https://arxiv.org/pdf/2504.17052.pdf", "abs": "https://arxiv.org/abs/2504.17052", "title": "Do Words Reflect Beliefs? Evaluating Belief Depth in Large Language Models", "authors": ["Shariar Kabir", "Kevin Esterling", "Yue Dong"], "categories": ["cs.CL"], "comment": "20 pages, 9 figures", "summary": "Large Language Models (LLMs) are increasingly shaping political discourse,\nyet their responses often display inconsistency when subjected to scrutiny.\nWhile prior research has primarily categorized LLM outputs as left- or\nright-leaning to assess their political stances, a critical question remains:\nDo these responses reflect genuine internal beliefs or merely surface-level\nalignment with training data? To address this, we propose a novel framework for\nevaluating belief depth by analyzing (1) argumentative consistency and (2)\nuncertainty quantification. We evaluate 12 LLMs on 19 economic policies from\nthe Political Compass Test, challenging their belief stability with both\nsupportive and opposing arguments. Our analysis reveals that LLMs exhibit\ntopic-specific belief stability rather than a uniform ideological stance.\nNotably, up to 95% of left-leaning models' responses and 89% of right-leaning\nmodels' responses remain consistent under the challenge, enabling semantic\nentropy to achieve high accuracy (AUROC=0.78), effectively distinguishing\nbetween surface-level alignment from genuine belief. These findings call into\nquestion the assumption that LLMs maintain stable, human-like political\nideologies, emphasizing the importance of conducting topic-specific reliability\nassessments for real-world applications."}
{"id": "2504.17204", "pdf": "https://arxiv.org/pdf/2504.17204.pdf", "abs": "https://arxiv.org/abs/2504.17204", "title": "Factually: Exploring Wearable Fact-Checking for Augmented Truth Discernment", "authors": ["Chitralekha Gupta", "Hanjun Wu", "Praveen Sasikumar", "Shreyas Sridhar", "Priambudi Bagaskara", "Suranga Nanayakkara"], "categories": ["cs.HC", "cs.ET"], "comment": "Presented at the 2025 ACM Workshop on Human-AI Interaction for\n  Augmented Reasoning, Report Number: CHI25-WS-AUGMENTED-REASONING", "summary": "Wearable devices are transforming human capabilities by seamlessly augmenting\ncognitive functions. In this position paper, we propose a voice-based,\ninteractive learning companion designed to amplify and extend cognitive\nabilities through informal learning. Our vision is threefold: (1) to enable\nusers to discover new knowledge on-the-go through contextual interactive\nquizzes, fostering critical thinking and mindfulness, (2) to proactively detect\nmisinformation, empowering users to critically assess information in real time,\nand (3) to provide spoken language correction and prompting hints for second\nlanguage learning and effective communication. As an initial step toward this\nvision, we present Factually - a proactive, wearable fact-checking system\nintegrated into devices like smartwatches or rings. Factually discreetly alerts\nusers to potential falsehoods via vibrotactile feedback, helping them assess\ninformation critically. We demonstrate its utility through three illustrative\nscenarios, highlighting its potential to extend cognitive abilities for\nreal-time misinformation detection. Early qualitative feedback suggests that\nFactually can enhance users' fact-checking capabilities, offering both\npractical and experiential benefits."}
{"id": "2504.17075", "pdf": "https://arxiv.org/pdf/2504.17075.pdf", "abs": "https://arxiv.org/abs/2504.17075", "title": "Agree to Disagree? A Meta-Evaluation of LLM Misgendering", "authors": ["Arjun Subramonian", "Vagrant Gautam", "Preethi Seshadri", "Dietrich Klakow", "Kai-Wei Chang", "Yizhou Sun"], "categories": ["cs.CL", "cs.CY"], "comment": "Work in progress", "summary": "Numerous methods have been proposed to measure LLM misgendering, including\nprobability-based evaluations (e.g., automatically with templatic sentences)\nand generation-based evaluations (e.g., with automatic heuristics or human\nvalidation). However, it has gone unexamined whether these evaluation methods\nhave convergent validity, that is, whether their results align. Therefore, we\nconduct a systematic meta-evaluation of these methods across three existing\ndatasets for LLM misgendering. We propose a method to transform each dataset to\nenable parallel probability- and generation-based evaluation. Then, by\nautomatically evaluating a suite of 6 models from 3 families, we find that\nthese methods can disagree with each other at the instance, dataset, and model\nlevels, conflicting on 20.2% of evaluation instances. Finally, with a human\nevaluation of 2400 LLM generations, we show that misgendering behaviour is\ncomplex and goes far beyond pronouns, which automatic evaluations are not\ncurrently designed to capture, suggesting essential disagreement with human\nevaluations. Based on our findings, we provide recommendations for future\nevaluations of LLM misgendering. Our results are also more widely relevant, as\nthey call into question broader methodological conventions in LLM evaluation,\nwhich often assume that different evaluation methods agree."}
{"id": "2504.17267", "pdf": "https://arxiv.org/pdf/2504.17267.pdf", "abs": "https://arxiv.org/abs/2504.17267", "title": "MV-Crafter: An Intelligent System for Music-guided Video Generation", "authors": ["Chuer Chen", "Shengqi Dang", "Yuqi Liu", "Nanxuan Zhao", "Yang Shi", "Nan Cao"], "categories": ["cs.HC", "cs.MM"], "comment": null, "summary": "Music videos, as a prevalent form of multimedia entertainment, deliver\nengaging audio-visual experiences to audiences and have gained immense\npopularity among singers and fans. Creators can express their interpretations\nof music naturally through visual elements. However, the creation process of\nmusic video demands proficiency in script design, video shooting, and\nmusic-video synchronization, posing significant challenges for\nnon-professionals. Previous work has designed automated music video generation\nframeworks. However, they suffer from complexity in input and poor output\nquality. In response, we present MV-Crafter, a system capable of producing\nhigh-quality music videos with synchronized music-video rhythm and style. Our\napproach involves three technical modules that simulate the human creation\nprocess: the script generation module, video generation module, and music-video\nsynchronization module. MV-Crafter leverages a large language model to generate\nscripts considering the musical semantics. To address the challenge of\nsynchronizing short video clips with music of varying lengths, we propose a\ndynamic beat matching algorithm and visual envelope-induced warping method to\nensure precise, monotonic music-video synchronization. Besides, we design a\nuser-friendly interface to simplify the creation process with intuitive editing\nfeatures. Extensive experiments have demonstrated that MV-Crafter provides an\neffective solution for improving the quality of generated music videos."}
{"id": "2504.17083", "pdf": "https://arxiv.org/pdf/2504.17083.pdf", "abs": "https://arxiv.org/abs/2504.17083", "title": "How Individual Traits and Language Styles Shape Preferences In Open-ended User-LLM Interaction: A Preliminary Study", "authors": ["Rendi Chevi", "Kentaro Inui", "Thamar Solorio", "Alham Fikri Aji"], "categories": ["cs.CL"], "comment": "Accepted at GenAICHI 2025 @ ACM CHI 2025", "summary": "What makes an interaction with the LLM more preferable for the user? While it\nis intuitive to assume that information accuracy in the LLM's responses would\nbe one of the influential variables, recent studies have found that inaccurate\nLLM's responses could still be preferable when they are perceived to be more\nauthoritative, certain, well-articulated, or simply verbose. These variables\ninterestingly fall under the broader category of language style, implying that\nthe style in the LLM's responses might meaningfully influence users'\npreferences. This hypothesized dynamic could have double-edged consequences:\nenhancing the overall user experience while simultaneously increasing their\nsusceptibility to risks such as LLM's misinformation or hallucinations. In this\nshort paper, we present our preliminary studies in exploring this subject.\nThrough a series of exploratory and experimental user studies, we found that\nLLM's language style does indeed influence user's preferences, but how and\nwhich language styles influence the preference varied across different user\npopulations, and more interestingly, moderated by the user's very own\nindividual traits. As a preliminary work, the findings in our studies should be\ninterpreted with caution, particularly given the limitations in our samples,\nwhich still need wider demographic diversity and larger sample sizes. Our\nfuture directions will first aim to address these limitations, which would\nenable a more comprehensive joint effect analysis between the language style,\nindividual traits, and preferences, and further investigate the potential\ncausal relationship between and beyond these variables."}
{"id": "2504.17331", "pdf": "https://arxiv.org/pdf/2504.17331.pdf", "abs": "https://arxiv.org/abs/2504.17331", "title": "Exploring Context-aware and LLM-driven Locomotion for Immersive Virtual Reality", "authors": ["Süleyman Özdel", "Kadir Burak Buldu", "Enkelejda Kasneci", "Efe Bozkir"], "categories": ["cs.HC", "cs.AI"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Locomotion plays a crucial role in shaping the user experience within virtual\nreality environments. In particular, hands-free locomotion offers a valuable\nalternative by supporting accessibility and freeing users from reliance on\nhandheld controllers. To this end, traditional speech-based methods often\ndepend on rigid command sets, limiting the naturalness and flexibility of\ninteraction. In this study, we propose a novel locomotion technique powered by\nlarge language models (LLMs), which allows users to navigate virtual\nenvironments using natural language with contextual awareness. We evaluate\nthree locomotion methods: controller-based teleportation, voice-based steering,\nand our language model-driven approach. Our evaluation measures include\neye-tracking data analysis, including explainable machine learning through SHAP\nanalysis as well as standardized questionnaires for usability, presence,\ncybersickness, and cognitive load to examine user attention and engagement. Our\nfindings indicate that the LLM-driven locomotion possesses comparable\nusability, presence, and cybersickness scores to established methods like\nteleportation, demonstrating its novel potential as a comfortable, natural\nlanguage-based, hands-free alternative. In addition, it enhances user attention\nwithin the virtual environment, suggesting greater engagement. Complementary to\nthese findings, SHAP analysis revealed that fixation, saccade, and\npupil-related features vary across techniques, indicating distinct patterns of\nvisual attention and cognitive processing. Overall, we state that our method\ncan facilitate hands-free locomotion in virtual spaces, especially in\nsupporting accessibility."}
{"id": "2504.17091", "pdf": "https://arxiv.org/pdf/2504.17091.pdf", "abs": "https://arxiv.org/abs/2504.17091", "title": "Co-CoT: A Prompt-Based Framework for Collaborative Chain-of-Thought Reasoning", "authors": ["Seunghyun Yoo"], "categories": ["cs.CL", "68T05"], "comment": "5 page", "summary": "Due to the proliferation of short-form content and the rapid adoption of AI,\nopportunities for deep, reflective thinking have significantly diminished,\nundermining users' critical thinking and reducing engagement with the reasoning\nbehind AI-generated outputs. To address this issue, we propose an Interactive\nChain-of-Thought (CoT) Framework that enhances human-centered explainability\nand responsible AI usage by making the model's inference process transparent,\nmodular, and user-editable. The framework decomposes reasoning into clearly\ndefined blocks that users can inspect, modify, and re-execute, encouraging\nactive cognitive engagement rather than passive consumption. It further\nintegrates a lightweight edit-adaptation mechanism inspired by preference\nlearning, allowing the system to align with diverse cognitive styles and user\nintentions. Ethical transparency is ensured through explicit metadata\ndisclosure, built-in bias checkpoint functionality, and privacy-preserving\nsafeguards. This work outlines the design principles and architecture necessary\nto promote critical engagement, responsible interaction, and inclusive\nadaptation in AI systems aimed at addressing complex societal challenges."}
{"id": "2504.17334", "pdf": "https://arxiv.org/pdf/2504.17334.pdf", "abs": "https://arxiv.org/abs/2504.17334", "title": "DataScout: Automatic Data Fact Retrieval for Statement Augmentation with an LLM-Based Agent", "authors": ["Chuer Chen", "Yuqi Liu", "Danqing Shi", "Shixiong Cao", "Nan Cao"], "categories": ["cs.HC", "cs.IR"], "comment": null, "summary": "A data story typically integrates data facts from multiple perspectives and\nstances to construct a comprehensive and objective narrative. However,\nretrieving these facts demands time for data search and challenges the\ncreator's analytical skills. In this work, we introduce DataScout, an\ninteractive system that automatically performs reasoning and stance-based data\nfacts retrieval to augment the user's statement. Particularly, DataScout\nleverages an LLM-based agent to construct a retrieval tree, enabling\ncollaborative control of its expansion between users and the agent. The\ninterface visualizes the retrieval tree as a mind map that eases users to\nintuitively steer the retrieval direction and effectively engage in reasoning\nand analysis. We evaluate the proposed system through case studies and in-depth\nexpert interviews. Our evaluation demonstrates that DataScout can effectively\nretrieve multifaceted data facts from different stances, helping users verify\ntheir statements and enhance the credibility of their stories."}
{"id": "2504.17119", "pdf": "https://arxiv.org/pdf/2504.17119.pdf", "abs": "https://arxiv.org/abs/2504.17119", "title": "The Rise of Small Language Models in Healthcare: A Comprehensive Survey", "authors": ["Muskan Garg", "Shaina Raza", "Shebuti Rayana", "Xingyi Liu", "Sunghwan Sohn"], "categories": ["cs.CL", "cs.AI"], "comment": "35 pages, 7 tables, 5 figures", "summary": "Despite substantial progress in healthcare applications driven by large\nlanguage models (LLMs), growing concerns around data privacy, and limited\nresources; the small language models (SLMs) offer a scalable and clinically\nviable solution for efficient performance in resource-constrained environments\nfor next-generation healthcare informatics. Our comprehensive survey presents a\ntaxonomic framework to identify and categorize them for healthcare\nprofessionals and informaticians. The timeline of healthcare SLM contributions\nestablishes a foundational framework for analyzing models across three\ndimensions: NLP tasks, stakeholder roles, and the continuum of care. We present\na taxonomic framework to identify the architectural foundations for building\nmodels from scratch; adapting SLMs to clinical precision through prompting,\ninstruction fine-tuning, and reasoning; and accessibility and sustainability\nthrough compression techniques. Our primary objective is to offer a\ncomprehensive survey for healthcare professionals, introducing recent\ninnovations in model optimization and equipping them with curated resources to\nsupport future research and development in the field. Aiming to showcase the\ngroundbreaking advancements in SLMs for healthcare, we present a comprehensive\ncompilation of experimental results across widely studied NLP tasks in\nhealthcare to highlight the transformative potential of SLMs in healthcare. The\nupdated repository is available at Github"}
{"id": "2504.17352", "pdf": "https://arxiv.org/pdf/2504.17352.pdf", "abs": "https://arxiv.org/abs/2504.17352", "title": "The Riemannian Means Field Classifier for EEG-Based BCI Data", "authors": ["Anton Andreev", "Grégoire Cattan", "Marco Congedo"], "categories": ["cs.HC", "eess.SP"], "comment": null, "summary": "A substantial amount of research has demonstrated the robustness and accuracy\nof the Riemannian minimum distance to mean (MDM) classifier for all kinds of\nEEG-based brain--computer interfaces (BCIs). This classifier is simple, fully\ndeterministic, robust to noise, computationally efficient, and prone to\ntransfer learning. Its training is very simple, requiring just the computation\nof a geometric mean of a symmetric positive-definite (SPD) matrix per class. We\npropose an improvement of the MDM involving a number of power means of SPD\nmatrices instead of the sole geometric mean. By the analysis of 20 public\ndatabases, 10 for the motor-imagery BCI paradigm and 10 for the P300 BCI\nparadigm, comprising 587 individuals in total, we show that the proposed\nclassifier clearly outperforms the MDM, approaching the state-of-the art in\nterms of performance while retaining the simplicity and the deterministic\nbehavior. In order to promote reproducible research, our code will be released\nas open source."}
{"id": "2504.17130", "pdf": "https://arxiv.org/pdf/2504.17130.pdf", "abs": "https://arxiv.org/abs/2504.17130", "title": "Steering the CensorShip: Uncovering Representation Vectors for LLM \"Thought\" Control", "authors": ["Hannah Cyberey", "David Evans"], "categories": ["cs.CL", "cs.CR", "cs.CY"], "comment": null, "summary": "Large language models (LLMs) have transformed the way we access information.\nThese models are often tuned to refuse to comply with requests that are\nconsidered harmful and to produce responses that better align with the\npreferences of those who control the models. To understand how this\n\"censorship\" works. We use representation engineering techniques to study\nopen-weights safety-tuned models. We present a method for finding a\nrefusal--compliance vector that detects and controls the level of censorship in\nmodel outputs. We also analyze recent reasoning LLMs, distilled from\nDeepSeek-R1, and uncover an additional dimension of censorship through \"thought\nsuppression\". We show a similar approach can be used to find a vector that\nsuppresses the model's reasoning process, allowing us to remove censorship by\napplying the negative multiples of this vector"}
{"id": "2504.17663", "pdf": "https://arxiv.org/pdf/2504.17663.pdf", "abs": "https://arxiv.org/abs/2504.17663", "title": "The Malicious Technical Ecosystem: Exposing Limitations in Technical Governance of AI-Generated Non-Consensual Intimate Images of Adults", "authors": ["Michelle L. Ding", "Harini Suresh"], "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.LG"], "comment": null, "summary": "In this paper, we adopt a survivor-centered approach to locate and dissect\nthe role of sociotechnical AI governance in preventing AI-Generated\nNon-Consensual Intimate Images (AIG-NCII) of adults, colloquially known as\n\"deep fake pornography.\" We identify a \"malicious technical ecosystem\" or\n\"MTE,\" comprising of open-source face-swapping models and nearly 200\n\"nudifying\" software programs that allow non-technical users to create AIG-NCII\nwithin minutes. Then, using the National Institute of Standards and Technology\n(NIST) AI 100-4 report as a reflection of current synthetic content governance\nmethods, we show how the current landscape of practices fails to effectively\nregulate the MTE for adult AIG-NCII, as well as flawed assumptions explaining\nthese gaps."}
{"id": "2504.17137", "pdf": "https://arxiv.org/pdf/2504.17137.pdf", "abs": "https://arxiv.org/abs/2504.17137", "title": "MIRAGE: A Metric-Intensive Benchmark for Retrieval-Augmented Generation Evaluation", "authors": ["Chanhee Park", "Hyeonseok Moon", "Chanjun Park", "Heuiseok Lim"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to NAACL2025 Findings", "summary": "Retrieval-Augmented Generation (RAG) has gained prominence as an effective\nmethod for enhancing the generative capabilities of Large Language Models\n(LLMs) through the incorporation of external knowledge. However, the evaluation\nof RAG systems remains a challenge, due to the intricate interplay between\nretrieval and generation components. This limitation has resulted in a scarcity\nof benchmarks that facilitate a detailed, component-specific assessment. In\nthis work, we present MIRAGE, a Question Answering dataset specifically\ndesigned for RAG evaluation. MIRAGE consists of 7,560 curated instances mapped\nto a retrieval pool of 37,800 entries, enabling an efficient and precise\nevaluation of both retrieval and generation tasks. We also introduce novel\nevaluation metrics aimed at measuring RAG adaptability, encompassing dimensions\nsuch as noise vulnerability, context acceptability, context insensitivity, and\ncontext misinterpretation. Through comprehensive experiments across various\nretriever-LLM configurations, we provide new insights into the optimal\nalignment of model pairs and the nuanced dynamics within RAG systems. The\ndataset and evaluation code are publicly available, allowing for seamless\nintegration and customization in diverse research settings\\footnote{The MIRAGE\ncode and data are available at https://github.com/nlpai-lab/MIRAGE."}
{"id": "2504.17677", "pdf": "https://arxiv.org/pdf/2504.17677.pdf", "abs": "https://arxiv.org/abs/2504.17677", "title": "INSIGHT: Bridging the Student-Teacher Gap in Times of Large Language Models", "authors": ["Jarne Thys", "Sebe Vanbrabant", "Davy Vanacken", "Gustavo Rovelo Ruiz"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "The rise of AI, especially Large Language Models, presents challenges and\nopportunities to integrate such technology into the classroom. AI has the\npotential to revolutionize education by helping teaching staff with various\ntasks, such as personalizing their teaching methods, but it also raises\nconcerns, for example, about the degradation of student-teacher interactions\nand user privacy. This paper introduces INSIGHT, a proof of concept to combine\nvarious AI tools to assist teaching staff and students in the process of\nsolving exercises. INSIGHT has a modular design that allows it to be integrated\ninto various higher education courses. We analyze students' questions to an LLM\nby extracting keywords, which we use to dynamically build an FAQ from students'\nquestions and provide new insights for the teaching staff to use for more\npersonalized face-to-face support. Future work could build upon INSIGHT by\nusing the collected data to provide adaptive learning and adjust content based\non student progress and learning styles to offer a more interactive and\ninclusive learning experience."}
{"id": "2504.17192", "pdf": "https://arxiv.org/pdf/2504.17192.pdf", "abs": "https://arxiv.org/abs/2504.17192", "title": "Paper2Code: Automating Code Generation from Scientific Papers in Machine Learning", "authors": ["Minju Seo", "Jinheon Baek", "Seongyun Lee", "Sung Ju Hwang"], "categories": ["cs.CL"], "comment": null, "summary": "Despite the rapid growth of machine learning research, corresponding code\nimplementations are often unavailable, making it slow and labor-intensive for\nresearchers to reproduce results and build upon prior work. In the meantime,\nrecent Large Language Models (LLMs) excel at understanding scientific documents\nand generating high-quality code. Inspired by this, we introduce PaperCoder, a\nmulti-agent LLM framework that transforms machine learning papers into\nfunctional code repositories. PaperCoder operates in three stages: planning,\nwhere it constructs a high-level roadmap, designs the system architecture with\ndiagrams, identifies file dependencies, and generates configuration files;\nanalysis, which focuses on interpreting implementation-specific details; and\ngeneration, where modular, dependency-aware code is produced. Moreover, each\nphase is instantiated through a set of specialized agents designed to\ncollaborate effectively across the pipeline. We then evaluate PaperCoder on\ngenerating code implementations from machine learning papers based on both\nmodel-based and human evaluations, specifically from the original paper\nauthors, with author-released repositories as ground truth if available. Our\nresults demonstrate the effectiveness of PaperCoder in creating high-quality,\nfaithful implementations. Furthermore, it consistently shows strengths in the\nrecently released PaperBench benchmark, surpassing strong baselines by\nsubstantial margins."}
{"id": "2504.17697", "pdf": "https://arxiv.org/pdf/2504.17697.pdf", "abs": "https://arxiv.org/abs/2504.17697", "title": "'The Boring and the Tedious': Invisible Labour in India's Gig-Economy", "authors": ["Pratyay Suvarnapathaki", "Viral Shah", "Saarthak Negi", "Nimmi Rangaswamy"], "categories": ["cs.HC"], "comment": "6 pages, 2 figures", "summary": "India's gig-based food delivery platforms, such as Swiggy and Zomato, provide\ncrucial income to marginalised communities but also entrench workers in cycles\nof invisible labour. Through 14 semi-structured interviews, we analyse waiting\ntime and repetitive UI itneractions as key burdens that contribute to 'digital\ndiscomfort' for gig based food delivery agents. We find that workers employ\ncreative strategies to navigate algorithmic management, yet remain constrained\nby platform-side 'gamification' and system opacity. We propose worker-centered\nGUI automation as a potential intervention to reduce friction while preserving\nagency. In conclusion, this position paper argues for rethinking HCI approaches\nin the Global South to prioritise worker autonomy over efficiency-driven design\noptimisations."}
{"id": "2504.17200", "pdf": "https://arxiv.org/pdf/2504.17200.pdf", "abs": "https://arxiv.org/abs/2504.17200", "title": "A RAG-Based Multi-Agent LLM System for Natural Hazard Resilience and Adaptation", "authors": ["Yangxinyu Xie", "Bowen Jiang", "Tanwi Mallick", "Joshua David Bergerson", "John K. Hutchison", "Duane R. Verner", "Jordan Branham", "M. Ross Alexander", "Robert B. Ross", "Yan Feng", "Leslie-Anne Levy", "Weijie Su", "Camillo J. Taylor"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are a transformational capability at the\nfrontier of artificial intelligence and machine learning that can support\ndecision-makers in addressing pressing societal challenges such as extreme\nnatural hazard events. As generalized models, LLMs often struggle to provide\ncontext-specific information, particularly in areas requiring specialized\nknowledge. In this work we propose a retrieval-augmented generation (RAG)-based\nmulti-agent LLM system to support analysis and decision-making in the context\nof natural hazards and extreme weather events. As a proof of concept, we\npresent WildfireGPT, a specialized system focused on wildfire hazards. The\narchitecture employs a user-centered, multi-agent design to deliver tailored\nrisk insights across diverse stakeholder groups. By integrating natural hazard\nand extreme weather projection data, observational datasets, and scientific\nliterature through an RAG framework, the system ensures both the accuracy and\ncontextual relevance of the information it provides. Evaluation across ten\nexpert-led case studies demonstrates that WildfireGPT significantly outperforms\nexisting LLM-based solutions for decision support."}
{"id": "2504.17705", "pdf": "https://arxiv.org/pdf/2504.17705.pdf", "abs": "https://arxiv.org/abs/2504.17705", "title": "LUIDA: Large-scale Unified Infrastructure for Digital Assessments based on Commercial Metaverse Platform", "authors": ["Yong-Hao Hu", "Sotaro Yokoi", "Yuji Hatada", "Yuichi Hiroi", "Takuji Narumi", "Takefumi Hiraki"], "categories": ["cs.HC"], "comment": null, "summary": "Online experiments using metaverse platforms have gained significant traction\nin Human-Computer Interaction and Virtual Reality (VR) research. However,\ncurrent research workflows are highly fragmented, as researchers must use\nseparate tools for system implementation, participant recruitment, experiment\nexecution, and data collection, reducing consistency and increasing workload.\nWe present LUIDA (Large-scale Unified Infrastructure for Digital Assessments),\na metaverse-based framework that integrates these fragmented processes. LUIDA\nautomatically allocates interconnected virtual environments for parallel\nexperiment execution and provides implementation templates adaptable to various\nVR research domains, requiring minimal metaverse development expertise. Our\nevaluation included two studies using a prototype built on Cluster, the\ncommercial metaverse platform. First, VR researchers using LUIDA to develop and\nrun experiments reported high usability scores (SUS: 73.75) and moderate\nworkload (NASA-TLX: 24.11) for overall usage, with interviews confirming\nstreamlined workflows compared to traditional laboratory experiments. Second,\nwe conducted three replicated experiments with public Cluster users, each\nrecruiting approximately 200 participants within one week. These experiments\nproduced results that closely matched the original studies, validating the\nexperimental integrity of LUIDA across research domains. After technical\nrefinements, we plan to release LUIDA as an open platform, providing a\nstandardized protocol to improve research efficiency and experimental\nreproducibility in VR studies."}
{"id": "2504.17220", "pdf": "https://arxiv.org/pdf/2504.17220.pdf", "abs": "https://arxiv.org/abs/2504.17220", "title": "Does Knowledge Distillation Matter for Large Language Model based Bundle Generation?", "authors": ["Kaidong Feng", "Zhu Sun", "Jie Yang", "Hui Fang", "Xinghua Qu", "Wenyuan Liu"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "LLMs are increasingly explored for bundle generation, thanks to their\nreasoning capabilities and knowledge. However, deploying large-scale LLMs\nintroduces significant efficiency challenges, primarily high computational\ncosts during fine-tuning and inference due to their massive parameterization.\nKnowledge distillation (KD) offers a promising solution, transferring expertise\nfrom large teacher models to compact student models. This study systematically\ninvestigates knowledge distillation approaches for bundle generation, aiming to\nminimize computational demands while preserving performance. We explore three\ncritical research questions: (1) how does the format of KD impact bundle\ngeneration performance? (2) to what extent does the quantity of distilled\nknowledge influence performance? and (3) how do different ways of utilizing the\ndistilled knowledge affect performance? We propose a comprehensive KD framework\nthat (i) progressively extracts knowledge (patterns, rules, deep thoughts);\n(ii) captures varying quantities of distilled knowledge through different\nstrategies; and (iii) exploits complementary LLM adaptation techniques\n(in-context learning, supervised fine-tuning, combination) to leverage\ndistilled knowledge in small student models for domain-specific adaptation and\nenhanced efficiency. Extensive experiments provide valuable insights into how\nknowledge format, quantity, and utilization methodologies collectively shape\nLLM-based bundle generation performance, exhibiting KD's significant potential\nfor more efficient yet effective LLM-based bundle generation."}
{"id": "2504.17113", "pdf": "https://arxiv.org/pdf/2504.17113.pdf", "abs": "https://arxiv.org/abs/2504.17113", "title": "Cybernetic Governance in a Coliving House", "authors": ["Daniel Kronovet", "Seth Frey", "Joseph DeSimone"], "categories": ["cs.CY", "cs.HC", "econ.GN", "q-fin.EC"], "comment": "19 pages, 5 figures, earlier working version at\n  https://ssrn.com/abstract=4856267", "summary": "We report an 18-month field experiment in distributed digital institutions: a\nnine-bedroom Los Angeles coliving house that runs without managers, while\nsustaining 98% occupancy and below-market rents.\n  Drawing on Elinor Ostrom's commons theory, we outline design principles and\nthree digital mechanisms that form the institutional core: 1) A\ncontinuous-auction chore scheduler turns regenerative labor into a time-indexed\npoints market; residents meet a 100-point monthly obligation by claiming tasks\nwhose value rises linearly with neglect. 2) A pairwise-preference layer lets\nparticipants asynchronously reprioritize tasks, translating meta-governance\ninto low-cognition spot inputs. 3) A symbolic \"hearts\" ledger tracks norm\ncompliance through automated enforcement, lightweight challenges, and\npeer-awarded karma. Together, these mechanisms operationalize cybernetic\nprinciples--human sensing, machine bookkeeping, real-time feedback--while\nminimizing dependence on privileged roles.\n  Our exploratory data (567 chore claims, 255 heart events, and 551 group\npurchases) show that such tooling can sustain reliable commons governance\nwithout continuous leadership, offering a transferable design palette for\nonline communities, coliving houses, and other digitally mediated collectives."}
{"id": "2504.17238", "pdf": "https://arxiv.org/pdf/2504.17238.pdf", "abs": "https://arxiv.org/abs/2504.17238", "title": "Crisp: Cognitive Restructuring of Negative Thoughts through Multi-turn Supportive Dialogues", "authors": ["Jinfeng Zhou", "Yuxuan Chen", "Jianing Yin", "Yongkang Huang", "Yihan Shi", "Xikun Zhang", "Libiao Peng", "Rongsheng Zhang", "Tangjie Lv", "Zhipeng Hu", "Hongning Wang", "Minlie Huang"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Cognitive Restructuring (CR) is a psychotherapeutic process aimed at\nidentifying and restructuring an individual's negative thoughts, arising from\nmental health challenges, into more helpful and positive ones via multi-turn\ndialogues. Clinician shortage and stigma urge the development of human-LLM\ninteractive psychotherapy for CR. Yet, existing efforts implement CR via simple\ntext rewriting, fixed-pattern dialogues, or a one-shot CR workflow, failing to\nalign with the psychotherapeutic process for effective CR. To address this gap,\nwe propose CRDial, a novel framework for CR, which creates multi-turn dialogues\nwith specifically designed identification and restructuring stages of negative\nthoughts, integrates sentence-level supportive conversation strategies, and\nadopts a multi-channel loop mechanism to enable iterative CR. With CRDial, we\ndistill Crisp, a large-scale and high-quality bilingual dialogue dataset, from\nLLM. We then train Crispers, Crisp-based conversational LLMs for CR, at 7B and\n14B scales. Extensive human studies show the superiority of Crispers in\npointwise, pairwise, and intervention evaluations."}
{"id": "2504.17117", "pdf": "https://arxiv.org/pdf/2504.17117.pdf", "abs": "https://arxiv.org/abs/2504.17117", "title": "AI for Accessible Education: Personalized Audio-Based Learning for Blind Students", "authors": ["Crystal Yang", "Paul Taele"], "categories": ["cs.CY", "cs.HC", "H.5.2; I.2.6; K.4.2; K.3.1"], "comment": "4 pages, CHI 2025 Workshop on Augmented Educators and AI: Shaping the\n  Future of Human and AI Cooperation in Learning", "summary": "Blind and visually impaired (BVI) students face significant challenges in\ntraditional educational settings. While screen readers and braille materials\noffer some accessibility, they often lack interactivity and real-time\nadaptability to individual learning needs. This paper presents Audemy, an\nAI-powered audio-based learning platform designed to provide personalized,\naccessible, and engaging educational experiences for BVI students. Audemy uses\nadaptive learning techniques to customize content based on student accuracy,\npacing preferences, and engagement patterns. The platform has been iteratively\ndeveloped with input from over 20 educators specializing in accessibility and\ncurrently serves over 2,000 BVI students. Educator insights show key\nconsiderations for accessible AI, including the importance of engagement,\nintuitive design, compatibility with existing assistive technologies, and the\nrole of positive reinforcement in maintaining student motivation. Beyond\naccessibility, this paper explores the ethical implications of AI in education,\nemphasizing data privacy, security, and transparency. Audemy demonstrates how\nAI can empower BVI students with personalized and equitable learning\nopportunities, advancing the broader goal of inclusive education."}
{"id": "2504.17252", "pdf": "https://arxiv.org/pdf/2504.17252.pdf", "abs": "https://arxiv.org/abs/2504.17252", "title": "Low-Resource Neural Machine Translation Using Recurrent Neural Networks and Transfer Learning: A Case Study on English-to-Igbo", "authors": ["Ocheme Anthony Ekle", "Biswarup Das"], "categories": ["cs.CL", "cs.LG", "68T50, 68T01", "I.2.7; I.2.1"], "comment": "25 pages, 14 combined figures (19 total), includes horizontal\n  layouts. Submitted to arXiv for open access", "summary": "In this study, we develop Neural Machine Translation (NMT) and\nTransformer-based transfer learning models for English-to-Igbo translation - a\nlow-resource African language spoken by over 40 million people across Nigeria\nand West Africa. Our models are trained on a curated and benchmarked dataset\ncompiled from Bible corpora, local news, Wikipedia articles, and Common Crawl,\nall verified by native language experts. We leverage Recurrent Neural Network\n(RNN) architectures, including Long Short-Term Memory (LSTM) and Gated\nRecurrent Units (GRU), enhanced with attention mechanisms to improve\ntranslation accuracy. To further enhance performance, we apply transfer\nlearning using MarianNMT pre-trained models within the SimpleTransformers\nframework. Our RNN-based system achieves competitive results, closely matching\nexisting English-Igbo benchmarks. With transfer learning, we observe a\nperformance gain of +4.83 BLEU points, reaching an estimated translation\naccuracy of 70%. These findings highlight the effectiveness of combining RNNs\nwith transfer learning to address the performance gap in low-resource language\ntranslation tasks."}
{"id": "2504.17238", "pdf": "https://arxiv.org/pdf/2504.17238.pdf", "abs": "https://arxiv.org/abs/2504.17238", "title": "Crisp: Cognitive Restructuring of Negative Thoughts through Multi-turn Supportive Dialogues", "authors": ["Jinfeng Zhou", "Yuxuan Chen", "Jianing Yin", "Yongkang Huang", "Yihan Shi", "Xikun Zhang", "Libiao Peng", "Rongsheng Zhang", "Tangjie Lv", "Zhipeng Hu", "Hongning Wang", "Minlie Huang"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Cognitive Restructuring (CR) is a psychotherapeutic process aimed at\nidentifying and restructuring an individual's negative thoughts, arising from\nmental health challenges, into more helpful and positive ones via multi-turn\ndialogues. Clinician shortage and stigma urge the development of human-LLM\ninteractive psychotherapy for CR. Yet, existing efforts implement CR via simple\ntext rewriting, fixed-pattern dialogues, or a one-shot CR workflow, failing to\nalign with the psychotherapeutic process for effective CR. To address this gap,\nwe propose CRDial, a novel framework for CR, which creates multi-turn dialogues\nwith specifically designed identification and restructuring stages of negative\nthoughts, integrates sentence-level supportive conversation strategies, and\nadopts a multi-channel loop mechanism to enable iterative CR. With CRDial, we\ndistill Crisp, a large-scale and high-quality bilingual dialogue dataset, from\nLLM. We then train Crispers, Crisp-based conversational LLMs for CR, at 7B and\n14B scales. Extensive human studies show the superiority of Crispers in\npointwise, pairwise, and intervention evaluations."}
{"id": "2504.17264", "pdf": "https://arxiv.org/pdf/2504.17264.pdf", "abs": "https://arxiv.org/abs/2504.17264", "title": "JurisCTC: Enhancing Legal Judgment Prediction via Cross-Domain Transfer and Contrastive Learning", "authors": ["Zhaolu Kang", "Hongtian Cai", "Xiangyang Ji", "Jinzhe Li", "Nanfei Gu"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted in International Joint Conference on Neural Networks (IJCNN)\n  2025", "summary": "In recent years, Unsupervised Domain Adaptation (UDA) has gained significant\nattention in the field of Natural Language Processing (NLP) owing to its\nability to enhance model generalization across diverse domains. However, its\napplication for knowledge transfer between distinct legal domains remains\nlargely unexplored. To address the challenges posed by lengthy and complex\nlegal texts and the limited availability of large-scale annotated datasets, we\npropose JurisCTC, a novel model designed to improve the accuracy of Legal\nJudgment Prediction (LJP) tasks. Unlike existing approaches, JurisCTC\nfacilitates effective knowledge transfer across various legal domains and\nemploys contrastive learning to distinguish samples from different domains.\nSpecifically, for the LJP task, we enable knowledge transfer between civil and\ncriminal law domains. Compared to other models and specific large language\nmodels (LLMs), JurisCTC demonstrates notable advancements, achieving peak\naccuracies of 76.59% and 78.83%, respectively."}
{"id": "2504.17393", "pdf": "https://arxiv.org/pdf/2504.17393.pdf", "abs": "https://arxiv.org/abs/2504.17393", "title": "Towards User-Centred Design of AI-Assisted Decision-Making in Law Enforcement", "authors": ["Vesna Nowack", "Dalal Alrajeh", "Carolina Gutierrez Muñoz", "Katie Thomas", "William Hobson", "Catherine Hamilton-Giachritsis", "Patrick Benjamin", "Tim Grant", "Juliane A. Kloess", "Jessica Woodhams"], "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "10 pages, 1 figure", "summary": "Artificial Intelligence (AI) has become an important part of our everyday\nlives, yet user requirements for designing AI-assisted systems in law\nenforcement remain unclear. To address this gap, we conducted qualitative\nresearch on decision-making within a law enforcement agency. Our study aimed to\nidentify limitations of existing practices, explore user requirements and\nunderstand the responsibilities that humans expect to undertake in these\nsystems.\n  Participants in our study highlighted the need for a system capable of\nprocessing and analysing large volumes of data efficiently to help in crime\ndetection and prevention. Additionally, the system should satisfy requirements\nfor scalability, accuracy, justification, trustworthiness and adaptability to\nbe adopted in this domain. Participants also emphasised the importance of\nhaving end users review the input data that might be challenging for AI to\ninterpret, and validate the generated output to ensure the system's accuracy.\nTo keep up with the evolving nature of the law enforcement domain, end users\nneed to help the system adapt to the changes in criminal behaviour and\ngovernment guidance, and technical experts need to regularly oversee and\nmonitor the system. Furthermore, user-friendly human interaction with the\nsystem is essential for its adoption and some of the participants confirmed\nthey would be happy to be in the loop and provide necessary feedback that the\nsystem can learn from. Finally, we argue that it is very unlikely that the\nsystem will ever achieve full automation due to the dynamic and complex nature\nof the law enforcement domain."}
{"id": "2504.17279", "pdf": "https://arxiv.org/pdf/2504.17279.pdf", "abs": "https://arxiv.org/abs/2504.17279", "title": "Evaluating and Mitigating Bias in AI-Based Medical Text Generation", "authors": ["Xiuying Chen", "Tairan Wang", "Juexiao Zhou", "Zirui Song", "Xin Gao", "Xiangliang Zhang"], "categories": ["cs.CL"], "comment": "12 pages, 8 figures, published in Nature Computational Science", "summary": "Artificial intelligence (AI) systems, particularly those based on deep\nlearning models, have increasingly achieved expert-level performance in medical\napplications. However, there is growing concern that such AI systems may\nreflect and amplify human bias, and reduce the quality of their performance in\nhistorically under-served populations. The fairness issue has attracted\nconsiderable research interest in the medical imaging classification field, yet\nit remains understudied in the text generation domain. In this study, we\ninvestigate the fairness problem in text generation within the medical field\nand observe significant performance discrepancies across different races,\nsexes, and age groups, including intersectional groups, various model scales,\nand different evaluation metrics. To mitigate this fairness issue, we propose\nan algorithm that selectively optimizes those underperformed groups to reduce\nbias. The selection rules take into account not only word-level accuracy but\nalso the pathology accuracy to the target reference, while ensuring that the\nentire process remains fully differentiable for effective model training. Our\nevaluations across multiple backbones, datasets, and modalities demonstrate\nthat our proposed algorithm enhances fairness in text generation without\ncompromising overall performance. Specifically, the disparities among various\ngroups across different metrics were diminished by more than 30% with our\nalgorithm, while the relative change in text generation accuracy was typically\nwithin 2%. By reducing the bias generated by deep learning models, our proposed\napproach can potentially alleviate concerns about the fairness and reliability\nof text generation diagnosis in medical domain.\n  Our code is publicly available to facilitate further research at\nhttps://github.com/iriscxy/GenFair."}
{"id": "2412.00247", "pdf": "https://arxiv.org/pdf/2412.00247.pdf", "abs": "https://arxiv.org/abs/2412.00247", "title": "WiReSens Toolkit: An Open-source Platform towards Accessible Wireless Tactile Sensing", "authors": ["Devin Murphy", "Junyi Zhu", "Paul Pu Liang", "Wojciech Matusik", "Yiyue Luo"], "categories": ["cs.HC"], "comment": null, "summary": "Past research has widely explored the design and fabrication of resistive\nmatrix-based tactile sensors as a means of creating touch-sensitive devices.\nHowever, developing portable, adaptive, and long-lasting tactile sensing\nsystems that incorporate these sensors remains challenging for individuals\nhaving limited prior experience with them. To address this, we developed the\nWiReSens Toolkit, an open-source platform for accessible wireless tactile\nsensing. Central to our approach is adaptive hardware for interfacing with\nresistive sensors and a web-based GUI that mediates access to complex\nfunctionalities for developing scalable tactile sensing systems, including 1)\nmulti-device programming and wireless visualization across three distinct\ncommunication protocols 2) autocalibration methods for adaptive sensitivity and\n3) intermittent data transmission for low-power operation. We validated the\ntoolkit's usability through a user study with 11 novice participants, who, on\naverage, successfully configured a tactile sensor with over 95\\% accuracy in\nunder five minutes, calibrated sensors 10x faster than baseline methods, and\ndemonstrated enhanced tactile data sense-making."}
{"id": "2504.17309", "pdf": "https://arxiv.org/pdf/2504.17309.pdf", "abs": "https://arxiv.org/abs/2504.17309", "title": "CoheMark: A Novel Sentence-Level Watermark for Enhanced Text Quality", "authors": ["Junyan Zhang", "Shuliang Liu", "Aiwei Liu", "Yubo Gao", "Jungang Li", "Xiaojie Gu", "Xuming Hu"], "categories": ["cs.CL"], "comment": "Published at the 1st workshop on GenAI Watermarking, collocated with\n  ICLR 2025", "summary": "Watermarking technology is a method used to trace the usage of content\ngenerated by large language models. Sentence-level watermarking aids in\npreserving the semantic integrity within individual sentences while maintaining\ngreater robustness. However, many existing sentence-level watermarking\ntechniques depend on arbitrary segmentation or generation processes to embed\nwatermarks, which can limit the availability of appropriate sentences. This\nlimitation, in turn, compromises the quality of the generated response. To\naddress the challenge of balancing high text quality with robust watermark\ndetection, we propose CoheMark, an advanced sentence-level watermarking\ntechnique that exploits the cohesive relationships between sentences for better\nlogical fluency. The core methodology of CoheMark involves selecting sentences\nthrough trained fuzzy c-means clustering and applying specific next sentence\nselection criteria. Experimental evaluations demonstrate that CoheMark achieves\nstrong watermark strength while exerting minimal impact on text quality."}
{"id": "2504.14539", "pdf": "https://arxiv.org/pdf/2504.14539.pdf", "abs": "https://arxiv.org/abs/2504.14539", "title": "Should Benevolent Deception be Allowed in EHMI? A Mechanism Explanation Based on Game Theory", "authors": ["Linkun Liu", "Jian Sun", "Ye Tian"], "categories": ["cs.HC"], "comment": null, "summary": "The application of external human-machine interface (EHMI) on autonomous\nvehicles (AVs) facilitates information exchange. Existing research fails to\nconsider the impact of the sequence of actions, as well as the effects of EHMI\napplications and deception, raising the question of whether benevolent,\nwell-intentioned deception should be permitted (i.e., misleading statements\nthat are intended to benefit both parties). We established a game theory based\nEHMI information disclosure framework for AVs in this study. In considering\nbenevolent deception, this framework divided the decision-making process into\nthree stages, respectively encompassing three key questions: whether to\ndisclose, when to disclose, and what type of intention information to disclose.\nThe results show that theoretical advantages of deception exist in certain\ncases when AV expects to maximize the safety of the interaction. In 40 out of\n484 cases (8.3%), safety can be enhanced through successful deception. Those\nsuccessful deceptions fall into two categories: 1) In 28 of these cases, the\nstraight-going AV expected the left-turning HV to yield, while HV exhibited\nlower speed and higher acceleration; 2) In 12 of these cases, AV expected HV to\nproceed first, while HV exhibited higher speed and lower acceleration. We also\nconducted a VR-based driving simulation experiment, and the results confirmed\nour conclusion. Additionally, we found that when participants had low trust in\nthe EHMI, its use negatively impacted interaction efficiency instead. This\nstudy aims to analyze the mechanisms of EHMI information disclosure and\ncontribute to the ongoing discourse on the ethical framework governing\nautonomous driving systems."}
{"id": "2504.17311", "pdf": "https://arxiv.org/pdf/2504.17311.pdf", "abs": "https://arxiv.org/abs/2504.17311", "title": "FLUKE: A Linguistically-Driven and Task-Agnostic Framework for Robustness Evaluation", "authors": ["Yulia Otmakhova", "Hung Thinh Truong", "Rahmad Mahendra", "Zenan Zhai", "Rongxin Zhu", "Daniel Beck", "Jey Han Lau"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present FLUKE (Framework for LingUistically-driven and tasK-agnostic\nrobustness Evaluation), a task-agnostic framework for assessing model\nrobustness through systematic minimal variations of test data. FLUKE introduces\ncontrolled variations across linguistic levels - from orthography to dialect\nand style varieties - and leverages large language models (LLMs) with human\nvalidation to generate modifications. We demonstrate FLUKE's utility by\nevaluating both fine-tuned models and LLMs across four diverse NLP tasks, and\nreveal that (1) the impact of linguistic variations is highly task-dependent,\nwith some tests being critical for certain tasks but irrelevant for others; (2)\nwhile LLMs have better overall robustness compared to fine-tuned models, they\nstill exhibit significant brittleness to certain linguistic variations; (3) all\nmodels show substantial vulnerability to negation modifications across most\ntasks. These findings highlight the importance of systematic robustness testing\nfor understanding model behaviors."}
{"id": "2504.16295", "pdf": "https://arxiv.org/pdf/2504.16295.pdf", "abs": "https://arxiv.org/abs/2504.16295", "title": "Subthreshold Jitter in VR Can Induce Visual Discomfort", "authors": ["Samuel J. Levulis", "Kevin W. Rio", "Pablo Ramon Soria", "James Wilmott", "Charlie S. Burlingham", "Phillip Guan"], "categories": ["cs.HC"], "comment": null, "summary": "Visual-vestibular conflicts (VVCs) are a primary contributor to visually\ninduced motion sickness (VIMS) in head-mounted displays (HMDs). However,\nvirtual reality (VR) comfort studies often rely on exposing seated or standing\nusers to experiences with high intensity visual motion (such as roller\ncoasters). These drastic VVCs tend to induce pronounced VIMS symptoms that can\nbe reliably detected across individuals using common survey measures. The\nconclusions from studies using these extreme motion-based conflicts may not\naccurately generalize to naturalistic use cases in VR where efforts are made to\nminimize, rather than maximize, VIMS symptoms. In this work, we show that a\nsubthreshold visual-vestibular conflict can induce measurable discomfort during\nnaturalistic, long duration use. We first present a psychophysical study,\nconducted outside of an HMD, to rigorously identify the perceptual thresholds\nfor sinusoidal noise in render pose (i.e., jitter) resulting in erroneous 3D\nmotion of rendered content. We next introduce subthreshold levels of jitter to\na Meta Quest 3 VR HMD and demonstrate that this can induce visual discomfort in\nparticipants playing the commercially-available game Cubism across a\nthree-session, repeated-measures study. Importantly, we did not identify\nstatistically significant comfort differences between control and jitter\nconditions with traditional pre- and post-test comparison of Simulator Sickness\nQuestionnaire (SSQ) scores. Significant differences were only identified using\nthe Motion Illness Symptoms Classification (MISC) survey administered every 10\nminutes across each 90 minute session. This highlights the benefits of\nincorporating time-resolved data points and suggests that lightweight, more\nfrequent surveys may be important tools for measuring visual discomfort in more\necologically-valid scenarios."}
{"id": "2504.17332", "pdf": "https://arxiv.org/pdf/2504.17332.pdf", "abs": "https://arxiv.org/abs/2504.17332", "title": "Bridging Cognition and Emotion: Empathy-Driven Multimodal Misinformation Detection", "authors": ["Zihan Wang", "Lu Yuan", "Zhengxuan Zhang", "Qing Zhao"], "categories": ["cs.CL"], "comment": null, "summary": "In the digital era, social media has become a major conduit for information\ndissemination, yet it also facilitates the rapid spread of misinformation.\nTraditional misinformation detection methods primarily focus on surface-level\nfeatures, overlooking the crucial roles of human empathy in the propagation\nprocess. To address this gap, we propose the Dual-Aspect Empathy Framework\n(DAE), which integrates cognitive and emotional empathy to analyze\nmisinformation from both the creator and reader perspectives. By examining\ncreators' cognitive strategies and emotional appeals, as well as simulating\nreaders' cognitive judgments and emotional responses using Large Language\nModels (LLMs), DAE offers a more comprehensive and human-centric approach to\nmisinformation detection. Moreover, we further introduce an empathy-aware\nfiltering mechanism to enhance response authenticity and diversity.\nExperimental results on benchmark datasets demonstrate that DAE outperforms\nexisting methods, providing a novel paradigm for multimodal misinformation\ndetection."}
{"id": "2504.15007", "pdf": "https://arxiv.org/pdf/2504.15007.pdf", "abs": "https://arxiv.org/abs/2504.15007", "title": "Shifts in Doctors' Eye Movements Between Real and AI-Generated Medical Images", "authors": ["David C Wong", "Bin Wang", "Gorkem Durak", "Marouane Tliba", "Mohamed Amine Kerkouri", "Aladine Chetouani", "Ahmet Enis Cetin", "Cagdas Topel", "Nicolo Gennaro", "Camila Vendrami", "Tugce Agirlar Trabzonlu", "Amir Ali Rahsepar", "Laetitia Perronne", "Matthew Antalek", "Onural Ozturk", "Gokcan Okur", "Andrew C. Gordon", "Ayis Pyrros", "Frank H Miller", "Amir A Borhani", "Hatice Savas", "Eric M. Hart", "Elizabeth A Krupinski", "Ulas Bagci"], "categories": ["cs.CV", "cs.HC"], "comment": "This paper was accepted at ETRA 2025 Japan", "summary": "Eye-tracking analysis plays a vital role in medical imaging, providing key\ninsights into how radiologists visually interpret and diagnose clinical cases.\nIn this work, we first analyze radiologists' attention and agreement by\nmeasuring the distribution of various eye-movement patterns, including saccades\ndirection, amplitude, and their joint distribution. These metrics help uncover\npatterns in attention allocation and diagnostic strategies. Furthermore, we\ninvestigate whether and how doctors' gaze behavior shifts when viewing\nauthentic (Real) versus deep-learning-generated (Fake) images. To achieve this,\nwe examine fixation bias maps, focusing on first, last, short, and longest\nfixations independently, along with detailed saccades patterns, to quantify\ndifferences in gaze distribution and visual saliency between authentic and\nsynthetic images."}
{"id": "2504.17353", "pdf": "https://arxiv.org/pdf/2504.17353.pdf", "abs": "https://arxiv.org/abs/2504.17353", "title": "M-MRE: Extending the Mutual Reinforcement Effect to Multimodal Information Extraction", "authors": ["Chengguang Gan", "Sunbowen Lee", "Zhixi Cai", "Yanbin Wei", "Lei Zheng", "Yunhao Liang", "Shiwen Ni", "Tatsunori Mori"], "categories": ["cs.CL", "cs.CV", "cs.MM"], "comment": null, "summary": "Mutual Reinforcement Effect (MRE) is an emerging subfield at the intersection\nof information extraction and model interpretability. MRE aims to leverage the\nmutual understanding between tasks of different granularities, enhancing the\nperformance of both coarse-grained and fine-grained tasks through joint\nmodeling. While MRE has been explored and validated in the textual domain, its\napplicability to visual and multimodal domains remains unexplored. In this\nwork, we extend MRE to the multimodal information extraction domain for the\nfirst time. Specifically, we introduce a new task: Multimodal Mutual\nReinforcement Effect (M-MRE), and construct a corresponding dataset to support\nthis task. To address the challenges posed by M-MRE, we further propose a\nPrompt Format Adapter (PFA) that is fully compatible with various Large\nVision-Language Models (LVLMs). Experimental results demonstrate that MRE can\nalso be observed in the M-MRE task, a multimodal text-image understanding\nscenario. This provides strong evidence that MRE facilitates mutual gains\nacross three interrelated tasks, confirming its generalizability beyond the\ntextual domain."}
{"id": "2504.17360", "pdf": "https://arxiv.org/pdf/2504.17360.pdf", "abs": "https://arxiv.org/abs/2504.17360", "title": "PatientDx: Merging Large Language Models for Protecting Data-Privacy in Healthcare", "authors": ["Jose G. Moreno", "Jesus Lovon", "M'Rick Robin-Charlet", "Christine Damase-Michel", "Lynda Tamine"], "categories": ["cs.CL"], "comment": null, "summary": "Fine-tuning of Large Language Models (LLMs) has become the default practice\nfor improving model performance on a given task. However, performance\nimprovement comes at the cost of training on vast amounts of annotated data\nwhich could be sensitive leading to significant data privacy concerns. In\nparticular, the healthcare domain is one of the most sensitive domains exposed\nto data privacy issues. In this paper, we present PatientDx, a framework of\nmodel merging that allows the design of effective LLMs for health-predictive\ntasks without requiring fine-tuning nor adaptation on patient data. Our\nproposal is based on recently proposed techniques known as merging of LLMs and\naims to optimize a building block merging strategy. PatientDx uses a pivotal\nmodel adapted to numerical reasoning and tunes hyperparameters on examples\nbased on a performance metric but without training of the LLM on these data.\nExperiments using the mortality tasks of the MIMIC-IV dataset show improvements\nup to 7% in terms of AUROC when compared to initial models. Additionally, we\nconfirm that when compared to fine-tuned models, our proposal is less prone to\ndata leak problems without hurting performance. Finally, we qualitatively show\nthe capabilities of our proposal through a case study. Our best model is\npublicly available at https://huggingface.co/ Jgmorenof/mistral\\_merged\\_0\\_4."}
{"id": "2504.17366", "pdf": "https://arxiv.org/pdf/2504.17366.pdf", "abs": "https://arxiv.org/abs/2504.17366", "title": "LiveLongBench: Tackling Long-Context Understanding for Spoken Texts from Live Streams", "authors": ["Yongxuan Wu", "Runyu Chen", "Peiyu Liu", "Hongjin Qian"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Long-context understanding poses significant challenges in natural language\nprocessing, particularly for real-world dialogues characterized by speech-based\nelements, high redundancy, and uneven information density. Although large\nlanguage models (LLMs) achieve impressive results on existing benchmarks, these\ndatasets fail to reflect the complexities of such texts, limiting their\napplicability to practical scenarios. To bridge this gap, we construct the\nfirst spoken long-text dataset, derived from live streams, designed to reflect\nthe redundancy-rich and conversational nature of real-world scenarios. We\nconstruct tasks in three categories: retrieval-dependent, reasoning-dependent,\nand hybrid. We then evaluate both popular LLMs and specialized methods to\nassess their ability to understand long-contexts in these tasks. Our results\nshow that current methods exhibit strong task-specific preferences and perform\npoorly on highly redundant inputs, with no single method consistently\noutperforming others. We propose a new baseline that better handles redundancy\nin spoken text and achieves strong performance across tasks. Our findings\nhighlight key limitations of current methods and suggest future directions for\nimproving long-context understanding. Finally, our benchmark fills a gap in\nevaluating long-context spoken language understanding and provides a practical\nfoundation for developing real-world e-commerce systems. The code and benchmark\nare available at https://github.com/Yarayx/livelongbench."}
{"id": "2504.17390", "pdf": "https://arxiv.org/pdf/2504.17390.pdf", "abs": "https://arxiv.org/abs/2504.17390", "title": "PicPersona-TOD : A Dataset for Personalizing Utterance Style in Task-Oriented Dialogue with Image Persona", "authors": ["Jihyun Lee", "Yejin Jeon", "Seungyeon Seo", "Gary Geunbae Lee"], "categories": ["cs.CL"], "comment": "Accepted in NAACL 2025 main", "summary": "Task-Oriented Dialogue (TOD) systems are designed to fulfill user requests\nthrough natural language interactions, yet existing systems often produce\ngeneric, monotonic responses that lack individuality and fail to adapt to\nusers' personal attributes. To address this, we introduce PicPersona-TOD, a\nnovel dataset that incorporates user images as part of the persona, enabling\npersonalized responses tailored to user-specific factors such as age or\nemotional context. This is facilitated by first impressions, dialogue\npolicy-guided prompting, and the use of external knowledge to reduce\nhallucinations. Human evaluations confirm that our dataset enhances user\nexperience, with personalized responses contributing to a more engaging\ninteraction. Additionally, we introduce a new NLG model, Pictor, which not only\npersonalizes responses, but also demonstrates robust performance across unseen\ndomains https://github.com/JihyunLee1/PicPersona."}
{"id": "2504.17445", "pdf": "https://arxiv.org/pdf/2504.17445.pdf", "abs": "https://arxiv.org/abs/2504.17445", "title": "Creating Targeted, Interpretable Topic Models with LLM-Generated Text Augmentation", "authors": ["Anna Lieb", "Maneesh Arora", "Eni Mustafaraj"], "categories": ["cs.CL"], "comment": "Presented at IC2S2 2024 in Philadelphia, USA", "summary": "Unsupervised machine learning techniques, such as topic modeling and\nclustering, are often used to identify latent patterns in unstructured text\ndata in fields such as political science and sociology. These methods overcome\ncommon concerns about reproducibility and costliness involved in the\nlabor-intensive process of human qualitative analysis. However, two major\nlimitations of topic models are their interpretability and their practicality\nfor answering targeted, domain-specific social science research questions. In\nthis work, we investigate opportunities for using LLM-generated text\naugmentation to improve the usefulness of topic modeling output. We use a\npolitical science case study to evaluate our results in a domain-specific\napplication, and find that topic modeling using GPT-4 augmentations creates\nhighly interpretable categories that can be used to investigate domain-specific\nresearch questions with minimal human guidance."}
{"id": "2504.17480", "pdf": "https://arxiv.org/pdf/2504.17480.pdf", "abs": "https://arxiv.org/abs/2504.17480", "title": "Unified Attacks to Large Language Model Watermarks: Spoofing and Scrubbing in Unauthorized Knowledge Distillation", "authors": ["Xin Yi", "Shunfan Zhengc", "Linlin Wanga", "Xiaoling Wang", "Liang He"], "categories": ["cs.CL"], "comment": null, "summary": "Watermarking has emerged as a critical technique for combating misinformation\nand protecting intellectual property in large language models (LLMs). A recent\ndiscovery, termed watermark radioactivity, reveals that watermarks embedded in\nteacher models can be inherited by student models through knowledge\ndistillation. On the positive side, this inheritance allows for the detection\nof unauthorized knowledge distillation by identifying watermark traces in\nstudent models. However, the robustness of watermarks against scrubbing attacks\nand their unforgeability in the face of spoofing attacks under unauthorized\nknowledge distillation remain largely unexplored. Existing watermark attack\nmethods either assume access to model internals or fail to simultaneously\nsupport both scrubbing and spoofing attacks. In this work, we propose\nContrastive Decoding-Guided Knowledge Distillation (CDG-KD), a unified\nframework that enables bidirectional attacks under unauthorized knowledge\ndistillation. Our approach employs contrastive decoding to extract corrupted or\namplified watermark texts via comparing outputs from the student model and\nweakly watermarked references, followed by bidirectional distillation to train\nnew student models capable of watermark removal and watermark forgery,\nrespectively. Extensive experiments show that CDG-KD effectively performs\nattacks while preserving the general performance of the distilled model. Our\nfindings underscore critical need for developing watermarking schemes that are\nrobust and unforgeable."}
{"id": "2504.17550", "pdf": "https://arxiv.org/pdf/2504.17550.pdf", "abs": "https://arxiv.org/abs/2504.17550", "title": "HalluLens: LLM Hallucination Benchmark", "authors": ["Yejin Bang", "Ziwei Ji", "Alan Schelten", "Anthony Hartshorn", "Tara Fowler", "Cheng Zhang", "Nicola Cancedda", "Pascale Fung"], "categories": ["cs.CL", "cs.AI"], "comment": "42 pages", "summary": "Large language models (LLMs) often generate responses that deviate from user\ninput or training data, a phenomenon known as \"hallucination.\" These\nhallucinations undermine user trust and hinder the adoption of generative AI\nsystems. Addressing hallucinations is essential for the advancement of LLMs.\nThis paper introduces a comprehensive hallucination benchmark, incorporating\nboth new extrinsic and existing intrinsic evaluation tasks, built upon clear\ntaxonomy of hallucination. A major challenge in benchmarking hallucinations is\nthe lack of a unified framework due to inconsistent definitions and\ncategorizations. We disentangle LLM hallucination from \"factuality,\" proposing\na clear taxonomy that distinguishes between extrinsic and intrinsic\nhallucinations, to promote consistency and facilitate research. Extrinsic\nhallucinations, where the generated content is not consistent with the training\ndata, are increasingly important as LLMs evolve. Our benchmark includes dynamic\ntest set generation to mitigate data leakage and ensure robustness against such\nleakage. We also analyze existing benchmarks, highlighting their limitations\nand saturation. The work aims to: (1) establish a clear taxonomy of\nhallucinations, (2) introduce new extrinsic hallucination tasks, with data that\ncan be dynamically regenerated to prevent saturation by leakage, (3) provide a\ncomprehensive analysis of existing benchmarks, distinguishing them from\nfactuality evaluations."}
{"id": "2504.17562", "pdf": "https://arxiv.org/pdf/2504.17562.pdf", "abs": "https://arxiv.org/abs/2504.17562", "title": "When Does Metadata Conditioning (NOT) Work for Language Model Pre-Training? A Study with Context-Free Grammars", "authors": ["Rei Higuchi", "Ryotaro Kawata", "Naoki Nishikawa", "Kazusato Oko", "Shoichiro Yamaguchi", "Sosuke Kobayashi", "Seiya Tokui", "Kohei Hayashi", "Daisuke Okanohara", "Taiji Suzuki"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The ability to acquire latent semantics is one of the key properties that\ndetermines the performance of language models. One convenient approach to\ninvoke this ability is to prepend metadata (e.g. URLs, domains, and styles) at\nthe beginning of texts in the pre-training data, making it easier for the model\nto access latent semantics before observing the entire text. Previous studies\nhave reported that this technique actually improves the performance of trained\nmodels in downstream tasks; however, this improvement has been observed only in\nspecific downstream tasks, without consistent enhancement in average next-token\nprediction loss. To understand this phenomenon, we closely investigate how\nprepending metadata during pre-training affects model performance by examining\nits behavior using artificial data. Interestingly, we found that this approach\nproduces both positive and negative effects on the downstream tasks. We\ndemonstrate that the effectiveness of the approach depends on whether latent\nsemantics can be inferred from the downstream task's prompt. Specifically,\nthrough investigations using data generated by probabilistic context-free\ngrammars, we show that training with metadata helps improve model's performance\nwhen the given context is long enough to infer the latent semantics. In\ncontrast, the technique negatively impacts performance when the context lacks\nthe necessary information to make an accurate posterior inference."}
{"id": "2504.17565", "pdf": "https://arxiv.org/pdf/2504.17565.pdf", "abs": "https://arxiv.org/abs/2504.17565", "title": "DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale Difficulty-Graded Data Training", "authors": ["Xiaoyu Tian", "Sitong Zhao", "Haotian Wang", "Shuaiting Chen", "Yiping Peng", "Yunjie Ji", "Han Zhao", "Xiangang Li"], "categories": ["cs.CL"], "comment": null, "summary": "Although large language models (LLMs) have recently achieved remarkable\nperformance on various complex reasoning benchmarks, the academic community\nstill lacks an in-depth understanding of base model training processes and data\nquality. To address this, we construct a large-scale, difficulty-graded\nreasoning dataset containing approximately 3.34 million unique queries of\nvarying difficulty levels and about 40 million distilled responses generated by\nmultiple models over several passes. Leveraging pass rate and Coefficient of\nVariation (CV), we precisely select the most valuable training data to enhance\nreasoning capability. Notably, we observe a training pattern shift, indicating\nthat reasoning-focused training based on base models requires higher learning\nrates for effective training. Using this carefully selected data, we\nsignificantly improve the reasoning capabilities of the base model, achieving a\npass rate of 79.2\\% on the AIME2024 mathematical reasoning benchmark. This\nresult surpasses most current distilled models and closely approaches\nstate-of-the-art performance. We provide detailed descriptions of our data\nprocessing, difficulty assessment, and training methodology, and have publicly\nreleased all datasets and methods to promote rapid progress in open-source\nlong-reasoning LLMs. The dataset is available at:\nhttps://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M"}
{"id": "2504.17574", "pdf": "https://arxiv.org/pdf/2504.17574.pdf", "abs": "https://arxiv.org/abs/2504.17574", "title": "RAGAT-Mind: A Multi-Granular Modeling Approach for Rumor Detection Based on MindSpore", "authors": ["Zhenkai Qin", "Guifang Yang", "Dongze Wu"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "As false information continues to proliferate across social media platforms,\neffective rumor detection has emerged as a pressing challenge in natural\nlanguage processing. This paper proposes RAGAT-Mind, a multi-granular modeling\napproach for Chinese rumor detection, built upon the MindSpore deep learning\nframework. The model integrates TextCNN for local semantic extraction,\nbidirectional GRU for sequential context learning, Multi-Head Self-Attention\nfor global dependency focusing, and Bidirectional Graph Convolutional Networks\n(BiGCN) for structural representation of word co-occurrence graphs. Experiments\non the Weibo1-Rumor dataset demonstrate that RAGAT-Mind achieves superior\nclassification performance, attaining 99.2% accuracy and a macro-F1 score of\n0.9919. The results validate the effectiveness of combining hierarchical\nlinguistic features with graph-based semantic structures. Furthermore, the\nmodel exhibits strong generalization and interpretability, highlighting its\npractical value for real-world rumor detection applications."}
{"id": "2504.17653", "pdf": "https://arxiv.org/pdf/2504.17653.pdf", "abs": "https://arxiv.org/abs/2504.17653", "title": "Towards a comprehensive taxonomy of online abusive language informed by machine leaning", "authors": ["Samaneh Hosseini Moghaddam", "Kelly Lyons", "Cheryl Regehr", "Vivek Goel", "Kaitlyn Regehr"], "categories": ["cs.CL"], "comment": null, "summary": "The proliferation of abusive language in online communications has posed\nsignificant risks to the health and wellbeing of individuals and communities.\nThe growing concern regarding online abuse and its consequences necessitates\nmethods for identifying and mitigating harmful content and facilitating\ncontinuous monitoring, moderation, and early intervention. This paper presents\na taxonomy for distinguishing key characteristics of abusive language within\nonline text. Our approach uses a systematic method for taxonomy development,\nintegrating classification systems of 18 existing multi-label datasets to\ncapture key characteristics relevant to online abusive language classification.\nThe resulting taxonomy is hierarchical and faceted, comprising 5 categories and\n17 dimensions. It classifies various facets of online abuse, including context,\ntarget, intensity, directness, and theme of abuse. This shared understanding\ncan lead to more cohesive efforts, facilitate knowledge exchange, and\naccelerate progress in the field of online abuse detection and mitigation among\nresearchers, policy makers, online platform owners, and other stakeholders."}
{"id": "2504.17665", "pdf": "https://arxiv.org/pdf/2504.17665.pdf", "abs": "https://arxiv.org/abs/2504.17665", "title": "Evaluating Grounded Reasoning by Code-Assisted Large Language Models for Mathematics", "authors": ["Zena Al-Khalili", "Nick Howell", "Dietrich Klakow"], "categories": ["cs.CL"], "comment": null, "summary": "Assisting LLMs with code generation improved their performance on\nmathematical reasoning tasks. However, the evaluation of code-assisted LLMs is\ngenerally restricted to execution correctness, lacking a rigorous evaluation of\ntheir generated programs. In this work, we bridge this gap by conducting an\nin-depth analysis of code-assisted LLMs' generated programs in response to math\nreasoning tasks. Our evaluation focuses on the extent to which LLMs ground\ntheir programs to math rules, and how that affects their end performance. For\nthis purpose, we assess the generations of five different LLMs, on two\ndifferent math datasets, both manually and automatically. Our results reveal\nthat the distribution of grounding depends on LLMs' capabilities and the\ndifficulty of math problems. Furthermore, mathematical grounding is more\neffective for closed-source models, while open-source models fail to employ\nmath rules in their solutions correctly. On MATH500, the percentage of grounded\nprograms decreased to half, while the ungrounded generations doubled in\ncomparison to ASDiv grade-school problems. Our work highlights the need for\nin-depth evaluation beyond execution accuracy metrics, toward a better\nunderstanding of code-assisted LLMs' capabilities and limits in the math\ndomain."}
{"id": "2504.17671", "pdf": "https://arxiv.org/pdf/2504.17671.pdf", "abs": "https://arxiv.org/abs/2504.17671", "title": "Data-Driven Calibration of Prediction Sets in Large Vision-Language Models Based on Inductive Conformal Prediction", "authors": ["Yuanchang Ye", "Weiyan Wen"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This study addresses the critical challenge of hallucination mitigation in\nLarge Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks\nthrough a Split Conformal Prediction (SCP) framework. While LVLMs excel in\nmulti-modal reasoning, their outputs often exhibit hallucinated content with\nhigh confidence, posing risks in safety-critical applications. We propose a\nmodel-agnostic uncertainty quantification method that integrates dynamic\nthreshold calibration and cross-modal consistency verification. By partitioning\ndata into calibration and test sets, the framework computes nonconformity\nscores to construct prediction sets with statistical guarantees under\nuser-defined risk levels ($\\alpha$). Key innovations include: (1) rigorous\ncontrol of \\textbf{marginal coverage} to ensure empirical error rates remain\nstrictly below $\\alpha$; (2) dynamic adjustment of prediction set sizes\ninversely with $\\alpha$, filtering low-confidence outputs; (3) elimination of\nprior distribution assumptions and retraining requirements. Evaluations on\nbenchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces\ntheoretical guarantees across all $\\alpha$ values. The framework achieves\nstable performance across varying calibration-to-test split ratios,\nunderscoring its robustness for real-world deployment in healthcare, autonomous\nsystems, and other safety-sensitive domains. This work bridges the gap between\ntheoretical reliability and practical applicability in multi-modal AI systems,\noffering a scalable solution for hallucination detection and uncertainty-aware\ndecision-making."}
{"id": "2504.17674", "pdf": "https://arxiv.org/pdf/2504.17674.pdf", "abs": "https://arxiv.org/abs/2504.17674", "title": "Energy Considerations of Large Language Model Inference and Efficiency Optimizations", "authors": ["Jared Fernandez", "Clara Na", "Vashisth Tiwari", "Yonatan Bisk", "Sasha Luccioni", "Emma Strubell"], "categories": ["cs.CL", "cs.LG"], "comment": "16 pages", "summary": "As large language models (LLMs) scale in size and adoption, their\ncomputational and environmental costs continue to rise. Prior benchmarking\nefforts have primarily focused on latency reduction in idealized settings,\noften overlooking the diverse real-world inference workloads that shape energy\nuse. In this work, we systematically analyze the energy implications of common\ninference efficiency optimizations across diverse Natural Language Processing\n(NLP) and generative Artificial Intelligence (AI) workloads, including\nconversational AI and code generation. We introduce a modeling approach that\napproximates real-world LLM workflows through a binning strategy for\ninput-output token distributions and batch size variations. Our empirical\nanalysis spans software frameworks, decoding strategies, GPU architectures,\nonline and offline serving settings, and model parallelism configurations. We\nshow that the effectiveness of inference optimizations is highly sensitive to\nworkload geometry, software stack, and hardware accelerators, demonstrating\nthat naive energy estimates based on FLOPs or theoretical GPU utilization\nsignificantly underestimate real-world energy consumption. Our findings reveal\nthat the proper application of relevant inference efficiency optimizations can\nreduce total energy use by up to 73% from unoptimized baselines. These insights\nprovide a foundation for sustainable LLM deployment and inform energy-efficient\ndesign strategies for future AI infrastructure."}
{"id": "2504.17685", "pdf": "https://arxiv.org/pdf/2504.17685.pdf", "abs": "https://arxiv.org/abs/2504.17685", "title": "Ensemble Bayesian Inference: Leveraging Small Language Models to Achieve LLM-level Accuracy in Profile Matching Tasks", "authors": ["Haru-Tada Sato", "Fuka Matsuzaki", "Jun-ichiro Takahashi"], "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 2 figures", "summary": "This study explores the potential of small language model(SLM) ensembles to\nachieve accuracy comparable to proprietary large language models (LLMs). We\npropose Ensemble Bayesian Inference (EBI), a novel approach that applies\nBayesian estimation to combine judgments from multiple SLMs, allowing them to\nexceed the performance limitations of individual models. Our experiments on\ndiverse tasks(aptitude assessments and consumer profile analysis in both\nJapanese and English) demonstrate EBI's effectiveness. Notably, we analyze\ncases where incorporating models with negative Lift values into ensembles\nimproves overall performance, and we examine the method's efficacy across\ndifferent languages. These findings suggest new possibilities for constructing\nhigh-performance AI systems with limited computational resources and for\neffectively utilizing models with individually lower performance. Building on\nexisting research on LLM performance evaluation, ensemble methods, and\nopen-source LLM utilization, we discuss the novelty and significance of our\napproach."}
{"id": "2504.17704", "pdf": "https://arxiv.org/pdf/2504.17704.pdf", "abs": "https://arxiv.org/abs/2504.17704", "title": "Safety in Large Reasoning Models: A Survey", "authors": ["Cheng Wang", "Yue Liu", "Baolong Li", "Duzhen Zhang", "Zhongzhi Li", "Junfeng Fang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Reasoning Models (LRMs) have exhibited extraordinary prowess in tasks\nlike mathematics and coding, leveraging their advanced reasoning capabilities.\nNevertheless, as these capabilities progress, significant concerns regarding\ntheir vulnerabilities and safety have arisen, which can pose challenges to\ntheir deployment and application in real-world settings. This paper presents a\ncomprehensive survey of LRMs, meticulously exploring and summarizing the newly\nemerged safety risks, attacks, and defense strategies. By organizing these\nelements into a detailed taxonomy, this work aims to offer a clear and\nstructured understanding of the current safety landscape of LRMs, facilitating\nfuture research and development to enhance the security and reliability of\nthese powerful models."}
{"id": "2504.17720", "pdf": "https://arxiv.org/pdf/2504.17720.pdf", "abs": "https://arxiv.org/abs/2504.17720", "title": "Multilingual Performance Biases of Large Language Models in Education", "authors": ["Vansh Gupta", "Sankalan Pal Chowdhury", "Vilém Zouhar", "Donya Rooein", "Mrinmaya Sachan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly being adopted in educational\nsettings. These applications expand beyond English, though current LLMs remain\nprimarily English-centric. In this work, we ascertain if their use in education\nsettings in non-English languages is warranted. We evaluated the performance of\npopular LLMs on four educational tasks: identifying student misconceptions,\nproviding targeted feedback, interactive tutoring, and grading translations in\nsix languages (Hindi, Arabic, Farsi, Telugu, Ukrainian, Czech) in addition to\nEnglish. We find that the performance on these tasks somewhat corresponds to\nthe amount of language represented in training data, with lower-resource\nlanguages having poorer task performance. Although the models perform\nreasonably well in most languages, the frequent performance drop from English\nis significant. Thus, we recommend that practitioners first verify that the LLM\nworks well in the target language for their educational task before deployment."}
{"id": "2504.17753", "pdf": "https://arxiv.org/pdf/2504.17753.pdf", "abs": "https://arxiv.org/abs/2504.17753", "title": "Conversational Assistants to support Heart Failure Patients: comparing a Neurosymbolic Architecture with ChatGPT", "authors": ["Anuja Tayal", "Devika Salunke", "Barbara Di Eugenio", "Paula Allen-Meares", "Eulalia Puig Abril", "Olga Garcia", "Carolyn Dickens", "Andrew Boyd"], "categories": ["cs.CL"], "comment": null, "summary": "Conversational assistants are becoming more and more popular, including in\nhealthcare, partly because of the availability and capabilities of Large\nLanguage Models. There is a need for controlled, probing evaluations with real\nstakeholders which can highlight advantages and disadvantages of more\ntraditional architectures and those based on generative AI. We present a\nwithin-group user study to compare two versions of a conversational assistant\nthat allows heart failure patients to ask about salt content in food. One\nversion of the system was developed in-house with a neurosymbolic architecture,\nand one is based on ChatGPT. The evaluation shows that the in-house system is\nmore accurate, completes more tasks and is less verbose than the one based on\nChatGPT; on the other hand, the one based on ChatGPT makes fewer speech errors\nand requires fewer clarifications to complete the task. Patients show no\npreference for one over the other."}
{"id": "2504.17768", "pdf": "https://arxiv.org/pdf/2504.17768.pdf", "abs": "https://arxiv.org/abs/2504.17768", "title": "The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs", "authors": ["Piotr Nawrot", "Robert Li", "Renjie Huang", "Sebastian Ruder", "Kelly Marchisio", "Edoardo M. Ponti"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Sparse attention offers a promising strategy to extend long-context\ncapabilities in Transformer LLMs, yet its viability, its efficiency-accuracy\ntrade-offs, and systematic scaling studies remain unexplored. To address this\ngap, we perform a careful comparison of training-free sparse attention methods\nat varying model scales, sequence lengths, and sparsity levels on a diverse\ncollection of long-sequence tasks-including novel ones that rely on natural\nlanguage while remaining controllable and easy to evaluate. Based on our\nexperiments, we report a series of key findings: 1) an isoFLOPS analysis\nreveals that for very long sequences, larger and highly sparse models are\npreferable to smaller and dense ones. 2) The level of sparsity attainable while\nstatistically guaranteeing accuracy preservation is higher during decoding than\nprefilling, and correlates with model size in the former. 3) There is no clear\nstrategy that performs best across tasks and phases, with different units of\nsparsification or budget adaptivity needed for different scenarios. Even\nmoderate sparsity levels often result in significant performance degradation on\nat least one task, highlighting that sparse attention is not a universal\nsolution. 4) We introduce and validate novel scaling laws specifically tailored\nfor sparse attention, providing evidence that our findings are likely to hold\ntrue beyond our range of experiments. Through these insights, we demonstrate\nthat sparse attention is a key tool to enhance the capabilities of Transformer\nLLMs for processing longer sequences, but requires careful evaluation of\ntrade-offs for performance-sensitive applications."}
{"id": "2504.16939", "pdf": "https://arxiv.org/pdf/2504.16939.pdf", "abs": "https://arxiv.org/abs/2504.16939", "title": "A Desideratum for Conversational Agents: Capabilities, Challenges, and Future Directions", "authors": ["Emre Can Acikgoz", "Cheng Qian", "Hongru Wang", "Vardhan Dongre", "Xiusi Chen", "Heng Ji", "Dilek Hakkani-Tür", "Gokhan Tur"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have propelled conversational\nAI from traditional dialogue systems into sophisticated agents capable of\nautonomous actions, contextual awareness, and multi-turn interactions with\nusers. Yet, fundamental questions about their capabilities, limitations, and\npaths forward remain open. This survey paper presents a desideratum for\nnext-generation Conversational Agents - what has been achieved, what challenges\npersist, and what must be done for more scalable systems that approach\nhuman-level intelligence. To that end, we systematically analyze LLM-driven\nConversational Agents by organizing their capabilities into three primary\ndimensions: (i) Reasoning - logical, systematic thinking inspired by human\nintelligence for decision making, (ii) Monitor - encompassing self-awareness\nand user interaction monitoring, and (iii) Control - focusing on tool\nutilization and policy following. Building upon this, we introduce a novel\ntaxonomy by classifying recent work on Conversational Agents around our\nproposed desideratum. We identify critical research gaps and outline key\ndirections, including realistic evaluations, long-term multi-turn reasoning\nskills, self-evolution capabilities, collaborative and multi-agent task\ncompletion, personalization, and proactivity. This work aims to provide a\nstructured foundation, highlight existing limitations, and offer insights into\npotential future research directions for Conversational Agents, ultimately\nadvancing progress toward Artificial General Intelligence (AGI). We maintain a\ncurated repository of papers at:\nhttps://github.com/emrecanacikgoz/awesome-conversational-agents."}
{"id": "2504.17004", "pdf": "https://arxiv.org/pdf/2504.17004.pdf", "abs": "https://arxiv.org/abs/2504.17004", "title": "(Im)possibility of Automated Hallucination Detection in Large Language Models", "authors": ["Amin Karbasi", "Omar Montasser", "John Sous", "Grigoris Velegkas"], "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Is automated hallucination detection possible? In this work, we introduce a\ntheoretical framework to analyze the feasibility of automatically detecting\nhallucinations produced by large language models (LLMs). Inspired by the\nclassical Gold-Angluin framework for language identification and its recent\nadaptation to language generation by Kleinberg and Mullainathan, we investigate\nwhether an algorithm, trained on examples drawn from an unknown target language\n$K$ (selected from a countable collection) and given access to an LLM, can\nreliably determine whether the LLM's outputs are correct or constitute\nhallucinations.\n  First, we establish an equivalence between hallucination detection and the\nclassical task of language identification. We prove that any hallucination\ndetection method can be converted into a language identification method, and\nconversely, algorithms solving language identification can be adapted for\nhallucination detection. Given the inherent difficulty of language\nidentification, this implies that hallucination detection is fundamentally\nimpossible for most language collections if the detector is trained using only\ncorrect examples from the target language.\n  Second, we show that the use of expert-labeled feedback, i.e., training the\ndetector with both positive examples (correct statements) and negative examples\n(explicitly labeled incorrect statements), dramatically changes this\nconclusion. Under this enriched training regime, automated hallucination\ndetection becomes possible for all countable language collections.\n  These results highlight the essential role of expert-labeled examples in\ntraining hallucination detectors and provide theoretical support for\nfeedback-based methods, such as reinforcement learning with human feedback\n(RLHF), which have proven critical for reliable LLM deployment."}
{"id": "2504.17038", "pdf": "https://arxiv.org/pdf/2504.17038.pdf", "abs": "https://arxiv.org/abs/2504.17038", "title": "SCALAR: A Part-of-speech Tagger for Identifiers", "authors": ["Christian D. Newman", "Brandon Scholten", "Sophia Testa", "Joshua A. C. Behler", "Syreen Banabilah", "Michael L. Collard", "Michael J. Decker", "Mohamed Wiem Mkaouer", "Marcos Zampieri", "Eman Abdullah AlOmar", "Reem Alsuhaibani", "Anthony Peruma", "Jonathan I. Maletic"], "categories": ["cs.SE", "cs.CL"], "comment": null, "summary": "The paper presents the Source Code Analysis and Lexical Annotation Runtime\n(SCALAR), a tool specialized for mapping (annotating) source code identifier\nnames to their corresponding part-of-speech tag sequence (grammar pattern).\nSCALAR's internal model is trained using scikit-learn's\nGradientBoostingClassifier in conjunction with a manually-curated oracle of\nidentifier names and their grammar patterns. This specializes the tagger to\nrecognize the unique structure of the natural language used by developers to\ncreate all types of identifiers (e.g., function names, variable names etc.).\nSCALAR's output is compared with a previous version of the tagger, as well as a\nmodern off-the-shelf part-of-speech tagger to show how it improves upon other\ntaggers' output for annotating identifiers. The code is available on Github"}
{"id": "2504.17365", "pdf": "https://arxiv.org/pdf/2504.17365.pdf", "abs": "https://arxiv.org/abs/2504.17365", "title": "TimeSoccer: An End-to-End Multimodal Large Language Model for Soccer Commentary Generation", "authors": ["Ling You", "Wenxuan Huang", "Xinni Xie", "Xiangyi Wei", "Bangyan Li", "Shaohui Lin", "Yang Li", "Changbo Wang"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Soccer is a globally popular sporting event, typically characterized by long\nmatches and distinctive highlight moments. Recent advances in Multimodal Large\nLanguage Models (MLLMs) offer promising capabilities in temporal grounding and\nvideo understanding, soccer commentary generation often requires precise\ntemporal localization and semantically rich descriptions over long-form video.\nHowever, existing soccer MLLMs often rely on the temporal a priori for caption\ngeneration, so they cannot process the soccer video end-to-end. While some\ntraditional approaches follow a two-step paradigm that is complex and fails to\ncapture the global context to achieve suboptimal performance. To solve the\nabove issues, we present TimeSoccer, the first end-to-end soccer MLLM for\nSingle-anchor Dense Video Captioning (SDVC) in full-match soccer videos.\nTimeSoccer jointly predicts timestamps and generates captions in a single pass,\nenabling global context modeling across 45-minute matches. To support long\nvideo understanding of soccer matches, we introduce MoFA-Select, a\ntraining-free, motion-aware frame compression module that adaptively selects\nrepresentative frames via a coarse-to-fine strategy, and incorporates\ncomplementary training paradigms to strengthen the model's ability to handle\nlong temporal sequences. Extensive experiments demonstrate that our TimeSoccer\nachieves State-of-The-Art (SoTA) performance on the SDVC task in an end-to-end\nform, generating high-quality commentary with accurate temporal alignment and\nstrong semantic relevance."}
{"id": "2504.17449", "pdf": "https://arxiv.org/pdf/2504.17449.pdf", "abs": "https://arxiv.org/abs/2504.17449", "title": "HMI: Hierarchical Knowledge Management for Efficient Multi-Tenant Inference in Pretrained Language Models", "authors": ["Jun Zhang", "Jue Wang", "Huan Li", "Lidan Shou", "Ke Chen", "Gang Chen", "Qin Xie", "Guiming Xie", "Xuejian Gong"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted by VLDBJ 2025", "summary": "The significant computational demands of pretrained language models (PLMs),\nwhich often require dedicated hardware, present a substantial challenge in\nserving them efficiently, especially in multi-tenant environments. To address\nthis, we introduce HMI, a Hierarchical knowledge management-based Multi-tenant\nInference system, designed to manage tenants with distinct PLMs\nresource-efficiently. Our approach is three-fold: Firstly, we categorize PLM\nknowledge into general, domain-specific, and task-specific. Leveraging insights\non knowledge acquisition across different model layers, we construct\nhierarchical PLMs (hPLMs) by extracting and storing knowledge at different\nlevels, significantly reducing GPU memory usage per tenant. Secondly, we\nestablish hierarchical knowledge management for hPLMs generated by various\ntenants in HMI. We manage domain-specific knowledge with acceptable storage\nincreases by constructing and updating domain-specific knowledge trees based on\nfrequency. We manage task-specific knowledge within limited GPU memory through\nparameter swapping. Finally, we propose system optimizations to enhance\nresource utilization and inference throughput. These include fine-grained\npipelining via hierarchical knowledge prefetching to overlap CPU and I/O\noperations with GPU computations, and optimizing parallel implementations with\nbatched matrix multiplications. Our experimental results demonstrate that the\nproposed HMI can efficiently serve up to 10,000 hPLMs (hBERTs and hGPTs) on a\nsingle GPU, with only a negligible compromise in accuracy."}
{"id": "2406.07494", "pdf": "https://arxiv.org/pdf/2406.07494.pdf", "abs": "https://arxiv.org/abs/2406.07494", "title": "CADS: A Systematic Literature Review on the Challenges of Abstractive Dialogue Summarization", "authors": ["Frederic Kirstein", "Jan Philip Wahle", "Bela Gipp", "Terry Ruas"], "categories": ["cs.CL", "cs.AI"], "comment": "Published in the Journal of Artificial Intelligence Research (JAIR)\n  (https://www.jair.org/index.php/jair/article/view/16674)", "summary": "Abstractive dialogue summarization is the task of distilling conversations\ninto informative and concise summaries. Although reviews have been conducted on\nthis topic, there is a lack of comprehensive work detailing the challenges of\ndialogue summarization, unifying the differing understanding of the task, and\naligning proposed techniques, datasets, and evaluation metrics with the\nchallenges. This article summarizes the research on Transformer-based\nabstractive summarization for English dialogues by systematically reviewing\n1262 unique research papers published between 2019 and 2024, relying on the\nSemantic Scholar and DBLP databases. We cover the main challenges present in\ndialog summarization (i.e., language, structure, comprehension, speaker,\nsalience, and factuality) and link them to corresponding techniques such as\ngraph-based approaches, additional training tasks, and planning strategies,\nwhich typically overly rely on BART-based encoder-decoder models. We find that\nwhile some challenges, like language, have seen considerable progress, mainly\ndue to training methods, others, such as comprehension, factuality, and\nsalience, remain difficult and hold significant research opportunities. We\ninvestigate how these approaches are typically assessed, covering the datasets\nfor the subdomains of dialogue (e.g., meeting, medical), the established\nautomatic metrics and human evaluation approaches for assessing scores and\nannotator agreement. We observe that only a few datasets span across all\nsubdomains. The ROUGE metric is the most used, while human evaluation is\nfrequently reported without sufficient detail on inner-annotator agreement and\nannotation guidelines. Additionally, we discuss the possible implications of\nthe recently explored large language models and conclude that despite a\npotential shift in relevance and difficulty, our described challenge taxonomy\nremains relevant."}
{"id": "2406.15231", "pdf": "https://arxiv.org/pdf/2406.15231.pdf", "abs": "https://arxiv.org/abs/2406.15231", "title": "Synthetic Lyrics Detection Across Languages and Genres", "authors": ["Yanis Labrak", "Markus Frohmann", "Gabriel Meseguer-Brocal", "Elena V. Epure"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Published in the TrustNLP Workshop at NAACL 2025", "summary": "In recent years, the use of large language models (LLMs) to generate music\ncontent, particularly lyrics, has gained in popularity. These advances provide\nvaluable tools for artists and enhance their creative processes, but they also\nraise concerns about copyright violations, consumer satisfaction, and content\nspamming. Previous research has explored content detection in various domains.\nHowever, no work has focused on the text modality, lyrics, in music. To address\nthis gap, we curated a diverse dataset of real and synthetic lyrics from\nmultiple languages, music genres, and artists. The generation pipeline was\nvalidated using both humans and automated methods. We performed a thorough\nevaluation of existing synthetic text detection approaches on lyrics, a\npreviously unexplored data type. We also investigated methods to adapt the\nbest-performing features to lyrics through unsupervised domain adaptation.\nFollowing both music and industrial constraints, we examined how well these\napproaches generalize across languages, scale with data availability, handle\nmultilingual language content, and perform on novel genres in few-shot\nsettings. Our findings show promising results that could inform policy\ndecisions around AI-generated music and enhance transparency for users."}
{"id": "2406.17276", "pdf": "https://arxiv.org/pdf/2406.17276.pdf", "abs": "https://arxiv.org/abs/2406.17276", "title": "OPT-Tree: Speculative Decoding with Adaptive Draft Tree Structure", "authors": ["Jikai Wang", "Yi Su", "Juntao Li", "Qingrong Xia", "Zi Ye", "Xinyu Duan", "Zhefeng Wang", "Min Zhang"], "categories": ["cs.CL"], "comment": "Published in TACL", "summary": "Autoregressive language models demonstrate excellent performance in various\nscenarios. However, the inference efficiency is limited by its\none-step-one-word generation mode, which has become a pressing problem recently\nas the models become increasingly larger. Speculative decoding employs a \"draft\nand then verify\" mechanism to allow multiple tokens to be generated in one\nstep, realizing lossless acceleration. Existing methods mainly adopt fixed\nheuristic draft structures, which fail to adapt to different situations to\nmaximize the acceptance length during verification. To alleviate this dilemma,\nwe proposed OPT-Tree, an algorithm to construct adaptive and scalable draft\ntrees. It searches the optimal tree structure that maximizes the mathematical\nexpectation of the acceptance length in each decoding step. Experimental\nresults reveal that OPT-Tree outperforms the existing draft structures and\nachieves a speed-up ratio of up to 3.2 compared with autoregressive decoding.\nIf the draft model is powerful enough and the node budget is sufficient, it can\ngenerate more than ten tokens in a single step. Our code is available at\nhttps://github.com/Jikai0Wang/OPT-Tree."}
{"id": "2409.06601", "pdf": "https://arxiv.org/pdf/2409.06601.pdf", "abs": "https://arxiv.org/abs/2409.06601", "title": "LaMsS: When Large Language Models Meet Self-Skepticism", "authors": ["Yetao Wu", "Yihong Wang", "Teng Chen", "Ningyuan Xi", "Qingqing Gu", "Hongyang Lei", "Luo Ji"], "categories": ["cs.CL", "cs.LG"], "comment": "11 pages, 6 figures, Published at ICLR 2025 Workshop on Scaling\n  Self-Improving Foundation Models,", "summary": "Hallucination is a major challenge for large language models (LLMs),\npreventing their further application in some fields. The skeptical thinking of\nhumankind could be useful for LLMs to self-cognition, self-reflection and\nalleviate their hallucinations. Inspired by this consideration, we propose a\nnovel approach called LaMsS, which combines the semantic understanding\ncapability of LLMs with self-skepticism. By introducing a series of skepticism\ntokens and augmenting them into the vocabulary, we conduct both pertaining and\nfinetuning, which allow the LLM to decode each normal token followed by a\nskeptical token, representing different skepticism levels. By calculating the\nresponse skepticism given a query, one can define a new self-aware LLM which is\nonly willing to answer with relative lower skepticism level than the threshold.\nBy examining the accuracy, AUC and AP of willingly answering questions, we\ndemonstrate that LaMsS achieves better performance than baselines on both\nmulti-choice questions and open-domain question-answering benchmarks, and can\ngeneralize to multi-task and out-of-domain settings. Our study sheds some\nlights on the self-skepticism modeling on further artificial intelligence.\nProject code and model checkpoints can be found in\nhttps://anonymous.4open.science/r/SM-1E76."}
{"id": "2409.11242", "pdf": "https://arxiv.org/pdf/2409.11242.pdf", "abs": "https://arxiv.org/abs/2409.11242", "title": "Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded Attributions and Learning to Refuse", "authors": ["Maojia Song", "Shang Hong Sim", "Rishabh Bhardwaj", "Hai Leong Chieu", "Navonil Majumder", "Soujanya Poria"], "categories": ["cs.CL"], "comment": "Published at ICLR 2025 (Oral)", "summary": "LLMs are an integral component of retrieval-augmented generation (RAG)\nsystems. While many studies focus on evaluating the overall quality of\nend-to-end RAG systems, there is a gap in understanding the appropriateness of\nLLMs for the RAG task. To address this, we introduce Trust-Score, a holistic\nmetric that evaluates the trustworthiness of LLMs within the RAG framework. Our\nresults show that various prompting methods, such as in-context learning, fail\nto effectively adapt LLMs to the RAG task as measured by Trust-Score.\nConsequently, we propose Trust-Align, a method to align LLMs for improved\nTrust-Score performance. 26 out of 27 models aligned using Trust-Align\nsubstantially outperform competitive baselines on ASQA, QAMPARI, and ELI5.\nSpecifically, in LLaMA-3-8b, Trust-Align outperforms FRONT on ASQA (up 12.56),\nQAMPARI (up 36.04), and ELI5 (up 17.69). Trust-Align also significantly\nenhances models' ability to correctly refuse and provide quality citations. We\nalso demonstrate the effectiveness of Trust-Align across different open-weight\nmodels, including the LLaMA series (1b to 8b), Qwen-2.5 series (0.5b to 7b),\nand Phi3.5 (3.8b). We release our code at\nhttps://github.com/declare-lab/trust-align."}
{"id": "2409.18986", "pdf": "https://arxiv.org/pdf/2409.18986.pdf", "abs": "https://arxiv.org/abs/2409.18986", "title": "Lab-AI: Using Retrieval Augmentation to Enhance Language Models for Personalized Lab Test Interpretation in Clinical Medicine", "authors": ["Xiaoyu Wang", "Haoyong Ouyang", "Balu Bhasuran", "Xiao Luo", "Karim Hanna", "Mia Liza A. Lustria", "Carl Yang", "Zhe He"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Accurate interpretation of lab results is crucial in clinical medicine, yet\nmost patient portals use universal normal ranges, ignoring conditional factors\nlike age and gender. This study introduces Lab-AI, an interactive system that\noffers personalized normal ranges using retrieval-augmented generation (RAG)\nfrom credible health sources. Lab-AI has two modules: factor retrieval and\nnormal range retrieval. We tested these on 122 lab tests: 40 with conditional\nfactors and 82 without. For tests with factors, normal ranges depend on\npatient-specific information. Our results show GPT-4-turbo with RAG achieved a\n0.948 F1 score for factor retrieval and 0.995 accuracy for normal range\nretrieval. GPT-4-turbo with RAG outperformed the best non-RAG system by 33.5%\nin factor retrieval and showed 132% and 100% improvements in question-level and\nlab-level performance, respectively, for normal range retrieval. These findings\nhighlight Lab-AI's potential to enhance patient understanding of lab results."}
{"id": "2409.19151", "pdf": "https://arxiv.org/pdf/2409.19151.pdf", "abs": "https://arxiv.org/abs/2409.19151", "title": "Can LLMs Really Learn to Translate a Low-Resource Language from One Grammar Book?", "authors": ["Seth Aycock", "David Stap", "Di Wu", "Christof Monz", "Khalil Sima'an"], "categories": ["cs.CL"], "comment": "Accepted at ICLR 2025 (Spotlight)", "summary": "Extremely low-resource (XLR) languages lack substantial corpora for training\nNLP models, motivating the use of all available resources such as dictionaries\nand grammar books. Machine Translation from One Book (Tanzer et al., 2024)\nsuggests that prompting long-context LLMs with one grammar book enables\nEnglish-Kalamang translation, an XLR language unseen by LLMs - a noteworthy\ncase of linguistics helping an NLP task. We investigate the source of this\ntranslation ability, finding almost all improvements stem from the book's\nparallel examples rather than its grammatical explanations. We find similar\nresults for Nepali and Guarani, seen low-resource languages, and we achieve\nperformance comparable to an LLM with a grammar book by simply fine-tuning an\nencoder-decoder translation model. We then investigate where grammar books help\nby testing two linguistic tasks, grammaticality judgment and gloss prediction,\nand we explore what kind of grammatical knowledge helps by introducing a\ntypological feature prompt that achieves leading results on these more relevant\ntasks. We thus emphasise the importance of task-appropriate data for XLR\nlanguages: parallel examples for translation, and grammatical data for\nlinguistic tasks. As we find no evidence that long-context LLMs can make\neffective use of grammatical explanations for XLR translation, we conclude data\ncollection for multilingual XLR tasks such as translation is best focused on\nparallel data over linguistic description."}
{"id": "2410.01952", "pdf": "https://arxiv.org/pdf/2410.01952.pdf", "abs": "https://arxiv.org/abs/2410.01952", "title": "TypedThinker: Diversify Large Language Model Reasoning with Typed Thinking", "authors": ["Danqing Wang", "Jianxin Ma", "Fei Fang", "Lei Li"], "categories": ["cs.CL"], "comment": "work in process", "summary": "Large Language Models (LLMs) have demonstrated strong reasoning capabilities\nin solving complex problems. However, current approaches primarily enhance\nreasoning through the elaboration of thoughts while neglecting the diversity of\nreasoning types. LLMs typically employ deductive reasoning, proceeding\nstep-by-step from given conditions, which limits their exploration during\nproblem-solving. Our analysis reveals that certain problems are exclusively\nsolvable through specific reasoning strategies like inductive, abductive, or\nanalogical reasoning. However, incorporating diverse reasoning approaches\npresents two key challenges: identifying the appropriate reasoning type for\neach problem and exploiting this approach during problem-solving. Therefore, we\npropose the TypedThinker that predicts suitable reasoning types based on the\nproblem and their previous effectiveness and provides relevant demonstrations\nto guide LLMs in applying these strategies. Experimental results show\nsignificant improvements across multiple benchmarks, with performance gains of\n3.4% for Mistral 7B, 6.5% for LLaMA3 8B, and 7% for Qwen 2 7B on logical and\nmathematical reasoning tasks. TypedThinker enhances LLM reasoning without\nrequiring knowledge distillation from larger models. It can be integrated into\nmore advanced systems like GPT-4o or specialized models like MetaMath to\ndiversify their reasoning approaches and improve their problem-solving\ncapabilities."}
{"id": "2410.02703", "pdf": "https://arxiv.org/pdf/2410.02703.pdf", "abs": "https://arxiv.org/abs/2410.02703", "title": "Selective Attention Improves Transformer", "authors": ["Yaniv Leviathan", "Matan Kalman", "Yossi Matias"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ICLR 2025", "summary": "Unneeded elements in the attention's context degrade performance. We\nintroduce Selective Attention, a simple parameter-free change to the standard\nattention mechanism which reduces attention to unneeded elements. Selective\nattention consistently improves language modeling and downstream task\nperformance in a variety of model sizes and context lengths. For example,\ntransformers trained with the language modeling objective on C4 with selective\nattention perform language modeling equivalently to standard transformers with\n~2X more heads and parameters in their attention modules. Selective attention\nalso allows decreasing the size of the attention's context buffer, leading to\nmeaningful reductions in the memory and compute requirements during inference.\nFor example, transformers trained on C4 with context sizes of 512, 1,024, and\n2,048 need 16X, 25X, and 47X less memory for their attention module,\nrespectively, when equipped with selective attention, as those without\nselective attention, with the same validation perplexity."}
{"id": "2410.05401", "pdf": "https://arxiv.org/pdf/2410.05401.pdf", "abs": "https://arxiv.org/abs/2410.05401", "title": "Post-hoc Study of Climate Microtargeting on Social Media Ads with LLMs: Thematic Insights and Fairness Evaluation", "authors": ["Tunazzina Islam", "Dan Goldwasser"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.SI"], "comment": null, "summary": "Climate change communication on social media increasingly employs\nmicrotargeting strategies to effectively reach and influence specific\ndemographic groups. This study presents a post-hoc analysis of microtargeting\npractices within climate campaigns by leveraging large language models (LLMs)\nto examine Facebook advertisements. Our analysis focuses on two key aspects:\ndemographic targeting and fairness. We evaluate the ability of LLMs to\naccurately predict the intended demographic targets, such as gender and age\ngroup, achieving an overall accuracy of 88.55%. Furthermore, we instruct the\nLLMs to generate explanations for their classifications, providing transparent\nreasoning behind each decision. These explanations reveal the specific thematic\nelements used to engage different demographic segments, highlighting distinct\nstrategies tailored to various audiences. Our findings show that young adults\nare primarily targeted through messages emphasizing activism and environmental\nconsciousness, while women are engaged through themes related to caregiving\nroles and social advocacy. In addition to evaluating the effectiveness of LLMs\nin detecting microtargeted messaging, we conduct a comprehensive fairness\nanalysis to identify potential biases in model predictions. Our findings\nindicate that while LLMs perform well overall, certain biases exist,\nparticularly in the classification of senior citizens and male audiences. By\nshowcasing the efficacy of LLMs in dissecting and explaining targeted\ncommunication strategies and by highlighting fairness concerns, this study\nprovides a valuable framework for future research aimed at enhancing\ntransparency, accountability, and inclusivity in social media-driven climate\ncampaigns."}
{"id": "2410.19878", "pdf": "https://arxiv.org/pdf/2410.19878.pdf", "abs": "https://arxiv.org/abs/2410.19878", "title": "Parameter-Efficient Fine-Tuning in Large Models: A Survey of Methodologies", "authors": ["Luping Wang", "Sheng Chen", "Linnan Jiang", "Shu Pan", "Runze Cai", "Sen Yang", "Fei Yang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The large models, as predicted by scaling raw forecasts, have made\ngroundbreaking progress in many fields, particularly in natural language\ngeneration tasks, where they have approached or even surpassed human levels.\nHowever, the unprecedented scale of their parameters brings significant\ncomputational and storage costs. These large models require substantial\ncomputational resources and GPU memory to operate. When adapting large models\nto specific downstream tasks, their massive parameter scale poses a significant\nchallenge in fine-tuning on hardware platforms with limited computational power\nand GPU memory. To address this issue, Parameter-Efficient Fine-Tuning (PEFT)\noffers a practical solution by efficiently adjusting the parameters of large\npre-trained models to suit various downstream tasks. Specifically, PEFT adjusts\nthe parameters of pre-trained large models to adapt to specific tasks or\ndomains, minimizing the introduction of additional parameters and the\ncomputational resources required. This review mainly introduces the preliminary\nknowledge of PEFT, the core ideas and principles of various PEFT algorithms,\nthe applications of PEFT, and potential future research directions. By reading\nthis review, we believe that interested parties can quickly grasp the PEFT\nmethodology, thereby accelerating its development and innovation."}
{"id": "2411.04950", "pdf": "https://arxiv.org/pdf/2411.04950.pdf", "abs": "https://arxiv.org/abs/2411.04950", "title": "Estimating the Influence of Sequentially Correlated Literary Properties in Textual Classification: A Data-Centric Hypothesis-Testing Approach", "authors": ["Gideon Yoffe", "Nachum Dershowitz", "Ariel Vishne", "Barak Sober"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce a data-centric hypothesis-testing framework to quantify the\ninfluence of sequentially correlated literary properties--such as thematic\ncontinuity--on textual classification tasks. Our method models label sequences\nas stochastic processes and uses an empirical autocovariance matrix to generate\nsurrogate labelings that preserve sequential dependencies. This enables\nstatistical testing to determine whether classification outcomes are primarily\ndriven by thematic structure or by non-sequential features like authorial\nstyle. Applying this framework across a diverse corpus of English prose, we\ncompare traditional (word n-grams and character k-mers) and neural\n(contrastively trained) embeddings in both supervised and unsupervised\nclassification settings. Crucially, our method identifies when classifications\nare confounded by sequentially correlated similarity, revealing that supervised\nand neural models are more prone to false positives--mistaking shared themes\nand cross-genre differences for stylistic signals. In contrast, unsupervised\nmodels using traditional features often yield high true positive rates with\nminimal false positives, especially in genre-consistent settings. By\ndisentangling sequential from non-sequential influences, our approach provides\na principled way to assess and interpret classification reliability. This is\nparticularly impactful for authorship attribution, forensic linguistics, and\nthe analysis of redacted or composite texts, where conventional methods may\nconflate theme with style. Our results demonstrate that controlling for\nsequential correlation is essential for reducing false positives and ensuring\nthat classification outcomes reflect genuine stylistic distinctions."}
{"id": "2412.08802", "pdf": "https://arxiv.org/pdf/2412.08802.pdf", "abs": "https://arxiv.org/abs/2412.08802", "title": "jina-clip-v2: Multilingual Multimodal Embeddings for Text and Images", "authors": ["Andreas Koukounas", "Georgios Mastrapas", "Sedigheh Eslami", "Bo Wang", "Mohammad Kalim Akram", "Michael Günther", "Isabelle Mohr", "Saba Sturua", "Nan Wang", "Han Xiao"], "categories": ["cs.CL", "cs.CV", "cs.IR", "68T50", "I.2.7; I.2.10"], "comment": "30 pages, 1-10 main paper, 10-12 refs, 12-30 benchmarks", "summary": "Contrastive Language-Image Pretraining (CLIP) has been widely used for\ncrossmodal information retrieval and multimodal understanding tasks. However,\nCLIP models are mainly optimized for crossmodal vision-language tasks and\nunderperform in single-mode text tasks. Moreover, these models are often\ntrained on English datasets and therefore lack multilingual understanding.\nAdditionally, from a visual understanding perspective, previous CLIP-based\nmodels exhibit insufficient understanding of visually rich documents. In this\nwork, we propose jina-clip-v2, a contrastive vision-language model trained on\ntext pairs, triplets and image-text pairs via a multi-task and multi-stage\ncontrastive learning paradigm in order to support both text-only and crossmodal\ntasks. We employ a multilingual text encoder and expand the training dataset to\ninclude multilingual texts from 29 non-English languages, including Hindi,\nChinese, German, French, and others, as well as images of visually rich\ndocuments. We evaluate the model's performance and show that jina-clip-v2\nachieves notable improvements over state-of-the-art CLIP-based models in\nzero-shot text-only retrieval, semantic textual similarity, and crossmodal\nretrieval tasks in both English and multilingual settings. jina-clip-v2 also\nprovides for flexibility in embedding dimensionality, enabling users to select\nthe granularity of the representations. jina-clip-v2 is publicly available at\nhttps://huggingface.co/jinaai/jina-clip-v2."}
{"id": "2501.14936", "pdf": "https://arxiv.org/pdf/2501.14936.pdf", "abs": "https://arxiv.org/abs/2501.14936", "title": "Context-Aware Neural Gradient Mapping for Fine-Grained Instruction Processing", "authors": ["David Boldo", "Lily Pemberton", "Gabriel Thistledown", "Jacob Fairchild", "Felix Kowalski"], "categories": ["cs.CL", "cs.AI"], "comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship", "summary": "The integration of contextual embeddings into the optimization processes of\nlarge language models is an advancement in natural language processing. The\nContext-Aware Neural Gradient Mapping framework introduces a dynamic gradient\nadjustment mechanism, incorporating contextual embeddings directly into the\noptimization process. This approach facilitates real-time parameter\nadjustments, enhancing task-specific generalization even in the presence of\nsparse or noisy data inputs. The mathematical foundation of this framework\nrelies on gradient descent modifications, where contextual embeddings are\nderived from a supplementary neural network trained to map input features to\noptimal adaptation gradients. By employing differential geometry principles,\nhigh-dimensional input dependencies are encoded into low-dimensional gradient\nmanifolds, enabling efficient adaptation without necessitating the retraining\nof the entire model. Empirical evaluations demonstrate that the proposed\nframework consistently outperforms baseline models across various metrics,\nincluding accuracy, robustness to noise, and computational efficiency. The\nintegration of context-specific embeddings allows for a more complex\nunderstanding of language, thereby improving the model's ability to handle\ndiverse linguistic phenomena. Furthermore, the computational efficiency\nachieved through this method demonstrates its scalability for large-scale\nlanguage models operating under diverse constraints."}
{"id": "2502.01673", "pdf": "https://arxiv.org/pdf/2502.01673.pdf", "abs": "https://arxiv.org/abs/2502.01673", "title": "Multilingual State Space Models for Structured Question Answering in Indic Languages", "authors": ["Arpita Vats", "Rahul Raja", "Mrinal Mathur", "Vinija Jain", "Aman Chadha"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at NAACL", "summary": "The diversity and complexity of Indic languages present unique challenges for\nnatural language processing (NLP) tasks, particularly in the domain of question\nanswering (QA).To address these challenges, this paper explores the application\nof State Space Models (SSMs),to build efficient and contextually aware QA\nsystems tailored for Indic languages. SSMs are particularly suited for this\ntask due to their ability to model long-term and short-term dependencies in\nsequential data, making them well-equipped to handle the rich morphology,\ncomplex syntax, and contextual intricacies characteristic of Indian languages.\nWe evaluated multiple SSM architectures across diverse datasets representing\nvarious Indic languages and conducted a comparative analysis of their\nperformance. Our results demonstrate that these models effectively capture\nlinguistic subtleties, leading to significant improvements in question\ninterpretation, context alignment, and answer generation. This work represents\nthe first application of SSMs to question answering tasks in Indic languages,\nestablishing a foundational benchmark for future research in this domain. We\npropose enhancements to existing SSM frameworks, optimizing their applicability\nto low-resource settings and multilingual scenarios prevalent in Indic\nlanguages."}
{"id": "2502.05346", "pdf": "https://arxiv.org/pdf/2502.05346.pdf", "abs": "https://arxiv.org/abs/2502.05346", "title": "Probabilistic Subspace Manifolds for Contextual Inference in Large Language Models", "authors": ["Christopher Nightingale", "Dominic Lavington", "Jonathan Thistlethwaite", "Sebastian Penhaligon", "Thomas Belinski", "David Boldo"], "categories": ["cs.CL"], "comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship", "summary": "Representing token embeddings as probability distributions over learned\nmanifolds allows for more flexible contextual inference, reducing\nrepresentational rigidity while enhancing semantic granularity. Comparative\nevaluations demonstrate that probabilistic embeddings improve neighborhood\nconsistency and decrease redundancy, ensuring that token relationships remain\nmore structurally coherent across fine-tuning iterations. The integration of\nprobabilistic subspaces within attention mechanisms facilitates more adaptive\ncontextual weighting, enabling models to capture latent dependencies that would\notherwise be obscured in conventional embeddings. Experimental results\nhighlight increased robustness against adversarial modifications, with\nprobabilistic embeddings preserving contextual integrity even under\nperturbation-based evaluation scenarios. Performance assessments indicate that\nprobabilistic representations achieve greater adaptability in domain-specific\napplications, mitigating the need for extensive retraining when shifting across\nlinguistic domains. Computational trade-offs remain within operationally\nfeasible limits, with marginal increases in inference latency balanced against\nthe benefits of enhanced representation stability and contextual\nexpressiveness. The capacity to encode structured uncertainty provides\nadvantages in generative modeling tasks, particularly where maintaining\ncoherence across extended sequences requires a representation framework capable\nof handling ambiguous or context-dependent linguistic constructs."}
{"id": "2502.11569", "pdf": "https://arxiv.org/pdf/2502.11569.pdf", "abs": "https://arxiv.org/abs/2502.11569", "title": "Towards Reasoning Ability of Small Language Models", "authors": ["Gaurav Srivastava", "Shuxiang Cao", "Xuan Wang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "# fixed some typos, added public slm reasoning leaderboard", "summary": "Reasoning has long been viewed as an emergent property of large language\nmodels (LLMs), appearing at or above a certain scale ($\\sim$100B parameters).\nHowever, recent studies challenge this assumption, showing that small language\nmodels (SLMs) can also achieve competitive reasoning performance. SLMs are\nincreasingly favored for their efficiency and deployability. However, there is\na lack of systematic study on the reasoning abilities of diverse SLMs,\nincluding those trained from scratch or derived from LLMs through quantization,\npruning, and distillation. This raises a critical question: Can SLMs achieve\nreasoning abilities comparable to LLMs? In this work, we systematically survey,\nbenchmark, and analyze 72 SLMs from six model families across 14 reasoning\nbenchmarks. For reliable evaluation, we examine four evaluation methods and\ncompare four LLM judges against human evaluations on 800 data points. We repeat\nall experiments three times to ensure a robust performance assessment.\nAdditionally, we analyze the impact of different prompting strategies in small\nmodels. Beyond accuracy, we also evaluate model robustness under adversarial\nconditions and intermediate reasoning steps. Our findings challenge the\nassumption that scaling is the only way to achieve strong reasoning. Instead,\nwe foresee a future where SLMs with strong reasoning capabilities can be\ndeveloped through structured training or post-training compression. They can\nserve as efficient alternatives to LLMs for reasoning-intensive tasks."}
{"id": "2502.13881", "pdf": "https://arxiv.org/pdf/2502.13881.pdf", "abs": "https://arxiv.org/abs/2502.13881", "title": "PSCon: Product Search Through Conversations", "authors": ["Jie Zou", "Mohammad Aliannejadi", "Evangelos Kanoulas", "Shuxi Han", "Heli Ma", "Zheng Wang", "Yang Yang", "Heng Tao Shen"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "11 pages. Accepted by SIGIR 2025", "summary": "Conversational Product Search ( CPS ) systems interact with users via natural\nlanguage to offer personalized and context-aware product lists. However, most\nexisting research on CPS is limited to simulated conversations, due to the lack\nof a real CPS dataset driven by human-like language. Moreover, existing\nconversational datasets for e-commerce are constructed for a particular market\nor a particular language and thus can not support cross-market and\nmulti-lingual usage. In this paper, we propose a CPS data collection protocol\nand create a new CPS dataset, called PSCon, which assists product search\nthrough conversations with human-like language. The dataset is collected by a\ncoached human-human data collection protocol and is available for dual markets\nand two languages. By formulating the task of CPS, the dataset allows for\ncomprehensive and in-depth research on six subtasks: user intent detection,\nkeyword extraction, system action prediction, question selection, item ranking,\nand response generation. Moreover, we present a concise analysis of the dataset\nand propose a benchmark model on the proposed CPS dataset. Our proposed dataset\nand model will be helpful for facilitating future research on CPS."}
{"id": "2502.17086", "pdf": "https://arxiv.org/pdf/2502.17086.pdf", "abs": "https://arxiv.org/abs/2502.17086", "title": "Automatically Evaluating the Paper Reviewing Capability of Large Language Models", "authors": ["Hyungyu Shin", "Jingyu Tang", "Yoonjoo Lee", "Nayoung Kim", "Hyunseung Lim", "Ji Yong Cho", "Hwajung Hong", "Moontae Lee", "Juho Kim"], "categories": ["cs.CL"], "comment": null, "summary": "Peer review is essential for scientific progress, but it faces challenges\nsuch as reviewer shortages and growing workloads. Although Large Language\nModels (LLMs) show potential for providing assistance, research has reported\nsignificant limitations in the reviews they generate. While the insights are\nvaluable, conducting the analysis is challenging due to the considerable time\nand effort required, especially given the rapid pace of LLM developments. To\naddress the challenge, we developed an automatic evaluation pipeline to assess\nthe LLMs' paper review capability by comparing them with expert-generated\nreviews. By constructing a dataset consisting of 676 OpenReview papers, we\nexamined the agreement between LLMs and experts in their strength and weakness\nidentifications. The results showed that LLMs lack balanced perspectives,\nsignificantly overlook novelty assessment when criticizing, and produce poor\nacceptance decisions. Our automated pipeline enables a scalable evaluation of\nLLMs' paper review capability over time."}
{"id": "2503.07269", "pdf": "https://arxiv.org/pdf/2503.07269.pdf", "abs": "https://arxiv.org/abs/2503.07269", "title": "SemEval-2025 Task 11: Bridging the Gap in Text-Based Emotion Detection", "authors": ["Shamsuddeen Hassan Muhammad", "Nedjma Ousidhoum", "Idris Abdulmumin", "Seid Muhie Yimam", "Jan Philip Wahle", "Terry Ruas", "Meriem Beloucif", "Christine De Kock", "Tadesse Destaw Belay", "Ibrahim Said Ahmad", "Nirmal Surange", "Daniela Teodorescu", "David Ifeoluwa Adelani", "Alham Fikri Aji", "Felermino Ali", "Vladimir Araujo", "Abinew Ali Ayele", "Oana Ignat", "Alexander Panchenko", "Yi Zhou", "Saif M. Mohammad"], "categories": ["cs.CL"], "comment": "SemEval2025 Task11 (Task Description Paper). arXiv admin note: text\n  overlap with arXiv:2502.11926", "summary": "We present our shared task on text-based emotion detection, covering more\nthan 30 languages from seven distinct language families. These languages are\npredominantly low-resource and are spoken across various continents. The data\ninstances are multi-labeled with six emotional classes, with additional\ndatasets in 11 languages annotated for emotion intensity. Participants were\nasked to predict labels in three tracks: (a) multilabel emotion detection, (b)\nemotion intensity score detection, and (c) cross-lingual emotion detection.\n  The task attracted over 700 participants. We received final submissions from\nmore than 200 teams and 93 system description papers. We report baseline\nresults, along with findings on the best-performing systems, the most common\napproaches, and the most effective methods across different tracks and\nlanguages. The datasets for this task are publicly available. The dataset is\navailable at SemEval2025 Task 11 https://brighter-dataset.github.io"}
{"id": "2503.10894", "pdf": "https://arxiv.org/pdf/2503.10894.pdf", "abs": "https://arxiv.org/abs/2503.10894", "title": "HyperDAS: Towards Automating Mechanistic Interpretability with Hypernetworks", "authors": ["Jiuding Sun", "Jing Huang", "Sidharth Baskaran", "Karel D'Oosterlinck", "Christopher Potts", "Michael Sklar", "Atticus Geiger"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ICLR 2025", "summary": "Mechanistic interpretability has made great strides in identifying neural\nnetwork features (e.g., directions in hidden activation space) that mediate\nconcepts(e.g., the birth year of a person) and enable predictable manipulation.\nDistributed alignment search (DAS) leverages supervision from counterfactual\ndata to learn concept features within hidden states, but DAS assumes we can\nafford to conduct a brute force search over potential feature locations. To\naddress this, we present HyperDAS, a transformer-based hypernetwork\narchitecture that (1) automatically locates the token-positions of the residual\nstream that a concept is realized in and (2) constructs features of those\nresidual stream vectors for the concept. In experiments with Llama3-8B,\nHyperDAS achieves state-of-the-art performance on the RAVEL benchmark for\ndisentangling concepts in hidden states. In addition, we review the design\ndecisions we made to mitigate the concern that HyperDAS (like all powerful\ninterpretabilty methods) might inject new information into the target model\nrather than faithfully interpreting it."}
{"id": "2503.21073", "pdf": "https://arxiv.org/pdf/2503.21073.pdf", "abs": "https://arxiv.org/abs/2503.21073", "title": "Shared Global and Local Geometry of Language Model Embeddings", "authors": ["Andrew Lee", "Melanie Weber", "Fernanda Viégas", "Martin Wattenberg"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Researchers have recently suggested that models share common representations.\nIn our work, we find that token embeddings of language models exhibit common\ngeometric structure. First, we find ``global'' similarities: token embeddings\noften share similar relative orientations. Next, we characterize local geometry\nin two ways: (1) by using Locally Linear Embeddings, and (2) by defining a\nsimple measure for the intrinsic dimension of each token embedding. Our\nintrinsic dimension demonstrates that token embeddings lie on a lower\ndimensional manifold. We qualitatively show that tokens with lower intrinsic\ndimensions often have semantically coherent clusters, while those with higher\nintrinsic dimensions do not. Both characterizations allow us to find\nsimilarities in the local geometry of token embeddings. Perhaps most\nsurprisingly, we find that alignment in token embeddings persists through the\nhidden states of language models, allowing us to develop an application for\ninterpretability. Namely, we introduce Emb2Emb, a simple method to transfer\nsteering vectors from one language model to another, despite the two models\nhaving different dimensions."}
{"id": "2504.02441", "pdf": "https://arxiv.org/pdf/2504.02441.pdf", "abs": "https://arxiv.org/abs/2504.02441", "title": "Cognitive Memory in Large Language Models", "authors": ["Lianlei Shan", "Shixian Luo", "Zezhou Zhu", "Yu Yuan", "Yong Wu"], "categories": ["cs.CL", "cs.AI"], "comment": "37 pages, 9 figures", "summary": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions."}
{"id": "2504.05058", "pdf": "https://arxiv.org/pdf/2504.05058.pdf", "abs": "https://arxiv.org/abs/2504.05058", "title": "Not All Data Are Unlearned Equally", "authors": ["Aravind Krishnan", "Siva Reddy", "Marius Mosbach"], "categories": ["cs.CL"], "comment": null, "summary": "Machine unlearning is concerned with the task of removing knowledge learned\nfrom particular data points from a trained model. In the context of large\nlanguage models (LLMs), unlearning has recently received increased attention,\nparticularly for removing knowledge about named entities from models for\nprivacy purposes. While various approaches have been proposed to address the\nunlearning problem, most existing approaches treat all data points to be\nunlearned equally, i.e., unlearning that Montreal is a city in Canada is\ntreated exactly the same as unlearning the phone number of the first author of\nthis paper. In this work, we show that this all data is equal assumption does\nnot hold for LLM unlearning. We study how the success of unlearning depends on\nthe frequency of the knowledge we want to unlearn in the pre-training data of a\nmodel and find that frequency strongly affects unlearning, i.e., more frequent\nknowledge is harder to unlearn. Additionally, we uncover a misalignment between\nprobability and generation-based evaluations of unlearning and show that this\nproblem worsens as models become larger. Overall, our experiments highlight the\nneed for better evaluation practices and novel methods for LLM unlearning that\ntake the training data of models into account."}
{"id": "2504.07315", "pdf": "https://arxiv.org/pdf/2504.07315.pdf", "abs": "https://arxiv.org/abs/2504.07315", "title": "Multilingual MFA: Forced Alignment on Low-Resource Related Languages", "authors": ["Alessio Tosolini", "Claire Bowern"], "categories": ["cs.CL"], "comment": null, "summary": "We compare the outcomes of multilingual and crosslingual training for related\nand unrelated Australian languages with similar phonological inventories. We\nuse the Montreal Forced Aligner to train acoustic models from scratch and adapt\na large English model, evaluating results against seen data, unseen data (seen\nlanguage), and unseen data and language. Results indicate benefits of adapting\nthe English baseline model for previously unseen languages."}
{"id": "2504.09818", "pdf": "https://arxiv.org/pdf/2504.09818.pdf", "abs": "https://arxiv.org/abs/2504.09818", "title": "Transferable text data distillation by trajectory matching", "authors": ["Rong Yao", "Hailin Hu", "Yifei Fu", "Hanting Chen", "Wenyi Fang", "Fanyi Du", "Kai Han", "Yunhe Wang"], "categories": ["cs.CL"], "comment": null, "summary": "In the realm of large language model (LLM), as the size of large models\nincreases, it also brings higher training costs. There is a urgent need to\nminimize the data size in LLM training. Compared with data selection method,\nthe data distillation method aims to synthesize a small number of data samples\nto achieve the training effect of the full data set and has better flexibility.\nDespite its successes in computer vision, the discreteness of text data has\nhitherto stymied its exploration in natural language processing (NLP). In this\nwork, we proposed a method that involves learning pseudo prompt data based on\ntrajectory matching and finding its nearest neighbor ID to achieve\ncross-architecture transfer. During the distillation process, we introduce a\nregularization loss to improve the robustness of our distilled data. To our\nbest knowledge, this is the first data distillation work suitable for text\ngeneration tasks such as instruction tuning. Evaluations on two benchmarks,\nincluding ARC-Easy and MMLU instruction tuning datasets, established the\nsuperiority of our distillation approach over the SOTA data selection method\nLESS. Furthermore, our method demonstrates a good transferability over LLM\nstructures (i.e., OPT to Llama)."}
{"id": "2504.12597", "pdf": "https://arxiv.org/pdf/2504.12597.pdf", "abs": "https://arxiv.org/abs/2504.12597", "title": "GeoSense: Evaluating Identification and Application of Geometric Principles in Multimodal Reasoning", "authors": ["Liangyu Xu", "Yingxiu Zhao", "Jingyun Wang", "Yingyao Wang", "Bu Pi", "Chen Wang", "Mingliang Zhang", "Jihao Gu", "Xiang Li", "Xiaoyong Zhu", "Jun Song", "Bo Zheng"], "categories": ["cs.CL"], "comment": "10 pages, 8 figures", "summary": "Geometry problem-solving (GPS), a challenging task requiring both visual\ncomprehension and symbolic reasoning, effectively measures the reasoning\ncapabilities of multimodal large language models (MLLMs). Humans exhibit strong\nreasoning ability in this task through accurate identification and adaptive\napplication of geometric principles within visual contexts. However, existing\nbenchmarks fail to jointly assess both dimensions of the human-like geometric\nreasoning mechanism in MLLMs, remaining a critical gap in assessing their\nability to tackle GPS. To this end, we introduce GeoSense, the first\ncomprehensive bilingual benchmark designed to systematically evaluate the\ngeometric reasoning abilities of MLLMs through the lens of geometric\nprinciples. GeoSense features a five-level hierarchical framework of geometric\nprinciples spanning plane and solid geometry, an intricately annotated dataset\nof 1,789 problems, and an innovative evaluation strategy. Through extensive\nexperiments on GeoSense with various open-source and closed-source MLLMs, we\nobserve that Gemini-2.0-pro-flash performs best, achieving an overall score of\n$65.3$. Our in-depth analysis reveals that the identification and application\nof geometric principles remain a bottleneck for leading MLLMs, jointly\nhindering their reasoning abilities. These findings underscore GeoSense's\npotential to guide future advancements in MLLMs' geometric reasoning\ncapabilities, paving the way for more robust and human-like reasoning in\nartificial intelligence."}
{"id": "2504.13471", "pdf": "https://arxiv.org/pdf/2504.13471.pdf", "abs": "https://arxiv.org/abs/2504.13471", "title": "From Large to Super-Tiny: End-to-End Optimization for Cost-Efficient LLMs", "authors": ["Jiliang Ni", "Jiachen Pu", "Zhongyi Yang", "Kun Zhou", "Hui Wang", "Xiaoliang Xiao", "Dakui Wang", "Xin Li", "Jingfeng Luo", "Conggang Hu"], "categories": ["cs.CL"], "comment": null, "summary": "In recent years, Large Language Models (LLMs) have significantly advanced\nartificial intelligence by optimizing traditional Natural Language Processing\n(NLP) pipelines, improving performance and generalization. This has spurred\ntheir integration into various systems. Many NLP systems, including ours,\nemploy a \"one-stage\" pipeline directly incorporating LLMs. While effective,\nthis approach incurs substantial costs and latency due to the need for large\nmodel parameters to achieve satisfactory outcomes. This paper introduces a\nthree-stage cost-efficient end-to-end LLM deployment pipeline-including\nprototyping, knowledge transfer, and model compression-to tackle the\ncost-performance dilemma in LLM-based frameworks. Our approach yields a super\ntiny model optimized for cost and performance in online systems, simplifying\nthe system architecture. Initially, by transforming complex tasks into a\nfunction call-based LLM-driven pipeline, an optimal performance prototype\nsystem is constructed to produce high-quality data as a teacher model. The\nsecond stage combines techniques like rejection fine-tuning, reinforcement\nlearning, and knowledge distillation to transfer knowledge to a smaller 0.5B\nstudent model, delivering effective performance at minimal cost. The final\nstage applies quantization and pruning to extremely compress models to 0.4B,\nachieving ultra-low latency and cost. The framework's modular design and\ncross-domain capabilities suggest potential applicability in other NLP areas."}
{"id": "2504.14992", "pdf": "https://arxiv.org/pdf/2504.14992.pdf", "abs": "https://arxiv.org/abs/2504.14992", "title": "Efficient Pretraining Length Scaling", "authors": ["Bohong Wu", "Shen Yan", "Sijun Zhang", "Jianqiao Lu", "Yutao Zeng", "Ya Wang", "Xun Zhou"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large language models have demonstrated the effectiveness\nof length scaling during post-training, yet its potential in pre-training\nremains underexplored. We present the Parallel Hidden Decoding Transformer\n(\\textit{PHD}-Transformer), a novel framework that enables efficient length\nscaling during pre-training while maintaining inference efficiency.\n\\textit{PHD}-Transformer achieves this through an innovative KV cache\nmanagement strategy that distinguishes between original tokens and hidden\ndecoding tokens. By retaining only the KV cache of original tokens for\nlong-range dependencies while immediately discarding hidden decoding tokens\nafter use, our approach maintains the same KV cache size as the vanilla\ntransformer while enabling effective length scaling. To further enhance\nperformance, we introduce two optimized variants: \\textit{PHD-SWA} employs\nsliding window attention to preserve local dependencies, while\n\\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate\nlinear growth in pre-filling time. Extensive experiments demonstrate consistent\nimprovements across multiple benchmarks."}
{"id": "2504.16427", "pdf": "https://arxiv.org/pdf/2504.16427.pdf", "abs": "https://arxiv.org/abs/2504.16427", "title": "Can Large Language Models Help Multimodal Language Analysis? MMLA: A Comprehensive Benchmark", "authors": ["Hanlei Zhang", "Zhuohang Li", "Yeshuang Zhu", "Hua Xu", "Peiwu Wang", "Haige Zhu", "Jie Zhou", "Jinchao Zhang"], "categories": ["cs.CL", "cs.AI", "cs.MM"], "comment": "23 pages, 5 figures", "summary": "Multimodal language analysis is a rapidly evolving field that leverages\nmultiple modalities to enhance the understanding of high-level semantics\nunderlying human conversational utterances. Despite its significance, little\nresearch has investigated the capability of multimodal large language models\n(MLLMs) to comprehend cognitive-level semantics. In this paper, we introduce\nMMLA, a comprehensive benchmark specifically designed to address this gap. MMLA\ncomprises over 61K multimodal utterances drawn from both staged and real-world\nscenarios, covering six core dimensions of multimodal semantics: intent,\nemotion, dialogue act, sentiment, speaking style, and communication behavior.\nWe evaluate eight mainstream branches of LLMs and MLLMs using three methods:\nzero-shot inference, supervised fine-tuning, and instruction tuning. Extensive\nexperiments reveal that even fine-tuned models achieve only about 60%~70%\naccuracy, underscoring the limitations of current MLLMs in understanding\ncomplex human language. We believe that MMLA will serve as a solid foundation\nfor exploring the potential of large language models in multimodal language\nanalysis and provide valuable resources to advance this field. The datasets and\ncode are open-sourced at https://github.com/thuiar/MMLA."}
{"id": "2406.14088", "pdf": "https://arxiv.org/pdf/2406.14088.pdf", "abs": "https://arxiv.org/abs/2406.14088", "title": "ReaL: Efficient RLHF Training of Large Language Models with Parameter Reallocation", "authors": ["Zhiyu Mei", "Wei Fu", "Kaiwei Li", "Guangju Wang", "Huanchen Zhang", "Yi Wu"], "categories": ["cs.DC", "cs.AI", "cs.CL", "cs.LG"], "comment": "11 pages (20 pages with references and the appendix), 17 figures.\n  Accepted by MLSys 25", "summary": "Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique for\nempowering large language model (LLM) applications. Compared with the\nsupervised training process of LLMs, the RLHF training process is much more\nsophisticated, requiring a diverse range of computation workloads with\nintricate dependencies between multiple LLM instances. Therefore, simply\nadopting the fixed parallelization strategies from supervised training for LLMs\ncan be insufficient for RLHF and result in low training efficiency. To overcome\nthis limitation, we propose a novel technique named parameter ReaLlocation,\nwhich dynamically adapts the parallelization strategies for different workloads\nduring training by redistributing LLM parameters across the training cluster.\nBuilding upon this idea, we introduce ReaL, a pioneering system for efficient\nRLHF training. ReaL introduces the concept of an execution plan, which defines\na fine-grained resource allocation and parallelization strategy particularly\ndesigned for RLHF training. Based on this concept, ReaL employs a tailored\nsearch algorithm with a lightweight run-time estimator to automatically\ndiscover an efficient execution plan for an instance of RLHF experiment.\nSubsequently, the runtime engine deploys the selected plan by effectively\nparallelizing computations and redistributing parameters. We evaluate ReaL on\nthe LLaMA models with up to 70 billion parameters and 128 GPUs. The\nexperimental results demonstrate that ReaL achieves speedups of up to\n$3.58\\times$ compared to baseline methods. Furthermore, the execution plans\ngenerated by ReaL exhibit an average of $81\\%$ performance improvement over\nheuristic approaches based on Megatron-LM in the long-context scenario. The\nsource code of ReaL is publicly available at\nhttps://github.com/openpsi-project/ReaLHF ."}
{"id": "2410.04612", "pdf": "https://arxiv.org/pdf/2410.04612.pdf", "abs": "https://arxiv.org/abs/2410.04612", "title": "Regressing the Relative Future: Efficient Policy Optimization for Multi-turn RLHF", "authors": ["Zhaolin Gao", "Wenhao Zhan", "Jonathan D. Chang", "Gokul Swamy", "Kianté Brantley", "Jason D. Lee", "Wen Sun"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success at tasks like\nsummarization that involve a single turn of interaction. However, they can\nstill struggle with multi-turn tasks like dialogue that require long-term\nplanning. Previous works on multi-turn dialogue extend single-turn\nreinforcement learning from human feedback (RLHF) methods to the multi-turn\nsetting by treating all prior dialogue turns as a long context. Such approaches\nsuffer from covariate shift: the conversations in the training set have\nprevious turns generated by some reference policy, which means that low\ntraining error may not necessarily correspond to good performance when the\nlearner is actually in the conversation loop. In response, we introduce\nREgressing the RELative FUture (REFUEL), an efficient policy optimization\napproach designed to address multi-turn RLHF in LLMs. REFUEL employs a single\nmodel to estimate $Q$-values and trains on self-generated data, addressing the\ncovariate shift issue. REFUEL frames the multi-turn RLHF problem as a sequence\nof regression tasks on iteratively collected datasets, enabling ease of\nimplementation. Theoretically, we prove that REFUEL can match the performance\nof any policy covered by the training set. Empirically, we evaluate our\nalgorithm by using Llama-3.1-70B-it to simulate a user in conversation with our\nmodel. REFUEL consistently outperforms state-of-the-art methods such as DPO and\nREBEL across various settings. Furthermore, despite having only 8 billion\nparameters, Llama-3-8B-it fine-tuned with REFUEL outperforms Llama-3.1-70B-it\non long multi-turn dialogues. Implementation of REFUEL can be found at\nhttps://github.com/ZhaolinGao/REFUEL/, and models trained by REFUEL can be\nfound at https://huggingface.co/Cornell-AGI."}
{"id": "2501.05255", "pdf": "https://arxiv.org/pdf/2501.05255.pdf", "abs": "https://arxiv.org/abs/2501.05255", "title": "CallNavi, A Challenge and Empirical Study on LLM Function Calling and Routing", "authors": ["Yewei Song", "Xunzhu Tang", "Cedric Lothritz", "Saad Ezzini", "Jacques Klein", "Tegawendé F. Bissyandé", "Andrey Boytsov", "Ulrick Ble", "Anne Goujon"], "categories": ["cs.SE", "cs.CL"], "comment": null, "summary": "API-driven chatbot systems are increasingly integral to software engineering\napplications, yet their effectiveness hinges on accurately generating and\nexecuting API calls. This is particularly challenging in scenarios requiring\nmulti-step interactions with complex parameterization and nested API\ndependencies. Addressing these challenges, this work contributes to the\nevaluation and assessment of AI-based software development through three key\nadvancements: (1) the introduction of a novel dataset specifically designed for\nbenchmarking API function selection, parameter generation, and nested API\nexecution; (2) an empirical evaluation of state-of-the-art language models,\nanalyzing their performance across varying task complexities in API function\ngeneration and parameter accuracy; and (3) a hybrid approach to API routing,\ncombining general-purpose large language models for API selection with\nfine-tuned models and prompt engineering for parameter generation. These\ninnovations significantly improve API execution in chatbot systems, offering\npractical methodologies for enhancing software design, testing, and operational\nworkflows in real-world software engineering contexts."}
{"id": "2501.15857", "pdf": "https://arxiv.org/pdf/2501.15857.pdf", "abs": "https://arxiv.org/abs/2501.15857", "title": "Are Transformers Able to Reason by Connecting Separated Knowledge in Training Data?", "authors": ["Yutong Yin", "Zhaoran Wang"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted by ICLR 2025", "summary": "Humans exhibit remarkable compositional reasoning by integrating knowledge\nfrom various sources. For example, if someone learns ( B = f(A) ) from one\nsource and ( C = g(B) ) from another, they can deduce ( C=g(B)=g(f(A)) ) even\nwithout encountering ( ABC ) together, showcasing the generalization ability of\nhuman intelligence. In this paper, we introduce a synthetic learning task,\n\"FTCT\" (Fragmented at Training, Chained at Testing), to validate the potential\nof Transformers in replicating this skill and interpret its inner mechanism. In\nthe training phase, data consist of separated knowledge fragments from an\noverall causal graph. During testing, Transformers must infer complete causal\ngraph traces by integrating these fragments. Our findings demonstrate that\nfew-shot Chain-of-Thought prompting enables Transformers to perform\ncompositional reasoning on FTCT by revealing correct combinations of fragments,\neven if such combinations were absent in the training data. Furthermore, the\nemergence of compositional reasoning ability is strongly correlated with the\nmodel complexity and training-testing data similarity. We propose, both\ntheoretically and empirically, that Transformers learn an underlying\ngeneralizable program from training, enabling effective compositional reasoning\nduring testing."}
{"id": "2503.10742", "pdf": "https://arxiv.org/pdf/2503.10742.pdf", "abs": "https://arxiv.org/abs/2503.10742", "title": "Keyframe-oriented Vision Token Pruning: Enhancing Efficiency of Large Vision Language Models on Long-Form Video Processing", "authors": ["Yudong Liu", "Jingwei Sun", "Yueqian Lin", "Jingyang Zhang", "Ming Yin", "Qinsi Wang", "Jianyi Zhang", "Hai Li", "Yiran Chen"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Vision language models (VLMs) demonstrate strong capabilities in jointly\nprocessing visual and textual data. However, they often incur substantial\ncomputational overhead due to redundant visual information, particularly in\nlong-form video scenarios. Existing approaches predominantly focus on either\nvision token pruning, which may overlook spatio-temporal dependencies, or\nkeyframe selection, which identifies informative frames but discards others,\nthus disrupting contextual continuity. In this work, we propose KVTP\n(Keyframe-oriented Vision Token Pruning), a novel framework that overcomes the\ndrawbacks of token pruning and keyframe selection. By adaptively assigning\npruning rates based on frame relevance to the query, KVTP effectively retains\nessential contextual information while significantly reducing redundant\ncomputation. To thoroughly evaluate the long-form video understanding\ncapacities of VLMs, we curated and reorganized subsets from VideoMME,\nEgoSchema, and NextQA into a unified benchmark named SparseKV-QA that\nhighlights real-world scenarios with sparse but crucial events. Our experiments\nwith VLMs of various scales show that KVTP can reduce token usage by 80%\nwithout compromising spatiotemporal and contextual consistency, significantly\ncutting computation while maintaining the performance. These results\ndemonstrate our approach's effectiveness in efficient long-video processing,\nfacilitating more scalable VLM deployment."}
{"id": "2504.11336", "pdf": "https://arxiv.org/pdf/2504.11336.pdf", "abs": "https://arxiv.org/abs/2504.11336", "title": "Looking beyond the next token", "authors": ["Abitha Thankaraj", "Yiding Jiang", "J. Zico Kolter", "Yonatan Bisk"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The structure of causal language model training assumes that each token can\nbe accurately predicted from the previous context. This contrasts with humans'\nnatural writing and reasoning process, where goals are typically known before\nthe exact argument or phrasings. While this mismatch has been well studied in\nthe literature, the working assumption has been that architectural changes are\nneeded to address this mismatch. We argue that rearranging and processing the\ntraining data sequences can allow models to more accurately imitate the true\ndata-generating process, and does not require any other changes to the\narchitecture or training infrastructure. We demonstrate that this technique,\nTrelawney, and the inference algorithms derived from it allow us to improve\nperformance on several key benchmarks that span planning, algorithmic\nreasoning, and story generation tasks. Finally, our method naturally enables\nthe generation of long-term goals at no additional cost. We investigate how\nusing the model's goal-generation capability can further improve planning and\nreasoning. Additionally, we believe Trelawney could potentially open doors to\nnew capabilities beyond the current language modeling paradigm."}
{"id": "2504.11364", "pdf": "https://arxiv.org/pdf/2504.11364.pdf", "abs": "https://arxiv.org/abs/2504.11364", "title": "Teaching Large Language Models to Reason through Learning and Forgetting", "authors": ["Tianwei Ni", "Allen Nie", "Sapana Chaudhary", "Yao Liu", "Huzefa Rangwala", "Rasool Fakoor"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Code: https://github.com/twni2016/llm-reasoning-uft", "summary": "Leveraging inference-time search in large language models has proven\neffective in further enhancing a trained model's capability to solve complex\nmathematical and reasoning problems. However, this approach significantly\nincreases computational costs and inference time, as the model must generate\nand evaluate multiple candidate solutions to identify a viable reasoning path.\nTo address this, we propose an effective approach that integrates search\ncapabilities directly into the model by fine-tuning it using both successful\n(learning) and failed reasoning paths (forgetting) derived from diverse search\nmethods. While fine-tuning the model with these data might seem\nstraightforward, we identify a critical issue: the model's search capability\ntends to degrade rapidly if fine-tuning is performed naively. We show that this\ndegradation can be substantially mitigated by employing a smaller learning\nrate. Extensive experiments on the challenging Game-of-24 and Countdown\nmathematical reasoning benchmarks show that our approach not only outperforms\nboth standard fine-tuning and inference-time search baselines but also\nsignificantly reduces inference time by 180$\\times$."}
{"id": "2504.14128", "pdf": "https://arxiv.org/pdf/2504.14128.pdf", "abs": "https://arxiv.org/abs/2504.14128", "title": "TALES: Text Adventure Learning Environment Suite", "authors": ["Christopher Zhang Cui", "Xingdi Yuan", "Ziang Xiao", "Prithviraj Ammanabrolu", "Marc-Alexandre Côté"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Reasoning is an essential skill to enable Large Language Models (LLMs) to\ninteract with the world. As tasks become more complex, they demand increasingly\nsophisticated and diverse reasoning capabilities for sequential\ndecision-making, requiring structured reasoning over the context history to\ndetermine the next best action. We introduce TALES, a diverse collection of\nsynthetic and human-written text-adventure games designed to challenge and\nevaluate diverse reasoning capabilities. We present results over a range of\nLLMs, open- and closed-weights, performing a qualitative analysis on the top\nperforming models. Despite an impressive showing on synthetic games, even the\ntop LLM-driven agents fail to achieve 15% on games designed for human\nenjoyment. Code and visualization of the experiments can be found at\nhttps://microsoft.github.io/tale-suite."}
