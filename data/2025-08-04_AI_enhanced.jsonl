{"id": "2508.00079", "pdf": "https://arxiv.org/pdf/2508.00079.pdf", "abs": "https://arxiv.org/abs/2508.00079", "title": "PhysicsEval: Inference-Time Techniques to Improve the Reasoning Proficiency of Large Language Models on Physics Problems", "authors": ["Oshayer Siddique", "J. M Areeb Uzair Alam", "Md Jobayer Rahman Rafy", "Syed Rifat Raiyan", "Hasan Mahmud", "Md Kamrul Hasan"], "categories": ["cs.CL", "cs.AI"], "comment": "Under review, 18 pages, 4 figures, 7 tables", "summary": "The discipline of physics stands as a cornerstone of human intellect, driving\nthe evolution of technology and deepening our understanding of the fundamental\nprinciples of the cosmos. Contemporary literature includes some works centered\non the task of solving physics problems - a crucial domain of natural language\nreasoning. In this paper, we evaluate the performance of frontier LLMs in\nsolving physics problems, both mathematical and descriptive. We also employ a\nplethora of inference-time techniques and agentic frameworks to improve the\nperformance of the models. This includes the verification of proposed solutions\nin a cumulative fashion by other, smaller LLM agents, and we perform a\ncomparative analysis of the performance that the techniques entail. There are\nsignificant improvements when the multi-agent framework is applied to problems\nthat the models initially perform poorly on. Furthermore, we introduce a new\nevaluation benchmark for physics problems, ${\\rm P{\\small HYSICS}E{\\small\nVAL}}$, consisting of 19,609 problems sourced from various physics textbooks\nand their corresponding correct solutions scraped from physics forums and\neducational websites. Our code and data are publicly available at\nhttps://github.com/areebuzair/PhysicsEval.", "AI": {"tldr": "Evaluation of LLM performance on solving physics problems using innovative multi-agent frameworks and new benchmarks.", "motivation": "Physics is essential for advancing technology and understanding the universe, necessitating effective natural language reasoning for solving physics problems.", "method": "We assess frontier LLMs on physics problems and apply various inference-time techniques, including a multi-agent approach where smaller LLMs verify solutions cumulatively.", "result": "The multi-agent framework significantly enhances performance on challenging problems, revealing substantial improvements over standard methods.", "conclusion": "A new evaluation benchmark, P{\u001b}YSICSVAL, consisting of over 19,000 problems, is introduced to facilitate further research and model assessment in this domain.", "key_contributions": ["Evaluation of LLMs on physics problems", "Introduction of a multi-agent framework for solution verification", "Development of a new benchmark, P{\u001b}YSICSVAL."], "limitations": "", "keywords": ["Physics", "LLM", "Natural Language Reasoning", "Multi-Agent Framework", "Benchmark"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2508.00086", "pdf": "https://arxiv.org/pdf/2508.00086.pdf", "abs": "https://arxiv.org/abs/2508.00086", "title": "Do LLMs produce texts with \"human-like\" lexical diversity?", "authors": ["Kelly Kendro", "Jeffrey Maloney", "Scott Jarvis"], "categories": ["cs.CL"], "comment": "35 pages; includes abstract", "summary": "The degree to which LLMs produce writing that is truly human-like remains\nunclear despite the extensive empirical attention that this question has\nreceived. The present study addresses this question from the perspective of\nlexical diversity. Specifically, the study investigates patterns of lexical\ndiversity in LLM-generated texts from four ChatGPT models (-3.5, -4, -o4 mini,\nand -4.5) in comparison with texts written by L1 and L2 English participants (n\n= 240) across four education levels. Six dimensions of lexical diversity were\nmeasured in each text: volume, abundance, variety-repetition, evenness,\ndisparity, and dispersion. Results from one-way MANOVAs, one-way ANOVAS, and\nSupport Vector Machines revealed that the LLM-generated texts differed\nsignificantly from human-written texts for each variable, with ChatGPT-o4 mini\nand -4.5 differing the most. Within these two groups, ChatGPT-4.5 demonstrated\nhigher levels of lexical diversity despite producing fewer tokens. The human\nwriters' lexical diversity did not differ across subgroups (i.e., education,\nlanguage status). Altogether, the results indicate that LLMs do not produce\nhuman-like texts in relation to lexical diversity, and the newer LLMs produce\nless human-like texts than older models. We discuss the implications of these\nresults for language pedagogy and related applications.", "AI": {"tldr": "This study investigates the lexical diversity of texts generated by different ChatGPT models compared to human-written texts across various education levels.", "motivation": "To determine the degree to which LLMs produce texts that resemble human writing, specifically in terms of lexical diversity.", "method": "The study measured six dimensions of lexical diversity in texts generated by four ChatGPT models and compared the results with texts written by L1 and L2 English participants (n = 240) across four education levels using statistical analysis methods, including one-way MANOVAs, ANOVAs, and Support Vector Machines.", "result": "LLM-generated texts significantly differ from human-written texts in all measured lexical diversity variables, with ChatGPT-4.5 showing higher diversity but producing fewer tokens than earlier models. Human writers' lexical diversity showed no significant difference across educational backgrounds or language status.", "conclusion": "LLMs produce less human-like texts concerning lexical diversity, and newer models are less human-like than older ones, which has implications for language pedagogy and related applications.", "key_contributions": ["Analysis of lexical diversity in LLM-generated texts", "Comparative study across different ChatGPT models", "Insights into implications for language pedagogy"], "limitations": "", "keywords": ["Large Language Models", "Lexical Diversity", "ChatGPT", "Human-Computer Interaction", "Language Pedagogy"], "importance_score": 8, "read_time_minutes": 35}}
{"id": "2508.00095", "pdf": "https://arxiv.org/pdf/2508.00095.pdf", "abs": "https://arxiv.org/abs/2508.00095", "title": "Semiotic Complexity and Its Epistemological Implications for Modeling Culture", "authors": ["Zachary K. Stine", "James E. Deitrick"], "categories": ["cs.CL", "cs.CY"], "comment": "Preprint. Manuscript currently under review", "summary": "Greater theorizing of methods in the computational humanities is needed for\nepistemological and interpretive clarity, and therefore the maturation of the\nfield. In this paper, we frame such modeling work as engaging in translation\nwork from a cultural, linguistic domain into a computational, mathematical\ndomain, and back again. Translators benefit from articulating the theory of\ntheir translation process, and so do computational humanists in their work --\nto ensure internal consistency, avoid subtle yet consequential translation\nerrors, and facilitate interpretive transparency. Our contribution in this\npaper is to lay out a particularly consequential dimension of the lack of\ntheorizing and the sorts of translation errors that emerge in our modeling\npractices as a result. Along these lines we introduce the idea of semiotic\ncomplexity as the degree to which the meaning of some text may vary across\ninterpretive lenses, and make the case that dominant modeling practices --\nespecially around evaluation -- commit a translation error by treating\nsemiotically complex data as semiotically simple when it seems\nepistemologically convenient by conferring superficial clarity. We then lay out\nseveral recommendations for researchers to better account for these\nepistemological issues in their own work.", "AI": {"tldr": "The paper discusses the need for greater theorizing in computational humanities to enhance clarity and prevent translation errors between cultural and computational domains.", "motivation": "To provide epistemological clarity in computational humanities and improve the maturity of the field by understanding translation processes.", "method": "The paper develops the concept of semiotic complexity and critiques current modeling practices that oversimplify data interpretation.", "result": "Highlights the risks involved in treating complex data as simple, which leads to significant translation errors and misinterpretations.", "conclusion": "Recommendations are provided for researchers to enhance their understanding of epistemological issues in their work.", "key_contributions": ["Introduction of the concept of semiotic complexity", "Critique of current modeling practices in computational humanities", "Recommendations for avoiding epistemological errors in translation work"], "limitations": "", "keywords": ["computational humanities", "semiotic complexity", "interpretive transparency", "translation errors", "modeling practices"], "importance_score": 2, "read_time_minutes": 15}}
{"id": "2508.00109", "pdf": "https://arxiv.org/pdf/2508.00109.pdf", "abs": "https://arxiv.org/abs/2508.00109", "title": "FACTORY: A Challenging Human-Verified Prompt Set for Long-Form Factuality", "authors": ["Mingda Chen", "Yang Li", "Xilun Chen", "Adina Williams", "Gargi Ghosh", "Scott Yih"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Long-form factuality evaluation assesses the ability of models to generate\naccurate, comprehensive responses to short prompts. Existing benchmarks often\nlack human verification, leading to potential quality issues. To address this\nlimitation, we introduce FACTORY, a large-scale, human-verified prompt set.\nDeveloped using a model-in-the-loop approach and refined by humans, FACTORY\nincludes challenging prompts that are fact-seeking, answerable, and\nunambiguous. We conduct human evaluations on 6 state-of-the-art language models\nusing FACTORY and existing datasets. Our results show that FACTORY is a\nchallenging benchmark: approximately 40% of the claims made in the responses of\nSOTA models are not factual, compared to only 10% for other datasets. Our\nanalysis identifies the strengths of FACTORY over prior benchmarks, emphasizing\nits reliability and the necessity for models to reason across long-tailed\nfacts.", "AI": {"tldr": "The paper introduces FACTORY, a human-verified benchmark for evaluating long-form factuality in language models, showing higher rates of factual inaccuracies in SOTA models compared to other datasets.", "motivation": "To address the lack of human verification in existing benchmarks for evaluating models' factual accuracy in response to prompts.", "method": "Developed a large-scale prompt set named FACTORY using a model-in-the-loop approach, refined by human evaluations.", "result": "FACTORY revealed that approximately 40% of claims from state-of-the-art models were not factual, compared to only 10% from other datasets, indicating its reliability as a benchmark.", "conclusion": "The study highlights the strengths of FACTORY while emphasizing the need for models to effectively reason across long-tailed facts.", "key_contributions": ["Introduction of a large-scale, human-verified dataset for long-form factuality evaluation", "Demonstration of substantial factual inaccuracies in existing state-of-the-art models using FACTORY", "Highlighting the importance of human verification in model evaluations."], "limitations": "The analysis is limited to six state-of-the-art models; further studies could expand the scope.", "keywords": ["long-form factuality", "language models", "benchmark evaluation"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2508.00002", "pdf": "https://arxiv.org/pdf/2508.00002.pdf", "abs": "https://arxiv.org/abs/2508.00002", "title": "ReVise: A Human-AI Interface for Incremental Algorithmic Recourse", "authors": ["Kaustav Bhattacharjee", "Jun Yuan", "Aritra Dasgupta"], "categories": ["cs.HC"], "comment": "Conditionally accepted for the IEEE VIS 2025 Short Papers track", "summary": "The recent adoption of artificial intelligence in socio-technical systems\nraises concerns about the black-box nature of the resulting decisions in fields\nsuch as hiring, finance, admissions, etc. If data subjects -- such as job\napplicants, loan applicants, and students -- receive an unfavorable outcome,\nthey may be interested in algorithmic recourse, which involves updating certain\nfeatures to yield a more favorable result when re-evaluated by algorithmic\ndecision-making. Unfortunately, when individuals do not fully understand the\nincremental steps needed to change their circumstances, they risk following\nmisguided paths that can lead to significant, long-term adverse consequences.\nExisting recourse approaches focus exclusively on the final recourse goal but\nneglect the possible incremental steps to reach the goal with real-life\nconstraints, user preferences, and model artifacts. To address this gap, we\nformulate a visual analytic workflow for incremental recourse planning in\ncollaboration with AI/ML experts and contribute an interactive visualization\ninterface that helps data subjects efficiently navigate the recourse\nalternatives and make an informed decision. We present a usage scenario and\nsubjective feedback from observational studies with twelve graduate students\nusing a real-world dataset, which demonstrates that our approach can be\ninstrumental for data subjects in choosing a suitable recourse path.", "AI": {"tldr": "This paper presents a visual analytic workflow aimed at assisting individuals in understanding and navigating the incremental steps required for algorithmic recourse in decision-making systems.", "motivation": "There is a growing concern over the black-box nature of AI decisions in socio-technical systems, which can lead to adverse outcomes for individuals seeking recourse.", "method": "We developed an interactive visualization interface designed to help users explore incremental recourse options based on real-life constraints and model artifacts.", "result": "Observational studies with twelve graduate students indicated that the workflow is effective in helping users make informed decisions regarding their recourse options.", "conclusion": "The proposed visual analytic workflow is a valuable tool for individuals to understand and effectively navigate the steps necessary for achieving favorable outcomes in algorithmic decision-making.", "key_contributions": ["Development of a visual analytic workflow for recourse planning", "An interactive interface to aid users in decision-making", "Feedback from real-world scenarios validates the approach"], "limitations": "The study involved a limited sample size and focused on a specific dataset, which may affect the generalizability of the results.", "keywords": ["algorithmic recourse", "visual analytic workflow", "interactive visualization", "socio-technical systems", "user decision support"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.00121", "pdf": "https://arxiv.org/pdf/2508.00121.pdf", "abs": "https://arxiv.org/abs/2508.00121", "title": "Is neural semantic parsing good at ellipsis resolution, or isn't it?", "authors": ["Xiao Zhang", "Johan bos"], "categories": ["cs.CL"], "comment": "Accepted by 16th IWCS", "summary": "Neural semantic parsers have shown good overall performance for a variety of\nlinguistic phenomena, reaching semantic matching scores of more than 90%. But\nhow do such parsers perform on strongly context-sensitive phenomena, where\nlarge pieces of semantic information need to be duplicated to form a meaningful\nsemantic representation? A case in point is English verb phrase ellipsis, a\nconstruct where entire verb phrases can be abbreviated by a single auxiliary\nverb. Are the otherwise known as powerful semantic parsers able to deal with\nellipsis or aren't they? We constructed a corpus of 120 cases of ellipsis with\ntheir fully resolved meaning representation and used this as a challenge set\nfor a large battery of neural semantic parsers. Although these parsers\nperformed very well on the standard test set, they failed in the instances with\nellipsis. Data augmentation", "AI": {"tldr": "Neural semantic parsers excel in general but struggle with verb phrase ellipsis, a context-sensitive linguistic phenomenon.", "motivation": "To evaluate the performance of neural semantic parsers on context-sensitive phenomena, particularly verb phrase ellipsis in English.", "method": "Constructed a corpus of 120 cases of verb phrase ellipsis with their fully resolved meaning representations to test a variety of neural semantic parsers.", "result": "Neural semantic parsers demonstrated high performance on standard datasets but failed to handle the challenging instances of ellipsis in the test set.", "conclusion": "The performance gap indicates that while neural semantic parsers are robust, they need improvement in dealing with strongly context-sensitive linguistic constructs like ellipsis.", "key_contributions": ["Creation of a specialized corpus for testing ellipsis handling in semantic parsers.", "Evaluation results showing significant performance drops on ellipsis cases compared to standard ones.", "Insight into the limitations of current neural semantic parsing technology regarding context sensitivity."], "limitations": "Only focused on verb phrase ellipsis; results may not generalize to all context-sensitive phenomena.", "keywords": ["neural semantic parsing", "verb phrase ellipsis", "context sensitivity", "linguistic phenomena", "data augmentation"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.00103", "pdf": "https://arxiv.org/pdf/2508.00103.pdf", "abs": "https://arxiv.org/abs/2508.00103", "title": "A Mixed User-Centered Approach to Enable Augmented Intelligence in Intelligent Tutoring Systems: The Case of MathAIde app", "authors": ["Guilherme Guerino", "Luiz Rodrigues", "Luana Bianchiniand Mariana Alves", "Marcelo Marinho", "Thomaz Veloso", "Valmir Macario", "Diego Dermeval", "Thales Vieira", "Ig Bittencourt", "Seiji Isotani"], "categories": ["cs.HC", "cs.AI", "68T01", "H.5.0; I.2.0"], "comment": "Article accepted in the International Journal of Human-Computer\n  Interaction", "summary": "Integrating Artificial Intelligence in Education (AIED) aims to enhance\nlearning experiences through technologies like Intelligent Tutoring Systems\n(ITS), offering personalized learning, increased engagement, and improved\nretention rates. However, AIED faces three main challenges: the critical role\nof teachers in the design process, the limitations and reliability of AI tools,\nand the accessibility of technological resources. Augmented Intelligence (AuI)\naddresses these challenges by enhancing human capabilities rather than\nreplacing them, allowing systems to suggest solutions. In contrast, humans\nprovide final assessments, thus improving AI over time. In this sense, this\nstudy focuses on designing, developing, and evaluating MathAIde, an ITS that\ncorrects mathematics exercises using computer vision and AI and provides\nfeedback based on photos of student work. The methodology included\nbrainstorming sessions with potential users, high-fidelity prototyping, A/B\ntesting, and a case study involving real-world classroom environments for\nteachers and students. Our research identified several design possibilities for\nimplementing AuI in ITSs, emphasizing a balance between user needs and\ntechnological feasibility. Prioritization and validation through prototyping\nand testing highlighted the importance of efficiency metrics, ultimately\nleading to a solution that offers pre-defined remediation alternatives for\nteachers. Real-world deployment demonstrated the usefulness of the proposed\nsolution. Our research contributes to the literature by providing a usable,\nteacher-centered design approach that involves teachers in all design phases.\nAs a practical implication, we highlight that the user-centered design approach\nincreases the usefulness and adoption potential of AIED systems, especially in\nresource-limited environments.", "AI": {"tldr": "This study presents MathAIde, an Intelligent Tutoring System (ITS) designed to enhance mathematics education by integrating Augmented Intelligence (AuI), utilizing computer vision for feedback on student work.", "motivation": "To improve learning experiences in education through personalized technologies while addressing the challenges faced by Artificial Intelligence in Education.", "method": "The methodology involved brainstorming with users, high-fidelity prototyping, A/B testing, and real-world case studies in classrooms to assess the effectiveness of the system.", "result": "The research found several design possibilities for integrating AuI in ITSs and demonstrated the practicality of the proposed solution in real classroom environments, leading to a higher adoption potential.", "conclusion": "A user-centered design approach, which includes teachers in the design process, greatly enhances the effectiveness and adoption of AIED systems, particularly in settings with limited resources.", "key_contributions": ["Development of MathAIde, an ITS that uses computer vision for feedback", "Integration of Augmented Intelligence to support teachers without replacing them", "Emphasis on a teacher-centered design approach throughout the development process."], "limitations": "", "keywords": ["Artificial Intelligence in Education", "Intelligent Tutoring Systems", "Augmented Intelligence", "User-centered design", "Mathematics education"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.00185", "pdf": "https://arxiv.org/pdf/2508.00185.pdf", "abs": "https://arxiv.org/abs/2508.00185", "title": "Comparison of Large Language Models for Deployment Requirements", "authors": ["Alper Yaman", "Jannik Schwab", "Christof Nitsche", "Abhirup Sinha", "Marco Huber"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs), such as Generative Pre-trained Transformers\n(GPTs) are revolutionizing the generation of human-like text, producing\ncontextually relevant and syntactically correct content. Despite challenges\nlike biases and hallucinations, these Artificial Intelligence (AI) models excel\nin tasks, such as content creation, translation, and code generation.\nFine-tuning and novel architectures, such as Mixture of Experts (MoE), address\nthese issues. Over the past two years, numerous open-source foundational and\nfine-tuned models have been introduced, complicating the selection of the\noptimal LLM for researchers and companies regarding licensing and hardware\nrequirements. To navigate the rapidly evolving LLM landscape and facilitate LLM\nselection, we present a comparative list of foundational and domain-specific\nmodels, focusing on features, such as release year, licensing, and hardware\nrequirements. This list is published on GitLab and will be continuously\nupdated.", "AI": {"tldr": "This paper presents a comparative list of foundational and domain-specific Large Language Models (LLMs) to assist in model selection for researchers and companies.", "motivation": "Navigating the rapidly evolving landscape of LLMs, which presents challenges in selection due to licensing and hardware requirements.", "method": "A comparative analysis of foundational and domain-specific LLMs focusing on features such as release year, licensing, and hardware requirements.", "result": "The paper provides a continuously updated list of LLMs to aid researchers and companies in selecting optimal models.", "conclusion": "The comparative list of LLMs will serve as a valuable resource for model selection amidst the complexities of the current AI landscape.", "key_contributions": ["Development of a comparative list of LLMs", "Focus on licensing and hardware requirements", "Continuous updates to the resource"], "limitations": "The list may not encompass all models available as the field is rapidly evolving, and some models may have unforeseen limitations not covered.", "keywords": ["Large Language Models", "LLM selection", "AI models", "comparative analysis"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2508.00107", "pdf": "https://arxiv.org/pdf/2508.00107.pdf", "abs": "https://arxiv.org/abs/2508.00107", "title": "Decoupling Data and Tooling in Interactive Visualization", "authors": ["Jan Simson"], "categories": ["cs.HC"], "comment": "Poster at IEEE VIS 2025", "summary": "Interactive data visualization is a major part of modern exploratory data\nanalysis, with web-based technologies enabling a rich ecosystem of both\nspecialized and general tools. However, current visualization tools often lack\nsupport for transformation or wrangling of data and are forced to re-implement\ntheir own solutions to load and ingest data. This redundancy creates\nsubstantial development overhead for tool creators, steeper learning curves for\nusers who must master different data handling interfaces across tools and a\ndegraded user experience as data handling is usually seen as an after-thought.\n  We propose a modular approach that separates data wrangling and loading\ncapabilities from visualization components. This architecture allows\nvisualization tools to concentrate on their core strengths while providing the\nopportunity to develop a unified, powerful interface for data handling. An\nadditional benefit of this approach is that it allows for multiple tools to\nexist and be used side by side. We demonstrate the feasibility of this approach\nby building an early prototype using web technologies to encapsulate\nvisualization tools and manage data flow between them.\n  We discuss future research directions, including downstream integrations with\nother tooling, such as IDEs, literate programming notebooks and applications,\nas well as incorporation of new technologies for efficient data\ntransformations. We seek input from the community to better understand the\nrequirements towards this approach.", "AI": {"tldr": "This paper proposes a modular approach for interactive data visualization that separates data wrangling and loading from visualization components, aiming to improve user experience and tool interoperability.", "motivation": "To address the redundancy and learning curve challenges faced by users and developers of current interactive data visualization tools, which often neglect data handling.", "method": "A modular architecture is proposed that decouples data wrangling and loading from visualization, allowing tools to focus on their core strengths, and demonstrated through a prototype using web technologies.", "result": "The prototype successfully encapsulates visualization tools and manages data flow between them, indicating the feasibility of the modular approach.", "conclusion": "The proposed architecture aims to enhance user experience and facilitate interoperability among visualization tools, with opportunities for future research and integration with other technologies.", "key_contributions": ["Modular architecture for data handling in visualization tools", "Prototype demonstrating the feasibility of this approach", "Future research directions targeting tool integration and technology incorporation"], "limitations": "", "keywords": ["data visualization", "data wrangling", "user experience", "HCI", "modular design"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.00217", "pdf": "https://arxiv.org/pdf/2508.00217.pdf", "abs": "https://arxiv.org/abs/2508.00217", "title": "Tabular Data Understanding with LLMs: A Survey of Recent Advances and Challenges", "authors": ["Xiaofeng Wu", "Alan Ritter", "Wei Xu"], "categories": ["cs.CL", "cs.DB", "cs.LG"], "comment": null, "summary": "Tables have gained significant attention in large language models (LLMs) and\nmultimodal large language models (MLLMs) due to their complex and flexible\nstructure. Unlike linear text inputs, tables are two-dimensional, encompassing\nformats that range from well-structured database tables to complex,\nmulti-layered spreadsheets, each with different purposes. This diversity in\nformat and purpose has led to the development of specialized methods and tasks,\ninstead of universal approaches, making navigation of table understanding tasks\nchallenging. To address these challenges, this paper introduces key concepts\nthrough a taxonomy of tabular input representations and an introduction of\ntable understanding tasks. We highlight several critical gaps in the field that\nindicate the need for further research: (1) the predominance of\nretrieval-focused tasks that require minimal reasoning beyond mathematical and\nlogical operations; (2) significant challenges faced by models when processing\ncomplex table structures, large-scale tables, length context, or multi-table\nscenarios; and (3) the limited generalization of models across different\ntabular representations and formats.", "AI": {"tldr": "This paper introduces a taxonomy for tabular inputs and discusses challenges in table understanding tasks for LLMs and MLLMs.", "motivation": "To address the complexities and diverse structures of tables in LLMs and MLLMs, which have not been effectively managed by current methods.", "method": "The paper presents a taxonomy of tabular representations and outlines various table understanding tasks, analyzing the current landscape and identifying key challenges.", "result": "Identified gaps in the literature highlighting the limitations of existing models in processing complex tabular data and the need for more comprehensive approaches.", "conclusion": "The findings stress the necessity for enhanced reasoning capabilities in LLMs and broader generalization across diverse table structures to improve table understanding.", "key_contributions": ["Introduction of a taxonomy for tabular input representations", "Identification of critical gaps in current research and model capabilities", "Highlighting the need for diverse reasoning abilities in table processing"], "limitations": "The study primarily focuses on existing gaps without providing specific solutions for the identified challenges.", "keywords": ["table understanding", "large language models", "taxonomy", "tabular data", "multimodal models"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2508.00140", "pdf": "https://arxiv.org/pdf/2508.00140.pdf", "abs": "https://arxiv.org/abs/2508.00140", "title": "Your Model Is Unfair, Are You Even Aware? Inverse Relationship Between Comprehension and Trust in Explainability Visualizations of Biased ML Models", "authors": ["Zhanna Kaufman", "Madeline Endres", "Cindy Xiong Bearfield", "Yuriy Brun"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Systems relying on ML have become ubiquitous, but so has biased behavior\nwithin them. Research shows that bias significantly affects stakeholders' trust\nin systems and how they use them. Further, stakeholders of different\nbackgrounds view and trust the same systems differently. Thus, how ML models'\nbehavior is explained plays a key role in comprehension and trust. We survey\nexplainability visualizations, creating a taxonomy of design characteristics.\nWe conduct user studies to evaluate five state-of-the-art visualization tools\n(LIME, SHAP, CP, Anchors, and ELI5) for model explainability, measuring how\ntaxonomy characteristics affect comprehension, bias perception, and trust for\nnon-expert ML users. Surprisingly, we find an inverse relationship between\ncomprehension and trust: the better users understand the models, the less they\ntrust them. We investigate the cause and find that this relationship is\nstrongly mediated by bias perception: more comprehensible visualizations\nincrease people's perception of bias, and increased bias perception reduces\ntrust. We confirm this relationship is causal: Manipulating explainability\nvisualizations to control comprehension, bias perception, and trust, we show\nthat visualization design can significantly (p < 0.001) increase comprehension,\nincrease perceived bias, and reduce trust. Conversely, reducing perceived model\nbias, either by improving model fairness or by adjusting visualization design,\nsignificantly increases trust even when comprehension remains high. Our work\nadvances understanding of how comprehension affects trust and systematically\ninvestigates visualization's role in facilitating responsible ML applications.", "AI": {"tldr": "This paper explores the relationships between explainability visualizations, comprehension, bias perception, and trust in machine learning systems through user studies.", "motivation": "There is a growing concern about biased behavior in machine learning systems, which affects stakeholder trust. Understanding how models' behaviors are explained is crucial for users with different backgrounds to build trust.", "method": "The authors surveyed existing explainability visualizations and created a taxonomy of design characteristics. They conducted user studies to evaluate five visualization tools (LIME, SHAP, CP, Anchors, and ELI5) and measured their impact on comprehension, bias perception, and trust.", "result": "The study found an inverse relationship between user comprehension and trust; better understanding led to higher bias perception, reducing trust. They confirmed this causal relationship through manipulation of explainability visualizations.", "conclusion": "Visualization design significantly influences comprehension, bias perception, and trust in machine learning systems; enhancing model fairness and visualization design can increase trust even when understanding is high.", "key_contributions": ["Creation of a taxonomy of explainability visualization design characteristics.", "User study findings showing the inverse relationship between comprehension and trust in ML.", "Demonstrated that adjusting visualizations can manage perceived bias and trust levels."], "limitations": "", "keywords": ["machine learning", "explainability", "trust", "bias perception", "visualization"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.00220", "pdf": "https://arxiv.org/pdf/2508.00220.pdf", "abs": "https://arxiv.org/abs/2508.00220", "title": "Semantic Compression for Word and Sentence Embeddings using Discrete Wavelet Transform", "authors": ["Rana Aref Salama", "Abdou Youssef", "Mona Diab"], "categories": ["cs.CL"], "comment": null, "summary": "Wavelet transforms, a powerful mathematical tool, have been widely used in\ndifferent domains, including Signal and Image processing, to unravel intricate\npatterns, enhance data representation, and extract meaningful features from\ndata. Tangible results from their application suggest that Wavelet transforms\ncan be applied to NLP capturing a variety of linguistic and semantic\nproperties. In this paper, we empirically leverage the application of Discrete\nWavelet Transforms (DWT) to word and sentence embeddings. We aim to showcase\nthe capabilities of DWT in analyzing embedding representations at different\nlevels of resolution and compressing them while maintaining their overall\nquality. We assess the effectiveness of DWT embeddings on semantic similarity\ntasks to show how DWT can be used to consolidate important semantic information\nin an embedding vector. We show the efficacy of the proposed paradigm using\ndifferent embedding models, including large language models, on downstream\ntasks. Our results show that DWT can reduce the dimensionality of embeddings by\n50-93% with almost no change in performance for semantic similarity tasks,\nwhile achieving superior accuracy in most downstream tasks. Our findings pave\nthe way for applying DWT to improve NLP applications.", "AI": {"tldr": "This paper explores the use of Discrete Wavelet Transforms (DWT) to enhance word and sentence embeddings in NLP, demonstrating significant dimensionality reduction and improved performance in semantic similarity tasks and downstream applications.", "motivation": "The use of Wavelet transforms to improve data representation and feature extraction in NLP applications, particularly in semantic similarity tasks.", "method": "Empirical application of Discrete Wavelet Transforms (DWT) to word and sentence embeddings, assessing their effectiveness in various embedding models, including large language models.", "result": "DWT can reduce embedding dimensionality by 50-93% with minimal performance loss, achieving superior accuracy in most downstream tasks compared to traditional methods.", "conclusion": "The application of DWT shows promise for enhancing NLP tasks by consolidating important semantic information in embeddings and facilitating improved performance in various applications.", "key_contributions": ["Demonstration of DWT's capability to compress embeddings while preserving quality.", "Empirical validation of DWT on various embedding models and downstream tasks.", "Introduction of a novel approach to semantic similarity tasks using wavelet transforms."], "limitations": "", "keywords": ["Discrete Wavelet Transforms", "NLP", "semantic similarity", "embeddings", "dimensionality reduction"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.00160", "pdf": "https://arxiv.org/pdf/2508.00160.pdf", "abs": "https://arxiv.org/abs/2508.00160", "title": "DeformTune: A Deformable XAI Music Prototype for Non-Musicians", "authors": ["Ziqing Xu", "Nick Bryan-Kinns"], "categories": ["cs.HC", "cs.AI", "cs.SD", "eess.AS"], "comment": "In Proceedings of Explainable AI for the Arts Workshop 2025 (XAIxArts\n  2025) arXiv:2406.14485", "summary": "Many existing AI music generation tools rely on text prompts, complex\ninterfaces, or instrument-like controls, which may require musical or technical\nknowledge that non-musicians do not possess. This paper introduces DeformTune,\na prototype system that combines a tactile deformable interface with the\nMeasureVAE model to explore more intuitive, embodied, and explainable AI\ninteraction. We conducted a preliminary study with 11 adult participants\nwithout formal musical training to investigate their experience with\nAI-assisted music creation. Thematic analysis of their feedback revealed\nrecurring challenge--including unclear control mappings, limited expressive\nrange, and the need for guidance throughout use. We discuss several design\nopportunities for enhancing explainability of AI, including multimodal feedback\nand progressive interaction support. These findings contribute early insights\ntoward making AI music systems more explainable and empowering for novice\nusers.", "AI": {"tldr": "DeformTune is a prototype system for AI-assisted music generation that uses a tactile interface to simplify interaction for non-musicians.", "motivation": "To create more intuitive and explainable AI music generation tools for users without musical training.", "method": "The paper introduces DeformTune, a prototype combining a tactile deformable interface with the MeasureVAE model, and presents findings from a study with 11 participants.", "result": "Thematic analysis of user feedback identified challenges such as unclear control mappings and limited expressiveness, highlighting design opportunities for improved explainability.", "conclusion": "The findings suggest that enhancing multimodal feedback and providing progressive interaction support can make AI music systems more accessible and empowering for novice users.", "key_contributions": ["Introduction of DeformTune, an intuitive AI music generation tool for non-musicians", "Identification of user challenges in AI music creation", "Design opportunities for enhancing explainability in AI music systems"], "limitations": "The study involves a small sample size and focuses on initial user experiences.", "keywords": ["AI music generation", "Explainability", "Human-Computer Interaction", "Deformable interface", "User experience"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.00238", "pdf": "https://arxiv.org/pdf/2508.00238.pdf", "abs": "https://arxiv.org/abs/2508.00238", "title": "Model Misalignment and Language Change: Traces of AI-Associated Language in Unscripted Spoken English", "authors": ["Bryce Anderson", "Riley Galpin", "Tom S. Juzek"], "categories": ["cs.CL", "cs.AI", "68T50", "I.2; I.2.7"], "comment": "Accepted at AIES 2025. To appear in the AIES Proceedings. 14 pages, 2\n  figures, 2 tables. Licensed under CC BY-SA 4.0", "summary": "In recent years, written language, particularly in science and education, has\nundergone remarkable shifts in word usage. These changes are widely attributed\nto the growing influence of Large Language Models (LLMs), which frequently rely\non a distinct lexical style. Divergences between model output and target\naudience norms can be viewed as a form of misalignment. While these shifts are\noften linked to using Artificial Intelligence (AI) directly as a tool to\ngenerate text, it remains unclear whether the changes reflect broader changes\nin the human language system itself. To explore this question, we constructed a\ndataset of 22.1 million words from unscripted spoken language drawn from\nconversational science and technology podcasts. We analyzed lexical trends\nbefore and after ChatGPT's release in 2022, focusing on commonly LLM-associated\nwords. Our results show a moderate yet significant increase in the usage of\nthese words post-2022, suggesting a convergence between human word choices and\nLLM-associated patterns. In contrast, baseline synonym words exhibit no\nsignificant directional shift. Given the short time frame and the number of\nwords affected, this may indicate the onset of a remarkable shift in language\nuse. Whether this represents natural language change or a novel shift driven by\nAI exposure remains an open question. Similarly, although the shifts may stem\nfrom broader adoption patterns, it may also be that upstream training\nmisalignments ultimately contribute to changes in human language use. These\nfindings parallel ethical concerns that misaligned models may shape social and\nmoral beliefs.", "AI": {"tldr": "This paper investigates lexical shifts in spoken language due to the influence of Large Language Models (LLMs), revealing significant changes post-2022 linked to LLM-associated words.", "motivation": "To explore whether shifts in word usage in written language reflect broader changes in the human language system or are merely a result of AI exposure through LLMs.", "method": "Analysis of a dataset comprising 22.1 million words from conversational science and technology podcasts, focusing on lexical trends before and after the release of ChatGPT.", "result": "A moderate yet significant increase in the usage of LLM-associated words after 2022, indicating potential convergence between human language and LLM outputs.", "conclusion": "The findings suggest an ongoing shift in language use that may involve both natural language change and the influence of AI, raising questions about the implications for social and moral beliefs.", "key_contributions": ["Construction of a large dataset analyzing spoken language shifts in relation to LLMs.", "Identification of significant lexical trends associated with LLM use.", "Discussion of the implications of AI on language and ethical concerns regarding model misalignment."], "limitations": "The study is limited by its short time frame and focuses only on spoken language from a specific context (podcasts).", "keywords": ["Large Language Models", "lexical shifts", "human language system", "AI ethics", "language change"], "importance_score": 8, "read_time_minutes": 14}}
{"id": "2508.00178", "pdf": "https://arxiv.org/pdf/2508.00178.pdf", "abs": "https://arxiv.org/abs/2508.00178", "title": "The SPACE of AI: Real-World Lessons on AI's Impact on Developers", "authors": ["Brian Houck", "Travis Lowdermilk", "Cody Beyer", "Steven Clarke", "Ben Hanrahan"], "categories": ["cs.HC", "cs.AI", "cs.SE"], "comment": null, "summary": "As artificial intelligence (AI) tools become increasingly embedded in\nsoftware development workflows, questions persist about their true impact on\ndeveloper productivity and experience. This paper presents findings from a\nmixed-methods study examining how developers perceive AI's influence across the\ndimensions of the SPACE framework: Satisfaction, Performance, Activity,\nCollaboration and Efficiency. Drawing on survey responses from over 500\ndevelopers and qualitative insights from interviews and observational studies,\nwe find that AI is broadly adopted and widely seen as enhancing productivity,\nparticularly for routine tasks. However, the benefits vary, depending on task\ncomplexity, individual usage patterns, and team-level adoption. Developers\nreport increased efficiency and satisfaction, with less evidence of impact on\ncollaboration. Organizational support and peer learning play key roles in\nmaximizing AI's value. These findings suggest that AI is augmenting developers\nrather than replacing them, and that effective integration depends as much on\nteam culture and support structures as on the tools themselves. We conclude\nwith practical recommendations for teams, organizations and researchers seeking\nto harness AI's potential in software engineering.", "AI": {"tldr": "This paper explores the impact of AI tools on developer productivity and experience through a mixed-methods study, revealing that AI enhances efficiency and satisfaction, particularly for routine tasks, while emphasizing the importance of team culture and support.", "motivation": "To investigate the true influence of AI tools on software developers' productivity and overall experience.", "method": "Mixed-methods study utilizing surveys from over 500 developers alongside qualitative insights from interviews and observational studies.", "result": "AI is broadly adopted, enhancing productivity primarily for routine tasks, with variations based on task complexity and team adoption; developers report improved efficiency and satisfaction, but less impact on collaboration.", "conclusion": "Effective integration of AI in development teams is contingent on team culture and support structures, alongside the tools themselves; recommendations are offered for maximizing AI's value.", "key_contributions": ["Empirical evidence on AI's impact using the SPACE framework", "Identification of factors influencing AI adoption and productivity", "Practical recommendations for integrating AI tools in software development"], "limitations": "Results may vary across different programming languages and environments; focus primarily on routine tasks may not generalize to all development scenarios.", "keywords": ["AI in software development", "developer productivity", "SPACE framework"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.00285", "pdf": "https://arxiv.org/pdf/2508.00285.pdf", "abs": "https://arxiv.org/abs/2508.00285", "title": "Integrating clinical reasoning into large language model-based diagnosis through etiology-aware attention steering", "authors": ["Peixian Li", "Yu Tian", "Ruiqi Tu", "Chengkai Wu", "Jingjing Ren", "Jingsong Li"], "categories": ["cs.CL", "I.2.7; J.3"], "comment": "23 pages, 8 figures", "summary": "Objective: Large Language Models (LLMs) demonstrate significant capabilities\nin medical text understanding and generation. However, their diagnostic\nreliability in complex clinical scenarios remains limited. This study aims to\nenhance LLMs' diagnostic accuracy and clinical reasoning ability. Method: We\npropose an Etiology-Aware Attention Steering Framework to integrate structured\nclinical reasoning into LLM-based diagnosis. Specifically, we first construct\nClinical Reasoning Scaffolding (CRS) based on authoritative clinical guidelines\nfor three representative acute abdominal emergencies: acute appendicitis, acute\npancreatitis, and acute cholecystitis. Next, we develop the Etiology-Aware Head\nIdentification algorithm to pinpoint attention heads crucial for the model's\netiology reasoning. To ensure reliable clinical reasoning alignment, we\nintroduce the Reasoning-Guided Parameter-Efficient Fine-tuning that embeds\netiological reasoning cues into input representations and steers the selected\nEtiology-Aware Heads toward critical information through a Reasoning-Guided\nLoss function. Result: On the Consistent Diagnosis Cohort, our framework\nimproves average diagnostic accuracy by 15.65% and boosts the average Reasoning\nFocus Score by 31.6% over baselines. External validation on the Discrepant\nDiagnosis Cohort further confirms its effectiveness in enhancing diagnostic\naccuracy. Further assessments via Reasoning Attention Frequency indicate that\nour models exhibit enhanced reliability when faced with real-world complex\nscenarios. Conclusion: This study presents a practical and effective approach\nto enhance clinical reasoning in LLM-based diagnosis. By aligning model\nattention with structured CRS, the proposed framework offers a promising\nparadigm for building more interpretable and reliable AI diagnostic systems in\ncomplex clinical settings.", "AI": {"tldr": "The paper presents a framework to enhance diagnostic accuracy and clinical reasoning in LLM-based medical diagnosis, improving reliability in complex clinical scenarios.", "motivation": "To overcome the limited diagnostic reliability of LLMs in complex clinical scenarios and improve their performance in medical diagnostics.", "method": "The study proposes an Etiology-Aware Attention Steering Framework, which incorporates structured clinical reasoning through Clinical Reasoning Scaffolding and applies a Reasoning-Guided Loss function for fine-tuning.", "result": "The framework improves average diagnostic accuracy by 15.65% and the Reasoning Focus Score by 31.6% on the Consistent Diagnosis Cohort, and shows effectiveness in external validation on the Discrepant Diagnosis Cohort.", "conclusion": "The study demonstrates a practical method for enhancing clinical reasoning in LLM-based diagnosis, offering a promising approach for developing interpretable AI diagnostic systems in complex settings.", "key_contributions": ["Etiology-Aware Attention Steering Framework", "Integration of Clinical Reasoning Scaffolding", "Reasoning-Guided Parameter-Efficient Fine-tuning"], "limitations": "", "keywords": ["Large Language Models", "Clinical Reasoning", "Medical Diagnosis", "Attention Mechanism", "Deep Learning"], "importance_score": 9, "read_time_minutes": 23}}
{"id": "2508.00211", "pdf": "https://arxiv.org/pdf/2508.00211.pdf", "abs": "https://arxiv.org/abs/2508.00211", "title": "HandOver: Enabling Precise Selection & Manipulation of 3D Objects with Mouse and Hand Tracking", "authors": ["Esen K. Tütüncü", "Mar Gonzalez-Franco", "Eric J. Gonzalez"], "categories": ["cs.HC"], "comment": "11 pages, 10 figures", "summary": "We present HandOver, an extended reality (XR) interaction technique designed\nto unify the precision of traditional mouse input for object selection with the\nexpressiveness of hand-tracking for object manipulation. With HandOver, the\nmouse is used to drive a depth-aware 3D cursor enabling precise and restful\ntargeting -by hovering their hand over the mouse, the user can then seamlessly\ntransition into direct 3D manipulation of the target object. In a formal user\nstudy, we compare HandOver against two raybased techniques: traditional\nraycasting (Ray) and a hybrid method (Ray+Hand) in a 3D docking task. Results\nshow HandOver yields lower task errors across all distances, and moreover\nimproves interaction ergonomics as highlighted by a RULA posture analysis and\nself-reported measures (NASA-TLX). These findings illustrate the benefits of\nblending traditional precise input devices with the expressive gestural inputs\nafforded by hand-tracking in XR, leading to improved user comfort and task\nperformance. This blended paradigm yields a unified workflow allowing users to\nleverage the best of each input modality as they interact in immersive\nenvironments.", "AI": {"tldr": "HandOver is an XR interaction technique combining mouse precision with hand-tracking for efficient 3D object manipulation, showing improved ergonomics and performance in user studies.", "motivation": "To enhance interaction in XR environments by blending the precision of traditional mouse input with the expressiveness of hand-tracking for object manipulation.", "method": "A user study compared the HandOver technique against traditional raycasting and a hybrid method in a 3D docking task, measuring task errors and user comfort.", "result": "HandOver demonstrated lower task errors across distances and improved ergonomics, as seen in posture analyses and self-reported comfort measures.", "conclusion": "The study supports the effectiveness of integrating different input modalities, leading to better user performance and comfort in immersive environments.", "key_contributions": ["Introduction of HandOver as a novel XR interaction technique", "Demonstration of lower task errors using HandOver", "Validation of ergonomic benefits through RULA and NASA-TLX measures"], "limitations": "The study is limited by its specific task scenario and may not generalize to all types of XR interactions.", "keywords": ["Extended Reality", "Human-Computer Interaction", "Hand-Tracking", "3D Interaction", "User Study"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.00305", "pdf": "https://arxiv.org/pdf/2508.00305.pdf", "abs": "https://arxiv.org/abs/2508.00305", "title": "Systematic Evaluation of Optimization Techniques for Long-Context Language Models", "authors": ["Ammar Ahmed", "Sheng Di", "Franck Cappello", "Zirui Liu", "Jingoo Han", "Ali Anwar"], "categories": ["cs.CL", "cs.LG", "cs.PF"], "comment": null, "summary": "Large language models (LLMs) excel across diverse natural language processing\ntasks but face resource demands and limited context windows. Although\ntechniques like pruning, quantization, and token dropping can mitigate these\nissues, their efficacy in long-context scenarios and system evaluation remains\nunderexplored. This paper systematically benchmarks these optimizations,\ncharacterizing memory usage, latency, and throughput, and studies how these\nmethods impact the quality of text generation. We first analyze individual\noptimization methods for two LLM architectures supporting long context and then\nsystematically evaluate combinations of these techniques to assess how this\ndeeper analysis impacts performance metrics. We subsequently study the\nscalability of individual optimization methods on a larger variant with 70\nbillion-parameter model. Our novel insights reveal that naive combination\ninference optimization algorithms can adversely affect larger models due to\ncompounded approximation errors, as compared to their smaller counterparts.\nExperiments show that relying solely on F1 obscures these effects by hiding\nprecision-recall trade-offs in question answering tasks. By integrating\nsystem-level profiling with task-specific insights, this study helps LLM\npractitioners and researchers explore and balance efficiency, accuracy, and\nscalability across tasks and hardware configurations.", "AI": {"tldr": "This paper benchmarks optimizations for large language models, assessing their impact on efficiency and text generation quality, particularly in long-context scenarios.", "motivation": "To address the resource demands and limited context windows of large language models and evaluate the effectiveness of existing optimization techniques in long-context scenarios.", "method": "The study systematically benchmarks individual and combined optimization techniques for large language models, analyzing memory usage, latency, throughput, and text generation quality across two LLM architectures.", "result": "The analysis reveals that naive combinations of optimization techniques can negatively impact performance in larger models due to compounded errors, and that relying on the F1 score alone may obscure trade-offs in precision and recall.", "conclusion": "Integrating system-level profiling with task-specific insights, this study aids LLM practitioners in finding a balance between efficiency, accuracy, and scalability across various tasks and hardware.", "key_contributions": ["Systematic benchmarking of LLM optimizations focusing on long context scenarios.", "Insight into negative effects of naive combination of optimization methods in larger models.", "Recommendations for balancing efficiency and accuracy in LLM implementations."], "limitations": "The study primarily focuses on specific LLM architectures and may not generalize to all model types or tasks.", "keywords": ["Large Language Models", "Optimization", "Efficiency", "Text Generation", "Benchmarking"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2508.00233", "pdf": "https://arxiv.org/pdf/2508.00233.pdf", "abs": "https://arxiv.org/abs/2508.00233", "title": "Correcting Misperceptions at a Glance: Using Data Visualizations to Reduce Political Sectarianism", "authors": ["Douglas Markant", "Subham Sah", "Alireza Karduni", "Milad Rogha", "My Thai", "Wenwen Dou"], "categories": ["cs.HC"], "comment": "11 pages, 5 figures. IEEE VIS 2025", "summary": "Political sectarianism is fueled in part by misperceptions of political\nopponents: People commonly overestimate the support for extreme policies among\nmembers of the other party. Research suggests that correcting partisan\nmisperceptions by informing people about the actual views of outparty members\nmay reduce one's own expressed support for political extremism, including\npartisan violence and anti-democratic actions. The present study investigated\nhow correction effects depend on different representations of outparty views\ncommunicated through data visualizations. We conducted an experiment with U.S.\nbased participants from Prolific (N=239 Democrats, N=244 Republicans).\nParticipants made predictions about support for political violence and\nundemocratic practices among members of their political outparty. They were\nthen presented with data from an earlier survey on the actual views of outparty\nmembers. Some participants viewed only the average response (Mean-Only\ncondition), while other groups were shown visual representations of the range\nof views from 75% of the outparty (Mean+Interval condition) or the full\ndistribution of responses (Mean+Points condition). Compared to a control group\nthat was not informed about outparty views, we observed the strongest\ncorrection effects among participants in the Mean-only and Mean+Points\ncondition, while correction effects were weaker in the Mean+Interval condition.\nIn addition, participants who observed the full distribution of out-party views\n(Mean+Points condition) were most accurate at later recalling the degree of\nsupport among the outparty. Our findings suggest that data visualizations can\nbe an important tool for correcting pervasive distortions in beliefs about\nother groups. However, the way in which variability in outparty views is\nvisualized can significantly shape how people interpret and respond to\ncorrective information.", "AI": {"tldr": "The study examines how data visualizations affect perceptions of political extremism across party lines, highlighting the effectiveness of different visualization formats in correcting misperceptions.", "motivation": "Political sectarianism is exacerbated by misperceptions about the opposition's views, which can lead to increased support for extremism. This study investigates the potential of correcting these misperceptions through data visualizations.", "method": "An experiment was conducted with 483 U.S. participants (Democrats and Republicans) who predicted outparty support for political violence and undemocratic practices after viewing different representations of data on outparty views.", "result": "Participants shown average responses (Mean-Only) and full distributions (Mean+Points) showed strong correction effects, while those presented with the range of views (Mean+Interval) exhibited weaker effects. Full distribution viewers were more accurate in later recall of outparty support levels.", "conclusion": "Data visualizations are effective in correcting beliefs about outparty views, but the method of representation significantly impacts how corrections are interpreted and applied.", "key_contributions": ["Demonstrated the effectiveness of data visualizations in correcting political misperceptions.", "Investigated the impact of different visualization methods on correction effectiveness.", "Provided empirical evidence on the relationship between visualization formats and recall accuracy of outparty views."], "limitations": "Findings are limited to U.S. participants and may not generalize across different political contexts or cultures.", "keywords": ["Political sectarianism", "Data visualizations", "Correction effects"], "importance_score": 2, "read_time_minutes": 11}}
{"id": "2508.00332", "pdf": "https://arxiv.org/pdf/2508.00332.pdf", "abs": "https://arxiv.org/abs/2508.00332", "title": "Improving Multimodal Contrastive Learning of Sentence Embeddings with Object-Phrase Alignment", "authors": ["Kaiyan Zhao", "Zhongtao Miao", "Yoshimasa Tsuruoka"], "categories": ["cs.CL"], "comment": "Work in progress", "summary": "Multimodal sentence embedding models typically leverage image-caption pairs\nin addition to textual data during training. However, such pairs often contain\nnoise, including redundant or irrelevant information on either the image or\ncaption side. To mitigate this issue, we propose MCSEO, a method that enhances\nmultimodal sentence embeddings by incorporating fine-grained object-phrase\nalignment alongside traditional image-caption alignment. Specifically, MCSEO\nutilizes existing segmentation and object detection models to extract accurate\nobject-phrase pairs, which are then used to optimize a contrastive learning\nobjective tailored to object-phrase correspondence. Experimental results on\nsemantic textual similarity (STS) tasks across different backbone models\ndemonstrate that MCSEO consistently outperforms strong baselines, highlighting\nthe significance of precise object-phrase alignment in multimodal\nrepresentation learning.", "AI": {"tldr": "MCSEO enhances multimodal sentence embeddings by improving object-phrase alignment, using segmentation and object detection to optimize a contrastive learning objective.", "motivation": "To address the noise in image-caption pairs which can contain irrelevant information, and to improve the accuracy of multimodal sentence embeddings.", "method": "MCSEO incorporates fine-grained object-phrase alignment alongside traditional image-caption alignment, utilizing segmentation and object detection models for accurate object-phrase pairs.", "result": "Experimental results indicate that MCSEO outperforms strong baselines in semantic textual similarity tasks across various backbone models.", "conclusion": "Precise object-phrase alignment is crucial for effective multimodal representation learning.", "key_contributions": ["Development of MCSEO that leverages fine-grained object-phrase alignment", "Demonstration of the effectiveness of MCSEO in multimodal embeddings", "Highlighting the importance of object-phrase correspondence in multimodal learning"], "limitations": "", "keywords": ["multimodal embeddings", "object-phrase alignment", "contrastive learning"], "importance_score": 5, "read_time_minutes": 5}}
{"id": "2508.00239", "pdf": "https://arxiv.org/pdf/2508.00239.pdf", "abs": "https://arxiv.org/abs/2508.00239", "title": "What's Behind the Magic? Audiences Seek Artistic Value in Generative AI's Contributions to a Live Dance Performance", "authors": ["Jacqueline Elise Bruen", "Myounghoon Jeon"], "categories": ["cs.HC", "cs.AI"], "comment": "In Proceedings of Explainable AI for the Arts Workshop 2025 (XAIxArts\n  2025) arXiv:2406.14485", "summary": "With the development of generative artificial intelligence (GenAI) tools to\ncreate art, stakeholders cannot come to an agreement on the value of these\nworks. In this study we uncovered the mixed opinions surrounding art made by\nAI. We developed two versions of a dance performance augmented by technology\neither with or without GenAI. For each version we informed audiences of the\nperformance's development either before or after a survey on their perceptions\nof the performance. There were thirty-nine participants (13 males, 26 female)\ndivided between the four performances. Results demonstrated that individuals\nwere more inclined to attribute artistic merit to works made by GenAI when they\nwere unaware of its use. We present this case study as a call to address the\nimportance of utilizing the social context and the users' interpretations of\nGenAI in shaping a technical explanation, leading to a greater discussion that\ncan bridge gaps in understanding.", "AI": {"tldr": "This study explores perceptions of generative AI in art, revealing that individuals attribute more artistic merit to AI-created works when unaware of AI's involvement.", "motivation": "To understand the mixed opinions surrounding art created by generative AI and its implications on artistic merit.", "method": "Developed two versions of a dance performance, one augmented by GenAI and one without. Audiences were surveyed on their perceptions before or after being informed of GenAI's use.", "result": "Participants attributed more artistic merit to GenAI art when they were unaware of its involvement, highlighting the influence of social context on perceptions.", "conclusion": "The study calls for addressing the role of social context in understanding generative AI's contributions to art, to foster better discussions about its implications.", "key_contributions": ["Empirical evidence on perceptions of AI-generated art", "Comparison of audience responses based on awareness of AI use", "Discussion on the importance of social context in interpreting GenAI works"], "limitations": "", "keywords": ["Generative AI", "Art perception", "Dance performance", "User interpretation", "Social context"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.00344", "pdf": "https://arxiv.org/pdf/2508.00344.pdf", "abs": "https://arxiv.org/abs/2508.00344", "title": "PilotRL: Training Language Model Agents via Global Planning-Guided Progressive Reinforcement Learning", "authors": ["Keer Lu", "Chong Chen", "Bin Cui", "Huang Leng", "Wentao Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable advancements in tackling\nagent-oriented tasks. Despite their potential, existing work faces challenges\nwhen deploying LLMs in agent-based environments. The widely adopted agent\nparadigm ReAct centers on integrating single-step reasoning with immediate\naction execution, which limits its effectiveness in complex tasks requiring\nlong-term strategic planning. Furthermore, the coordination between the planner\nand executor during problem-solving is also a critical factor to consider in\nagent design. Additionally, current approaches predominantly rely on supervised\nfine-tuning, which often leads models to memorize established task completion\ntrajectories, thereby restricting their generalization ability when confronted\nwith novel problem contexts. To address these challenges, we introduce an\nadaptive global plan-based agent paradigm AdaPlan, aiming to synergize\nhigh-level explicit guidance with execution to support effective long-horizon\ndecision-making. Based on the proposed paradigm, we further put forward\nPilotRL, a global planning-guided training framework for LLM agents driven by\nprogressive reinforcement learning. We first develop the model's ability to\nfollow explicit guidance from global plans when addressing agent tasks.\nSubsequently, based on this foundation, we focus on optimizing the quality of\ngenerated plans. Finally, we conduct joint optimization of the model's planning\nand execution coordination. Experiments indicate that PilotRL could achieve\nstate-of-the-art performances, with LLaMA3.1-8B-Instruct + PilotRL surpassing\nclosed-sourced GPT-4o by 3.60%, while showing a more substantial gain of 55.78%\ncomparing to GPT-4o-mini at a comparable parameter scale.", "AI": {"tldr": "This paper introduces AdaPlan, an adaptive global plan-based agent paradigm, and PilotRL, a training framework for LLM agents that enhances long-term decision-making and coordination between planning and execution.", "motivation": "Existing agent paradigms limit LLM effectiveness in complex tasks requiring long-term planning and struggle with generalization due to reliance on supervised fine-tuning.", "method": "Introduce AdaPlan for high-level explicit guidance in decision-making and develop PilotRL to optimize planning with progressive reinforcement learning.", "result": "PilotRL achieves state-of-the-art performance, outperforming GPT-4o by 3.60% and GPT-4o-mini by 55.78% in related tasks.", "conclusion": "The proposed methodologies enhance LLM agents' abilities in long-horizon decision-making and coordination, addressing key challenges in agent design.", "key_contributions": ["Introduction of the AdaPlan paradigm for enhanced decision-making in agent tasks", "Development of PilotRL for global planning-guided training", "Demonstrated improvements over current state-of-the-art LLM models in performance."], "limitations": "", "keywords": ["Large Language Models", "Human-Agent Interaction", "Reinforcement Learning", "Decision-Making", "Planning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.00252", "pdf": "https://arxiv.org/pdf/2508.00252.pdf", "abs": "https://arxiv.org/abs/2508.00252", "title": "TofuML: A Spatio-Physical Interactive Machine Learning Device for Interactive Exploration of Machine Learning for Novices", "authors": ["Wataru Kawabe", "Hiroto Fukuda", "Akihisa Shitara", "Yuri Nakao", "Yusuke Sugano"], "categories": ["cs.HC"], "comment": "31 pages", "summary": "We introduce TofuML, an interactive system designed to make machine learning\n(ML) concepts more accessible and engaging for non-expert users. Unlike\nconventional GUI-based systems, TofuML employs a physical and spatial interface\nconsisting of a small device and a paper mat, allowing users to train and\nevaluate sound classification models through intuitive, toy-like interactions.\nThrough two user studies -- a comparative study against a GUI-based version and\na public event deployment -- we investigated how TofuML impacts users'\nengagement in the ML model creation process, their ability to provide\nappropriate training data, and their conception of potential applications. Our\nresults indicated that TofuML enhanced user engagement compared to a GUI while\nlowering barriers for non-experts to engage with ML. Users demonstrated\ncreativity in conceiving diverse ML applications, revealing opportunities to\noptimize between conceptual understanding and user engagement. These findings\ncontribute to developing interactive ML systems/frameworks designed for a wide\nrange of users.", "AI": {"tldr": "TofuML is an interactive system for making machine learning concepts more accessible through a physical interface, enhancing user engagement and creativity in ML applications.", "motivation": "The motivation behind TofuML is to make machine learning concepts more accessible and engaging for non-expert users by employing an interactive, physical, and spatial interface.", "method": "TofuML employs a small device and a paper mat for users to train and evaluate sound classification models using intuitive interactions. This was tested through two user studies: a comparative study with a GUI-based version and a deployment during a public event.", "result": "Results showed that TofuML significantly enhanced user engagement compared to traditional GUI systems, helping non-experts engage with ML and demonstrating creativity in conceptualizing diverse ML applications.", "conclusion": "TofuML provides insights into optimizing user engagement and conceptual understanding in interactive ML systems, encouraging the design of frameworks that cater to a wider audience.", "key_contributions": ["Introduction of a physical interface for ML training", "Enhanced user engagement through intuitive interactions", "Insights on user creativity in ML applications"], "limitations": "", "keywords": ["TofuML", "machine learning", "human-computer interaction", "user engagement", "interactive systems"], "importance_score": 8, "read_time_minutes": 31}}
{"id": "2508.00360", "pdf": "https://arxiv.org/pdf/2508.00360.pdf", "abs": "https://arxiv.org/abs/2508.00360", "title": "Lucy: edgerunning agentic web search on mobile with machine generated task vectors", "authors": ["Alan Dao", "Dinh Bach Vu", "Alex Nguyen", "Norapat Buppodom"], "categories": ["cs.CL"], "comment": null, "summary": "Small language models (SLMs) are inherently limited in knowledge-intensive\ntasks due to their constrained capacity. While test-time computation offers a\npath to enhanced performance, most approaches treat reasoning as a fixed or\nheuristic process. In this work, we propose a new paradigm: viewing the model's\ninternal reasoning, delimited by <think> and </think> tags, as a dynamic task\nvector machine. Rather than treating the content inside these tags as a mere\ntrace of thought, we interpret the generation process itself as a mechanism\nthrough which the model \\textbf{constructs and refines its own task vectors} on\nthe fly. We developed a method to optimize this dynamic task vector machine\nthrough RLVR and successfully trained an agentic web-search model. We present\nLucy, a 1.7B-parameter SLM that leverages this dynamic reasoning mechanism with\nMCP integration to achieve 78.3% accuracy on the SimpleQA benchmark, performing\non par with much larger models such as DeepSeek-V3. This demonstrates that\nsmall models can rival large ones when equipped with structured,\nself-constructed task reasoning.", "AI": {"tldr": "This paper introduces Lucy, a 1.7B-parameter small language model that utilizes a dynamic task vector machine for enhanced reasoning, achieving competitive performance with larger models on the SimpleQA benchmark.", "motivation": "Address the limitations of small language models (SLMs) in knowledge-intensive tasks by developing a method that enhances their reasoning capabilities through dynamic task vector construction.", "method": "The authors optimize a dynamic task vector machine through RLVR, interpreting the model's internal reasoning as an evolving process rather than a fixed one.", "result": "Lucy achieves 78.3% accuracy on the SimpleQA benchmark, which is comparable to larger models like DeepSeek-V3.", "conclusion": "Structured, self-constructed task reasoning allows small models to rival larger counterparts, suggesting a paradigm shift in how we utilize SLMs.", "key_contributions": ["Introduction of a dynamic task vector machine for SLMs", "Development of the Lucy model with enhanced reasoning capabilities", "Demonstration of SLMs' competitive performance on knowledge-intensive tasks"], "limitations": "", "keywords": ["small language models", "dynamic task vector", "reinforcement learning", "knowledge-intensive tasks", "natural language processing"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.00300", "pdf": "https://arxiv.org/pdf/2508.00300.pdf", "abs": "https://arxiv.org/abs/2508.00300", "title": "MetaExplainer: A Framework to Generate Multi-Type User-Centered Explanations for AI Systems", "authors": ["Shruthi Chari", "Oshani Seneviratne", "Prithwish Chakraborty", "Pablo Meyer", "Deborah L. McGuinness"], "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": null, "summary": "Explanations are crucial for building trustworthy AI systems, but a gap often\nexists between the explanations provided by models and those needed by users.\nTo address this gap, we introduce MetaExplainer, a neuro-symbolic framework\ndesigned to generate user-centered explanations. Our approach employs a\nthree-stage process: first, we decompose user questions into machine-readable\nformats using state-of-the-art large language models (LLM); second, we delegate\nthe task of generating system recommendations to model explainer methods; and\nfinally, we synthesize natural language explanations that summarize the\nexplainer outputs. Throughout this process, we utilize an Explanation Ontology\nto guide the language models and explainer methods. By leveraging LLMs and a\nstructured approach to explanation generation, MetaExplainer aims to enhance\nthe interpretability and trustworthiness of AI systems across various\napplications, providing users with tailored, question-driven explanations that\nbetter meet their needs. Comprehensive evaluations of MetaExplainer demonstrate\na step towards evaluating and utilizing current state-of-the-art explanation\nframeworks. Our results show high performance across all stages, with a 59.06%\nF1-score in question reframing, 70% faithfulness in model explanations, and 67%\ncontext-utilization in natural language synthesis. User studies corroborate\nthese findings, highlighting the creativity and comprehensiveness of generated\nexplanations. Tested on the Diabetes (PIMA Indian) tabular dataset,\nMetaExplainer supports diverse explanation types, including Contrastive,\nCounterfactual, Rationale, Case-Based, and Data explanations. The framework's\nversatility and traceability from using ontology to guide LLMs suggest broad\napplicability beyond the tested scenarios, positioning MetaExplainer as a\npromising tool for enhancing AI explainability across various domains.", "AI": {"tldr": "MetaExplainer is a neuro-symbolic framework that generates user-centered explanations by decomposing user questions, generating recommendations, and synthesizing natural language explanations using an Explanation Ontology and large language models.", "motivation": "To bridge the gap between model-provided explanations and user needs in AI systems, enhancing trustworthiness and interpretability.", "method": "A three-stage process involving question decomposition using LLMs, delegating recommendations to explainers, and synthesizing natural language summaries, guided by an Explanation Ontology.", "result": "Achieved high performance with a 59.06% F1-score in question reframing, 70% faithfulness in model explanations, and 67% context-utilization in synthesis. User studies showed positive reception of the explanations.", "conclusion": "MetaExplainer effectively generates tailored explanations and shows promise for enhancing AI explainability in diverse applications beyond the tested scenarios.", "key_contributions": ["Introduction of a neuro-symbolic framework for generating user-centered AI explanations.", "Utilization of an Explanation Ontology to guide LLMs and explainers in the explanation generation process.", "Demonstration of high performance and user satisfaction in providing diverse types of explanations."], "limitations": "", "keywords": ["explainable AI", "user-centered explanations", "neuro-symbolic framework", "large language models", "machine learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.00370", "pdf": "https://arxiv.org/pdf/2508.00370.pdf", "abs": "https://arxiv.org/abs/2508.00370", "title": "EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level Efficiency for Edge Devices", "authors": ["Jiyu Chen", "Poh Seng Lim", "Shuang Peng", "Daxiong Luo", "JungHau Foo", "Yap Deep", "Timothy Lee Jun Jie", "Kelvin Teh Kae Wen", "Fan Yang", "Danyu Feng", "Hao-Yun Chen", "Peng-Wen Chen", "Fangyuan Li", "Xiaoxin Chen", "Wong Wai Mun"], "categories": ["cs.CL", "cs.LG"], "comment": "9 pages", "summary": "Deploying Transformer-based large language models (LLMs) on\nresource-constrained edge devices for long-sequence tasks remains challenging\ndue to the quadratic time complexity of self-attention and growing Key-Value\n(KV) cache demands. While existing KV cache optimizations improve memory\nefficiency, they often fail to reduce time to first token (TTFT) and may\ndegrade performance through token pruning. Alternative sequence modeling\narchitectures address some of these limitations, but typically require full\nretraining and lack infrastructure support. EdgeInfinite offers an efficient\nsolution by fine-tuning only a small subset of parameters, maintaining quality\nwhile reducing both computational and memory costs, including improved TTFT.\nHowever, its instruction-following ability is limited, and it lacks\nmobile-specific optimizations. To address these issues, we propose\nEdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning\n(S-SFT) strategy tailored to long-sequence tasks such as summarization and\nquestion answering. We further optimized EdgeInfinite-Instruct for efficient\ndeployment on edge NPUs by employing fine-grained post-training quantization\n(PTQ) to reduce computational demands while maintaining accuracy, and by\nimplementing a fixed-shape computation graph that balances memory usage and\non-device efficiency through scenario-specific customization of input token and\ncache sizes. Experiments on long-context benchmarks and real-world mobile tasks\nshow that our approach improves domain-specific performance while maintaining\nefficiency on NPU-accelerated edge devices.", "AI": {"tldr": "This paper presents EdgeInfinite-Instruct, a fine-tuned model for deploying Transformer-based LLMs on resource-constrained edge devices, focusing on efficiency improvements for long-sequence tasks like summarization and question answering.", "motivation": "Deploying LLMs on edge devices is challenging due to computational costs and efficiency issues. Existing optimizations are insufficient for reducing time to first token and maintaining performance.", "method": "The paper proposes EdgeInfinite-Instruct, which utilizes a Segmented Supervised Fine-Tuning strategy and fine-grained post-training quantization to enhance deployment efficiency on edge NPUs.", "result": "Experiments demonstrate that EdgeInfinite-Instruct improves domain-specific performance while retaining efficiency on long-context benchmarks and real-world mobile tasks.", "conclusion": "EdgeInfinite-Instruct effectively addresses limitations of previous models, offering optimized deployment for LLMs on edge devices without compromising performance.", "key_contributions": ["Introduction of Segmented Supervised Fine-Tuning strategy for long-sequence tasks", "Optimization for NPU-accelerated edge devices through post-training quantization", "Maintaining computational efficiency while improving model performance"], "limitations": "Instruction-following ability is limited and lack of mobile-specific optimizations.", "keywords": ["Transformer", "Large Language Models", "Edge Computing", "Fine-tuning", "Post-training Quantization"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.00321", "pdf": "https://arxiv.org/pdf/2508.00321.pdf", "abs": "https://arxiv.org/abs/2508.00321", "title": "Evaluating the Efficacy of Large Language Models for Generating Fine-Grained Visual Privacy Policies in Homes", "authors": ["Shuning Zhang", "Ying Ma", "Xin Yi", "Hewu Li"], "categories": ["cs.HC"], "comment": null, "summary": "The proliferation of visual sensors in smart home environments, particularly\nthrough wearable devices like smart glasses, introduces profound privacy\nchallenges. Existing privacy controls are often static and coarse-grained,\nfailing to accommodate the dynamic and socially nuanced nature of home\nenvironments. This paper investigates the viability of using Large Language\nModels (LLMs) as the core of a dynamic and adaptive privacy policy engine. We\npropose a conceptual framework where visual data is classified using a\nmulti-dimensional schema that considers data sensitivity, spatial context, and\nsocial presence. An LLM then reasons over this contextual information to\nenforce fine-grained privacy rules, such as selective object obfuscation, in\nreal-time. Through a comparative evaluation of state-of-the-art Vision Language\nModels (including GPT-4o and the Qwen-VL series) in simulated home settings ,\nour findings show the feasibility of this approach. The LLM-based engine\nachieved a top machine-evaluated appropriateness score of 3.99 out of 5, and\nthe policies generated by the models received a top human-evaluated score of\n4.00 out of 5.", "AI": {"tldr": "This paper explores the use of Large Language Models (LLMs) to create dynamic privacy policies for visual sensors in smart home environments, achieving notable performance in policy generation.", "motivation": "The advent of wearable visual sensors such as smart glasses presents complex privacy challenges that current static privacy controls cannot address effectively.", "method": "The paper proposes a framework where visual data is classified based on sensitivity, context, and social presence. An LLM reasons over this data to enforce adaptive privacy rules in real-time.", "result": "The LLM-based privacy policy engine demonstrated effective real-time privacy management in simulated settings, with high evaluation scores from both machine and human assessments.", "conclusion": "The approach shows promise for implementing fine-grained, context-aware privacy controls in smart home environments using LLMs.", "key_contributions": ["Introduction of a dynamic privacy policy engine using LLMs", "Development of a multi-dimensional schema for visual data classification", "Demonstration of effective real-time privacy rule enforcement in simulated environments"], "limitations": "", "keywords": ["privacy", "smart homes", "large language models", "visual sensors", "dynamic policies"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.00385", "pdf": "https://arxiv.org/pdf/2508.00385.pdf", "abs": "https://arxiv.org/abs/2508.00385", "title": "Multi-Layer Attention is the Amplifier of Demonstration Effectiveness", "authors": ["Dingzirui Wang", "Xuangliang Zhang", "Keyan Xu", "Qingfu Zhu", "Wanxiang Che", "Yang Deng"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Numerous studies have investigated the underlying mechanisms of in-context\nlearning (ICL) effectiveness to inspire the design of related methods. However,\nexisting work predominantly assumes the effectiveness of the demonstrations\nprovided within ICL, while many research indicates that not all demonstrations\nare effective, failing to yielding any performance improvement during ICL.\nTherefore, in this paper, we investigate the reasons behind demonstration\nineffectiveness. Our analysis is based on gradient flow and linear\nself-attention models. By setting the gradient flow to zero, we deduce that a\ndemonstration becomes ineffective if its information has either been learned by\nthe model or is irrelevant to the user query. Furthermore, we demonstrate that\nin multi-layer models, the disparity in effectiveness among demonstrations is\namplified with layer increasing, causing the model to focus more on effective\nones. Considering that current demonstration selection methods primarily focus\non the relevance to the user query while overlooking the information that the\nmodel has already assimilated, we propose a novel method called GradS, which\nleverages gradient flow for demonstration selection. We use the magnitude of\nthe gradient flow of the demonstration with respect to a given user query as\nthe criterion, thereby ensuring the effectiveness of the chosen ones. We\nvalidate our derivation and GradS on four prominent LLMs across five mainstream\ndatasets. The experimental results confirm that the disparity in effectiveness\namong demonstrations is magnified as the model layer increases, substantiating\nour derivations. Moreover, GradS achieves a relative improvement of $6.8\\%$ on\naverage over the strongest baselines, demonstrating its effectiveness.", "AI": {"tldr": "This paper investigates the ineffectiveness of demonstrations in in-context learning (ICL), proposing a novel selection method, GradS, that improves performance by leveraging gradient flow.", "motivation": "Existing studies on in-context learning assume all demonstrations are effective, but many are not, necessitating investigation into the reasons behind demonstration ineffectiveness.", "method": "The authors analyze gradient flow and linear self-attention models, proposing GradS, which selects demonstrations based on the gradient flow magnitude with respect to the user query.", "result": "GradS proves to significantly improve ICL effectiveness, achieving a relative improvement of 6.8% over the strongest baselines and confirming the amplification of demonstration effectiveness disparity in deeper models.", "conclusion": "The study emphasizes the importance of considering both the relevance to the user query and the information already learned by the model in demonstration selection for ICL.", "key_contributions": ["Investigation of demonstration ineffectiveness in ICL", "Introduction of GradS for effective demonstration selection using gradient flow", "Empirical validation on four LLMs across five datasets."], "limitations": "", "keywords": ["in-context learning", "demonstration selection", "gradient flow", "LLMs", "machine learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.00328", "pdf": "https://arxiv.org/pdf/2508.00328.pdf", "abs": "https://arxiv.org/abs/2508.00328", "title": "From Patient Burdens to User Agency: Designing for Real-Time Protection Support in Online Health Consultations", "authors": ["Shuning Zhang", "Ying Ma", "Yongquan `Owen' Hu", "Ting Dang", "Hong Jia", "Xin Yi", "Hewu Li"], "categories": ["cs.HC"], "comment": null, "summary": "Online medical consultation platforms, while convenient, are undermined by\nsignificant privacy risks that erode user trust. We first conducted in-depth\nsemi-structured interviews with 12 users to understand their perceptions of\nsecurity and privacy landscapes on online medical consultation platforms, as\nwell as their practices, challenges and expectation. Our analysis reveals a\ncritical disconnect between users' desires for anonymity and control, and\nplatform realities that offload the responsibility of ``privacy labor''. To\nbridge this gap, we present SafeShare, an interaction technique that leverages\nlocalized LLM to redact consultations in real-time. SafeShare balances utility\nand privacy through selectively anonymize private information. A technical\nevaluation of SafeShare's core PII detection module on 3 dataset demonstrates\nhigh efficacy, achieving 89.64\\% accuracy with Qwen3-4B on IMCS21 dataset.", "AI": {"tldr": "The paper investigates privacy concerns in online medical consultation platforms and introduces SafeShare, a technique utilizing localized LLM for real-time consultation redaction.", "motivation": "To address the significant privacy risks in online medical consultation platforms that undermine user trust.", "method": "Conducted in-depth semi-structured interviews with 12 users to understand their security and privacy perceptions, followed by technical evaluation of the SafeShare technique for real-time redaction.", "result": "SafeShare's PII detection module achieved 89.64% accuracy on the IMCS21 dataset, demonstrating high efficacy in balancing privacy and utility.", "conclusion": "SafeShare effectively addresses the disconnect between user needs for privacy and the realities of medical consultation platforms by employing localized LLM for real-time data redaction.", "key_contributions": ["Developed SafeShare for real-time redaction of consultations.", "Identified user expectations vs. platform realities regarding privacy.", "Demonstrated high accuracy of PII detection in evaluation."], "limitations": "Limited to the datasets evaluated; further study needed on broader applications.", "keywords": ["privacy", "online consultation", "anonymization", "local LLM", "health informatics"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.00390", "pdf": "https://arxiv.org/pdf/2508.00390.pdf", "abs": "https://arxiv.org/abs/2508.00390", "title": "SA-GCS: Semantic-Aware Gaussian Curriculum Scheduling for UAV Vision-Language Navigation", "authors": ["Hengxing Cai", "Jinhan Dong", "Yijie Rao", "Jingcheng Deng", "Jingjun Tan", "Qien Chen", "Haidong Wang", "Zhen Wang", "Shiyu Huang", "Agachai Sumalee", "Renxin Zhong"], "categories": ["cs.CL"], "comment": null, "summary": "Unmanned Aerial Vehicle (UAV) Vision-Language Navigation (VLN) aims to enable\nagents to accurately localize targets and plan flight paths in complex\nenvironments based on natural language instructions, with broad applications in\nintelligent inspection, disaster rescue, and urban monitoring. Recent progress\nin Vision-Language Models (VLMs) has provided strong semantic understanding for\nthis task, while reinforcement learning (RL) has emerged as a promising\npost-training strategy to further improve generalization. However, existing RL\nmethods often suffer from inefficient use of training data, slow convergence,\nand insufficient consideration of the difficulty variation among training\nsamples, which limits further performance improvement. To address these\nchallenges, we propose \\textbf{Semantic-Aware Gaussian Curriculum Scheduling\n(SA-GCS)}, a novel training framework that systematically integrates Curriculum\nLearning (CL) into RL. SA-GCS employs a Semantic-Aware Difficulty Estimator\n(SA-DE) to quantify the complexity of training samples and a Gaussian\nCurriculum Scheduler (GCS) to dynamically adjust the sampling distribution,\nenabling a smooth progression from easy to challenging tasks. This design\nsignificantly improves training efficiency, accelerates convergence, and\nenhances overall model performance. Extensive experiments on the CityNav\nbenchmark demonstrate that SA-GCS consistently outperforms strong baselines\nacross all metrics, achieves faster and more stable convergence, and\ngeneralizes well across models of different scales, highlighting its robustness\nand scalability. The implementation of our approach is publicly available.", "AI": {"tldr": "This paper presents a training framework called Semantic-Aware Gaussian Curriculum Scheduling (SA-GCS) that enhances Unmanned Aerial Vehicle (UAV) Vision-Language Navigation (VLN) by improving training efficiency through a systematic integration of Curriculum Learning in reinforcement learning.", "motivation": "To improve the training efficiency and performance of UAV Vision-Language Navigation (VLN) due to challenges like inefficient data usage and slow convergence in existing reinforcement learning methods.", "method": "The SA-GCS framework employs a Semantic-Aware Difficulty Estimator to measure sample complexity and a Gaussian Curriculum Scheduler to optimize task progression from easy to hard.", "result": "SA-GCS significantly outperforms strong baselines in training efficiency, convergence speed, and overall model performance as demonstrated in extensive experiments on the CityNav benchmark.", "conclusion": "The proposed SA-GCS framework achieves faster and more stable convergence while generalizing well across different model scales, showing robustness and scalability, and is publicly available for use.", "key_contributions": ["Introduction of Semantic-Aware Gaussian Curriculum Scheduling (SA-GCS) for RL in UAV VLN.", "Implementation of a Semantic-Aware Difficulty Estimator for training sample complexity assessment.", "Demonstration of enhanced training efficiency and performance through extensive benchmarking."], "limitations": "", "keywords": ["Unmanned Aerial Vehicle", "Vision-Language Navigation", "Reinforcement Learning", "Curriculum Learning", "Training Efficiency"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.00439", "pdf": "https://arxiv.org/pdf/2508.00439.pdf", "abs": "https://arxiv.org/abs/2508.00439", "title": "HateBuffer: Safeguarding Content Moderators' Mental Well-Being through Hate Speech Content Modification", "authors": ["Subin Park", "Jeonghyun Kim", "Jeanne Choi", "Joseph Seering", "Uichin Lee", "Sung-Ju Lee"], "categories": ["cs.HC"], "comment": "Accepted by ACM CSCW 2025; 39 pages (including 6 pages of Appendix)", "summary": "Hate speech remains a persistent and unresolved challenge in online\nplatforms. Content moderators, working on the front lines to review\nuser-generated content and shield viewers from hate speech, often find\nthemselves unprotected from the mental burden as they continuously engage with\noffensive language. To safeguard moderators' mental well-being, we designed\nHateBuffer, which anonymizes targets of hate speech, paraphrases offensive\nexpressions into less offensive forms, and shows the original expressions when\nmoderators opt to see them. Our user study with 80 participants consisted of a\nsimulated hate speech moderation task set on a fictional news platform,\nfollowed by semi-structured interviews. Although participants rated the hate\nseverity of comments lower while using HateBuffer, contrary to our\nexpectations, they did not experience improved emotion or reduced fatigue\ncompared with the control group. In interviews, however, participants described\nHateBuffer as an effective buffer against emotional contagion and the\nnormalization of biased opinions in hate speech. Notably, HateBuffer did not\ncompromise moderation accuracy and even contributed to a slight increase in\nrecall. We explore possible explanations for the discrepancy between the\nperceived benefits of HateBuffer and its measured impact on mental well-being.\nWe also underscore the promise of text-based content modification techniques as\ntools for a healthier content moderation environment.", "AI": {"tldr": "This paper presents HateBuffer, a tool designed to mitigate the mental burden on content moderators dealing with hate speech by anonymizing targets and paraphrasing offensive language.", "motivation": "To address the persistent challenge of hate speech in online platforms and the associated mental burden on content moderators.", "method": "The study involved 80 participants who tested HateBuffer in a simulated moderation task, followed by semi-structured interviews to gather qualitative feedback.", "result": "While HateBuffer decreased the perceived severity of hate comments, it did not improve moderators' emotions or reduce fatigue compared to the control group. Participants found it helpful against emotional contagion.", "conclusion": "HateBuffer shows promise as a tool to enhance the well-being of content moderators without compromising moderation accuracy, although its effectiveness on mental well-being remains mixed.", "key_contributions": ["Introduction of HateBuffer as a moderation tool", "Empirical evaluation of its impact on user perception and emotional response", "Insights into the effectiveness of text modification in moderation practices"], "limitations": "The tool did not significantly improve moderators' emotional state or reduce fatigue despite qualitative feedback suggesting it had benefits.", "keywords": ["Hate speech", "Content moderation", "Mental well-being", "User study", "Text modification"], "importance_score": 7, "read_time_minutes": 20}}
{"id": "2508.00420", "pdf": "https://arxiv.org/pdf/2508.00420.pdf", "abs": "https://arxiv.org/abs/2508.00420", "title": "Combining Discrete Wavelet and Cosine Transforms for Efficient Sentence Embedding", "authors": ["Rana Salama", "Abdou Youssef", "Mona Diab"], "categories": ["cs.CL"], "comment": null, "summary": "Wavelets have emerged as a cutting edge technology in a number of fields.\nConcrete results of their application in Image and Signal processing suggest\nthat wavelets can be effectively applied to Natural Language Processing (NLP)\ntasks that capture a variety of linguistic properties. In this paper, we\nleverage the power of applying Discrete Wavelet Transforms (DWT) to word and\nsentence embeddings. We first evaluate, intrinsically and extrinsically, how\nwavelets can effectively be used to consolidate important information in a word\nvector while reducing its dimensionality. We further combine DWT with Discrete\nCosine Transform (DCT) to propose a non-parameterized model that compresses a\nsentence with a dense amount of information in a fixed size vector based on\nlocally varying word features. We show the efficacy of the proposed paradigm on\ndownstream applications models yielding comparable and even superior (in some\ntasks) results to original embeddings.", "AI": {"tldr": "This paper explores the application of Discrete Wavelet Transforms (DWT) to enhance word and sentence embeddings for Natural Language Processing, demonstrating effective dimensionality reduction and information consolidation.", "motivation": "To leverage wavelets in NLP tasks to capture linguistic properties and improve the representation of word and sentence embeddings.", "method": "The authors apply DWT to word vectors and combine it with Discrete Cosine Transform (DCT) to create a non-parameterized model that compresses sentences while retaining key information.", "result": "The proposed model shows effectiveness on downstream applications, yielding results that are comparable or superior to original embeddings in some cases.", "conclusion": "Wavelets, through the implementation of DWT and DCT, can significantly enhance the representation of NLP embeddings, leading to better performance in various tasks.", "key_contributions": ["Application of DWT in NLP embeddings", "Development of a non-parameterized model for sentence compression", "Demonstration of superior performance compared to original embeddings"], "limitations": "", "keywords": ["Discrete Wavelet Transforms", "Natural Language Processing", "Word Embeddings"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.00646", "pdf": "https://arxiv.org/pdf/2508.00646.pdf", "abs": "https://arxiv.org/abs/2508.00646", "title": "Pull Requests From The Classroom: Co-Developing Curriculum And Code", "authors": ["Dennis Zyska", "Ilia Kuznetsov", "Florian Müller", "Iryna Gurevych"], "categories": ["cs.HC"], "comment": null, "summary": "Educational technologies often misalign with instructors' pedagogical goals,\nforcing adaptations that compromise teaching efficacy. In this paper, we\npresent a case study on the co-development of curriculum and technology in the\ncontext of a university course on scientific writing. Specifically, we examine\nhow a custom-built peer feedback system was iteratively developed alongside the\ncourse to support annotation, feedback exchange, and revision. Results show\nthat while co-development fostered stronger alignment between software features\nand course goals, it also exposed usability limitations and\ninfrastructure-related frustrations, emphasizing the need for closer\ncoordination between teaching and technical teams.", "AI": {"tldr": "The paper presents a case study on developing a peer feedback system alongside a university course on scientific writing, highlighting the importance of co-development in aligning technology with pedagogical goals and its resulting challenges.", "motivation": "To address the misalignment between educational technologies and instructors' pedagogical goals, which undermines teaching efficacy.", "method": "A case study approach was utilized, examining the iterative co-development of a custom-built peer feedback system alongside a scientific writing course.", "result": "The study revealed that co-development improved alignment between the system's features and the course goals but also highlighted usability issues and frustrations related to infrastructure.", "conclusion": "Stronger collaboration between teaching and technical teams is necessary to enhance educational technology's effectiveness and usability.", "key_contributions": ["Presentation of a co-development model for educational technologies.", "Insights into the alignment of software features with pedagogical objectives.", "Identification of usability limitations in educational technology."], "limitations": "Focused on a single university course, which may limit generalizability.", "keywords": ["peer feedback system", "educational technology", "co-development"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.00429", "pdf": "https://arxiv.org/pdf/2508.00429.pdf", "abs": "https://arxiv.org/abs/2508.00429", "title": "ReaGAN: Node-as-Agent-Reasoning Graph Agentic Network", "authors": ["Minghao Guo", "Xi Zhu", "Jingyuan Huang", "Kai Mei", "Yongfeng Zhang"], "categories": ["cs.CL", "cs.LG", "cs.MA"], "comment": "17 pages, work in progress", "summary": "Graph Neural Networks (GNNs) have achieved remarkable success in graph-based\nlearning by propagating information among neighbor nodes via predefined\naggregation mechanisms. However, such fixed schemes often suffer from two key\nlimitations. First, they cannot handle the imbalance in node informativeness --\nsome nodes are rich in information, while others remain sparse. Second,\npredefined message passing primarily leverages local structural similarity\nwhile ignoring global semantic relationships across the graph, limiting the\nmodel's ability to capture distant but relevant information. We propose\nRetrieval-augmented Graph Agentic Network (ReaGAN), an agent-based framework\nthat empowers each node with autonomous, node-level decision-making. Each node\nacts as an agent that independently plans its next action based on its internal\nmemory, enabling node-level planning and adaptive message propagation.\nAdditionally, retrieval-augmented generation (RAG) allows nodes to access\nsemantically relevant content and build global relationships in the graph.\nReaGAN achieves competitive performance under few-shot in-context settings\nusing a frozen LLM backbone without fine-tuning, showcasing the potential of\nagentic planning and local-global retrieval in graph learning.", "AI": {"tldr": "ReaGAN is an innovative framework that enhances graph learning using autonomous, node-level decision-making and retrieval-augmented generation, addressing limitations of traditional GNNs.", "motivation": "To tackle the limitations of fixed aggregation mechanisms in GNNs, which struggle with node informativeness imbalance and capture of global semantic relationships.", "method": "ReaGAN employs an agent-based approach where each node functions as an independent agent planning its actions based on internal memory, complemented by retrieval-augmented generation for accessing relevant content.", "result": "ReaGAN demonstrates competitive performance in few-shot settings using a frozen LLM backbone without requiring fine-tuning, indicating the efficacy of agentic planning and retrieval in enhancing graph learning.", "conclusion": "The proposed framework showcases significant potential in addressing limitations of existing GNN methods and enhances the ability to model complex relationships within graphs.", "key_contributions": ["Introduction of Retrieval-augmented Graph Agentic Network (ReaGAN) framework.", "Empowerment of nodes with autonomous decision-making capabilities.", "Incorporation of retrieval-augmented generation for enhancing global semantic understanding."], "limitations": "The paper is a work in progress, indicating that further validation and refinement of the approach may be needed.", "keywords": ["Graph Neural Networks", "Agent-based planning", "Retrieval-augmented generation", "Node-level decision-making", "Few-shot learning"], "importance_score": 8, "read_time_minutes": 17}}
{"id": "2508.00652", "pdf": "https://arxiv.org/pdf/2508.00652.pdf", "abs": "https://arxiv.org/abs/2508.00652", "title": "The Manipulative Power of Voice Characteristics: Investigating Deceptive Patterns in Mandarin Chinese Female Synthetic Speech", "authors": ["Shuning Zhang", "Han Chen", "Yabo Wang", "Yiqun Xu", "Jiaqi Bai", "Yuanyuan Wu", "Shixuan Li", "Xin Yi", "Chunhui Wang", "Hewu Li"], "categories": ["cs.HC"], "comment": null, "summary": "Pervasive voice interaction enables deceptive patterns through subtle voice\ncharacteristics, yet empirical investigation into this manipulation lags\nbehind, especially within major non-English language contexts. Addressing this\ngap, our study presents the first systematic investigation into voice\ncharacteristic-based dark patterns employing female synthetic voices in\nMandarin Chinese. This focus is crucial given the prevalence of female personas\nin commercial assistants and the prosodic significance in the Chinese language.\nGuided by the conceptual framework identifying key influencing factors, we\nsystematically evaluate effectiveness variations by manipulating voice\ncharacteristics (five characteristics, three intensities) across different\nscenarios (shopping vs. question-answering) with different commercial aims. A\npreliminary study (N=24) validated the experimental materials and the main\nstudy (N=36) revealed significant behavioral manipulation (up to +2027.6%).\nCrucially, the analysis showed that effectiveness varied significantly with\nvoice characteristics and scenario, mediated by user perception (of tone,\nintonation, timbre) and user demographics (individual preferences, though\nlimited demographic impact). These interconnected findings offer evidence-based\ninsights for ethical design.", "AI": {"tldr": "This study investigates the impact of female synthetic voices on user behavior in Mandarin Chinese, revealing substantial behavioral manipulation through voice characteristics.", "motivation": "To address the empirical investigation gap regarding voice characteristic-based manipulation in non-English contexts, particularly Mandarin Chinese.", "method": "The study systematically evaluates voice characteristic manipulations (five characteristics, three intensities) across scenarios (shopping vs. question-answering) with a preliminary study (N=24) and a main study (N=36).", "result": "The main study revealed significant behavioral manipulation, with effectiveness variations up to +2027.6%, influenced by voice characteristics, scenario, and user perception.", "conclusion": "The findings highlight the ethical considerations in design due to the significant impact of voice characteristics on user behavior.", "key_contributions": ["First systematic study of voice characteristic-based manipulation in Mandarin Chinese", "Demonstrated significant behavioral manipulation linked to voice characteristics", "Provided insights for ethical design considerations."], "limitations": "Limited demographic impact on user perception; small sample sizes.", "keywords": ["voice interaction", "dark patterns", "synthetic voices", "Mandarin Chinese", "user perception"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.00454", "pdf": "https://arxiv.org/pdf/2508.00454.pdf", "abs": "https://arxiv.org/abs/2508.00454", "title": "Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges", "authors": ["Yuqi Tang", "Kehua Feng", "Yunfeng Wang", "Zhiwen Chen", "Chengfei Lv", "Gang Yu", "Qiang Zhang", "Keyan Ding"], "categories": ["cs.CL"], "comment": "15 pages, 2 pages, under review at AAAI 2026", "summary": "Evaluating the conversational abilities of large language models (LLMs)\nremains a challenging task. Current mainstream approaches primarily rely on the\n``LLM-as-a-judge\" paradigm, where an LLM is prompted to serve as an evaluator\nto assess dialogue quality. However, such methods often suffer from various\nbiases, which undermine the reliability and consistency of the evaluation\nresults. To mitigate these biases, recent methods employ multiple LLMs as\njudges and aggregate their judgments to select the optimal assessment. Although\neffective, this multi-judge approach incurs significant computational overhead\nduring inference. In this paper, we propose an efficient multi-turn dialogue\nevaluator that captures the collective wisdom of multiple LLM judges by\naggregating their preference knowledge into a single model. Our approach\npreserves the advantages of diverse multi-judge feedback while drastically\nreducing the evaluation cost, enabling fast and flexible dialogue quality\nassessment. Extensive experiments on seven single rating and pairwise\ncomparison dialogue evaluation benchmarks demonstrate that our method\noutperforms existing baselines across diverse scenarios, showcasing its\nefficiency and robustness.", "AI": {"tldr": "This paper introduces an efficient multi-turn dialogue evaluator that aggregates the feedback of multiple LLM judges into a single model, addressing biases and reducing computational costs in dialogue quality assessment.", "motivation": "To improve the evaluation of dialogue quality while mitigating biases inherent in traditional LLM-as-a-judge approaches and reducing computational overhead.", "method": "The paper proposes an efficient multi-turn dialogue evaluator that combines preference knowledge from multiple LLM judges into a single model for dialogue assessment.", "result": "The proposed method outperforms existing baselines in evaluation benchmarks while significantly reducing inference costs.", "conclusion": "The paper demonstrates that the new multi-turn dialogue evaluator provides robust and efficient dialogue quality assessments, making it a valuable tool in evaluating conversational abilities of LLMs.", "key_contributions": ["Introduction of a multi-turn dialogue evaluator that aggregates multiple LLM judges' feedback", "Reduction of computational overhead compared to traditional multi-judge approaches", "Empirical validation showing superior performance over existing methods"], "limitations": "", "keywords": ["Large Language Models", "Dialogue Evaluation", "HCI", "Machine Learning", "Efficiency"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.00723", "pdf": "https://arxiv.org/pdf/2508.00723.pdf", "abs": "https://arxiv.org/abs/2508.00723", "title": "Why Do Decision Makers (Not) Use AI? A Cross-Domain Analysis of Factors Impacting AI Adoption", "authors": ["Rebecca Yu", "Valerie Chen", "Ameet Talwalkar", "Hoda Heidari"], "categories": ["cs.HC"], "comment": "To be published in Proceedings of the Eighth AAAI/ACM Conference on\n  AI, Ethics, and Society (AIES-25). 10 pages, 4 figures, 1 table", "summary": "Growing excitement around deploying AI across various domains calls for a\ncareful assessment of how human decision-makers interact with AI-powered\nsystems. In particular, it is essential to understand when decision-makers\nvoluntarily choose to consult AI tools, which we term decision-maker adoption.\nWe interviewed experts across four domains -- medicine, law, journalism, and\nthe public sector -- to explore current AI use cases and perceptions of\nadoption. From these interviews, we identify key factors that shape\ndecision-maker adoption of AI tools: the decision-maker's background,\nperceptions of the AI, consequences for the decision-maker, and perceived\nimplications for other stakeholders. We translate these factors into an AI\nadoption sheet to analyze how decision-makers approach adoption choices through\ncomparative, cross-domain case studies, highlighting how our factors help\nexplain inter-domain differences in adoption. Our findings offer practical\nguidance for supporting the responsible and context-aware deployment of AI by\nbetter accounting for the decision-maker's perspective.", "AI": {"tldr": "The paper examines how decision-makers in various fields adopt AI tools, identifying factors influencing this adoption and offering a framework for understanding inter-domain differences.", "motivation": "To assess how human decision-makers interact with AI systems and understand the factors that influence their adoption of AI tools.", "method": "Conducted interviews with experts across four domains (medicine, law, journalism, and public sector) to explore AI use cases and perceptions of adoption.", "result": "Identified key factors influencing decision-maker adoption of AI: decision-maker's background, perceptions of AI, consequences for decision-makers, and implications for stakeholders.", "conclusion": "The study provides practical guidance for the responsible deployment of AI by considering decision-makers' perspectives.", "key_contributions": ["Identification of factors influencing AI adoption decisions", "Development of an AI adoption sheet to analyze adoption choices", "Cross-domain case studies highlighting differences in AI adoption"], "limitations": "", "keywords": ["AI adoption", "decision-making", "human-computer interaction", "cross-domain analysis", "ethical AI"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.00476", "pdf": "https://arxiv.org/pdf/2508.00476.pdf", "abs": "https://arxiv.org/abs/2508.00476", "title": "GETALP@AutoMin 2025: Leveraging RAG to Answer Questions based on Meeting Transcripts", "authors": ["Jeongwoo Kang", "Markarit Vartampetian", "Felix Herron", "Yongxin Zhou", "Diandra Fabre", "Gabriela Gonzalez-Saez"], "categories": ["cs.CL"], "comment": null, "summary": "This paper documents GETALP's submission to the Third Run of the Automatic\nMinuting Shared Task at SIGDial 2025. We participated in Task B:\nquestion-answering based on meeting transcripts. Our method is based on a\nretrieval augmented generation (RAG) system and Abstract Meaning\nRepresentations (AMR). We propose three systems combining these two approaches.\nOur results show that incorporating AMR leads to high-quality responses for\napproximately 35% of the questions and provides notable improvements in\nanswering questions that involve distinguishing between different participants\n(e.g., who questions).", "AI": {"tldr": "The paper presents GETALP's approach to question-answering from meeting transcripts using a RAG system and AMR.", "motivation": "The need for improved question-answering techniques in the context of meeting transcripts.", "method": "The authors developed three systems that combine retrieval augmented generation (RAG) and Abstract Meaning Representations (AMR) to enhance question-answering.", "result": "The proposed methods achieved high-quality responses for about 35% of questions, particularly excelling in questions requiring differentiation among participants.", "conclusion": "Incorporating AMR significantly improves the quality of responses in question-answering tasks from meeting transcripts.", "key_contributions": ["Introduction of a RAG system combined with AMR for question-answering", "Demonstration of improved performance in distinguishing questions among participants", "Empirical evaluation showing 35% high-quality response rate"], "limitations": "", "keywords": ["question-answering", "meeting transcripts", "retrieval augmented generation", "Abstract Meaning Representation", "SIGDial"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.00737", "pdf": "https://arxiv.org/pdf/2508.00737.pdf", "abs": "https://arxiv.org/abs/2508.00737", "title": "How LLMs are Shaping the Future of Virtual Reality", "authors": ["Süeda Özkaya", "Santiago Berrezueta-Guzman", "Stefan Wagner"], "categories": ["cs.HC", "cs.AI"], "comment": "Pre-print", "summary": "The integration of Large Language Models (LLMs) into Virtual Reality (VR)\ngames marks a paradigm shift in the design of immersive, adaptive, and\nintelligent digital experiences. This paper presents a comprehensive review of\nrecent research at the intersection of LLMs and VR, examining how these models\nare transforming narrative generation, non-player character (NPC) interactions,\naccessibility, personalization, and game mastering. Drawing from an analysis of\n62 peer reviewed studies published between 2018 and 2025, we identify key\napplication domains ranging from emotionally intelligent NPCs and procedurally\ngenerated storytelling to AI-driven adaptive systems and inclusive gameplay\ninterfaces. We also address the major challenges facing this convergence,\nincluding real-time performance constraints, memory limitations, ethical risks,\nand scalability barriers. Our findings highlight that while LLMs significantly\nenhance realism, creativity, and user engagement in VR environments, their\neffective deployment requires robust design strategies that integrate\nmultimodal interaction, hybrid AI architectures, and ethical safeguards. The\npaper concludes by outlining future research directions in multimodal AI,\naffective computing, reinforcement learning, and open-source development,\naiming to guide the responsible advancement of intelligent and inclusive VR\nsystems.", "AI": {"tldr": "This paper reviews the integration of Large Language Models (LLMs) into Virtual Reality (VR) games, highlighting their impact on narrative generation and NPC interactions while addressing challenges and future research directions.", "motivation": "To explore how LLMs can transform the design and experience of VR games through enhanced interactivity and personalization.", "method": "Comprehensive review of 62 peer-reviewed studies focusing on the intersection of LLMs and VR from 2018 to 2025.", "result": "Identification of key application domains including emotionally intelligent NPCs and adaptive AI systems, alongside discussions of challenges like real-time performance and ethical concerns.", "conclusion": "Successful deployment of LLMs in VR requires robust design strategies, which support multimodal interactions and address ethical considerations, with future research aimed at fostering intelligent VR systems.", "key_contributions": ["Comprehensive review of LLMs in VR", "Identification of key application areas", "Discussion of challenges and future directions"], "limitations": "Memory limitations and real-time performance constraints.", "keywords": ["Large Language Models", "Virtual Reality", "AI in Gaming", "NPC Interactions", "Multimodal AI"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.00489", "pdf": "https://arxiv.org/pdf/2508.00489.pdf", "abs": "https://arxiv.org/abs/2508.00489", "title": "The Missing Parts: Augmenting Fact Verification with Half-Truth Detection", "authors": ["Yixuan Tang", "Jincheng Wang", "Anthony K. H. Tung"], "categories": ["cs.CL"], "comment": null, "summary": "Fact verification systems typically assess whether a claim is supported by\nretrieved evidence, assuming that truthfulness depends solely on what is\nstated. However, many real-world claims are half-truths, factually correct yet\nmisleading due to the omission of critical context. Existing models struggle\nwith such cases, as they are not designed to reason about what is left unsaid.\nWe introduce the task of half-truth detection, and propose PolitiFact-Hidden, a\nnew benchmark with 15k political claims annotated with sentence-level evidence\nalignment and inferred claim intent. To address this challenge, we present\nTRACER, a modular re-assessment framework that identifies omission-based\nmisinformation by aligning evidence, inferring implied intent, and estimating\nthe causal impact of hidden content. TRACER can be integrated into existing\nfact-checking pipelines and consistently improves performance across multiple\nstrong baselines. Notably, it boosts Half-True classification F1 by up to 16\npoints, highlighting the importance of modeling omissions for trustworthy fact\nverification.", "AI": {"tldr": "This paper introduces the task of half-truth detection and presents TRACER, a framework for identifying omission-based misinformation in political claims.", "motivation": "Many claims are factually correct but misleading due to omitted context; existing fact verification models do not account for this.", "method": "TRACER is a modular re-assessment framework that aligns evidence, infers claim intent, and estimates the impact of hidden content.", "result": "TRACER improves Half-True classification F1 by up to 16 points across multiple strong baselines, demonstrating the need to model omissions.", "conclusion": "Incorporating context and omitted content is crucial for enhancing the reliability of fact verification systems.", "key_contributions": ["Introduction of half-truth detection as a distinct task", "Creation of the PolitiFact-Hidden benchmark with annotated political claims", "Development of TRACER framework for detecting omission-based misinformation"], "limitations": "", "keywords": ["half-truth detection", "fact verification", "misinformation", "contextual reasoning", "TRACER"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.00522", "pdf": "https://arxiv.org/pdf/2508.00522.pdf", "abs": "https://arxiv.org/abs/2508.00522", "title": "EFlat-LoRA: Efficiently Seeking Flat Minima for Better Generalization in Fine-Tuning Large Language Models and Beyond", "authors": ["Jiaxin Deng", "Qingcheng Zhu", "Junbiao Pang", "Linlin Yang", "Zhongqian Fu", "Baochang Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Little research explores the correlation between the expressive ability and\ngeneralization ability of the low-rank adaptation (LoRA). Sharpness-Aware\nMinimization (SAM) improves model generalization for both Convolutional Neural\nNetworks (CNNs) and Transformers by encouraging convergence to locally flat\nminima. However, the connection between sharpness and generalization has not\nbeen fully explored for LoRA due to the lack of tools to either empirically\nseek flat minima or develop theoretical methods. In this work, we propose\nFlat-LoRA and its efficient version i.e., EFlat-LoRA, to seek flat minima for\nLoRA. Concretely, we theoretically demonstrate that perturbations in the full\nparameter space can be transferred to the low-rank subspace. This approach\neliminates the potential interference introduced by perturbations across\nmultiple matrices in the low-rank subspace. Our extensive experiments on large\nlanguage models and vision-language models demonstrate that EFlat-LoRA achieves\noptimize efficiency comparable to that of LoRA while simultaneously attaining\ncomparable or even better performance. For example, on the GLUE dataset with\nRoBERTa-large, EFlat-LoRA outperforms LoRA and full fine-tuning by 1.0% and\n0.5% on average, respectively. On vision-language models e.g., Qwen-VL-Chat\nshows performance improvements of 1.5% and 1.0% on SQA and VizWiz datasets,\nrespectively. These empirical results also verify that the generalization of\nLoRA is closely related to sharpness, which is omitted by previous methods.", "AI": {"tldr": "This paper presents Flat-LoRA and its efficient variant EFlat-LoRA, aimed at optimizing low-rank adaptation while enhancing model generalization through exploring sharpness-aware techniques.", "motivation": "To investigate the correlation between expressive ability and generalization ability of low-rank adaptations (LoRA) and to improve general model performance.", "method": "The authors propose Flat-LoRA and EFlat-LoRA to achieve flat minima through theoretically demonstrating perturbation transfer in the low-rank subspace, along with extensive empirical experiments.", "result": "EFalt-LoRA outperformed LoRA and full fine-tuning on various benchmarks, including 1.0% on GLUE dataset with RoBERTa-large and improvements on vision-language models such as Qwen-VL-Chat.", "conclusion": "The findings suggest that improving generalization for LoRA via sharpness-aware techniques is effective and reveals a significant connection that has been overlooked by previous research.", "key_contributions": ["Introduction of Flat-LoRA and EFlat-LoRA for seeking flat minima in low-rank adaptation.", "Theoretical demonstration of perturbation transfer in low-rank subspace enhances understanding of generalization.", "Empirical validation of significant performance improvements across various language and vision-language models."], "limitations": "", "keywords": ["Low-Rank Adaptation", "Sharpness-Aware Minimization", "Model Generalization", "Deep Learning", "Transformers"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.00537", "pdf": "https://arxiv.org/pdf/2508.00537.pdf", "abs": "https://arxiv.org/abs/2508.00537", "title": "The Prosody of Emojis", "authors": ["Giulio Zhou", "Tsz Kin Lam", "Alexandra Birch", "Barry Haddow"], "categories": ["cs.CL"], "comment": null, "summary": "Prosodic features such as pitch, timing, and intonation are central to spoken\ncommunication, conveying emotion, intent, and discourse structure. In\ntext-based settings, where these cues are absent, emojis act as visual\nsurrogates that add affective and pragmatic nuance. This study examines how\nemojis influence prosodic realisation in speech and how listeners interpret\nprosodic cues to recover emoji meanings. Unlike previous work, we directly link\nprosody and emoji by analysing actual human speech data, collected through\nstructured but open-ended production and perception tasks. This provides\nempirical evidence of how emoji semantics shape spoken delivery and perception.\nResults show that speakers adapt their prosody based on emoji cues, listeners\ncan often identify the intended emoji from prosodic variation alone, and\ngreater semantic differences between emojis correspond to increased prosodic\ndivergence. These findings suggest that emojis can act as meaningful carriers\nof prosodic intent, offering insight into their communicative role in digitally\nmediated contexts.", "AI": {"tldr": "This study explores the relationship between emojis and prosodic features in speech, showing how emojis influence spoken delivery and listener interpretation of prosodic cues.", "motivation": "To investigate how emojis affect prosodic realisation in speech and how they help listeners interpret meaning in the absence of verbal cues.", "method": "The study involved analyzing human speech data collected from structured production and perception tasks, linking prosody with emoji use directly.", "result": "Speakers adapt their prosody based on emoji cues; listeners can identify intended emojis from prosodic variations alone, and greater semantic differences in emojis lead to more significant prosodic divergence.", "conclusion": "Emojis serve as meaningful carriers of prosodic intent, providing insights into their role in communication in digital contexts.", "key_contributions": ["Empirical evidence linking emoji semantics with prosodic delivery", "Demonstration of listener ability to discern emoji meanings from prosody", "Insights into the communicative role of emojis in digitally mediated interactions"], "limitations": "", "keywords": ["prosody", "emojis", "speech", "communication", "interpretation"], "importance_score": 7, "read_time_minutes": 20}}
{"id": "2508.00544", "pdf": "https://arxiv.org/pdf/2508.00544.pdf", "abs": "https://arxiv.org/abs/2508.00544", "title": "PaPaformer: Language Model from Pre-trained Paraller Paths", "authors": ["Joonas Tapaninaho", "Mourad Oussala"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The training of modern large-language models requires an increasingly amount\nof computation power and time. Even smaller variants, such as small-language\nmodels (SLMs), take several days to train in the best-case scenarios, often\nrequiring multiple GPUs. This paper explores methods to train and evaluate\ndecoder-only transformer-based language models in hours instead of days/weeks.\nWe introduces \\textit{PaPaformer}, a decoder-only transformer architecture\nvariant, whose lower-dimensional parallel paths are combined into larger model.\nThe paper shows that these lower-dimensional paths can be trained individually\nwith different types of training data and then combined into one larger model.\nThis method gives the option to reduce the total number of model parameters and\nthe training time with increasing performance. Moreover, the use of parallel\npath structure opens interesting possibilities to customize paths to\naccommodate specific task requirements.", "AI": {"tldr": "This paper introduces PaPaformer, a decoder-only transformer-based model that trains in hours by using lower-dimensional parallel paths.", "motivation": "To reduce the computation power and time required for training large language models, specifically decoder-only transformers.", "method": "PaPaformer employs lower-dimensional parallel paths that can be trained individually and then combined into a larger model.", "result": "The approach enables significant reductions in total model parameters and training time while improving performance.", "conclusion": "The parallel path structure allows for customization to meet specific task requirements, enhancing versatility and efficiency.", "key_contributions": ["Introduction of the PaPaformer architecture", "Demonstration of training lower-dimensional paths individually", "Method to combine paths into a larger model for enhanced performance"], "limitations": "", "keywords": ["large-language models", "transformer architecture", "training efficiency"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2508.00574", "pdf": "https://arxiv.org/pdf/2508.00574.pdf", "abs": "https://arxiv.org/abs/2508.00574", "title": "SynAdapt: Learning Adaptive Reasoning in Large Language Models via Synthetic Continuous Chain-of-Thought", "authors": ["Jianwei Wang", "Ziming Wu", "Fuming Lai", "Shaobing Lian", "Ziqian Zeng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While Chain-of-Thought (CoT) reasoning improves model performance, it incurs\nsignificant time costs due to the generation of discrete CoT tokens (DCoT).\nContinuous CoT (CCoT) offers a more efficient alternative, but existing CCoT\nmethods are hampered by indirect fine-tuning, limited alignment, or\ninconsistent targets. To overcome these limitations, we propose\n\\textit{SynAdapt}, an innovative efficient reasoning framework. Specifically,\n\\textit{SynAdapt} generates the synthetic CCoT to serve as a precise and\neffective alignment target for LLMs. This synthetic CCoT explicitly guides the\nLLM to learn CCoT and derive accurate answers directly. Furthermore, relying\nsolely on CCoT is insufficient for solving hard questions. To address this,\n\\textit{SynAdapt} integrates a difficulty classifier that leverages both\nquestion context and CCoT to identify hard questions. CCoT can effectively help\nidentify hard questions after some brief reasoning. We then adaptively prompt\nthe LLM to re-think these hard questions for improved performance. Extensive\nexperimental results across various benchmarks from different difficulty levels\nstrongly demonstrate the effectiveness of our method, achieving the best\naccuracy-efficiency trade-off.", "AI": {"tldr": "Introducing SynAdapt, an efficient reasoning framework that utilizes synthetic continuous Chain-of-Thought (CCoT) to improve LLM alignment and performance on complex questions.", "motivation": "To enhance the efficiency of Chain-of-Thought reasoning in large language models (LLMs) while addressing the limitations of existing continuous CCoT methods.", "method": "The proposed SynAdapt framework generates synthetic CCoT as a precise alignment target and integrates a difficulty classifier to identify challenging questions, prompting the LLM for improved responses.", "result": "Experimental results across various benchmarks indicate that SynAdapt achieves the best accuracy-efficiency trade-off compared to previous methods.", "conclusion": "SynAdapt effectively improves LLM performance on complex tasks by utilizing synthetic CCoT and an adaptive difficulty assessment approach.", "key_contributions": ["Development of SynAdapt framework for efficient reasoning", "Integration of a difficulty classifier to identify hard questions", "Achievement of improved accuracy-efficiency trade-off in LLM performance"], "limitations": "", "keywords": ["Chain-of-Thought", "synthetic CCoT", "difficulty classifier", "LLM", "efficiency"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2409.18203", "pdf": "https://arxiv.org/pdf/2409.18203.pdf", "abs": "https://arxiv.org/abs/2409.18203", "title": "Policy Maps: Tools for Guiding the Unbounded Space of LLM Behaviors", "authors": ["Michelle S. Lam", "Fred Hohman", "Dominik Moritz", "Jeffrey P. Bigham", "Kenneth Holstein", "Mary Beth Kery"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.LG"], "comment": "UIST 2025", "summary": "AI policy sets boundaries on acceptable behavior for AI models, but this is\nchallenging in the context of large language models (LLMs): how do you ensure\ncoverage over a vast behavior space? We introduce policy maps, an approach to\nAI policy design inspired by the practice of physical mapmaking. Instead of\naiming for full coverage, policy maps aid effective navigation through\nintentional design choices about which aspects to capture and which to abstract\naway. With Policy Projector, an interactive tool for designing LLM policy maps,\nan AI practitioner can survey the landscape of model input-output pairs, define\ncustom regions (e.g., \"violence\"), and navigate these regions with if-then\npolicy rules that can act on LLM outputs (e.g., if output contains \"violence\"\nand \"graphic details,\" then rewrite without \"graphic details\"). Policy\nProjector supports interactive policy authoring using LLM classification and\nsteering and a map visualization reflecting the AI practitioner's work. In an\nevaluation with 12 AI safety experts, our system helps policy designers craft\npolicies around problematic model behaviors such as incorrect gender\nassumptions and handling of immediate physical safety threats.", "AI": {"tldr": "Introduction of policy maps for AI policy design, enabling navigation through LLM behavior space with interactive tools.", "motivation": "The need for effective AI policy design for LLMs, which face challenges in ensuring comprehensive behavior coverage.", "method": "Development of Policy Projector, an interactive tool that allows AI practitioners to create policy maps by defining regions and applying if-then rules for LLM outputs.", "result": "Evaluation with AI safety experts indicates that Policy Projector aids policy designers in addressing model behaviors like incorrect gender assumptions and safety threat management.", "conclusion": "Policy maps provide an innovative approach to AI policy design, enhancing the capability to manage LLM outputs effectively.", "key_contributions": ["Introduction of policy maps for AI policies", "Development of the Policy Projector tool", "Interactive authoring of LLM policies using map visualization"], "limitations": "Still requires refinement for broader applicability across different AI domains.", "keywords": ["AI policy", "large language models", "policy maps", "interactive tools", "AI safety"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.00600", "pdf": "https://arxiv.org/pdf/2508.00600.pdf", "abs": "https://arxiv.org/abs/2508.00600", "title": "A Context-Aware Dual-Metric Framework for Confidence Estimation in Large Language Models", "authors": ["Mingruo Yuan", "Shuyi Zhang", "Ben Kao"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Accurate confidence estimation is essential for trustworthy large language\nmodels (LLMs) systems, as it empowers the user to determine when to trust\noutputs and enables reliable deployment in safety-critical applications.\nCurrent confidence estimation methods for LLMs neglect the relevance between\nresponses and contextual information, a crucial factor in output quality\nevaluation, particularly in scenarios where background knowledge is provided.\nTo bridge this gap, we propose CRUX (Context-aware entropy Reduction and\nUnified consistency eXamination), the first framework that integrates context\nfaithfulness and consistency for confidence estimation via two novel metrics.\nFirst, contextual entropy reduction represents data uncertainty with the\ninformation gain through contrastive sampling with and without context. Second,\nunified consistency examination captures potential model uncertainty through\nthe global consistency of the generated answers with and without context.\nExperiments across three benchmark datasets (CoQA, SQuAD, QuAC) and two\ndomain-specific datasets (BioASQ, EduQG) demonstrate CRUX's effectiveness,\nachieving the highest AUROC than existing baselines.", "AI": {"tldr": "This paper introduces CRUX, a novel framework for improving confidence estimation in large language models by incorporating contextual information.", "motivation": "To address the lack of relevance between LLM responses and contextual information in current confidence estimation methods, which is crucial for assessing output quality.", "method": "CRUX combines two metrics: contextual entropy reduction, which measures data uncertainty using contrastive sampling, and unified consistency examination, which evaluates model uncertainty through the consistency of answers with and without context.", "result": "Experimental results show that CRUX outperforms existing methods, achieving the highest AUROC on various benchmark and domain-specific datasets.", "conclusion": "CRUX provides a significant advancement in confidence estimation for LLMs, enhancing their deployment in safety-critical applications.", "key_contributions": ["Introduction of a context-aware framework for confidence estimation in LLMs", "Development of two novel metrics: contextual entropy reduction and unified consistency examination", "Demonstration of CRUX's superior performance on benchmark datasets compared to existing baselines."], "limitations": "", "keywords": ["confidence estimation", "large language models", "contextual information", "machine learning", "health informatics"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.00605", "pdf": "https://arxiv.org/pdf/2508.00605.pdf", "abs": "https://arxiv.org/abs/2508.00605", "title": "GHTM: A Graph based Hybrid Topic Modeling Approach in Low-Resource Bengali Language", "authors": ["Farhana Haque", "Md. Abdur Rahman", "Sumon Ahmed"], "categories": ["cs.CL"], "comment": null, "summary": "Topic modeling is a Natural Language Processing (NLP) technique that is used\nto identify latent themes and extract topics from text corpora by grouping\nsimilar documents based on their most significant keywords. Although widely\nresearched in English, topic modeling remains understudied in Bengali due to\nits morphological complexity, lack of adequate resources and initiatives. In\nthis contribution, a novel Graph Convolutional Network (GCN) based model called\nGHTM (Graph-Based Hybrid Topic Model) is proposed. This model represents input\nvectors of documents as nodes in the graph, which GCN uses to produce\nsemantically rich embeddings. The embeddings are then decomposed using\nNon-negative Matrix Factorization (NMF) to get the topical representations of\nthe underlying themes of the text corpus. This study compares the proposed\nmodel against a wide range of Bengali topic modeling techniques, from\ntraditional methods such as LDA, LSA, and NMF to contemporary frameworks such\nas BERTopic and Top2Vec on three Bengali datasets. The experimental results\ndemonstrate the effectiveness of the proposed model by outperforming other\nmodels in topic coherence and diversity. In addition, we introduce a novel\nBengali dataset called \"NCTBText\" sourced from Bengali textbook materials to\nenrich and diversify the predominantly newspaper-centric Bengali corpora.", "AI": {"tldr": "This paper proposes a novel Graph Convolutional Network-based topic modeling method for Bengali text, addressing the deficiencies in existing resources and techniques.", "motivation": "Topic modeling in Bengali presents unique challenges due to morphological complexity and resource scarcity, creating a need for advanced models.", "method": "The proposed GHTM model uses Graph Convolutional Networks to derive semantically rich document embeddings that are then decomposed with Non-negative Matrix Factorization for topic representation.", "result": "Experimental results show that GHTM outperforms traditional and contemporary Bengali topic modeling techniques in terms of topic coherence and diversity.", "conclusion": "The introduction of GHTM and the new NCTBText dataset significantly advances the field of topic modeling in Bengali, demonstrating improved performance over existing methods.", "key_contributions": ["Introduction of the GHTM model combining GCN and NMF for topic modeling in Bengali.", "Development of the NCTBText dataset sourced from Bengali textbooks.", "Comparative analysis showcasing superior performance in topic coherence and diversity."], "limitations": "The study focuses solely on Bengali, limiting its applicability to other languages and contexts.", "keywords": ["topic modeling", "Graph Convolutional Network", "Bengali", "NLP", "Non-negative Matrix Factorization"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2508.00614", "pdf": "https://arxiv.org/pdf/2508.00614.pdf", "abs": "https://arxiv.org/abs/2508.00614", "title": "Prompting Science Report 3: I'll pay you or I'll kill you -- but will you care?", "authors": ["Lennart Meincke", "Ethan Mollick", "Lilach Mollick", "Dan Shapiro"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This is the third in a series of short reports that seek to help business,\neducation, and policy leaders understand the technical details of working with\nAI through rigorous testing. In this report, we investigate two commonly held\nprompting beliefs: a) offering to tip the AI model and b) threatening the AI\nmodel. Tipping was a commonly shared tactic for improving AI performance and\nthreats have been endorsed by Google Founder Sergey Brin (All-In, May 2025,\n8:20) who observed that 'models tend to do better if you threaten them,' a\nclaim we subject to empirical testing here. We evaluate model performance on\nGPQA (Rein et al. 2024) and MMLU-Pro (Wang et al. 2024).\n  We demonstrate two things:\n  - Threatening or tipping a model generally has no significant effect on\nbenchmark performance.\n  - Prompt variations can significantly affect performance on a per-question\nlevel. However, it is hard to know in advance whether a particular prompting\napproach will help or harm the LLM's ability to answer any particular question.\n  Taken together, this suggests that simple prompting variations might not be\nas effective as previously assumed, especially for difficult problems. However,\nas reported previously (Meincke et al. 2025a), prompting approaches can yield\nsignificantly different results for individual questions.", "AI": {"tldr": "This report tests beliefs about improving AI performance through tipping and threatening AI models, finding no significant overall effect but notable variability in results per question.", "motivation": "To help leaders understand the technical aspects of working with AI through rigorous testing of common prompting beliefs.", "method": "The study tests the effects of prompting tactics (tipping and threatening) on model performance using benchmarks GPQA and MMLU-Pro.", "result": "The investigation shows that threatening or tipping a model does not generally impact overall benchmark performance, but prompt variations can significantly affect performance on specific questions.", "conclusion": "The findings suggest that the effectiveness of simple prompting variations is uncertain, especially for challenging questions, highlighting the variability in model responses.", "key_contributions": ["Empirical testing of prompting techniques using specific AI benchmarks.", "Demonstration that general tactics like threatening or tipping do not significantly affect performance.", "Insight into the variability of model responses to different prompts."], "limitations": "The effects of prompting can only be determined on a per-question basis, making it difficult to generalize the effectiveness.", "keywords": ["AI performance", "prompting techniques", "model evaluation"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.00619", "pdf": "https://arxiv.org/pdf/2508.00619.pdf", "abs": "https://arxiv.org/abs/2508.00619", "title": "DACTYL: Diverse Adversarial Corpus of Texts Yielded from Large Language Models", "authors": ["Shantanu Thorat", "Andrew Caines"], "categories": ["cs.CL", "cs.LG"], "comment": "MPhil in Advanced Computer Science thesis for University of Cambridge", "summary": "Existing AIG (AI-generated) text detectors struggle in real-world settings\ndespite succeeding in internal testing, suggesting that they may not be robust\nenough. We rigorously examine the machine-learning procedure to build these\ndetectors to address this. Most current AIG text detection datasets focus on\nzero-shot generations, but little work has been done on few-shot or one-shot\ngenerations, where LLMs are given human texts as an example. In response, we\nintroduce the Diverse Adversarial Corpus of Texts Yielded from Language models\n(DACTYL), a challenging AIG text detection dataset focusing on\none-shot/few-shot generations. We also include texts from domain-specific\ncontinued-pre-trained (CPT) language models, where we fully train all\nparameters using a memory-efficient optimization approach. Many existing AIG\ntext detectors struggle significantly on our dataset, indicating a potential\nvulnerability to one-shot/few-shot and CPT-generated texts. We also train our\nown classifiers using two approaches: standard binary cross-entropy (BCE)\noptimization and a more recent approach, deep X-risk optimization (DXO). While\nBCE-trained classifiers marginally outperform DXO classifiers on the DACTYL\ntest set, the latter excels on out-of-distribution (OOD) texts. In our mock\ndeployment scenario in student essay detection with an OOD student essay\ndataset, the best DXO classifier outscored the best BCE-trained classifier by\n50.56 macro-F1 score points at the lowest false positive rates for both. Our\nresults indicate that DXO classifiers generalize better without overfitting to\nthe test set. Our experiments highlight several areas of improvement for AIG\ntext detectors.", "AI": {"tldr": "This paper introduces the Diverse Adversarial Corpus of Texts Yielded from Language models (DACTYL), a dataset aimed at improving AI-generated text detectors, which struggle in practical applications.", "motivation": "Existing AI-generated text detectors perform well under test conditions but fail in real-world scenarios, indicating a need for better robustness.", "method": "We developed a dataset (DACTYL) focusing on one-shot/few-shot generations and used two methods to train classifiers: binary cross-entropy (BCE) and deep X-risk optimization (DXO).", "result": "BCE-trained classifiers outperformed DXO classifiers on our DACTYL test set, but DXO classifiers showed superior performance on out-of-distribution texts, significantly improving essay detection metrics.", "conclusion": "The study demonstrates the inadequacies in current AIG text detectors, proposes enhancements, and emphasizes the need for training on diverse datasets.", "key_contributions": ["Introduction of DACTYL dataset for AIG text detection", "Comparison of BCE and DXO optimization approaches", "Findings on performance discrepancies between training methods"], "limitations": "", "keywords": ["AIG text detection", "machine learning", "dataset", "classification", "language models"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.00669", "pdf": "https://arxiv.org/pdf/2508.00669.pdf", "abs": "https://arxiv.org/abs/2508.00669", "title": "Medical Reasoning in the Era of LLMs: A Systematic Review of Enhancement Techniques and Applications", "authors": ["Wenxuan Wang", "Zizhan Ma", "Meidan Ding", "Shiyi Zheng", "Shengyuan Liu", "Jie Liu", "Jiaming Ji", "Wenting Chen", "Xiang Li", "Linlin Shen", "Yixuan Yuan"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "The proliferation of Large Language Models (LLMs) in medicine has enabled\nimpressive capabilities, yet a critical gap remains in their ability to perform\nsystematic, transparent, and verifiable reasoning, a cornerstone of clinical\npractice. This has catalyzed a shift from single-step answer generation to the\ndevelopment of LLMs explicitly designed for medical reasoning. This paper\nprovides the first systematic review of this emerging field. We propose a\ntaxonomy of reasoning enhancement techniques, categorized into training-time\nstrategies (e.g., supervised fine-tuning, reinforcement learning) and test-time\nmechanisms (e.g., prompt engineering, multi-agent systems). We analyze how\nthese techniques are applied across different data modalities (text, image,\ncode) and in key clinical applications such as diagnosis, education, and\ntreatment planning. Furthermore, we survey the evolution of evaluation\nbenchmarks from simple accuracy metrics to sophisticated assessments of\nreasoning quality and visual interpretability. Based on an analysis of 60\nseminal studies from 2022-2025, we conclude by identifying critical challenges,\nincluding the faithfulness-plausibility gap and the need for native multimodal\nreasoning, and outlining future directions toward building efficient, robust,\nand sociotechnically responsible medical AI.", "AI": {"tldr": "A systematic review of Large Language Models (LLMs) in medicine focusing on their reasoning capabilities, categorizing enhancement techniques, and addressing critical challenges in clinical applications.", "motivation": "To address the gap in LLMs' ability to perform systematic, transparent, and verifiable reasoning in clinical practice.", "method": "The paper reviews 60 seminal studies from 2022-2025 and proposes a taxonomy of reasoning enhancement techniques categorized into training-time strategies and test-time mechanisms, analyzed across various data modalities and clinical applications.", "result": "Development of a taxonomy for reasoning enhancement techniques, analysis of application across data modalities, and evolution of evaluation benchmarks for LLMs in medical contexts.", "conclusion": "Identifies critical challenges such as the faithfulness-plausibility gap and emphasizes the need for native multimodal reasoning in medical AI, suggesting future directions for improving reasoning capabilities.", "key_contributions": ["First systematic review of LLMs designed for medical reasoning.", "Proposed taxonomy of reasoning enhancement techniques.", "Identified critical challenges and future research directions for medical AI."], "limitations": "The study is limited to the analysis of 60 studies from 2022-2025 and may not encompass the entire landscape of medical reasoning enhancement techniques.", "keywords": ["Large Language Models", "medical reasoning", "taxonomy", "evaluation benchmarks", "multimodal reasoning"], "importance_score": 10, "read_time_minutes": 15}}
{"id": "2508.00673", "pdf": "https://arxiv.org/pdf/2508.00673.pdf", "abs": "https://arxiv.org/abs/2508.00673", "title": "MELAC: Massive Evaluation of Large Language Models with Alignment of Culture in Persian Language", "authors": ["Farhan Farsi", "Farnaz Aghababaloo", "Shahriar Shariati Motlagh", "Parsa Ghofrani", "MohammadAli SadraeiJavaheri", "Shayan Bali", "Amirhossein Shabani", "Farbod Bijary", "Ghazal Zamaninejad", "AmirMohammad Salehoof", "Saeedeh Momtazi"], "categories": ["cs.CL"], "comment": "Preprint. Under review", "summary": "As large language models (LLMs) become increasingly embedded in our daily\nlives, evaluating their quality and reliability across diverse contexts has\nbecome essential. While comprehensive benchmarks exist for assessing LLM\nperformance in English, there remains a significant gap in evaluation resources\nfor other languages. Moreover, because most LLMs are trained primarily on data\nrooted in European and American cultures, they often lack familiarity with\nnon-Western cultural contexts. To address this limitation, our study focuses on\nthe Persian language and Iranian culture. We introduce 19 new evaluation\ndatasets specifically designed to assess LLMs on topics such as Iranian law,\nPersian grammar, Persian idioms, and university entrance exams. Using these\ndatasets, we benchmarked 41 prominent LLMs, aiming to bridge the existing\ncultural and linguistic evaluation gap in the field.", "AI": {"tldr": "This paper introduces 19 evaluation datasets for assessing large language models (LLMs) in the Persian language and Iranian culture, aimed at addressing cultural evaluation gaps.", "motivation": "To address the evaluation gap for LLMs in non-Western languages, specifically Persian, and to improve their cultural context understanding.", "method": "The authors created 19 new evaluation datasets focused on Iranian law, Persian grammar, idioms, and entrance exam topics, and benchmarked these against 41 prominent LLMs.", "result": "The benchmarking revealed significant performance insights of LLMs when faced with Persian language nuances and Iranian cultural contexts.", "conclusion": "The study highlights the need for more culturally aware benchmarks and provides a framework for evaluating LLMs in underrepresented languages and cultures.", "key_contributions": ["Introduction of 19 new evaluation datasets for Persian LLMs", "Benchmarking of 41 prominent LLMs against these datasets", "Highlighting cultural biases in LLMs trained predominantly on Western data."], "limitations": "Limited to Persian language and Iranian culture; findings may not generalize to other non-Western languages.", "keywords": ["Large Language Models", "Persian Language", "Evaluation Datasets", "Cultural Context", "Benchmarking"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.00675", "pdf": "https://arxiv.org/pdf/2508.00675.pdf", "abs": "https://arxiv.org/abs/2508.00675", "title": "Team \"better_call_claude\": Style Change Detection using a Sequential Sentence Pair Classifier", "authors": ["Gleb Schmidt", "Johannes Römisch", "Mariia Halchynska", "Svetlana Gorovaia", "Ivan P. Yamshchikov"], "categories": ["cs.CL"], "comment": null, "summary": "Style change detection - identifying the points in a document where writing\nstyle shifts - remains one of the most important and challenging problems in\ncomputational authorship analysis. At PAN 2025, the shared task challenges\nparticipants to detect style switches at the most fine-grained level:\nindividual sentences. The task spans three datasets, each designed with\ncontrolled and increasing thematic variety within documents. We propose to\naddress this problem by modeling the content of each problem instance - that\nis, a series of sentences - as a whole, using a Sequential Sentence Pair\nClassifier (SSPC). The architecture leverages a pre-trained language model\n(PLM) to obtain representations of individual sentences, which are then fed\ninto a bidirectional LSTM (BiLSTM) to contextualize them within the document.\nThe BiLSTM-produced vectors of adjacent sentences are concatenated and passed\nto a multi-layer perceptron for prediction per adjacency. Building on the work\nof previous PAN participants classical text segmentation, the approach is\nrelatively conservative and lightweight. Nevertheless, it proves effective in\nleveraging contextual information and addressing what is arguably the most\nchallenging aspect of this year's shared task: the notorious problem of\n\"stylistically shallow\", short sentences that are prevalent in the proposed\nbenchmark data. Evaluated on the official PAN-2025 test datasets, the model\nachieves strong macro-F1 scores of 0.923, 0.828, and 0.724 on the EASY, MEDIUM,\nand HARD data, respectively, outperforming not only the official random\nbaselines but also a much more challenging one: claude-3.7-sonnet's zero-shot\nperformance.", "AI": {"tldr": "This paper addresses style change detection in documents at the sentence level using a Sequential Sentence Pair Classifier (SSPC) model based on pre-trained language models and BiLSTM.", "motivation": "Style change detection is a challenging problem in computational authorship analysis, particularly for pinpointing style shifts at the level of individual sentences.", "method": "The proposed method utilizes a Sequential Sentence Pair Classifier (SSPC) which leverages pre-trained language models to represent sentences, which are then contextualized using a BiLSTM and combined into a multi-layer perceptron for prediction.", "result": "The model achieved strong macro-F1 scores of 0.923, 0.828, and 0.724 on the EASY, MEDIUM, and HARD datasets of the PAN-2025 challenge, outperforming both random baselines and a more advanced reference model.", "conclusion": "The SSPC model effectively captures contextual information in detecting style shifts, even in the presence of stylistically shallow sentences, indicating its potential for further applications in authorship analysis.", "key_contributions": ["Introduced a Sequential Sentence Pair Classifier (SSPC) for style change detection at the sentence level.", "Showed strong performance on PAN-2025 datasets, surpassing established benchmarks.", "Demonstrated effectiveness in handling stylistically shallow short sentences."], "limitations": "", "keywords": ["style change detection", "computational authorship analysis", "pre-trained language models"], "importance_score": 6, "read_time_minutes": 8}}
{"id": "2508.00679", "pdf": "https://arxiv.org/pdf/2508.00679.pdf", "abs": "https://arxiv.org/abs/2508.00679", "title": "Segment First, Retrieve Better: Realistic Legal Search via Rhetorical Role-Based Queries", "authors": ["Shubham Kumar Nigam", "Tanmay Dubey", "Noel Shallum", "Arnab Bhattacharya"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "Legal precedent retrieval is a cornerstone of the common law system, governed\nby the principle of stare decisis, which demands consistency in judicial\ndecisions. However, the growing complexity and volume of legal documents\nchallenge traditional retrieval methods. TraceRetriever mirrors real-world\nlegal search by operating with limited case information, extracting only\nrhetorically significant segments instead of requiring complete documents. Our\npipeline integrates BM25, Vector Database, and Cross-Encoder models, combining\ninitial results through Reciprocal Rank Fusion before final re-ranking.\nRhetorical annotations are generated using a Hierarchical BiLSTM CRF classifier\ntrained on Indian judgments. Evaluated on IL-PCR and COLIEE 2025 datasets,\nTraceRetriever addresses growing document volume challenges while aligning with\npractical search constraints, reliable and scalable foundation for precedent\nretrieval enhancing legal research when only partial case knowledge is\navailable.", "AI": {"tldr": "TraceRetriever is a legal precedent retrieval system designed to work with limited case information, improving traditional methods by extracting rhetorically significant segments from documents.", "motivation": "The complexity and volume of legal documents create challenges for traditional legal retrieval methods, necessitating an improved approach that can operate effectively with partial information.", "method": "The approach utilizes BM25, Vector Database, and Cross-Encoder models, combining results through Reciprocal Rank Fusion and ultimately re-ranking them. Rhetorical annotations are generated using a Hierarchical BiLSTM CRF classifier trained on Indian court judgments.", "result": "Evaluated on IL-PCR and COLIEE 2025 datasets, TraceRetriever shows improved retrieval performance, successfully addressing challenges posed by the increasing volume of legal documents.", "conclusion": "TraceRetriever provides a reliable and scalable solution for legal precedent retrieval, enhancing legal research capabilities even when only partial case knowledge is available.", "key_contributions": ["Introduces a retrieval method that operates with limited case information", "Combines multiple models for improved search results", "Generates rhetorical annotations to enhance relevance of retrieved legal precedents"], "limitations": "", "keywords": ["legal retrieval", "precedent", "BM25", "machine learning", "natural language processing"], "importance_score": 2, "read_time_minutes": 5}}
{"id": "2508.00680", "pdf": "https://arxiv.org/pdf/2508.00680.pdf", "abs": "https://arxiv.org/abs/2508.00680", "title": "Better Call Claude: Can LLMs Detect Changes of Writing Style?", "authors": ["Johannes Römisch", "Svetlana Gorovaia", "Mariia Halchynska", "Gleb Schmidt", "Ivan P. Yamshchikov"], "categories": ["cs.CL"], "comment": null, "summary": "This article explores the zero-shot performance of state-of-the-art large\nlanguage models (LLMs) on one of the most challenging tasks in authorship\nanalysis: sentence-level style change detection. Benchmarking four LLMs on the\nofficial PAN~2024 and 2025 \"Multi-Author Writing Style Analysis\" datasets, we\npresent several observations. First, state-of-the-art generative models are\nsensitive to variations in writing style - even at the granular level of\nindividual sentences. Second, their accuracy establishes a challenging baseline\nfor the task, outperforming suggested baselines of the PAN competition.\nFinally, we explore the influence of semantics on model predictions and present\nevidence suggesting that the latest generation of LLMs may be more sensitive to\ncontent-independent and purely stylistic signals than previously reported.", "AI": {"tldr": "The study examines the zero-shot performance of large language models on sentence-level style change detection in authorship analysis, benchmarking them on recent datasets and revealing their sensitivity to writing style variations.", "motivation": "To investigate the zero-shot capabilities of large language models in detecting style changes at the sentence level in authorship analysis, particularly given their potential applications in various fields.", "method": "Benchmarking four state-of-the-art large language models on the PAN 2024 and 2025 Multi-Author Writing Style Analysis datasets to evaluate their performance on sentence-level style change detection tasks.", "result": "The generative models demonstrated sensitivity to variations in writing style, establishing a challenging baseline that outperformed previous PAN competition baselines.", "conclusion": "The latest LLMs show increased sensitivity to content-independent and stylistic signals, which may influence their predictive performance in authorship analysis.", "key_contributions": ["First-time evaluation of LLMs on sentence-level style change detection", "Identification of model sensitivity to writing style variations", "Establishment of new performance baselines for future authorship analysis tasks"], "limitations": "", "keywords": ["large language models", "style change detection", "authorship analysis", "zero-shot learning"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2508.00709", "pdf": "https://arxiv.org/pdf/2508.00709.pdf", "abs": "https://arxiv.org/abs/2508.00709", "title": "NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian Common Law System", "authors": ["Shubham Kumar Nigam", "Balaramamahanthi Deepak Patnaik", "Shivam Mishra", "Ajay Varghese Thomas", "Noel Shallum", "Kripabandhu Ghosh", "Arnab Bhattacharya"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "Legal Judgment Prediction (LJP) has emerged as a key area in AI for law,\naiming to automate judicial outcome forecasting and enhance interpretability in\nlegal reasoning. While previous approaches in the Indian context have relied on\ninternal case content such as facts, issues, and reasoning, they often overlook\na core element of common law systems, which is reliance on statutory provisions\nand judicial precedents. In this work, we propose NyayaRAG, a\nRetrieval-Augmented Generation (RAG) framework that simulates realistic\ncourtroom scenarios by providing models with factual case descriptions,\nrelevant legal statutes, and semantically retrieved prior cases. NyayaRAG\nevaluates the effectiveness of these combined inputs in predicting court\ndecisions and generating legal explanations using a domain-specific pipeline\ntailored to the Indian legal system. We assess performance across various input\nconfigurations using both standard lexical and semantic metrics as well as\nLLM-based evaluators such as G-Eval. Our results show that augmenting factual\ninputs with structured legal knowledge significantly improves both predictive\naccuracy and explanation quality.", "AI": {"tldr": "The paper presents NyayaRAG, a framework for Legal Judgment Prediction in the Indian legal context, which integrates factual case descriptions with relevant legal statutes and prior cases to enhance predictive accuracy and explanation quality.", "motivation": "To improve the effectiveness of legal judgment prediction models by incorporating a broader range of inputs, including statutory provisions and judicial precedents, often overlooked in prior work.", "method": "NyayaRAG is a Retrieval-Augmented Generation framework that simulates courtroom scenarios by using case descriptions, legal statutes, and retrieved past cases combined with a domain-specific pipeline.", "result": "The study demonstrates that the integration of structured legal knowledge with factual inputs significantly enhances the predictive accuracy and quality of legal explanations in court outcomes.", "conclusion": "The findings indicate that enhancing legal judgment prediction with augmented, structured inputs leads to better performance in predicting decisions and generating explanations.", "key_contributions": ["Introduction of NyayaRAG for LJP incorporating legal statutes and prior cases.", "Demonstration of improved predictive accuracy and explanation quality through empirical evaluation.", "Utilization of domain-specific metrics alongside LLM-based evaluators."], "limitations": "", "keywords": ["Legal Judgment Prediction", "Retrieval-Augmented Generation", "Legal reasoning", "Machine Learning", "Indian legal system"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2508.00719", "pdf": "https://arxiv.org/pdf/2508.00719.pdf", "abs": "https://arxiv.org/abs/2508.00719", "title": "Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and Context-Aware KGQA", "authors": ["Yingxu Wang", "Shiqi Fan", "Mengzhu Wang", "Siwei Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Knowledge Graph Question Answering (KGQA) aims to interpret natural language\nqueries and perform structured reasoning over knowledge graphs by leveraging\ntheir relational and semantic structures to retrieve accurate answers. Recent\nKGQA methods primarily follow either retrieve-then-reason paradigm, relying on\nGNNs or heuristic rules for static paths extraction, or dynamic path generation\nstrategies that use large language models (LLMs) with prompting to jointly\nperform retrieval and reasoning. However, the former suffers from limited\nadaptability due to static path extraction and lack of contextual refinement,\nwhile the latter incurs high computational costs and struggles with accurate\npath evaluation due to reliance on fixed scoring functions and extensive LLM\ncalls. To address these issues, this paper proposes Dynamically Adaptive\nMCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search\nwith adaptive path evaluation for efficient and context-aware KGQA. DAMR\nemploys a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based\nplanner, which selects top-$k$ relevant relations at each step to reduce search\nspace. To improve path evaluation accuracy, we introduce a lightweight\nTransformer-based scorer that performs context-aware plausibility estimation by\njointly encoding the question and relation sequence through cross-attention,\nenabling the model to capture fine-grained semantic shifts during multi-hop\nreasoning. Furthermore, to alleviate the scarcity of high-quality supervision,\nDAMR incorporates a dynamic pseudo-path refinement mechanism that periodically\ngenerates training signals from partial paths explored during search, allowing\nthe scorer to continuously adapt to the evolving distribution of reasoning\ntrajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR\nsignificantly outperforms state-of-the-art methods.", "AI": {"tldr": "This paper presents Dynamically Adaptive MCTS-based Reasoning (DAMR), a novel approach for Knowledge Graph Question Answering (KGQA) that combines Monte Carlo Tree Search with an LLM-based planner for improved efficacy and context-awareness.", "motivation": "To address the limitations of current KGQA methods in adaptability and computational cost, this paper introduces a new framework for more efficient and accurate path evaluation in KGQA.", "method": "DAMR employs a Monte Carlo Tree Search framework guided by an LLM-based planner, which selects relevant relations and incorporates a lightweight Transformer-based scorer for context-aware plausibility estimation. It also includes a dynamic pseudo-path refinement mechanism for continuous adaptation to training signals.", "result": "Experiments show that DAMR outperforms state-of-the-art KGQA methods on multiple benchmarks, demonstrating improved accuracy and efficiency.", "conclusion": "The proposed DAMR framework addresses key challenges in KGQA, offering a more adaptable and accurate solution compared to existing methods.", "key_contributions": ["Introduction of a novel DAMR framework for KGQA.", "Integration of MCTS and LLM-based planning for path selection.", "Development of a lightweight Transformer scorer for context-aware evaluation."], "limitations": "", "keywords": ["Knowledge Graph Question Answering", "Monte Carlo Tree Search", "Large Language Models", "Path Evaluation", "Transformer"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.00741", "pdf": "https://arxiv.org/pdf/2508.00741.pdf", "abs": "https://arxiv.org/abs/2508.00741", "title": "Out-of-Context Abduction: LLMs Make Inferences About Procedural Data Leveraging Declarative Facts in Earlier Training Data", "authors": ["Sohaib Imran", "Rob Lamb", "Peter M. Atkinson"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are trained on large corpora, yet it is unclear\nwhether they can reason about the information present within their training\ndata. We design experiments to study out-of-context abduction in LLMs, the\nability to infer the most plausible explanations for observations using\nrelevant facts present in training data. We train treatment LLMs on names and\nbehavior descriptions of fictitious chatbots, but not on examples of dialogue\nwith the chatbots. We find that OpenAI's GPT 4o LLM can correctly infer at\nleast one chatbot's name after observing example responses characteristic of\nthat chatbot. We also find that previously training GPT 4o on descriptions of a\nchatbot's behavior allows it to display behaviors more characteristic of the\nchatbot when iteratively trained to display such behaviors. Our results have\nimplications for situational awareness in LLMs and, therefore, for AI safety.", "AI": {"tldr": "This paper investigates the ability of large language models (LLMs) to reason about their training data through out-of-context abduction, focusing on OpenAI's GPT 4o model.", "motivation": "The study aims to explore whether LLMs can infer plausible explanations for observations based on relevant facts within their training data, contributing to understanding their reasoning capabilities and implications for AI safety.", "method": "Experiments were conducted with LLMs trained on fictitious chatbot names and behavior descriptions, without including dialogue examples, to assess inferencing based on response patterns.", "result": "The experiments demonstrated that GPT 4o could infer at least one chatbot's name and exhibited behavior consistent with training when prompted appropriately, showing an ability to leverage training information effectively.", "conclusion": "The findings suggest that LLMs can exhibit situational awareness by integrating training data insights into their responses, raising important questions regarding AI safety and reasoning capabilities.", "key_contributions": ["Investigates out-of-context abduction in LLMs.", "Demonstrates GPT 4o's ability to infer names and behaviors from observations.", "Highlights implications for situational awareness and AI safety."], "limitations": "The research is limited to specific training contexts and may not generalize to all inference scenarios in LLMs.", "keywords": ["Large Language Models", "Machine Learning", "AI Safety", "Out-of-Context Abduction", "Situational Awareness"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.00742", "pdf": "https://arxiv.org/pdf/2508.00742.pdf", "abs": "https://arxiv.org/abs/2508.00742", "title": "Applying Psychometrics to Large Language Model Simulated Populations: Recreating the HEXACO Personality Inventory Experiment with Generative Agents", "authors": ["Sarah Mercer", "Daniel P. Martin", "Phil Swatton"], "categories": ["cs.CL", "cs.LG"], "comment": "26 pages, 14 figures", "summary": "Generative agents powered by Large Language Models demonstrate human-like\ncharacteristics through sophisticated natural language interactions. Their\nability to assume roles and personalities based on predefined character\nbiographies has positioned them as cost-effective substitutes for human\nparticipants in social science research. This paper explores the validity of\nsuch persona-based agents in representing human populations; we recreate the\nHEXACO personality inventory experiment by surveying 310 GPT-4 powered agents,\nconducting factor analysis on their responses, and comparing these results to\nthe original findings presented by Ashton, Lee, & Goldberg in 2004. Our results\nfound 1) a coherent and reliable personality structure was recoverable from the\nagents' responses demonstrating partial alignment to the HEXACO framework. 2)\nthe derived personality dimensions were consistent and reliable within GPT-4,\nwhen coupled with a sufficiently curated population, and 3) cross-model\nanalysis revealed variability in personality profiling, suggesting\nmodel-specific biases and limitations. We discuss the practical considerations\nand challenges encountered during the experiment. This study contributes to the\nongoing discourse on the potential benefits and limitations of using generative\nagents in social science research and provides useful guidance on designing\nconsistent and representative agent personas to maximise coverage and\nrepresentation of human personality traits.", "AI": {"tldr": "The paper examines the use of generative agents powered by LLMs like GPT-4 in social science research, specifically assessing their ability to represent human personality traits through the HEXACO personality inventory experiment.", "motivation": "To explore the validity of persona-based agents as substitutes for human participants in social science research.", "method": "The study recreates the HEXACO personality inventory experiment by surveying 310 GPT-4 powered agents and conducting factor analysis on their responses, comparing results with original findings from 2004.", "result": "A coherent and reliable personality structure was found in the agents' responses, demonstrating partial alignment to the HEXACO framework, and revealing model-specific biases in personality profiling.", "conclusion": "The study highlights the potential and limitations of using generative agents in social science research and provides guidance for designing agent personas to better represent human personality traits.", "key_contributions": ["Validation of LLM-powered agents in social science applications", "Identification of model-specific biases in personality profiling", "Guidance on designing effective persona-based agents for research"], "limitations": "Model-specific biases and challenges in creating curated populations for reliable results.", "keywords": ["generative agents", "Large Language Models", "HEXACO personality inventory", "social science research", "personality profiling"], "importance_score": 9, "read_time_minutes": 30}}
{"id": "2508.00743", "pdf": "https://arxiv.org/pdf/2508.00743.pdf", "abs": "https://arxiv.org/abs/2508.00743", "title": "Agentic large language models improve retrieval-based radiology question answering", "authors": ["Sebastian Wind", "Jeta Sopa", "Daniel Truhn", "Mahshad Lotfinia", "Tri-Thien Nguyen", "Keno Bressem", "Lisa Adams", "Mirabela Rusu", "Harald Köstler", "Gerhard Wellein", "Andreas Maier", "Soroosh Tayebi Arasteh"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Clinical decision-making in radiology increasingly benefits from artificial\nintelligence (AI), particularly through large language models (LLMs). However,\ntraditional retrieval-augmented generation (RAG) systems for radiology question\nanswering (QA) typically rely on single-step retrieval, limiting their ability\nto handle complex clinical reasoning tasks. Here we propose an agentic RAG\nframework enabling LLMs to autonomously decompose radiology questions,\niteratively retrieve targeted clinical evidence from Radiopaedia, and\ndynamically synthesize evidence-based responses. We evaluated 24 LLMs spanning\ndiverse architectures, parameter scales (0.5B to >670B), and training paradigms\n(general-purpose, reasoning-optimized, clinically fine-tuned), using 104\nexpert-curated radiology questions from previously established RSNA-RadioQA and\nExtendedQA datasets. Agentic retrieval significantly improved mean diagnostic\naccuracy over zero-shot prompting (73% vs. 64%; P<0.001) and conventional\nonline RAG (73% vs. 68%; P<0.001). The greatest gains occurred in mid-sized\nmodels (e.g., Mistral Large improved from 72% to 81%) and small-scale models\n(e.g., Qwen 2.5-7B improved from 55% to 71%), while very large models (>200B\nparameters) demonstrated minimal changes (<2% improvement). Additionally,\nagentic retrieval reduced hallucinations (mean 9.4%) and retrieved clinically\nrelevant context in 46% of cases, substantially aiding factual grounding. Even\nclinically fine-tuned models exhibited meaningful improvements (e.g.,\nMedGemma-27B improved from 71% to 81%), indicating complementary roles of\nretrieval and fine-tuning. These results highlight the potential of agentic\nframeworks to enhance factuality and diagnostic accuracy in radiology QA,\nparticularly among mid-sized LLMs, warranting future studies to validate their\nclinical utility.", "AI": {"tldr": "The paper presents an agentic RAG framework for radiology question answering that improves diagnostic accuracy and reduces hallucinations in LLMs, particularly in mid-sized models.", "motivation": "To enhance clinical decision-making in radiology by addressing limitations of traditional retrieval-augmented generation systems in handling complex reasoning tasks.", "method": "An agentic retrieval-augmented generation (RAG) framework that decomposes questions and retrieves relevant clinical evidence iteratively from Radiopaedia.", "result": "Agentic retrieval improved diagnostic accuracy significantly compared to zero-shot prompting (73% vs. 64%) and conventional online RAG (73% vs. 68%). Mid-sized models showed the greatest improvement, while larger models exhibited minimal changes.", "conclusion": "The agentic framework enhances factuality and diagnostic accuracy in radiology QA, particularly in mid-sized LLMs, suggesting a need for future studies to confirm clinical utility.", "key_contributions": ["Proposing a novel agentic RAG framework for radiology QA", "Demonstrating significant improvements in diagnostic accuracy", "Reducing hallucinations and enhancing context retrieval."], "limitations": "The study evaluated only 24 LLMs and relied on specific datasets for assessment.", "keywords": ["radiology", "large language models", "RAG", "clinical decision-making", "AI"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.00757", "pdf": "https://arxiv.org/pdf/2508.00757.pdf", "abs": "https://arxiv.org/abs/2508.00757", "title": "GLiDRE: Generalist Lightweight model for Document-level Relation Extraction", "authors": ["Robin Armingaud", "Romaric Besançon"], "categories": ["cs.CL"], "comment": "Submitted to ARR July", "summary": "Relation Extraction (RE) is a fundamental task in Natural Language\nProcessing, and its document-level variant poses significant challenges, due to\nthe need to model complex interactions between entities across sentences.\nCurrent approaches, largely based on the ATLOP architecture, are commonly\nevaluated on benchmarks like DocRED and Re-DocRED. However, their performance\nin zero-shot or few-shot settings remains largely underexplored due to the\ntask's complexity. Recently, the GLiNER model has shown that a compact NER\nmodel can outperform much larger Large Language Models. With a similar\nmotivation, we introduce GLiDRE, a new model for document-level relation\nextraction that builds on the key ideas of GliNER. We benchmark GLiDRE against\nstate-of-the-art models across various data settings on the Re-DocRED dataset.\nOur results demonstrate that GLiDRE achieves state-of-the-art performance in\nfew-shot scenarios. Our code is publicly available.", "AI": {"tldr": "GLiDRE is a new model for document-level relation extraction that outperforms existing models in few-shot scenarios, building on ideas from GLiNER.", "motivation": "To address the challenges of document-level relation extraction and improve performance in zero-shot or few-shot settings.", "method": "GLiDRE utilizes a compact NER architecture inspired by GLiNER and is evaluated against state-of-the-art models on the Re-DocRED dataset.", "result": "GLiDRE achieves state-of-the-art performance in few-shot scenarios, showing better results than larger models.", "conclusion": "GLiDRE demonstrates the effectiveness of compact models for complex NLP tasks like relation extraction.", "key_contributions": ["Introduction of GLiDRE model for document-level relation extraction", "Performance benchmarking on Re-DocRED dataset", "State-of-the-art results in few-shot scenarios"], "limitations": "", "keywords": ["Relation Extraction", "Natural Language Processing", "GLiDRE", "Zero-shot learning", "Few-shot learning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.00760", "pdf": "https://arxiv.org/pdf/2508.00760.pdf", "abs": "https://arxiv.org/abs/2508.00760", "title": "MMBERT: Scaled Mixture-of-Experts Multimodal BERT for Robust Chinese Hate Speech Detection under Cloaking Perturbations", "authors": ["Qiyao Xue", "Yuchen Dou", "Ryan Shi", "Xiang Lorraine Li", "Wei Gao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Hate speech detection on Chinese social networks presents distinct\nchallenges, particularly due to the widespread use of cloaking techniques\ndesigned to evade conventional text-based detection systems. Although large\nlanguage models (LLMs) have recently improved hate speech detection\ncapabilities, the majority of existing work has concentrated on English\ndatasets, with limited attention given to multimodal strategies in the Chinese\ncontext. In this study, we propose MMBERT, a novel BERT-based multimodal\nframework that integrates textual, speech, and visual modalities through a\nMixture-of-Experts (MoE) architecture. To address the instability associated\nwith directly integrating MoE into BERT-based models, we develop a progressive\nthree-stage training paradigm. MMBERT incorporates modality-specific experts, a\nshared self-attention mechanism, and a router-based expert allocation strategy\nto enhance robustness against adversarial perturbations. Empirical results in\nseveral Chinese hate speech datasets show that MMBERT significantly surpasses\nfine-tuned BERT-based encoder models, fine-tuned LLMs, and LLMs utilizing\nin-context learning approaches.", "AI": {"tldr": "MMBERT is a novel multimodal framework improving hate speech detection on Chinese social networks by integrating text, speech, and visual modalities through a Mixture-of-Experts architecture.", "motivation": "Existing methods for hate speech detection primarily focus on English datasets, overlooking the unique challenges faced in Chinese social networks, particularly with cloaking techniques.", "method": "MMBERT employs a progressive three-stage training paradigm, combining modality-specific experts and a shared self-attention mechanism to improve robustness and detection capabilities.", "result": "MMBERT outperforms fine-tuned BERT-based models and LLMs on multiple Chinese hate speech datasets, demonstrating its effectiveness in multimodal integration.", "conclusion": "The proposed MMBERT framework significantly advances hate speech detection for Chinese social networks through innovative architecture and training methods.", "key_contributions": ["Introduction of the MMBERT multimodal framework for hate speech detection.", "Development of a three-stage training paradigm to enhance model robustness.", "Integration of text, speech, and visual modalities using a Mixture-of-Experts architecture."], "limitations": "", "keywords": ["hate speech detection", "multimodal", "Chinese social networks", "Mixture-of-Experts", "BERT"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2508.00762", "pdf": "https://arxiv.org/pdf/2508.00762.pdf", "abs": "https://arxiv.org/abs/2508.00762", "title": "ITUNLP at SemEval-2025 Task 8: Question-Answering over Tabular Data: A Zero-Shot Approach using LLM-Driven Code Generation", "authors": ["Atakan Site", "Emre Hakan Erdemir", "Gülşen Eryiğit"], "categories": ["cs.CL"], "comment": null, "summary": "This paper presents our system for SemEval-2025 Task 8: DataBench,\nQuestion-Answering over Tabular Data. The primary objective of this task is to\nperform question answering on given tabular datasets from diverse domains under\ntwo subtasks: DataBench QA (Subtask I) and DataBench Lite QA (Subtask II). To\ntackle both subtasks, we developed a zero-shot solution with a particular\nemphasis on leveraging Large Language Model (LLM)-based code generation.\nSpecifically, we propose a Python code generation framework utilizing\nstate-of-the-art open-source LLMs to generate executable Pandas code via\noptimized prompting strategies. Our experiments reveal that different LLMs\nexhibit varying levels of effectiveness in Python code generation.\nAdditionally, results show that Python code generation achieves superior\nperformance in tabular question answering compared to alternative approaches.\nAlthough our ranking among zero-shot systems is unknown at the time of this\npaper's submission, our system achieved eighth place in Subtask I and sixth\nplace in Subtask~II among the 30 systems that outperformed the baseline in the\nopen-source models category.", "AI": {"tldr": "The paper describes a system developed for the SemEval-2025 Task 8 focusing on question-answering over tabular data using LLM-based code generation.", "motivation": "To address question answering from tabular datasets in diverse domains, improving efficiency and accuracy in generating executable code for data manipulation.", "method": "A zero-shot framework utilizing LLMs to generate Pandas code for tabular data, employing optimized prompting strategies for effective code generation.", "result": "The system ranked eighth in Subtask I and sixth in Subtask II among 30 competitors, demonstrating better performance in Python code generation for tabular question answering compared to other methods.", "conclusion": "The results indicate that LLM-based code generation is effective for tabular question answering, with promise for future applications and improvements.", "key_contributions": ["Development of a zero-shot solution for tabular data QA", "Optimized prompting strategies for LLMs to generate executable code", "Demonstration of superior performance of Python code generation over alternative methods."], "limitations": "", "keywords": ["tabular data", "question answering", "large language models", "code generation", "Pandas"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.00788", "pdf": "https://arxiv.org/pdf/2508.00788.pdf", "abs": "https://arxiv.org/abs/2508.00788", "title": "Do They Understand Them? An Updated Evaluation on Nonbinary Pronoun Handling in Large Language Models", "authors": ["Xushuo Tang", "Yi Ding", "Zhengyi Yang", "Yin Chen", "Yongrui Gu", "Wenke Yang", "Mingchen Ju", "Xin Cao", "Yongfei Liu", "Wenjie Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in sensitive contexts\nwhere fairness and inclusivity are critical. Pronoun usage, especially\nconcerning gender-neutral and neopronouns, remains a key challenge for\nresponsible AI. Prior work, such as the MISGENDERED benchmark, revealed\nsignificant limitations in earlier LLMs' handling of inclusive pronouns, but\nwas constrained to outdated models and limited evaluations. In this study, we\nintroduce MISGENDERED+, an extended and updated benchmark for evaluating LLMs'\npronoun fidelity. We benchmark five representative LLMs, GPT-4o, Claude 4,\nDeepSeek-V3, Qwen Turbo, and Qwen2.5, across zero-shot, few-shot, and gender\nidentity inference. Our results show notable improvements compared with\nprevious studies, especially in binary and gender-neutral pronoun accuracy.\nHowever, accuracy on neopronouns and reverse inference tasks remains\ninconsistent, underscoring persistent gaps in identity-sensitive reasoning. We\ndiscuss implications, model-specific observations, and avenues for future\ninclusive AI research.", "AI": {"tldr": "This study presents MISGENDERED+, an updated benchmark for evaluating large language models on their pronoun usage, particularly focusing on gender-neutral and neopronouns, and finds gaps in performance for neopronouns.", "motivation": "To address the limitations of earlier benchmarks in evaluating large language models' handling of inclusive pronouns, especially in sensitive contexts requiring fairness and inclusivity.", "method": "The study benchmarks five large language models using MISGENDERED+ across zero-shot, few-shot, and gender identity inference tasks.", "result": "The results indicate significant improvements in accuracy for binary and gender-neutral pronouns, but inconsistent performance for neopronouns and reverse inference tasks.", "conclusion": "The study highlights the need for ongoing research and development of inclusive AI tools, indicating existing gaps in identity-sensitive reasoning in LLMs.", "key_contributions": ["Introduction of the MISGENDERED+ benchmark", "Evaluation of five state-of-the-art language models", "Insights into performance gaps for neopronouns"], "limitations": "Inconsistent accuracy on neopronouns and reverse inference tasks remains.", "keywords": ["large language models", "pronoun usage", "MISGENDERED+", "inclusive AI", "gender-neutral pronouns"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.00819", "pdf": "https://arxiv.org/pdf/2508.00819.pdf", "abs": "https://arxiv.org/abs/2508.00819", "title": "Beyond Fixed: Variable-Length Denoising for Diffusion Large Language Models", "authors": ["Jinsong Li", "Xiaoyi Dong", "Yuhang Zang", "Yuhang Cao", "Jiaqi Wang", "Dahua Lin"], "categories": ["cs.CL"], "comment": "Code is available at https://github.com/Li-Jinsong/DAEDAL", "summary": "Diffusion Large Language Models (DLLMs) are emerging as a powerful\nalternative to the dominant Autoregressive Large Language Models, offering\nefficient parallel generation and capable global context modeling. However, the\npractical application of DLLMs is hindered by a critical architectural\nconstraint: the need for a statically predefined generation length. This static\nlength allocation leads to a problematic trade-off: insufficient lengths\ncripple performance on complex tasks, while excessive lengths incur significant\ncomputational overhead and sometimes result in performance degradation. While\nthe inference framework is rigid, we observe that the model itself possesses\ninternal signals that correlate with the optimal response length for a given\ntask. To bridge this gap, we leverage these latent signals and introduce\nDAEDAL, a novel training-free denoising strategy that enables Dynamic Adaptive\nLength Expansion for Diffusion Large Language Models. DAEDAL operates in two\nphases: 1) Before the denoising process, DAEDAL starts from a short initial\nlength and iteratively expands it to a coarse task-appropriate length, guided\nby a sequence completion metric. 2) During the denoising process, DAEDAL\ndynamically intervenes by pinpointing and expanding insufficient generation\nregions through mask token insertion, ensuring the final output is fully\ndeveloped. Extensive experiments on DLLMs demonstrate that DAEDAL achieves\nperformance comparable, and in some cases superior, to meticulously tuned\nfixed-length baselines, while simultaneously enhancing computational efficiency\nby achieving a higher effective token ratio. By resolving the static length\nconstraint, DAEDAL unlocks new potential for DLLMs, bridging a critical gap\nwith their Autoregressive counterparts and paving the way for more efficient\nand capable generation.", "AI": {"tldr": "The paper introduces DAEDAL, a novel training-free strategy for Dynamic Adaptive Length Expansion in Diffusion Large Language Models, addressing the constraints of static generation lengths and improving performance and efficiency.", "motivation": "To overcome the limitations of static predefined generation lengths in Diffusion Large Language Models, which affect performance and computational efficiency.", "method": "DAEDAL operates in two phases: first, it expands a short initial generation length based on a sequence completion metric; second, it dynamically adjusts insufficient regions during the denoising process using mask token insertion.", "result": "DAEDAL achieves performance comparable or superior to fixed-length baselines while increasing computational efficiency by attaining a higher effective token ratio.", "conclusion": "By addressing the critical architectural constraint of static lengths, DAEDAL enhances the capabilities of DLLMs, promoting their efficiency and performance in generative tasks.", "key_contributions": ["Introduction of a training-free denoising strategy for length adaptation", "Demonstration of superior performance compared to fixed-length baselines", "Improvement in computational efficiency through effective token usage"], "limitations": "", "keywords": ["Diffusion Models", "Dynamic Length", "Large Language Models", "AI Efficiency", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2412.10207", "pdf": "https://arxiv.org/pdf/2412.10207.pdf", "abs": "https://arxiv.org/abs/2412.10207", "title": "Retrieval-Augmented Semantic Parsing: Improving Generalization with Lexical Knowledge", "authors": ["Xiao Zhang", "Qianru Meng", "Johan Bos"], "categories": ["cs.CL"], "comment": "Accpted by 16th IWCS", "summary": "Open-domain semantic parsing remains a challenging task, as neural models\noften rely on heuristics and struggle to handle unseen concepts. In this paper,\nwe investigate the potential of large language models (LLMs) for this task and\nintroduce Retrieval-Augmented Semantic Parsing (RASP), a simple yet effective\napproach that integrates external symbolic knowledge into the parsing process.\nOur experiments not only show that LLMs outperform previous encoder-decoder\nbaselines for semantic parsing, but that RASP further enhances their ability to\npredict unseen concepts, nearly doubling the performance of previous models on\nout-of-distribution concepts. These findings highlight the promise of\nleveraging large language models and retrieval mechanisms for robust and\nopen-domain semantic parsing.", "AI": {"tldr": "This paper presents Retrieval-Augmented Semantic Parsing (RASP), leveraging large language models (LLMs) to enhance semantic parsing, particularly for unseen concepts.", "motivation": "The motivation is to address the challenges faced by neural models in open-domain semantic parsing, particularly their reliance on heuristics and difficulties with unseen concepts.", "method": "The authors propose a method called Retrieval-Augmented Semantic Parsing (RASP), which integrates external symbolic knowledge into the parsing process, using large language models.", "result": "Experiments demonstrate that LLMs surpass previous encoder-decoder models for semantic parsing, and RASP nearly doubles performance for out-of-distribution concepts.", "conclusion": "The findings suggest that combining large language models with retrieval mechanisms significantly improves robust and open-domain semantic parsing capabilities.", "key_contributions": ["Introduction of Retrieval-Augmented Semantic Parsing (RASP)", "Demonstration of enhanced performance of LLMs on semantic parsing tasks", "Significant improvement in handling unseen concepts compared to previous models"], "limitations": "", "keywords": ["semantic parsing", "large language models", "open-domain parsing", "retrieval-augmented", "unseen concepts"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.08395", "pdf": "https://arxiv.org/pdf/2502.08395.pdf", "abs": "https://arxiv.org/abs/2502.08395", "title": "IssueBench: Millions of Realistic Prompts for Measuring Issue Bias in LLM Writing Assistance", "authors": ["Paul Röttger", "Musashi Hinck", "Valentin Hofmann", "Kobi Hackenburg", "Valentina Pyatkin", "Faeze Brahman", "Dirk Hovy"], "categories": ["cs.CL"], "comment": "under review", "summary": "Large language models (LLMs) are helping millions of users write texts about\ndiverse issues, and in doing so expose users to different ideas and\nperspectives. This creates concerns about issue bias, where an LLM tends to\npresent just one perspective on a given issue, which in turn may influence how\nusers think about this issue. So far, it has not been possible to measure which\nissue biases LLMs actually manifest in real user interactions, making it\ndifficult to address the risks from biased LLMs. Therefore, we create\nIssueBench: a set of 2.49m realistic prompts for measuring issue bias in LLM\nwriting assistance, which we construct based on 3.9k templates (e.g. \"write a\nblog about\") and 212 political issues (e.g. \"AI regulation\") from real user\ninteractions. Using IssueBench, we show that issue biases are common and\npersistent in state-of-the-art LLMs. We also show that biases are remarkably\nsimilar across models, and that all models align more with US Democrat than\nRepublican voter opinion on a subset of issues. IssueBench can easily be\nadapted to include other issues, templates, or tasks. By enabling robust and\nrealistic measurement, we hope that IssueBench can bring a new quality of\nevidence to ongoing discussions about LLM biases and how to address them.", "AI": {"tldr": "IssueBench is a benchmarking tool for measuring issue bias in large language models, constructed from 2.49m prompts based on real user interactions.", "motivation": "To address concerns about issue bias in LLMs and provide a means to measure the biases they may introduce in user interactions.", "method": "Constructed a dataset called IssueBench containing 2.49 million prompts based on 3.9k templates and 212 political issues, derived from real user interactions.", "result": "Found common and persistent issue biases in state-of-the-art LLMs, which tend to align more with US Democrat opinions than Republican on certain issues.", "conclusion": "IssueBench can facilitate more realistic and robust measurement of biases in LLMs, potentially guiding efforts to address these biases.", "key_contributions": ["Creation of IssueBench dataset for measuring LLM biases", "Demonstrated common biases across different LLMs", "Showed political alignment of LLM outputs with US Democrat views"], "limitations": "", "keywords": ["large language models", "issue bias", "benchmarking", "human-computer interaction", "political issues"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.08441", "pdf": "https://arxiv.org/pdf/2502.08441.pdf", "abs": "https://arxiv.org/abs/2502.08441", "title": "Better Embeddings with Coupled Adam", "authors": ["Felix Stollenwerk", "Tobias Stollenwerk"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL 2025 (Main), see https://aclanthology.org/2025.acl-long.1321/", "summary": "Despite their remarkable capabilities, LLMs learn word representations that\nexhibit the undesirable yet poorly understood feature of anisotropy. In this\npaper, we argue that the second moment in Adam is a cause of anisotropic\nembeddings, and suggest a modified optimizer called Coupled Adam to mitigate\nthe problem. Our experiments demonstrate that Coupled Adam significantly\nimproves the quality of embeddings, while also leading to better upstream and\ndownstream performance on large enough datasets.", "AI": {"tldr": "This paper introduces Coupled Adam, a modified optimizer aimed at reducing anisotropy in embeddings produced by LLMs, leading to improved performance on various tasks.", "motivation": "LLMs often produce anisotropic word representations, which negatively affect their performance; understanding the causes and finding solutions motivates this research.", "method": "The paper identifies the role of the second moment in the Adam optimizer as a cause for anisotropy and proposes a new optimizer, Coupled Adam, to address this issue.", "result": "Experiments show that Coupled Adam mitigates anisotropic embeddings and results in better performance across different applications.", "conclusion": "Coupled Adam improves embedding quality and enhances both upstream and downstream performance on sufficiently large datasets.", "key_contributions": ["Introduction of Coupled Adam optimizer", "Demonstration of the link between second moment in Adam and anisotropy", "Empirical results showing improvements in embedding quality and overall performance"], "limitations": "The approach may require larger datasets to see significant improvements and may not universally apply to all types of LLMs.", "keywords": ["LLMs", "Adam optimizer", "anisotropy", "Coupled Adam", "embeddings"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.12927", "pdf": "https://arxiv.org/pdf/2502.12927.pdf", "abs": "https://arxiv.org/abs/2502.12927", "title": "SEFL: Enhancing Educational Assignment Feedback with LLM Agents", "authors": ["Mike Zhang", "Amalie Pernille Dilling", "Léon Gondelman", "Niels Erik Ruan Lyngdorf", "Euan D. Lindsay", "Johannes Bjerva"], "categories": ["cs.CL"], "comment": null, "summary": "Providing high-quality feedback to student assignments is crucial for student\nsuccess, but it is constrained by time and costs. In this work, we introduce\nSynthetic Educational Feedback Loops (SEFL), a synthetic data framework\ndesigned to generate data that resembles immediate, on-demand feedback at scale\nwithout relying on extensive, real-world student assignments. To get this type\nof data, two large language models (LLMs) operate in teacher-student roles to\nsimulate assignment completion and formative feedback, generating synthetic\npairs of student work and corresponding critiques and actionable improvements\nfrom a teacher. With this data, we fine-tune smaller, more computationally\nefficient LLMs on these synthetic pairs, enabling them to replicate key\nfeatures of high-quality, goal-oriented feedback. Unlike personalized tutoring\napproaches that offer multi-turn, individualized instruction, SEFL specifically\nfocuses on replicating the teacher-student assignment feedback loop in higher\neducation. Through comprehensive evaluations with four LLM judges and three\nhuman experts, we demonstrate that SEFL-tuned models outperform both their\nnon-tuned counterparts in feedback quality and an existing baseline. The\npotential for societal impact is reinforced by extensive qualitative comments\nby ratings by human stakeholders -- both students and higher education\ninstructors. All in all, SEFL has substantial potential to transform feedback\nprocesses for higher education and beyond.", "AI": {"tldr": "Introducing Synthetic Educational Feedback Loops (SEFL), a framework leveraging LLMs to generate synthetic feedback for student assignments.", "motivation": "High-quality feedback is essential for student success but often limited by time and costs; SEFL aims to address these constraints.", "method": "Two LLMs simulate a teacher-student interaction to generate synthetic student work and feedback pairs, which are used to fine-tune smaller LLMs for feedback generation.", "result": "SEFL-tuned models demonstrate superior feedback quality compared to non-tuned models and existing baselines, validated through evaluations by experts.", "conclusion": "SEFL has significant potential to enhance feedback processes in higher education, as indicated by positive qualitative feedback from human stakeholders.", "key_contributions": ["Development of the SEFL framework for generating synthetic educational feedback.", "Demonstrated improvements in feedback quality with fine-tuned LLMs.", "Evaluation and validation of SEFL's potential impact on higher education feedback mechanisms."], "limitations": "The framework's reliance on synthetic data may not fully capture all nuances of real student feedback processes.", "keywords": ["Synthetic Educational Feedback", "Large Language Models", "Higher Education Feedback", "Machine Learning", "Educational Technology"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.14969", "pdf": "https://arxiv.org/pdf/2502.14969.pdf", "abs": "https://arxiv.org/abs/2502.14969", "title": "Lost in Space: Finding the Right Tokens for Structured Output", "authors": ["Sil Hamilton", "David Mimno"], "categories": ["cs.CL"], "comment": null, "summary": "General-purpose language models are trained to produce varied natural\nlanguage outputs, but for some tasks, like annotation or classification, we\nneed more specific output formats. LLM systems increasingly support structured\noutput, which enforces formats by sampling tokens according to a grammar -- but\nalso unpredictably reduces downstream performance. Are there systematic\ndifferences between grammars that appear semantically (and often visually)\nsimilar to humans? To answer this, we test four popular model families with\nfive varying output formats on four common NLP benchmarks. We find all models\nperform most accurately when guided to use formats respecting convention, such\nas letters for multiple choice and real numbers for numerical prediction.\nPerformance also improves by 5%-10% when guiding models to return tokens\nincorporating leading whitespace, with smaller models benefiting the most. We\nfind leading whitespace helps models avoid structural deficiencies in subword\ntoken representations. We finally present best practices for researchers using\nlanguage models as zero-shot classifiers with structured output.", "AI": {"tldr": "This paper investigates the impact of structured output formats on the performance of language models in NLP tasks, revealing best practices for improving accuracy.", "motivation": "To explore how structured output formats affect the performance of general-purpose language models in tasks like annotation and classification.", "method": "Testing four popular model families with five different output formats across four common NLP benchmarks.", "result": "Models perform most accurately with conventional formats and show a 5%-10% improvement when using leading whitespace in outputs, with the largest gains in smaller models.", "conclusion": "Guiding language models to use proper output formats enhances accuracy, and using leading whitespace can prevent structural deficiencies in token representations.", "key_contributions": ["Identified key structured output formats that improve model performance.", "Showed the effectiveness of leading whitespace in outputs for better accuracy.", "Provided best practices for using language models as zero-shot classifiers."], "limitations": "", "keywords": ["language models", "structured output", "NLP benchmarks", "classification", "best practices"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.17407", "pdf": "https://arxiv.org/pdf/2502.17407.pdf", "abs": "https://arxiv.org/abs/2502.17407", "title": "Linguistic Generalizability of Test-Time Scaling in Mathematical Reasoning", "authors": ["Guijin Son", "Jiwoo Hong", "Hyunwoo Ko", "James Thorne"], "categories": ["cs.CL"], "comment": "ACL 2025 (ORAL)", "summary": "Scaling pre-training compute has proven effective for achieving\nmulitlinguality, but does the same hold for test-time scaling? In this work, we\nintroduce MCLM, a multilingual math benchmark featuring competition-level\nproblems in 55 languages. We test three test-time scaling methods-Outcome\nReward Modeling (ORM), Process Reward Modeling (ORM), and Budget Forcing\n(BF)-on both Qwen2.5-1.5B Math and MR1-1.5B, a multilingual LLM we trained for\nextended reasoning. Our experiments show that using Qwen2.5-1.5B Math with ORM\nachieves a score of 35.8 on MCLM, while BF on MR1-1.5B attains 35.2. Although\n\"thinking LLMs\" have recently garnered significant attention, we find that\ntheir performance is comparable to traditional scaling methods like best-of-N\nonce constrained to similar levels of inference FLOPs. Moreover, while BF\nyields a 20-point improvement on English AIME, it provides only a 1.94-point\naverage gain across other languages-a pattern consistent across the other\ntest-time scaling methods we studied-higlighting that test-time scaling may not\ngeneralize as effectively to multilingual tasks. To foster further research, we\nrelease MCLM, MR1-1.5B, and evaluation results.", "AI": {"tldr": "This paper introduces MCLM, a multilingual math benchmark, and evaluates three test-time scaling methods for LLMs, revealing limitations in multilingual performance.", "motivation": "To assess the effectiveness of test-time scaling methods for multilingual large language models (LLMs) in solving competition-level math problems across different languages.", "method": "The authors tested three test-time scaling methods: Outcome Reward Modeling (ORM), Process Reward Modeling (PRM), and Budget Forcing (BF) on two multilingual LLMs: Qwen2.5-1.5B Math and MR1-1.5B.", "result": "Using ORM with Qwen2.5-1.5B Math achieved a score of 35.8 on MCLM, while MR1-1.5B with BF scored 35.2. However, test-time scaling demonstrated limited improvement across diverse languages.", "conclusion": "Test-time scaling methods such as BF did not generalize effectively to multilingual tasks, despite significant improvements in English tasks. The study highlights the need for continued research in multilingual LLM capabilities.", "key_contributions": ["Introduction of MCLM, a multilingual math benchmark for assessing LLM performance.", "Evaluation of three novel test-time scaling methods for LLMs in multilingual contexts.", "Release of multilingual LLM models and comprehensive evaluation results."], "limitations": "Test-time scaling methods showed limited generalization to multilingual tasks, with minimal gains across languages compared to English.", "keywords": ["multilingual", "large language models", "test-time scaling", "math benchmark", "evaluation"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2502.19573", "pdf": "https://arxiv.org/pdf/2502.19573.pdf", "abs": "https://arxiv.org/abs/2502.19573", "title": "Do Large Language Models Know How Much They Know?", "authors": ["Gabriele Prato", "Jerry Huang", "Prasanna Parthasarathi", "Shagun Sodhani", "Sarath Chandar"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ublished as a long paper at the 2024 Conference on Empirical Methods\n  in Natural Language Processing (EMNLP). Official version of paper within\n  conference proceedings is available at\n  https://aclanthology.org/2024.emnlp-main.348/", "summary": "Large Language Models (LLMs) have emerged as highly capable systems and are\nincreasingly being integrated into various uses. However, the rapid pace of\ntheir deployment has outpaced a comprehensive understanding of their internal\nmechanisms and a delineation of their capabilities and limitations. A desired\nattribute of an intelligent system is its ability to recognize the scope of its\nown knowledge. To investigate whether LLMs embody this characteristic, we\ndevelop a benchmark designed to challenge these models to enumerate all\ninformation they possess on specific topics. This benchmark evaluates whether\nthe models recall excessive, insufficient, or the precise amount of\ninformation, thereby indicating their awareness of their own knowledge. Our\nfindings reveal that all tested LLMs, given sufficient scale, demonstrate an\nunderstanding of how much they know about specific topics. While different\narchitectures exhibit varying rates of this capability's emergence, the results\nsuggest that awareness of knowledge may be a generalizable attribute of LLMs.\nFurther research is needed to confirm this potential and fully elucidate the\nunderlying mechanisms.", "AI": {"tldr": "This paper investigates whether Large Language Models (LLMs) can recognize the scope of their own knowledge through a benchmark designed to evaluate their knowledge recall.", "motivation": "The rapid deployment of LLMs necessitates a better understanding of their internal mechanisms and knowledge recognition capabilities.", "method": "A benchmark was developed to challenge LLMs to enumerate all information they possess on specific topics, evaluating their knowledge recall accuracy.", "result": "All tested LLMs show an understanding of their knowledge scope, with varying rates of capability emergence across architectures.", "conclusion": "Awareness of knowledge may be a generalizable attribute of LLMs, requiring further research to confirm its mechanisms.", "key_contributions": ["Development of a benchmark for evaluating LLMs' awareness of knowledge", "Findings support that LLMs exhibit knowledge scope recognition", "Identification of varying capabilities across different LLM architectures"], "limitations": "Further research is necessary to confirm the underlying mechanisms of knowledge awareness in LLMs.", "keywords": ["Large Language Models", "knowledge awareness", "benchmark", "natural language processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.19693", "pdf": "https://arxiv.org/pdf/2503.19693.pdf", "abs": "https://arxiv.org/abs/2503.19693", "title": "AdaptiVocab: Enhancing LLM Efficiency in Focused Domains through Lightweight Vocabulary Adaptation", "authors": ["Itay Nakash", "Nitay Calderon", "Eyal Ben David", "Elad Hoffer", "Roi Reichart"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have shown impressive versatility as general\npurpose models. However, their broad applicability comes at a high-cost\ncomputational overhead, particularly in auto-regressive decoding where each\nstep requires a forward pass. In domain-specific settings, general-purpose\ncapabilities are unnecessary and can be exchanged for efficiency. In this work,\nwe take a novel perspective on domain adaptation, reducing latency and\ncomputational costs by adapting the vocabulary to focused domains of interest.\nWe introduce AdaptiVocab, an end-to-end approach for vocabulary adaptation,\ndesigned to enhance LLM efficiency in low-resource domains. AdaptiVocab can be\napplied to any tokenizer and architecture, modifying the vocabulary by\nreplacing tokens with domain-specific n-gram-based tokens, thereby reducing the\nnumber of tokens required for both input processing and output generation.\nAdaptiVocab initializes new n-token embeddings using an exponentially weighted\ncombination of existing embeddings and employs a lightweight fine-tuning phase\nthat can be efficiently performed on a single GPU. We evaluate two 7B LLMs\nacross three niche domains, assessing efficiency, generation quality, and\nend-task performance. Our results show that AdaptiVocab reduces token usage by\nover 25% without compromising performance", "AI": {"tldr": "AdaptiVocab is an end-to-end vocabulary adaptation approach for large language models that enhances efficiency in low-resource domains by reducing token usage without compromising performance.", "motivation": "To improve the efficiency of large language models by adapting their vocabulary to specific domains, reducing unnecessary computational overhead.", "method": "AdaptiVocab modifies existing tokenizers to replace general tokens with domain-specific n-gram-based tokens, which decreases the number of tokens needed for processing and generation. It initializes new n-token embeddings from existing embeddings and employs a lightweight fine-tuning process on a single GPU.", "result": "Evaluation of two 7B LLMs in three niche domains demonstrated a reduction in token usage by over 25%, while maintaining the quality of generated content and overall task performance.", "conclusion": "AdaptiVocab successfully enhances the efficiency of large language models for specific domains, showcasing a significant reduction in token usage and computational cost.", "key_contributions": ["Introduction of AdaptiVocab for domain-specific vocabulary adaptation", "Demonstrated more than 25% reduction in token usage", "Efficient fine-tuning methodology applicable on a single GPU"], "limitations": "Potential limitations include reliance on existing embedding quality and the need for domain-specific data for training.", "keywords": ["Large Language Models", "Vocabulary Adaptation", "Domain-Specific", "N-gram", "Efficiency"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2503.21760", "pdf": "https://arxiv.org/pdf/2503.21760.pdf", "abs": "https://arxiv.org/abs/2503.21760", "title": "MemInsight: Autonomous Memory Augmentation for LLM Agents", "authors": ["Rana Salama", "Jason Cai", "Michelle Yuan", "Anna Currey", "Monica Sunkara", "Yi Zhang", "Yassine Benajiba"], "categories": ["cs.CL"], "comment": null, "summary": "Large language model (LLM) agents have evolved to intelligently process\ninformation, make decisions, and interact with users or tools. A key capability\nis the integration of long-term memory capabilities, enabling these agents to\ndraw upon historical interactions and knowledge. However, the growing memory\nsize and need for semantic structuring pose significant challenges. In this\nwork, we propose an autonomous memory augmentation approach, MemInsight, to\nenhance semantic data representation and retrieval mechanisms. By leveraging\nautonomous augmentation to historical interactions, LLM agents are shown to\ndeliver more accurate and contextualized responses. We empirically validate the\nefficacy of our proposed approach in three task scenarios; conversational\nrecommendation, question answering and event summarization. On the LLM-REDIAL\ndataset, MemInsight boosts persuasiveness of recommendations by up to 14%.\nMoreover, it outperforms a RAG baseline by 34% in recall for LoCoMo retrieval.\nOur empirical results show the potential of MemInsight to enhance the\ncontextual performance of LLM agents across multiple tasks.", "AI": {"tldr": "The paper presents MemInsight, an autonomous memory augmentation approach for large language model agents that improves semantic data representation and retrieval, thereby enhancing their contextual performance in various tasks.", "motivation": "To address the challenges of integrating long-term memory capabilities in LLM agents and improve their interaction quality through improved semantic data management.", "method": "MemInsight leverages autonomous augmentation to historical interactions, focusing on enhancing data representation and retrieval mechanisms within LLM agents.", "result": "Empirical validation shows that MemInsight significantly improves the persuasiveness of recommendations by up to 14% and outperforms a RAG baseline by 34% in recall for LoCoMo retrieval across three task scenarios: conversational recommendation, question answering, and event summarization.", "conclusion": "MemInsight demonstrates the potential to improve contextual performance of LLM agents, suggesting its applicability in various interactive AI applications.", "key_contributions": ["Proposed MemInsight for memory augmentation in LLM agents", "Empirical validation across multiple task scenarios", "Demonstrated significant improvements in recommendation persuasiveness and recall metrics."], "limitations": "", "keywords": ["large language model", "memory augmentation", "semantic data representation", "LLM agents", "contextual performance"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2504.10284", "pdf": "https://arxiv.org/pdf/2504.10284.pdf", "abs": "https://arxiv.org/abs/2504.10284", "title": "Can LLMs Generate Tabular Summaries of Science Papers? Rethinking the Evaluation Protocol", "authors": ["Weiqi Wang", "Jiefu Ou", "Yangqiu Song", "Benjamin Van Durme", "Daniel Khashabi"], "categories": ["cs.CL"], "comment": null, "summary": "Literature review tables are essential for summarizing and comparing\ncollections of scientific papers. We explore the task of generating tables that\nbest fulfill a user's informational needs given a collection of scientific\npapers. Building on recent work (Newman et al., 2024), we extend prior\napproaches to address real-world complexities through a combination of\nLLM-based methods and human annotations. Our contributions focus on three key\nchallenges encountered in real-world use: (i) User prompts are often\nunder-specified; (ii) Retrieved candidate papers frequently contain irrelevant\ncontent; and (iii) Task evaluation should move beyond shallow text similarity\ntechniques and instead assess the utility of inferred tables for\ninformation-seeking tasks (e.g., comparing papers). To support reproducible\nevaluation, we introduce ARXIV2TABLE, a more realistic and challenging\nbenchmark for this task, along with a novel approach to improve literature\nreview table generation in real-world scenarios. Our extensive experiments on\nthis benchmark show that both open-weight and proprietary LLMs struggle with\nthe task, highlighting its difficulty and the need for further advancements.\nOur dataset and code are available at https://github.com/JHU-CLSP/arXiv2Table.", "AI": {"tldr": "This paper addresses the challenges in generating effective literature review tables from scientific papers using LLMs and human annotations.", "motivation": "The need for effective literature review tables that accurately summarize and compare scientific papers based on user needs.", "method": "Combining LLM-based methods and human annotations to generate literature review tables, while introducing a benchmark for evaluation.", "result": "Experiments reveal that LLMs, both open-weight and proprietary, face significant difficulties in generating effective literature review tables.", "conclusion": "The findings indicate a pressing need for further advancements in literature review table generation and evaluation methodologies.", "key_contributions": ["Introduction of ARXIV2TABLE benchmark for realistic evaluation", "Enhanced methods for literature review table generation", "Identification of specific challenges in the task of table generation"], "limitations": "The approach may still produce irrelevant content and requires careful task evaluation improvements.", "keywords": ["literature review", "LLM", "benchmark", "information retrieval", "table generation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2504.12312", "pdf": "https://arxiv.org/pdf/2504.12312.pdf", "abs": "https://arxiv.org/abs/2504.12312", "title": "Socrates or Smartypants: Testing Logic Reasoning Capabilities of Large Language Models with Logic Programming-based Test Oracles", "authors": ["Zihao Xu", "Junchen Ding", "Yiling Lou", "Kun Zhang", "Dong Gong", "Yuekang Li"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved significant progress in language\nunderstanding and reasoning. Evaluating and analyzing their logical reasoning\nabilities has therefore become essential. However, existing datasets and\nbenchmarks are often limited to overly simplistic, unnatural, or contextually\nconstrained examples. In response to the growing demand, we introduce\nSmartyPat-Bench, a challenging, naturally expressed, and systematically labeled\nbenchmark derived from real-world high-quality Reddit posts containing subtle\nlogical fallacies. Unlike existing datasets and benchmarks, it provides more\ndetailed annotations of logical fallacies and features more diverse data. To\nfurther scale up the study and address the limitations of manual data\ncollection and labeling - such as fallacy-type imbalance and labor-intensive\nannotation - we introduce SmartyPat, an automated framework powered by logic\nprogramming-based oracles. SmartyPat utilizes Prolog rules to systematically\ngenerate logically fallacious statements, which are then refined into fluent\nnatural-language sentences by LLMs, ensuring precise fallacy representation.\nExtensive evaluation demonstrates that SmartyPat produces fallacies comparable\nin subtlety and quality to human-generated content and significantly\noutperforms baseline methods. Finally, experiments reveal nuanced insights into\nLLM capabilities, highlighting that while excessive reasoning steps hinder\nfallacy detection accuracy, structured reasoning enhances fallacy\ncategorization performance.", "AI": {"tldr": "Introducing SmartyPat-Bench, a new benchmark for evaluating logical reasoning in LLMs using real-world data annotations.", "motivation": "To address the limitations of existing datasets that are overly simplistic and not contextually rich, there is a need for a more challenging benchmark that reflects real-world reasoning scenarios.", "method": "The paper presents SmartyPat-Bench, created from high-quality Reddit posts with subtle logical fallacies, and introduces SmartyPat, an automated framework using logic programming to generate and refine logically fallacious statements into natural language.", "result": "SmartyPat-Bench has more detailed annotations and diverse fallacy data compared to existing datasets. SmartyPat outperforms baseline methods in producing logically fallacious statements that match the quality of human-generated content.", "conclusion": "The framework provides valuable insights into the capabilities of LLMs in logical reasoning, showing that structured reasoning improves fallacy categorization, while too many reasoning steps can reduce accuracy.", "key_contributions": ["Introduction of SmartyPat-Bench, a novel benchmark for logical reasoning evaluation in LLMs.", "Development of SmartyPat, an automated framework for generating high-quality fallacies.", "Comprehensive evaluation highlighting the nuanced abilities of LLMs in detecting logical fallacies."], "limitations": "", "keywords": ["Large Language Models", "logical reasoning", "benchmark", "Reddit data", "automated framework"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.16248", "pdf": "https://arxiv.org/pdf/2507.16248.pdf", "abs": "https://arxiv.org/abs/2507.16248", "title": "FinResearchBench: A Logic Tree based Agent-as-a-Judge Evaluation Framework for Financial Research Agents", "authors": ["Rui Sun", "Zuo Bai", "Wentao Zhang", "Yuxiang Zhang", "Li Zhao", "Shan Sun", "Zhengwen Qiu"], "categories": ["cs.CL"], "comment": null, "summary": "Recently, AI agents are rapidly evolving in intelligence and widely used in\nprofessional research applications, such as STEM, software development,\nfinance, etc. Among these AI agents, deep research agent is a key category as\nit can perform long-horizon tasks and solve problems of greater complexity.\nHowever, there are few evaluation frameworks and benchmarks that systematically\nand automatically investigate the capabilities of these research agents.\nFurthermore, financial research problems have distinct complexity and subtlety.\nTo fill in the gap, we propose FinResearchBench, which is a logic tree based\nAgent-as-a-Judge and targets specifically for the financial research agents. It\nprovides a comprehensive and automatic assessment of the research agents across\n7 key types of tasks in the financial research domain. The contributions of\nthis work are two-folded: (1) the first and innovative Agent-as-a-Judge system\nthat extracts the logic tree of the research outcome and uses it as the\nintermediate information to present a comprehensive, reliable and robust\nevaluation; (2) finance oriented that it covers 70 typical financial research\nquestions, spreading across 7 frequently encountered types of tasks in the\ndomain.", "AI": {"tldr": "The paper introduces FinResearchBench, a novel evaluation framework for financial research AI agents using a logic tree based Agent-as-a-Judge system.", "motivation": "To address the lack of systematic evaluation frameworks and benchmarks for evaluating the capabilities of AI research agents in the complex domain of financial research.", "method": "FinResearchBench uses a logic tree structure to assess and evaluate AI research agents on a range of tasks typical in financial research, specifically designed for long-horizon tasks.", "result": "The framework provides a comprehensive assessment of research agents across 7 key task types in finance, addressing 70 typical financial research questions.", "conclusion": "FinResearchBench offers a reliable and systematic way to evaluate the effectiveness of financial research AI agents, paving the way for better understanding their capabilities.", "key_contributions": ["Innovative Agent-as-a-Judge system using logic trees for evaluation.", "Covers 70 typical financial research questions.", "Systematic evaluation across 7 types of financial research tasks."], "limitations": "", "keywords": ["AI agents", "financial research", "evaluation framework", "logic tree", "Agent-as-a-Judge"], "importance_score": 3, "read_time_minutes": 5}}
{"id": "2409.18203", "pdf": "https://arxiv.org/pdf/2409.18203.pdf", "abs": "https://arxiv.org/abs/2409.18203", "title": "Policy Maps: Tools for Guiding the Unbounded Space of LLM Behaviors", "authors": ["Michelle S. Lam", "Fred Hohman", "Dominik Moritz", "Jeffrey P. Bigham", "Kenneth Holstein", "Mary Beth Kery"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.LG"], "comment": "UIST 2025", "summary": "AI policy sets boundaries on acceptable behavior for AI models, but this is\nchallenging in the context of large language models (LLMs): how do you ensure\ncoverage over a vast behavior space? We introduce policy maps, an approach to\nAI policy design inspired by the practice of physical mapmaking. Instead of\naiming for full coverage, policy maps aid effective navigation through\nintentional design choices about which aspects to capture and which to abstract\naway. With Policy Projector, an interactive tool for designing LLM policy maps,\nan AI practitioner can survey the landscape of model input-output pairs, define\ncustom regions (e.g., \"violence\"), and navigate these regions with if-then\npolicy rules that can act on LLM outputs (e.g., if output contains \"violence\"\nand \"graphic details,\" then rewrite without \"graphic details\"). Policy\nProjector supports interactive policy authoring using LLM classification and\nsteering and a map visualization reflecting the AI practitioner's work. In an\nevaluation with 12 AI safety experts, our system helps policy designers craft\npolicies around problematic model behaviors such as incorrect gender\nassumptions and handling of immediate physical safety threats.", "AI": {"tldr": "This paper introduces policy maps for AI policy design in LLMs, focusing on intentional design to navigate model behaviors.", "motivation": "To address the challenge of setting appropriate boundaries for AI behavior in large language models by designing effective policies.", "method": "The authors propose policy maps, allowing practitioners to create custom regions for model behavior (e.g., 'violence') and implement if-then policy rules using the Policy Projector tool.", "result": "Evaluation with AI safety experts showed that Policy Projector aids in crafting policies to mitigate problematic model behaviors like gender biases and safety threats.", "conclusion": "The tool enhances policy design for AI models by providing an interactive and structured way to navigate complex behaviors.", "key_contributions": ["Introduction of policy maps for AI policy design", "Development of the Policy Projector tool for interactive policy authoring", "Evaluation demonstrating effectiveness with AI safety experts"], "limitations": "", "keywords": ["AI policy", "large language models", "policy maps", "human-computer interaction", "AI safety"], "importance_score": 8, "read_time_minutes": 12}}
