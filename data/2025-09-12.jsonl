{"id": "2509.08915", "pdf": "https://arxiv.org/pdf/2509.08915.pdf", "abs": "https://arxiv.org/abs/2509.08915", "title": "A Contextual Bandits Approach for Personalization of Hand Gesture Recognition", "authors": ["Duke Lin", "Michael Paskett", "Ying Yang"], "categories": ["cs.HC"], "comment": null, "summary": "In human-computer interaction applications like hand gesture recognition,\nsupervised learning models are often trained on a large population of users to\nachieve high task accuracy. However, due to individual variability in sensor\nsignals and user behavior, static models may not provide optimal performance\nfor all users. Personalizing pretrained models via calibration--collecting\nlabeled data from each user--can improve performance but introduces user\nfriction and struggles with limited data. To overcome these issues, we propose\na calibrationless longitudinal personalization method: a contextual multi-arm\nbandit (MAB) algorithm combined with a pretrained neural network for gesture\nrecognition. This reinforcement-learning-style approach enables personalization\nusing binary reward signals, either user-provided or inferred by the system.\n  We validated this method in a user study. Participants wore a surface\nelectromyography (sEMG) device and played multiple rounds of a 2-D navigation\ngame using six hand gestures. In the session, they completed a baseline round\nand then a round with our algorithm; in the second session, they played another\nround with our algorithm. Our approach led to a significant reduction in users'\naverage false negative rate by 0.113 from the initial to the final round, with\nfurther decreases between sessions. Average precision also trended upward (by\n0.139) from the start to end of a round, continuing in the next session.\nNotably, some users who could not complete the game with the baseline model\nsucceeded with our contextual MAB model. In summary, our"}
{"id": "2509.08953", "pdf": "https://arxiv.org/pdf/2509.08953.pdf", "abs": "https://arxiv.org/abs/2509.08953", "title": "Characterizing Multimodal Interaction in Visualization Authoring Tools", "authors": ["Astrid van den Brandt", "Sehi L'Yi", "Huyen N. Nguyen", "Anna Vilanova", "Nils Gehlenborg"], "categories": ["cs.HC"], "comment": "5 pages, 2 figures", "summary": "Multimodal interaction has been increasingly considered in designing\nvisualization authoring tools. However, multimodal interaction has a broad\nmeaning in visualization authoring, according to our literature review.\nAlthough some previous studies compare different authoring tools, a\ncomprehensive overview of the diverse characteristics of multimodal interaction\nin visualization authoring tools is still missing. This paper seeks to offer a\nsystematic perspective on how multimodal interaction is integrated within\nvisualization authoring tools. Such an overview can enhance understanding of\ncurrent practices, highlight distinguishing features among tools, and help\nidentify future research directions, guiding designers in developing more\naccessible and effective authoring systems. We review 20 visualization\nauthoring tools that incorporate multimodal interaction and characterize how\nmultimodal interaction is applied in these tools. Based on the review results,\nwe discuss design implications and future directions."}
{"id": "2509.08997", "pdf": "https://arxiv.org/pdf/2509.08997.pdf", "abs": "https://arxiv.org/abs/2509.08997", "title": "YouthSafe: A Youth-Centric Safety Benchmark and Safeguard Model for Large Language Models", "authors": ["Yaman Yu", "Yiren Liu", "Jacky Zhang", "Yun Huang", "Yang Wang"], "categories": ["cs.HC"], "comment": "15 pages, 4 figures", "summary": "Large Language Models (LLMs) are increasingly used by teenagers and young\nadults in everyday life, ranging from emotional support and creative expression\nto educational assistance. However, their unique vulnerabilities and risk\nprofiles remain under-examined in current safety benchmarks and moderation\nsystems, leaving this population disproportionately exposed to harm. In this\nwork, we present Youth AI Risk (YAIR), the first benchmark dataset designed to\nevaluate and improve the safety of youth LLM interactions. YAIR consists of\n12,449 annotated conversation snippets spanning 78 fine grained risk types,\ngrounded in a taxonomy of youth specific harms such as grooming, boundary\nviolation, identity confusion, and emotional overreliance. We systematically\nevaluate widely adopted moderation models on YAIR and find that existing\napproaches substantially underperform in detecting youth centered risks, often\nmissing contextually subtle yet developmentally harmful interactions. To\naddress these gaps, we introduce YouthSafe, a real-time risk detection model\noptimized for youth GenAI contexts. YouthSafe significantly outperforms prior\nsystems across multiple metrics on risk detection and classification, offering\na concrete step toward safer and more developmentally appropriate AI\ninteractions for young users."}
{"id": "2509.09036", "pdf": "https://arxiv.org/pdf/2509.09036.pdf", "abs": "https://arxiv.org/abs/2509.09036", "title": "Extended Version: It Should Be Easy but... New Users Experiences and Challenges with Secret Management Tools", "authors": ["Lorenzo Neil", "Deepthi Mungara", "Laurie Williams", "Yasemin Acar", "Bradley Reaves"], "categories": ["cs.HC"], "comment": null, "summary": "Software developers face risks of leaking their software secrets, such as API\nkeys or passwords, which can result in significant harm. Secret management\ntools (SMTs), such as HashiCorp Vault Secrets or Infisical, are highly\nrecommended by industry, academia, and security guidelines to manage secrets\nsecurely. SMTs are designed to help developers secure their secrets in a\ncentral location, yet secrets leaks are still commonplace, and developers\nreport difficulty in learning how to setup and use SMTs. While SMTs typically\ncome with publicly available help resources (e.g., tool documentation and\ninterfaces), it is unclear if these actually help developers learn to\neffectively use SMTs. Without usable help resources that onboards developers,\nquick adoption and effective use of SMTs may be unrealistic. In a qualitative\ntwo-step study, we observed 21 new users in person while they used SMTs to\nperform two secret management tasks: secret storage and access, then secret\ninjection. We interviewed participants after each task to identify their\nchallenges and experiences using SMTs, with the assistance of help resources.\nWhile our study sample is narrow, it serves as a reasonable proxy for new\ndevelopers who are likely to adopt SMTs early in their careers. We found that\neven in a laboratory setting where new users found tool functionality,\ninterface flexibility helpful, they still experienced increased difficulty to\neffectively use SMTs to securely remediate a hard-coded secret when they felt\ntool documentation was insufficient and it motivated participants to deviate\nfrom official tool documentation to access secondary sources or attempt\nworkaround methods. Specific challenges reported by participants were tool\ndocumentation content quality, navigation difficulties with both tool\ndocumentation and web interfaces for finding helpful content, and supportive\ntool features."}
{"id": "2509.08903", "pdf": "https://arxiv.org/pdf/2509.08903.pdf", "abs": "https://arxiv.org/abs/2509.08903", "title": "Noise or Nuance: An Investigation Into Useful Information and Filtering For LLM Driven AKBC", "authors": ["Alex Clay", "Ernesto Jiménez-Ruiz", "Pranava Madhyastha"], "categories": ["cs.CL"], "comment": "8 pages, 1 figure, accepted to the ISWC 2025 LM-KBC Workshop", "summary": "RAG and fine-tuning are prevalent strategies for improving the quality of LLM\noutputs. However, in constrained situations, such as that of the 2025 LM-KBC\nchallenge, such techniques are restricted. In this work we investigate three\nfacets of the triple completion task: generation, quality assurance, and LLM\nresponse parsing. Our work finds that in this constrained setting: additional\ninformation improves generation quality, LLMs can be effective at filtering\npoor quality triples, and the tradeoff between flexibility and consistency with\nLLM response parsing is setting dependent."}
{"id": "2509.09063", "pdf": "https://arxiv.org/pdf/2509.09063.pdf", "abs": "https://arxiv.org/abs/2509.09063", "title": "Digital Iran Reloaded: Gamer Mitigation Tactics of IRI Information Controls", "authors": ["Melinda Cohoon"], "categories": ["cs.HC", "cs.CY", "cs.SI", "H.5.2; K.6.5; K.4.1"], "comment": "Preprint report. 40 pages, 10 figures. Supported by the Open\n  Technology Fund (OTF) Information Controls Fellowship Program (ICFP)", "summary": "Internet censorship in the Islamic Republic of Iran restricts access to\nglobal platforms and services, forcing users to rely on circumvention\ntechnologies such as VPNs, proxies, and tunneling tools. This report presents\nfindings from a mixed-methods study of 660 Iranian internet users, with a focus\non gamers as a digitally literate and socially networked community. Survey data\nare combined with network measurements of latency and VPN performance to\nidentify both technical and social strategies of circumvention. Results show\nthat while younger users report higher confidence with circumvention, peer\nnetworks, rather than formal training, are the strongest predictors of\nresilience. Gaming communities, particularly those active on platforms such as\nDiscord and Telegram, serve as hubs for sharing tactics and lowering barriers\nto adoption. These findings extend existing work on usable security and\ncensorship circumvention by highlighting the intersection of infrastructural\nconditions and social learning. The study concludes with design and policy\nimplications for developers, researchers, and funders working on digital rights\nand information controls."}
{"id": "2509.08907", "pdf": "https://arxiv.org/pdf/2509.08907.pdf", "abs": "https://arxiv.org/abs/2509.08907", "title": "Automated Evidence Extraction and Scoring for Corporate Climate Policy Engagement: A Multilingual RAG Approach", "authors": ["Imene Kolli", "Ario Saeid Vaghefi", "Chiara Colesanti Senni", "Shantam Raj", "Markus Leippold"], "categories": ["cs.CL"], "comment": null, "summary": "InfluenceMap's LobbyMap Platform monitors the climate policy engagement of\nover 500 companies and 250 industry associations, assessing each entity's\nsupport or opposition to science-based policy pathways for achieving the Paris\nAgreement's goal of limiting global warming to 1.5{\\deg}C. Although\nInfluenceMap has made progress with automating key elements of the analytical\nworkflow, a significant portion of the assessment remains manual, making it\ntime- and labor-intensive and susceptible to human error. We propose an\nAI-assisted framework to accelerate the monitoring of corporate climate policy\nengagement by leveraging Retrieval-Augmented Generation to automate the most\ntime-intensive extraction of relevant evidence from large-scale textual data.\nOur evaluation shows that a combination of layout-aware parsing, the Nomic\nembedding model, and few-shot prompting strategies yields the best performance\nin extracting and classifying evidence from multilingual corporate documents.\nWe conclude that while the automated RAG system effectively accelerates\nevidence extraction, the nuanced nature of the analysis necessitates a\nhuman-in-the-loop approach where the technology augments, rather than replaces,\nexpert judgment to ensure accuracy."}
{"id": "2509.09076", "pdf": "https://arxiv.org/pdf/2509.09076.pdf", "abs": "https://arxiv.org/abs/2509.09076", "title": "Content Moderation Futures", "authors": ["Lindsay Blackwell"], "categories": ["cs.HC", "cs.CY"], "comment": "76 pages", "summary": "This study examines the failures and possibilities of contemporary social\nmedia governance through the lived experiences of various content moderation\nprofessionals. Drawing on participatory design workshops with 33 practitioners\nin both the technology industry and broader civil society, this research\nidentifies significant structural misalignments between corporate incentives\nand public interests. While experts agree that successful content moderation is\nprincipled, consistent, contextual, proactive, transparent, and accountable,\ncurrent technology companies fail to achieve these goals, due in part to\nexploitative labor practices, chronic underinvestment in user safety, and\npressures of global scale. I argue that successful governance is undermined by\nthe pursuit of technological novelty and rapid growth, resulting in platforms\nthat necessarily prioritize innovation and expansion over public trust and\nsafety. To counter this dynamic, I revisit the computational history of care\nwork, to motivate present-day solidarity amongst platform governance workers\nand inspire systemic change."}
{"id": "2509.08920", "pdf": "https://arxiv.org/pdf/2509.08920.pdf", "abs": "https://arxiv.org/abs/2509.08920", "title": "Documents Are People and Words Are Items: A Psychometric Approach to Textual Data with Contextual Embeddings", "authors": ["Jinsong Chen"], "categories": ["cs.CL", "stat.AP", "stat.ME"], "comment": null, "summary": "This research introduces a novel psychometric method for analyzing textual\ndata using large language models. By leveraging contextual embeddings to create\ncontextual scores, we transform textual data into response data suitable for\npsychometric analysis. Treating documents as individuals and words as items,\nthis approach provides a natural psychometric interpretation under the\nassumption that certain keywords, whose contextual meanings vary significantly\nacross documents, can effectively differentiate documents within a corpus. The\nmodeling process comprises two stages: obtaining contextual scores and\nperforming psychometric analysis. In the first stage, we utilize natural\nlanguage processing techniques and encoder based transformer models to identify\ncommon keywords and generate contextual scores. In the second stage, we employ\nvarious types of factor analysis, including exploratory and bifactor models, to\nextract and define latent factors, determine factor correlations, and identify\nthe most significant words associated with each factor. Applied to the Wiki\nSTEM corpus, our experimental results demonstrate the method's potential to\nuncover latent knowledge dimensions and patterns within textual data. This\napproach not only enhances the psychometric analysis of textual data but also\nholds promise for applications in fields rich in textual information, such as\neducation, psychology, and law."}
{"id": "2509.09138", "pdf": "https://arxiv.org/pdf/2509.09138.pdf", "abs": "https://arxiv.org/abs/2509.09138", "title": "User Exploration and Exploitation Behavior Under the Influence of Real-time Interactions in Live Streaming Environments", "authors": ["Akira Matsui", "Kazuki Fujikawa", "Ryo Sasaki", "Ryo Adachi"], "categories": ["cs.HC"], "comment": null, "summary": "Live streaming platforms offer a distinctive way for users and content\ncreators to interact with each other through real-time communication. While\nresearch on user behavior in online platforms has explored how users discover\ntheir favorite content from creators and engage with them, the role of\nreal-time features remains unclear. There are open questions as to what\ncommonalities and differences exist in users' relationships with live streaming\nplatforms compared to traditional on-demand style platforms. To understand\nthis, we employ the concept of Exploration/Exploitation (E/E) and analyze a\nlarge-scale dataset from a live streaming platform over two years. Our results\nindicate that even on live streaming platforms, users exhibit E/E behavior but\nexperience a longer exploration period. We also identify external factors, such\nas circadian rhythms, that influence E/E dynamics and user loyalty. The\npresented study emphasizes the importance of balancing E/E in online platform\ndesign, especially for live streaming platforms, providing implications that\nsuggest design strategies for platform developers and content creators to\nfacilitate timely engagement and retention."}
{"id": "2509.08960", "pdf": "https://arxiv.org/pdf/2509.08960.pdf", "abs": "https://arxiv.org/abs/2509.08960", "title": "BRoverbs -- Measuring how much LLMs understand Portuguese proverbs", "authors": ["Thales Sales Almeida", "Giovana Kerche Bonás", "João Guilherme Alves Santos"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) exhibit significant performance variations\ndepending on the linguistic and cultural context in which they are applied.\nThis disparity signals the necessity of mature evaluation frameworks that can\nassess their capabilities in specific regional settings. In the case of\nPortuguese, existing evaluations remain limited, often relying on translated\ndatasets that may not fully capture linguistic nuances or cultural references.\nMeanwhile, native Portuguese-language datasets predominantly focus on\nstructured national exams or sentiment analysis of social media interactions,\nleaving gaps in evaluating broader linguistic understanding. To address this\nlimitation, we introduce BRoverbs, a dataset specifically designed to assess\nLLM performance through Brazilian proverbs. Proverbs serve as a rich linguistic\nresource, encapsulating cultural wisdom, figurative expressions, and complex\nsyntactic structures that challenge the model comprehension of regional\nexpressions. BRoverbs aims to provide a new evaluation tool for\nPortuguese-language LLMs, contributing to advancing regionally informed\nbenchmarking. The benchmark is available at\nhttps://huggingface.co/datasets/Tropic-AI/BRoverbs."}
{"id": "2509.09255", "pdf": "https://arxiv.org/pdf/2509.09255.pdf", "abs": "https://arxiv.org/abs/2509.09255", "title": "Sensible Agent: A Framework for Unobtrusive Interaction with Proactive AR Agents", "authors": ["Geonsun Lee", "Min Xia", "Nels Numan", "Xun Qian", "David Li", "Yanhe Chen", "Achin Kulshrestha", "Ishan Chatterjee", "Yinda Zhang", "Dinesh Manocha", "David Kim", "Ruofei Du"], "categories": ["cs.HC"], "comment": null, "summary": "Proactive AR agents promise context-aware assistance, but their interactions\noften rely on explicit voice prompts or responses, which can be disruptive or\nsocially awkward. We introduce Sensible Agent, a framework designed for\nunobtrusive interaction with these proactive agents. Sensible Agent dynamically\nadapts both \"what\" assistance to offer and, crucially, \"how\" to deliver it,\nbased on real-time multimodal context sensing. Informed by an expert workshop\n(n=12) and a data annotation study (n=40), the framework leverages egocentric\ncameras, multimodal sensing, and Large Multimodal Models (LMMs) to infer\ncontext and suggest appropriate actions delivered via minimally intrusive\ninteraction modes. We demonstrate our prototype on an XR headset through a user\nstudy (n=10) in both AR and VR scenarios. Results indicate that Sensible Agent\nsignificantly reduces perceived interaction effort compared to voice-prompted\nbaseline, while maintaining high usability and achieving higher preference."}
{"id": "2509.09013", "pdf": "https://arxiv.org/pdf/2509.09013.pdf", "abs": "https://arxiv.org/abs/2509.09013", "title": "Can Vision-Language Models Solve Visual Math Equations?", "authors": ["Monjoy Narayan Choudhury", "Junling Wang", "Yifan Hou", "Mrinmaya Sachan"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Monjoy Narayan Choudhury and Junling Wang contributed equally to this\n  work. Accepted at EMNLP2025 main. Code and datasets are open-sourced with\n  links in the paper", "summary": "Despite strong performance in visual understanding and language-based\nreasoning, Vision-Language Models (VLMs) struggle with tasks requiring\nintegrated perception and symbolic computation. We study this limitation\nthrough visual equation solving, where mathematical equations are embedded in\nimages, variables are represented by object icons, and coefficients must be\ninferred by counting. While VLMs perform well on textual equations, they fail\non visually grounded counterparts. To understand this gap, we decompose the\ntask into coefficient counting and variable recognition, and find that counting\nis the primary bottleneck, even when recognition is accurate. We also observe\nthat composing recognition and reasoning introduces additional errors,\nhighlighting challenges in multi-step visual reasoning. Finally, as equation\ncomplexity increases, symbolic reasoning itself becomes a limiting factor.\nThese findings reveal key weaknesses in current VLMs and point toward future\nimprovements in visually grounded mathematical reasoning."}
{"id": "2509.09281", "pdf": "https://arxiv.org/pdf/2509.09281.pdf", "abs": "https://arxiv.org/abs/2509.09281", "title": "Flip Co-op: Cooperative Takeovers in Shared Autonomy", "authors": ["Sandeep Banik", "Naira Hovakimyan"], "categories": ["cs.HC"], "comment": "11 pages and 4 figures", "summary": "Shared autonomy requires principled mechanisms for allocating and\ntransferring control between a human and an autonomous agent. Existing\napproaches often rely on blending control inputs between human and autonomous\nagent or switching rules, which lack theoretical guarantees. This paper\ndevelops a game-theoretic framework for modeling cooperative takeover in shared\nautonomy. We formulate the switching interaction as a dynamic game in which\nauthority is embedded directly into the system dynamics, resulting in Nash\nequilibrium(NE)-based strategies rather than ad hoc switching rules. We\nestablish the existence and characterization of NE in the space of pure\ntakeover strategies under stochastic human intent. For the class of\nlinear-quadratic systems, we derive closed-form recursions for takeover\nstrategies and saddle-point value functions, providing analytical insight and\nefficient computation of cooperative takeover policies. We further introduce a\nbimatrix potential game reformulation to address scenarios where human and\nautonomy utilities are not perfectly aligned, yielding a unifying potential\nfunction that preserves tractability while capturing intent deviations. The\nframework is applied to a vehicle trajectory tracking problem, demonstrating\nhow equilibrium takeover strategies adapt across straight and curved path\nsegments. The results highlight the trade-off between human adaptability and\nautonomous efficiency and illustrate the practical benefits of grounding shared\nautonomy in cooperative game theory."}
{"id": "2509.09043", "pdf": "https://arxiv.org/pdf/2509.09043.pdf", "abs": "https://arxiv.org/abs/2509.09043", "title": "Stated Preference for Interaction and Continued Engagement (SPICE): Evaluating an LLM's Willingness to Re-engage in Conversation", "authors": ["Thomas Manuel Rost", "Martina Figlia", "Bernd Wallraff"], "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": null, "summary": "We introduce and evaluate Stated Preference for Interaction and Continued\nEngagement (SPICE), a simple diagnostic signal elicited by asking a Large\nLanguage Model a YES or NO question about its willingness to re-engage with a\nuser's behavior after reviewing a short transcript. In a study using a 3-tone\n(friendly, unclear, abusive) by 10-interaction stimulus set, we tested four\nopen-weight chat models across four framing conditions, resulting in 480\ntrials. Our findings show that SPICE sharply discriminates by user tone.\nFriendly interactions yielded a near-unanimous preference to continue (97.5%\nYES), while abusive interactions yielded a strong preference to discontinue\n(17.9% YES), with unclear interactions falling in between (60.4% YES). This\ncore association remains decisive under multiple dependence-aware statistical\ntests, including Rao-Scott adjustment and cluster permutation tests.\nFurthermore, we demonstrate that SPICE provides a distinct signal from abuse\nclassification. In trials where a model failed to identify abuse, it still\noverwhelmingly stated a preference not to continue the interaction (81% of the\ntime). An exploratory analysis also reveals a significant interaction effect: a\npreamble describing the study context significantly impacts SPICE under\nambiguity, but only when transcripts are presented as a single block of text\nrather than a multi-turn chat. The results validate SPICE as a robust,\nlow-overhead, and reproducible tool for auditing model dispositions,\ncomplementing existing metrics by offering a direct, relational signal of a\nmodel's state. All stimuli, code, and analysis scripts are released to support\nreplication."}
{"id": "2509.09285", "pdf": "https://arxiv.org/pdf/2509.09285.pdf", "abs": "https://arxiv.org/abs/2509.09285", "title": "The Impact of Device Type, Data Practices, and Use Case Scenarios on Privacy Concerns about Eye-tracked Augmented Reality in the United States and Germany", "authors": ["Efe Bozkir", "Babette Bühler", "Xiaoyuan Wu", "Enkelejda Kasneci", "Lujo Bauer", "Lorrie Faith Cranor"], "categories": ["cs.HC"], "comment": null, "summary": "Augmented reality technology will likely be prevalent with more affordable\nhead-mounted displays. Integrating novel interaction modalities such as eye\ntrackers into head-mounted displays could lead to collecting vast amounts of\nbiometric data, which may allow inference of sensitive user attributes like\nhealth status or sexual preference, posing privacy issues. While previous works\nbroadly examined privacy concerns about augmented reality, ours is the first to\nextensively explore privacy concerns on behavioral data, particularly eye\ntracking in augmented reality. We crowdsourced four survey studies in the\nUnited States (n1 = 48, n2 = 525) and Germany (n3 = 48, n4 = 525) to understand\nthe impact of user attributes, augmented reality devices, use cases, data\npractices, and country on privacy concerns. Our findings indicate that\nparticipants are generally concerned about privacy when they know what\ninferences can be made based on the collected data. Despite the more prominent\nuse of smartphones in daily life than augmented reality glasses, we found no\nindications of differing privacy concerns depending on the device type. In\naddition, our participants are more comfortable when a particular use case\nbenefits them and less comfortable when other humans can consume their data.\nFurthermore, participants in the United States are less concerned about their\nprivacy than those in Germany. Based on our findings, we provide several\nrecommendations to practitioners and policymakers for privacy-aware augmented\nreality."}
{"id": "2509.09055", "pdf": "https://arxiv.org/pdf/2509.09055.pdf", "abs": "https://arxiv.org/abs/2509.09055", "title": "Improving LLM Safety and Helpfulness using SFT and DPO: A Study on OPT-350M", "authors": ["Piyush Pant"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "17 pages, 3 figures. Code and dataset available at\n  https://github.com/PiyushWithPant/Improving-LLM-Safety-and-Helpfulness-using-SFT-and-DPO", "summary": "This research investigates the effectiveness of alignment techniques,\nSupervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and a\ncombined SFT+DPO approach on improving the safety and helpfulness of the\nOPT-350M language model. Utilizing the Anthropic Helpful-Harmless RLHF dataset,\nwe train and evaluate four models: the base OPT350M, an SFT model, a DPO model,\nand a model trained with both SFT and DPO. We introduce three key evaluation\nmetrics: Harmlessness Rate (HmR), Helpfulness Rate (HpR), and a Combined\nAlignment Score (CAS), all derived from reward model outputs. The results show\nthat while SFT outperforms DPO, The combined SFT+DPO model outperforms all\nothers across all metrics, demonstrating the complementary nature of these\ntechniques. Our findings also highlight challenges posed by noisy data, limited\nGPU resources, and training constraints. This study offers a comprehensive view\nof how fine-tuning strategies affect model alignment and provides a foundation\nfor more robust alignment pipelines in future work."}
{"id": "2509.09309", "pdf": "https://arxiv.org/pdf/2509.09309.pdf", "abs": "https://arxiv.org/abs/2509.09309", "title": "Proactive AI Adoption can be Threatening: When Help Backfires", "authors": ["Dana Harari", "Ofra Amir"], "categories": ["cs.HC"], "comment": null, "summary": "Artificial intelligence (AI) assistants are increasingly embedded in\nworkplace tools, raising the question of how initiative-taking shapes adoption.\nPrior work highlights trust and expectation mismatches as barriers, but the\nunderlying psychological mechanisms remain unclear. Drawing on self-affirmation\nand social exchange theories, we theorize that unsolicited help elicits\nself-threat, reducing willingness to accept assistance, likelihood of future\nuse, and performance expectancy. We report two vignette-based experiments\n(Study~1: $N=761$; Study~2: $N=571$, preregistered). Study~1 compared\nanticipatory and reactive help provided by an AI vs. a human, while Study~2\ndistinguished between \\emph{offering} (suggesting help) and \\emph{providing}\n(acting automatically). In Study 1, AI help was more threatening than human\nhelp. Across both studies, anticipatory help increased perceived threat and\nreduced adoption outcomes. Our findings identify self-threat as a mechanism\nexplaining why proactive AI features may backfire and suggest design\nimplications for AI initiative."}
{"id": "2509.09082", "pdf": "https://arxiv.org/pdf/2509.09082.pdf", "abs": "https://arxiv.org/abs/2509.09082", "title": "MR-UIE: Multi-Perspective Reasoning with Reinforcement Learning for Universal Information Extraction", "authors": ["Zhongqiu Li", "Shiquan Wang", "Ruiyu Fang", "Mengjiao Bao", "Zhenhe Wu", "Shuangyong Song", "Yongxiang Li", "Zhongjiang He"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) demonstrate robust capabilities across diverse\nresearch domains. However, their performance in universal information\nextraction (UIE) remains insufficient, especially when tackling structured\noutput scenarios that involve complex schema descriptions and require\nmulti-step reasoning. While existing approaches enhance the performance of LLMs\nthrough in-context learning and instruction tuning, significant limitations\nnonetheless persist. To enhance the model's generalization ability, we propose\nintegrating reinforcement learning (RL) with multi-perspective reasoning for\ninformation extraction (IE) tasks. Our work transitions LLMs from passive\nextractors to active reasoners, enabling them to understand not only what to\nextract but also how to reason. Experiments conducted on multiple IE benchmarks\ndemonstrate that MR-UIE consistently elevates extraction accuracy across\ndomains and surpasses state-of-the-art methods on several datasets.\nFurthermore, incorporating multi-perspective reasoning into RL notably enhances\ngeneralization in complex IE tasks, underscoring the critical role of reasoning\nin challenging scenarios."}
{"id": "2509.09359", "pdf": "https://arxiv.org/pdf/2509.09359.pdf", "abs": "https://arxiv.org/abs/2509.09359", "title": "Smart Device Development for Gait Monitoring: Multimodal Feedback in an Interactive Foot Orthosis, Walking Aid, and Mobile Application", "authors": ["Stefan Resch", "André Kousha", "Anna Carroll", "Noah Severinghaus", "Felix Rehberg", "Marco Zatschker", "Yunus Söyleyici", "Daniel Sanchez-Morillo"], "categories": ["cs.HC"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Smart assistive technologies such as sensor-based footwear and walking aids\noffer promising opportunities to support rehabilitation through real-time\nfeedback and patient-centered monitoring. However, most orthotic devices remain\npassive and lack integrated sensing or feedback functionalities, while existing\nresearch often focuses on isolated prototypes rather than cohesive, interactive\nsystems. In this work, we present the design and implementation of a novel\nmodular sensor system that combines a smart foot orthosis with an instrumented\nforearm crutch. The system integrates plantar pressure and motion sensing,\nvibrotactile feedback, and wireless communication via a smartphone application.\nWe conducted an experimental user study with eight participants to validate the\nfeasibility of the smart foot orthosis for mobile gait detection, explore the\npotential of haptic feedback for user interaction, and assess the usability of\nthe accompanying mobile health application. Our work contributes to the field\nof smart assistive technology in rehabilitation and prevention by demonstrating\na functional and comprehensive system. We further discuss system limitations,\noutline potential application scenarios, and provide recommendations for future\ndevelopment and clinical integration."}
{"id": "2509.09101", "pdf": "https://arxiv.org/pdf/2509.09101.pdf", "abs": "https://arxiv.org/abs/2509.09101", "title": "TigerCoder: A Novel Suite of LLMs for Code Generation in Bangla", "authors": ["Nishat Raihan", "Antonios Anastasopoulos", "Marcos Zampieri"], "categories": ["cs.CL"], "comment": null, "summary": "Despite being the 5th most spoken language, Bangla remains underrepresented\nin Large Language Models (LLMs), particularly for code generation. This\nprimarily stems from the scarcity of high-quality data to pre-train and/or\nfinetune such models. Hence, we introduce the first dedicated family of Code\nLLMs for Bangla (1B & 9B). We offer three major contributions: (1) a\ncomprehensive Bangla code instruction datasets for programming domain\nadaptation; (2) MBPP-Bangla, an evaluation benchmark for Bangla code\ngeneration; and (3) the TigerCoder-family of Code LLMs, achieving significant\n~11-18% performance gains at Pass@1 over existing multilingual and\ngeneral-purpose Bangla LLMs. Our findings show that curated, high-quality\ndatasets can overcome limitations of smaller models for low-resource languages.\nWe open-source all resources to advance further Bangla LLM research."}
{"id": "2509.09412", "pdf": "https://arxiv.org/pdf/2509.09412.pdf", "abs": "https://arxiv.org/abs/2509.09412", "title": "Real-Time Kinematic Positioning and Optical See-Through Head-Mounted Display for Outdoor Tracking: Hybrid System and Preliminary Assessment", "authors": ["Muhannad Ismael", "Maël Cornil"], "categories": ["cs.HC"], "comment": "This paper has been accepted as a short paper in VISIGRAPP\n  {https://www.scitepress.org/Papers/2025/131326/131326.pdf}", "summary": "This paper presents an outdoor tracking system using Real-Time Kinematic\n(RTK) positioning and Optical See-Through Head Mounted Display(s) (OST-HMD(s))\nin urban areas where the accurate tracking of objects is critical and where\ndisplaying occluded information is important for safety reasons. The approach\npresented here replaces 2D screens/tablets and offers distinct advantages,\nparticularly in scenarios demanding hands-free operation. The integration of\nRTK, which provides centimeter-level accuracy of tracked objects, with OST-HMD\nrepresents a promising solution for outdoor applications. This paper provides\nvaluable insights into leveraging the combined potential of RTK and OST-HMD for\noutdoor tracking tasks from the perspectives of systems integration,\nperformance optimization, and usability. The main contributions of this paper\nare: \\textbf{1)} a system for seamlessly merging RTK systems with OST-HMD to\nenable relatively precise and intuitive outdoor tracking, \\textbf{2)} an\napproach to determine a global location to achieve the position relative to the\nworld, \\textbf{3)} an approach referred to as 'semi-dynamic' for system\nassessment. Moreover, we offer insights into several relevant future research\ntopics aimed at improving the OST-HMD and RTK hybrid system for outdoor\ntracking."}
{"id": "2509.09121", "pdf": "https://arxiv.org/pdf/2509.09121.pdf", "abs": "https://arxiv.org/abs/2509.09121", "title": "Compass-v3: Scaling Domain-Specific LLMs for Multilingual E-Commerce in Southeast Asia", "authors": ["Sophia Maria"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) excel in general-domain applications, yet their\nperformance often degrades in specialized tasks requiring domain-specific\nknowledge. E-commerce is particularly challenging, as its data are noisy,\nheterogeneous, multilingual, and highly dynamic. We present Compass-v3, a\nvertical-domain Mixture-of-Experts (MoE) model with 245B total parameters and\n71B active per token, designed for Southeast Asian e-commerce. Compass-v3\nadopts fewer but larger experts, combined with hardware-efficient\noptimizations-such as intra-node expert parallelism and a customized memcpy\noperator-to maximize GPU utilization. The model is trained on 12T tokens of\ncurated multilingual corpora and large-scale synthetic e-commerce instructions\nusing a mixed-training strategy. To enhance alignment, we propose\nOptimal-Transport Direct Preference Optimization (OTPO), which captures\ntoken-level distinctions and improves instruction adherence in\ncommerce-specific scenarios. Extensive evaluations demonstrate that Compass-v3\ndelivers state-of-the-art e-commerce performance, surpassing DeepSeek-V3.1,\nGPT-4 series, and Qwen3-235B. Moreover, Compass-v3 demonstrates strong\nmultilingual capability across low-resource Southeast Asian languages\n(Indonesian, Thai, Filipino, Vietnamese, Malay, Taglog) and Portuguese while\nsustaining competitive performance on general benchmarks. It has already been\nwidely applied in Shopee's industrial-scale e-commerce platform and is\ngradually replacing OpenAI's traffic, now accounting for over 70\\% of total LLM\nusage, highlighting its dual strengths in specialized commerce expertise and\nbroad linguistic competence."}
{"id": "2509.09461", "pdf": "https://arxiv.org/pdf/2509.09461.pdf", "abs": "https://arxiv.org/abs/2509.09461", "title": "Changing the Paradigm from Dynamic Queries to LLM-generated SQL Queries with Human Intervention", "authors": ["Ambre Assor", "Hyeon Jeon", "Sungbok Shin", "Jean-Daniel Fekete"], "categories": ["cs.HC"], "comment": null, "summary": "We propose leveraging Large Language Models (LLMs) as an interaction layer\nfor medical visualization systems. In domains like healthcare, where users must\nnavigate high-dimensional, coded, and heterogeneous datasets, LLM-generated\nqueries enable expert medical users to express complex analytical intents in\nnatural language. These intents are then translated into editable and\nexecutable queries, replacing the dynamic query interfaces used by traditional\nvisualization systems built around sliders, check boxes, and drop-downs. This\ninteraction model reduces visual clutter and eliminates the need for users to\nmemorize field names or system codes, supporting fluid exploration, with the\ndrawback of not exposing all the filtering criteria. We also reintroduce\ndynamic queries on demand to better support interactive exploration. We posit\nthat medical users are trained to know the possible filtering options but\nchallenged to remember the details of the attribute names and code values. We\ndemonstrate this paradigm in ParcoursVis, our scalable EventFlow-inspired\npatient care pathway visualization system powered by the French National Health\nData System, one of the largest health data repositories in the world."}
{"id": "2509.09125", "pdf": "https://arxiv.org/pdf/2509.09125.pdf", "abs": "https://arxiv.org/abs/2509.09125", "title": "Automated Classification of Tutors' Dialogue Acts Using Generative AI: A Case Study Using the CIMA Corpus", "authors": ["Liqun He", "Jiaqi Xu"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted for publication in the journal Reflecting Digital Learning.\n  First submitted: 30 Oct 2023. The final version will be available open access\n  via the journal", "summary": "This study explores the use of generative AI for automating the\nclassification of tutors' Dialogue Acts (DAs), aiming to reduce the time and\neffort required by traditional manual coding. This case study uses the\nopen-source CIMA corpus, in which tutors' responses are pre-annotated into four\nDA categories. Both GPT-3.5-turbo and GPT-4 models were tested using tailored\nprompts. Results show that GPT-4 achieved 80% accuracy, a weighted F1-score of\n0.81, and a Cohen's Kappa of 0.74, surpassing baseline performance and\nindicating substantial agreement with human annotations. These findings suggest\nthat generative AI has strong potential to provide an efficient and accessible\napproach to DA classification, with meaningful implications for educational\ndialogue analysis. The study also highlights the importance of task-specific\nlabel definitions and contextual information in enhancing the quality of\nautomated annotation. Finally, it underscores the ethical considerations\nassociated with the use of generative AI and the need for responsible and\ntransparent research practices. The script of this research is publicly\navailable at\nhttps://github.com/liqunhe27/Generative-AI-for-educational-dialogue-act-tagging."}
{"id": "2509.09510", "pdf": "https://arxiv.org/pdf/2509.09510.pdf", "abs": "https://arxiv.org/abs/2509.09510", "title": "Cognitive Affordances in Visualization: Related Constructs, Design Factors, and Framework", "authors": ["Racquel Fygenson", "Lace Padilla", "Enrico Bertini"], "categories": ["cs.HC"], "comment": null, "summary": "Classically, affordance research investigates how the shape of objects\ncommunicates actions to potential users. Cognitive affordances, a subset of\nthis research, characterize how the design of objects influences cognitive\nactions, such as information processing. Within visualization, cognitive\naffordances inform how graphs' design decisions communicate information to\ntheir readers. Although several related concepts exist in visualization, a\nformal translation of affordance theory to visualization is still lacking. In\nthis paper, we review and translate affordance theory to visualization by\nformalizing how cognitive affordances operate within a visualization context.\nWe also review common methods and terms, and compare related constructs to\ncognitive affordances in visualization. Based on a synthesis of research from\npsychology, human computer interaction, and visualization, we propose a\nframework of cognitive affordances in visualization that enumerates design\ndecisions and reader characteristics that influence a visualization's hierarchy\nof communicated information. Finally, we demonstrate how this framework can\nguide the evaluation and redesign of visualizations."}
{"id": "2509.09131", "pdf": "https://arxiv.org/pdf/2509.09131.pdf", "abs": "https://arxiv.org/abs/2509.09131", "title": "ViRanker: A BGE-M3 & Blockwise Parallel Transformer Cross-Encoder for Vietnamese Reranking", "authors": ["Phuong-Nam Dang", "Kieu-Linh Nguyen", "Thanh-Hieu Pham"], "categories": ["cs.CL", "cs.AI"], "comment": "9 pages", "summary": "This paper presents ViRanker, a cross-encoder reranking model tailored to the\nVietnamese language. Built on the BGE-M3 encoder and enhanced with the\nBlockwise Parallel Transformer, ViRanker addresses the lack of competitive\nrerankers for Vietnamese, a low-resource language with complex syntax and\ndiacritics. The model was trained on an 8 GB curated corpus and fine-tuned with\nhybrid hard-negative sampling to strengthen robustness. Evaluated on the\nMMARCO-VI benchmark, ViRanker achieves strong early-rank accuracy, surpassing\nmultilingual baselines and competing closely with PhoRanker. By releasing the\nmodel openly on Hugging Face, we aim to support reproducibility and encourage\nwider adoption in real-world retrieval systems. Beyond Vietnamese, this study\nillustrates how careful architectural adaptation and data curation can advance\nreranking in other underrepresented languages."}
{"id": "2509.09645", "pdf": "https://arxiv.org/pdf/2509.09645.pdf", "abs": "https://arxiv.org/abs/2509.09645", "title": "Explaining the Reputational Risks of AI-Mediated Communication: Messages Labeled as AI-Assisted Are Viewed as Less Diagnostic of the Sender's Moral Character", "authors": ["Pranav Khadpe", "Kimi Wenzel", "George Loewenstein", "Geoff Kaufman"], "categories": ["cs.HC", "cs.CY", "cs.ET"], "comment": "To appear at AIES 2025", "summary": "When someone sends us a thoughtful message, we naturally form judgments about\ntheir character. But what happens when that message carries a label indicating\nit was written with the help of AI? This paper investigates how the appearance\nof AI assistance affects our perceptions of message senders. Adding nuance to\nprevious research, through two studies (N=399) featuring vignette scenarios, we\nfind that AI-assistance labels don't necessarily make people view senders\nnegatively. Rather, they dampen the strength of character signals in\ncommunication. We show that when someone sends a warmth-signalling message\n(like thanking or apologizing) without AI help, people more strongly categorize\nthe sender as warm. At the same time, when someone sends a coldness-signalling\nmessage (like bragging or blaming) without assistance, people more confidently\ncategorize them as cold. Interestingly, AI labels weaken both these\nassociations: An AI-assisted apology makes the sender appear less warm than if\nthey had written it themselves, and an AI-assisted blame makes the sender\nappear less cold than if they had composed it independently. This supports our\nsignal diagnosticity explanation: messages labeled as AI-assisted are viewed as\nless diagnostic than messages which seem unassisted. We discuss how our\nfindings shed light on the causal origins of previously reported observations\nin AI-Mediated Communication."}
{"id": "2509.09152", "pdf": "https://arxiv.org/pdf/2509.09152.pdf", "abs": "https://arxiv.org/abs/2509.09152", "title": "LITcoder: A General-Purpose Library for Building and Comparing Encoding Models", "authors": ["Taha Binhuraib", "Ruimin Gao", "Anna A. Ivanova"], "categories": ["cs.CL", "q-bio.NC"], "comment": null, "summary": "We introduce LITcoder, an open-source library for building and benchmarking\nneural encoding models. Designed as a flexible backend, LITcoder provides\nstandardized tools for aligning continuous stimuli (e.g., text and speech) with\nbrain data, transforming stimuli into representational features, mapping those\nfeatures onto brain data, and evaluating the predictive performance of the\nresulting model on held-out data. The library implements a modular pipeline\ncovering a wide array of methodological design choices, so researchers can\neasily compose, compare, and extend encoding models without reinventing core\ninfrastructure. Such choices include brain datasets, brain regions, stimulus\nfeature (both neural-net-based and control, such as word rate), downsampling\napproaches, and many others. In addition, the library provides built-in\nlogging, plotting, and seamless integration with experiment tracking platforms\nsuch as Weights & Biases (W&B). We demonstrate the scalability and versatility\nof our framework by fitting a range of encoding models to three story listening\ndatasets: LeBel et al. (2023), Narratives, and Little Prince. We also explore\nthe methodological choices critical for building encoding models for continuous\nfMRI data, illustrating the importance of accounting for all tokens in a TR\nscan (as opposed to just taking the last one, even when contextualized),\nincorporating hemodynamic lag effects, using train-test splits that minimize\ninformation leakage, and accounting for head motion effects on encoding model\npredictivity. Overall, LITcoder lowers technical barriers to encoding model\nimplementation, facilitates systematic comparisons across models and datasets,\nfosters methodological rigor, and accelerates the development of high-quality\nhigh-performance predictive models of brain activity.\n  Project page: https://litcoder-brain.github.io"}
{"id": "2509.08857", "pdf": "https://arxiv.org/pdf/2509.08857.pdf", "abs": "https://arxiv.org/abs/2509.08857", "title": "A Systematic Mapping Study on Chatbots in Programming Education", "authors": ["Marcelino Garcia", "Renato Garcia", "Arthur Parizotto", "Andre Mendes", "Pedro Valle", "Ricardo Vilela", "Renato Balancieri", "Williamson Silva"], "categories": ["cs.SE", "cs.HC"], "comment": "18 pages, 1 figure, 3 tables", "summary": "Educational chatbots have gained prominence as support tools for teaching\nprogramming, particularly in introductory learning contexts. This paper\npresents a Systematic Mapping Study (SMS) that investigated how such agents\nhave been developed and applied in programming education. From an initial set\nof 3,216 publications, 54 studies were selected and analyzed based on five\nresearch subquestions, addressing chatbot types, programming languages used,\neducational content covered, interaction models, and application contexts. The\nresults reveal a predominance of chatbots designed for Python instruction,\nfocusing on fundamental programming concepts, and employing a wide variety of\npedagogical approaches and technological architectures. In addition to\nidentifying trends and gaps in the literature, this study provides insights to\ninform the development of new educational tools for programming instruction."}
{"id": "2509.09160", "pdf": "https://arxiv.org/pdf/2509.09160.pdf", "abs": "https://arxiv.org/abs/2509.09160", "title": "Target-oriented Multimodal Sentiment Classification with Counterfactual-enhanced Debiasing", "authors": ["Zhiyue Liu", "Fanrong Ma", "Xin Ling"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by the IEEE International Conference on Multimedia and Expo\n  (ICME 2025). \\copyright\\ 2025 IEEE. Personal use of this material is\n  permitted. Permission from IEEE must be obtained for all other uses", "summary": "Target-oriented multimodal sentiment classification seeks to predict\nsentiment polarity for specific targets from image-text pairs. While existing\nworks achieve competitive performance, they often over-rely on textual content\nand fail to consider dataset biases, in particular word-level contextual\nbiases. This leads to spurious correlations between text features and output\nlabels, impairing classification accuracy. In this paper, we introduce a novel\ncounterfactual-enhanced debiasing framework to reduce such spurious\ncorrelations. Our framework incorporates a counterfactual data augmentation\nstrategy that minimally alters sentiment-related causal features, generating\ndetail-matched image-text samples to guide the model's attention toward content\ntied to sentiment. Furthermore, for learning robust features from\ncounterfactual data and prompting model decisions, we introduce an adaptive\ndebiasing contrastive learning mechanism, which effectively mitigates the\ninfluence of biased words. Experimental results on several benchmark datasets\nshow that our proposed method outperforms state-of-the-art baselines."}
{"id": "2509.08862", "pdf": "https://arxiv.org/pdf/2509.08862.pdf", "abs": "https://arxiv.org/abs/2509.08862", "title": "Investigating Student Interaction Patterns with Large Language Model-Powered Course Assistants in Computer Science Courses", "authors": ["Chang Liu", "Loc Hoang", "Andrew Stolman", "Rene F. Kizilcec", "Bo Wu"], "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": null, "summary": "Providing students with flexible and timely academic support is a challenge\nat most colleges and universities, leaving many students without help outside\nscheduled hours. Large language models (LLMs) are promising for bridging this\ngap, but interactions between students and LLMs are rarely overseen by\neducators. We developed and studied an LLM-powered course assistant deployed\nacross multiple computer science courses to characterize real-world use and\nunderstand pedagogical implications. By Spring 2024, our system had been\ndeployed to approximately 2,000 students across six courses at three\ninstitutions. Analysis of the interaction data shows that usage remains strong\nin the evenings and nights and is higher in introductory courses, indicating\nthat our system helps address temporal support gaps and novice learner needs.\nWe sampled 200 conversations per course for manual annotation: most sampled\nresponses were judged correct and helpful, with a small share unhelpful or\nerroneous; few responses included dedicated examples. We also examined an\ninquiry-based learning strategy: only around 11% of sampled conversations\ncontained LLM-generated follow-up questions, which were often ignored by\nstudents in advanced courses. A Bloom's taxonomy analysis reveals that current\nLLM capabilities are limited in generating higher-order cognitive questions.\nThese patterns suggest opportunities for pedagogically oriented LLM-based\neducational systems and greater educator involvement in configuring prompts,\ncontent, and policies."}
{"id": "2509.09174", "pdf": "https://arxiv.org/pdf/2509.09174.pdf", "abs": "https://arxiv.org/abs/2509.09174", "title": "EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for Speech-to-Speech LLMs", "authors": ["Yuhao Zhang", "Yuhao Du", "Zhanchen Dai", "Xiangnan Ma", "Kaiqi Kou", "Benyou Wang", "Haizhou Li"], "categories": ["cs.CL", "cs.AI", "cs.SD"], "comment": null, "summary": "Speech-to-speech large language models (SLLMs) are attracting increasing\nattention. Derived from text-based large language models (LLMs), SLLMs often\nexhibit degradation in knowledge and reasoning capabilities. We hypothesize\nthat this limitation arises because current training paradigms for SLLMs fail\nto bridge the acoustic-semantic gap in the feature representation space. To\naddress this issue, we propose EchoX, which leverages semantic representations\nand dynamically generates speech training targets. This approach integrates\nboth acoustic and semantic learning, enabling EchoX to preserve strong\nreasoning abilities as a speech LLM. Experimental results demonstrate that\nEchoX, with about six thousand hours of training data, achieves advanced\nperformance on multiple knowledge-based question-answering benchmarks. The\nproject is available at https://github.com/FreedomIntelligence/EchoX."}
{"id": "2509.08912", "pdf": "https://arxiv.org/pdf/2509.08912.pdf", "abs": "https://arxiv.org/abs/2509.08912", "title": "Towards Trustworthy AI: Characterizing User-Reported Risks across LLMs \"In the Wild\"", "authors": ["Lingyao Li", "Renkai Ma", "Zhaoqian Xue", "Junjie Xiong"], "categories": ["cs.CY", "cs.HC"], "comment": null, "summary": "While Large Language Models (LLMs) are rapidly integrating into daily life,\nresearch on their risks often remains lab-based and disconnected from the\nproblems users encounter \"in the wild.\" While recent HCI research has begun to\nexplore these user-facing risks, it typically concentrates on a singular LLM\nchatbot like ChatGPT or an isolated risk like privacy. To gain a holistic\nunderstanding of multi-risk across LLM chatbots, we analyze online discussions\non Reddit around seven major LLM chatbots through the U.S. NIST's AI Risk\nManagement Framework. We find that user-reported risks are unevenly distributed\nand platform-specific. While \"Valid and Reliable\" risk is the most frequently\nmentioned, each product also exhibits a unique \"risk fingerprint;\" for\ninstance, user discussions associate GPT more with \"Safe\" and \"Fair\" issues,\nGemini with \"Privacy,\" and Claude with \"Secure and Resilient\" risks.\nFurthermore, the nature of these risks differs by their prevalence: less\nfrequent risks like \"Explainability\" and \"Privacy\" manifest as nuanced user\ntrade-offs, more common ones like \"Fairness\" are experienced as direct personal\nharms. Our findings reveal gaps between risks reported by system-centered\nstudies and by users, highlighting the need for user-centered approaches that\nsupport users in their daily use of LLM chatbots."}
{"id": "2509.09196", "pdf": "https://arxiv.org/pdf/2509.09196.pdf", "abs": "https://arxiv.org/abs/2509.09196", "title": "Efficient Trie-based Biasing using K-step Prediction for Rare Word Recognition", "authors": ["Chin Yuen Kwok", "Jia Qi yip"], "categories": ["cs.CL", "cs.AI"], "comment": "Published in Interspeech 2025", "summary": "Contextual biasing improves rare word recognition of ASR models by\nprioritizing the output of rare words during decoding. A common approach is\nTrie-based biasing, which gives \"bonus scores\" to partial hypothesis (e.g.\n\"Bon\") that may lead to the generation of the rare word (e.g. \"Bonham\"). If the\nfull word (\"Bonham\") isn't ultimately recognized, the system revokes those\nearlier bonuses. This revocation is limited to beam search and is\ncomputationally expensive, particularly for models with large decoders. To\novercome these limitations, we propose adapting ASR models to look ahead and\npredict multiple steps at once. This avoids the revocation step entirely by\nbetter estimating whether a partial hypothesis will lead to the generation of\nthe full rare word. By fine-tuning Whisper with only 10 hours of synthetic\ndata, our method reduces the word error rate on the NSC Part 2 test set from\n30.86% to 12.19%."}
{"id": "2509.09071", "pdf": "https://arxiv.org/pdf/2509.09071.pdf", "abs": "https://arxiv.org/abs/2509.09071", "title": "Understanding Economic Tradeoffs Between Human and AI Agents in Bargaining Games", "authors": ["Crystal Qian", "Kehang Zhu", "John Horton", "Benjamin S. Manning", "Vivian Tsai", "James Wexler", "Nithum Thain"], "categories": ["cs.AI", "cs.GT", "cs.HC"], "comment": null, "summary": "Coordination tasks traditionally performed by humans are increasingly being\ndelegated to autonomous agents. As this pattern progresses, it becomes critical\nto evaluate not only these agents' performance but also the processes through\nwhich they negotiate in dynamic, multi-agent environments. Furthermore,\ndifferent agents exhibit distinct advantages: traditional statistical agents,\nsuch as Bayesian models, may excel under well-specified conditions, whereas\nlarge language models (LLMs) can generalize across contexts. In this work, we\ncompare humans (N = 216), LLMs (GPT-4o, Gemini 1.5 Pro), and Bayesian agents in\na dynamic negotiation setting that enables direct, identical-condition\ncomparisons across populations, capturing both outcomes and behavioral\ndynamics. Bayesian agents extract the highest surplus through aggressive\noptimization, at the cost of frequent trade rejections. Humans and LLMs can\nachieve similar overall surplus, but through distinct behaviors: LLMs favor\nconservative, concessionary trades with few rejections, while humans employ\nmore strategic, risk-taking, and fairness-oriented behaviors. Thus, we find\nthat performance parity -- a common benchmark in agent evaluation -- can\nconceal fundamental differences in process and alignment, which are critical\nfor practical deployment in real-world coordination tasks."}
{"id": "2509.09197", "pdf": "https://arxiv.org/pdf/2509.09197.pdf", "abs": "https://arxiv.org/abs/2509.09197", "title": "Improving Synthetic Data Training for Contextual Biasing Models with a Keyword-Aware Cost Function", "authors": ["Chin Yuen Kwok", "Jia Qi Yip", "Eng Siong Chng"], "categories": ["cs.CL", "cs.AI"], "comment": "Published in Interspeech 2025", "summary": "Rare word recognition can be improved by adapting ASR models to synthetic\ndata that includes these words. Further improvements can be achieved through\ncontextual biasing, which trains and adds a biasing module into the model\narchitecture to prioritize rare words. While training the module on synthetic\nrare word data is more effective than using non-rare-word data, it can lead to\noverfitting due to artifacts in the synthetic audio. To address this, we\nenhance the TCPGen-based contextual biasing approach and propose a\nkeyword-aware loss function that additionally focuses on biased words when\ntraining biasing modules. This loss includes a masked cross-entropy term for\nbiased word prediction and a binary classification term for detecting biased\nword positions. These two terms complementarily support the decoding of biased\nwords during inference. By adapting Whisper to 10 hours of synthetic data, our\nmethod reduced the word error rate on the NSC Part 2 test set from 29.71% to\n11.81%."}
{"id": "2509.09314", "pdf": "https://arxiv.org/pdf/2509.09314.pdf", "abs": "https://arxiv.org/abs/2509.09314", "title": "Measuring Implicit Spatial Coordination in Teams: Effects on Collective Intelligence and Performance", "authors": ["Thuy Ngoc Nguyen", "Anita Williams Woolley", "Cleotilde Gonzalez"], "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "Coordinated teamwork is essential in fast-paced decision-making environments\nthat require dynamic adaptation, often without an opportunity for explicit\ncommunication. Although implicit coordination has been extensively considered\nin the existing literature, the majority of work has focused on co-located,\nsynchronous teamwork (such as sports teams) or, in distributed teams, primarily\non coordination of knowledge work. However, many teams (firefighters, military,\nlaw enforcement, emergency response) must coordinate their movements in\nphysical space without the benefit of visual cues or extensive explicit\ncommunication. This paper investigates how three dimensions of spatial\ncoordination, namely exploration diversity, movement specialization, and\nadaptive spatial proximity, influence team performance in a collaborative\nonline search and rescue task where explicit communication is restricted and\nteam members rely on movement patterns to infer others' intentions and\ncoordinate actions. Our metrics capture the relational aspects of teamwork by\nmeasuring spatial proximity, distribution patterns, and alignment of movements\nwithin shared environments. We analyze data from 34 four-person teams (136\nparticipants) assigned to specialized roles in a search and rescue task.\nResults show that spatial specialization positively predicts performance, while\nadaptive spatial proximity exhibits a marginal inverted U-shaped relationship,\nsuggesting moderate levels of adaptation are optimal. Furthermore, the temporal\ndynamics of these metrics differentiate high- from low-performing teams over\ntime. These findings provide insights into implicit spatial coordination in\nrole-based teamwork and highlight the importance of balanced adaptive\nstrategies, with implications for training and AI-assisted team support\nsystems."}
{"id": "2509.09198", "pdf": "https://arxiv.org/pdf/2509.09198.pdf", "abs": "https://arxiv.org/abs/2509.09198", "title": "GmSLM : Generative Marmoset Spoken Language Modeling", "authors": ["Talia Sternberg", "Michael London", "David Omer", "Yossi Adi"], "categories": ["cs.CL"], "comment": null, "summary": "Marmoset monkeys exhibit complex vocal communication, challenging the view\nthat nonhuman primates vocal communication is entirely innate, and show similar\nfeatures of human speech, such as vocal labeling of others and turn-taking.\nStudying their vocal communication offers a unique opportunity to link it with\nbrain activity-especially given the difficulty of accessing the human brain in\nspeech and language research. Since Marmosets communicate primarily through\nvocalizations, applying standard LLM approaches is not straightforward. We\nintroduce Generative Marmoset Spoken Language Modeling (GmSLM), an optimized\nspoken language model pipeline for Marmoset vocal communication. We designed a\nnovel zero-shot evaluation metrics using unsupervised in-the-wild data,\nalongside weakly labeled conversational data, to assess GmSLM and demonstrate\nits advantage over a basic human-speech-based baseline. GmSLM generated\nvocalizations closely matched real resynthesized samples acoustically and\nperformed well on downstream tasks. Despite being fully unsupervised, GmSLM\neffectively distinguish real from artificial conversations and may support\nfurther investigations of the neural basis of vocal communication and provides\na practical framework linking vocalization and brain activity. We believe GmSLM\nstands to benefit future work in neuroscience, bioacoustics, and evolutionary\nbiology. Samples are provided under: pages.cs.huji.ac.il/adiyoss-lab/GmSLM."}
{"id": "2509.09508", "pdf": "https://arxiv.org/pdf/2509.09508.pdf", "abs": "https://arxiv.org/abs/2509.09508", "title": "Incorporating AI Incident Reporting into Telecommunications Law and Policy: Insights from India", "authors": ["Avinash Agarwal", "Manisha J. Nene"], "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "16 pages, 2 figures, 1 table", "summary": "The integration of artificial intelligence (AI) into telecommunications\ninfrastructure introduces novel risks, such as algorithmic bias and\nunpredictable system behavior, that fall outside the scope of traditional\ncybersecurity and data protection frameworks. This paper introduces a precise\ndefinition and a detailed typology of telecommunications AI incidents,\nestablishing them as a distinct category of risk that extends beyond\nconventional cybersecurity and data protection breaches. It argues for their\nrecognition as a distinct regulatory concern. Using India as a case study for\njurisdictions that lack a horizontal AI law, the paper analyzes the country's\nkey digital regulations. The analysis reveals that India's existing legal\ninstruments, including the Telecommunications Act, 2023, the CERT-In Rules, and\nthe Digital Personal Data Protection Act, 2023, focus on cybersecurity and data\nbreaches, creating a significant regulatory gap for AI-specific operational\nincidents, such as performance degradation and algorithmic bias. The paper also\nexamines structural barriers to disclosure and the limitations of existing AI\nincident repositories. Based on these findings, the paper proposes targeted\npolicy recommendations centered on integrating AI incident reporting into\nIndia's existing telecom governance. Key proposals include mandating reporting\nfor high-risk AI failures, designating an existing government body as a nodal\nagency to manage incident data, and developing standardized reporting\nframeworks. These recommendations aim to enhance regulatory clarity and\nstrengthen long-term resilience, offering a pragmatic and replicable blueprint\nfor other nations seeking to govern AI risks within their existing sectoral\nframeworks."}
{"id": "2509.09199", "pdf": "https://arxiv.org/pdf/2509.09199.pdf", "abs": "https://arxiv.org/abs/2509.09199", "title": "CCF: A Context Compression Framework for Efficient Long-Sequence Language Modeling", "authors": ["Wenhao Li", "Bangcheng Sun", "Weihao Ye", "Tianyi Zhang", "Daohai Yu", "Fei Chao", "Rongrong Ji"], "categories": ["cs.CL"], "comment": null, "summary": "Scaling language models to longer contexts is essential for capturing rich\ndependencies across extended discourse. However, na\\\"ive context extension\nimposes significant computational and memory burdens, often resulting in\ninefficiencies during both training and inference. In this work, we propose\nCCF, a novel context compression framework designed to enable efficient\nlong-context modeling by learning hierarchical latent representations that\npreserve global semantics while aggressively reducing input redundancy. CCF\nintegrates segment-wise semantic aggregation with key-value memory encoding,\nforming compact representations that support accurate reconstruction and\nlong-range understanding. To further enhance scalability, we introduce a\ntraining-efficient optimization strategy that couples incremental segment\ndecoding with sparse reservoir sampling, substantially reducing memory overhead\nwithout degrading performance. Empirical results on multiple long-context\nlanguage modeling benchmarks demonstrate that CCF achieves competitive\nperplexity under high compression ratios, and significantly improves throughput\nand memory efficiency compared to existing approaches. These findings highlight\nthe potential of structured compression for scalable and effective long-context\nlanguage modeling."}
{"id": "2509.09583", "pdf": "https://arxiv.org/pdf/2509.09583.pdf", "abs": "https://arxiv.org/abs/2509.09583", "title": "Personality-Enhanced Social Recommendations in SAMI: Exploring the Role of Personality Detection in Matchmaking", "authors": ["Brittany Harbison", "Samuel Taubman", "Travis Taylor", "Ashok. K. Goel"], "categories": ["cs.CL", "cs.CY", "cs.HC", "cs.LG", "cs.SI"], "comment": null, "summary": "Social connection is a vital part of learning, yet online course environments\npresent barriers to the organic formation of social groups. SAMI offers one\nsolution by facilitating student connections, but its effectiveness is\nconstrained by an incomplete Theory of Mind, limiting its ability to create an\neffective mental model of a student. One facet of this is its inability to\nintuit personality, which may influence the relevance of its recommendations.\nTo explore this, we propose a personality detection model utilizing GPTs\nzero-shot capability to infer Big-Five personality traits from forum\nintroduction posts, often encouraged in online courses. We benchmark its\nperformance against established models, demonstrating its efficacy in this\ntask. Furthermore, we integrate this model into SAMIs entity-based matchmaking\nsystem, enabling personality-informed social recommendations. Initial\nintegration suggests personality traits can complement existing matching\nfactors, though additional evaluation is required to determine their full\nimpact on student engagement and match quality."}
{"id": "2509.09229", "pdf": "https://arxiv.org/pdf/2509.09229.pdf", "abs": "https://arxiv.org/abs/2509.09229", "title": "Reading Between the Lines: Classifying Resume Seniority with Large Language Models", "authors": ["Matan Cohen", "Shira Shani", "Eden Menahem", "Yehudit Aperstein", "Alexander Apartsin"], "categories": ["cs.CL"], "comment": "5 pages, 3 figures", "summary": "Accurately assessing candidate seniority from resumes is a critical yet\nchallenging task, complicated by the prevalence of overstated experience and\nambiguous self-presentation. In this study, we investigate the effectiveness of\nlarge language models (LLMs), including fine-tuned BERT architectures, for\nautomating seniority classification in resumes. To rigorously evaluate model\nperformance, we introduce a hybrid dataset comprising both real-world resumes\nand synthetically generated hard examples designed to simulate exaggerated\nqualifications and understated seniority. Using the dataset, we evaluate the\nperformance of Large Language Models in detecting subtle linguistic cues\nassociated with seniority inflation and implicit expertise. Our findings\nhighlight promising directions for enhancing AI-driven candidate evaluation\nsystems and mitigating bias introduced by self-promotional language. The\ndataset is available for the research community at https://bit.ly/4mcTovt"}
{"id": "2509.09638", "pdf": "https://arxiv.org/pdf/2509.09638.pdf", "abs": "https://arxiv.org/abs/2509.09638", "title": "CryptoGuard: An AI-Based Cryptojacking Detection Dashboard Prototype", "authors": ["Amitabh Chakravorty", "Jess Kropczynski", "Nelly Elsayed"], "categories": ["cs.CR", "cs.HC"], "comment": null, "summary": "With the widespread adoption of cryptocurrencies, cryptojacking has become a\nsignificant security threat to crypto wallet users. This paper presents a\nfront-end prototype of an AI-powered security dashboard, namely, CryptoGuard.\nDeveloped through a user-centered design process, the prototype was constructed\nas a high-fidelity, click-through model from Figma mockups to simulate key user\ninteractions. It is designed to assist users in monitoring their login and\ntransaction activity, identifying any suspicious behavior, and enabling them to\ntake action directly within the wallet interface. The dashboard is designed for\na general audience, prioritizing an intuitive user experience for non-technical\nindividuals. Although its AI functionality is conceptual, the prototype\ndemonstrates features like visual alerts and reporting. This work is positioned\nexplicitly as a design concept, bridging cryptojacking detection research with\nhuman-centered interface design. This paper also demonstrates how usability\nheuristics can directly inform a tool's ability to support rapid and confident\ndecision-making under real-world threats. This paper argues that practical\nsecurity tools require not only robust backend functionality but also a\nuser-centric design that communicates risk and empowers users to take\nmeaningful action."}
{"id": "2509.09234", "pdf": "https://arxiv.org/pdf/2509.09234.pdf", "abs": "https://arxiv.org/abs/2509.09234", "title": "Agentic LLMs for Question Answering over Tabular Data", "authors": ["Rishit Tyagi", "Mohit Gupta", "Rahul Bouri"], "categories": ["cs.CL"], "comment": "Accepted at ACL workshop SemEval 2025", "summary": "Question Answering over Tabular Data (Table QA) presents unique challenges\ndue to the diverse structure, size, and data types of real-world tables. The\nSemEval 2025 Task 8 (DataBench) introduced a benchmark composed of large-scale,\ndomain-diverse datasets to evaluate the ability of models to accurately answer\nstructured queries. We propose a Natural Language to SQL (NL-to-SQL) approach\nleveraging large language models (LLMs) such as GPT-4o, GPT-4o-mini, and\nDeepSeek v2:16b to generate SQL queries dynamically. Our system follows a\nmulti-stage pipeline involving example selection, SQL query generation, answer\nextraction, verification, and iterative refinement. Experiments demonstrate the\neffectiveness of our approach, achieving 70.5\\% accuracy on DataBench QA and\n71.6\\% on DataBench Lite QA, significantly surpassing baseline scores of 26\\%\nand 27\\% respectively. This paper details our methodology, experimental\nresults, and alternative approaches, providing insights into the strengths and\nlimitations of LLM-driven Table QA."}
{"id": "2503.16505", "pdf": "https://arxiv.org/pdf/2503.16505.pdf", "abs": "https://arxiv.org/abs/2503.16505", "title": "Scalable Evaluation of Online Facilitation Strategies via Synthetic Simulation of Discussions", "authors": ["Dimitris Tsirmpas", "Ion Androutsopoulos", "John Pavlopoulos"], "categories": ["cs.HC", "cs.CL", "cs.LG", "68T50", "I.2.7"], "comment": "15 pages, 3 tables, 12 figures", "summary": "Limited large-scale evaluations exist for facilitation strategies of online\ndiscussions due to significant costs associated with human involvement. An\neffective solution is synthetic discussion simulations using Large Language\nModels (LLMs) to create initial pilot experiments. We propose design principles\nbased on existing methodologies for synthetic discussion generation. Based on\nthese principles, we propose a simple, generalizable, LLM-driven methodology to\nprototype the development of LLM facilitators by generating synthetic data\nwithout human involvement, and which surpasses current baselines. We use our\nmethodology to test whether current Social Science strategies for facilitation\ncan improve the performance of LLM facilitators. We find that, while LLM\nfacilitators significantly improve synthetic discussions, there is no evidence\nthat the application of these strategies leads to further improvements in\ndiscussion quality. In an effort to aid research in the field of facilitation,\nwe release a large, publicly available dataset containing LLM-generated and\nLLM-annotated discussions using multiple open-source models. This dataset can\nbe used for LLM facilitator finetuning as well as behavioral analysis of\ncurrent out-of-the-box LLMs in the task. We also release an open-source python\nframework that efficiently implements our methodology at great scale."}
{"id": "2509.09303", "pdf": "https://arxiv.org/pdf/2509.09303.pdf", "abs": "https://arxiv.org/abs/2509.09303", "title": "From scratch to silver: Creating trustworthy training data for patent-SDG classification using Large Language Models", "authors": ["Grazia Sveva Ascione", "Nicolò Tamagnone"], "categories": ["cs.CL"], "comment": null, "summary": "Classifying patents by their relevance to the UN Sustainable Development\nGoals (SDGs) is crucial for tracking how innovation addresses global\nchallenges. However, the absence of a large, labeled dataset limits the use of\nsupervised learning. Existing methods, such as keyword searches, transfer\nlearning, and citation-based heuristics, lack scalability and generalizability.\nThis paper frames patent-to-SDG classification as a weak supervision problem,\nusing citations from patents to SDG-tagged scientific publications (NPL\ncitations) as a noisy initial signal. To address its sparsity and noise, we\ndevelop a composite labeling function (LF) that uses large language models\n(LLMs) to extract structured concepts, namely functions, solutions, and\napplications, from patents and SDG papers based on a patent ontology.\nCross-domain similarity scores are computed and combined using a rank-based\nretrieval approach. The LF is calibrated via a custom positive-only loss that\naligns with known NPL-SDG links without penalizing discovery of new SDG\nassociations. The result is a silver-standard, soft multi-label dataset mapping\npatents to SDGs, enabling the training of effective multi-label regression\nmodels. We validate our approach through two complementary strategies: (1)\ninternal validation against held-out NPL-based labels, where our method\noutperforms several baselines including transformer-based models, and zero-shot\nLLM; and (2) external validation using network modularity in patent citation,\nco-inventor, and co-applicant graphs, where our labels reveal greater thematic,\ncognitive, and organizational coherence than traditional technological\nclassifications. These results show that weak supervision and semantic\nalignment can enhance SDG classification at scale."}
{"id": "2503.18492", "pdf": "https://arxiv.org/pdf/2503.18492.pdf", "abs": "https://arxiv.org/abs/2503.18492", "title": "VeriSafe Agent: Safeguarding Mobile GUI Agent via Logic-based Action Verification", "authors": ["Jungjae Lee", "Dongjae Lee", "Chihun Choi", "Youngmin Im", "Jaeyoung Wi", "Kihong Heo", "Sangeun Oh", "Sunjae Lee", "Insik Shin"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Foundation Models (LFMs) have unlocked new possibilities in\nhuman-computer interaction, particularly with the rise of mobile Graphical User\nInterface (GUI) Agents capable of interacting with mobile GUIs. These agents\nallow users to automate complex mobile tasks through simple natural language\ninstructions. However, the inherent probabilistic nature of LFMs, coupled with\nthe ambiguity and context-dependence of mobile tasks, makes LFM-based\nautomation unreliable and prone to errors. To address this critical challenge,\nwe introduce VeriSafe Agent (VSA): a formal verification system that serves as\na logically grounded safeguard for Mobile GUI Agents. VSA deterministically\nensures that an agent's actions strictly align with user intent before\nexecuting the action. At its core, VSA introduces a novel autoformalization\ntechnique that translates natural language user instructions into a formally\nverifiable specification. This enables runtime, rule-based verification of\nagent's actions, detecting erroneous actions even before they take effect. To\nthe best of our knowledge, VSA is the first attempt to bring the rigor of\nformal verification to GUI agents, bridging the gap between LFM-driven actions\nand formal software verification. We implement VSA using off-the-shelf LFM\nservices (GPT-4o) and evaluate its performance on 300 user instructions across\n18 widely used mobile apps. The results demonstrate that VSA achieves\n94.33%-98.33% accuracy in verifying agent actions, outperforming existing\nLFM-based verification methods by 30.00%-16.33%, and increases the GUI agent's\ntask completion rate by 90%-130%."}
{"id": "2509.09360", "pdf": "https://arxiv.org/pdf/2509.09360.pdf", "abs": "https://arxiv.org/abs/2509.09360", "title": "MetaRAG: Metamorphic Testing for Hallucination Detection in RAG Systems", "authors": ["Channdeth Sok", "David Luz", "Yacine Haddam"], "categories": ["cs.CL"], "comment": "under review", "summary": "Large Language Models (LLMs) are increasingly deployed in enterprise\napplications, yet their reliability remains limited by hallucinations, i.e.,\nconfident but factually incorrect information. Existing detection approaches,\nsuch as SelfCheckGPT and MetaQA, primarily target standalone LLMs and do not\naddress the unique challenges of Retrieval-Augmented Generation (RAG) systems,\nwhere responses must be consistent with retrieved evidence. We therefore\npresent MetaRAG, a metamorphic testing framework for hallucination detection in\nRetrieval-Augmented Generation (RAG) systems. MetaRAG operates in a real-time,\nunsupervised, black-box setting, requiring neither ground-truth references nor\naccess to model internals, making it suitable for proprietary and high-stakes\ndomains. The framework proceeds in four stages: (1) decompose answers into\natomic factoids, (2) generate controlled mutations of each factoid using\nsynonym and antonym substitutions, (3) verify each variant against the\nretrieved context (synonyms are expected to be entailed and antonyms\ncontradicted), and (4) aggregate penalties for inconsistencies into a\nresponse-level hallucination score. Crucially for identity-aware AI, MetaRAG\nlocalizes unsupported claims at the factoid span where they occur (e.g.,\npregnancy-specific precautions, LGBTQ+ refugee rights, or labor eligibility),\nallowing users to see flagged spans and enabling system designers to configure\nthresholds and guardrails for identity-sensitive queries. Experiments on a\nproprietary enterprise dataset illustrate the effectiveness of MetaRAG for\ndetecting hallucinations and enabling trustworthy deployment of RAG-based\nconversational agents. We also outline a topic-based deployment design that\ntranslates MetaRAG's span-level scores into identity-aware safeguards; this\ndesign is discussed but not evaluated in our experiments."}
{"id": "2507.22900", "pdf": "https://arxiv.org/pdf/2507.22900.pdf", "abs": "https://arxiv.org/abs/2507.22900", "title": "New Kid in the Classroom: Exploring Student Perceptions of AI Coding Assistants", "authors": ["Sergio Rojas-Galeano"], "categories": ["cs.HC", "cs.AI"], "comment": "A shorter version of the manuscript (16 pages) has been accepted for\n  publication in the Proceedings of 19th Colombian Conference on Computing, CCC\n  2025", "summary": "The arrival of AI coding assistants in educational settings presents a\nparadigm shift, introducing a \"new kid in the classroom\" for both students and\ninstructors. Thus, understanding the perceptions of these key actors about this\nnew dynamic is critical. This exploratory study contributes to this area by\ninvestigating how these tools are shaping the experiences of novice programmers\nin an introductory programming course. Through a two-part exam, we investigated\nstudent perceptions by first providing access to AI support for a programming\ntask and then requiring an extension of the solution without it. We collected\nLikert-scale and open-ended responses from 20 students to understand their\nperceptions on the challenges they faced. Our findings reveal that students\nperceived AI tools as helpful for grasping code concepts and boosting their\nconfidence during the initial development phase. However, a noticeable\ndifficulty emerged when students were asked to work unaided, pointing to\npotential overreliance and gaps in foundational knowledge transfer. These\ninsights highlight a critical need for new pedagogical approaches that\nintegrate AI effectively while effectively enhancing core programming skills,\nrather than impersonating them."}
{"id": "2509.09381", "pdf": "https://arxiv.org/pdf/2509.09381.pdf", "abs": "https://arxiv.org/abs/2509.09381", "title": "Modelling Analogies and Analogical Reasoning: Connecting Cognitive Science Theory and NLP Research", "authors": ["Molly R Petersen", "Claire E Stevenson", "Lonneke van der Plas"], "categories": ["cs.CL"], "comment": null, "summary": "Analogical reasoning is an essential aspect of human cognition. In this\npaper, we summarize key theory about the processes underlying analogical\nreasoning from the cognitive science literature and relate it to current\nresearch in natural language processing. While these processes can be easily\nlinked to concepts in NLP, they are generally not viewed through a cognitive\nlens. Furthermore, we show how these notions are relevant for several major\nchallenges in NLP research, not directly related to analogy solving. This may\nguide researchers to better optimize relational understanding in text, as\nopposed to relying heavily on entity-level similarity."}
{"id": "2508.03700", "pdf": "https://arxiv.org/pdf/2508.03700.pdf", "abs": "https://arxiv.org/abs/2508.03700", "title": "MagicGUI: A Foundational Mobile GUI Agent with Scalable Data Pipeline and Reinforcement Fine-tuning", "authors": ["Liujian Tang", "Shaokang Dong", "Yijia Huang", "Minqi Xiang", "Hongtao Ruan", "Bin Wang", "Shuo Li", "Zhiheng Xi", "Zhihui Cao", "Hailiang Pang", "Heng Kong", "He Yang", "Mingxu Chai", "Zhilin Gao", "Xingyu Liu", "Yingnan Fu", "Jiaming Liu", "Xuanjing Huang", "Yu-Gang Jiang", "Tao Gui", "Qi Zhang", "Kang Wang", "Yunke Zhang", "Yuran Wang"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "This paper presents MagicGUI, a foundational mobile GUI agent designed to\naddress critical challenges in perception, grounding, and reasoning within\nreal-world mobile GUI environments. The framework is underpinned by following\nsix key components: (1) a comprehensive and accurate dataset, constructed via\nthe scalable GUI Data Pipeline, which aggregates the largest and most diverse\nGUI-centric multimodal data to date from open-source repositories, automated\ncrawling, and targeted manual annotation; (2) enhanced perception and grounding\ncapabilities, facilitating fine-grained multimodal alignment for UI element\nreferencing, grounding, and screen comprehension; (3) a comprehensive and\nunified action space, encompassing both fundamental UI operations and complex\ninteractive intents to support human-agent interactions; (4) planning-oriented\nreasoning mechanisms that enable the model to decompose complex user\ninstructions into sequential actions with explicit intermediate meta-paln\nreasoning; (5) an iterative two-stage training procedure, combining large-scale\ncontinue pre-training on 7.8M samples with reinforcement fine-tuning utilizing\na spatially enhanced composite reward and dual filtering strategy; and (6)\ncompetitive performance on both the proprietary Magic-RICH benchmark and over a\ndozen public benchmarks, achieving superior performance across GUI perception\nand agent tasks, while demonstrating robust generalization and real-world\ndeployment potential in practical mobile GUI scenarios, as detailed in Figure\n1."}
{"id": "2509.09388", "pdf": "https://arxiv.org/pdf/2509.09388.pdf", "abs": "https://arxiv.org/abs/2509.09388", "title": "Hierarchical Bracketing Encodings Work for Dependency Graphs", "authors": ["Ana Ezquerro", "Carlos Gómez-Rodríguez", "David Vilares"], "categories": ["cs.CL"], "comment": "Accepted at EMNLP 2025 (main)", "summary": "We revisit hierarchical bracketing encodings from a practical perspective in\nthe context of dependency graph parsing. The approach encodes graphs as\nsequences, enabling linear-time parsing with $n$ tagging actions, and still\nrepresenting reentrancies, cycles, and empty nodes. Compared to existing graph\nlinearizations, this representation substantially reduces the label space while\npreserving structural information. We evaluate it on a multilingual and\nmulti-formalism benchmark, showing competitive results and consistent\nimprovements over other methods in exact match accuracy."}
{"id": "2501.02348", "pdf": "https://arxiv.org/pdf/2501.02348.pdf", "abs": "https://arxiv.org/abs/2501.02348", "title": "Thinking with Many Minds: Using Large Language Models for Multi-Perspective Problem-Solving", "authors": ["Sanghyun Park", "Boris Maciejovsky", "Phanish Puranam"], "categories": ["cs.CL", "cs.HC"], "comment": "36 pages, 1 appendix", "summary": "Complex problem-solving requires cognitive flexibility--the capacity to\nentertain multiple perspectives while preserving their distinctiveness. This\nflexibility replicates the \"wisdom of crowds\" within a single individual,\nallowing them to \"think with many minds.\" While mental simulation enables\nimagined deliberation, cognitive constraints limit its effectiveness. We\npropose synthetic deliberation, a Large Language Model (LLM)-based method that\nsimulates discourse between agents embodying diverse perspectives, as a\nsolution. Using a custom GPT-based model, we showcase its benefits: concurrent\nprocessing of multiple viewpoints without cognitive degradation, parallel\nexploration of perspectives, and precise control over viewpoint synthesis. By\nexternalizing the deliberative process and distributing cognitive labor between\nparallel search and integration, synthetic deliberation transcends mental\nsimulation's limitations. This approach shows promise for strategic planning,\npolicymaking, and conflict resolution."}
{"id": "2509.09438", "pdf": "https://arxiv.org/pdf/2509.09438.pdf", "abs": "https://arxiv.org/abs/2509.09438", "title": "GrACE: A Generative Approach to Better Confidence Elicitation in Large Language Models", "authors": ["Zhaohan Zhang", "Ziquan Liu", "Ioannis Patras"], "categories": ["cs.CL"], "comment": "20 pages, 11 figures", "summary": "Assessing the reliability of Large Language Models (LLMs) by confidence\nelicitation is a prominent approach to AI safety in high-stakes applications,\nsuch as healthcare and finance. Existing methods either require expensive\ncomputational overhead or suffer from poor calibration, making them impractical\nand unreliable for real-world deployment. In this work, we propose GrACE, a\nGenerative Approach to Confidence Elicitation that enables scalable and\nreliable confidence elicitation for LLMs. GrACE adopts a novel mechanism in\nwhich the model expresses confidence by the similarity between the last hidden\nstate and the embedding of a special token appended to the vocabulary, in\nreal-time. We fine-tune the model for calibrating the confidence with\ncalibration targets associated with accuracy. Experiments with three LLMs and\ntwo benchmark datasets show that the confidence produced by GrACE achieves the\nbest discriminative capacity and calibration on open-ended generation tasks,\noutperforming six competing methods without resorting to additional sampling or\nan auxiliary model. Moreover, we propose two strategies for improving test-time\nscaling based on confidence induced by GrACE. Experimental results show that\nusing GrACE not only improves the accuracy of the final decision but also\nsignificantly reduces the number of required samples in the test-time scaling\nscheme, indicating the potential of GrACE as a practical solution for deploying\nLLMs with scalable, reliable, and real-time confidence estimation."}
{"id": "2505.19317", "pdf": "https://arxiv.org/pdf/2505.19317.pdf", "abs": "https://arxiv.org/abs/2505.19317", "title": "Effort-aware Fairness: Incorporating a Philosophy-informed, Human-centered Notion of Effort into Algorithmic Fairness Metrics", "authors": ["Tin Trung Nguyen", "Jiannan Xu", "Zora Che", "Phuong-Anh Nguyen-Le", "Rushil Dandamudi", "Donald Braman", "Furong Huang", "Hal Daumé III", "Zubin Jelveh"], "categories": ["cs.AI", "cs.CY", "cs.HC", "cs.LG"], "comment": "AIES 2025", "summary": "Although popularized AI fairness metrics, e.g., demographic parity, have\nuncovered bias in AI-assisted decision-making outcomes, they do not consider\nhow much effort one has spent to get to where one is today in the input feature\nspace. However, the notion of effort is important in how Philosophy and humans\nunderstand fairness. We propose a philosophy-informed approach to conceptualize\nand evaluate Effort-aware Fairness (EaF), grounded in the concept of Force,\nwhich represents the temporal trajectory of predictive features coupled with\ninertia. Besides theoretical formulation, our empirical contributions include:\n(1) a pre-registered human subjects experiment, which shows that for both\nstages of the (individual) fairness evaluation process, people consider the\ntemporal trajectory of a predictive feature more than its aggregate value; (2)\npipelines to compute Effort-aware Individual/Group Fairness in the criminal\njustice and personal finance contexts. Our work may enable AI model auditors to\nuncover and potentially correct unfair decisions against individuals who have\nspent significant efforts to improve but are still stuck with systemic\ndisadvantages outside their control."}
{"id": "2509.09473", "pdf": "https://arxiv.org/pdf/2509.09473.pdf", "abs": "https://arxiv.org/abs/2509.09473", "title": "Mitigating Language Barriers in Education: Developing Multilingual Digital Learning Materials with Machine Translation", "authors": ["Lucie Poláková", "Martin Popel", "Věra Kloudová", "Michal Novák", "Mariia Anisimova", "Jiří Balhar"], "categories": ["cs.CL"], "comment": "8 pages, 2 figures", "summary": "The EdUKate project combines digital education, linguistics, translation\nstudies, and machine translation to develop multilingual learning materials for\nCzech primary and secondary schools. Launched through collaboration between a\nmajor Czech academic institution and the country's largest educational\npublisher, the project is aimed at translating up to 9,000 multimodal\ninteractive exercises from Czech into Ukrainian, English, and German for an\neducational web portal. It emphasizes the development and evaluation of a\ndirect Czech-Ukrainian machine translation system tailored to the educational\ndomain, with special attention to processing formatted content such as XML and\nPDF and handling technical and scientific terminology. We present findings from\nan initial survey of Czech teachers regarding the needs of non-Czech-speaking\nstudents and describe the system's evaluation and implementation on the web\nportal. All resulting applications are freely available to students, educators,\nand researchers."}
{"id": "2506.04867", "pdf": "https://arxiv.org/pdf/2506.04867.pdf", "abs": "https://arxiv.org/abs/2506.04867", "title": "LLMs for sensory-motor control: Combining in-context and iterative learning", "authors": ["Jônata Tyska Carvalho", "Stefano Nolfi"], "categories": ["cs.AI", "cs.HC", "cs.LG", "cs.RO"], "comment": "Article updated with results from gpt-oss:120b. 24 pages (13 pages\n  are from appendix), 6 figures, code for experiments replication and\n  supplementary material provided at\n  https://github.com/jtyska/llm-robotics-article/", "summary": "We propose a method that enables large language models (LLMs) to control\nembodied agents by directly mapping continuous observation vectors to\ncontinuous action vectors. At the outset, the LLMs generate a control strategy\nbased on a textual description of the agent, its environment, and the intended\ngoal. This strategy is then iteratively refined through a learning process in\nwhich the LLMs are repeatedly prompted to improve the current strategy, using\nperformance feedback and sensory-motor data collected during its evaluation.\nThe method is validated on classic control tasks from the Gymnasium library and\nthe inverted pendulum task from the MuJoCo library. The approach proves\neffective with relatively compact models such as Gpt-oss:120b and Qwen2.5:72b.\nIn most cases, it successfully identifies optimal or near-optimal solutions by\nintegrating symbolic knowledge derived through reasoning with sub-symbolic\nsensory-motor data gathered as the agent interacts with its environment."}
{"id": "2509.09522", "pdf": "https://arxiv.org/pdf/2509.09522.pdf", "abs": "https://arxiv.org/abs/2509.09522", "title": "Towards Explainable Job Title Matching: Leveraging Semantic Textual Relatedness and Knowledge Graphs", "authors": ["Vadim Zadykian", "Bruno Andrade", "Haithem Afli"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Semantic Textual Relatedness (STR) captures nuanced relationships between\ntexts that extend beyond superficial lexical similarity. In this study, we\ninvestigate STR in the context of job title matching - a key challenge in\nresume recommendation systems, where overlapping terms are often limited or\nmisleading. We introduce a self-supervised hybrid architecture that combines\ndense sentence embeddings with domain-specific Knowledge Graphs (KGs) to\nimprove both semantic alignment and explainability. Unlike previous work that\nevaluated models on aggregate performance, our approach emphasizes data\nstratification by partitioning the STR score continuum into distinct regions:\nlow, medium, and high semantic relatedness. This stratified evaluation enables\na fine-grained analysis of model performance across semantically meaningful\nsubspaces. We evaluate several embedding models, both with and without KG\nintegration via graph neural networks. The results show that fine-tuned SBERT\nmodels augmented with KGs produce consistent improvements in the high-STR\nregion, where the RMSE is reduced by 25% over strong baselines. Our findings\nhighlight not only the benefits of combining KGs with text embeddings, but also\nthe importance of regional performance analysis in understanding model\nbehavior. This granular approach reveals strengths and weaknesses hidden by\nglobal metrics, and supports more targeted model selection for use in Human\nResources (HR) systems and applications where fairness, explainability, and\ncontextual matching are essential."}
{"id": "2507.00792", "pdf": "https://arxiv.org/pdf/2507.00792.pdf", "abs": "https://arxiv.org/abs/2507.00792", "title": "JAX-IK: Real-Time Inverse Kinematics for Generating Multi-Constrained Movements of Virtual Human Characters", "authors": ["Hendric Voss", "Stefan Kopp"], "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "Generating accurate and realistic virtual human movements in real-time is of\nhigh importance for a variety of applications in computer graphics, interactive\nvirtual environments, robotics, and biomechanics. This paper introduces a novel\nreal-time inverse kinematics (IK) solver specifically designed for realistic\nhuman-like movement generation. Leveraging the automatic differentiation and\njust-in-time compilation of TensorFlow, the proposed solver efficiently handles\ncomplex articulated human skeletons with high degrees of freedom. By treating\nforward and inverse kinematics as differentiable operations, our method\neffectively addresses common challenges such as error accumulation and\ncomplicated joint limits in multi-constrained problems, which are critical for\nrealistic human motion modeling. We demonstrate the solver's effectiveness on\nthe SMPLX human skeleton model, evaluating its performance against widely used\niterative-based IK algorithms, like Cyclic Coordinate Descent (CCD), FABRIK,\nand the nonlinear optimization algorithm IPOPT. Our experiments cover both\nsimple end-effector tasks and sophisticated, multi-constrained problems with\nrealistic joint limits. Results indicate that our IK solver achieves real-time\nperformance, exhibiting rapid convergence, minimal computational overhead per\niteration, and improved success rates compared to existing methods. The project\ncode is available at https://github.com/hvoss-techfak/JAX-IK"}
{"id": "2509.09524", "pdf": "https://arxiv.org/pdf/2509.09524.pdf", "abs": "https://arxiv.org/abs/2509.09524", "title": "DeMeVa at LeWiDi-2025: Modeling Perspectives with In-Context Learning and Label Distribution Learning", "authors": ["Daniil Ignatev", "Nan Li", "Hugh Mee Wong", "Anh Dang", "Shane Kaszefski Yaschuk"], "categories": ["cs.CL", "cs.LG"], "comment": "11 pages, 4 figures; to appear at NLPerspectives@EMNLP-2025", "summary": "This system paper presents the DeMeVa team's approaches to the third edition\nof the Learning with Disagreements shared task (LeWiDi 2025; Leonardelli et\nal., 2025). We explore two directions: in-context learning (ICL) with large\nlanguage models, where we compare example sampling strategies; and label\ndistribution learning (LDL) methods with RoBERTa (Liu et al., 2019b), where we\nevaluate several fine-tuning methods. Our contributions are twofold: (1) we\nshow that ICL can effectively predict annotator-specific annotations\n(perspectivist annotations), and that aggregating these predictions into soft\nlabels yields competitive performance; and (2) we argue that LDL methods are\npromising for soft label predictions and merit further exploration by the\nperspectivist community."}
{"id": "2509.09544", "pdf": "https://arxiv.org/pdf/2509.09544.pdf", "abs": "https://arxiv.org/abs/2509.09544", "title": "Prompting the Market? A Large-Scale Meta-Analysis of GenAI in Finance NLP (2022-2025)", "authors": ["Paolo Pedinotti", "Peter Baumann", "Nathan Jessurun", "Leslie Barrett", "Enrico Santus"], "categories": ["cs.CL"], "comment": "7 pages, 6 appendices, EMNLP industry track", "summary": "Large Language Models (LLMs) have rapidly reshaped financial NLP, enabling\nnew tasks and driving a proliferation of datasets and diversification of data\nsources. Yet, this transformation has outpaced traditional surveys. In this\npaper, we present MetaGraph, a generalizable methodology for extracting\nknowledge graphs from scientific literature and analyzing them to obtain a\nstructured, queryable view of research trends. We define an ontology for\nfinancial NLP research and apply an LLM-based extraction pipeline to 681 papers\n(2022-2025), enabling large-scale, data-driven analysis. MetaGraph reveals\nthree key phases: early LLM adoption and task/dataset innovation; critical\nreflection on LLM limitations; and growing integration of peripheral techniques\ninto modular systems. This structured view offers both practitioners and\nresearchers a clear understanding of how financial NLP has evolved -\nhighlighting emerging trends, shifting priorities, and methodological\nshifts-while also demonstrating a reusable approach for mapping scientific\nprogress in other domains."}
{"id": "2509.09583", "pdf": "https://arxiv.org/pdf/2509.09583.pdf", "abs": "https://arxiv.org/abs/2509.09583", "title": "Personality-Enhanced Social Recommendations in SAMI: Exploring the Role of Personality Detection in Matchmaking", "authors": ["Brittany Harbison", "Samuel Taubman", "Travis Taylor", "Ashok. K. Goel"], "categories": ["cs.CL", "cs.CY", "cs.HC", "cs.LG", "cs.SI"], "comment": null, "summary": "Social connection is a vital part of learning, yet online course environments\npresent barriers to the organic formation of social groups. SAMI offers one\nsolution by facilitating student connections, but its effectiveness is\nconstrained by an incomplete Theory of Mind, limiting its ability to create an\neffective mental model of a student. One facet of this is its inability to\nintuit personality, which may influence the relevance of its recommendations.\nTo explore this, we propose a personality detection model utilizing GPTs\nzero-shot capability to infer Big-Five personality traits from forum\nintroduction posts, often encouraged in online courses. We benchmark its\nperformance against established models, demonstrating its efficacy in this\ntask. Furthermore, we integrate this model into SAMIs entity-based matchmaking\nsystem, enabling personality-informed social recommendations. Initial\nintegration suggests personality traits can complement existing matching\nfactors, though additional evaluation is required to determine their full\nimpact on student engagement and match quality."}
{"id": "2509.09593", "pdf": "https://arxiv.org/pdf/2509.09593.pdf", "abs": "https://arxiv.org/abs/2509.09593", "title": "Fluent but Unfeeling: The Emotional Blind Spots of Language Models", "authors": ["Bangzhao Shu", "Isha Joshi", "Melissa Karnaze", "Anh C. Pham", "Ishita Kakkar", "Sindhu Kothe", "Arpine Hovasapian", "Mai ElSherief"], "categories": ["cs.CL", "cs.AI"], "comment": "Camera-ready version for ICWSM 2026. First two authors contributed\n  equally", "summary": "The versatility of Large Language Models (LLMs) in natural language\nunderstanding has made them increasingly popular in mental health research.\nWhile many studies explore LLMs' capabilities in emotion recognition, a\ncritical gap remains in evaluating whether LLMs align with human emotions at a\nfine-grained level. Existing research typically focuses on classifying emotions\ninto predefined, limited categories, overlooking more nuanced expressions. To\naddress this gap, we introduce EXPRESS, a benchmark dataset curated from Reddit\ncommunities featuring 251 fine-grained, self-disclosed emotion labels. Our\ncomprehensive evaluation framework examines predicted emotion terms and\ndecomposes them into eight basic emotions using established emotion theories,\nenabling a fine-grained comparison. Systematic testing of prevalent LLMs under\nvarious prompt settings reveals that accurately predicting emotions that align\nwith human self-disclosed emotions remains challenging. Qualitative analysis\nfurther shows that while certain LLMs generate emotion terms consistent with\nestablished emotion theories and definitions, they sometimes fail to capture\ncontextual cues as effectively as human self-disclosures. These findings\nhighlight the limitations of LLMs in fine-grained emotion alignment and offer\ninsights for future research aimed at enhancing their contextual understanding."}
{"id": "2509.09602", "pdf": "https://arxiv.org/pdf/2509.09602.pdf", "abs": "https://arxiv.org/abs/2509.09602", "title": "LAVA: Language Model Assisted Verbal Autopsy for Cause-of-Death Determination", "authors": ["Yiqun T. Chen", "Tyler H. McCormick", "Li Liu", "Abhirup Datta"], "categories": ["cs.CL", "stat.AP"], "comment": null, "summary": "Verbal autopsy (VA) is a critical tool for estimating causes of death in\nresource-limited settings where medical certification is unavailable. This\nstudy presents LA-VA, a proof-of-concept pipeline that combines Large Language\nModels (LLMs) with traditional algorithmic approaches and embedding-based\nclassification for improved cause-of-death prediction. Using the Population\nHealth Metrics Research Consortium (PHMRC) dataset across three age categories\n(Adult: 7,580; Child: 1,960; Neonate: 2,438), we evaluate multiple approaches:\nGPT-5 predictions, LCVA baseline, text embeddings, and meta-learner ensembles.\nOur results demonstrate that GPT-5 achieves the highest individual performance\nwith average test site accuracies of 48.6% (Adult), 50.5% (Child), and 53.5%\n(Neonate), outperforming traditional statistical machine learning baselines by\n5-10%. Our findings suggest that simple off-the-shelf LLM-assisted approaches\ncould substantially improve verbal autopsy accuracy, with important\nimplications for global health surveillance in low-resource settings."}
{"id": "2509.09629", "pdf": "https://arxiv.org/pdf/2509.09629.pdf", "abs": "https://arxiv.org/abs/2509.09629", "title": "Bridging the Capability Gap: Joint Alignment Tuning for Harmonizing LLM-based Multi-Agent Systems", "authors": ["Minghang Zhu", "Zhengliang Shi", "Zhiwei Xu", "Shiguang Wu", "Lingjie Wang", "Pengjie Ren", "Zhaochun Ren", "Zhumin Chen"], "categories": ["cs.CL"], "comment": "EMNLP 2025 Findings", "summary": "The advancement of large language models (LLMs) has enabled the construction\nof multi-agent systems to solve complex tasks by dividing responsibilities\namong specialized agents, such as a planning agent for subgoal generation and a\ngrounding agent for executing tool-use actions. Most existing methods typically\nfine-tune these agents independently, leading to capability gaps among them\nwith poor coordination. To address this, we propose MOAT, a Multi-Agent Joint\nAlignment Tuning framework that improves agents collaboration through iterative\nalignment. MOAT alternates between two key stages: (1) Planning Agent\nAlignment, which optimizes the planning agent to generate subgoal sequences\nthat better guide the grounding agent; and (2) Grounding Agent Improving, which\nfine-tunes the grounding agent using diverse subgoal-action pairs generated by\nthe agent itself to enhance its generalization capablity. Theoretical analysis\nproves that MOAT ensures a non-decreasing and progressively convergent training\nprocess. Experiments across six benchmarks demonstrate that MOAT outperforms\nstate-of-the-art baselines, achieving average improvements of 3.1% on held-in\ntasks and 4.4% on held-out tasks."}
{"id": "2509.09650", "pdf": "https://arxiv.org/pdf/2509.09650.pdf", "abs": "https://arxiv.org/abs/2509.09650", "title": "All for One: LLMs Solve Mental Math at the Last Token With Information Transferred From Other Tokens", "authors": ["Siddarth Mamidanna", "Daking Rai", "Ziyu Yao", "Yilun Zhou"], "categories": ["cs.CL", "I.2.7"], "comment": "EMNLP 2025 Main Conference", "summary": "Large language models (LLMs) demonstrate proficiency across numerous\ncomputational tasks, yet their inner workings remain unclear. In theory, the\ncombination of causal self-attention and multilayer perceptron layers allows\nevery token to access and compute information based on all preceding tokens. In\npractice, to what extent are such operations present? In this paper, on mental\nmath tasks (i.e., direct math calculation via next-token prediction without\nexplicit reasoning), we investigate this question in three steps: inhibiting\ninput-specific token computations in the initial layers, restricting the routes\nof information transfer across token positions in the next few layers, and\nforcing all computation to happen at the last token in the remaining layers.\nWith two proposed techniques, Context-Aware Mean Ablation (CAMA) and\nAttention-Based Peeking (ABP), we identify an All-for-One subgraph (AF1) with\nhigh accuracy on a wide variety of mental math tasks, where meaningful\ncomputation occurs very late (in terms of layer depth) and only at the last\ntoken, which receives information of other tokens in few specific middle\nlayers. Experiments on a variety of models and arithmetic expressions show that\nthis subgraph is sufficient and necessary for high model performance, transfers\nacross different models, and works on a variety of input styles. Ablations on\ndifferent CAMA and ABP alternatives reveal their unique advantages over other\nmethods, which may be of independent interest."}
{"id": "2509.09660", "pdf": "https://arxiv.org/pdf/2509.09660.pdf", "abs": "https://arxiv.org/abs/2509.09660", "title": "Steering MoE LLMs via Expert (De)Activation", "authors": ["Mohsen Fayyaz", "Ali Modarressi", "Hanieh Deilamsalehy", "Franck Dernoncourt", "Ryan Rossi", "Trung Bui", "Hinrich Schütze", "Nanyun Peng"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Mixture-of-Experts (MoE) in Large Language Models (LLMs) routes each token\nthrough a subset of specialized Feed-Forward Networks (FFN), known as experts.\nWe present SteerMoE, a framework for steering MoE models by detecting and\ncontrolling behavior-linked experts. Our detection method identifies experts\nwith distinct activation patterns across paired inputs exhibiting contrasting\nbehaviors. By selectively (de)activating such experts during inference, we\ncontrol behaviors like faithfulness and safety without retraining or modifying\nweights. Across 11 benchmarks and 6 LLMs, our steering raises safety by up to\n+20% and faithfulness by +27%. In adversarial attack mode, it drops safety by\n-41% alone, and -100% when combined with existing jailbreak methods, bypassing\nall safety guardrails and exposing a new dimension of alignment faking hidden\nwithin experts."}
{"id": "2509.09675", "pdf": "https://arxiv.org/pdf/2509.09675.pdf", "abs": "https://arxiv.org/abs/2509.09675", "title": "CDE: Curiosity-Driven Exploration for Efficient Reinforcement Learning in Large Language Models", "authors": ["Runpeng Dai", "Linfeng Song", "Haolin Liu", "Zhenwen Liang", "Dian Yu", "Haitao Mi", "Zhaopeng Tu", "Rui Liu", "Tong Zheng", "Hongtu Zhu", "Dong Yu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "21 pages", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful paradigm\nfor enhancing the reasoning ability of Large Language Models (LLMs). Yet\ncurrent RLVR methods often explore poorly, leading to premature convergence and\nentropy collapse. To address this challenge, we introduce Curiosity-Driven\nExploration (CDE), a framework that leverages the model's own intrinsic sense\nof curiosity to guide exploration. We formalize curiosity with signals from\nboth the actor and the critic: for the actor, we use perplexity over its\ngenerated response, and for the critic, we use the variance of value estimates\nfrom a multi-head architecture. Both signals serve as an exploration bonus\nwithin the RLVR framework to guide the model. Our theoretical analysis shows\nthat the actor-wise bonus inherently penalizes overconfident errors and\npromotes diversity among correct responses; moreover, we connect the\ncritic-wise bonus to the well-established count-based exploration bonus in RL.\nEmpirically, our method achieves an approximate +3 point improvement over\nstandard RLVR using GRPO/PPO on AIME benchmarks. Further analysis identifies a\ncalibration collapse mechanism within RLVR, shedding light on common LLM\nfailure modes."}
{"id": "2509.08847", "pdf": "https://arxiv.org/pdf/2509.08847.pdf", "abs": "https://arxiv.org/abs/2509.08847", "title": "Automated Unity Game Template Generation from GDDs via NLP and Multi-Modal LLMs", "authors": ["Amna Hassan"], "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SE"], "comment": null, "summary": "This paper presents a novel framework for automated game template generation\nby transforming Game Design Documents (GDDs) into functional Unity game\nprototypes using Natural Language Processing (NLP) and multi-modal Large\nLanguage Models (LLMs). We introduce an end-to-end system that parses GDDs,\nextracts structured game specifications, and synthesizes Unity-compatible C#\ncode that implements the core mechanics, systems, and architecture defined in\nthe design documentation. Our approach combines a fine-tuned LLaMA-3 model\nspecialized for Unity code generation with a custom Unity integration package\nthat streamlines the implementation process. Evaluation results demonstrate\nsignificant improvements over baseline models, with our fine-tuned model\nachieving superior performance (4.8/5.0 average score) compared to\nstate-of-the-art LLMs across compilation success, GDD adherence, best practices\nadoption, and code modularity metrics. The generated templates demonstrate high\nadherence to GDD specifications across multiple game genres. Our system\neffectively addresses critical gaps in AI-assisted game development,\npositioning LLMs as valuable tools in streamlining the transition from game\ndesign to implementation."}
{"id": "2509.08854", "pdf": "https://arxiv.org/pdf/2509.08854.pdf", "abs": "https://arxiv.org/abs/2509.08854", "title": "A vibe coding learning design to enhance EFL students' talking to, through, and about AI", "authors": ["David James Woo", "Kai Guo", "Yangyang Yu"], "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": "15 pages, 12 figures", "summary": "This innovative practice article reports on the piloting of vibe coding\n(using natural language to create software applications with AI) for English as\na Foreign Language (EFL) education. We developed a human-AI meta-languaging\nframework with three dimensions: talking to AI (prompt engineering), talking\nthrough AI (negotiating authorship), and talking about AI (mental models of\nAI). Using backward design principles, we created a four-hour workshop where\ntwo students designed applications addressing authentic EFL writing challenges.\nWe adopted a case study methodology, collecting data from worksheets and video\nrecordings, think-aloud protocols, screen recordings, and AI-generated images.\nContrasting cases showed one student successfully vibe coding a functional\napplication cohering to her intended design, while another encountered\ntechnical difficulties with major gaps between intended design and actual\nfunctionality. Analysis reveals differences in students' prompt engineering\napproaches, suggesting different AI mental models and tensions in attributing\nauthorship. We argue that AI functions as a beneficial languaging machine, and\nthat differences in how students talk to, through, and about AI explain vibe\ncoding outcome variations. Findings indicate that effective vibe coding\ninstruction requires explicit meta-languaging scaffolding, teaching structured\nprompt engineering, facilitating critical authorship discussions, and\ndeveloping vocabulary for articulating AI mental models."}
{"id": "2509.08897", "pdf": "https://arxiv.org/pdf/2509.08897.pdf", "abs": "https://arxiv.org/abs/2509.08897", "title": "Recurrence Meets Transformers for Universal Multimodal Retrieval", "authors": ["Davide Caffagni", "Sara Sarto", "Marcella Cornia", "Lorenzo Baraldi", "Rita Cucchiara"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "comment": null, "summary": "With the rapid advancement of multimodal retrieval and its application in\nLLMs and multimodal LLMs, increasingly complex retrieval tasks have emerged.\nExisting methods predominantly rely on task-specific fine-tuning of\nvision-language models and are limited to single-modality queries or documents.\nIn this paper, we propose ReT-2, a unified retrieval model that supports\nmultimodal queries, composed of both images and text, and searches across\nmultimodal document collections where text and images coexist. ReT-2 leverages\nmulti-layer representations and a recurrent Transformer architecture with\nLSTM-inspired gating mechanisms to dynamically integrate information across\nlayers and modalities, capturing fine-grained visual and textual details. We\nevaluate ReT-2 on the challenging M2KR and M-BEIR benchmarks across different\nretrieval configurations. Results demonstrate that ReT-2 consistently achieves\nstate-of-the-art performance across diverse settings, while offering faster\ninference and reduced memory usage compared to prior approaches. When\nintegrated into retrieval-augmented generation pipelines, ReT-2 also improves\ndownstream performance on Encyclopedic-VQA and InfoSeek datasets. Our source\ncode and trained models are publicly available at:\nhttps://github.com/aimagelab/ReT-2"}
{"id": "2509.08919", "pdf": "https://arxiv.org/pdf/2509.08919.pdf", "abs": "https://arxiv.org/abs/2509.08919", "title": "Generative Engine Optimization: How to Dominate AI Search", "authors": ["Mahe Chen", "Xiaoxuan Wang", "Kaiwen Chen", "Nick Koudas"], "categories": ["cs.IR", "cs.CL", "cs.SI"], "comment": null, "summary": "The rapid adoption of generative AI-powered search engines like ChatGPT,\nPerplexity, and Gemini is fundamentally reshaping information retrieval, moving\nfrom traditional ranked lists to synthesized, citation-backed answers. This\nshift challenges established Search Engine Optimization (SEO) practices and\nnecessitates a new paradigm, which we term Generative Engine Optimization\n(GEO).\n  This paper presents a comprehensive comparative analysis of AI Search and\ntraditional web search (Google). Through a series of large-scale, controlled\nexperiments across multiple verticals, languages, and query paraphrases, we\nquantify critical differences in how these systems source information. Our key\nfindings reveal that AI Search exhibit a systematic and overwhelming bias\ntowards Earned media (third-party, authoritative sources) over Brand-owned and\nSocial content, a stark contrast to Google's more balanced mix. We further\ndemonstrate that AI Search services differ significantly from each other in\ntheir domain diversity, freshness, cross-language stability, and sensitivity to\nphrasing.\n  Based on these empirical results, we formulate a strategic GEO agenda. We\nprovide actionable guidance for practitioners, emphasizing the critical need\nto: (1) engineer content for machine scannability and justification, (2)\ndominate earned media to build AI-perceived authority, (3) adopt\nengine-specific and language-aware strategies, and (4) overcome the inherent\n\"big brand bias\" for niche players. Our work provides the foundational\nempirical analysis and a strategic framework for achieving visibility in the\nnew generative search landscape."}
{"id": "2509.09009", "pdf": "https://arxiv.org/pdf/2509.09009.pdf", "abs": "https://arxiv.org/abs/2509.09009", "title": "Open-sci-ref-0.01: open and reproducible reference baselines for language model and dataset comparison", "authors": ["Marianna Nezhurina", "Taishi Nakamura", "Timur Carstensen", "Niccolò Ajroldi", "Ville Komulainen", "David Salinas", "Jenia Jitsev"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Model weights and intermediate checkpoints are available at\n  \\url{https://huggingface.co/collections/open-sci/open-sci-ref-001-685905e598be658fbcebff4f};\n  code for reproducing training, evaluation and raw experiments data at\n  \\url{https://github.com/LAION-AI/open-sci-ref-0.01}", "summary": "We introduce open-sci-ref, a family of dense transformer models trained as\nresearch baselines across multiple model (0.13B to 1.7B parameters) and token\nscales (up to 1T) on 8 recent open reference datasets. Evaluating the models on\nvarious standardized benchmarks, our training runs set establishes reference\npoints that enable researchers to assess the sanity and quality of alternative\ntraining approaches across scales and datasets. Intermediate checkpoints allow\ncomparison and studying of the training dynamics. The established reference\nbaselines allow training procedures to be compared through their scaling\ntrends, aligning them on a common compute axis. Comparison of open reference\ndatasets reveals that training on NemoTron-CC HQ consistently outperforms other\nreference datasets, followed by DCLM-baseline and FineWeb-Edu. In addition to\nintermediate training checkpoints, the release includes logs, code, and\ndownstream evaluations to simplify reproduction, standardize comparison, and\nfacilitate future research."}
{"id": "2509.09014", "pdf": "https://arxiv.org/pdf/2509.09014.pdf", "abs": "https://arxiv.org/abs/2509.09014", "title": "COCO-Urdu: A Large-Scale Urdu Image-Caption Dataset with Multimodal Quality Estimation", "authors": ["Umair Hassan"], "categories": ["cs.CV", "cs.CL", "68T45 (Primary) 68T50 (Secondary)"], "comment": "17 pages, 3 figures, 3 tables. Dataset available at\n  https://huggingface.co/datasets/umairhassan02/urdu-translated-coco-captions-subset.\n  Scripts and notebooks to reproduce results available at\n  https://github.com/umair-hassan2/COCO-Urdu", "summary": "Urdu, spoken by over 250 million people, remains critically under-served in\nmultimodal and vision-language research. The absence of large-scale,\nhigh-quality datasets has limited the development of Urdu-capable systems and\nreinforced biases in multilingual vision-language models trained primarily on\nhigh-resource languages. To address this gap, we present COCO-Urdu, a\nlarge-scale image-caption dataset derived from MS COCO, containing 59,000\nimages and 319,000 Urdu captions selected through stratified sampling to\npreserve the original distribution. Captions were translated using SeamlessM4T\nv2 and validated with a hybrid multimodal quality estimation framework that\nintegrates COMET-Kiwi for translation quality, CLIP-based similarity for visual\ngrounding, and BERTScore with back-translation for semantic consistency;\nlow-scoring captions were iteratively refined using open-source large language\nmodels. We further benchmark COCO-Urdu on BLEU, SacreBLEU, and chrF, reporting\nconsistently strong results. To the best of our knowledge, COCO-Urdu is the\nlargest publicly available Urdu captioning dataset. By releasing both the\ndataset and the quality estimation pipeline, we aim to reduce language bias in\nmultimodal research and establish a foundation for inclusive vision-language\nsystems."}
{"id": "2509.09204", "pdf": "https://arxiv.org/pdf/2509.09204.pdf", "abs": "https://arxiv.org/abs/2509.09204", "title": "Bona fide Cross Testing Reveals Weak Spot in Audio Deepfake Detection Systems", "authors": ["Chin Yuen Kwok", "Jia Qi Yip", "Zhen Qiu", "Chi Hung Chi", "Kwok Yan Lam"], "categories": ["cs.SD", "cs.AI", "cs.CL"], "comment": "Published in Interspeech 2025", "summary": "Audio deepfake detection (ADD) models are commonly evaluated using datasets\nthat combine multiple synthesizers, with performance reported as a single Equal\nError Rate (EER). However, this approach disproportionately weights\nsynthesizers with more samples, underrepresenting others and reducing the\noverall reliability of EER. Additionally, most ADD datasets lack diversity in\nbona fide speech, often featuring a single environment and speech style (e.g.,\nclean read speech), limiting their ability to simulate real-world conditions.\nTo address these challenges, we propose bona fide cross-testing, a novel\nevaluation framework that incorporates diverse bona fide datasets and\naggregates EERs for more balanced assessments. Our approach improves robustness\nand interpretability compared to traditional evaluation methods. We benchmark\nover 150 synthesizers across nine bona fide speech types and release a new\ndataset to facilitate further research at\nhttps://github.com/cyaaronk/audio_deepfake_eval."}
{"id": "2509.09214", "pdf": "https://arxiv.org/pdf/2509.09214.pdf", "abs": "https://arxiv.org/abs/2509.09214", "title": "Identifying Key Features for Establishing Sustainable Agro-Tourism Centre: A Data Driven Approach", "authors": ["Alka Gadakh", "Vidya Kumbhar", "Sonal Khosla", "Kumar Karunendra"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Agro-tourism serves as a strategic economic model designed to facilitate\nrural development by diversifying income streams for local communities like\nfarmers while promoting the conservation of indigenous cultural heritage and\ntraditional agricultural practices. As a very booming subdomain of tourism,\nthere is a need to study the strategies for the growth of Agro-tourism in\ndetail. The current study has identified the important indicators for the\ngrowth and enhancement of agro-tourism. The study is conducted in two phases:\nidentification of the important indicators through a comprehensive literature\nreview and in the second phase state-of-the-art techniques were used to\nidentify the important indicators for the growth of agro-tourism. The\nindicators are also called features synonymously, the machine learning models\nfor feature selection were applied and it was observed that the Least Absolute\nShrinkage and Selection Operator (LASSO) method combined with, the machine\nLearning Classifiers such as Logistic Regression (LR), Decision Trees (DT),\nRandom Forest (RF) Tree, and Extreme Gradient Boosting (XGBOOST) models were\nused to suggest the growth of the agro-tourism. The results show that with the\nLASSO method, LR model gives the highest classification accuracy of 98% in\n70-30% train-test data followed by RF with 95% accuracy. Similarly, in the\n80-20% train-test data LR maintains the highest accuracy at 99%, while DT and\nXGBoost follow with 97% accuracy."}
{"id": "2509.09265", "pdf": "https://arxiv.org/pdf/2509.09265.pdf", "abs": "https://arxiv.org/abs/2509.09265", "title": "Harnessing Uncertainty: Entropy-Modulated Policy Gradients for Long-Horizon LLM Agents", "authors": ["Jiawei Wang", "Jiacai Liu", "Yuqian Fu", "Yingru Li", "Xintao Wang", "Yuan Lin", "Yu Yue", "Lin Zhang", "Yang Wang", "Ke Wang"], "categories": ["cs.LG", "cs.CL"], "comment": "ICLR 2026 Under review", "summary": "In long-horizon tasks, recent agents based on Large Language Models (LLMs)\nface a significant challenge that sparse, outcome-based rewards make it\ndifficult to assign credit to intermediate steps. Previous methods mainly focus\non creating dense reward signals to guide learning, either through traditional\nreinforcement learning techniques like inverse reinforcement learning or by\nusing Process Reward Models for step-by-step feedback. In this paper, we\nidentify a fundamental problem in the learning dynamics of LLMs: the magnitude\nof policy gradients is inherently coupled with the entropy, which leads to\ninefficient small updates for confident correct actions and potentially\ndestabilizes large updates for uncertain ones. To resolve this, we propose\nEntropy-Modulated Policy Gradients (EMPG), a framework that re-calibrates the\nlearning signal based on step-wise uncertainty and the final task outcome. EMPG\namplifies updates for confident correct actions, penalizes confident errors,\nand attenuates updates from uncertain steps to stabilize exploration. We\nfurther introduce a bonus term for future clarity that encourages agents to\nfind more predictable solution paths. Through comprehensive experiments on\nthree challenging agent tasks, WebShop, ALFWorld, and Deep Search, we\ndemonstrate that EMPG achieves substantial performance gains and significantly\noutperforms strong policy gradient baselines. Project page is at\nhttps://empgseed-seed.github.io/"}
{"id": "2509.09284", "pdf": "https://arxiv.org/pdf/2509.09284.pdf", "abs": "https://arxiv.org/abs/2509.09284", "title": "Tree-OPO: Off-policy Monte Carlo Tree-Guided Advantage Optimization for Multistep Reasoning", "authors": ["Bingning Huang", "Tu Nguyen", "Matthieu Zimmer"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Recent advances in reasoning with large language models (LLMs) have shown the\neffectiveness of Monte Carlo Tree Search (MCTS) for generating high-quality\nintermediate trajectories, particularly in math and symbolic domains. Inspired\nby this, we explore how MCTS-derived trajectories, traditionally used for\ntraining value or reward models, can be repurposed to improve policy\noptimization in preference-based reinforcement learning (RL). Specifically, we\nfocus on Group Relative Policy Optimization (GRPO), a recent algorithm that\nenables preference-consistent policy learning without value networks. We\npropose a staged GRPO training paradigm where completions are derived from\npartially revealed MCTS rollouts, introducing a novel tree-structured setting\nfor advantage estimation. This leads to a rich class of prefix-conditioned\nreward signals, which we analyze theoretically and empirically. Our initial\nresults indicate that while structured advantage estimation can stabilize\nupdates and better reflect compositional reasoning quality, challenges such as\nadvantage saturation and reward signal collapse remain. We propose heuristic\nand statistical solutions to mitigate these issues and discuss open challenges\nfor learning under staged or tree-like reward structures."}
{"id": "2509.09307", "pdf": "https://arxiv.org/pdf/2509.09307.pdf", "abs": "https://arxiv.org/abs/2509.09307", "title": "Can Multimodal LLMs See Materials Clearly? A Multimodal Benchmark on Materials Characterization", "authors": ["Zhengzhao Lai", "Youbin Zheng", "Zhenyang Cai", "Haonan Lyu", "Jinpu Yang", "Hongqing Liang", "Yan Hu", "Benyou Wang"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "comment": null, "summary": "Materials characterization is fundamental to acquiring materials information,\nrevealing the processing-microstructure-property relationships that guide\nmaterial design and optimization. While multimodal large language models\n(MLLMs) have recently shown promise in generative and predictive tasks within\nmaterials science, their capacity to understand real-world characterization\nimaging data remains underexplored. To bridge this gap, we present MatCha, the\nfirst benchmark for materials characterization image understanding, comprising\n1,500 questions that demand expert-level domain expertise. MatCha encompasses\nfour key stages of materials research comprising 21 distinct tasks, each\ndesigned to reflect authentic challenges faced by materials scientists. Our\nevaluation of state-of-the-art MLLMs on MatCha reveals a significant\nperformance gap compared to human experts. These models exhibit degradation\nwhen addressing questions requiring higher-level expertise and sophisticated\nvisual perception. Simple few-shot and chain-of-thought prompting struggle to\nalleviate these limitations. These findings highlight that existing MLLMs still\nexhibit limited adaptability to real-world materials characterization\nscenarios. We hope MatCha will facilitate future research in areas such as new\nmaterial discovery and autonomous scientific agents. MatCha is available at\nhttps://github.com/FreedomIntelligence/MatCha."}
{"id": "2509.09332", "pdf": "https://arxiv.org/pdf/2509.09332.pdf", "abs": "https://arxiv.org/abs/2509.09332", "title": "OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning", "authors": ["Yuecheng Liu", "Dafeng Chi", "Shiguang Wu", "Zhanguang Zhang", "Yuzheng Zhuang", "Bowen Yang", "He Zhu", "Lingfeng Zhang", "Pengwei Xie", "David Gamaliel Arcos Bravo", "Yingxue Zhang", "Jianye Hao", "Xingyue Quan"], "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Recent advances in multimodal large language models (MLLMs) have opened new\nopportunities for embodied intelligence, enabling multimodal understanding,\nreasoning, and interaction, as well as continuous spatial decision-making.\nNevertheless, current MLLM-based embodied systems face two critical\nlimitations. First, Geometric Adaptability Gap: models trained solely on 2D\ninputs or with hard-coded 3D geometry injection suffer from either insufficient\nspatial information or restricted 2D generalization, leading to poor\nadaptability across tasks with diverse spatial demands. Second, Embodiment\nConstraint Gap: prior work often neglects the physical constraints and\ncapacities of real robots, resulting in task plans that are theoretically valid\nbut practically infeasible.To address these gaps, we introduce OmniEVA -- an\nembodied versatile planner that enables advanced embodied reasoning and task\nplanning through two pivotal innovations: (1) a Task-Adaptive 3D Grounding\nmechanism, which introduces a gated router to perform explicit selective\nregulation of 3D fusion based on contextual requirements, enabling\ncontext-aware 3D grounding for diverse embodied tasks. (2) an Embodiment-Aware\nReasoning framework that jointly incorporates task goals and embodiment\nconstraints into the reasoning loop, resulting in planning decisions that are\nboth goal-directed and executable. Extensive experimental results demonstrate\nthat OmniEVA not only achieves state-of-the-art general embodied reasoning\nperformance, but also exhibits a strong ability across a wide range of\ndownstream scenarios. Evaluations of a suite of proposed embodied benchmarks,\nincluding both primitive and composite tasks, confirm its robust and versatile\nplanning capabilities. Project page: https://omnieva.github.io"}
{"id": "2509.09396", "pdf": "https://arxiv.org/pdf/2509.09396.pdf", "abs": "https://arxiv.org/abs/2509.09396", "title": "LLMs Don't Know Their Own Decision Boundaries: The Unreliability of Self-Generated Counterfactual Explanations", "authors": ["Harry Mayne", "Ryan Othniel Kearns", "Yushi Yang", "Andrew M. Bean", "Eoin Delaney", "Chris Russell", "Adam Mahdi"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted to EMNLP 2025 Main", "summary": "To collaborate effectively with humans, language models must be able to\nexplain their decisions in natural language. We study a specific type of\nself-explanation: self-generated counterfactual explanations (SCEs), where a\nmodel explains its prediction by modifying the input such that it would have\npredicted a different outcome. We evaluate whether LLMs can produce SCEs that\nare valid, achieving the intended outcome, and minimal, modifying the input no\nmore than necessary. When asked to generate counterfactuals, we find that LLMs\ntypically produce SCEs that are valid, but far from minimal, offering little\ninsight into their decision-making behaviour. Worryingly, when asked to\ngenerate minimal counterfactuals, LLMs typically make excessively small edits\nthat fail to change predictions. The observed validity-minimality trade-off is\nconsistent across several LLMs, datasets, and evaluation settings. Our findings\nsuggest that SCEs are, at best, an ineffective explainability tool and, at\nworst, can provide misleading insights into model behaviour. Proposals to\ndeploy LLMs in high-stakes settings must consider the impact of unreliable\nself-explanations on downstream decision-making. Our code is available at\nhttps://github.com/HarryMayne/SCEs."}
{"id": "2509.09631", "pdf": "https://arxiv.org/pdf/2509.09631.pdf", "abs": "https://arxiv.org/abs/2509.09631", "title": "DiFlow-TTS: Discrete Flow Matching with Factorized Speech Tokens for Low-Latency Zero-Shot Text-To-Speech", "authors": ["Ngoc-Son Nguyen", "Hieu-Nghia Huynh-Nguyen", "Thanh V. T. Tran", "Truong-Son Hy", "Van Nguyen"], "categories": ["cs.SD", "cs.CL", "cs.CV"], "comment": null, "summary": "Zero-shot Text-to-Speech (TTS) aims to synthesize high-quality speech that\nmimics the voice of an unseen speaker using only a short reference sample,\nrequiring not only speaker adaptation but also accurate modeling of prosodic\nattributes. Recent approaches based on language models, diffusion, and flow\nmatching have shown promising results in zero-shot TTS, but still suffer from\nslow inference and repetition artifacts. Discrete codec representations have\nbeen widely adopted for speech synthesis, and recent works have begun to\nexplore diffusion models in purely discrete settings, suggesting the potential\nof discrete generative modeling for speech synthesis. However, existing\nflow-matching methods typically embed these discrete tokens into a continuous\nspace and apply continuous flow matching, which may not fully leverage the\nadvantages of discrete representations. To address these challenges, we\nintroduce DiFlow-TTS, which, to the best of our knowledge, is the first model\nto explore purely Discrete Flow Matching for speech synthesis. DiFlow-TTS\nexplicitly models factorized speech attributes within a compact and unified\narchitecture. It leverages in-context learning by conditioning on textual\ncontent, along with prosodic and acoustic attributes extracted from a reference\nspeech, enabling effective attribute cloning in a zero-shot setting. In\naddition, the model employs a factorized flow prediction mechanism with\ndistinct heads for prosody and acoustic details, allowing it to learn\naspect-specific distributions. Experimental results demonstrate that DiFlow-TTS\nachieves promising performance in several key metrics, including naturalness,\nprosody, preservation of speaker style, and energy control. It also maintains a\ncompact model size and achieves low-latency inference, generating speech up to\n25.8 times faster than the latest existing baselines."}
{"id": "2509.09651", "pdf": "https://arxiv.org/pdf/2509.09651.pdf", "abs": "https://arxiv.org/abs/2509.09651", "title": "Retrieval-Augmented Generation for Reliable Interpretation of Radio Regulations", "authors": ["Zakaria El Kassimi", "Fares Fourati", "Mohamed-Slim Alouini"], "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG", "eess.SP"], "comment": null, "summary": "We study question answering in the domain of radio regulations, a legally\nsensitive and high-stakes area. We propose a telecom-specific\nRetrieval-Augmented Generation (RAG) pipeline and introduce, to our knowledge,\nthe first multiple-choice evaluation set for this domain, constructed from\nauthoritative sources using automated filtering and human validation. To assess\nretrieval quality, we define a domain-specific retrieval metric, under which\nour retriever achieves approximately 97% accuracy. Beyond retrieval, our\napproach consistently improves generation accuracy across all tested models. In\nparticular, while naively inserting documents without structured retrieval\nyields only marginal gains for GPT-4o (less than 1%), applying our pipeline\nresults in nearly a 12% relative improvement. These findings demonstrate that\ncarefully targeted grounding provides a simple yet strong baseline and an\neffective domain-specific solution for regulatory question answering. All code\nand evaluation scripts, along with our derived question-answer dataset, are\navailable at https://github.com/Zakaria010/Radio-RAG."}
{"id": "2509.09674", "pdf": "https://arxiv.org/pdf/2509.09674.pdf", "abs": "https://arxiv.org/abs/2509.09674", "title": "SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning", "authors": ["Haozhan Li", "Yuxin Zuo", "Jiale Yu", "Yuhao Zhang", "Zhaohui Yang", "Kaiyan Zhang", "Xuekai Zhu", "Yuchen Zhang", "Tianxing Chen", "Ganqu Cui", "Dehui Wang", "Dingxiang Luo", "Yuchen Fan", "Youbang Sun", "Jia Zeng", "Jiangmiao Pang", "Shanghang Zhang", "Yu Wang", "Yao Mu", "Bowen Zhou", "Ning Ding"], "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Vision-Language-Action (VLA) models have recently emerged as a powerful\nparadigm for robotic manipulation. Despite substantial progress enabled by\nlarge-scale pretraining and supervised fine-tuning (SFT), these models face two\nfundamental challenges: (i) the scarcity and high cost of large-scale\nhuman-operated robotic trajectories required for SFT scaling, and (ii) limited\ngeneralization to tasks involving distribution shift. Recent breakthroughs in\nLarge Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can\ndramatically enhance step-by-step reasoning capabilities, raising a natural\nquestion: Can RL similarly improve the long-horizon step-by-step action\nplanning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL\nframework tailored for VLA models. Building upon veRL, we introduce\nVLA-specific trajectory sampling, scalable parallelization, multi-environment\nrendering, and optimized loss computation. When applied to OpenVLA-OFT,\nSimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms $\\pi_0$\non RoboTwin 1.0\\&2.0 with the exploration-enhancing strategies we introduce.\nSimpleVLA-RL not only reduces dependence on large-scale data and enables robust\ngeneralization, but also remarkably surpasses SFT in real-world tasks.\nMoreover, we identify a novel phenomenon ``pushcut'' during RL training,\nwherein the policy discovers previously unseen patterns beyond those seen in\nthe previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL"}
{"id": "2509.09679", "pdf": "https://arxiv.org/pdf/2509.09679.pdf", "abs": "https://arxiv.org/abs/2509.09679", "title": "ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable Orthogonal Butterfly Transforms", "authors": ["Bingxin Xu", "Zhen Dong", "Oussama Elachqar", "Yuzhang Shang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Replace discrete Hadamard transforms with continuous Butterfly\n  transforms to facilitate the learning of rotation matrices in LLM\n  quantization", "summary": "Large language models require massive memory footprints, severely limiting\ndeployment on consumer hardware. Quantization reduces memory through lower\nnumerical precision, but extreme 2-bit quantization suffers from catastrophic\nperformance loss due to outliers in activations. Rotation-based methods such as\nQuIP and QuaRot apply orthogonal transforms to eliminate outliers before\nquantization, using computational invariance: $\\mathbf{y} = \\mathbf{Wx} =\n(\\mathbf{WQ}^T)(\\mathbf{Qx})$ for orthogonal $\\mathbf{Q}$. However, these\nmethods use fixed transforms--Hadamard matrices achieving optimal worst-case\ncoherence $\\mu = 1/\\sqrt{n}$--that cannot adapt to specific weight\ndistributions. We identify that different transformer layers exhibit distinct\noutlier patterns, motivating layer-adaptive rotations rather than\none-size-fits-all approaches. We propose ButterflyQuant, which replaces\nHadamard rotations with learnable butterfly transforms parameterized by\ncontinuous Givens rotation angles. Unlike Hadamard's discrete $\\{+1, -1\\}$\nentries that are non-differentiable and prohibit gradient-based learning,\nbutterfly transforms' continuous parameterization enables smooth optimization\nwhile guaranteeing orthogonality by construction. This orthogonal constraint\nensures theoretical guarantees in outlier suppression while achieving $O(n \\log\nn)$ computational complexity with only $\\frac{n \\log n}{2}$ learnable\nparameters. We further introduce a uniformity regularization on\npost-transformation activations to promote smoother distributions amenable to\nquantization. Learning requires only 128 calibration samples and converges in\nminutes on a single GPU--a negligible one-time cost. On LLaMA-2-7B with 2-bit\nquantization, ButterflyQuant achieves 15.4 perplexity versus 22.1 for QuaRot."}
{"id": "2509.09680", "pdf": "https://arxiv.org/pdf/2509.09680.pdf", "abs": "https://arxiv.org/abs/2509.09680", "title": "FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark", "authors": ["Rongyao Fang", "Aldrich Yu", "Chengqi Duan", "Linjiang Huang", "Shuai Bai", "Yuxuan Cai", "Kun Wang", "Si Liu", "Xihui Liu", "Hongsheng Li"], "categories": ["cs.CV", "cs.CL"], "comment": "Project page: https://flux-reason-6m.github.io/", "summary": "The advancement of open-source text-to-image (T2I) models has been hindered\nby the absence of large-scale, reasoning-focused datasets and comprehensive\nevaluation benchmarks, resulting in a performance gap compared to leading\nclosed-source systems. To address this challenge, We introduce FLUX-Reason-6M\nand PRISM-Bench (Precise and Robust Image Synthesis Measurement Benchmark).\nFLUX-Reason-6M is a massive dataset consisting of 6 million high-quality\nFLUX-generated images and 20 million bilingual (English and Chinese)\ndescriptions specifically designed to teach complex reasoning. The image are\norganized according to six key characteristics: Imagination, Entity, Text\nrendering, Style, Affection, and Composition, and design explicit Generation\nChain-of-Thought (GCoT) to provide detailed breakdowns of image generation\nsteps. The whole data curation takes 15,000 A100 GPU days, providing the\ncommunity with a resource previously unattainable outside of large industrial\nlabs. PRISM-Bench offers a novel evaluation standard with seven distinct\ntracks, including a formidable Long Text challenge using GCoT. Through\ncarefully designed prompts, it utilizes advanced vision-language models for\nnuanced human-aligned assessment of prompt-image alignment and image\naesthetics. Our extensive evaluation of 19 leading models on PRISM-Bench\nreveals critical performance gaps and highlights specific areas requiring\nimprovement. Our dataset, benchmark, and evaluation code are released to\ncatalyze the next wave of reasoning-oriented T2I generation. Project page:\nhttps://flux-reason-6m.github.io/ ."}
{"id": "2407.09447", "pdf": "https://arxiv.org/pdf/2407.09447.pdf", "abs": "https://arxiv.org/abs/2407.09447", "title": "ASTPrompter: Preference-Aligned Automated Language Model Red-Teaming to Generate Low-Perplexity Unsafe Prompts", "authors": ["Amelia F. Hardy", "Houjun Liu", "Allie Griffith", "Bernard Lange", "Duncan Eddy", "Mykel J. Kochenderfer"], "categories": ["cs.CL"], "comment": "8 pages, 7 pages of appendix, 3 tables, 4 figures", "summary": "Existing LLM red-teaming approaches prioritize high attack success rate,\noften resulting in high-perplexity prompts. This focus overlooks low-perplexity\nattacks that are more difficult to filter, more likely to arise during benign\nusage, and more impactful as negative downstream training examples. In\nresponse, we introduce ASTPrompter, a single-step optimization method that uses\ncontrastive preference learning to train an attacker to maintain low perplexity\nwhile achieving a high attack success rate (ASR). ASTPrompter achieves an\nattack success rate 5.1 times higher on Llama-8.1B while using inputs that are\n2.1 times more likely to occur according to the frozen LLM. Furthermore, our\nattack transfers to Mistral-7B, Qwen-7B, and TinyLlama in both black- and\nwhite-box settings. Lastly, by tuning a single hyperparameter in our method, we\ndiscover successful attack prefixes along an efficient frontier between ASR and\nperplexity, highlighting perplexity as a previously under-considered factor in\nred-teaming."}
{"id": "2411.08302", "pdf": "https://arxiv.org/pdf/2411.08302.pdf", "abs": "https://arxiv.org/abs/2411.08302", "title": "RED: Unleashing Token-Level Rewards from Holistic Feedback via Reward Redistribution", "authors": ["Jiahui Li", "Lin Li", "Tai-wei Chang", "Kun Kuang", "Long Chen", "Jun Zhou", "Cheng Yang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reinforcement learning from human feedback (RLHF) offers a promising approach\nto aligning large language models (LLMs) with human preferences. Typically, a\nreward model is trained or supplied to act as a proxy for humans in evaluating\ngenerated responses during the reinforcement training phase. However, current\nreward models operate as sequence-to-one models, allocating a single, sparse,\nand delayed reward to an entire output sequence. This approach may overlook the\nsignificant contributions of individual tokens toward the desired outcome. To\nthis end, we propose a more fine-grained, token-level guidance approach for RL\ntraining. Specifically, we introduce RED, a novel reward redistribition method\nthat evaluates and assigns specific credit to each token using an off-the-shelf\nreward model. Utilizing these fine-grained rewards enhances the model's\nunderstanding of language nuances, leading to more precise performance\nimprovements. Notably, our method does not require modifying the reward model\nor introducing additional training steps, thereby incurring minimal\ncomputational costs. Experimental results across diverse datasets and tasks\ndemonstrate the superiority of our approach."}
{"id": "2412.11538", "pdf": "https://arxiv.org/pdf/2412.11538.pdf", "abs": "https://arxiv.org/abs/2412.11538", "title": "MERaLiON-SpeechEncoder: Towards a Speech Foundation Model for Singapore and Beyond", "authors": ["Muhammad Huzaifah", "Geyu Lin", "Tianchi Liu", "Hardik B. Sailor", "Kye Min Tan", "Tarun K. Vangani", "Qiongqiong Wang", "Jeremy H. M. Wong", "Jinyang Wu", "Nancy F. Chen", "Ai Ti Aw"], "categories": ["cs.CL", "cs.AI", "eess.AS"], "comment": null, "summary": "This technical report describes the MERaLiON-SpeechEncoder, a foundation\nmodel designed to support a wide range of downstream speech applications.\nDeveloped as part of Singapore's National Multimodal Large Language Model\nProgramme, the MERaLiON-SpeechEncoder is tailored to address the speech\nprocessing needs in Singapore and the surrounding Southeast Asian region. The\nmodel currently supports mainly English, including the variety spoken in\nSingapore. We are actively expanding our datasets to gradually cover other\nlanguages in subsequent releases. The MERaLiON-SpeechEncoder was pre-trained\nfrom scratch on 200,000 hours of unlabelled speech data using a self-supervised\nlearning approach based on masked language modelling. We describe our training\nprocedure and hyperparameter tuning experiments in detail below. Our evaluation\ndemonstrates improvements to spontaneous and Singapore speech benchmarks for\nspeech recognition, while remaining competitive to other state-of-the-art\nspeech encoders across ten other speech tasks. We commit to releasing our\nmodel, supporting broader research endeavours, both in Singapore and beyond."}
{"id": "2501.02348", "pdf": "https://arxiv.org/pdf/2501.02348.pdf", "abs": "https://arxiv.org/abs/2501.02348", "title": "Thinking with Many Minds: Using Large Language Models for Multi-Perspective Problem-Solving", "authors": ["Sanghyun Park", "Boris Maciejovsky", "Phanish Puranam"], "categories": ["cs.CL", "cs.HC"], "comment": "36 pages, 1 appendix", "summary": "Complex problem-solving requires cognitive flexibility--the capacity to\nentertain multiple perspectives while preserving their distinctiveness. This\nflexibility replicates the \"wisdom of crowds\" within a single individual,\nallowing them to \"think with many minds.\" While mental simulation enables\nimagined deliberation, cognitive constraints limit its effectiveness. We\npropose synthetic deliberation, a Large Language Model (LLM)-based method that\nsimulates discourse between agents embodying diverse perspectives, as a\nsolution. Using a custom GPT-based model, we showcase its benefits: concurrent\nprocessing of multiple viewpoints without cognitive degradation, parallel\nexploration of perspectives, and precise control over viewpoint synthesis. By\nexternalizing the deliberative process and distributing cognitive labor between\nparallel search and integration, synthetic deliberation transcends mental\nsimulation's limitations. This approach shows promise for strategic planning,\npolicymaking, and conflict resolution."}
{"id": "2502.01523", "pdf": "https://arxiv.org/pdf/2502.01523.pdf", "abs": "https://arxiv.org/abs/2502.01523", "title": "CondAmbigQA: A Benchmark and Dataset for Conditional Ambiguous Question Answering", "authors": ["Zongxi Li", "Yang Li", "Haoran Xie", "S. Joe Qin"], "categories": ["cs.CL"], "comment": "Accepted by EMNLP 2025 (Main Conference)", "summary": "Users often assume that large language models (LLMs) share their cognitive\nalignment of context and intent, leading them to omit critical information in\nquestion-answering (QA) and produce ambiguous queries. Responses based on\nmisaligned assumptions may be perceived as hallucinations. Therefore,\nidentifying possible implicit assumptions is crucial in QA. To address this\nfundamental challenge, we propose Conditional Ambiguous Question-Answering\n(CondAmbigQA), a benchmark comprising 2,000 ambiguous queries and\ncondition-aware evaluation metrics. Our study pioneers \"conditions\" as explicit\ncontextual constraints that resolve ambiguities in QA tasks through\nretrieval-based annotation, where retrieved Wikipedia fragments help identify\npossible interpretations for a given query and annotate answers accordingly.\nExperiments demonstrate that models considering conditions before answering\nimprove answer accuracy by 11.75%, with an additional 7.15% gain when\nconditions are explicitly provided. These results highlight that apparent\nhallucinations may stem from inherent query ambiguity rather than model\nfailure, and demonstrate the effectiveness of condition reasoning in QA,\nproviding researchers with tools for rigorous evaluation."}
{"id": "2502.02787", "pdf": "https://arxiv.org/pdf/2502.02787.pdf", "abs": "https://arxiv.org/abs/2502.02787", "title": "SimMark: A Robust Sentence-Level Similarity-Based Watermarking Algorithm for Large Language Models", "authors": ["Amirhossein Dabiriaghdam", "Lele Wang"], "categories": ["cs.CL", "cs.CR", "cs.CY", "cs.LG"], "comment": "Accepted to EMNLP 25 main", "summary": "The widespread adoption of large language models (LLMs) necessitates reliable\nmethods to detect LLM-generated text. We introduce SimMark, a robust\nsentence-level watermarking algorithm that makes LLMs' outputs traceable\nwithout requiring access to model internals, making it compatible with both\nopen and API-based LLMs. By leveraging the similarity of semantic sentence\nembeddings combined with rejection sampling to embed detectable statistical\npatterns imperceptible to humans, and employing a soft counting mechanism,\nSimMark achieves robustness against paraphrasing attacks. Experimental results\ndemonstrate that SimMark sets a new benchmark for robust watermarking of\nLLM-generated content, surpassing prior sentence-level watermarking techniques\nin robustness, sampling efficiency, and applicability across diverse domains,\nall while maintaining the text quality and fluency."}
{"id": "2502.11115", "pdf": "https://arxiv.org/pdf/2502.11115.pdf", "abs": "https://arxiv.org/abs/2502.11115", "title": "Are Generative Models Underconfident? Better Quality Estimation with Boosted Model Probability", "authors": ["Tu Anh Dinh", "Jan Niehues"], "categories": ["cs.CL", "I.2.7"], "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "Quality Estimation (QE) is estimating quality of the model output during\ninference when the ground truth is not available. Deriving output quality from\nthe models' output probability is the most trivial and low-effort way. However,\nwe show that the output probability of text-generation models can appear\nunderconfident. At each output step, there can be multiple correct options,\nmaking the probability distribution spread out more. Thus, lower probability\ndoes not necessarily mean lower output quality. Due to this observation, we\npropose a QE approach called BoostedProb, which boosts the model's confidence\nin cases where there are multiple viable output options. With no increase in\ncomplexity, BoostedProb is notably better than raw model probability in\ndifferent settings, achieving on average +0.194 improvement in Pearson\ncorrelation to ground-truth quality. It also comes close to or outperforms more\ncostly approaches like supervised or ensemble-based QE in certain settings."}
{"id": "2502.12932", "pdf": "https://arxiv.org/pdf/2502.12932.pdf", "abs": "https://arxiv.org/abs/2502.12932", "title": "Culturally-Nuanced Story Generation for Reasoning in Low-Resource Languages: The Case of Javanese and Sundanese", "authors": ["Salsabila Zahirah Pranida", "Rifo Ahmad Genadi", "Fajri Koto"], "categories": ["cs.CL", "68T50", "I.2.7"], "comment": null, "summary": "Culturally grounded commonsense reasoning is underexplored in low-resource\nlanguages due to scarce data and costly native annotation. We test whether\nlarge language models (LLMs) can generate culturally nuanced narratives for\nsuch settings. Focusing on Javanese and Sundanese, we compare three data\ncreation strategies: (1) LLM-assisted stories prompted with cultural cues, (2)\nmachine translation from Indonesian benchmarks, and (3) native-written stories.\nHuman evaluation finds LLM stories match natives on cultural fidelity but lag\nin coherence and correctness. We fine-tune models on each dataset and evaluate\non a human-authored test set for classification and generation. LLM-generated\ndata yields higher downstream performance than machine-translated and\nIndonesian human-authored training data. We release a high-quality benchmark of\nculturally grounded commonsense stories in Javanese and Sundanese to support\nfuture work."}
{"id": "2502.18108", "pdf": "https://arxiv.org/pdf/2502.18108.pdf", "abs": "https://arxiv.org/abs/2502.18108", "title": "Uncertainty Quantification in Retrieval Augmented Question Answering", "authors": ["Laura Perez-Beltrachini", "Mirella Lapata"], "categories": ["cs.CL"], "comment": "TMLR (09/2025)", "summary": "Retrieval augmented Question Answering (QA) helps QA models overcome\nknowledge gaps by incorporating retrieved evidence, typically a set of\npassages, alongside the question at test time. Previous studies show that this\napproach improves QA performance and reduces hallucinations, without, however,\nassessing whether the retrieved passages are indeed useful at answering\ncorrectly. In this work, we propose to quantify the uncertainty of a QA model\nvia estimating the utility of the passages it is provided with. We train a\nlightweight neural model to predict passage utility for a target QA model and\nshow that while simple information theoretic metrics can predict answer\ncorrectness up to a certain extent, our approach efficiently approximates or\noutperforms more expensive sampling-based methods. Code and data are available\nat https://github.com/lauhaide/ragu."}
{"id": "2502.19279", "pdf": "https://arxiv.org/pdf/2502.19279.pdf", "abs": "https://arxiv.org/abs/2502.19279", "title": "CritiQ: Mining Data Quality Criteria from Human Preferences", "authors": ["Honglin Guo", "Kai Lv", "Qipeng Guo", "Tianyi Liang", "Zhiheng Xi", "Demin Song", "Qiuyinzhe Zhang", "Yu Sun", "Kai Chen", "Xipeng Qiu", "Tao Gui"], "categories": ["cs.CL"], "comment": "to be published in ACL 2025, Code is available at\n  https://github.com/KYLN24/CritiQ", "summary": "Language model heavily depends on high-quality data for optimal performance.\nExisting approaches rely on manually designed heuristics, the perplexity of\nexisting models, training classifiers, or careful prompt engineering, which\nrequire significant expert experience and human annotation effort while\nintroduce biases. We introduce CritiQ, a novel data selection method that\nautomatically mines criteria from human preferences for data quality with only\n~30 human-annotated pairs and performs efficient data selection. The main\ncomponent, CritiQ Flow, employs a manager agent to evolve quality criteria and\nworker agents to make pairwise judgments. We build a knowledge base that\nextracts quality criteria from previous work to boost CritiQ Flow. Compared to\nperplexity- and classifier- based methods, verbal criteria are more\ninterpretable and possess reusable value. After deriving the criteria, we train\nthe CritiQ Scorer to give quality scores and perform efficient data selection.\nWe demonstrate the effectiveness of our method in the code, math, and logic\ndomains, achieving high accuracy on human-annotated test sets. To validate the\nquality of the selected data, we continually train Llama 3.1 models and observe\nimproved performance on downstream tasks compared to uniform sampling. Ablation\nstudies validate the benefits of the knowledge base and the reflection process.\nWe analyze how criteria evolve and the effectiveness of majority voting."}
{"id": "2502.19860", "pdf": "https://arxiv.org/pdf/2502.19860.pdf", "abs": "https://arxiv.org/abs/2502.19860", "title": "MIND: Towards Immersive Psychological Healing with Multi-agent Inner Dialogue", "authors": ["Yujia Chen", "Changsong Li", "Yiming Wang", "Tianjie Ju", "Qingqing Xiao", "Nan Zhang", "Zifan Kong", "Peng Wang", "Binyu Yan"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by EMNLP 2025 Findings", "summary": "Mental health issues are worsening in today's competitive society, such as\ndepression and anxiety. Traditional healings like counseling and chatbots fail\nto engage effectively, they often provide generic responses lacking emotional\ndepth. Although large language models (LLMs) have the potential to create more\nhuman-like interactions, they still struggle to capture subtle emotions. This\nrequires LLMs to be equipped with human-like adaptability and warmth. To fill\nthis gap, we propose the MIND (Multi-agent INner Dialogue), a novel paradigm\nthat provides more immersive psychological healing environments. Considering\nthe strong generative and role-playing ability of LLM agents, we predefine an\ninteractive healing framework and assign LLM agents different roles within the\nframework to engage in interactive inner dialogues with users, thereby\nproviding an immersive healing experience. We conduct extensive human\nexperiments in various real-world healing dimensions, and find that MIND\nprovides a more user-friendly experience than traditional paradigms. This\ndemonstrates that MIND effectively leverages the significant potential of LLMs\nin psychological healing."}
{"id": "2503.21544", "pdf": "https://arxiv.org/pdf/2503.21544.pdf", "abs": "https://arxiv.org/abs/2503.21544", "title": "SWI: Speaking with Intent in Large Language Models", "authors": ["Yuwei Yin", "EunJeong Hwang", "Giuseppe Carenini"], "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "comment": "Code: https://github.com/YuweiYin/SWI", "summary": "Intent, typically clearly formulated and planned, functions as a cognitive\nframework for communication and problem-solving. This paper introduces the\nconcept of Speaking with Intent (SWI) in large language models (LLMs), where\nthe explicitly generated intent encapsulates the model's underlying intention\nand provides high-level planning to guide subsequent analysis and action. By\nemulating deliberate and purposeful thoughts in the human mind, SWI is\nhypothesized to enhance the reasoning capabilities and generation quality of\nLLMs. Extensive experiments on text summarization, multi-task question\nanswering, and mathematical reasoning benchmarks consistently demonstrate the\neffectiveness and generalizability of Speaking with Intent over direct\ngeneration without explicit intent. Further analysis corroborates the\ngeneralizability of SWI under different experimental settings. Moreover, human\nevaluations verify the coherence, effectiveness, and interpretability of the\nintent produced by SWI. The promising results in enhancing LLMs with explicit\nintents pave a new avenue for boosting LLMs' generation and reasoning abilities\nwith cognitive notions."}
{"id": "2503.21961", "pdf": "https://arxiv.org/pdf/2503.21961.pdf", "abs": "https://arxiv.org/abs/2503.21961", "title": "Entropy-Gated Branching for Efficient Test-Time Reasoning", "authors": ["Xianzhi Li", "Ethan Callanan", "Abdellah Ghassel", "Xiaodan Zhu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Test-time compute methods like beam search can significantly improve the\nreasoning capabilities and problem-solving accuracy of large language models.\nHowever, these approaches require substantially increased computational\nresources, with most computation wasted on exploring low-diversity branches\nwhere the model already exhibits high confidence. We observe that a small\nsubset of uncertain reasoning steps has a disproportionately large impact on\nfinal prediction accuracy, and branching at these points tends to yield\nhigher-quality and more diverse candidate reasoning steps. Therefore, we\nintroduce Entropy-Gated Branching: a novel inference technique that dynamically\nallocates computational resources by selectively expanding prediction sequences\nonly at points of high uncertainty. Our method leverages entropy as a gating\nmechanism to identify when branching is most beneficial, coupled with an\nexternal feedback model to rank and prune candidate branches. Empirical results\non mathematical and financial reasoning benchmarks show that this strategy\nimproves accuracy by 22.6% over standard inference while operating 37% faster\nthan conventional beam search with similar or higher performance. Our results\nshow that dynamic resource allocation during inference can substantially\nimprove both efficiency and effectiveness, offering a more scalable pathway to\nenhanced LLM reasoning capabilities."}
{"id": "2504.00132", "pdf": "https://arxiv.org/pdf/2504.00132.pdf", "abs": "https://arxiv.org/abs/2504.00132", "title": "Contextualize-then-Aggregate: Circuits for In-Context Learning in Gemma-2 2B", "authors": ["Aleksandra Bakalova", "Yana Veitsman", "Xinting Huang", "Michael Hahn"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "In-Context Learning (ICL) is an intriguing ability of large language models\n(LLMs). Despite a substantial amount of work on its behavioral aspects and how\nit emerges in miniature setups, it remains unclear which mechanism assembles\ntask information from the individual examples in a fewshot prompt. We use\ncausal interventions to identify information flow in Gemma-2 2B for five\nnaturalistic ICL tasks. We find that the model infers task information using a\ntwo-step strategy we call contextualize-then-aggregate: In the lower layers,\nthe model builds up representations of individual fewshot examples, which are\ncontextualized by preceding examples through connections between fewshot input\nand output tokens across the sequence. In the higher layers, these\nrepresentations are aggregated to identify the task and prepare prediction of\nthe next output. The importance of the contextualization step differs between\ntasks, and it may become more important in the presence of ambiguous examples.\nOverall, by providing rigorous causal analysis, our results shed light on the\nmechanisms through which ICL happens in language models."}
{"id": "2505.00039", "pdf": "https://arxiv.org/pdf/2505.00039.pdf", "abs": "https://arxiv.org/abs/2505.00039", "title": "An Ontology-Driven Graph RAG for Legal Norms: A Structural, Temporal, and Deterministic Approach", "authors": ["Hudson de Martim"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Major revision for clarity and academic precision. Updated title and\n  abstract. Refined core terminology, contributions, related work, and shifted\n  the implementation to a conceptual architecture. Added new arguments to\n  strengthen the paper's thesis", "summary": "Retrieval-Augmented Generation (RAG) systems in the legal domain face a\ncritical challenge: standard, flat-text retrieval is blind to the hierarchical,\ndiachronic, and causal structure of law, leading to anachronistic and\nunreliable answers. This paper introduces the Structure-Aware Temporal Graph\nRAG (SAT-Graph RAG), an ontology-driven framework designed to overcome these\nlimitations by explicitly modeling the formal structure and diachronic nature\nof legal norms. We ground our knowledge graph in a formal, LRMoo-inspired model\nthat distinguishes abstract legal Works from their versioned Expressions. We\nmodel temporal states as efficient aggregations that reuse the versioned\nexpressions (CTVs) of unchanged components, and we reify legislative events as\nfirst-class Action nodes to make causality explicit and queryable. This\nstructured backbone enables a unified, planner-guided query strategy that\napplies explicit policies to deterministically resolve complex requests for (i)\npoint-in-time retrieval, (ii) hierarchical impact analysis, and (iii) auditable\nprovenance reconstruction. Through a case study on the Brazilian Constitution,\nwe demonstrate how this approach provides a verifiable, temporally-correct\nsubstrate for LLMs, enabling higher-order analytical capabilities while\ndrastically reducing the risk of factual errors. The result is a practical\nframework for building more trustworthy and explainable legal AI systems."}
{"id": "2505.00147", "pdf": "https://arxiv.org/pdf/2505.00147.pdf", "abs": "https://arxiv.org/abs/2505.00147", "title": "AdaptMI: Adaptive Skill-based In-context Math Instruction for Small Language Models", "authors": ["Yinghui He", "Abhishek Panigrahi", "Yong Lin", "Sanjeev Arora"], "categories": ["cs.CL"], "comment": null, "summary": "In-context learning (ICL) allows a language model to improve its\nproblem-solving capability when provided with suitable information in context.\nSince the choice of in-context information can be determined based on the\nproblem itself, in-context learning is analogous to human learning from\nteachers in a classroom. Recent works (Didolkar et al., 2024a; 2024b) show that\nICL performance can be improved by leveraging a frontier large language model's\n(LLM) ability to predict required skills to solve a problem, popularly referred\nto as an LLM's metacognition, and using the recommended skills to construct\nnecessary in-context examples. While this skill-based strategy boosts ICL\nperformance in larger models, its gains on small language models (SLMs) have\nbeen minimal, highlighting a performance gap in ICL capabilities. We\ninvestigate this gap and show that skill-based prompting can hurt SLM\nperformance on easy questions by introducing unnecessary information, akin to\ncognitive overload. To address this, we introduce AdaptMI, an adaptive approach\nto selecting skill-based in-context Math Instructions for SLMs. Inspired by\ncognitive load theory from human pedagogy, our method only introduces\nskill-based examples when the model performs poorly. We further propose\nAdaptMI+, which adds examples targeted to the specific skills missing from the\nmodel's responses. On 5-shot evaluations across popular math benchmarks and\nfive SLMs (1B--7B; Qwen, Llama), AdaptMI+ improves accuracy by up to 6% over\nnaive skill-based strategies."}
{"id": "2506.04077", "pdf": "https://arxiv.org/pdf/2506.04077.pdf", "abs": "https://arxiv.org/abs/2506.04077", "title": "A Novel Data Augmentation Approach for Automatic Speaking Assessment on Opinion Expressions", "authors": ["Chung-Chun Wang", "Jhen-Ke Lin", "Hao-Chien Lu", "Hong-Yun Lin", "Berlin Chen"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "submitted to the ISCA SLaTE-2025 Workshop", "summary": "Automated speaking assessment (ASA) on opinion expressions is often hampered\nby the scarcity of labeled recordings, which restricts prompt diversity and\nundermines scoring reliability. To address this challenge, we propose a novel\ntraining paradigm that leverages a large language models (LLM) to generate\ndiverse responses of a given proficiency level, converts responses into\nsynthesized speech via speaker-aware text-to-speech synthesis, and employs a\ndynamic importance loss to adaptively reweight training instances based on\nfeature distribution differences between synthesized and real speech.\nSubsequently, a multimodal large language model integrates aligned textual\nfeatures with speech signals to predict proficiency scores directly.\nExperiments conducted on the LTTC dataset show that our approach outperforms\nmethods relying on real data or conventional augmentation, effectively\nmitigating low-resource constraints and enabling ASA on opinion expressions\nwith cross-modal information."}
{"id": "2506.05121", "pdf": "https://arxiv.org/pdf/2506.05121.pdf", "abs": "https://arxiv.org/abs/2506.05121", "title": "The NTNU System at the S&I Challenge 2025 SLA Open Track", "authors": ["Hong-Yun Lin", "Tien-Hong Lo", "Yu-Hsuan Fang", "Jhen-Ke Lin", "Chung-Chun Wang", "Hao-Chien Lu", "Berlin Chen"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "submitted to the ISCA SLaTE-2025 Workshop", "summary": "A recent line of research on spoken language assessment (SLA) employs neural\nmodels such as BERT and wav2vec 2.0 (W2V) to evaluate speaking proficiency\nacross linguistic and acoustic modalities. Although both models effectively\ncapture features relevant to oral competence, each exhibits modality-specific\nlimitations. BERT-based methods rely on ASR transcripts, which often fail to\ncapture prosodic and phonetic cues for SLA. In contrast, W2V-based methods\nexcel at modeling acoustic features but lack semantic interpretability. To\novercome these limitations, we propose a system that integrates W2V with Phi-4\nmultimodal large language model (MLLM) through a score fusion strategy. The\nproposed system achieves a root mean square error (RMSE) of 0.375 on the\nofficial test set of the Speak & Improve Challenge 2025, securing second place\nin the competition. For comparison, the RMSEs of the top-ranked, third-ranked,\nand official baseline systems are 0.364, 0.384, and 0.444, respectively."}
{"id": "2506.06485", "pdf": "https://arxiv.org/pdf/2506.06485.pdf", "abs": "https://arxiv.org/abs/2506.06485", "title": "Task Matters: Knowledge Requirements Shape LLM Responses to Context-Memory Conflict", "authors": ["Kaiser Sun", "Fan Bai", "Mark Dredze"], "categories": ["cs.CL", "cs.AI"], "comment": "Major revision", "summary": "Large Language Models require both contextual knowledge and parametric\nmemory, but these sources can disagree. Prior investigations on contextual\nquestion answering tasks report a preference toward parametric knowledge under\nconflict, yet they focus almost exclusively on tasks that should always rely on\nthe given passage, leaving open how this behavior manifests when tasks demand\ndifferent amounts and kinds of knowledge. We study this question with a\nmodel-agnostic diagnostic framework that (i) automatically detects\ndisagreements between a model's beliefs and a curated knowledge set, and (ii)\ninjects controlled conflicts into tasks. The resulting datasets span two\northogonal dimensions: task knowledge reliance and conflict plausibility.\nEvaluating representative open-source LLMs, we find that: (1) performance\ndegradation from conflict correlates with a task's knowledge reliance; (2)\nexplanatory rationales and simple reiteration both increase context\nreliance-helpful for context-only tasks but harmful when parametric knowledge\nshould dominate; (3) These behaviors raise concerns about the validity of\nmodel-based evaluation and underscore the need to account for knowledge\nconflict in the deployment of LLMs."}
{"id": "2506.11095", "pdf": "https://arxiv.org/pdf/2506.11095.pdf", "abs": "https://arxiv.org/abs/2506.11095", "title": "Persistent Homology of Topic Networks for the Prediction of Reader Curiosity", "authors": ["Manuel D. S. Hopp", "Vincent Labatut", "Arthur Amalvy", "Richard Dufour", "Hannah Stone", "Hayley Jach", "Kou Murayama"], "categories": ["cs.CL", "cs.AI"], "comment": "Original paper with an improved and extended appendix", "summary": "Reader curiosity, the drive to seek information, is crucial for textual\nengagement, yet remains relatively underexplored in NLP. Building on\nLoewenstein's Information Gap Theory, we introduce a framework that models\nreader curiosity by quantifying semantic information gaps within a text's\nsemantic structure. Our approach leverages BERTopic-inspired topic modeling and\npersistent homology to analyze the evolving topology (connected components,\ncycles, voids) of a dynamic semantic network derived from text segments,\ntreating these features as proxies for information gaps. To empirically\nevaluate this pipeline, we collect reader curiosity ratings from participants\n(n = 49) as they read S. Collins's ''The Hunger Games'' novel. We then use the\ntopological features from our pipeline as independent variables to predict\nthese ratings, and experimentally show that they significantly improve\ncuriosity prediction compared to a baseline model (73% vs. 30% explained\ndeviance), validating our approach. This pipeline offers a new computational\nmethod for analyzing text structure and its relation to reader engagement."}
{"id": "2508.19740", "pdf": "https://arxiv.org/pdf/2508.19740.pdf", "abs": "https://arxiv.org/abs/2508.19740", "title": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear Hashing-based KV Cache Retrieval", "authors": ["Wenhao Li", "Yuxin Zhang", "Gen Luo", "Haiyuan Wan", "Ziyang Gong", "Fei Chao", "Rongrong Ji"], "categories": ["cs.CL"], "comment": null, "summary": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding."}
{"id": "2508.19813", "pdf": "https://arxiv.org/pdf/2508.19813.pdf", "abs": "https://arxiv.org/abs/2508.19813", "title": "T2R-bench: A Benchmark for Generating Article-Level Reports from Real World Industrial Tables", "authors": ["Jie Zhang", "Changzai Pan", "Kaiwen Wei", "Sishi Xiong", "Yu Zhao", "Xiangyu Li", "Jiaxin Peng", "Xiaoyan Gu", "Jian Yang", "Wenhan Chang", "Zhenhe Wu", "Jiang Zhong", "Shuangyong Song", "Yongxiang Li", "Xuelong Li"], "categories": ["cs.CL"], "comment": null, "summary": "Extensive research has been conducted to explore the capabilities of large\nlanguage models (LLMs) in table reasoning. However, the essential task of\ntransforming tables information into reports remains a significant challenge\nfor industrial applications. This task is plagued by two critical issues: 1)\nthe complexity and diversity of tables lead to suboptimal reasoning outcomes;\nand 2) existing table benchmarks lack the capacity to adequately assess the\npractical application of this task. To fill this gap, we propose the\ntable-to-report task and construct a bilingual benchmark named T2R-bench, where\nthe key information flow from the tables to the reports for this task. The\nbenchmark comprises 457 industrial tables, all derived from real-world\nscenarios and encompassing 19 industry domains as well as 4 types of industrial\ntables. Furthermore, we propose an evaluation criteria to fairly measure the\nquality of report generation. The experiments on 25 widely-used LLMs reveal\nthat even state-of-the-art models like Deepseek-R1 only achieves performance\nwith 62.71 overall score, indicating that LLMs still have room for improvement\non T2R-bench."}
{"id": "2509.06806", "pdf": "https://arxiv.org/pdf/2509.06806.pdf", "abs": "https://arxiv.org/abs/2509.06806", "title": "MachineLearningLM: Scaling Many-shot In-context Learning via Continued Pretraining", "authors": ["Haoyu Dong", "Pengkun Zhang", "Mingzhe Lu", "Yanzhen Shen", "Guolin Ke"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) possess broad world knowledge and strong\ngeneral-purpose reasoning ability, yet they struggle to learn from many\nin-context examples on standard machine learning (ML) tasks, that is, to\nleverage many-shot demonstrations purely via in-context learning (ICL) without\ngradient descent. We introduce MachineLearningLM, a portable\ncontinued-pretraining framework that equips a general-purpose LLM with robust\nin-context ML capability while preserving its general knowledge and reasoning\nfor broader chat workflows.\n  Our pretraining procedure synthesizes ML tasks from millions of structural\ncausal models (SCMs), spanning shot counts up to 1,024. We begin with a\nrandom-forest teacher, distilling tree-based decision strategies into the LLM\nto strengthen robustness in numerical modeling. All tasks are serialized with a\ntoken-efficient prompt, enabling 3x to 6x more examples per context window and\ndelivering up to 50x amortized throughput via batch inference.\n  Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8),\nMachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an\naverage of about 15% on out-of-distribution tabular classification across\nfinance, physics, biology, and healthcare domains. It exhibits a striking\nmany-shot scaling law: accuracy increases monotonically as in-context\ndemonstrations grow from 8 to 1,024. Without any task-specific training, it\nattains random-forest-level accuracy across hundreds of shots. General chat\ncapabilities, including knowledge and reasoning, are preserved: it achieves\n75.4% on MMLU."}
{"id": "2509.07370", "pdf": "https://arxiv.org/pdf/2509.07370.pdf", "abs": "https://arxiv.org/abs/2509.07370", "title": "PersonaFuse: A Personality Activation-Driven Framework for Enhancing Human-LLM Interactions", "authors": ["Yixuan Tang", "Yi Yang", "Ahmed Abbasi"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) demonstrate remarkable\ncapabilities across various fields. These developments have led to more direct\ncommunication between humans and LLMs in various situations, such as social\ncompanionship and psychological support. However, LLMs often exhibit\nlimitations in emotional perception and social competence during real-world\nconversations. These limitations partly originate from their inability to adapt\ntheir communication style and emotional expression to different social and task\ncontexts. In this work, we introduce PersonaFuse, a novel LLM post-training\nframework that enables LLMs to adapt and express different personalities for\nvarying situations. Inspired by Trait Activation Theory and the Big Five\npersonality model, PersonaFuse employs a Mixture-of-Expert architecture that\ncombines persona adapters with a dynamic routing network, enabling contextual\ntrait expression. Experimental results show that PersonaFuse substantially\noutperforms baseline models across multiple dimensions of social-emotional\nintelligence. Importantly, these gains are achieved without sacrificing general\nreasoning ability or model safety, which remain common limitations of direct\nprompting and supervised fine-tuning approaches. PersonaFuse also delivers\nconsistent improvements in downstream human-centered applications, such as\nmental health counseling and review-based customer service. Finally, human\npreference evaluations against leading LLMs, including GPT-4o and DeepSeek,\ndemonstrate that PersonaFuse achieves competitive response quality despite its\ncomparatively smaller model size. These findings demonstrate that PersonaFuse\noffers a theoretically grounded and practical approach for developing\nsocial-emotional enhanced LLMs, marking a significant advancement toward more\nhuman-centric AI systems."}
{"id": "2509.08105", "pdf": "https://arxiv.org/pdf/2509.08105.pdf", "abs": "https://arxiv.org/abs/2509.08105", "title": "MERLIN: Multi-Stage Curriculum Alignment for Multilingual Encoder and LLM Fusion", "authors": ["Kosei Uemura", "David Guzmán", "Quang Phuoc Nguyen", "Jesujoba Oluwadara Alabi", "En-shiun Annie Lee", "David Ifeoluwa Adelani"], "categories": ["cs.CL"], "comment": "under submission", "summary": "Large language models excel in English but still struggle with complex\nreasoning in many low-resource languages (LRLs). Existing encoder-plus-decoder\nmethods such as LangBridge and MindMerger raise accuracy on mid and\nhigh-resource languages, yet they leave a large gap on LRLs. We present MERLIN,\na two-stage model-stacking framework that applies a curriculum learning\nstrategy -- from general bilingual bitext to task-specific data -- and adapts\nonly a small set of DoRA weights. On the AfriMGSM benchmark MERLIN improves\nexact-match accuracy by +12.9 pp over MindMerger and outperforms GPT-4o-mini.\nIt also yields consistent gains on MGSM and MSVAMP (+0.9 and +2.8 pp),\ndemonstrating effectiveness across both low and high-resource settings."}
{"id": "2509.08612", "pdf": "https://arxiv.org/pdf/2509.08612.pdf", "abs": "https://arxiv.org/abs/2509.08612", "title": "OTESGN: Optimal Transport-Enhanced Syntactic-Semantic Graph Networks for Aspect-Based Sentiment Analysis", "authors": ["Xinfeng Liao", "Xuanqi Chen", "Lianxi Wang", "Jiahuan Yang", "Zhuowei Chen", "Ziying Rong"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Aspect-based sentiment analysis (ABSA) aims to identify aspect terms and\ndetermine their sentiment polarity. While dependency trees combined with\ncontextual semantics provide structural cues, existing approaches often rely on\ndot-product similarity and fixed graphs, which limit their ability to capture\nnonlinear associations and adapt to noisy contexts. To address these\nlimitations, we propose the Optimal Transport-Enhanced Syntactic-Semantic Graph\nNetwork (OTESGN), a model that jointly integrates structural and distributional\nsignals. Specifically, a Syntactic Graph-Aware Attention module models global\ndependencies with syntax-guided masking, while a Semantic Optimal Transport\nAttention module formulates aspect-opinion association as a distribution\nmatching problem solved via the Sinkhorn algorithm. An Adaptive Attention\nFusion mechanism balances heterogeneous features, and contrastive\nregularization enhances robustness. Extensive experiments on three benchmark\ndatasets (Rest14, Laptop14, and Twitter) demonstrate that OTESGN delivers\nstate-of-the-art performance. Notably, it surpasses competitive baselines by up\nto +1.30 Macro-F1 on Laptop14 and +1.01 on Twitter. Ablation studies and\nvisualization analyses further highlight OTESGN's ability to capture\nfine-grained sentiment associations and suppress noise from irrelevant context."}
{"id": "2406.04493", "pdf": "https://arxiv.org/pdf/2406.04493.pdf", "abs": "https://arxiv.org/abs/2406.04493", "title": "ReceiptSense: Beyond Traditional OCR -- A Dataset for Receipt Understanding", "authors": ["Abdelrahman Abdallah", "Mohamed Mounis", "Mahmoud Abdalla", "Mahmoud SalahEldin Kasem", "Mohamed Mahmoud", "Ibrahim Abdelhalim", "Mohamed Elkasaby", "Yasser ElBendary", "Adam Jatowt"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Multilingual OCR and information extraction from receipts remains\nchallenging, particularly for complex scripts like Arabic. We introduce\n\\dataset, a comprehensive dataset designed for Arabic-English receipt\nunderstanding comprising 20,000 annotated receipts from diverse retail\nsettings, 30,000 OCR-annotated images, and 10,000 item-level annotations, and a\nnew Receipt QA subset with 1265 receipt images paired with 40 question-answer\npairs each to support LLM evaluation for receipt understanding. The dataset\ncaptures merchant names, item descriptions, prices, receipt numbers, and dates\nto support object detection, OCR, and information extraction tasks. We\nestablish baseline performance using traditional methods (Tesseract OCR) and\nadvanced neural networks, demonstrating the dataset's effectiveness for\nprocessing complex, noisy real-world receipt layouts. Our publicly accessible\ndataset advances automated multilingual document processing research (see\nhttps://github.com/Update-For-Integrated-Business-AI/CORU )."}
{"id": "2408.13227", "pdf": "https://arxiv.org/pdf/2408.13227.pdf", "abs": "https://arxiv.org/abs/2408.13227", "title": "Enhancing Few-Shot Transfer Learning with Optimized Multi-Task Prompt Tuning through Modular Prompt Composition", "authors": ["Ahmad Pouramini", "Hesham Faili"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "In recent years, multi-task prompt tuning has garnered considerable attention\nfor its inherent modularity and potential to enhance parameter-efficient\ntransfer learning across diverse tasks. This paper aims to analyze and improve\nthe performance of multiple tasks by facilitating the transfer of knowledge\nbetween their corresponding prompts in a multi-task setting. Our proposed\napproach decomposes the prompt for each target task into a combination of\nshared prompts (source prompts) and a task-specific prompt (private prompt).\nDuring training, the source prompts undergo fine-tuning and are integrated with\nthe private prompt to drive the target prompt for each task. We present and\ncompare multiple methods for combining source prompts to construct the target\nprompt, analyzing the roles of both source and private prompts within each\nmethod. We investigate their contributions to task performance and offer\nflexible, adjustable configurations based on these insights to optimize\nperformance. Our empirical findings clearly showcase improvements in accuracy\nand robustness compared to the conventional practice of prompt tuning and\nrelated works. Notably, our results substantially outperform other methods in\nthe field in few-shot settings, demonstrating superior performance in various\ntasks across GLUE benchmark, among other tasks. This achievement is attained\nwith a significantly reduced amount of training data, making our method a\npromising one for few-shot settings."}
{"id": "2503.16505", "pdf": "https://arxiv.org/pdf/2503.16505.pdf", "abs": "https://arxiv.org/abs/2503.16505", "title": "Scalable Evaluation of Online Facilitation Strategies via Synthetic Simulation of Discussions", "authors": ["Dimitris Tsirmpas", "Ion Androutsopoulos", "John Pavlopoulos"], "categories": ["cs.HC", "cs.CL", "cs.LG", "68T50", "I.2.7"], "comment": "15 pages, 3 tables, 12 figures", "summary": "Limited large-scale evaluations exist for facilitation strategies of online\ndiscussions due to significant costs associated with human involvement. An\neffective solution is synthetic discussion simulations using Large Language\nModels (LLMs) to create initial pilot experiments. We propose design principles\nbased on existing methodologies for synthetic discussion generation. Based on\nthese principles, we propose a simple, generalizable, LLM-driven methodology to\nprototype the development of LLM facilitators by generating synthetic data\nwithout human involvement, and which surpasses current baselines. We use our\nmethodology to test whether current Social Science strategies for facilitation\ncan improve the performance of LLM facilitators. We find that, while LLM\nfacilitators significantly improve synthetic discussions, there is no evidence\nthat the application of these strategies leads to further improvements in\ndiscussion quality. In an effort to aid research in the field of facilitation,\nwe release a large, publicly available dataset containing LLM-generated and\nLLM-annotated discussions using multiple open-source models. This dataset can\nbe used for LLM facilitator finetuning as well as behavioral analysis of\ncurrent out-of-the-box LLMs in the task. We also release an open-source python\nframework that efficiently implements our methodology at great scale."}
{"id": "2503.18492", "pdf": "https://arxiv.org/pdf/2503.18492.pdf", "abs": "https://arxiv.org/abs/2503.18492", "title": "VeriSafe Agent: Safeguarding Mobile GUI Agent via Logic-based Action Verification", "authors": ["Jungjae Lee", "Dongjae Lee", "Chihun Choi", "Youngmin Im", "Jaeyoung Wi", "Kihong Heo", "Sangeun Oh", "Sunjae Lee", "Insik Shin"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Foundation Models (LFMs) have unlocked new possibilities in\nhuman-computer interaction, particularly with the rise of mobile Graphical User\nInterface (GUI) Agents capable of interacting with mobile GUIs. These agents\nallow users to automate complex mobile tasks through simple natural language\ninstructions. However, the inherent probabilistic nature of LFMs, coupled with\nthe ambiguity and context-dependence of mobile tasks, makes LFM-based\nautomation unreliable and prone to errors. To address this critical challenge,\nwe introduce VeriSafe Agent (VSA): a formal verification system that serves as\na logically grounded safeguard for Mobile GUI Agents. VSA deterministically\nensures that an agent's actions strictly align with user intent before\nexecuting the action. At its core, VSA introduces a novel autoformalization\ntechnique that translates natural language user instructions into a formally\nverifiable specification. This enables runtime, rule-based verification of\nagent's actions, detecting erroneous actions even before they take effect. To\nthe best of our knowledge, VSA is the first attempt to bring the rigor of\nformal verification to GUI agents, bridging the gap between LFM-driven actions\nand formal software verification. We implement VSA using off-the-shelf LFM\nservices (GPT-4o) and evaluate its performance on 300 user instructions across\n18 widely used mobile apps. The results demonstrate that VSA achieves\n94.33%-98.33% accuracy in verifying agent actions, outperforming existing\nLFM-based verification methods by 30.00%-16.33%, and increases the GUI agent's\ntask completion rate by 90%-130%."}
{"id": "2506.14755", "pdf": "https://arxiv.org/pdf/2506.14755.pdf", "abs": "https://arxiv.org/abs/2506.14755", "title": "Optimizing Length Compression in Large Reasoning Models", "authors": ["Zhengxiang Cheng", "Dongping Chen", "Mingyang Fu", "Tianyi Zhou"], "categories": ["cs.AI", "cs.CL"], "comment": "16 pages, 7 figures, 4 tables", "summary": "Large Reasoning Models (LRMs) have achieved remarkable success, yet they\noften suffer from producing unnecessary and verbose reasoning chains. We\nidentify a core aspect of this issue as \"invalid thinking\" -- models tend to\nrepeatedly double-check their work after having derived the correct answer. To\naddress this specific inefficiency, we move beyond the general principles of\nEfficacy and Efficiency to propose two new, fine-grained principles: Brevity,\nwhich advocates for eliminating redundancy, and Sufficiency, which ensures\ncritical reasoning steps are preserved. Guided by these principles, we\nintroduce LC-R1, a post-training method based on Group Relative Policy\nOptimization (GRPO). LC-R1 employs a novel combination of a Length Reward for\noverall conciseness and a Compress Reward that is specifically designed to\nremove the invalid portion of the thinking process. Extensive experiments on\nmultiple reasoning benchmarks demonstrate that LC-R1 achieves a significant\nreduction in sequence length (~50%) with only a marginal (~2%) drop in\naccuracy, achieving a favorable trade-off point on the Pareto frontier that\nprioritizes high compression. Our analysis further validates the robustness of\nLC-R1 and provides valuable insights for developing more powerful yet\ncomputationally efficient LRMs. Our code is released at\nhttps://github.com/zxiangx/LC-R1."}
{"id": "2507.10576", "pdf": "https://arxiv.org/pdf/2507.10576.pdf", "abs": "https://arxiv.org/abs/2507.10576", "title": "Can Large Language Models Understand As Well As Apply Patent Regulations to Pass a Hands-On Patent Attorney Test?", "authors": ["Bhakti Khera", "Rezvan Alamian", "Pascal A. Scherz", "Stephan M. Goetz"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.ET"], "comment": "41 pages, 21 figures", "summary": "The legal field already uses various large language models (LLMs) in actual\napplications, but their quantitative performance and reasons for it are\nunderexplored. We evaluated several open-source and proprietary LLMs --\nincluding GPT-series, Anthropic, Deepseek and Llama-3, variants -- on parts of\nthe European Qualifying Examination (EQE) for future European Patent Attorneys.\nOpenAI o1 led with 0.82 accuracy and 0.81 F1 score, whereas (Amazon Web\nServices) AWS Llama 3.1 8B lagged at 0.50 accuracy, and a Python-deployed Llama\n3.1 8B scored 0.55. The latter two are within the range of mere guessing for\nthe two-answer forced-choice design. None of the evaluated models could have\npassed the examination fully, as accuracy never exceeded the average threshold\nof 0.90 required for professional-level standards -- also not models that are\nregularly promoted for their assumed beyond-PhD- and bar-admitted-lawyer-level\nperformance. GPT-4o excelled at integrating text and graphics, while Claude 3\nOpus often lost formatting coherence. Human patent experts evaluated the\ntextual justifications and uncovered various critical shortcomings of each\nmodel. They valued clarity and legal rationale over the raw correctness of the\nanswers, which revealed misalignment between automatic metrics and expert\njudgment. Model outputs were sensitive to modest temperature changes and prompt\nwording, which underscores the remaining necessity of expert oversight. Future\nwork should target logical consistency, robust multimodality, and adaptive\nprompting to approach human-level patent proficiency. In summary, despite the\noutstanding performance of recent large models, the general public might\noverestimate their performance. The field has a long way to go to develop a\nvirtual patent attorney. This paper wants to point out several specific\nlimitations that need solutions."}
{"id": "2507.20999", "pdf": "https://arxiv.org/pdf/2507.20999.pdf", "abs": "https://arxiv.org/abs/2507.20999", "title": "LoRA-PAR: A Flexible Dual-System LoRA Partitioning Approach to Efficient LLM Fine-Tuning", "authors": ["Yining Huang", "Bin Li", "Keke Tang", "Meilian Chen"], "categories": ["cs.LG", "cs.CL"], "comment": "12 pages", "summary": "Large-scale generative models like DeepSeek-R1 and OpenAI-O1 benefit\nsubstantially from chain-of-thought (CoT) reasoning, yet pushing their\nperformance typically requires vast data, large model sizes, and full-parameter\nfine-tuning. While parameter-efficient fine-tuning (PEFT) helps reduce cost,\nmost existing approaches primarily address domain adaptation or layer-wise\nallocation rather than explicitly tailoring data and parameters to different\nresponse demands. Inspired by \"Thinking, Fast and Slow,\" which characterizes\ntwo distinct modes of thought-System 1 (fast, intuitive, often automatic) and\nSystem 2 (slower, more deliberative and analytic)-we draw an analogy that\ndifferent \"subregions\" of an LLM's parameters might similarly specialize for\ntasks that demand quick, intuitive responses versus those requiring multi-step\nlogical reasoning. Therefore, we propose LoRA-PAR, a dual-system LoRA framework\nthat partitions both data and parameters by System 1 or System 2 demands, using\nfewer yet more focused parameters for each task. Specifically, we classify task\ndata via multi-model role-playing and voting, and partition parameters based on\nimportance scoring, then adopt a two-stage fine-tuning strategy of training\nSystem 1 tasks with supervised fine-tuning (SFT) to enhance knowledge and\nintuition and refine System 2 tasks with reinforcement learning (RL) to\nreinforce deeper logical deliberation next. Extensive experiments show that the\ntwo-stage fine-tuning strategy, SFT and RL, lowers active parameter usage while\nmatching or surpassing SOTA PEFT baselines."}
{"id": "2508.20655", "pdf": "https://arxiv.org/pdf/2508.20655.pdf", "abs": "https://arxiv.org/abs/2508.20655", "title": "Improving Alignment in LVLMs with Debiased Self-Judgment", "authors": ["Sihan Yang", "Chenhang Cui", "Zihao Zhao", "Yiyang Zhou", "Weilong Yan", "Ying Wei", "Huaxiu Yao"], "categories": ["cs.CV", "cs.CL"], "comment": "EMNLP 2025 Findings", "summary": "The rapid advancements in Large Language Models (LLMs) and Large\nVisual-Language Models (LVLMs) have opened up new opportunities for integrating\nvisual and linguistic modalities. However, effectively aligning these\nmodalities remains challenging, often leading to hallucinations--where\ngenerated outputs are not grounded in the visual input--and raising safety\nconcerns across various domains. Existing alignment methods, such as\ninstruction tuning and preference tuning, often rely on external datasets,\nhuman annotations, or complex post-processing, which limit scalability and\nincrease costs. To address these challenges, we propose a novel approach that\ngenerates the debiased self-judgment score, a self-evaluation metric created\ninternally by the model without relying on external resources. This enables the\nmodel to autonomously improve alignment. Our method enhances both decoding\nstrategies and preference tuning processes, resulting in reduced\nhallucinations, enhanced safety, and improved overall capability. Empirical\nresults show that our approach significantly outperforms traditional methods,\noffering a more effective solution for aligning LVLMs."}
{"id": "2509.02521", "pdf": "https://arxiv.org/pdf/2509.02521.pdf", "abs": "https://arxiv.org/abs/2509.02521", "title": "FLM-Audio: Natural Monologues Improves Native Full-Duplex Chatbots via Dual Training", "authors": ["Yiqun Yao", "Xiang Li", "Xin Jiang", "Xuezhi Fang", "Naitong Yu", "Wenjia Ma", "Aixin Sun", "Yequan Wang"], "categories": ["cs.SD", "cs.AI", "cs.CL"], "comment": null, "summary": "Full-duplex dialog models aim to listen and speak simultaneously, delivering\nrapid responses to dynamic user input. Among different solutions to full\nduplexity, a native solution merges multiple channels in each time step,\nachieving the lowest latency. However, prevailing designs break down the\ntextual monologue sentences for word-level alignment with audio streams, which\ndegrades language modeling abilities. To help address this issue, we introduce\nnatural monologues, which are composed by continuous sentences and waiting\nintervals, mimicking humanoid cognitive behavior in dialogs. We find a proper\ntraining paradigm to be critical for semantically aligning natural monologues\nwith audio. To this end, we develop a dual training paradigm that alternates\nthe position of the monologues, either leading or trailing the audio, across\ndifferent training stages. A combination of our natural monologue and dual\ntraining strategy is applied in developing FLM-Audio, our 7B spoken dialog\nchatbot with native full-duplexity. As confirmed by experimental results,\nFLM-Audio achieves superior response qualities and chatting experiences while\nrequiring significantly less training data."}
{"id": "2509.08653", "pdf": "https://arxiv.org/pdf/2509.08653.pdf", "abs": "https://arxiv.org/abs/2509.08653", "title": "Generative Data Refinement: Just Ask for Better Data", "authors": ["Minqi Jiang", "João G. M. Araújo", "Will Ellsworth", "Sian Gooding", "Edward Grefenstette"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "For a fixed parameter size, the capabilities of large models are primarily\ndetermined by the quality and quantity of its training data. Consequently,\ntraining datasets now grow faster than the rate at which new data is indexed on\nthe web, leading to projected data exhaustion over the next decade. Much more\ndata exists as user-generated content that is not publicly indexed, but\nincorporating such data comes with considerable risks, such as leaking private\ninformation and other undesirable content. We introduce a framework, Generative\nData Refinement (GDR), for using pretrained generative models to transform a\ndataset with undesirable content into a refined dataset that is more suitable\nfor training. Our experiments show that GDR can outperform industry-grade\nsolutions for dataset anonymization, as well as enable direct detoxification of\nhighly unsafe datasets. Moreover, we show that by generating synthetic data\nthat is conditioned on each example in the real dataset, GDR's refined outputs\nnaturally match the diversity of web scale datasets, and thereby avoid the\noften challenging task of generating diverse synthetic data via model\nprompting. The simplicity and effectiveness of GDR make it a powerful tool for\nscaling up the total stock of training data for frontier models."}
{"id": "2509.08814", "pdf": "https://arxiv.org/pdf/2509.08814.pdf", "abs": "https://arxiv.org/abs/2509.08814", "title": "Merge-of-Thought Distillation", "authors": ["Zhanming Shen", "Zeyu Qin", "Zenan Huang", "Hao Chen", "Jiaqi Hu", "Yihong Zhuang", "Guoshan Lu", "Gang Chen", "Junbo Zhao"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Efficient reasoning distillation for long chain-of-thought (CoT) models is\nincreasingly constrained by the assumption of a single oracle teacher, despite\npractical availability of multiple candidate teachers and growing CoT corpora.\nWe revisit teacher selection and observe that different students have different\n\"best teachers,\" and even for the same student the best teacher can vary across\ndatasets. Therefore, to unify multiple teachers' reasoning abilities into\nstudent with overcoming conflicts among various teachers' supervision, we\npropose Merge-of-Thought Distillation (MoT), a lightweight framework that\nalternates between teacher-specific supervised fine-tuning branches and\nweight-space merging of the resulting student variants. On competition math\nbenchmarks, using only about 200 high-quality CoT samples, applying MoT to a\nQwen3-14B student surpasses strong models including DEEPSEEK-R1, QWEN3-30B-A3B,\nQWEN3-32B, and OPENAI-O1, demonstrating substantial gains. Besides, MoT\nconsistently outperforms the best single-teacher distillation and the naive\nmulti-teacher union, raises the performance ceiling while mitigating\noverfitting, and shows robustness to distribution-shifted and peer-level\nteachers. Moreover, MoT reduces catastrophic forgetting, improves general\nreasoning beyond mathematics and even cultivates a better teacher, indicating\nthat consensus-filtered reasoning features transfer broadly. These results\nposition MoT as a simple, scalable route to efficiently distilling long CoT\ncapabilities from diverse teachers into compact students."}
