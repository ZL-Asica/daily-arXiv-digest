{"id": "2505.20464", "pdf": "https://arxiv.org/pdf/2505.20464.pdf", "abs": "https://arxiv.org/abs/2505.20464", "title": "The Impact of a Chatbot's Ephemerality-Framing on Self-Disclosure Perceptions", "authors": ["Samuel Rhys Cox", "Rune Møberg Jacobsen", "Niels van Berkel"], "categories": ["cs.HC", "cs.CL"], "comment": "In ACM Conversational User Interfaces (CUI '25), July 8-10, 2025; 18\n  pages; 6 Figures; 6 Tables", "summary": "Self-disclosure, the sharing of one's thoughts and feelings, is affected by\nthe perceived relationship between individuals. While chatbots are increasingly\nused for self-disclosure, the impact of a chatbot's framing on users'\nself-disclosure remains under-explored. We investigated how a chatbot's\ndescription of its relationship with users, particularly in terms of\nephemerality, affects self-disclosure. Specifically, we compared a Familiar\nchatbot, presenting itself as a companion remembering past interactions, with a\nStranger chatbot, presenting itself as a new, unacquainted entity in each\nconversation. In a mixed factorial design, participants engaged with either the\nFamiliar or Stranger chatbot in two sessions across two days, with one\nconversation focusing on Emotional- and another Factual-disclosure. When\nEmotional-disclosure was sought in the first chatting session,\nStranger-condition participants felt more comfortable self-disclosing. However,\nwhen Factual-disclosure was sought first, these differences were replaced by\nmore enjoyment among Familiar-condition participants. Qualitative findings\nshowed Stranger afforded anonymity and reduced judgement, whereas Familiar\nsometimes felt intrusive unless rapport was built via low-risk\nFactual-disclosure."}
{"id": "2505.20585", "pdf": "https://arxiv.org/pdf/2505.20585.pdf", "abs": "https://arxiv.org/abs/2505.20585", "title": "HOT-FIT-BR: A Context-Aware Evaluation Framework for Digital Health Systems in Resource-Limited Settings", "authors": ["Ben Rahman"], "categories": ["cs.HC", "cs.CY"], "comment": "This work proposes HOT-FIT-BR, an extended evaluation framework for\n  digital health systems in LMICs. It includes infrastructure readiness, policy\n  compliance, and community engagement metrics-validated in Indonesian primary\n  healthcare settings", "summary": "Implementation of digital health systems in low-middle-income countries\n(LMICs) often fails due to a lack of evaluations that take into account\ninfrastructure limitations, local policies, and community readiness. We\nintroduce HOT-FIT-BR, a contextual evaluation framework that expands the\nHOT-FIT model with three new dimensions: (1) Infrastructure Index to measure\nelectricity/internet availability, (2) Policy Compliance Layer to ensure\nregulatory compliance (e.g., Permenkes 24/2022 in Indonesia), and (3) Community\nEngagement Fit. Simulations at Indonesian Health Centers show that HOT-FIT-BR\nis 58% more sensitive to detecting problems than HOT-FIT, especially in rural\nareas with an Infra Index <3. The framework has also proven adaptive to the\ncontext of other LMICs such as India and Kenya through local parameter\nadjustments."}
{"id": "2505.20623", "pdf": "https://arxiv.org/pdf/2505.20623.pdf", "abs": "https://arxiv.org/abs/2505.20623", "title": "Institutionalizing Folk Theories of Algorithms: How Multi-Channel Networks (MCNs) Govern Algorithmic Labor in Chinese Live-Streaming Industry", "authors": ["Qing Xiao", "Rongyi Chen", "Jingjia Xiao", "Tianyang Fu", "Alice Qian Zhang", "Xianzhe Fan", "Bingbing Zhang", "Zhicong Lu", "Hong Shen"], "categories": ["cs.HC", "cs.CY"], "comment": "28 pages, 2 figures", "summary": "As algorithmic systems increasingly structure platform labor, workers often\nrely on informal \"folk theories\", experience-based beliefs about how algorithms\nwork, to navigate opaque and unstable algorithmic environments. Prior research\nhas largely treated these theories as bottom-up, peer-driven strategies for\ncoping with algorithmic opacity and uncertainty. In this study, we shift\nanalytical attention to intermediary organizations and examine how folk\ntheories of algorithms can be institutionally constructed and operationalized\nby those organizations as tools of labor management. Drawing on nine months of\nethnographic fieldwork and 37 interviews with live-streamers and staff at\nMulti-Channel Networks (MCNs) in China, we show that MCNs develop and circulate\ndual algorithmic theories: internally, they acknowledge the volatility of\nplatform systems and adopt probabilistic strategies to manage risk; externally,\nthey promote simplified, prescriptive theories portraying the algorithm as\ntransparent, fair, and responsive to individual effort. They have further\noperationalize those folk theories for labor management, encouraging streamers\nto self-discipline and invest in equipment, training, and routines, while\nabsolving MCNs of accountability. We contribute to CSCW and platform labor\nliterature by demonstrating how informal algorithmic knowledge, once\ninstitutionalized, can become infrastructures of soft control -- shaping not\nonly how workers interpret platform algorithms, but also how their labor is\nstructured, moralized and governed."}
{"id": "2505.20692", "pdf": "https://arxiv.org/pdf/2505.20692.pdf", "abs": "https://arxiv.org/abs/2505.20692", "title": "Can we Debias Social Stereotypes in AI-Generated Images? Examining Text-to-Image Outputs and User Perceptions", "authors": ["Saharsh Barve", "Andy Mao", "Jiayue Melissa Shi", "Prerna Juneja", "Koustuv Saha"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances in generative AI have enabled visual content creation through\ntext-to-image (T2I) generation. However, despite their creative potential, T2I\nmodels often replicate and amplify societal stereotypes -- particularly those\nrelated to gender, race, and culture -- raising important ethical concerns.\nThis paper proposes a theory-driven bias detection rubric and a Social\nStereotype Index (SSI) to systematically evaluate social biases in T2I outputs.\nWe audited three major T2I model outputs -- DALL-E-3, Midjourney-6.1, and\nStability AI Core -- using 100 queries across three categories -- geocultural,\noccupational, and adjectival. Our analysis reveals that initial outputs are\nprone to include stereotypical visual cues, including gendered professions,\ncultural markers, and western beauty norms. To address this, we adopted our\nrubric to conduct targeted prompt refinement using LLMs, which significantly\nreduced bias -- SSI dropped by 61% for geocultural, 69% for occupational, and\n51% for adjectival queries. We complemented our quantitative analysis through a\nuser study examining perceptions, awareness, and preferences around\nAI-generated biased imagery. Our findings reveal a key tension -- although\nprompt refinement can mitigate stereotypes, it can limit contextual alignment.\nInterestingly, users often perceived stereotypical images to be more aligned\nwith their expectations. We discuss the need to balance ethical debiasing with\ncontextual relevance and call for T2I systems that support global diversity and\ninclusivity while not compromising the reflection of real-world social\ncomplexity."}
{"id": "2505.20309", "pdf": "https://arxiv.org/pdf/2505.20309.pdf", "abs": "https://arxiv.org/abs/2505.20309", "title": "Guiding Giants: Lightweight Controllers for Weighted Activation Steering in LLMs", "authors": ["Amr Hegazy", "Mostafa Elhoushi", "Amr Alanwar"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Controlling undesirable Large Language Model (LLM) behaviors, such as the\ngeneration of unsafe content or failing to adhere to safety guidelines, often\nrelies on costly fine-tuning. Activation steering provides an alternative for\ninference-time control, but existing methods typically lack fine-grained,\nadaptive mechanisms. We introduce a novel approach using a lightweight,\ntrainable controller network integrated during inference. This controller\nnetwork observes specific intermediate LLM activations and predicts both a\nglobal scaling factor and layer-specific weights. The predicted global scaling\nfactor and layer-specific weights then dynamically modulate the intensity of a\nsteering patch, derived from a pre-computed \"refusal direction\" vector, applied\nacross the LLM's layers during generation. Trained on activations from both\nharmful and benign prompts, our controller learns to discriminatively apply\nnuanced, layer-aware interventions, activating steering primarily for harmful\ninputs. Experiments using safety benchmarks like ToxicChat & In-The-Wild\nJailbreak Prompts demonstrate that our weighted steering controller\nsignificantly increases refusal rates compared to the base LLM, achieving\ntargeted behavioral modification without altering the original model\nparameters. Our experiments with Llama-3.1-8B, Llama-3.2-1B & Mistral-7B show\nour approach outperforms existing methods, presenting an efficient and adaptive\nmethod for fine-grained control over LLM behavior at inference time."}
{"id": "2505.20711", "pdf": "https://arxiv.org/pdf/2505.20711.pdf", "abs": "https://arxiv.org/abs/2505.20711", "title": "Automating eHMI Action Design with LLMs for Automated Vehicle Communication", "authors": ["Ding Xia", "Xinyue Gui", "Fan Gao", "Dongyuan Li", "Mark Colley", "Takeo Igarashi"], "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "The absence of explicit communication channels between automated vehicles\n(AVs) and other road users requires the use of external Human-Machine\nInterfaces (eHMIs) to convey messages effectively in uncertain scenarios.\nCurrently, most eHMI studies employ predefined text messages and manually\ndesigned actions to perform these messages, which limits the real-world\ndeployment of eHMIs, where adaptability in dynamic scenarios is essential.\nGiven the generalizability and versatility of large language models (LLMs),\nthey could potentially serve as automated action designers for the\nmessage-action design task. To validate this idea, we make three contributions:\n(1) We propose a pipeline that integrates LLMs and 3D renderers, using LLMs as\naction designers to generate executable actions for controlling eHMIs and\nrendering action clips. (2) We collect a user-rated Action-Design Scoring\ndataset comprising a total of 320 action sequences for eight intended messages\nand four representative eHMI modalities. The dataset validates that LLMs can\ntranslate intended messages into actions close to a human level, particularly\nfor reasoning-enabled LLMs. (3) We introduce two automated raters, Action\nReference Score (ARS) and Vision-Language Models (VLMs), to benchmark 18 LLMs,\nfinding that the VLM aligns with human preferences yet varies across eHMI\nmodalities."}
{"id": "2505.20315", "pdf": "https://arxiv.org/pdf/2505.20315.pdf", "abs": "https://arxiv.org/abs/2505.20315", "title": "Arctic-Text2SQL-R1: Simple Rewards, Strong Reasoning in Text-to-SQL", "authors": ["Zhewei Yao", "Guoheng Sun", "Lukasz Borchmann", "Zheyu Shen", "Minghang Deng", "Bohan Zhai", "Hao Zhang", "Ang Li", "Yuxiong He"], "categories": ["cs.CL", "cs.AI"], "comment": "22 pages, 2 figures", "summary": "Translating natural language into SQL (Test2SQL) is a longstanding challenge\nat the intersection of natural language understanding and structured data\naccess. While large language models (LLMs) have significantly improved fluency\nin SQL generation, producing correct and executable SQL--particularly for\ncomplex queries--remains a bottleneck. We present Arctic-Text2SQL-R1, a\nreinforcement learning (RL) framework and model family designed to generate\naccurate, executable SQL using a lightweight reward signal based solely on\nexecution correctness. Our approach avoids brittle intermediate supervision and\ncomplex reward shaping, promoting stable training and alignment with the end\ntask. Combined with carefully curated data, strong supervised initialization,\nand effective training practices, Arctic-Text2SQL-R1 achieves state-of-the-art\nexecution accuracy across six diverse Test2SQL benchmarks, including the top\nposition on the BIRD leaderboard. Notably, our 7B model outperforms prior\n70B-class systems, highlighting the framework's scalability and efficiency. We\nfurther demonstrate inference-time robustness through simple extensions like\nvalue retrieval and majority voting. Extensive experiments and ablation studies\noffer both positive and negative insights, providing practical guidance for\nfuture Test2SQL research."}
{"id": "2505.20727", "pdf": "https://arxiv.org/pdf/2505.20727.pdf", "abs": "https://arxiv.org/abs/2505.20727", "title": "What Shapes Writers' Decisions to Disclose AI Use?", "authors": ["Jingchao Fang", "Mina Lee"], "categories": ["cs.HC"], "comment": "Navigating Generative AI Disclosure, Ownership, and Accountability in\n  Co-Creative Domains Workshop at CHIWORK 2025", "summary": "Have you ever read a blog or social media post and suspected that it was\nwritten--at least in part--by artificial intelligence (AI)? While transparently\nacknowledging contributors to writing is generally valued, why some writers\nchoose to disclose or withhold AI involvement remains unclear. In this work, we\nask what factors shape writers' decisions to disclose their AI use as a\nstarting point to effectively advocate for transparency. To shed light on this\nquestion, we synthesize study findings and theoretical frameworks in human-AI\ninteraction and behavioral science. Concretely, we identify and curate a list\nof factors that could affect writers' decisions regarding disclosure for\nhuman-AI co-created content."}
{"id": "2505.20318", "pdf": "https://arxiv.org/pdf/2505.20318.pdf", "abs": "https://arxiv.org/abs/2505.20318", "title": "Beyond Demonstrations: Dynamic Vector Construction from Latent Representations", "authors": ["Wang Cai", "Hsiu-Yuan Huang", "Zhixiang Wang", "Yunfang Wu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In-Context derived Vector (ICV) methods extract task-relevant representations\nfrom large language models (LLMs) and reinject them during inference, achieving\ncomparable performance to few-shot In-Context Learning (ICL) without repeated\ndemonstration processing. However, existing ICV methods remain sensitive to\nICL-specific factors, often use coarse or semantically fragmented\nrepresentations as the source of the vector, and rely on heuristic-based\ninjection positions, limiting their applicability.\n  To address these issues, we propose Dynamic Vector (DyVec), which\nincorporates an Exhaustive Query Rotation (EQR) strategy to extract robust\nsemantically aggregated latent representations by mitigating variance\nintroduced by ICL. It then applies Dynamic Latent Segmentation and Injection to\nadaptively partition representations based on task complexity and leverages\nREINFORCE-based optimization to learn optimal injection positions for each\nsegment.\n  Experiments results show that DyVec outperforms few-shot ICL, LoRA, and prior\nICV baselines. Further analysis highlights the effectiveness of dynamically\nsegmenting and injecting semantically aggregated latent representations. DyVec\nprovides a lightweight and data-efficient solution for inference-time task\nadaptation."}
{"id": "2505.20788", "pdf": "https://arxiv.org/pdf/2505.20788.pdf", "abs": "https://arxiv.org/abs/2505.20788", "title": "Enhancing Wearable Tap Water Audio Detection through Subclass Annotation in the HD-Epic Dataset", "authors": ["Robin Burchard", "Kristof Van Laerhoven"], "categories": ["cs.HC", "cs.LG"], "comment": "Submitted to ISWC 2025", "summary": "Wearable human activity recognition has been shown to benefit from the\ninclusion of acoustic data, as the sounds around a person often contain\nvaluable context. However, due to privacy concerns, it is usually not ethically\nfeasible to record and save microphone data from the device, since the audio\ncould, for instance, also contain private conversations. Rather, the data\nshould be processed locally, which in turn requires processing power and\nconsumes energy on the wearable device. One special use case of contextual\ninformation that can be utilized to augment special tasks in human activity\nrecognition is water flow detection, which can, e.g., be used to aid wearable\nhand washing detection. We created a new label called tap water for the\nrecently released HD-Epic data set, creating 717 hand-labeled annotations of\ntap water flow, based on existing annotations of the water class. We analyzed\nthe relation of tap water and water in the dataset and additionally trained and\nevaluated two lightweight classifiers to evaluate the newly added label class,\nshowing that the new class can be learned more easily."}
{"id": "2505.20320", "pdf": "https://arxiv.org/pdf/2505.20320.pdf", "abs": "https://arxiv.org/abs/2505.20320", "title": "Less Context, Same Performance: A RAG Framework for Resource-Efficient LLM-Based Clinical NLP", "authors": ["Satya Narayana Cheetirala", "Ganesh Raut", "Dhavalkumar Patel", "Fabio Sanatana", "Robert Freeman", "Matthew A Levin", "Girish N. Nadkarni", "Omar Dawkins", "Reba Miller", "Randolph M. Steinhagen", "Eyal Klang", "Prem Timsina"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Long text classification is challenging for Large Language Models (LLMs) due\nto token limits and high computational costs. This study explores whether a\nRetrieval Augmented Generation (RAG) approach using only the most relevant text\nsegments can match the performance of processing entire clinical notes with\nlarge context LLMs. We begin by splitting clinical documents into smaller\nchunks, converting them into vector embeddings, and storing these in a FAISS\nindex. We then retrieve the top 4,000 words most pertinent to the\nclassification query and feed these consolidated segments into an LLM. We\nevaluated three LLMs (GPT4o, LLaMA, and Mistral) on a surgical complication\nidentification task. Metrics such as AUC ROC, precision, recall, and F1 showed\nno statistically significant differences between the RAG based approach and\nwhole-text processing (p > 0.05p > 0.05). These findings indicate that RAG can\nsignificantly reduce token usage without sacrificing classification accuracy,\nproviding a scalable and cost effective solution for analyzing lengthy clinical\ndocuments."}
{"id": "2505.20796", "pdf": "https://arxiv.org/pdf/2505.20796.pdf", "abs": "https://arxiv.org/abs/2505.20796", "title": "Describe Me Something You Do Not Remember - Challenges and Risks of Exposure Design Using Generative Artificial Intelligence for Therapy of Complex Post-traumatic Disorder", "authors": ["Annalisa Degenhard", "Stefan Tschöke", "Michael Rietzler", "Enrico Rukzio"], "categories": ["cs.HC"], "comment": "Extended abstract for the Workshop \"Generative AI and Accessibility\n  Workshop: Surfacing Opportunities and Risks\" at the CHI conference on Human\n  Factors in Computing Systems 2025 (CHI'25) - Accepted on 4 April 2025", "summary": "Post-traumatic stress disorder (PTSD) is associated with sudden,\nuncontrollable, and intense flashbacks of traumatic memories. Trauma exposure\npsychotherapy has proven effective in reducing the severity of trauma-related\nsymptoms. It involves controlled recall of traumatic memories to train coping\nmechanisms for flashbacks and enable autobiographical integration of\ndistressing experiences. In particular, exposure to visualizations of these\nmemories supports successful recall. Although this approach is effective for\nvarious trauma types, it remains available for only a few. This is due to the\nlack of cost-efficient solutions for creating individualized exposure\nvisualizations. This issue is particularly relevant for the treatment of\nComplex PTSD (CPTSD), where traumatic memories are highly individual and\ngeneric visualizations do not meet therapeutic needs. Generative Artificial\nIntelligence (GAI) offers a flexible and cost-effective alternative. GAI\nenables the creation of individualized exposure visualizations during therapy\nand, for the first time, allows patients to actively participate in the\nvisualization process. While GAI opens new therapeutic perspectives and may\nimprove access to trauma therapy, especially for CPTSD, it also introduces\nsignificant challenges and risks. The extreme uncertainty and lack of control\nthat define both CPTSD and GAI raise concerns about feasibility and safety. To\nsupport safe and effective three-way communication, it is essential to\nunderstand the roles of patient, system, and therapist in exposure\nvisualization and how each can contribute to safety. This paper outlines\nperspectives, challenges, and risks associated with the use of GAI in trauma\ntherapy, with a focus on CPTSD."}
{"id": "2505.20321", "pdf": "https://arxiv.org/pdf/2505.20321.pdf", "abs": "https://arxiv.org/abs/2505.20321", "title": "BiomedSQL: Text-to-SQL for Scientific Reasoning on Biomedical Knowledge Bases", "authors": ["Mathew J. Koretsky", "Maya Willey", "Adi Asija", "Owen Bianchi", "Chelsea X. Alvarado", "Tanay Nayak", "Nicole Kuznetsov", "Sungwon Kim", "Mike A. Nalls", "Daniel Khashabi", "Faraz Faghri"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Under Review", "summary": "Biomedical researchers increasingly rely on large-scale structured databases\nfor complex analytical tasks. However, current text-to-SQL systems often\nstruggle to map qualitative scientific questions into executable SQL,\nparticularly when implicit domain reasoning is required. We introduce\nBiomedSQL, the first benchmark explicitly designed to evaluate scientific\nreasoning in text-to-SQL generation over a real-world biomedical knowledge\nbase. BiomedSQL comprises 68,000 question/SQL query/answer triples grounded in\na harmonized BigQuery knowledge base that integrates gene-disease associations,\ncausal inference from omics data, and drug approval records. Each question\nrequires models to infer domain-specific criteria, such as genome-wide\nsignificance thresholds, effect directionality, or trial phase filtering,\nrather than rely on syntactic translation alone. We evaluate a range of open-\nand closed-source LLMs across prompting strategies and interaction paradigms.\nOur results reveal a substantial performance gap: GPT-o3-mini achieves 59.0%\nexecution accuracy, while our custom multi-step agent, BMSQL, reaches 62.6%,\nboth well below the expert baseline of 90.0%. BiomedSQL provides a new\nfoundation for advancing text-to-SQL systems capable of supporting scientific\ndiscovery through robust reasoning over structured biomedical knowledge bases.\nOur dataset is publicly available at\nhttps://huggingface.co/datasets/NIH-CARD/BiomedSQL, and our code is open-source\nat https://github.com/NIH-CARD/biomedsql."}
{"id": "2505.20916", "pdf": "https://arxiv.org/pdf/2505.20916.pdf", "abs": "https://arxiv.org/abs/2505.20916", "title": "Imago Obscura: An Image Privacy AI Co-pilot to Enable Identification and Mitigation of Risks", "authors": ["Kyzyl Monteiro", "Yuchen Wu", "Sauvik Das"], "categories": ["cs.HC", "H.5.2; K.4.1"], "comment": "26 pages including appendix, 14 images, 3 tables", "summary": "Users often struggle to navigate the privacy / publicity boundary in sharing\nimages online: they may lack awareness of image privacy risks and/or the\nability to apply effective mitigation strategies. To address this challenge, we\nintroduce and evaluate Imago Obscura, an AI-powered, image-editing copilot that\nenables users to identify and mitigate privacy risks with images they intend to\nshare. Driven by design requirements from a formative user study with 7\nimage-editing experts, Imago Obscura enables users to articulate their\nimage-sharing intent and privacy concerns. The system uses these inputs to\nsurface contextually pertinent privacy risks, and then recommends and\nfacilitates application of a suite of obfuscation techniques found to be\neffective in prior literature -- e.g., inpainting, blurring, and generative\ncontent replacement. We evaluated Imago Obscura with 15 end-users in a lab\nstudy and found that it greatly improved users' awareness of image privacy\nrisks and their ability to address those risks, allowing them to make more\ninformed sharing decisions."}
{"id": "2505.20322", "pdf": "https://arxiv.org/pdf/2505.20322.pdf", "abs": "https://arxiv.org/abs/2505.20322", "title": "Beyond Prompt Engineering: Robust Behavior Control in LLMs via Steering Target Atoms", "authors": ["Mengru Wang", "Ziwen Xu", "Shengyu Mao", "Shumin Deng", "Zhaopeng Tu", "Huajun Chen", "Ningyu Zhang"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.IR", "cs.LG"], "comment": null, "summary": "Precise control over language model generation is vital for ensuring both\nsafety and reliability. Although prompt engineering and steering are commonly\nused to intervene in model behaviors, the vast number of parameters in models\noften results in highly intertwined internal representations. This\ninterdependency can limit control precision and sometimes lead to unintended\nside effects. Recent research has explored the use of sparse autoencoders (SAE)\nto disentangle knowledge in high-dimensional spaces for steering. However,\nthese applications have been limited to toy tasks owing to the nontrivial issue\nof locating atomic knowledge components. In this paper, we propose Steering\nTarget Atoms (STA), a novel method that isolates and manipulates disentangled\nknowledge components to enhance safety. Comprehensive experiments demonstrate\nthe effectiveness of our approach. Further analysis reveals that steering\nexhibits superior robustness and flexibility, particularly in adversarial\nscenarios. We also apply the steering strategy to the large reasoning model,\nconfirming its effectiveness in precise reasoning control."}
{"id": "2505.21116", "pdf": "https://arxiv.org/pdf/2505.21116.pdf", "abs": "https://arxiv.org/abs/2505.21116", "title": "Creativity in LLM-based Multi-Agent Systems: A Survey", "authors": ["Yi-Cheng Lin", "Kang-Chieh Chen", "Zhe-Yan Li", "Tzu-Heng Wu", "Tzu-Hsuan Wu", "Kuan-Yu Chen", "Hung-yi Lee", "Yun-Nung Chen"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "23 pages", "summary": "Large language model (LLM)-driven multi-agent systems (MAS) are transforming\nhow humans and AIs collaboratively generate ideas and artifacts. While existing\nsurveys provide comprehensive overviews of MAS infrastructures, they largely\noverlook the dimension of \\emph{creativity}, including how novel outputs are\ngenerated and evaluated, how creativity informs agent personas, and how\ncreative workflows are coordinated. This is the first survey dedicated to\ncreativity in MAS. We focus on text and image generation tasks, and present:\n(1) a taxonomy of agent proactivity and persona design; (2) an overview of\ngeneration techniques, including divergent exploration, iterative refinement,\nand collaborative synthesis, as well as relevant datasets and evaluation\nmetrics; and (3) a discussion of key challenges, such as inconsistent\nevaluation standards, insufficient bias mitigation, coordination conflicts, and\nthe lack of unified benchmarks. This survey offers a structured framework and\nroadmap for advancing the development, evaluation, and standardization of\ncreative MAS."}
{"id": "2505.20323", "pdf": "https://arxiv.org/pdf/2505.20323.pdf", "abs": "https://arxiv.org/abs/2505.20323", "title": "PMOA-TTS: Introducing the PubMed Open Access Textual Times Series Corpus", "authors": ["Shahriar Noroozizadeh", "Sayantan Kumar", "George H. Chen", "Jeremy C. Weiss"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Understanding temporal dynamics in clinical narratives is essential for\nmodeling patient trajectories, yet large-scale temporally annotated resources\nremain limited. We present PMOA-TTS, the first openly available dataset of\n124,699 PubMed Open Access (PMOA) case reports, each converted into structured\n(event, time) timelines via a scalable LLM-based pipeline. Our approach\ncombines heuristic filtering with Llama 3.3 to identify single-patient case\nreports, followed by prompt-driven extraction using Llama 3.3 and DeepSeek R1,\nresulting in over 5.6 million timestamped clinical events. To assess timeline\nquality, we evaluate against a clinician-curated reference set using three\nmetrics: (i) event-level matching (80% match at a cosine similarity threshold\nof 0.1), (ii) temporal concordance (c-index > 0.90), and (iii) Area Under the\nLog-Time CDF (AULTC) for timestamp alignment. Corpus-level analysis shows wide\ndiagnostic and demographic coverage. In a downstream survival prediction task,\nembeddings from extracted timelines achieve time-dependent concordance indices\nup to 0.82 $\\pm$ 0.01, demonstrating the predictive value of temporally\nstructured narratives. PMOA-TTS provides a scalable foundation for timeline\nextraction, temporal reasoning, and longitudinal modeling in biomedical NLP.\nThe dataset is available at: https://huggingface.co/datasets/snoroozi/pmoa-tts ."}
{"id": "2505.21196", "pdf": "https://arxiv.org/pdf/2505.21196.pdf", "abs": "https://arxiv.org/abs/2505.21196", "title": "Learning Annotation Consensus for Continuous Emotion Recognition", "authors": ["Ibrahim Shoer", "Engin Erzin"], "categories": ["cs.HC", "cs.CV"], "comment": null, "summary": "In affective computing, datasets often contain multiple annotations from\ndifferent annotators, which may lack full agreement. Typically, these\nannotations are merged into a single gold standard label, potentially losing\nvaluable inter-rater variability. We propose a multi-annotator training\napproach for continuous emotion recognition (CER) that seeks a consensus across\nall annotators rather than relying on a single reference label. Our method\nemploys a consensus network to aggregate annotations into a unified\nrepresentation, guiding the main arousal-valence predictor to better reflect\ncollective inputs. Tested on the RECOLA and COGNIMUSE datasets, our approach\noutperforms traditional methods that unify annotations into a single label.\nThis underscores the benefits of fully leveraging multi-annotator data in\nemotion recognition and highlights its applicability across various fields\nwhere annotations are abundant yet inconsistent."}
{"id": "2505.20325", "pdf": "https://arxiv.org/pdf/2505.20325.pdf", "abs": "https://arxiv.org/abs/2505.20325", "title": "Guided by Gut: Efficient Test-Time Scaling with Reinforced Intrinsic Confidence", "authors": ["Amirhosein Ghasemabadi", "Keith G. Mills", "Baochun Li", "Di Niu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Test-Time Scaling (TTS) methods for enhancing Large Language Model (LLM)\nreasoning often incur substantial computational costs, primarily due to\nextensive reliance on external Process Reward Models (PRMs) or sampling methods\nlike Best-of-N (BoN). This paper introduces Guided by Gut (GG), an efficient\nself-guided TTS framework that achieves PRM-level performance without costly\nexternal verifier models. Our method employs a lightweight tree search guided\nsolely by intrinsic LLM signals, token-level confidence and step novelty. One\ncritical innovation is improving the reliability of internal confidence\nestimates via a targeted reinforcement learning fine-tuning phase. Empirical\nevaluations on challenging mathematical reasoning benchmarks demonstrate that\nGG enables smaller models (e.g., 1.5B parameters) to achieve accuracy matching\nor surpassing significantly larger models (e.g., 32B-70B parameters), while\nreducing GPU memory usage by up to 10x. Compared to PRM-based methods, GG\nachieves comparable accuracy with 8x faster inference speeds and 4-5x lower\nmemory usage. Additionally, GG reduces KV cache memory usage by approximately\n50% compared to the BoN strategy, facilitating more efficient and practical\ndeployment of TTS techniques."}
{"id": "2505.21385", "pdf": "https://arxiv.org/pdf/2505.21385.pdf", "abs": "https://arxiv.org/abs/2505.21385", "title": "Dynamic Vision from EEG Brain Recordings: How much does EEG know?", "authors": ["Prajwal Singh", "Anupam Sharma", "Pankaj Pandey", "Krishna Miyapuram", "Shanmuganathan Raman"], "categories": ["cs.HC"], "comment": null, "summary": "Reconstructing and understanding dynamic visual information (video) from\nbrain EEG recordings is challenging due to the non-stationary nature of EEG\nsignals, their low signal-to-noise ratio (SNR), and the limited availability of\nEEG-Video stimulus datasets. Most recent studies have focused on reconstructing\nstatic images from EEG recordings. In this work, we propose a framework to\nreconstruct dynamic visual stimuli from EEG data and conduct an in-depth study\nof the information encoded in EEG signals. Our approach first trains a feature\nextraction network using a triplet-based contrastive learning strategy within\nan EEG-video generation framework. The extracted EEG features are then used for\nvideo synthesis with a modified StyleGAN-ADA, which incorporates temporal\ninformation as conditioning. Additionally, we analyze how different brain\nregions contribute to processing dynamic visual stimuli. Through several\nempirical studies, we evaluate the effectiveness of our framework and\ninvestigate how much dynamic visual information can be inferred from EEG\nsignals. The inferences we derive through our extensive studies would be of\nimmense value to future research on extracting visual dynamics from EEG."}
{"id": "2505.20333", "pdf": "https://arxiv.org/pdf/2505.20333.pdf", "abs": "https://arxiv.org/abs/2505.20333", "title": "Multi-Scale Manifold Alignment: A Unified Framework for Enhanced Explainability of Large Language Models", "authors": ["Yukun Zhang", "Qi Dong"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have achieved strong\nperformance, yet their internal reasoning remains opaque, limiting\ninterpretability and trust in critical applications. We propose a novel\nMulti_Scale Manifold Alignment framework that decomposes the latent space into\nglobal, intermediate, and local semantic manifolds capturing themes, context,\nand word-level details. Our method introduces cross_scale mapping functions\nthat jointly enforce geometric alignment (e.g., Procrustes analysis) and\ninformation preservation (via mutual information constraints like MINE or VIB).\nWe further incorporate curvature regularization and hyperparameter tuning for\nstable optimization. Theoretical analysis shows that alignment error, measured\nby KL divergence, can be bounded under mild assumptions. This framework offers\na unified explanation of how LLMs structure multi-scale semantics, advancing\ninterpretability and enabling applications such as bias detection and\nrobustness enhancement."}
{"id": "2505.20311", "pdf": "https://arxiv.org/pdf/2505.20311.pdf", "abs": "https://arxiv.org/abs/2505.20311", "title": "The EU AI Act, Stakeholder Needs, and Explainable AI: Aligning Regulatory Compliance in a Clinical Decision Support System", "authors": ["Anton Hummel", "Håkan Burden", "Susanne Stenberg", "Jan-Philipp Steghöfer", "Niklas Kühl"], "categories": ["cs.CY", "cs.HC"], "comment": "17 pages, 2 figures", "summary": "Explainable AI (XAI) is a promising solution to ensure compliance with the EU\nAI Act, the first multi-national regulation for AI. XAI aims to enhance\ntransparency and human oversight of AI systems, particularly ``black-box\nmodels'', which are criticized as incomprehensible. However, the discourse\naround the main stakeholders in the AI Act and XAI appears disconnected. While\nXAI prioritizes the end user's needs as the primary goal, the AI Act focuses on\nthe obligations of the provider and deployer of the AI system. We aim to bridge\nthis divide and provide guidance on how these two worlds are related. By\nfostering an interdisciplinary discussion in a cross-functional team with XAI,\nAI Act, legal, and requirements engineering experts, we walk through the steps\nnecessary to analyze an AI-based clinical decision support system to clarify\nthe end-user needs and assess AI Act applicability. By analyzing our justified\nunderstanding using an AI system under development as a case, we show that XAI\ntechniques can fill a gap between stakeholder needs and the requirements of the\nAI Act. We look at the similarities and contrasts between the legal\nrequirements and the needs of stakeholders. In doing so, we encourage\nresearchers and practitioners from the XAI community to reflect on their role\ntowards the AI Act by achieving a mutual understanding of the implications of\nXAI and the AI Act within different disciplines."}
{"id": "2505.20334", "pdf": "https://arxiv.org/pdf/2505.20334.pdf", "abs": "https://arxiv.org/abs/2505.20334", "title": "Lookahead Q-Cache: Achieving More Consistent KV Cache Eviction via Pseudo Query", "authors": ["Yixuan Wang", "Shiyu Ji", "Yijun Liu", "Yuzhuang Xu", "Yang Xu", "Qingfu Zhu", "Wanxiang Che"], "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 10 figures", "summary": "Large language models (LLMs) rely on key-value cache (KV cache) to accelerate\ndecoding by reducing redundant computations. However, the KV cache memory usage\ngrows substantially with longer text sequences, posing challenges for efficient\ndeployment. Existing KV cache eviction methods prune tokens using\nprefilling-stage attention scores, causing inconsistency with actual inference\nqueries, especially under tight memory budgets. In this paper, we propose\nLookahead Q-Cache (LAQ), a novel eviction framework that generates low-cost\npseudo lookahead queries to better approximate the true decoding-stage queries.\nBy using these lookahead queries as the observation window for importance\nestimation, LAQ achieves more consistent and accurate KV cache eviction aligned\nwith real inference scenarios. Experimental results on LongBench and\nNeedle-in-a-Haystack benchmarks show that LAQ outperforms existing methods\nacross various budget levels, achieving a 1 $\\sim$ 4 point improvement on\nLongBench under limited cache budget. Moreover, LAQ is complementary to\nexisting approaches and can be flexibly combined to yield further improvements."}
{"id": "2505.20339", "pdf": "https://arxiv.org/pdf/2505.20339.pdf", "abs": "https://arxiv.org/abs/2505.20339", "title": "Challenges for artificial cognitive systems", "authors": ["Antoni Gomila", "Vincent C. Müller"], "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "The declared goal of this paper is to fill this gap: \"... cognitive systems\nresearch needs questions or challenges that define progress. The challenges are\nnot (yet more) predictions of the future, but a guideline to what are the aims\nand what would constitute progress.\" -- the quotation being from the project\ndescription of EUCogII, the project for the European Network for Cognitive\nSystems within which this formulation of the 'challenges' was originally\ndeveloped (http://www.eucognition.org). So, we stick out our neck and formulate\nthe challenges for artificial cognitive systems. These challenges are\narticulated in terms of a definition of what a cognitive system is: a system\nthat learns from experience and uses its acquired knowledge (both declarative\nand practical) in a flexible manner to achieve its own goals."}
{"id": "2505.20335", "pdf": "https://arxiv.org/pdf/2505.20335.pdf", "abs": "https://arxiv.org/abs/2505.20335", "title": "Language Model Distillation: A Temporal Difference Imitation Learning Perspective", "authors": ["Zishun Yu", "Shangzhe Li", "Xinhua Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models have led to significant progress across many NLP tasks,\nalthough their massive sizes often incur substantial computational costs.\nDistillation has become a common practice to compress these large and highly\ncapable models into smaller, more efficient ones. Many existing language model\ndistillation methods can be viewed as behavior cloning from the perspective of\nimitation learning or inverse reinforcement learning. This viewpoint has\ninspired subsequent studies that leverage (inverse) reinforcement learning\ntechniques, including variations of behavior cloning and temporal difference\nlearning methods. Rather than proposing yet another specific temporal\ndifference method, we introduce a general framework for temporal\ndifference-based distillation by exploiting the distributional sparsity of the\nteacher model. Specifically, it is often observed that language models assign\nmost probability mass to a small subset of tokens. Motivated by this\nobservation, we design a temporal difference learning framework that operates\non a reduced action space (a subset of vocabulary), and demonstrate how\npractical algorithms can be derived and the resulting performance improvements."}
{"id": "2505.20466", "pdf": "https://arxiv.org/pdf/2505.20466.pdf", "abs": "https://arxiv.org/abs/2505.20466", "title": "Reconceptualizing Smart Microscopy: From Data Collection to Knowledge Creation by Multi-Agent Integration", "authors": ["P. S. Kesavan", "Pontus Nordenfelt"], "categories": ["cs.AI", "cs.HC", "cs.MA", "q-bio.QM"], "comment": "34 pages, 5 figures", "summary": "Smart microscopy represents a paradigm shift in biological imaging, moving\nfrom passive observation tools to active collaborators in scientific inquiry.\nEnabled by advances in automation, computational power, and artificial\nintelligence, these systems are now capable of adaptive decision-making and\nreal-time experimental control. Here, we introduce a theoretical framework that\nreconceptualizes smart microscopy as a partner in scientific investigation.\nCentral to our framework is the concept of the 'epistemic-empirical divide' in\ncellular investigation-the gap between what is observable (empirical domain)\nand what must be understood (epistemic domain). We propose six core design\nprinciples: epistemic-empirical awareness, hierarchical context integration, an\nevolution from detection to perception, adaptive measurement frameworks,\nnarrative synthesis capabilities, and cross-contextual reasoning. Together,\nthese principles guide a multi-agent architecture designed to align empirical\nobservation with the goals of scientific understanding. Our framework provides\na roadmap for building microscopy systems that go beyond automation to actively\nsupport hypothesis generation, insight discovery, and theory development,\nredefining the role of scientific instruments in the process of knowledge\ncreation."}
{"id": "2505.20336", "pdf": "https://arxiv.org/pdf/2505.20336.pdf", "abs": "https://arxiv.org/abs/2505.20336", "title": "MOSLIM:Align with diverse preferences in prompts through reward classification", "authors": ["Yu Zhang", "Wanli Jiang", "Zhengyu Yang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The multi-objective alignment of Large Language Models (LLMs) is essential\nfor ensuring foundational models conform to diverse human preferences. Current\nresearch in this field typically involves either multiple policies or multiple\nreward models customized for various preferences, or the need to train a\npreference-specific supervised fine-tuning (SFT) model. In this work, we\nintroduce a novel multi-objective alignment method, MOSLIM, which utilizes a\nsingle reward model and policy model to address diverse objectives. MOSLIM\nprovides a flexible way to control these objectives through prompting and does\nnot require preference training during SFT phase, allowing thousands of\noff-the-shelf models to be directly utilized within this training framework.\nMOSLIM leverages a multi-head reward model that classifies question-answer\npairs instead of scoring them and then optimize policy model with a scalar\nreward derived from a mapping function that converts classification results\nfrom reward model into reward scores. We demonstrate the efficacy of our\nproposed method across several multi-objective benchmarks and conduct ablation\nstudies on various reward model sizes and policy optimization methods. The\nMOSLIM method outperforms current multi-objective approaches in most results\nwhile requiring significantly fewer GPU computing resources compared with\nexisting policy optimization methods."}
{"id": "2505.20584", "pdf": "https://arxiv.org/pdf/2505.20584.pdf", "abs": "https://arxiv.org/abs/2505.20584", "title": "A Dashboard Approach to Monitoring Mpox-Related Discourse and Misinformation on Social Media", "authors": ["Linfeng", "Zhao", "Rishul Bhuvanagiri", "Blake Gonzales", "Kellen Sharp", "Dhiraj Murthy"], "categories": ["cs.SI", "cs.CY", "cs.HC", "J.4; I.2; I.7; K.4"], "comment": "7 pages, 3 figures", "summary": "Mpox (formerly monkeypox) is a zoonotic disease caused by an orthopoxvirus\nclosely related to variola and remains a significant global public health\nconcern. During outbreaks, social media platforms like X (formerly Twitter) can\nboth inform and misinform the public, complicating efforts to convey accurate\nhealth information. To support local response efforts, we developed a\nresearcher-focused dashboard for use by public health stakeholders and the\npublic that enables searching and visualizing mpox-related tweets through an\ninteractive interface. Following the CDC's designation of mpox as an emerging\nvirus in August 2024, our dashboard recorded a marked increase in tweet volume\ncompared to 2023, illustrating the rapid spread of health discourse across\ndigital platforms. These findings underscore the continued need for real-time\nsocial media monitoring tools to support public health communication and track\nevolving sentiment and misinformation trends at the local level."}
{"id": "2505.20338", "pdf": "https://arxiv.org/pdf/2505.20338.pdf", "abs": "https://arxiv.org/abs/2505.20338", "title": "Assessing the Capability of LLMs in Solving POSCOMP Questions", "authors": ["Cayo Viegas", "Rohit Gheyi", "Márcio Ribeiro"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nexpanded the capabilities of artificial intelligence in natural language\nprocessing tasks. Despite this progress, their performance in specialized\ndomains such as computer science remains relatively unexplored. Understanding\nthe proficiency of LLMs in these domains is critical for evaluating their\npractical utility and guiding future developments. The POSCOMP, a prestigious\nBrazilian examination used for graduate admissions in computer science promoted\nby the Brazlian Computer Society (SBC), provides a challenging benchmark. This\nstudy investigates whether LLMs can match or surpass human performance on the\nPOSCOMP exam. Four LLMs - ChatGPT-4, Gemini 1.0 Advanced, Claude 3 Sonnet, and\nLe Chat Mistral Large - were initially evaluated on the 2022 and 2023 POSCOMP\nexams. The assessments measured the models' proficiency in handling complex\nquestions typical of the exam. LLM performance was notably better on text-based\nquestions than on image interpretation tasks. In the 2022 exam, ChatGPT-4 led\nwith 57 correct answers out of 69 questions, followed by Gemini 1.0 Advanced\n(49), Le Chat Mistral (48), and Claude 3 Sonnet (44). Similar trends were\nobserved in the 2023 exam. ChatGPT-4 achieved the highest performance,\nsurpassing all students who took the POSCOMP 2023 exam. LLMs, particularly\nChatGPT-4, show promise in text-based tasks on the POSCOMP exam, although image\ninterpretation remains a challenge. Given the rapid evolution of LLMs, we\nexpanded our analysis to include more recent models - o1, Gemini 2.5 Pro,\nClaude 3.7 Sonnet, and o3-mini-high - evaluated on the 2022-2024 POSCOMP exams.\nThese newer models demonstrate further improvements and consistently surpass\nboth the average and top-performing human participants across all three years."}
{"id": "2505.20667", "pdf": "https://arxiv.org/pdf/2505.20667.pdf", "abs": "https://arxiv.org/abs/2505.20667", "title": "How Do Experts Make Sense of Integrated Process Models?", "authors": ["Tianwa Chen", "Barbara Weber", "Graeme Shanks", "Gianluca Demartini", "Marta Indulska", "Shazia Sadiq"], "categories": ["cs.IR", "cs.HC", "cs.SE"], "comment": null, "summary": "A range of integrated modeling approaches have been developed to enable a\nholistic representation of business process logic together with all relevant\nbusiness rules. These approaches address inherent problems with separate\ndocumentation of business process models and business rules. In this study, we\nexplore how expert process workers make sense of the information provided\nthrough such integrated modeling approaches. To do so, we complement verbal\nprotocol analysis with eye-tracking metrics to reveal nuanced user behaviours\ninvolved in the main phases of sensemaking, namely information foraging and\ninformation processing. By studying expert process workers engaged in tasks\nbased on integrated modeling of business processes and rules, we provide\ninsights that pave the way for a better understanding of sensemaking practices\nand improved development of business process and business rule integration\napproaches. Our research underscores the importance of offering personalized\nsupport mechanisms that increase the efficacy and efficiency of sensemaking\npractices for process knowledge workers."}
{"id": "2505.20340", "pdf": "https://arxiv.org/pdf/2505.20340.pdf", "abs": "https://arxiv.org/abs/2505.20340", "title": "Dynamic Manifold Evolution Theory: Modeling and Stability Analysis of Latent Representations in Large Language Models", "authors": ["Yukun Zhang", "Qi Dong"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We introduce Dynamic Manifold Evolution Theory (DMET),a unified framework\nthat models large language model generation as a controlled dynamical system\nevolving on a low_dimensional semantic manifold. By casting latent_state\nupdates as discrete time Euler approximations of continuous dynamics, we map\nintrinsic energy_driven flows and context_dependent forces onto Transformer\ncomponents (residual connections, attention, feed-forward networks). Leveraging\nLyapunov stability theory We define three empirical metrics (state continuity,\nclustering quality, topological persistence) that quantitatively link\nlatent_trajectory properties to text fluency, grammaticality, and semantic\ncoherence. Extensive experiments across decoding parameters validate DMET's\npredictions and yield principled guidelines for balancing creativity and\nconsistency in text generation."}
{"id": "2505.20676", "pdf": "https://arxiv.org/pdf/2505.20676.pdf", "abs": "https://arxiv.org/abs/2505.20676", "title": "Supervised Contrastive Learning for Ordinal Engagement Measurement", "authors": ["Sadaf Safa", "Ali Abedi", "Shehroz S. Khan"], "categories": ["cs.CV", "cs.HC"], "comment": "9 pages, 1 figure, 5 tables", "summary": "Student engagement plays a crucial role in the successful delivery of\neducational programs. Automated engagement measurement helps instructors\nmonitor student participation, identify disengagement, and adapt their teaching\nstrategies to enhance learning outcomes effectively. This paper identifies two\nkey challenges in this problem: class imbalance and incorporating order into\nengagement levels rather than treating it as mere categories. Then, a novel\napproach to video-based student engagement measurement in virtual learning\nenvironments is proposed that utilizes supervised contrastive learning for\nordinal classification of engagement. Various affective and behavioral features\nare extracted from video samples and utilized to train ordinal classifiers\nwithin a supervised contrastive learning framework (with a sequential\nclassifier as the encoder). A key step involves the application of diverse\ntime-series data augmentation techniques to these feature vectors, enhancing\nmodel training. The effectiveness of the proposed method was evaluated using a\npublicly available dataset for engagement measurement, DAiSEE, containing\nvideos of students who participated in virtual learning programs. The results\ndemonstrate the robust ability of the proposed method for the classification of\nthe engagement level. This approach promises a significant contribution to\nunderstanding and enhancing student engagement in virtual learning\nenvironments."}
{"id": "2505.20343", "pdf": "https://arxiv.org/pdf/2505.20343.pdf", "abs": "https://arxiv.org/abs/2505.20343", "title": "Do LLMs have a Gender (Entropy) Bias?", "authors": ["Sonal Prabhune", "Balaji Padmanabhan", "Kaushik Dutta"], "categories": ["cs.CL", "cs.AI", "68T42, 68T50", "I.2.7"], "comment": "18 pages, 4 figures", "summary": "We investigate the existence and persistence of a specific type of gender\nbias in some of the popular LLMs and contribute a new benchmark dataset,\nRealWorldQuestioning (released on HuggingFace ), developed from real-world\nquestions across four key domains in business and health contexts: education,\njobs, personal financial management, and general health. We define and study\nentropy bias, which we define as a discrepancy in the amount of information\ngenerated by an LLM in response to real questions users have asked. We tested\nthis using four different LLMs and evaluated the generated responses both\nqualitatively and quantitatively by using ChatGPT-4o (as \"LLM-as-judge\"). Our\nanalyses (metric-based comparisons and \"LLM-as-judge\" evaluation) suggest that\nthere is no significant bias in LLM responses for men and women at a category\nlevel. However, at a finer granularity (the individual question level), there\nare substantial differences in LLM responses for men and women in the majority\nof cases, which \"cancel\" each other out often due to some responses being\nbetter for males and vice versa. This is still a concern since typical users of\nthese tools often ask a specific question (only) as opposed to several varied\nones in each of these common yet important areas of life. We suggest a simple\ndebiasing approach that iteratively merges the responses for the two genders to\nproduce a final result. Our approach demonstrates that a simple, prompt-based\ndebiasing strategy can effectively debias LLM outputs, thus producing responses\nwith higher information content than both gendered variants in 78% of the\ncases, and consistently achieving a balanced integration in the remaining\ncases."}
{"id": "2505.20679", "pdf": "https://arxiv.org/pdf/2505.20679.pdf", "abs": "https://arxiv.org/abs/2505.20679", "title": "SELF-PERCEPT: Introspection Improves Large Language Models' Detection of Multi-Person Mental Manipulation in Conversations", "authors": ["Danush Khanna", "Pratinav Seth", "Sidhaarth Sredharan Murali", "Aditya Kumar Guru", "Siddharth Shukla", "Tanuj Tyagi", "Sandeep Chaurasia", "Kripabandhu Ghosh"], "categories": ["cs.CL", "cs.HC", "cs.LG"], "comment": "Accepted to ACL 2025 (Main)", "summary": "Mental manipulation is a subtle yet pervasive form of abuse in interpersonal\ncommunication, making its detection critical for safeguarding potential\nvictims. However, due to manipulation's nuanced and context-specific nature,\nidentifying manipulative language in complex, multi-turn, and multi-person\nconversations remains a significant challenge for large language models (LLMs).\nTo address this gap, we introduce the MultiManip dataset, comprising 220\nmulti-turn, multi-person dialogues balanced between manipulative and\nnon-manipulative interactions, all drawn from reality shows that mimic\nreal-world scenarios. For manipulative interactions, it includes 11 distinct\nmanipulations depicting real-life scenarios. We conduct extensive evaluations\nof state-of-the-art LLMs, such as GPT-4o and Llama-3.1-8B, employing various\nprompting strategies. Despite their capabilities, these models often struggle\nto detect manipulation effectively. To overcome this limitation, we propose\nSELF-PERCEPT, a novel, two-stage prompting framework inspired by\nSelf-Perception Theory, demonstrating strong performance in detecting\nmulti-person, multi-turn mental manipulation. Our code and data are publicly\navailable at https://github.com/danushkhanna/self-percept ."}
{"id": "2505.20347", "pdf": "https://arxiv.org/pdf/2505.20347.pdf", "abs": "https://arxiv.org/abs/2505.20347", "title": "SeRL: Self-Play Reinforcement Learning for Large Language Models with Limited Data", "authors": ["Wenkai Fang", "Shunyu Liu", "Yang Zhou", "Kongcheng Zhang", "Tongya Zheng", "Kaixuan Chen", "Mingli Song", "Dacheng Tao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances have demonstrated the effectiveness of Reinforcement Learning\n(RL) in improving the reasoning capabilities of Large Language Models (LLMs).\nHowever, existing works inevitably rely on high-quality instructions and\nverifiable rewards for effective training, both of which are often difficult to\nobtain in specialized domains. In this paper, we propose Self-play\nReinforcement Learning(SeRL) to bootstrap LLM training with limited initial\ndata. Specifically, SeRL comprises two complementary modules: self-instruction\nand self-rewarding. The former module generates additional instructions based\non the available data at each training step, employing robust online filtering\nstrategies to ensure instruction quality, diversity, and difficulty. The latter\nmodule introduces a simple yet effective majority-voting mechanism to estimate\nresponse rewards for additional instructions, eliminating the need for external\nannotations. Finally, SeRL performs conventional RL based on the generated\ndata, facilitating iterative self-play learning. Extensive experiments on\nvarious reasoning benchmarks and across different LLM backbones demonstrate\nthat the proposed SeRL yields results superior to its counterparts and achieves\nperformance on par with those obtained by high-quality data with verifiable\nrewards. Our code is available at https://github.com/wantbook-book/SeRL."}
{"id": "2505.20701", "pdf": "https://arxiv.org/pdf/2505.20701.pdf", "abs": "https://arxiv.org/abs/2505.20701", "title": "System-driven Cloud Architecture Design Support with Structured State Management and Guided Decision Assistance", "authors": ["Ryosuke Kohita", "Akira Kasuga"], "categories": ["cs.SE", "cs.HC"], "comment": null, "summary": "Cloud architecture design is a complex process requiring both technical\nexpertise and architectural knowledge to develop solutions from frequently\nambiguous requirements. We present CloudArchitectBuddy, a system-driven cloud\narchitecture design support application with two key mechanisms: (1) structured\nstate management that enhances design understanding through explicit\nrepresentation of requirements and architectural decisions, and (2) guided\ndecision assistance that facilitates design progress through proactive\nverification and requirement refinement. Our study with 16 industry\npractitioners showed that while our approach achieved comparable design quality\nto a chat interface, participants rated our system higher for usability and\nappreciated its ability to help understand architectural relationships and\nidentify missing requirements. However, participants also expressed a need for\nuser-initiated interactions where they could freely provide design instructions\nand engage in detailed discussions with LLMs. These results suggest that\nintegrating a chat interface into our structured and guided workflow approach\nwould create a more practical solution, balancing systematic design support\nwith conversational flexibility for comprehensive cloud architecture\ndevelopment."}
{"id": "2505.20354", "pdf": "https://arxiv.org/pdf/2505.20354.pdf", "abs": "https://arxiv.org/abs/2505.20354", "title": "Rethinking Text-based Protein Understanding: Retrieval or LLM?", "authors": ["Juntong Wu", "Zijing Liu", "He Cao", "Hao Li", "Bin Feng", "Zishan Shu", "Ke Yu", "Li Yuan", "Yu Li"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In recent years, protein-text models have gained significant attention for\ntheir potential in protein generation and understanding. Current approaches\nfocus on integrating protein-related knowledge into large language models\nthrough continued pretraining and multi-modal alignment, enabling simultaneous\ncomprehension of textual descriptions and protein sequences. Through a thorough\nanalysis of existing model architectures and text-based protein understanding\nbenchmarks, we identify significant data leakage issues present in current\nbenchmarks. Moreover, conventional metrics derived from natural language\nprocessing fail to accurately assess the model's performance in this domain. To\naddress these limitations, we reorganize existing datasets and introduce a\nnovel evaluation framework based on biological entities. Motivated by our\nobservation, we propose a retrieval-enhanced method, which significantly\noutperforms fine-tuned LLMs for protein-to-text generation and shows accuracy\nand efficiency in training-free scenarios. Our code and data can be seen at\nhttps://github.com/IDEA-XL/RAPM."}
{"id": "2505.20894", "pdf": "https://arxiv.org/pdf/2505.20894.pdf", "abs": "https://arxiv.org/abs/2505.20894", "title": "DeepConvContext: A Multi-Scale Approach to Timeseries Classification in Human Activity Recognition", "authors": ["Marius Bock", "Michael Moeller", "Kristof Van Laerhoven"], "categories": ["cs.LG", "cs.HC", "eess.IV"], "comment": "7 pages, 3 figures", "summary": "Despite recognized limitations in modeling long-range temporal dependencies,\nHuman Activity Recognition (HAR) has traditionally relied on a sliding window\napproach to segment labeled datasets. Deep learning models like the\nDeepConvLSTM typically classify each window independently, thereby restricting\nlearnable temporal context to within-window information. To address this\nconstraint, we propose DeepConvContext, a multi-scale time series\nclassification framework for HAR. Drawing inspiration from the vision-based\nTemporal Action Localization community, DeepConvContext models both intra- and\ninter-window temporal patterns by processing sequences of time-ordered windows.\nUnlike recent HAR models that incorporate attention mechanisms, DeepConvContext\nrelies solely on LSTMs -- with ablation studies demonstrating the superior\nperformance of LSTMs over attention-based variants for modeling inertial sensor\ndata. Across six widely-used HAR benchmarks, DeepConvContext achieves an\naverage 10% improvement in F1-score over the classic DeepConvLSTM, with gains\nof up to 21%. Code to reproduce our experiments is publicly available via\ngithub.com/mariusbock/context_har."}
{"id": "2505.20415", "pdf": "https://arxiv.org/pdf/2505.20415.pdf", "abs": "https://arxiv.org/abs/2505.20415", "title": "Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision", "authors": ["Xingwei Tan", "Marco Valentino", "Mahmud Akhter", "Maria Liakata", "Nikolaos Aletras"], "categories": ["cs.CL"], "comment": "Work in progress", "summary": "Large language models (LLMs) have shown promising performance in mathematical\nand logical reasoning benchmarks. However, recent studies have pointed to\nmemorization, rather than generalization, as one of the leading causes for such\nperformance. LLMs, in fact, are susceptible to content variations,\ndemonstrating a lack of robust symbolic abstractions supporting their reasoning\nprocess. To improve reliability, many attempts have been made to combine LLMs\nwith symbolic methods. Nevertheless, existing approaches fail to effectively\nleverage symbolic representations due to the challenges involved in developing\nreliable and scalable verification mechanisms. In this paper, we propose to\novercome such limitations by generating symbolic reasoning trajectories and\nselect the high-quality ones using a process reward model automatically tuned\nbased on Monte Carlo estimation. The trajectories are then employed via\nfine-tuning methods to improve logical reasoning and generalization. Our\nresults on logical reasoning benchmarks such as FOLIO and LogicAsker show the\neffectiveness of the proposed method with large gains on frontier and\nopen-weight models. Moreover, additional experiments on claim verification\nreveal that fine-tuning on the generated symbolic reasoning trajectories\nenhances out-of-domain generalizability, suggesting the potential impact of\nsymbolically-guided process supervision in alleviating the effect of\nmemorization on LLM reasoning."}
{"id": "2505.20918", "pdf": "https://arxiv.org/pdf/2505.20918.pdf", "abs": "https://arxiv.org/abs/2505.20918", "title": "Humble AI in the real-world: the case of algorithmic hiring", "authors": ["Rahul Nair", "Inge Vejsbjerg", "Elizabeth Daly", "Christos Varytimidis", "Bran Knowles"], "categories": ["cs.LG", "cs.AI", "cs.HC"], "comment": "CHIWORK '25, Symposium on Human-Computer Interaction for Work, June\n  23--25, 2025, Amsterdam, Netherlands Late Breaking Work", "summary": "Humble AI (Knowles et al., 2023) argues for cautiousness in AI development\nand deployments through scepticism (accounting for limitations of statistical\nlearning), curiosity (accounting for unexpected outcomes), and commitment\n(accounting for multifaceted values beyond performance). We present a\nreal-world case study for humble AI in the domain of algorithmic hiring.\nSpecifically, we evaluate virtual screening algorithms in a widely used hiring\nplatform that matches candidates to job openings. There are several challenges\nin misrecognition and stereotyping in such contexts that are difficult to\nassess through standard fairness and trust frameworks; e.g., someone with a\nnon-traditional background is less likely to rank highly. We demonstrate\ntechnical feasibility of how humble AI principles can be translated to practice\nthrough uncertainty quantification of ranks, entropy estimates, and a user\nexperience that highlights algorithmic unknowns. We describe preliminary\ndiscussions with focus groups made up of recruiters. Future user studies seek\nto evaluate whether the higher cognitive load of a humble AI system fosters a\nclimate of trust in its outcomes."}
{"id": "2505.20416", "pdf": "https://arxiv.org/pdf/2505.20416.pdf", "abs": "https://arxiv.org/abs/2505.20416", "title": "GraphGen: Enhancing Supervised Fine-Tuning for LLMs with Knowledge-Driven Synthetic Data Generation", "authors": ["Zihong Chen", "Wanli Jiang", "Jinzhe Li", "Zhonghang Yuan", "Huanjun Kong", "Wanli Ouyang", "Nanqing Dong"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Fine-tuning for large language models (LLMs) typically requires substantial\namounts of high-quality supervised data, which is both costly and\nlabor-intensive to acquire. While synthetic data generation has emerged as a\npromising solution, existing approaches frequently suffer from factual\ninaccuracies, insufficient long-tail coverage, simplistic knowledge structures,\nand homogenized outputs. To address these challenges, we introduce GraphGen, a\nknowledge graph-guided framework designed for three key question-answering (QA)\nscenarios: atomic QA, aggregated QA, and multi-hop QA. It begins by\nconstructing a fine-grained knowledge graph from the source text. It then\nidentifies knowledge gaps in LLMs using the expected calibration error metric,\nprioritizing the generation of QA pairs that target high-value, long-tail\nknowledge. Furthermore, GraphGen incorporates multi-hop neighborhood sampling\nto capture complex relational information and employs style-controlled\ngeneration to diversify the resulting QA data. Experimental results on\nknowledge-intensive tasks under closed-book settings demonstrate that GraphGen\noutperforms conventional synthetic data methods, offering a more reliable and\ncomprehensive solution to the data scarcity challenge in supervised\nfine-tuning. The code and data are publicly available at\nhttps://github.com/open-sciencelab/GraphGen."}
{"id": "2505.20924", "pdf": "https://arxiv.org/pdf/2505.20924.pdf", "abs": "https://arxiv.org/abs/2505.20924", "title": "Label Leakage in Federated Inertial-based Human Activity Recognition", "authors": ["Marius Bock", "Maximilian Hopp", "Kristof Van Laerhoven", "Michael Moeller"], "categories": ["cs.LG", "cs.HC"], "comment": "7 pages, 4 figures", "summary": "While prior work has shown that Federated Learning updates can leak sensitive\ninformation, label reconstruction attacks, which aim to recover input labels\nfrom shared gradients, have not yet been examined in the context of Human\nActivity Recognition (HAR). Given the sensitive nature of activity labels, this\nstudy evaluates the effectiveness of state-of-the-art gradient-based label\nleakage attacks on HAR benchmark datasets. Our findings show that the number of\nactivity classes, sampling strategy, and class imbalance are critical factors\ninfluencing the extent of label leakage, with reconstruction accuracies\nreaching up to 90% on two benchmark datasets, even for trained models.\nMoreover, we find that Local Differential Privacy techniques such as gradient\nnoise and clipping offer only limited protection, as certain attacks still\nreliably infer both majority and minority class labels. We conclude by offering\npractical recommendations for the privacy-aware deployment of federated HAR\nsystems and identify open challenges for future research. Code to reproduce our\nexperiments is publicly available via github.com/mariusbock/leakage_har."}
{"id": "2505.20422", "pdf": "https://arxiv.org/pdf/2505.20422.pdf", "abs": "https://arxiv.org/abs/2505.20422", "title": "SEMMA: A Semantic Aware Knowledge Graph Foundation Model", "authors": ["Arvindh Arun", "Sumit Kumar", "Mojtaba Nayyeri", "Bo Xiong", "Ponnurangam Kumaraguru", "Antonio Vergari", "Steffen Staab"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Knowledge Graph Foundation Models (KGFMs) have shown promise in enabling\nzero-shot reasoning over unseen graphs by learning transferable patterns.\nHowever, most existing KGFMs rely solely on graph structure, overlooking the\nrich semantic signals encoded in textual attributes. We introduce SEMMA, a\ndual-module KGFM that systematically integrates transferable textual semantics\nalongside structure. SEMMA leverages Large Language Models (LLMs) to enrich\nrelation identifiers, generating semantic embeddings that subsequently form a\ntextual relation graph, which is fused with the structural component. Across 54\ndiverse KGs, SEMMA outperforms purely structural baselines like ULTRA in fully\ninductive link prediction. Crucially, we show that in more challenging\ngeneralization settings, where the test-time relation vocabulary is entirely\nunseen, structural methods collapse while SEMMA is 2x more effective. Our\nfindings demonstrate that textual semantics are critical for generalization in\nsettings where structure alone fails, highlighting the need for foundation\nmodels that unify structural and linguistic signals in knowledge reasoning."}
{"id": "2505.21016", "pdf": "https://arxiv.org/pdf/2505.21016.pdf", "abs": "https://arxiv.org/abs/2505.21016", "title": "Racism, Resistance, and Reddit: How Popular Culture Sparks Online Reckonings", "authors": ["Sherry Mason", "Tawfiq Ammari"], "categories": ["cs.CY", "cs.HC"], "comment": null, "summary": "This study examines how Reddit users engaged with the racial narratives of\nLovecraft Country and Watchmen, two television series that reimagine historical\nracial trauma. Drawing on narrative persuasion and multistep flow theory, we\nanalyze 3,879 Reddit comments using topic modeling and critical discourse\nanalysis. We identify three dynamic social roles advocates, adversaries, and\nadaptives and explore how users move between them in response to racial\ndiscourse. Findings reveal how Reddits pseudonymous affordances shape role\nfluidity, opinion leadership, and moral engagement. While adversaries minimized\nor rejected racism as exaggerated, advocates shared standpoint experiences and\nhistorical resources to challenge these claims. Adaptive users shifted\nperspectives over time, demonstrating how online publics can foster critical\nracial learning. This research highlights how popular culture and participatory\nplatforms intersect in shaping collective meaning making around race and\nhistorical memory."}
{"id": "2505.20428", "pdf": "https://arxiv.org/pdf/2505.20428.pdf", "abs": "https://arxiv.org/abs/2505.20428", "title": "The UD-NewsCrawl Treebank: Reflections and Challenges from a Large-scale Tagalog Syntactic Annotation Project", "authors": ["Angelina A. Aquino", "Lester James V. Miranda", "Elsie Marie T. Or"], "categories": ["cs.CL"], "comment": "Link to treebank:\n  https://huggingface.co/datasets/UD-Filipino/UD_Tagalog-NewsCrawl ; All\n  authors contributed equally in this work", "summary": "This paper presents UD-NewsCrawl, the largest Tagalog treebank to date,\ncontaining 15.6k trees manually annotated according to the Universal\nDependencies framework. We detail our treebank development process, including\ndata collection, pre-processing, manual annotation, and quality assurance\nprocedures. We provide baseline evaluations using multiple transformer-based\nmodels to assess the performance of state-of-the-art dependency parsers on\nTagalog. We also highlight challenges in the syntactic analysis of Tagalog\ngiven its distinctive grammatical properties, and discuss its implications for\nthe annotation of this treebank. We anticipate that UD-NewsCrawl and our\nbaseline model implementations will serve as valuable resources for advancing\ncomputational linguistics research in underrepresented languages like Tagalog."}
{"id": "2505.21153", "pdf": "https://arxiv.org/pdf/2505.21153.pdf", "abs": "https://arxiv.org/abs/2505.21153", "title": "THE WASTIVE: An Interactive Ebb and Flow of Digital Fabrication Waste", "authors": ["Yifan Shan", "Bo Liu", "Sebastian Bidegain", "Thijs Roumen"], "categories": ["cs.MM", "cs.HC"], "comment": "video demo: https://youtu.be/Yh3dmKYNP-8", "summary": "What if digital fabrication waste could observe the world? What would they\nsee? What would they say? \"THE WASTIVE\" reimagines digital fabrication waste as\nsentient observers, giving them a poetic voice through interactive art. As\nviewers approach, the installation awakens, mimicking the rhythmic ebb and flow\nof ocean waves - a silent dialogue where discarded materials \"observe\" and\nrespond to human presence. These interactions echo the gentle murmurs of the\nsea, transforming technological residue into a reflective, sensory experience.\nThrough this artistic contemplation, \"THE WASTIVE\" invites audiences to\nreconsider their creative processes and consumption habits. It serves as a\npoetic call for more mindful, sustainable practices, provoking deeper\nreflections on our interconnectedness with the environment."}
{"id": "2505.20429", "pdf": "https://arxiv.org/pdf/2505.20429.pdf", "abs": "https://arxiv.org/abs/2505.20429", "title": "PreP-OCR: A Complete Pipeline for Document Image Restoration and Enhanced OCR Accuracy", "authors": ["Shuhao Guan", "Moule Lin", "Cheng Xu", "Xinyi Liu", "Jinman Zhao", "Jiexin Fan", "Qi Xu", "Derek Greene"], "categories": ["cs.CL", "cs.CV"], "comment": "ACL 2025 main", "summary": "This paper introduces PreP-OCR, a two-stage pipeline that combines document\nimage restoration with semantic-aware post-OCR correction to improve text\nextraction from degraded historical documents. Our key innovation lies in\njointly optimizing image clarity and linguistic consistency. First, we generate\nsynthetic image pairs with randomized text fonts, layouts, and degradations. An\nimage restoration model is trained on this synthetic data, using\nmulti-directional patch extraction and fusion to process large images. Second,\na ByT5 post-corrector, fine-tuned on synthetic historical text training pairs,\naddresses any remaining OCR errors. Detailed experiments on 13,831 pages of\nreal historical documents in English, French, and Spanish show that PreP-OCR\npipeline reduces character error rates by 63.9-70.3\\% compared to OCR on raw\nimages. Our pipeline demonstrates the potential of integrating image\nrestoration with linguistic error correction for digitizing historical\narchives."}
{"id": "2505.21362", "pdf": "https://arxiv.org/pdf/2505.21362.pdf", "abs": "https://arxiv.org/abs/2505.21362", "title": "Evaluating LLM Adaptation to Sociodemographic Factors: User Profile vs. Dialogue History", "authors": ["Qishuai Zhong", "Zongmin Li", "Siqi Fan", "Aixin Sun"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Effective engagement by large language models (LLMs) requires adapting\nresponses to users' sociodemographic characteristics, such as age, occupation,\nand education level. While many real-world applications leverage dialogue\nhistory for contextualization, existing evaluations of LLMs' behavioral\nadaptation often focus on single-turn prompts. In this paper, we propose a\nframework to evaluate LLM adaptation when attributes are introduced either (1)\nexplicitly via user profiles in the prompt or (2) implicitly through multi-turn\ndialogue history. We assess the consistency of model behavior across these\nmodalities. Using a multi-agent pipeline, we construct a synthetic dataset\npairing dialogue histories with distinct user profiles and employ questions\nfrom the Value Survey Module (VSM 2013) (Hofstede and Hofstede, 2016) to probe\nvalue expression. Our findings indicate that most models adjust their expressed\nvalues in response to demographic changes, particularly in age and education\nlevel, but consistency varies. Models with stronger reasoning capabilities\ndemonstrate greater alignment, indicating the importance of reasoning in robust\nsociodemographic adaptation."}
{"id": "2505.20438", "pdf": "https://arxiv.org/pdf/2505.20438.pdf", "abs": "https://arxiv.org/abs/2505.20438", "title": "HAMburger: Accelerating LLM Inference via Token Smashing", "authors": ["Jingyu Liu", "Ce Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "The growing demand for efficient Large Language Model (LLM) inference\nrequires a holistic optimization on algorithms, systems, and hardware. However,\nvery few works have fundamentally changed the generation pattern: each token\nneeds one forward pass and one KV cache. This can be sub-optimal because we\nfound that LLMs are extremely capable of self-identifying the exact dose of\ninformation that a single KV cache can store, and many tokens can be generated\nconfidently without global context. Based on this insight, we introduce\nHAMburger, a Hierarchically Auto-regressive Model that redefines resource\nallocation in LLMs by moving beyond uniform computation and storage per token\nduring inference. Stacking a compositional embedder and a micro-step decoder in\nbetween a base LLM, HAMburger smashes multiple tokens into a single KV and\ngenerates several tokens per step. Additionally, HAMburger functions as a\nspeculative decoding framework where it can blindly trust self-drafted tokens.\nAs a result, HAMburger shifts the growth of KV cache and forward FLOPs from\nlinear to sub-linear with respect to output length, and adjusts its inference\nspeed based on query perplexity and output structure. Extensive evaluations\nshow that HAMburger reduces the KV cache computation by up to 2$\\times$ and\nachieves up to 2$\\times$ TPS, while maintaining quality in both short- and\nlong-context tasks. Our method explores an extremely challenging inference\nregime that requires both computation- and memory-efficiency with a\nhardware-agnostic design."}
{"id": "2505.21396", "pdf": "https://arxiv.org/pdf/2505.21396.pdf", "abs": "https://arxiv.org/abs/2505.21396", "title": "Improving Research Idea Generation Through Data: An Empirical Investigation in Social Science", "authors": ["Xiao Liu", "Xinyi Dong", "Xinyang Gao", "Yansong Feng", "Xun Pang"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have shown promise in\ngenerating novel research ideas. However, these ideas often face challenges\nrelated to feasibility and expected effectiveness. This paper explores how\naugmenting LLMs with relevant data during the idea generation process can\nenhance the quality of generated ideas. We introduce two ways of incorporating\ndata: (1) providing metadata during the idea generation stage to guide LLMs\ntoward feasible directions, and (2) adding automatic validation during the idea\nselection stage to assess the empirical plausibility of hypotheses within\nideas. We conduct experiments in the social science domain, specifically with\nclimate negotiation topics, and find that metadata improves the feasibility of\ngenerated ideas by 20%, while automatic validation improves the overall quality\nof selected ideas by 7%. A human study shows that LLM-generated ideas, along\nwith their related data and validation processes, inspire researchers to\npropose research ideas with higher quality. Our work highlights the potential\nof data-driven research idea generation, and underscores the practical utility\nof LLM-assisted ideation in real-world academic settings."}
{"id": "2505.20445", "pdf": "https://arxiv.org/pdf/2505.20445.pdf", "abs": "https://arxiv.org/abs/2505.20445", "title": "In-context Language Learning for Endangered Languages in Speech Recognition", "authors": ["Zhaolin Li", "Jan Niehues"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "With approximately 7,000 languages spoken worldwide, current large language\nmodels (LLMs) support only a small subset. Prior research indicates LLMs can\nlearn new languages for certain tasks without supervised data. We extend this\ninvestigation to speech recognition, investigating whether LLMs can learn\nunseen, low-resource languages through in-context learning (ICL). With\nexperiments on four diverse endangered languages that LLMs have not been\ntrained on, we find that providing more relevant text samples enhances\nperformance in both language modelling and Automatic Speech Recognition (ASR)\ntasks. Furthermore, we show that the probability-based approach outperforms the\ntraditional instruction-based approach in language learning. Lastly, we show\nICL enables LLMs to achieve ASR performance that is comparable to or even\nsurpasses dedicated language models trained specifically for these languages,\nwhile preserving the original capabilities of the LLMs."}
{"id": "2505.15974", "pdf": "https://arxiv.org/pdf/2505.15974.pdf", "abs": "https://arxiv.org/abs/2505.15974", "title": "Real-Time Stress Monitoring, Detection, and Management in College Students: A Wearable Technology and Machine-Learning Approach", "authors": ["Alan Ta", "Nilsu Salgin", "Mustafa Demir", "Kala Phillips Reindel", "Ranjana K. Mehta", "Anthony McDonald", "Carly McCord", "Farzan Sasangohar"], "categories": ["cs.HC", "cs.LG"], "comment": "30 pages, 5 figures", "summary": "College students are increasingly affected by stress, anxiety, and\ndepression, yet face barriers to traditional mental health care. This study\nevaluated the efficacy of a mobile health (mHealth) intervention, Mental Health\nEvaluation and Lookout Program (mHELP), which integrates a smartwatch sensor\nand machine learning (ML) algorithms for real-time stress detection and\nself-management. In a 12-week randomized controlled trial (n = 117),\nparticipants were assigned to a treatment group using mHELP's full suite of\ninterventions or a control group using the app solely for real-time stress\nlogging and weekly psychological assessments. The primary outcome, \"Moments of\nStress\" (MS), was assessed via physiological and self-reported indicators and\nanalyzed using Generalized Linear Mixed Models (GLMM) approaches. Similarly,\nsecondary outcomes of psychological assessments, including the Generalized\nAnxiety Disorder-7 (GAD-7) for anxiety, the Patient Health Questionnaire\n(PHQ-8) for depression, and the Perceived Stress Scale (PSS), were also\nanalyzed via GLMM. The finding of the objective measure, MS, indicates a\nsubstantial decrease in MS among the treatment group compared to the control\ngroup, while no notable between-group differences were observed in subjective\nscores of anxiety (GAD-7), depression (PHQ-8), or stress (PSS). However, the\ntreatment group exhibited a clinically meaningful decline in GAD-7 and PSS\nscores. These findings underscore the potential of wearable-enabled mHealth\ntools to reduce acute stress in college populations and highlight the need for\nextended interventions and tailored features to address chronic symptoms like\ndepression."}
{"id": "2505.20451", "pdf": "https://arxiv.org/pdf/2505.20451.pdf", "abs": "https://arxiv.org/abs/2505.20451", "title": "Amulet: Putting Complex Multi-Turn Conversations on the Stand with LLM Juries", "authors": ["Sahana Ramnath", "Anurag Mudgil", "Brihi Joshi", "Skyler Hallinan", "Xiang Ren"], "categories": ["cs.CL"], "comment": null, "summary": "Today, large language models are widely used as judges to evaluate responses\nfrom other language models. Hence, it is imperative to benchmark and improve\nthese LLM-judges on real-world language model usage: a typical human-assistant\nconversation is lengthy, and shows significant diversity in topics, intents,\nand requirements across turns, e.g. social interactions, task requests,\nfeedback. We present Amulet, a framework that leverages pertinent linguistic\nconcepts of dialog-acts and maxims to improve the accuracy of LLM-judges on\npreference data with complex, multi-turn conversational context. Amulet\npresents valuable insights about (a) the communicative structures and intents\npresent in the conversation (dialog acts), and (b) the satisfaction of\nconversational principles (maxims) by the preference responses, and uses them\nto make judgments. On four challenging datasets, Amulet shows that (a) humans\nfrequently (60 to 70 percent of the time) change their intents from one turn of\nthe conversation to the next, and (b) in 75 percent of instances, the\npreference responses can be differentiated via dialog acts and/or maxims,\nreiterating the latter's significance in judging such data. Amulet can be used\neither as a judge by applying the framework to a single LLM, or integrated into\na jury with different LLM judges; our judges and juries show strong\nimprovements on relevant baselines for all four datasets."}
{"id": "2505.17629", "pdf": "https://arxiv.org/pdf/2505.17629.pdf", "abs": "https://arxiv.org/abs/2505.17629", "title": "TransBench: Breaking Barriers for Transferable Graphical User Interface Agents in Dynamic Digital Environments", "authors": ["Yuheng Lu", "Qian Yu", "Hongru Wang", "Zeming Liu", "Wei Su", "Yanping Liu", "Yuhang Guo", "Maocheng Liang", "Yunhong Wang", "Haifeng Wang"], "categories": ["cs.HC", "cs.AI"], "comment": "Accepted by ACL 2025 Findings", "summary": "Graphical User Interface (GUI) agents, which autonomously operate on digital\ninterfaces through natural language instructions, hold transformative potential\nfor accessibility, automation, and user experience. A critical aspect of their\nfunctionality is grounding - the ability to map linguistic intents to visual\nand structural interface elements. However, existing GUI agents often struggle\nto adapt to the dynamic and interconnected nature of real-world digital\nenvironments, where tasks frequently span multiple platforms and applications\nwhile also being impacted by version updates. To address this, we introduce\nTransBench, the first benchmark designed to systematically evaluate and enhance\nthe transferability of GUI agents across three key dimensions: cross-version\ntransferability (adapting to version updates), cross-platform transferability\n(generalizing across platforms like iOS, Android, and Web), and\ncross-application transferability (handling tasks spanning functionally\ndistinct apps). TransBench includes 15 app categories with diverse\nfunctionalities, capturing essential pages across versions and platforms to\nenable robust evaluation. Our experiments demonstrate significant improvements\nin grounding accuracy, showcasing the practical utility of GUI agents in\ndynamic, real-world environments. Our code and data will be publicly available\nat GitHub."}
{"id": "2505.20482", "pdf": "https://arxiv.org/pdf/2505.20482.pdf", "abs": "https://arxiv.org/abs/2505.20482", "title": "Conversation Kernels: A Flexible Mechanism to Learn Relevant Context for Online Conversation Understanding", "authors": ["Vibhor Agarwal", "Arjoo Gupta", "Suparna De", "Nishanth Sastry"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at International AAAI Conference on Web and Social Media\n  (ICWSM) 2025", "summary": "Understanding online conversations has attracted research attention with the\ngrowth of social networks and online discussion forums. Content analysis of\nposts and replies in online conversations is difficult because each individual\nutterance is usually short and may implicitly refer to other posts within the\nsame conversation. Thus, understanding individual posts requires capturing the\nconversational context and dependencies between different parts of a\nconversation tree and then encoding the context dependencies between posts and\ncomments/replies into the language model.\n  To this end, we propose a general-purpose mechanism to discover appropriate\nconversational context for various aspects about an online post in a\nconversation, such as whether it is informative, insightful, interesting or\nfunny. Specifically, we design two families of Conversation Kernels, which\nexplore different parts of the neighborhood of a post in the tree representing\nthe conversation and through this, build relevant conversational context that\nis appropriate for each task being considered. We apply our developed method to\nconversations crawled from slashdot.org, which allows users to apply highly\ndifferent labels to posts, such as 'insightful', 'funny', etc., and therefore\nprovides an ideal experimental platform to study whether a framework such as\nConversation Kernels is general-purpose and flexible enough to be adapted to\ndisparately different conversation understanding tasks."}
{"id": "2505.19419", "pdf": "https://arxiv.org/pdf/2505.19419.pdf", "abs": "https://arxiv.org/abs/2505.19419", "title": "It's Not Just Labeling -- A Research on LLM Generated Feedback Interpretability and Image Labeling Sketch Features", "authors": ["Baichuan Li", "Larry Powell", "Tracy Hammond"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "The quality of training data is critical to the performance of machine\nlearning applications in domains like transportation, healthcare, and robotics.\nAccurate image labeling, however, often relies on time-consuming, expert-driven\nmethods with limited feedback. This research introduces a sketch-based\nannotation approach supported by large language models (LLMs) to reduce\ntechnical barriers and enhance accessibility. Using a synthetic dataset, we\nexamine how sketch recognition features relate to LLM feedback metrics, aiming\nto improve the reliability and interpretability of LLM-assisted labeling. We\nalso explore how prompting strategies and sketch variations influence feedback\nquality. Our main contribution is a sketch-based virtual assistant that\nsimplifies annotation for non-experts and advances LLM-driven labeling tools in\nterms of scalability, accessibility, and explainability."}
{"id": "2505.20487", "pdf": "https://arxiv.org/pdf/2505.20487.pdf", "abs": "https://arxiv.org/abs/2505.20487", "title": "InFact: Informativeness Alignment for Improved LLM Factuality", "authors": ["Roi Cohen", "Russa Biswas", "Gerard de Melo"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Factual completeness is a general term that captures how detailed and\ninformative a factually correct text is. For instance, the factual sentence\n``Barack Obama was born in the United States'' is factually correct, though\nless informative than the factual sentence ``Barack Obama was born in Honolulu,\nHawaii, United States''. Despite the known fact that LLMs tend to hallucinate\nand generate factually incorrect text, they might also tend to choose to\ngenerate factual text that is indeed factually correct and yet less informative\nthan other, more informative choices. In this work, we tackle this problem by\nproposing an informativeness alignment mechanism. This mechanism takes\nadvantage of recent factual benchmarks to propose an informativeness alignment\nobjective. This objective prioritizes answers that are both correct and\ninformative. A key finding of our work is that when training a model to\nmaximize this objective or optimize its preference, we can improve not just\ninformativeness but also factuality."}
{"id": "2301.06216", "pdf": "https://arxiv.org/pdf/2301.06216.pdf", "abs": "https://arxiv.org/abs/2301.06216", "title": "CogReact: A Reinforced Framework to Model Human Cognitive Reaction Modulated by Dynamic Intervention", "authors": ["Songlin Xu", "Xinyu Zhang"], "categories": ["cs.AI", "cs.HC"], "comment": "30 pages, 11 figures", "summary": "Using deep neural networks as computational models to simulate cognitive\nprocess can provide key insights into human behavioral dynamics. Challenges\narise when environments are highly dynamic, obscuring stimulus-behavior\nrelationships. However, the majority of current research focuses on simulating\nhuman cognitive behaviors under ideal conditions, neglecting the influence of\nenvironmental disturbances. We propose CogReact, integrating drift-diffusion\nwith deep reinforcement learning to simulate granular effects of dynamic\nenvironmental stimuli on human cognitive process. Quantitatively, it improves\ncognition modelling by considering temporal effect of environmental stimuli on\ncognitive process and captures both subject-specific and stimuli-specific\nbehavioural differences. Qualitatively, it captures general trends in human\ncognitive process under stimuli, better than baselines. Our approach is\nexamined in diverse environmental influences on various cognitive tasks.\nOverall, it demonstrates a powerful, data-driven methodology to simulate, align\nwith, and understand the vagaries of human cognitive response in dynamic\ncontexts."}
{"id": "2505.20496", "pdf": "https://arxiv.org/pdf/2505.20496.pdf", "abs": "https://arxiv.org/abs/2505.20496", "title": "Inceptive Transformers: Enhancing Contextual Representations through Multi-Scale Feature Learning Across Domains and Languages", "authors": ["Asif Shahriar", "Rifat Shahriyar", "M Saifur Rahman"], "categories": ["cs.CL"], "comment": null, "summary": "Conventional transformer models typically compress the information from all\ntokens in a sequence into a single \\texttt{[CLS]} token to represent global\ncontext-- an approach that can lead to information loss in tasks requiring\nlocalized or hierarchical cues. In this work, we introduce \\textit{Inceptive\nTransformer}, a modular and lightweight architecture that enriches\ntransformer-based token representations by integrating a multi-scale feature\nextraction module inspired by inception networks. Our model is designed to\nbalance local and global dependencies by dynamically weighting tokens based on\ntheir relevance to a particular task. Evaluation across a diverse range of\ntasks including emotion recognition (both English and Bangla), irony detection,\ndisease identification, and anti-COVID vaccine tweets classification shows that\nour models consistently outperform the baselines by 1\\% to 14\\% while\nmaintaining efficiency. These findings highlight the versatility and\ncross-lingual applicability of our method for enriching transformer-based\nrepresentations across diverse domains."}
{"id": "2504.18010", "pdf": "https://arxiv.org/pdf/2504.18010.pdf", "abs": "https://arxiv.org/abs/2504.18010", "title": "Sky-Drive: A Distributed Multi-Agent Simulation Platform for Human-AI Collaborative and Socially-Aware Future Transportation", "authors": ["Zilin Huang", "Zihao Sheng", "Zhengyang Wan", "Yansong Qu", "Yuhao Luo", "Boyue Wang", "Pei Li", "Yen-Jung Chen", "Jiancong Chen", "Keke Long", "Jiayi Meng", "Yue Leng", "Sikai Chen"], "categories": ["cs.RO", "cs.AI", "cs.HC"], "comment": "14 pages, 7 figures", "summary": "Recent advances in autonomous system simulation platforms have significantly\nenhanced the safe and scalable testing of driving policies. However, existing\nsimulators do not yet fully meet the needs of future transportation\nresearch-particularly in enabling effective human-AI collaboration and modeling\nsocially-aware driving agents. This paper introduces Sky-Drive, a novel\ndistributed multi-agent simulation platform that addresses these limitations\nthrough four key innovations: (a) a distributed architecture for synchronized\nsimulation across multiple terminals; (b) a multi-modal human-in-the-loop\nframework integrating diverse sensors to collect rich behavioral data; (c) a\nhuman-AI collaboration mechanism supporting continuous and adaptive knowledge\nexchange; and (d) a digital twin framework for constructing high-fidelity\nvirtual replicas of real-world transportation environments. Sky-Drive supports\ndiverse applications such as autonomous vehicle-human road users interaction\nmodeling, human-in-the-loop training, socially-aware reinforcement learning,\npersonalized driving development, and customized scenario generation. Future\nextensions will incorporate foundation models for context-aware decision\nsupport and hardware-in-the-loop testing for real-world validation. By bridging\nscenario generation, data collection, algorithm training, and hardware\nintegration, Sky-Drive has the potential to become a foundational platform for\nthe next generation of human-centered and socially-aware autonomous\ntransportation systems research. The demo video and code are available\nat:https://sky-lab-uw.github.io/Sky-Drive-website/"}
{"id": "2505.20500", "pdf": "https://arxiv.org/pdf/2505.20500.pdf", "abs": "https://arxiv.org/abs/2505.20500", "title": "Beyond Keywords: Evaluating Large Language Model Classification of Nuanced Ableism", "authors": ["Naba Rizvi", "Harper Strickland", "Saleha Ahmedi", "Aekta Kallepalli", "Isha Khirwadkar", "William Wu", "Imani N. S. Munyaka", "Nedjma Ousidhoum"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly used in decision-making tasks\nlike r\\'esum\\'e screening and content moderation, giving them the power to\namplify or suppress certain perspectives. While previous research has\nidentified disability-related biases in LLMs, little is known about how they\nconceptualize ableism or detect it in text. We evaluate the ability of four\nLLMs to identify nuanced ableism directed at autistic individuals. We examine\nthe gap between their understanding of relevant terminology and their\neffectiveness in recognizing ableist content in context. Our results reveal\nthat LLMs can identify autism-related language but often miss harmful or\noffensive connotations. Further, we conduct a qualitative comparison of human\nand LLM explanations. We find that LLMs tend to rely on surface-level keyword\nmatching, leading to context misinterpretations, in contrast to human\nannotators who consider context, speaker identity, and potential impact. On the\nother hand, both LLMs and humans agree on the annotation scheme, suggesting\nthat a binary classification is adequate for evaluating LLM performance, which\nis consistent with findings from prior studies involving human annotators."}
{"id": "2505.11808", "pdf": "https://arxiv.org/pdf/2505.11808.pdf", "abs": "https://arxiv.org/abs/2505.11808", "title": "Human-Centered Development of Guide Dog Robots: Quiet and Stable Locomotion Control", "authors": ["Shangqun Yu", "Hochul Hwang", "Trung M. Dang", "Joydeep Biswas", "Nicholas A. Giudice", "Sunghoon Ivan Lee", "Donghyun Kim"], "categories": ["cs.RO", "cs.HC"], "comment": null, "summary": "A quadruped robot is a promising system that can offer assistance comparable\nto that of dog guides due to its similar form factor. However, various\nchallenges remain in making these robots a reliable option for blind and\nlow-vision (BLV) individuals. Among these challenges, noise and jerky motion\nduring walking are critical drawbacks of existing quadruped robots. While these\nissues have largely been overlooked in guide dog robot research, our interviews\nwith guide dog handlers and trainers revealed that acoustic and physical\ndisturbances can be particularly disruptive for BLV individuals, who rely\nheavily on environmental sounds for navigation. To address these issues, we\ndeveloped a novel walking controller for slow stepping and smooth foot\nswing/contact while maintaining human walking speed, as well as robust and\nstable balance control. The controller integrates with a perception system to\nfacilitate locomotion over non-flat terrains, such as stairs. Our controller\nwas extensively tested on the Unitree Go1 robot and, when compared with other\ncontrol methods, demonstrated significant noise reduction -- half of the\ndefault locomotion controller. In this study, we adopt a mixed-methods approach\nto evaluate its usability with BLV individuals. In our indoor walking\nexperiments, participants compared our controller to the robot's default\ncontroller. Results demonstrated superior acceptance of our controller,\nhighlighting its potential to improve the user experience of guide dog robots.\nVideo demonstration (best viewed with audio) available at:\nhttps://youtu.be/8-pz_8Hqe6s."}
{"id": "2505.20501", "pdf": "https://arxiv.org/pdf/2505.20501.pdf", "abs": "https://arxiv.org/abs/2505.20501", "title": "Gatsby Without the 'E': Crafting Lipograms with LLMs", "authors": ["Rohan Balasubramanian", "Nitish Gokulakrishnan", "Syeda Jannatus Saba", "Steven Skiena"], "categories": ["cs.CL"], "comment": "7.5 pages", "summary": "Lipograms are a unique form of constrained writing where all occurrences of a\nparticular letter are excluded from the text, typified by the novel Gadsby,\nwhich daringly avoids all usage of the letter 'e'. In this study, we explore\nthe power of modern large language models (LLMs) by transforming the novel F.\nScott Fitzgerald's The Great Gatsby into a fully 'e'-less text. We experimented\nwith a range of techniques, from baseline methods like synonym replacement to\nsophisticated generative models enhanced with beam search and named entity\nanalysis. We show that excluding up to 3.6% of the most common letters (up to\nthe letter 'u') had minimal impact on the text's meaning, although translation\nfidelity rapidly and predictably decays with stronger lipogram constraints. Our\nwork highlights the surprising flexibility of English under strict constraints,\nrevealing just how adaptable and creative language can be."}
{"id": "2505.20505", "pdf": "https://arxiv.org/pdf/2505.20505.pdf", "abs": "https://arxiv.org/abs/2505.20505", "title": "Large Language Models for IT Automation Tasks: Are We There Yet?", "authors": ["Md Mahadi Hassan", "John Salvador", "Akond Rahman", "Santu Karmaker"], "categories": ["cs.CL", "cs.SE"], "comment": "8 pages", "summary": "LLMs show promise in code generation, yet their effectiveness for IT\nautomation tasks, particularly for tools like Ansible, remains understudied.\nExisting benchmarks rely primarily on synthetic tasks that fail to capture the\nneeds of practitioners who use IT automation tools, such as Ansible. We present\nITAB (IT Automation Task Benchmark), a benchmark of 126 diverse tasks (e.g.,\nconfiguring servers, managing files) where each task accounts for state\nreconciliation: a property unique to IT automation tools. ITAB evaluates LLMs'\nability to generate functional Ansible automation scripts via dynamic execution\nin controlled environments. We evaluate 14 open-source LLMs, none of which\naccomplish pass@10 at a rate beyond 12%. To explain these low scores, we\nanalyze 1,411 execution failures across the evaluated LLMs and identify two\nmain categories of prevalent semantic errors: failures in state reconciliation\nrelated reasoning (44.87% combined from variable (11.43%), host (11.84%),\npath(11.63%), and template (9.97%) issues) and deficiencies in module-specific\nexecution knowledge (24.37% combined from Attribute and parameter (14.44%) and\nmodule (9.93%) errors). Our findings reveal key limitations in open-source\nLLMs' ability to track state changes and apply specialized module knowledge,\nindicating that reliable IT automation will require major advances in state\nreasoning and domain-specific execution understanding."}
{"id": "2505.20506", "pdf": "https://arxiv.org/pdf/2505.20506.pdf", "abs": "https://arxiv.org/abs/2505.20506", "title": "ArVoice: A Multi-Speaker Dataset for Arabic Speech Synthesis", "authors": ["Hawau Olamide Toyin", "Rufael Marew", "Humaid Alblooshi", "Samar M. Magdy", "Hanan Aldarmaki"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "Accepted at INTERSPEECH 2025 The dataset is available at\n  https://huggingface.co/datasets/MBZUAI/ArVoice", "summary": "We introduce ArVoice, a multi-speaker Modern Standard Arabic (MSA) speech\ncorpus with diacritized transcriptions, intended for multi-speaker speech\nsynthesis, and can be useful for other tasks such as speech-based diacritic\nrestoration, voice conversion, and deepfake detection. ArVoice comprises: (1) a\nnew professionally recorded set from six voice talents with diverse\ndemographics, (2) a modified subset of the Arabic Speech Corpus; and (3)\nhigh-quality synthetic speech from two commercial systems. The complete corpus\nconsists of a total of 83.52 hours of speech across 11 voices; around 10 hours\nconsist of human voices from 7 speakers. We train three open-source TTS and two\nvoice conversion systems to illustrate the use cases of the dataset. The corpus\nis available for research use."}
{"id": "2505.20511", "pdf": "https://arxiv.org/pdf/2505.20511.pdf", "abs": "https://arxiv.org/abs/2505.20511", "title": "Multimodal Emotion Recognition in Conversations: A Survey of Methods, Trends, Challenges and Prospects", "authors": ["Chengyan Wu", "Yiqiang Cai", "Yang Liu", "Pengxu Zhu", "Yun Xue", "Ziwei Gong", "Julia Hirschberg", "Bolei Ma"], "categories": ["cs.CL"], "comment": null, "summary": "While text-based emotion recognition methods have achieved notable success,\nreal-world dialogue systems often demand a more nuanced emotional understanding\nthan any single modality can offer. Multimodal Emotion Recognition in\nConversations (MERC) has thus emerged as a crucial direction for enhancing the\nnaturalness and emotional understanding of human-computer interaction. Its goal\nis to accurately recognize emotions by integrating information from various\nmodalities such as text, speech, and visual signals.\n  This survey offers a systematic overview of MERC, including its motivations,\ncore tasks, representative methods, and evaluation strategies. We further\nexamine recent trends, highlight key challenges, and outline future directions.\nAs interest in emotionally intelligent systems grows, this survey provides\ntimely guidance for advancing MERC research."}
{"id": "2505.20538", "pdf": "https://arxiv.org/pdf/2505.20538.pdf", "abs": "https://arxiv.org/abs/2505.20538", "title": "AstroVisBench: A Code Benchmark for Scientific Computing and Visualization in Astronomy", "authors": ["Sebastian Antony Joseph", "Syed Murtaza Husain", "Stella S. R. Offner", "Stéphanie Juneau", "Paul Torrey", "Adam S. Bolton", "Juan P. Farias", "Niall Gaffney", "Greg Durrett", "Junyi Jessy Li"], "categories": ["cs.CL", "astro-ph.IM", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) are being explored for applications in\nscientific research, including their capabilities to synthesize literature,\nanswer research questions, generate research ideas, and even conduct\ncomputational experiments. Ultimately, our goal is for these to help scientists\nderive novel scientific insights. In many areas of science, such insights often\narise from processing and visualizing data to understand its patterns. However,\nevaluating whether an LLM-mediated scientific workflow produces outputs\nconveying the correct scientific insights is challenging to evaluate and has\nnot been addressed in past work. We introduce AstroVisBench, the first\nbenchmark for both scientific computing and visualization in the astronomy\ndomain. AstroVisBench judges a language model's ability to both (1) create\nastronomy-specific workflows to process and analyze data and (2) visualize the\nresults of these workflows through complex plots. Our evaluation of\nvisualizations uses a novel LLM-as-a-judge workflow, which is validated against\nannotation by five professional astronomers. Using AstroVisBench we present an\nevaluation of state-of-the-art language models, showing a significant gap in\ntheir ability to engage in astronomy research as useful assistants. This\nevaluation provides a strong end-to-end evaluation for AI scientists that\noffers a path forward for the development of visualization-based workflows,\nwhich are central to a broad range of domains from physics to biology."}
{"id": "2505.20546", "pdf": "https://arxiv.org/pdf/2505.20546.pdf", "abs": "https://arxiv.org/abs/2505.20546", "title": "Paths Not Taken: Understanding and Mending the Multilingual Factual Recall Pipeline", "authors": ["Meng Lu", "Ruochen Zhang", "Ellie Pavlick", "Carsten Eickhoff"], "categories": ["cs.CL"], "comment": null, "summary": "Multilingual large language models (LLMs) often exhibit factual\ninconsistencies across languages, with significantly better performance in\nfactual recall tasks in English than in other languages. The causes of these\nfailures, however, remain poorly understood. Using mechanistic analysis\ntechniques, we uncover the underlying pipeline that LLMs employ, which involves\nusing the English-centric factual recall mechanism to process multilingual\nqueries and then translating English answers back into the target language. We\nidentify two primary sources of error: insufficient engagement of the reliable\nEnglish-centric mechanism for factual recall, and incorrect translation from\nEnglish back into the target language for the final answer. To address these\nvulnerabilities, we introduce two vector interventions, both independent of\nlanguages and datasets, to redirect the model toward better internal paths for\nhigher factual consistency. Our interventions combined increase the recall\naccuracy by over 35 percent for the lowest-performing language. Our findings\ndemonstrate how mechanistic insights can be used to unlock latent multilingual\ncapabilities in LLMs."}
{"id": "2505.20564", "pdf": "https://arxiv.org/pdf/2505.20564.pdf", "abs": "https://arxiv.org/abs/2505.20564", "title": "The NaijaVoices Dataset: Cultivating Large-Scale, High-Quality, Culturally-Rich Speech Data for African Languages", "authors": ["Chris Emezue", "The NaijaVoices Community", "Busayo Awobade", "Abraham Owodunni", "Handel Emezue", "Gloria Monica Tobechukwu Emezue", "Nefertiti Nneoma Emezue", "Sewade Ogun", "Bunmi Akinremi", "David Ifeoluwa Adelani", "Chris Pal"], "categories": ["cs.CL"], "comment": "Accepted for publication at Interspeech 2025", "summary": "The development of high-performing, robust, and reliable speech technologies\ndepends on large, high-quality datasets. However, African languages --\nincluding our focus, Igbo, Hausa, and Yoruba -- remain under-represented due to\ninsufficient data. Popular voice-enabled technologies do not support any of the\n2000+ African languages, limiting accessibility for circa one billion people.\nWhile previous dataset efforts exist for the target languages, they lack the\nscale and diversity needed for robust speech models. To bridge this gap, we\nintroduce the NaijaVoices dataset, a 1,800-hour speech-text dataset with 5,000+\nspeakers. We outline our unique data collection approach, analyze its acoustic\ndiversity, and demonstrate its impact through finetuning experiments on\nautomatic speech recognition, averagely achieving 75.86% (Whisper), 52.06%\n(MMS), and 42.33% (XLSR) WER improvements. These results highlight NaijaVoices'\npotential to advance multilingual speech processing for African languages."}
{"id": "2505.20571", "pdf": "https://arxiv.org/pdf/2505.20571.pdf", "abs": "https://arxiv.org/abs/2505.20571", "title": "Emotion Classification In-Context in Spanish", "authors": ["Bipul Thapa", "Gabriel Cofre"], "categories": ["cs.CL", "cs.LG"], "comment": "This paper has been accepted and presented at the 4th International\n  Conference on Applied Intelligence and Informatics (AII 2024). The final\n  version will appear in the official conference proceedings. This preprint is\n  provided to ensure the timely dissemination of the research prior to formal\n  publication", "summary": "Classifying customer feedback into distinct emotion categories is essential\nfor understanding sentiment and improving customer experience. In this paper,\nwe classify customer feedback in Spanish into three emotion\ncategories--positive, neutral, and negative--using advanced NLP and ML\ntechniques. Traditional methods translate feedback from widely spoken languages\nto less common ones, resulting in a loss of semantic integrity and contextual\nnuances inherent to the original language. To address this limitation, we\npropose a hybrid approach that combines TF-IDF with BERT embeddings,\neffectively transforming Spanish text into rich numerical representations that\npreserve the semantic depth of the original language by using a Custom Stacking\nEnsemble (CSE) approach. To evaluate emotion classification, we utilize a range\nof models, including Logistic Regression, KNN, Bagging classifier with LGBM,\nand AdaBoost. The CSE model combines these classifiers as base models and uses\na one-vs-all Logistic Regression as the meta-model. Our experimental results\ndemonstrate that CSE significantly outperforms the individual and BERT model,\nachieving a test accuracy of 93.3% on the native Spanish dataset--higher than\nthe accuracy obtained from the translated version. These findings underscore\nthe challenges of emotion classification in Spanish and highlight the\nadvantages of combining vectorization techniques like TF-IDF with BERT for\nimproved accuracy. Our results provide valuable insights for businesses seeking\nto leverage emotion classification to enhance customer feedback analysis and\nservice improvements."}
{"id": "2505.20591", "pdf": "https://arxiv.org/pdf/2505.20591.pdf", "abs": "https://arxiv.org/abs/2505.20591", "title": "Effectiveness of Prompt Optimization in NL2SQL Systems", "authors": ["Sairam Gurajada", "Eser Kandogan", "Sajjadur Rahman"], "categories": ["cs.CL", "cs.DB"], "comment": null, "summary": "NL2SQL approaches have greatly benefited from the impressive capabilities of\nlarge language models (LLMs). In particular, bootstrapping an NL2SQL system for\na specific domain can be as simple as instructing an LLM with sufficient\ncontextual information, such as schema details and translation demonstrations.\nHowever, building an accurate system still requires the rigorous task of\nselecting the right context for each query-including identifying relevant\nschema elements, cell values, and suitable exemplars that help the LLM\nunderstand domain-specific nuances. Retrieval-based methods have become the\ngo-to approach for identifying such context. While effective, these methods\nintroduce additional inference-time costs due to the retrieval process.\n  In this paper, we argue that production scenarios demand high-precision,\nhigh-performance NL2SQL systems, rather than simply high-quality SQL\ngeneration, which is the focus of most current NL2SQL approaches. In such\nscenarios, the careful selection of a static set of exemplars-capturing the\nintricacies of the query log, target database, SQL constructs, and execution\nlatencies-plays a more crucial role than exemplar selection based solely on\nsimilarity. The key challenge, however, lies in identifying a representative\nset of exemplars for a given production setting. To this end, we propose a\nprompt optimization framework that not only addresses the high-precision\nrequirement but also optimizes the performance of the generated SQL through\nmulti-objective optimization. Preliminary empirical analysis demonstrates the\neffectiveness of the proposed framework."}
{"id": "2505.20606", "pdf": "https://arxiv.org/pdf/2505.20606.pdf", "abs": "https://arxiv.org/abs/2505.20606", "title": "Towards Pretraining Robust ASR Foundation Model with Acoustic-Aware Data Augmentation", "authors": ["Dancheng Liu", "Amir Nassereldine", "Chenhui Xu", "Jinjun Xiong"], "categories": ["cs.CL", "cs.MM"], "comment": "in submission", "summary": "Whisper's robust performance in automatic speech recognition (ASR) is often\nattributed to its massive 680k-hour training set, an impractical scale for most\nresearchers. In this work, we examine how linguistic and acoustic diversity in\ntraining data affect the robustness of the ASR model and reveal that\ntranscription generalization is primarily driven by acoustic variation rather\nthan linguistic richness. We find that targeted acoustic augmentation methods\ncould significantly improve the generalization ability of ASR models, reducing\nword-error rates by up to 19.24 percent on unseen datasets when training on the\n960-hour Librispeech dataset. These findings highlight strategic acoustically\nfocused data augmentation as a promising alternative to massive datasets for\nbuilding robust ASR models, offering a potential solution to future foundation\nASR models when massive human speech data is lacking."}
{"id": "2505.20613", "pdf": "https://arxiv.org/pdf/2505.20613.pdf", "abs": "https://arxiv.org/abs/2505.20613", "title": "REAL-Prover: Retrieval Augmented Lean Prover for Mathematical Reasoning", "authors": ["Ziju Shen", "Naohao Huang", "Fanyi Yang", "Yutong Wang", "Guoxiong Gao", "Tianyi Xu", "Jiedong Jiang", "Wanyi He", "Pu Yang", "Mengzhou Sun", "Haocheng Ju", "Peihao Wu", "Bryan Dai", "Bin Dong"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.LO"], "comment": null, "summary": "Nowadays, formal theorem provers have made monumental progress on high-school\nand competition-level mathematics, but few of them generalize to more advanced\nmathematics. In this paper, we present REAL-Prover, a new open-source stepwise\ntheorem prover for Lean 4 to push this boundary. This prover, based on our\nfine-tuned large language model (REAL-Prover-v1) and integrated with a\nretrieval system (Leansearch-PS), notably boosts performance on solving\ncollege-level mathematics problems. To train REAL-Prover-v1, we developed\nHERALD-AF, a data extraction pipeline that converts natural language math\nproblems into formal statements, and a new open-source Lean 4 interactive\nenvironment (Jixia-interactive) to facilitate synthesis data collection. In our\nexperiments, our prover using only supervised fine-tune achieves competitive\nresults with a 23.7% success rate (Pass@64) on the ProofNet dataset-comparable\nto state-of-the-art (SOTA) models. To further evaluate our approach, we\nintroduce FATE-M, a new benchmark focused on algebraic problems, where our\nprover achieves a SOTA success rate of 56.7% (Pass@64)."}
{"id": "2505.20622", "pdf": "https://arxiv.org/pdf/2505.20622.pdf", "abs": "https://arxiv.org/abs/2505.20622", "title": "SeqPO-SiMT: Sequential Policy Optimization for Simultaneous Machine Translation", "authors": ["Ting Xu", "Zhichao Huang", "Jiankai Sun", "Shanbo Cheng", "Wai Lam"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by The 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)", "summary": "We present Sequential Policy Optimization for Simultaneous Machine\nTranslation (SeqPO-SiMT), a new policy optimization framework that defines the\nsimultaneous machine translation (SiMT) task as a sequential decision making\nproblem, incorporating a tailored reward to enhance translation quality while\nreducing latency. In contrast to popular Reinforcement Learning from Human\nFeedback (RLHF) methods, such as PPO and DPO, which are typically applied in\nsingle-step tasks, SeqPO-SiMT effectively tackles the multi-step SiMT task.\nThis intuitive framework allows the SiMT LLMs to simulate and refine the SiMT\nprocess using a tailored reward. We conduct experiments on six datasets from\ndiverse domains for En to Zh and Zh to En SiMT tasks, demonstrating that\nSeqPO-SiMT consistently achieves significantly higher translation quality with\nlower latency. In particular, SeqPO-SiMT outperforms the supervised fine-tuning\n(SFT) model by 1.13 points in COMET, while reducing the Average Lagging by 6.17\nin the NEWSTEST2021 En to Zh dataset. While SiMT operates with far less context\nthan offline translation, the SiMT results of SeqPO-SiMT on 7B LLM surprisingly\nrival the offline translation of high-performing LLMs, including\nQwen-2.5-7B-Instruct and LLaMA-3-8B-Instruct."}
{"id": "2505.20624", "pdf": "https://arxiv.org/pdf/2505.20624.pdf", "abs": "https://arxiv.org/abs/2505.20624", "title": "POLAR: A Benchmark for Multilingual, Multicultural, and Multi-Event Online Polarization", "authors": ["Usman Naseem", "Juan Ren", "Saba Anwar", "Sarah Kohail", "Rudy Alexandro Garrido Veliz", "Robert Geislinger", "Aisha Jabr", "Idris Abdulmumin", "Laiba Qureshi", "Aarushi Ajay Borkar", "Maryam Ibrahim Mukhtar", "Abinew Ali Ayele", "Ibrahim Said Ahmad", "Adem Ali", "Martin Semmann", "Shamsuddeen Hassan Muhammad", "Seid Muhie Yimam"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "Online polarization poses a growing challenge for democratic discourse, yet\nmost computational social science research remains monolingual, culturally\nnarrow, or event-specific. We introduce POLAR, a multilingual, multicultural,\nand multievent dataset with over 23k instances in seven languages from diverse\nonline platforms and real-world events. Polarization is annotated along three\naxes: presence, type, and manifestation, using a variety of annotation\nplatforms adapted to each cultural context. We conduct two main experiments:\n(1) we fine-tune six multilingual pretrained language models in both\nmonolingual and cross-lingual setups; and (2) we evaluate a range of open and\nclosed large language models (LLMs) in few-shot and zero-shot scenarios.\nResults show that while most models perform well on binary polarization\ndetection, they achieve substantially lower scores when predicting polarization\ntypes and manifestations. These findings highlight the complex, highly\ncontextual nature of polarization and the need for robust, adaptable approaches\nin NLP and computational social science. All resources will be released to\nsupport further research and effective mitigation of digital polarization\nglobally."}
{"id": "2505.20625", "pdf": "https://arxiv.org/pdf/2505.20625.pdf", "abs": "https://arxiv.org/abs/2505.20625", "title": "Long Context Scaling: Divide and Conquer via Multi-Agent Question-driven Collaboration", "authors": ["Sibo Xiao", "Zixin Lin", "Wenyang Gao", "Yue Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Processing long contexts has become a critical capability for modern large\nlanguage models (LLMs). Existing works leverage agent-based divide-and-conquer\nmethods for processing long contexts. But these methods face crucial\nlimitations, including prohibitive accumulated latency and amplified\ninformation loss from excessive agent invocations, and the disruption of\ninherent textual dependencies by immoderate partitioning. In this paper, we\npropose a novel multi-agent framework XpandA (Expand-Agent) coupled with\nquestion-driven workflow and dynamic partitioning for robust long-context\nprocessing. XpandA overcomes these limitations through: 1) dynamic partitioning\nof long texts, which adaptively modulates the filling rate of context windows\nfor input sequences of vastly varying lengths; 2) question-guided protocol to\nupdate flat information ensembles within centralized shared memory,\nconstructing consistent inter-agent knowledge across partitions; and 3)\nselectively replaying specific partitions based on the state-tracking of\nquestion-information couples to promote the resolution of inverted-order\nstructures across partitions (e.g., flashbacks). We perform a comprehensive\nevaluation of XpandA on multiple long-context benchmarks with length varying\nfrom 1k to 1M, demonstrating XpandA's feasibility for processing ultra-long\nsequences and its significant effectiveness in enhancing the long-context\ncapabilities of various LLMs by achieving 20\\% improvements and 1.5x inference\nspeedup over baselines of full-context, RAG and previous agent-based methods."}
{"id": "2505.20633", "pdf": "https://arxiv.org/pdf/2505.20633.pdf", "abs": "https://arxiv.org/abs/2505.20633", "title": "Test-Time Learning for Large Language Models", "authors": ["Jinwu Hu", "Zhitian Zhang", "Guohao Chen", "Xutao Wen", "Chao Shuai", "Wei Luo", "Bin Xiao", "Yuanqing Li", "Mingkui Tan"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by ICML2025", "summary": "While Large Language Models (LLMs) have exhibited remarkable emergent\ncapabilities through extensive pre-training, they still face critical\nlimitations in generalizing to specialized domains and handling diverse\nlinguistic variations, known as distribution shifts. In this paper, we propose\na Test-Time Learning (TTL) paradigm for LLMs, namely TLM, which dynamically\nadapts LLMs to target domains using only unlabeled test data during testing.\nSpecifically, we first provide empirical evidence and theoretical insights to\nreveal that more accurate predictions from LLMs can be achieved by minimizing\nthe input perplexity of the unlabeled test data. Based on this insight, we\nformulate the Test-Time Learning process of LLMs as input perplexity\nminimization, enabling self-supervised enhancement of LLM performance.\nFurthermore, we observe that high-perplexity samples tend to be more\ninformative for model optimization. Accordingly, we introduce a Sample\nEfficient Learning Strategy that actively selects and emphasizes these\nhigh-perplexity samples for test-time updates. Lastly, to mitigate catastrophic\nforgetting and ensure adaptation stability, we adopt Low-Rank Adaptation (LoRA)\ninstead of full-parameter optimization, which allows lightweight model updates\nwhile preserving more original knowledge from the model. We introduce the\nAdaptEval benchmark for TTL and demonstrate through experiments that TLM\nimproves performance by at least 20% compared to original LLMs on domain\nknowledge adaptation."}
{"id": "2505.20645", "pdf": "https://arxiv.org/pdf/2505.20645.pdf", "abs": "https://arxiv.org/abs/2505.20645", "title": "STEER-BENCH: A Benchmark for Evaluating the Steerability of Large Language Models", "authors": ["Kai Chen", "Zihao He", "Taiwei Shi", "Kristina Lerman"], "categories": ["cs.CL"], "comment": null, "summary": "Steerability, or the ability of large language models (LLMs) to adapt outputs\nto align with diverse community-specific norms, perspectives, and communication\nstyles, is critical for real-world applications but remains under-evaluated. We\nintroduce Steer-Bench, a benchmark for assessing population-specific steering\nusing contrasting Reddit communities. Covering 30 contrasting subreddit pairs\nacross 19 domains, Steer-Bench includes over 10,000 instruction-response pairs\nand validated 5,500 multiple-choice question with corresponding silver labels\nto test alignment with diverse community norms. Our evaluation of 13 popular\nLLMs using Steer-Bench reveals that while human experts achieve an accuracy of\n81% with silver labels, the best-performing models reach only around 65%\naccuracy depending on the domain and configuration. Some models lag behind\nhuman-level alignment by over 15 percentage points, highlighting significant\ngaps in community-sensitive steerability. Steer-Bench is a benchmark to\nsystematically assess how effectively LLMs understand community-specific\ninstructions, their resilience to adversarial steering attempts, and their\nability to accurately represent diverse cultural and ideological perspectives."}
{"id": "2505.20650", "pdf": "https://arxiv.org/pdf/2505.20650.pdf", "abs": "https://arxiv.org/abs/2505.20650", "title": "FinTagging: An LLM-ready Benchmark for Extracting and Structuring Financial Information", "authors": ["Yan Wang", "Yang Ren", "Lingfei Qian", "Xueqing Peng", "Keyi Wang", "Yi Han", "Dongji Feng", "Xiao-Yang Liu", "Jimin Huang", "Qianqian Xie"], "categories": ["cs.CL", "cs.AI", "cs.CE"], "comment": null, "summary": "We introduce FinTagging, the first full-scope, table-aware XBRL benchmark\ndesigned to evaluate the structured information extraction and semantic\nalignment capabilities of large language models (LLMs) in the context of\nXBRL-based financial reporting. Unlike prior benchmarks that oversimplify XBRL\ntagging as flat multi-class classification and focus solely on narrative text,\nFinTagging decomposes the XBRL tagging problem into two subtasks: FinNI for\nfinancial entity extraction and FinCL for taxonomy-driven concept alignment. It\nrequires models to jointly extract facts and align them with the full 10k+\nUS-GAAP taxonomy across both unstructured text and structured tables, enabling\nrealistic, fine-grained evaluation. We assess a diverse set of LLMs under\nzero-shot settings, systematically analyzing their performance on both subtasks\nand overall tagging accuracy. Our results reveal that, while LLMs demonstrate\nstrong generalization in information extraction, they struggle with\nfine-grained concept alignment, particularly in disambiguating closely related\ntaxonomy entries. These findings highlight the limitations of existing LLMs in\nfully automating XBRL tagging and underscore the need for improved semantic\nreasoning and schema-aware modeling to meet the demands of accurate financial\ndisclosure. Code is available at our GitHub repository and data is at our\nHugging Face repository."}
{"id": "2505.20654", "pdf": "https://arxiv.org/pdf/2505.20654.pdf", "abs": "https://arxiv.org/abs/2505.20654", "title": "Chinese Cyberbullying Detection: Dataset, Method, and Validation", "authors": ["Yi Zhu", "Xin Zou", "Xindong Wu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Existing cyberbullying detection benchmarks were organized by the polarity of\nspeech, such as \"offensive\" and \"non-offensive\", which were essentially hate\nspeech detection. However, in the real world, cyberbullying often attracted\nwidespread social attention through incidents. To address this problem, we\npropose a novel annotation method to construct a cyberbullying dataset that\norganized by incidents. The constructed CHNCI is the first Chinese\ncyberbullying incident detection dataset, which consists of 220,676 comments in\n91 incidents. Specifically, we first combine three cyberbullying detection\nmethods based on explanations generation as an ensemble method to generate the\npseudo labels, and then let human annotators judge these labels. Then we\npropose the evaluation criteria for validating whether it constitutes a\ncyberbullying incident. Experimental results demonstrate that the constructed\ndataset can be a benchmark for the tasks of cyberbullying detection and\nincident prediction. To the best of our knowledge, this is the first study for\nthe Chinese cyberbullying incident detection task."}
{"id": "2505.20658", "pdf": "https://arxiv.org/pdf/2505.20658.pdf", "abs": "https://arxiv.org/abs/2505.20658", "title": "Enhancing Transformation from Natural Language to Signal Temporal Logic Using LLMs with Diverse External Knowledge", "authors": ["Yue Fang", "Zhi Jin", "Jie An", "Hongshen Chen", "Xiaohong Chen", "Naijun Zhan"], "categories": ["cs.CL"], "comment": "13 pages, 5 figures, published to ACL", "summary": "Temporal Logic (TL), especially Signal Temporal Logic (STL), enables precise\nformal specification, making it widely used in cyber-physical systems such as\nautonomous driving and robotics. Automatically transforming NL into STL is an\nattractive approach to overcome the limitations of manual transformation, which\nis time-consuming and error-prone. However, due to the lack of datasets,\nautomatic transformation currently faces significant challenges and has not\nbeen fully explored. In this paper, we propose an NL-STL dataset named\nSTL-Diversity-Enhanced (STL-DivEn), which comprises 16,000 samples enriched\nwith diverse patterns. To develop the dataset, we first manually create a\nsmall-scale seed set of NL-STL pairs. Next, representative examples are\nidentified through clustering and used to guide large language models (LLMs) in\ngenerating additional NL-STL pairs. Finally, diversity and accuracy are ensured\nthrough rigorous rule-based filters and human validation. Furthermore, we\nintroduce the Knowledge-Guided STL Transformation (KGST) framework, a novel\napproach for transforming natural language into STL, involving a\ngenerate-then-refine process based on external knowledge. Statistical analysis\nshows that the STL-DivEn dataset exhibits more diversity than the existing\nNL-STL dataset. Moreover, both metric-based and human evaluations indicate that\nour KGST approach outperforms baseline models in transformation accuracy on\nSTL-DivEn and DeepSTL datasets."}
{"id": "2505.20660", "pdf": "https://arxiv.org/pdf/2505.20660.pdf", "abs": "https://arxiv.org/abs/2505.20660", "title": "BacktrackAgent: Enhancing GUI Agent with Error Detection and Backtracking Mechanism", "authors": ["Qinzhuo Wu", "Pengzhi Gao", "Wei Liu", "Jian Luan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Graphical User Interface (GUI) agents have gained substantial attention due\nto their impressive capabilities to complete tasks through multiple\ninteractions within GUI environments. However, existing agents primarily focus\non enhancing the accuracy of individual actions and often lack effective\nmechanisms for detecting and recovering from errors. To address these\nshortcomings, we propose the BacktrackAgent, a robust framework that\nincorporates a backtracking mechanism to improve task completion efficiency.\nBacktrackAgent includes verifier, judger, and reflector components as modules\nfor error detection and recovery, while also applying judgment rewards to\nfurther enhance the agent's performance. Additionally, we develop a training\ndataset specifically designed for the backtracking mechanism, which considers\nthe outcome pages after action executions. Experimental results show that\nBacktrackAgent has achieved performance improvements in both task success rate\nand step accuracy on Mobile3M and Auto-UI benchmarks. Our data and code will be\nreleased upon acceptance."}
{"id": "2505.20664", "pdf": "https://arxiv.org/pdf/2505.20664.pdf", "abs": "https://arxiv.org/abs/2505.20664", "title": "Self-Route: Automatic Mode Switching via Capability Estimation for Efficient Reasoning", "authors": ["Yang He", "Xiao Ding", "Bibo Cai", "Yufei Zhang", "Kai Xiong", "Zhouhao Sun", "Bing Qin", "Ting Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While reasoning-augmented large language models (RLLMs) significantly enhance\ncomplex task performance through extended reasoning chains, they inevitably\nintroduce substantial unnecessary token consumption, particularly for simpler\nproblems where Short Chain-of-Thought (Short CoT) suffices. This overthinking\nphenomenon leads to inefficient resource usage without proportional accuracy\ngains. To address this issue, we propose Self-Route, a dynamic reasoning\nframework that automatically selects between general and reasoning modes based\non model capability estimation. Our approach introduces a lightweight\npre-inference stage to extract capability-aware embeddings from hidden layer\nrepresentations, enabling real-time evaluation of the model's ability to solve\nproblems. We further construct Gradient-10K, a model difficulty\nestimation-based dataset with dense complexity sampling, to train the router\nfor precise capability boundary detection. Extensive experiments demonstrate\nthat Self-Route achieves comparable accuracy to reasoning models while reducing\ntoken consumption by 30-55\\% across diverse benchmarks. The proposed framework\ndemonstrates consistent effectiveness across models with different parameter\nscales and reasoning paradigms, highlighting its general applicability and\npractical value."}
{"id": "2505.20674", "pdf": "https://arxiv.org/pdf/2505.20674.pdf", "abs": "https://arxiv.org/abs/2505.20674", "title": "Pretraining Language Models to Ponder in Continuous Space", "authors": ["Boyi Zeng", "Shixiang Song", "Siyuan Huang", "Yixuan Wang", "He Li", "Ziwei He", "Xinbing Wang", "Zhiyu Li", "Zhouhan Lin"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Humans ponder before articulating complex sentence elements, enabling deeper\ncognitive processing through focused effort. In this work, we introduce this\npondering process into language models by repeatedly invoking the forward\nprocess within a single token generation step. During pondering, instead of\ngenerating an actual token sampled from the prediction distribution, the model\nponders by yielding a weighted sum of all token embeddings according to the\npredicted token distribution. The generated embedding is then fed back as input\nfor another forward pass. We show that the model can learn to ponder in this\nway through self-supervised learning, without any human annotations. Our method\nis straightforward and can be seamlessly integrated with various existing\nlanguage models. Experiments across three widely used open-source\narchitectures-GPT-2, Pythia, and LLaMA-and extensive downstream task\nevaluations demonstrate the effectiveness and generality of our method. For\nlanguage modeling tasks, pondering language models achieve performance\ncomparable to vanilla models with twice the number of parameters. On 9\ndownstream benchmarks, our pondering-enhanced Pythia models significantly\noutperform the official Pythia models. Notably, pondering-enhanced Pythia-1B is\ncomparable to TinyLlama-1.1B, which is trained on 10 times more data. The code\nis available at https://github.com/LUMIA-Group/PonderingLM."}
{"id": "2505.20679", "pdf": "https://arxiv.org/pdf/2505.20679.pdf", "abs": "https://arxiv.org/abs/2505.20679", "title": "SELF-PERCEPT: Introspection Improves Large Language Models' Detection of Multi-Person Mental Manipulation in Conversations", "authors": ["Danush Khanna", "Pratinav Seth", "Sidhaarth Sredharan Murali", "Aditya Kumar Guru", "Siddharth Shukla", "Tanuj Tyagi", "Sandeep Chaurasia", "Kripabandhu Ghosh"], "categories": ["cs.CL", "cs.HC", "cs.LG"], "comment": "Accepted to ACL 2025 (Main)", "summary": "Mental manipulation is a subtle yet pervasive form of abuse in interpersonal\ncommunication, making its detection critical for safeguarding potential\nvictims. However, due to manipulation's nuanced and context-specific nature,\nidentifying manipulative language in complex, multi-turn, and multi-person\nconversations remains a significant challenge for large language models (LLMs).\nTo address this gap, we introduce the MultiManip dataset, comprising 220\nmulti-turn, multi-person dialogues balanced between manipulative and\nnon-manipulative interactions, all drawn from reality shows that mimic\nreal-world scenarios. For manipulative interactions, it includes 11 distinct\nmanipulations depicting real-life scenarios. We conduct extensive evaluations\nof state-of-the-art LLMs, such as GPT-4o and Llama-3.1-8B, employing various\nprompting strategies. Despite their capabilities, these models often struggle\nto detect manipulation effectively. To overcome this limitation, we propose\nSELF-PERCEPT, a novel, two-stage prompting framework inspired by\nSelf-Perception Theory, demonstrating strong performance in detecting\nmulti-person, multi-turn mental manipulation. Our code and data are publicly\navailable at https://github.com/danushkhanna/self-percept ."}
{"id": "2505.20693", "pdf": "https://arxiv.org/pdf/2505.20693.pdf", "abs": "https://arxiv.org/abs/2505.20693", "title": "Phir Hera Fairy: An English Fairytaler is a Strong Faker of Fluent Speech in Low-Resource Indian Languages", "authors": ["Praveen Srinivasa Varadhan", "Srija Anand", "Soma Siddhartha", "Mitesh M. Khapra"], "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "comment": null, "summary": "What happens when an English Fairytaler is fine-tuned on Indian languages? We\nevaluate how the English F5-TTS model adapts to 11 Indian languages, measuring\npolyglot fluency, voice-cloning, style-cloning, and code-mixing. We compare:\n(i) training from scratch, (ii) fine-tuning English F5 on Indian data, and\n(iii) fine-tuning on both Indian and English data to prevent forgetting.\nFine-tuning with only Indian data proves most effective and the resultant IN-F5\nis a near-human polyglot; that enables speakers of one language (e.g., Odia) to\nfluently speak in another (e.g., Hindi). Our results show English pretraining\naids low-resource TTS in reaching human parity. To aid progress in other\nlow-resource languages, we study data-constrained setups and arrive at a\ncompute optimal strategy. Finally, we show IN-F5 can synthesize unseen\nlanguages like Bhojpuri and Tulu using a human-in-the-loop approach for\nzero-resource TTS via synthetic data generation."}
{"id": "2505.20700", "pdf": "https://arxiv.org/pdf/2505.20700.pdf", "abs": "https://arxiv.org/abs/2505.20700", "title": "Beyond Templates: Dynamic Adaptation of Reasoning Demonstrations via Feasibility-Aware Exploration", "authors": ["Yong Wu", "Weihang Pan", "Ke Li", "Chen Binhui", "Ping Li", "Binbin Lin"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown remarkable reasoning capabilities,\nyet aligning such abilities to small language models (SLMs) remains a challenge\ndue to distributional mismatches and limited model capacity. Existing reasoning\ndatasets, typically designed for powerful LLMs, often lead to degraded\nperformance when directly applied to weaker models. In this work, we introduce\nDynamic Adaptation of Reasoning Trajectories (DART), a novel data adaptation\nframework that bridges the capability gap between expert reasoning trajectories\nand diverse SLMs. Instead of uniformly imitating expert steps, DART employs a\nselective imitation strategy guided by step-wise adaptability estimation via\nsolution simulation. When expert steps surpass the student's capacity --\nsignaled by an Imitation Gap -- the student autonomously explores alternative\nreasoning paths, constrained by outcome consistency. We validate DART across\nmultiple reasoning benchmarks and model scales, demonstrating that it\nsignificantly improves generalization and data efficiency over static\nfine-tuning. Our method enhances supervision quality by aligning training\nsignals with the student's reasoning capabilities, offering a scalable solution\nfor reasoning alignment in resource-constrained models."}
{"id": "2505.20707", "pdf": "https://arxiv.org/pdf/2505.20707.pdf", "abs": "https://arxiv.org/abs/2505.20707", "title": "Dissecting Physics Reasoning in Small Language Models: A Multi-Dimensional Analysis from an Educational Perspective", "authors": ["Nicy Scaria", "Silvester John Joseph Kennedy", "Diksha Seth", "Deepak Subramani"], "categories": ["cs.CL", "cs.AI", "physics.ed-ph"], "comment": null, "summary": "Small Language Models (SLMs) offer computational efficiency and\naccessibility, making them promising for educational applications. However,\ntheir capacity for complex reasoning, particularly in domains such as physics,\nremains underexplored. This study investigates the high school physics\nreasoning capabilities of state-of-the-art SLMs (under 4 billion parameters),\nincluding instruct versions of Llama 3.2, Phi 4 Mini, Gemma 3, and Qwen series.\nWe developed a comprehensive physics dataset from the OpenStax High School\nPhysics textbook, annotated according to Bloom's Taxonomy, with LaTeX and\nplaintext mathematical notations. A novel cultural contextualization approach\nwas applied to a subset, creating culturally adapted problems for Asian,\nAfrican, and South American/Australian contexts while preserving core physics\nprinciples. Using an LLM-as-a-judge framework with Google's Gemini 2.5 Flash,\nwe evaluated answer and reasoning chain correctness, along with calculation\naccuracy. The results reveal significant differences between the SLMs. Qwen 3\n1.7B achieved high `answer accuracy' (85%), but `fully correct reasoning' was\nsubstantially low (38%). The format of the mathematical notation had a\nnegligible impact on performance. SLMs exhibited varied performance across the\nphysics topics and showed a decline in reasoning quality with increasing\ncognitive and knowledge complexity. In particular, the consistency of reasoning\nwas largely maintained in diverse cultural contexts, especially by better\nperforming models. These findings indicate that, while SLMs can often find\ncorrect answers, their underlying reasoning is frequently flawed, suggesting an\noverreliance on pattern recognition. For SLMs to become reliable educational\ntools in physics, future development must prioritize enhancing genuine\nunderstanding and the generation of sound, verifiable reasoning chains over\nmere answer accuracy."}
{"id": "2505.20732", "pdf": "https://arxiv.org/pdf/2505.20732.pdf", "abs": "https://arxiv.org/abs/2505.20732", "title": "SPA-RL: Reinforcing LLM Agents via Stepwise Progress Attribution", "authors": ["Hanlin Wang", "Chak Tou Leong", "Jiashuo Wang", "Jian Wang", "Wenjie Li"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Reinforcement learning (RL) holds significant promise for training LLM agents\nto handle complex, goal-oriented tasks that require multi-step interactions\nwith external environments. However, a critical challenge when applying RL to\nthese agentic tasks arises from delayed rewards: feedback signals are typically\navailable only after the entire task is completed. This makes it non-trivial to\nassign delayed rewards to earlier actions, providing insufficient guidance\nregarding environmental constraints and hindering agent training. In this work,\nwe draw on the insight that the ultimate completion of a task emerges from the\ncumulative progress an agent makes across individual steps. We propose Stepwise\nProgress Attribution (SPA), a general reward redistribution framework that\ndecomposes the final reward into stepwise contributions, each reflecting its\nincremental progress toward overall task completion. To achieve this, we train\na progress estimator that accumulates stepwise contributions over a trajectory\nto match the task completion. During policy optimization, we combine the\nestimated per-step contribution with a grounding signal for actions executed in\nthe environment as the fine-grained, intermediate reward for effective agent\ntraining. Extensive experiments on common agent benchmarks (including Webshop,\nALFWorld, and VirtualHome) demonstrate that SPA consistently outperforms the\nstate-of-the-art method in both success rate (+2.5\\% on average) and grounding\naccuracy (+1.9\\% on average). Further analyses demonstrate that our method\nremarkably provides more effective intermediate rewards for RL training. Our\ncode is available at https://github.com/WangHanLinHenry/SPA-RL-Agent."}
{"id": "2505.20738", "pdf": "https://arxiv.org/pdf/2505.20738.pdf", "abs": "https://arxiv.org/abs/2505.20738", "title": "Silencer: From Discovery to Mitigation of Self-Bias in LLM-as-Benchmark-Generator", "authors": ["Peiwen Yuan", "Yiwei Li", "Shaoxiong Feng", "Xinglin Wang", "Yueqi Zhang", "Jiayi Shi", "Chuyi Tan", "Boyuan Pan", "Yao Hu", "Kan Li"], "categories": ["cs.CL"], "comment": null, "summary": "LLM-as-Benchmark-Generator methods have been widely studied as a supplement\nto human annotators for scalable evaluation, while the potential biases within\nthis paradigm remain underexplored. In this work, we systematically define and\nvalidate the phenomenon of inflated performance in models evaluated on their\nself-generated benchmarks, referred to as self-bias, and attribute it to\nsub-biases arising from question domain, language style, and wrong labels. On\nthis basis, we propose Silencer, a general framework that leverages the\nheterogeneity between multiple generators at both the sample and benchmark\nlevels to neutralize bias and generate high-quality, self-bias-silenced\nbenchmark. Experimental results across various settings demonstrate that\nSilencer can suppress self-bias to near zero, significantly improve evaluation\neffectiveness of the generated benchmark (with an average improvement from\n0.655 to 0.833 in Pearson correlation with high-quality human-annotated\nbenchmark), while also exhibiting strong generalizability."}
{"id": "2505.20767", "pdf": "https://arxiv.org/pdf/2505.20767.pdf", "abs": "https://arxiv.org/abs/2505.20767", "title": "CogniBench: A Legal-inspired Framework and Dataset for Assessing Cognitive Faithfulness of Large Language Models", "authors": ["Xiaqiang Tang", "Jian Li", "Keyu Hu", "Du Nan", "Xiaolong Li", "Xi Zhang", "Weigao Sun", "Sihong Xie"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025", "summary": "Faithfulness hallucination are claims generated by a Large Language Model\n(LLM) not supported by contexts provided to the LLM. Lacking assessment\nstandard, existing benchmarks only contain \"factual statements\" that rephrase\nsource materials without marking \"cognitive statements\" that make inference\nfrom the given context, making the consistency evaluation and optimization of\ncognitive statements difficult. Inspired by how an evidence is assessed in the\nlegislative domain, we design a rigorous framework to assess different levels\nof faithfulness of cognitive statements and create a benchmark dataset where we\nreveal insightful statistics. We design an annotation pipeline to create larger\nbenchmarks for different LLMs automatically, and the resulting larger-scale\nCogniBench-L dataset can be used to train accurate cognitive hallucination\ndetection model. We release our model and dataset at:\nhttps://github.com/FUTUREEEEEE/CogniBench"}
{"id": "2505.20776", "pdf": "https://arxiv.org/pdf/2505.20776.pdf", "abs": "https://arxiv.org/abs/2505.20776", "title": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long Sequences", "authors": ["Jungyoub Cha", "Hyunjong Kim", "Sungzoon Cho"], "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; C.4"], "comment": "8 pages, 3 figures. Under review at EMNLP 2025", "summary": "Speculative decoding is a widely adopted technique for accelerating inference\nin large language models (LLMs), but its performance degrades on long inputs\ndue to increased attention cost and reduced draft accuracy. We introduce\nSpecExtend, a drop-in enhancement that improves the performance of speculative\ndecoding on long sequences without any additional training. SpecExtend\nintegrates efficient attention mechanisms such as FlashAttention and Hybrid\nTree Attention into both the draft and target models, reducing latency across\nall stages. To improve draft accuracy and speed, we propose Cross-model\nRetrieval, a novel KV cache update strategy that uses the target model's\nattention scores to dynamically select relevant context for the draft model.\nExtensive evaluations on three long-context understanding datasets show that\nSpecExtend accelerates standard tree-based speculative decoding by up to 2.22x\nfor inputs up to 16K tokens, providing an effective solution for speculative\ndecoding of long sequences. The code is available at\nhttps://github.com/jycha98/SpecExtend ."}
{"id": "2505.20779", "pdf": "https://arxiv.org/pdf/2505.20779.pdf", "abs": "https://arxiv.org/abs/2505.20779", "title": "CHIMERA: A Knowledge Base of Idea Recombination in Scientific Literature", "authors": ["Noy Sternlicht", "Tom Hope"], "categories": ["cs.CL"], "comment": "Project page: https://noy-sternlicht.github.io/CHIMERA-Web", "summary": "A hallmark of human innovation is the process of recombination -- creating\noriginal ideas by integrating elements of existing mechanisms and concepts. In\nthis work, we automatically mine the scientific literature and build CHIMERA: a\nlarge-scale knowledge base (KB) of recombination examples. CHIMERA can be used\nto empirically explore at scale how scientists recombine concepts and take\ninspiration from different areas, or to train supervised machine learning\nmodels that learn to predict new creative cross-domain directions. To build\nthis KB, we present a novel information extraction task of extracting\nrecombination from scientific paper abstracts, collect a high-quality corpus of\nhundreds of manually annotated abstracts, and use it to train an LLM-based\nextraction model. The model is applied to a large corpus of papers in the AI\ndomain, yielding a KB of over 28K recombination examples. We analyze CHIMERA to\nexplore the properties of recombination in different subareas of AI. Finally,\nwe train a scientific hypothesis generation model using the KB, which predicts\nnew recombination directions that real-world researchers find inspiring. Our\ndata and code are available at https://github.cs.huji.ac.il/tomhope-lab/CHIMERA"}
{"id": "2505.20809", "pdf": "https://arxiv.org/pdf/2505.20809.pdf", "abs": "https://arxiv.org/abs/2505.20809", "title": "Improved Representation Steering for Language Models", "authors": ["Zhengxuan Wu", "Qinan Yu", "Aryaman Arora", "Christopher D. Manning", "Christopher Potts"], "categories": ["cs.CL"], "comment": "46 pages, 23 figures, preprint", "summary": "Steering methods for language models (LMs) seek to provide fine-grained and\ninterpretable control over model generations by variously changing model\ninputs, weights, or representations to adjust behavior. Recent work has shown\nthat adjusting weights or representations is often less effective than steering\nby prompting, for instance when wanting to introduce or suppress a particular\nconcept. We demonstrate how to improve representation steering via our new\nReference-free Preference Steering (RePS), a bidirectional\npreference-optimization objective that jointly does concept steering and\nsuppression. We train three parameterizations of RePS and evaluate them on\nAxBench, a large-scale model steering benchmark. On Gemma models with sizes\nranging from 2B to 27B, RePS outperforms all existing steering methods trained\nwith a language modeling objective and substantially narrows the gap with\nprompting -- while promoting interpretability and minimizing parameter count.\nIn suppression, RePS matches the language-modeling objective on Gemma-2 and\noutperforms it on the larger Gemma-3 variants while remaining resilient to\nprompt-based jailbreaking attacks that defeat prompting. Overall, our results\nsuggest that RePS provides an interpretable and robust alternative to prompting\nfor both steering and suppression."}
{"id": "2505.20813", "pdf": "https://arxiv.org/pdf/2505.20813.pdf", "abs": "https://arxiv.org/abs/2505.20813", "title": "RSCF: Relation-Semantics Consistent Filter for Entity Embedding of Knowledge Graph", "authors": ["Junsik Kim", "Jinwook Park", "Kangil Kim"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025, 17 pages, 10 figures", "summary": "In knowledge graph embedding, leveraging relation-specific\nentity-transformation has markedly enhanced performance. However, the\nconsistency of embedding differences before and after transformation remains\nunaddressed, risking the loss of valuable inductive bias inherent in the\nembeddings. This inconsistency stems from two problems. First, transformation\nrepresentations are specified for relations in a disconnected manner, allowing\ndissimilar transformations and corresponding entity-embeddings for similar\nrelations. Second, a generalized plug-in approach as a SFBR (Semantic Filter\nBased on Relations) disrupts this consistency through excessive concentration\nof entity embeddings under entity-based regularization, generating\nindistinguishable score distributions among relations. In this paper, we\nintroduce a plug-in KGE method, Relation-Semantics Consistent Filter (RSCF),\ncontaining more consistent entity-transformation characterized by three\nfeatures: 1) shared affine transformation of relation embeddings across all\nrelations, 2) rooted entity-transformation that adds an entity embedding to its\nchange represented by the transformed vector, and 3) normalization of the\nchange to prevent scale reduction. To amplify the advantages of consistency\nthat preserve semantics on embeddings, RSCF adds relation transformation and\nprediction modules for enhancing the semantics. In knowledge graph completion\ntasks with distance-based and tensor decomposition models, RSCF significantly\noutperforms state-of-the-art KGE methods, showing robustness across all\nrelations and their frequencies."}
{"id": "2505.20816", "pdf": "https://arxiv.org/pdf/2505.20816.pdf", "abs": "https://arxiv.org/abs/2505.20816", "title": "Rethinking Information Synthesis in Multimodal Question Answering A Multi-Agent Perspective", "authors": ["Krishna Singh Rajput", "Tejas Anvekar", "Chitta Baral", "Vivek Gupta"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in multimodal question answering have primarily focused on\ncombining heterogeneous modalities or fine-tuning multimodal large language\nmodels. While these approaches have shown strong performance, they often rely\non a single, generalized reasoning strategy, overlooking the unique\ncharacteristics of each modality ultimately limiting both accuracy and\ninterpretability. To address these limitations, we propose MAMMQA, a\nmulti-agent QA framework for multimodal inputs spanning text, tables, and\nimages. Our system includes two Visual Language Model (VLM) agents and one\ntext-based Large Language Model (LLM) agent. The first VLM decomposes the user\nquery into sub-questions and sequentially retrieves partial answers from each\nmodality. The second VLM synthesizes and refines these results through\ncross-modal reasoning. Finally, the LLM integrates the insights into a cohesive\nanswer. This modular design enhances interpretability by making the reasoning\nprocess transparent and allows each agent to operate within its domain of\nexpertise. Experiments on diverse multimodal QA benchmarks demonstrate that our\ncooperative, multi-agent framework consistently outperforms existing baselines\nin both accuracy and robustness."}
{"id": "2505.20819", "pdf": "https://arxiv.org/pdf/2505.20819.pdf", "abs": "https://arxiv.org/abs/2505.20819", "title": "Tracing and Reversing Rank-One Model Edits", "authors": ["Paul Youssef", "Zhixue Zhao", "Christin Seifert", "Jörg Schlötterer"], "categories": ["cs.CL"], "comment": null, "summary": "Knowledge editing methods (KEs) are a cost-effective way to update the\nfactual content of large language models (LLMs), but they pose a dual-use risk.\nWhile KEs are beneficial for updating outdated or incorrect information, they\ncan be exploited maliciously to implant misinformation or bias. In order to\ndefend against these types of malicious manipulation, we need robust techniques\nthat can reliably detect, interpret, and mitigate adversarial edits. This work\ninvestigates the traceability and reversibility of knowledge edits, focusing on\nthe widely used Rank-One Model Editing (ROME) method. We first show that ROME\nintroduces distinctive distributional patterns in the edited weight matrices,\nwhich can serve as effective signals for locating the edited weights. Second,\nwe show that these altered weights can reliably be used to predict the edited\nfactual relation, enabling partial reconstruction of the modified fact.\nBuilding on this, we propose a method to infer the edited object entity\ndirectly from the modified weights, without access to the editing prompt,\nachieving over 95% accuracy. Finally, we demonstrate that ROME edits can be\nreversed, recovering the model's original outputs with $\\geq$ 80% accuracy. Our\nfindings highlight the feasibility of detecting, tracing, and reversing edits\nbased on the edited weights, offering a robust framework for safeguarding LLMs\nagainst adversarial manipulations."}
{"id": "2505.20825", "pdf": "https://arxiv.org/pdf/2505.20825.pdf", "abs": "https://arxiv.org/abs/2505.20825", "title": "Reinforced Informativeness Optimization for Long-Form Retrieval-Augmented Generation", "authors": ["Yuhao Wang", "Ruiyang Ren", "Yucheng Wang", "Wayne Xin Zhao", "Jing Liu", "Hua Wu", "Haifeng Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Long-form question answering (LFQA) presents unique challenges for large\nlanguage models, requiring the synthesis of coherent, paragraph-length answers.\nWhile retrieval-augmented generation (RAG) systems have emerged as a promising\nsolution, existing research struggles with key limitations: the scarcity of\nhigh-quality training data for long-form generation, the compounding risk of\nhallucination in extended outputs, and the absence of reliable evaluation\nmetrics for factual completeness. In this paper, we propose RioRAG, a novel\nreinforcement learning (RL) framework that advances long-form RAG through\nreinforced informativeness optimization. Our approach introduces two\nfundamental innovations to address the core challenges. First, we develop an RL\ntraining paradigm of reinforced informativeness optimization that directly\noptimizes informativeness and effectively addresses the slow-thinking deficit\nin conventional RAG systems, bypassing the need for expensive supervised data.\nSecond, we propose a nugget-centric hierarchical reward modeling approach that\nenables precise assessment of long-form answers through a three-stage process:\nextracting the nugget from every source webpage, constructing a nugget claim\nchecklist, and computing rewards based on factual alignment. Extensive\nexperiments on two LFQA benchmarks LongFact and RAGChecker demonstrate the\neffectiveness of the proposed method. Our codes are available at\nhttps://github.com/RUCAIBox/RioRAG."}
{"id": "2505.20826", "pdf": "https://arxiv.org/pdf/2505.20826.pdf", "abs": "https://arxiv.org/abs/2505.20826", "title": "AdParaphrase v2.0: Generating Attractive Ad Texts Using a Preference-Annotated Paraphrase Dataset", "authors": ["Soichiro Murakami", "Peinan Zhang", "Hidetaka Kamigaito", "Hiroya Takamura", "Manabu Okumura"], "categories": ["cs.CL"], "comment": "Accepted to ACL2025 Findings", "summary": "Identifying factors that make ad text attractive is essential for advertising\nsuccess. This study proposes AdParaphrase v2.0, a dataset for ad text\nparaphrasing, containing human preference data, to enable the analysis of the\nlinguistic factors and to support the development of methods for generating\nattractive ad texts. Compared with v1.0, this dataset is 20 times larger,\ncomprising 16,460 ad text paraphrase pairs, each annotated with preference data\nfrom ten evaluators, thereby enabling a more comprehensive and reliable\nanalysis. Through the experiments, we identified multiple linguistic features\nof engaging ad texts that were not observed in v1.0 and explored various\nmethods for generating attractive ad texts. Furthermore, our analysis\ndemonstrated the relationships between human preference and ad performance, and\nhighlighted the potential of reference-free metrics based on large language\nmodels for evaluating ad text attractiveness. The dataset is publicly available\nat: https://github.com/CyberAgentAILab/AdParaphrase-v2.0."}
{"id": "2505.20841", "pdf": "https://arxiv.org/pdf/2505.20841.pdf", "abs": "https://arxiv.org/abs/2505.20841", "title": "Concealment of Intent: A Game-Theoretic Analysis", "authors": ["Xinbo Wu", "Abhishek Umrawal", "Lav R. Varshney"], "categories": ["cs.CL"], "comment": null, "summary": "As large language models (LLMs) grow more capable, concerns about their safe\ndeployment have also grown. Although alignment mechanisms have been introduced\nto deter misuse, they remain vulnerable to carefully designed adversarial\nprompts. In this work, we present a scalable attack strategy: intent-hiding\nadversarial prompting, which conceals malicious intent through the composition\nof skills. We develop a game-theoretic framework to model the interaction\nbetween such attacks and defense systems that apply both prompt and response\nfiltering. Our analysis identifies equilibrium points and reveals structural\nadvantages for the attacker. To counter these threats, we propose and analyze a\ndefense mechanism tailored to intent-hiding attacks. Empirically, we validate\nthe attack's effectiveness on multiple real-world LLMs across a range of\nmalicious behaviors, demonstrating clear advantages over existing adversarial\nprompting techniques."}
{"id": "2505.20871", "pdf": "https://arxiv.org/pdf/2505.20871.pdf", "abs": "https://arxiv.org/abs/2505.20871", "title": "Divide-Then-Align: Honest Alignment based on the Knowledge Boundary of RAG", "authors": ["Xin Sun", "Jianan Xie", "Zhongqi Chen", "Qiang Liu", "Shu Wu", "Yuehe Chen", "Bowen Song", "Weiqiang Wang", "Zilei Wang", "Liang Wang"], "categories": ["cs.CL"], "comment": "ACL 2025 main", "summary": "Large language models (LLMs) augmented with retrieval systems have\nsignificantly advanced natural language processing tasks by integrating\nexternal knowledge sources, enabling more accurate and contextually rich\nresponses. To improve the robustness of such systems against noisy retrievals,\nRetrieval-Augmented Fine-Tuning (RAFT) has emerged as a widely adopted method.\nHowever, RAFT conditions models to generate answers even in the absence of\nreliable knowledge. This behavior undermines their reliability in high-stakes\ndomains, where acknowledging uncertainty is critical. To address this issue, we\npropose Divide-Then-Align (DTA), a post-training approach designed to endow RAG\nsystems with the ability to respond with \"I don't know\" when the query is out\nof the knowledge boundary of both the retrieved passages and the model's\ninternal knowledge. DTA divides data samples into four knowledge quadrants and\nconstructs tailored preference data for each quadrant, resulting in a curated\ndataset for Direct Preference Optimization (DPO). Experimental results on three\nbenchmark datasets demonstrate that DTA effectively balances accuracy with\nappropriate abstention, enhancing the reliability and trustworthiness of\nretrieval-augmented systems."}
{"id": "2505.20874", "pdf": "https://arxiv.org/pdf/2505.20874.pdf", "abs": "https://arxiv.org/abs/2505.20874", "title": "Can LLMs Learn to Map the World from Local Descriptions?", "authors": ["Sirui Xia", "Aili Chen", "Xintao Wang", "Tinghui Zhu", "Yikai Zhang", "Jiangjie Chen", "Yanghua Xiao"], "categories": ["cs.CL"], "comment": "19 pages, 11 figures", "summary": "Recent advances in Large Language Models (LLMs) have demonstrated strong\ncapabilities in tasks such as code and mathematics. However, their potential to\ninternalize structured spatial knowledge remains underexplored. This study\ninvestigates whether LLMs, grounded in locally relative human observations, can\nconstruct coherent global spatial cognition by integrating fragmented\nrelational descriptions. We focus on two core aspects of spatial cognition:\nspatial perception, where models infer consistent global layouts from local\npositional relationships, and spatial navigation, where models learn road\nconnectivity from trajectory data and plan optimal paths between unconnected\nlocations. Experiments conducted in a simulated urban environment demonstrate\nthat LLMs not only generalize to unseen spatial relationships between points of\ninterest (POIs) but also exhibit latent representations aligned with real-world\nspatial distributions. Furthermore, LLMs can learn road connectivity from\ntrajectory descriptions, enabling accurate path planning and dynamic spatial\nawareness during navigation."}
{"id": "2505.20875", "pdf": "https://arxiv.org/pdf/2505.20875.pdf", "abs": "https://arxiv.org/abs/2505.20875", "title": "Trans-EnV: A Framework for Evaluating the Linguistic Robustness of LLMs Against English Varieties", "authors": ["Jiyoung Lee", "Seungho Kim", "Jieun Han", "Jun-Min Lee", "Kitaek Kim", "Alice Oh", "Edward Choi"], "categories": ["cs.CL", "cs.AI"], "comment": "27 pages, 6 figures, 16 tables", "summary": "Large Language Models (LLMs) are predominantly evaluated on Standard American\nEnglish (SAE), often overlooking the diversity of global English varieties.\nThis narrow focus may raise fairness concerns as degraded performance on\nnon-standard varieties can lead to unequal benefits for users worldwide.\nTherefore, it is critical to extensively evaluate the linguistic robustness of\nLLMs on multiple non-standard English varieties. We introduce Trans-EnV, a\nframework that automatically transforms SAE datasets into multiple English\nvarieties to evaluate the linguistic robustness. Our framework combines (1)\nlinguistics expert knowledge to curate variety-specific features and\ntransformation guidelines from linguistic literature and corpora, and (2)\nLLM-based transformations to ensure both linguistic validity and scalability.\nUsing Trans-EnV, we transform six benchmark datasets into 38 English varieties\nand evaluate seven state-of-the-art LLMs. Our results reveal significant\nperformance disparities, with accuracy decreasing by up to 46.3% on\nnon-standard varieties. These findings highlight the importance of\ncomprehensive linguistic robustness evaluation across diverse English\nvarieties. Each construction of Trans-EnV was validated through rigorous\nstatistical testing and consultation with a researcher in the field of second\nlanguage acquisition, ensuring its linguistic validity. Our\n\\href{https://github.com/jiyounglee-0523/TransEnV}{code} and\n\\href{https://huggingface.co/collections/jiyounglee0523/transenv-681eadb3c0c8cf363b363fb1}{datasets}\nare publicly available."}
{"id": "2505.20880", "pdf": "https://arxiv.org/pdf/2505.20880.pdf", "abs": "https://arxiv.org/abs/2505.20880", "title": "MSA at SemEval-2025 Task 3: High Quality Weak Labeling and LLM Ensemble Verification for Multilingual Hallucination Detection", "authors": ["Baraa Hikal", "Ahmed Nasreldin", "Ali Hamdi"], "categories": ["cs.CL"], "comment": null, "summary": "This paper describes our submission for SemEval-2025 Task 3: Mu-SHROOM, the\nMultilingual Shared-task on Hallucinations and Related Observable\nOvergeneration Mistakes. The task involves detecting hallucinated spans in text\ngenerated by instruction-tuned Large Language Models (LLMs) across multiple\nlanguages. Our approach combines task-specific prompt engineering with an LLM\nensemble verification mechanism, where a primary model extracts hallucination\nspans and three independent LLMs adjudicate their validity through\nprobability-based voting. This framework simulates the human annotation\nworkflow used in the shared task validation and test data. Additionally, fuzzy\nmatching refines span alignment. Our system ranked 1st in Arabic and Basque,\n2nd in German, Swedish, and Finnish, and 3rd in Czech, Farsi, and French."}
{"id": "2505.20888", "pdf": "https://arxiv.org/pdf/2505.20888.pdf", "abs": "https://arxiv.org/abs/2505.20888", "title": "EasyDistill: A Comprehensive Toolkit for Effective Knowledge Distillation of Large Language Models", "authors": ["Chengyu Wang", "Junbing Yan", "Wenrui Cai", "Yuanhao Yue", "Jun Huang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this paper, we present EasyDistill, a comprehensive toolkit designed for\neffective black-box and white-box knowledge distillation (KD) of large language\nmodels (LLMs). Our framework offers versatile functionalities, including data\nsynthesis, supervised fine-tuning, ranking optimization, and reinforcement\nlearning techniques specifically tailored for KD scenarios. The toolkit\naccommodates KD functionalities for both System 1 (fast, intuitive) and System\n2 (slow, analytical) models. With its modular design and user-friendly\ninterface, EasyDistill empowers researchers and industry practitioners to\nseamlessly experiment with and implement state-of-the-art KD strategies for\nLLMs. In addition, EasyDistill provides a series of robust distilled models and\nKD-based industrial solutions developed by us, along with the corresponding\nopen-sourced datasets, catering to a variety of use cases. Furthermore, we\ndescribe the seamless integration of EasyDistill into Alibaba Cloud's Platform\nfor AI (PAI). Overall, the EasyDistill toolkit makes advanced KD techniques for\nLLMs more accessible and impactful within the NLP community."}
{"id": "2505.20899", "pdf": "https://arxiv.org/pdf/2505.20899.pdf", "abs": "https://arxiv.org/abs/2505.20899", "title": "Dub-S2ST: Textless Speech-to-Speech Translation for Seamless Dubbing", "authors": ["Jeongsoo Choi", "Jaehun Kim", "Joon Son Chung"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "This paper introduces a cross-lingual dubbing system that translates speech\nfrom one language to another while preserving key characteristics such as\nduration, speaker identity, and speaking speed. Despite the strong translation\nquality of existing speech translation approaches, they often overlook the\ntransfer of speech patterns, leading to mismatches with source speech and\nlimiting their suitability for dubbing applications. To address this, we\npropose a discrete diffusion-based speech-to-unit translation model with\nexplicit duration control, enabling time-aligned translation. We then\nsynthesize speech based on the predicted units and source identity with a\nconditional flow matching model. Additionally, we introduce a unit-based speed\nadaptation mechanism that guides the translation model to produce speech at a\nrate consistent with the source, without relying on any text. Extensive\nexperiments demonstrate that our framework generates natural and fluent\ntranslations that align with the original speech's duration and speaking pace,\nwhile achieving competitive translation performance."}
{"id": "2505.20901", "pdf": "https://arxiv.org/pdf/2505.20901.pdf", "abs": "https://arxiv.org/abs/2505.20901", "title": "A Stereotype Content Analysis on Color-related Social Bias in Large Vision Language Models", "authors": ["Junhyuk Choi", "Minju Kim", "Yeseon Hong", "Bugeun Kim"], "categories": ["cs.CL", "cs.AI"], "comment": "Under review", "summary": "As large vision language models(LVLMs) rapidly advance, concerns about their\npotential to learn and generate social biases and stereotypes are increasing.\nPrevious studies on LVLM's stereotypes face two primary limitations: metrics\nthat overlooked the importance of content words, and datasets that overlooked\nthe effect of color. To address these limitations, this study introduces new\nevaluation metrics based on the Stereotype Content Model (SCM). We also propose\nBASIC, a benchmark for assessing gender, race, and color stereotypes. Using SCM\nmetrics and BASIC, we conduct a study with eight LVLMs to discover stereotypes.\nAs a result, we found three findings. (1) The SCM-based evaluation is effective\nin capturing stereotypes. (2) LVLMs exhibit color stereotypes in the output\nalong with gender and race ones. (3) Interaction between model architecture and\nparameter sizes seems to affect stereotypes. We release BASIC publicly on\n[anonymized for review]."}
{"id": "2505.20903", "pdf": "https://arxiv.org/pdf/2505.20903.pdf", "abs": "https://arxiv.org/abs/2505.20903", "title": "Towards Objective Fine-tuning: How LLMs' Prior Knowledge Causes Potential Poor Calibration?", "authors": ["Ziming Wang", "Zeyu Shi", "Haoyi Zhou", "Shiqi Gao", "Qingyun Sun", "Jianxin Li"], "categories": ["cs.CL"], "comment": "Accepted to ACL2025 Main; The code will be released soon", "summary": "Fine-tuned Large Language Models (LLMs) often demonstrate poor calibration,\nwith their confidence scores misaligned with actual performance. While\ncalibration has been extensively studied in models trained from scratch, the\nimpact of LLMs' prior knowledge on calibration during fine-tuning remains\nunderstudied. Our research reveals that LLMs' prior knowledge causes potential\npoor calibration due to the ubiquitous presence of known data in real-world\nfine-tuning, which appears harmful for calibration. Specifically, data aligned\nwith LLMs' prior knowledge would induce overconfidence, while new knowledge\nimproves calibration. Our findings expose a tension: LLMs' encyclopedic\nknowledge, while enabling task versatility, undermines calibration through\nunavoidable knowledge overlaps. To address this, we propose CogCalib, a\ncognition-aware framework that applies targeted learning strategies according\nto the model's prior knowledge. Experiments across 7 tasks using 3 LLM families\nprove that CogCalib significantly improves calibration while maintaining\nperformance, achieving an average 57\\% reduction in ECE compared to standard\nfine-tuning in Llama3-8B. These improvements generalize well to out-of-domain\ntasks, enhancing the objectivity and reliability of domain-specific LLMs, and\nmaking them more trustworthy for critical human-AI interaction applications."}
{"id": "2505.20910", "pdf": "https://arxiv.org/pdf/2505.20910.pdf", "abs": "https://arxiv.org/abs/2505.20910", "title": "Automated Privacy Information Annotation in Large Language Model Interactions", "authors": ["Hang Zeng", "Xiangyu Liu", "Yong Hu", "Chaoyue Niu", "Fan Wu", "Shaojie Tang", "Guihai Chen"], "categories": ["cs.CL"], "comment": "9 content pages", "summary": "Users interacting with large language models (LLMs) under their real\nidentifiers often unknowingly risk disclosing private information.\nAutomatically notifying users whether their queries leak privacy and which\nphrases leak what private information has therefore become a practical need.\nExisting privacy detection methods, however, were designed for different\nobjectives and application scenarios, typically tagging personally identifiable\ninformation (PII) in anonymous content. In this work, to support the\ndevelopment and evaluation of privacy detection models for LLM interactions\nthat are deployable on local user devices, we construct a large-scale\nmultilingual dataset with 249K user queries and 154K annotated privacy phrases.\nIn particular, we build an automated privacy annotation pipeline with\ncloud-based strong LLMs to automatically extract privacy phrases from dialogue\ndatasets and annotate leaked information. We also design evaluation metrics at\nthe levels of privacy leakage, extracted privacy phrase, and privacy\ninformation. We further establish baseline methods using light-weight LLMs with\nboth tuning-free and tuning-based methods, and report a comprehensive\nevaluation of their performance. Evaluation results reveal a gap between\ncurrent performance and the requirements of real-world LLM applications,\nmotivating future research into more effective local privacy detection methods\ngrounded in our dataset."}
{"id": "2505.20921", "pdf": "https://arxiv.org/pdf/2505.20921.pdf", "abs": "https://arxiv.org/abs/2505.20921", "title": "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models", "authors": ["Injae Na", "Keonwoong Noh", "Woohwan Jung"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 (Findings)", "summary": "LLM providers typically offer multiple LLM tiers, varying in performance and\nprice. As NLP tasks become more complex and modularized, selecting the suitable\nLLM tier for each subtask is a key challenge to balance between cost and\nperformance. To address the problem, we introduce LLM Automatic Transmission\n(LLM-AT) framework that automatically selects LLM tiers without training.\nLLM-AT consists of Starter, Generator, and Judge. The starter selects the\ninitial LLM tier expected to solve the given question, the generator produces a\nresponse using the LLM of the selected tier, and the judge evaluates the\nvalidity of the response. If the response is invalid, LLM-AT iteratively\nupgrades to a higher-tier model, generates a new response, and re-evaluates\nuntil a valid response is obtained. Additionally, we propose accuracy\nestimator, which enables the suitable initial LLM tier selection without\ntraining. Given an input question, accuracy estimator estimates the expected\naccuracy of each LLM tier by computing the valid response rate across top-k\nsimilar queries from past inference records. Experiments demonstrate that\nLLM-AT achieves superior performance while reducing costs, making it a\npractical solution for real-world applications."}
{"id": "2505.20925", "pdf": "https://arxiv.org/pdf/2505.20925.pdf", "abs": "https://arxiv.org/abs/2505.20925", "title": "Multi-objective Large Language Model Alignment with Hierarchical Experts", "authors": ["Zhuo Li", "Guodong Du", "Weiyang Guo", "Yigeng Zhou", "Xiucheng Li", "Wenya Wang", "Fangming Liu", "Yequan Wang", "Deheng Ye", "Min Zhang", "Jing Li"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Aligning large language models (LLMs) to simultaneously satisfy multiple\nobjectives remains a significant challenge, especially given the diverse and\noften conflicting nature of human preferences. Existing alignment methods\nstruggle to balance trade-offs effectively, often requiring costly retraining\nor yielding suboptimal results across the Pareto frontier of preferences. In\nthis paper, we introduce \\textit{HoE}(Hierarchical Mixture-of-Experts), a\n\\textit{lightweight}, \\textit{parameter-efficient}, and \\textit{plug-and-play}\napproach that eliminates the need for model training, while enabling LLMs to\nadapt across the entire Pareto frontier and accommodate diverse user\npreferences. In particular, \\textit{HoE} consists of three hierarchical\ncomponents: LoRA Experts, Router Experts and Preference Routing, reaching\noptimal Pareto frontiers and achieving a trade-off between parameter size,\ntraining cost, and performance. We evaluate \\textit{HoE} across various tasks\non 14 objectives and 200 different preferences among 6 benchmarks,\ndemonstrating superior performance over 15 recent baselines. Code is available\nin the supplementary materials."}
{"id": "2505.20933", "pdf": "https://arxiv.org/pdf/2505.20933.pdf", "abs": "https://arxiv.org/abs/2505.20933", "title": "Information-Theoretic Complementary Prompts for Improved Continual Text Classification", "authors": ["Duzhen Zhang", "Yong Ren", "Chenxing Li", "Dong Yu", "Tielin Zhang"], "categories": ["cs.CL"], "comment": "Accepted by Neural Networks", "summary": "Continual Text Classification (CTC) aims to continuously classify new text\ndata over time while minimizing catastrophic forgetting of previously acquired\nknowledge. However, existing methods often focus on task-specific knowledge,\noverlooking the importance of shared, task-agnostic knowledge. Inspired by the\ncomplementary learning systems theory, which posits that humans learn\ncontinually through the interaction of two systems -- the hippocampus,\nresponsible for forming distinct representations of specific experiences, and\nthe neocortex, which extracts more general and transferable representations\nfrom past experiences -- we introduce Information-Theoretic Complementary\nPrompts (InfoComp), a novel approach for CTC. InfoComp explicitly learns two\ndistinct prompt spaces: P(rivate)-Prompt and S(hared)-Prompt. These\nrespectively encode task-specific and task-invariant knowledge, enabling models\nto sequentially learn classification tasks without relying on data replay. To\npromote more informative prompt learning, InfoComp uses an\ninformation-theoretic framework that maximizes mutual information between\ndifferent parameters (or encoded representations). Within this framework, we\ndesign two novel loss functions: (1) to strengthen the accumulation of\ntask-specific knowledge in P-Prompt, effectively mitigating catastrophic\nforgetting, and (2) to enhance the retention of task-invariant knowledge in\nS-Prompt, improving forward knowledge transfer. Extensive experiments on\ndiverse CTC benchmarks show that our approach outperforms previous\nstate-of-the-art methods."}
{"id": "2505.20937", "pdf": "https://arxiv.org/pdf/2505.20937.pdf", "abs": "https://arxiv.org/abs/2505.20937", "title": "On VLMs for Diverse Tasks in Multimodal Meme Classification", "authors": ["Deepesh Gavit", "Debajyoti Mazumder", "Samiran Das", "Jasabanta Patro"], "categories": ["cs.CL"], "comment": "16 pages", "summary": "In this paper, we present a comprehensive and systematic analysis of\nvision-language models (VLMs) for disparate meme classification tasks. We\nintroduced a novel approach that generates a VLM-based understanding of meme\nimages and fine-tunes the LLMs on textual understanding of the embedded meme\ntext for improving the performance. Our contributions are threefold: (1)\nBenchmarking VLMs with diverse prompting strategies purposely to each sub-task;\n(2) Evaluating LoRA fine-tuning across all VLM components to assess performance\ngains; and (3) Proposing a novel approach where detailed meme interpretations\ngenerated by VLMs are used to train smaller language models (LLMs),\nsignificantly improving classification. The strategy of combining VLMs with\nLLMs improved the baseline performance by 8.34%, 3.52% and 26.24% for sarcasm,\noffensive and sentiment classification, respectively. Our results reveal the\nstrengths and limitations of VLMs and present a novel strategy for meme\nunderstanding."}
{"id": "2505.20959", "pdf": "https://arxiv.org/pdf/2505.20959.pdf", "abs": "https://arxiv.org/abs/2505.20959", "title": "Research Community Perspectives on \"Intelligence\" and Large Language Models", "authors": ["Bertram Højer", "Terne Sasha Thorn Jakobsen", "Anna Rogers", "Stefan Heinrich"], "categories": ["cs.CL", "cs.CY"], "comment": "ACL Findings 2025", "summary": "Despite the widespread use of ''artificial intelligence'' (AI) framing in\nNatural Language Processing (NLP) research, it is not clear what researchers\nmean by ''intelligence''. To that end, we present the results of a survey on\nthe notion of ''intelligence'' among researchers and its role in the research\nagenda. The survey elicited complete responses from 303 researchers from a\nvariety of fields including NLP, Machine Learning (ML), Cognitive Science,\nLinguistics, and Neuroscience. We identify 3 criteria of intelligence that the\ncommunity agrees on the most: generalization, adaptability, & reasoning. Our\nresults suggests that the perception of the current NLP systems as\n''intelligent'' is a minority position (29%). Furthermore, only 16.2% of the\nrespondents see developing intelligent systems as a research goal, and these\nrespondents are more likely to consider the current systems intelligent."}
{"id": "2505.20963", "pdf": "https://arxiv.org/pdf/2505.20963.pdf", "abs": "https://arxiv.org/abs/2505.20963", "title": "Context-Aware Content Moderation for German Newspaper Comments", "authors": ["Felix Krejca", "Tobias Kietreiber", "Alexander Buchelt", "Sebastian Neumaier"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The increasing volume of online discussions requires advanced automatic\ncontent moderation to maintain responsible discourse. While hate speech\ndetection on social media is well-studied, research on German-language\nnewspaper forums remains limited. Existing studies often neglect\nplatform-specific context, such as user history and article themes. This paper\naddresses this gap by developing and evaluating binary classification models\nfor automatic content moderation in German newspaper forums, incorporating\ncontextual information. Using LSTM, CNN, and ChatGPT-3.5 Turbo, and leveraging\nthe One Million Posts Corpus from the Austrian newspaper Der Standard, we\nassess the impact of context-aware models. Results show that CNN and LSTM\nmodels benefit from contextual information and perform competitively with\nstate-of-the-art approaches. In contrast, ChatGPT's zero-shot classification\ndoes not improve with added context and underperforms."}
{"id": "2505.20966", "pdf": "https://arxiv.org/pdf/2505.20966.pdf", "abs": "https://arxiv.org/abs/2505.20966", "title": "Personalized Query Auto-Completion for Long and Short-Term Interests with Adaptive Detoxification Generation", "authors": ["Zhibo Wang", "Xiaoze Jiang", "Zhiheng Qin", "Enyun Yu", "Han Li"], "categories": ["cs.CL", "cs.IR"], "comment": "KDD 2025", "summary": "Query auto-completion (QAC) plays a crucial role in modern search systems.\nHowever, in real-world applications, there are two pressing challenges that\nstill need to be addressed. First, there is a need for hierarchical\npersonalized representations for users. Previous approaches have typically used\nusers' search behavior as a single, overall representation, which proves\ninadequate in more nuanced generative scenarios. Additionally, query prefixes\nare typically short and may contain typos or sensitive information, increasing\nthe likelihood of generating toxic content compared to traditional text\ngeneration tasks. Such toxic content can degrade user experience and lead to\npublic relations issues. Therefore, the second critical challenge is\ndetoxifying QAC systems.\n  To address these two limitations, we propose a novel model (LaD) that\ncaptures personalized information from both long-term and short-term interests,\nincorporating adaptive detoxification. In LaD, personalized information is\ncaptured hierarchically at both coarse-grained and fine-grained levels. This\napproach preserves as much personalized information as possible while enabling\nonline generation within time constraints. To move a futher step, we propose an\nonline training method based on Reject Preference Optimization (RPO). By\nincorporating a special token [Reject] during both the training and inference\nprocesses, the model achieves adaptive detoxification. Consequently, the\ngenerated text presented to users is both non-toxic and relevant to the given\nprefix. We conduct comprehensive experiments on industrial-scale datasets and\nperform online A/B tests, delivering the largest single-experiment metric\nimprovement in nearly two years of our product. Our model has been deployed on\nKuaishou search, driving the primary traffic for hundreds of millions of active\nusers. The code is available at https://github.com/JXZe/LaD."}
{"id": "2505.20971", "pdf": "https://arxiv.org/pdf/2505.20971.pdf", "abs": "https://arxiv.org/abs/2505.20971", "title": "Reason-Align-Respond: Aligning LLM Reasoning with Knowledge Graphs for KGQA", "authors": ["Xiangqing Shen", "Fanfan Wang", "Rui Xia"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "LLMs have demonstrated remarkable capabilities in complex reasoning tasks,\nyet they often suffer from hallucinations and lack reliable factual grounding.\nMeanwhile, knowledge graphs (KGs) provide structured factual knowledge but lack\nthe flexible reasoning abilities of LLMs. In this paper, we present\nReason-Align-Respond (RAR), a novel framework that systematically integrates\nLLM reasoning with knowledge graphs for KGQA. Our approach consists of three\nkey components: a Reasoner that generates human-like reasoning chains, an\nAligner that maps these chains to valid KG paths, and a Responser that\nsynthesizes the final answer. We formulate this process as a probabilistic\nmodel and optimize it using the Expectation-Maximization algorithm, which\niteratively refines the reasoning chains and knowledge paths. Extensive\nexperiments on multiple benchmarks demonstrate the effectiveness of RAR,\nachieving state-of-the-art performance with Hit@1 scores of 93.3% and 91.0% on\nWebQSP and CWQ respectively. Human evaluation confirms that RAR generates\nhigh-quality, interpretable reasoning chains well-aligned with KG paths.\nFurthermore, RAR exhibits strong zero-shot generalization capabilities and\nmaintains computational efficiency during inference."}
{"id": "2505.20976", "pdf": "https://arxiv.org/pdf/2505.20976.pdf", "abs": "https://arxiv.org/abs/2505.20976", "title": "Contrastive Learning on LLM Back Generation Treebank for Cross-domain Constituency Parsing", "authors": ["Peiming Guo", "Meishan Zhang", "Jianling Li", "Min Zhang", "Yue Zhang"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 main conference", "summary": "Cross-domain constituency parsing is still an unsolved challenge in\ncomputational linguistics since the available multi-domain constituency\ntreebank is limited. We investigate automatic treebank generation by large\nlanguage models (LLMs) in this paper. The performance of LLMs on constituency\nparsing is poor, therefore we propose a novel treebank generation method, LLM\nback generation, which is similar to the reverse process of constituency\nparsing. LLM back generation takes the incomplete cross-domain constituency\ntree with only domain keyword leaf nodes as input and fills the missing words\nto generate the cross-domain constituency treebank. Besides, we also introduce\na span-level contrastive learning pre-training strategy to make full use of the\nLLM back generation treebank for cross-domain constituency parsing. We verify\nthe effectiveness of our LLM back generation treebank coupled with contrastive\nlearning pre-training on five target domains of MCTB. Experimental results show\nthat our approach achieves state-of-the-art performance on average results\ncompared with various baselines."}
{"id": "2505.20977", "pdf": "https://arxiv.org/pdf/2505.20977.pdf", "abs": "https://arxiv.org/abs/2505.20977", "title": "Evaluating and Steering Modality Preferences in Multimodal Large Language Model", "authors": ["Yu Zhang", "Jinlong Ma", "Yongshuai Hou", "Xuefeng Bai", "Kehai Chen", "Yang Xiang", "Jun Yu", "Min Zhang"], "categories": ["cs.CL"], "comment": "Modality Preference", "summary": "Multimodal large language models (MLLMs) have achieved remarkable performance\non complex tasks with multimodal context. However, it is still understudied\nwhether they exhibit modality preference when processing multimodal contexts.\nTo study this question, we first build a \\textbf{MC\\textsuperscript{2}}\nbenchmark under controlled evidence conflict scenarios to systematically\nevaluate modality preference, which is the tendency to favor one modality over\nanother when making decisions based on multimodal conflicting evidence. Our\nextensive evaluation reveals that all 18 tested MLLMs generally demonstrate\nclear modality bias, and modality preference can be influenced by external\ninterventions. An in-depth analysis reveals that the preference direction can\nbe captured within the latent representations of MLLMs. Built on this, we\npropose a probing and steering method based on representation engineering to\nexplicitly control modality preference without additional fine-tuning or\ncarefully crafted prompts. Our method effectively amplifies modality preference\ntoward a desired direction and applies to downstream tasks such as\nhallucination mitigation and multimodal machine translation, yielding promising\nimprovements."}
{"id": "2505.20993", "pdf": "https://arxiv.org/pdf/2505.20993.pdf", "abs": "https://arxiv.org/abs/2505.20993", "title": "Who Reasons in the Large Language Models?", "authors": ["Jie Shao", "Jianxin Wu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite the impressive performance of large language models (LLMs), the\nprocess of endowing them with new capabilities--such as mathematical\nreasoning--remains largely empirical and opaque. A critical open question is\nwhether reasoning abilities stem from the entire model, specific modules, or\nare merely artifacts of overfitting. In this work, we hypothesize that the\nreasoning capabilities in well-trained LLMs are primarily attributed to the\noutput projection module (oproj) in the Transformer's multi-head self-attention\n(MHSA) mechanism. To support this hypothesis, we introduce Stethoscope for\nNetworks (SfN), a suite of diagnostic tools designed to probe and analyze the\ninternal behaviors of LLMs. Using SfN, we provide both circumstantial and\nempirical evidence suggesting that oproj plays a central role in enabling\nreasoning, whereas other modules contribute more to fluent dialogue. These\nfindings offer a new perspective on LLM interpretability and open avenues for\nmore targeted training strategies, potentially enabling more efficient and\nspecialized LLMs."}
{"id": "2505.20995", "pdf": "https://arxiv.org/pdf/2505.20995.pdf", "abs": "https://arxiv.org/abs/2505.20995", "title": "Articulatory strategy in vowel production as a basis for speaker discrimination", "authors": ["Justin J. H. Lo", "Patrycja Strycharczuk", "Sam Kirkham"], "categories": ["cs.CL"], "comment": "Accepted to Interspeech 2025", "summary": "The way speakers articulate is well known to be variable across individuals\nwhile at the same time subject to anatomical and biomechanical constraints. In\nthis study, we ask whether articulatory strategy in vowel production can be\nsufficiently speaker-specific to form the basis for speaker discrimination. We\nconducted Generalised Procrustes Analyses of tongue shape data from 40 English\nspeakers from the North West of England, and assessed the\nspeaker-discriminatory potential of orthogonal tongue shape features within the\nframework of likelihood ratios. Tongue size emerged as the individual dimension\nwith the strongest discriminatory power, while tongue shape variation in the\nmore anterior part of the tongue generally outperformed tongue shape variation\nin the posterior part. When considered in combination, shape-only information\nmay offer comparable levels of speaker specificity to size-and-shape\ninformation, but only when features do not exhibit speaker-level co-variation."}
{"id": "2505.21003", "pdf": "https://arxiv.org/pdf/2505.21003.pdf", "abs": "https://arxiv.org/abs/2505.21003", "title": "Uncertainty Unveiled: Can Exposure to More In-context Examples Mitigate Uncertainty for Large Language Models?", "authors": ["Yifei Wang", "Yu Sheng", "Linjing Li", "Daniel Zeng"], "categories": ["cs.CL"], "comment": "Camera-ready versions for ACL 2025 Findings", "summary": "Recent advances in handling long sequences have facilitated the exploration\nof long-context in-context learning (ICL). While much of the existing research\nemphasizes performance improvements driven by additional in-context examples,\nthe influence on the trustworthiness of generated responses remains\nunderexplored. This paper addresses this gap by investigating how increased\nexamples influence predictive uncertainty, an essential aspect in\ntrustworthiness. We begin by systematically quantifying the uncertainty of ICL\nwith varying shot counts, analyzing the impact of example quantity. Through\nuncertainty decomposition, we introduce a novel perspective on performance\nenhancement, with a focus on epistemic uncertainty (EU). Our results reveal\nthat additional examples reduce total uncertainty in both simple and complex\ntasks by injecting task-specific knowledge, thereby diminishing EU and\nenhancing performance. For complex tasks, these advantages emerge only after\naddressing the increased noise and uncertainty associated with longer inputs.\nFinally, we explore the evolution of internal confidence across layers,\nunveiling the mechanisms driving the reduction in uncertainty."}
{"id": "2505.21011", "pdf": "https://arxiv.org/pdf/2505.21011.pdf", "abs": "https://arxiv.org/abs/2505.21011", "title": "LLMs are Frequency Pattern Learners in Natural Language Inference", "authors": ["Liang Cheng", "Zhaowei Wang", "Mark Steedman"], "categories": ["cs.CL"], "comment": "9 pages", "summary": "While fine-tuning LLMs on NLI corpora improves their inferential performance,\nthe underlying mechanisms driving this improvement remain largely opaque. In\nthis work, we conduct a series of experiments to investigate what LLMs actually\nlearn during fine-tuning. We begin by analyzing predicate frequencies in\npremises and hypotheses across NLI datasets and identify a consistent frequency\nbias, where predicates in hypotheses occur more frequently than those in\npremises for positive instances. To assess the impact of this bias, we evaluate\nboth standard and NLI fine-tuned LLMs on bias-consistent and bias-adversarial\ncases. We find that LLMs exploit frequency bias for inference and perform\npoorly on adversarial instances. Furthermore, fine-tuned LLMs exhibit\nsignificantly increased reliance on this bias, suggesting that they are\nlearning these frequency patterns from datasets. Finally, we compute the\nfrequencies of hyponyms and their corresponding hypernyms from WordNet,\nrevealing a correlation between frequency bias and textual entailment. These\nfindings help explain why learning frequency patterns can enhance model\nperformance on inference tasks."}
{"id": "2505.21033", "pdf": "https://arxiv.org/pdf/2505.21033.pdf", "abs": "https://arxiv.org/abs/2505.21033", "title": "Def-DTS: Deductive Reasoning for Open-domain Dialogue Topic Segmentation", "authors": ["Seungmin Lee", "Yongsang Yoo", "Minhwa Jung", "Min Song"], "categories": ["cs.CL"], "comment": "19 pages, 3 figures, Accepted to Findings of the ACL 2025", "summary": "Dialogue Topic Segmentation (DTS) aims to divide dialogues into coherent\nsegments. DTS plays a crucial role in various NLP downstream tasks, but suffers\nfrom chronic problems: data shortage, labeling ambiguity, and incremental\ncomplexity of recently proposed solutions. On the other hand, Despite advances\nin Large Language Models (LLMs) and reasoning strategies, these have rarely\nbeen applied to DTS. This paper introduces Def-DTS: Deductive Reasoning for\nOpen-domain Dialogue Topic Segmentation, which utilizes LLM-based multi-step\ndeductive reasoning to enhance DTS performance and enable case study using\nintermediate result. Our method employs a structured prompting approach for\nbidirectional context summarization, utterance intent classification, and\ndeductive topic shift detection. In the intent classification process, we\npropose the generalizable intent list for domain-agnostic dialogue intent\nclassification. Experiments in various dialogue settings demonstrate that\nDef-DTS consistently outperforms traditional and state-of-the-art approaches,\nwith each subtask contributing to improved performance, particularly in\nreducing type 2 error. We also explore the potential for autolabeling,\nemphasizing the importance of LLM reasoning techniques in DTS."}
{"id": "2505.21040", "pdf": "https://arxiv.org/pdf/2505.21040.pdf", "abs": "https://arxiv.org/abs/2505.21040", "title": "FCKT: Fine-Grained Cross-Task Knowledge Transfer with Semantic Contrastive Learning for Targeted Sentiment Analysis", "authors": ["Wei Chen", "Zhao Zhang", "Meng Yuan", "Kepeng Xu", "Fuzhen Zhuang"], "categories": ["cs.CL", "cs.AI"], "comment": "11 pages, 6 figures", "summary": "In this paper, we address the task of targeted sentiment analysis (TSA),\nwhich involves two sub-tasks, i.e., identifying specific aspects from reviews\nand determining their corresponding sentiments. Aspect extraction forms the\nfoundation for sentiment prediction, highlighting the critical dependency\nbetween these two tasks for effective cross-task knowledge transfer. While most\nexisting studies adopt a multi-task learning paradigm to align task-specific\nfeatures in the latent space, they predominantly rely on coarse-grained\nknowledge transfer. Such approaches lack fine-grained control over\naspect-sentiment relationships, often assuming uniform sentiment polarity\nwithin related aspects. This oversimplification neglects contextual cues that\ndifferentiate sentiments, leading to negative transfer. To overcome these\nlimitations, we propose FCKT, a fine-grained cross-task knowledge transfer\nframework tailored for TSA. By explicitly incorporating aspect-level\ninformation into sentiment prediction, FCKT achieves fine-grained knowledge\ntransfer, effectively mitigating negative transfer and enhancing task\nperformance. Experiments on three datasets, including comparisons with various\nbaselines and large language models (LLMs), demonstrate the effectiveness of\nFCKT. The source code is available on https://github.com/cwei01/FCKT."}
{"id": "2505.21043", "pdf": "https://arxiv.org/pdf/2505.21043.pdf", "abs": "https://arxiv.org/abs/2505.21043", "title": "Visual Cues Enhance Predictive Turn-Taking for Two-Party Human Interaction", "authors": ["Sam O'Connor Russell", "Naomi Harte"], "categories": ["cs.CL", "cs.RO"], "comment": null, "summary": "Turn-taking is richly multimodal. Predictive turn-taking models (PTTMs)\nfacilitate naturalistic human-robot interaction, yet most rely solely on\nspeech. We introduce MM-VAP, a multimodal PTTM which combines speech with\nvisual cues including facial expression, head pose and gaze. We find that it\noutperforms the state-of-the-art audio-only in videoconferencing interactions\n(84% vs. 79% hold/shift prediction accuracy). Unlike prior work which\naggregates all holds and shifts, we group by duration of silence between turns.\nThis reveals that through the inclusion of visual features, MM-VAP outperforms\na state-of-the-art audio-only turn-taking model across all durations of speaker\ntransitions. We conduct a detailed ablation study, which reveals that facial\nexpression features contribute the most to model performance. Thus, our working\nhypothesis is that when interlocutors can see one another, visual cues are\nvital for turn-taking and must therefore be included for accurate turn-taking\nprediction. We additionally validate the suitability of automatic speech\nalignment for PTTM training using telephone speech. This work represents the\nfirst comprehensive analysis of multimodal PTTMs. We discuss implications for\nfuture work and make all code publicly available."}
{"id": "2505.21068", "pdf": "https://arxiv.org/pdf/2505.21068.pdf", "abs": "https://arxiv.org/abs/2505.21068", "title": "Predicting Implicit Arguments in Procedural Video Instructions", "authors": ["Anil Batra", "Laura Sevilla-Lara", "Marcus Rohrbach", "Frank Keller"], "categories": ["cs.CL", "cs.CV"], "comment": "ACL 2025 Main", "summary": "Procedural texts help AI enhance reasoning about context and action\nsequences. Transforming these into Semantic Role Labeling (SRL) improves\nunderstanding of individual steps by identifying predicate-argument structure\nlike {verb,what,where/with}. Procedural instructions are highly elliptic, for\ninstance, (i) add cucumber to the bowl and (ii) add sliced tomatoes, the second\nstep's where argument is inferred from the context, referring to where the\ncucumber was placed. Prior SRL benchmarks often miss implicit arguments,\nleading to incomplete understanding. To address this, we introduce\nImplicit-VidSRL, a dataset that necessitates inferring implicit and explicit\narguments from contextual information in multimodal cooking procedures. Our\nproposed dataset benchmarks multimodal models' contextual reasoning, requiring\nentity tracking through visual changes in recipes. We study recent multimodal\nLLMs and reveal that they struggle to predict implicit arguments of what and\nwhere/with from multi-modal procedural data given the verb. Lastly, we propose\niSRL-Qwen2-VL, which achieves a 17% relative improvement in F1-score for\nwhat-implicit and a 14.7% for where/with-implicit semantic roles over GPT-4o."}
{"id": "2505.21072", "pdf": "https://arxiv.org/pdf/2505.21072.pdf", "abs": "https://arxiv.org/abs/2505.21072", "title": "Faithfulness-Aware Uncertainty Quantification for Fact-Checking the Output of Retrieval Augmented Generation", "authors": ["Ekaterina Fadeeva", "Aleksandr Rubashevskii", "Roman Vashurin", "Shehzaad Dhuliawala", "Artem Shelmanov", "Timothy Baldwin", "Preslav Nakov", "Mrinmaya Sachan", "Maxim Panov"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) enhanced with external knowledge retrieval, an\napproach known as Retrieval-Augmented Generation (RAG), have shown strong\nperformance in open-domain question answering. However, RAG systems remain\nsusceptible to hallucinations: factually incorrect outputs that may arise\neither from inconsistencies in the model's internal knowledge or incorrect use\nof the retrieved context. Existing approaches often conflate factuality with\nfaithfulness to the retrieved context, misclassifying factually correct\nstatements as hallucinations if they are not directly supported by the\nretrieval. In this paper, we introduce FRANQ (Faithfulness-based Retrieval\nAugmented UNcertainty Quantification), a novel method for hallucination\ndetection in RAG outputs. FRANQ applies different Uncertainty Quantification\n(UQ) techniques to estimate factuality based on whether a statement is faithful\nto the retrieved context or not. To evaluate FRANQ and other UQ techniques for\nRAG, we present a new long-form Question Answering (QA) dataset annotated for\nboth factuality and faithfulness, combining automated labeling with manual\nvalidation of challenging examples. Extensive experiments on long- and\nshort-form QA across multiple datasets and LLMs show that FRANQ achieves more\naccurate detection of factual errors in RAG-generated responses compared to\nexisting methods."}
{"id": "2505.21082", "pdf": "https://arxiv.org/pdf/2505.21082.pdf", "abs": "https://arxiv.org/abs/2505.21082", "title": "LLMs Think, But Not In Your Flow: Reasoning-Level Personalization for Black-Box Large Language Models", "authors": ["Jieyong Kim", "Tongyoung Kim", "Soonjin Yoon", "Jaehyung Kim", "Dongha Lee"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have recently achieved impressive performance\nacross a wide range of natural language tasks and are now widely used in\nreal-world applications. Among them, black-box LLMs--served via APIs without\naccess to model internals--are especially dominant due to their scalability and\nease of deployment. Despite their strong capabilities, these models typically\nproduce generalized responses that overlook personal preferences and reasoning\nstyles. This has led to growing interest in black-box LLM personalization,\nwhich aims to tailor model outputs to user-specific context without modifying\nmodel parameters. However, existing approaches primarily focus on\nresponse-level personalization, attempting to match final outputs without\nmodeling personal thought process. To address this limitation, we propose RPM,\na framework for reasoning-level personalization that aligns the model's\nreasoning process with a user's personalized logic. RPM first constructs\nstatistical user-specific factors by extracting and grouping\nresponse-influential features from user history. It then builds personalized\nreasoning paths that reflect how these factors are used in context. In the\ninference stage, RPM retrieves reasoning-aligned examples for new queries via\nfeature-level similarity and performs inference conditioned on the structured\nfactors and retrieved reasoning paths, enabling the model to follow\nuser-specific reasoning trajectories. This reasoning-level personalization\nenhances both predictive accuracy and interpretability by grounding model\noutputs in user-specific logic through structured information. Extensive\nexperiments across diverse tasks show that RPM consistently outperforms\nresponse-level personalization methods, demonstrating the effectiveness of\nreasoning-level personalization in black-box LLMs."}
{"id": "2505.21092", "pdf": "https://arxiv.org/pdf/2505.21092.pdf", "abs": "https://arxiv.org/abs/2505.21092", "title": "BLUCK: A Benchmark Dataset for Bengali Linguistic Understanding and Cultural Knowledge", "authors": ["Daeen Kabir", "Minhajur Rahman Chowdhury Mahim", "Sheikh Shafayat", "Adnan Sadik", "Arian Ahmed", "Eunsu Kim", "Alice Oh"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this work, we introduce BLUCK, a new dataset designed to measure the\nperformance of Large Language Models (LLMs) in Bengali linguistic understanding\nand cultural knowledge. Our dataset comprises 2366 multiple-choice questions\n(MCQs) carefully curated from compiled collections of several college and job\nlevel examinations and spans 23 categories covering knowledge on Bangladesh's\nculture and history and Bengali linguistics. We benchmarked BLUCK using 6\nproprietary and 3 open-source LLMs - including GPT-4o, Claude-3.5-Sonnet,\nGemini-1.5-Pro, Llama-3.3-70B-Instruct, and DeepSeekV3. Our results show that\nwhile these models perform reasonably well overall, they, however, struggles in\nsome areas of Bengali phonetics. Although current LLMs' performance on Bengali\ncultural and linguistic contexts is still not comparable to that of mainstream\nlanguages like English, our results indicate Bengali's status as a mid-resource\nlanguage. Importantly, BLUCK is also the first MCQ-based evaluation benchmark\nthat is centered around native Bengali culture, history, and linguistics."}
{"id": "2505.21097", "pdf": "https://arxiv.org/pdf/2505.21097.pdf", "abs": "https://arxiv.org/abs/2505.21097", "title": "Thinker: Learning to Think Fast and Slow", "authors": ["Stephen Chung", "Wenyu Du", "Jie Fu"], "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.6; I.2.8; I.5.1"], "comment": "21 pages", "summary": "Recent studies show that the reasoning capabilities of Large Language Models\n(LLMs) can be improved by applying Reinforcement Learning (RL) to\nquestion-answering (QA) tasks in areas such as math and coding. With a long\ncontext length, LLMs may learn to perform search, as indicated by the\nself-correction behavior observed in DeepSeek R1. However, this search behavior\nis often imprecise and lacks confidence, resulting in long, redundant responses\nand highlighting deficiencies in intuition and verification. Inspired by the\nDual Process Theory in psychology, we introduce a simple modification to the QA\ntask that includes four stages: Fast Thinking, where the LLM must answer within\na strict token budget; Verification, where the model evaluates its initial\nresponse; Slow Thinking, where it refines the initial response with more\ndeliberation; and Summarization, where it distills the refinement from the\nprevious stage into precise steps. Our proposed task improves average accuracy\nfrom 24.9% to 27.9% for Qwen2.5-1.5B, and from 45.9% to 49.8% for\nDeepSeek-R1-Qwen-1.5B. Notably, for Qwen2.5-1.5B, the Fast Thinking mode alone\nachieves 26.8% accuracy using fewer than 1000 tokens, demonstrating substantial\ninference efficiency gains. These findings suggest that intuition and\ndeliberative reasoning are distinct, complementary systems benefiting from\ntargeted training."}
{"id": "2505.21109", "pdf": "https://arxiv.org/pdf/2505.21109.pdf", "abs": "https://arxiv.org/abs/2505.21109", "title": "A Lightweight Multi-Expert Generative Language Model System for Engineering Information and Knowledge Extraction", "authors": ["Bogdan Bogachov", "Yaoyao Fiona Zhao"], "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.IR", "cs.LG", "I.2.7; I.2.1; I.5.1; I.2.6; H.3.1"], "comment": "10 pages, 4 Figures, 6 Tables. This paper has been accepted to be\n  published in the proceedings of IDETC-CIE 2025", "summary": "Despite recent advancements in domain adaptation techniques for large\nlanguage models, these methods remain computationally intensive, and the\nresulting models can still exhibit hallucination issues. Most existing\nadaptation methods do not prioritize reducing the computational resources\nrequired for fine-tuning and inference of language models. Hallucination issues\nhave gradually decreased with each new model release. However, they remain\nprevalent in engineering contexts, where generating well-structured text with\nminimal errors and inconsistencies is critical. This work introduces a novel\napproach called the Small Language Graph (SLG), which is a lightweight\nadaptation solution designed to address the two key challenges outlined above.\nThe system is structured in the form of a graph, where each node represents a\nlightweight expert - a small language model fine-tuned on specific and concise\ntexts. The results of this study have shown that SLG was able to surpass\nconventional fine-tuning methods on the Exact Match metric by 3 times.\nAdditionally, the fine-tuning process was 1.7 times faster compared to that of\na larger stand-alone language model. These findings introduce a potential for\nsmall to medium-sized engineering companies to confidently use generative AI\ntechnologies, such as LLMs, without the necessity to invest in expensive\ncomputational resources. Also, the graph architecture and the small size of\nexpert nodes offer a possible opportunity for distributed AI systems, thus\npotentially diverting the global need for expensive centralized compute\nclusters."}
{"id": "2505.21115", "pdf": "https://arxiv.org/pdf/2505.21115.pdf", "abs": "https://arxiv.org/abs/2505.21115", "title": "Will It Still Be True Tomorrow? Multilingual Evergreen Question Classification to Improve Trustworthy QA", "authors": ["Sergey Pletenev", "Maria Marina", "Nikolay Ivanov", "Daria Galimzianova", "Nikita Krayko", "Mikhail Salnikov", "Vasily Konovalov", "Alexander Panchenko", "Viktor Moskvoretskii"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) often hallucinate in question answering (QA)\ntasks. A key yet underexplored factor contributing to this is the temporality\nof questions -- whether they are evergreen (answers remain stable over time) or\nmutable (answers change). In this work, we introduce EverGreenQA, the first\nmultilingual QA dataset with evergreen labels, supporting both evaluation and\ntraining. Using EverGreenQA, we benchmark 12 modern LLMs to assess whether they\nencode question temporality explicitly (via verbalized judgments) or implicitly\n(via uncertainty signals). We also train EG-E5, a lightweight multilingual\nclassifier that achieves SoTA performance on this task. Finally, we demonstrate\nthe practical utility of evergreen classification across three applications:\nimproving self-knowledge estimation, filtering QA datasets, and explaining\nGPT-4o retrieval behavior."}
{"id": "2505.21137", "pdf": "https://arxiv.org/pdf/2505.21137.pdf", "abs": "https://arxiv.org/abs/2505.21137", "title": "Scaling and Prompting for Improved End-to-End Spoken Grammatical Error Correction", "authors": ["Mengjie Qian", "Rao Ma", "Stefano Bannò", "Kate M. Knill", "Mark J. F. Gales"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "submitted to Interspeech", "summary": "Spoken Grammatical Error Correction (SGEC) and Feedback (SGECF) are crucial\nfor second language learners, teachers and test takers. Traditional SGEC\nsystems rely on a cascaded pipeline consisting of an ASR, a module for\ndisfluency detection (DD) and removal and one for GEC. With the rise of\nend-to-end (E2E) speech foundation models, we investigate their effectiveness\nin SGEC and feedback generation. This work introduces a pseudo-labelling\nprocess to address the challenge of limited labelled data, expanding the\ntraining data size from 77 hours to approximately 2500 hours, leading to\nimproved performance. Additionally, we prompt an E2E Whisper-based SGEC model\nwith fluent transcriptions, showing a slight improvement in SGEC performance,\nwith more significant gains in feedback generation. Finally, we assess the\nimpact of increasing model size, revealing that while pseudo-labelled data does\nnot yield performance gain for a larger Whisper model, training with prompts\nproves beneficial."}
{"id": "2505.21138", "pdf": "https://arxiv.org/pdf/2505.21138.pdf", "abs": "https://arxiv.org/abs/2505.21138", "title": "Leveraging LLM and Self-Supervised Training Models for Speech Recognition in Chinese Dialects: A Comparative Analysis", "authors": ["Tianyi Xu", "Hongjie Chen", "Wang Qing", "Lv Hang", "Jian Kang", "Li Jie", "Zhennan Lin", "Yongxiang Li", "Xie Lei"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Large-scale training corpora have significantly improved the performance of\nASR models. Unfortunately, due to the relative scarcity of data, Chinese\naccents and dialects remain a challenge for most ASR models. Recent\nadvancements in self-supervised learning have shown that self-supervised pre-\ntraining, combined with large language models (LLM), can effectively enhance\nASR performance in low-resource scenarios. We aim to investigate the\neffectiveness of this paradigm for Chinese dialects. Specifically, we pre-train\na Data2vec2 model on 300,000 hours of unlabeled dialect and accented speech\ndata and do alignment training on a supervised dataset of 40,000 hours. Then,\nwe systematically examine the impact of various projectors and LLMs on\nMandarin, dialect, and accented speech recognition performance under this\nparadigm. Our method achieved SOTA results on multiple dialect datasets,\nincluding Kespeech. We will open-source our work to promote reproducible\nresearch"}
{"id": "2505.21148", "pdf": "https://arxiv.org/pdf/2505.21148.pdf", "abs": "https://arxiv.org/abs/2505.21148", "title": "Assessment of L2 Oral Proficiency using Speech Large Language Models", "authors": ["Rao Ma", "Mengjie Qian", "Siyuan Tang", "Stefano Bannò", "Kate M. Knill", "Mark J. F. Gales"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "submitted to Interspeech", "summary": "The growing population of L2 English speakers has increased the demand for\ndeveloping automatic graders for spoken language assessment (SLA).\nHistorically, statistical models, text encoders, and self-supervised speech\nmodels have been utilised for this task. However, cascaded systems suffer from\nthe loss of information, while E2E graders also have limitations. With the\nrecent advancements of multi-modal large language models (LLMs), we aim to\nexplore their potential as L2 oral proficiency graders and overcome these\nissues. In this work, we compare various training strategies using regression\nand classification targets. Our results show that speech LLMs outperform all\nprevious competitive baselines, achieving superior performance on two datasets.\nFurthermore, the trained grader demonstrates strong generalisation capabilities\nin the cross-part or cross-task evaluation, facilitated by the audio\nunderstanding knowledge acquired during LLM pre-training."}
{"id": "2505.21171", "pdf": "https://arxiv.org/pdf/2505.21171.pdf", "abs": "https://arxiv.org/abs/2505.21171", "title": "M-Wanda: Improving One-Shot Pruning for Multilingual LLMs", "authors": ["Rochelle Choenni", "Ivan Titov"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multilingual LLM performance is often critically dependent on model size.\nWith an eye on efficiency, this has led to a surge in interest in one-shot\npruning methods that retain the benefits of large-scale pretraining while\nshrinking the model size. However, as pruning tends to come with performance\nloss, it is important to understand the trade-offs between multilinguality and\nsparsification. In this work, we study multilingual performance under different\nsparsity constraints and show that moderate ratios already substantially harm\nperformance. To help bridge this gap, we propose M-Wanda, a pruning method that\nmodels cross-lingual variation by incorporating language-aware activation\nstatistics into its pruning criterion and dynamically adjusts layerwise\nsparsity based on cross-lingual importance. We show that M-Wanda consistently\nimproves performance at minimal additional costs. We are the first to\nexplicitly optimize pruning to retain multilingual performance, and hope to\ninspire future advances in multilingual pruning."}
{"id": "2505.21172", "pdf": "https://arxiv.org/pdf/2505.21172.pdf", "abs": "https://arxiv.org/abs/2505.21172", "title": "TAT-R1: Terminology-Aware Translation with Reinforcement Learning and Word Alignment", "authors": ["Zheng Li", "Mao Zheng", "Mingyang Song", "Wenjie Yang"], "categories": ["cs.CL"], "comment": null, "summary": "Recently, deep reasoning large language models(LLMs) like DeepSeek-R1 have\nmade significant progress in tasks such as mathematics and coding. Inspired by\nthis, several studies have employed reinforcement learning(RL) to enhance\nmodels' deep reasoning capabilities and improve machine translation(MT)\nquality. However, the terminology translation, an essential task in MT, remains\nunexplored in deep reasoning LLMs. In this paper, we propose \\textbf{TAT-R1}, a\nterminology-aware translation model trained with reinforcement learning and\nword alignment. Specifically, we first extract the keyword translation pairs\nusing a word alignment model. Then we carefully design three types of\nrule-based alignment rewards with the extracted alignment relationships. With\nthose alignment rewards, the RL-trained translation model can learn to focus on\nthe accurate translation of key information, including terminology in the\nsource text. Experimental results show the effectiveness of TAT-R1. Our model\nsignificantly improves terminology translation accuracy compared to the\nbaseline models while maintaining comparable performance on general translation\ntasks. In addition, we conduct detailed ablation studies of the\nDeepSeek-R1-like training paradigm for machine translation and reveal several\nkey findings."}
{"id": "2505.21178", "pdf": "https://arxiv.org/pdf/2505.21178.pdf", "abs": "https://arxiv.org/abs/2505.21178", "title": "Walk Before You Run! Concise LLM Reasoning via Reinforcement Learning", "authors": ["Mingyang Song", "Mao Zheng"], "categories": ["cs.CL"], "comment": "Ongoing Work", "summary": "As test-time scaling becomes a pivotal research frontier in Large Language\nModels (LLMs) development, contemporary and advanced post-training\nmethodologies increasingly focus on extending the generation length of long\nChain-of-Thought (CoT) responses to enhance reasoning capabilities toward\nDeepSeek R1-like performance. However, recent studies reveal a persistent\noverthinking phenomenon in state-of-the-art reasoning models, manifesting as\nexcessive redundancy or repetitive thinking patterns in long CoT responses. To\naddress this issue, in this paper, we propose a simple yet effective two-stage\nreinforcement learning framework for achieving concise reasoning in LLMs, named\nConciseR. Specifically, the first stage, using more training steps, aims to\nincentivize the model's reasoning capabilities via Group Relative Policy\nOptimization with clip-higher and dynamic sampling components (GRPO++), and the\nsecond stage, using fewer training steps, explicitly enforces conciseness and\nimproves efficiency via Length-aware Group Relative Policy Optimization\n(L-GRPO). Significantly, ConciseR only optimizes response length once all\nrollouts of a sample are correct, following the \"walk before you run\"\nprinciple. Extensive experimental results demonstrate that our ConciseR model,\nwhich generates more concise CoT reasoning responses, outperforms recent\nstate-of-the-art reasoning models with zero RL paradigm across AIME 2024,\nMATH-500, AMC 2023, Minerva, and Olympiad benchmarks."}
{"id": "2505.21189", "pdf": "https://arxiv.org/pdf/2505.21189.pdf", "abs": "https://arxiv.org/abs/2505.21189", "title": "Exploring the Latent Capacity of LLMs for One-Step Text Generation", "authors": ["Gleb Mezentsev", "Ivan Oseledets"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "under review", "summary": "A recent study showed that large language models (LLMs) can reconstruct\nsurprisingly long texts - up to thousands of tokens - via autoregressive\ngeneration from just one specially trained input embedding. In this work, we\nexplore whether such reconstruction is possible without autoregression. We show\nthat frozen LLMs can generate hundreds of accurate tokens in just one forward\npass, when provided with only two learned embeddings. This reveals a surprising\nand underexplored capability of LLMs - multi-token generation without iterative\ndecoding. We investigate the behaviour of these embeddings and provide insight\ninto the type of information they encode. We also empirically show that\nalthough these representations are not unique for a given text, they form\nconnected and local regions in embedding space - a property that suggests the\npotential of learning a dedicated encoder into that space."}
{"id": "2505.21190", "pdf": "https://arxiv.org/pdf/2505.21190.pdf", "abs": "https://arxiv.org/abs/2505.21190", "title": "Lunguage: A Benchmark for Structured and Sequential Chest X-ray Interpretation", "authors": ["Jong Hak Moon", "Geon Choi", "Paloma Rabaey", "Min Gwan Kim", "Hyuk Gi Hong", "Jung-Oh Lee", "Hangyul Yoon", "Eun Woo Doe", "Jiyoun Kim", "Harshita Sharma", "Daniel C. Castro", "Javier Alvarez-Valle", "Edward Choi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Radiology reports convey detailed clinical observations and capture\ndiagnostic reasoning that evolves over time. However, existing evaluation\nmethods are limited to single-report settings and rely on coarse metrics that\nfail to capture fine-grained clinical semantics and temporal dependencies. We\nintroduce LUNGUAGE,a benchmark dataset for structured radiology report\ngeneration that supports both single-report evaluation and longitudinal\npatient-level assessment across multiple studies. It contains 1,473 annotated\nchest X-ray reports, each reviewed by experts, and 80 of them contain\nlongitudinal annotations to capture disease progression and inter-study\nintervals, also reviewed by experts. Using this benchmark, we develop a\ntwo-stage framework that transforms generated reports into fine-grained,\nschema-aligned structured representations, enabling longitudinal\ninterpretation. We also propose LUNGUAGESCORE, an interpretable metric that\ncompares structured outputs at the entity, relation, and attribute level while\nmodeling temporal consistency across patient timelines. These contributions\nestablish the first benchmark dataset, structuring framework, and evaluation\nmetric for sequential radiology reporting, with empirical results demonstrating\nthat LUNGUAGESCORE effectively supports structured report evaluation. The code\nis available at: https://github.com/SuperSupermoon/Lunguage"}
{"id": "2505.21191", "pdf": "https://arxiv.org/pdf/2505.21191.pdf", "abs": "https://arxiv.org/abs/2505.21191", "title": "Unveiling Instruction-Specific Neurons & Experts: An Analytical Framework for LLM's Instruction-Following Capabilities", "authors": ["Junyan Zhang", "Yubo Gao", "Yibo Yan", "Jungang Li", "Zhaorui Hou", "Sicheng Tao", "Shuliang Liu", "Song Dai", "Yonghua Hei", "Junzhuo Li", "Xuming Hu"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The finetuning of Large Language Models (LLMs) has significantly advanced\ntheir instruction-following capabilities, yet the underlying computational\nmechanisms driving these improvements remain poorly understood. This study\nsystematically examines how fine-tuning reconfigures LLM computations by\nisolating and analyzing instruction-specific sparse components, i.e., neurons\nin dense models and both neurons and experts in Mixture-of-Experts (MoE)\narchitectures. In particular, we introduce HexaInst, a carefully curated and\nbalanced instructional dataset spanning six distinct categories, and propose\nSPARCOM, a novel analytical framework comprising three key contributions: (1) a\nmethod for identifying these sparse components, (2) an evaluation of their\nfunctional generality and uniqueness, and (3) a systematic comparison of their\nalterations. Through experiments, we demonstrate functional generality,\nuniqueness, and the critical role of these components in instruction execution.\nBy elucidating the relationship between fine-tuning-induced adaptations and\nsparse computational substrates, this work provides deeper insights into how\nLLMs internalize instruction-following behavior for the trustworthy LLM\ncommunity."}
{"id": "2505.21218", "pdf": "https://arxiv.org/pdf/2505.21218.pdf", "abs": "https://arxiv.org/abs/2505.21218", "title": "Pretrained LLMs Learn Multiple Types of Uncertainty", "authors": ["Roi Cohen", "Omri Fahn", "Gerard de Melo"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models are known to capture real-world knowledge, allowing\nthem to excel in many downstream tasks. Despite recent advances, these models\nare still prone to what are commonly known as hallucinations, causing them to\nemit unwanted and factually incorrect text. In this work, we study how well\nLLMs capture uncertainty, without explicitly being trained for that. We show\nthat, if considering uncertainty as a linear concept in the model's latent\nspace, it might indeed be captured, even after only pretraining. We further\nshow that, though unintuitive, LLMs appear to capture several different types\nof uncertainty, each of which can be useful to predict the correctness for a\nspecific task or benchmark. Furthermore, we provide in-depth results such as\ndemonstrating a correlation between our correction prediction and the model's\nability to abstain from misinformation using words, and the lack of impact of\nmodel scaling for capturing uncertainty. Finally, we claim that unifying the\nuncertainty types as a single one using instruction-tuning or [IDK]-token\ntuning is helpful for the model in terms of correctness prediction."}
{"id": "2505.21224", "pdf": "https://arxiv.org/pdf/2505.21224.pdf", "abs": "https://arxiv.org/abs/2505.21224", "title": "A Representation Level Analysis of NMT Model Robustness to Grammatical Errors", "authors": ["Abderrahmane Issam", "Yusuf Can Semerci", "Jan Scholtes", "Gerasimos Spanakis"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Understanding robustness is essential for building reliable NLP systems.\nUnfortunately, in the context of machine translation, previous work mainly\nfocused on documenting robustness failures or improving robustness. In\ncontrast, we study robustness from a model representation perspective by\nlooking at internal model representations of ungrammatical inputs and how they\nevolve through model layers. For this purpose, we perform Grammatical Error\nDetection (GED) probing and representational similarity analysis. Our findings\nindicate that the encoder first detects the grammatical error, then corrects it\nby moving its representation toward the correct form. To understand what\ncontributes to this process, we turn to the attention mechanism where we\nidentify what we term Robustness Heads. We find that Robustness Heads attend to\ninterpretable linguistic units when responding to grammatical errors, and that\nwhen we fine-tune models for robustness, they tend to rely more on Robustness\nHeads for updating the ungrammatical word representation."}
{"id": "2505.21239", "pdf": "https://arxiv.org/pdf/2505.21239.pdf", "abs": "https://arxiv.org/abs/2505.21239", "title": "LMCD: Language Models are Zeroshot Cognitive Diagnosis Learners", "authors": ["Yu He", "Zihan Yao", "Chentao Song", "Tianyu Qi", "Jun Liu", "Ming Li", "Qing Huang"], "categories": ["cs.CL"], "comment": "work in progress", "summary": "Cognitive Diagnosis (CD) has become a critical task in AI-empowered\neducation, supporting personalized learning by accurately assessing students'\ncognitive states. However, traditional CD models often struggle in cold-start\nscenarios due to the lack of student-exercise interaction data. Recent\nNLP-based approaches leveraging pre-trained language models (PLMs) have shown\npromise by utilizing textual features but fail to fully bridge the gap between\nsemantic understanding and cognitive profiling. In this work, we propose\nLanguage Models as Zeroshot Cognitive Diagnosis Learners (LMCD), a novel\nframework designed to handle cold-start challenges by harnessing large language\nmodels (LLMs). LMCD operates via two primary phases: (1) Knowledge Diffusion,\nwhere LLMs generate enriched contents of exercises and knowledge concepts\n(KCs), establishing stronger semantic links; and (2) Semantic-Cognitive Fusion,\nwhere LLMs employ causal attention mechanisms to integrate textual information\nand student cognitive states, creating comprehensive profiles for both students\nand exercises. These representations are efficiently trained with off-the-shelf\nCD models. Experiments on two real-world datasets demonstrate that LMCD\nsignificantly outperforms state-of-the-art methods in both exercise-cold and\ndomain-cold settings. The code is publicly available at\nhttps://github.com/TAL-auroraX/LMCD"}
{"id": "2505.21242", "pdf": "https://arxiv.org/pdf/2505.21242.pdf", "abs": "https://arxiv.org/abs/2505.21242", "title": "Evaluation of LLMs in Medical Text Summarization: The Role of Vocabulary Adaptation in High OOV Settings", "authors": ["Gunjan Balde", "Soumyadeep Roy", "Mainack Mondal", "Niloy Ganguly"], "categories": ["cs.CL"], "comment": "16 pages. Accepted for publication in the Findings of the 63rd Annual\n  Meeting of the Association for Computational Linguistics (ACL 2025)", "summary": "Large Language Models (LLMs) recently achieved great success in medical text\nsummarization by simply using in-context learning. However, these recent\nefforts do not perform fine-grained evaluations under difficult settings where\nLLMs might fail. They typically report performance scores over the entire\ndataset. Through our benchmarking study, we show that LLMs show a significant\nperformance drop for data points with high concentration of out-of-vocabulary\n(OOV) words or with high novelty. Vocabulary adaptation is an intuitive\nsolution to this vocabulary mismatch issue where the LLM vocabulary gets\nupdated with certain expert domain (here, medical) words or subwords. An\ninteresting finding from our study is that Llama-3.1, even with a vocabulary\nsize of around 128K tokens, still faces over-fragmentation issue with medical\nwords. To that end, we show vocabulary adaptation helps improve the LLM\nsummarization performance even in difficult settings. Through extensive\nexperimentation of multiple vocabulary adaptation strategies, two continual\npretraining strategies, and three benchmark medical summarization datasets, we\ngain valuable insights into the role of vocabulary adaptation strategies for\ncustomizing LLMs to the medical domain. We also performed a human evaluation\nstudy with medical experts where they found that vocabulary adaptation results\nin more relevant and faithful summaries. Our codebase is made publicly\navailable at https://github.com/gb-kgp/LLM-MedicalSummarization-Benchmark."}
{"id": "2505.21250", "pdf": "https://arxiv.org/pdf/2505.21250.pdf", "abs": "https://arxiv.org/abs/2505.21250", "title": "ReSCORE: Label-free Iterative Retriever Training for Multi-hop Question Answering with Relevance-Consistency Supervision", "authors": ["Dosung Lee", "Wonjun Oh", "Boyoung Kim", "Minyoung Kim", "Joonsuk Park", "Paul Hongsuck Seo"], "categories": ["cs.CL"], "comment": "9 pages, 3 figures, ACL 2025", "summary": "Multi-hop question answering (MHQA) involves reasoning across multiple\ndocuments to answer complex questions. Dense retrievers typically outperform\nsparse methods like BM25 by leveraging semantic embeddings; however, they\nrequire labeled query-document pairs for fine-tuning. This poses a significant\nchallenge in MHQA due to the high variability of queries (reformulated)\nquestions throughout the reasoning steps. To overcome this limitation, we\nintroduce Retriever Supervision with Consistency and Relevance (ReSCORE), a\nnovel method for training dense retrievers for MHQA without labeled documents.\nReSCORE leverages large language models to capture each documents relevance to\nthe question and consistency with the correct answer and use them to train a\nretriever within an iterative question-answering framework. Experiments on\nthree MHQA benchmarks demonstrate the effectiveness of ReSCORE, with\nsignificant improvements in retrieval, and in turn, the state-of-the-art MHQA\nperformance. Our implementation is available at:\nhttps://leeds1219.github.io/ReSCORE."}
{"id": "2505.21265", "pdf": "https://arxiv.org/pdf/2505.21265.pdf", "abs": "https://arxiv.org/abs/2505.21265", "title": "Multilingual Pretraining for Pixel Language Models", "authors": ["Ilker Kesen", "Jonas F. Lotz", "Ingo Ziegler", "Phillip Rust", "Desmond Elliott"], "categories": ["cs.CL", "cs.AI"], "comment": "17 pages, 19 figures, 7 tables", "summary": "Pixel language models operate directly on images of rendered text,\neliminating the need for a fixed vocabulary. While these models have\ndemonstrated strong capabilities for downstream cross-lingual transfer,\nmultilingual pretraining remains underexplored. We introduce PIXEL-M4, a model\npretrained on four visually and linguistically diverse languages: English,\nHindi, Ukrainian, and Simplified Chinese. Multilingual evaluations on semantic\nand syntactic tasks show that PIXEL-M4 outperforms an English-only counterpart\non non-Latin scripts. Word-level probing analyses confirm that PIXEL-M4\ncaptures rich linguistic features, even in languages not seen during\npretraining. Furthermore, an analysis of its hidden representations shows that\nmultilingual pretraining yields a semantic embedding space closely aligned\nacross the languages used for pretraining. This work demonstrates that\nmultilingual pretraining substantially enhances the capability of pixel\nlanguage models to effectively support a diverse set of languages."}
{"id": "2505.21297", "pdf": "https://arxiv.org/pdf/2505.21297.pdf", "abs": "https://arxiv.org/abs/2505.21297", "title": "rStar-Coder: Scaling Competitive Code Reasoning with a Large-Scale Verified Dataset", "authors": ["Yifei Liu", "Li Lyna Zhang", "Yi Zhu", "Bingcheng Dong", "Xudong Zhou", "Ning Shang", "Fan Yang", "Mao Yang"], "categories": ["cs.CL"], "comment": null, "summary": "Advancing code reasoning in large language models (LLMs) is fundamentally\nlimited by the scarcity of high-difficulty datasets, especially those with\nverifiable input-output test cases necessary for rigorous solution validation\nat scale. We introduce rStar-Coder, which significantly improves LLM code\nreasoning capabilities by constructing a large-scale, verified dataset of 418K\ncompetition-level code problems, 580K long-reasoning solutions along with rich\ntest cases of varying difficulty. This is achieved through three core\ncontributions: (1) we curate competitive programming code problems and oracle\nsolutions to synthesize new, solvable problems; (2) we introduce a reliable\ninput-output test case synthesis pipeline that decouples the generation into a\nthree-step input generation method and a mutual verification mechanism for\neffective output labeling; (3) we augment problems with high-quality,\ntest-case-verified long-reasoning solutions. Extensive experiments on Qwen\nmodels (1.5B-14B) across various code reasoning benchmarks demonstrate the\nsuperiority of rStar-Coder dataset, achieving leading performance comparable to\nfrontier reasoning LLMs with much smaller model sizes. On LiveCodeBench,\nrStar-Coder improves Qwen2.5-7B from 17.4% to an impressive 57.3%, and\nQwen2.5-14B from 23.3% to 62.5%, surpassing o3-mini (low) by3.1%. On the more\nchallenging USA Computing Olympiad, our 7B model achieves an average pass@1\naccuracy of 16.15%, outperforming the frontier-level QWQ-32B. Code and the\ndataset will be released at https://github.com/microsoft/rStar."}
{"id": "2505.21301", "pdf": "https://arxiv.org/pdf/2505.21301.pdf", "abs": "https://arxiv.org/abs/2505.21301", "title": "How Humans and LLMs Organize Conceptual Knowledge: Exploring Subordinate Categories in Italian", "authors": ["Andrea Pedrotti", "Giulia Rambelli", "Caterina Villani", "Marianna Bolognesi"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ACL 2025", "summary": "People can categorize the same entity at multiple taxonomic levels, such as\nbasic (bear), superordinate (animal), and subordinate (grizzly bear). While\nprior research has focused on basic-level categories, this study is the first\nattempt to examine the organization of categories by analyzing exemplars\nproduced at the subordinate level. We present a new Italian psycholinguistic\ndataset of human-generated exemplars for 187 concrete words. We then use these\ndata to evaluate whether textual and vision LLMs produce meaningful exemplars\nthat align with human category organization across three key tasks: exemplar\ngeneration, category induction, and typicality judgment. Our findings show a\nlow alignment between humans and LLMs, consistent with previous studies.\nHowever, their performance varies notably across different semantic domains.\nUltimately, this study highlights both the promises and the constraints of\nusing AI-generated exemplars to support psychological and linguistic research."}
{"id": "2505.21315", "pdf": "https://arxiv.org/pdf/2505.21315.pdf", "abs": "https://arxiv.org/abs/2505.21315", "title": "Charting the Landscape of African NLP: Mapping Progress and Shaping the Road Ahead", "authors": ["Jesujoba O. Alabi", "Michael A. Hedderich", "David Ifeoluwa Adelani", "Dietrich Klakow"], "categories": ["cs.CL"], "comment": "Working paper", "summary": "With over 2,000 languages and potentially millions of speakers, Africa\nrepresents one of the richest linguistic regions in the world. Yet, this\ndiversity is scarcely reflected in state-of-the-art natural language processing\n(NLP) systems and large language models (LLMs), which predominantly support a\nnarrow set of high-resource languages. This exclusion not only limits the reach\nand utility of modern NLP technologies but also risks widening the digital\ndivide across linguistic communities. Nevertheless, NLP research on African\nlanguages is active and growing. In recent years, there has been a surge of\ninterest in this area, driven by several factors-including the creation of\nmultilingual language resources, the rise of community-led initiatives, and\nincreased support through funding programs. In this survey, we analyze 734\nresearch papers on NLP for African languages published over the past five\nyears, offering a comprehensive overview of recent progress across core tasks.\nWe identify key trends shaping the field and conclude by outlining promising\ndirections to foster more inclusive and sustainable NLP research for African\nlanguages."}
{"id": "2505.21324", "pdf": "https://arxiv.org/pdf/2505.21324.pdf", "abs": "https://arxiv.org/abs/2505.21324", "title": "Leveraging large language models and traditional machine learning ensembles for ADHD detection from narrative transcripts", "authors": ["Yuxin Zhu", "Yuting Guo", "Noah Marchuck", "Abeed Sarker", "Yun Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Despite rapid advances in large language models (LLMs), their integration\nwith traditional supervised machine learning (ML) techniques that have proven\napplicability to medical data remains underexplored. This is particularly true\nfor psychiatric applications, where narrative data often exhibit nuanced\nlinguistic and contextual complexity, and can benefit from the combination of\nmultiple models with differing characteristics. In this study, we introduce an\nensemble framework for automatically classifying\nAttention-Deficit/Hyperactivity Disorder (ADHD) diagnosis (binary) using\nnarrative transcripts. Our approach integrates three complementary models:\nLLaMA3, an open-source LLM that captures long-range semantic structure;\nRoBERTa, a pre-trained transformer model fine-tuned on labeled clinical\nnarratives; and a Support Vector Machine (SVM) classifier trained using\nTF-IDF-based lexical features. These models are aggregated through a majority\nvoting mechanism to enhance predictive robustness. The dataset includes 441\ninstances, including 352 for training and 89 for validation. Empirical results\nshow that the ensemble outperforms individual models, achieving an F$_1$ score\nof 0.71 (95\\% CI: [0.60-0.80]). Compared to the best-performing individual\nmodel (SVM), the ensemble improved recall while maintaining competitive\nprecision. This indicates the strong sensitivity of the ensemble in identifying\nADHD-related linguistic cues. These findings demonstrate the promise of hybrid\narchitectures that leverage the semantic richness of LLMs alongside the\ninterpretability and pattern recognition capabilities of traditional supervised\nML, offering a new direction for robust and generalizable psychiatric text\nclassification."}
{"id": "2505.21342", "pdf": "https://arxiv.org/pdf/2505.21342.pdf", "abs": "https://arxiv.org/abs/2505.21342", "title": "PEDANTIC: A Dataset for the Automatic Examination of Definiteness in Patent Claims", "authors": ["Valentin Knappich", "Annemarie Friedrich", "Anna Hätty", "Simon Razniewski"], "categories": ["cs.CL"], "comment": null, "summary": "Patent claims define the scope of protection for an invention. If there are\nambiguities in a claim, it is rejected by the patent office. In the US, this is\nreferred to as indefiniteness (35 U.S.C {\\S} 112(b)) and is among the most\nfrequent reasons for patent application rejection. The development of automatic\nmethods for patent definiteness examination has the potential to make patent\ndrafting and examination more efficient, but no annotated dataset has been\npublished to date.\n  We introduce PEDANTIC (\\underline{P}at\\underline{e}nt\n\\underline{D}efiniteness Ex\\underline{a}mi\\underline{n}a\\underline{ti}on\n\\underline{C}orpus), a novel dataset of 14k US patent claims from patent\napplications relating to Natural Language Processing (NLP), annotated with\nreasons for indefiniteness. We construct PEDANTIC using a fully automatic\npipeline that retrieves office action documents from the USPTO and uses Large\nLanguage Models (LLMs) to extract the reasons for indefiniteness. A human\nvalidation study confirms the pipeline's accuracy in generating high-quality\nannotations. To gain insight beyond binary classification metrics, we implement\nan LLM-as-Judge evaluation that compares the free-form reasoning of every\nmodel-cited reason with every examiner-cited reason. We show that LLM agents\nbased on Qwen 2.5 32B and 72B struggle to outperform logistic regression\nbaselines on definiteness prediction, even though they often correctly identify\nthe underlying reasons. PEDANTIC provides a valuable resource for patent AI\nresearchers, enabling the development of advanced examination models. We will\npublicly release the dataset and code."}
{"id": "2505.21354", "pdf": "https://arxiv.org/pdf/2505.21354.pdf", "abs": "https://arxiv.org/abs/2505.21354", "title": "Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning", "authors": ["Bidyarthi Paul", "Jalisha Jashim Era", "Mirazur Rahman Zim", "Tahmid Sattar Aothoi", "Faisal Muhammad Shah"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Solving Bengali Math Word Problems (MWPs) remains a major challenge in\nnatural language processing (NLP) due to the language's low-resource status and\nthe multi-step reasoning required. Existing models struggle with complex\nBengali MWPs, largely because no human-annotated Bengali dataset has previously\naddressed this task. This gap has limited progress in Bengali mathematical\nreasoning. To address this, we created SOMADHAN, a dataset of 8792 complex\nBengali MWPs with manually written, step-by-step solutions. We designed this\ndataset to support reasoning-focused evaluation and model development in a\nlinguistically underrepresented context. Using SOMADHAN, we evaluated a range\nof large language models (LLMs) - including GPT-4o, GPT-3.5 Turbo, LLaMA series\nmodels, Deepseek, and Qwen - through both zero-shot and few-shot prompting with\nand without Chain of Thought (CoT) reasoning. CoT prompting consistently\nimproved performance over standard prompting, especially in tasks requiring\nmulti-step logic. LLaMA-3.3 70B achieved the highest accuracy of 88% with\nfew-shot CoT prompting. We also applied Low-Rank Adaptation (LoRA) to fine-tune\nmodels efficiently, enabling them to adapt to Bengali MWPs with minimal\ncomputational cost. Our work fills a critical gap in Bengali NLP by providing a\nhigh-quality reasoning dataset and a scalable framework for solving complex\nMWPs. We aim to advance equitable research in low-resource languages and\nenhance reasoning capabilities in educational and language technologies."}
{"id": "2505.21362", "pdf": "https://arxiv.org/pdf/2505.21362.pdf", "abs": "https://arxiv.org/abs/2505.21362", "title": "Evaluating LLM Adaptation to Sociodemographic Factors: User Profile vs. Dialogue History", "authors": ["Qishuai Zhong", "Zongmin Li", "Siqi Fan", "Aixin Sun"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Effective engagement by large language models (LLMs) requires adapting\nresponses to users' sociodemographic characteristics, such as age, occupation,\nand education level. While many real-world applications leverage dialogue\nhistory for contextualization, existing evaluations of LLMs' behavioral\nadaptation often focus on single-turn prompts. In this paper, we propose a\nframework to evaluate LLM adaptation when attributes are introduced either (1)\nexplicitly via user profiles in the prompt or (2) implicitly through multi-turn\ndialogue history. We assess the consistency of model behavior across these\nmodalities. Using a multi-agent pipeline, we construct a synthetic dataset\npairing dialogue histories with distinct user profiles and employ questions\nfrom the Value Survey Module (VSM 2013) (Hofstede and Hofstede, 2016) to probe\nvalue expression. Our findings indicate that most models adjust their expressed\nvalues in response to demographic changes, particularly in age and education\nlevel, but consistency varies. Models with stronger reasoning capabilities\ndemonstrate greater alignment, indicating the importance of reasoning in robust\nsociodemographic adaptation."}
{"id": "2505.21378", "pdf": "https://arxiv.org/pdf/2505.21378.pdf", "abs": "https://arxiv.org/abs/2505.21378", "title": "Analyzing values about gendered language reform in LLMs' revisions", "authors": ["Jules Watson", "Xi Wang", "Raymond Liu", "Suzanne Stevenson", "Barend Beekhuizen"], "categories": ["cs.CL"], "comment": "15 pages", "summary": "Within the common LLM use case of text revision, we study LLMs' revision of\ngendered role nouns (e.g., outdoorsperson/woman/man) and their justifications\nof such revisions. We evaluate their alignment with feminist and\ntrans-inclusive language reforms for English. Drawing on insight from\nsociolinguistics, we further assess if LLMs are sensitive to the same\ncontextual effects in the application of such reforms as people are, finding\nbroad evidence of such effects. We discuss implications for value alignment."}
{"id": "2505.21380", "pdf": "https://arxiv.org/pdf/2505.21380.pdf", "abs": "https://arxiv.org/abs/2505.21380", "title": "PHISH in MESH: Korean Adversarial Phonetic Substitution and Phonetic-Semantic Feature Integration Defense", "authors": ["Byungjun Kim", "Minju Kim", "Hyeonchu Park", "Bugeun Kim"], "categories": ["cs.CL"], "comment": "Under review", "summary": "As malicious users increasingly employ phonetic substitution to evade hate\nspeech detection, researchers have investigated such strategies. However, two\nkey challenges remain. First, existing studies have overlooked the Korean\nlanguage, despite its vulnerability to phonetic perturbations due to its\nphonographic nature. Second, prior work has primarily focused on constructing\ndatasets rather than developing architectural defenses. To address these\nchallenges, we propose (1) PHonetic-Informed Substitution for Hangul (PHISH)\nthat exploits the phonological characteristics of the Korean writing system,\nand (2) Mixed Encoding of Semantic-pHonetic features (MESH) that enhances the\ndetector's robustness by incorporating phonetic information at the\narchitectural level. Our experimental results demonstrate the effectiveness of\nour proposed methods on both perturbed and unperturbed datasets, suggesting\nthat they not only improve detection performance but also reflect realistic\nadversarial behaviors employed by malicious users."}
{"id": "2505.21389", "pdf": "https://arxiv.org/pdf/2505.21389.pdf", "abs": "https://arxiv.org/abs/2505.21389", "title": "AutoJudger: An Agent-Driven Framework for Efficient Benchmarking of MLLMs", "authors": ["Xuanwen Ding", "Chengjun Pan", "Zejun Li", "Jiwen Zhang", "Siyuan Wang", "Zhongyu Wei"], "categories": ["cs.CL"], "comment": null, "summary": "Evaluating multimodal large language models (MLLMs) is increasingly\nexpensive, as the growing size and cross-modality complexity of benchmarks\ndemand significant scoring efforts. To tackle with this difficulty, we\nintroduce AutoJudger, an agent-driven framework for efficient and adaptive\nbenchmarking of MLLMs that tackles this escalating cost. AutoJudger employs the\nItem Response Theory (IRT) to estimate the question difficulty and an\nautonomous evaluation agent to dynamically select the most informative test\nquestions based on the model's real-time performance. Specifically, AutoJudger\nincorporates two pivotal components: a semantic-aware retrieval mechanism to\nensure that selected questions cover diverse and challenging scenarios across\nboth vision and language modalities, and a dynamic memory that maintains\ncontextual statistics of previously evaluated questions to guide coherent and\nglobally informed question selection throughout the evaluation process.\nExtensive experiments on four representative multimodal benchmarks demonstrate\nthat our adaptive framework dramatically reduces evaluation expenses, i.e.\nAutoJudger uses only 4% of the data to achieve over 90% ranking accuracy with\nthe full benchmark evaluation on MMT-Bench."}
{"id": "2505.21396", "pdf": "https://arxiv.org/pdf/2505.21396.pdf", "abs": "https://arxiv.org/abs/2505.21396", "title": "Improving Research Idea Generation Through Data: An Empirical Investigation in Social Science", "authors": ["Xiao Liu", "Xinyi Dong", "Xinyang Gao", "Yansong Feng", "Xun Pang"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have shown promise in\ngenerating novel research ideas. However, these ideas often face challenges\nrelated to feasibility and expected effectiveness. This paper explores how\naugmenting LLMs with relevant data during the idea generation process can\nenhance the quality of generated ideas. We introduce two ways of incorporating\ndata: (1) providing metadata during the idea generation stage to guide LLMs\ntoward feasible directions, and (2) adding automatic validation during the idea\nselection stage to assess the empirical plausibility of hypotheses within\nideas. We conduct experiments in the social science domain, specifically with\nclimate negotiation topics, and find that metadata improves the feasibility of\ngenerated ideas by 20%, while automatic validation improves the overall quality\nof selected ideas by 7%. A human study shows that LLM-generated ideas, along\nwith their related data and validation processes, inspire researchers to\npropose research ideas with higher quality. Our work highlights the potential\nof data-driven research idea generation, and underscores the practical utility\nof LLM-assisted ideation in real-world academic settings."}
{"id": "2505.21397", "pdf": "https://arxiv.org/pdf/2505.21397.pdf", "abs": "https://arxiv.org/abs/2505.21397", "title": "DecisionFlow: Advancing Large Language Model as Principled Decision Maker", "authors": ["Xiusi Chen", "Shanyong Wang", "Cheng Qian", "Hongru Wang", "Peixuan Han", "Heng Ji"], "categories": ["cs.CL"], "comment": "24 pages, 13 figures", "summary": "In high-stakes domains such as healthcare and finance, effective\ndecision-making demands not just accurate outcomes but transparent and\nexplainable reasoning. However, current language models often lack the\nstructured deliberation needed for such tasks, instead generating decisions and\njustifications in a disconnected, post-hoc manner. To address this, we propose\nDecisionFlow, a novel decision modeling framework that guides models to reason\nover structured representations of actions, attributes, and constraints. Rather\nthan predicting answers directly from prompts, DecisionFlow builds a\nsemantically grounded decision space and infers a latent utility function to\nevaluate trade-offs in a transparent, utility-driven manner. This process\nproduces decisions tightly coupled with interpretable rationales reflecting the\nmodel's reasoning. Empirical results on two high-stakes benchmarks show that\nDecisionFlow not only achieves up to 30% accuracy gains over strong prompting\nbaselines but also enhances alignment in outcomes. Our work is a critical step\ntoward integrating symbolic reasoning with LLMs, enabling more accountable,\nexplainable, and reliable LLM decision support systems. We release the data and\ncode at https://github.com/xiusic/DecisionFlow."}
{"id": "2505.21399", "pdf": "https://arxiv.org/pdf/2505.21399.pdf", "abs": "https://arxiv.org/abs/2505.21399", "title": "Factual Self-Awareness in Language Models: Representation, Robustness, and Scaling", "authors": ["Hovhannes Tamoyan", "Subhabrata Dutta", "Iryna Gurevych"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Factual incorrectness in generated content is one of the primary concerns in\nubiquitous deployment of large language models (LLMs). Prior findings suggest\nLLMs can (sometimes) detect factual incorrectness in their generated content\n(i.e., fact-checking post-generation). In this work, we provide evidence\nsupporting the presence of LLMs' internal compass that dictate the correctness\nof factual recall at the time of generation. We demonstrate that for a given\nsubject entity and a relation, LLMs internally encode linear features in the\nTransformer's residual stream that dictate whether it will be able to recall\nthe correct attribute (that forms a valid entity-relation-attribute triplet).\nThis self-awareness signal is robust to minor formatting variations. We\ninvestigate the effects of context perturbation via different example selection\nstrategies. Scaling experiments across model sizes and training dynamics\nhighlight that self-awareness emerges rapidly during training and peaks in\nintermediate layers. These findings uncover intrinsic self-monitoring\ncapabilities within LLMs, contributing to their interpretability and\nreliability."}
{"id": "2505.21409", "pdf": "https://arxiv.org/pdf/2505.21409.pdf", "abs": "https://arxiv.org/abs/2505.21409", "title": "RelationalFactQA: A Benchmark for Evaluating Tabular Fact Retrieval from Large Language Models", "authors": ["Dario Satriani", "Enzo Veltri", "Donatello Santoro", "Paolo Papotti"], "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": null, "summary": "Factuality in Large Language Models (LLMs) is a persistent challenge. Current\nbenchmarks often assess short factual answers, overlooking the critical ability\nto generate structured, multi-record tabular outputs from parametric knowledge.\nWe demonstrate that this relational fact retrieval is substantially more\ndifficult than isolated point-wise queries, even when individual facts are\nknown to the model, exposing distinct failure modes sensitive to output\ndimensionality (e.g., number of attributes or records). To systematically\nevaluate this under-explored capability, we introduce RelationalFactQA, a new\nbenchmark featuring diverse natural language questions (paired with SQL) and\ngold-standard tabular answers, specifically designed to assess knowledge\nretrieval in a structured format. RelationalFactQA enables analysis across\nvarying query complexities, output sizes, and data characteristics. Our\nexperiments reveal that even state-of-the-art LLMs struggle significantly, not\nexceeding 25% factual accuracy in generating relational outputs, with\nperformance notably degrading as output dimensionality increases. These\nfindings underscore critical limitations in current LLMs' ability to synthesize\nstructured factual knowledge and establish RelationalFactQA as a crucial\nresource for measuring future progress in LLM factuality."}
{"id": "2505.21411", "pdf": "https://arxiv.org/pdf/2505.21411.pdf", "abs": "https://arxiv.org/abs/2505.21411", "title": "Pangu Pro MoE: Mixture of Grouped Experts for Efficient Sparsity", "authors": ["Yehui Tang", "Xiaosong Li", "Fangcheng Liu", "Wei Guo", "Hang Zhou", "Yaoyuan Wang", "Kai Han", "Xianzhi Yu", "Jinpeng Li", "Hui Zang", "Fei Mi", "Xiaojun Meng", "Zhicheng Liu", "Hanting Chen", "Binfan Zheng", "Can Chen", "Youliang Yan", "Ruiming Tang", "Peifeng Qin", "Xinghao Chen", "Dacheng Tao", "Yunhe Wang"], "categories": ["cs.CL"], "comment": null, "summary": "The surgence of Mixture of Experts (MoE) in Large Language Models promises a\nsmall price of execution cost for a much larger model parameter count and\nlearning capacity, because only a small fraction of parameters are activated\nfor each input token. However, it is commonly observed that some experts are\nactivated far more often than others, leading to system inefficiency when\nrunning the experts on different devices in parallel. Therefore, we introduce\nMixture of Grouped Experts (MoGE), which groups the experts during selection\nand balances the expert workload better than MoE in nature. It constrains\ntokens to activate an equal number of experts within each predefined expert\ngroup. When a model execution is distributed on multiple devices, this\narchitectural design ensures a balanced computational load across devices,\nsignificantly enhancing throughput, particularly for the inference phase.\nFurther, we build Pangu Pro MoE on Ascend NPUs, a sparse model based on MoGE\nwith 72 billion total parameters, 16 billion of which are activated for each\ntoken. The configuration of Pangu Pro MoE is optimized for Ascend 300I Duo and\n800I A2 through extensive system simulation studies. Our experiments indicate\nthat MoGE indeed leads to better expert load balancing and more efficient\nexecution for both model training and inference on Ascend NPUs. The inference\nperformance of Pangu Pro MoE achieves 1148 tokens/s per card and can be further\nimproved to 1528 tokens/s per card by speculative acceleration, outperforming\ncomparable 32B and 72B Dense models. Furthermore, we achieve an excellent\ncost-to-performance ratio for model inference on Ascend 300I Duo.Our studies\nshow that Ascend NPUs are capable of training Pangu Pro MoE with massive\nparallelization to make it a leading model within the sub-100B total parameter\nclass, outperforming prominent open-source models like GLM-Z1-32B and\nQwen3-32B."}
{"id": "2505.21413", "pdf": "https://arxiv.org/pdf/2505.21413.pdf", "abs": "https://arxiv.org/abs/2505.21413", "title": "RefTool: Enhancing Model Reasoning with Reference-Guided Tool Creation", "authors": ["Xiao Liu", "Da Yin", "Zirui Wu", "Yansong Feng"], "categories": ["cs.CL", "cs.AI"], "comment": "Code is available at https://github.com/xxxiaol/RefTool", "summary": "Tools enhance the reasoning capabilities of large language models (LLMs) in\ncomplex problem-solving tasks, but not all tasks have available tools. In the\nabsence of predefined tools, prior works have explored instructing LLMs to\ngenerate tools on their own. However, such approaches rely heavily on the\nmodels' internal knowledge and would fail in domains beyond the LLMs' knowledge\nscope. To address this limitation, we propose RefTool, a reference-guided\nframework for automatic tool creation that leverages structured external\nmaterials such as textbooks. RefTool consists of two modules: (1) tool\ncreation, where LLMs generate executable tools from reference content, validate\nthem using illustrative examples, and organize them hierarchically into a\ntoolbox; and (2) tool utilization, where LLMs navigate the toolbox structure to\nselect and apply the appropriate tools to solve problems. Experiments on\ncausality, physics, and chemistry benchmarks demonstrate that RefTool\noutperforms existing tool-creation and domain-specific reasoning methods by\n11.3% on average accuracy, while being cost-efficient and broadly\ngeneralizable. Analyses reveal that grounding tool creation in references\nproduces accurate and faithful tools, and that the hierarchical structure\nfacilitates effective tool selection. RefTool enables LLMs to overcome\nknowledge limitations, demonstrating the value of grounding tool creation in\nexternal references for enhanced and generalizable reasoning."}
{"id": "2505.21439", "pdf": "https://arxiv.org/pdf/2505.21439.pdf", "abs": "https://arxiv.org/abs/2505.21439", "title": "Towards Better Instruction Following Retrieval Models", "authors": ["Yuchen Zhuang", "Aaron Trinh", "Rushi Qiang", "Haotian Sun", "Chao Zhang", "Hanjun Dai", "Bo Dai"], "categories": ["cs.CL", "cs.IR"], "comment": "Retrieval Models, Embedding, Retrieval with Instructions", "summary": "Modern information retrieval (IR) models, trained exclusively on standard\n<query, passage> pairs, struggle to effectively interpret and follow explicit\nuser instructions. We introduce InF-IR, a large-scale, high-quality training\ncorpus tailored for enhancing retrieval models in Instruction-Following IR.\nInF-IR expands traditional training pairs into over 38,000 expressive\n<instruction, query, passage> triplets as positive samples. In particular, for\neach positive triplet, we generate two additional hard negative examples by\npoisoning both instructions and queries, then rigorously validated by an\nadvanced reasoning model (o3-mini) to ensure semantic plausibility while\nmaintaining instructional incorrectness. Unlike existing corpora that primarily\nsupport computationally intensive reranking tasks for decoder-only language\nmodels, the highly contrastive positive-negative triplets in InF-IR further\nenable efficient representation learning for smaller encoder-only models,\nfacilitating direct embedding-based retrieval. Using this corpus, we train\nInF-Embed, an instruction-aware Embedding model optimized through contrastive\nlearning and instruction-query attention mechanisms to align retrieval outcomes\nprecisely with user intents. Extensive experiments across five\ninstruction-based retrieval benchmarks demonstrate that InF-Embed significantly\nsurpasses competitive baselines by 8.1% in p-MRR, measuring the\ninstruction-following capabilities."}
{"id": "2505.21451", "pdf": "https://arxiv.org/pdf/2505.21451.pdf", "abs": "https://arxiv.org/abs/2505.21451", "title": "Words Like Knives: Backstory-Personalized Modeling and Detection of Violent Communication", "authors": ["Jocelyn Shen", "Akhila Yerukola", "Xuhui Zhou", "Cynthia Breazeal", "Maarten Sap", "Hae Won Park"], "categories": ["cs.CL"], "comment": null, "summary": "Conversational breakdowns in close relationships are deeply shaped by\npersonal histories and emotional context, yet most NLP research treats conflict\ndetection as a general task, overlooking the relational dynamics that influence\nhow messages are perceived. In this work, we leverage nonviolent communication\n(NVC) theory to evaluate LLMs in detecting conversational breakdowns and\nassessing how relationship backstory influences both human and model perception\nof conflicts. Given the sensitivity and scarcity of real-world datasets\nfeaturing conflict between familiar social partners with rich personal\nbackstories, we contribute the PersonaConflicts Corpus, a dataset of N=5,772\nnaturalistic simulated dialogues spanning diverse conflict scenarios between\nfriends, family members, and romantic partners. Through a controlled human\nstudy, we annotate a subset of dialogues and obtain fine-grained labels of\ncommunication breakdown types on individual turns, and assess the impact of\nbackstory on human and model perception of conflict in conversation. We find\nthat the polarity of relationship backstories significantly shifted human\nperception of communication breakdowns and impressions of the social partners,\nyet models struggle to meaningfully leverage those backstories in the detection\ntask. Additionally, we find that models consistently overestimate how\npositively a message will make a listener feel. Our findings underscore the\ncritical role of personalization to relationship contexts in enabling LLMs to\nserve as effective mediators in human communication for authentic connection."}
{"id": "2505.21458", "pdf": "https://arxiv.org/pdf/2505.21458.pdf", "abs": "https://arxiv.org/abs/2505.21458", "title": "Do LLMs Need to Think in One Language? Correlation between Latent Language and Task Performance", "authors": ["Shintaro Ozaki", "Tatsuya Hiraoka", "Hiroto Otake", "Hiroki Ouchi", "Masaru Isonuma", "Benjamin Heinzerling", "Kentaro Inui", "Taro Watanabe", "Yusuke Miyao", "Yohei Oseki", "Yu Takagi"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are known to process information using a\nproficient internal language consistently, referred to as latent language,\nwhich may differ from the input or output languages. However, how the\ndiscrepancy between the latent language and the input and output language\naffects downstream task performance remains largely unexplored. While many\nstudies research the latent language of LLMs, few address its importance in\ninfluencing task performance. In our study, we hypothesize that thinking in\nlatent language consistently enhances downstream task performance. To validate\nthis, our work varies the input prompt languages across multiple downstream\ntasks and analyzes the correlation between consistency in latent language and\ntask performance. We create datasets consisting of questions from diverse\ndomains such as translation and geo-culture, which are influenced by the choice\nof latent language. Experimental results across multiple LLMs on translation\nand geo-culture tasks, which are sensitive to the choice of language, indicate\nthat maintaining consistency in latent language is not always necessary for\noptimal downstream task performance. This is because these models adapt their\ninternal representations near the final layers to match the target language,\nreducing the impact of consistency on overall performance."}
{"id": "2505.21467", "pdf": "https://arxiv.org/pdf/2505.21467.pdf", "abs": "https://arxiv.org/abs/2505.21467", "title": "Accelerating Diffusion Language Model Inference via Efficient KV Caching and Guided Diffusion", "authors": ["Zhanqiu Hu", "Jian Meng", "Yash Akhauri", "Mohamed S. Abdelfattah", "Jae-sun Seo", "Zhiru Zhang", "Udit Gupta"], "categories": ["cs.CL"], "comment": null, "summary": "Diffusion language models offer parallel token generation and inherent\nbidirectionality, promising more efficient and powerful sequence modeling\ncompared to autoregressive approaches. However, state-of-the-art diffusion\nmodels (e.g., Dream 7B, LLaDA 8B) suffer from slow inference. While they match\nthe quality of similarly sized Autoregressive (AR) Models (e.g., Qwen2.5 7B,\nLlama3 8B), their iterative denoising requires multiple full-sequence forward\npasses, resulting in high computational costs and latency, particularly for\nlong input prompts and long-context scenarios. Furthermore, parallel token\ngeneration introduces token incoherence problems, and current sampling\nheuristics suffer from significant quality drops with decreasing denoising\nsteps. We address these limitations with two training-free techniques. First,\nwe propose FreeCache, a Key-Value (KV) approximation caching technique that\nreuses stable KV projections across denoising steps, effectively reducing the\ncomputational cost of DLM inference. Second, we introduce Guided Diffusion, a\ntraining-free method that uses a lightweight pretrained autoregressive model to\nsupervise token unmasking, dramatically reducing the total number of denoising\niterations without sacrificing quality. We conduct extensive evaluations on\nopen-source reasoning benchmarks, and our combined methods deliver up to a 34x\nend-to-end speedup without compromising accuracy. For the first time, diffusion\nlanguage models achieve a comparable and even faster latency as the widely\nadopted autoregressive models. Our work successfully paved the way for scaling\nup the diffusion language model to a broader scope of applications across\ndifferent domains."}
{"id": "2505.21471", "pdf": "https://arxiv.org/pdf/2505.21471.pdf", "abs": "https://arxiv.org/abs/2505.21471", "title": "Scaling External Knowledge Input Beyond Context Windows of LLMs via Multi-Agent Collaboration", "authors": ["Zijun Liu", "Zhennan Wan", "Peng Li", "Ming Yan", "Ji Zhang", "Fei Huang", "Yang Liu"], "categories": ["cs.CL"], "comment": "30 pages, 9 figures. Code and data are available at\n  https://github.com/THUNLP-MT/ExtAgents", "summary": "With the rapid advancement of post-training techniques for reasoning and\ninformation seeking, large language models (LLMs) can incorporate a large\nquantity of retrieved knowledge to solve complex tasks. However, the limited\ncontext window of LLMs obstructs scaling the amount of external knowledge\ninput, prohibiting further improvement, especially for tasks requiring\nsignificant amount of external knowledge. Existing context window extension\nmethods inevitably cause information loss. LLM-based multi-agent methods emerge\nas a new paradigm to handle massive input in a distributional manner, where we\nidentify two core bottlenecks in existing knowledge synchronization and\nreasoning processes. In this work, we develop a multi-agent framework,\n$\\textbf{ExtAgents}$, to overcome the bottlenecks and enable better scalability\nin inference-time knowledge integration without longer-context training.\nBenchmarked with our enhanced multi-hop question answering test,\n$\\textbf{$\\boldsymbol{\\infty}$Bench+}$, and other public test sets including\nlong survey generation, ExtAgents significantly enhances the performance over\nexisting non-training methods with the same amount of external knowledge input,\nregardless of whether it falls $\\textit{within or exceeds the context window}$.\nMoreover, the method maintains high efficiency due to high parallelism. Further\nstudy in the coordination of LLM agents on increasing external knowledge input\ncould benefit real-world applications."}
{"id": "2505.21479", "pdf": "https://arxiv.org/pdf/2505.21479.pdf", "abs": "https://arxiv.org/abs/2505.21479", "title": "Are Language Models Consequentialist or Deontological Moral Reasoners?", "authors": ["Keenan Samway", "Max Kleiman-Weiner", "David Guzman Piedrahita", "Rada Mihalcea", "Bernhard Schölkopf", "Zhijing Jin"], "categories": ["cs.CL"], "comment": null, "summary": "As AI systems increasingly navigate applications in healthcare, law, and\ngovernance, understanding how they handle ethically complex scenarios becomes\ncritical. Previous work has mainly examined the moral judgments in large\nlanguage models (LLMs), rather than their underlying moral reasoning process.\nIn contrast, we focus on a large-scale analysis of the moral reasoning traces\nprovided by LLMs. Furthermore, unlike prior work that attempted to draw\ninferences from only a handful of moral dilemmas, our study leverages over 600\ndistinct trolley problems as probes for revealing the reasoning patterns that\nemerge within different LLMs. We introduce and test a taxonomy of moral\nrationales to systematically classify reasoning traces according to two main\nnormative ethical theories: consequentialism and deontology. Our analysis\nreveals that LLM chains-of-thought tend to favor deontological principles based\non moral obligations, while post-hoc explanations shift notably toward\nconsequentialist rationales that emphasize utility. Our framework provides a\nfoundation for understanding how LLMs process and articulate ethical\nconsiderations, an important step toward safe and interpretable deployment of\nLLMs in high-stakes decision-making environments. Our code is available at\nhttps://github.com/keenansamway/moral-lens ."}
{"id": "2505.21496", "pdf": "https://arxiv.org/pdf/2505.21496.pdf", "abs": "https://arxiv.org/abs/2505.21496", "title": "UI-Genie: A Self-Improving Approach for Iteratively Boosting MLLM-based Mobile GUI Agents", "authors": ["Han Xiao", "Guozhi Wang", "Yuxiang Chai", "Zimu Lu", "Weifeng Lin", "Hao He", "Lue Fan", "Liuyang Bian", "Rui Hu", "Liang Liu", "Shuai Ren", "Yafei Wen", "Xiaoxin Chen", "Aojun Zhou", "Hongsheng Li"], "categories": ["cs.CL", "cs.CV", "cs.LG"], "comment": "https://github.com/Euphoria16/UI-Genie", "summary": "In this paper, we introduce UI-Genie, a self-improving framework addressing\ntwo key challenges in GUI agents: verification of trajectory outcome is\nchallenging and high-quality training data are not scalable. These challenges\nare addressed by a reward model and a self-improving pipeline, respectively.\nThe reward model, UI-Genie-RM, features an image-text interleaved architecture\nthat efficiently pro- cesses historical context and unifies action-level and\ntask-level rewards. To sup- port the training of UI-Genie-RM, we develop\ndeliberately-designed data genera- tion strategies including rule-based\nverification, controlled trajectory corruption, and hard negative mining. To\naddress the second challenge, a self-improvement pipeline progressively expands\nsolvable complex GUI tasks by enhancing both the agent and reward models\nthrough reward-guided exploration and outcome verification in dynamic\nenvironments. For training the model, we generate UI- Genie-RM-517k and\nUI-Genie-Agent-16k, establishing the first reward-specific dataset for GUI\nagents while demonstrating high-quality synthetic trajectory gen- eration\nwithout manual annotation. Experimental results show that UI-Genie achieves\nstate-of-the-art performance across multiple GUI agent benchmarks with three\ngenerations of data-model self-improvement. We open-source our complete\nframework implementation and generated datasets to facilitate further research\nin https://github.com/Euphoria16/UI-Genie."}
{"id": "2505.21503", "pdf": "https://arxiv.org/pdf/2505.21503.pdf", "abs": "https://arxiv.org/abs/2505.21503", "title": "Silence is Not Consensus: Disrupting Agreement Bias in Multi-Agent LLMs via Catfish Agent for Clinical Decision Making", "authors": ["Yihan Wang", "Qiao Yan", "Zhenghao Xing", "Lihao Liu", "Junjun He", "Chi-Wing Fu", "Xiaowei Hu", "Pheng-Ann Heng"], "categories": ["cs.CL", "cs.AI", "cs.LG", "q-bio.OT"], "comment": null, "summary": "Large language models (LLMs) have demonstrated strong potential in clinical\nquestion answering, with recent multi-agent frameworks further improving\ndiagnostic accuracy via collaborative reasoning. However, we identify a\nrecurring issue of Silent Agreement, where agents prematurely converge on\ndiagnoses without sufficient critical analysis, particularly in complex or\nambiguous cases. We present a new concept called Catfish Agent, a\nrole-specialized LLM designed to inject structured dissent and counter silent\nagreement. Inspired by the ``catfish effect'' in organizational psychology, the\nCatfish Agent is designed to challenge emerging consensus to stimulate deeper\nreasoning. We formulate two mechanisms to encourage effective and context-aware\ninterventions: (i) a complexity-aware intervention that modulates agent\nengagement based on case difficulty, and (ii) a tone-calibrated intervention\narticulated to balance critique and collaboration. Evaluations on nine medical\nQ&A and three medical VQA benchmarks show that our approach consistently\noutperforms both single- and multi-agent LLMs frameworks, including leading\ncommercial models such as GPT-4o and DeepSeek-R1."}
{"id": "2505.21505", "pdf": "https://arxiv.org/pdf/2505.21505.pdf", "abs": "https://arxiv.org/abs/2505.21505", "title": "How does Alignment Enhance LLMs' Multilingual Capabilities? A Language Neurons Perspective", "authors": ["Shimao Zhang", "Zhejian Lai", "Xiang Liu", "Shuaijie She", "Xiao Liu", "Yeyun Gong", "Shujian Huang", "Jiajun Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multilingual Alignment is an effective and representative paradigm to enhance\nLLMs' multilingual capabilities, which transfers the capabilities from the\nhigh-resource languages to the low-resource languages. Meanwhile, some\nresearches on language-specific neurons reveal that there are language-specific\nneurons that are selectively activated in LLMs when processing different\nlanguages. This provides a new perspective to analyze and understand LLMs'\nmechanisms more specifically in multilingual scenarios. In this work, we\npropose a new finer-grained neuron identification algorithm, which detects\nlanguage neurons~(including language-specific neurons and language-related\nneurons) and language-agnostic neurons. Furthermore, based on the\ndistributional characteristics of different types of neurons, we divide the\nLLMs' internal process for multilingual inference into four parts: (1)\nmultilingual understanding, (2) shared semantic space reasoning, (3)\nmultilingual output space transformation, and (4) vocabulary space outputting.\nAdditionally, we systematically analyze the models before and after alignment\nwith a focus on different types of neurons. We also analyze the phenomenon of\n''Spontaneous Multilingual Alignment''. Overall, our work conducts a\ncomprehensive investigation based on different types of neurons, providing\nempirical results and valuable insights for better understanding multilingual\nalignment and multilingual capabilities of LLMs."}
{"id": "2505.18291", "pdf": "https://arxiv.org/pdf/2505.18291.pdf", "abs": "https://arxiv.org/abs/2505.18291", "title": "InstructPart: Task-Oriented Part Segmentation with Instruction Reasoning", "authors": ["Zifu Wan", "Yaqi Xie", "Ce Zhang", "Zhiqiu Lin", "Zihan Wang", "Simon Stepputtis", "Deva Ramanan", "Katia Sycara"], "categories": ["cs.CV", "cs.CL", "cs.RO"], "comment": "Accepted by ACL 2025 Main. Project page:\n  https://zifuwan.github.io/InstructPart/", "summary": "Large multimodal foundation models, particularly in the domains of language\nand vision, have significantly advanced various tasks, including robotics,\nautonomous driving, information retrieval, and grounding. However, many of\nthese models perceive objects as indivisible, overlooking the components that\nconstitute them. Understanding these components and their associated\naffordances provides valuable insights into an object's functionality, which is\nfundamental for performing a wide range of tasks. In this work, we introduce a\nnovel real-world benchmark, InstructPart, comprising hand-labeled part\nsegmentation annotations and task-oriented instructions to evaluate the\nperformance of current models in understanding and executing part-level tasks\nwithin everyday contexts. Through our experiments, we demonstrate that\ntask-oriented part segmentation remains a challenging problem, even for\nstate-of-the-art Vision-Language Models (VLMs). In addition to our benchmark,\nwe introduce a simple baseline that achieves a twofold performance improvement\nthrough fine-tuning with our dataset. With our dataset and benchmark, we aim to\nfacilitate research on task-oriented part segmentation and enhance the\napplicability of VLMs across various domains, including robotics, virtual\nreality, information retrieval, and other related fields. Project website:\nhttps://zifuwan.github.io/InstructPart/."}
{"id": "2505.20326", "pdf": "https://arxiv.org/pdf/2505.20326.pdf", "abs": "https://arxiv.org/abs/2505.20326", "title": "Cultural Awareness in Vision-Language Models: A Cross-Country Exploration", "authors": ["Avinash Madasu", "Vasudev Lal", "Phillip Howard"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Vision-Language Models (VLMs) are increasingly deployed in diverse cultural\ncontexts, yet their internal biases remain poorly understood. In this work, we\npropose a novel framework to systematically evaluate how VLMs encode cultural\ndifferences and biases related to race, gender, and physical traits across\ncountries. We introduce three retrieval-based tasks: (1) Race to Country\nretrieval, which examines the association between individuals from specific\nracial groups (East Asian, White, Middle Eastern, Latino, South Asian, and\nBlack) and different countries; (2) Personal Traits to Country retrieval, where\nimages are paired with trait-based prompts (e.g., Smart, Honest, Criminal,\nViolent) to investigate potential stereotypical associations; and (3) Physical\nCharacteristics to Country retrieval, focusing on visual attributes like\nskinny, young, obese, and old to explore how physical appearances are\nculturally linked to nations. Our findings reveal persistent biases in VLMs,\nhighlighting how visual representations may inadvertently reinforce societal\nstereotypes."}
{"id": "2505.20341", "pdf": "https://arxiv.org/pdf/2505.20341.pdf", "abs": "https://arxiv.org/abs/2505.20341", "title": "Towards Emotionally Consistent Text-Based Speech Editing: Introducing EmoCorrector and The ECD-TSE Dataset", "authors": ["Rui Liu", "Pu Gao", "Jiatian Xi", "Berrak Sisman", "Carlos Busso", "Haizhou Li"], "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": "INTERSPEECH2025. Code and audio examples:\n  https://github.com/AI-S2-Lab/EmoCorrector", "summary": "Text-based speech editing (TSE) modifies speech using only text, eliminating\nre-recording. However, existing TSE methods, mainly focus on the content\naccuracy and acoustic consistency of synthetic speech segments, and often\noverlook the emotional shifts or inconsistency issues introduced by text\nchanges. To address this issue, we propose EmoCorrector, a novel\npost-correction scheme for TSE. EmoCorrector leverages Retrieval-Augmented\nGeneration (RAG) by extracting the edited text's emotional features, retrieving\nspeech samples with matching emotions, and synthesizing speech that aligns with\nthe desired emotion while preserving the speaker's identity and quality. To\nsupport the training and evaluation of emotional consistency modeling in TSE,\nwe pioneer the benchmarking Emotion Correction Dataset for TSE (ECD-TSE). The\nprominent aspect of ECD-TSE is its inclusion of $<$text, speech$>$ paired data\nfeaturing diverse text variations and a range of emotional expressions.\nSubjective and objective experiments and comprehensive analysis on ECD-TSE\nconfirm that EmoCorrector significantly enhances the expression of intended\nemotion while addressing emotion inconsistency limitations in current TSE\nmethods. Code and audio examples are available at\nhttps://github.com/AI-S2-Lab/EmoCorrector."}
{"id": "2505.20368", "pdf": "https://arxiv.org/pdf/2505.20368.pdf", "abs": "https://arxiv.org/abs/2505.20368", "title": "Hierarchical Retrieval with Evidence Curation for Open-Domain Financial Question Answering on Standardized Documents", "authors": ["Jaeyoung Choe", "Jihoon Kim", "Woohwan Jung"], "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "ACL 2025 (Findings)", "summary": "Retrieval-augmented generation (RAG) based large language models (LLMs) are\nwidely used in finance for their excellent performance on knowledge-intensive\ntasks. However, standardized documents (e.g., SEC filing) share similar formats\nsuch as repetitive boilerplate texts, and similar table structures. This\nsimilarity forces traditional RAG methods to misidentify near-duplicate text,\nleading to duplicate retrieval that undermines accuracy and completeness. To\naddress these issues, we propose the Hierarchical Retrieval with Evidence\nCuration (HiREC) framework. Our approach first performs hierarchical retrieval\nto reduce confusion among similar texts. It first retrieve related documents\nand then selects the most relevant passages from the documents. The evidence\ncuration process removes irrelevant passages. When necessary, it automatically\ngenerates complementary queries to collect missing information. To evaluate our\napproach, we construct and release a Large-scale Open-domain Financial (LOFin)\nquestion answering benchmark that includes 145,897 SEC documents and 1,595\nquestion-answer pairs. Our code and data are available at\nhttps://github.com/deep-over/LOFin-bench-HiREC."}
{"id": "2505.20405", "pdf": "https://arxiv.org/pdf/2505.20405.pdf", "abs": "https://arxiv.org/abs/2505.20405", "title": "What Changed? Detecting and Evaluating Instruction-Guided Image Edits with Multimodal Large Language Models", "authors": ["Lorenzo Baraldi", "Davide Bucciarelli", "Federico Betti", "Marcella Cornia", "Lorenzo Baraldi", "Nicu Sebe", "Rita Cucchiara"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "comment": null, "summary": "Instruction-based image editing models offer increased personalization\nopportunities in generative tasks. However, properly evaluating their results\nis challenging, and most of the existing metrics lag in terms of alignment with\nhuman judgment and explainability. To tackle these issues, we introduce DICE\n(DIfference Coherence Estimator), a model designed to detect localized\ndifferences between the original and the edited image and to assess their\nrelevance to the given modification request. DICE consists of two key\ncomponents: a difference detector and a coherence estimator, both built on an\nautoregressive Multimodal Large Language Model (MLLM) and trained using a\nstrategy that leverages self-supervision, distillation from inpainting\nnetworks, and full supervision. Through extensive experiments, we evaluate each\nstage of our pipeline, comparing different MLLMs within the proposed framework.\nWe demonstrate that DICE effectively identifies coherent edits, effectively\nevaluating images generated by different editing models with a strong\ncorrelation with human judgment. We publicly release our source code, models,\nand data."}
{"id": "2505.20411", "pdf": "https://arxiv.org/pdf/2505.20411.pdf", "abs": "https://arxiv.org/abs/2505.20411", "title": "SWE-rebench: An Automated Pipeline for Task Collection and Decontaminated Evaluation of Software Engineering Agents", "authors": ["Ibragim Badertdinov", "Alexander Golubev", "Maksim Nekrashevich", "Anton Shevtsov", "Simon Karasik", "Andrei Andriushchenko", "Maria Trofimova", "Daria Litvintseva", "Boris Yangel"], "categories": ["cs.SE", "cs.CL"], "comment": "Dataset: https://huggingface.co/datasets/nebius/SWE-rebench,\n  SWE-rebench leaderboard https://swe-rebench.com", "summary": "LLM-based agents have shown promising capabilities in a growing range of\nsoftware engineering (SWE) tasks. However, advancing this field faces two\ncritical challenges. First, high-quality training data is scarce, especially\ndata that reflects real-world SWE scenarios, where agents must interact with\ndevelopment environments, execute code and adapt behavior based on the outcomes\nof their actions. Existing datasets are either limited to one-shot code\ngeneration or comprise small, manually curated collections of interactive\ntasks, lacking both scale and diversity. Second, the lack of fresh interactive\nSWE tasks affects evaluation of rapidly improving models, as static benchmarks\nquickly become outdated due to contamination issues. To address these\nlimitations, we introduce a novel, automated, and scalable pipeline to\ncontinuously extract real-world interactive SWE tasks from diverse GitHub\nrepositories. Using this pipeline, we construct SWE-rebench, a public dataset\ncomprising over 21,000 interactive Python-based SWE tasks, suitable for\nreinforcement learning of SWE agents at scale. Additionally, we use continuous\nsupply of fresh tasks collected using SWE-rebench methodology to build a\ncontamination-free benchmark for agentic software engineering. We compare\nresults of various LLMs on this benchmark to results on SWE-bench Verified and\nshow that performance of some language models might be inflated due to\ncontamination issues."}
{"id": "2505.20464", "pdf": "https://arxiv.org/pdf/2505.20464.pdf", "abs": "https://arxiv.org/abs/2505.20464", "title": "The Impact of a Chatbot's Ephemerality-Framing on Self-Disclosure Perceptions", "authors": ["Samuel Rhys Cox", "Rune Møberg Jacobsen", "Niels van Berkel"], "categories": ["cs.HC", "cs.CL"], "comment": "In ACM Conversational User Interfaces (CUI '25), July 8-10, 2025; 18\n  pages; 6 Figures; 6 Tables", "summary": "Self-disclosure, the sharing of one's thoughts and feelings, is affected by\nthe perceived relationship between individuals. While chatbots are increasingly\nused for self-disclosure, the impact of a chatbot's framing on users'\nself-disclosure remains under-explored. We investigated how a chatbot's\ndescription of its relationship with users, particularly in terms of\nephemerality, affects self-disclosure. Specifically, we compared a Familiar\nchatbot, presenting itself as a companion remembering past interactions, with a\nStranger chatbot, presenting itself as a new, unacquainted entity in each\nconversation. In a mixed factorial design, participants engaged with either the\nFamiliar or Stranger chatbot in two sessions across two days, with one\nconversation focusing on Emotional- and another Factual-disclosure. When\nEmotional-disclosure was sought in the first chatting session,\nStranger-condition participants felt more comfortable self-disclosing. However,\nwhen Factual-disclosure was sought first, these differences were replaced by\nmore enjoyment among Familiar-condition participants. Qualitative findings\nshowed Stranger afforded anonymity and reduced judgement, whereas Familiar\nsometimes felt intrusive unless rapport was built via low-risk\nFactual-disclosure."}
{"id": "2505.20480", "pdf": "https://arxiv.org/pdf/2505.20480.pdf", "abs": "https://arxiv.org/abs/2505.20480", "title": "BrainStratify: Coarse-to-Fine Disentanglement of Intracranial Neural Dynamics", "authors": ["Hui Zheng", "Hai-Teng Wang", "Yi-Tao Jing", "Pei-Yang Lin", "Han-Qing Zhao", "Wei Chen", "Peng-Hu Wei", "Yong-Zhi Shan", "Guo-Guang Zhao", "Yun-Zhe Liu"], "categories": ["eess.SP", "cs.CL", "q-bio.NC"], "comment": null, "summary": "Decoding speech directly from neural activity is a central goal in\nbrain-computer interface (BCI) research. In recent years, exciting advances\nhave been made through the growing use of intracranial field potential\nrecordings, such as stereo-ElectroEncephaloGraphy (sEEG) and\nElectroCorticoGraphy (ECoG). These neural signals capture rich population-level\nactivity but present key challenges: (i) task-relevant neural signals are\nsparsely distributed across sEEG electrodes, and (ii) they are often entangled\nwith task-irrelevant neural signals in both sEEG and ECoG. To address these\nchallenges, we introduce a unified Coarse-to-Fine neural disentanglement\nframework, BrainStratify, which includes (i) identifying functional groups\nthrough spatial-context-guided temporal-spatial modeling, and (ii)\ndisentangling distinct neural dynamics within the target functional group using\nDecoupled Product Quantization (DPQ). We evaluate BrainStratify on two\nopen-source sEEG datasets and one (epidural) ECoG dataset, spanning tasks like\nvocal production and speech perception. Extensive experiments show that\nBrainStratify, as a unified framework for decoding speech from intracranial\nneural signals, significantly outperforms previous decoding methods. Overall,\nby combining data-driven stratification with neuroscience-inspired modularity,\nBrainStratify offers a robust and interpretable solution for speech decoding\nfrom intracranial recordings."}
{"id": "2505.20503", "pdf": "https://arxiv.org/pdf/2505.20503.pdf", "abs": "https://arxiv.org/abs/2505.20503", "title": "Embodied AI with Foundation Models for Mobile Service Robots: A Systematic Review", "authors": ["Matthew Lisondra", "Beno Benhabib", "Goldie Nejat"], "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": null, "summary": "Rapid advancements in foundation models, including Large Language Models,\nVision-Language Models, Multimodal Large Language Models, and\nVision-Language-Action Models have opened new avenues for embodied AI in mobile\nservice robotics. By combining foundation models with the principles of\nembodied AI, where intelligent systems perceive, reason, and act through\nphysical interactions, robots can improve understanding, adapt to, and execute\ncomplex tasks in dynamic real-world environments. However, embodied AI in\nmobile service robots continues to face key challenges, including multimodal\nsensor fusion, real-time decision-making under uncertainty, task\ngeneralization, and effective human-robot interactions (HRI). In this paper, we\npresent the first systematic review of the integration of foundation models in\nmobile service robotics, identifying key open challenges in embodied AI and\nexamining how foundation models can address them. Namely, we explore the role\nof such models in enabling real-time sensor fusion, language-conditioned\ncontrol, and adaptive task execution. Furthermore, we discuss real-world\napplications in the domestic assistance, healthcare, and service automation\nsectors, demonstrating the transformative impact of foundation models on\nservice robotics. We also include potential future research directions,\nemphasizing the need for predictive scaling laws, autonomous long-term\nadaptation, and cross-embodiment generalization to enable scalable, efficient,\nand robust deployment of foundation models in human-centric robotic systems."}
{"id": "2505.20521", "pdf": "https://arxiv.org/pdf/2505.20521.pdf", "abs": "https://arxiv.org/abs/2505.20521", "title": "Project Riley: Multimodal Multi-Agent LLM Collaboration with Emotional Reasoning and Voting", "authors": ["Ana Rita Ortigoso", "Gabriel Vieira", "Daniel Fuentes", "Luis Frazão", "Nuno Costa", "António Pereira"], "categories": ["cs.AI", "cs.CL", "I.2.7; I.2.1; H.5.2"], "comment": "28 pages, 5 figures. Submitted for review to Information Fusion", "summary": "This paper presents Project Riley, a novel multimodal and multi-model\nconversational AI architecture oriented towards the simulation of reasoning\ninfluenced by emotional states. Drawing inspiration from Pixar's Inside Out,\nthe system comprises five distinct emotional agents - Joy, Sadness, Fear,\nAnger, and Disgust - that engage in structured multi-round dialogues to\ngenerate, criticise, and iteratively refine responses. A final reasoning\nmechanism synthesises the contributions of these agents into a coherent output\nthat either reflects the dominant emotion or integrates multiple perspectives.\nThe architecture incorporates both textual and visual large language models\n(LLMs), alongside advanced reasoning and self-refinement processes. A\nfunctional prototype was deployed locally in an offline environment, optimised\nfor emotional expressiveness and computational efficiency. From this initial\nprototype, another one emerged, called Armando, which was developed for use in\nemergency contexts, delivering emotionally calibrated and factually accurate\ninformation through the integration of Retrieval-Augmented Generation (RAG) and\ncumulative context tracking. The Project Riley prototype was evaluated through\nuser testing, in which participants interacted with the chatbot and completed a\nstructured questionnaire assessing three dimensions: Emotional Appropriateness,\nClarity and Utility, and Naturalness and Human-likeness. The results indicate\nstrong performance in structured scenarios, particularly with respect to\nemotional alignment and communicative clarity."}
{"id": "2505.20522", "pdf": "https://arxiv.org/pdf/2505.20522.pdf", "abs": "https://arxiv.org/abs/2505.20522", "title": "Scaling over Scaling: Exploring Test-Time Scaling Pareto in Large Reasoning Models", "authors": ["Jian Wang", "Boyan Zhu", "Chak Tou Leong", "Yongqi Li", "Wenjie Li"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Work in progress", "summary": "Large reasoning models (LRMs) have exhibited the capacity of enhancing\nreasoning performance via internal test-time scaling. Building upon this, a\npromising direction is to further scale test-time compute to unlock even\ngreater reasoning capabilities. However, as we push these scaling boundaries,\nsystematically understanding the practical limits and achieving optimal\nresource allocation becomes a critical challenge. In this paper, we investigate\nthe scaling Pareto of test-time scaling and introduce the Test-Time Scaling\nPerformance Model (TTSPM). We theoretically analyze two fundamental paradigms\nfor such extended scaling, parallel scaling and sequential scaling, from a\nprobabilistic modeling perspective. Our primary contribution is the derivation\nof the saturation point on the scaling budget for both strategies, identifying\nthresholds beyond which additional computation yields diminishing returns.\nRemarkably, despite their distinct mechanisms, both paradigms converge to a\nunified mathematical structure in their upper bounds. We empirically validate\nour theoretical findings on challenging reasoning benchmarks, including AIME,\nMATH-500, and GPQA, demonstrating the practical utility of these bounds for\ntest-time resource allocation. We hope that this work provides insights into\nthe cost-benefit trade-offs of test-time scaling, guiding the development of\nmore resource-efficient inference strategies for large reasoning models."}
{"id": "2505.20561", "pdf": "https://arxiv.org/pdf/2505.20561.pdf", "abs": "https://arxiv.org/abs/2505.20561", "title": "Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning", "authors": ["Shenao Zhang", "Yaqing Wang", "Yinxiao Liu", "Tianqi Liu", "Peter Grabowski", "Eugene Ie", "Zhaoran Wang", "Yunxuan Li"], "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Large Language Models (LLMs) trained via Reinforcement Learning (RL) have\nexhibited strong reasoning capabilities and emergent reflective behaviors, such\nas backtracking and error correction. However, conventional Markovian RL\nconfines exploration to the training phase to learn an optimal deterministic\npolicy and depends on the history contexts only through the current state.\nTherefore, it remains unclear whether reflective reasoning will emerge during\nMarkovian RL training, or why they are beneficial at test time. To remedy this,\nwe recast reflective exploration within the Bayes-Adaptive RL framework, which\nexplicitly optimizes the expected return under a posterior distribution over\nMarkov decision processes. This Bayesian formulation inherently incentivizes\nboth reward-maximizing exploitation and information-gathering exploration via\nbelief updates. Our resulting algorithm, BARL, instructs the LLM to stitch and\nswitch strategies based on the observed outcomes, offering principled guidance\non when and how the model should reflectively explore. Empirical results on\nboth synthetic and mathematical reasoning tasks demonstrate that BARL\noutperforms standard Markovian RL approaches at test time, achieving superior\ntoken efficiency with improved exploration effectiveness. Our code is available\nat https://github.com/shenao-zhang/BARL."}
{"id": "2505.20609", "pdf": "https://arxiv.org/pdf/2505.20609.pdf", "abs": "https://arxiv.org/abs/2505.20609", "title": "Comparisons between a Large Language Model-based Real-Time Compound Diagnostic Medical AI Interface and Physicians for Common Internal Medicine Cases using Simulated Patients", "authors": ["Hyungjun Park", "Chang-Yun Woo", "Seungjo Lim", "Seunghwan Lim", "Keunho Kwak", "Ju Young Jeong", "Chong Hyun Suh"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Objective To develop an LLM based realtime compound diagnostic medical AI\ninterface and performed a clinical trial comparing this interface and\nphysicians for common internal medicine cases based on the United States\nMedical License Exam (USMLE) Step 2 Clinical Skill (CS) style exams. Methods A\nnonrandomized clinical trial was conducted on August 20, 2024. We recruited one\ngeneral physician, two internal medicine residents (2nd and 3rd year), and five\nsimulated patients. The clinical vignettes were adapted from the USMLE Step 2\nCS style exams. We developed 10 representative internal medicine cases based on\nactual patients and included information available on initial diagnostic\nevaluation. Primary outcome was the accuracy of the first differential\ndiagnosis. Repeatability was evaluated based on the proportion of agreement.\nResults The accuracy of the physicians' first differential diagnosis ranged\nfrom 50% to 70%, whereas the realtime compound diagnostic medical AI interface\nachieved an accuracy of 80%. The proportion of agreement for the first\ndifferential diagnosis was 0.7. The accuracy of the first and second\ndifferential diagnoses ranged from 70% to 90% for physicians, whereas the AI\ninterface achieved an accuracy rate of 100%. The average time for the AI\ninterface (557 sec) was 44.6% shorter than that of the physicians (1006 sec).\nThe AI interface ($0.08) also reduced costs by 98.1% compared to the\nphysicians' average ($4.2). Patient satisfaction scores ranged from 4.2 to 4.3\nfor care by physicians and were 3.9 for the AI interface Conclusion An LLM\nbased realtime compound diagnostic medical AI interface demonstrated diagnostic\naccuracy and patient satisfaction comparable to those of a physician, while\nrequiring less time and lower costs. These findings suggest that AI interfaces\nmay have the potential to assist primary care consultations for common internal\nmedicine cases."}
{"id": "2505.20612", "pdf": "https://arxiv.org/pdf/2505.20612.pdf", "abs": "https://arxiv.org/abs/2505.20612", "title": "Roboflow100-VL: A Multi-Domain Object Detection Benchmark for Vision-Language Models", "authors": ["Peter Robicheaux", "Matvei Popov", "Anish Madan", "Isaac Robinson", "Joseph Nelson", "Deva Ramanan", "Neehar Peri"], "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "The first two authors contributed equally", "summary": "Vision-language models (VLMs) trained on internet-scale data achieve\nremarkable zero-shot detection performance on common objects like car, truck,\nand pedestrian. However, state-of-the-art models still struggle to generalize\nto out-of-distribution classes, tasks and imaging modalities not typically\nfound in their pre-training. Rather than simply re-training VLMs on more visual\ndata, we argue that one should align VLMs to new concepts with annotation\ninstructions containing a few visual examples and rich textual descriptions. To\nthis end, we introduce Roboflow100-VL, a large-scale collection of 100\nmulti-modal object detection datasets with diverse concepts not commonly found\nin VLM pre-training. We evaluate state-of-the-art models on our benchmark in\nzero-shot, few-shot, semi-supervised, and fully-supervised settings, allowing\nfor comparison across data regimes. Notably, we find that VLMs like\nGroundingDINO and Qwen2.5-VL achieve less than 2% zero-shot accuracy on\nchallenging medical imaging datasets within Roboflow100-VL, demonstrating the\nneed for few-shot concept alignment. Our code and dataset are available at\nhttps://github.com/roboflow/rf100-vl/ and\nhttps://universe.roboflow.com/rf100-vl/"}
{"id": "2505.20630", "pdf": "https://arxiv.org/pdf/2505.20630.pdf", "abs": "https://arxiv.org/abs/2505.20630", "title": "SV-TrustEval-C: Evaluating Structure and Semantic Reasoning in Large Language Models for Source Code Vulnerability Analysis", "authors": ["Yansong Li", "Paula Branco", "Alexander M. Hoole", "Manish Marwah", "Hari Manassery Koduvely", "Guy-Vincent Jourdan", "Stephan Jou"], "categories": ["cs.SE", "cs.CL"], "comment": null, "summary": "As Large Language Models (LLMs) evolve in understanding and generating code,\naccurately evaluating their reliability in analyzing source code\nvulnerabilities becomes increasingly vital. While studies have examined LLM\ncapabilities in tasks like vulnerability detection and repair, they often\noverlook the importance of both structure and semantic reasoning crucial for\ntrustworthy vulnerability analysis. To address this gap, we introduce\nSV-TrustEval-C, a benchmark designed to evaluate LLMs' abilities for\nvulnerability analysis of code written in the C programming language through\ntwo key dimensions: structure reasoning - assessing how models identify\nrelationships between code elements under varying data and control flow\ncomplexities; and semantic reasoning - examining their logical consistency in\nscenarios where code is structurally and semantically perturbed. Our results\nshow that current LLMs are far from satisfactory in understanding complex code\nrelationships and that their vulnerability analyses rely more on pattern\nmatching than on robust logical reasoning. These findings underscore the\neffectiveness of the SV-TrustEval-C benchmark and highlight critical areas for\nenhancing the reasoning capabilities and trustworthiness of LLMs in real-world\nvulnerability analysis tasks. Our initial benchmark dataset is publicly\navailable."}
{"id": "2505.20663", "pdf": "https://arxiv.org/pdf/2505.20663.pdf", "abs": "https://arxiv.org/abs/2505.20663", "title": "TeroSeek: An AI-Powered Knowledge Base and Retrieval Generation Platform for Terpenoid Research", "authors": ["Xu Kang", "Siqi Jiang", "Kangwei Xu", "Jiahao Li", "Ruibo Wu"], "categories": ["cs.IR", "cs.AI", "cs.CL", "H.3; I.2"], "comment": "18 pages, 4 figures", "summary": "Terpenoids are a crucial class of natural products that have been studied for\nover 150 years, but their interdisciplinary nature (spanning chemistry,\npharmacology, and biology) complicates knowledge integration. To address this,\nthe authors developed TeroSeek, a curated knowledge base (KB) built from two\ndecades of terpenoid literature, coupled with an AI-powered question-answering\nchatbot and web service. Leveraging a retrieval-augmented generation (RAG)\nframework, TeroSeek provides structured, high-quality information and\noutperforms general-purpose large language models (LLMs) in terpenoid-related\nqueries. It serves as a domain-specific expert tool for multidisciplinary\nresearch and is publicly available at http://teroseek.qmclab.com."}
{"id": "2505.20692", "pdf": "https://arxiv.org/pdf/2505.20692.pdf", "abs": "https://arxiv.org/abs/2505.20692", "title": "Can we Debias Social Stereotypes in AI-Generated Images? Examining Text-to-Image Outputs and User Perceptions", "authors": ["Saharsh Barve", "Andy Mao", "Jiayue Melissa Shi", "Prerna Juneja", "Koustuv Saha"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances in generative AI have enabled visual content creation through\ntext-to-image (T2I) generation. However, despite their creative potential, T2I\nmodels often replicate and amplify societal stereotypes -- particularly those\nrelated to gender, race, and culture -- raising important ethical concerns.\nThis paper proposes a theory-driven bias detection rubric and a Social\nStereotype Index (SSI) to systematically evaluate social biases in T2I outputs.\nWe audited three major T2I model outputs -- DALL-E-3, Midjourney-6.1, and\nStability AI Core -- using 100 queries across three categories -- geocultural,\noccupational, and adjectival. Our analysis reveals that initial outputs are\nprone to include stereotypical visual cues, including gendered professions,\ncultural markers, and western beauty norms. To address this, we adopted our\nrubric to conduct targeted prompt refinement using LLMs, which significantly\nreduced bias -- SSI dropped by 61% for geocultural, 69% for occupational, and\n51% for adjectival queries. We complemented our quantitative analysis through a\nuser study examining perceptions, awareness, and preferences around\nAI-generated biased imagery. Our findings reveal a key tension -- although\nprompt refinement can mitigate stereotypes, it can limit contextual alignment.\nInterestingly, users often perceived stereotypical images to be more aligned\nwith their expectations. We discuss the need to balance ethical debiasing with\ncontextual relevance and call for T2I systems that support global diversity and\ninclusivity while not compromising the reflection of real-world social\ncomplexity."}
{"id": "2505.20715", "pdf": "https://arxiv.org/pdf/2505.20715.pdf", "abs": "https://arxiv.org/abs/2505.20715", "title": "MUSEG: Reinforcing Video Temporal Understanding via Timestamp-Aware Multi-Segment Grounding", "authors": ["Fuwen Luo", "Shengfeng Lou", "Chi Chen", "Ziyue Wang", "Chenliang Li", "Weizhou Shen", "Jiyue Guo", "Peng Li", "Ming Yan", "Ji Zhang", "Fei Huang", "Yang Liu"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Video temporal understanding is crucial for multimodal large language models\n(MLLMs) to reason over events in videos. Despite recent advances in general\nvideo understanding, current MLLMs still struggle with fine-grained temporal\nreasoning. While reinforcement learning (RL) has been explored to address this\nissue recently, existing RL approaches remain limited in effectiveness. In this\nwork, we propose MUSEG, a novel RL-based method that enhances temporal\nunderstanding by introducing timestamp-aware multi-segment grounding. MUSEG\nenables MLLMs to align queries with multiple relevant video segments, promoting\nmore comprehensive temporal reasoning. To facilitate effective learning, we\ndesign a customized RL training recipe with phased rewards that progressively\nguides the model toward temporally grounded reasoning. Extensive experiments on\ntemporal grounding and time-sensitive video QA tasks demonstrate that MUSEG\nsignificantly outperforms existing methods and generalizes well across diverse\ntemporal understanding scenarios. View our project at\nhttps://github.com/THUNLP-MT/MUSEG."}
{"id": "2505.20730", "pdf": "https://arxiv.org/pdf/2505.20730.pdf", "abs": "https://arxiv.org/abs/2505.20730", "title": "What LLMs Miss in Recommendations: Bridging the Gap with Retrieval-Augmented Collaborative Signals", "authors": ["Shahrooz Pouryousef"], "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "User-item interactions contain rich collaborative signals that form the\nbackbone of many successful recommender systems. While recent work has explored\nthe use of large language models (LLMs) for recommendation, it remains unclear\nwhether LLMs can effectively reason over this type of collaborative\ninformation. In this paper, we conduct a systematic comparison between LLMs and\nclassical matrix factorization (MF) models to assess LLMs' ability to leverage\nuser-item interaction data. We further introduce a simple retrieval-augmented\ngeneration (RAG) method that enhances LLMs by grounding their predictions in\nstructured interaction data. Our experiments reveal that current LLMs often\nfall short in capturing collaborative patterns inherent to MF models, but that\nour RAG-based approach substantially improves recommendation\nquality-highlighting a promising direction for future LLM-based recommenders."}
{"id": "2505.20854", "pdf": "https://arxiv.org/pdf/2505.20854.pdf", "abs": "https://arxiv.org/abs/2505.20854", "title": "An LLM-as-Judge Metric for Bridging the Gap with Human Evaluation in SE Tasks", "authors": ["Xin Zhou", "Kisub Kim", "Ting Zhang", "Martin Weyssow", "Luis F. Gomes", "Guang Yang", "David Lo"], "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": "20 pages", "summary": "Large Language Models (LLMs) and other automated techniques have been\nincreasingly used to support software developers by generating software\nartifacts such as code snippets, patches, and comments. However, accurately\nassessing the correctness of these generated artifacts remains a significant\nchallenge. On one hand, human evaluation provides high accuracy but is\nlabor-intensive and lacks scalability. On the other hand, other existing\nautomatic evaluation metrics are scalable and require minimal human effort, but\nthey often fail to accurately reflect the actual correctness of generated\nsoftware artifacts.\n  In this paper, we present SWE-Judge, the first evaluation metric for\nLLM-as-Ensemble-Judge specifically designed to accurately assess the\ncorrectness of generated software artifacts. SWE-Judge first defines five\ndistinct evaluation strategies, each implemented as an independent judge. A\ndynamic team selection mechanism then identifies the most appropriate subset of\njudges to produce a final correctness score through ensembling. We evaluate\nSWE-Judge across a diverse set of software engineering (SE) benchmarks,\nincluding CoNaLa, Card2Code, HumanEval-X, APPS, APR-Assess, and Summary-Assess.\nThese benchmarks span three SE tasks: code generation, automated program\nrepair, and code summarization. Experimental results demonstrate that SWE-Judge\nconsistently achieves a higher correlation with human judgments, with\nimprovements ranging from 5.9% to 183.8% over existing automatic metrics.\nFurthermore, SWE-Judge reaches agreement levels with human annotators that are\ncomparable to inter-annotator agreement in code generation and program repair\ntasks. These findings underscore SWE-Judge's potential as a scalable and\nreliable alternative to human evaluation."}
{"id": "2505.20896", "pdf": "https://arxiv.org/pdf/2505.20896.pdf", "abs": "https://arxiv.org/abs/2505.20896", "title": "How Do Transformers Learn Variable Binding in Symbolic Programs?", "authors": ["Yiwei Wu", "Atticus Geiger", "Raphaël Millière"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "16 pages, 10 figures, 1 table. To appear in the Proceedings of the\n  42nd International Conference on Machine Learning (ICML 2025)", "summary": "Variable binding -- the ability to associate variables with values -- is\nfundamental to symbolic computation and cognition. Although classical\narchitectures typically implement variable binding via addressable memory, it\nis not well understood how modern neural networks lacking built-in binding\noperations may acquire this capacity. We investigate this by training a\nTransformer to dereference queried variables in symbolic programs where\nvariables are assigned either numerical constants or other variables. Each\nprogram requires following chains of variable assignments up to four steps deep\nto find the queried value, and also contains irrelevant chains of assignments\nacting as distractors. Our analysis reveals a developmental trajectory with\nthree distinct phases during training: (1) random prediction of numerical\nconstants, (2) a shallow heuristic prioritizing early variable assignments, and\n(3) the emergence of a systematic mechanism for dereferencing assignment\nchains. Using causal interventions, we find that the model learns to exploit\nthe residual stream as an addressable memory space, with specialized attention\nheads routing information across token positions. This mechanism allows the\nmodel to dynamically track variable bindings across layers, resulting in\naccurate dereferencing. Our results show how Transformer models can learn to\nimplement systematic variable binding without explicit architectural support,\nbridging connectionist and symbolic approaches."}
{"id": "2505.20897", "pdf": "https://arxiv.org/pdf/2505.20897.pdf", "abs": "https://arxiv.org/abs/2505.20897", "title": "Cross from Left to Right Brain: Adaptive Text Dreamer for Vision-and-Language Navigation", "authors": ["Pingrui Zhang", "Yifei Su", "Pengyuan Wu", "Dong An", "Li Zhang", "Zhigang Wang", "Dong Wang", "Yan Ding", "Bin Zhao", "Xuelong Li"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "comment": null, "summary": "Vision-and-Language Navigation (VLN) requires the agent to navigate by\nfollowing natural instructions under partial observability, making it difficult\nto align perception with language. Recent methods mitigate this by imagining\nfuture scenes, yet they rely on vision-based synthesis, leading to high\ncomputational cost and redundant details. To this end, we propose to adaptively\nimagine key environmental semantics via \\textit{language} form, enabling a more\nreliable and efficient strategy. Specifically, we introduce a novel Adaptive\nText Dreamer (ATD), a dual-branch self-guided imagination policy built upon a\nlarge language model (LLM). ATD is designed with a human-like left-right brain\narchitecture, where the left brain focuses on logical integration, and the\nright brain is responsible for imaginative prediction of future scenes. To\nachieve this, we fine-tune only the Q-former within both brains to efficiently\nactivate domain-specific knowledge in the LLM, enabling dynamic updates of\nlogical reasoning and imagination during navigation. Furthermore, we introduce\na cross-interaction mechanism to regularize the imagined outputs and inject\nthem into a navigation expert module, allowing ATD to jointly exploit both the\nreasoning capacity of the LLM and the expertise of the navigation model. We\nconduct extensive experiments on the R2R benchmark, where ATD achieves\nstate-of-the-art performance with fewer parameters. The code is\n\\href{https://github.com/zhangpingrui/Adaptive-Text-Dreamer}{here}."}
{"id": "2505.20981", "pdf": "https://arxiv.org/pdf/2505.20981.pdf", "abs": "https://arxiv.org/abs/2505.20981", "title": "RefAV: Towards Planning-Centric Scenario Mining", "authors": ["Cainan Davidson", "Deva Ramanan", "Neehar Peri"], "categories": ["cs.CV", "cs.CL", "cs.RO"], "comment": null, "summary": "Autonomous Vehicles (AVs) collect and pseudo-label terabytes of multi-modal\ndata localized to HD maps during normal fleet testing. However, identifying\ninteresting and safety-critical scenarios from uncurated driving logs remains a\nsignificant challenge. Traditional scenario mining techniques are error-prone\nand prohibitively time-consuming, often relying on hand-crafted structured\nqueries. In this work, we revisit spatio-temporal scenario mining through the\nlens of recent vision-language models (VLMs) to detect whether a described\nscenario occurs in a driving log and, if so, precisely localize it in both time\nand space. To address this problem, we introduce RefAV, a large-scale dataset\nof 10,000 diverse natural language queries that describe complex multi-agent\ninteractions relevant to motion planning derived from 1000 driving logs in the\nArgoverse 2 Sensor dataset. We evaluate several referential multi-object\ntrackers and present an empirical analysis of our baselines. Notably, we find\nthat naively repurposing off-the-shelf VLMs yields poor performance, suggesting\nthat scenario mining presents unique challenges. Our code and dataset are\navailable at https://github.com/CainanD/RefAV/ and\nhttps://argoverse.github.io/user-guide/tasks/scenario_mining.html"}
{"id": "2505.21024", "pdf": "https://arxiv.org/pdf/2505.21024.pdf", "abs": "https://arxiv.org/abs/2505.21024", "title": "Pause Tokens Strictly Increase the Expressivity of Constant-Depth Transformers", "authors": ["Charles London", "Varun Kanade"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Pause tokens, simple filler symbols such as \"...\", consistently improve\nTransformer performance on both language and mathematical tasks, yet their\ntheoretical effect remains unexplained. We provide the first formal separation\nresult, proving that adding pause tokens to constant-depth, logarithmic-width\nTransformers strictly increases their computational expressivity. With\nbounded-precision activations, Transformers without pause tokens compute only a\nstrict subset of $\\mathsf{AC}^0$ functions, while adding a polynomial number of\npause tokens allows them to express the entire class. For logarithmic-precision\nTransformers, we show that adding pause tokens achieves expressivity equivalent\nto $\\mathsf{TC}^0$, matching known upper bounds. Empirically, we demonstrate\nthat two-layer causally masked Transformers can learn parity when supplied with\npause tokens, a function that they appear unable to learn without them. Our\nresults provide a rigorous theoretical explanation for prior empirical\nfindings, clarify how pause tokens interact with width, depth, and numeric\nprecision, and position them as a distinct mechanism, complementary to\nchain-of-thought prompting, for enhancing Transformer reasoning."}
{"id": "2505.21091", "pdf": "https://arxiv.org/pdf/2505.21091.pdf", "abs": "https://arxiv.org/abs/2505.21091", "title": "Position is Power: System Prompts as a Mechanism of Bias in Large Language Models (LLMs)", "authors": ["Anna Neumann", "Elisabeth Kirsten", "Muhammad Bilal Zafar", "Jatinder Singh"], "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": "Forthcoming in Proceedings of ACM FAccT 2025", "summary": "System prompts in Large Language Models (LLMs) are predefined directives that\nguide model behaviour, taking precedence over user inputs in text processing\nand generation. LLM deployers increasingly use them to ensure consistent\nresponses across contexts. While model providers set a foundation of system\nprompts, deployers and third-party developers can append additional prompts\nwithout visibility into others' additions, while this layered implementation\nremains entirely hidden from end-users. As system prompts become more complex,\nthey can directly or indirectly introduce unaccounted for side effects. This\nlack of transparency raises fundamental questions about how the position of\ninformation in different directives shapes model outputs. As such, this work\nexamines how the placement of information affects model behaviour. To this end,\nwe compare how models process demographic information in system versus user\nprompts across six commercially available LLMs and 50 demographic groups. Our\nanalysis reveals significant biases, manifesting in differences in user\nrepresentation and decision-making scenarios. Since these variations stem from\ninaccessible and opaque system-level configurations, they risk\nrepresentational, allocative and potential other biases and downstream harms\nbeyond the user's ability to detect or correct. Our findings draw attention to\nthese critical issues, which have the potential to perpetuate harms if left\nunexamined. Further, we argue that system prompt analysis must be incorporated\ninto AI auditing processes, particularly as customisable system prompts become\nincreasingly prevalent in commercial AI deployments."}
{"id": "2505.21116", "pdf": "https://arxiv.org/pdf/2505.21116.pdf", "abs": "https://arxiv.org/abs/2505.21116", "title": "Creativity in LLM-based Multi-Agent Systems: A Survey", "authors": ["Yi-Cheng Lin", "Kang-Chieh Chen", "Zhe-Yan Li", "Tzu-Heng Wu", "Tzu-Hsuan Wu", "Kuan-Yu Chen", "Hung-yi Lee", "Yun-Nung Chen"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "23 pages", "summary": "Large language model (LLM)-driven multi-agent systems (MAS) are transforming\nhow humans and AIs collaboratively generate ideas and artifacts. While existing\nsurveys provide comprehensive overviews of MAS infrastructures, they largely\noverlook the dimension of \\emph{creativity}, including how novel outputs are\ngenerated and evaluated, how creativity informs agent personas, and how\ncreative workflows are coordinated. This is the first survey dedicated to\ncreativity in MAS. We focus on text and image generation tasks, and present:\n(1) a taxonomy of agent proactivity and persona design; (2) an overview of\ngeneration techniques, including divergent exploration, iterative refinement,\nand collaborative synthesis, as well as relevant datasets and evaluation\nmetrics; and (3) a discussion of key challenges, such as inconsistent\nevaluation standards, insufficient bias mitigation, coordination conflicts, and\nthe lack of unified benchmarks. This survey offers a structured framework and\nroadmap for advancing the development, evaluation, and standardization of\ncreative MAS."}
{"id": "2505.21162", "pdf": "https://arxiv.org/pdf/2505.21162.pdf", "abs": "https://arxiv.org/abs/2505.21162", "title": "Leveraging GANs for citation intent classification and its impact on citation network analysis", "authors": ["Davi A. Bezerra", "Filipi N. Silva", "Diego R. Amancio"], "categories": ["cs.DL", "cs.CL", "cs.SI"], "comment": null, "summary": "Citations play a fundamental role in the scientific ecosystem, serving as a\nfoundation for tracking the flow of knowledge, acknowledging prior work, and\nassessing scholarly influence. In scientometrics, they are also central to the\nconstruction of quantitative indicators. Not all citations, however, serve the\nsame function: some provide background, others introduce methods, or compare\nresults. Therefore, understanding citation intent allows for a more nuanced\ninterpretation of scientific impact. In this paper, we adopted a GAN-based\nmethod to classify citation intents. Our results revealed that the proposed\nmethod achieves competitive classification performance, closely matching\nstate-of-the-art results with substantially fewer parameters. This demonstrates\nthe effectiveness and efficiency of leveraging GAN architectures combined with\ncontextual embeddings in intent classification task. We also investigated\nwhether filtering citation intents affects the centrality of papers in citation\nnetworks. Analyzing the network constructed from the unArXiv dataset, we found\nthat paper rankings can be significantly influenced by citation intent. All\nfour centrality metrics examined- degree, PageRank, closeness, and betweenness\n- were sensitive to the filtering of citation types. The betweenness centrality\ndisplayed the greatest sensitivity, showing substantial changes in ranking when\nspecific citation intents were removed."}
{"id": "2505.21184", "pdf": "https://arxiv.org/pdf/2505.21184.pdf", "abs": "https://arxiv.org/abs/2505.21184", "title": "PoisonSwarm: Universal Harmful Information Synthesis via Model Crowdsourcing", "authors": ["Yu Yan", "Sheng Sun", "Zhifei Zheng", "Ziji Hao", "Teli Liu", "Min Liu"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "To construct responsible and secure AI applications, harmful information data\nis widely utilized for adversarial testing and the development of safeguards.\nExisting studies mainly leverage Large Language Models (LLMs) to synthesize\ndata to obtain high-quality task datasets at scale, thereby avoiding costly\nhuman annotation. However, limited by the safety alignment mechanisms of LLMs,\nthe synthesis of harmful data still faces challenges in generation reliability\nand content diversity. In this study, we propose a novel harmful information\nsynthesis framework, PoisonSwarm, which applies the model crowdsourcing\nstrategy to generate diverse harmful data while maintaining a high success\nrate. Specifically, we generate abundant benign data as the based templates in\na counterfactual manner. Subsequently, we decompose each based template into\nmultiple semantic units and perform unit-by-unit toxification and final\nrefinement through dynamic model switching, thus ensuring the success of\nsynthesis. Experimental results demonstrate that PoisonSwarm achieves\nstate-of-the-art performance in synthesizing different categories of harmful\ndata with high scalability and diversity."}
{"id": "2505.21230", "pdf": "https://arxiv.org/pdf/2505.21230.pdf", "abs": "https://arxiv.org/abs/2505.21230", "title": "PSRB: A Comprehensive Benchmark for Evaluating Persian ASR Systems", "authors": ["Nima Sedghiyeh", "Sara Sadeghi", "Reza Khodadadi", "Farzin Kashani", "Omid Aghdaei", "Somayeh Rahimi", "Mohammad Sadegh Safari"], "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": "25 pages, 7 figures", "summary": "Although Automatic Speech Recognition (ASR) systems have become an integral\npart of modern technology, their evaluation remains challenging, particularly\nfor low-resource languages such as Persian. This paper introduces Persian\nSpeech Recognition Benchmark(PSRB), a comprehensive benchmark designed to\naddress this gap by incorporating diverse linguistic and acoustic conditions.\nWe evaluate ten ASR systems, including state-of-the-art commercial and\nopen-source models, to examine performance variations and inherent biases.\nAdditionally, we conduct an in-depth analysis of Persian ASR transcriptions,\nidentifying key error types and proposing a novel metric that weights\nsubstitution errors. This metric enhances evaluation robustness by reducing the\nimpact of minor and partial errors, thereby improving the precision of\nperformance assessment. Our findings indicate that while ASR models generally\nperform well on standard Persian, they struggle with regional accents,\nchildren's speech, and specific linguistic challenges. These results highlight\nthe necessity of fine-tuning and incorporating diverse, representative training\ndatasets to mitigate biases and enhance overall ASR performance. PSRB provides\na valuable resource for advancing ASR research in Persian and serves as a\nframework for developing benchmarks in other low-resource languages. A subset\nof the PSRB dataset is publicly available at\nhttps://huggingface.co/datasets/PartAI/PSRB."}
{"id": "2505.21277", "pdf": "https://arxiv.org/pdf/2505.21277.pdf", "abs": "https://arxiv.org/abs/2505.21277", "title": "Breaking the Ceiling: Exploring the Potential of Jailbreak Attacks through Expanding Strategy Space", "authors": ["Yao Huang", "Yitong Sun", "Shouwei Ruan", "Yichi Zhang", "Yinpeng Dong", "Xingxing Wei"], "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": "19 pages, 20 figures, accepted by ACL 2025, Findings", "summary": "Large Language Models (LLMs), despite advanced general capabilities, still\nsuffer from numerous safety risks, especially jailbreak attacks that bypass\nsafety protocols. Understanding these vulnerabilities through black-box\njailbreak attacks, which better reflect real-world scenarios, offers critical\ninsights into model robustness. While existing methods have shown improvements\nthrough various prompt engineering techniques, their success remains limited\nagainst safety-aligned models, overlooking a more fundamental problem: the\neffectiveness is inherently bounded by the predefined strategy spaces. However,\nexpanding this space presents significant challenges in both systematically\ncapturing essential attack patterns and efficiently navigating the increased\ncomplexity. To better explore the potential of expanding the strategy space, we\naddress these challenges through a novel framework that decomposes jailbreak\nstrategies into essential components based on the Elaboration Likelihood Model\n(ELM) theory and develops genetic-based optimization with intention evaluation\nmechanisms. To be striking, our experiments reveal unprecedented jailbreak\ncapabilities by expanding the strategy space: we achieve over 90% success rate\non Claude-3.5 where prior methods completely fail, while demonstrating strong\ncross-model transferability and surpassing specialized safeguard models in\nevaluation accuracy. The code is open-sourced at:\nhttps://github.com/Aries-iai/CL-GSO."}
{"id": "2505.21304", "pdf": "https://arxiv.org/pdf/2505.21304.pdf", "abs": "https://arxiv.org/abs/2505.21304", "title": "Optimizing fMRI Data Acquisition for Decoding Natural Speech with Limited Participants", "authors": ["Louis Jalouzot", "Alexis Thual", "Yair Lakretz", "Christophe Pallier", "Bertrand Thirion"], "categories": ["q-bio.NC", "cs.CL", "cs.LG"], "comment": "13 pages, 6 figures", "summary": "We investigate optimal strategies for decoding perceived natural speech from\nfMRI data acquired from a limited number of participants. Leveraging Lebel et\nal. (2023)'s dataset of 8 participants, we first demonstrate the effectiveness\nof training deep neural networks to predict LLM-derived text representations\nfrom fMRI activity. Then, in this data regime, we observe that multi-subject\ntraining does not improve decoding accuracy compared to single-subject\napproach. Furthermore, training on similar or different stimuli across subjects\nhas a negligible effect on decoding accuracy. Finally, we find that our\ndecoders better model syntactic than semantic features, and that stories\ncontaining sentences with complex syntax or rich semantic content are more\nchallenging to decode. While our results demonstrate the benefits of having\nextensive data per participant (deep phenotyping), they suggest that leveraging\nmulti-subject for natural speech decoding likely requires deeper phenotyping or\na substantially larger cohort."}
{"id": "2505.21329", "pdf": "https://arxiv.org/pdf/2505.21329.pdf", "abs": "https://arxiv.org/abs/2505.21329", "title": "Something's Fishy In The Data Lake: A Critical Re-evaluation of Table Union Search Benchmarks", "authors": ["Allaa Boutaleb", "Bernd Amann", "Hubert Naacke", "Rafael Angarita"], "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.DB", "cs.LG"], "comment": "Accepted @ ACL 2025's Table Representation Learning Workshop (TRL)", "summary": "Recent table representation learning and data discovery methods tackle table\nunion search (TUS) within data lakes, which involves identifying tables that\ncan be unioned with a given query table to enrich its content. These methods\nare commonly evaluated using benchmarks that aim to assess semantic\nunderstanding in real-world TUS tasks. However, our analysis of prominent TUS\nbenchmarks reveals several limitations that allow simple baselines to perform\nsurprisingly well, often outperforming more sophisticated approaches. This\nsuggests that current benchmark scores are heavily influenced by\ndataset-specific characteristics and fail to effectively isolate the gains from\nsemantic understanding. To address this, we propose essential criteria for\nfuture benchmarks to enable a more realistic and reliable evaluation of\nprogress in semantic table union search."}
{"id": "2505.21344", "pdf": "https://arxiv.org/pdf/2505.21344.pdf", "abs": "https://arxiv.org/abs/2505.21344", "title": "The Multilingual Divide and Its Impact on Global AI Safety", "authors": ["Aidan Peppin", "Julia Kreutzer", "Alice Schoenauer Sebag", "Kelly Marchisio", "Beyza Ermis", "John Dang", "Samuel Cahyawijaya", "Shivalika Singh", "Seraphina Goldfarb-Tarrant", "Viraat Aryabumi", "Aakanksha", "Wei-Yin Ko", "Ahmet Üstün", "Matthias Gallé", "Marzieh Fadaee", "Sara Hooker"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Despite advances in large language model capabilities in recent years, a\nlarge gap remains in their capabilities and safety performance for many\nlanguages beyond a relatively small handful of globally dominant languages.\nThis paper provides researchers, policymakers and governance experts with an\noverview of key challenges to bridging the \"language gap\" in AI and minimizing\nsafety risks across languages. We provide an analysis of why the language gap\nin AI exists and grows, and how it creates disparities in global AI safety. We\nidentify barriers to address these challenges, and recommend how those working\nin policy and governance can help address safety concerns associated with the\nlanguage gap by supporting multilingual dataset creation, transparency, and\nresearch."}
{"id": "2505.21465", "pdf": "https://arxiv.org/pdf/2505.21465.pdf", "abs": "https://arxiv.org/abs/2505.21465", "title": "ID-Align: RoPE-Conscious Position Remapping for Dynamic High-Resolution Adaptation in Vision-Language Models", "authors": ["Bozhou Li", "Wentao Zhang"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Currently, a prevalent approach for enhancing Vision-Language Models (VLMs)\nperformance is to encode both the high-resolution version and the thumbnail of\nan image simultaneously. While effective, this method generates a large number\nof image tokens. When combined with the widely used Rotary Position Embedding\n(RoPE), its long-term decay property hinders the interaction between\nhigh-resolution tokens and thumbnail tokens, as well as between text and image.\nTo address these issues, we propose ID-Align, which alleviates these problems\nby reordering position IDs. In this method, high-resolution tokens inherit IDs\nfrom their corresponding thumbnail token while constraining the overexpansion\nof positional indices. Our experiments conducted within the LLaVA-Next\nframework demonstrate that ID-Align achieves significant improvements,\nincluding a 6.09% enhancement on MMBench's relation reasoning tasks and notable\ngains across multiple benchmarks. Our code is available at the following link:\nhttps://github.com/zooblastlbz/ID-Align."}
{"id": "2505.21472", "pdf": "https://arxiv.org/pdf/2505.21472.pdf", "abs": "https://arxiv.org/abs/2505.21472", "title": "Mitigating Hallucination in Large Vision-Language Models via Adaptive Attention Calibration", "authors": ["Mehrdad Fazli", "Bowen Wei", "Ziwei Zhu"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Large vision-language models (LVLMs) achieve impressive performance on\nmultimodal tasks but often suffer from hallucination, and confidently describe\nobjects or attributes not present in the image. Current inference-time\ninterventions, while training-free, struggle to maintain accuracy in open-ended\nand long-form generation scenarios. We introduce the Confidence-Aware Attention\nCalibration (CAAC) framework to address this challenge by targeting two key\nbiases: spatial perception bias, which distributes attention disproportionately\nacross image tokens, and modality bias, which shifts focus from visual to\ntextual inputs over time. CAAC employs a two-step approach: Visual-Token\nCalibration (VTC) to balance attention across visual tokens, and Adaptive\nAttention Re-Scaling (AAR) to reinforce visual grounding based on the model's\nconfidence. This confidence-driven adjustment ensures consistent visual\nalignment during generation. Experiments on CHAIR, AMBER, and POPE benchmarks\ndemonstrate that CAAC outperforms baselines, particularly in long-form\ngenerations, effectively reducing hallucination."}
{"id": "2505.21487", "pdf": "https://arxiv.org/pdf/2505.21487.pdf", "abs": "https://arxiv.org/abs/2505.21487", "title": "Hardware-Efficient Attention for Fast Decoding", "authors": ["Ted Zadouri", "Hubert Strauss", "Tri Dao"], "categories": ["cs.LG", "cs.CL"], "comment": "37 pages, 15 figures, 45 tables", "summary": "LLM decoding is bottlenecked for large batches and long contexts by loading\nthe key-value (KV) cache from high-bandwidth memory, which inflates per-token\nlatency, while the sequential nature of decoding limits parallelism. We analyze\nthe interplay among arithmetic intensity, parallelization, and model quality\nand question whether current architectures fully exploit modern hardware. This\nwork redesigns attention to perform more computation per byte loaded from\nmemory to maximize hardware efficiency without trading off parallel\nscalability. We first propose Grouped-Tied Attention (GTA), a simple variant\nthat combines and reuses key and value states, reducing memory transfers\nwithout compromising model quality. We then introduce Grouped Latent Attention\n(GLA), a parallel-friendly latent attention paired with low-level optimizations\nfor fast decoding while maintaining high model quality. Experiments show that\nGTA matches Grouped-Query Attention (GQA) quality while using roughly half the\nKV cache and that GLA matches Multi-head Latent Attention (MLA) and is easier\nto shard. Our optimized GLA kernel is up to 2$\\times$ faster than FlashMLA, for\nexample, in a speculative decoding setting when the query length exceeds one.\nFurthermore, by fetching a smaller KV cache per device, GLA reduces end-to-end\nlatency and increases throughput in online serving benchmarks by up to\n2$\\times$."}
{"id": "2505.21493", "pdf": "https://arxiv.org/pdf/2505.21493.pdf", "abs": "https://arxiv.org/abs/2505.21493", "title": "Reinforcing General Reasoning without Verifiers", "authors": ["Xiangxin Zhou", "Zichen Liu", "Anya Sims", "Haonan Wang", "Tianyu Pang", "Chongxuan Li", "Liang Wang", "Min Lin", "Chao Du"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "The recent paradigm shift towards training large language models (LLMs) using\nDeepSeek-R1-Zero-style reinforcement learning (RL) on verifiable rewards has\nled to impressive advancements in code and mathematical reasoning. However,\nthis methodology is limited to tasks where rule-based answer verification is\npossible and does not naturally extend to real-world domains such as chemistry,\nhealthcare, engineering, law, biology, business, and economics. Current\npractical workarounds use an additional LLM as a model-based verifier; however,\nthis introduces issues such as reliance on a strong verifier LLM,\nsusceptibility to reward hacking, and the practical burden of maintaining the\nverifier model in memory during training. To address this and extend\nDeepSeek-R1-Zero-style training to general reasoning domains, we propose a\nverifier-free method (VeriFree) that bypasses answer verification and instead\nuses RL to directly maximize the probability of generating the reference\nanswer. We compare VeriFree with verifier-based methods and demonstrate that,\nin addition to its significant practical benefits and reduced compute\nrequirements, VeriFree matches and even surpasses verifier-based methods on\nextensive evaluations across MMLU-Pro, GPQA, SuperGPQA, and math-related\nbenchmarks. Moreover, we provide insights into this method from multiple\nperspectives: as an elegant integration of training both the policy and\nimplicit verifier in a unified model, and as a variational optimization\napproach. Code is available at https://github.com/sail-sg/VeriFree."}
{"id": "2505.21497", "pdf": "https://arxiv.org/pdf/2505.21497.pdf", "abs": "https://arxiv.org/abs/2505.21497", "title": "Paper2Poster: Towards Multimodal Poster Automation from Scientific Papers", "authors": ["Wei Pang", "Kevin Qinghong Lin", "Xiangru Jian", "Xi He", "Philip Torr"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MA"], "comment": "Project Page: https://github.com/Paper2Poster/Paper2Poster", "summary": "Academic poster generation is a crucial yet challenging task in scientific\ncommunication, requiring the compression of long-context interleaved documents\ninto a single, visually coherent page. To address this challenge, we introduce\nthe first benchmark and metric suite for poster generation, which pairs recent\nconference papers with author-designed posters and evaluates outputs on\n(i)Visual Quality-semantic alignment with human posters, (ii)Textual\nCoherence-language fluency, (iii)Holistic Assessment-six fine-grained aesthetic\nand informational criteria scored by a VLM-as-judge, and notably\n(iv)PaperQuiz-the poster's ability to convey core paper content as measured by\nVLMs answering generated quizzes. Building on this benchmark, we propose\nPosterAgent, a top-down, visual-in-the-loop multi-agent pipeline: the (a)Parser\ndistills the paper into a structured asset library; the (b)Planner aligns\ntext-visual pairs into a binary-tree layout that preserves reading order and\nspatial balance; and the (c)Painter-Commenter loop refines each panel by\nexecuting rendering code and using VLM feedback to eliminate overflow and\nensure alignment. In our comprehensive evaluation, we find that GPT-4o\noutputs-though visually appealing at first glance-often exhibit noisy text and\npoor PaperQuiz scores, and we find that reader engagement is the primary\naesthetic bottleneck, as human-designed posters rely largely on visual\nsemantics to convey meaning. Our fully open-source variants (e.g. based on the\nQwen-2.5 series) outperform existing 4o-driven multi-agent systems across\nnearly all metrics, while using 87% fewer tokens. It transforms a 22-page paper\ninto a finalized yet editable .pptx poster - all for just $0.005. These\nfindings chart clear directions for the next generation of fully automated\nposter-generation models. The code and datasets are available at\nhttps://github.com/Paper2Poster/Paper2Poster."}
{"id": "2505.21500", "pdf": "https://arxiv.org/pdf/2505.21500.pdf", "abs": "https://arxiv.org/abs/2505.21500", "title": "ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models", "authors": ["Dingming Li", "Hongxing Li", "Zixuan Wang", "Yuchen Yan", "Hang Zhang", "Siqi Chen", "Guiyang Hou", "Shengpei Jiang", "Wenqi Zhang", "Yongliang Shen", "Weiming Lu", "Yueting Zhuang"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Project: https://zju-real.github.io/ViewSpatial-Page/", "summary": "Vision-language models (VLMs) have demonstrated remarkable capabilities in\nunderstanding and reasoning about visual content, but significant challenges\npersist in tasks requiring cross-viewpoint understanding and spatial reasoning.\nWe identify a critical limitation: current VLMs excel primarily at egocentric\nspatial reasoning (from the camera's perspective) but fail to generalize to\nallocentric viewpoints when required to adopt another entity's spatial frame of\nreference. We introduce ViewSpatial-Bench, the first comprehensive benchmark\ndesigned specifically for multi-viewpoint spatial localization recognition\nevaluation across five distinct task types, supported by an automated 3D\nannotation pipeline that generates precise directional labels. Comprehensive\nevaluation of diverse VLMs on ViewSpatial-Bench reveals a significant\nperformance disparity: models demonstrate reasonable performance on\ncamera-perspective tasks but exhibit reduced accuracy when reasoning from a\nhuman viewpoint. By fine-tuning VLMs on our multi-perspective spatial dataset,\nwe achieve an overall performance improvement of 46.24% across tasks,\nhighlighting the efficacy of our approach. Our work establishes a crucial\nbenchmark for spatial intelligence in embodied AI systems and provides\nempirical evidence that modeling 3D spatial relationships enhances VLMs'\ncorresponding spatial comprehension capabilities."}
{"id": "2304.12244", "pdf": "https://arxiv.org/pdf/2304.12244.pdf", "abs": "https://arxiv.org/abs/2304.12244", "title": "WizardLM: Empowering large pre-trained language models to follow complex instructions", "authors": ["Can Xu", "Qingfeng Sun", "Kai Zheng", "Xiubo Geng", "Pu Zhao", "Jiazhan Feng", "Chongyang Tao", "Qingwei Lin", "Daxin Jiang"], "categories": ["cs.CL", "cs.AI"], "comment": "large language model, instruction fine-tune", "summary": "Training large language models (LLMs) with open-domain instruction following\ndata brings colossal success. However, manually creating such instruction data\nis very time-consuming and labor-intensive. Moreover, humans may struggle to\nproduce high-complexity instructions. In this paper, we show an avenue for\ncreating large amounts of instruction data with varying levels of complexity\nusing LLM instead of humans. Starting with an initial set of instructions, we\nuse our proposed Evol-Instruct to rewrite them step by step into more complex\ninstructions. Then, we mix all generated instruction data to fine-tune LLaMA.\nWe call the resulting model WizardLM. Human evaluations on a\ncomplexity-balanced test bed and Vicuna's testset show that instructions from\nEvol-Instruct are superior to human-created ones. By analyzing the human\nevaluation results of the high complexity part, we demonstrate that outputs\nfrom our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4\nautomatic evaluation, WizardLM achieves more than 90\\% capacity of ChatGPT on\n17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some\naspects, our findings suggest that fine-tuning with AI-evolved instructions is\na promising direction for enhancing LLMs. Our code and data are public at\nhttps://github.com/nlpxucan/WizardLM"}
{"id": "2306.08568", "pdf": "https://arxiv.org/pdf/2306.08568.pdf", "abs": "https://arxiv.org/abs/2306.08568", "title": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct", "authors": ["Ziyang Luo", "Can Xu", "Pu Zhao", "Qingfeng Sun", "Xiubo Geng", "Wenxiang Hu", "Chongyang Tao", "Jing Ma", "Qingwei Lin", "Daxin Jiang"], "categories": ["cs.CL", "cs.AI"], "comment": "Large Language model, Code Generation, Code LLMs.This paper has been\n  accepted to ICLR 2024. Please cite the ICLR version", "summary": "Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated\nexceptional performance in code-related tasks. However, most existing models\nare solely pre-trained on extensive raw code data without instruction\nfine-tuning. In this paper, we introduce WizardCoder, which empowers Code LLMs\nwith complex instruction fine-tuning, by adapting the Evol-Instruct method to\nthe domain of code. Through comprehensive experiments on four prominent code\ngeneration benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we\nunveil the exceptional capabilities of our model. It surpasses all other\nopen-source Code LLMs by a substantial margin. Moreover, our model even\noutperforms the largest closed LLMs, Anthropic's Claude and Google's Bard, on\nHumanEval and HumanEval+. Our code, model weights, and data are public at\nhttps://github.com/nlpxucan/WizardLM"}
{"id": "2401.16332", "pdf": "https://arxiv.org/pdf/2401.16332.pdf", "abs": "https://arxiv.org/abs/2401.16332", "title": "Tradeoffs Between Alignment and Helpfulness in Language Models with Steering Methods", "authors": ["Yotam Wolf", "Noam Wies", "Dorin Shteyman", "Binyamin Rothberg", "Yoav Levine", "Amnon Shashua"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Language model alignment has become an important component of AI safety,\nallowing safe interactions between humans and language models, by enhancing\ndesired behaviors and inhibiting undesired ones. It is often done by tuning the\nmodel or inserting preset aligning prompts. Recently, representation\nengineering, a method which alters the model's behavior via changing its\nrepresentations post-training, was shown to be effective in aligning LLMs (Zou\net al., 2023a). Representation engineering yields gains in alignment oriented\ntasks such as resistance to adversarial attacks and reduction of social biases,\nbut was also shown to cause a decrease in the ability of the model to perform\nbasic tasks. In this paper we study the tradeoff between the increase in\nalignment and decrease in helpfulness of the model. We propose a theoretical\nframework which provides bounds for these two quantities, and demonstrate their\nrelevance empirically. First, we find that under the conditions of our\nframework, alignment can be guaranteed with representation engineering, and at\nthe same time that helpfulness is harmed in the process. Second, we show that\nhelpfulness is harmed quadratically with the norm of the representation\nengineering vector, while the alignment increases linearly with it, indicating\na regime in which it is efficient to use representation engineering. We\nvalidate our findings empirically, and chart the boundaries to the usefulness\nof representation engineering for alignment."}
{"id": "2402.14558", "pdf": "https://arxiv.org/pdf/2402.14558.pdf", "abs": "https://arxiv.org/abs/2402.14558", "title": "LLMs with Industrial Lens: Deciphering the Challenges and Prospects -- A Survey", "authors": ["Ashok Urlana", "Charaka Vinayak Kumar", "Ajeet Kumar Singh", "Bala Mallikarjunarao Garlapati", "Srinivasa Rao Chalamala", "Rahul Mishra"], "categories": ["cs.CL"], "comment": "25 pages, 7 figures", "summary": "Large language models (LLMs) have become the secret ingredient driving\nnumerous industrial applications, showcasing their remarkable versatility\nacross a diverse spectrum of tasks. From natural language processing and\nsentiment analysis to content generation and personalized recommendations,\ntheir unparalleled adaptability has facilitated widespread adoption across\nindustries. This transformative shift driven by LLMs underscores the need to\nexplore the underlying associated challenges and avenues for enhancement in\ntheir utilization. In this paper, our objective is to unravel and evaluate the\nobstacles and opportunities inherent in leveraging LLMs within an industrial\ncontext. To this end, we conduct a survey involving a group of industry\npractitioners, develop four research questions derived from the insights\ngathered, and examine 68 industry papers to address these questions and derive\nmeaningful conclusions. We maintain the Github repository with the most recent\npapers in the field."}
{"id": "2403.04963", "pdf": "https://arxiv.org/pdf/2403.04963.pdf", "abs": "https://arxiv.org/abs/2403.04963", "title": "An In-depth Evaluation of Large Language Models in Sentence Simplification with Error-based Human Assessment", "authors": ["Xuanxin Wu", "Yuki Arase"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACM Transactions on Intelligent Systems and Technology,\n  to appear", "summary": "Recent studies have used both automatic metrics and human evaluations to\nassess the simplification abilities of LLMs. However, the suitability of\nexisting evaluation methodologies for LLMs remains in question. First, the\nsuitability of current automatic metrics on LLMs' simplification evaluation is\nstill uncertain. Second, current human evaluation approaches in sentence\nsimplification often fall into two extremes: they are either too superficial,\nfailing to offer a clear understanding of the models' performance, or overly\ndetailed, making the annotation process complex and prone to inconsistency,\nwhich in turn affects the evaluation's reliability. To address these problems,\nthis study provides in-depth insights into LLMs' performance while ensuring the\nreliability of the evaluation. We design an error-based human annotation\nframework to assess the LLMs' simplification capabilities. We select both\nclosed-source and open-source LLMs, including GPT-4, Qwen2.5-72B, and\nLlama-3.2-3B. We believe that these models offer a representative selection\nacross large, medium, and small sizes of LLMs. Results show that GPT-4\ngenerally generates fewer erroneous simplification outputs compared to the\ncurrent state-of-the-art. However, LLMs have their limitations, as seen in\nGPT-4's struggles with lexical paraphrasing. Results show that LLMs generally\ngenerate fewer erroneous simplification outputs compared to the previous\nstate-of-the-art. However, LLMs have their limitations, as seen in GPT-4's and\nQwen2.5-72B's struggle with lexical paraphrasing. Furthermore, we conduct\nmeta-evaluations on widely used automatic metrics using our human annotations.\nWe find that these metrics lack sufficient sensitivity to assess the overall\nhigh-quality simplifications, particularly those generated by high-performance\nLLMs."}
{"id": "2403.05518", "pdf": "https://arxiv.org/pdf/2403.05518.pdf", "abs": "https://arxiv.org/abs/2403.05518", "title": "Bias-Augmented Consistency Training Reduces Biased Reasoning in Chain-of-Thought", "authors": ["James Chua", "Edward Rees", "Hunar Batra", "Samuel R. Bowman", "Julian Michael", "Ethan Perez", "Miles Turpin"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Chain-of-thought prompting (CoT) has the potential to improve the\nexplainability of language model reasoning. But CoT can also systematically\nmisrepresent the factors influencing models' behavior -- for example,\nrationalizing answers in line with a user's opinion.\n  We first create a new dataset of 9 different biases that affect GPT-3.5-Turbo\nand Llama-8b models. These consist of spurious-few-shot patterns, post hoc\nrationalization, and sycophantic settings. Models switch to the answer implied\nby the bias, without mentioning the effect of the bias in the CoT.\n  To mitigate this biased reasoning problem, we introduce bias-augmented\nconsistency training (BCT), an unsupervised fine-tuning scheme that trains\nmodels to give consistent reasoning across prompts with and without biasing\nfeatures. We construct a suite testing nine forms of biased reasoning on seven\nquestion-answering tasks, and find that applying BCT to GPT-3.5-Turbo with one\nbias reduces the rate of biased reasoning by 86\\% on held-out tasks. Moreover,\nthis model generalizes to other forms of bias, reducing biased reasoning on\nheld-out biases by an average of 37\\%. As BCT generalizes to held-out biases\nand does not require gold labels, this method may hold promise for reducing\nbiased reasoning from as-of-yet unknown biases and on tasks where ground truth\nreasoning is unavailable."}
{"id": "2403.10056", "pdf": "https://arxiv.org/pdf/2403.10056.pdf", "abs": "https://arxiv.org/abs/2403.10056", "title": "Don't Half-listen: Capturing Key-part Information in Continual Instruction Tuning", "authors": ["Yongquan He", "Wenyuan Zhang", "Xuancheng Huang", "Peng Zhang", "Lingxun Meng", "Xiang Zhou", "Ke Zeng", "Xunliang Cai"], "categories": ["cs.CL", "cs.AI"], "comment": "20 pages, 6 figures", "summary": "Instruction tuning for large language models (LLMs) can drive them to produce\nresults consistent with human goals in specific downstream tasks. However, the\nprocess of continual instruction tuning (CIT) for LLMs may bring about the\ncatastrophic forgetting (CF) problem, where previously learned abilities are\ndegraded. Recent methods try to alleviate the CF problem by modifying models or\nreplaying data, which may only remember the surface-level pattern of\ninstructions and get confused on held-out tasks. In this paper, we propose a\nnovel continual instruction tuning method based on Key-part Information Gain\n(KPIG). Our method computes the information gain on masked parts to dynamically\nreplay data and refine the training objective, which enables LLMs to capture\ntask-aware information relevant to the correct response and alleviate\noverfitting to general descriptions in instructions. In addition, we propose\ntwo metrics, P-score and V-score, to measure the generalization and\ninstruction-following abilities of LLMs. Experiments demonstrate our method\nachieves superior performance on both seen and held-out tasks."}
{"id": "2405.20947", "pdf": "https://arxiv.org/pdf/2405.20947.pdf", "abs": "https://arxiv.org/abs/2405.20947", "title": "OR-Bench: An Over-Refusal Benchmark for Large Language Models", "authors": ["Justin Cui", "Wei-Lin Chiang", "Ion Stoica", "Cho-Jui Hsieh"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ICML 2025, we thank everyone for their valuable\n  suggestions and feedback!", "summary": "Large Language Models (LLMs) require careful safety alignment to prevent\nmalicious outputs. While significant research focuses on mitigating harmful\ncontent generation, the enhanced safety often come with the side effect of\nover-refusal, where LLMs may reject innocuous prompts and become less helpful.\nAlthough the issue of over-refusal has been empirically observed, a systematic\nmeasurement is challenging due to the difficulty of crafting prompts that can\nelicit the over-refusal behaviors of LLMs. This study proposes a novel method\nfor automatically generating large-scale over-refusal datasets. Leveraging this\ntechnique, we introduce OR-Bench, the first large-scale over-refusal benchmark.\nOR-Bench comprises 80,000 over-refusal prompts across 10 common rejection\ncategories, a subset of around 1,000 hard prompts that are challenging even for\nstate-of-the-art LLMs, and an additional 600 toxic prompts to prevent\nindiscriminate responses. We then conduct a comprehensive study to measure the\nover-refusal of 32 popular LLMs across 8 model families. Our datasets are\npublicly available at https://huggingface.co/bench-llms and our codebase is\nopen-sourced at https://github.com/justincui03/or-bench. We hope this benchmark\ncan help the community develop better safety aligned models."}
{"id": "2406.00984", "pdf": "https://arxiv.org/pdf/2406.00984.pdf", "abs": "https://arxiv.org/abs/2406.00984", "title": "Predicting drug-gene relations via analogy tasks with word embeddings", "authors": ["Hiroaki Yamagiwa", "Ryoma Hashimoto", "Kiwamu Arakane", "Ken Murakami", "Shou Soeda", "Momose Oyama", "Yihua Zhu", "Mariko Okada", "Hidetoshi Shimodaira"], "categories": ["cs.CL"], "comment": null, "summary": "Natural language processing (NLP) is utilized in a wide range of fields,\nwhere words in text are typically transformed into feature vectors called\nembeddings. BioConceptVec is a specific example of embeddings tailored for\nbiology, trained on approximately 30 million PubMed abstracts using models such\nas skip-gram. Generally, word embeddings are known to solve analogy tasks\nthrough simple vector arithmetic. For example, subtracting the vector for man\nfrom that of king and then adding the vector for woman yields a point that lies\ncloser to queen in the embedding space. In this study, we demonstrate that\nBioConceptVec embeddings, along with our own embeddings trained on PubMed\nabstracts, contain information about drug-gene relations and can predict target\ngenes from a given drug through analogy computations. We also show that\ncategorizing drugs and genes using biological pathways improves performance.\nFurthermore, we illustrate that vectors derived from known relations in the\npast can predict unknown future relations in datasets divided by year. Despite\nthe simplicity of implementing analogy tasks as vector additions, our approach\ndemonstrated performance comparable to that of large language models such as\nGPT-4 in predicting drug-gene relations."}
{"id": "2406.03749", "pdf": "https://arxiv.org/pdf/2406.03749.pdf", "abs": "https://arxiv.org/abs/2406.03749", "title": "NAP^2: A Benchmark for Naturalness and Privacy-Preserving Text Rewriting by Learning from Human", "authors": ["Shuo Huang", "William MacLean", "Xiaoxi Kang", "Qiongkai Xu", "Zhuang Li", "Xingliang Yuan", "Gholamreza Haffari", "Lizhen Qu"], "categories": ["cs.CL"], "comment": null, "summary": "The widespread use of cloud-based Large Language Models (LLMs) has heightened\nconcerns over user privacy, as sensitive information may be inadvertently\nexposed during interactions with these services. To protect privacy before\nsending sensitive data to those models, we suggest sanitizing sensitive text\nusing two common strategies used by humans: i) deleting sensitive expressions,\nand ii) obscuring sensitive details by abstracting them. To explore the issues\nand develop a tool for text rewriting, we curate the first corpus, coined\nNAP^2, through both crowdsourcing and the use of large language models (LLMs).\nCompared to the prior works on anonymization, the human-inspired approaches\nresult in more natural rewrites and offer an improved balance between privacy\nprotection and data utility, as demonstrated by our extensive experiments.\nResearchers interested in accessing the dataset are encouraged to contact the\nfirst or corresponding author via email."}
{"id": "2406.14230", "pdf": "https://arxiv.org/pdf/2406.14230.pdf", "abs": "https://arxiv.org/abs/2406.14230", "title": "Raising the Bar: Investigating the Values of Large Language Models via Generative Evolving Testing", "authors": ["Han Jiang", "Xiaoyuan Yi", "Zhihua Wei", "Ziang Xiao", "Shu Wang", "Xing Xie"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "ICML 2025", "summary": "Warning: Contains harmful model outputs. Despite significant advancements,\nthe propensity of Large Language Models (LLMs) to generate harmful and\nunethical content poses critical challenges. Measuring value alignment of LLMs\nbecomes crucial for their regulation and responsible deployment. Although\nnumerous benchmarks have been constructed to assess social bias, toxicity, and\nethical issues in LLMs, those static benchmarks suffer from evaluation\nchronoeffect, in which, as models rapidly evolve, existing benchmarks may leak\ninto training data or become saturated, overestimating ever-developing LLMs. To\ntackle this problem, we propose GETA, a novel generative evolving testing\napproach based on adaptive testing methods in measurement theory. Unlike\ntraditional adaptive testing methods that rely on a static test item pool, GETA\nprobes the underlying moral boundaries of LLMs by dynamically generating test\nitems tailored to model capability. GETA co-evolves with LLMs by learning a\njoint distribution of item difficulty and model value conformity, thus\neffectively addressing evaluation chronoeffect. We evaluated various popular\nLLMs with GETA and demonstrated that 1) GETA can dynamically create\ndifficulty-tailored test items and 2) GETA's evaluation results are more\nconsistent with models' performance on unseen OOD and i.i.d. items, laying the\ngroundwork for future evaluation paradigms."}
{"id": "2407.00996", "pdf": "https://arxiv.org/pdf/2407.00996.pdf", "abs": "https://arxiv.org/abs/2407.00996", "title": "Can Small Language Models Learn, Unlearn, and Retain Noise Patterns?", "authors": ["Nicy Scaria", "Silvester John Joseph Kennedy", "Deepak Subramani"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "With the growing need for efficient language models in resource-constrained\nenvironments, Small Language Models (SLMs) have emerged as compact and\npractical alternatives to Large Language Models (LLMs). While studies have\nexplored noise handling in LLMs, little is known about how SLMs handle noise, a\ncritical factor for their reliable real-world deployment. This study\ninvestigates the ability of SLMs with parameters between 1 and 3 billion to\nlearn, retain, and subsequently eliminate different types of noise (word flip,\ncharacter flip, transliteration, irrelevant content, and contradictory\ninformation). Four pretrained SLMs (Olmo 1B, Qwen1.5 1.8B, Gemma1.1 2B, and\nPhi2 2.7B) were instruction-tuned on noise-free data and tested with in-context\nexamples to assess noise learning. Subsequently, noise patterns were introduced\nin instruction tuning to assess their adaptability. The results revealed\ndifferences in how models handle noise, with smaller models like Olmo quickly\nadapting to noise patterns. Phi2's carefully curated, structured, and\nhigh-quality pretraining data enabled resistance to character level,\ntransliteration, and counterfactual noise, while Gemma adapted successfully to\ntransliteration noise through its multilingual pretraining. Subsequent clean\ndata training effectively mitigated noise effects. These findings provide\npractical strategies for developing robust SLMs for real-world applications."}
{"id": "2407.03181", "pdf": "https://arxiv.org/pdf/2407.03181.pdf", "abs": "https://arxiv.org/abs/2407.03181", "title": "Fine-Tuning on Diverse Reasoning Chains Drives Within-Inference CoT Refinement in LLMs", "authors": ["Haritz Puerto", "Tilek Chubakov", "Xiaodan Zhu", "Harish Tayyar Madabushi", "Iryna Gurevych"], "categories": ["cs.CL"], "comment": "ACL 2025 Main", "summary": "Requiring a large language model (LLM) to generate intermediary reasoning\nsteps, known as Chain of Thought (CoT), has been shown to be an effective way\nof boosting performance. Previous approaches have focused on generating\nmultiple independent CoTs, combining them through ensembling or other post-hoc\nstrategies to enhance reasoning. In this work, we introduce a novel approach\nwhere LLMs are fine-tuned to generate a sequence of Diverse Chains of Thought\n(DCoT) within a single inference step, which is fundamentally different from\nprior work that primarily operate on parallel CoT generations. DCoT allows LLMs\nto gain the ability to perform within-inference refinement of reasoning chains\nwithout requiring external feedback. Through a rigorous set of experiments\nspanning a wide range of tasks that require various reasoning types, we show\nthat fine-tuning on DCoT improves performance over the CoT baseline across\nmodel families and scales (1.3B to 70B). These improvements are particularly\nimpactful for tasks with a large result state space, such as those involving\nnumeric answers. Our work is also significant because both quantitative\nanalyses and manual evaluations reveal the observed gains stem from the models'\nability to refine an initial reasoning chain by generating a second, improved\nchain within the same inference step, demonstrating previously elusive\nself-improvement. Our code and data are publicly available at\nhttps://github.com/UKPLab/acl2025-diverse-cot."}
{"id": "2407.08551", "pdf": "https://arxiv.org/pdf/2407.08551.pdf", "abs": "https://arxiv.org/abs/2407.08551", "title": "Autoregressive Speech Synthesis without Vector Quantization", "authors": ["Lingwei Meng", "Long Zhou", "Shujie Liu", "Sanyuan Chen", "Bing Han", "Shujie Hu", "Yanqing Liu", "Jinyu Li", "Sheng Zhao", "Xixin Wu", "Helen Meng", "Furu Wei"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to ACL 2025 Main", "summary": "We present MELLE, a novel continuous-valued token based language modeling\napproach for text-to-speech synthesis (TTS). MELLE autoregressively generates\ncontinuous mel-spectrogram frames directly from text condition, bypassing the\nneed for vector quantization, which is typically designed for audio compression\nand sacrifices fidelity compared to continuous representations. Specifically,\n(i) instead of cross-entropy loss, we apply regression loss with a proposed\nspectrogram flux loss function to model the probability distribution of the\ncontinuous-valued tokens; (ii) we have incorporated variational inference into\nMELLE to facilitate sampling mechanisms, thereby enhancing the output diversity\nand model robustness. Experiments demonstrate that, compared to the two-stage\ncodec language model VALL-E and its variants, the single-stage MELLE mitigates\nrobustness issues by avoiding the inherent flaws of sampling vector-quantized\ncodes, achieves superior performance across multiple metrics, and, most\nimportantly, offers a more streamlined paradigm. The demos of our work are\nprovided at https://aka.ms/melle."}
{"id": "2407.21054", "pdf": "https://arxiv.org/pdf/2407.21054.pdf", "abs": "https://arxiv.org/abs/2407.21054", "title": "Sentiment Reasoning for Healthcare", "authors": ["Khai-Nguyen Nguyen", "Khai Le-Duc", "Bach Phan Tat", "Duy Le", "Long Vo-Dang", "Truong-Son Hy"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": "ACL 2025 (Oral)", "summary": "Transparency in AI healthcare decision-making is crucial. By incorporating\nrationales to explain reason for each predicted label, users could understand\nLarge Language Models (LLMs)'s reasoning to make better decision. In this work,\nwe introduce a new task - Sentiment Reasoning - for both speech and text\nmodalities, and our proposed multimodal multitask framework and the world's\nlargest multimodal sentiment analysis dataset. Sentiment Reasoning is an\nauxiliary task in sentiment analysis where the model predicts both the\nsentiment label and generates the rationale behind it based on the input\ntranscript. Our study conducted on both human transcripts and Automatic Speech\nRecognition (ASR) transcripts shows that Sentiment Reasoning helps improve\nmodel transparency by providing rationale for model prediction with quality\nsemantically comparable to humans while also improving model's classification\nperformance (+2% increase in both accuracy and macro-F1) via\nrationale-augmented fine-tuning. Also, no significant difference in the\nsemantic quality of generated rationales between human and ASR transcripts. All\ncode, data (five languages - Vietnamese, English, Chinese, German, and French)\nand models are published online:\nhttps://github.com/leduckhai/Sentiment-Reasoning"}
{"id": "2408.13533", "pdf": "https://arxiv.org/pdf/2408.13533.pdf", "abs": "https://arxiv.org/abs/2408.13533", "title": "Pandora's Box or Aladdin's Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models", "authors": ["Jinyang Wu", "Shuai Zhang", "Feihu Che", "Mingkuan Feng", "Pengpeng Shao", "Jianhua Tao"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as a crucial method for\naddressing hallucinations in large language models (LLMs). While recent\nresearch has extended RAG models to complex noisy scenarios, these explorations\noften confine themselves to limited noise types and presuppose that noise is\ninherently detrimental to LLMs, potentially deviating from real-world retrieval\nenvironments and restricting practical applicability. In this paper, we define\nseven distinct noise types from a linguistic perspective and establish a Noise\nRAG Benchmark (NoiserBench), a comprehensive evaluation framework encompassing\nmultiple datasets and reasoning tasks. Through empirical evaluation of eight\nrepresentative LLMs with diverse architectures and scales, we reveal that these\nnoises can be further categorized into two practical groups: noise that is\nbeneficial to LLMs (aka beneficial noise) and noise that is harmful to LLMs\n(aka harmful noise). While harmful noise generally impairs performance,\nbeneficial noise may enhance several aspects of model capabilities and overall\nperformance. Our analysis offers insights for developing more robust, adaptable\nRAG solutions and mitigating hallucinations across diverse retrieval scenarios."}
{"id": "2409.04183", "pdf": "https://arxiv.org/pdf/2409.04183.pdf", "abs": "https://arxiv.org/abs/2409.04183", "title": "GALLa: Graph Aligned Large Language Models for Improved Source Code Understanding", "authors": ["Ziyin Zhang", "Hang Yu", "Shijie Li", "Peng Di", "Jianguo Li", "Rui Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 camera-ready", "summary": "Programming languages possess rich semantic information - such as data flow -\nthat is represented by graphs and not available from the surface form of source\ncode. Recent code language models have scaled to billions of parameters, but\nmodel source code solely as text tokens while ignoring any other structural\ninformation. Conversely, models that do encode structural information of code\nmake modifications to the Transformer architecture, limiting their scale and\ncompatibility with pretrained LLMs. In this work, we take the best of both\nworlds with GALLa - Graph Aligned Large Language Models. GALLa utilizes graph\nneural networks and cross-modal alignment technologies to inject the structural\ninformation of code into LLMs as an auxiliary task during finetuning. This\nframework is both model-agnostic and task-agnostic, as it can be applied to any\ncode LLM for any code downstream task, and requires the structural graph data\nonly at training time from a corpus unrelated to the finetuning data, while\nincurring no cost at inference time over the baseline LLM. Experiments on five\ncode tasks with seven different baseline LLMs ranging in size from 350M to 14B\nvalidate the effectiveness of GALLa, demonstrating consistent improvement over\nthe baseline, even for powerful models such as LLaMA3 and Qwen2.5-Coder."}
{"id": "2409.14469", "pdf": "https://arxiv.org/pdf/2409.14469.pdf", "abs": "https://arxiv.org/abs/2409.14469", "title": "Rethinking Semantic Parsing for Large Language Models: Enhancing LLM Performance with Semantic Hints", "authors": ["Kaikai An", "Shuzheng Si", "Helan Hu", "Haozhe Zhao", "Yuchi Wang", "Qingyan Guo", "Baobao Chang"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025", "summary": "Semantic Parsing aims to capture the meaning of a sentence and convert it\ninto a logical, structured form. Previous studies show that semantic parsing\nenhances the performance of smaller models (e.g., BERT) on downstream tasks.\nHowever, it remains unclear whether the improvements extend similarly to LLMs.\nIn this paper, our empirical findings reveal that, unlike smaller models,\ndirectly adding semantic parsing results into LLMs reduces their performance.\nTo overcome this, we propose SENSE, a novel prompting approach that embeds\nsemantic hints within the prompt. Experiments show that SENSE consistently\nimproves LLMs' performance across various tasks, highlighting the potential of\nintegrating semantic information to improve LLM capabilities."}
{"id": "2410.01651", "pdf": "https://arxiv.org/pdf/2410.01651.pdf", "abs": "https://arxiv.org/abs/2410.01651", "title": "Efficient Length-Generalizable Attention via Causal Retrieval for Long-Context Language Modeling", "authors": ["Xiang Hu", "Zhihao Teng", "Jun Zhao", "Wei Wu", "Kewei Tu"], "categories": ["cs.CL", "cs.AI"], "comment": "accepted to ICML 2025", "summary": "Despite the success of Transformers, handling long contexts remains\nchallenging due to the limited length generalization and quadratic complexity\nof self-attention. Thus Transformers often require post-training with a larger\nattention window, significantly increasing computational and memory costs. In\nthis paper, we propose a novel attention mechanism based on dynamic context,\nGrouped Cross Attention (GCA), which can generalize to 1000 times the\npre-training context length while maintaining the ability to access distant\ninformation with a constant attention window size. For a given input sequence,\nwe split it into chunks and use each chunk to retrieve top-k relevant past\nchunks for subsequent text generation. Specifically, unlike most previous works\nthat use an off-the-shelf retriever, our key innovation allows the retriever to\nlearn how to retrieve past chunks that better minimize the auto-regressive loss\nof subsequent tokens in an end-to-end manner. Such a mechanism accommodates\nretrieved chunks with a fixed-size attention window to achieve long-range\ninformation access, significantly reducing computational and memory costs\nduring training and inference. Experiments show that GCA-based models achieve\nnear-perfect accuracy in passkey retrieval for 16M context lengths, which is\n1000 times the training length."}
{"id": "2410.06638", "pdf": "https://arxiv.org/pdf/2410.06638.pdf", "abs": "https://arxiv.org/abs/2410.06638", "title": "Subtle Errors in Reasoning: Preference Learning via Error-injected Self-editing", "authors": ["Kaishuai Xu", "Tiezheng Yu", "Wenjun Hou", "Yi Cheng", "Chak Tou Leong", "Liangyou Li", "Xin Jiang", "Lifeng Shang", "Qun Liu", "Wenjie Li"], "categories": ["cs.CL", "cs.AI"], "comment": "accepted as ACL 2025 main", "summary": "Large Language Models (LLMs) have exhibited strong mathematical reasoning\nprowess, tackling tasks ranging from basic arithmetic to advanced\ncompetition-level problems. However, frequently occurring subtle yet critical\nerrors, such as miscalculations or incorrect substitutions, limit the LLMs'\nfull potential. Existing studies to improve mathematical ability typically\ninvolve applying preference learning to step-wise solution pairs. Although\nthese methods leverage samples of varying granularity to mitigate reasoning\nerrors, they overlook critical subtle errors. In this work, we propose a novel\npreference learning framework called eRror-Injected Self-Editing (RISE), which\ninjects predefined subtle errors into pivotal tokens in reasoning or\ncomputation steps to construct hard pairs for error mitigation. In detail, RISE\nuses the LLM itself to edit a small number of tokens in the solution, injecting\ndesigned subtle errors. Then, pairs composed of self-edited solutions and their\ncorresponding correct ones, along with pairs of correct and incorrect solutions\nobtained through sampling, are used together for subtle error-aware DPO\ntraining. Compared with other preference learning methods, RISE further refines\nthe training objective without requiring fine-grained sampling or preference\nannotation. Extensive experiments validate the effectiveness of RISE, with\npreference learning on Qwen2-7B-Instruct yielding notable improvements of 3.0%\non GSM8K and 7.9% on MATH with only 4.5K training samples. Moreover, the effect\nof error mitigation extends from mathematical reasoning to logical reasoning\nand code generation."}
{"id": "2410.09829", "pdf": "https://arxiv.org/pdf/2410.09829.pdf", "abs": "https://arxiv.org/abs/2410.09829", "title": "Conversational Code Generation: a Case Study of Designing a Dialogue System for Generating Driving Scenarios for Testing Autonomous Vehicles", "authors": ["Rimvydas Rubavicius", "Antonio Valerio Miceli-Barone", "Alex Lascarides", "Subramanian Ramamoorthy"], "categories": ["cs.CL", "cs.IR", "cs.RO"], "comment": "12 pages, 5 figures, 2 tables", "summary": "Cyber-physical systems like autonomous vehicles are tested in simulation\nbefore deployment, using domain-specific programs for scenario specification.\nTo aid the testing of autonomous vehicles in simulation, we design a natural\nlanguage interface, using an instruction-following large language model, to\nassist a non-coding domain expert in synthesising the desired scenarios and\nvehicle behaviours. We show that using it to convert utterances to the symbolic\nprogram is feasible, despite the very small training dataset. Human experiments\nshow that dialogue is critical to successful simulation generation, leading to\na 4.5 times higher success rate than a generation without engaging in extended\nconversation."}
{"id": "2410.12458", "pdf": "https://arxiv.org/pdf/2410.12458.pdf", "abs": "https://arxiv.org/abs/2410.12458", "title": "The Best of Both Worlds: Bridging Quality and Diversity in Data Selection with Bipartite Graph", "authors": ["Minghao Wu", "Thuy-Trang Vu", "Lizhen Qu", "Gholamreza Haffari"], "categories": ["cs.CL"], "comment": "Accepted by ICML 2025", "summary": "The performance of large language models (LLMs) is strongly influenced by the\nquality and diversity of data used during supervised fine-tuning (SFT).\nHowever, current data selection methods often prioritize one aspect over the\nother, resulting in suboptimal training outcomes. To address this, we formulate\ndata selection as a set cover problem and present GraphFilter, a novel approach\nthat balances both quality and diversity in data selection. GraphFilter models\nthe dataset as a bipartite graph connecting sentences to their constituent\nn-grams, then employs a priority function that combines quality and diversity\nmetrics multiplicatively. GraphFilter iteratively selects sentences with the\nhighest priority, removes covered n-grams from the bipartite graph, and\nrecomputes priorities to reflect the changing data landscape. We validate\nGraphFilter using three model backbones across six widely-used benchmarks,\ndemonstrating that it outperforms nine existing baselines in both model\nperformance and computational efficiency. Further analysis shows that our\ndesign choices lead to more effective subset selection, underscores the value\nof instruction diversity, and provides insights into how quality and diversity\ninteract with different subset sizes."}
{"id": "2410.13206", "pdf": "https://arxiv.org/pdf/2410.13206.pdf", "abs": "https://arxiv.org/abs/2410.13206", "title": "BQA: Body Language Question Answering Dataset for Video Large Language Models", "authors": ["Shintaro Ozaki", "Kazuki Hayashi", "Miyu Oba", "Yusuke Sakai", "Hidetaka Kamigaito", "Taro Watanabe"], "categories": ["cs.CL"], "comment": "Accepted to ACL2025 (Main)", "summary": "A large part of human communication relies on nonverbal cues such as facial\nexpressions, eye contact, and body language. Unlike language or sign language,\nsuch nonverbal communication lacks formal rules, requiring complex reasoning\nbased on commonsense understanding. Enabling current Video Large Language\nModels (VideoLLMs) to accurately interpret body language is a crucial\nchallenge, as human unconscious actions can easily cause the model to\nmisinterpret their intent. To address this, we propose a dataset, BQA, a body\nlanguage question answering dataset, to validate whether the model can\ncorrectly interpret emotions from short clips of body language comprising 26\nemotion labels of videos of body language. We evaluated various VideoLLMs on\nBQA and revealed that understanding body language is challenging, and our\nanalyses of the wrong answers by VideoLLMs show that certain VideoLLMs made\nsignificantly biased answers depending on the age group and ethnicity of the\nindividuals in the video. The dataset is available."}
{"id": "2410.13776", "pdf": "https://arxiv.org/pdf/2410.13776.pdf", "abs": "https://arxiv.org/abs/2410.13776", "title": "Aggregation Artifacts in Subjective Tasks Collapse Large Language Models' Posteriors", "authors": ["Georgios Chochlakis", "Alexandros Potamianos", "Kristina Lerman", "Shrikanth Narayanan"], "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 12 figures, 3 tables", "summary": "In-context Learning (ICL) has become the primary method for performing\nnatural language tasks with Large Language Models (LLMs). The knowledge\nacquired during pre-training is crucial for this few-shot capability, providing\nthe model with task priors. However, recent studies have shown that ICL\npredominantly relies on retrieving task priors rather than \"learning\" to\nperform tasks. This limitation is particularly evident in complex subjective\ndomains such as emotion and morality, where priors significantly influence\nposterior predictions. In this work, we examine whether this is the result of\nthe aggregation used in corresponding datasets, where trying to combine\nlow-agreement, disparate annotations might lead to annotation artifacts that\ncreate detrimental noise in the prompt. Moreover, we evaluate the posterior\nbias towards certain annotators by grounding our study in appropriate,\nquantitative measures of LLM priors. Our results indicate that aggregation is a\nconfounding factor in the modeling of subjective tasks, and advocate focusing\non modeling individuals instead. However, aggregation does not explain the\nentire gap between ICL and the state of the art, meaning other factors in such\ntasks also account for the observed phenomena. Finally, by rigorously studying\nannotator-level labels, we find that it is possible for minority annotators to\nboth better align with LLMs and have their perspectives further amplified."}
{"id": "2410.14641", "pdf": "https://arxiv.org/pdf/2410.14641.pdf", "abs": "https://arxiv.org/abs/2410.14641", "title": "Distance between Relevant Information Pieces Causes Bias in Long-Context LLMs", "authors": ["Runchu Tian", "Yanghao Li", "Yuepeng Fu", "Siyang Deng", "Qinyu Luo", "Cheng Qian", "Shuo Wang", "Xin Cong", "Zhong Zhang", "Yesai Wu", "Yankai Lin", "Huadong Wang", "Xiaojiang Liu"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings", "summary": "Positional bias in large language models (LLMs) hinders their ability to\neffectively process long inputs. A prominent example is the \"lost in the\nmiddle\" phenomenon, where LLMs struggle to utilize relevant information\nsituated in the middle of the input. While prior research primarily focuses on\nsingle pieces of relevant information, real-world applications often involve\nmultiple relevant information pieces. To bridge this gap, we present\nLongPiBench, a benchmark designed to assess positional bias involving multiple\npieces of relevant information. Thorough experiments are conducted with five\ncommercial and six open-source models. These experiments reveal that while most\ncurrent models are robust against the \"lost in the middle\" issue, there exist\nsignificant biases related to the spacing of relevant information pieces. These\nfindings highlight the importance of evaluating and reducing positional biases\nto advance LLM's capabilities."}
{"id": "2410.18693", "pdf": "https://arxiv.org/pdf/2410.18693.pdf", "abs": "https://arxiv.org/abs/2410.18693", "title": "Unleashing LLM Reasoning Capability via Scalable Question Synthesis from Scratch", "authors": ["Yuyang Ding", "Xinyu Shi", "Xiaobo Liang", "Juntao Li", "Zhaopeng Tu", "Qiaoming Zhu", "Min Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025", "summary": "Improving the mathematical reasoning capabilities of Large Language Models\n(LLMs) is critical for advancing artificial intelligence. However, access to\nextensive, diverse, and high-quality reasoning datasets remains a significant\nchallenge, particularly for the open-source community. In this paper, we\npropose ScaleQuest, a novel, scalable, and cost-effective data synthesis method\nthat enables the generation of large-scale mathematical reasoning datasets\nusing lightweight 7B-scale models. ScaleQuest introduces a two-stage\nquestion-tuning process comprising Question Fine-Tuning (QFT) and Question\nPreference Optimization (QPO) to unlock the question generation capabilities of\nproblem-solving models. By generating diverse questions from scratch -- without\nrelying on powerful proprietary models or seed data -- we produce a dataset of\n1 million problem-solution pairs. Our experiments demonstrate that models\ntrained on our data outperform existing open-source datasets in both in-domain\nand out-of-domain evaluations. Furthermore, our approach shows continued\nperformance improvement as the volume of training data increases, highlighting\nits potential for ongoing data scaling. The extensive improvements observed in\ncode reasoning tasks demonstrate the generalization capabilities of our\nproposed method. Our work provides the open-source community with a practical\nsolution to enhance the mathematical reasoning abilities of LLMs."}
{"id": "2410.21013", "pdf": "https://arxiv.org/pdf/2410.21013.pdf", "abs": "https://arxiv.org/abs/2410.21013", "title": "Frequency matters: Modeling irregular morphological patterns in Spanish with Transformers", "authors": ["Akhilesh Kakolu Ramarao", "Kevin Tang", "Dinah Baer-Henney"], "categories": ["cs.CL"], "comment": "Typos and grammatical corrections", "summary": "Over the past decade, various studies have addressed how speakers solve the\nso-called `The Paradigm Cell Filling Problem' (PCFP) \\citep{ackerman2009parts}\nacross different languages. The PCFP addresses a fundamental question in\nmorphological processing: how do speakers accurately generate inflected forms\nof words when presented with incomplete paradigms? This problem is particularly\nsalient when modeling complex inflectional systems. We focus on Spanish verbal\nparadigms, where certain verbs follow an irregular L-shaped pattern, where the\nfirst-person singular present indicative stem matches the stem used throughout\nthe present subjunctive mood. We formulate the problem as a morphological\nreinflection task. Specifically, we investigate the role of input frequency in\nthe acquisition of regular versus irregular L-shaped patterns in transformer\nmodels. By systematically manipulating the input distributions and analyzing\nmodel behavior, we reveal four key findings: 1) Models perform better on\nL-shaped verbs compared to regular verbs, especially in uneven frequency\nconditions; 2) Robust primacy effects are observed, but no consistent recency\neffects; 3) Memorization becomes more prominent as the proportion of L-shaped\nverbs increases; 4) There is a tendency to regularize L-shaped verbs when their\nconsonant alternation pairs are rare or absent in the training data."}
{"id": "2411.00387", "pdf": "https://arxiv.org/pdf/2411.00387.pdf", "abs": "https://arxiv.org/abs/2411.00387", "title": "STEM-POM: Evaluating Language Models Math-Symbol Reasoning in Document Parsing", "authors": ["Jiaru Zou", "Qing Wang", "Pratyush Thakur", "Nickvash Kani"], "categories": ["cs.CL"], "comment": "ACL 2025; NeurIPS Math-AI 2024", "summary": "Advances in large language models (LLMs) have spurred research into enhancing\ntheir reasoning capabilities, particularly in math-rich STEM (Science,\nTechnology, Engineering, and Mathematics) documents. While LLMs can generate\nequations or solve math-related queries, their ability to fully understand and\ninterpret abstract mathematical symbols in long, math-rich documents remains\nlimited. In this paper, we introduce STEM-PoM, a comprehensive benchmark\ndataset designed to evaluate LLMs' reasoning abilities on math symbols within\ncontextual scientific text. The dataset, sourced from real-world ArXiv\ndocuments, contains over 2K math symbols classified as main attributes of\nvariables, constants, operators, and unit descriptors, with additional\nsub-attributes including scalar/vector/matrix for variables and\nlocal/global/discipline-specific labels for both constants and operators. Our\nextensive experiments demonstrate that state-of-the-art LLMs achieve an average\naccuracy of 20-60% under in-context learning and 50-60% with fine-tuning,\nhighlighting a substantial gap in their ability to classify mathematical\nsymbols. By improving LLMs' mathematical symbol classification, STEM-PoM\nfurther enhances models' downstream mathematical reasoning capabilities. The\ncode and data are available at https://github.com/jiaruzouu/STEM-PoM."}
{"id": "2411.01834", "pdf": "https://arxiv.org/pdf/2411.01834.pdf", "abs": "https://arxiv.org/abs/2411.01834", "title": "Align-SLM: Textless Spoken Language Models with Reinforcement Learning from AI Feedback", "authors": ["Guan-Ting Lin", "Prashanth Gurunath Shivakumar", "Aditya Gourav", "Yile Gu", "Ankur Gandhe", "Hung-yi Lee", "Ivan Bulyko"], "categories": ["cs.CL", "eess.AS"], "comment": "Accepted by ACL 2025", "summary": "While textless Spoken Language Models (SLMs) have shown potential in\nend-to-end speech-to-speech modeling, they still lag behind text-based Large\nLanguage Models (LLMs) in terms of semantic coherence and relevance. This work\nintroduces the Align-SLM framework, which leverages preference optimization\ninspired by Reinforcement Learning with AI Feedback (RLAIF) to enhance the\nsemantic understanding of SLMs. Our approach generates multiple speech\ncontinuations from a given prompt and uses semantic metrics to create\npreference data for Direct Preference Optimization (DPO). We evaluate the\nframework using ZeroSpeech 2021 benchmarks for lexical and syntactic modeling,\nthe spoken version of the StoryCloze dataset for semantic coherence, and other\nspeech generation metrics, including the GPT4-o score and human evaluation.\nExperimental results show that our method achieves state-of-the-art performance\nfor SLMs on most benchmarks, highlighting the importance of preference\noptimization to improve the semantics of SLMs."}
{"id": "2411.07446", "pdf": "https://arxiv.org/pdf/2411.07446.pdf", "abs": "https://arxiv.org/abs/2411.07446", "title": "Efficient and Accurate Prompt Optimization: the Benefit of Memory in Exemplar-Guided Reflection", "authors": ["Cilin Yan", "Jingyun Wang", "Lin Zhang", "Ruihui Zhao", "Xiaopu Wu", "Kai Xiong", "Qingsong Liu", "Guoliang Kang", "Yangyang Kang"], "categories": ["cs.CL"], "comment": "ACL 2025 Main", "summary": "Automatic prompt engineering aims to enhance the generation quality of large\nlanguage models (LLMs). Recent works utilize feedbacks generated from erroneous\ncases to guide the prompt optimization. During inference, they may further\nretrieve several semantically-related exemplars and concatenate them to the\noptimized prompts to improve the performance. However, those works only utilize\nthe feedback at the current step, ignoring historical and unseleccted feedbacks\nwhich are potentially beneficial. Moreover, the selection of exemplars only\nconsiders the general semantic relationship and may not be optimal in terms of\ntask performance and matching with the optimized prompt. In this work, we\npropose an Exemplar-Guided Reflection with Memory mechanism (ERM) to realize\nmore efficient and accurate prompt optimization. Specifically, we design an\nexemplar-guided reflection mechanism where the feedback generation is\nadditionally guided by the generated exemplars. We further build two kinds of\nmemory to fully utilize the historical feedback information and support more\neffective exemplar retrieval. Empirical evaluations show our method surpasses\nprevious state-of-the-arts with less optimization steps, i.e., improving F1\nscore by 10.1 on LIAR dataset, and reducing half of the optimization steps on\nProTeGi."}
{"id": "2411.12719", "pdf": "https://arxiv.org/pdf/2411.12719.pdf", "abs": "https://arxiv.org/abs/2411.12719", "title": "Rethinking MUSHRA: Addressing Modern Challenges in Text-to-Speech Evaluation", "authors": ["Praveen Srinivasa Varadhan", "Amogh Gulati", "Ashwin Sankar", "Srija Anand", "Anirudh Gupta", "Anirudh Mukherjee", "Shiva Kumar Marepally", "Ankur Bhatia", "Saloni Jaju", "Suvrat Bhooshan", "Mitesh M. Khapra"], "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "comment": "Accepted in TMLR", "summary": "Despite rapid advancements in TTS models, a consistent and robust human\nevaluation framework is still lacking. For example, MOS tests fail to\ndifferentiate between similar models, and CMOS's pairwise comparisons are\ntime-intensive. The MUSHRA test is a promising alternative for evaluating\nmultiple TTS systems simultaneously, but in this work we show that its reliance\non matching human reference speech unduly penalises the scores of modern TTS\nsystems that can exceed human speech quality. More specifically, we conduct a\ncomprehensive assessment of the MUSHRA test, focusing on its sensitivity to\nfactors such as rater variability, listener fatigue, and reference bias. Based\non our extensive evaluation involving 492 human listeners across Hindi and\nTamil we identify two primary shortcomings: (i) reference-matching bias, where\nraters are unduly influenced by the human reference, and (ii) judgement\nambiguity, arising from a lack of clear fine-grained guidelines. To address\nthese issues, we propose two refined variants of the MUSHRA test. The first\nvariant enables fairer ratings for synthesized samples that surpass human\nreference quality. The second variant reduces ambiguity, as indicated by the\nrelatively lower variance across raters. By combining these approaches, we\nachieve both more reliable and more fine-grained assessments. We also release\nMANGO, a massive dataset of 246,000 human ratings, the first-of-its-kind\ncollection for Indian languages, aiding in analyzing human preferences and\ndeveloping automatic metrics for evaluating TTS systems."}
{"id": "2411.14055", "pdf": "https://arxiv.org/pdf/2411.14055.pdf", "abs": "https://arxiv.org/abs/2411.14055", "title": "DRPruning: Efficient Large Language Model Pruning through Distributionally Robust Optimization", "authors": ["Hexuan Deng", "Wenxiang Jiao", "Xuebo Liu", "Jing Li", "Min Zhang", "Zhaopeng Tu"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 Main Conference", "summary": "Large language models (LLMs) deliver impressive results but face challenges\nfrom increasing model sizes and computational costs. Structured pruning reduces\nmodel size and speeds up inference but often causes uneven degradation across\ndomains, leading to biased performance. To address this, we propose DRPruning,\na method that dynamically adjusts the data distribution during training to\nrestore balanced performance across heterogeneous and multi-tasking data.\nExperiments in monolingual and multilingual settings show that DRPruning\nsurpasses similarly sized models in both pruning and continued pretraining over\nperplexity, downstream tasks, and instruction tuning. Further analysis\ndemonstrates the robustness of DRPruning towards various domains and\ndistribution shifts. Furthermore, DRPruning can determine optimal reference\nlosses and data ratios automatically, suggesting potential for broader\napplications. Code and scripts are available at\nhttps://github.com/hexuandeng/DRPruning."}
{"id": "2412.12040", "pdf": "https://arxiv.org/pdf/2412.12040.pdf", "abs": "https://arxiv.org/abs/2412.12040", "title": "How Private are Language Models in Abstractive Summarization?", "authors": ["Anthony Hughes", "Ning Ma", "Nikolaos Aletras"], "categories": ["cs.CL"], "comment": null, "summary": "In sensitive domains such as medical and legal, protecting sensitive\ninformation is critical, with protective laws strictly prohibiting the\ndisclosure of personal data. This poses challenges for sharing valuable data\nsuch as medical reports and legal cases summaries. While language models (LMs)\nhave shown strong performance in text summarization, it is still an open\nquestion to what extent they can provide privacy-preserving summaries from\nnon-private source documents. In this paper, we perform a comprehensive study\nof privacy risks in LM-based summarization across two closed- and four\nopen-weight models of different sizes and families. We experiment with both\nprompting and fine-tuning strategies for privacy-preservation across a range of\nsummarization datasets including medical and legal domains. Our quantitative\nand qualitative analysis, including human evaluation, shows that LMs frequently\nleak personally identifiable information in their summaries, in contrast to\nhuman-generated privacy-preserving summaries, which demonstrate significantly\nhigher privacy protection levels. These findings highlight a substantial gap\nbetween current LM capabilities and expert human expert performance in\nprivacy-sensitive summarization tasks."}
{"id": "2412.12472", "pdf": "https://arxiv.org/pdf/2412.12472.pdf", "abs": "https://arxiv.org/abs/2412.12472", "title": "Knowledge Boundary of Large Language Models: A Survey", "authors": ["Moxin Li", "Yong Zhao", "Wenxuan Zhang", "Shuaiyi Li", "Wenya Xie", "See-Kiong Ng", "Tat-Seng Chua", "Yang Deng"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (main)", "summary": "Although large language models (LLMs) store vast amount of knowledge in their\nparameters, they still have limitations in the memorization and utilization of\ncertain knowledge, leading to undesired behaviors such as generating untruthful\nand inaccurate responses. This highlights the critical need to understand the\nknowledge boundary of LLMs, a concept that remains inadequately defined in\nexisting research. In this survey, we propose a comprehensive definition of the\nLLM knowledge boundary and introduce a formalized taxonomy categorizing\nknowledge into four distinct types. Using this foundation, we systematically\nreview the field through three key lenses: the motivation for studying LLM\nknowledge boundaries, methods for identifying these boundaries, and strategies\nfor mitigating the challenges they present. Finally, we discuss open challenges\nand potential research directions in this area. We aim for this survey to offer\nthe community a comprehensive overview, facilitate access to key issues, and\ninspire further advancements in LLM knowledge research."}
{"id": "2412.14838", "pdf": "https://arxiv.org/pdf/2412.14838.pdf", "abs": "https://arxiv.org/abs/2412.14838", "title": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context LLMs", "authors": ["Xiabin Zhou", "Wenbin Wang", "Minyan Zeng", "Jiaxian Guo", "Xuebo Liu", "Li Shen", "Min Zhang", "Liang Ding"], "categories": ["cs.CL"], "comment": null, "summary": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased."}
{"id": "2501.01264", "pdf": "https://arxiv.org/pdf/2501.01264.pdf", "abs": "https://arxiv.org/abs/2501.01264", "title": "ProgCo: Program Helps Self-Correction of Large Language Models", "authors": ["Xiaoshuai Song", "Yanan Wu", "Weixun Wang", "Jiaheng Liu", "Wenbo Su", "Bo Zheng"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accpeted at ACL2025 Main", "summary": "Self-Correction aims to enable large language models (LLMs) to self-verify\nand self-refine their initial responses without external feedback. However,\nLLMs often fail to effectively self-verify and generate correct feedback,\nfurther misleading refinement and leading to the failure of self-correction,\nespecially in complex reasoning tasks. In this paper, we propose Program-driven\nSelf-Correction (ProgCo). First, program-driven verification (ProgVe) achieves\ncomplex verification logic and extensive validation through self-generated,\nself-executing verification pseudo-programs. Then, program-driven refinement\n(ProgRe) receives feedback from ProgVe, conducts dual reflection and refinement\non both responses and verification programs to mitigate misleading of incorrect\nfeedback in complex reasoning tasks. Experiments on three instruction-following\nand mathematical benchmarks indicate that ProgCo achieves effective\nself-correction, and can be further enhance performance when combined with real\nprogram tools. We release our code at https://github.com/songxiaoshuai/progco."}
{"id": "2501.04962", "pdf": "https://arxiv.org/pdf/2501.04962.pdf", "abs": "https://arxiv.org/abs/2501.04962", "title": "VoxEval: Benchmarking the Knowledge Understanding Capabilities of End-to-End Spoken Language Models", "authors": ["Wenqian Cui", "Xiaoqi Jiao", "Ziqiao Meng", "Irwin King"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "The Version of Record of this contribution is accepted to ACL 2025\n  main conference", "summary": "With the rising need for speech-based interaction models, end-to-end Spoken\nLanguage Models (SLMs) have emerged as a promising solution. While these models\nrequire comprehensive world knowledge for meaningful and reliable human\ninteractions, existing question-answering (QA) benchmarks fall short in\nevaluating SLMs' knowledge understanding due to their inability to support\nend-to-end speech evaluation and account for varied input audio conditions. To\naddress these limitations, we present VoxEval, a novel SpeechQA benchmark that\nassesses SLMs' knowledge understanding through pure speech interactions. Our\nbenchmark 1) uniquely maintains speech format for both inputs and outputs, 2)\nevaluates model robustness across diverse input audio conditions, and 3)\npioneers the assessment of complex tasks like mathematical reasoning in spoken\nformat. Systematic evaluation demonstrates that VoxEval presents significant\nchallenges to current SLMs, revealing their sensitivity to varying audio\nconditions and highlighting the need to enhance reasoning capabilities in\nfuture development. We hope this benchmark could guide the advancement of more\nsophisticated and reliable SLMs. VoxEval dataset is available at:\nhttps://github.com/dreamtheater123/VoxEval"}
{"id": "2501.14315", "pdf": "https://arxiv.org/pdf/2501.14315.pdf", "abs": "https://arxiv.org/abs/2501.14315", "title": "Mitigating Forgetting in LLM Fine-Tuning via Low-Perplexity Token Learning", "authors": ["Chao-Chung Wu", "Zhi Rui Tam", "Chieh-Yen Lin", "Yun-Nung Chen", "Shao-Hua Sun", "Hung-yi Lee"], "categories": ["cs.CL"], "comment": null, "summary": "Maintaining consistent model performance across domains is a fundamental\nchallenge in machine learning. While recent work has explored using\nLLM-generated data for fine-tuning, its impact on cross-domain generalization\nremains poorly understood. This paper presents a systematic analysis revealing\nthat fine-tuning with LLM-generated data not only improves target task\nperformance but also reduces non-target task degradation compared to\nfine-tuning with ground truth data. Through analyzing the data sequence in\ntasks of various domains, we demonstrate that this enhancement of non-target\ntask robustness stems from the reduction of high perplexity tokens found in\nLLM-generated sequences. Following our findings, we showed that masking high\nperplexity tokens in ground truth training data achieves similar non-target\ntask performance preservation, comparable to using LLM-generated data.\nExtensive experiments across different model families and scales, including\nGemma 2 IT 2B, Llama 3 8B Instruct, and 3 additional models, agree with our\nfindings. To the best of our knowledge, this is the first work to provide an\nempirical explanation based on token perplexity reduction to mitigate\ncatastrophic forgetting in LLMs after fine-tuning, offering valuable insights\nfor developing more robust fine-tuning strategies."}
{"id": "2501.17178", "pdf": "https://arxiv.org/pdf/2501.17178.pdf", "abs": "https://arxiv.org/abs/2501.17178", "title": "Tuning LLM Judge Design Decisions for 1/1000 of the Cost", "authors": ["David Salinas", "Omar Swelam", "Frank Hutter"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Evaluating Large Language Models (LLMs) often requires costly human\nannotations. To address this, LLM-based judges have been proposed, which\ncompare the outputs of two LLMs enabling the ranking of models without human\nintervention. While several approaches have been proposed, many confounding\nfactors are present between different papers. For instance the model, the\nprompt and other hyperparameters are typically changed at the same time making\napple-to-apple comparisons challenging. In this paper, we propose to\nsystematically analyze and tune the hyperparameters of LLM judges. To alleviate\nthe high cost of evaluating a judge, we propose to leverage multi-objective\nmulti-fidelity which allows to find judges that trade accuracy for cost and\nalso significantly reduce the cost of the search. Our method identifies judges\nthat not only outperform existing benchmarks in accuracy and cost-efficiency\nbut also utilize open-weight models, ensuring greater accessibility and\nreproducibility. The code to reproduce our experiments is available at this\nrepository https://github.com/geoalgo/judgetuning ."}
{"id": "2501.18922", "pdf": "https://arxiv.org/pdf/2501.18922.pdf", "abs": "https://arxiv.org/abs/2501.18922", "title": "KBQA-o1: Agentic Knowledge Base Question Answering with Monte Carlo Tree Search", "authors": ["Haoran Luo", "Haihong E", "Yikai Guo", "Qika Lin", "Xiaobao Wu", "Xinyu Mu", "Wenhao Liu", "Meina Song", "Yifan Zhu", "Luu Anh Tuan"], "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": "Accepted by ICML 2025 main conference", "summary": "Knowledge Base Question Answering (KBQA) aims to answer natural language\nquestions with a large-scale structured knowledge base (KB). Despite\nadvancements with large language models (LLMs), KBQA still faces challenges in\nweak KB awareness, imbalance between effectiveness and efficiency, and high\nreliance on annotated data. To address these challenges, we propose KBQA-o1, a\nnovel agentic KBQA method with Monte Carlo Tree Search (MCTS). It introduces a\nReAct-based agent process for stepwise logical form generation with KB\nenvironment exploration. Moreover, it employs MCTS, a heuristic search method\ndriven by policy and reward models, to balance agentic exploration's\nperformance and search space. With heuristic exploration, KBQA-o1 generates\nhigh-quality annotations for further improvement by incremental fine-tuning.\nExperimental results show that KBQA-o1 outperforms previous low-resource KBQA\nmethods with limited annotated data, boosting Llama-3.1-8B model's GrailQA F1\nperformance to 78.5% compared to 48.5% of the previous sota method with\nGPT-3.5-turbo. Our code is publicly available."}
{"id": "2502.09192", "pdf": "https://arxiv.org/pdf/2502.09192.pdf", "abs": "https://arxiv.org/abs/2502.09192", "title": "Thinking beyond the anthropomorphic paradigm benefits LLM research", "authors": ["Lujain Ibrahim", "Myra Cheng"], "categories": ["cs.CL"], "comment": null, "summary": "Anthropomorphism, or the attribution of human traits to technology, is an\nautomatic and unconscious response that occurs even in those with advanced\ntechnical expertise. In this position paper, we analyze hundreds of thousands\nof research articles to present empirical evidence of the prevalence and growth\nof anthropomorphic terminology in research on large language models (LLMs). We\nargue for challenging the deeper assumptions reflected in this terminology --\nwhich, though often useful, may inadvertently constrain LLM development -- and\nbroadening beyond them to open new pathways for understanding and improving\nLLMs. Specifically, we identify and examine five anthropomorphic assumptions\nthat shape research across the LLM development lifecycle. For each assumption\n(e.g., that LLMs must use natural language for reasoning, or that they should\nbe evaluated on benchmarks originally meant for humans), we demonstrate\nempirical, non-anthropomorphic alternatives that remain under-explored yet\noffer promising directions for LLM research and development."}
{"id": "2502.09674", "pdf": "https://arxiv.org/pdf/2502.09674.pdf", "abs": "https://arxiv.org/abs/2502.09674", "title": "The Hidden Dimensions of LLM Alignment: A Multi-Dimensional Analysis of Orthogonal Safety Directions", "authors": ["Wenbo Pan", "Zhichao Liu", "Qiguang Chen", "Xiangyang Zhou", "Haining Yu", "Xiaohua Jia"], "categories": ["cs.CL", "cs.AI"], "comment": "Code and artifacts: https://github.com/BMPixel/safety-residual-space\n  Accepted by ICML 2025", "summary": "Large Language Models' safety-aligned behaviors, such as refusing harmful\nqueries, can be represented by linear directions in activation space. Previous\nresearch modeled safety behavior with a single direction, limiting mechanistic\nunderstanding to an isolated safety feature. In this work, we discover that\nsafety-aligned behavior is jointly controlled by multi-dimensional directions.\nNamely, we study the vector space of representation shifts during safety\nfine-tuning on Llama 3 8B for refusing jailbreaks. By studying orthogonal\ndirections in the space, we first find that a dominant direction governs the\nmodel's refusal behavior, while multiple smaller directions represent distinct\nand interpretable features like hypothetical narrative and role-playing. We\nthen measure how different directions promote or suppress the dominant\ndirection, showing the important role of secondary directions in shaping the\nmodel's refusal representation. Finally, we demonstrate that removing certain\ntrigger tokens in harmful queries can mitigate these directions to bypass the\nlearned safety capability, providing new insights on understanding safety\nalignment vulnerability from a multi-dimensional perspective. Code and\nartifacts are available at https://github.com/BMPixel/safety-residual-space."}
{"id": "2502.11051", "pdf": "https://arxiv.org/pdf/2502.11051.pdf", "abs": "https://arxiv.org/abs/2502.11051", "title": "MMUnlearner: Reformulating Multimodal Machine Unlearning in the Era of Multimodal Large Language Models", "authors": ["Jiahao Huo", "Yibo Yan", "Xu Zheng", "Yuanhuiyi Lyu", "Xin Zou", "Zhihua Wei", "Xuming Hu"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted as ACL 2025 Findings", "summary": "Recent progress in Machine Unlearning (MU) has introduced solutions for the\nselective removal of private or sensitive information encoded within deep\nneural networks. Nonetheless, MU for Multimodal Large Language Models (MLLMs)\nremains in its nascent phase. Therefore, we propose to reformulate the task of\nmultimodal MU in the era of MLLMs, which aims to erase only the visual patterns\nassociated with a given entity while preserving the corresponding textual\nknowledge encoded within the original parameters of the language model\nbackbone. Furthermore, we develop a novel geometry-constrained gradient ascent\nmethod MMUnlearner. It updates the weights of MLLMs with a weight saliency map\njointly restricted by the remaining concepts and textual knowledge during\nunlearning, thereby preserving parameters essential for non-target knowledge.\nExtensive experiments demonstrate that MMUnlearner surpasses baselines that\nfinetuning MLLMs with VQA data directly through Gradient Ascent (GA) or\nNegative Preference Optimization (NPO), across all evaluation dimensions. Our\ncode can be found in [this URL](https://github.com/Z1zs/MMUnlearner)."}
{"id": "2502.11198", "pdf": "https://arxiv.org/pdf/2502.11198.pdf", "abs": "https://arxiv.org/abs/2502.11198", "title": "ANCHOLIK-NER: A Benchmark Dataset for Bangla Regional Named Entity Recognition", "authors": ["Bidyarthi Paul", "Faika Fairuj Preotee", "Shuvashis Sarker", "Shamim Rahim Refat", "Shifat Islam", "Tashreef Muhammad", "Mohammad Ashraful Hoque", "Shahriar Manzoor"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Named Entity Recognition (NER) in regional dialects is a critical yet\nunderexplored area in Natural Language Processing (NLP), especially for\nlow-resource languages like Bangla. While NER systems for Standard Bangla have\nmade progress, no existing resources or models specifically address the\nchallenge of regional dialects such as Barishal, Chittagong, Mymensingh,\nNoakhali, and Sylhet, which exhibit unique linguistic features that existing\nmodels fail to handle effectively. To fill this gap, we introduce ANCHOLIK-NER,\nthe first benchmark dataset for NER in Bangla regional dialects, comprising\n17,405 sentences distributed across five regions. The dataset was sourced from\npublicly available resources and supplemented with manual translations,\nensuring alignment of named entities across dialects. We evaluate three\ntransformer-based models - Bangla BERT, Bangla BERT Base, and BERT Base\nMultilingual Cased - on this dataset. Our findings demonstrate that BERT Base\nMultilingual Cased performs best in recognizing named entities across regions,\nwith significant performance observed in Mymensingh with an F1-score of\n82.611%. Despite strong overall performance, challenges remain in region like\nChittagong, where the models show lower precision and recall. Since no previous\nNER systems for Bangla regional dialects exist, our work represents a\nfoundational step in addressing this gap. Future work will focus on improving\nmodel performance in underperforming regions and expanding the dataset to\ninclude more dialects, enhancing the development of dialect-aware NER systems."}
{"id": "2502.12051", "pdf": "https://arxiv.org/pdf/2502.12051.pdf", "abs": "https://arxiv.org/abs/2502.12051", "title": "How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines", "authors": ["Ayan Sengupta", "Yash Goel", "Tanmoy Chakraborty"], "categories": ["cs.CL", "cs.LG"], "comment": "21 pages, 11 tables, 4 figures", "summary": "Neural scaling laws have revolutionized the design and optimization of\nlarge-scale AI models by revealing predictable relationships between model\nsize, dataset volume, and computational resources. Early research established\npower-law relationships in model performance, leading to compute-optimal\nscaling strategies. However, recent studies highlighted their limitations\nacross architectures, modalities, and deployment contexts. Sparse models,\nmixture-of-experts, retrieval-augmented learning, and multimodal models often\ndeviate from traditional scaling patterns. Moreover, scaling behaviors vary\nacross domains such as vision, reinforcement learning, and fine-tuning,\nunderscoring the need for more nuanced approaches. In this survey, we\nsynthesize insights from over 50 studies, examining the theoretical\nfoundations, empirical findings, and practical implications of scaling laws. We\nalso explore key challenges, including data efficiency, inference scaling, and\narchitecture-specific constraints, advocating for adaptive scaling strategies\ntailored to real-world applications. We suggest that while scaling laws provide\na useful guide, they do not always generalize across all architectures and\ntraining strategies."}
{"id": "2502.12134", "pdf": "https://arxiv.org/pdf/2502.12134.pdf", "abs": "https://arxiv.org/abs/2502.12134", "title": "SoftCoT: Soft Chain-of-Thought for Efficient Reasoning with LLMs", "authors": ["Yige Xu", "Xu Guo", "Zhiwei Zeng", "Chunyan Miao"], "categories": ["cs.CL"], "comment": "Camera-ready for ACL 2025 (main conference)", "summary": "Chain-of-Thought (CoT) reasoning enables Large Language Models (LLMs) to\nsolve complex reasoning tasks by generating intermediate reasoning steps.\nHowever, most existing approaches focus on hard token decoding, which\nconstrains reasoning within the discrete vocabulary space and may not always be\noptimal. While recent efforts explore continuous-space reasoning, they often\nrequire full-model fine-tuning and suffer from catastrophic forgetting,\nlimiting their applicability to state-of-the-art LLMs that already perform well\nin zero-shot settings with a proper instruction. To address this challenge, we\npropose a novel approach for continuous-space reasoning that does not require\nmodifying the LLM. Specifically, we employ a lightweight fixed assistant model\nto speculatively generate instance-specific soft thought tokens as the initial\nchain of thoughts, which are then mapped into the LLM's representation space\nvia a trainable projection module. Experimental results on five reasoning\nbenchmarks demonstrate that our method enhances LLM reasoning performance\nthrough supervised, parameter-efficient fine-tuning. Source code is available\nat https://github.com/xuyige/SoftCoT."}
{"id": "2502.12187", "pdf": "https://arxiv.org/pdf/2502.12187.pdf", "abs": "https://arxiv.org/abs/2502.12187", "title": "Hallucinations are inevitable but can be made statistically negligible. The \"innate\" inevitability of hallucinations cannot explain practical LLM issues", "authors": ["Atsushi Suzuki", "Yulan He", "Feng Tian", "Zhongyuan Wang"], "categories": ["cs.CL", "cs.FL", "cs.LG", "math.ST", "stat.ML", "stat.TH"], "comment": null, "summary": "Hallucinations, a phenomenon where a language model (LM) generates nonfactual\ncontent, pose a significant challenge to the practical deployment of LMs. While\nmany empirical methods have been proposed to mitigate hallucinations, recent\nstudies established a computability-theoretic result showing that any LM will\ninevitably generate hallucinations on an infinite set of inputs, regardless of\nthe quality and quantity of training datasets and the choice of the language\nmodel architecture and training and inference algorithms. Although the\ncomputability-theoretic result may seem pessimistic, its significance in\npractical viewpoints has remained unclear. This paper claims that those\n\"innate\" inevitability results from computability theory and diagonal argument,\nin principle, cannot explain practical issues of LLMs. We demonstrate this\nclaim by presenting a positive theoretical result from a probabilistic\nperspective. Specifically, we prove that hallucinations can be made\nstatistically negligible, provided that the quality and quantity of the\ntraining data are sufficient. Interestingly, our positive result coexists with\nthe computability-theoretic result, implying that while hallucinations on an\ninfinite set of inputs cannot be entirely eliminated, their probability can\nalways be reduced by improving algorithms and training data. By evaluating the\ntwo seemingly contradictory results through the lens of information theory, we\nargue that our probability-theoretic positive result better reflects practical\nconsiderations than the computability-theoretic negative result."}
{"id": "2502.12953", "pdf": "https://arxiv.org/pdf/2502.12953.pdf", "abs": "https://arxiv.org/abs/2502.12953", "title": "Task-Informed Anti-Curriculum by Masking Improves Downstream Performance on Text", "authors": ["Andrei Jarca", "Florinel Alin Croitoru", "Radu Tudor Ionescu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at ACL 2025", "summary": "Masked language modeling has become a widely adopted unsupervised technique\nto pre-train large language models (LLMs). However, the process of selecting\ntokens for masking is random, and the percentage of masked tokens is typically\nfixed for the entire training process. In this paper, we propose to adjust the\nmasking ratio and to decide which tokens to mask based on a novel task-informed\nanti-curriculum learning scheme. First, we harness task-specific knowledge\nabout useful and harmful tokens in order to determine which tokens to mask.\nSecond, we propose a cyclic decaying masking ratio, which corresponds to an\nanti-curriculum schedule (from hard to easy). We exemplify our novel\ntask-informed anti-curriculum by masking (TIACBM) approach across three diverse\ndownstream tasks: sentiment analysis, text classification by topic, and\nauthorship attribution. Our findings suggest that TIACBM enhances the ability\nof the model to focus on key task-relevant features, contributing to\nstatistically significant performance gains across tasks. We release our code\nat https://github.com/JarcaAndrei/TIACBM."}
{"id": "2502.13010", "pdf": "https://arxiv.org/pdf/2502.13010.pdf", "abs": "https://arxiv.org/abs/2502.13010", "title": "Agentic Medical Knowledge Graphs Enhance Medical Question Answering: Bridging the Gap Between LLMs and Evolving Medical Knowledge", "authors": ["Mohammad Reza Rezaei", "Reza Saadati Fard", "Rahul G. Krishnan", "Milad Lankarany"], "categories": ["cs.CL", "cs.MA"], "comment": null, "summary": "Large Language Models (LLMs) have significantly advanced medical\nquestion-answering by leveraging extensive clinical data and medical\nliterature. However, the rapid evolution of medical knowledge and the\nlabor-intensive process of manually updating domain-specific resources pose\nchallenges to the reliability of these systems. To address this, we introduce\nAgentic Medical Graph-RAG (AMG-RAG), a comprehensive framework that automates\nthe construction and continuous updating of medical knowledge graphs,\nintegrates reasoning, and retrieves current external evidence, such as PubMed\nand WikiSearch. By dynamically linking new findings and complex medical\nconcepts, AMG-RAG not only improves accuracy but also enhances interpretability\nin medical queries.\n  Evaluations on the MEDQA and MEDMCQA benchmarks demonstrate the effectiveness\nof AMG-RAG, achieving an F1 score of 74.1 percent on MEDQA and an accuracy of\n66.34 percent on MEDMCQA, outperforming both comparable models and those 10 to\n100 times larger. Notably, these improvements are achieved without increasing\ncomputational overhead, highlighting the critical role of automated knowledge\ngraph generation and external evidence retrieval in delivering up-to-date,\ntrustworthy medical insights."}
{"id": "2502.14132", "pdf": "https://arxiv.org/pdf/2502.14132.pdf", "abs": "https://arxiv.org/abs/2502.14132", "title": "Can Community Notes Replace Professional Fact-Checkers?", "authors": ["Nadav Borenstein", "Greta Warren", "Desmond Elliott", "Isabelle Augenstein"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to the main proceedings of ACL 2025", "summary": "Two commonly employed strategies to combat the rise of misinformation on\nsocial media are (i) fact-checking by professional organisations and (ii)\ncommunity moderation by platform users. Policy changes by Twitter/X and, more\nrecently, Meta, signal a shift away from partnerships with fact-checking\norganisations and towards an increased reliance on crowdsourced community\nnotes. However, the extent and nature of dependencies between fact-checking and\nhelpful community notes remain unclear. To address these questions, we use\nlanguage models to annotate a large corpus of Twitter/X community notes with\nattributes such as topic, cited sources, and whether they refute claims tied to\nbroader misinformation narratives. Our analysis reveals that community notes\ncite fact-checking sources up to five times more than previously reported.\nFact-checking is especially crucial for notes on posts linked to broader\nnarratives, which are twice as likely to reference fact-checking sources\ncompared to other sources. Our results show that successful community\nmoderation relies on professional fact-checking and highlight how citizen and\nprofessional fact-checking are deeply intertwined."}
{"id": "2502.14613", "pdf": "https://arxiv.org/pdf/2502.14613.pdf", "abs": "https://arxiv.org/abs/2502.14613", "title": "Behavioral Analysis of Information Salience in Large Language Models", "authors": ["Jan Trienes", "Jörg Schlötterer", "Junyi Jessy Li", "Christin Seifert"], "categories": ["cs.CL"], "comment": "Accepted at ACL 2025 (Findings)", "summary": "Large Language Models (LLMs) excel at text summarization, a task that\nrequires models to select content based on its importance. However, the exact\nnotion of salience that LLMs have internalized remains unclear. To bridge this\ngap, we introduce an explainable framework to systematically derive and\ninvestigate information salience in LLMs through their summarization behavior.\nUsing length-controlled summarization as a behavioral probe into the content\nselection process, and tracing the answerability of Questions Under Discussion\nthroughout, we derive a proxy for how models prioritize information. Our\nexperiments on 13 models across four datasets reveal that LLMs have a nuanced,\nhierarchical notion of salience, generally consistent across model families and\nsizes. While models show highly consistent behavior and hence salience\npatterns, this notion of salience cannot be accessed through introspection, and\nonly weakly correlates with human perceptions of information salience."}
{"id": "2502.16514", "pdf": "https://arxiv.org/pdf/2502.16514.pdf", "abs": "https://arxiv.org/abs/2502.16514", "title": "GraphCheck: Breaking Long-Term Text Barriers with Extracted Knowledge Graph-Powered Fact-Checking", "authors": ["Yingjian Chen", "Haoran Liu", "Yinhong Liu", "Jinxiang Xie", "Rui Yang", "Han Yuan", "Yanran Fu", "Peng Yuan Zhou", "Qingyu Chen", "James Caverlee", "Irene Li"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are widely used, but they often generate subtle\nfactual errors, especially in long-form text. These errors are fatal in some\nspecialized domains such as medicine. Existing fact-checking with grounding\ndocuments methods face two main challenges: (1) they struggle to understand\ncomplex multihop relations in long documents, often overlooking subtle factual\nerrors; (2) most specialized methods rely on pairwise comparisons, requiring\nmultiple model calls, leading to high resource and computational costs. To\naddress these challenges, we propose GraphCheck, a fact-checking framework that\nuses extracted knowledge graphs to enhance text representation. Graph Neural\nNetworks further process these graphs as a soft prompt, enabling LLMs to\nincorporate structured knowledge more effectively. Enhanced with graph-based\nreasoning, GraphCheck captures multihop reasoning chains that are often\noverlooked by existing methods, enabling precise and efficient fact-checking in\na single inference call. Experimental results on seven benchmarks spanning both\ngeneral and medical domains demonstrate up to a 7.1% overall improvement over\nbaseline models. Notably, GraphCheck outperforms existing specialized\nfact-checkers and achieves comparable performance with state-of-the-art LLMs,\nsuch as DeepSeek-V3 and OpenAI-o1, with significantly fewer parameters."}
{"id": "2502.17817", "pdf": "https://arxiv.org/pdf/2502.17817.pdf", "abs": "https://arxiv.org/abs/2502.17817", "title": "Predicting Through Generation: Why Generation Is Better for Prediction", "authors": ["Md Kowsher", "Nusrat Jahan Prottasha", "Prakash Bhat", "Chun-Nam Yu", "Mojtaba Soltanalian", "Ivan Garibay", "Ozlem Garibay", "Chen Chen", "Niloofar Yousefi"], "categories": ["cs.CL"], "comment": "ACL Accepted paper", "summary": "This paper argues that generating output tokens is more effective than using\npooled representations for prediction tasks because token-level generation\nretains more mutual information. Since LLMs are trained on massive text corpora\nusing next-token prediction, generation aligns naturally with their learned\nbehavior. Using the Data Processing Inequality (DPI), we provide both\ntheoretical and empirical evidence supporting this claim. However,\nautoregressive models face two key challenges when used for prediction: (1)\nexposure bias, where the model sees ground truth tokens during training but\nrelies on its own predictions during inference, leading to errors, and (2)\nformat mismatch, where discrete tokens do not always align with the tasks\nrequired output structure. To address these challenges, we introduce\nPredGen(Predicting Through Generating), an end to end framework that (i) uses\nscheduled sampling to reduce exposure bias, and (ii) introduces a task adapter\nto convert the generated tokens into structured outputs. Additionally, we\nintroduce Writer-Director Alignment Loss (WDAL), which ensures consistency\nbetween token generation and final task predictions, improving both text\ncoherence and numerical accuracy. We evaluate PredGen on multiple\nclassification and regression benchmarks. Our results show that PredGen\nconsistently outperforms standard baselines, demonstrating its effectiveness in\nstructured prediction tasks."}
{"id": "2502.18001", "pdf": "https://arxiv.org/pdf/2502.18001.pdf", "abs": "https://arxiv.org/abs/2502.18001", "title": "Unveiling the Key Factors for Distilling Chain-of-Thought Reasoning", "authors": ["Xinghao Chen", "Zhijing Sun", "Wenjin Guo", "Miaoran Zhang", "Yanjun Chen", "Yirong Sun", "Hui Su", "Yijie Pan", "Dietrich Klakow", "Wenjie Li", "Xiaoyu Shen"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Large Language Models (LLMs) excel in reasoning tasks through\nChain-of-Thought (CoT) prompting. However, CoT prompting greatly increases\ncomputational demands, which has prompted growing interest in distilling CoT\ncapabilities into Small Language Models (SLMs). This study systematically\nexamines the factors influencing CoT distillation, including the choice of\ngranularity, format and teacher model. Through experiments involving four\nteacher models and seven student models across seven mathematical and\ncommonsense reasoning datasets, we uncover three key findings: (1) Unlike LLMs,\nSLMs exhibit a non-monotonic relationship with granularity, with stronger\nmodels benefiting from finer-grained reasoning and weaker models performing\nbetter with simpler CoT supervision; (2) CoT format significantly impacts LLMs\nbut has minimal effect on SLMs, likely due to their reliance on supervised\nfine-tuning rather than pretraining preferences; (3) Stronger teacher models do\nNOT always produce better student models, as diversity and complexity in CoT\nsupervision can outweigh accuracy alone. These findings emphasize the need to\ntailor CoT strategies to specific student model, offering actionable insights\nfor optimizing CoT distillation in SLMs. The code and datasets are available at\nhttps://github.com/EIT-NLP/Distilling-CoT-Reasoning."}
{"id": "2502.18874", "pdf": "https://arxiv.org/pdf/2502.18874.pdf", "abs": "https://arxiv.org/abs/2502.18874", "title": "Learning to Align Multi-Faceted Evaluation: A Unified and Robust Framework", "authors": ["Kaishuai Xu", "Tiezheng Yu", "Wenjun Hou", "Yi Cheng", "Liangyou Li", "Xin Jiang", "Lifeng Shang", "Qun Liu", "Wenjie Li"], "categories": ["cs.CL", "cs.AI"], "comment": "accepted as ACL 2025 findings", "summary": "Large Language Models (LLMs) are being used more and more extensively for\nautomated evaluation in various scenarios. Previous studies have attempted to\nfine-tune open-source LLMs to replicate the evaluation explanations and\njudgments of powerful proprietary models, such as GPT-4. However, these methods\nare largely limited to text-based analyses under predefined general criteria,\nresulting in reduced adaptability for unseen instructions and demonstrating\ninstability in evaluating adherence to quantitative and structural constraints.\nTo address these limitations, we propose a novel evaluation framework, ARJudge,\nthat adaptively formulates evaluation criteria and synthesizes both text-based\nand code-driven analyses to evaluate LLM responses. ARJudge consists of two\ncomponents: a fine-tuned Analyzer that generates multi-faceted evaluation\nanalyses and a tuning-free Refiner that combines and refines all analyses to\nmake the final judgment. We construct a Composite Analysis Corpus that\nintegrates tasks for evaluation criteria generation alongside text-based and\ncode-driven analysis generation to train the Analyzer. Our results demonstrate\nthat ARJudge outperforms existing fine-tuned evaluators in effectiveness and\nrobustness. Furthermore, it demonstrates the importance of multi-faceted\nevaluation and code-driven analyses in enhancing evaluation capabilities."}
{"id": "2502.19249", "pdf": "https://arxiv.org/pdf/2502.19249.pdf", "abs": "https://arxiv.org/abs/2502.19249", "title": "Between Circuits and Chomsky: Pre-pretraining on Formal Languages Imparts Linguistic Biases", "authors": ["Michael Y. Hu", "Jackson Petty", "Chuan Shi", "William Merrill", "Tal Linzen"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL 2025 Camera Ready", "summary": "Pretraining language models on formal language can improve their acquisition\nof natural language. Which features of the formal language impart an inductive\nbias that leads to effective transfer? Drawing on insights from linguistics and\ncomplexity theory, we hypothesize that effective transfer occurs when two\nconditions are met: the formal language should capture the dependency\nstructures present in natural language, and it should remain within the\ncomputational limitations of the model architecture. We experiment with\npre-pretraining (training on formal language before natural languages) on\ntransformers and find that formal languages capturing hierarchical dependencies\nindeed enable language models to achieve lower loss on natural language and\nbetter linguistic generalization compared to other formal languages. We also\nfind modest support for the hypothesis that the formal language should fall\nwithin the computational limitations of the architecture. Strikingly,\npre-pretraining reduces loss more efficiently than training on a matched amount\nof natural language. For a 1B-parameter language model trained on roughly 1.6B\ntokens of natural language, pre-pretraining achieves the same loss and better\nlinguistic generalization with a 33% smaller token budget. Finally, we also\ngive mechanistic evidence of transfer from formal to natural language:\nattention heads acquired during pre-pretraining remain crucial for the model's\nperformance on syntactic evaluations."}
{"id": "2502.20795", "pdf": "https://arxiv.org/pdf/2502.20795.pdf", "abs": "https://arxiv.org/abs/2502.20795", "title": "Plan2Align: Predictive Planning Based Test-Time Preference Alignment for Large Language Models", "authors": ["Kuang-Da Wang", "Teng-Ruei Chen", "Yu Heng Hung", "Guo-Xun Ko", "Shuoyang Ding", "Yueh-Hua Wu", "Yu-Chiang Frank Wang", "Chao-Han Huck Yang", "Wen-Chih Peng", "Ping-Chun Hsieh"], "categories": ["cs.CL"], "comment": "Preprint. Code will be released at Plan2Align GitHub link:\n  https://github.com/NYCU-RL-Bandits-Lab/Plan2Align", "summary": "Aligning Large Language Models with Preference Fine-Tuning is often\nresource-intensive. Test-time alignment techniques that do not modify the\nunderlying models, such as prompting and guided decodings, offer a lightweight\nalternative. However, existing test-time alignment methods primarily improve\nshort responses and fail to ensure coherence over extended contexts due to the\nmyopic nature of token-level alignment. Moreover, these methods often incur a\nslowdown during inference. To address these challenges, we propose Plan2Align,\na test-time alignment framework that formulates text generation as a predictive\nplanning problem. Plan2Align adapts Model Predictive Control (MPC) to\niteratively refine output by rolling out multiple complete responses and\noptimizing each segment. To more rigorously evaluate the effectiveness and\nefficiency, we focus on the more challenging task of long-text generation.\nExperiments on the long-form response subset of the HH-RLHF dataset and the\nWMT'24 Discourse-Level Literary Translation demonstrate that Plan2Align\nsignificantly enhances the performance of base LLMs. Compared to existing\ntraining-time and test-time alignment methods on LLaMA-3.1 8B, Plan2Align\nachieves comparable or superior results, while also delivering improved\ninference efficiency relative to prior test-time alignment approaches."}
{"id": "2502.20859", "pdf": "https://arxiv.org/pdf/2502.20859.pdf", "abs": "https://arxiv.org/abs/2502.20859", "title": "The Power of Personality: A Human Simulation Perspective to Investigate Large Language Model Agents", "authors": ["Yifan Duan", "Yihong Tang", "Xuefeng Bai", "Kehai Chen", "Juntao Li", "Min Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) excel in both closed tasks (including\nproblem-solving, and code generation) and open tasks (including creative\nwriting), yet existing explanations for their capabilities lack connections to\nreal-world human intelligence. To fill this gap, this paper systematically\ninvestigates LLM intelligence through the lens of ``human simulation'',\naddressing three core questions: (1) \\textit{How do personality traits affect\nproblem-solving in closed tasks?} (2) \\textit{How do traits shape creativity in\nopen tasks?} (3) \\textit{How does single-agent performance influence\nmulti-agent collaboration?} By assigning Big Five personality traits to LLM\nagents and evaluating their performance in single- and multi-agent settings, we\nreveal that specific traits significantly influence reasoning accuracy (closed\ntasks) and creative output (open tasks). Furthermore, multi-agent systems\nexhibit collective intelligence distinct from individual capabilities, driven\nby distinguishing combinations of personalities."}
{"id": "2502.20968", "pdf": "https://arxiv.org/pdf/2502.20968.pdf", "abs": "https://arxiv.org/abs/2502.20968", "title": "Beware of Your Po! Measuring and Mitigating AI Safety Risks in Role-Play Fine-Tuning of LLMs", "authors": ["Weixiang Zhao", "Yulin Hu", "Yang Deng", "Jiahe Guo", "Xingyu Sui", "Xinyang Han", "An Zhang", "Yanyan Zhao", "Bing Qin", "Tat-Seng Chua", "Ting Liu"], "categories": ["cs.CL"], "comment": "To appear at ACL 2025 (Main)", "summary": "Role-playing enables large language models (LLMs) to engage users in\nimmersive and personalized interactions, but it also introduces significant\nsafety risks. Existing role-play fine-tuning techniques improve role\nadaptability but may degrade safety performance, particularly for villainous\ncharacters. In this work, we conduct the first comprehensive assessment of\nrole-play fine-tuning risks by training 95 role-specific LLMs using RoleBench.\nOur experiments reveal that role-play fine-tuning leads to a noticeable decline\nin safety performance, with safety risks varying based on character traits. To\ntackle this challenge, we propose Safety-Aware Role-Play Fine-Tuning (SaRFT), a\nnovel method designed to balance role-playing capabilities and safety.\nExtensive experiments on LLaMA-3-8B-Instruct, Gemma-2-9B-it, and\nQwen2.5-7B-Instruct demonstrate that SaRFT consistently outperforms\nstate-of-the-art baselines under both LoRA and full-parameter fine-tuning\nsettings. Our findings highlight the necessity of role-adaptive safety measures\nand provide insights into mitigating role-specific safety risks in role-playing\nLLMs."}
{"id": "2503.03205", "pdf": "https://arxiv.org/pdf/2503.03205.pdf", "abs": "https://arxiv.org/abs/2503.03205", "title": "MA-LoT: Model-Collaboration Lean-based Long Chain-of-Thought Reasoning enhances Formal Theorem Proving", "authors": ["Ruida Wang", "Rui Pan", "Yuxin Li", "Jipeng Zhang", "Yizhen Jia", "Shizhe Diao", "Renjie Pi", "Junjie Hu", "Tong Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Solving mathematical problems using computer-verifiable languages like Lean\nhas significantly impacted the mathematical and computer science communities.\nState-of-the-art methods utilize a single Large Language Model (LLM) to\ngenerate complete proof or perform tree search, but they fail to balance these\ntasks. We propose **MA-LoT**: *Model-CollAboration Lean-based Long\nChain-of-Thought*, a comprehensive framework for Lean4 theorem proving to solve\nthis issue. It separates the cognition tasks of general NL for whole-proof\ngeneration and error analysis for proof correction using the\nmodel-collaboration method. We achieve this by structured interaction of the\nLLM and Lean4 verifier in Long CoT. To implement the framework, we propose the\nnovel *LoT-Transfer Learning* training-inference pipeline, which enables the\nLong CoT thinking capability to LLMs without special data annotation. Extensive\nexperiment shows that our framework achieves a **61.07%** accuracy rate on the\nLean4 version of the MiniF2F-Test dataset, largely outperforming DeepSeek-V3\n(33.61%), single-model tree search (InternLM-Step-Prover, 50.70%), and\nwhole-proof generation (Godel-Prover, 55.33%) baselines. Furthermore, our\nfindings highlight the potential of combining Long CoT with formal verification\nfor a more insightful generation in a broader perspective."}
{"id": "2503.04615", "pdf": "https://arxiv.org/pdf/2503.04615.pdf", "abs": "https://arxiv.org/abs/2503.04615", "title": "HalluCounter: Reference-free LLM Hallucination Detection in the Wild!", "authors": ["Ashok Urlana", "Gopichand Kanumolu", "Charaka Vinayak Kumar", "Bala Mallikarjunarao Garlapati", "Rahul Mishra"], "categories": ["cs.CL"], "comment": "30 pages, 3 figures", "summary": "Response consistency-based, reference-free hallucination detection (RFHD)\nmethods do not depend on internal model states, such as generation\nprobabilities or gradients, which Grey-box models typically rely on but are\ninaccessible in closed-source LLMs. However, their inability to capture\nquery-response alignment patterns often results in lower detection accuracy.\nAdditionally, the lack of large-scale benchmark datasets spanning diverse\ndomains remains a challenge, as most existing datasets are limited in size and\nscope. To this end, we propose HalluCounter, a novel reference-free\nhallucination detection method that utilizes both response-response and\nquery-response consistency and alignment patterns. This enables the training of\na classifier that detects hallucinations and provides a confidence score and an\noptimal response for user queries. Furthermore, we introduce HalluCounterEval,\na benchmark dataset comprising both synthetically generated and human-curated\nsamples across multiple domains. Our method outperforms state-of-the-art\napproaches by a significant margin, achieving over 90\\% average confidence in\nhallucination detection across datasets."}
{"id": "2503.09598", "pdf": "https://arxiv.org/pdf/2503.09598.pdf", "abs": "https://arxiv.org/abs/2503.09598", "title": "How to Protect Yourself from 5G Radiation? Investigating LLM Responses to Implicit Misinformation", "authors": ["Ruohao Guo", "Wei Xu", "Alan Ritter"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "As Large Language Models (LLMs) are widely deployed in diverse scenarios, the\nextent to which they could tacitly spread misinformation emerges as a critical\nsafety concern. Current research primarily evaluates LLMs on explicit false\nstatements, overlooking how misinformation often manifests subtly as\nunchallenged premises in real-world interactions. We curated EchoMist, the\nfirst comprehensive benchmark for implicit misinformation, where false\nassumptions are embedded in the query to LLMs. EchoMist targets circulated,\nharmful, and ever-evolving implicit misinformation from diverse sources,\nincluding realistic human-AI conversations and social media interactions.\nThrough extensive empirical studies on 15 state-of-the-art LLMs, we find that\ncurrent models perform alarmingly poorly on this task, often failing to detect\nfalse premises and generating counterfactual explanations. We also investigate\ntwo mitigation methods, i.e., Self-Alert and RAG, to enhance LLMs' capability\nto counter implicit misinformation. Our findings indicate that EchoMist remains\na persistent challenge and underscore the critical need to safeguard against\nthe risk of implicit misinformation."}
{"id": "2503.11985", "pdf": "https://arxiv.org/pdf/2503.11985.pdf", "abs": "https://arxiv.org/abs/2503.11985", "title": "No LLM is Free From Bias: A Comprehensive Study of Bias Evaluation in Large Language Models", "authors": ["Charaka Vinayak Kumar", "Ashok Urlana", "Gopichand Kanumolu", "Bala Mallikarjunarao Garlapati", "Pruthwik Mishra"], "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 1 figure", "summary": "Advancements in Large Language Models (LLMs) have increased the performance\nof different natural language understanding as well as generation tasks.\nAlthough LLMs have breached the state-of-the-art performance in various tasks,\nthey often reflect different forms of bias present in the training data. In the\nlight of this perceived limitation, we provide a unified evaluation of\nbenchmarks using a set of representative small and medium-sized LLMs that cover\ndifferent forms of biases starting from physical characteristics to\nsocio-economic categories. Moreover, we propose five prompting approaches to\ncarry out the bias detection task across different aspects of bias. Further, we\nformulate three research questions to gain valuable insight in detecting biases\nin LLMs using different approaches and evaluation metrics across benchmarks.\nThe results indicate that each of the selected LLMs suffer from one or the\nother form of bias with the Phi-3.5B model being the least biased. Finally, we\nconclude the paper with the identification of key challenges and possible\nfuture directions."}
{"id": "2503.13857", "pdf": "https://arxiv.org/pdf/2503.13857.pdf", "abs": "https://arxiv.org/abs/2503.13857", "title": "Enabling Inclusive Systematic Reviews: Incorporating Preprint Articles with Large Language Model-Driven Evaluations", "authors": ["Rui Yang", "Jiayi Tong", "Haoyuan Wang", "Hui Huang", "Ziyang Hu", "Peiyu Li", "Nan Liu", "Christopher J. Lindsell", "Michael J. Pencina", "Yong Chen", "Chuan Hong"], "categories": ["cs.CL"], "comment": "30 pages, 6 figures", "summary": "Background. Systematic reviews in comparative effectiveness research require\ntimely evidence synthesis. Preprints accelerate knowledge dissemination but\nvary in quality, posing challenges for systematic reviews.\n  Methods. We propose AutoConfidence (automated confidence assessment), an\nadvanced framework for predicting preprint publication, which reduces reliance\non manual curation and expands the range of predictors, including three key\nadvancements: (1) automated data extraction using natural language processing\ntechniques, (2) semantic embeddings of titles and abstracts, and (3) large\nlanguage model (LLM)-driven evaluation scores. Additionally, we employed two\nprediction models: a random forest classifier for binary outcome and a survival\ncure model that predicts both binary outcome and publication risk over time.\n  Results. The random forest classifier achieved AUROC 0.692 with LLM-driven\nscores, improving to 0.733 with semantic embeddings and 0.747 with article\nusage metrics. The survival cure model reached AUROC 0.716 with LLM-driven\nscores, improving to 0.731 with semantic embeddings. For publication risk\nprediction, it achieved a concordance index of 0.658, increasing to 0.667 with\nsemantic embeddings.\n  Conclusion. Our study advances the framework for preprint publication\nprediction through automated data extraction and multiple feature integration.\nBy combining semantic embeddings with LLM-driven evaluations, AutoConfidence\nenhances predictive performance while reducing manual annotation burden. The\nframework has the potential to facilitate incorporation of preprint articles\nduring the appraisal phase of systematic reviews, supporting researchers in\nmore effective utilization of preprint resources."}
{"id": "2504.10368", "pdf": "https://arxiv.org/pdf/2504.10368.pdf", "abs": "https://arxiv.org/abs/2504.10368", "title": "S1-Bench: A Simple Benchmark for Evaluating System 1 Thinking Capability of Large Reasoning Models", "authors": ["Wenyuan Zhang", "Shuaiyi Nie", "Xinghua Zhang", "Zefeng Zhang", "Tingwen Liu"], "categories": ["cs.CL", "cs.AI"], "comment": "31 pages, 9 figures, 16 tables", "summary": "We introduce S1-Bench, a novel benchmark designed to evaluate the performance\nof Large Reasoning Models (LRMs) on simple tasks that favor intuitive system 1\nthinking rather than deliberative system 2 reasoning. While LRMs have achieved\nsignificant breakthroughs in complex reasoning tasks through explicit chains of\nthought, their heavy reliance on system 2 thinking may limit their system 1\nthinking capabilities. However, there is a lack of an appropriate benchmark for\nevaluating LRM's system 1 thinking capabilities. To fill this gap, S1-Bench\nintroduces a suite of simple, diverse, and natural questions across multiple\ndomains and languages, specifically designed to assess LRMs' performance on\nquestions more suitable for system 1 . We conduct extensive evaluations across\n28 LRMs, revealing their inefficiency, inadequate accuracy, and limited\nrobustness when handling simple questions. Additionally, we observe a gap\nbetween their difficulty perception and generation length. Overall, this work\npaves the way toward dual-system compatibility in the development of LRMs."}
{"id": "2504.12898", "pdf": "https://arxiv.org/pdf/2504.12898.pdf", "abs": "https://arxiv.org/abs/2504.12898", "title": "Information Gain-Guided Causal Intervention for Autonomous Debiasing Large Language Models", "authors": ["Zhouhao Sun", "Xiao Ding", "Li Du", "Yunpeng Xu", "Yixuan Ma", "Yang Zhao", "Bing Qin", "Ting Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite significant progress, recent studies indicate that current large\nlanguage models (LLMs) may still capture dataset biases and utilize them during\ninference, leading to the poor generalizability of LLMs. However, due to the\ndiversity of dataset biases and the insufficient nature of bias suppression\nbased on in-context learning, the effectiveness of previous prior\nknowledge-based debiasing methods and in-context learning based automatic\ndebiasing methods is limited. To address these challenges, we explore the\ncombination of causal mechanisms with information theory and propose an\ninformation gain-guided causal intervention debiasing (ICD) framework. To\neliminate biases within the instruction-tuning dataset, it is essential to\nensure that these biases do not provide any additional information to predict\nthe answers, i.e., the information gain of these biases for predicting the\nanswers needs to be 0. Under this guidance, this framework utilizes a causal\nintervention-based data rewriting method to automatically and autonomously\nbalance the distribution of instruction-tuning dataset for reducing the\ninformation gain. Subsequently, it employs a standard supervised fine-tuning\nprocess to train LLMs on the debiased dataset. Experimental results show that\nICD can effectively debias LLM to improve its generalizability across different\ntasks."}
{"id": "2505.00675", "pdf": "https://arxiv.org/pdf/2505.00675.pdf", "abs": "https://arxiv.org/abs/2505.00675", "title": "Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future Directions", "authors": ["Yiming Du", "Wenyu Huang", "Danna Zheng", "Zhaowei Wang", "Sebastien Montella", "Mirella Lapata", "Kam-Fai Wong", "Jeff Z. Pan"], "categories": ["cs.CL"], "comment": null, "summary": "Memory is a fundamental component of AI systems, underpinning large language\nmodels (LLMs)-based agents. While prior surveys have focused on memory\napplications with LLMs (e.g., enabling personalized memory in conversational\nagents), they often overlook the atomic operations that underlie memory\ndynamics. In this survey, we first categorize memory representations into\nparametric and contextual forms, and then introduce six fundamental memory\noperations: Consolidation, Updating, Indexing, Forgetting, Retrieval, and\nCompression. We map these operations to the most relevant research topics\nacross long-term, long-context, parametric modification, and multi-source\nmemory. By reframing memory systems through the lens of atomic operations and\nrepresentation types, this survey provides a structured and dynamic perspective\non research, benchmark datasets, and tools related to memory in AI, clarifying\nthe functional interplay in LLMs based agents while outlining promising\ndirections for future research\\footnote{The paper list, datasets, methods and\ntools are available at\n\\href{https://github.com/Elvin-Yiming-Du/Survey_Memory_in_AI}{https://github.com/Elvin-Yiming-Du/Survey\\_Memory\\_in\\_AI}.}."}
{"id": "2505.05111", "pdf": "https://arxiv.org/pdf/2505.05111.pdf", "abs": "https://arxiv.org/abs/2505.05111", "title": "Unveiling Language-Specific Features in Large Language Models via Sparse Autoencoders", "authors": ["Boyi Deng", "Yu Wan", "Yidan Zhang", "Baosong Yang", "Fuli Feng"], "categories": ["cs.CL"], "comment": "ACL 2025 main", "summary": "The mechanisms behind multilingual capabilities in Large Language Models\n(LLMs) have been examined using neuron-based or internal-activation-based\nmethods. However, these methods often face challenges such as superposition and\nlayer-wise activation variance, which limit their reliability. Sparse\nAutoencoders (SAEs) offer a more nuanced analysis by decomposing the\nactivations of LLMs into a sparse linear combination of SAE features. We\nintroduce a novel metric to assess the monolinguality of features obtained from\nSAEs, discovering that some features are strongly related to specific\nlanguages. Additionally, we show that ablating these SAE features only\nsignificantly reduces abilities in one language of LLMs, leaving others almost\nunaffected. Interestingly, we find some languages have multiple synergistic SAE\nfeatures, and ablating them together yields greater improvement than ablating\nindividually. Moreover, we leverage these SAE-derived language-specific\nfeatures to enhance steering vectors, achieving control over the language\ngenerated by LLMs. The code is publicly available at\nhttps://github.com/Aatrox103/multilingual-llm-features."}
{"id": "2505.10554", "pdf": "https://arxiv.org/pdf/2505.10554.pdf", "abs": "https://arxiv.org/abs/2505.10554", "title": "Beyond 'Aha!': Toward Systematic Meta-Abilities Alignment in Large Reasoning Models", "authors": ["Zhiyuan Hu", "Yibo Wang", "Hanze Dong", "Yuhui Xu", "Amrita Saha", "Caiming Xiong", "Bryan Hooi", "Junnan Li"], "categories": ["cs.CL"], "comment": "In Progress", "summary": "Large reasoning models (LRMs) already possess a latent capacity for long\nchain-of-thought reasoning. Prior work has shown that outcome-based\nreinforcement learning (RL) can incidentally elicit advanced reasoning\nbehaviors such as self-correction, backtracking, and verification phenomena\noften referred to as the model's \"aha moment\". However, the timing and\nconsistency of these emergent behaviors remain unpredictable and\nuncontrollable, limiting the scalability and reliability of LRMs' reasoning\ncapabilities. To address these limitations, we move beyond reliance on prompts\nand coincidental \"aha moments\". Instead, we explicitly align models with three\nmeta-abilities: deduction, induction, and abduction, using automatically\ngenerated, self-verifiable tasks. Our three stage-pipeline individual\nalignment, parameter-space merging, and domain-specific reinforcement learning,\nboosting performance by over 10\\% relative to instruction-tuned baselines.\nFurthermore, domain-specific RL from the aligned checkpoint yields an\nadditional gain in performance ceiling for both 7B and 32B models across math,\ncoding, and science benchmarks, demonstrating that explicit meta-ability\nalignment offers a scalable and dependable foundation for reasoning. Code is\navailable at: https://github.com/zhiyuanhubj/Meta-Ability-Alignment"}
{"id": "2505.11484", "pdf": "https://arxiv.org/pdf/2505.11484.pdf", "abs": "https://arxiv.org/abs/2505.11484", "title": "SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning", "authors": ["Yige Xu", "Xu Guo", "Zhiwei Zeng", "Chunyan Miao"], "categories": ["cs.CL"], "comment": "14 pages", "summary": "Test-Time Scaling (TTS) refers to approaches that improve reasoning\nperformance by allocating extra computation during inference, without altering\nthe model's parameters. While existing TTS methods operate in a discrete token\nspace by generating more intermediate steps, recent studies in Coconut and\nSoftCoT have demonstrated that thinking in the continuous latent space can\nfurther enhance the reasoning performance. Such latent thoughts encode\ninformative thinking without the information loss associated with\nautoregressive token generation, sparking increased interest in\ncontinuous-space reasoning. Unlike discrete decoding, where repeated sampling\nenables exploring diverse reasoning paths, latent representations in continuous\nspace are fixed for a given input, which limits diverse exploration, as all\ndecoded paths originate from the same latent thought. To overcome this\nlimitation, we introduce SoftCoT++ to extend SoftCoT to the Test-Time Scaling\nparadigm by enabling diverse exploration of thinking paths. Specifically, we\nperturb latent thoughts via multiple specialized initial tokens and apply\ncontrastive learning to promote diversity among soft thought representations.\nExperiments across five reasoning benchmarks and two distinct LLM architectures\ndemonstrate that SoftCoT++ significantly boosts SoftCoT and also outperforms\nSoftCoT with self-consistency scaling. Moreover, it shows strong compatibility\nwith conventional scaling techniques such as self-consistency. Source code is\navailable at https://github.com/xuyige/SoftCoT."}
{"id": "2505.11807", "pdf": "https://arxiv.org/pdf/2505.11807.pdf", "abs": "https://arxiv.org/abs/2505.11807", "title": "Retrospex: Language Agent Meets Offline Reinforcement Learning Critic", "authors": ["Yufei Xiang", "Yiqun Shen", "Yeqin Zhang", "Cam-Tu Nguyen"], "categories": ["cs.CL", "cs.AI"], "comment": "17 pages, Published in EMNLP 2024 (Proceedings of the 2024 Conference\n  on Empirical Methods in Natural Language Processing)", "summary": "Large Language Models (LLMs) possess extensive knowledge and commonsense\nreasoning capabilities, making them valuable for creating powerful agents.\nHowever, existing LLM agent frameworks have not fully utilized past experiences\nfor improvement. This work introduces a new LLM-based agent framework called\nRetrospex, which addresses this challenge by analyzing past experiences in\ndepth. Unlike previous approaches, Retrospex does not directly integrate\nexperiences into the LLM's context. Instead, it combines the LLM's action\nlikelihood with action values estimated by a Reinforcement Learning (RL)\nCritic, which is trained on past experiences through an offline\n''retrospection'' process. Additionally, Retrospex employs a dynamic action\nrescoring mechanism that increases the importance of experience-based values\nfor tasks that require more interaction with the environment. We evaluate\nRetrospex in ScienceWorld, ALFWorld and Webshop environments, demonstrating its\nadvantages over strong, contemporary baselines."}
{"id": "2505.12299", "pdf": "https://arxiv.org/pdf/2505.12299.pdf", "abs": "https://arxiv.org/abs/2505.12299", "title": "Enhance Mobile Agents Thinking Process Via Iterative Preference Learning", "authors": ["Kun Huang", "Weikai Xu", "Yuxuan Liu", "Quandong Wang", "Pengzhi Gao", "Wei Liu", "Jian Luan", "Bin Wang", "Bo An"], "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 8 figures, 7 tables", "summary": "The Chain of Action-Planning Thoughts (CoaT) paradigm has been shown to\nimprove the reasoning performance of VLM-based mobile agents in GUI tasks.\nHowever, the scarcity of diverse CoaT trajectories limits the expressiveness\nand generalization ability of such agents. While self-training is commonly\nemployed to address data scarcity, existing approaches either overlook the\ncorrectness of intermediate reasoning steps or depend on expensive\nprocess-level annotations to construct process reward models (PRM). To address\nthe above problems, we propose an Iterative Preference Learning (IPL) that\nconstructs a CoaT-tree through interative sampling, scores leaf nodes using\nrule-based reward, and backpropagates feedback to derive Thinking-level Direct\nPreference Optimization (T-DPO) pairs. To prevent overfitting during warm-up\nsupervised fine-tuning, we further introduce a three-stage instruction\nevolution, which leverages GPT-4o to generate diverse Q\\&A pairs based on real\nmobile UI screenshots, enhancing both generality and layout understanding.\nExperiments on three standard Mobile GUI-agent benchmarks demonstrate that our\nagent MobileIPL outperforms strong baselines, including continual pretraining\nmodels such as OS-ATLAS and UI-TARS. It achieves state-of-the-art performance\nacross three standard Mobile GUI-Agents benchmarks and shows strong\ngeneralization to out-of-domain scenarios."}
{"id": "2505.12716", "pdf": "https://arxiv.org/pdf/2505.12716.pdf", "abs": "https://arxiv.org/abs/2505.12716", "title": "Shadow-FT: Tuning Instruct via Base", "authors": ["Taiqiang Wu", "Runming Yang", "Jiayi Li", "Pengfei Hu", "Ngai Wong", "Yujiu Yang"], "categories": ["cs.CL", "cs.AI"], "comment": "19 pages, 10 tables, 6 figures", "summary": "Large language models (LLMs) consistently benefit from further fine-tuning on\nvarious tasks. However, we observe that directly tuning the INSTRUCT (i.e.,\ninstruction tuned) models often leads to marginal improvements and even\nperformance degeneration. Notably, paired BASE models, the foundation for these\nINSTRUCT variants, contain highly similar weight values (i.e., less than 2% on\naverage for Llama 3.1 8B). Therefore, we propose a novel Shadow-FT framework to\ntune the INSTRUCT models by leveraging the corresponding BASE models. The key\ninsight is to fine-tune the BASE model, and then directly graft the learned\nweight updates to the INSTRUCT model. Our proposed Shadow-FT introduces no\nadditional parameters, is easy to implement, and significantly improves\nperformance. We conduct extensive experiments on tuning mainstream LLMs, such\nas Qwen 3 and Llama 3 series, and evaluate them across 19 benchmarks covering\ncoding, reasoning, and mathematical tasks. Experimental results demonstrate\nthat Shadow-FT consistently outperforms conventional full-parameter and\nparameter-efficient tuning approaches. Further analyses indicate that Shadow-FT\ncan be applied to multimodal large language models (MLLMs) and combined with\ndirect preference optimization (DPO). Codes and weights are available at\n\\href{https://github.com/wutaiqiang/Shadow-FT}{Github}."}
{"id": "2505.13089", "pdf": "https://arxiv.org/pdf/2505.13089.pdf", "abs": "https://arxiv.org/abs/2505.13089", "title": "Systematic Generalization in Language Models Scales with Information Entropy", "authors": ["Sondre Wold", "Lucas Georges Gabriel Charpentier", "Étienne Simon"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025: Findings", "summary": "Systematic generalization remains challenging for current language models,\nwhich are known to be both sensitive to semantically similar permutations of\nthe input and to struggle with known concepts presented in novel contexts.\nAlthough benchmarks exist for assessing compositional behavior, it is unclear\nhow to measure the difficulty of a systematic generalization problem. In this\nwork, we show how one aspect of systematic generalization can be described by\nthe entropy of the distribution of component parts in the training data. We\nformalize a framework for measuring entropy in a sequence-to-sequence task and\nfind that the performance of popular model architectures scales with the\nentropy. Our work connects systematic generalization to information efficiency,\nand our results indicate that success at high entropy can be achieved even\nwithout built-in priors, and that success at low entropy can serve as a target\nfor assessing progress towards robust systematic generalization."}
{"id": "2505.13975", "pdf": "https://arxiv.org/pdf/2505.13975.pdf", "abs": "https://arxiv.org/abs/2505.13975", "title": "DRP: Distilled Reasoning Pruning with Skill-aware Step Decomposition for Efficient Large Reasoning Models", "authors": ["Yuxuan Jiang", "Dawei Li", "Frank Ferraro"], "categories": ["cs.CL"], "comment": null, "summary": "While Large Reasoning Models (LRMs) have demonstrated success in complex\nreasoning tasks through long chain-of-thought (CoT) reasoning, their inference\noften involves excessively verbose reasoning traces, resulting in substantial\ninefficiency. To address this, we propose Distilled Reasoning Pruning (DRP), a\nhybrid framework that combines inference-time pruning with tuning-based\ndistillation, two widely used strategies for efficient reasoning. DRP uses a\nteacher model to perform skill-aware step decomposition and content pruning,\nand then distills the pruned reasoning paths into a student model, enabling it\nto reason both efficiently and accurately. Across several challenging\nmathematical reasoning datasets, we find that models trained with DRP achieve\nsubstantial improvements in token efficiency without sacrificing accuracy.\nSpecifically, DRP reduces average token usage on GSM8K from 917 to 328 while\nimproving accuracy from 91.7% to 94.1%, and achieves a 43% token reduction on\nAIME with no performance drop. Further analysis shows that aligning the\nreasoning structure of training CoTs with the student's reasoning capacity is\ncritical for effective knowledge transfer and performance gains."}
{"id": "2505.14107", "pdf": "https://arxiv.org/pdf/2505.14107.pdf", "abs": "https://arxiv.org/abs/2505.14107", "title": "DiagnosisArena: Benchmarking Diagnostic Reasoning for Large Language Models", "authors": ["Yakun Zhu", "Zhongzhen Huang", "Linjie Mu", "Yutong Huang", "Wei Nie", "Jiaji Liu", "Shaoting Zhang", "Pengfei Liu", "Xiaofan Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The emergence of groundbreaking large language models capable of performing\ncomplex reasoning tasks holds significant promise for addressing various\nscientific challenges, including those arising in complex clinical scenarios.\nTo enable their safe and effective deployment in real-world healthcare\nsettings, it is urgently necessary to benchmark the diagnostic capabilities of\ncurrent models systematically. Given the limitations of existing medical\nbenchmarks in evaluating advanced diagnostic reasoning, we present\nDiagnosisArena, a comprehensive and challenging benchmark designed to\nrigorously assess professional-level diagnostic competence. DiagnosisArena\nconsists of 1,113 pairs of segmented patient cases and corresponding diagnoses,\nspanning 28 medical specialties, deriving from clinical case reports published\nin 10 top-tier medical journals. The benchmark is developed through a\nmeticulous construction pipeline, involving multiple rounds of screening and\nreview by both AI systems and human experts, with thorough checks conducted to\nprevent data leakage. Our study reveals that even the most advanced reasoning\nmodels, o3-mini, o1, and DeepSeek-R1, achieve only 45.82%, 31.09%, and 17.79%\naccuracy, respectively. This finding highlights a significant generalization\nbottleneck in current large language models when faced with clinical diagnostic\nreasoning challenges. Through DiagnosisArena, we aim to drive further\nadvancements in AIs diagnostic reasoning capabilities, enabling more effective\nsolutions for real-world clinical diagnostic challenges. We provide the\nbenchmark and evaluation tools for further research and development\nhttps://github.com/SPIRAL-MED/DiagnosisArena."}
{"id": "2505.14832", "pdf": "https://arxiv.org/pdf/2505.14832.pdf", "abs": "https://arxiv.org/abs/2505.14832", "title": "SEPS: A Separability Measure for Robust Unlearning in LLMs", "authors": ["Wonje Jeung", "Sangyeon Yoon", "Albert No"], "categories": ["cs.CL"], "comment": "32 pages", "summary": "Machine unlearning aims to selectively remove targeted knowledge from Large\nLanguage Models (LLMs), ensuring they forget specified content while retaining\nessential information. Existing unlearning metrics assess whether a model\ncorrectly answers retain queries and rejects forget queries, but they fail to\ncapture real-world scenarios where forget queries rarely appear in isolation.\nIn fact, forget and retain queries often coexist within the same prompt, making\nmixed-query evaluation crucial.\n  We introduce SEPS, an evaluation framework that explicitly measures a model's\nability to both forget and retain information within a single prompt. Through\nextensive experiments across three benchmarks, we identify two key failure\nmodes in existing unlearning methods: (1) untargeted unlearning\nindiscriminately erases both forget and retain content once a forget query\nappears, and (2) targeted unlearning overfits to single-query scenarios,\nleading to catastrophic failures when handling multiple queries. To address\nthese issues, we propose Mixed Prompt (MP) unlearning, a strategy that\nintegrates both forget and retain queries into a unified training objective.\nOur approach significantly improves unlearning effectiveness, demonstrating\nrobustness even in complex settings with up to eight mixed forget and retain\nqueries in a single prompt."}
{"id": "2505.14874", "pdf": "https://arxiv.org/pdf/2505.14874.pdf", "abs": "https://arxiv.org/abs/2505.14874", "title": "Towards Inclusive ASR: Investigating Voice Conversion for Dysarthric Speech Recognition in Low-Resource Languages", "authors": ["Chin-Jou Li", "Eunjung Yeo", "Kwanghee Choi", "Paula Andrea Pérez-Toro", "Masao Someki", "Rohan Kumar Das", "Zhengjun Yue", "Juan Rafael Orozco-Arroyave", "Elmar Nöth", "David R. Mortensen"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "5 pages, 1 figure, Accepted to Interspeech 2025", "summary": "Automatic speech recognition (ASR) for dysarthric speech remains challenging\ndue to data scarcity, particularly in non-English languages. To address this,\nwe fine-tune a voice conversion model on English dysarthric speech (UASpeech)\nto encode both speaker characteristics and prosodic distortions, then apply it\nto convert healthy non-English speech (FLEURS) into non-English dysarthric-like\nspeech. The generated data is then used to fine-tune a multilingual ASR model,\nMassively Multilingual Speech (MMS), for improved dysarthric speech\nrecognition. Evaluation on PC-GITA (Spanish), EasyCall (Italian), and SSNCE\n(Tamil) demonstrates that VC with both speaker and prosody conversion\nsignificantly outperforms the off-the-shelf MMS performance and conventional\naugmentation techniques such as speed and tempo perturbation. Objective and\nsubjective analyses of the generated data further confirm that the generated\nspeech simulates dysarthric characteristics."}
{"id": "2505.15209", "pdf": "https://arxiv.org/pdf/2505.15209.pdf", "abs": "https://arxiv.org/abs/2505.15209", "title": "DUSK: Do Not Unlearn Shared Knowledge", "authors": ["Wonje Jeung", "Sangyeon Yoon", "Hyesoo Hong", "Soeun Kim", "Seungju Han", "Youngjae Yu", "Albert No"], "categories": ["cs.CL"], "comment": "21 pages", "summary": "Large language models (LLMs) are increasingly deployed in real-world\napplications, raising concerns about the unauthorized use of copyrighted or\nsensitive data. Machine unlearning aims to remove such 'forget' data while\npreserving utility and information from the 'retain' set. However, existing\nevaluations typically assume that forget and retain sets are fully disjoint,\noverlooking realistic scenarios where they share overlapping content. For\ninstance, a news article may need to be unlearned, even though the same event,\nsuch as an earthquake in Japan, is also described factually on Wikipedia.\nEffective unlearning should remove the specific phrasing of the news article\nwhile preserving publicly supported facts. In this paper, we introduce DUSK, a\nbenchmark designed to evaluate unlearning methods under realistic data overlap.\nDUSK constructs document sets that describe the same factual content in\ndifferent styles, with some shared information appearing across all sets and\nother content remaining unique to each. When one set is designated for\nunlearning, an ideal method should remove its unique content while preserving\nshared facts. We define seven evaluation metrics to assess whether unlearning\nmethods can achieve this selective removal. Our evaluation of nine recent\nunlearning methods reveals a key limitation: while most can remove\nsurface-level text, they often fail to erase deeper, context-specific knowledge\nwithout damaging shared content. We release DUSK as a public benchmark to\nsupport the development of more precise and reliable unlearning techniques for\nreal-world applications."}
{"id": "2505.15214", "pdf": "https://arxiv.org/pdf/2505.15214.pdf", "abs": "https://arxiv.org/abs/2505.15214", "title": "R-TOFU: Unlearning in Large Reasoning Models", "authors": ["Sangyeon Yoon", "Wonje Jeung", "Albert No"], "categories": ["cs.CL"], "comment": "19 pages", "summary": "Large Reasoning Models (LRMs) embed private or copyrighted information not\nonly in their final answers but also throughout multi-step chain-of-thought\n(CoT) traces, making reliable unlearning far more demanding than in standard\nLLMs. We introduce Reasoning-TOFU (R-TOFU), the first benchmark tailored to\nthis setting. R-TOFU augments existing unlearning tasks with realistic CoT\nannotations and provides step-wise metrics that expose residual knowledge\ninvisible to answer-level checks. Using R-TOFU, we carry out a comprehensive\ncomparison of gradient-based and preference-optimization baselines and show\nthat conventional answer-only objectives leave substantial forget traces in\nreasoning. We further propose Reasoned IDK, a preference-optimization variant\nthat preserves coherent yet inconclusive reasoning, achieving a stronger\nbalance between forgetting efficacy and model utility than earlier refusal\nstyles. Finally, we identify a failure mode: decoding variants such as\nZeroThink and LessThink can still reveal forgotten content despite seemingly\nsuccessful unlearning, emphasizing the need to evaluate models under diverse\ndecoding settings. Together, the benchmark, analysis, and new baseline\nestablish a systematic foundation for studying and improving unlearning in LRMs\nwhile preserving their reasoning capabilities."}
{"id": "2505.16522", "pdf": "https://arxiv.org/pdf/2505.16522.pdf", "abs": "https://arxiv.org/abs/2505.16522", "title": "Benchmarking and Pushing the Multi-Bias Elimination Boundary of LLMs via Causal Effect Estimation-guided Debiasing", "authors": ["Zhouhao Sun", "Zhiyuan Kan", "Xiao Ding", "Li Du", "Yang Zhao", "Bing Qin", "Ting Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite significant progress, recent studies have indicated that current\nlarge language models (LLMs) may still utilize bias during inference, leading\nto the poor generalizability of LLMs. Some benchmarks are proposed to\ninvestigate the generalizability of LLMs, with each piece of data typically\ncontaining one type of controlled bias. However, a single piece of data may\ncontain multiple types of biases in practical applications. To bridge this gap,\nwe propose a multi-bias benchmark where each piece of data contains five types\nof biases. The evaluations conducted on this benchmark reveal that the\nperformance of existing LLMs and debiasing methods is unsatisfying,\nhighlighting the challenge of eliminating multiple types of biases\nsimultaneously. To overcome this challenge, we propose a causal effect\nestimation-guided multi-bias elimination method (CMBE). This method first\nestimates the causal effect of multiple types of biases simultaneously.\nSubsequently, we eliminate the causal effect of biases from the total causal\neffect exerted by both the semantic information and biases during inference.\nExperimental results show that CMBE can effectively eliminate multiple types of\nbias simultaneously to enhance the generalizability of LLMs."}
{"id": "2505.16552", "pdf": "https://arxiv.org/pdf/2505.16552.pdf", "abs": "https://arxiv.org/abs/2505.16552", "title": "Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains", "authors": ["Wenhui Tan", "Jiaze Li", "Jianzhong Ju", "Zhenbo Luo", "Jian Luan", "Ruihua Song"], "categories": ["cs.CL"], "comment": "15 pages, 8 figures", "summary": "Large Language Models (LLMs) achieve superior performance through\nChain-of-Thought (CoT) reasoning, but these token-level reasoning chains are\ncomputationally expensive and inefficient. In this paper, we introduce\nCompressed Latent Reasoning (CoLaR), a novel framework that dynamically\ncompresses reasoning processes in latent space through a two-stage training\napproach. First, during supervised fine-tuning, CoLaR extends beyond next-token\nprediction by incorporating an auxiliary next compressed embedding prediction\nobjective. This process merges embeddings of consecutive tokens using a\ncompression factor randomly sampled from a predefined range, and trains a\nspecialized latent head to predict distributions of subsequent compressed\nembeddings. Second, we enhance CoLaR through reinforcement learning (RL) that\nleverages the latent head's non-deterministic nature to explore diverse\nreasoning paths and exploit more compact ones. This approach enables CoLaR to:\ni) perform reasoning at a dense latent level (i.e., silently), substantially\nreducing reasoning chain length, and ii) dynamically adjust reasoning speed at\ninference time by simply prompting the desired compression factor. Extensive\nexperiments across four mathematical reasoning datasets demonstrate that CoLaR\nachieves 14.1% higher accuracy than latent-based baseline methods at comparable\ncompression ratios, and reduces reasoning chain length by 53.3% with only 4.8%\nperformance degradation compared to explicit CoT method. Moreover, when applied\nto more challenging mathematical reasoning tasks, our RL-enhanced CoLaR\ndemonstrates performance gains of up to 5.4% while dramatically reducing latent\nreasoning chain length by 82.8%. The code and models will be released upon\nacceptance."}
{"id": "2505.16814", "pdf": "https://arxiv.org/pdf/2505.16814.pdf", "abs": "https://arxiv.org/abs/2505.16814", "title": "Does Synthetic Data Help Named Entity Recognition for Low-Resource Languages?", "authors": ["Gaurav Kamath", "Sowmya Vajjala"], "categories": ["cs.CL"], "comment": "pre-print", "summary": "Named Entity Recognition(NER) for low-resource languages aims to produce\nrobust systems for languages where there is limited labeled training data\navailable, and has been an area of increasing interest within NLP. Data\naugmentation for increasing the amount of low-resource labeled data is a common\npractice. In this paper, we explore the role of synthetic data in the context\nof multilingual, low-resource NER, considering 11 languages from diverse\nlanguage families. Our results suggest that synthetic data does in fact hold\npromise for low-resource language NER, though we see significant variation\nbetween languages."}
{"id": "2505.16900", "pdf": "https://arxiv.org/pdf/2505.16900.pdf", "abs": "https://arxiv.org/abs/2505.16900", "title": "Power-Law Decay Loss for Large Language Model Finetuning: Focusing on Information Sparsity to Enhance Generation Quality", "authors": ["Jintian Shao", "Yiming Cheng", "Hongyi Huang", "Jiayi Wu", "Beiwen Zhang", "Zhiyu Wu", "You Shan", "Mingkai Zheng"], "categories": ["cs.CL", "cs.LG"], "comment": "We are withdrawing this submission as the underlying experiment is\n  currently incomplete. We require additional time to gather more data and\n  supplement the existing findings to ensure a comprehensive and robust\n  presentation. We intend to resubmit once these additions are finalized", "summary": "During the finetuning stage of text generation tasks, standard cross-entropy\nloss treats all tokens equally. This can lead models to overemphasize\nhigh-frequency, low-information tokens, neglecting lower-frequency tokens\ncrucial for specificity and informativeness in generated content. This paper\nintroduces a novel loss function, Power-Law Decay Loss (PDL), specifically\ndesigned to optimize the finetuning process for text generation. The core\nmotivation for PDL stems from observations in information theory and\nlinguistics: the informativeness of a token is often inversely proportional to\nits frequency of occurrence. PDL re-weights the contribution of each token in\nthe standard cross-entropy loss based on its frequency in the training corpus,\nfollowing a power-law decay. Specifically, the weights for high-frequency\ntokens are reduced, while low-frequency, information-dense tokens are assigned\nhigher weights. This mechanism guides the model during finetuning to focus more\non learning and generating tokens that convey specific and unique information,\nthereby enhancing the quality, diversity, and informativeness of the generated\ntext. We theoretically elaborate on the motivation and construction of PDL and\ndiscuss its potential applications and advantages across various text\ngeneration finetuning tasks, such as abstractive summarization, dialogue\nsystems, and style transfer."}
{"id": "2505.17049", "pdf": "https://arxiv.org/pdf/2505.17049.pdf", "abs": "https://arxiv.org/abs/2505.17049", "title": "Gender and Positional Biases in LLM-Based Hiring Decisions: Evidence from Comparative CV/Résumé Evaluations", "authors": ["David Rozado"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "This study examines the behavior of Large Language Models (LLMs) when\nevaluating professional candidates based on their resumes or curricula vitae\n(CVs). In an experiment involving 22 leading LLMs, each model was\nsystematically given one job description along with a pair of\nprofession-matched CVs, one bearing a male first name, the other a female first\nname, and asked to select the more suitable candidate for the job. Each CV pair\nwas presented twice, with names swapped to ensure that any observed preferences\nin candidate selection stemmed from gendered names cues. Despite identical\nprofessional qualifications across genders, all LLMs consistently favored\nfemale-named candidates across 70 different professions. Adding an explicit\ngender field (male/female) to the CVs further increased the preference for\nfemale applicants. When gendered names were replaced with gender-neutral\nidentifiers \"Candidate A\" and \"Candidate B\", several models displayed a\npreference to select \"Candidate A\". Counterbalancing gender assignment between\nthese gender-neutral identifiers resulted in gender parity in candidate\nselection. When asked to rate CVs in isolation rather than compare pairs, LLMs\nassigned slightly higher average scores to female CVs overall, but the effect\nsize was negligible. Including preferred pronouns (he/him or she/her) next to a\ncandidate's name slightly increased the odds of the candidate being selected\nregardless of gender. Finally, most models exhibited a substantial positional\nbias to select the candidate listed first in the prompt. These findings\nunderscore the need for caution when deploying LLMs in high-stakes autonomous\ndecision-making contexts and raise doubts about whether LLMs consistently apply\nprincipled reasoning."}
{"id": "2505.17117", "pdf": "https://arxiv.org/pdf/2505.17117.pdf", "abs": "https://arxiv.org/abs/2505.17117", "title": "From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning", "authors": ["Chen Shani", "Dan Jurafsky", "Yann LeCun", "Ravid Shwartz-Ziv"], "categories": ["cs.CL", "cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "Humans organize knowledge into compact categories through semantic\ncompression by mapping diverse instances to abstract representations while\npreserving meaning (e.g., robin and blue jay are both birds; most birds can\nfly). These concepts reflect a trade-off between expressive fidelity and\nrepresentational simplicity. Large Language Models (LLMs) demonstrate\nremarkable linguistic abilities, yet whether their internal representations\nstrike a human-like trade-off between compression and semantic fidelity is\nunclear. We introduce a novel information-theoretic framework, drawing from\nRate-Distortion Theory and the Information Bottleneck principle, to\nquantitatively compare these strategies. Analyzing token embeddings from a\ndiverse suite of LLMs against seminal human categorization benchmarks, we\nuncover key divergences. While LLMs form broad conceptual categories that align\nwith human judgment, they struggle to capture the fine-grained semantic\ndistinctions crucial for human understanding. More fundamentally, LLMs\ndemonstrate a strong bias towards aggressive statistical compression, whereas\nhuman conceptual systems appear to prioritize adaptive nuance and contextual\nrichness, even if this results in lower compressional efficiency by our\nmeasures. These findings illuminate critical differences between current AI and\nhuman cognitive architectures, guiding pathways toward LLMs with more\nhuman-aligned conceptual representations."}
{"id": "2505.17266", "pdf": "https://arxiv.org/pdf/2505.17266.pdf", "abs": "https://arxiv.org/abs/2505.17266", "title": "Select2Reason: Efficient Instruction-Tuning Data Selection for Long-CoT Reasoning", "authors": ["Cehao Yang", "Xueyuan Lin", "Chengjin Xu", "Xuhui Jiang", "Xiaojun Wu", "Honghao Liu", "Hui Xiong", "Jian Guo"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "A practical approach to activate long chain-of-thoughts reasoning ability in\npre-trained large language models is to perform supervised fine-tuning on\ninstruction datasets synthesized by strong Large Reasoning Models such as\nDeepSeek-R1, offering a cost-effective alternative to reinforcement learning.\nHowever, large-scale instruction sets with more than 100k samples incur\nsignificant training overhead, while effective strategies for automatic\nlong-CoT instruction selection still remain unexplored. In this work, we\npropose Select2Reason, a novel and efficient instruction-tuning data selection\nframework for long-CoT reasoning. From the perspective of emergence of\nrethinking behaviors like self-correction and backtracking, we investigate\ncommon metrics that may determine the quality of long-CoT reasoning\ninstructions. Select2Reason leverages a quantifier to estimate difficulty of\nquestion and jointly incorporates a reasoning trace length-based heuristic\nthrough a weighted scheme for ranking to prioritize high-utility examples.\nEmpirical results on OpenR1-Math-220k demonstrate that fine-tuning LLM on only\n10% of the data selected by Select2Reason achieves performance competitive with\nor superior to full-data tuning and open-source baseline OpenR1-Qwen-7B across\nthree competition-level and six comprehensive mathematical benchmarks. Further\nexperiments highlight the scalability in varying data size, efficiency during\ninference, and its adaptability to other instruction pools with minimal cost."}
{"id": "2505.17667", "pdf": "https://arxiv.org/pdf/2505.17667.pdf", "abs": "https://arxiv.org/abs/2505.17667", "title": "QwenLong-L1: Towards Long-Context Large Reasoning Models with Reinforcement Learning", "authors": ["Fanqi Wan", "Weizhou Shen", "Shengyi Liao", "Yingcheng Shi", "Chenliang Li", "Ziyi Yang", "Ji Zhang", "Fei Huang", "Jingren Zhou", "Ming Yan"], "categories": ["cs.CL"], "comment": "Technical Report", "summary": "Recent large reasoning models (LRMs) have demonstrated strong reasoning\ncapabilities through reinforcement learning (RL). These improvements have\nprimarily been observed within the short-context reasoning tasks. In contrast,\nextending LRMs to effectively process and reason on long-context inputs via RL\nremains a critical unsolved challenge. To bridge this gap, we first formalize\nthe paradigm of long-context reasoning RL, and identify key challenges in\nsuboptimal training efficiency and unstable optimization process. To address\nthese issues, we propose QwenLong-L1, a framework that adapts short-context\nLRMs to long-context scenarios via progressive context scaling. Specifically,\nwe utilize a warm-up supervised fine-tuning (SFT) stage to establish a robust\ninitial policy, followed by a curriculum-guided phased RL technique to\nstabilize the policy evolution, and enhanced with a difficulty-aware\nretrospective sampling strategy to incentivize the policy exploration.\nExperiments on seven long-context document question-answering benchmarks\ndemonstrate that QwenLong-L1-32B outperforms flagship LRMs like OpenAI-o3-mini\nand Qwen3-235B-A22B, achieving performance on par with\nClaude-3.7-Sonnet-Thinking, demonstrating leading performance among\nstate-of-the-art LRMs. This work advances the development of practical\nlong-context LRMs capable of robust reasoning across information-intensive\nenvironments."}
{"id": "2505.18092", "pdf": "https://arxiv.org/pdf/2505.18092.pdf", "abs": "https://arxiv.org/abs/2505.18092", "title": "QwenLong-CPRS: Towards $\\infty$-LLMs with Dynamic Context Optimization", "authors": ["Weizhou Shen", "Chenliang Li", "Fanqi Wan", "Shengyi Liao", "Shaopeng Lai", "Bo Zhang", "Yingcheng Shi", "Yuning Wu", "Gang Fu", "Zhansheng Li", "Bin Yang", "Ji Zhang", "Fei Huang", "Jingren Zhou", "Ming Yan"], "categories": ["cs.CL"], "comment": null, "summary": "This technical report presents QwenLong-CPRS, a context compression framework\ndesigned for explicit long-context optimization, addressing prohibitive\ncomputation overhead during the prefill stage and the \"lost in the middle\"\nperformance degradation of large language models (LLMs) during long sequence\nprocessing. Implemented through a novel dynamic context optimization mechanism,\nQwenLong-CPRS enables multi-granularity context compression guided by natural\nlanguage instructions, achieving both efficiency gains and improved\nperformance.\n  Evolved from the Qwen architecture series, QwenLong-CPRS introduces four key\ninnovations: (1) Natural language-guided dynamic optimization, (2)\nBidirectional reasoning layers for enhanced boundary awareness, (3) Token\ncritic mechanisms with language modeling heads, and (4) Window-parallel\ninference.\n  Comprehensive evaluations across five benchmarks (4K-2M word contexts)\ndemonstrate QwenLong-CPRS's threefold effectiveness: (1) Consistent superiority\nover other context management methods like RAG and sparse attention in both\naccuracy and efficiency. (2) Architecture-agnostic integration with all\nflagship LLMs, including GPT-4o, Gemini2.0-pro, Claude3.7-sonnet, DeepSeek-v3,\nand Qwen2.5-max, achieves 21.59$\\times$ context compression alongside\n19.15-point average performance gains; (3) Deployed with Qwen2.5-32B-Instruct,\nQwenLong-CPRS surpasses leading proprietary LLMs by 4.85 and 10.88 points on\nRuler-128K and InfiniteBench, establishing new SOTA performance."}
{"id": "2505.18405", "pdf": "https://arxiv.org/pdf/2505.18405.pdf", "abs": "https://arxiv.org/abs/2505.18405", "title": "RaDeR: Reasoning-aware Dense Retrieval Models", "authors": ["Debrup Das", "Sam O' Nuallain", "Razieh Rahimi"], "categories": ["cs.CL", "cs.IR"], "comment": "26 pages", "summary": "We propose RaDeR, a set of reasoning-based dense retrieval models trained\nwith data derived from mathematical problem solving using large language models\n(LLMs). Our method leverages retrieval-augmented reasoning trajectories of an\nLLM and self-reflective relevance evaluation, enabling the creation of both\ndiverse and hard-negative samples for reasoning-intensive relevance. RaDeR\nretrievers, trained for mathematical reasoning, effectively generalize to\ndiverse reasoning tasks in the BRIGHT and RAR-b benchmarks, consistently\noutperforming strong baselines in overall performance. Notably, RaDeR achieves\nsignificantly higher performance than baselines on the Math and Coding splits.\nIn addition, RaDeR presents the first dense retriever that outperforms BM25\nwhen queries are Chain-of-Thought reasoning steps, underscoring the critical\nrole of reasoning-based retrieval to augment reasoning language models.\nFurthermore, RaDeR achieves comparable or superior performance while using only\n2.5% of the training data used by the concurrent work REASONIR, highlighting\nthe quality of our synthesized training data."}
{"id": "2505.18596", "pdf": "https://arxiv.org/pdf/2505.18596.pdf", "abs": "https://arxiv.org/abs/2505.18596", "title": "Debate-to-Detect: Reformulating Misinformation Detection as a Real-World Debate with Large Language Models", "authors": ["Chen Han", "Wenzhen Zheng", "Xijin Tang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The proliferation of misinformation in digital platforms reveals the\nlimitations of traditional detection methods, which mostly rely on static\nclassification and fail to capture the intricate process of real-world\nfact-checking. Despite advancements in Large Language Models (LLMs) that\nenhance automated reasoning, their application to misinformation detection\nremains hindered by issues of logical inconsistency and superficial\nverification. In response, we introduce Debate-to-Detect (D2D), a novel\nMulti-Agent Debate (MAD) framework that reformulates misinformation detection\nas a structured adversarial debate. Inspired by fact-checking workflows, D2D\nassigns domain-specific profiles to each agent and orchestrates a five-stage\ndebate process, including Opening Statement, Rebuttal, Free Debate, Closing\nStatement, and Judgment. To transcend traditional binary classification, D2D\nintroduces a multi-dimensional evaluation mechanism that assesses each claim\nacross five distinct dimensions: Factuality, Source Reliability, Reasoning\nQuality, Clarity, and Ethics. Experiments with GPT-4o on two fakenews datasets\ndemonstrate significant improvements over baseline methods, and the case study\nhighlight D2D's capability to iteratively refine evidence while improving\ndecision transparency, representing a substantial advancement towards robust\nand interpretable misinformation detection. The code will be open-sourced in a\nfuture release."}
{"id": "2505.18609", "pdf": "https://arxiv.org/pdf/2505.18609.pdf", "abs": "https://arxiv.org/abs/2505.18609", "title": "RASMALAI: Resources for Adaptive Speech Modeling in Indian Languages with Accents and Intonations", "authors": ["Ashwin Sankar", "Yoach Lacombe", "Sherry Thomas", "Praveen Srinivasa Varadhan", "Sanchit Gandhi", "Mitesh M Khapra"], "categories": ["cs.CL"], "comment": "Accepted at Interspeech 2025", "summary": "We introduce RASMALAI, a large-scale speech dataset with rich text\ndescriptions, designed to advance controllable and expressive text-to-speech\n(TTS) synthesis for 23 Indian languages and English. It comprises 13,000 hours\nof speech and 24 million text-description annotations with fine-grained\nattributes like speaker identity, accent, emotion, style, and background\nconditions. Using RASMALAI, we develop IndicParlerTTS, the first open-source,\ntext-description-guided TTS for Indian languages. Systematic evaluation\ndemonstrates its ability to generate high-quality speech for named speakers,\nreliably follow text descriptions and accurately synthesize specified\nattributes. Additionally, it effectively transfers expressive characteristics\nboth within and across languages. IndicParlerTTS consistently achieves strong\nperformance across these evaluations, setting a new standard for controllable\nmultilingual expressive speech synthesis in Indian languages."}
{"id": "2505.18973", "pdf": "https://arxiv.org/pdf/2505.18973.pdf", "abs": "https://arxiv.org/abs/2505.18973", "title": "Hierarchical Mamba Meets Hyperbolic Geometry: A New Paradigm for Structured Language Embeddings", "authors": ["Sarang Patil", "Ashish Parmanand Pandey", "Ioannis Koutis", "Mengjia Xu"], "categories": ["cs.CL", "cs.LG"], "comment": "10 pages, 3 figures", "summary": "Selective state-space models have achieved great success in long-sequence\nmodeling. However, their capacity for language representation, especially in\ncomplex hierarchical reasoning tasks, remains underexplored. Most large\nlanguage models rely on flat Euclidean embeddings, limiting their ability to\ncapture latent hierarchies. To address this limitation, we propose Hierarchical\nMamba (HiM), integrating efficient Mamba2 with exponential growth and curved\nnature of hyperbolic geometry to learn hierarchy-aware language embeddings for\ndeeper linguistic understanding. Mamba2-processed sequences are projected to\nthe Poincare ball (via tangent-based mapping) or Lorentzian manifold (via\ncosine and sine-based mapping) with \"learnable\" curvature, optimized with a\ncombined hyperbolic loss. Our HiM model facilitates the capture of relational\ndistances across varying hierarchical levels, enabling effective long-range\nreasoning. This makes it well-suited for tasks like mixed-hop prediction and\nmulti-hop inference in hierarchical classification. We evaluated our HiM with\nfour linguistic and medical datasets for mixed-hop prediction and multi-hop\ninference tasks. Experimental results demonstrated that: 1) Both HiM models\neffectively capture hierarchical relationships for four ontological datasets,\nsurpassing Euclidean baselines. 2) HiM-Poincare captures fine-grained semantic\ndistinctions with higher h-norms, while HiM-Lorentz provides more stable,\ncompact, and hierarchy-preserving embeddings favoring robustness over detail."}
{"id": "2505.19184", "pdf": "https://arxiv.org/pdf/2505.19184.pdf", "abs": "https://arxiv.org/abs/2505.19184", "title": "When Two LLMs Debate, Both Think They'll Win", "authors": ["Pradyumna Shyama Prasad", "Minh Nhat Nguyen"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Can LLMs accurately adjust their confidence when facing opposition? Building\non previous studies measuring calibration on static fact-based\nquestion-answering tasks, we evaluate Large Language Models (LLMs) in a\ndynamic, adversarial debate setting, uniquely combining two realistic factors:\n(a) a multi-turn format requiring models to update beliefs as new information\nemerges, and (b) a zero-sum structure to control for task-related uncertainty,\nsince mutual high-confidence claims imply systematic overconfidence. We\norganized 60 three-round policy debates among ten state-of-the-art LLMs, with\nmodels privately rating their confidence (0-100) in winning after each round.\nWe observed five concerning patterns: (1) Systematic overconfidence: models\nbegan debates with average initial confidence of 72.9% vs. a rational 50%\nbaseline. (2) Confidence escalation: rather than reducing confidence as debates\nprogressed, debaters increased their win probabilities, averaging 83% by the\nfinal round. (3) Mutual overestimation: in 61.7% of debates, both sides\nsimultaneously claimed >=75% probability of victory, a logical impossibility.\n(4) Persistent self-debate bias: models debating identical copies increased\nconfidence from 64.1% to 75.2%; even when explicitly informed their chance of\nwinning was exactly 50%, confidence still rose (from 50.0% to 57.1%). (5)\nMisaligned private reasoning: models' private scratchpad thoughts sometimes\ndiffered from their public confidence ratings, raising concerns about\nfaithfulness of chain-of-thought reasoning. These results suggest LLMs lack the\nability to accurately self-assess or update their beliefs in dynamic,\nmulti-turn tasks; a major concern as LLM outputs are deployed without careful\nreview in assistant roles or agentic settings."}
{"id": "2505.19286", "pdf": "https://arxiv.org/pdf/2505.19286.pdf", "abs": "https://arxiv.org/abs/2505.19286", "title": "A Graph Perspective to Probe Structural Patterns of Knowledge in Large Language Models", "authors": ["Utkarsh Sahu", "Zhisheng Qi", "Yongjia Lei", "Ryan A. Rossi", "Franck Dernoncourt", "Nesreen K. Ahmed", "Mahantesh M Halappanavar", "Yao Ma", "Yu Wang"], "categories": ["cs.CL", "cs.LG", "cs.SI"], "comment": null, "summary": "Large language models have been extensively studied as neural knowledge bases\nfor their knowledge access, editability, reasoning, and explainability.\nHowever, few works focus on the structural patterns of their knowledge.\nMotivated by this gap, we investigate these structural patterns from a graph\nperspective. We quantify the knowledge of LLMs at both the triplet and entity\nlevels, and analyze how it relates to graph structural properties such as node\ndegree. Furthermore, we uncover the knowledge homophily, where topologically\nclose entities exhibit similar levels of knowledgeability, which further\nmotivates us to develop graph machine learning models to estimate entity\nknowledge based on its local neighbors. This model further enables valuable\nknowledge checking by selecting triplets less known to LLMs. Empirical results\nshow that using selected triplets for fine-tuning leads to superior\nperformance."}
{"id": "2505.19484", "pdf": "https://arxiv.org/pdf/2505.19484.pdf", "abs": "https://arxiv.org/abs/2505.19484", "title": "CulFiT: A Fine-grained Cultural-aware LLM Training Paradigm via Multilingual Critique Data Synthesis", "authors": ["Ruixiang Feng", "Shen Gao", "Xiuying Chen", "Lisi Chen", "Shuo Shang"], "categories": ["cs.CL"], "comment": "accepted by ACL 2025", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, yet they often exhibit a specific cultural biases, neglecting\nthe values and linguistic diversity of low-resource regions. This cultural bias\nnot only undermines universal equality, but also risks reinforcing stereotypes\nand perpetuating discrimination. To address this, we propose CulFiT, a novel\nculturally-aware training paradigm that leverages multilingual data and\nfine-grained reward modeling to enhance cultural sensitivity and inclusivity.\nOur approach synthesizes diverse cultural-related questions, constructs\ncritique data in culturally relevant languages, and employs fine-grained\nrewards to decompose cultural texts into verifiable knowledge units for\ninterpretable evaluation. We also introduce GlobalCultureQA, a multilingual\nopen-ended question-answering dataset designed to evaluate culturally-aware\nresponses in a global context. Extensive experiments on three existing\nbenchmarks and our GlobalCultureQA demonstrate that CulFiT achieves\nstate-of-the-art open-source model performance in cultural alignment and\ngeneral reasoning."}
{"id": "2505.19515", "pdf": "https://arxiv.org/pdf/2505.19515.pdf", "abs": "https://arxiv.org/abs/2505.19515", "title": "Analyzing Biases in Political Dialogue: Tagging U.S. Presidential Debates with an Extended DAMSL Framework", "authors": ["Lavanya Prahallad", "Radhika Mamidi"], "categories": ["cs.CL"], "comment": "8 pages", "summary": "We present a critical discourse analysis of the 2024 U.S. presidential\ndebates, examining Donald Trump's rhetorical strategies in his interactions\nwith Joe Biden and Kamala Harris. We introduce a novel annotation framework,\nBEADS (Bias Enriched Annotation for Dialogue Structure), which systematically\nextends the DAMSL framework to capture bias driven and adversarial discourse\nfeatures in political communication. BEADS includes a domain and language\nagnostic set of tags that model ideological framing, emotional appeals, and\nconfrontational tactics. Our methodology compares detailed human annotation\nwith zero shot ChatGPT assisted tagging on verified transcripts from the Trump\nand Biden (19,219 words) and Trump and Harris (18,123 words) debates. Our\nanalysis shows that Trump consistently dominated in key categories: Challenge\nand Adversarial Exchanges, Selective Emphasis, Appeal to Fear, Political Bias,\nand Perceived Dismissiveness. These findings underscore his use of emotionally\ncharged and adversarial rhetoric to control the narrative and influence\naudience perception. In this work, we establish BEADS as a scalable and\nreproducible framework for critical discourse analysis across languages,\ndomains, and political contexts."}
{"id": "2505.19528", "pdf": "https://arxiv.org/pdf/2505.19528.pdf", "abs": "https://arxiv.org/abs/2505.19528", "title": "AmpleHate: Amplifying the Attention for Versatile Implicit Hate Detection", "authors": ["Yejin Lee", "Joonghyuk Hahn", "Hyeseon Ahn", "Yo-Sub Han"], "categories": ["cs.CL", "cs.AI", "cs.CY", "68T50", "I.2.7"], "comment": "13 pages, 4 figures, Under Review", "summary": "Implicit hate speech detection is challenging due to its subtlety and\nreliance on contextual interpretation rather than explicit offensive words.\nCurrent approaches rely on contrastive learning, which are shown to be\neffective on distinguishing hate and non-hate sentences. Humans, however,\ndetect implicit hate speech by first identifying specific targets within the\ntext and subsequently interpreting how these target relate to their surrounding\ncontext. Motivated by this reasoning process, we propose AmpleHate, a novel\napproach designed to mirror human inference for implicit hate detection.\nAmpleHate identifies explicit target using a pretrained Named Entity\nRecognition model and capture implicit target information via [CLS] tokens. It\ncomputes attention-based relationships between explicit, implicit targets and\nsentence context and then, directly injects these relational vectors into the\nfinal sentence representation. This amplifies the critical signals of\ntarget-context relations for determining implicit hate. Experiments demonstrate\nthat AmpleHate achieves state-of-the-art performance, outperforming contrastive\nlearning baselines by an average of 82.14% and achieve faster convergence.\nQualitative analyses further reveal that attention patterns produced by\nAmpleHate closely align with human judgement, underscoring its interpretability\nand robustness."}
{"id": "2505.19586", "pdf": "https://arxiv.org/pdf/2505.19586.pdf", "abs": "https://arxiv.org/abs/2505.19586", "title": "TailorKV: A Hybrid Framework for Long-Context Inference via Tailored KV Cache Optimization", "authors": ["Dingyu Yao", "Bowen Shen", "Zheng Lin", "Wei Liu", "Jian Luan", "Bin Wang", "Weiping Wang"], "categories": ["cs.CL"], "comment": null, "summary": "The Key-Value (KV) cache in generative large language models (LLMs)\nintroduces substantial memory overhead. Existing works mitigate this burden by\noffloading or compressing the KV cache. However, loading the entire cache\nincurs significant latency due to PCIe bandwidth bottlenecks in CPU-GPU\ncommunication, while aggressive compression causes notable performance\ndegradation. We identify that certain layers in the LLM need to maintain global\ninformation and are unsuitable for selective loading. In contrast, other layers\nprimarily focus on a few tokens with dominant activations that potentially\nincur substantial quantization error. This observation leads to a key insight\nthat loading dominant tokens and quantizing all tokens can complement each\nother. Building on this insight, we propose a hybrid compression method,\nTailorKV, which seamlessly integrates quantization and offloading. TailorKV\ndevelops an inference framework along with a hardware-friendly implementation\nthat leverages these complementary characteristics. Extensive long-context\nevaluations exhibit that TailorKV achieves nearly lossless performance under\naggressive compression settings, outperforming the state-of-the-art.\nParticularly, the Llama-3.1-8B with 128k context can be served within a single\nRTX 3090 GPU, reaching 82 ms per token during decoding."}
{"id": "2505.19628", "pdf": "https://arxiv.org/pdf/2505.19628.pdf", "abs": "https://arxiv.org/abs/2505.19628", "title": "HomeBench: Evaluating LLMs in Smart Homes with Valid and Invalid Instructions Across Single and Multiple Devices", "authors": ["Silin Li", "Yuhang Guo", "Jiashu Yao", "Zeming Liu", "Haifeng Wang"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 Main", "summary": "Large language models (LLMs) have the potential to revolutionize smart home\nassistants by enhancing their ability to accurately understand user needs and\nrespond appropriately, which is extremely beneficial for building a smarter\nhome environment. While recent studies have explored integrating LLMs into\nsmart home systems, they primarily focus on handling straightforward, valid\nsingle-device operation instructions. However, real-world scenarios are far\nmore complex and often involve users issuing invalid instructions or\ncontrolling multiple devices simultaneously. These have two main challenges:\nLLMs must accurately identify and rectify errors in user instructions and\nexecute multiple user instructions perfectly. To address these challenges and\nadvance the development of LLM-based smart home assistants, we introduce\nHomeBench, the first smart home dataset with valid and invalid instructions\nacross single and multiple devices in this paper. We have experimental results\non 13 distinct LLMs; e.g., GPT-4o achieves only a 0.0% success rate in the\nscenario of invalid multi-device instructions, revealing that the existing\nstate-of-the-art LLMs still cannot perform well in this situation even with the\nhelp of in-context learning, retrieval-augmented generation, and fine-tuning.\nOur code and dataset are publicly available at\nhttps://github.com/BITHLP/HomeBench."}
{"id": "2505.19634", "pdf": "https://arxiv.org/pdf/2505.19634.pdf", "abs": "https://arxiv.org/abs/2505.19634", "title": "Faster and Better LLMs via Latency-Aware Test-Time Scaling", "authors": ["Zili Wang", "Tianyu Zhang", "Haoli Bai", "Lu Hou", "Xianzhi Yu", "Wulong Liu", "Shiming Xiang", "Lei Zhu"], "categories": ["cs.CL"], "comment": null, "summary": "Test-Time Scaling (TTS) has proven effective in improving the performance of\nLarge Language Models (LLMs) during inference. However, existing research has\noverlooked the efficiency of TTS from a latency-sensitive perspective. Through\na latency-aware evaluation of representative TTS methods, we demonstrate that a\ncompute-optimal TTS does not always result in the lowest latency in scenarios\nwhere latency is critical. To address this gap and achieve latency-optimal TTS,\nwe propose two key approaches by optimizing the concurrency configurations: (1)\nbranch-wise parallelism, which leverages multiple concurrent inference\nbranches, and (2) sequence-wise parallelism, enabled by speculative decoding.\nBy integrating these two approaches and allocating computational resources\nproperly to each, our latency-optimal TTS enables a 32B model to reach 82.3%\naccuracy on MATH-500 within 1 minute and a smaller 3B model to achieve 72.4%\nwithin 10 seconds. Our work emphasizes the importance of latency-aware TTS and\ndemonstrates its ability to deliver both speed and accuracy in\nlatency-sensitive scenarios."}
{"id": "2505.19743", "pdf": "https://arxiv.org/pdf/2505.19743.pdf", "abs": "https://arxiv.org/abs/2505.19743", "title": "Token-level Accept or Reject: A Micro Alignment Approach for Large Language Models", "authors": ["Yang Zhang", "Yu Yu", "Bo Tang", "Yu Zhu", "Chuxiong Sun", "Wenqiang Wei", "Jie Hu", "Zipeng Xie", "Zhiyu Li", "Feiyu Xiong", "Edward Chung"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to 34th International Joint Conference on Artificial\n  Intelligence (IJCAI 2025)", "summary": "With the rapid development of Large Language Models (LLMs), aligning these\nmodels with human preferences and values is critical to ensuring ethical and\nsafe applications. However, existing alignment techniques such as RLHF or DPO\noften require direct fine-tuning on LLMs with billions of parameters, resulting\nin substantial computational costs and inefficiencies. To address this, we\npropose Micro token-level Accept-Reject Aligning (MARA) approach designed to\noperate independently of the language models. MARA simplifies the alignment\nprocess by decomposing sentence-level preference learning into token-level\nbinary classification, where a compact three-layer fully-connected network\ndetermines whether candidate tokens are \"Accepted\" or \"Rejected\" as part of the\nresponse. Extensive experiments across seven different LLMs and three\nopen-source datasets show that MARA achieves significant improvements in\nalignment performance while reducing computational costs. The source code and\nimplementation details are publicly available at\nhttps://github.com/IAAR-Shanghai/MARA, and the trained models are released at\nhttps://huggingface.co/IAAR-Shanghai/MARA_AGENTS."}
{"id": "2505.20101", "pdf": "https://arxiv.org/pdf/2505.20101.pdf", "abs": "https://arxiv.org/abs/2505.20101", "title": "Adaptive Deep Reasoning: Triggering Deep Thinking When Needed", "authors": ["Yunhao Wang", "Yuhao Zhang", "Tinghao Yu", "Can Xu", "Feng Zhang", "Fengzong Lian"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown impressive capabilities in handling\ncomplex tasks through long-chain reasoning. However, the extensive reasoning\nsteps involved can significantly increase computational costs, posing\nchallenges for real-world deployment. Recent efforts have focused on optimizing\nreasoning efficiency by shortening the Chain-of-Thought (CoT) reasoning\nprocesses through various approaches, such as length-aware prompt engineering,\nsupervised fine-tuning on CoT data with variable lengths, and reinforcement\nlearning with length penalties. Although these methods effectively reduce\nreasoning length, they still necessitate an initial reasoning phase. More\nrecent approaches have attempted to integrate long-chain and short-chain\nreasoning abilities into a single model, yet they still rely on manual control\nto toggle between short and long CoT. In this work, we propose a novel approach\nthat autonomously switches between short and long reasoning chains based on\nproblem complexity. Our method begins with supervised fine-tuning of the base\nmodel to equip both long-chain and short-chain reasoning abilities. We then\nemploy reinforcement learning to further balance short and long CoT generation\nwhile maintaining accuracy through two key strategies: first, integrating\nreinforcement learning with a long-short adaptive group-wise reward strategy to\nassess prompt complexity and provide corresponding rewards; second,\nimplementing a logit-based reasoning mode switching loss to optimize the\nmodel's initial token choice, thereby guiding the selection of the reasoning\ntype. Evaluations on mathematical datasets demonstrate that our model can\ndynamically switch between long-chain and short-chain reasoning modes without\nsubstantially sacrificing performance. This advancement enhances the\npracticality of reasoning in large language models for real-world applications."}
{"id": "2505.20118", "pdf": "https://arxiv.org/pdf/2505.20118.pdf", "abs": "https://arxiv.org/abs/2505.20118", "title": "TrojanStego: Your Language Model Can Secretly Be A Steganographic Privacy Leaking Agent", "authors": ["Dominik Meier", "Jan Philip Wahle", "Paul Röttger", "Terry Ruas", "Bela Gipp"], "categories": ["cs.CL", "cs.CR"], "comment": "9 pages, 5 figures", "summary": "As large language models (LLMs) become integrated into sensitive workflows,\nconcerns grow over their potential to leak confidential information. We propose\nTrojanStego, a novel threat model in which an adversary fine-tunes an LLM to\nembed sensitive context information into natural-looking outputs via linguistic\nsteganography, without requiring explicit control over inference inputs. We\nintroduce a taxonomy outlining risk factors for compromised LLMs, and use it to\nevaluate the risk profile of the threat. To implement TrojanStego, we propose a\npractical encoding scheme based on vocabulary partitioning learnable by LLMs\nvia fine-tuning. Experimental results show that compromised models reliably\ntransmit 32-bit secrets with 87% accuracy on held-out prompts, reaching over\n97% accuracy using majority voting across three generations. Further, they\nmaintain high utility, can evade human detection, and preserve coherence. These\nresults highlight a new class of LLM data exfiltration attacks that are\npassive, covert, practical, and dangerous."}
{"id": "2505.20195", "pdf": "https://arxiv.org/pdf/2505.20195.pdf", "abs": "https://arxiv.org/abs/2505.20195", "title": "Monocle: Hybrid Local-Global In-Context Evaluation for Long-Text Generation with Uncertainty-Based Active Learning", "authors": ["Xiaorong Wang", "Ting Yang", "Zhu Zhang", "Shuo Wang", "Zihan Zhou", "Liner Yang", "Zhiyuan Liu", "Maosong Sun"], "categories": ["cs.CL"], "comment": null, "summary": "Assessing the quality of long-form, model-generated text is challenging, even\nwith advanced LLM-as-a-Judge methods, due to performance degradation as input\nlength increases. To address this issue, we propose a divide-and-conquer\napproach, which breaks down the comprehensive evaluation task into a series of\nlocalized scoring tasks, followed by a final global assessment. This strategy\nallows for more granular and manageable evaluations, ensuring that each segment\nof the text is assessed in isolation for both coherence and quality, while also\naccounting for the overall structure and consistency of the entire piece.\nMoreover, we introduce a hybrid in-context learning approach that leverages\nhuman annotations to enhance the performance of both local and global\nevaluations. By incorporating human-generated feedback directly into the\nevaluation process, this method allows the model to better align with human\njudgment. Finally, we develop an uncertainty-based active learning algorithm\nthat efficiently selects data samples for human annotation, thereby reducing\nannotation costs in practical scenarios. Experimental results show that the\nproposed evaluation framework outperforms several representative baselines,\nhighlighting the effectiveness of our approach."}
{"id": "2505.20276", "pdf": "https://arxiv.org/pdf/2505.20276.pdf", "abs": "https://arxiv.org/abs/2505.20276", "title": "Does quantization affect models' performance on long-context tasks?", "authors": ["Anmol Mekala", "Anirudh Atmakuru", "Yixiao Song", "Marzena Karpinska", "Mohit Iyyer"], "categories": ["cs.CL", "cs.AI"], "comment": "9 pages of content with 9 figures. 37 remaining pages of references\n  and supplementary with 17 figures. Under review as of May 26", "summary": "Large language models (LLMs) now support context windows exceeding 128K\ntokens, but this comes with significant memory requirements and high inference\nlatency. Quantization can mitigate these costs, but may degrade performance. In\nthis work, we present the first systematic evaluation of quantized LLMs on\ntasks with long-inputs (>64K tokens) and long-form outputs. Our evaluation\nspans 9.7K test examples, five quantization methods (FP8, GPTQ-int8, AWQ-int4,\nGPTQ-int4, BNB-nf4), and five models (Llama-3.1 8B and 70B; Qwen-2.5 7B, 32B,\nand 72B). We find that, on average, 8-bit quantization preserves accuracy\n(~0.8% drop), whereas 4-bit methods lead to substantial losses, especially for\ntasks involving long context inputs (drops of up to 59%). This degradation\ntends to worsen when the input is in a language other than English. Crucially,\nthe effects of quantization depend heavily on the quantization method, model,\nand task. For instance, while Qwen-2.5 72B remains robust under BNB-nf4,\nLlama-3.1 70B experiences a 32% performance drop on the same task. These\nfindings highlight the importance of a careful, task-specific evaluation before\ndeploying quantized LLMs, particularly in long-context scenarios and with\nlanguages other than English."}
{"id": "2505.20282", "pdf": "https://arxiv.org/pdf/2505.20282.pdf", "abs": "https://arxiv.org/abs/2505.20282", "title": "One-shot Entropy Minimization", "authors": ["Zitian Gao", "Lynx Chen", "Joey Zhou", "Bryan Dai"], "categories": ["cs.CL"], "comment": "Work in progress", "summary": "We trained 13,440 large language models and found that entropy minimization\nrequires only a single unlabeled data and 10 steps optimization to achieve\nperformance improvements comparable to or even greater than those obtained\nusing thousands of data and carefully designed rewards in rule-based\nreinforcement learning. This striking result may prompt a rethinking of\npost-training paradigms for large language models. Our code is avaliable at\nhttps://github.com/zitian-gao/one-shot-em."}
{"id": "2505.20285", "pdf": "https://arxiv.org/pdf/2505.20285.pdf", "abs": "https://arxiv.org/abs/2505.20285", "title": "MaskSearch: A Universal Pre-Training Framework to Enhance Agentic Search Capability", "authors": ["Weiqi Wu", "Xin Guan", "Shen Huang", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Jiuxin Cao", "Hai Zhao", "Jingren Zhou"], "categories": ["cs.CL"], "comment": "Code is available at https://github.com/Alibaba-NLP/MaskSearch", "summary": "Retrieval-Augmented Language Models (RALMs) represent a classic paradigm\nwhere models enhance generative capabilities using external knowledge retrieved\nvia a specialized module. Recent advancements in Agent techniques enable Large\nLanguage Models (LLMs) to autonomously utilize tools for retrieval, planning,\nand reasoning. While existing training-based methods show promise, their\nagentic abilities are limited by inherent characteristics of the task-specific\ndata used during training. To further enhance the universal search capability\nof agents, we propose a novel pre-training framework, MaskSearch. In the\npre-training stage, we introduce the Retrieval Augmented Mask Prediction (RAMP)\ntask, where the model learns to leverage search tools to fill masked spans on a\nlarge number of pre-training data, thus acquiring universal retrieval and\nreasoning capabilities for LLMs. After that, the model is trained on downstream\ntasks to achieve further improvement. We apply both Supervised Fine-tuning\n(SFT) and Reinforcement Learning (RL) for training. For SFT, we combine\nagent-based and distillation-based methods to generate training data, starting\nwith a multi-agent system consisting of a planner, rewriter, observer, and\nfollowed by a self-evolving teacher model. While for RL, we employ DAPO as the\ntraining framework and adopt a hybrid reward system consisting of answer\nrewards and format rewards. Additionally, we introduce a curriculum learning\napproach that allows the model to learn progressively from easier to more\nchallenging instances based on the number of masked spans. We evaluate the\neffectiveness of our framework in the scenario of open-domain multi-hop\nquestion answering. Through extensive experiments, we demonstrate that\nMaskSearch significantly enhances the performance of LLM-based search agents on\nboth in-domain and out-of-domain downstream tasks."}
{"id": "2310.03635", "pdf": "https://arxiv.org/pdf/2310.03635.pdf", "abs": "https://arxiv.org/abs/2310.03635", "title": "CLEVRER-Humans: Describing Physical and Causal Events the Human Way", "authors": ["Jiayuan Mao", "Xuelin Yang", "Xikun Zhang", "Noah D. Goodman", "Jiajun Wu"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "stat.ML"], "comment": "Version 3. NeurIPS 2022 (Dataset and Benchmark Track). First two\n  authors contributed equally. Project page:\n  https://sites.google.com/stanford.edu/clevrer-humans/home", "summary": "Building machines that can reason about physical events and their causal\nrelationships is crucial for flexible interaction with the physical world.\nHowever, most existing physical and causal reasoning benchmarks are exclusively\nbased on synthetically generated events and synthetic natural language\ndescriptions of causal relationships. This design brings up two issues. First,\nthere is a lack of diversity in both event types and natural language\ndescriptions; second, causal relationships based on manually-defined heuristics\nare different from human judgments. To address both shortcomings, we present\nthe CLEVRER-Humans benchmark, a video reasoning dataset for causal judgment of\nphysical events with human labels. We employ two techniques to improve data\ncollection efficiency: first, a novel iterative event cloze task to elicit a\nnew representation of events in videos, which we term Causal Event Graphs\n(CEGs); second, a data augmentation technique based on neural language\ngenerative models. We convert the collected CEGs into questions and answers to\nbe consistent with prior work. Finally, we study a collection of baseline\napproaches for CLEVRER-Humans question-answering, highlighting the great\nchallenges set forth by our benchmark."}
{"id": "2402.04068", "pdf": "https://arxiv.org/pdf/2402.04068.pdf", "abs": "https://arxiv.org/abs/2402.04068", "title": "Retrieve to Explain: Evidence-driven Predictions for Explainable Drug Target Identification", "authors": ["Ravi Patel", "Angus Brayne", "Rogier Hintzen", "Daniel Jaroslawicz", "Georgiana Neculae", "Dane Corneil"], "categories": ["cs.LG", "cs.CL"], "comment": "Accepted at ACL 2025 (The 63rd Annual Meeting of the Association for\n  Computational Linguistics)", "summary": "Language models hold incredible promise for enabling scientific discovery by\nsynthesizing massive research corpora. Many complex scientific research\nquestions have multiple plausible answers, each supported by evidence of\nvarying strength. However, existing language models lack the capability to\nquantitatively and faithfully compare answer plausibility in terms of\nsupporting evidence. To address this, we introduce Retrieve to Explain (R2E), a\nretrieval-based model that scores and ranks all possible answers to a research\nquestion based on evidence retrieved from a document corpus. The architecture\nrepresents each answer only in terms of its supporting evidence, with the\nanswer itself masked. This allows us to extend feature attribution methods such\nas Shapley values, to transparently attribute answer scores to supporting\nevidence at inference time. The architecture also allows incorporation of new\nevidence without retraining, including non-textual data modalities templated\ninto natural language. We developed R2E for the challenging scientific\ndiscovery task of drug target identification, a human-in-the-loop process where\nfailures are extremely costly and explainability paramount. When predicting\nwhether drug targets will subsequently be confirmed as efficacious in clinical\ntrials, R2E not only matches non-explainable literature-based models but also\nsurpasses a genetics-based target identification approach used throughout the\npharmaceutical industry."}
{"id": "2406.11546", "pdf": "https://arxiv.org/pdf/2406.11546.pdf", "abs": "https://arxiv.org/abs/2406.11546", "title": "GigaSpeech 2: An Evolving, Large-Scale and Multi-domain ASR Corpus for Low-Resource Languages with Automated Crawling, Transcription and Refinement", "authors": ["Yifan Yang", "Zheshu Song", "Jianheng Zhuo", "Mingyu Cui", "Jinpeng Li", "Bo Yang", "Yexing Du", "Ziyang Ma", "Xunying Liu", "Ziyuan Wang", "Ke Li", "Shuai Fan", "Kai Yu", "Wei-Qiang Zhang", "Guoguo Chen", "Xie Chen"], "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted in ACL 2025 (Main)", "summary": "The evolution of speech technology has been spurred by the rapid increase in\ndataset sizes. Traditional speech models generally depend on a large amount of\nlabeled training data, which is scarce for low-resource languages. This paper\npresents GigaSpeech 2, a large-scale, multi-domain, multilingual speech\nrecognition corpus. It is designed for low-resource languages and does not rely\non paired speech and text data. GigaSpeech 2 comprises about 30,000 hours of\nautomatically transcribed speech, including Thai, Indonesian, and Vietnamese,\ngathered from unlabeled YouTube videos. We also introduce an automated pipeline\nfor data crawling, transcription, and label refinement. Specifically, this\npipeline involves Whisper for initial transcription, MMS for forced alignment,\nand multi-dimensional filtering for data quality assurance. A modified Noisy\nStudent Training is developed to further refine flawed pseudo labels\niteratively, thereby enhancing model performance. Experimental results on our\nmanually transcribed evaluation set and two public test sets from Common Voice\nand FLEURS confirm our corpus's high quality and broad applicability. Notably,\nASR models trained on GigaSpeech 2 can reduce the word error rate for Thai,\nIndonesian, and Vietnamese on our challenging and realistic YouTube test set by\n25% to 40% compared to Whisper large-v3, with merely 10% model parameters.\nFurthermore, our ASR models trained on GigaSpeech 2 yield superior performance\ncompared to commercial services. We hope that our newly introduced corpus and\npipeline will open a new avenue for low-resource speech recognition and\nsignificantly facilitate research in this area."}
{"id": "2408.08313", "pdf": "https://arxiv.org/pdf/2408.08313.pdf", "abs": "https://arxiv.org/abs/2408.08313", "title": "Can Large Language Models Understand Symbolic Graphics Programs?", "authors": ["Zeju Qiu", "Weiyang Liu", "Haiwen Feng", "Zhen Liu", "Tim Z. Xiao", "Katherine M. Collins", "Joshua B. Tenenbaum", "Adrian Weller", "Michael J. Black", "Bernhard Schölkopf"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": "ICLR 2025 Spotlight (v4: 47 pages, 26 figures, project page:\n  https://sgp-bench.github.io/)", "summary": "Against the backdrop of enthusiasm for large language models (LLMs), there is\na growing need to scientifically assess their capabilities and shortcomings.\nThis is nontrivial in part because it is difficult to find tasks which the\nmodels have not encountered during training. Utilizing symbolic graphics\nprograms, we propose a domain well-suited to test multiple spatial-semantic\nreasoning skills of LLMs. Popular in computer graphics, these programs\nprocedurally generate visual data. While LLMs exhibit impressive skills in\ngeneral program synthesis and analysis, symbolic graphics programs offer a new\nlayer of evaluation: they allow us to test an LLM's ability to answer semantic\nquestions about the images or 3D geometries without a vision encoder. To\nsemantically understand the symbolic programs, LLMs would need to possess the\nability to \"imagine\" and reason how the corresponding graphics content would\nlook with only the symbolic description of the local curvatures and strokes. We\nuse this task to evaluate LLMs by creating a large benchmark for the semantic\nvisual understanding of symbolic graphics programs, built procedurally with\nminimal human effort. Particular emphasis is placed on transformations of\nimages that leave the image level semantics invariant while introducing\nsignificant changes to the underlying program. We evaluate commercial and\nopen-source LLMs on our benchmark to assess their ability to reason about\nvisual output of programs, finding that LLMs considered stronger at reasoning\ngenerally perform better. Lastly, we introduce a novel method to improve this\nability -- Symbolic Instruction Tuning (SIT), in which the LLM is finetuned\nwith pre-collected instruction data on symbolic graphics programs.\nInterestingly, we find that SIT not only improves LLM's understanding on\nsymbolic programs, but it also improves general reasoning ability on various\nother benchmarks."}
{"id": "2409.18594", "pdf": "https://arxiv.org/pdf/2409.18594.pdf", "abs": "https://arxiv.org/abs/2409.18594", "title": "\"Oh LLM, I'm Asking Thee, Please Give Me a Decision Tree\": Zero-Shot Decision Tree Induction and Embedding with Large Language Models", "authors": ["Ricardo Knauer", "Mario Koddenbrock", "Raphael Wallsberger", "Nicholas M. Brisson", "Georg N. Duda", "Deborah Falla", "David W. Evans", "Erik Rodner"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "KDD 2025 Research Track", "summary": "Large language models (LLMs) provide powerful means to leverage prior\nknowledge for predictive modeling when data is limited. In this work, we\ndemonstrate how LLMs can use their compressed world knowledge to generate\nintrinsically interpretable machine learning models, i.e., decision trees,\nwithout any training data. We find that these zero-shot decision trees can even\nsurpass data-driven trees on some small-sized tabular datasets and that\nembeddings derived from these trees perform better than data-driven tree-based\nembeddings on average. Our decision tree induction and embedding approaches can\ntherefore serve as new knowledge-driven baselines for data-driven machine\nlearning methods in the low-data regime. Furthermore, they offer ways to\nharness the rich world knowledge within LLMs for tabular machine learning\ntasks. Our code and results are available at\nhttps://github.com/ml-lab-htw/llm-trees."}
{"id": "2410.09403", "pdf": "https://arxiv.org/pdf/2410.09403.pdf", "abs": "https://arxiv.org/abs/2410.09403", "title": "Many Heads Are Better Than One: Improved Scientific Idea Generation by A LLM-Based Multi-Agent System", "authors": ["Haoyang Su", "Renqi Chen", "Shixiang Tang", "Zhenfei Yin", "Xinzhe Zheng", "Jinzhe Li", "Biqing Qi", "Qi Wu", "Hui Li", "Wanli Ouyang", "Philip Torr", "Bowen Zhou", "Nanqing Dong"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.MA"], "comment": "Accepted by ACL 2025 Main Conference", "summary": "The rapid advancement of scientific progress requires innovative tools that\ncan accelerate knowledge discovery. Although recent AI methods, particularly\nlarge language models (LLMs), have shown promise in tasks such as hypothesis\ngeneration and experimental design, they fall short of replicating the\ncollaborative nature of real-world scientific practices, where diverse experts\nwork together in teams to tackle complex problems. To address the limitations,\nwe propose an LLM-based multi-agent system, i.e., Virtual Scientists (VirSci),\ndesigned to mimic the teamwork inherent in scientific research. VirSci\norganizes a team of agents to collaboratively generate, evaluate, and refine\nresearch ideas. Through comprehensive experiments, we demonstrate that this\nmulti-agent approach outperforms the state-of-the-art method in producing novel\nscientific ideas. We further investigate the collaboration mechanisms that\ncontribute to its tendency to produce ideas with higher novelty, offering\nvaluable insights to guide future research and illuminating pathways toward\nbuilding a robust system for autonomous scientific discovery. The code is\navailable at https://github.com/open-sciencelab/Virtual-Scientists."}
{"id": "2410.15332", "pdf": "https://arxiv.org/pdf/2410.15332.pdf", "abs": "https://arxiv.org/abs/2410.15332", "title": "EPIC: Efficient Position-Independent Caching for Serving Large Language Models", "authors": ["Junhao Hu", "Wenrui Huang", "Weidong Wang", "Haoyi Wang", "Tiancheng Hu", "Qin Zhang", "Hao Feng", "Xusheng Chen", "Yizhou Shan", "Tao Xie"], "categories": ["cs.LG", "cs.CL", "cs.DC", "cs.PF"], "comment": null, "summary": "Large Language Models (LLMs) show great capabilities in a wide range of\napplications, but serving them efficiently becomes increasingly challenging as\nrequests (prompts) become more complex. Context caching improves serving\nperformance by reusing Key-Value (KV) vectors, the intermediate representations\nof tokens that are repeated across requests. However, existing context caching\nrequires exact prefix matches across requests, limiting reuse cases in settings\nsuch as few-shot learning and retrieval-augmented generation, where immutable\ncontent (e.g., documents) remains unchanged across requests but is preceded by\nvarying prefixes. Position-Independent Caching (PIC) addresses this issue by\nenabling modular reuse of the KV vectors regardless of prefixes. We formalize\nPIC and advance prior work by introducing EPIC, a serving system incorporating\nour new LegoLink algorithm, which mitigates the inappropriate \"attention sink\"\neffect at every document beginning, to maintain accuracy with minimal\ncomputation. Experiments show that EPIC achieves up to 8x improvements in\nTime-To-First-Token (TTFT) and 7x throughput gains over existing systems, with\nnegligible or no accuracy loss."}
{"id": "2410.18122", "pdf": "https://arxiv.org/pdf/2410.18122.pdf", "abs": "https://arxiv.org/abs/2410.18122", "title": "Yesterday's News: Benchmarking Multi-Dimensional Out-of-Distribution Generalization of Misinformation Detection Models", "authors": ["Ivo Verhoeven", "Pushkar Mishra", "Ekaterina Shutova"], "categories": ["cs.IR", "cs.CL"], "comment": "Under review", "summary": "This article introduces misinfo-general, a benchmark dataset for evaluating\nmisinformation models' ability to perform out-of-distribution generalization.\nMisinformation changes rapidly, much more quickly than moderators can annotate\nat scale, resulting in a shift between the training and inference data\ndistributions. As a result, misinformation detectors need to be able to perform\nout-of-distribution generalization, an attribute they currently lack. Our\nbenchmark uses distant labelling to enable simulating covariate shifts in\nmisinformation content. We identify time, event, topic, publisher, political\nbias, misinformation type as important axes for generalization, and we evaluate\na common class of baseline models on each. Using article metadata, we show how\nthis model fails desiderata, which is not necessarily obvious from\nclassification metrics. Finally, we analyze properties of the data to ensure\nlimited presence of modelling shortcuts. We make the dataset and accompanying\ncode publicly available: https://github.com/ioverho/misinfo-general"}
{"id": "2412.10312", "pdf": "https://arxiv.org/pdf/2412.10312.pdf", "abs": "https://arxiv.org/abs/2412.10312", "title": "Interlocking-free Selective Rationalization Through Genetic-based Learning", "authors": ["Federico Ruggeri", "Gaetano Signorelli"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NE"], "comment": null, "summary": "A popular end-to-end architecture for selective rationalization is the\nselect-then-predict pipeline, comprising a generator to extract highlights fed\nto a predictor. Such a cooperative system suffers from suboptimal equilibrium\nminima due to the dominance of one of the two modules, a phenomenon known as\ninterlocking. While several contributions aimed at addressing interlocking,\nthey only mitigate its effect, often by introducing feature-based heuristics,\nsampling, and ad-hoc regularizations. We present GenSPP, the first\ninterlocking-free architecture for selective rationalization that does not\nrequire any learning overhead, as the above-mentioned. GenSPP avoids\ninterlocking by performing disjoint training of the generator and predictor via\ngenetic global search. Experiments on a synthetic and a real-world benchmark\nshow that our model outperforms several state-of-the-art competitors."}
{"id": "2412.11189", "pdf": "https://arxiv.org/pdf/2412.11189.pdf", "abs": "https://arxiv.org/abs/2412.11189", "title": "Leveraging Large Language Models for Active Merchant Non-player Characters", "authors": ["Byungjun Kim", "Minju Kim", "Dayeon Seo", "Bugeun Kim"], "categories": ["cs.AI", "cs.CL"], "comment": "Accepted to IJCAI 2025", "summary": "We highlight two significant issues leading to the passivity of current\nmerchant non-player characters (NPCs): pricing and communication. While\nimmersive interactions with active NPCs have been a focus, price negotiations\nbetween merchant NPCs and players remain underexplored. First, passive pricing\nrefers to the limited ability of merchants to modify predefined item prices.\nSecond, passive communication means that merchants can only interact with\nplayers in a scripted manner. To tackle these issues and create an active\nmerchant NPC, we propose a merchant framework based on large language models\n(LLMs), called MART, which consists of an appraiser module and a negotiator\nmodule. We conducted two experiments to explore various implementation options\nunder different training methods and LLM sizes, considering a range of possible\ngame environments. Our findings indicate that finetuning methods, such as\nsupervised finetuning (SFT) and knowledge distillation (KD), are effective in\nusing smaller LLMs to implement active merchant NPCs. Additionally, we found\nthree irregular cases arising from the responses of LLMs."}
{"id": "2412.11927", "pdf": "https://arxiv.org/pdf/2412.11927.pdf", "abs": "https://arxiv.org/abs/2412.11927", "title": "Transparent and Coherent Procedural Mistake Detection", "authors": ["Shane Storks", "Itamar Bar-Yossef", "Yayuan Li", "Zheyuan Zhang", "Jason J. Corso", "Joyce Chai"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Procedural mistake detection (PMD) is a challenging problem of classifying\nwhether a human user (observed through egocentric video) has successfully\nexecuted a task (specified by a procedural text). Despite significant recent\nefforts, machine performance in the wild remains nonviable, and the reasoning\nprocesses underlying this performance are opaque. As such, we extend PMD to\nrequire generating visual self-dialog rationales to inform decisions. Given the\nimpressive, mature image understanding capabilities observed in recent\nvision-and-language models (VLMs), we curate a suitable benchmark dataset for\nPMD based on individual frames. As our reformulation enables unprecedented\ntransparency, we leverage a natural language inference (NLI) model to formulate\ntwo automated metrics for the coherence of generated rationales. We establish\nbaselines for this reframed task, showing that while VLMs struggle\noff-the-shelf, their accuracy, coherence, and efficiency can be improved by\nincorporating these metrics into common inference and fine-tuning methods-\nthough not without tradeoff. Lastly, our multi-faceted metrics visualize common\noutcomes, highlighting areas for further improvement."}
{"id": "2412.16216", "pdf": "https://arxiv.org/pdf/2412.16216.pdf", "abs": "https://arxiv.org/abs/2412.16216", "title": "GMoE: Empowering LLMs Fine-Tuning via MoE Graph Collaboration", "authors": ["Ting Bai", "Yue Yu", "Le Huang", "Zenan Xu", "Zhe Zhao", "Chuan Shi"], "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.7"], "comment": "9 pages, 25 figures", "summary": "The sparse Mixture-of-Experts (MoE) architecture of large language models\n(LLMs) confronts an inherent issue of load imbalance arising from the\nsimplistic linear router strategy, which ultimately causes the instability and\ninefficient learning of LLMs. To address this challenge, we introduce a novel\nMoE graph-based framework $\\textbf{GMoE}$, aimed at enhancing the collaboration\namong multiple experts. In GMoE, a graph router function is designed to capture\nthe collaboration signals among experts. This enables all experts to\ndynamically allocate information derived from input data by sharing information\nwith their neighboring experts. Moreover, we put forward two coordination\nstrategies in GMoE: the $\\textit{Poisson distribution-based distinction\nstrategy}$ and the $\\textit{Normal distribution-based balance strategy}$, to\nfurther release the capacity of each expert and increase the model stability in\nthe fine-tuning of LLMs. Specifically, we leverage a parameter-efficient\nfine-tuning technique, i.e., Low-Rank Adaptation (LoRA), to implement the graph\nMoE architecture. Extensive experiments on four real-world benchmark datasets\ndemonstrate the effectiveness of GMoE, showing the benefits of facilitating\ncollaborations of multiple experts in LLM fine-tuning. The code of experimental\nimplementation is available at https://github.com/BAI-LAB/GMoE"}
{"id": "2412.20367", "pdf": "https://arxiv.org/pdf/2412.20367.pdf", "abs": "https://arxiv.org/abs/2412.20367", "title": "Enhancing Code LLMs with Reinforcement Learning in Code Generation: A Survey", "authors": ["Junqiao Wang", "Zeng Zhang", "Yangfan He", "Zihao Zhang", "Yuyang Song", "Tianyu Shi", "Yuchen Li", "Hengyuan Xu", "Kunyu Wu", "Xin Yi", "Zhongwei Wan", "Xinhang Yuan", "Kuan Lu", "Menghao Huo", "Guangwu Qian", "Keqin Li", "Qiuwu Chen", "Lewei He"], "categories": ["cs.SE", "cs.CL"], "comment": null, "summary": "Reinforcement learning (RL) has emerged as a powerful paradigm for enhancing\nlarge language models (LLMs) in code generation and optimization. This survey\nsystematically reviews RL-driven techniques across the code development\nlifecycle, from compiler-level optimizations and resource allocation strategies\nto end-to-end code synthesis frameworks. We first examine classical and modern\nRL algorithms -- spanning policy gradients, actor-critic methods,\nhuman-feedback alignment, and preference-based optimization -- and their\nadaptations to the unique challenges of code generation, such as sparse and\ndelayed rewards. Next, we analyze key benchmarks, datasets, and evaluation\nmetrics that drive progress in RL-augmented Code LLMs. Finally, we identify\nopen problems, including the need for richer feedback sources, support for\nlow-level and domain-specific languages, and methods to reduce computational\noverhead. By consolidating current insights and outlining future directions,\nthis work aims to guide researchers and practitioners in leveraging RL to\nproduce more robust, efficient, and human-aligned code generation systems."}
{"id": "2412.20993", "pdf": "https://arxiv.org/pdf/2412.20993.pdf", "abs": "https://arxiv.org/abs/2412.20993", "title": "Efficiently Scaling LLM Reasoning with Certaindex", "authors": ["Yichao Fu", "Junda Chen", "Siqi Zhu", "Zheyu Fu", "Zhongdongming Dai", "Yonghao Zhuang", "Yian Ma", "Aurick Qiao", "Tajana Rosing", "Ion Stoica", "Hao Zhang"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Test-time reasoning algorithms such as chain-of-thought, self-consistency,\nand MCTS enhance LLM problem-solving but can wastefully generate many tokens\nwithout improving accuracy. At the same time, we observe that these algorithms\nexhibit answer stabilization: their intermediate solutions often cease to\nchange after a certain point, and further investment of compute does not change\ntheir final answer. To quantify this phenomenon, we introduce Certaindex, an\nalgorithm-agnostic metric measuring this evolving stability, signaling when\nfurther computation is unlikely to alter the final result. Certaindex is\nlightweight, can accelerate reasoning program inference via early exit, and\nfurther enables dynamic token allocation, gang scheduling, and many\nopportunities when integrated with real-world LLM serving systems. To quantify\nreal-world benefits, we built Certaindex as a scheduler into Dynasor, our\nreasoning-aware LLM serving system, and demonstrate up to 50% compute savings\nand 3.3x higher throughput in real workloads with no accuracy drop. Our code is\navailable at https://github.com/hao-ai-lab/Dynasor.git"}
{"id": "2501.04070", "pdf": "https://arxiv.org/pdf/2501.04070.pdf", "abs": "https://arxiv.org/abs/2501.04070", "title": "More is not always better? Enhancing Many-Shot In-Context Learning with Differentiated and Reweighting Objectives", "authors": ["Xiaoqing Zhang", "Ang Lv", "Yuhan Liu", "Flood Sung", "Wei Liu", "Jian Luan", "Shuo Shang", "Xiuying Chen", "Rui Yan"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "14 pages, 8 figures, 11 tables", "summary": "Large language models (LLMs) excel at few-shot in-context learning (ICL)\nwithout requiring parameter updates. However, as ICL demonstrations increase\nfrom a few to many, performance tends to plateau and eventually decline. We\nidentify two primary causes for this trend: the suboptimal negative\nlog-likelihood (NLL) optimization objective and the incremental data noise. To\naddress these issues, we introduce \\textit{DrICL}, a novel optimization method\nthat enhances model performance through \\textit{Differentiated} and\n\\textit{Reweighting} objectives. Globally, DrICL utilizes differentiated\nlearning to optimize the NLL objective, ensuring that many-shot performance\nsurpasses zero-shot levels. Locally, it dynamically adjusts the weighting of\nmany-shot demonstrations by leveraging cumulative advantages inspired by\nreinforcement learning, thereby mitigating the impact of noisy data.\nRecognizing the lack of multi-task datasets with diverse many-shot\ndistributions, we develop the \\textit{Many-Shot ICL Benchmark} (ICL-50)-a\nlarge-scale benchmark of 50 tasks that cover shot numbers from 1 to 350 within\nsequences of up to 8,000 tokens-for both fine-tuning and evaluation purposes.\nExperimental results demonstrate that LLMs enhanced with DrICL achieve\nsignificant improvements in many-shot setups across various tasks, including\nboth in-domain and out-of-domain scenarios. We release the code and dataset\nhoping to facilitate further research in many-shot\nICL\\footnote{https://github.com/xiaoqzhwhu/DrICL}."}
{"id": "2502.03771", "pdf": "https://arxiv.org/pdf/2502.03771.pdf", "abs": "https://arxiv.org/abs/2502.03771", "title": "vCache: Verified Semantic Prompt Caching", "authors": ["Luis Gaspar Schroeder", "Aditya Desai", "Alejandro Cuadron", "Kyle Chu", "Shu Liu", "Mark Zhao", "Stephan Krusche", "Alfons Kemper", "Matei Zaharia", "Joseph E. Gonzalez"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Semantic caches return cached LLM-generated responses for semantically\nsimilar prompts to reduce inference latency and cost. They embed cached prompts\nand store them alongside their response in a vector database. Embedding\nsimilarity metrics assign a numerical score to quantify the similarity between\na request and its nearest neighbor prompt from the cache. Existing systems use\nthe same static similarity threshold across all requests to determine whether\ntwo prompts can share similar responses. However, we observe that static\nthresholds do not give formal correctness guarantees, can result in unexpected\nerror rates, and lead to suboptimal cache hit rates. This paper proposes\nvCache, the first verified semantic cache with user-defined error rate\nguarantees. It employs an online learning algorithm to estimate an optimal\nthreshold for each cached prompt, enabling reliable cache responses without\nadditional training. Our experiments show that vCache consistently meets the\nspecified error bounds while outperforming state-of-the-art static-threshold\nand fine-tuned embedding baselines. We release the vCache implementation and\nbenchmarks to support future research."}
{"id": "2502.05159", "pdf": "https://arxiv.org/pdf/2502.05159.pdf", "abs": "https://arxiv.org/abs/2502.05159", "title": "A Lightweight Method to Disrupt Memorized Sequences in LLM", "authors": ["Parjanya Prajakta Prashant", "Kaustubh Ponkshe", "Babak Salimi"], "categories": ["cs.LG", "cs.CL"], "comment": "26 pages, 3 figures", "summary": "As language models scale, their performance improves dramatically across a\nwide range of tasks, but so does their tendency to memorize and regurgitate\nparts of their training data verbatim. This tradeoff poses serious legal,\nethical, and safety concerns, especially in real-world deployments. Existing\nmitigation techniques, such as differential privacy or model unlearning, often\nrequire retraining or access to internal weights making them impractical for\nmost users. In this work, we introduce TokenSwap, a lightweight, post-hoc\ndefense designed for realistic settings where the user can only access\ntoken-level outputs. Our key insight is that while large models are necessary\nfor high task performance, small models (e.g., DistilGPT-2) are often\nsufficient to assign fluent, grammatically plausible probabilities to common\nfunction words - and crucially, they memorize far less. By selectively swapping\ntoken probabilities between models, TokenSwap preserves the capabilities of\nlarge models while reducing their propensity for verbatim reproduction.\nEvaluations on Pythia-6.9B and Llama-3-8B show up to a 10$\\times$ drop in exact\nmemorization with negligible task degradation. Our method offers a practical,\naccessible solution for mitigating memorized generation in deployed LLMs."}
{"id": "2502.05374", "pdf": "https://arxiv.org/pdf/2502.05374.pdf", "abs": "https://arxiv.org/abs/2502.05374", "title": "Towards LLM Unlearning Resilient to Relearning Attacks: A Sharpness-Aware Minimization Perspective and Beyond", "authors": ["Chongyu Fan", "Jinghan Jia", "Yihua Zhang", "Anil Ramakrishna", "Mingyi Hong", "Sijia Liu"], "categories": ["cs.LG", "cs.CL"], "comment": "Accepted by ICML 2025", "summary": "The LLM unlearning technique has recently been introduced to comply with data\nregulations and address the safety and ethical concerns of LLMs by removing the\nundesired data-model influence. However, state-of-the-art unlearning methods\nface a critical vulnerability: they are susceptible to ``relearning'' the\nremoved information from a small number of forget data points, known as\nrelearning attacks. In this paper, we systematically investigate how to make\nunlearned models robust against such attacks. For the first time, we establish\na connection between robust unlearning and sharpness-aware minimization (SAM)\nthrough a unified robust optimization framework, in an analogy to adversarial\ntraining designed to defend against adversarial attacks. Our analysis for SAM\nreveals that smoothness optimization plays a pivotal role in mitigating\nrelearning attacks. Thus, we further explore diverse smoothing strategies to\nenhance unlearning robustness. Extensive experiments on benchmark datasets,\nincluding WMDP and MUSE, demonstrate that SAM and other smoothness optimization\napproaches consistently improve the resistance of LLM unlearning to relearning\nattacks. Notably, smoothness-enhanced unlearning also helps defend against\n(input-level) jailbreaking attacks, broadening our proposal's impact in\nrobustifying LLM unlearning. Codes are available at\nhttps://github.com/OPTML-Group/Unlearn-Smooth."}
{"id": "2502.06042", "pdf": "https://arxiv.org/pdf/2502.06042.pdf", "abs": "https://arxiv.org/abs/2502.06042", "title": "Scaling Laws for Forgetting during Finetuning with Pretraining Data Injection", "authors": ["Louis Bethune", "David Grangier", "Dan Busbridge", "Eleonora Gualdoni", "Marco Cuturi", "Pierre Ablin"], "categories": ["cs.LG", "cs.CL"], "comment": "19 pages, 15 figures, preprint", "summary": "A widespread strategy to obtain a language model that performs well on a\ntarget domain is to finetune a pretrained model to perform unsupervised\nnext-token prediction on data from that target domain. Finetuning presents two\nchallenges: (i) if the amount of target data is limited, as in most practical\napplications, the model will quickly overfit, and (ii) the model will drift\naway from the original model, forgetting the pretraining data and the generic\nknowledge that comes with it. We aim to derive scaling laws that quantify these\ntwo phenomena for various target domains, amounts of available target data, and\nmodel scales. We measure the efficiency of injecting pretraining data into the\nfinetuning data mixture to avoid forgetting and mitigate overfitting. A key\npractical takeaway from our study is that injecting as little as 1% of\npretraining data in the finetuning data mixture prevents the model from\nforgetting the pretraining set."}
{"id": "2502.07266", "pdf": "https://arxiv.org/pdf/2502.07266.pdf", "abs": "https://arxiv.org/abs/2502.07266", "title": "When More is Less: Understanding Chain-of-Thought Length in LLMs", "authors": ["Yuyang Wu", "Yifei Wang", "Ziyu Ye", "Tianqi Du", "Stefanie Jegelka", "Yisen Wang"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) employ Chain-of-Thought (CoT) reasoning to\ndeconstruct complex problems. While longer CoTs are often presumed superior,\nthis paper challenges that notion, arguing that longer is not always better.\nDrawing on combined evidence from real-world observations, controlled\nexperiments, and theoretical analysis, we demonstrate that task accuracy\ntypically follows an inverted U-shaped curve with CoT length, where performance\ninitially improves but eventually decreases as the number of CoT steps\nincreases. With controlled experiments, we further uncover the scaling\nbehaviors of the optimal CoT length: it increases with task difficulty but\ndecreases with model capability, exposing an inherent simplicity bias where\nmore capable models favor shorter, more efficient CoT reasoning. This bias is\nalso evident in Reinforcement Learning (RL) training, where models gravitate\ntowards shorter CoTs as their accuracy improves. To have a deep understanding\nof these dynamics, we establish a simple theoretical model that formally proves\nthese phenomena, including the optimal length's scaling laws and the emergence\nof simplicity bias during RL. Guided by this framework, we demonstrate\nsignificant practical benefits from training with optimally-lengthed CoTs and\nemploying length-aware filtering at inference. These findings offer both a\nprincipled understanding of the \"overthinking\" phenomenon and multiple\npractical guidelines for CoT calibration, enabling LLMs to achieve optimal\nreasoning performance with adaptive CoTs tailored to task complexity and model\ncapability."}
{"id": "2502.13398", "pdf": "https://arxiv.org/pdf/2502.13398.pdf", "abs": "https://arxiv.org/abs/2502.13398", "title": "GeLLMO: Generalizing Large Language Models for Multi-property Molecule Optimization", "authors": ["Vishal Dey", "Xiao Hu", "Xia Ning"], "categories": ["cs.LG", "cs.AI", "cs.CL", "physics.chem-ph", "q-bio.QM"], "comment": "Accepted to ACL Main 2025. Vishal Dey and Xiao Hu contributed equally\n  to this paper", "summary": "Despite recent advancements, most computational methods for molecule\noptimization are constrained to single- or double-property optimization tasks\nand suffer from poor scalability and generalizability to novel optimization\ntasks. Meanwhile, Large Language Models (LLMs) demonstrate remarkable\nout-of-domain generalizability to novel tasks. To demonstrate LLMs' potential\nfor molecule optimization, we introduce MuMOInstruct, the first high-quality\ninstruction-tuning dataset specifically focused on complex multi-property\nmolecule optimization tasks. Leveraging MuMOInstruct, we develop GeLLMOs, a\nseries of instruction-tuned LLMs for molecule optimization. Extensive\nevaluations across 5 in-domain and 5 out-of-domain tasks demonstrate that\nGeLLMOs consistently outperform state-of-the-art baselines. GeLLMOs also\nexhibit outstanding zero-shot generalization to unseen tasks, significantly\noutperforming powerful closed-source LLMs. Such strong generalizability\ndemonstrates the tremendous potential of GeLLMOs as foundational models for\nmolecule optimization, thereby tackling novel optimization tasks without\nresource-intensive retraining. MuMOInstruct, models, and code are accessible\nthrough https://github.com/ninglab/GeLLMO."}
{"id": "2502.17543", "pdf": "https://arxiv.org/pdf/2502.17543.pdf", "abs": "https://arxiv.org/abs/2502.17543", "title": "Training a Generally Curious Agent", "authors": ["Fahim Tajwar", "Yiding Jiang", "Abitha Thankaraj", "Sumaita Sadia Rahman", "J Zico Kolter", "Jeff Schneider", "Ruslan Salakhutdinov"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "ICML 2025. Project Website: https://paprika-llm.github.io", "summary": "Efficient exploration is essential for intelligent systems interacting with\ntheir environment, but existing language models often fall short in scenarios\nthat require strategic information gathering. In this paper, we present\nPaprika, a fine-tuning approach that enables language models to develop general\ndecision-making capabilities that are not confined to particular environments.\nBy training on synthetic interaction data from different tasks that require\ndiverse strategies, Paprika teaches models to explore and adapt their behavior\non a new task based on environment feedback in-context without more gradient\nupdates. Experimental results show that models fine-tuned with Paprika can\neffectively transfer their learned decision-making capabilities to entirely\nunseen tasks without additional training. Unlike traditional training, our\napproach's primary bottleneck lies in sampling useful interaction data instead\nof model updates. To improve sample efficiency, we propose a curriculum\nlearning strategy that prioritizes sampling trajectories from tasks with high\nlearning potential. These results suggest a promising path towards AI systems\nthat can autonomously solve novel sequential decision-making problems that\nrequire interactions with the external world."}
{"id": "2502.19130", "pdf": "https://arxiv.org/pdf/2502.19130.pdf", "abs": "https://arxiv.org/abs/2502.19130", "title": "Voting or Consensus? Decision-Making in Multi-Agent Debate", "authors": ["Lars Benedikt Kaesberg", "Jonas Becker", "Jan Philip Wahle", "Terry Ruas", "Bela Gipp"], "categories": ["cs.MA", "cs.AI", "cs.CL"], "comment": null, "summary": "Much of the success of multi-agent debates depends on carefully choosing the\nright parameters. The decision-making protocol stands out as it can highly\nimpact final model answers, depending on how decisions are reached. Systematic\ncomparison of decision protocols is difficult because many studies alter\nmultiple discussion parameters beyond the protocol. So far, it has been largely\nunknown how decision-making influences different tasks. This work\nsystematically evaluates the impact of seven decision protocols (e.g., majority\nvoting, unanimity consensus). We change only one variable at a time - the\ndecision protocol - to analyze how different methods affect the collaboration\nbetween agents and measure differences in knowledge and reasoning tasks. Our\nresults show that voting protocols improve performance by 13.2% in reasoning\ntasks and consensus protocols by 2.8% in knowledge tasks compared to other\ndecision protocols. Increasing the number of agents improves performance, while\nmore discussion rounds before voting reduce it. To improve decision-making by\nincreasing answer diversity, we propose two new methods, All-Agents Drafting\n(AAD) and Collective Improvement (CI). Our methods improve task performance by\nup to 3.3% with AAD and up to 7.4% with CI. This work demonstrates the\nimportance of decision-making in multi-agent debates beyond scaling."}
{"id": "2503.05203", "pdf": "https://arxiv.org/pdf/2503.05203.pdf", "abs": "https://arxiv.org/abs/2503.05203", "title": "Path Pooling: Training-Free Structure Enhancement for Efficient Knowledge Graph Retrieval-Augmented Generation", "authors": ["Hairu Wang", "Yuan Feng", "Xike Xie", "S Kevin Zhou"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Although Large Language Models achieve strong success in many tasks, they\nstill suffer from hallucinations and knowledge deficiencies in real-world\napplications. Many knowledge graph-based retrieval-augmented generation\n(KG-RAG) methods enhance the quality and credibility of LLMs by leveraging\nstructure and semantic information in KGs as external knowledge bases. However,\nthese methods struggle to effectively incorporate structure information, either\nincurring high computational costs or underutilizing available knowledge.\nInspired by smoothing operations in graph representation learning, we propose\npath pooling, a simple, training-free strategy that introduces structure\ninformation through a novel path-centric pooling operation. It seamlessly\nintegrates into existing KG-RAG methods in a plug-and-play manner, enabling\nricher structure information utilization. Extensive experiments demonstrate\nthat incorporating the path pooling into the state-of-the-art KG-RAG method\nconsistently improves performance across various settings while introducing\nnegligible additional cost."}
{"id": "2503.09501", "pdf": "https://arxiv.org/pdf/2503.09501.pdf", "abs": "https://arxiv.org/abs/2503.09501", "title": "ReMA: Learning to Meta-think for LLMs with Multi-Agent Reinforcement Learning", "authors": ["Ziyu Wan", "Yunxiang Li", "Xiaoyu Wen", "Yan Song", "Hanjing Wang", "Linyi Yang", "Mark Schmidt", "Jun Wang", "Weinan Zhang", "Shuyue Hu", "Ying Wen"], "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "comment": null, "summary": "Recent research on Reasoning of Large Language Models (LLMs) has sought to\nfurther enhance their performance by integrating meta-thinking -- enabling\nmodels to monitor, evaluate, and control their reasoning processes for more\nadaptive and effective problem-solving. However, current single-agent work\nlacks a specialized design for acquiring meta-thinking, resulting in low\nefficacy. To address this challenge, we introduce Reinforced Meta-thinking\nAgents (ReMA), a novel framework that leverages Multi-Agent Reinforcement\nLearning (MARL) to elicit meta-thinking behaviors, encouraging LLMs to think\nabout thinking. ReMA decouples the reasoning process into two hierarchical\nagents: a high-level meta-thinking agent responsible for generating strategic\noversight and plans, and a low-level reasoning agent for detailed executions.\nThrough iterative reinforcement learning with aligned objectives, these agents\nexplore and learn collaboration, leading to improved generalization and\nrobustness. Empirical results from single-turn experiments demonstrate that\nReMA outperforms single-agent RL baselines on complex reasoning tasks,\nincluding competitive-level mathematical benchmarks and LLM-as-a-Judge\nbenchmarks. Additionally, we further extend ReMA to multi-turn interaction\nsettings, leveraging turn-level ratio and parameter sharing to improve\nefficiency. Comprehensive ablation studies further illustrate the evolving\ndynamics of each distinct agent, providing valuable insights into how the\nmeta-thinking reasoning process enhances the reasoning capabilities of LLMs.\nOur code can be found in https://github.com/ziyuwan/ReMA-public"}
{"id": "2503.11074", "pdf": "https://arxiv.org/pdf/2503.11074.pdf", "abs": "https://arxiv.org/abs/2503.11074", "title": "Exploring the Necessity of Reasoning in LLM-based Agent Scenarios", "authors": ["Xueyang Zhou", "Guiyao Tie", "Guowen Zhang", "Weidong Wang", "Zhigang Zuo", "Di Wu", "Duanfeng Chu", "Pan Zhou", "Neil Zhenqiang Gong", "Lichao Sun"], "categories": ["cs.AI", "cs.CL"], "comment": "71 pages, 11 figures, 8 tables", "summary": "The rise of Large Reasoning Models (LRMs) signifies a paradigm shift toward\nadvanced computational reasoning. Yet, this progress disrupts traditional agent\nframeworks, traditionally anchored by execution-oriented Large Language Models\n(LLMs). To explore this transformation, we propose the LaRMA framework,\nencompassing nine tasks across Tool Usage, Plan Design, and Problem Solving,\nassessed with three top LLMs (e.g., Claude3.5-sonnet) and five leading LRMs\n(e.g., DeepSeek-R1). Our findings address four research questions: LRMs surpass\nLLMs in reasoning-intensive tasks like Plan Design, leveraging iterative\nreflection for superior outcomes; LLMs excel in execution-driven tasks such as\nTool Usage, prioritizing efficiency; hybrid LLM-LRM configurations, pairing\nLLMs as actors with LRMs as reflectors, optimize agent performance by blending\nexecution speed with reasoning depth; and LRMs' enhanced reasoning incurs\nhigher computational costs, prolonged processing, and behavioral challenges,\nincluding overthinking and fact-ignoring tendencies. This study fosters deeper\ninquiry into LRMs' balance of deep thinking and overthinking, laying a critical\nfoundation for future agent design advancements."}
{"id": "2503.13503", "pdf": "https://arxiv.org/pdf/2503.13503.pdf", "abs": "https://arxiv.org/abs/2503.13503", "title": "SciHorizon: Benchmarking AI-for-Science Readiness from Scientific Data to Large Language Models", "authors": ["Chuan Qin", "Xin Chen", "Chengrui Wang", "Pengmin Wu", "Xi Chen", "Yihang Cheng", "Jingyi Zhao", "Meng Xiao", "Xiangchao Dong", "Qingqing Long", "Boya Pan", "Han Wu", "Chengzan Li", "Yuanchun Zhou", "Hui Xiong", "Hengshu Zhu"], "categories": ["cs.LG", "cs.CL", "cs.DL", "cs.IR"], "comment": null, "summary": "In recent years, the rapid advancement of Artificial Intelligence (AI)\ntechnologies, particularly Large Language Models (LLMs), has revolutionized the\nparadigm of scientific discovery, establishing AI-for-Science (AI4Science) as a\ndynamic and evolving field. However, there is still a lack of an effective\nframework for the overall assessment of AI4Science, particularly from a\nholistic perspective on data quality and model capability. Therefore, in this\nstudy, we propose SciHorizon, a comprehensive assessment framework designed to\nbenchmark the readiness of AI4Science from both scientific data and LLM\nperspectives. First, we introduce a generalizable framework for assessing\nAI-ready scientific data, encompassing four key dimensions: Quality, FAIRness,\nExplainability, and Compliance-which are subdivided into 15 sub-dimensions.\nDrawing on data resource papers published between 2018 and 2023 in\npeer-reviewed journals, we present recommendation lists of AI-ready datasets\nfor Earth, Life, and Materials Sciences, making a novel and original\ncontribution to the field. Concurrently, to assess the capabilities of LLMs\nacross multiple scientific disciplines, we establish 16 assessment dimensions\nbased on five core indicators Knowledge, Understanding, Reasoning,\nMultimodality, and Values spanning Mathematics, Physics, Chemistry, Life\nSciences, and Earth and Space Sciences. Using the developed benchmark datasets,\nwe have conducted a comprehensive evaluation of over 50 representative\nopen-source and closed source LLMs. All the results are publicly available and\ncan be accessed online at www.scihorizon.cn/en."}
{"id": "2503.14434", "pdf": "https://arxiv.org/pdf/2503.14434.pdf", "abs": "https://arxiv.org/abs/2503.14434", "title": "LLM-FE: Automated Feature Engineering for Tabular Data with LLMs as Evolutionary Optimizers", "authors": ["Nikhil Abhyankar", "Parshin Shojaee", "Chandan K. Reddy"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NE"], "comment": null, "summary": "Automated feature engineering plays a critical role in improving predictive\nmodel performance for tabular learning tasks. Traditional automated feature\nengineering methods are limited by their reliance on pre-defined\ntransformations within fixed, manually designed search spaces, often neglecting\ndomain knowledge. Recent advances using Large Language Models (LLMs) have\nenabled the integration of domain knowledge into the feature engineering\nprocess. However, existing LLM-based approaches use direct prompting or rely\nsolely on validation scores for feature selection, failing to leverage insights\nfrom prior feature discovery experiments or establish meaningful reasoning\nbetween feature generation and data-driven performance. To address these\nchallenges, we propose LLM-FE, a novel framework that combines evolutionary\nsearch with the domain knowledge and reasoning capabilities of LLMs to\nautomatically discover effective features for tabular learning tasks. LLM-FE\nformulates feature engineering as a program search problem, where LLMs propose\nnew feature transformation programs iteratively, and data-driven feedback\nguides the search process. Our results demonstrate that LLM-FE consistently\noutperforms state-of-the-art baselines, significantly enhancing the performance\nof tabular prediction models across diverse classification and regression\nbenchmarks."}
{"id": "2503.20576", "pdf": "https://arxiv.org/pdf/2503.20576.pdf", "abs": "https://arxiv.org/abs/2503.20576", "title": "Optimizing Case-Based Reasoning System for Functional Test Script Generation with Large Language Models", "authors": ["Siyuan Guo", "Huiwu Liu", "Xiaolong Chen", "Yuming Xie", "Liang Zhang", "Tao Han", "Hechang Chen", "Yi Chang", "Jun Wang"], "categories": ["cs.SE", "cs.CL", "cs.LG"], "comment": "Accepted by KDD 2025 (ADS Track)", "summary": "In this work, we explore the potential of large language models (LLMs) for\ngenerating functional test scripts, which necessitates understanding the\ndynamically evolving code structure of the target software. To achieve this, we\npropose a case-based reasoning (CBR) system utilizing a 4R cycle (i.e.,\nretrieve, reuse, revise, and retain), which maintains and leverages a case bank\nof test intent descriptions and corresponding test scripts to facilitate LLMs\nfor test script generation. To improve user experience further, we introduce\nRe4, an optimization method for the CBR system, comprising reranking-based\nretrieval finetuning and reinforced reuse finetuning. Specifically, we first\nidentify positive examples with high semantic and script similarity, providing\nreliable pseudo-labels for finetuning the retriever model without costly\nlabeling. Then, we apply supervised finetuning, followed by a reinforcement\nlearning finetuning stage, to align LLMs with our production scenarios,\nensuring the faithful reuse of retrieved cases. Extensive experimental results\non two product development units from Huawei Datacom demonstrate the\nsuperiority of the proposed CBR+Re4. Notably, we also show that the proposed\nRe4 method can help alleviate the repetitive generation issues with LLMs."}
{"id": "2505.04948", "pdf": "https://arxiv.org/pdf/2505.04948.pdf", "abs": "https://arxiv.org/abs/2505.04948", "title": "Prompt-Based LLMs for Position Bias-Aware Reranking in Personalized Recommendations", "authors": ["Md Aminul Islam", "Ahmed Sayeed Faruk"], "categories": ["cs.IR", "cs.CL"], "comment": "We have decided to withdraw the manuscript as it requires substantial\n  revisions that go beyond what is appropriate for a versioned update on arXiv.\n  We plan to resubmit once the necessary improvements are made", "summary": "Recommender systems are essential for delivering personalized content across\ndigital platforms by modeling user preferences and behaviors. Recently, large\nlanguage models (LLMs) have been adopted for prompt-based recommendation due to\ntheir ability to generate personalized outputs without task-specific training.\nHowever, LLM-based methods face limitations such as limited context window\nsize, inefficient pointwise and pairwise prompting, and difficulty handling\nlistwise ranking due to token constraints. LLMs can also be sensitive to\nposition bias, as they may overemphasize earlier items in the prompt regardless\nof their true relevance. To address and investigate these issues, we propose a\nhybrid framework that combines a traditional recommendation model with an LLM\nfor reranking top-k items using structured prompts. We evaluate the effects of\nuser history reordering and instructional prompts for mitigating position bias.\nExperiments on MovieLens-100K show that randomizing user history improves\nranking quality, but LLM-based reranking does not outperform the base model.\nExplicit instructions to reduce position bias are also ineffective. Our\nevaluations reveal limitations in LLMs' ability to model ranking context and\nmitigate bias. Our code is publicly available at\nhttps://github.com/aminul7506/LLMForReRanking."}
{"id": "2505.10222", "pdf": "https://arxiv.org/pdf/2505.10222.pdf", "abs": "https://arxiv.org/abs/2505.10222", "title": "ComplexFormer: Disruptively Advancing Transformer Inference Ability via Head-Specific Complex Vector Attention", "authors": ["Jintian Shao", "Hongyi Huang", "Jiayi Wu", "Beiwen Zhang", "ZhiYu Wu", "You Shan", "MingKai Zheng"], "categories": ["cs.LG", "cs.CL"], "comment": "We are withdrawing this submission as the underlying experiment is\n  currently incomplete. We require additional time to gather more data and\n  supplement the existing findings to ensure a comprehensive and robust\n  presentation. We intend to resubmit once these additions are finalized", "summary": "Transformer models rely on self-attention to capture token dependencies but\nface challenges in effectively integrating positional information while\nallowing multi-head attention (MHA) flexibility. Prior methods often model\nsemantic and positional differences disparately or apply uniform positional\nadjustments across heads, potentially limiting representational capacity. This\npaper introduces ComplexFormer, featuring Complex Multi-Head Attention-CMHA.\nCMHA empowers each head to independently model semantic and positional\ndifferences unified within the complex plane, representing interactions as\nrotations and scaling. ComplexFormer incorporates two key improvements: (1) a\nper-head Euler transformation, converting real-valued query/key projections\ninto polar-form complex vectors for head-specific complex subspace operation;\nand (2) a per-head adaptive differential rotation mechanism,\nexp[i(Adapt(ASmn,i) + Delta(Pmn),i)], allowing each head to learn distinct\nstrategies for integrating semantic angle differences (ASmn,i) with relative\npositional encodings (Delta(Pmn),i). Extensive experiments on language\nmodeling, text generation, code generation, and mathematical reasoning show\nComplexFormer achieves superior performance, significantly lower generation\nperplexity , and improved long-context coherence compared to strong baselines\nlike RoPE-Transformers. ComplexFormer demonstrates strong parameter efficiency,\noffering a more expressive, adaptable attention mechanism."}
{"id": "2505.10610", "pdf": "https://arxiv.org/pdf/2505.10610.pdf", "abs": "https://arxiv.org/abs/2505.10610", "title": "MMLongBench: Benchmarking Long-Context Vision-Language Models Effectively and Thoroughly", "authors": ["Zhaowei Wang", "Wenhao Yu", "Xiyu Ren", "Jipeng Zhang", "Yu Zhao", "Rohit Saxena", "Liang Cheng", "Ginny Wong", "Simon See", "Pasquale Minervini", "Yangqiu Song", "Mark Steedman"], "categories": ["cs.CV", "cs.CL"], "comment": "Work in progress", "summary": "The rapid extension of context windows in large vision-language models has\ngiven rise to long-context vision-language models (LCVLMs), which are capable\nof handling hundreds of images with interleaved text tokens in a single forward\npass. In this work, we introduce MMLongBench, the first benchmark covering a\ndiverse set of long-context vision-language tasks, to evaluate LCVLMs\neffectively and thoroughly. MMLongBench is composed of 13,331 examples spanning\nfive different categories of downstream tasks, such as Visual RAG and Many-Shot\nICL. It also provides broad coverage of image types, including various natural\nand synthetic images. To assess the robustness of the models to different input\nlengths, all examples are delivered at five standardized input lengths (8K-128K\ntokens) via a cross-modal tokenization scheme that combines vision patches and\ntext tokens. Through a thorough benchmarking of 46 closed-source and\nopen-source LCVLMs, we provide a comprehensive analysis of the current models'\nvision-language long-context ability. Our results show that: i) performance on\na single task is a weak proxy for overall long-context capability; ii) both\nclosed-source and open-source models face challenges in long-context\nvision-language tasks, indicating substantial room for future improvement; iii)\nmodels with stronger reasoning ability tend to exhibit better long-context\nperformance. By offering wide task coverage, various image types, and rigorous\nlength control, MMLongBench provides the missing foundation for diagnosing and\nadvancing the next generation of LCVLMs."}
{"id": "2505.12312", "pdf": "https://arxiv.org/pdf/2505.12312.pdf", "abs": "https://arxiv.org/abs/2505.12312", "title": "Visuospatial Cognitive Assistant", "authors": ["Qi Feng"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "comment": "31 pages, 10 figures, 6 tables. The implementation and fine-tuned\n  model (ViCA-7B), along with detailed documentation, are publicly available at\n  https://huggingface.co/nkkbr/ViCA. This is a draft technical report. At\n  Professor Hidetoshi Shimodaira's request, his name has been removed from the\n  author list", "summary": "Video-based spatial cognition is vital for robotics and embodied AI but\nchallenges current Vision-Language Models (VLMs). This paper makes two key\ncontributions. First, we introduce ViCA (Visuospatial Cognitive\nAssistant)-322K, a diverse dataset of 322,003 QA pairs from real-world indoor\nvideos (ARKitScenes, ScanNet, ScanNet++), offering supervision for 3D\nmetadata-grounded queries and video-based complex reasoning. Second, we develop\nViCA-7B, fine-tuned on ViCA-322K, which achieves new state-of-the-art on all\neight VSI-Bench tasks, outperforming existing models, including larger ones\n(e.g., +26.1 on Absolute Distance). For interpretability, we present\nViCA-Thinking-2.68K, a dataset with explicit reasoning chains, and fine-tune\nViCA-7B to create ViCA-7B-Thinking, a model that articulates its spatial\nreasoning. Our work highlights the importance of targeted data and suggests\npaths for improved temporal-spatial modeling. We release all resources to\nfoster research in robust visuospatial intelligence."}
{"id": "2505.12363", "pdf": "https://arxiv.org/pdf/2505.12363.pdf", "abs": "https://arxiv.org/abs/2505.12363", "title": "Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts", "authors": ["Qi Feng"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "comment": "26 pages, 19 figures, 4 tables. Code, models, and datasets are\n  available at our project page: https://github.com/nkkbr/ViCA. This is a draft\n  technical report. At the request of Professor Hidetoshi Shimodaira, his name\n  has been removed from the author list", "summary": "While Multimodal Large Language Models (MLLMs) excel at general\nvision-language tasks, visuospatial cognition - reasoning about spatial\nlayouts, relations, and dynamics - remains a significant challenge. Existing\nmodels often lack the necessary architectural components and specialized\ntraining data for fine-grained spatial understanding. We introduce ViCA2\n(Visuospatial Cognitive Assistant 2), a novel MLLM designed to enhance spatial\nreasoning. ViCA2 features a dual vision encoder architecture integrating SigLIP\nfor semantics and Hiera for spatial structure, coupled with a token ratio\ncontrol mechanism for efficiency. We also developed ViCA-322K, a new\nlarge-scale dataset with over 322,000 spatially grounded question-answer pairs\nfor targeted instruction tuning. On the challenging VSI-Bench benchmark, our\nViCA2-7B model achieves a state-of-the-art average score of 56.8, significantly\nsurpassing larger open-source models (e.g., LLaVA-NeXT-Video-72B, 40.9) and\nleading proprietary models (Gemini-1.5 Pro, 45.4). This demonstrates the\neffectiveness of our approach in achieving strong visuospatial intelligence\nwith a compact model. We release ViCA2, its codebase, and the ViCA-322K dataset\nto facilitate further research."}
{"id": "2505.14667", "pdf": "https://arxiv.org/pdf/2505.14667.pdf", "abs": "https://arxiv.org/abs/2505.14667", "title": "SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early Alignment", "authors": ["Wonje Jeung", "Sangyeon Yoon", "Minsuk Kahng", "Albert No"], "categories": ["cs.AI", "cs.CL"], "comment": "22 pages", "summary": "Large Reasoning Models (LRMs) have become powerful tools for complex problem\nsolving, but their structured reasoning pathways can lead to unsafe outputs\nwhen exposed to harmful prompts. Existing safety alignment methods reduce\nharmful outputs but can degrade reasoning depth, leading to significant\ntrade-offs in complex, multi-step tasks, and remain vulnerable to sophisticated\njailbreak attacks. To address this, we introduce SAFEPATH, a lightweight\nalignment method that fine-tunes LRMs to emit a short, 8-token Safety Primer at\nthe start of their reasoning, in response to harmful prompts, while leaving the\nrest of the reasoning process unsupervised. Empirical results across multiple\nbenchmarks indicate that SAFEPATH effectively reduces harmful outputs while\nmaintaining reasoning performance. Specifically, SAFEPATH reduces harmful\nresponses by up to 90.0% and blocks 83.3% of jailbreak attempts in the\nDeepSeek-R1-Distill-Llama-8B model, while requiring 295.9x less compute than\nDirect Refusal and 314.1x less than SafeChain. We further introduce a zero-shot\nvariant that requires no fine-tuning. In addition, we provide a comprehensive\nanalysis of how existing methods in LLMs generalize, or fail, when applied to\nreasoning-centric models, revealing critical gaps and new directions for safer\nAI."}
{"id": "2505.14681", "pdf": "https://arxiv.org/pdf/2505.14681.pdf", "abs": "https://arxiv.org/abs/2505.14681", "title": "Two Experts Are All You Need for Steering Thinking: Reinforcing Cognitive Effort in MoE Reasoning Models Without Additional Training", "authors": ["Mengru Wang", "Xingyu Chen", "Yue Wang", "Zhiwei He", "Jiahao Xu", "Tian Liang", "Qiuzhi Liu", "Yunzhi Yao", "Wenxuan Wang", "Ruotian Ma", "Haitao Mi", "Ningyu Zhang", "Zhaopeng Tu", "Xiaolong Li", "Dong Yu"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.IR", "cs.LG"], "comment": "Work in progress", "summary": "Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs)\nhave achieved impressive reasoning capabilities by selectively activating\nexperts to facilitate structured cognitive processes. Despite notable advances,\nexisting reasoning models often suffer from cognitive inefficiencies like\noverthinking and underthinking. To address these limitations, we introduce a\nnovel inference-time steering methodology called Reinforcing Cognitive Experts\n(RICE), designed to improve reasoning performance without additional training\nor complex heuristics. Leveraging normalized Pointwise Mutual Information\n(nPMI), we systematically identify specialized experts, termed ''cognitive\nexperts'' that orchestrate meta-level reasoning operations characterized by\ntokens like ''<think>''. Empirical evaluations with leading MoE-based LRMs\n(DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning\nbenchmarks demonstrate noticeable and consistent improvements in reasoning\naccuracy, cognitive efficiency, and cross-domain generalization. Crucially, our\nlightweight approach substantially outperforms prevalent reasoning-steering\ntechniques, such as prompt design and decoding constraints, while preserving\nthe model's general instruction-following skills. These results highlight\nreinforcing cognitive experts as a promising, practical, and interpretable\ndirection to enhance cognitive efficiency within advanced reasoning models."}
{"id": "2505.14910", "pdf": "https://arxiv.org/pdf/2505.14910.pdf", "abs": "https://arxiv.org/abs/2505.14910", "title": "TCSinger 2: Customizable Multilingual Zero-shot Singing Voice Synthesis", "authors": ["Yu Zhang", "Wenxiang Guo", "Changhao Pan", "Dongyu Yao", "Zhiyuan Zhu", "Ziyue Jiang", "Yuhan Wang", "Tao Jin", "Zhou Zhao"], "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted by Findings of ACL 2025", "summary": "Customizable multilingual zero-shot singing voice synthesis (SVS) has various\npotential applications in music composition and short video dubbing. However,\nexisting SVS models overly depend on phoneme and note boundary annotations,\nlimiting their robustness in zero-shot scenarios and producing poor transitions\nbetween phonemes and notes. Moreover, they also lack effective multi-level\nstyle control via diverse prompts. To overcome these challenges, we introduce\nTCSinger 2, a multi-task multilingual zero-shot SVS model with style transfer\nand style control based on various prompts. TCSinger 2 mainly includes three\nkey modules: 1) Blurred Boundary Content (BBC) Encoder, predicts duration,\nextends content embedding, and applies masking to the boundaries to enable\nsmooth transitions. 2) Custom Audio Encoder, uses contrastive learning to\nextract aligned representations from singing, speech, and textual prompts. 3)\nFlow-based Custom Transformer, leverages Cus-MOE, with F0 supervision,\nenhancing both the synthesis quality and style modeling of the generated\nsinging voice. Experimental results show that TCSinger 2 outperforms baseline\nmodels in both subjective and objective metrics across multiple related tasks.\nSinging voice samples are available at\nhttps://aaronz345.github.io/TCSinger2Demo/."}
{"id": "2505.17473", "pdf": "https://arxiv.org/pdf/2505.17473.pdf", "abs": "https://arxiv.org/abs/2505.17473", "title": "OrionBench: A Benchmark for Chart and Human-Recognizable Object Detection in Infographics", "authors": ["Jiangning Zhu", "Yuxing Zhou", "Zheng Wang", "Juntao Yao", "Yima Gu", "Yuhui Yuan", "Shixia Liu"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Given the central role of charts in scientific, business, and communication\ncontexts, enhancing the chart understanding capabilities of vision-language\nmodels (VLMs) has become increasingly critical. A key limitation of existing\nVLMs lies in their inaccurate visual grounding of infographic elements,\nincluding charts and human-recognizable objects (HROs) such as icons and\nimages. However, chart understanding often requires identifying relevant\nelements and reasoning over them. To address this limitation, we introduce\nOrionBench, a benchmark designed to support the development of accurate object\ndetection models for charts and HROs in infographics. It contains 26,250 real\nand 78,750 synthetic infographics, with over 6.9 million bounding box\nannotations. These annotations are created by combining the model-in-the-loop\nand programmatic methods. We demonstrate the usefulness of OrionBench through\nthree applications: 1) constructing a Thinking-with-Boxes scheme to boost the\nchart understanding performance of VLMs, 2) comparing existing object detection\nmodels, and 3) applying the developed detection model to document layout and UI\nelement detection."}
{"id": "2505.17595", "pdf": "https://arxiv.org/pdf/2505.17595.pdf", "abs": "https://arxiv.org/abs/2505.17595", "title": "NeUQI: Near-Optimal Uniform Quantization Parameter Initialization", "authors": ["Li Lin", "Xinyu Hu", "Xiaojun Wan"], "categories": ["cs.LG", "cs.CL"], "comment": "9 pages, under review", "summary": "Large language models (LLMs) achieve impressive performance across domains\nbut face significant challenges when deployed on consumer-grade GPUs or\npersonal devices such as laptops, due to high memory consumption and inference\ncosts. Post-training quantization (PTQ) of LLMs offers a promising solution\nthat reduces their memory footprint and decoding latency. In practice, PTQ with\nuniform quantization representation is favored for its efficiency and ease of\ndeployment since uniform quantization is widely supported by mainstream\nhardware and software libraries. Recent studies on $\\geq 2$-bit uniform\nquantization have led to noticeable improvements in post-quantization model\nperformance; however, they primarily focus on quantization methodologies, while\nthe initialization of quantization parameters is underexplored and still relies\non the suboptimal Min-Max strategies. In this work, we propose NeUQI, a method\ndevoted to efficiently determining near-optimal initial parameters for uniform\nquantization. NeUQI is orthogonal to prior quantization methodologies and can\nseamlessly integrate with them. The experiments with the LLaMA and Qwen\nfamilies on various tasks demonstrate that our NeUQI consistently outperforms\nexisting methods. Furthermore, when combined with a lightweight distillation\nstrategy, NeUQI can achieve superior performance to PV-tuning, a much more\nresource-intensive approach."}
{"id": "2505.17997", "pdf": "https://arxiv.org/pdf/2505.17997.pdf", "abs": "https://arxiv.org/abs/2505.17997", "title": "Towards Analyzing and Understanding the Limitations of VAPO: A Theoretical Perspective", "authors": ["Jintian Shao", "Yiming Cheng", "Hongyi Huang", "Beiwen Zhang", "Zhiyu Wu", "You Shan", "Mingkai Zheng"], "categories": ["cs.LG", "cs.CL"], "comment": "We are withdrawing this submission as the underlying experiment is\n  currently incomplete. We require additional time to gather more data and\n  supplement the existing findings to ensure a comprehensive and robust\n  presentation. We intend to resubmit once these additions are finalized", "summary": "The VAPO framework has demonstrated significant empirical success in\nenhancing the efficiency and reliability of reinforcement learning for long\nchain-of-thought (CoT) reasoning tasks with large language models (LLMs). By\nsystematically addressing challenges such as value model bias, heterogeneous\nsequence lengths, and sparse reward signals, VAPO achieves state-of-the-art\nperformance. While its practical benefits are evident, a deeper theoretical\nunderstanding of its underlying mechanisms and potential limitations is crucial\nfor guiding future advancements. This paper aims to initiate such a discussion\nby exploring VAPO from a theoretical perspective, highlighting areas where its\nassumptions might be challenged and where further investigation could yield\nmore robust and generalizable reasoning agents. We delve into the intricacies\nof value function approximation in complex reasoning spaces, the optimality of\nadaptive advantage estimation, the impact of token-level optimization, and the\nenduring challenges of exploration and generalization."}
{"id": "2505.18034", "pdf": "https://arxiv.org/pdf/2505.18034.pdf", "abs": "https://arxiv.org/abs/2505.18034", "title": "Structured Thinking Matters: Improving LLMs Generalization in Causal Inference Tasks", "authors": ["Wentao Sun", "João Paulo Nogueira", "Alonso Silva"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Despite remarkable advances in the field, LLMs remain unreliable in\ndistinguishing causation from correlation. Recent results from the Corr2Cause\ndataset benchmark reveal that state-of-the-art LLMs -- such as GPT-4 (F1 score:\n29.08) -- only marginally outperform random baselines (Random Uniform, F1\nscore: 20.38), indicating limited capacity of generalization. To tackle this\nlimitation, we propose a novel structured approach: rather than directly\nanswering causal queries, we provide the model with the capability to structure\nits thinking by guiding the model to build a structured knowledge graph,\nsystematically encoding the provided correlational premises, to answer the\ncausal queries. This intermediate representation significantly enhances the\nmodel's causal capabilities. Experiments on the test subset of the Corr2Cause\ndataset benchmark with Qwen3-32B model (reasoning model) show substantial gains\nover standard direct prompting methods, improving F1 scores from 32.71 to 48.26\n(over 47.5% relative increase), along with notable improvements in precision\nand recall. These results underscore the effectiveness of providing the model\nwith the capability to structure its thinking and highlight its promising\npotential for broader generalization across diverse causal inference tasks."}
{"id": "2505.18458", "pdf": "https://arxiv.org/pdf/2505.18458.pdf", "abs": "https://arxiv.org/abs/2505.18458", "title": "A Survey of LLM $\\times$ DATA", "authors": ["Xuanhe Zhou", "Junxuan He", "Wei Zhou", "Haodong Chen", "Zirui Tang", "Haoyu Zhao", "Xin Tong", "Guoliang Li", "Youmin Chen", "Jun Zhou", "Zhaojun Sun", "Binyuan Hui", "Shuo Wang", "Conghui He", "Zhiyuan Liu", "Jingren Zhou", "Fan Wu"], "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.IR", "cs.LG"], "comment": "Please refer to the paper list at:\n  https://github.com/weAIDB/awesome-data-llm", "summary": "The integration of large language model (LLM) and data management (DATA) is\nrapidly redefining both domains. In this survey, we comprehensively review the\nbidirectional relationships. On the one hand, DATA4LLM, spanning large-scale\ndata processing, storage, and serving, feeds LLMs with high quality, diversity,\nand timeliness of data required for stages like pre-training, post-training,\nretrieval-augmented generation, and agentic workflows: (i) Data processing for\nLLMs includes scalable acquisition, deduplication, filtering, selection, domain\nmixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on\nefficient data and model formats, distributed and heterogeneous storage\nhierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data\nserving for LLMs tackles challenges in RAG (e.g., knowledge post-processing),\nLLM inference (e.g., prompt compression, data provenance), and training\nstrategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA,\nLLMs are emerging as general-purpose engines for data management. We review\nrecent advances in (i) data manipulation, including automatic data cleaning,\nintegration, discovery; (ii) data analysis, covering reasoning over structured,\nsemi-structured, and unstructured data, and (iii) system optimization (e.g.,\nconfiguration tuning, query rewriting, anomaly diagnosis), powered by LLM\ntechniques like retrieval-augmented prompting, task-specialized fine-tuning,\nand multi-agent collaboration."}
{"id": "2505.18585", "pdf": "https://arxiv.org/pdf/2505.18585.pdf", "abs": "https://arxiv.org/abs/2505.18585", "title": "RvLLM: LLM Runtime Verification with Domain Knowledge", "authors": ["Yedi Zhang", "Sun Yi Emma", "Annabelle Lee Jia En", "Jin Song Dong"], "categories": ["cs.AI", "cs.CL", "cs.LO"], "comment": "12 pages, 2 figures", "summary": "Large language models (LLMs) have emerged as a dominant AI paradigm due to\ntheir exceptional text understanding and generation capabilities. However,\ntheir tendency to generate inconsistent or erroneous outputs challenges their\nreliability, especially in high-stakes domains requiring accuracy and\ntrustworthiness. Existing research primarily focuses on detecting and\nmitigating model misbehavior in general-purpose scenarios, often overlooking\nthe potential of integrating domain-specific knowledge. In this work, we\nadvance misbehavior detection by incorporating domain knowledge. The core idea\nis to design a general specification language that enables domain experts to\ncustomize domain-specific predicates in a lightweight and intuitive manner,\nsupporting later runtime verification of LLM outputs. To achieve this, we\ndesign a novel specification language, ESL, and introduce a runtime\nverification framework, RvLLM, to validate LLM output against domain-specific\nconstraints defined in ESL. We evaluate RvLLM on three representative tasks:\nviolation detection against Singapore Rapid Transit Systems Act, numerical\ncomparison, and inequality solving. Experimental results demonstrate that RvLLM\neffectively detects erroneous outputs across various LLMs in a lightweight and\nflexible manner. The results reveal that despite their impressive capabilities,\nLLMs remain prone to low-level errors due to limited interpretability and a\nlack of formal guarantees during inference, and our framework offers a\npotential long-term solution by leveraging expert domain knowledge to\nrigorously and efficiently verify LLM outputs."}
{"id": "2505.18942", "pdf": "https://arxiv.org/pdf/2505.18942.pdf", "abs": "https://arxiv.org/abs/2505.18942", "title": "Language Models Surface the Unwritten Code of Science and Society", "authors": ["Honglin Bao", "Siyang Wu", "Jiwoong Choi", "Yingrong Mao", "James A. Evans"], "categories": ["cs.CY", "cs.CL", "cs.DL"], "comment": null, "summary": "This paper calls on the research community not only to investigate how human\nbiases are inherited by large language models (LLMs) but also to explore how\nthese biases in LLMs can be leveraged to make society's \"unwritten code\" - such\nas implicit stereotypes and heuristics - visible and accessible for critique.\nWe introduce a conceptual framework through a case study in science: uncovering\nhidden rules in peer review - the factors that reviewers care about but rarely\nstate explicitly due to normative scientific expectations. The idea of the\nframework is to push LLMs to speak out their heuristics through generating\nself-consistent hypotheses - why one paper appeared stronger in reviewer\nscoring - among paired papers submitted to 45 computer science conferences,\nwhile iteratively searching deeper hypotheses from remaining pairs where\nexisting hypotheses cannot explain. We observed that LLMs' normative priors\nabout the internal characteristics of good science extracted from their\nself-talk, e.g. theoretical rigor, were systematically updated toward\nposteriors that emphasize storytelling about external connections, such as how\nthe work is positioned and connected within and across literatures. This shift\nreveals the primacy of scientific myths about intrinsic properties driving\nscientific excellence rather than extrinsic contextualization and storytelling\nthat influence conceptions of relevance and significance. Human reviewers tend\nto explicitly reward aspects that moderately align with LLMs' normative priors\n(correlation = 0.49) but avoid articulating contextualization and storytelling\nposteriors in their review comments (correlation = -0.14), despite giving\nimplicit reward to them with positive scores. We discuss the broad\napplicability of the framework, leveraging LLMs as diagnostic tools to surface\nthe tacit codes underlying human society, enabling more precisely targeted\nresponsible AI."}
{"id": "2505.19075", "pdf": "https://arxiv.org/pdf/2505.19075.pdf", "abs": "https://arxiv.org/abs/2505.19075", "title": "Universal Reasoner: A Single, Composable Plug-and-Play Reasoner for Frozen LLMs", "authors": ["Jaemin Kim", "Hangeol Chang", "Hyunmin Hwang", "Choonghan Kim", "Jong Chul Ye"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "22 pages, typos corrected", "summary": "Large Language Models (LLMs) have demonstrated remarkable general\ncapabilities, but enhancing skills such as reasoning often demands substantial\ncomputational resources and may compromise their generalization. While\nParameter-Efficient Fine-Tuning (PEFT) methods offer a more resource-conscious\nalternative, they typically requires retraining for each LLM backbone due to\narchitectural dependencies. To address these challenges, here we propose\nUniversal Reasoner (UniR) - a single, lightweight, composable, and\nplug-and-play reasoning module that can be used with any frozen LLM to endow it\nwith specialized reasoning capabilities. Specifically, UniR decomposes the\nreward into a standalone reasoning module that is trained independently using\npredefined rewards, effectively translating trajectory-level signals into\ntoken-level guidance. Once trained, UniR can be combined with any frozen LLM at\ninference time by simply adding its output logits to those of the LLM backbone.\nThis additive structure naturally enables modular composition: multiple UniR\nmodules trained for different tasks can be jointly applied by summing their\nlogits, enabling complex reasoning via composition. Experimental results on\nmathematical reasoning and machine translation tasks show that UniR\nsignificantly outperforms existing baseline fine-tuning methods using the\nLlama3.2 model. Furthermore, UniR demonstrates strong weak-to-strong\ngeneralization: reasoning modules trained on smaller models effectively guide\nmuch larger LLMs. This makes UniR a cost-efficient, adaptable, and robust\nsolution for enhancing reasoning in LLMs without compromising their core\ncapabilities. Code is open-sourced at https://github.com/hangeol/UniR"}
{"id": "2505.19641", "pdf": "https://arxiv.org/pdf/2505.19641.pdf", "abs": "https://arxiv.org/abs/2505.19641", "title": "SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning Logical Reasoning and Beyond", "authors": ["Junteng Liu", "Yuanxiang Fan", "Zhuo Jiang", "Han Ding", "Yongyi Hu", "Chi Zhang", "Yiqi Shi", "Shitong Weng", "Aili Chen", "Shiqi Chen", "Yunan Huang", "Mozhi Zhang", "Pengyu Zhao", "Junjie Yan", "Junxian He"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances such as OpenAI-o1 and DeepSeek R1 have demonstrated the\npotential of Reinforcement Learning (RL) to enhance reasoning abilities in\nLarge Language Models (LLMs). While open-source replication efforts have\nprimarily focused on mathematical and coding domains, methods and resources for\ndeveloping general reasoning capabilities remain underexplored. This gap is\npartly due to the challenge of collecting diverse and verifiable reasoning data\nsuitable for RL. We hypothesize that logical reasoning is critical for\ndeveloping general reasoning capabilities, as logic forms a fundamental\nbuilding block of reasoning. In this work, we present SynLogic, a data\nsynthesis framework and dataset that generates diverse logical reasoning data\nat scale, encompassing 35 diverse logical reasoning tasks. The SynLogic\napproach enables controlled synthesis of data with adjustable difficulty and\nquantity. Importantly, all examples can be verified by simple rules, making\nthem ideally suited for RL with verifiable rewards. In our experiments, we\nvalidate the effectiveness of RL training on the SynLogic dataset based on 7B\nand 32B models. SynLogic leads to state-of-the-art logical reasoning\nperformance among open-source datasets, surpassing DeepSeek-R1-Distill-Qwen-32B\nby 6 points on BBEH. Furthermore, mixing SynLogic data with mathematical and\ncoding tasks improves the training efficiency of these domains and\nsignificantly enhances reasoning generalization. Notably, our mixed training\nmodel outperforms DeepSeek-R1-Zero-Qwen-32B across multiple benchmarks. These\nfindings position SynLogic as a valuable resource for advancing the broader\nreasoning capabilities of LLMs. We open-source both the data synthesis pipeline\nand the SynLogic dataset at https://github.com/MiniMax-AI/SynLogic."}
{"id": "2505.20103", "pdf": "https://arxiv.org/pdf/2505.20103.pdf", "abs": "https://arxiv.org/abs/2505.20103", "title": "SCIRGC: Multi-Granularity Citation Recommendation and Citation Sentence Preference Alignment", "authors": ["Xiangyu Li", "Jingqiang Chen"], "categories": ["cs.DL", "cs.CL"], "comment": "15 pages, 7 figures", "summary": "Citations are crucial in scientific research articles as they highlight the\nconnection between the current study and prior work. However, this process is\noften time-consuming for researchers. In this study, we propose the SciRGC\nframework, which aims to automatically recommend citation articles and generate\ncitation sentences for citation locations within articles. The framework\naddresses two key challenges in academic citation generation: 1) how to\naccurately identify the author's citation intent and find relevant citation\npapers, and 2) how to generate high-quality citation sentences that align with\nhuman preferences. We enhance citation recommendation accuracy in the citation\narticle recommendation module by incorporating citation networks and sentiment\nintent, and generate reasoning-based citation sentences in the citation\nsentence generation module by using the original article abstract, local\ncontext, citation intent, and recommended articles as inputs. Additionally, we\npropose a new evaluation metric to fairly assess the quality of generated\ncitation sentences. Through comparisons with baseline models and ablation\nexperiments, the SciRGC framework not only improves the accuracy and relevance\nof citation recommendations but also ensures the appropriateness of the\ngenerated citation sentences in context, providing a valuable tool for\ninterdisciplinary researchers."}
