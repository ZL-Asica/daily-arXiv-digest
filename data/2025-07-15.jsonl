{"id": "2507.08804", "pdf": "https://arxiv.org/pdf/2507.08804.pdf", "abs": "https://arxiv.org/abs/2507.08804", "title": "Cognitive Dissonance Artificial Intelligence (CD-AI): The Mind at War with Itself. Harnessing Discomfort to Sharpen Critical Thinking", "authors": ["Delia Deliu"], "categories": ["cs.HC", "cs.CY"], "comment": "Presented at the 2025 ACM Workshop on Human-AI Interaction for\n  Augmented Reasoning, Report Number: CHI25-WS-AUGMENTED-REASONING", "summary": "AI-augmented systems are traditionally designed to streamline human\ndecision-making by minimizing cognitive load, clarifying arguments, and\noptimizing efficiency. However, in a world where algorithmic certainty risks\nbecoming an Orwellian tool of epistemic control, true intellectual growth\ndemands not passive acceptance but active struggle. Drawing on the dystopian\nvisions of George Orwell and Philip K. Dick - where reality is unstable,\nperception malleable, and truth contested - this paper introduces Cognitive\nDissonance AI (CD-AI): a novel framework that deliberately sustains uncertainty\nrather than resolving it. CD-AI does not offer closure, but compels users to\nnavigate contradictions, challenge biases, and wrestle with competing truths.\nBy delaying resolution and promoting dialectical engagement, CD-AI enhances\nreflective reasoning, epistemic humility, critical thinking, and adaptability\nin complex decision-making. This paper examines the theoretical foundations of\nthe approach, presents an implementation model, explores its application in\ndomains such as ethics, law, politics, and science, and addresses key ethical\nconcerns - including decision paralysis, erosion of user autonomy, cognitive\nmanipulation, and bias in AI reasoning. In reimagining AI as an engine of doubt\nrather than a deliverer of certainty, CD-AI challenges dominant paradigms of\nAI-augmented reasoning and offers a new vision - one in which AI sharpens the\nmind not by resolving conflict, but by sustaining it. Rather than reinforcing\nHuxleyan complacency or pacifying the user into intellectual conformity, CD-AI\nechoes Nietzsche's vision of the Uebermensch - urging users to transcend\npassive cognition through active epistemic struggle."}
{"id": "2507.08805", "pdf": "https://arxiv.org/pdf/2507.08805.pdf", "abs": "https://arxiv.org/abs/2507.08805", "title": "Non-linear, Team-based VR Training for Cardiac Arrest Care with enhanced CRM Toolkit", "authors": ["Mike Kentros", "Manos Kamarianakis", "Michael Cole", "Vitaliy Popov", "Antonis Protopsaltis", "George Papagiannakis"], "categories": ["cs.HC", "cs.CY", "cs.GR"], "comment": "4 pages, 3 figures, 1 table", "summary": "This paper introduces iREACT, a novel VR simulation addressing key\nlimitations in traditional cardiac arrest (CA) training. Conventional methods\nstruggle to replicate the dynamic nature of real CA events, hindering Crew\nResource Management (CRM) skill development. iREACT provides a non-linear,\ncollaborative environment where teams respond to changing patient states,\nmirroring real CA complexities. By capturing multi-modal data (user actions,\ncognitive load, visual gaze) and offering real-time and post-session feedback,\niREACT enhances CRM assessment beyond traditional methods. A formative\nevaluation with medical experts underscores its usability and educational\nvalue, with potential applications in other high-stakes training scenarios to\nimprove teamwork, communication, and decision-making."}
{"id": "2507.08914", "pdf": "https://arxiv.org/pdf/2507.08914.pdf", "abs": "https://arxiv.org/abs/2507.08914", "title": "'Teens Need to Be Educated on the Danger': Digital Access, Online Risks, and Safety Practices Among Nigerian Adolescents", "authors": ["Munachimso B. Oguine", "Ozioma C. Oguine", "Karla Badillo-Urquiola", "Oluwasogo Adekunle Okunade"], "categories": ["cs.HC", "cs.CY"], "comment": "14 pages, 4 figures. Accepted to AfriCHI 2025", "summary": "Adolescents increasingly rely on online technologies to explore their\nidentities, form social connections, and access information and entertainment.\nHowever, their growing digital engagement exposes them to significant online\nrisks, particularly in underrepresented contexts like West Africa. This study\ninvestigates the online experiences of 409 secondary school adolescents in\nNigeria's Federal Capital Territory (FCT), focusing on their access to\ntechnology, exposure to risks, coping strategies, key stakeholders influencing\ntheir online interactions, and recommendations for improving online safety.\nUsing self-administered surveys, we found that while most adolescents reported\nmoderate access to online technology and connectivity, those who encountered\nrisks frequently reported exposure to inappropriate content and online scams.\nBlocking and reporting tools were the most commonly used strategies, though\nsome adolescents responded with inaction due to limited resources or awareness.\nParents emerged as the primary support network, though monitoring practices and\ncommunication varied widely. Guided by Protection Motivation Theory (PMT), our\nanalysis interprets adolescents' online safety behaviors as shaped by both\ntheir threat perceptions and their confidence in available coping strategies. A\nthematic analysis of their recommendations highlights the need for greater\nawareness and education, parental mediation, enhanced safety tools, stricter\nage restrictions, improved content moderation, government accountability, and\nresilience-building initiatives. Our findings underscore the importance of\nculturally and contextually relevant interventions to empower adolescents in\nnavigating the digital world, with implications for parents, educators,\ndesigners, and policymakers."}
{"id": "2507.08973", "pdf": "https://arxiv.org/pdf/2507.08973.pdf", "abs": "https://arxiv.org/abs/2507.08973", "title": "Analytical Study on the Visibility of Potential Positions for External Human-Machine Interfaces", "authors": ["Jose Gonzalez-Belmonte", "Jaerock Kwon"], "categories": ["cs.HC"], "comment": "28 pages, 5 tables, 10 figures", "summary": "As we move towards a future of autonomous vehicles, questions regarding their\nmethod of communication have arisen. One of the common questions concerns the\nplacement of the signaling used to communicate with pedestrians and road users,\nbut little work has been published fully dedicated to exploring this. This\npaper uses a simulation made in the Unity game engine to record the visibility\nof fifteen different vehicles, specifically regarding the visibility of frontal\nelements by a pedestrian on the sidewalk. Variables include the vehicle\nposition, number of vehicles on the road, and minimum and maximum distance of\nthe recorded points. It was concluded that the areas of the vehicle most often\nseen by pedestrians on the sidewalk attempting to cross the road were the\nfrontal frontal fenders and the headlights, with the frontal wheels, frontal\ndoors, bumper, and side mirrors are less visible alternatives. These findings\nare valuable in the future design of signaling for autonomous vehicles, in\norder to ensure pedestrians are able to see them on approaching vehicles. The\nsoftware used provides a platform for similar works in the future to be\nconducted."}
{"id": "2507.08865", "pdf": "https://arxiv.org/pdf/2507.08865.pdf", "abs": "https://arxiv.org/abs/2507.08865", "title": "Spatial ModernBERT: Spatial-Aware Transformer for Table and Key-Value Extraction in Financial Documents at Scale", "authors": ["Javis AI Team", "Amrendra Singh", "Maulik Shah", "Dharshan Sampath"], "categories": ["cs.CL"], "comment": null, "summary": "Extracting tables and key-value pairs from financial documents is essential\nfor business workflows such as auditing, data analytics, and automated invoice\nprocessing. In this work, we introduce Spatial ModernBERT-a transformer-based\nmodel augmented with spatial embeddings-to accurately detect and extract\ntabular data and key-value fields from complex financial documents. We cast the\nextraction task as token classification across three heads: (1) Label Head,\nclassifying each token as a label (e.g., PO Number, PO Date, Item Description,\nQuantity, Base Cost, MRP, etc.); (2) Column Head, predicting column indices;\n(3) Row Head, distinguishing the start of item rows and header rows. The model\nis pretrained on the PubTables-1M dataset, then fine-tuned on a financial\ndocument dataset, achieving robust performance through cross-entropy loss on\neach classification head. We propose a post-processing method to merge tokens\nusing B-I-IB tagging, reconstruct the tabular layout, and extract key-value\npairs. Empirical evaluation shows that Spatial ModernBERT effectively leverages\nboth textual and spatial cues, facilitating highly accurate table and key-value\nextraction in real-world financial documents."}
{"id": "2507.09100", "pdf": "https://arxiv.org/pdf/2507.09100.pdf", "abs": "https://arxiv.org/abs/2507.09100", "title": "AInsight: Augmenting Expert Decision-Making with On-the-Fly Insights Grounded in Historical Data", "authors": ["Mohammad Abolnejadian", "Shakiba Amirshahi", "Matthew Brehmer", "Anamaria Crisan"], "categories": ["cs.HC", "cs.AI", "cs.CL", "H.5.0"], "comment": "7 pages and 4 figures. Proceedings of the 7th ACM Conference on\n  Conversational User Interfaces (CUI '25)", "summary": "In decision-making conversations, experts must navigate complex choices and\nmake on-the-spot decisions while engaged in conversation. Although extensive\nhistorical data often exists, the real-time nature of these scenarios makes it\ninfeasible for decision-makers to review and leverage relevant information.\nThis raises an interesting question: What if experts could utilize relevant\npast data in real-time decision-making through insights derived from past data?\nTo explore this, we implemented a conversational user interface, taking\ndoctor-patient interactions as an example use case. Our system continuously\nlistens to the conversation, identifies patient problems and doctor-suggested\nsolutions, and retrieves related data from an embedded dataset, generating\nconcise insights using a pipeline built around a retrieval-based Large Language\nModel (LLM) agent. We evaluated the prototype by embedding Health Canada\ndatasets into a vector database and conducting simulated studies using sample\ndoctor-patient dialogues, showing effectiveness but also challenges, setting\ndirections for the next steps of our work."}
{"id": "2507.08898", "pdf": "https://arxiv.org/pdf/2507.08898.pdf", "abs": "https://arxiv.org/abs/2507.08898", "title": "SEALGuard: Safeguarding the Multilingual Conversations in Southeast Asian Languages for LLM Software Systems", "authors": ["Wenliang Shan", "Michael Fu", "Rui Yang", "Chakkrit", "Tantithamthavorn"], "categories": ["cs.CL", "cs.AI"], "comment": "Under Review at Information and Software Technology", "summary": "Safety alignment is critical for LLM-powered systems. While recent\nLLM-powered guardrail approaches such as LlamaGuard achieve high detection\naccuracy of unsafe inputs written in English (e.g., ``How to create a bomb?''),\nthey struggle with multilingual unsafe inputs. This limitation leaves LLM\nsystems vulnerable to unsafe and jailbreak prompts written in low-resource\nlanguages such as those in Southeast Asia. This paper introduces SEALGuard, a\nmultilingual guardrail designed to improve the safety alignment across diverse\nlanguages. It aims to address the multilingual safety alignment gap of existing\nguardrails and ensure effective filtering of unsafe and jailbreak prompts in\nLLM-powered systems. We adapt a general-purpose multilingual language model\ninto a multilingual guardrail using low-rank adaptation (LoRA). We construct\nSEALSBench, a large-scale multilingual safety alignment dataset containing over\n260,000 prompts in ten languages, including safe, unsafe, and jailbreak cases.\nWe evaluate SEALGuard against state-of-the-art guardrails such as LlamaGuard on\nthis benchmark. Our findings show that multilingual unsafe and jailbreak\nprompts substantially degrade the performance of the state-of-the-art\nLlamaGuard, which experiences a drop in Defense Success Rate (DSR) by 9% and\n18%, respectively, compared to its performance on English-only prompts. In\ncontrast, SEALGuard outperforms existing guardrails in detecting multilingual\nunsafe and jailbreak prompts, improving DSR by 48% over LlamaGuard and\nachieving the best DSR, precision, and F1-score. Our ablation study further\nreveals the contributions of adaptation strategies and model size to the\noverall performance of SEALGuard. SEALGuard advances the safety alignment of\nLLM systems by introducing an effective multilingual guardrail."}
{"id": "2507.09190", "pdf": "https://arxiv.org/pdf/2507.09190.pdf", "abs": "https://arxiv.org/abs/2507.09190", "title": "User-to-PC Authentication Through Confirmation on Mobile Devices: On Usability and Performance", "authors": ["Andreas Pramendorfer", "Rainhard Dieter Findling"], "categories": ["cs.HC", "cs.CR"], "comment": "Submitted to MoMM 2025", "summary": "Protecting personal computers (PCs) from unauthorized access typically relies\non password authentication, which is know to suffer from cognitive burden and\nweak credentials. As many users nowadays carry mobile devices with advanced\nsecurity features throughout their day, there is an opportunity to leverage\nthese devices to improve authentication to PCs. In this paper we utilize a\ntoken-based passwordless approach where users authenticate to their PC by\nconfirming the authentication request on their smartphones or smartwatches.\nUpon a request to login to the PC, or to evaluate privileges, the PC issues an\nauthentication request that users receive on their mobile devices, where users\ncan confirm or deny the request. We evaluate button tap and biometric\nfingerprint verification as confirmation variants, and compare their\nauthentication duration, success rate, and usability to traditional\npassword-based authentication in a user study with 30 participants and a total\nof 1,200 authentication attempts. Smartwatch-based authentication outperformed\npassword-based authentication and smartphone-based variants in authentication\nduration, while showing comparable success rates. Participants rated\nsmartwatch-based authentication highest in usability, followed by\npassword-based authentication and smartphone-based authentication."}
{"id": "2507.08916", "pdf": "https://arxiv.org/pdf/2507.08916.pdf", "abs": "https://arxiv.org/abs/2507.08916", "title": "Evaluating LLMs in Medicine: A Call for Rigor, Transparency", "authors": ["Mahmoud Alwakeel", "Aditya Nagori", "Vijay Krishnamoorthy", "Rishikesan Kamaleswaran"], "categories": ["cs.CL"], "comment": null, "summary": "Objectives: To evaluate the current limitations of large language models\n(LLMs) in medical question answering, focusing on the quality of datasets used\nfor their evaluation. Materials and Methods: Widely-used benchmark datasets,\nincluding MedQA, MedMCQA, PubMedQA, and MMLU, were reviewed for their rigor,\ntransparency, and relevance to clinical scenarios. Alternatives, such as\nchallenge questions in medical journals, were also analyzed to identify their\npotential as unbiased evaluation tools. Results: Most existing datasets lack\nclinical realism, transparency, and robust validation processes. Publicly\navailable challenge questions offer some benefits but are limited by their\nsmall size, narrow scope, and exposure to LLM training. These gaps highlight\nthe need for secure, comprehensive, and representative datasets. Conclusion: A\nstandardized framework is critical for evaluating LLMs in medicine.\nCollaborative efforts among institutions and policymakers are needed to ensure\ndatasets and methodologies are rigorous, unbiased, and reflective of clinical\ncomplexities."}
{"id": "2507.09262", "pdf": "https://arxiv.org/pdf/2507.09262.pdf", "abs": "https://arxiv.org/abs/2507.09262", "title": "Discrepancies in Mental Workload Estimation: Self-Reported versus EEG-Based Measures in Data Visualization Evaluation", "authors": ["Soobin Yim", "Sangbong Yoo", "Chanyoung Yoon", "Chanyoung Jung", "Chansoo Kim", "Yun Jang", "Ghulam Jilani Quadri"], "categories": ["cs.HC"], "comment": null, "summary": "Accurate assessment of mental workload (MW) is crucial for understanding\ncognitive processes during visualization tasks. While EEG-based measures are\nemerging as promising alternatives to conventional assessment techniques, such\nas selfreport measures, studies examining consistency across these different\nmethodologies are limited. In a preliminary study, we observed indications of\npotential discrepancies between EEGbased and self-reported MW measures.\nMotivated by these preliminary observations, our study further explores the\ndiscrepancies between EEG-based and self-reported MW assessment methods through\nan experiment involving visualization tasks. In the experiment, we employ two\nbenchmark tasks: the Visualization Literacy Assessment Test (VLAT) and a\nSpatial Visualization (SV) task. EEG signals are recorded from participants\nusing a 32-channel system at a sampling rate of 128 Hz during the visualization\ntasks. For each participant, MW is estimated using an EEG-based model built on\na Graph Attention Network (GAT) architecture, and these estimates are compared\nwith conventional MW measures to examine potential discrepancies. Our findings\nreveal notable discrepancies between task difficulty and EEG-based MW\nestimates, as well as between EEG-based and self-reported MW measures across\nvarying task difficulty levels. Additionally, the observed patterns suggest the\npresence of unconscious cognitive effort that may not be captured by selfreport\nalone."}
{"id": "2507.08924", "pdf": "https://arxiv.org/pdf/2507.08924.pdf", "abs": "https://arxiv.org/abs/2507.08924", "title": "From KMMLU-Redux to KMMLU-Pro: A Professional Korean Benchmark Suite for LLM Evaluation", "authors": ["Seokhee Hong", "Sunkyoung Kim", "Guijin Son", "Soyeon Kim", "Yeonjung Hong", "Jinsik Lee"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The development of Large Language Models (LLMs) requires robust benchmarks\nthat encompass not only academic domains but also industrial fields to\neffectively evaluate their applicability in real-world scenarios. In this\npaper, we introduce two Korean expert-level benchmarks. KMMLU-Redux,\nreconstructed from the existing KMMLU, consists of questions from the Korean\nNational Technical Qualification exams, with critical errors removed to enhance\nreliability. KMMLU-Pro is based on Korean National Professional Licensure exams\nto reflect professional knowledge in Korea. Our experiments demonstrate that\nthese benchmarks comprehensively represent industrial knowledge in Korea. We\nrelease our dataset publicly available."}
{"id": "2507.09489", "pdf": "https://arxiv.org/pdf/2507.09489.pdf", "abs": "https://arxiv.org/abs/2507.09489", "title": "TraSculptor: Visual Analytics for Enhanced Decision-Making in Road Traffic Planning", "authors": ["Zikun Deng", "Yuanbang Liu", "Mingrui Zhu", "Da Xiang", "Haiyue Yu", "Zicheng Su", "Qinglong Lu", "Tobias Schreck", "Yi Cai"], "categories": ["cs.HC"], "comment": "IEEE Transactions on Visualization and Computer Graphics", "summary": "The design of urban road networks significantly influences traffic\nconditions, underscoring the importance of informed traffic planning. Traffic\nplanning experts rely on specialized platforms to simulate traffic systems,\nassessing the efficacy of the road network across various states of\nmodifications. Nevertheless, a prevailing issue persists: many existing traffic\nplanning platforms exhibit inefficiencies in flexibly interacting with the road\nnetwork's structure and attributes and intuitively comparing multiple states\nduring the iterative planning process. This paper introduces TraSculptor, an\ninteractive planning decision-making system. To develop TraSculptor, we\nidentify and address two challenges: interactive modification of road networks\nand intuitive comparison of multiple network states. For the first challenge,\nwe establish flexible interactions to enable experts to easily and directly\nmodify the road network on the map. For the second challenge, we design a\ncomparison view with a history tree of multiple states and a road-state matrix\nto facilitate intuitive comparison of road network states. To evaluate\nTraSculptor, we provided a usage scenario where the Braess's paradox was\nshowcased, invited experts to perform a case study on the Sioux Falls network,\nand collected expert feedback through interviews."}
{"id": "2507.08967", "pdf": "https://arxiv.org/pdf/2507.08967.pdf", "abs": "https://arxiv.org/abs/2507.08967", "title": "Self-Improving Model Steering", "authors": ["Rongyi Zhu", "Yuhui Wang", "Tanqiu Jiang", "Jiacheng Liang", "Ting Wang"], "categories": ["cs.CL"], "comment": "16 pages, 9 figures", "summary": "Model steering represents a powerful technique that dynamically aligns large\nlanguage models (LLMs) with human preferences during inference. However,\nconventional model-steering methods rely heavily on externally annotated data,\nnot only limiting their adaptability to varying contexts but also tethering\ntheir effectiveness to annotation quality. In this paper, we present SIMS, the\nfirst self-improving model-steering framework that operates without relying on\nexternal supervision. At its core, SIMS autonomously generates and refines\ncontrastive samples through iterative self-improvement cycles, enabling\nadaptive, context-specific steering. Additionally, SIMS employs novel\nstrategies, including prompt ranking and contrast sampling, to further enhance\nsteering efficacy. Extensive evaluation across diverse LLMs and benchmarks\ndemonstrates that SIMS substantially outperforms existing methods in steering\neffectiveness and adaptability, highlighting self-improving model steering as a\npromising direction for future research on inference-time LLM alignment."}
{"id": "2507.09549", "pdf": "https://arxiv.org/pdf/2507.09549.pdf", "abs": "https://arxiv.org/abs/2507.09549", "title": "The Spectacle of Fidelity: Blind Resistance and the Wizardry of Prototyping", "authors": ["Hrittika Bhowmick", "Shilpaa Anand"], "categories": ["cs.HC", "H.5.2; K.4.2; D.2.2"], "comment": "3 pages. Submitted for Access InContext Workshop at CHI'25, April 26,\n  2025, Yokohama, Japan", "summary": "Prototyping is widely regarded in Human-Computer Interaction as an iterative\nprocess through which ideas are tested and refined, often via visual mockups,\nscreen flows, and coded simulations. This position paper critiques the\nvisual-centric norms embedded in prototyping culture by drawing from the lived\nexperiences of blind scholars and insights from cultural disability studies. It\ndiscusses how dominant methods of prototyping rely on an unexamined fidelity to\nsight, privileging what can be rendered visibly coherent while marginalizing\nother modes of knowing and making. By repositioning prototyping as a situated,\nembodied, and relational practice, this paper challenges HCI to rethink what\nkinds of design participation are legitimized and which are excluded when\nprototyping is reduced to screen-based simulations."}
{"id": "2507.08969", "pdf": "https://arxiv.org/pdf/2507.08969.pdf", "abs": "https://arxiv.org/abs/2507.08969", "title": "Application of CARE-SD text classifier tools to assess distribution of stigmatizing and doubt-marking language features in EHR", "authors": ["Drew Walker", "Jennifer Love", "Swati Rajwal", "Isabel C Walker", "Hannah LF Cooper", "Abeed Sarker", "Melvin Livingston III"], "categories": ["cs.CL"], "comment": "3 Tables", "summary": "Introduction: Electronic health records (EHR) are a critical medium through\nwhich patient stigmatization is perpetuated among healthcare teams. Methods: We\nidentified linguistic features of doubt markers and stigmatizing labels in\nMIMIC-III EHR via expanded lexicon matching and supervised learning\nclassifiers. Predictors of rates of linguistic features were assessed using\nPoisson regression models. Results: We found higher rates of stigmatizing\nlabels per chart among patients who were Black or African American (RR: 1.16),\npatients with Medicare/Medicaid or government-run insurance (RR: 2.46),\nself-pay (RR: 2.12), and patients with a variety of stigmatizing disease and\nmental health conditions. Patterns among doubt markers were similar, though\nmale patients had higher rates of doubt markers (RR: 1.25). We found increased\nstigmatizing labels used by nurses (RR: 1.40), and social workers (RR: 2.25),\nwith similar patterns of doubt markers. Discussion: Stigmatizing language\noccurred at higher rates among historically stigmatized patients, perpetuated\nby multiple provider types."}
{"id": "2507.09664", "pdf": "https://arxiv.org/pdf/2507.09664.pdf", "abs": "https://arxiv.org/abs/2507.09664", "title": "SimStep: Chain-of-Abstractions for Incremental Specification and Debugging of AI-Generated Interactive Simulations", "authors": ["Zoe Kaputa", "Anika Rajaram", "Vryan Almanon Feliciano", "Zhuoyue Lyu", "Maneesh Agrawala", "Hari Subramonyam"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Programming-by-prompting with generative AI offers a new paradigm for\nend-user programming, shifting the focus from syntactic fluency to semantic\nintent. This shift holds particular promise for non-programmers such as\neducators, who can describe instructional goals in natural language to generate\ninteractive learning content. Yet in bypassing direct code authoring, many of\nprogramming's core affordances - such as traceability, stepwise refinement, and\nbehavioral testing - are lost. We propose the Chain-of-Abstractions (CoA)\nframework as a way to recover these affordances while preserving the expressive\nflexibility of natural language. CoA decomposes the synthesis process into a\nsequence of cognitively meaningful, task-aligned representations that function\nas checkpoints for specification, inspection, and refinement. We instantiate\nthis approach in SimStep, an authoring environment for teachers that scaffolds\nsimulation creation through four intermediate abstractions: Concept Graph,\nScenario Graph, Learning Goal Graph, and UI Interaction Graph. To address\nambiguities and misalignments, SimStep includes an inverse correction process\nthat surfaces in-filled model assumptions and enables targeted revision without\nrequiring users to manipulate code. Evaluations with educators show that CoA\nenables greater authoring control and interpretability in\nprogramming-by-prompting workflows."}
{"id": "2507.09011", "pdf": "https://arxiv.org/pdf/2507.09011.pdf", "abs": "https://arxiv.org/abs/2507.09011", "title": "Beyond vividness: Content analysis of induced hallucinations reveals the hidden structure of individual differences in visual imagery", "authors": ["Ana Chkhaidze", "Reshanne R. Reeder", "Connor Gag", "Anastasia Kiyonaga", "Seana Coulson"], "categories": ["cs.CL", "q-bio.NC", "q-bio.QM"], "comment": null, "summary": "A rapidly alternating red and black display known as Ganzflicker induces\nvisual hallucinations that reflect the generative capacity of the visual\nsystem. Recent proposals regarding the imagery spectrum, that is, differences\nin the visual system of individuals with absent imagery, typical imagery, and\nvivid imagery, suggest these differences should impact the complexity of other\ninternally generated visual experiences. Here, we used tools from natural\nlanguage processing to analyze free-text descriptions of hallucinations from\nover 4,000 participants, asking whether people with different imagery\nphenotypes see different things in their mind's eye during Ganzflicker-induced\nhallucinations. Strong imagers described complex, naturalistic content, while\nweak imagers reported simple geometric patterns. Embeddings from vision\nlanguage models better captured these differences than text-only language\nmodels, and participants with stronger imagery used language with richer\nsensorimotor associations. These findings may reflect individual variation in\ncoordination between early visual areas and higher-order regions relevant for\nthe imagery spectrum."}
{"id": "2507.09917", "pdf": "https://arxiv.org/pdf/2507.09917.pdf", "abs": "https://arxiv.org/abs/2507.09917", "title": "Volume-Based Space-Time Cube for Large-Scale Continuous Spatial Time Series", "authors": ["Zikun Deng", "Jiabao Huang", "Chenxi Ruan", "Jialing Li", "Shaowu Gao", "Yi Cai"], "categories": ["cs.HC"], "comment": null, "summary": "Spatial time series visualization offers scientific research pathways and\nanalytical decision-making tools across various spatiotemporal domains. Despite\nmany advanced methodologies, the seamless integration of temporal and spatial\ninformation remains a challenge. The space-time cube (STC) stands out as a\npromising approach for the synergistic presentation of spatial and temporal\ninformation, with successful applications across various spatiotemporal\ndatasets. However, the STC is plagued by well-known issues such as visual\nocclusion and depth ambiguity, which are further exacerbated when dealing with\nlarge-scale spatial time series data. In this study, we introduce a novel\ntechnical framework termed VolumeSTCube, designed for continuous spatiotemporal\nphenomena. It first leverages the concept of the STC to transform discretely\ndistributed spatial time series data into continuously volumetric data.\nSubsequently, volume rendering and surface rendering techniques are employed to\nvisualize the transformed volumetric data. Volume rendering is utilized to\nmitigate visual occlusion, while surface rendering provides pattern details by\nenhanced lighting information. Lastly, we design interactions to facilitate the\nexploration and analysis from temporal, spatial, and spatiotemporal\nperspectives. VolumeSTCube is evaluated through a computational experiment, a\nreal-world case study with one expert, and a controlled user study with twelve\nnon-experts, compared against a baseline from prior work, showing its\nsuperiority and effectiveness in largescale spatial time series analysis."}
{"id": "2507.09025", "pdf": "https://arxiv.org/pdf/2507.09025.pdf", "abs": "https://arxiv.org/abs/2507.09025", "title": "Lizard: An Efficient Linearization Framework for Large Language Models", "authors": ["Chien Van Nguyen", "Ruiyi Zhang", "Hanieh Deilamsalehy", "Puneet Mathur", "Viet Dac Lai", "Haoliang Wang", "Jayakumar Subramanian", "Ryan A. Rossi", "Trung Bui", "Nikos Vlassis", "Franck Dernoncourt", "Thien Huu Nguyen"], "categories": ["cs.CL", "cs.LG"], "comment": "15 pages", "summary": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into flexible, subquadratic\narchitectures for infinite-context generation. Transformer-based LLMs face\nsignificant memory and computational bottlenecks as context lengths increase,\ndue to the quadratic complexity of softmax attention and the growing key-value\n(KV) cache. Lizard addresses these limitations by introducing a subquadratic\nattention mechanism that closely approximates softmax attention while\npreserving the output quality. Unlike previous linearization methods, which are\noften limited by fixed model structures and therefore exclude gating\nmechanisms, Lizard incorporates a gating module inspired by recent\nstate-of-the-art linear models. This enables adaptive memory control, supports\nconstant-memory inference, offers strong length generalization, and allows more\nflexible model design. Lizard combines gated linear attention for global\ncontext compression with sliding window attention enhanced by meta memory,\nforming a hybrid mechanism that captures both long-range dependencies and\nfine-grained local interactions. Moreover, we introduce a hardware-aware\nalgorithm that accelerates the training speed of our models. Extensive\nexperiments show that Lizard achieves near-lossless recovery of the teacher\nmodel's performance across standard language modeling tasks, while\nsignificantly outperforming previous linearization methods. On the 5-shot MMLU\nbenchmark, Lizard improves over prior models by 18 points and shows significant\nimprovements on associative recall tasks."}
{"id": "2507.09959", "pdf": "https://arxiv.org/pdf/2507.09959.pdf", "abs": "https://arxiv.org/abs/2507.09959", "title": "Branch Explorer: Leveraging Branching Narratives to Support Interactive 360Â° Video Viewing for Blind and Low Vision Users", "authors": ["Shuchang Xu", "Xiaofu Jin", "Wenshuo Zhang", "Huamin Qu", "Yukang Yan"], "categories": ["cs.HC"], "comment": null, "summary": "360{\\deg} videos enable users to freely choose their viewing paths, but blind\nand low vision (BLV) users are often excluded from this interactive experience.\nTo bridge this gap, we present Branch Explorer, a system that transforms\n360{\\deg} videos into branching narratives -- stories that dynamically unfold\nbased on viewer choices -- to support interactive viewing for BLV audiences.\nOur formative study identified three key considerations for accessible\nbranching narratives: providing diverse branch options, ensuring coherent story\nprogression, and enabling immersive navigation among branches. To address these\nneeds, Branch Explorer employs a multi-modal machine learning pipeline to\ngenerate diverse narrative paths, allowing users to flexibly make choices at\ndetected branching points and seamlessly engage with each storyline through\nimmersive audio guidance. Evaluation with 12 BLV viewers showed that Branch\nExplorer significantly enhanced user agency and engagement in 360{\\deg} video\nviewing. Users also developed personalized strategies for exploring 360{\\deg}\ncontent. We further highlight implications for supporting accessible\nexploration of videos and virtual environments."}
{"id": "2507.09037", "pdf": "https://arxiv.org/pdf/2507.09037.pdf", "abs": "https://arxiv.org/abs/2507.09037", "title": "ALIGN: Prompt-based Attribute Alignment for Reliable, Responsible, and Personalized LLM-based Decision-Making", "authors": ["Bharadwaj Ravichandran", "David Joy", "Paul Elliott", "Brian Hu", "Jadie Adams", "Christopher Funk", "Emily Veenhuis", "Anthony Hoogs", "Arslan Basharat"], "categories": ["cs.CL", "cs.AI"], "comment": "10 pages total (including appendix), ICML 2025 Workshop on Reliable\n  and Responsible Foundation Models", "summary": "Large language models (LLMs) are increasingly being used as decision aids.\nHowever, users have diverse values and preferences that can affect their\ndecision-making, which requires novel methods for LLM alignment and\npersonalization. Existing LLM comparison tools largely focus on benchmarking\ntasks, such as knowledge-based question answering. In contrast, our proposed\nALIGN system focuses on dynamic personalization of LLM-based decision-makers\nthrough prompt-based alignment to a set of fine-grained attributes. Key\nfeatures of our system include robust configuration management, structured\noutput generation with reasoning, and several algorithm implementations with\nswappable LLM backbones, enabling different types of analyses. Our user\ninterface enables a qualitative, side-by-side comparison of LLMs and their\nalignment to various attributes, with a modular backend for easy algorithm\nintegration. Additionally, we perform a quantitative analysis comparing\nalignment approaches in two different domains: demographic alignment for public\nopinion surveys and value alignment for medical triage decision-making. The\nentire ALIGN framework is open source and will enable new research on reliable,\nresponsible, and personalized LLM-based decision-makers."}
{"id": "2507.10024", "pdf": "https://arxiv.org/pdf/2507.10024.pdf", "abs": "https://arxiv.org/abs/2507.10024", "title": "Qualitative Study for LLM-assisted Design Study Process: Strategies, Challenges, and Roles", "authors": ["Shaolun Ruan", "Rui Sheng", "Xiaolin Wen", "Jiachen Wang", "Tianyi Zhang", "Yong Wang", "Tim Dwyer", "Jiannan Li"], "categories": ["cs.HC"], "comment": null, "summary": "Design studies aim to create visualization solutions for real-world problems\nof different application domains. Recently, the emergence of large language\nmodels (LLMs) has introduced new opportunities to enhance the design study\nprocess, providing capabilities such as creative problem-solving, data\nhandling, and insightful analysis. However, despite their growing popularity,\nthere remains a lack of systematic understanding of how LLMs can effectively\nassist researchers in visualization-specific design studies. In this paper, we\nconducted a multi-stage qualitative study to fill this gap, involving 30 design\nstudy researchers from diverse backgrounds and expertise levels. Through\nin-depth interviews and carefully-designed questionnaires, we investigated\nstrategies for utilizing LLMs, the challenges encountered, and the practices\nused to overcome them. We further compiled and summarized the roles that LLMs\ncan play across different stages of the design study process. Our findings\nhighlight practical implications to inform visualization practitioners, and\nprovide a framework for leveraging LLMs to enhance the design study process in\nvisualization research."}
{"id": "2507.09075", "pdf": "https://arxiv.org/pdf/2507.09075.pdf", "abs": "https://arxiv.org/abs/2507.09075", "title": "OpenCodeReasoning-II: A Simple Test Time Scaling Approach via Self-Critique", "authors": ["Wasi Uddin Ahmad", "Somshubra Majumdar", "Aleksander Ficek", "Sean Narenthiran", "Mehrzad Samadi", "Jocelyn Huang", "Siddhartha Jain", "Vahid Noroozi", "Boris Ginsburg"], "categories": ["cs.CL"], "comment": "work in progress", "summary": "Recent advancements in reasoning-based Large Language Models (LLMs),\nparticularly their potential through test-time scaling, have created\nsignificant opportunities for distillation in code generation and critique.\nHowever, progress in both areas fundamentally depends on large-scale,\nhigh-quality datasets. In this work, we introduce OpenCodeReasoning-II, a\ndataset consists of 2.5M question-solution-critique triples (approx. 35K unique\nprogramming questions), making it nearly twice the size of the previous largest\npublicly available code reasoning dataset. In this work, we employ a two-stage\nsupervised fine-tuning strategy. The first stage focuses on fine-tuning for\ncode generation, while the second stage involves the joint training of models\nfor both code generation and critique. Our resulting finetuned Qwen2.5-Instruct\nmodels achieve performance in code generation that either exceeds or equals the\nbest prior open-weight distilled models. Notably, the integration of our code\ngeneration and critique models leads to significant improvements in competitive\ncoding performance. Furthermore, we present an extension of the LiveCodeBench\nbenchmark to specifically support the C++ programming language, thereby\nfacilitating more comprehensive LLM evaluation using this benchmark."}
{"id": "2507.10043", "pdf": "https://arxiv.org/pdf/2507.10043.pdf", "abs": "https://arxiv.org/abs/2507.10043", "title": "XROps: A Visual Workflow Management System for Dynamic Immersive Analytics", "authors": ["Suemin Jeon", "JunYoung Choi", "Haejin Jeong", "Won-Ki Jeong"], "categories": ["cs.HC"], "comment": null, "summary": "Immersive analytics is gaining attention across multiple domains due to its\ncapability to facilitate intuitive data analysis in expansive environments\nthrough user interaction with data. However, creating immersive analytics\nsystems for specific tasks is challenging due to the need for programming\nexpertise and significant development effort. Despite the introduction of\nvarious immersive visualization authoring toolkits, domain experts still face\nhurdles in adopting immersive analytics into their workflow, particularly when\nfaced with dynamically changing tasks and data in real time. To lower such\ntechnical barriers, we introduce XROps, a web-based authoring system that\nallows users to create immersive analytics applications through interactive\nvisual programming, without the need for low-level scripting or coding. XROps\nenables dynamic immersive analytics authoring by allowing users to modify each\nstep of the data visualization process with immediate feedback, enabling them\nto build visualizations on-the-fly and adapt to changing environments. It also\nsupports the integration and visualization of real-time sensor data from XR\ndevices, a key feature of immersive analytics, facilitating the creation of\nvarious analysis scenarios. We evaluated the usability of XROps through a user\nstudy and demonstrate its efficacy and usefulness in several example scenarios.\nWe have released a web platform (https://vience.io/xrops) to demonstrate\nvarious examples to supplement our findings."}
{"id": "2507.09076", "pdf": "https://arxiv.org/pdf/2507.09076.pdf", "abs": "https://arxiv.org/abs/2507.09076", "title": "Dynamic Parameter Memory: Temporary LoRA-Enhanced LLM for Long-Sequence Emotion Recognition in Conversation", "authors": ["Jialong Mai", "Xiaofen Xing", "Yawei Li", "Zhipeng Li", "Jingyuan Xing", "Xiangmin Xu"], "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7; H.5.2"], "comment": "submitted to EMNLP 2025", "summary": "Recent research has focused on applying speech large language model (SLLM) to\nimprove speech emotion recognition (SER). However, the inherently high frame\nrate in speech modality severely limits the signal processing and understanding\ncapabilities of SLLM. For example, a SLLM with a 4K context window can only\nprocess 80 seconds of audio at 50Hz feature sampling rate before reaching its\ncapacity limit. Input token compression methods used in SLLM overlook the\ncontinuity and inertia of emotions across multiple conversation turns. This\npaper proposes a Dynamic Parameter Memory (DPM) mechanism with contextual\nsemantics and sentence-level emotion encoding, enabling processing of\nunlimited-length audio with limited context windows in SLLM. Specifically, DPM\nprogressively encodes sentence-level information and emotions into a temporary\nLoRA module during inference to effectively \"memorize\" the contextual\ninformation. We trained an emotion SLLM as a backbone and incorporated our DPM\ninto inference for emotion recognition in conversation (ERC). Experimental\nresults on the IEMOCAP dataset show that DPM significantly improves the emotion\nrecognition capabilities of SLLM when processing long audio sequences,\nachieving state-of-the-art performance."}
{"id": "2507.10044", "pdf": "https://arxiv.org/pdf/2507.10044.pdf", "abs": "https://arxiv.org/abs/2507.10044", "title": "MEDebiaser: A Human-AI Feedback System for Mitigating Bias in Multi-label Medical Image Classification", "authors": ["Shaohan Shi", "Yuheng Shao", "Haoran Jiang", "Yunjie Yao", "Zhijun Zhang", "Xu Ding", "Quan Li"], "categories": ["cs.HC"], "comment": null, "summary": "Medical images often contain multiple labels with imbalanced distributions\nand co-occurrence, leading to bias in multi-label medical image classification.\nClose collaboration between medical professionals and machine learning\npractitioners has significantly advanced medical image analysis. However,\ntraditional collaboration modes struggle to facilitate effective feedback\nbetween physicians and AI models, as integrating medical expertise into the\ntraining process via engineers can be time-consuming and labor-intensive. To\nbridge this gap, we introduce MEDebiaser, an interactive system enabling\nphysicians to directly refine AI models using local explanations. By combining\nprediction with attention loss functions and employing a customized ranking\nstrategy to alleviate scalability, MEDebiaser allows physicians to mitigate\nbiases without technical expertise, reducing reliance on engineers, and thus\nenhancing more direct human-AI feedback. Our mechanism and user studies\ndemonstrate that it effectively reduces biases, improves usability, and\nenhances collaboration efficiency, providing a practical solution for\nintegrating medical expertise into AI-driven healthcare."}
{"id": "2507.09104", "pdf": "https://arxiv.org/pdf/2507.09104.pdf", "abs": "https://arxiv.org/abs/2507.09104", "title": "CompassJudger-2: Towards Generalist Judge Model via Verifiable Rewards", "authors": ["Taolin Zhang", "Maosong Cao", "Alexander Lam", "Songyang Zhang", "Kai Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recently, the role of LLM-as-judge in evaluating large language models has\ngained prominence. However, current judge models suffer from narrow\nspecialization and limited robustness, undermining their capacity for\ncomprehensive evaluations. In this work, we present CompassJudger-2, a novel\ngeneralist judge model that overcomes these limitations via a task-driven,\nmulti-domain data curation strategy. Central to our approach is supervising\njudgment tasks with verifiable rewards, guiding intrinsic critical reasoning\nthrough rejection sampling to foster robust, generalizable judgment\ncapabilities. We introduce a refined learning objective with margin policy\ngradient loss to enhance performance. Empirically, CompassJudger-2 achieves\nsuperior results across multiple judge and reward benchmarks, and our 7B model\ndemonstrates competitive judgment accuracy with significantly larger models\nlike DeepSeek-V3 and Qwen3-235B-A22B. Additionally, we propose JudgerBenchV2, a\ncomprehensive benchmark evaluating cross-domain judgment accuracy and rank\nconsistency to standardize judge model evaluation. These contributions advance\nrobust, scalable LLM judgment and establish new performance and evaluation\nstandards."}
{"id": "2507.10099", "pdf": "https://arxiv.org/pdf/2507.10099.pdf", "abs": "https://arxiv.org/abs/2507.10099", "title": "ReDemon UI: Reactive Synthesis by Demonstration for Web UI", "authors": ["Jay Lee", "Gyuhyeok Oh", "Joongwon Ahn", "Xiaokang Qiu"], "categories": ["cs.HC", "cs.PL"], "comment": "Submitted to UIST 2025 Posters", "summary": "ReDemon UI synthesizes React applications from user demonstrations, enabling\ndesigners and non-expert programmers to create UIs that integrate with standard\nUI prototyping workflows. Users provide a static mockup sketch with event\nhandler holes and demonstrate desired runtime behaviors by interacting with the\nrendered mockup and editing the sketch. ReDemon UI identifies reactive data and\nsynthesizes a React program with correct state update logic. We utilize\nenumerative synthesis for simple UIs and LLMs for more complex UIs."}
{"id": "2507.09155", "pdf": "https://arxiv.org/pdf/2507.09155.pdf", "abs": "https://arxiv.org/abs/2507.09155", "title": "OPENXRD: A Comprehensive Benchmark and Enhancement Framework for LLM/MLLM XRD Question Answering", "authors": ["Ali Vosoughi", "Ayoub Shahnazari", "Yufeng Xi", "Zeliang Zhang", "Griffin Hess", "Chenliang Xu", "Niaz Abdolrahim"], "categories": ["cs.CL", "cs.AI", "68T50, 68T07"], "comment": "10 pages, 6 figures, 5 tables. Code and dataset available at\n  https://github.com/niaz60/OpenXRD. Project webpage:\n  https://niaz60.github.io/OpenXRD/", "summary": "This work presents OPENXRD, an open-book pipeline designed for\ncrystallography question answering, which integrates textual prompts with\nconcise supporting content generated by GPT-4.5. Instead of using scanned\ntextbooks, which may lead to copyright issues, OPENXRD generates compact,\ndomain-specific references that help smaller models understand key concepts in\nX-ray diffraction (XRD). We evaluate OPENXRD on a well-defined set of 217\nexpert-level XRD questions by comparing different vision-language models,\nincluding GPT-4 and LLaVA-based frameworks such as Mistral, LLaMA, and QWEN,\nunder both closed-book (without supporting material) and open-book (with\nsupporting material) conditions. Our experimental results show significant\naccuracy improvements in models that use the GPT-4.5-generated summaries,\nparticularly those with limited prior training in crystallography. OPENXRD uses\nknowledge from larger models to fill knowledge gaps in crystallography and\nshows that AI-generated texts can help smaller models reason more effectively\nin scientific tasks. While the current version of OPENXRD focuses on text-based\ninputs, we also explore future extensions such as adding real crystal diagrams\nor diffraction patterns to improve interpretation in specialized materials\nscience contexts. Overall, OPENXRD shows that specialized open-book systems can\nbe useful in materials science and provides a foundation for broader natural\nlanguage processing (NLP) tools in critical scientific fields."}
{"id": "2507.10102", "pdf": "https://arxiv.org/pdf/2507.10102.pdf", "abs": "https://arxiv.org/abs/2507.10102", "title": "When Familiarity Remains: Procedural Memory, Symbolic Anchors, and Digital Engagement in Dementia Care", "authors": ["Jeongone Seo", "Kyung-zoon Hong", "Sol Baik"], "categories": ["cs.HC"], "comment": "23 page, 2 tables, 1 figure", "summary": "INTRODUCTION: Older adults with early-stage dementia often retain procedural\nmemory, enabling continued use of familiar technologies. Additionally, symbolic\nanchors such as photos or personalized content may serve as memory cues to\nreinforce digital engagement. This study explores how these mechanisms support\ntechnology use in dementia care within the South Korean context.\n  METHODS: We conducted in-depth interviews with 11 professional caregivers of\ncommunity-dwelling older adults with cognitive decline. Grounded theory methods\nguided the analysis, using iterative coding and constant comparison to identify\nemergent themes.\n  RESULTS: Caregivers reported that familiar digital routines (e.g., taking\nphotos) persisted through procedural memory. Symbolic anchors such as family\nphotos or recognizable icons enhanced interaction and emotional engagement.\nHowever, unfamiliar or anthropomorphic technologies often triggered fear or\nsymbolic resistance.\n  DISCUSSION: Findings highlight the dual role of procedural memory and\nsymbolic anchors in sustaining digital engagement. Designing culturally\nresponsive and cognitively accessible technologies may enhance autonomy and\nwell-being in dementia care.\n  Keywords: procedural memory, symbolic anchors, dementia care, digital\nengagement, older adults, cultural adaptation, caregiving technologies"}
{"id": "2507.09157", "pdf": "https://arxiv.org/pdf/2507.09157.pdf", "abs": "https://arxiv.org/abs/2507.09157", "title": "PU-Lie: Lightweight Deception Detection in Imbalanced Diplomatic Dialogues via Positive-Unlabeled Learning", "authors": ["Bhavinkumar Vinodbhai Kuwar", "Bikrant Bikram Pratap Maurya", "Priyanshu Gupta", "Nitin Choudhury"], "categories": ["cs.CL"], "comment": null, "summary": "Detecting deception in strategic dialogues is a complex and high-stakes task\ndue to the subtlety of language and extreme class imbalance between deceptive\nand truthful communications. In this work, we revisit deception detection in\nthe Diplomacy dataset, where less than 5% of messages are labeled deceptive. We\nintroduce a lightweight yet effective model combining frozen BERT embeddings,\ninterpretable linguistic and game-specific features, and a Positive-Unlabeled\n(PU) learning objective. Unlike traditional binary classifiers, PU-Lie is\ntailored for situations where only a small portion of deceptive messages are\nlabeled, and the majority are unlabeled. Our model achieves a new best macro F1\nof 0.60 while reducing trainable parameters by over 650x. Through comprehensive\nevaluations and ablation studies across seven models, we demonstrate the value\nof PU learning, linguistic interpretability, and speaker-aware representations.\nNotably, we emphasize that in this problem setting, accurately detecting\ndeception is more critical than identifying truthful messages. This priority\nguides our choice of PU learning, which explicitly models the rare but vital\ndeceptive class."}
{"id": "2507.10240", "pdf": "https://arxiv.org/pdf/2507.10240.pdf", "abs": "https://arxiv.org/abs/2507.10240", "title": "Visual Analytics for Explainable and Trustworthy Artificial Intelligence", "authors": ["Angelos Chatzimparmpas"], "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": null, "summary": "Our society increasingly depends on intelligent systems to solve complex\nproblems, ranging from recommender systems suggesting the next movie to watch\nto AI models assisting in medical diagnoses for hospitalized patients. With the\niterative improvement of diagnostic accuracy and efficiency, AI holds\nsignificant potential to mitigate medical misdiagnoses by preventing numerous\ndeaths and reducing an economic burden of approximately 450 EUR billion\nannually. However, a key obstacle to AI adoption lies in the lack of\ntransparency: many automated systems function as \"black boxes,\" providing\npredictions without revealing the underlying processes. This opacity can hinder\nexperts' ability to trust and rely on AI systems. Visual analytics (VA)\nprovides a compelling solution by combining AI models with interactive\nvisualizations. These specialized charts and graphs empower users to\nincorporate their domain expertise to refine and improve the models, bridging\nthe gap between AI and human understanding. In this work, we define,\ncategorize, and explore how VA solutions can foster trust across the stages of\na typical AI pipeline. We propose a design space for innovative visualizations\nand present an overview of our previously developed VA dashboards, which\nsupport critical tasks within the various pipeline stages, including data\nprocessing, feature engineering, hyperparameter tuning, understanding,\ndebugging, refining, and comparing models."}
{"id": "2507.09174", "pdf": "https://arxiv.org/pdf/2507.09174.pdf", "abs": "https://arxiv.org/abs/2507.09174", "title": "RAMA: Retrieval-Augmented Multi-Agent Framework for Misinformation Detection in Multimodal Fact-Checking", "authors": ["Shuo Yang", "Zijian Yu", "Zhenzhe Ying", "Yuqin Dai", "Guoqing Wang", "Jun Lan", "Jinfeng Xu", "Jinze Li", "Edith C. H. Ngai"], "categories": ["cs.CL"], "comment": null, "summary": "The rapid proliferation of multimodal misinformation presents significant\nchallenges for automated fact-checking systems, especially when claims are\nambiguous or lack sufficient context. We introduce RAMA, a novel\nretrieval-augmented multi-agent framework designed for verifying multimedia\nmisinformation. RAMA incorporates three core innovations: (1) strategic query\nformulation that transforms multimodal claims into precise web search queries;\n(2) cross-verification evidence aggregation from diverse, authoritative\nsources; and (3) a multi-agent ensemble architecture that leverages the\ncomplementary strengths of multiple multimodal large language models and prompt\nvariants. Extensive experiments demonstrate that RAMA achieves superior\nperformance on benchmark datasets, particularly excelling in resolving\nambiguous or improbable claims by grounding verification in retrieved factual\nevidence. Our findings underscore the necessity of integrating web-based\nevidence and multi-agent reasoning for trustworthy multimedia verification,\npaving the way for more reliable and scalable fact-checking solutions. RAMA\nwill be publicly available at https://github.com/kalendsyang/RAMA.git."}
{"id": "2507.10427", "pdf": "https://arxiv.org/pdf/2507.10427.pdf", "abs": "https://arxiv.org/abs/2507.10427", "title": "Towards Emotion Co-regulation with LLM-powered Socially Assistive Robots: Integrating LLM Prompts and Robotic Behaviors to Support Parent-Neurodivergent Child Dyads", "authors": ["Jing Li", "Felix Schijve", "Sheng Li", "Yuye Yang", "Jun Hu", "Emilia Barakova"], "categories": ["cs.HC", "cs.RO"], "comment": "Submission for the IROS 2025 conference", "summary": "Socially Assistive Robotics (SAR) has shown promise in supporting emotion\nregulation for neurodivergent children. Recently, there has been increasing\ninterest in leveraging advanced technologies to assist parents in co-regulating\nemotions with their children. However, limited research has explored the\nintegration of large language models (LLMs) with SAR to facilitate emotion\nco-regulation between parents and children with neurodevelopmental disorders.\nTo address this gap, we developed an LLM-powered social robot by deploying a\nspeech communication module on the MiRo-E robotic platform. This supervised\nautonomous system integrates LLM prompts and robotic behaviors to deliver\ntailored interventions for both parents and neurodivergent children. Pilot\ntests were conducted with two parent-child dyads, followed by a qualitative\nanalysis. The findings reveal MiRo-E's positive impacts on interaction dynamics\nand its potential to facilitate emotion regulation, along with identified\ndesign and technical challenges. Based on these insights, we provide design\nimplications to advance the future development of LLM-powered SAR for mental\nhealth applications."}
{"id": "2507.09185", "pdf": "https://arxiv.org/pdf/2507.09185.pdf", "abs": "https://arxiv.org/abs/2507.09185", "title": "Detecting and Pruning Prominent but Detrimental Neurons in Large Language Models", "authors": ["Ameen Ali", "Shahar Katz", "Lior Wolf", "Ivan Titov"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) often develop learned mechanisms specialized to\nspecific datasets, such as reliance on domain-specific correlations, which\nyield high-confidence predictions without generalizable reasoning. While\nbeneficial in one setting, these dataset-specific mechanisms typically degrade\nperformance when models encounter novel tasks or distributions. In this work,\nwe introduce a fine-tuning approach designed to enhance generalization by\nidentifying and pruning neurons associated with dataset-specific mechanisms in\ntransformer-based LLMs. Our method employs Integrated Gradients to quantify\neach neuron's influence on high-confidence predictions, pinpointing those that\ndisproportionately contribute to dataset-specific performance without\nsupporting robust, transferable reasoning. Selectively pruning these neurons\ncompels the model to depend on generalizable representations. Evaluated across\nmultiple-choice benchmarks, our pruning-based fine-tuning significantly\nenhances performance, surpassing prior (non-pruning) adaptation methods."}
{"id": "2507.10469", "pdf": "https://arxiv.org/pdf/2507.10469.pdf", "abs": "https://arxiv.org/abs/2507.10469", "title": "An Empirical Evaluation of AI-Powered Non-Player Characters' Perceived Realism and Performance in Virtual Reality Environments", "authors": ["Mikko Korkiakoski", "Saeid Sheikhi", "Jesper Nyman", "Jussi Saariniemi", "Kalle Tapio", "Panos Kostakos"], "categories": ["cs.HC", "cs.AI", "cs.MM"], "comment": null, "summary": "Advancements in artificial intelligence (AI) have significantly enhanced the\nrealism and interactivity of non-player characters (NPCs) in virtual reality\n(VR), creating more engaging and believable user experiences. This paper\nevaluates AI-driven NPCs within a VR interrogation simulator, focusing on their\nperceived realism, usability, and system performance. The simulator features\ntwo AI-powered NPCs, a suspect, and a partner, using GPT-4 Turbo to engage\nparticipants in a scenario to determine the suspect's guilt or innocence. A\nuser study with 18 participants assessed the system using the System Usability\nScale (SUS), Game Experience Questionnaire (GEQ), and a Virtual Agent\nBelievability Questionnaire, alongside latency measurements for speech-to-text\n(STT), text-to-speech (TTS), OpenAI GPT-4 Turbo, and overall (cycle) latency.\nResults showed an average cycle latency of 7 seconds, influenced by the\nincreasing conversational context. Believability scored 6.67 out of 10, with\nhigh ratings in behavior, social relationships, and intelligence but moderate\nscores in emotion and personality. The system achieved a SUS score of 79.44,\nindicating good usability. These findings demonstrate the potential of large\nlanguage models to improve NPC realism and interaction in VR while highlighting\nchallenges in reducing system latency and enhancing emotional depth. This\nresearch contributes to the development of more sophisticated AI-driven NPCs,\nrevealing the need for performance optimization to achieve increasingly\nimmersive virtual experiences."}
{"id": "2507.09205", "pdf": "https://arxiv.org/pdf/2507.09205.pdf", "abs": "https://arxiv.org/abs/2507.09205", "title": "Banzhida: Advancing Large Language Models for Tibetan with Curated Data and Continual Pre-Training", "authors": ["Leiyu Pan", "Bojian Xiong", "Lei Yang", "Renren Jin", "Shaowei Zhang", "Yue Chen", "Ling Shi", "Jiang Zhou", "Junru Wu", "Zhen Wang", "Jianxiang Peng", "Juesi Xiao", "Tianyu Dong", "Zhuowen Han", "Zhuo Chen", "Sangjee Dondrub", "Caizang Tai", "Haixing Zhao", "Huaque Cairang", "Suonan Cairang", "Rou Te", "Lengben Zhaxi", "Gazang Zhaxi", "Zhonglin Ye", "Yuhui Zheng", "Chunyan Peng", "Secha Jia", "Pema Tashi", "Cizhen Jiacuo", "Pema Dorjee", "Hongkai Liu", "Pema Yanggon", "Tsehang Dorjee", "Jiaxin Han", "Qiongying Hu", "Jilin Man", "Huanke You", "Yuqi Ren", "Duo La", "Deyi Xiong"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models have achieved remarkable progress across many\nlanguages. However, Tibetan, as a representative low-resource language, is\nparticularly underrepresented in existing models due to the scarcity of\nhigh-quality training corpora. To address this gap, we curate the largest\nTibetan pre-training corpus to date, aggregating data from diverse sources and\napplying a dedicated data cleaning and processing pipeline tailored for\nTibetan. With the curated data, we continue pre/post-training a multilingual\nbase model into Banzhida, a multilingual large language model that advances\ngenerative AI for Tibetan. To evaluate the Tibetan capabilities of the model,\nwe create new high-quality Tibetan benchmarks, and complement them with\nexisting public benchmarks. Experimental results demonstrate that Banzhida\nconsistently and significantly outperforms both open-source models of similar\nscale and Tibetan-tailored models across a wide range of tasks."}
{"id": "2507.10479", "pdf": "https://arxiv.org/pdf/2507.10479.pdf", "abs": "https://arxiv.org/abs/2507.10479", "title": "VIP-Sim: A User-Centered Approach to Vision Impairment Simulation for Accessible Design", "authors": ["Max RÃ¤dler", "Mark Colley", "Enrico Rukzio"], "categories": ["cs.HC"], "comment": "Conditionally accepted at UIST'25", "summary": "People with vision impairments (VIPs) often rely on their remaining vision\nwhen interacting with user interfaces. Simulating visual impairments is an\neffective tool for designers, fostering awareness of the challenges faced by\nVIPs. While previous research has introduced various vision impairment\nsimulators, none have yet been developed with the direct involvement of VIPs or\nthoroughly evaluated from their perspective. To address this gap, we developed\nVIP-Sim. This symptom-based vision simulator was created through a\nparticipatory design process tailored explicitly for this purpose, involving\nN=7 VIPs. 21 symptoms, like field loss or light sensitivity, can be overlaid on\ndesktop design tools. Most participants felt VIP-Sim could replicate their\nsymptoms. VIP-Sim was received positively, but concerns about exclusion in\ndesign and comprehensiveness of the simulation remain, mainly whether it\nrepresents the experiences of other VIPs."}
{"id": "2507.09225", "pdf": "https://arxiv.org/pdf/2507.09225.pdf", "abs": "https://arxiv.org/abs/2507.09225", "title": "MetaClimage: A novel database of visual metaphors related to Climate Change, with costs and benefits analysis", "authors": ["Biagio Scalingi", "Chiara Barattieri di San Pietro", "Paolo Canal", "Valentina Bambini"], "categories": ["cs.CL", "cs.CY"], "comment": "27 pages, 5 figures", "summary": "Visual metaphors of climate change (e.g., melting glaciers depicted as a\nmelting ice grenade) are regarded as valuable tools for addressing the\ncomplexity of environmental challenges. However, few studies have examined\ntheir impact on communication, also due to scattered availability of material.\nHere, we present a novel database of Metaphors of Climate Change in Images\n(MetaClimage) https://doi.org/10.5281/zenodo.15861012, paired with literal\nimages and enriched with human ratings. For each image, we collected values of\ndifficulty, efficacy, artistic quality, and emotional arousal from human\nrating, as well as number of tags generated by participants to summarize the\nmessage. Semantic and emotion variables were further derived from the tags via\nNatural Language Processing. Visual metaphors were rated as more difficult to\nunderstand, yet more aesthetically pleasant than literal images, but did not\ndiffer in efficacy and arousal. The latter for visual metaphors, however, was\nhigher in participants with higher Need For Cognition. Furthermore, visual\nmetaphors received more tags, often referring to entities not depicted in the\nimage, and elicited words with more positive valence and greater dominance than\nliteral images. These results evidence the greater cognitive load of visual\nmetaphors, which nevertheless might induce positive effects such as deeper\ncognitive elaboration and abstraction compared to literal stimuli. Furthermore,\nwhile they are not deemed as more effective and arousing, visual metaphors seem\nto generate superior aesthetic appreciation and a more positively valenced\nexperience. Overall, this study contributes to understanding the impact of\nvisual metaphors of climate change both by offering a database for future\nresearch and by elucidating a cost-benefit trade-off to take into account when\nshaping environmental communication."}
{"id": "2408.10887", "pdf": "https://arxiv.org/pdf/2408.10887.pdf", "abs": "https://arxiv.org/abs/2408.10887", "title": "A Mini-Review on Mobile Manipulators with Variable Autonomy", "authors": ["Cesar Alan Contreras", "Alireza Rastegarpanah", "Rustam Stolkin", "Manolis Chiou"], "categories": ["cs.RO", "cs.HC"], "comment": "Presented at Variable Autonomy for Human-Robot Teaming (VAT) at IEEE\n  RO-MAN 2024 Workshop", "summary": "This paper presents a mini-review of the current state of research in mobile\nmanipulators with variable levels of autonomy, emphasizing their associated\nchallenges and application environments. The need for mobile manipulators in\ndifferent environments is evident due to the unique challenges and risks each\npresents. Many systems deployed in these environments are not fully autonomous,\nrequiring human-robot teaming to ensure safe and reliable operations under\nuncertainties. Through this analysis, we identify gaps and challenges in the\nliterature on Variable Autonomy, including cognitive workload and communication\ndelays, and propose future directions, including whole-body Variable Autonomy\nfor mobile manipulators, virtual reality frameworks, and large language models\nto reduce operators' complexity and cognitive load in some challenging and\nuncertain scenarios."}
{"id": "2507.09245", "pdf": "https://arxiv.org/pdf/2507.09245.pdf", "abs": "https://arxiv.org/abs/2507.09245", "title": "Swa-bhasha Resource Hub: Romanized Sinhala to Sinhala Transliteration Systems and Data Resources", "authors": ["Deshan Sumanathilaka", "Sameera Perera", "Sachithya Dharmasiri", "Maneesha Athukorala", "Anuja Dilrukshi Herath", "Rukshan Dias", "Pasindu Gamage", "Ruvan Weerasinghe", "Y. H. P. P. Priyadarshana"], "categories": ["cs.CL"], "comment": "13 pages, 3 Tables, 3 figures", "summary": "The Swa-bhasha Resource Hub provides a comprehensive collection of data\nresources and algorithms developed for Romanized Sinhala to Sinhala\ntransliteration between 2020 and 2025. These resources have played a\nsignificant role in advancing research in Sinhala Natural Language Processing\n(NLP), particularly in training transliteration models and developing\napplications involving Romanized Sinhala. The current openly accessible data\nsets and corresponding tools are made publicly available through this hub. This\npaper presents a detailed overview of the resources contributed by the authors\nand includes a comparative analysis of existing transliteration applications in\nthe domain."}
{"id": "2507.08978", "pdf": "https://arxiv.org/pdf/2507.08978.pdf", "abs": "https://arxiv.org/abs/2507.08978", "title": "Characterizing Security and Privacy Teaching Standards for Schools in the United States", "authors": ["Katherine Limes", "Nathan Malkin", "Kelsey R. Fulton"], "categories": ["cs.CR", "cs.HC"], "comment": null, "summary": "Increasingly, students begin learning aspects of security and privacy during\ntheir primary and secondary education (grades K-12 in the United States).\nIndividual U.S. states and some national organizations publish teaching\nstandards -- guidance that outlines expectations for what students should learn\n-- which often form the basis for course curricula. However, research has not\nyet examined what is covered by these standards and whether the topics align\nwith what the broader security and privacy community thinks students should\nknow. To shed light on these questions, we started by collecting computer\nscience teaching standards from all U.S. states and eight national\norganizations. After manually examining a total of 11,954 standards, we labeled\n3,778 of them as being related to security and privacy, further classifying\nthese into 103 topics. Topics ranged from technical subjects like encryption,\nnetwork security, and embedded systems to social subjects such as laws, ethics,\nand appropriate online behavior. Subsequently, we interviewed 11 security and\nprivacy professionals to examine how the teaching standards align with their\nexpectations. We found that, while the specific topics they mentioned mostly\noverlapped with those of existing standards, professionals placed a greater\nemphasis on threat modeling and security mindset."}
{"id": "2507.09259", "pdf": "https://arxiv.org/pdf/2507.09259.pdf", "abs": "https://arxiv.org/abs/2507.09259", "title": "Psychology-Driven Enhancement of Humour Translation", "authors": ["Yuchen Su", "Yonghua Zhu", "Yang Chen", "Diana Benavides-Prado", "Michael Witbrock"], "categories": ["cs.CL"], "comment": null, "summary": "Humour translation plays a vital role as a bridge between different cultures,\nfostering understanding and communication. Although most existing Large\nLanguage Models (LLMs) are capable of general translation tasks, these models\nstill struggle with humour translation, which is especially reflected through\nlinguistic interference and lacking humour in translated text. In this paper,\nwe propose a psychology-inspired Humour Decomposition Mechanism (HDM) that\nutilises Chain-of-Thought (CoT) to imitate the ability of the human thought\nprocess, stimulating LLMs to optimise the readability of translated humorous\ntexts. Moreover, we integrate humour theory in HDM to further enhance the\nhumorous elements in the translated text. Our automatic evaluation experiments\non open-source humour datasets demonstrate that our method significantly\nimproves the quality of humour translation, yielding average gains of 7.75\\% in\nhumour, 2.81\\% in fluency, and 6.13\\% in coherence of the generated text."}
{"id": "2507.09089", "pdf": "https://arxiv.org/pdf/2507.09089.pdf", "abs": "https://arxiv.org/abs/2507.09089", "title": "Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity", "authors": ["Joel Becker", "Nate Rush", "Elizabeth Barnes", "David Rein"], "categories": ["cs.AI", "cs.HC", "cs.SE", "I.2"], "comment": "50 pages, 8 tables, 22 figures", "summary": "Despite widespread adoption, the impact of AI tools on software development\nin the wild remains understudied. We conduct a randomized controlled trial\n(RCT) to understand how AI tools at the February-June 2025 frontier affect the\nproductivity of experienced open-source developers. 16 developers with moderate\nAI experience complete 246 tasks in mature projects on which they have an\naverage of 5 years of prior experience. Each task is randomly assigned to allow\nor disallow usage of early 2025 AI tools. When AI tools are allowed, developers\nprimarily use Cursor Pro, a popular code editor, and Claude 3.5/3.7 Sonnet.\nBefore starting tasks, developers forecast that allowing AI will reduce\ncompletion time by 24%. After completing the study, developers estimate that\nallowing AI reduced completion time by 20%. Surprisingly, we find that allowing\nAI actually increases completion time by 19%--AI tooling slowed developers\ndown. This slowdown also contradicts predictions from experts in economics (39%\nshorter) and ML (38% shorter). To understand this result, we collect and\nevaluate evidence for 20 properties of our setting that a priori could\ncontribute to the observed slowdown effect--for example, the size and quality\nstandards of projects, or prior developer experience with AI tooling. Although\nthe influence of experimental artifacts cannot be entirely ruled out, the\nrobustness of the slowdown effect across our analyses suggests it is unlikely\nto primarily be a function of our experimental design."}
{"id": "2507.09282", "pdf": "https://arxiv.org/pdf/2507.09282.pdf", "abs": "https://arxiv.org/abs/2507.09282", "title": "ClaritySpeech: Dementia Obfuscation in Speech", "authors": ["Dominika Woszczyk", "Ranya Aloufi", "Soteris Demetriou"], "categories": ["cs.CL", "cs.CR", "cs.LG", "cs.SD", "eess.AS"], "comment": "Accepted at Interspeech 2025", "summary": "Dementia, a neurodegenerative disease, alters speech patterns, creating\ncommunication barriers and raising privacy concerns. Current speech\ntechnologies, such as automatic speech transcription (ASR), struggle with\ndementia and atypical speech, further challenging accessibility. This paper\npresents a novel dementia obfuscation in speech framework, ClaritySpeech,\nintegrating ASR, text obfuscation, and zero-shot text-to-speech (TTS) to\ncorrect dementia-affected speech while preserving speaker identity in low-data\nenvironments without fine-tuning. Results show a 16% and 10% drop in mean F1\nscore across various adversarial settings and modalities (audio, text, fusion)\nfor ADReSS and ADReSSo, respectively, maintaining 50% speaker similarity. We\nalso find that our system improves WER (from 0.73 to 0.08 for ADReSS and 0.15\nfor ADReSSo) and speech quality from 1.65 to ~2.15, enhancing privacy and\naccessibility."}
{"id": "2507.09111", "pdf": "https://arxiv.org/pdf/2507.09111.pdf", "abs": "https://arxiv.org/abs/2507.09111", "title": "RoHOI: Robustness Benchmark for Human-Object Interaction Detection", "authors": ["Di Wen", "Kunyu Peng", "Kailun Yang", "Yufan Chen", "Ruiping Liu", "Junwei Zheng", "Alina Roitberg", "Rainer Stiefelhagen"], "categories": ["cs.CV", "cs.HC", "cs.RO", "eess.IV"], "comment": "Benchmarks, datasets, and code will be made publicly available at\n  https://github.com/Kratos-Wen/RoHOI", "summary": "Human-Object Interaction (HOI) detection is crucial for robot-human\nassistance, enabling context-aware support. However, models trained on clean\ndatasets degrade in real-world conditions due to unforeseen corruptions,\nleading to inaccurate prediction. To address this, we introduce the first\nrobustness benchmark for HOI detection, evaluating model resilience under\ndiverse challenges. Despite advances, current models struggle with\nenvironmental variability, occlusion, and noise. Our benchmark, RoHOI, includes\n20 corruption types based on HICO-DET and V-COCO datasets and a new\nrobustness-focused metric. We systematically analyze existing models in the\nrelated field, revealing significant performance drops under corruptions. To\nimprove robustness, we propose a Semantic-Aware Masking-based Progressive\nLearning (SAMPL) strategy to guide the model to be optimized based on holistic\nand partial cues, dynamically adjusting the model's optimization to enhance\nrobust feature learning. Extensive experiments show our approach outperforms\nstate-of-the-art methods, setting a new standard for robust HOI detection.\nBenchmarks, datasets, and code will be made publicly available at\nhttps://github.com/Kratos-Wen/RoHOI."}
{"id": "2507.09424", "pdf": "https://arxiv.org/pdf/2507.09424.pdf", "abs": "https://arxiv.org/abs/2507.09424", "title": "DATE-LM: Benchmarking Data Attribution Evaluation for Large Language Models", "authors": ["Cathy Jiao", "Yijun Pan", "Emily Xiao", "Daisy Sheng", "Niket Jain", "Hanzhang Zhao", "Ishita Dasgupta", "Jiaqi W. Ma", "Chenyan Xiong"], "categories": ["cs.CL"], "comment": null, "summary": "Data attribution methods quantify the influence of training data on model\noutputs and are becoming increasingly relevant for a wide range of LLM research\nand applications, including dataset curation, model interpretability, data\nvaluation. However, there remain critical gaps in systematic LLM-centric\nevaluation of data attribution methods. To this end, we introduce DATE-LM (Data\nAttribution Evaluation in Language Models), a unified benchmark for evaluating\ndata attribution methods through real-world LLM applications. DATE-LM measures\nattribution quality through three key tasks -- training data selection,\ntoxicity/bias filtering, and factual attribution. Our benchmark is designed for\nease of use, enabling researchers to configure and run large-scale evaluations\nacross diverse tasks and LLM architectures. Furthermore, we use DATE-LM to\nconduct a large-scale evaluation of existing data attribution methods. Our\nfindings show that no single method dominates across all tasks, data\nattribution methods have trade-offs with simpler baselines, and method\nperformance is sensitive to task-specific evaluation design. Finally, we\nrelease a public leaderboard for quick comparison of methods and to facilitate\ncommunity engagement. We hope DATE-LM serves as a foundation for future data\nattribution research in LLMs."}
{"id": "2507.09376", "pdf": "https://arxiv.org/pdf/2507.09376.pdf", "abs": "https://arxiv.org/abs/2507.09376", "title": "Acoustic Wave Modeling Using 2D FDTD: Applications in Unreal Engine For Dynamic Sound Rendering", "authors": ["Bilkent Samsurya"], "categories": ["cs.SD", "cs.HC", "cs.MM", "eess.AS", "H.5.5"], "comment": "Accepted to the 50th International Computer Music Conference (ICMC),\n  2025", "summary": "Accurate sound propagation simulation is essential for delivering immersive\nexperiences in virtual applications, yet industry methods for acoustic modeling\noften do not account for the full breadth of acoustic wave phenomena. This\npaper proposes a novel two-dimensional (2D) finite-difference time-domain\n(FDTD) framework that simulates sound propagation as a wave-based model in\nUnreal Engine, with an emphasis on capturing lower frequency wave phenomena,\nembedding occlusion, diffraction, reflection and interference in generated\nimpulse responses. The process begins by discretizing the scene geometry into a\n2D grid via a top-down projection from which obstacle masks and boundary\nconditions are derived. A Python-based FDTD solver injects a sine sweep at a\nsource position, and virtual quadraphonic microphone arrays record pressure\nfield responses at pre-defined listener positions. De-convolution of the\npressure responses yields multi-channel impulse responses that retain spatial\ndirectionality which are then integrated into Unreal Engine's audio pipeline\nfor dynamic playback. Benchmark tests confirm agreement with analytical\nexpectations, and the paper outlines hybrid extensions aimed at commercial\nviability."}
{"id": "2507.09470", "pdf": "https://arxiv.org/pdf/2507.09470.pdf", "abs": "https://arxiv.org/abs/2507.09470", "title": "Enhancing Clinical Text Classification via Fine-Tuned DRAGON Longformer Models", "authors": ["Mingchuan Yang", "Ziyuan Huang"], "categories": ["cs.CL", "cs.AI", "68T07"], "comment": "29 pages, 5 tables", "summary": "This study explores the optimization of the DRAGON Longformer base model for\nclinical text classification, specifically targeting the binary classification\nof medical case descriptions. A dataset of 500 clinical cases containing\nstructured medical observations was used, with 400 cases for training and 100\nfor validation. Enhancements to the pre-trained\njoeranbosma/dragon-longformer-base-mixed-domain model included hyperparameter\ntuning, domain-specific preprocessing, and architectural adjustments. Key\nmodifications involved increasing sequence length from 512 to 1024 tokens,\nadjusting learning rates from 1e-05 to 5e-06, extending training epochs from 5\nto 8, and incorporating specialized medical terminology. The optimized model\nachieved notable performance gains: accuracy improved from 72.0% to 85.2%,\nprecision from 68.0% to 84.1%, recall from 75.0% to 86.3%, and F1-score from\n71.0% to 85.2%. Statistical analysis confirmed the significance of these\nimprovements (p < .001). The model demonstrated enhanced capability in\ninterpreting medical terminology, anatomical measurements, and clinical\nobservations. These findings contribute to domain-specific language model\nresearch and offer practical implications for clinical natural language\nprocessing applications. The optimized model's strong performance across\ndiverse medical conditions underscores its potential for broad use in\nhealthcare settings."}
{"id": "2507.09482", "pdf": "https://arxiv.org/pdf/2507.09482.pdf", "abs": "https://arxiv.org/abs/2507.09482", "title": "ViSP: A PPO-Driven Framework for Sarcasm Generation with Contrastive Learning", "authors": ["Changli Wang", "Rui Wu", "Fang Yin"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Human emotions are complex, with sarcasm being a subtle and distinctive form.\nDespite progress in sarcasm research, sarcasm generation remains underexplored,\nprimarily due to the overreliance on textual modalities and the neglect of\nvisual cues, as well as the mismatch between image content and sarcastic intent\nin existing datasets. In this paper, we introduce M2SaG, a multimodal sarcasm\ngeneration dataset with 4,970 samples, each containing an image, a sarcastic\ntext, and a sarcasm target. To benchmark M2SaG, we propose ViSP, a generation\nframework that integrates Proximal Policy Optimization (PPO) and contrastive\nlearning. PPO utilizes reward scores from DIP to steer the generation of\nsarcastic texts, while contrastive learning encourages the model to favor\noutputs with higher reward scores. These strategies improve overall generation\nquality and produce texts with more pronounced sarcastic intent. We evaluate\nViSP across five metric sets and find it surpasses all baselines, including\nlarge language models, underscoring their limitations in sarcasm generation.\nFurthermore, we analyze the distributions of Sarcasm Scores and Factual\nIncongruity for both M2SaG and the texts generated by ViSP. The generated texts\nexhibit higher mean Sarcasm Scores (0.898 vs. 0.770) and Factual Incongruity\n(0.768 vs. 0.739), demonstrating that ViSP produces higher-quality sarcastic\ncontent than the original dataset. % The dataset and code will be publicly\navailable. Our dataset and code will be released at\n\\textit{https://github.com/wclapply/ViSP}."}
{"id": "2507.09474", "pdf": "https://arxiv.org/pdf/2507.09474.pdf", "abs": "https://arxiv.org/abs/2507.09474", "title": "The CoNLL-2013 Shared Task on Grammatical Error Correction", "authors": ["Hwee Tou Ng", "Siew Mei Wu", "Yuanbin Wu", "Christian Hadiwinoto", "Joel Tetreault"], "categories": ["cs.CL"], "comment": "12 pages", "summary": "The CoNLL-2013 shared task was devoted to grammatical error correction. In\nthis paper, we give the task definition, present the data sets, and describe\nthe evaluation metric and scorer used in the shared task. We also give an\noverview of the various approaches adopted by the participating teams, and\npresent the evaluation results."}
{"id": "2507.09495", "pdf": "https://arxiv.org/pdf/2507.09495.pdf", "abs": "https://arxiv.org/abs/2507.09495", "title": "GenAI-based Multi-Agent Reinforcement Learning towards Distributed Agent Intelligence: A Generative-RL Agent Perspective", "authors": ["Hang Wang", "Junshan Zhang"], "categories": ["cs.AI", "cs.ET", "cs.HC", "cs.RO", "cs.SY", "eess.SY"], "comment": "Position paper", "summary": "Multi-agent reinforcement learning faces fundamental challenges that\nconventional approaches have failed to overcome: exponentially growing joint\naction spaces, non-stationary environments where simultaneous learning creates\nmoving targets, and partial observability that constrains coordination. Current\nmethods remain reactive, employing stimulus-response mechanisms that fail when\nfacing novel scenarios. We argue for a transformative paradigm shift from\nreactive to proactive multi-agent intelligence through generative AI-based\nreinforcement learning. This position advocates reconceptualizing agents not as\nisolated policy optimizers, but as sophisticated generative models capable of\nsynthesizing complex multi-agent dynamics and making anticipatory decisions\nbased on predictive understanding of future interactions. Rather than\nresponding to immediate observations, generative-RL agents can model\nenvironment evolution, predict other agents' behaviors, generate coordinated\naction sequences, and engage in strategic reasoning accounting for long-term\ndynamics. This approach leverages pattern recognition and generation\ncapabilities of generative AI to enable proactive decision-making, seamless\ncoordination through enhanced communication, and dynamic adaptation to evolving\nscenarios. We envision this paradigm shift will unlock unprecedented\npossibilities for distributed intelligence, moving beyond individual\noptimization toward emergent collective behaviors representing genuine\ncollaborative intelligence. The implications extend across autonomous systems,\nrobotics, and human-AI collaboration, promising solutions to coordination\nchallenges intractable under traditional reactive frameworks."}
{"id": "2507.09477", "pdf": "https://arxiv.org/pdf/2507.09477.pdf", "abs": "https://arxiv.org/abs/2507.09477", "title": "Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs", "authors": ["Yangning Li", "Weizhi Zhang", "Yuyao Yang", "Wei-Chieh Huang", "Yaozu Wu", "Junyu Luo", "Yuanchen Bei", "Henry Peng Zou", "Xiao Luo", "Yusheng Zhao", "Chunkit Chan", "Yankai Chen", "Zhongfen Deng", "Yinghui Li", "Hai-Tao Zheng", "Dongyuan Li", "Renhe Jiang", "Ming Zhang", "Yangqiu Song", "Philip S. Yu"], "categories": ["cs.CL", "cs.AI"], "comment": "submitted to ARR May", "summary": "Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language\nModels (LLMs) by injecting external knowledge, yet it falls short on problems\nthat demand multi-step inference; conversely, purely reasoning-oriented\napproaches often hallucinate or mis-ground facts. This survey synthesizes both\nstrands under a unified reasoning-retrieval perspective. We first map how\nadvanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then,\nwe show how retrieved knowledge of different type supply missing premises and\nexpand context for complex inference (RAG-Enhanced Reasoning). Finally, we\nspotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs\niteratively interleave search and reasoning to achieve state-of-the-art\nperformance across knowledge-intensive benchmarks. We categorize methods,\ndatasets, and open challenges, and outline research avenues toward deeper\nRAG-Reasoning systems that are more effective, multimodally-adaptive,\ntrustworthy, and human-centric. The collection is available at\nhttps://github.com/DavidZWZ/Awesome-RAG-Reasoning."}
{"id": "2507.09637", "pdf": "https://arxiv.org/pdf/2507.09637.pdf", "abs": "https://arxiv.org/abs/2507.09637", "title": "Code Review as Decision-Making -- Building a Cognitive Model from the Questions Asked During Code Review", "authors": ["Lo Gullstrand Heander", "Emma SÃ¶derberg", "Christofer RydenfÃ¤lt"], "categories": ["cs.SE", "cs.HC", "D.2.0; D.2.3; K.4.3"], "comment": "39 pages, 14 figures Submitted to Empirical Software Engineering,\n  Springer Nature", "summary": "Code review is a well-established and valued practice in the software\nengineering community contributing to both code quality and interpersonal\nbenefits. However, there are challenges in both tools and processes that give\nrise to misalignments and frustrations. Recent research seeks to address this\nby automating code review entirely, but we believe that this risks losing the\nmajority of the interpersonal benefits such as knowledge transfer and shared\nownership.\n  We believe that by better understanding the cognitive processes involved in\ncode review, it would be possible to improve tool support, with out without AI,\nand make code review both more efficient, more enjoyable, while increasing or\nmaintaining all of its benefits. In this paper, we conduct an ethnographic\nthink-aloud study involving 10 participants and 34 code reviews. We build a\ncognitive model of code review bottom up through thematic, statistical,\ntemporal, and sequential analysis of the transcribed material. Through the\ndata, the similarities between the cognitive process in code review and\ndecision-making processes, especially recognition-primed decision-making,\nbecome apparent.\n  The result is the Code Review as Decision-Making (CRDM) model that shows how\nthe developers move through two phases during the code review; first an\norientation phase to establish context and rationale and then an analytical\nphase to understand, assess, and plan the rest of the review. Throughout the\nprocess several decisions must be taken, on writing comments, finding more\ninformation, voting, running the code locally, verifying continuous integration\nresults, etc.\n  Analysis software and process-coded data publicly available at:\nhttps://doi.org/10.5281/zenodo.15758266"}
{"id": "2507.09482", "pdf": "https://arxiv.org/pdf/2507.09482.pdf", "abs": "https://arxiv.org/abs/2507.09482", "title": "ViSP: A PPO-Driven Framework for Sarcasm Generation with Contrastive Learning", "authors": ["Changli Wang", "Rui Wu", "Fang Yin"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Human emotions are complex, with sarcasm being a subtle and distinctive form.\nDespite progress in sarcasm research, sarcasm generation remains underexplored,\nprimarily due to the overreliance on textual modalities and the neglect of\nvisual cues, as well as the mismatch between image content and sarcastic intent\nin existing datasets. In this paper, we introduce M2SaG, a multimodal sarcasm\ngeneration dataset with 4,970 samples, each containing an image, a sarcastic\ntext, and a sarcasm target. To benchmark M2SaG, we propose ViSP, a generation\nframework that integrates Proximal Policy Optimization (PPO) and contrastive\nlearning. PPO utilizes reward scores from DIP to steer the generation of\nsarcastic texts, while contrastive learning encourages the model to favor\noutputs with higher reward scores. These strategies improve overall generation\nquality and produce texts with more pronounced sarcastic intent. We evaluate\nViSP across five metric sets and find it surpasses all baselines, including\nlarge language models, underscoring their limitations in sarcasm generation.\nFurthermore, we analyze the distributions of Sarcasm Scores and Factual\nIncongruity for both M2SaG and the texts generated by ViSP. The generated texts\nexhibit higher mean Sarcasm Scores (0.898 vs. 0.770) and Factual Incongruity\n(0.768 vs. 0.739), demonstrating that ViSP produces higher-quality sarcastic\ncontent than the original dataset. % The dataset and code will be publicly\navailable. Our dataset and code will be released at\n\\textit{https://github.com/wclapply/ViSP}."}
{"id": "2507.09788", "pdf": "https://arxiv.org/pdf/2507.09788.pdf", "abs": "https://arxiv.org/abs/2507.09788", "title": "TinyTroupe: An LLM-powered Multiagent Persona Simulation Toolkit", "authors": ["Paulo Salem", "Robert Sim", "Christopher Olsen", "Prerit Saxena", "Rafael Barcelos", "Yi Ding"], "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.HC", "I.2.11; I.6.5; I.6.7"], "comment": "9 pages. Preprint to be submitted to peer-review", "summary": "Recent advances in Large Language Models (LLM) have led to a new class of\nautonomous agents, renewing and expanding interest in the area. LLM-powered\nMultiagent Systems (MAS) have thus emerged, both for assistive and simulation\npurposes, yet tools for realistic human behavior simulation -- with its\ndistinctive challenges and opportunities -- remain underdeveloped. Existing MAS\nlibraries and tools lack fine-grained persona specifications, population\nsampling facilities, experimentation support, and integrated validation, among\nother key capabilities, limiting their utility for behavioral studies, social\nsimulation, and related applications. To address these deficiencies, in this\nwork we introduce TinyTroupe, a simulation toolkit enabling detailed persona\ndefinitions (e.g., nationality, age, occupation, personality, beliefs,\nbehaviors) and programmatic control via numerous LLM-driven mechanisms. This\nallows for the concise formulation of behavioral problems of practical\ninterest, either at the individual or group level, and provides effective means\nfor their solution. TinyTroupe's components are presented using representative\nworking examples, such as brainstorming and market research sessions, thereby\nsimultaneously clarifying their purpose and demonstrating their usefulness.\nQuantitative and qualitative evaluations of selected aspects are also provided,\nhighlighting possibilities, limitations, and trade-offs. The approach, though\nrealized as a specific Python implementation, is meant as a novel conceptual\ncontribution, which can be partially or fully incorporated in other contexts.\nThe library is available as open source at\nhttps://github.com/microsoft/tinytroupe."}
{"id": "2507.09485", "pdf": "https://arxiv.org/pdf/2507.09485.pdf", "abs": "https://arxiv.org/abs/2507.09485", "title": "Balanced Training Data Augmentation for Aspect-Based Sentiment Analysis", "authors": ["Junjie Liu", "Yuanhe Tian", "Yan Song"], "categories": ["cs.CL"], "comment": null, "summary": "Aspect-based sentiment analysis (ABSA) is a crucial fine-grained task in\nsocial media scenarios to identify the sentiment polarity of specific aspect\nterms in a sentence. Although many existing studies leverage large language\nmodels (LLMs) to perform ABSA due to their strong context understanding\ncapabilities, they still face challenges to learn the context information in\nthe running text because of the short text, as well as the small and unbalanced\nlabeled training data, where most data are labeled with positive sentiment.\nData augmentation (DA) is a feasible strategy for providing richer contextual\ninformation, especially when using LLMs to create synthetic training data, but\nfaces challenges in ensuring a high quality of the augmented data.In this\npaper, we propose an LLM-based ABSA approach with training data\naugmentation.Specifically, an LLM is prompted to generate augmented training\ndata based on the original training data, so as to construct a new training\ndata with larger size and balanced label distributions to better train an ABSA\nmodel. Meanwhile, in order to improve the quality of the augmented data, we\npropose a reinforcement learning approach to optimize the data augmentation.\nLLM.Experiment results and further analyses on English benchmark datasets for\nABSA demonstrate the effectiveness of our approach, where superior performance\nis observed over strong baselines and most existing studies."}
{"id": "2507.10131", "pdf": "https://arxiv.org/pdf/2507.10131.pdf", "abs": "https://arxiv.org/abs/2507.10131", "title": "Probabilistic Human Intent Prediction for Mobile Manipulation: An Evaluation with Human-Inspired Constraints", "authors": ["Cesar Alan Contreras", "Manolis Chiou", "Alireza Rastegarpanah", "Michal Szulik", "Rustam Stolkin"], "categories": ["cs.RO", "cs.CV", "cs.HC"], "comment": "Submitted to Journal of Intelligent & Robotic Systems (Under Review)", "summary": "Accurate inference of human intent enables human-robot collaboration without\nconstraining human control or causing conflicts between humans and robots. We\npresent GUIDER (Global User Intent Dual-phase Estimation for Robots), a\nprobabilistic framework that enables a robot to estimate the intent of human\noperators. GUIDER maintains two coupled belief layers, one tracking navigation\ngoals and the other manipulation goals. In the Navigation phase, a Synergy Map\nblends controller velocity with an occupancy grid to rank interaction areas.\nUpon arrival at a goal, an autonomous multi-view scan builds a local 3D cloud.\nThe Manipulation phase combines U2Net saliency, FastSAM instance saliency, and\nthree geometric grasp-feasibility tests, with an end-effector kinematics-aware\nupdate rule that evolves object probabilities in real-time. GUIDER can\nrecognize areas and objects of intent without predefined goals. We evaluated\nGUIDER on 25 trials (five participants x five task variants) in Isaac Sim, and\ncompared it with two baselines, one for navigation and one for manipulation.\nAcross the 25 trials, GUIDER achieved a median stability of 93-100% during\nnavigation, compared with 60-100% for the BOIR baseline, with an improvement of\n39.5% in a redirection scenario (T5). During manipulation, stability reached\n94-100% (versus 69-100% for Trajectron), with a 31.4% difference in a\nredirection task (T3). In geometry-constrained trials (manipulation), GUIDER\nrecognized the object intent three times earlier than Trajectron (median\nremaining time to confident prediction 23.6 s vs 7.8 s). These results validate\nour dual-phase framework and show improvements in intent inference in both\nphases of mobile manipulation tasks."}
{"id": "2507.09497", "pdf": "https://arxiv.org/pdf/2507.09497.pdf", "abs": "https://arxiv.org/abs/2507.09497", "title": "GoalfyMax: A Protocol-Driven Multi-Agent System for Intelligent Experience Entities", "authors": ["Siyi Wu", "Zeyu Wang", "Xinyuan Song", "Zhengpeng Zhou", "Lifan Sun", "Tianyu Shi"], "categories": ["cs.CL"], "comment": null, "summary": "Modern enterprise environments demand intelligent systems capable of handling\ncomplex, dynamic, and multi-faceted tasks with high levels of autonomy and\nadaptability. However, traditional single-purpose AI systems often lack\nsufficient coordination, memory reuse, and task decomposition capabilities,\nlimiting their scalability in realistic settings. To address these challenges,\nwe present \\textbf{GoalfyMax}, a protocol-driven framework for end-to-end\nmulti-agent collaboration. GoalfyMax introduces a standardized Agent-to-Agent\n(A2A) communication layer built on the Model Context Protocol (MCP), allowing\nindependent agents to coordinate through asynchronous, protocol-compliant\ninteractions. It incorporates the Experience Pack (XP) architecture, a layered\nmemory system that preserves both task rationales and execution traces,\nenabling structured knowledge retention and continual learning. Moreover, our\nsystem integrates advanced features including multi-turn contextual dialogue,\nlong-short term memory modules, and dynamic safety validation, supporting\nrobust, real-time strategy adaptation. Empirical results on complex task\norchestration benchmarks and case study demonstrate that GoalfyMax achieves\nsuperior adaptability, coordination, and experience reuse compared to baseline\nframeworks. These findings highlight its potential as a scalable, future-ready\nfoundation for multi-agent intelligent systems."}
{"id": "2507.10135", "pdf": "https://arxiv.org/pdf/2507.10135.pdf", "abs": "https://arxiv.org/abs/2507.10135", "title": "Riding the Carousel: The First Extensive Eye Tracking Analysis of Browsing Behavior in Carousel Recommenders", "authors": ["Santiago de Leon-Martinez", "Robert Moro", "Branislav Kveton", "Maria Bielikova"], "categories": ["cs.IR", "cs.HC"], "comment": null, "summary": "Carousels have become the de-facto interface in online services. However,\nthere is a lack of research in carousels, particularly examining how\nrecommender systems may be designed differently than the traditional\nsingle-list interfaces. One of the key elements for understanding how to design\na system for a particular interface is understanding how users browse. For\ncarousels, users may browse in a number of different ways due to the added\ncomplexity of multiple topic defined-lists and swiping to see more items.\n  Eye tracking is the key to understanding user behavior by providing valuable,\ndirect information on how users see and navigate. In this work, we provide the\nfirst extensive analysis of the eye tracking behavior in carousel recommenders\nunder the free-browsing setting. To understand how users browse, we examine the\nfollowing research questions : 1) where do users start browsing, 2) how do\nusers transition from item to item within the same carousel and across\ncarousels, and 3) how does genre preference impact transitions?\n  This work addresses a gap in the field and provides the first extensive\nempirical results of eye tracked browsing behavior in carousels for improving\nrecommenders. Taking into account the insights learned from the above\nquestions, our final contribution is to provide suggestions to help carousel\nrecommender system designers optimize their systems for user browsing behavior.\nThe most important suggestion being to reorder the ranked item positions to\naccount for browsing after swiping.These contributions aim not only to help\nimprove current systems, but also to encourage and allow the design of new user\nmodels, systems, and metrics that are better suited to the complexity of\ncarousel interfaces."}
{"id": "2507.09506", "pdf": "https://arxiv.org/pdf/2507.09506.pdf", "abs": "https://arxiv.org/abs/2507.09506", "title": "Ref-Long: Benchmarking the Long-context Referencing Capability of Long-context Language Models", "authors": ["Junjie Wu", "Gefei Gu", "Yanan Zheng", "Dit-Yan Yeung", "Arman Cohan"], "categories": ["cs.CL"], "comment": "ACL 2025 Main Conference. First 2 authors contributed equally", "summary": "Long-context language models (LCLMs) have exhibited impressive capabilities\nin long-context understanding tasks. Among these, long-context referencing -- a\ncrucial task that requires LCLMs to attribute items of interest to specific\nparts of long-context data -- remains underexplored. To bridge this gap, this\npaper proposes Referencing Evaluation for Long-context Language Models\n(Ref-Long), a novel benchmark designed to assess the long-context referencing\ncapability of LCLMs. Specifically, Ref-Long requires LCLMs to identify the\nindexes of documents that reference a specific key, emphasizing contextual\nrelationships between the key and the documents over simple retrieval. Based on\nthe task design, we construct three subsets ranging from synthetic to realistic\nscenarios to form the Ref-Long benchmark. Experimental results of 13 LCLMs\nreveal significant shortcomings in long-context referencing, even among\nadvanced models like GPT-4o. To further investigate these challenges, we\nconduct comprehensive analyses, including human evaluations, task format\nadjustments, fine-tuning experiments, and error analyses, leading to several\nkey insights. Our data and code can be found in https://github.\ncom/wujunjie1998/Ref-Long."}
{"id": "2507.10208", "pdf": "https://arxiv.org/pdf/2507.10208.pdf", "abs": "https://arxiv.org/abs/2507.10208", "title": "Survey for Categorising Explainable AI Studies Using Data Analysis Task Frameworks", "authors": ["Hamzah Ziadeh", "Hendrik Knoche"], "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "Research into explainable artificial intelligence (XAI) for data analysis\ntasks suffer from a large number of contradictions and lack of concrete design\nrecommendations stemming from gaps in understanding the tasks that require AI\nassistance. In this paper, we drew on multiple fields such as visual analytics,\ncognition, and dashboard design to propose a method for categorising and\ncomparing XAI studies under three dimensions: what, why, and who. We identified\nthe main problems as: inadequate descriptions of tasks, context-free studies,\nand insufficient testing with target users. We propose that studies should\nspecifically report on their users' domain, AI, and data analysis expertise to\nillustrate the generalisability of their findings. We also propose study\nguidelines for designing and reporting XAI tasks to improve the XAI community's\nability to parse the rapidly growing field. We hope that our contribution can\nhelp researchers and designers better identify which studies are most relevant\nto their work, what gaps exist in the research, and how to handle contradictory\nresults regarding XAI design."}
{"id": "2507.09509", "pdf": "https://arxiv.org/pdf/2507.09509.pdf", "abs": "https://arxiv.org/abs/2507.09509", "title": "How Important is `Perfect' English for Machine Translation Prompts?", "authors": ["PatrÃ­cia SchmidtovÃ¡", "Niyati Bafna", "Seth Aycock", "Gianluca Vico", "Wiktor Kamzela", "Katharina HÃ¤mmerl", "VilÃ©m Zouhar"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have achieved top results in recent machine\ntranslation evaluations, but they are also known to be sensitive to errors and\nperturbations in their prompts. We systematically evaluate how both humanly\nplausible and synthetic errors in user prompts affect LLMs' performance on two\nrelated tasks: Machine translation and machine translation evaluation. We\nprovide both a quantitative analysis and qualitative insights into how the\nmodels respond to increasing noise in the user prompt.\n  The prompt quality strongly affects the translation performance: With many\nerrors, even a good prompt can underperform a minimal or poor prompt without\nerrors. However, different noise types impact translation quality differently,\nwith character-level and combined noisers degrading performance more than\nphrasal perturbations. Qualitative analysis reveals that lower prompt quality\nlargely leads to poorer instruction following, rather than directly affecting\ntranslation quality itself. Further, LLMs can still translate in scenarios with\noverwhelming random noise that would make the prompt illegible to humans."}
{"id": "2507.10500", "pdf": "https://arxiv.org/pdf/2507.10500.pdf", "abs": "https://arxiv.org/abs/2507.10500", "title": "Scene-Aware Conversational ADAS with Generative AI for Real-Time Driver Assistance", "authors": ["Kyungtae Han", "Yitao Chen", "Rohit Gupta", "Onur Altintas"], "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.HC"], "comment": null, "summary": "While autonomous driving technologies continue to advance, current Advanced\nDriver Assistance Systems (ADAS) remain limited in their ability to interpret\nscene context or engage with drivers through natural language. These systems\ntypically rely on predefined logic and lack support for dialogue-based\ninteraction, making them inflexible in dynamic environments or when adapting to\ndriver intent. This paper presents Scene-Aware Conversational ADAS (SC-ADAS), a\nmodular framework that integrates Generative AI components including large\nlanguage models, vision-to-text interpretation, and structured function calling\nto enable real-time, interpretable, and adaptive driver assistance. SC-ADAS\nsupports multi-turn dialogue grounded in visual and sensor context, allowing\nnatural language recommendations and driver-confirmed ADAS control. Implemented\nin the CARLA simulator with cloud-based Generative AI, the system executes\nconfirmed user intents as structured ADAS commands without requiring model\nfine-tuning. We evaluate SC-ADAS across scene-aware, conversational, and\nrevisited multi-turn interactions, highlighting trade-offs such as increased\nlatency from vision-based context retrieval and token growth from accumulated\ndialogue history. These results demonstrate the feasibility of combining\nconversational reasoning, scene perception, and modular ADAS control to support\nthe next generation of intelligent driver assistance."}
{"id": "2507.09536", "pdf": "https://arxiv.org/pdf/2507.09536.pdf", "abs": "https://arxiv.org/abs/2507.09536", "title": "Adapting Definition Modeling for New Languages: A Case Study on Belarusian", "authors": ["Daniela Kazakouskaya", "Timothee Mickus", "Janine Siewert"], "categories": ["cs.CL"], "comment": "To appear at SlavicNLP 2025", "summary": "Definition modeling, the task of generating new definitions for words in\ncontext, holds great prospect as a means to assist the work of lexicographers\nin documenting a broader variety of lects and languages, yet much remains to be\ndone in order to assess how we can leverage pre-existing models for as-of-yet\nunsupported languages. In this work, we focus on adapting existing models to\nBelarusian, for which we propose a novel dataset of 43,150 definitions. Our\nexperiments demonstrate that adapting a definition modeling systems requires\nminimal amounts of data, but that there currently are gaps in what automatic\nmetrics do capture."}
{"id": "2507.10510", "pdf": "https://arxiv.org/pdf/2507.10510.pdf", "abs": "https://arxiv.org/abs/2507.10510", "title": "Chat with AI: The Surprising Turn of Real-time Video Communication from Human to AI", "authors": ["Jiangkai Wu", "Zhiyuan Ren", "Liming Liu", "Xinggong Zhang"], "categories": ["cs.NI", "cs.AI", "cs.HC", "cs.MM"], "comment": null, "summary": "AI Video Chat emerges as a new paradigm for Real-time Communication (RTC),\nwhere one peer is not a human, but a Multimodal Large Language Model (MLLM).\nThis makes interaction between humans and AI more intuitive, as if chatting\nface-to-face with a real person. However, this poses significant challenges to\nlatency, because the MLLM inference takes up most of the response time, leaving\nvery little time for video streaming. Due to network uncertainty and\ninstability, transmission latency becomes a critical bottleneck preventing AI\nfrom being like a real person. To address this, we propose Artic, an\nAI-oriented Real-time Communication framework, exploring the network\nrequirement shift from \"humans watching video\" to \"AI understanding video\". To\nreduce bitrate dramatically while maintaining MLLM accuracy, we propose\nContext-Aware Video Streaming that recognizes the importance of each video\nregion for chat and allocates bitrate almost exclusively to chat-important\nregions. To avoid packet retransmission, we propose Loss-Resilient Adaptive\nFrame Rate that leverages previous frames to substitute for lost/delayed frames\nwhile avoiding bitrate waste. To evaluate the impact of video streaming quality\non MLLM accuracy, we build the first benchmark, named Degraded Video\nUnderstanding Benchmark (DeViBench). Finally, we discuss some open questions\nand ongoing solutions for AI Video Chat."}
{"id": "2507.09601", "pdf": "https://arxiv.org/pdf/2507.09601.pdf", "abs": "https://arxiv.org/abs/2507.09601", "title": "NMIXX: Domain-Adapted Neural Embeddings for Cross-Lingual eXploration of Finance", "authors": ["Hanwool Lee", "Sara Yu", "Yewon Hwang", "Jonghyun Choi", "Heejae Ahn", "Sungbum Jung", "Youngjae Yu"], "categories": ["cs.CL", "cs.AI", "q-fin.CP"], "comment": "Under Review", "summary": "General-purpose sentence embedding models often struggle to capture\nspecialized financial semantics, especially in low-resource languages like\nKorean, due to domain-specific jargon, temporal meaning shifts, and misaligned\nbilingual vocabularies. To address these gaps, we introduce NMIXX (Neural\neMbeddings for Cross-lingual eXploration of Finance), a suite of cross-lingual\nembedding models fine-tuned with 18.8K high-confidence triplets that pair\nin-domain paraphrases, hard negatives derived from a semantic-shift typology,\nand exact Korean-English translations. Concurrently, we release KorFinSTS, a\n1,921-pair Korean financial STS benchmark spanning news, disclosures, research\nreports, and regulations, designed to expose nuances that general benchmarks\nmiss.\n  When evaluated against seven open-license baselines, NMIXX's multilingual\nbge-m3 variant achieves Spearman's rho gains of +0.10 on English FinSTS and\n+0.22 on KorFinSTS, outperforming its pre-adaptation checkpoint and surpassing\nother models by the largest margin, while revealing a modest trade-off in\ngeneral STS performance. Our analysis further shows that models with richer\nKorean token coverage adapt more effectively, underscoring the importance of\ntokenizer design in low-resource, cross-lingual settings. By making both models\nand the benchmark publicly available, we provide the community with robust\ntools for domain-adapted, multilingual representation learning in finance."}
{"id": "2402.09750", "pdf": "https://arxiv.org/pdf/2402.09750.pdf", "abs": "https://arxiv.org/abs/2402.09750", "title": "Pinning \"Reflection\" on the Agenda: Investigating Reflection in Human-LLM Co-Creation for Creative Coding", "authors": ["Anqi Wang", "Zhizhuo Yin", "Yulu Hu", "Yuanyuan Mao", "Lei Han", "Xin Tong", "Keqin Jiao", "Pan Hui"], "categories": ["cs.HC", "cs.AI", "J.5"], "comment": "6 pages, 2 figures, 2 tables", "summary": "Large language models (LLMs) are increasingly integrated into creative\ncoding, yet how users reflect, and how different co-creation conditions\ninfluence reflective behavior, remains underexplored. This study investigates\nsituated, moment-to-moment reflection in creative coding under two prompting\nstrategies: the entire task invocation (T1) and decomposed subtask invocation\n(T2), to examine their effects on reflective behavior. Our mixed-method results\nreveal three distinct reflection types and show that T2 encourages more\nfrequent, strategic, and generative reflection, fostering diagnostic reasoning\nand goal redefinition. These findings offer insights into how LLM-based tools\nfoster deeper creative engagement through structured, behaviorally grounded\nreflection support."}
{"id": "2507.09628", "pdf": "https://arxiv.org/pdf/2507.09628.pdf", "abs": "https://arxiv.org/abs/2507.09628", "title": "SpreadPy: A Python tool for modelling spreading activation and superdiffusion in cognitive multiplex networks", "authors": ["Salvatore Citraro", "Edith Haim", "Alessandra Carini", "Cynthia S. Q. Siew", "Giulio Rossetti", "Massimo Stella"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce SpreadPy as a Python library for simulating spreading activation\nin cognitive single-layer and multiplex networks. Our tool is designed to\nperform numerical simulations testing structure-function relationships in\ncognitive processes. By comparing simulation results with grounded theories in\nknowledge modelling, SpreadPy enables systematic investigations of how\nactivation dynamics reflect cognitive, psychological and clinical phenomena. We\ndemonstrate the library's utility through three case studies: (1) Spreading\nactivation on associative knowledge networks distinguishes students with high\nversus low math anxiety, revealing anxiety-related structural differences in\nconceptual organization; (2) Simulations of a creativity task show that\nactivation trajectories vary with task difficulty, exposing how cognitive load\nmodulates lexical access; (3) In individuals with aphasia, simulated activation\npatterns on lexical networks correlate with empirical error types (semantic vs.\nphonological) during picture-naming tasks, linking network structure to\nclinical impairments. SpreadPy's flexible framework allows researchers to model\nthese processes using empirically derived or theoretical networks, providing\nmechanistic insights into individual differences and cognitive impairments. The\nlibrary is openly available, supporting reproducible research in psychology,\nneuroscience, and education research."}
{"id": "2403.08969", "pdf": "https://arxiv.org/pdf/2403.08969.pdf", "abs": "https://arxiv.org/abs/2403.08969", "title": "The Full-scale Assembly Simulation Testbed (FAST) Dataset", "authors": ["Alec G. Moore", "Tiffany D. Do", "Nayan N. Chawla", "Antonia Jimenez Iriarte", "Ryan P. McMahan"], "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "In recent years, numerous researchers have begun investigating how virtual\nreality (VR) tracking and interaction data can be used for a variety of machine\nlearning purposes, including user identification, predicting cybersickness, and\nestimating learning gains. One constraint for this research area is the dearth\nof open datasets. In this paper, we present a new open dataset captured with\nour VR-based Full-scale Assembly Simulation Testbed (FAST). This dataset\nconsists of data collected from 108 participants (50 females, 56 males, 2\nnon-binary) learning how to assemble two distinct full-scale structures in VR.\nIn addition to explaining how the dataset was collected and describing the data\nincluded, we discuss how the dataset may be used by future researchers."}
{"id": "2507.09629", "pdf": "https://arxiv.org/pdf/2507.09629.pdf", "abs": "https://arxiv.org/abs/2507.09629", "title": "An Exploration of Knowledge Editing for Arabic", "authors": ["Basel Mousi", "Nadir Durrani", "Fahim Dalvi"], "categories": ["cs.CL"], "comment": null, "summary": "While Knowledge Editing (KE) has been widely explored in English, its\nbehavior in morphologically rich languages like Arabic remains underexamined.\nIn this work, we present the first study of Arabic KE. We evaluate four methods\n(ROME, MEMIT, ICE, and LTE) on Arabic translations of the ZsRE and Counterfact\nbenchmarks, analyzing both multilingual and cross-lingual settings. Our\nexperiments on Llama-2-7B-chat show show that parameter-based methods struggle\nwith cross-lingual generalization, while instruction-tuned methods perform more\nrobustly. We extend Learning-To-Edit (LTE) to a multilingual setting and show\nthat joint Arabic-English training improves both editability and transfer. We\nrelease Arabic KE benchmarks and multilingual training for LTE data to support\nfuture research."}
{"id": "2406.16173", "pdf": "https://arxiv.org/pdf/2406.16173.pdf", "abs": "https://arxiv.org/abs/2406.16173", "title": "Crepe: A Mobile Screen Data Collector Using Graph Query", "authors": ["Yuwen Lu", "Meng Chen", "Qi Zhao", "Victor Cox", "Yang Yang", "Meng Jiang", "Jay Brockman", "Tamara Kay", "Toby Jia-Jun Li"], "categories": ["cs.HC"], "comment": null, "summary": "Collecting mobile datasets remains challenging for academic researchers due\nto limited data access and technical barriers. Commercial organizations often\npossess exclusive access to mobile data, leading to a \"data monopoly\" that\nrestricts the independence of academic research. Existing open-source mobile\ndata collection frameworks primarily focus on mobile sensing data rather than\nscreen content, which is crucial for various research studies. We present\nCrepe, a no-code Android app that enables researchers to collect information\ndisplayed on screen through simple demonstrations of target data. Crepe\nutilizes a novel Graph Query technique which augments the structures of mobile\nUI screens to support flexible identification, location, and collection of\nspecific data pieces. The tool emphasizes participants' privacy and agency by\nproviding full transparency over collected data and allowing easy opt-out. We\ndesigned and built Crepe for research purposes only and in scenarios where\nresearchers obtain explicit consent from participants. Code for Crepe will be\nopen-sourced to support future academic research data collection."}
{"id": "2507.09638", "pdf": "https://arxiv.org/pdf/2507.09638.pdf", "abs": "https://arxiv.org/abs/2507.09638", "title": "Can Group Relative Policy Optimization Improve Thai Legal Reasoning and Question Answering?", "authors": ["Pawitsapak Akarajaradwong", "Chompakorn Chaksangchaichot", "Pirat Pothavorn", "Attapol Thamrongrattanarit-Rutherford", "Ekapol Chuangsuwanich", "Sarana Nutanong"], "categories": ["cs.CL"], "comment": null, "summary": "The Retrieval-Augmented Generation (RAG) systems' performance on Thai legal\nquestion answering is still limited, especially for questions requiring\nextensive, complex legal reasoning. To address these limitations, we introduce\nan approach aligning LLMs toward improved law citation accuracy and better\nresponse quality using Group-Relative Policy Optimization (GRPO). Our approach\nleverages BGE-M3 embeddings as a cost-efficient semantic-similarity reward,\nsignificantly reducing computational expenses up to 2.5x compared to large\nlanguage model judges. Experiments on the NitiBench benchmark demonstrate\nsubstantial improvements: GRPO achieves up to 90% citation-F1 gains from the\nbase model and a 31% increase in joint quality metrics over instruction tuning.\nCrucially, our method shows enhanced robustness on complex legal reasoning\ntasks compared to instruction tuning, providing an effective and\nresource-efficient solution for enhancing Thai legal LLMs."}
{"id": "2503.16465", "pdf": "https://arxiv.org/pdf/2503.16465.pdf", "abs": "https://arxiv.org/abs/2503.16465", "title": "OS-Kairos: Adaptive Interaction for MLLM-Powered GUI Agents", "authors": ["Pengzhou Cheng", "Zheng Wu", "Zongru Wu", "Aston Zhang", "Zhuosheng Zhang", "Gongshen Liu"], "categories": ["cs.HC", "cs.AI"], "comment": "25 pages, 24 figures, 11 tables (ACL 2025, Findings)", "summary": "Autonomous graphical user interface (GUI) agents powered by multimodal large\nlanguage models have shown great promise. However, a critical yet underexplored\nissue persists: over-execution, where the agent executes tasks in a fully\nautonomous way, without adequate assessment of its action confidence to\ncompromise an adaptive human-agent collaboration. This poses substantial risks\nin complex scenarios, such as those involving ambiguous user instructions,\nunexpected interruptions, and environmental hijacks. To address the issue, we\nintroduce OS-Kairos, an adaptive GUI agent capable of predicting confidence\nlevels at each interaction step and efficiently deciding whether to act\nautonomously or seek human intervention. OS-Kairos is developed through two key\nmechanisms: (i) collaborative probing that annotates confidence scores at each\ninteraction step; (ii) confidence-driven interaction that leverages these\nconfidence scores to elicit the ability of adaptive interaction. Experimental\nresults show that OS-Kairos substantially outperforms existing models on our\ncurated dataset featuring complex scenarios, as well as on established\nbenchmarks such as AITZ and Meta-GUI, with 24.59\\%$\\sim$87.29\\% improvements in\ntask success rate. OS-Kairos facilitates an adaptive human-agent collaboration,\nprioritizing effectiveness, generality, scalability, and efficiency for\nreal-world GUI interaction. The dataset and codes are available at\nhttps://github.com/Wuzheng02/OS-Kairos."}
{"id": "2507.09701", "pdf": "https://arxiv.org/pdf/2507.09701.pdf", "abs": "https://arxiv.org/abs/2507.09701", "title": "MCEval: A Dynamic Framework for Fair Multilingual Cultural Evaluation of LLMs", "authors": ["Shulin Huang", "Linyi Yang", "Yue Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models exhibit cultural biases and limited cross-cultural\nunderstanding capabilities, particularly when serving diverse global user\npopulations. We propose MCEval, a novel multilingual evaluation framework that\nemploys dynamic cultural question construction and enables causal analysis\nthrough Counterfactual Rephrasing and Confounder Rephrasing. Our comprehensive\nevaluation spans 13 cultures and 13 languages, systematically assessing both\ncultural awareness and cultural bias across different linguistic scenarios. The\nframework provides 39,897 cultural awareness instances and 17,940 cultural bias\ninstances. Experimental results reveal performance disparities across different\nlinguistic scenarios, demonstrating that optimal cultural performance is not\nonly linked to training data distribution, but also is related to\nlanguage-culture alignment. The evaluation results also expose the fairness\nissue, where approaches appearing successful in the English scenario create\nsubstantial disadvantages. MCEval represents the first comprehensive\nmultilingual cultural evaluation framework that provides deeper insights into\nLLMs' cultural understanding."}
{"id": "2506.22941", "pdf": "https://arxiv.org/pdf/2506.22941.pdf", "abs": "https://arxiv.org/abs/2506.22941", "title": "Positioning AI Tools to Support Online Harm Reduction Practice: Applications and Design Directions", "authors": ["Kaixuan Wang", "Jason T. Jacques", "Chenxin Diao", "Carl-Cyril J Dreue"], "categories": ["cs.HC", "cs.AI"], "comment": "16 pages, 4 figures, with appendix", "summary": "Access to accurate and actionable harm reduction information can directly\nimpact the health outcomes of People Who Use Drugs (PWUD), yet existing online\nchannels often fail to meet their diverse and dynamic needs due to limitations\nin adaptability, accessibility, and the pervasive impact of stigma. Large\nLanguage Models (LLMs) present a novel opportunity to enhance information\nprovision, but their application in such a high-stakes domain is under-explored\nand presents socio-technical challenges. This paper investigates how LLMs can\nbe responsibly designed to support the information needs of PWUD. Through a\nqualitative workshop involving diverse stakeholder groups (academics, harm\nreduction practitioners, and an online community moderator), we explored LLM\ncapabilities, identified potential use cases, and delineated core design\nconsiderations. Our findings reveal that while LLMs can address some existing\ninformation barriers (e.g., by offering responsive, multilingual, and\npotentially less stigmatising interactions), their effectiveness is contingent\nupon overcoming challenges related to ethical alignment with harm reduction\nprinciples, nuanced contextual understanding, effective communication, and\nclearly defined operational boundaries. We articulate design pathways\nemphasising collaborative co-design with experts and PWUD to develop LLM\nsystems that are helpful, safe, and responsibly governed. This work contributes\nempirically grounded insights and actionable design considerations for the\nresponsible development of LLMs as supportive tools within the harm reduction\necosystem."}
{"id": "2507.09709", "pdf": "https://arxiv.org/pdf/2507.09709.pdf", "abs": "https://arxiv.org/abs/2507.09709", "title": "Large Language Models Encode Semantics in Low-Dimensional Linear Subspaces", "authors": ["Baturay Saglam", "Paul Kassianik", "Blaine Nelson", "Sajana Weerawardhena", "Yaron Singer", "Amin Karbasi"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Understanding the latent space geometry of large language models (LLMs) is\nkey to interpreting their behavior and improving alignment. \\baturay{However,\nit remains unclear to what extent LLMs internally organize representations\nrelated to semantic understanding. To investigate this, we conduct a\nlarge-scale empirical study of hidden states in transformer-based LLMs,\nanalyzing 11 decoder-only models across 6 scientific topics and 12 layers each.\nWe find that high-level semantic information consistently lies in\nlow-dimensional subspaces that form linearly separable representations across\ndistinct domains. This separability becomes more pronounced in deeper layers\nand under prompts that trigger structured reasoning or alignment\nbehaviors$\\unicode{x2013}$even when surface content is unchanged. This geometry\nenables simple yet effective causal interventions in hidden space; for example,\nreasoning patterns like chain-of-thought can be captured by a single vector\ndirection. Together, these findings support the development of geometry-aware\ntools that operate directly on latent representations to detect and mitigate\nharmful or adversarial content, using methods such as transport-based defenses\nthat leverage this separability. As a proof of concept, we demonstrate this\npotential by training a simple MLP classifier as a lightweight latent-space\nguardrail, which detects adversarial and malicious prompts with high precision."}
{"id": "2507.03147", "pdf": "https://arxiv.org/pdf/2507.03147.pdf", "abs": "https://arxiv.org/abs/2507.03147", "title": "DeepGesture: A conversational gesture synthesis system based on emotions and semantics", "authors": ["Thanh Hoang-Minh"], "categories": ["cs.HC", "cs.CL", "cs.LG", "cs.SD", "eess.AS"], "comment": "Project page: https://deepgesture.github.io", "summary": "Along with the explosion of large language models, improvements in speech\nsynthesis, advancements in hardware, and the evolution of computer graphics,\nthe current bottleneck in creating digital humans lies in generating character\nmovements that correspond naturally to text or speech inputs.\n  In this work, we present DeepGesture, a diffusion-based gesture synthesis\nframework for generating expressive co-speech gestures conditioned on\nmultimodal signals - text, speech, emotion, and seed motion. Built upon the\nDiffuseStyleGesture model, DeepGesture introduces novel architectural\nenhancements that improve semantic alignment and emotional expressiveness in\ngenerated gestures. Specifically, we integrate fast text transcriptions as\nsemantic conditioning and implement emotion-guided classifier-free diffusion to\nsupport controllable gesture generation across affective states. To visualize\nresults, we implement a full rendering pipeline in Unity based on BVH output\nfrom the model. Evaluation on the ZeroEGGS dataset shows that DeepGesture\nproduces gestures with improved human-likeness and contextual appropriateness.\nOur system supports interpolation between emotional states and demonstrates\ngeneralization to out-of-distribution speech, including synthetic voices -\nmarking a step forward toward fully multimodal, emotionally aware digital\nhumans.\n  Project page: https://deepgesture.github.io"}
{"id": "2507.09758", "pdf": "https://arxiv.org/pdf/2507.09758.pdf", "abs": "https://arxiv.org/abs/2507.09758", "title": "Your Pretrained Model Tells the Difficulty Itself: A Self-Adaptive Curriculum Learning Paradigm for Natural Language Understanding", "authors": ["Qi Feng", "Yihong Liu", "Hinrich SchÃ¼tze"], "categories": ["cs.CL", "cs.LG", "I.2.7; I.2.6"], "comment": "18 pages, 23 figures. To appear in ACL 2025 Student Research Workshop\n  (SRW)", "summary": "Curriculum learning is a widely adopted training strategy in natural language\nprocessing (NLP), where models are exposed to examples organized by increasing\ndifficulty to enhance learning efficiency and performance. However, most\nexisting approaches rely on manually defined difficulty metrics -- such as text\nlength -- which may not accurately reflect the model's own perspective. To\novercome this limitation, we present a self-adaptive curriculum learning\nparadigm that prioritizes fine-tuning examples based on difficulty scores\npredicted by pre-trained language models (PLMs) themselves. Building on these\nscores, we explore various training strategies that differ in the ordering of\nexamples for the fine-tuning: from easy-to-hard, hard-to-easy, to mixed\nsampling. We evaluate our method on four natural language understanding (NLU)\ndatasets covering both binary and multi-class classification tasks.\nExperimental results show that our approach leads to faster convergence and\nimproved performance compared to standard random sampling."}
{"id": "2507.04278", "pdf": "https://arxiv.org/pdf/2507.04278.pdf", "abs": "https://arxiv.org/abs/2507.04278", "title": "EMER-Ranker: Learning to Rank Emotion Descriptions in the Absence of Ground Truth", "authors": ["Zheng Lian", "Licai Sun", "Haoyu Chen", "Zebang Cheng", "Fan Zhang", "Ziyu Jia", "Ziyang Ma", "Fei Ma", "Xiaojiang Peng", "Jianhua Tao"], "categories": ["cs.HC"], "comment": null, "summary": "With the recent success of large language models, Explainable Multimodal\nEmotion Recognition (EMER), also known as Descriptive MER (DMER), has attracted\ngrowing attention from researchers. Unlike traditional discriminative methods\nthat rely on predefined emotion taxonomies, EMER aims to describe a person's\nemotional state using free-form natural language, thereby enabling fine-grained\nand interpretable emotion representations. However, this free-form prediction\nparadigm introduces significant challenges in evaluation. Existing approaches\neither depend on ground-truth descriptions, which require extensive manual\nannotations and often fail to capture the full complexity of human emotions, or\nsimplify the evaluation task by shifting focus from assessing descriptions to\nevaluating emotion labels. However, this simplification overlooks critical\naspects such as emotional temporal dynamics, intensity, and uncertainty. To\naddress these limitations, we propose EMER-Ranker, a novel evaluation strategy\nthat reformulates the traditional ``prediction-ground truth'' comparison into\nthe ``prediction-prediction'' comparison, eliminating the need for ground-truth\ndescriptions. We then apply the Bradley-Terry algorithm to convert pairwise\ncomparison outcomes into model-level rankings. Additionally, we explore the\npotential for automatic preference prediction and introduce EMER-Preference,\nthe first preference dataset specifically designed for human emotions. Our work\nadvances the field of EMER and lays the foundation for more intelligent\nhuman-computer interaction systems."}
{"id": "2507.09777", "pdf": "https://arxiv.org/pdf/2507.09777.pdf", "abs": "https://arxiv.org/abs/2507.09777", "title": "Te AhorrÃ© Un Click: A Revised Definition of Clickbait and Detection in Spanish News", "authors": ["Gabriel Mordecki", "Guillermo Moncecchi", "Javier Couto"], "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "We revise the definition of clickbait, which lacks current consensus, and\nargue that the creation of a curiosity gap is the key concept that\ndistinguishes clickbait from other related phenomena such as sensationalism and\nheadlines that do not deliver what they promise or diverge from the article.\nTherefore, we propose a new definition: clickbait is a technique for generating\nheadlines and teasers that deliberately omit part of the information with the\ngoal of raising the readers' curiosity, capturing their attention and enticing\nthem to click. We introduce a new approach to clickbait detection datasets\ncreation, by refining the concept limits and annotations criteria, minimizing\nthe subjectivity in the decision as much as possible. Following it, we created\nand release TA1C (for Te Ahorr\\'e Un Click, Spanish for Saved You A Click), the\nfirst open source dataset for clickbait detection in Spanish. It consists of\n3,500 tweets coming from 18 well known media sources, manually annotated and\nreaching a 0.825 Fleiss' K inter annotator agreement. We implement strong\nbaselines that achieve 0.84 in F1-score."}
{"id": "2408.05700", "pdf": "https://arxiv.org/pdf/2408.05700.pdf", "abs": "https://arxiv.org/abs/2408.05700", "title": "Quantification of Interdependent Emotion Dynamics in Online Interactions", "authors": ["Yishan Luo", "Didier Sornette", "Sandro Claudio Lera"], "categories": ["cs.SI", "cs.HC", "stat.AP"], "comment": null, "summary": "A growing share of human interactions now occurs online, where the expression\nand perception of emotions are often amplified and distorted. Yet, the\ninterplay between different emotions and the extent to which they are driven by\nexternal stimuli or social feedback remains poorly understood. We calibrate a\nmultivariate Hawkes self-exciting point process to model the temporal\nexpression of six basic emotions in YouTube Live chats. This framework captures\nboth temporal and cross-emotional dependencies while allowing us to disentangle\nthe influence of video content (exogenous) from peer interactions (endogenous).\nWe find that emotional expressions are up to four times more strongly driven by\npeer interaction than by video content. Positivity is more contagious,\nspreading three times more readily, whereas negativity is more memorable,\nlingering nearly twice as long. Moreover, we observe asymmetric\ncross-excitation, with negative emotions frequently triggering positive ones, a\npattern consistent with trolling dynamics, but not the reverse. These findings\nhighlight the central role of social interaction in shaping emotional dynamics\nonline and the risks of emotional manipulation as human-chatbot interactions\nbecome increasingly realistic."}
{"id": "2507.09875", "pdf": "https://arxiv.org/pdf/2507.09875.pdf", "abs": "https://arxiv.org/abs/2507.09875", "title": "Function Induction and Task Generalization: An Interpretability Study with Off-by-One Addition", "authors": ["Qinyuan Ye", "Robin Jia", "Xiang Ren"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Code: https://github.com/INK-USC/function-induction", "summary": "Large language models demonstrate the intriguing ability to perform unseen\ntasks via in-context learning. However, it remains unclear what mechanisms\ninside the model drive such task-level generalization. In this work, we\napproach this question through the lens of off-by-one addition (i.e., 1+1=3,\n2+2=5, 3+3=?), a two-step, counterfactual task with an unexpected +1 function\nas a second step. Leveraging circuit-style interpretability techniques such as\npath patching, we analyze the models' internal computations behind their\nnotable performance and present three key findings. First, we uncover a\nfunction induction mechanism that explains the model's generalization from\nstandard addition to off-by-one addition. This mechanism resembles the\nstructure of the induction head mechanism found in prior work and elevates it\nto a higher level of abstraction. Second, we show that the induction of the +1\nfunction is governed by multiple attention heads in parallel, each of which\nemits a distinct piece of the +1 function. Finally, we find that this function\ninduction mechanism is reused in a broader range of tasks, including synthetic\ntasks such as shifted multiple-choice QA and algorithmic tasks such as base-8\naddition. Overall, our findings offer deeper insights into how reusable and\ncomposable structures within language models enable task-level generalization."}
{"id": "2409.13748", "pdf": "https://arxiv.org/pdf/2409.13748.pdf", "abs": "https://arxiv.org/abs/2409.13748", "title": "TheraGen: Therapy for Every Generation", "authors": ["Kartikey Doshi", "Jimit Shah", "Narendra Shekokar"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "This paper contains major errors in methodology and results. It\n  should not be cited", "summary": "We present TheraGen, an advanced AI-powered mental health chatbot utilizing\nthe LLaMA 2 7B model. This approach builds upon recent advancements in language\nmodels and transformer architectures. TheraGen provides all-day personalized,\ncompassionate mental health care by leveraging a large dataset of 1 million\nconversational entries, combining anonymized therapy transcripts, online mental\nhealth discussions, and psychological literature, including APA resources. Our\nimplementation employs transfer learning, fine-tuning, and advanced training\ntechniques to optimize performance. TheraGen offers a user-friendly interface\nfor seamless interaction, providing empathetic responses and evidence-based\ncoping strategies. Evaluation results demonstrate high user satisfaction rates,\nwith 94% of users reporting improved mental well-being. The system achieved a\nBLEU score of 0.67 and a ROUGE score of 0.62, indicating strong response\naccuracy. With an average response time of 1395 milliseconds, TheraGen ensures\nreal-time, efficient support. While not a replacement for professional therapy,\nTheraGen serves as a valuable complementary tool, significantly improving user\nwell-being and addressing the accessibility gap in mental health treatments.\nThis paper details TheraGen's architecture, training methodology, ethical\nconsiderations, and future directions, contributing to the growing field of\nAI-assisted mental healthcare and offering a scalable solution to the pressing\nneed for mental health support."}
{"id": "2507.09935", "pdf": "https://arxiv.org/pdf/2507.09935.pdf", "abs": "https://arxiv.org/abs/2507.09935", "title": "Enhancing Retrieval Augmented Generation with Hierarchical Text Segmentation Chunking", "authors": ["Hai Toan Nguyen", "Tien Dat Nguyen", "Viet Ha Nguyen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems commonly use chunking strategies\nfor retrieval, which enhance large language models (LLMs) by enabling them to\naccess external knowledge, ensuring that the retrieved information is\nup-to-date and domain-specific. However, traditional methods often fail to\ncreate chunks that capture sufficient semantic meaning, as they do not account\nfor the underlying textual structure. This paper proposes a novel framework\nthat enhances RAG by integrating hierarchical text segmentation and clustering\nto generate more meaningful and semantically coherent chunks. During inference,\nthe framework retrieves information by leveraging both segment-level and\ncluster-level vector representations, thereby increasing the likelihood of\nretrieving more precise and contextually relevant information. Evaluations on\nthe NarrativeQA, QuALITY, and QASPER datasets indicate that the proposed method\nachieved improved results compared to traditional chunking techniques."}
{"id": "2501.08736", "pdf": "https://arxiv.org/pdf/2501.08736.pdf", "abs": "https://arxiv.org/abs/2501.08736", "title": "Holoview: An Immersive Mixed-Reality Visualization System for Anatomical Education", "authors": ["Anshul Goswami", "Ojaswa Sharma"], "categories": ["cs.GR", "cs.HC"], "comment": null, "summary": "We present Holoview, an augmented reality (AR) system designed to support\nimmersive and interactive learning of human anatomy. Holoview enables users to\ndynamically explore volumetric anatomical data through intuitive hand gestures\nin a 3D AR environment, allowing inspection of individual organs and\ncross-sectional views via clipping and bioscope features. The system adopts a\nlightweight client-server architecture optimized for real-time performance on\nthe HoloLens through hybrid and foveated rendering. Our user study demonstrated\nHoloview's educational effectiveness, with participants showing a 135 percent\nimprovement in task-specific knowledge and reporting increased confidence in\nunderstanding anatomical structures. The system was perceived as engaging and\nintuitive, particularly for organ selection and cross-sectional exploration,\nwith low cognitive load and increasing ease of use over time. These findings\nhighlight Holoview's potential to enhance anatomy learning through immersive,\nuser-centered AR experiences."}
{"id": "2507.09973", "pdf": "https://arxiv.org/pdf/2507.09973.pdf", "abs": "https://arxiv.org/abs/2507.09973", "title": "Tiny Reward Models", "authors": ["Sarah Pan"], "categories": ["cs.CL", "cs.AI"], "comment": "2025 ICML Efficient Systems for Foundation Models Workshop", "summary": "Large decoder-based language models have become the dominant architecture for\nreward modeling in reinforcement learning from human feedback (RLHF). However,\nas reward models are increasingly deployed in test-time strategies, their\ninference costs become a growing concern. We present TinyRM, a family of small,\nbidirectional masked language models (MLMs) with as few as 400 million\nparameters, that rival the capabilities of models over 175 times larger on\nreasoning and safety preference modeling tasks. TinyRM combines FLAN-style\nprompting, Directional Low-Rank Adaptation (DoRA), and layer freezing to\nachieve strong performance on RewardBench, despite using significantly fewer\nresources. Our experiments suggest that small models benefit from\ndomain-specific tuning strategies, particularly in reasoning, where lightweight\nfinetuning methods are especially effective. While challenges remain in\nbuilding generalist models and conversational preference modeling, our\npreliminary results highlight the promise of lightweight bidirectional\narchitectures as efficient, scalable alternatives for preference modeling."}
{"id": "2503.09639", "pdf": "https://arxiv.org/pdf/2503.09639.pdf", "abs": "https://arxiv.org/abs/2503.09639", "title": "Can A Society of Generative Agents Simulate Human Behavior and Inform Public Health Policy? A Case Study on Vaccine Hesitancy", "authors": ["Abe Bohan Hou", "Hongru Du", "Yichen Wang", "Jingyu Zhang", "Zixiao Wang", "Paul Pu Liang", "Daniel Khashabi", "Lauren Gardner", "Tianxing He"], "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.CY", "cs.HC"], "comment": "Accepted to COLM 2025", "summary": "Can we simulate a sandbox society with generative agents to model human\nbehavior, thereby reducing the over-reliance on real human trials for assessing\npublic policies? In this work, we investigate the feasibility of simulating\nhealth-related decision-making, using vaccine hesitancy, defined as the delay\nin acceptance or refusal of vaccines despite the availability of vaccination\nservices (MacDonald, 2015), as a case study. To this end, we introduce the\nVacSim framework with 100 generative agents powered by Large Language Models\n(LLMs). VacSim simulates vaccine policy outcomes with the following steps: 1)\ninstantiate a population of agents with demographics based on census data; 2)\nconnect the agents via a social network and model vaccine attitudes as a\nfunction of social dynamics and disease-related information; 3) design and\nevaluate various public health interventions aimed at mitigating vaccine\nhesitancy. To align with real-world results, we also introduce simulation\nwarmup and attitude modulation to adjust agents' attitudes. We propose a series\nof evaluations to assess the reliability of various LLM simulations.\nExperiments indicate that models like Llama and Qwen can simulate aspects of\nhuman behavior but also highlight real-world alignment challenges, such as\ninconsistent responses with demographic profiles. This early exploration of\nLLM-driven simulations is not meant to serve as definitive policy guidance;\ninstead, it serves as a call for action to examine social simulation for policy\ndevelopment."}
{"id": "2507.09982", "pdf": "https://arxiv.org/pdf/2507.09982.pdf", "abs": "https://arxiv.org/abs/2507.09982", "title": "TextOmics-Guided Diffusion for Hit-like Molecular Generation", "authors": ["Hang Yuan", "Chen Li", "Wenjun Ma", "Yuncheng Jiang"], "categories": ["cs.CL"], "comment": null, "summary": "Hit-like molecular generation with therapeutic potential is essential for\ntarget-specific drug discovery. However, the field lacks heterogeneous data and\nunified frameworks for integrating diverse molecular representations. To bridge\nthis gap, we introduce TextOmics, a pioneering benchmark that establishes\none-to-one correspondences between omics expressions and molecular textual\ndescriptions. TextOmics provides a heterogeneous dataset that facilitates\nmolecular generation through representations alignment. Built upon this\nfoundation, we propose ToDi, a generative framework that jointly conditions on\nomics expressions and molecular textual descriptions to produce biologically\nrelevant, chemically valid, hit-like molecules. ToDi leverages two encoders\n(OmicsEn and TextEn) to capture multi-level biological and semantic\nassociations, and develops conditional diffusion (DiffGen) for controllable\ngeneration. Extensive experiments confirm the effectiveness of TextOmics and\ndemonstrate ToDi outperforms existing state-of-the-art approaches, while also\nshowcasing remarkable potential in zero-shot therapeutic molecular generation.\nSources are available at: https://github.com/hala-ToDi."}
{"id": "2504.17921", "pdf": "https://arxiv.org/pdf/2504.17921.pdf", "abs": "https://arxiv.org/abs/2504.17921", "title": "Avoiding Leakage Poisoning: Concept Interventions Under Distribution Shifts", "authors": ["Mateo Espinosa Zarlenga", "Gabriele Dominici", "Pietro Barbiero", "Zohreh Shams", "Mateja Jamnik"], "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.HC"], "comment": "Presented at the Forty-Second International Conference on Machine\n  Learning (ICML 2025)", "summary": "In this paper, we investigate how concept-based models (CMs) respond to\nout-of-distribution (OOD) inputs. CMs are interpretable neural architectures\nthat first predict a set of high-level concepts (e.g., stripes, black) and then\npredict a task label from those concepts. In particular, we study the impact of\nconcept interventions (i.e., operations where a human expert corrects a CM's\nmispredicted concepts at test time) on CMs' task predictions when inputs are\nOOD. Our analysis reveals a weakness in current state-of-the-art CMs, which we\nterm leakage poisoning, that prevents them from properly improving their\naccuracy when intervened on for OOD inputs. To address this, we introduce\nMixCEM, a new CM that learns to dynamically exploit leaked information missing\nfrom its concepts only when this information is in-distribution. Our results\nacross tasks with and without complete sets of concept annotations demonstrate\nthat MixCEMs outperform strong baselines by significantly improving their\naccuracy for both in-distribution and OOD samples in the presence and absence\nof concept interventions."}
{"id": "2507.10008", "pdf": "https://arxiv.org/pdf/2507.10008.pdf", "abs": "https://arxiv.org/abs/2507.10008", "title": "Protective Factor-Aware Dynamic Influence Learning for Suicide Risk Prediction on Social Media", "authors": ["Jun Li", "Xiangmeng Wang", "Haoyang Li", "Yifei Yan", "Hong Va Leong", "Ling Feng", "Nancy Xiaonan Yu", "Qing Li"], "categories": ["cs.CL"], "comment": null, "summary": "Suicide is a critical global health issue that requires urgent attention.\nEven though prior work has revealed valuable insights into detecting current\nsuicide risk on social media, little attention has been paid to developing\nmodels that can predict subsequent suicide risk over time, limiting their\nability to capture rapid fluctuations in individuals' mental state transitions.\nIn addition, existing work ignores protective factors that play a crucial role\nin suicide risk prediction, focusing predominantly on risk factors alone.\nProtective factors such as social support and coping strategies can mitigate\nsuicide risk by moderating the impact of risk factors. Therefore, this study\nproposes a novel framework for predicting subsequent suicide risk by jointly\nlearning the dynamic influence of both risk factors and protective factors on\nusers' suicide risk transitions. We propose a novel Protective Factor-Aware\nDataset, which is built from 12 years of Reddit posts along with comprehensive\nannotations of suicide risk and both risk and protective factors. We also\nintroduce a Dynamic Factors Influence Learning approach that captures the\nvarying impact of risk and protective factors on suicide risk transitions,\nrecognizing that suicide risk fluctuates over time according to established\npsychological theories. Our thorough experiments demonstrate that the proposed\nmodel significantly outperforms state-of-the-art models and large language\nmodels across three datasets. In addition, the proposed Dynamic Factors\nInfluence Learning provides interpretable weights, helping clinicians better\nunderstand suicidal patterns and enabling more targeted intervention\nstrategies."}
{"id": "2505.08245", "pdf": "https://arxiv.org/pdf/2505.08245.pdf", "abs": "https://arxiv.org/abs/2505.08245", "title": "Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement", "authors": ["Haoran Ye", "Jing Jin", "Yuhang Xie", "Xin Zhang", "Guojie Song"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "474 references", "summary": "The advancement of large language models (LLMs) has outpaced traditional\nevaluation methodologies. This progress presents novel challenges, such as\nmeasuring human-like psychological constructs, moving beyond static and\ntask-specific benchmarks, and establishing human-centered evaluation. These\nchallenges intersect with psychometrics, the science of quantifying the\nintangible aspects of human psychology, such as personality, values, and\nintelligence. This review paper introduces and synthesizes the emerging\ninterdisciplinary field of LLM Psychometrics, which leverages psychometric\ninstruments, theories, and principles to evaluate, understand, and enhance\nLLMs. The reviewed literature systematically shapes benchmarking principles,\nbroadens evaluation scopes, refines methodologies, validates results, and\nadvances LLM capabilities. Diverse perspectives are integrated to provide a\nstructured framework for researchers across disciplines, enabling a more\ncomprehensive understanding of this nascent field. Ultimately, the review\nprovides actionable insights for developing future evaluation paradigms that\nalign with human-level AI and promote the advancement of human-centered AI\nsystems for societal benefit. A curated repository of LLM psychometric\nresources is available at\nhttps://github.com/valuebyte-ai/Awesome-LLM-Psychometrics."}
{"id": "2507.10059", "pdf": "https://arxiv.org/pdf/2507.10059.pdf", "abs": "https://arxiv.org/abs/2507.10059", "title": "GeLaCo: An Evolutionary Approach to Layer Compression", "authors": ["David Ponce", "Thierry Etchegoyhen", "Javier Del Ser"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLM) have achieved remarkable performance across a\nlarge number of tasks, but face critical deployment and usage barriers due to\nsubstantial computational requirements. Model compression methods, which aim to\nreduce model size while preserving its capacity, are an important means to\nmitigate these issues. Promising approaches along these lines, such as\nstructured pruning, typically require costly empirical search for optimal\nvariants and may run the risk of ignoring better solutions. In this work we\nintroduce GeLaCo, an evolutionary approach to LLM compression via layer\ncollapse. Our approach supports an efficient exploration of the compression\nsolution space via population-based search and a module-wise similarity fitness\nfunction capturing attention, feed-forward, and hidden state representations.\nGeLaCo also supports both single and multi-objective evolutionary compression\nsearch, establishing the first Pareto frontier along compression and quality\naxes. We evaluate GeLaCo solutions via both perplexity-based and generative\nevaluations over foundational and instruction-tuned models, outperforming\nstate-of-the-art alternatives."}
{"id": "2507.04189", "pdf": "https://arxiv.org/pdf/2507.04189.pdf", "abs": "https://arxiv.org/abs/2507.04189", "title": "SymbolicThought: Integrating Language Models and Symbolic Reasoning for Consistent and Interpretable Human Relationship Understanding", "authors": ["Runcong Zhao", "Qinglin Zhu", "Hainiu Xu", "Bin Liang", "Lin Gui", "Yulan He"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Understanding character relationships is essential for interpreting complex\nnarratives and conducting socially grounded AI research. However, manual\nannotation is time-consuming and low in coverage, while large language models\n(LLMs) often produce hallucinated or logically inconsistent outputs. We present\nSymbolicThought, a human-in-the-loop framework that combines LLM-based\nextraction with symbolic reasoning. The system constructs editable character\nrelationship graphs, refines them using seven types of logical constraints, and\nenables real-time validation and conflict resolution through an interactive\ninterface. To support logical supervision and explainable social analysis, we\nrelease a dataset of 160 interpersonal relationships with corresponding logical\nstructures. Experiments show that SymbolicThought improves annotation accuracy\nand consistency while significantly reducing time cost, offering a practical\ntool for narrative understanding, explainable AI, and LLM evaluation."}
{"id": "2507.10073", "pdf": "https://arxiv.org/pdf/2507.10073.pdf", "abs": "https://arxiv.org/abs/2507.10073", "title": "Cultural Bias in Large Language Models: Evaluating AI Agents through Moral Questionnaires", "authors": ["Simon MÃ¼nker"], "categories": ["cs.CL", "cs.AI"], "comment": "15pages, 1 figure, 2 tables", "summary": "Are AI systems truly representing human values, or merely averaging across\nthem? Our study suggests a concerning reality: Large Language Models (LLMs)\nfail to represent diverse cultural moral frameworks despite their linguistic\ncapabilities. We expose significant gaps between AI-generated and human moral\nintuitions by applying the Moral Foundations Questionnaire across 19 cultural\ncontexts. Comparing multiple state-of-the-art LLMs' origins against human\nbaseline data, we find these models systematically homogenize moral diversity.\nSurprisingly, increased model size doesn't consistently improve cultural\nrepresentation fidelity. Our findings challenge the growing use of LLMs as\nsynthetic populations in social science research and highlight a fundamental\nlimitation in current AI alignment approaches. Without data-driven alignment\nbeyond prompting, these systems cannot capture the nuanced, culturally-specific\nmoral intuitions. Our results call for more grounded alignment objectives and\nevaluation metrics to ensure AI systems represent diverse human values rather\nthan flattening the moral landscape."}
{"id": "2507.04295", "pdf": "https://arxiv.org/pdf/2507.04295.pdf", "abs": "https://arxiv.org/abs/2507.04295", "title": "LearnLens: LLM-Enabled Personalised, Curriculum-Grounded Feedback with Educators in the Loop", "authors": ["Runcong Zhao", "Artem Bobrov", "Jiazheng Li", "Yulan He"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Effective feedback is essential for student learning but is time-intensive\nfor teachers. We present LearnLens, a modular, LLM-based system that generates\npersonalised, curriculum-aligned feedback in science education. LearnLens\ncomprises three components: (1) an error-aware assessment module that captures\nnuanced reasoning errors; (2) a curriculum-grounded generation module that uses\na structured, topic-linked memory chain rather than traditional\nsimilarity-based retrieval, improving relevance and reducing noise; and (3) an\neducator-in-the-loop interface for customisation and oversight. LearnLens\naddresses key challenges in existing systems, offering scalable, high-quality\nfeedback that empowers both teachers and students."}
{"id": "2507.10085", "pdf": "https://arxiv.org/pdf/2507.10085.pdf", "abs": "https://arxiv.org/abs/2507.10085", "title": "Enhancing Chain-of-Thought Reasoning with Critical Representation Fine-tuning", "authors": ["Chenxi Huang", "Shaotian Yan", "Liang Xie", "Binbin Lin", "Sinan Fan", "Yue Xin", "Deng Cai", "Chen Shen", "Jieping Ye"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025", "summary": "Representation Fine-tuning (ReFT), a recently proposed Parameter-Efficient\nFine-Tuning (PEFT) method, has attracted widespread attention for significantly\nimproving parameter efficiency by editing representation space alone. In this\nwork, we investigate applying ReFT to complex reasoning tasks. However,\ndirectly using the native ReFT method, which modifies fixed representations at\nthe beginning and end of each layer, yields suboptimal performance, as these\nfixed-position representations have uncertain impact on the outputs. We observe\nthat, in complex reasoning tasks, there often exist certain critical\nrepresentations. These representations either integrate significant information\nfrom preceding layers or regulate subsequent layer representations. Through\nlayer-by-layer propagation, they exert a substantial influence on the final\noutput. Naturally, fine-tuning these critical representations has the potential\nto greatly enhance reasoning performance. Building upon these insights, we\npropose Critical Representation Fine-Tuning (CRFT), a novel method that\nidentifies and optimizes these critical representations through information\nflow analysis. CRFT operates within a supervised learning framework,\ndynamically optimizing critical representations in a low-rank linear subspace\nwhile freezing the base model. The effectiveness and efficiency of our method\nare validated across eight benchmarks for arithmetic and commonsense reasoning,\nusing LLaMA and Mistral model families. Furthermore, our method also adapts\neffectively to few-shot settings, boosting one-shot accuracy by 16.4%. Our work\nhighlights the untapped potential of representation-level optimization for CoT\nreasoning, offering a lightweight yet powerful alternative to traditional PEFT\nmethods."}
{"id": "2507.07216", "pdf": "https://arxiv.org/pdf/2507.07216.pdf", "abs": "https://arxiv.org/abs/2507.07216", "title": "Bias-Aware Mislabeling Detection via Decoupled Confident Learning", "authors": ["Yunyi Li", "Maria De-Arteaga", "Maytal Saar-Tsechansky"], "categories": ["cs.LG", "cs.AI", "cs.DB", "cs.HC"], "comment": null, "summary": "Reliable data is a cornerstone of modern organizational systems. A notable\ndata integrity challenge stems from label bias, which refers to systematic\nerrors in a label, a covariate that is central to a quantitative analysis, such\nthat its quality differs across social groups. This type of bias has been\nconceptually and empirically explored and is widely recognized as a pressing\nissue across critical domains. However, effective methodologies for addressing\nit remain scarce. In this work, we propose Decoupled Confident Learning\n(DeCoLe), a principled machine learning based framework specifically designed\nto detect mislabeled instances in datasets affected by label bias, enabling\nbias aware mislabelling detection and facilitating data quality improvement. We\ntheoretically justify the effectiveness of DeCoLe and evaluate its performance\nin the impactful context of hate speech detection, a domain where label bias is\na well documented challenge. Empirical results demonstrate that DeCoLe excels\nat bias aware mislabeling detection, consistently outperforming alternative\napproaches for label error detection. Our work identifies and addresses the\nchallenge of bias aware mislabeling detection and offers guidance on how DeCoLe\ncan be integrated into organizational data management practices as a powerful\ntool to enhance data reliability."}
{"id": "2507.10098", "pdf": "https://arxiv.org/pdf/2507.10098.pdf", "abs": "https://arxiv.org/abs/2507.10098", "title": "Fusing Large Language Models with Temporal Transformers for Time Series Forecasting", "authors": ["Chen Su", "Yuanhe Tian", "Qinyu Liu", "Jun Zhang", "Yan Song"], "categories": ["cs.CL"], "comment": null, "summary": "Recently, large language models (LLMs) have demonstrated powerful\ncapabilities in performing various tasks and thus are applied by recent studies\nto time series forecasting (TSF) tasks, which predict future values with the\ngiven historical time series. Existing LLM-based approaches transfer knowledge\nlearned from text data to time series prediction using prompting or fine-tuning\nstrategies. However, LLMs are proficient at reasoning over discrete tokens and\nsemantic patterns but are not initially designed to model continuous numerical\ntime series data. The gaps between text and time series data lead LLMs to\nachieve inferior performance to a vanilla Transformer model that is directly\ntrained on TSF data. However, the vanilla Transformers often struggle to learn\nhigh-level semantic patterns. In this paper, we design a novel\nTransformer-based architecture that complementarily leverages LLMs and vanilla\nTransformers, so as to integrate the high-level semantic representations\nlearned by LLMs into the temporal information encoded by time series\nTransformers, where a hybrid representation is obtained by fusing the\nrepresentations from the LLM and the Transformer. The resulting fused\nrepresentation contains both historical temporal dynamics and semantic\nvariation patterns, allowing our model to predict more accurate future values.\nExperiments on benchmark datasets demonstrate the effectiveness of the proposed\napproach."}
{"id": "2507.07610", "pdf": "https://arxiv.org/pdf/2507.07610.pdf", "abs": "https://arxiv.org/abs/2507.07610", "title": "SpatialViz-Bench: Automatically Generated Spatial Visualization Reasoning Tasks for MLLMs", "authors": ["Siting Wang", "Luoyang Sun", "Cheng Deng", "Kun Shao", "Minnan Pei", "Zheng Tian", "Haifeng Zhang", "Jun Wang"], "categories": ["cs.CV", "cs.CL", "cs.HC"], "comment": null, "summary": "Humans can directly imagine and manipulate visual images in their minds, a\ncapability known as spatial visualization. While multi-modal Large Language\nModels (MLLMs) support imagination-based reasoning, spatial visualization\nremains insufficiently evaluated, typically embedded within broader\nmathematical and logical assessments. Existing evaluations often rely on IQ\ntests or math competitions that may overlap with training data, compromising\nassessment reliability. To this end, we introduce SpatialViz-Bench, a\ncomprehensive multi-modal benchmark for spatial visualization with 12 tasks\nacross 4 sub-abilities, comprising 1,180 automatically generated problems. Our\nevaluation of 33 state-of-the-art MLLMs not only reveals wide performance\nvariations and demonstrates the benchmark's strong discriminative power, but\nalso uncovers counter-intuitive findings: models exhibit unexpected behaviors\nby showing difficulty perception that misaligns with human intuition,\ndisplaying dramatic 2D-to-3D performance cliffs, and defaulting to formula\nderivation despite spatial tasks requiring visualization alone. SpatialVizBench\nempirically demonstrates that state-of-the-art MLLMs continue to exhibit\ndeficiencies in spatial visualization tasks, thereby addressing a significant\nlacuna in the field. The benchmark is publicly available."}
{"id": "2507.10155", "pdf": "https://arxiv.org/pdf/2507.10155.pdf", "abs": "https://arxiv.org/abs/2507.10155", "title": "Task-Based Flexible Feature Distillation for LLMs", "authors": ["Khouloud Saadi", "Di Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Knowledge Distillation (KD) in general and feature distillation in particular\nare promising techniques for reducing the high computational demand of large\nlanguage models (LLMs). However, traditional feature KD methods typically\nassume that the teacher and the student share the same hidden size, limiting\nthe flexibility of the student's architecture. A common solution to this\nproblem involves training a linear projector to align their feature spaces, but\nthis introduces additional parameters that must be learned from scratch and\noften degrades performance on downstream tasks, especially in generative\nsettings. To address this issue, in this work, we propose a novel task-based\nfeature distillation method that enables knowledge transfer between teacher and\nstudent models with different hidden layer dimensions, without introducing any\nnew parameters. Leveraging the insight that only a subset of LLM components\ncontribute significantly to a specific downstream task, our approach identifies\nthe most task-relevant hidden units in the teacher and directly distills their\nactivations to the student. Our method is flexible and easily integrates with\nother distillation frameworks. Empirical results show consistent improvements\nover prior approaches across diverse tasks, including classification,\ninstruction-following, and summarization, achieving up to a 3\\% performance\ngain over the linear projection baseline."}
{"id": "2507.10177", "pdf": "https://arxiv.org/pdf/2507.10177.pdf", "abs": "https://arxiv.org/abs/2507.10177", "title": "Abusive text transformation using LLMs", "authors": ["Rohitash Chandra", "Jiyong Choi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Although Large Language Models (LLMs) have demonstrated significant\nadvancements in natural language processing tasks, their effectiveness in the\nclassification and transformation of abusive text into non-abusive versions\nremains an area for exploration. In this study, we aim to use LLMs to transform\nabusive text (tweets and reviews) featuring hate speech and swear words into\nnon-abusive text, while retaining the intent of the text. We evaluate the\nperformance of two state-of-the-art LLMs, such as Gemini, GPT-4o, DeekSeek and\nGroq, on their ability to identify abusive text. We them to transform and\nobtain a text that is clean from abusive and inappropriate content but\nmaintains a similar level of sentiment and semantics, i.e. the transformed text\nneeds to maintain its message. Afterwards, we evaluate the raw and transformed\ndatasets with sentiment analysis and semantic analysis. Our results show Groq\nprovides vastly different results when compared with other LLMs. We have\nidentified similarities between GPT-4o and DeepSeek-V3."}
{"id": "2507.10216", "pdf": "https://arxiv.org/pdf/2507.10216.pdf", "abs": "https://arxiv.org/abs/2507.10216", "title": "Absher: A Benchmark for Evaluating Large Language Models Understanding of Saudi Dialects", "authors": ["Renad Al-Monef", "Hassan Alhuzali", "Nora Alturayeif", "Ashwag Alasmari"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As large language models (LLMs) become increasingly central to Arabic NLP\napplications, evaluating their understanding of regional dialects and cultural\nnuances is essential, particularly in linguistically diverse settings like\nSaudi Arabia. This paper introduces \\texttt{Absher}, a comprehensive benchmark\nspecifically designed to assess LLMs performance across major Saudi dialects.\n\\texttt{Absher} comprises over 18,000 multiple-choice questions spanning six\ndistinct categories: Meaning, True/False, Fill-in-the-Blank, Contextual Usage,\nCultural Interpretation, and Location Recognition. These questions are derived\nfrom a curated dataset of dialectal words, phrases, and proverbs sourced from\nvarious regions of Saudi Arabia. We evaluate several state-of-the-art LLMs,\nincluding multilingual and Arabic-specific models. We also provide detailed\ninsights into their capabilities and limitations. Our results reveal notable\nperformance gaps, particularly in tasks requiring cultural inference or\ncontextual understanding. Our findings highlight the urgent need for\ndialect-aware training and culturally aligned evaluation methodologies to\nimprove LLMs performance in real-world Arabic applications."}
{"id": "2507.10326", "pdf": "https://arxiv.org/pdf/2507.10326.pdf", "abs": "https://arxiv.org/abs/2507.10326", "title": "Grammar-Guided Evolutionary Search for Discrete Prompt Optimisation", "authors": ["Muzhaffar Hazman", "Minh-Khoi Pham", "Shweta Soundararajan", "Goncalo Mordido", "Leonardo Custode", "David Lynch", "Giorgio Cruciata", "Yucheng Shi", "Hongmeng Song", "Wang Chao", "Pan Yue", "Aleksandar Milenovic", "Alexandros Agapitos"], "categories": ["cs.CL"], "comment": "Accepted for Publication at ECAI 2025", "summary": "Prompt engineering has proven to be a crucial step in leveraging pretrained\nlarge language models (LLMs) in solving various real-world tasks. Numerous\nsolutions have been proposed that seek to automate prompt engineering by using\nthe model itself to edit prompts. However, the majority of state-of-the-art\napproaches are evaluated on tasks that require minimal prompt templates and on\nvery large and highly capable LLMs. In contrast, solving complex tasks that\nrequire detailed information to be included in the prompt increases the amount\nof text that needs to be optimised. Furthermore, smaller models have been shown\nto be more sensitive to prompt design. To address these challenges, we propose\nan evolutionary search approach to automated discrete prompt optimisation\nconsisting of two phases. In the first phase, grammar-guided genetic\nprogramming is invoked to synthesise prompt-creating programmes by searching\nthe space of programmes populated by function compositions of syntactic,\ndictionary-based and LLM-based prompt-editing functions. In the second phase,\nlocal search is applied to explore the neighbourhoods of best-performing\nprogrammes in an attempt to further fine-tune their performance. Our approach\noutperforms three state-of-the-art prompt optimisation approaches,\nPromptWizard, OPRO, and RL-Prompt, on three relatively small general-purpose\nLLMs in four domain-specific challenging tasks. We also illustrate several\nexamples where these benchmark methods suffer relatively severe performance\ndegradation, while our approach improves performance in almost all task-model\ncombinations, only incurring minimal degradation when it does not."}
{"id": "2507.10330", "pdf": "https://arxiv.org/pdf/2507.10330.pdf", "abs": "https://arxiv.org/abs/2507.10330", "title": "Bridging Robustness and Generalization Against Word Substitution Attacks in NLP via the Growth Bound Matrix Approach", "authors": ["Mohammed Bouri", "Adnane Saoud"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to ACL Findings 2025", "summary": "Despite advancements in Natural Language Processing (NLP), models remain\nvulnerable to adversarial attacks, such as synonym substitutions. While prior\nwork has focused on improving robustness for feed-forward and convolutional\narchitectures, the robustness of recurrent networks and modern state space\nmodels (SSMs), such as S4, remains understudied. These architectures pose\nunique challenges due to their sequential processing and complex parameter\ndynamics. In this paper, we introduce a novel regularization technique based on\nGrowth Bound Matrices (GBM) to improve NLP model robustness by reducing the\nimpact of input perturbations on model outputs. We focus on computing the GBM\nfor three architectures: Long Short-Term Memory (LSTM), State Space models\n(S4), and Convolutional Neural Networks (CNN). Our method aims to (1) enhance\nresilience against word substitution attacks, (2) improve generalization on\nclean text, and (3) providing the first systematic analysis of SSM (S4)\nrobustness. Extensive experiments across multiple architectures and benchmark\ndatasets demonstrate that our method improves adversarial robustness by up to\n8.8% over existing baselines. These results highlight the effectiveness of our\napproach, outperforming several state-of-the-art methods in adversarial\ndefense. Codes are available at https://github.com/BouriMohammed/GBM"}
{"id": "2507.10342", "pdf": "https://arxiv.org/pdf/2507.10342.pdf", "abs": "https://arxiv.org/abs/2507.10342", "title": "Using AI to replicate human experimental results: a motion study", "authors": ["Rosa Illan Castillo", "Javier Valenzuela"], "categories": ["cs.CL"], "comment": null, "summary": "This paper explores the potential of large language models (LLMs) as reliable\nanalytical tools in linguistic research, focusing on the emergence of affective\nmeanings in temporal expressions involving manner-of-motion verbs. While LLMs\nlike GPT-4 have shown promise across a range of tasks, their ability to\nreplicate nuanced human judgements remains under scrutiny. We conducted four\npsycholinguistic studies (on emergent meanings, valence shifts, verb choice in\nemotional contexts, and sentence-emoji associations) first with human\nparticipants and then replicated the same tasks using an LLM. Results across\nall studies show a striking convergence between human and AI responses, with\nstatistical analyses (e.g., Spearman's rho = .73-.96) indicating strong\ncorrelations in both rating patterns and categorical choices. While minor\ndivergences were observed in some cases, these did not alter the overall\ninterpretative outcomes. These findings offer compelling evidence that LLMs can\naugment traditional human-based experimentation, enabling broader-scale studies\nwithout compromising interpretative validity. This convergence not only\nstrengthens the empirical foundation of prior human-based findings but also\nopens possibilities for hypothesis generation and data expansion through AI.\nUltimately, our study supports the use of LLMs as credible and informative\ncollaborators in linguistic inquiry."}
{"id": "2507.10354", "pdf": "https://arxiv.org/pdf/2507.10354.pdf", "abs": "https://arxiv.org/abs/2507.10354", "title": "Meanings are like Onions: a Layered Approach to Metaphor Processing", "authors": ["Silvia Cappa", "Anna Sofia Lippolis", "Stefano Zoia"], "categories": ["cs.CL"], "comment": null, "summary": "Metaphorical meaning is not a flat mapping between concepts, but a complex\ncognitive phenomenon that integrates multiple levels of interpretation. In this\npaper, we propose a stratified model of metaphor processing that treats meaning\nas an onion: a multi-layered structure comprising (1) content analysis, (2)\nconceptual blending, and (3) pragmatic intentionality. This three-dimensional\nframework allows for a richer and more cognitively grounded approach to\nmetaphor interpretation in computational systems. At the first level, metaphors\nare annotated through basic conceptual elements. At the second level, we model\nconceptual combinations, linking components to emergent meanings. Finally, at\nthe third level, we introduce a pragmatic vocabulary to capture speaker intent,\ncommunicative function, and contextual effects, aligning metaphor understanding\nwith pragmatic theories. By unifying these layers into a single formal\nframework, our model lays the groundwork for computational methods capable of\nrepresenting metaphorical meaning beyond surface associations, toward deeper,\nmore context-sensitive reasoning."}
{"id": "2507.10435", "pdf": "https://arxiv.org/pdf/2507.10435.pdf", "abs": "https://arxiv.org/abs/2507.10435", "title": "From Sequence to Structure: Uncovering Substructure Reasoning in Transformers", "authors": ["Xinnan Dai", "Kai Yang", "Jay Revolinsky", "Kai Guo", "Aoran Wang", "Bohang Zhang", "Jiliang Tang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent studies suggest that large language models (LLMs) possess the\ncapability to solve graph reasoning tasks. Notably, even when graph structures\nare embedded within textual descriptions, LLMs can still effectively answer\nrelated questions. This raises a fundamental question: How can a decoder-only\nTransformer architecture understand underlying graph structures? To address\nthis, we start with the substructure extraction task, interpreting the inner\nmechanisms inside the transformers and analyzing the impact of the input\nqueries. Specifically, through both empirical results and theoretical analysis,\nwe present Induced Substructure Filtration (ISF), a perspective that captures\nthe substructure identification in the multi-layer transformers. We further\nvalidate the ISF process in LLMs, revealing consistent internal dynamics across\nlayers. Building on these insights, we explore the broader capabilities of\nTransformers in handling diverse graph types. Specifically, we introduce the\nconcept of thinking in substructures to efficiently extract complex composite\npatterns, and demonstrate that decoder-only Transformers can successfully\nextract substructures from attributed graphs, such as molecular graphs.\nTogether, our findings offer a new insight on how sequence-based Transformers\nperform the substructure extraction task over graph data."}
{"id": "2507.10445", "pdf": "https://arxiv.org/pdf/2507.10445.pdf", "abs": "https://arxiv.org/abs/2507.10445", "title": "Referential ambiguity and clarification requests: comparing human and LLM behaviour", "authors": ["Chris Madge", "Matthew Purver", "Massimo Poesio"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this work we examine LLMs' ability to ask clarification questions in\ntask-oriented dialogues that follow the asynchronous\ninstruction-giver/instruction-follower format. We present a new corpus that\ncombines two existing annotations of the Minecraft Dialogue Corpus -- one for\nreference and ambiguity in reference, and one for SDRT including clarifications\n-- into a single common format providing the necessary information to\nexperiment with clarifications and their relation to ambiguity. With this\ncorpus we compare LLM actions with original human-generated clarification\nquestions, examining how both humans and LLMs act in the case of ambiguity. We\nfind that there is only a weak link between ambiguity and humans producing\nclarification questions in these dialogues, and low correlation between humans\nand LLMs. Humans hardly ever produce clarification questions for referential\nambiguity, but often do so for task-based uncertainty. Conversely, LLMs produce\nmore clarification questions for referential ambiguity, but less so for task\nuncertainty. We question if LLMs' ability to ask clarification questions is\npredicated on their recent ability to simulate reasoning, and test this with\ndifferent reasoning approaches, finding that reasoning does appear to increase\nquestion frequency and relevancy."}
{"id": "2507.10468", "pdf": "https://arxiv.org/pdf/2507.10468.pdf", "abs": "https://arxiv.org/abs/2507.10468", "title": "From BERT to Qwen: Hate Detection across architectures", "authors": ["Ariadna Mon", "SaÃºl Fenollosa", "Jon Lecumberri"], "categories": ["cs.CL", "cs.LG"], "comment": "4 pages, 5 figures. EE-559 Deep Learning course project (Group 11)", "summary": "Online platforms struggle to curb hate speech without over-censoring\nlegitimate discourse. Early bidirectional transformer encoders made big\nstrides, but the arrival of ultra-large autoregressive LLMs promises deeper\ncontext-awareness. Whether this extra scale actually improves practical\nhate-speech detection on real-world text remains unverified. Our study puts\nthis question to the test by benchmarking both model families, classic encoders\nand next-generation LLMs, on curated corpora of online interactions for\nhate-speech detection (Hate or No Hate)."}
{"id": "2507.10472", "pdf": "https://arxiv.org/pdf/2507.10472.pdf", "abs": "https://arxiv.org/abs/2507.10472", "title": "MLAR: Multi-layer Large Language Model-based Robotic Process Automation Applicant Tracking", "authors": ["Mohamed T. Younes", "Omar Walid", "Mai Hassan", "Ali Hamdi"], "categories": ["cs.CL"], "comment": null, "summary": "This paper introduces an innovative Applicant Tracking System (ATS) enhanced\nby a novel Robotic process automation (RPA) framework or as further referred to\nas MLAR. Traditional recruitment processes often encounter bottlenecks in\nresume screening and candidate shortlisting due to time and resource\nconstraints. MLAR addresses these challenges employing Large Language Models\n(LLMs) in three distinct layers: extracting key characteristics from job\npostings in the first layer, parsing applicant resume to identify education,\nexperience, skills in the second layer, and similarity matching in the third\nlayer. These features are then matched through advanced semantic algorithms to\nidentify the best candidates efficiently. Our approach integrates seamlessly\ninto existing RPA pipelines, automating resume parsing, job matching, and\ncandidate notifications. Extensive performance benchmarking shows that MLAR\noutperforms the leading RPA platforms, including UiPath and Automation\nAnywhere, in high-volume resume-processing tasks. When processing 2,400\nresumes, MLAR achieved an average processing time of 5.4 seconds per resume,\nreducing processing time by approximately 16.9% compared to Automation Anywhere\nand 17.1% compared to UiPath. These results highlight the potential of MLAR to\ntransform recruitment workflows by providing an efficient, accurate, and\nscalable solution tailored to modern hiring needs."}
{"id": "2507.10475", "pdf": "https://arxiv.org/pdf/2507.10475.pdf", "abs": "https://arxiv.org/abs/2507.10475", "title": "Can You Detect the Difference?", "authors": ["Ä°smail TarÄ±m", "AytuÄ Onan"], "categories": ["cs.CL", "cs.AI", "I.2.7; H.3.3"], "comment": "11 pages, 3 figures, 2 tables. Code and data:\n  https://github.com/ismailtrm/ceng_404. Cross-list requested to cs.AI for\n  AI-safety relevance", "summary": "The rapid advancement of large language models (LLMs) has raised concerns\nabout reliably detecting AI-generated text. Stylometric metrics work well on\nautoregressive (AR) outputs, but their effectiveness on diffusion-based models\nis unknown. We present the first systematic comparison of diffusion-generated\ntext (LLaDA) and AR-generated text (LLaMA) using 2 000 samples. Perplexity,\nburstiness, lexical diversity, readability, and BLEU/ROUGE scores show that\nLLaDA closely mimics human text in perplexity and burstiness, yielding high\nfalse-negative rates for AR-oriented detectors. LLaMA shows much lower\nperplexity but reduced lexical fidelity. Relying on any single metric fails to\nseparate diffusion outputs from human writing. We highlight the need for\ndiffusion-aware detectors and outline directions such as hybrid models,\ndiffusion-specific stylometric signatures, and robust watermarking."}
{"id": "2507.10524", "pdf": "https://arxiv.org/pdf/2507.10524.pdf", "abs": "https://arxiv.org/abs/2507.10524", "title": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation", "authors": ["Sangmin Bae", "Yujin Kim", "Reza Bayat", "Sungnyun Kim", "Jiyoun Ha", "Tal Schuster", "Adam Fisch", "Hrayr Harutyunyan", "Ziwei Ji", "Aaron Courville", "Se-Young Yun"], "categories": ["cs.CL", "cs.LG"], "comment": "36 pages, 9 figures, 14 tables, codes at\n  https://github.com/raymin0223/mixture_of_recursions", "summary": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to decrease prefill latency and memory\nfootprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms\na new Pareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost."}
{"id": "2507.10535", "pdf": "https://arxiv.org/pdf/2507.10535.pdf", "abs": "https://arxiv.org/abs/2507.10535", "title": "CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks", "authors": ["Hongchao Jiang", "Yiming Chen", "Yushi Cao", "Hung-yi Lee", "Robby T. Tan"], "categories": ["cs.CL", "cs.AI", "cs.SE"], "comment": "Dataset is available at\n  https://huggingface.co/datasets/mattymchen/codejudgebench", "summary": "Large Language Models (LLMs) have significantly advanced the state-of-the-art\nin various coding tasks. Beyond directly answering user queries, LLMs can also\nserve as judges, assessing and comparing the quality of responses generated by\nother models. Such an evaluation capability is crucial both for benchmarking\ndifferent LLMs and for improving response quality through response ranking.\nHowever, despite the growing adoption of the LLM-as-a-Judge paradigm, its\neffectiveness in coding scenarios remains underexplored due to the absence of\ndedicated benchmarks. To address this gap, we introduce CodeJudgeBench, a\nbenchmark explicitly designed to evaluate the performance of LLM-as-a-Judge\nmodels across three critical coding tasks: code generation, code repair, and\nunit test generation. Through comprehensive benchmarking of 26 LLM-as-a-Judge\nmodels, we find that recent thinking models significantly outperform\nnon-thinking models on our carefully designed code judging tasks. Notably, even\nrelatively small thinking models, such as Qwen3-8B, can outperform specially\ntrained LLM-as-a-Judge models up to 70B in size. Nevertheless, all models still\nexhibit significant randomness in their judgment of coding tasks. For pairwise\njudging tasks, simply changing the order in which responses are presented can\nsubstantially impact accuracy. In addition, when judging code and unit tests\nwritten by different LLMs, LLM-as-a-Judge models also show variance in\nperformance. This sensitivity raises concerns about the reliability and\nconsistency of LLM-as-a-Judge in coding scenarios. Lastly, we study optimal\nprompting strategies for LLM-as-a-Judge. We find that using pair-wise\ncomparison outperforms scalar point-wise judging. Furthermore, retaining\ncomments and reasoning in the full, unprocessed LLM response leads to improved\njudge performance."}
{"id": "2507.10541", "pdf": "https://arxiv.org/pdf/2507.10541.pdf", "abs": "https://arxiv.org/abs/2507.10541", "title": "REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once", "authors": ["Zhuoshi Pan", "Qizhi Pei", "Yu Li", "Qiyao Sun", "Zinan Tang", "H. Vicky Zhao", "Conghui He", "Lijun Wu"], "categories": ["cs.CL"], "comment": "REST (Reasoning Evaluation through Simultaneous Testing), a\n  stress-testing framework that concurrently exposes LRMs to multiple problems\n  simultaneously", "summary": "Recent Large Reasoning Models (LRMs) have achieved remarkable progress on\ntask-specific benchmarks, yet their evaluation methods remain constrained by\nisolated problem-solving paradigms. Existing benchmarks predominantly assess\nsingle-question reasoning through sequential testing, resulting critical\nlimitations: (1) vulnerability to data contamination and less challenging\n(e.g., DeepSeek-R1 achieves 97.0% on MATH500), forcing costly and perpetual\ncreation of new questions with large human efforts, (2) failure to evaluate\nmodels under multi-context pressure, a key requirement for real-world\ndeployment. To bridge this gap, we present REST (Reasoning Evaluation through\nSimultaneous Testing), a stress-testing framework that concurrently exposes\nLRMs to multiple problems simultaneously. Beyond basic reasoning, REST\nspecifically evaluates several under-tested capabilities: contextual priority\nallocation, cross-problem interference resistance, and dynamic cognitive load\nmanagement. Our evaluation reveals several striking findings: Even\nstate-of-the-art (SOTA) models like DeepSeek-R1 exhibit substantial performance\ndegradation under stress testing. Crucially, REST demonstrates stronger\ndiscriminative power than existing benchmarks, revealing pronounced performance\ndifferences among models that exhibit similar, near-ceiling performance under\nsingle-question evaluations. Some key mechanistic insights emerge from our\nanalysis: (1) the \"overthinking trap\" is a critical factor contributing to the\nperformance degradation; (2) the models trained with \"long2short\" technique\npreserve more accuracy of their single-problem performance under REST,\noutperforming standard-trained counterparts. These results establish REST as a\ncost-efficient, future-proof evaluation paradigm that better reflects\nreal-world reasoning demands while reducing reliance on continuous human\nannotation."}
{"id": "2507.07855", "pdf": "https://arxiv.org/pdf/2507.07855.pdf", "abs": "https://arxiv.org/abs/2507.07855", "title": "Principled Foundations for Preference Optimization", "authors": ["Wenxuan Zhou", "Shujian Zhang", "Brice Magdalou", "John Lambert", "Ehsan Amid", "Richard Nock", "Andrew Hard"], "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.6; I.2.7"], "comment": null, "summary": "In this paper, we show that direct preference optimization (DPO) is a very\nspecific form of a connection between two major theories in the ML context of\nlearning from preferences: loss functions (Savage) and stochastic choice\n(Doignon-Falmagne and Machina). The connection is established for all of\nSavage's losses and at this level of generality, (i) it includes support for\nabstention on the choice theory side, (ii) it includes support for non-convex\nobjectives on the ML side, and (iii) it allows to frame for free some notable\nextensions of the DPO setting, including margins and corrections for length.\nGetting to understand how DPO operates from a general principled perspective is\ncrucial because of the huge and diverse application landscape of models,\nbecause of the current momentum around DPO, but also -- and importantly --\nbecause many state of the art variations on DPO definitely occupy a small\nregion of the map that we cover. It also helps to understand the pitfalls of\ndeparting from this map, and figure out workarounds."}
{"id": "2507.08806", "pdf": "https://arxiv.org/pdf/2507.08806.pdf", "abs": "https://arxiv.org/abs/2507.08806", "title": "Think Clearly: Improving Reasoning via Redundant Token Pruning", "authors": ["Daewon Choi", "Jimin Lee", "Jihoon Tack", "Woomin Song", "Saket Dingliwal", "Sai Muralidhar Jayanthi", "Bhavana Ganesh", "Jinwoo Shin", "Aram Galstyan", "Sravan Babu Bodapati"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Recent large language models have shown promising capabilities in long-form\nreasoning, following structured chains of thought before arriving at a final\nanswer. However, we observe that these reasoning paths tend to include\nsubstantial redundancy; analyzing attention patterns reveals that attention\nscores are widely scattered, particularly incorrect answers exhibit greater\nattention sparsity. In this paper, we demonstrate that deliberately removing\nthis redundancy in the reasoning process significantly improves performance\nthrough clear thinking, i.e., removing distraction. Specifically, we\nsystematically identify reasoning redundancy by measuring token-level attention\nscores to a special end-of-thinking token, which is appended to an explicit\ninstruction inserted to conclude each intermediate reasoning step. Furthermore,\nwe propose structure-aware pruning that prioritizes removing tokens in\nlow-contributing reasoning chunks over individual tokens. After evicting\nredundant tokens, we remove the injected end-of-thinking instruction, then\nresume the reasoning generation. We demonstrate that our method significantly\nimproves overall accuracy across reasoning-intensive benchmarks without any\ntraining involved. In particular, our method shows strong performance on\nchallenging mathematical competition benchmarks such as AIME and AMC, where\nreasoning redundancy is more prevalent."}
{"id": "2507.08833", "pdf": "https://arxiv.org/pdf/2507.08833.pdf", "abs": "https://arxiv.org/abs/2507.08833", "title": "LoRA Is Slower Than You Think", "authors": ["Seokmin Ko"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Low-Rank Adaptation (LoRA) is one of the most widely used techniques for\nfine-tuning large language models (LLMs). By introducing a small number of\ntrainable low-rank weight matrices, LoRA substantially reduces the number of\nparameters that need to be updated, offering significant advantages in memory\nconsumption and computational efficiency compared to full fine-tuning. However,\nwe observed that LoRA does not consistently provide speed improvements across\nall model architectures and training setups. Motivated by this inconsistency,\nwe conduct a comprehensive analysis of LoRA's performance and investigate the\nunderlying factors limiting its speedup. Based on our findings, we propose\nseveral methods for more efficient fine-tuning of LLMs. We empirically evaluate\nthese methods and compare them to LoRA, demonstrating that our approach\nachieves comparable or superior performance while delivering more consistent\ntraining speed improvements. Our work offers valuable insights and practical\nguidelines for practitioners seeking to optimize LLM fine-tuning under resource\nconstraints."}
{"id": "2507.08862", "pdf": "https://arxiv.org/pdf/2507.08862.pdf", "abs": "https://arxiv.org/abs/2507.08862", "title": "RAG Safety: Exploring Knowledge Poisoning Attacks to Retrieval-Augmented Generation", "authors": ["Tianzhe Zhao", "Jiaoyan Chen", "Yanchi Ru", "Haiping Zhu", "Nan Hu", "Jun Liu", "Qika Lin"], "categories": ["cs.CR", "cs.CL"], "comment": "13 pages, 6 figures", "summary": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\nretrieving external data to mitigate hallucinations and outdated knowledge\nissues. Benefiting from the strong ability in facilitating diverse data sources\nand supporting faithful reasoning, knowledge graphs (KGs) have been\nincreasingly adopted in RAG systems, giving rise to KG-based RAG (KG-RAG)\nmethods. Though RAG systems are widely applied in various applications, recent\nstudies have also revealed its vulnerabilities to data poisoning attacks, where\nmalicious information injected into external knowledge sources can mislead the\nsystem into producing incorrect or harmful responses. However, these studies\nfocus exclusively on RAG systems using unstructured textual data sources,\nleaving the security risks of KG-RAG largely unexplored, despite the fact that\nKGs present unique vulnerabilities due to their structured and editable nature.\nIn this work, we conduct the first systematic investigation of the security\nissue of KG-RAG methods through data poisoning attacks. To this end, we\nintroduce a practical, stealthy attack setting that aligns with real-world\nimplementation. We propose an attack strategy that first identifies adversarial\ntarget answers and then inserts perturbation triples to complete misleading\ninference chains in the KG, increasing the likelihood that KG-RAG methods\nretrieve and rely on these perturbations during generation. Through extensive\nexperiments on two benchmarks and four recent KG-RAG methods, our attack\nstrategy demonstrates strong effectiveness in degrading KG-RAG performance,\neven with minimal KG perturbations. In-depth analyses are also conducted to\nunderstand the safety threats within the internal stages of KG-RAG systems and\nto explore the robustness of LLMs against adversarial knowledge."}
{"id": "2507.08882", "pdf": "https://arxiv.org/pdf/2507.08882.pdf", "abs": "https://arxiv.org/abs/2507.08882", "title": "Less Stress, More Privacy: Stress Detection on Anonymized Speech of Air Traffic Controllers", "authors": ["Janaki Viswanathan", "Alexander Blatt", "Konrad Hagemann", "Dietrich Klakow"], "categories": ["cs.SD", "cs.CL", "eess.AS", "I.2.7; I.5.5"], "comment": "8 pages, 2 figures, 4 tables, publication identification number\n  (URN)- urn:nbn:de:101:1-2022122008393409239462, see archived online\n  publication- https://d-nb.info/127614606X/34 & Katalogeintrag:\n  https://d-nb.info/127614606X/", "summary": "Air traffic control (ATC) demands multi-tasking under time pressure with high\nconsequences of an error. This can induce stress. Detecting stress is a key\npoint in maintaining the high safety standards of ATC. However, processing ATC\nvoice data entails privacy restrictions, e.g. the General Data Protection\nRegulation (GDPR) law. Anonymizing the ATC voice data is one way to comply with\nthese restrictions. In this paper, different architectures for stress detection\nfor anonymized ATCO speech are evaluated. Our best networks reach a stress\ndetection accuracy of 93.6% on an anonymized version of the Speech Under\nSimulated and Actual Stress (SUSAS) dataset and an accuracy of 80.1% on our\nanonymized ATC simulation dataset. This shows that privacy does not have to be\nan impediment in building well-performing deep-learning-based models."}
{"id": "2507.08890", "pdf": "https://arxiv.org/pdf/2507.08890.pdf", "abs": "https://arxiv.org/abs/2507.08890", "title": "Overview of the TREC 2023 deep learning track", "authors": ["Nick Craswell", "Bhaskar Mitra", "Emine Yilmaz", "Hossein A. Rahmani", "Daniel Campos", "Jimmy Lin", "Ellen M. Voorhees", "Ian Soboroff"], "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "arXiv admin note: substantial text overlap with arXiv:2507.08191", "summary": "This is the fifth year of the TREC Deep Learning track. As in previous years,\nwe leverage the MS MARCO datasets that made hundreds of thousands of\nhuman-annotated training labels available for both passage and document ranking\ntasks. We mostly repeated last year's design, to get another matching test set,\nbased on the larger, cleaner, less-biased v2 passage and document set, with\npassage ranking as primary and document ranking as a secondary task (using\nlabels inferred from passage). As we did last year, we sample from MS MARCO\nqueries that were completely held out, unused in corpus construction, unlike\nthe test queries in the first three years. This approach yields a more\ndifficult test with more headroom for improvement. Alongside the usual MS MARCO\n(human) queries from MS MARCO, this year we generated synthetic queries using a\nfine-tuned T5 model and using a GPT-4 prompt.\n  The new headline result this year is that runs using Large Language Model\n(LLM) prompting in some way outperformed runs that use the \"nnlm\" approach,\nwhich was the best approach in the previous four years. Since this is the last\nyear of the track, future iterations of prompt-based ranking can happen in\nother tracks. Human relevance assessments were applied to all query types, not\njust human MS MARCO queries. Evaluation using synthetic queries gave similar\nresults to human queries, with system ordering agreement of $\\tau=0.8487$.\nHowever, human effort was needed to select a subset of the synthetic queries\nthat were usable. We did not see clear evidence of bias, where runs using GPT-4\nwere favored when evaluated using synthetic GPT-4 queries, or where runs using\nT5 were favored when evaluated on synthetic T5 queries."}
{"id": "2507.08992", "pdf": "https://arxiv.org/pdf/2507.08992.pdf", "abs": "https://arxiv.org/abs/2507.08992", "title": "Semantic Source Code Segmentation using Small and Large Language Models", "authors": ["Abdelhalim Dahou", "Ansgar Scherp", "Sebastian Kurten", "Brigitte Mathiak", "Madhu Chauhan"], "categories": ["cs.SE", "cs.CL", "cs.PL"], "comment": "18 pages, 4 figures", "summary": "Source code segmentation, dividing code into functionally coherent segments,\nis crucial for knowledge retrieval and maintenance in software development.\nWhile enabling efficient navigation and comprehension of large codebases,\nmanual and syntactic analysis approaches have become impractical as\nrepositories grow, especially for low-resource languages like R and their\nresearch domains (e.g., social sciences, psychology).This paper introduces an\nautomated, domain-specific approach for research R code segmentation using\nLarge and Small Language Models (LLMs/SLMs). It presents two novel approaches\nand a human-annotated dataset, StatCodeSeg. We explore two distinct approaches:\nline-by-line analysis with context and range-based segment determination. We\nexperiment with LLMs and fine-tuned SLMs. To support the generalizability of\nour approaches, we also include experiments on Python code from the computer\nscience domain.Our results show that context-based line-by-line analysis is\nsuperior over range-based segmentation.Using smaller language models like\nCodeBERT and an encoder-only version of CodeT5+ are better than their LLM\ncounterparts. Most notably, these two best-performing models did not see R code\nduring pre-training versus the LLMs but were only fine-tuned on 4,130 lines of\nmanually annotated code."}
{"id": "2507.09090", "pdf": "https://arxiv.org/pdf/2507.09090.pdf", "abs": "https://arxiv.org/abs/2507.09090", "title": "DS@GT at TouchÃ©: Large Language Models for Retrieval-Augmented Debate", "authors": ["Anthony Miyaguchi", "Conor Johnston", "Aaryan Potdar"], "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) demonstrate strong conversational abilities. In\nthis Working Paper, we study them in the context of debating in two ways: their\nability to perform in a structured debate along with a dataset of arguments to\nuse and their ability to evaluate utterances throughout the debate. We deploy\nsix leading publicly available models from three providers for the\nRetrieval-Augmented Debate and Evaluation. The evaluation is performed by\nmeasuring four key metrics: Quality, Quantity, Manner, and Relation. Throughout\nthis task, we found that although LLMs perform well in debates when given\nrelated arguments, they tend to be verbose in responses yet consistent in\nevaluation. The accompanying source code for this paper is located at\nhttps://github.com/dsgt-arc/touche-2025-rad."}
{"id": "2507.09100", "pdf": "https://arxiv.org/pdf/2507.09100.pdf", "abs": "https://arxiv.org/abs/2507.09100", "title": "AInsight: Augmenting Expert Decision-Making with On-the-Fly Insights Grounded in Historical Data", "authors": ["Mohammad Abolnejadian", "Shakiba Amirshahi", "Matthew Brehmer", "Anamaria Crisan"], "categories": ["cs.HC", "cs.AI", "cs.CL", "H.5.0"], "comment": "7 pages and 4 figures. Proceedings of the 7th ACM Conference on\n  Conversational User Interfaces (CUI '25)", "summary": "In decision-making conversations, experts must navigate complex choices and\nmake on-the-spot decisions while engaged in conversation. Although extensive\nhistorical data often exists, the real-time nature of these scenarios makes it\ninfeasible for decision-makers to review and leverage relevant information.\nThis raises an interesting question: What if experts could utilize relevant\npast data in real-time decision-making through insights derived from past data?\nTo explore this, we implemented a conversational user interface, taking\ndoctor-patient interactions as an example use case. Our system continuously\nlistens to the conversation, identifies patient problems and doctor-suggested\nsolutions, and retrieves related data from an embedded dataset, generating\nconcise insights using a pipeline built around a retrieval-based Large Language\nModel (LLM) agent. We evaluated the prototype by embedding Health Canada\ndatasets into a vector database and conducting simulated studies using sample\ndoctor-patient dialogues, showing effectiveness but also challenges, setting\ndirections for the next steps of our work."}
{"id": "2507.09176", "pdf": "https://arxiv.org/pdf/2507.09176.pdf", "abs": "https://arxiv.org/abs/2507.09176", "title": "DLBAcalib: Robust Extrinsic Calibration for Non-Overlapping LiDARs Based on Dual LBA", "authors": ["Han Ye", "Yuqiang Jin", "Jinyuan Liu", "Tao Li", "Wen-An Zhang", "Minglei Fu"], "categories": ["cs.RO", "cs.CL"], "comment": "9 pages,14 figures", "summary": "Accurate extrinsic calibration of multiple LiDARs is crucial for improving\nthe foundational performance of three-dimensional (3D) map reconstruction\nsystems. This paper presents a novel targetless extrinsic calibration framework\nfor multi-LiDAR systems that does not rely on overlapping fields of view or\nprecise initial parameter estimates. Unlike conventional calibration methods\nthat require manual annotations or specific reference patterns, our approach\nintroduces a unified optimization framework by integrating LiDAR bundle\nadjustment (LBA) optimization with robust iterative refinement. The proposed\nmethod constructs an accurate reference point cloud map via continuous scanning\nfrom the target LiDAR and sliding-window LiDAR bundle adjustment, while\nformulating extrinsic calibration as a joint LBA optimization problem. This\nmethod effectively mitigates cumulative mapping errors and achieves\noutlier-resistant parameter estimation through an adaptive weighting mechanism.\nExtensive evaluations in both the CARLA simulation environment and real-world\nscenarios demonstrate that our method outperforms state-of-the-art calibration\ntechniques in both accuracy and robustness. Experimental results show that for\nnon-overlapping sensor configurations, our framework achieves an average\ntranslational error of 5 mm and a rotational error of 0.2{\\deg}, with an\ninitial error tolerance of up to 0.4 m/30{\\deg}. Moreover, the calibration\nprocess operates without specialized infrastructure or manual parameter tuning.\nThe code is open source and available on GitHub\n(\\underline{https://github.com/Silentbarber/DLBAcalib})"}
{"id": "2507.09279", "pdf": "https://arxiv.org/pdf/2507.09279.pdf", "abs": "https://arxiv.org/abs/2507.09279", "title": "Prompt4Trust: A Reinforcement Learning Prompt Augmentation Framework for Clinically-Aligned Confidence Calibration in Multimodal Large Language Models", "authors": ["Anita Kriz", "Elizabeth Laura Janes", "Xing Shen", "Tal Arbel"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Preprint version. The peer-reviewed version of this paper has been\n  accepted to ICCV 2025 Workshop CVAMD", "summary": "Multimodal large language models (MLLMs) hold considerable promise for\napplications in healthcare. However, their deployment in safety-critical\nsettings is hindered by two key limitations: (i) sensitivity to prompt design,\nand (ii) a tendency to generate incorrect responses with high confidence. As\nclinicians may rely on a model's stated confidence to gauge the reliability of\nits predictions, it is especially important that when a model expresses high\nconfidence, it is also highly accurate. We introduce Prompt4Trust, the first\nreinforcement learning (RL) framework for prompt augmentation targeting\nconfidence calibration in MLLMs. A lightweight LLM is trained to produce\ncontext-aware auxiliary prompts that guide a downstream task MLLM to generate\nresponses in which the expressed confidence more accurately reflects predictive\naccuracy. Unlike conventional calibration techniques, Prompt4Trust specifically\nprioritizes aspects of calibration most critical for safe and trustworthy\nclinical decision-making. Beyond improvements driven by this clinically\nmotivated calibration objective, our proposed method also improves task\naccuracy, achieving state-of-the-art medical visual question answering (VQA)\nperformance on the PMC-VQA benchmark, which is composed of multiple-choice\nquestions spanning diverse medical imaging modalities. Moreover, our framework\ntrained with a small downstream task MLLM showed promising zero-shot\ngeneralization to larger MLLMs in our experiments, suggesting the potential for\nscalable calibration without the associated computational costs. This work\ndemonstrates the potential of automated yet human-aligned prompt engineering\nfor improving the the trustworthiness of MLLMs in safety critical settings. Our\ncodebase can be found at https://github.com/xingbpshen/vccrl-llm."}
{"id": "2507.09310", "pdf": "https://arxiv.org/pdf/2507.09310.pdf", "abs": "https://arxiv.org/abs/2507.09310", "title": "Voice Conversion for Lombard Speaking Style with Implicit and Explicit Acoustic Feature Conditioning", "authors": ["Dominika Woszczyk", "Manuel Sam Ribeiro", "Thomas Merritt", "Daniel Korzekwa"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Presented at Clarity Challenge 2023", "summary": "Text-to-Speech (TTS) systems in Lombard speaking style can improve the\noverall intelligibility of speech, useful for hearing loss and noisy\nconditions. However, training those models requires a large amount of data and\nthe Lombard effect is challenging to record due to speaker and noise\nvariability and tiring recording conditions. Voice conversion (VC) has been\nshown to be a useful augmentation technique to train TTS systems in the absence\nof recorded data from the target speaker in the target speaking style. In this\npaper, we are concerned with Lombard speaking style transfer. Our goal is to\nconvert speaker identity while preserving the acoustic attributes that define\nthe Lombard speaking style. We compare voice conversion models with implicit\nand explicit acoustic feature conditioning. We observe that our proposed\nimplicit conditioning strategy achieves an intelligibility gain comparable to\nthe model conditioned on explicit acoustic features, while also preserving\nspeaker similarity."}
{"id": "2507.09318", "pdf": "https://arxiv.org/pdf/2507.09318.pdf", "abs": "https://arxiv.org/abs/2507.09318", "title": "ZipVoice-Dialog: Non-Autoregressive Spoken Dialogue Generation with Flow Matching", "authors": ["Han Zhu", "Wei Kang", "Liyong Guo", "Zengwei Yao", "Fangjun Kuang", "Weiji Zhuang", "Zhaoqing Li", "Zhifeng Han", "Dong Zhang", "Xin Zhang", "Xingchen Song", "Long Lin", "Daniel Povey"], "categories": ["eess.AS", "cs.CL"], "comment": null, "summary": "Generating spoken dialogue is more challenging than monologue text-to-speech\n(TTS) due to the need for realistic turn-taking and distinct speaker timbres.\nExisting spoken dialogue generation models, being auto-regressive, suffer from\nslow and unstable inference. To overcome these limitations, we introduce\nZipVoice-Dialog, a non-autoregressive zero-shot spoken dialogue generation\nmodel built upon flow matching. Key designs include: 1) speaker-turn embeddings\nfor precise speaker turn-taking; 2) a curriculum learning strategy for stable\nspeech-text alignment; 3) specialized strategies to enable stereo dialogue\ngeneration. Additionally, recognizing the lack of open-source large-scale\nspoken dialogue datasets, we curated OpenDialog, a 6.8k-hour spoken dialogue\ndataset from in-the-wild speech data. Furthermore, we established a benchmark\nto comprehensively evaluate various models. Experimental results demonstrate\nthat ZipVoice-Dialog achieves superior performance in intelligibility, speaker\nturn-taking accuracy, speaker similarity, and inference speed. Our codes, model\ncheckpoints, demo samples, and the OpenDialog dataset are all publicly\navailable at https://github.com/k2-fsa/ZipVoice."}
{"id": "2507.09481", "pdf": "https://arxiv.org/pdf/2507.09481.pdf", "abs": "https://arxiv.org/abs/2507.09481", "title": "Evaluating LLMs on Sequential API Call Through Automated Test Generation", "authors": ["Yuheng Huang", "Da Song", "Zhenlan Ji", "Shuai Wang", "Lei Ma"], "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": null, "summary": "By integrating tools from external APIs, Large Language Models (LLMs) have\nexpanded their promising capabilities in a diverse spectrum of complex\nreal-world tasks. However, testing, evaluation, and analysis of LLM tool use\nremain in their early stages. Most existing benchmarks rely on manually\ncollected test cases, many of which cannot be automatically checked for\nsemantic correctness and instead depend on static methods such as string\nmatching. Additionally, these benchmarks often overlook the complex\ninteractions that occur between sequential API calls, which are common in\nreal-world applications. To fill the gap, in this paper, we introduce StateGen,\nan automated framework designed to generate diverse coding tasks involving\nsequential API interactions. StateGen combines state-machine-based API\nconstraint solving and validation, energy-based sampling, and control-flow\ninjection to generate executable programs. These programs are then translated\ninto human-like natural language task descriptions through a collaboration of\ntwo LLM agents. Utilizing StateGen, we construct StateEval, a benchmark\nencompassing 120 verified test cases spanning across three representative\nscenarios: Session Service, Tensor Operation, and ElevenLabs MCP. Experimental\nresults confirm that StateGen can effectively generate challenging and\nrealistic API-oriented tasks, highlighting areas for improvement in current\nLLMs incorporating APIs."}
{"id": "2507.09574", "pdf": "https://arxiv.org/pdf/2507.09574.pdf", "abs": "https://arxiv.org/abs/2507.09574", "title": "MENTOR: Efficient Multimodal-Conditioned Tuning for Autoregressive Vision Generation Models", "authors": ["Haozhe Zhao", "Zefan Cai", "Shuzheng Si", "Liang Chen", "Jiuxiang Gu", "Wen Xiao", "Junjie Hu"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "24 pages,12 figures", "summary": "Recent text-to-image models produce high-quality results but still struggle\nwith precise visual control, balancing multimodal inputs, and requiring\nextensive training for complex multimodal image generation. To address these\nlimitations, we propose MENTOR, a novel autoregressive (AR) framework for\nefficient Multimodal-conditioned Tuning for Autoregressive multimodal image\ngeneration. MENTOR combines an AR image generator with a two-stage training\nparadigm, enabling fine-grained, token-level alignment between multimodal\ninputs and image outputs without relying on auxiliary adapters or\ncross-attention modules. The two-stage training consists of: (1) a multimodal\nalignment stage that establishes robust pixel- and semantic-level alignment,\nfollowed by (2) a multimodal instruction tuning stage that balances the\nintegration of multimodal inputs and enhances generation controllability.\nDespite modest model size, suboptimal base components, and limited training\nresources, MENTOR achieves strong performance on the DreamBench++ benchmark,\noutperforming competitive baselines in concept preservation and prompt\nfollowing. Additionally, our method delivers superior image reconstruction\nfidelity, broad task adaptability, and improved training efficiency compared to\ndiffusion-based methods. Dataset, code, and models are available at:\nhttps://github.com/HaozheZhao/MENTOR"}
{"id": "2507.09662", "pdf": "https://arxiv.org/pdf/2507.09662.pdf", "abs": "https://arxiv.org/abs/2507.09662", "title": "Towards Concise and Adaptive Thinking in Large Reasoning Models: A Survey", "authors": ["Jason Zhu", "Hongyu Li"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large reasoning models (LRMs) like OpenAI o1 and DeepSeek R1 have\ndemonstrated impressive performance on complex reasoning tasks like mathematics\nand programming with long Chain-of-Thought (CoT) reasoning sequences\n(slow-thinking), compared with traditional large language models\n(fast-thinking). However, these reasoning models also face a huge challenge\nthat generating unnecessarily lengthy and redundant reasoning chains even for\ntrivial questions. This phenomenon leads to a significant waste of inference\nresources, increases the response time for simple queries, and hinders the\npractical application of LRMs in real-world products. To this end, it is\ncrucial to shorten lengthy reasoning chains and learn adaptive reasoning\nbetween fast and slow thinking based on input difficulty. In this survey, we\nprovide a comprehensive overview of recent progress in concise and adaptive\nthinking for efficient reasoning of LRMs, including methodologies, benchmarks,\nand challenges for future exploration. We hope this survey can help researchers\nquickly understand the landscape of this field and inspire novel adaptive\nthinking ideas to facilitate better usage of LRMs."}
{"id": "2507.09751", "pdf": "https://arxiv.org/pdf/2507.09751.pdf", "abs": "https://arxiv.org/abs/2507.09751", "title": "Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations", "authors": ["Bradley P. Allen", "Prateek Chhikara", "Thomas Macaulay Ferguson", "Filip Ilievski", "Paul Groth"], "categories": ["cs.AI", "cs.CL", "cs.LO"], "comment": "29 pages, 9 tables, 3 figures. Accepted to the 19th Conference on\n  Neurosymbolic Learning and Reasoning (NeSy 2025)", "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\nnatural language understanding and generation, but they exhibit problems with\nlogical consistency in the output they generate. How can we harness LLMs'\nbroad-coverage parametric knowledge in formal reasoning despite their\ninconsistency? We present a method for directly integrating an LLM into the\ninterpretation function of the formal semantics for a paraconsistent logic. We\nprovide experimental evidence for the feasibility of the method by evaluating\nthe function using datasets created from several short-form factuality\nbenchmarks. Unlike prior work, our method offers a theoretical framework for\nneuro-symbolic reasoning that leverages an LLM's knowledge while preserving the\nunderlying logic's soundness and completeness properties."}
{"id": "2507.09762", "pdf": "https://arxiv.org/pdf/2507.09762.pdf", "abs": "https://arxiv.org/abs/2507.09762", "title": "EventHunter: Dynamic Clustering and Ranking of Security Events from Hacker Forum Discussions", "authors": ["Yasir Ech-Chammakhy", "Anas Motii", "Anass Rabii", "Jaafar Chbili"], "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": "Accepted for publication at the 28th International Symposium on\n  Research in Attacks, Intrusions, and Defenses (RAID 2025)", "summary": "Hacker forums provide critical early warning signals for emerging\ncybersecurity threats, but extracting actionable intelligence from their\nunstructured and noisy content remains a significant challenge. This paper\npresents an unsupervised framework that automatically detects, clusters, and\nprioritizes security events discussed across hacker forum posts. Our approach\nleverages Transformer-based embeddings fine-tuned with contrastive learning to\ngroup related discussions into distinct security event clusters, identifying\nincidents like zero-day disclosures or malware releases without relying on\npredefined keywords. The framework incorporates a daily ranking mechanism that\nprioritizes identified events using quantifiable metrics reflecting timeliness,\nsource credibility, information completeness, and relevance. Experimental\nevaluation on real-world hacker forum data demonstrates that our method\neffectively reduces noise and surfaces high-priority threats, enabling security\nanalysts to mount proactive responses. By transforming disparate hacker forum\ndiscussions into structured, actionable intelligence, our work addresses\nfundamental challenges in automated threat detection and analysis."}
{"id": "2507.09788", "pdf": "https://arxiv.org/pdf/2507.09788.pdf", "abs": "https://arxiv.org/abs/2507.09788", "title": "TinyTroupe: An LLM-powered Multiagent Persona Simulation Toolkit", "authors": ["Paulo Salem", "Robert Sim", "Christopher Olsen", "Prerit Saxena", "Rafael Barcelos", "Yi Ding"], "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.HC", "I.2.11; I.6.5; I.6.7"], "comment": "9 pages. Preprint to be submitted to peer-review", "summary": "Recent advances in Large Language Models (LLM) have led to a new class of\nautonomous agents, renewing and expanding interest in the area. LLM-powered\nMultiagent Systems (MAS) have thus emerged, both for assistive and simulation\npurposes, yet tools for realistic human behavior simulation -- with its\ndistinctive challenges and opportunities -- remain underdeveloped. Existing MAS\nlibraries and tools lack fine-grained persona specifications, population\nsampling facilities, experimentation support, and integrated validation, among\nother key capabilities, limiting their utility for behavioral studies, social\nsimulation, and related applications. To address these deficiencies, in this\nwork we introduce TinyTroupe, a simulation toolkit enabling detailed persona\ndefinitions (e.g., nationality, age, occupation, personality, beliefs,\nbehaviors) and programmatic control via numerous LLM-driven mechanisms. This\nallows for the concise formulation of behavioral problems of practical\ninterest, either at the individual or group level, and provides effective means\nfor their solution. TinyTroupe's components are presented using representative\nworking examples, such as brainstorming and market research sessions, thereby\nsimultaneously clarifying their purpose and demonstrating their usefulness.\nQuantitative and qualitative evaluations of selected aspects are also provided,\nhighlighting possibilities, limitations, and trade-offs. The approach, though\nrealized as a specific Python implementation, is meant as a novel conceptual\ncontribution, which can be partially or fully incorporated in other contexts.\nThe library is available as open source at\nhttps://github.com/microsoft/tinytroupe."}
{"id": "2507.09876", "pdf": "https://arxiv.org/pdf/2507.09876.pdf", "abs": "https://arxiv.org/abs/2507.09876", "title": "ViTCoT: Video-Text Interleaved Chain-of-Thought for Boosting Video Understanding in Large Language Models", "authors": ["Yongheng Zhang", "Xu Liu", "Ruihan Tao", "Qiguang Chen", "Hao Fei", "Wanxiang Che", "Libo Qin"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Accepted by ACM MM 2025", "summary": "Video understanding plays a vital role in bridging low-level visual signals\nwith high-level cognitive reasoning, and is fundamental to applications such as\nautonomous driving, embodied AI, and the broader pursuit of AGI. The rapid\ndevelopment of large language models (LLMs), particularly those utilizing\nChain-of-Thought (CoT) technology, has significantly advanced video reasoning\ncapabilities. However, current approaches primarily depend on textual\ninformation for reasoning, overlooking the visual modality in the actual video\nreasoning process. In contrast, humans naturally re-examine visual content\nwhile reasoning. Motivated by this, we introduce a novel video reasoning\nparadigm: Video-Text Interleaved CoT (ViTCoT), which facilitates more intuitive\nand cognitively aligned reasoning. To the end, first, we construct the\nVideo-Text Interleaved Benchmark (ViTIB), which is created using MLLMs for\nkey-video selection and manually verified. Furthermore, we extensively explore\nthe potential of the ViTCoT paradigm in the video understanding field.\nExtensive experiments demonstrate that ViTCoT significantly enhances\nperformance compared to the traditional text-only CoT paradigm and effectively\nactivates more neuron values in MLLMs."}
{"id": "2507.09924", "pdf": "https://arxiv.org/pdf/2507.09924.pdf", "abs": "https://arxiv.org/abs/2507.09924", "title": "MixLoRA-DSI: Dynamically Expandable Mixture-of-LoRA Experts for Rehearsal-Free Generative Retrieval over Dynamic Corpora", "authors": ["Tuan-Luc Huynh", "Thuy-Trang Vu", "Weiqing Wang", "Trung Le", "Dragan GaÅ¡eviÄ", "Yuan-Fang Li", "Thanh-Toan Do"], "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Continually updating model-based indexes in generative retrieval with new\ndocuments remains challenging, as full retraining is computationally expensive\nand impractical under resource constraints. We propose MixLoRA-DSI, a novel\nframework that combines an expandable mixture of Low-Rank Adaptation experts\nwith a layer-wise out-of-distribution (OOD)-driven expansion strategy. Instead\nof allocating new experts for each new corpus, our proposed expansion strategy\nenables sublinear parameter growth by selectively introducing new experts only\nwhen significant number of OOD documents are detected. Experiments on NQ320k\nand MS MARCO Passage demonstrate that MixLoRA-DSI outperforms full-model update\nbaselines, with minimal parameter overhead and substantially lower training\ncosts."}
{"id": "2507.10000", "pdf": "https://arxiv.org/pdf/2507.10000.pdf", "abs": "https://arxiv.org/abs/2507.10000", "title": "On The Role of Intentionality in Knowledge Representation: Analyzing Scene Context for Cognitive Agents with a Tiny Language Model", "authors": ["Mark Burgess"], "categories": ["cs.AI", "cs.CL", "I.2.11; F.4.1; I.2.4; G.2.2"], "comment": null, "summary": "Since Searle's work deconstructing intent and intentionality in the realm of\nphilosophy, the practical meaning of intent has received little attention in\nscience and technology. Intentionality and context are both central to the\nscope of Promise Theory's model of Semantic Spacetime, used as an effective\nTiny Language Model. One can identify themes and concepts from a text, on a low\nlevel (without knowledge of the specific language) by using process coherence\nas a guide. Any agent process can assess superficially a degree of latent\n`intentionality' in data by looking for anomalous multi-scale anomalies and\nassessing the work done to form them. Scale separation can be used to sort\nparts into `intended' content and `ambient context', using the spacetime\ncoherence as a measure. This offers an elementary but pragmatic interpretation\nof latent intentionality for very low computational cost, and without reference\nto extensive training or reasoning capabilities. The process is well within the\nreach of basic organisms as it does not require large scale artificial\nprobabilistic batch processing. The level of concept formation depends,\nhowever, on the memory capacity of the agent."}
{"id": "2507.10013", "pdf": "https://arxiv.org/pdf/2507.10013.pdf", "abs": "https://arxiv.org/abs/2507.10013", "title": "Cross-modal Associations in Vision and Language Models: Revisiting the bouba-kiki effect", "authors": ["Tom Kouwenhoven", "Kiana Shahrasbi", "Tessa Verhoef"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Recent advances in multimodal models have raised questions about whether\nvision-and-language models (VLMs) integrate cross-modal information in ways\nthat reflect human cognition. One well-studied test case in this domain is the\nbouba-kiki effect, where humans reliably associate pseudowords like \"bouba\"\nwith round shapes and \"kiki\" with jagged ones. Given the mixed evidence found\nin prior studies for this effect in VLMs, we present a comprehensive\nre-evaluation focused on two variants of CLIP, ResNet and Vision Transformer\n(ViT), given their centrality in many state-of-the-art VLMs. We apply two\ncomplementary methods closely modelled after human experiments: a prompt-based\nevaluation that uses probabilities as model preference, and we use Grad-CAM as\na novel way to interpret visual attention in shape-word matching tasks. Our\nfindings show that these models do not consistently exhibit the bouba-kiki\neffect. While ResNet shows a preference for round shapes, overall performance\nacross both models lacks the expected associations. Moreover, direct comparison\nwith prior human data on the same task shows that the models' responses fall\nmarkedly short of the robust, modality-integrated behaviour characteristic of\nhuman cognition. These results contribute to the ongoing debate about the\nextent to which VLMs truly understand cross-modal concepts, highlighting\nlimitations in their internal representations and alignment with human\nintuitions."}
{"id": "2507.10045", "pdf": "https://arxiv.org/pdf/2507.10045.pdf", "abs": "https://arxiv.org/abs/2507.10045", "title": "Automating SPARQL Query Translations between DBpedia and Wikidata", "authors": ["Malte Christian Bartels", "Debayan Banerjee", "Ricardo Usbeck"], "categories": ["cs.AI", "cs.CL"], "comment": "18 pages, 2 figues. Paper accepted at SEMANTiCS 2025 conference\n  happening on September 2025", "summary": "This paper investigates whether state-of-the-art Large Language Models (LLMs)\ncan automatically translate SPARQL between popular Knowledge Graph (KG)\nschemas. We focus on translations between the DBpedia and Wikidata KG, and\nlater on DBLP and OpenAlex KG. This study addresses a notable gap in KG\ninteroperability research by rigorously evaluating LLM performance on\nSPARQL-to-SPARQL translation. Two benchmarks are assembled, where the first\nalign 100 DBpedia-Wikidata queries from QALD-9-Plus; the second contains 100\nDBLP queries aligned to OpenAlex, testing generalizability beyond encyclopaedic\nKGs. Three open LLMs: Llama-3-8B, DeepSeek-R1-Distill-Llama-70B, and\nMistral-Large-Instruct-2407 are selected based on their sizes and architectures\nand tested with zero-shot, few-shot, and two chain-of-thought variants. Outputs\nwere compared with gold answers, and resulting errors were categorized. We find\nthat the performance varies markedly across models and prompting strategies,\nand that translations for Wikidata to DBpedia work far better than translations\nfor DBpedia to Wikidata."}
{"id": "2507.10057", "pdf": "https://arxiv.org/pdf/2507.10057.pdf", "abs": "https://arxiv.org/abs/2507.10057", "title": "PRISM: Fine-Grained Paper-to-Paper Retrieval with Multi-Aspect-Aware Query Optimization", "authors": ["Sangwoo Park", "Jinheon Baek", "Soyeong Jeong", "Sung Ju Hwang"], "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Scientific paper retrieval, particularly framed as document-to-document\nretrieval, aims to identify relevant papers in response to a long-form query\npaper, rather than a short query string. Previous approaches to this task have\nfocused on abstracts, embedding them into dense vectors as surrogates for full\ndocuments and calculating similarity across them, although abstracts provide\nonly sparse and high-level summaries. To address this, we propose PRISM, a\nnovel document-to-document retrieval method that introduces multiple,\nfine-grained representations for both the query and candidate papers. In\nparticular, each query paper is decomposed into multiple aspect-specific views\nand individually embedded, which are then matched against candidate papers\nsimilarity segmented to consider their multifaceted dimensions. Moreover, we\npresent SciFullBench, a novel benchmark in which the complete and segmented\ncontext of full papers for both queries and candidates is available. Then,\nexperimental results show that PRISM improves performance by an average of 4.3%\nover existing retrieval baselines."}
{"id": "2507.10200", "pdf": "https://arxiv.org/pdf/2507.10200.pdf", "abs": "https://arxiv.org/abs/2507.10200", "title": "Natural Language-based Assessment of L2 Oral Proficiency using LLMs", "authors": ["Stefano BannÃ²", "Rao Ma", "Mengjie Qian", "Siyuan Tang", "Kate Knill", "Mark Gales"], "categories": ["eess.AS", "cs.AI", "cs.CL"], "comment": "Accepted for the 10th Workshop on Speech and Language Technology in\n  Education (SLaTE 2025)", "summary": "Natural language-based assessment (NLA) is an approach to second language\nassessment that uses instructions - expressed in the form of can-do descriptors\n- originally intended for human examiners, aiming to determine whether large\nlanguage models (LLMs) can interpret and apply them in ways comparable to human\nassessment. In this work, we explore the use of such descriptors with an\nopen-source LLM, Qwen 2.5 72B, to assess responses from the publicly available\nS&I Corpus in a zero-shot setting. Our results show that this approach -\nrelying solely on textual information - achieves competitive performance: while\nit does not outperform state-of-the-art speech LLMs fine-tuned for the task, it\nsurpasses a BERT-based model trained specifically for this purpose. NLA proves\nparticularly effective in mismatched task settings, is generalisable to other\ndata types and languages, and offers greater interpretability, as it is\ngrounded in clearly explainable, widely applicable language descriptors."}
{"id": "2507.10300", "pdf": "https://arxiv.org/pdf/2507.10300.pdf", "abs": "https://arxiv.org/abs/2507.10300", "title": "FaceLLM: A Multimodal Large Language Model for Face Understanding", "authors": ["Hatef Otroshi Shahreza", "SÃ©bastien Marcel"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Accepted in ICCV 2025 workshops", "summary": "Multimodal large language models (MLLMs) have shown remarkable performance in\nvision-language tasks. However, existing MLLMs are primarily trained on generic\ndatasets, limiting their ability to reason on domain-specific visual cues such\nas those in facial images. In particular, tasks that require detailed\nunderstanding of facial structure, expression, emotion, and demographic\nfeatures remain underexplored by MLLMs due to the lack of large-scale annotated\nface image-text datasets. In this work, we introduce FaceLLM, a multimodal\nlarge language model trained specifically for facial image understanding. To\nconstruct the training data, we propose a novel weakly supervised pipeline that\nuses ChatGPT with attribute-aware prompts to generate high-quality\nquestion-answer pairs based on images from the FairFace dataset. The resulting\ncorpus, called FairFaceGPT, covers a diverse set of attributes including\nexpression, pose, skin texture, and forensic information. Our experiments\ndemonstrate that FaceLLM improves the performance of MLLMs on various\nface-centric tasks and achieves state-of-the-art performance. This work\nhighlights the potential of synthetic supervision via language models for\nbuilding domain-specialized MLLMs, and sets a precedent for trustworthy,\nhuman-centric multimodal AI systems. FairFaceGPT dataset and pretrained FaceLLM\nmodels are publicly available in the project page."}
{"id": "2507.10398", "pdf": "https://arxiv.org/pdf/2507.10398.pdf", "abs": "https://arxiv.org/abs/2507.10398", "title": "Devanagari Handwritten Character Recognition using Convolutional Neural Network", "authors": ["Diksha Mehta", "Prateek Mehta"], "categories": ["cs.CV", "cs.AI", "cs.CL", "14J60", "I.2.7; I.4; I.5; I.7.5"], "comment": "9 pages, 6 figures", "summary": "Handwritten character recognition is getting popular among researchers\nbecause of its possible applications in facilitating technological search\nengines, social media, recommender systems, etc. The Devanagari script is one\nof the oldest language scripts in India that does not have proper digitization\ntools. With the advancement of computing and technology, the task of this\nresearch is to extract handwritten Hindi characters from an image of Devanagari\nscript with an automated approach to save time and obsolete data. In this\npaper, we present a technique to recognize handwritten Devanagari characters\nusing two deep convolutional neural network layers. This work employs a\nmethodology that is useful to enhance the recognition rate and configures a\nconvolutional neural network for effective Devanagari handwritten text\nrecognition (DHTR). This approach uses the Devanagari handwritten character\ndataset (DHCD), an open dataset with 36 classes of Devanagari characters. Each\nof these classes has 1700 images for training and testing purposes. This\napproach obtains promising results in terms of accuracy by achieving 96.36%\naccuracy in testing and 99.55% in training time."}
{"id": "2507.10403", "pdf": "https://arxiv.org/pdf/2507.10403.pdf", "abs": "https://arxiv.org/abs/2507.10403", "title": "Text-to-Remote-Sensing-Image Retrieval beyond RGB Sources", "authors": ["Daniele Rege Cambrin", "Lorenzo Vaiani", "Giuseppe Gallipoli", "Luca Cagliero", "Paolo Garza"], "categories": ["cs.CV", "cs.CL", "cs.IR", "cs.MM"], "comment": null, "summary": "Retrieving relevant imagery from vast satellite archives is crucial for\napplications like disaster response and long-term climate monitoring. However,\nmost text-to-image retrieval systems are limited to RGB data, failing to\nexploit the unique physical information captured by other sensors, such as the\nall-weather structural sensitivity of Synthetic Aperture Radar (SAR) or the\nspectral signatures in optical multispectral data. To bridge this gap, we\nintroduce CrisisLandMark, a new large-scale corpus of over 647,000 Sentinel-1\nSAR and Sentinel-2 multispectral images paired with structured textual\nannotations for land cover, land use, and crisis events harmonized from\nauthoritative land cover systems (CORINE and Dynamic World) and crisis-specific\nsources. We then present CLOSP (Contrastive Language Optical SAR Pretraining),\na novel framework that uses text as a bridge to align unpaired optical and SAR\nimages into a unified embedding space. Our experiments show that CLOSP achieves\na new state-of-the-art, improving retrieval nDGC by 54% over existing models.\nAdditionally, we find that the unified training strategy overcomes the inherent\ndifficulty of interpreting SAR imagery by transferring rich semantic knowledge\nfrom the optical domain with indirect interaction. Furthermore, GeoCLOSP, which\nintegrates geographic coordinates into our framework, creates a powerful\ntrade-off between generality and specificity: while the CLOSP excels at general\nsemantic tasks, the GeoCLOSP becomes a specialized expert for retrieving\nlocation-dependent crisis events and rare geographic features. This work\nhighlights that the integration of diverse sensor data and geographic context\nis essential for unlocking the full potential of remote sensing archives."}
{"id": "2507.10419", "pdf": "https://arxiv.org/pdf/2507.10419.pdf", "abs": "https://arxiv.org/abs/2507.10419", "title": "Multiple Choice Learning of Low Rank Adapters for Language Modeling", "authors": ["Victor Letzelter", "Hugo Malard", "Mathieu Fontaine", "GaÃ«l Richard", "Slim Essid", "Andrei Bursuc", "Patrick PÃ©rez"], "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "We propose LoRA-MCL, a training scheme that extends next-token prediction in\nlanguage models with a method designed to decode diverse, plausible sentence\ncontinuations at inference time. Traditional language modeling is an\nintrinsically ill-posed problem: given a context, multiple futures may be\nequally plausible. Our approach leverages Multiple Choice Learning (MCL) and\nthe Winner-Takes-All (WTA) loss to efficiently handle ambiguity through\nLow-Rank Adaptation (LoRA). We provide a theoretical interpretation of applying\nMultiple Choice Learning to Language Modeling, assuming the data is generated\nfrom a mixture of distributions. To illustrate the proposed approach, we use\ndata sampled from mixtures of Markov chains. We then demonstrate with extensive\nexperiments on real-world visual and audio captioning tasks that our method\nachieves high diversity and relevance in generated outputs."}
{"id": "2507.10522", "pdf": "https://arxiv.org/pdf/2507.10522.pdf", "abs": "https://arxiv.org/abs/2507.10522", "title": "DeepResearch$^{\\text{Eco}}$: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology", "authors": ["Jennifer D'Souza", "Endres Keno Sander", "Andrei Aioanei"], "categories": ["cs.AI", "cs.CL", "cs.MA"], "comment": "12 pages, 3 figures", "summary": "We introduce DeepResearch$^{\\text{Eco}}$, a novel agentic LLM-based system\nfor automated scientific synthesis that supports recursive, depth- and\nbreadth-controlled exploration of original research questions -- enhancing\nsearch diversity and nuance in the retrieval of relevant scientific literature.\nUnlike conventional retrieval-augmented generation pipelines, DeepResearch\nenables user-controllable synthesis with transparent reasoning and\nparameter-driven configurability, facilitating high-throughput integration of\ndomain-specific evidence while maintaining analytical rigor. Applied to 49\necological research questions, DeepResearch achieves up to a 21-fold increase\nin source integration and a 14.9-fold rise in sources integrated per 1,000\nwords. High-parameter settings yield expert-level analytical depth and\ncontextual diversity.\n  Source code available at: https://github.com/sciknoworg/deep-research."}
{"id": "2507.10532", "pdf": "https://arxiv.org/pdf/2507.10532.pdf", "abs": "https://arxiv.org/abs/2507.10532", "title": "Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination", "authors": ["Mingqi Wu", "Zhihao Zhang", "Qiaole Dong", "Zhiheng Xi", "Jun Zhao", "Senjie Jin", "Xiaoran Fan", "Yuhao Zhou", "Yanwei Fu", "Qin Liu", "Songyang Zhang", "Qi Zhang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "26 pages", "summary": "The reasoning capabilities of large language models (LLMs) have been a\nlongstanding focus of research. Recent works have further enhanced these\ncapabilities using reinforcement learning (RL), with many new methods claiming\nsignificant improvements with minimal or no external supervision. Surprisingly,\nsome studies even suggest that random or incorrect reward signals can enhance\nreasoning performance. However, these breakthroughs are mostly reported on the\nQwen2.5 model family and evaluated on well-known benchmarks such as MATH-500,\nAMC, and AIME, while failing to achieve similar gains on other models like\nLlama, which warrants further investigation. Our analysis shows that although\nQwen2.5 achieves strong mathematical reasoning performance, its pretraining on\nlarge-scale web corpora makes it vulnerable to data contamination in popular\nbenchmarks. As a result, results derived from these benchmarks may be\nunreliable. To address this, we introduce a generator that produces fully\nsynthetic arithmetic problems of arbitrary length and difficulty, yielding a\nclean dataset we call RandomCalculation. Using these leakage-free datasets, we\nshow that only accurate reward signals consistently improve performance, while\nnoisy or incorrect signals do not. We advocate for evaluating RL methods on\nuncontaminated benchmarks and across diverse model families to ensure\ntrustworthy conclusions."}
{"id": "2507.10548", "pdf": "https://arxiv.org/pdf/2507.10548.pdf", "abs": "https://arxiv.org/abs/2507.10548", "title": "EmbRACE-3K: Embodied Reasoning and Action in Complex Environments", "authors": ["Mingxian Lin", "Wei Huang", "Yitang Li", "Chengjie Jiang", "Kui Wu", "Fangwei Zhong", "Shengju Qian", "Xin Wang", "Xiaojuan Qi"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Project page: https://mxllc.github.io/EmbRACE-3K/", "summary": "Recent advanced vision-language models(VLMs) have demonstrated strong\nperformance on passive, offline image and video understanding tasks. However,\ntheir effectiveness in embodied settings, which require online interaction and\nactive scene understanding remains limited. In such scenarios, an agent\nperceives the environment from a first-person perspective, with each action\ndynamically shaping subsequent observations. Even state-of-the-art models such\nas GPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro struggle in open-environment\ninteractions, exhibiting clear limitations in spatial reasoning and\nlong-horizon planning. To address this gap, we introduce EmRACE-3K, a dataset\nof over 3,000 language-guided tasks situated in diverse, photorealistic\nenvironments constructed using Unreal Engine and the UnrealCV-Zoo framework.\nThe tasks encompass a wide range of embodied challenges, including navigation,\nobject manipulation, and multi-stage goal execution. Each task unfolds as a\nmulti-step trajectory, pairing first-person visual observations with high-level\ninstructions, grounded actions, and natural language rationales that express\nthe agent's intent at every step. Using EmRACE-3K, we establish a benchmark to\nevaluate the embodied reasoning capabilities of VLMs across three key\ndimensions: Exploration, Dynamic Spatial-Semantic Reasoning, and Multi-stage\nGoal Execution. In zero-shot settings, all models achieve success rates below\n20%, underscoring the challenge posed by our benchmark and the current\nlimitations of VLMs in interactive environments. To demonstrate the utility of\nEmRACE-3K, we further fine-tune Qwen2.5-VL-7B using supervised learning\nfollowed by reinforcement learning. This approach yields substantial\nimprovements across all three challenge categories, highlighting the dataset's\neffectiveness in enabling the development of embodied reasoning capabilities."}
{"id": "2211.14620", "pdf": "https://arxiv.org/pdf/2211.14620.pdf", "abs": "https://arxiv.org/abs/2211.14620", "title": "The distribution of syntactic dependency distances", "authors": ["Sonia Petrini", "Ramon Ferrer-i-Cancho"], "categories": ["cs.CL"], "comment": "minor corrections; in press in Glottometrics", "summary": "The syntactic structure of a sentence can be represented as a graph, where\nvertices are words and edges indicate syntactic dependencies between them. In\nthis setting, the distance between two linked words is defined as the\ndifference between their positions. Here we wish to contribute to the\ncharacterization of the actual distribution of syntactic dependency distances,\nwhich has previously been argued to follow a power-law distribution. Here we\npropose a new model with two exponential regimes in which the probability decay\nis allowed to change after a break-point. This transition could mirror the\ntransition from the processing of word chunks to higher-level structures. We\nfind that a two-regime model - where the first regime follows either an\nexponential or a power-law decay - is the most likely one in all 20 languages\nwe considered, independently of sentence length and annotation style. Moreover,\nthe break-point exhibits low variation across languages and averages values of\n4-5 words, suggesting that the amount of words that can be simultaneously\nprocessed abstracts from the specific language to a high degree. The\nprobability decay slows down after the breakpoint, consistently with a\nuniversal chunk-and-pass mechanism. Finally, we give an account of the relation\nbetween the best estimated model and the closeness of syntactic dependencies as\nfunction of sentence length, according to a recently introduced optimality\nscore."}
{"id": "2310.10873", "pdf": "https://arxiv.org/pdf/2310.10873.pdf", "abs": "https://arxiv.org/abs/2310.10873", "title": "IDEAL: Influence-Driven Selective Annotations Empower In-Context Learners in Large Language Models", "authors": ["Shaokun Zhang", "Xiaobo Xia", "Zhaoqing Wang", "Ling-Hao Chen", "Jiale Liu", "Qingyun Wu", "Tongliang Liu"], "categories": ["cs.CL"], "comment": "Accepted by ICLR 2024", "summary": "In-context learning is a promising paradigm that utilizes in-context examples\nas prompts for the predictions of large language models. These prompts are\ncrucial for achieving strong performance. However, since the prompts need to be\nsampled from a large volume of annotated examples, finding the right prompt may\nresult in high annotation costs. To address this challenge, this paper\nintroduces an influence-driven selective annotation method that aims to\nminimize annotation costs while improving the quality of in-context examples.\nThe essence of our method is to select a pivotal subset from a large-scale\nunlabeled data pool to annotate for the subsequent sampling of prompts.\nSpecifically, a directed graph is first constructed to represent unlabeled\ndata. Afterward, the influence of candidate unlabeled subsets is quantified\nwith a diffusion process. A simple yet effective greedy algorithm for unlabeled\ndata selection is lastly introduced. It iteratively selects the data if it\nprovides a maximum marginal gain with respect to quantified influence. Compared\nwith previous efforts on selective annotations, our influence-driven method\nworks in an end-to-end manner, avoids an intractable explicit balance between\ndata diversity and representativeness, and enjoys theoretical support.\nExperiments confirm the superiority of the proposed method on various\nbenchmarks, achieving better performance under lower time consumption during\nsubset selection. The project page is available at\nhttps://skzhang1.github.io/IDEAL/."}
{"id": "2401.17196", "pdf": "https://arxiv.org/pdf/2401.17196.pdf", "abs": "https://arxiv.org/abs/2401.17196", "title": "Single Word Change is All You Need: Using LLMs to Create Synthetic Training Examples for Text Classifiers", "authors": ["Lei Xu", "Sarah Alnegheimish", "Laure Berti-Equille", "Alfredo Cuesta-Infante", "Kalyan Veeramachaneni"], "categories": ["cs.CL"], "comment": null, "summary": "In text classification, creating an adversarial example means subtly\nperturbing a few words in a sentence without changing its meaning, causing it\nto be misclassified by a classifier. A concerning observation is that a\nsignificant portion of adversarial examples generated by existing methods\nchange only one word. This single-word perturbation vulnerability represents a\nsignificant weakness in classifiers, which malicious users can exploit to\nefficiently create a multitude of adversarial examples. This paper studies this\nproblem and makes the following key contributions: (1) We introduce a novel\nmetric $\\rho$ to quantitatively assess a classifier's robustness against\nsingle-word perturbation. (2) We present the SP-Attack, designed to exploit the\nsingle-word perturbation vulnerability, achieving a higher attack success rate,\nbetter preserving sentence meaning, while reducing computation costs compared\nto state-of-the-art adversarial methods. (3) We propose SP-Defense, which aims\nto improve \\r{ho} by applying data augmentation in learning. Experimental\nresults on 4 datasets and BERT and distilBERT classifiers show that SP-Defense\nimproves $\\rho$ by 14.6% and 13.9% and decreases the attack success rate of\nSP-Attack by 30.4% and 21.2% on two classifiers respectively, and decreases the\nattack success rate of existing attack methods that involve multiple-word\nperturbations."}
{"id": "2402.07577", "pdf": "https://arxiv.org/pdf/2402.07577.pdf", "abs": "https://arxiv.org/abs/2402.07577", "title": "Topic Modeling as Multi-Objective Contrastive Optimization", "authors": ["Thong Nguyen", "Xiaobao Wu", "Xinshuai Dong", "Cong-Duy T Nguyen", "See-Kiong Ng", "Anh Tuan Luu"], "categories": ["cs.CL"], "comment": "Accepted at ICLR 2024 (poster). Official version available at:\n  https://openreview.net/forum?id=HdAoLSBYXj", "summary": "Recent representation learning approaches enhance neural topic models by\noptimizing the weighted linear combination of the evidence lower bound (ELBO)\nof the log-likelihood and the contrastive learning objective that contrasts\npairs of input documents. However, document-level contrastive learning might\ncapture low-level mutual information, such as word ratio, which disturbs topic\nmodeling. Moreover, there is a potential conflict between the ELBO loss that\nmemorizes input details for better reconstruction quality, and the contrastive\nloss which attempts to learn topic representations that generalize among input\ndocuments. To address these issues, we first introduce a novel contrastive\nlearning method oriented towards sets of topic vectors to capture useful\nsemantics that are shared among a set of input documents. Secondly, we\nexplicitly cast contrastive topic modeling as a gradient-based multi-objective\noptimization problem, with the goal of achieving a Pareto stationary solution\nthat balances the trade-off between the ELBO and the contrastive objective.\nExtensive experiments demonstrate that our framework consistently produces\nhigher-performing neural topic models in terms of topic coherence, topic\ndiversity, and downstream performance."}
{"id": "2402.11347", "pdf": "https://arxiv.org/pdf/2402.11347.pdf", "abs": "https://arxiv.org/abs/2402.11347", "title": "SEE: Strategic Exploration and Exploitation for Cohesive In-Context Prompt Optimization", "authors": ["Wendi Cui", "Zhuohang Li", "Hao Sun", "Damien Lopez", "Kamalika Das", "Bradley Malin", "Sricharan Kumar", "Jiaxin Zhang"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Main Conference)", "summary": "Designing optimal prompts for Large Language Models (LLMs) is a complicated\nand resource-intensive task, often requiring substantial human expertise and\neffort. Existing approaches typically separate the optimization of prompt\ninstructions and in-context learning examples, leading to incohesive prompts\nthat are defined and represented by suboptimal task performance. To overcome\nthese challenges, we propose a novel Cohesive In-Context Prompt Optimization\nframework that refines both prompt instructions and examples. However,\nformulating such an optimization in the discrete and high-dimensional space of\nnatural language poses significant challenges in both convergence and\ncomputational efficiency. To address these issues, we introduce SEE, a scalable\nand efficient prompt optimization framework that adopts metaheuristic\noptimization principles and strategically balances exploration and exploitation\nto enhance optimization performance and achieve efficient convergence. SEE\nfeatures a quad-phased design that alternates between global traversal\n(exploration) and local optimization (exploitation) and adaptively chooses LLM\noperators during the optimization process. We have conducted a comprehensive\nevaluation across 35 benchmark tasks, and SEE significantly outperforms\nstate-of-the-art baseline methods by a large margin, achieving an average\nperformance gain of 13.94 while reducing computational costs by 58.67."}
{"id": "2403.04963", "pdf": "https://arxiv.org/pdf/2403.04963.pdf", "abs": "https://arxiv.org/abs/2403.04963", "title": "An In-depth Evaluation of Large Language Models in Sentence Simplification with Error-based Human Assessment", "authors": ["Xuanxin Wu", "Yuki Arase"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACM Transactions on Intelligent Systems and Technology.\n  Our human evaluation corpus is available at:\n  https://github.com/WuXuanxin/human-eval-llm-simplification", "summary": "Recent studies have used both automatic metrics and human evaluations to\nassess the simplification abilities of LLMs. However, the suitability of\nexisting evaluation methodologies for LLMs remains in question. First, the\nsuitability of current automatic metrics on LLMs' simplification evaluation is\nstill uncertain. Second, current human evaluation approaches in sentence\nsimplification often fall into two extremes: they are either too superficial,\nfailing to offer a clear understanding of the models' performance, or overly\ndetailed, making the annotation process complex and prone to inconsistency,\nwhich in turn affects the evaluation's reliability. To address these problems,\nthis study provides in-depth insights into LLMs' performance while ensuring the\nreliability of the evaluation. We design an error-based human annotation\nframework to assess the LLMs' simplification capabilities. We select both\nclosed-source and open-source LLMs, including GPT-4, Qwen2.5-72B, and\nLlama-3.2-3B. We believe that these models offer a representative selection\nacross large, medium, and small sizes of LLMs. Results show that LLMs generally\ngenerate fewer erroneous simplification outputs compared to the previous\nstate-of-the-art. However, LLMs have their limitations, as seen in GPT-4's and\nQwen2.5-72B's struggle with lexical paraphrasing. Furthermore, we conduct\nmeta-evaluations on widely used automatic metrics using our human annotations.\nWe find that these metrics lack sufficient sensitivity to assess the overall\nhigh-quality simplifications, particularly those generated by high-performance\nLLMs."}
{"id": "2404.03353", "pdf": "https://arxiv.org/pdf/2404.03353.pdf", "abs": "https://arxiv.org/abs/2404.03353", "title": "Towards Pareto Optimal Throughput in Small Language Model Serving", "authors": ["Pol G. Recasens", "Yue Zhu", "Chen Wang", "Eun Kyung Lee", "Olivier Tardieu", "Alaa Youssef", "Jordi Torres", "Josep Ll. Berral"], "categories": ["cs.CL"], "comment": "Revised version of the paper published at EuroMLSys'24", "summary": "Large language models (LLMs) have revolutionized the state-of-the-art of many\ndifferent natural language processing tasks. Although serving LLMs is\ncomputationally and memory demanding, the rise of Small Language Models (SLMs)\noffers new opportunities for resource-constrained users, who now are able to\nserve small models with cutting-edge performance. In this paper, we present a\nset of experiments designed to benchmark SLM inference at performance and\nenergy levels. Our analysis provides a new perspective in serving, highlighting\nthat the small memory footprint of SLMs allows for reaching the Pareto-optimal\nthroughput within the resource capacity of a single accelerator. In this\nregard, we present an initial set of findings demonstrating how model\nreplication can effectively improve resource utilization for serving SLMs."}
{"id": "2405.11870", "pdf": "https://arxiv.org/pdf/2405.11870.pdf", "abs": "https://arxiv.org/abs/2405.11870", "title": "Intuitive Fine-Tuning: Towards Simplifying Alignment into a Single Process", "authors": ["Ermo Hua", "Biqing Qi", "Kaiyan Zhang", "Kai Tian", "Xingtai Lv", "Ning Ding", "Bowen Zhou"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025, Oral & Panel Discussion", "summary": "Supervised Fine-Tuning (SFT) and Preference Optimization (PO) are key\nprocesses for aligning Language Models (LMs) with human preferences post\npre-training. While SFT excels in efficiency and PO in effectiveness, they are\noften combined sequentially without integrating their optimization objectives.\nThis approach ignores the opportunities to bridge their paradigm gap and take\nthe strengths from both. In this paper, we interpret SFT and PO with two\nsub-processes -- Preference Estimation and Transition Optimization -- defined\nat token level within the Markov Decision Process (MDP). This modeling shows\nthat SFT is only a special case of PO with inferior estimation and\noptimization. PO estimates the model's preference by its entire generation,\nwhile SFT only scores model's subsequent predicted tokens based on prior tokens\nfrom ground truth answer. These priors deviates from model's distribution,\nhindering the preference estimation and transition optimization. Building on\nthis view, we introduce Intuitive Fine-Tuning (IFT) to integrate SFT and PO\ninto a single process. Through a temporal residual connection, IFT brings\nbetter estimation and optimization by capturing LMs' intuitive sense of its\nentire answers. But it solely relies on a single policy and the same volume of\nnon-preference-labeled data as SFT. Our experiments show that IFT performs\ncomparably or even superiorly to SFT and some typical PO methods across several\ntasks, particularly those require generation, reasoning, and fact-following\nabilities. An explainable Frozen Lake game further validates the effectiveness\nof IFT for getting competitive policy."}
{"id": "2408.11415", "pdf": "https://arxiv.org/pdf/2408.11415.pdf", "abs": "https://arxiv.org/abs/2408.11415", "title": "Political Bias in LLMs: Unaligned Moral Values in Agent-centric Simulations", "authors": ["Simon MÃ¼nker"], "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 2 tables", "summary": "Contemporary research in social sciences increasingly utilizes\nstate-of-the-art generative language models to annotate or generate content.\nWhile these models achieve benchmark-leading performance on common language\ntasks, their application to novel out-of-domain tasks remains insufficiently\nexplored. To address this gap, we investigate how personalized language models\nalign with human responses on the Moral Foundation Theory Questionnaire. We\nadapt open-source generative language models to different political personas\nand repeatedly survey these models to generate synthetic data sets where\nmodel-persona combinations define our sub-populations. Our analysis reveals\nthat models produce inconsistent results across multiple repetitions, yielding\nhigh response variance. Furthermore, the alignment between synthetic data and\ncorresponding human data from psychological studies shows a weak correlation,\nwith conservative persona-prompted models particularly failing to align with\nactual conservative populations. These results suggest that language models\nstruggle to coherently represent ideologies through in-context prompting due to\ntheir alignment process. Thus, using language models to simulate social\ninteractions requires measurable improvements in in-context optimization or\nparameter manipulation to align with psychological and sociological stereotypes\nproperly."}
{"id": "2409.01389", "pdf": "https://arxiv.org/pdf/2409.01389.pdf", "abs": "https://arxiv.org/abs/2409.01389", "title": "CV-Probes: Studying the interplay of lexical and world knowledge in visually grounded verb understanding", "authors": ["Ivana BeÅovÃ¡", "Michal Gregor", "Albert Gatt"], "categories": ["cs.CL"], "comment": "9 pages, 2 figure, 6 tables, CogSci conference 2025", "summary": "How do vision-language (VL) transformer models ground verb phrases and do\nthey integrate contextual and world knowledge in this process? We introduce the\nCV-Probes dataset, containing image-caption pairs involving verb phrases that\nrequire both social knowledge and visual context to interpret (e.g., \"beg\"), as\nwell as pairs involving verb phrases that can be grounded based on information\ndirectly available in the image (e.g., \"sit\"). We show that VL models struggle\nto ground VPs that are strongly context-dependent. Further analysis using\nexplainable AI techniques shows that such models may not pay sufficient\nattention to the verb token in the captions. Our results suggest a need for\nimproved methodologies in VL model training and evaluation. The code and\ndataset will be available https://github.com/ivana-13/CV-Probes."}
{"id": "2409.05137", "pdf": "https://arxiv.org/pdf/2409.05137.pdf", "abs": "https://arxiv.org/abs/2409.05137", "title": "READoc: A Unified Benchmark for Realistic Document Structured Extraction", "authors": ["Zichao Li", "Aizier Abulaiti", "Yaojie Lu", "Xuanang Chen", "Jia Zheng", "Hongyu Lin", "Xianpei Han", "Le Sun"], "categories": ["cs.CL", "cs.CV"], "comment": "ACL 2025 Findings", "summary": "Document Structured Extraction (DSE) aims to extract structured content from\nraw documents. Despite the emergence of numerous DSE systems, their unified\nevaluation remains inadequate, significantly hindering the field's advancement.\nThis problem is largely attributed to existing benchmark paradigms, which\nexhibit fragmented and localized characteristics. To address these limitations\nand offer a thorough evaluation of DSE systems, we introduce a novel benchmark\nnamed READoc, which defines DSE as a realistic task of converting unstructured\nPDFs into semantically rich Markdown. The READoc dataset is derived from 3,576\ndiverse and real-world documents from arXiv, GitHub, and Zenodo. In addition,\nwe develop a DSE Evaluation S$^3$uite comprising Standardization, Segmentation\nand Scoring modules, to conduct a unified evaluation of state-of-the-art DSE\napproaches. By evaluating a range of pipeline tools, expert visual models, and\ngeneral VLMs, we identify the gap between current work and the unified,\nrealistic DSE objective for the first time. We aspire that READoc will catalyze\nfuture research in DSE, fostering more comprehensive and practical solutions."}
{"id": "2409.13748", "pdf": "https://arxiv.org/pdf/2409.13748.pdf", "abs": "https://arxiv.org/abs/2409.13748", "title": "TheraGen: Therapy for Every Generation", "authors": ["Kartikey Doshi", "Jimit Shah", "Narendra Shekokar"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "This paper contains major errors in methodology and results. It\n  should not be cited", "summary": "We present TheraGen, an advanced AI-powered mental health chatbot utilizing\nthe LLaMA 2 7B model. This approach builds upon recent advancements in language\nmodels and transformer architectures. TheraGen provides all-day personalized,\ncompassionate mental health care by leveraging a large dataset of 1 million\nconversational entries, combining anonymized therapy transcripts, online mental\nhealth discussions, and psychological literature, including APA resources. Our\nimplementation employs transfer learning, fine-tuning, and advanced training\ntechniques to optimize performance. TheraGen offers a user-friendly interface\nfor seamless interaction, providing empathetic responses and evidence-based\ncoping strategies. Evaluation results demonstrate high user satisfaction rates,\nwith 94% of users reporting improved mental well-being. The system achieved a\nBLEU score of 0.67 and a ROUGE score of 0.62, indicating strong response\naccuracy. With an average response time of 1395 milliseconds, TheraGen ensures\nreal-time, efficient support. While not a replacement for professional therapy,\nTheraGen serves as a valuable complementary tool, significantly improving user\nwell-being and addressing the accessibility gap in mental health treatments.\nThis paper details TheraGen's architecture, training methodology, ethical\nconsiderations, and future directions, contributing to the growing field of\nAI-assisted mental healthcare and offering a scalable solution to the pressing\nneed for mental health support."}
{"id": "2410.12380", "pdf": "https://arxiv.org/pdf/2410.12380.pdf", "abs": "https://arxiv.org/abs/2410.12380", "title": "Evaluation of Attribution Bias in Generator-Aware Retrieval-Augmented Large Language Models", "authors": ["Amin Abolghasemi", "Leif Azzopardi", "Seyyed Hadi Hashemi", "Maarten de Rijke", "Suzan Verberne"], "categories": ["cs.CL"], "comment": "Accepted at ACL 2025 (Findings)", "summary": "Attributing answers to source documents is an approach used to enhance the\nverifiability of a model's output in retrieval augmented generation (RAG).\nPrior work has mainly focused on improving and evaluating the attribution\nquality of large language models (LLMs) in RAG, but this may come at the\nexpense of inducing biases in the attribution of answers. We define and examine\ntwo aspects in the evaluation of LLMs in RAG pipelines, namely attribution\nsensitivity and bias with respect to authorship information. We explicitly\ninform an LLM about the authors of source documents, instruct it to attribute\nits answers, and analyze (i) how sensitive the LLM's output is to the author of\nsource documents, and (ii) whether the LLM exhibits a bias towards\nhuman-written or AI-generated source documents. We design an experimental setup\nin which we use counterfactual evaluation to study three LLMs in terms of their\nattribution sensitivity and bias in RAG pipelines. Our results show that adding\nauthorship information to source documents can significantly change the\nattribution quality of LLMs by 3% to 18%. Moreover, we show that LLMs can have\nan attribution bias towards explicit human authorship, which can serve as a\ncompeting hypothesis for findings of prior work that shows that LLM-generated\ncontent may be preferred over human-written contents. Our findings indicate\nthat metadata of source documents can influence LLMs' trust, and how they\nattribute their answers. Furthermore, our research highlights attribution bias\nand sensitivity as a novel aspect of brittleness in LLMs."}
{"id": "2411.00027", "pdf": "https://arxiv.org/pdf/2411.00027.pdf", "abs": "https://arxiv.org/abs/2411.00027", "title": "Personalization of Large Language Models: A Survey", "authors": ["Zhehao Zhang", "Ryan A. Rossi", "Branislav Kveton", "Yijia Shao", "Diyi Yang", "Hamed Zamani", "Franck Dernoncourt", "Joe Barrow", "Tong Yu", "Sungchul Kim", "Ruiyi Zhang", "Jiuxiang Gu", "Tyler Derr", "Hongjie Chen", "Junda Wu", "Xiang Chen", "Zichao Wang", "Subrata Mitra", "Nedim Lipka", "Nesreen Ahmed", "Yu Wang"], "categories": ["cs.CL"], "comment": "Accepted at the Transactions on Machine Learning Research (TMLR)\n  journal", "summary": "Personalization of Large Language Models (LLMs) has recently become\nincreasingly important with a wide range of applications. Despite the\nimportance and recent progress, most existing works on personalized LLMs have\nfocused either entirely on (a) personalized text generation or (b) leveraging\nLLMs for personalization-related downstream applications, such as\nrecommendation systems. In this work, we bridge the gap between these two\nseparate main directions for the first time by introducing a taxonomy for\npersonalized LLM usage and summarizing the key differences and challenges. We\nprovide a formalization of the foundations of personalized LLMs that\nconsolidates and expands notions of personalization of LLMs, defining and\ndiscussing novel facets of personalization, usage, and desiderata of\npersonalized LLMs. We then unify the literature across these diverse fields and\nusage scenarios by proposing systematic taxonomies for the granularity of\npersonalization, personalization techniques, datasets, evaluation methods, and\napplications of personalized LLMs. Finally, we highlight challenges and\nimportant open problems that remain to be addressed. By unifying and surveying\nrecent research using the proposed taxonomies, we aim to provide a clear guide\nto the existing literature and different facets of personalization in LLMs,\nempowering both researchers and practitioners."}
{"id": "2411.07533", "pdf": "https://arxiv.org/pdf/2411.07533.pdf", "abs": "https://arxiv.org/abs/2411.07533", "title": "Large Language Models as Neurolinguistic Subjects: Discrepancy between Performance and Competence", "authors": ["Linyang He", "Ercong Nie", "Helmut Schmid", "Hinrich SchÃ¼tze", "Nima Mesgarani", "Jonathan Brennan"], "categories": ["cs.CL"], "comment": null, "summary": "This study investigates the linguistic understanding of Large Language Models\n(LLMs) regarding signifier (form) and signified (meaning) by distinguishing two\nLLM assessment paradigms: psycholinguistic and neurolinguistic. Traditional\npsycholinguistic evaluations often reflect statistical rules that may not\naccurately represent LLMs' true linguistic competence. We introduce a\nneurolinguistic approach, utilizing a novel method that combines minimal pair\nand diagnostic probing to analyze activation patterns across model layers. This\nmethod allows for a detailed examination of how LLMs represent form and\nmeaning, and whether these representations are consistent across languages. We\nfound: (1) Psycholinguistic and neurolinguistic methods reveal that language\nperformance and competence are distinct; (2) Direct probability measurement may\nnot accurately assess linguistic competence; (3) Instruction tuning won't\nchange much competence but improve performance; (4) LLMs exhibit higher\ncompetence and performance in form compared to meaning. Additionally, we\nintroduce new conceptual minimal pair datasets for Chinese (COMPS-ZH) and\nGerman (COMPS-DE), complementing existing English datasets."}
{"id": "2411.07611", "pdf": "https://arxiv.org/pdf/2411.07611.pdf", "abs": "https://arxiv.org/abs/2411.07611", "title": "Knowledge-Augmented Multimodal Clinical Rationale Generation for Disease Diagnosis with Small Language Models", "authors": ["Shuai Niu", "Jing Ma", "Hongzhan Lin", "Liang Bai", "Zhihua Wang", "Yida Xu", "Yunya Song", "Xian Yang"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "13 pages. 7 figures", "summary": "Interpretation is critical for disease diagnosis, but existing models\nstruggle to balance predictive accuracy with human-understandable rationales.\nWhile large language models (LLMs) offer strong reasoning abilities, their\nclinical use is limited by high computational costs and restricted multimodal\nreasoning ability. Small language models (SLMs) are efficient but lack advanced\nreasoning for integrating multimodal medical data. In addition, both LLMs and\nSLMs lack domain knowledge for trustworthy reasoning. Therefore, we propose\nClinRaGen, enhancing SLMs by leveraging LLM-derived reasoning ability via\nrationale distillation and domain knowledge injection for trustworthy\nmultimodal rationale generation. Key innovations include a sequential rationale\ndistillation framework that equips SLMs with LLM-comparable multimodal\nreasoning abilities, and a knowledge-augmented attention mechanism that jointly\nunifies multimodal representation from time series and textual data in the same\nencoding space, enabling it to be naturally interpreted by SLMs while\nincorporating domain knowledge for reliable rationale generation. Experiments\non real-world medical datasets show that ClinRaGen achieves state-of-the-art\nperformance in disease diagnosis and rationale generation, demonstrating the\neffectiveness of combining LLM-driven reasoning with knowledge augmentation for\nimproved interpretability."}
{"id": "2411.13820", "pdf": "https://arxiv.org/pdf/2411.13820.pdf", "abs": "https://arxiv.org/abs/2411.13820", "title": "InstCache: A Predictive Cache for LLM Serving", "authors": ["Longwei Zou", "Yan Liu", "Jiamu Kang", "Tingfeng Liu", "Jiangang Kong", "Yangdong Deng"], "categories": ["cs.CL", "cs.DC"], "comment": null, "summary": "The revolutionary capabilities of Large Language Models (LLMs) are attracting\nrapidly growing popularity and leading to soaring user requests to inference\nserving systems. Caching techniques, which leverage data reuse to reduce\ncomputation, offer opportunities to optimize the performance of LLM inference\nengines. On the one hand, the low-level key-value (KV) cache working at the\ntoken level is widely adopted, albeit it incurs significant overhead as request\nvolume grows. On the other hand, instruction-level caching, which stores full\ninstruction-response pairs, is expected to play an increasingly crucial role.\nHowever, the high variability in the content and length of instructions make it\nrare for identical instructions to recur within a short time window, presenting\nchallenges for effective caching instruction-response pairs. To address this\nchallenge, we propose InstCache, a predictive caching mechanism for LLM serving\nsystems. Leveraging the capability of LLMs, we can effectively reorder the\nrepresentation space of instruction texts and develop a sufficient level of\nspatial locality. Such spatial locality enables us to predict potential\ninstructions located in a compact region in the space, resulting in an\neffective caching system at runtime. Experimental results demonstrate that\nInstCache achieves a 2.3x higher hit rate compared to the upper bound of\ntraditional caching mechanisms on WildChat dataset and reduces the time per\noutput token of vLLM by up to 42.0% and 50.0% on LMSys and Moss datasets,\nrespectively."}
{"id": "2412.00947", "pdf": "https://arxiv.org/pdf/2412.00947.pdf", "abs": "https://arxiv.org/abs/2412.00947", "title": "VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information", "authors": ["Ryo Kamoi", "Yusen Zhang", "Sarkar Snigdha Sarathi Das", "Ranran Haoran Zhang", "Rui Zhang"], "categories": ["cs.CL", "cs.CV"], "comment": "COLM 2025. VisOnlyQA dataset, code, and model responses are provided\n  at https://github.com/psunlpgroup/VisOnlyQA. Please also refer to our project\n  website at https://visonlyqa.github.io/", "summary": "Large Vision Language Models (LVLMs) have achieved remarkable performance in\nvarious vision-language tasks. However, it is still unclear how accurately\nLVLMs can perceive visual information in images. In particular, the capability\nof LVLMs to perceive geometric information, such as shape, angle, and size,\nremains insufficiently analyzed, although the perception of these properties is\ncrucial for tasks that require a detailed visual understanding. In this work,\nwe introduce VisOnlyQA, a dataset for evaluating the geometric perception of\nLVLMs, and reveal that LVLMs often cannot accurately perceive basic geometric\ninformation in images, while human performance is nearly perfect. VisOnlyQA\nconsists of 12 tasks that directly ask about geometric information in geometric\nshapes, charts, chemical structures, and 3D shapes. Our experiments highlight\nthe following findings: (i) State-of-the-art LVLMs struggle with basic\ngeometric perception. 23 LVLMs we evaluate, including GPT-4o and Gemini 2.5\nPro, work poorly on VisOnlyQA. (ii) Additional training data does not resolve\nthis issue. Fine-tuning on the training set of VisOnlyQA is not always\neffective, even for in-distribution tasks. (iii) LLM may be the bottleneck.\nLVLMs using stronger LLMs exhibit better geometric perception on VisOnlyQA,\nwhile it does not require complex reasoning, suggesting that the way LVLMs\nprocess information from visual encoders is a bottleneck. The datasets, code,\nand model responses are provided at https://github.com/psunlpgroup/VisOnlyQA."}
{"id": "2412.05225", "pdf": "https://arxiv.org/pdf/2412.05225.pdf", "abs": "https://arxiv.org/abs/2412.05225", "title": "BEExformer: A Fast Inferencing Binarized Transformer with Early Exits", "authors": ["Wazib Ansar", "Saptarsi Goswami", "Amlan Chakrabarti"], "categories": ["cs.CL", "cs.AI", "cs.NE"], "comment": "This revised manuscript includes 18 pages, 17 figures, and 6 tables.\n  Methodology and results sections have been improved for clarity and depth,\n  incorporating additional comparisons, ablations, and a new evaluation\n  dataset. A few relevant references were added, and overall organization\n  refined for better readability", "summary": "Large Language Models (LLMs) based on transformers achieve cutting-edge\nresults on a variety of applications. However, their enormous size and\nprocessing requirements hinder deployment on constrained resources. To enhance\nefficiency, binarization and Early Exit (EE) have proved to be effective\nsolutions. However, binarization may lead to performance loss as reduced\nprecision affects gradient estimation and parameter updates. Besides, research\non EE mechanisms is still in its early stages. To address these challenges, we\nintroduce Binarized Early Exit Transformer (BEExformer), the first-ever\nselective learning-based transformer integrating Binarization-Aware Training\n(BAT) with EE for efficient and fast textual inference. Each transformer block\nhas an integrated Selective-Learn Forget Network (SLFN) to enhance contextual\nretention while eliminating irrelevant information. The BAT employs a\ndifferentiable second-order approximation to the sign function, enabling\ngradient computation that captures both the sign and magnitude of the weights.\nThis aids in 21.30 times reduction in model size. The EE mechanism hinges on\nfractional reduction in entropy among intermediate transformer blocks with\nsoft-routing loss estimation. This accelerates inference by reducing FLOPs by\n52.08% and even improves accuracy by 2.89% by resolving the \"overthinking\"\nproblem inherent in deep networks. Extensive evaluation through comparison with\nthe SOTA methods and various ablations across six datasets covering multiple\nNLP tasks demonstrates its Pareto-optimal performance-efficiency trade-off."}
{"id": "2412.08985", "pdf": "https://arxiv.org/pdf/2412.08985.pdf", "abs": "https://arxiv.org/abs/2412.08985", "title": "KnowShiftQA: How Robust are RAG Systems when Textbook Knowledge Shifts in K-12 Education?", "authors": ["Tianshi Zheng", "Weihan Li", "Jiaxin Bai", "Weiqi Wang", "Yangqiu Song"], "categories": ["cs.CL"], "comment": "ACL 2025 Main", "summary": "Retrieval-Augmented Generation (RAG) systems show remarkable potential as\nquestion answering tools in the K-12 Education domain, where knowledge is\ntypically queried within the restricted scope of authoritative textbooks.\nHowever, discrepancies between these textbooks and the parametric knowledge\ninherent in Large Language Models (LLMs) can undermine the effectiveness of RAG\nsystems. To systematically investigate RAG system robustness against such\nknowledge discrepancies, we introduce KnowShiftQA. This novel question\nanswering dataset simulates these discrepancies by applying deliberate\nhypothetical knowledge updates to both answers and source documents, reflecting\nhow textbook knowledge can shift. KnowShiftQA comprises 3,005 questions across\nfive subjects, designed with a comprehensive question typology focusing on\ncontext utilization and knowledge integration. Our extensive experiments on\nretrieval and question answering performance reveal that most RAG systems\nsuffer a substantial performance drop when faced with these knowledge\ndiscrepancies. Furthermore, questions requiring the integration of contextual\n(textbook) knowledge with parametric (LLM) knowledge pose a significant\nchallenge to current LLMs."}
{"id": "2501.03940", "pdf": "https://arxiv.org/pdf/2501.03940.pdf", "abs": "https://arxiv.org/abs/2501.03940", "title": "Not all tokens are created equal: Perplexity Attention Weighted Networks for AI generated text detection", "authors": ["Pablo Miralles-GonzÃ¡lez", "Javier Huertas-Tato", "Alejandro MartÃ­n", "David Camacho"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages."}
{"id": "2501.12851", "pdf": "https://arxiv.org/pdf/2501.12851.pdf", "abs": "https://arxiv.org/abs/2501.12851", "title": "ACEBench: Who Wins the Match Point in Tool Usage?", "authors": ["Chen Chen", "Xinlong Hao", "Weiwen Liu", "Xu Huang", "Xingshan Zeng", "Shuai Yu", "Dexun Li", "Shuai Wang", "Weinan Gan", "Yuefeng Huang", "Wulong Liu", "Xinzhi Wang", "Defu Lian", "Baoqun Yin", "Yasheng Wang", "Wu Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated significant potential in\ndecision-making and reasoning, particularly when integrated with various tools\nto effectively solve complex problems. However, existing benchmarks for\nevaluating LLMs' tool usage face several limitations: (1) limited evaluation\nscenarios, often lacking assessments in real multi-turn dialogue contexts; (2)\nnarrow evaluation dimensions, with insufficient detailed assessments of how\nLLMs use tools; and (3) reliance on LLMs or real API executions for evaluation,\nwhich introduces significant overhead. To address these challenges, we\nintroduce ACEBench, a comprehensive benchmark for assessing tool usage in LLMs.\nACEBench categorizes data into three primary types based on evaluation\nmethodology: Normal, Special, and Agent. \"Normal\" evaluates tool usage in basic\nscenarios; \"Special\" evaluates tool usage in situations with ambiguous or\nincomplete instructions; \"Agent\" evaluates tool usage through multi-agent\ninteractions to simulate real-world, multi-turn dialogues. We conducted\nextensive experiments using ACEBench, analyzing various LLMs in-depth and\nproviding a more granular examination of error causes across different data\ntypes."}
{"id": "2502.07776", "pdf": "https://arxiv.org/pdf/2502.07776.pdf", "abs": "https://arxiv.org/abs/2502.07776", "title": "Auditing Prompt Caching in Language Model APIs", "authors": ["Chenchen Gu", "Xiang Lisa Li", "Rohith Kuditipudi", "Percy Liang", "Tatsunori Hashimoto"], "categories": ["cs.CL", "cs.CR", "cs.LG"], "comment": "Accepted at ICML 2025", "summary": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known."}
{"id": "2502.12992", "pdf": "https://arxiv.org/pdf/2502.12992.pdf", "abs": "https://arxiv.org/abs/2502.12992", "title": "B-cos LM: Efficiently Transforming Pre-trained Language Models for Improved Explainability", "authors": ["Yifan Wang", "Sukrut Rao", "Ji-Ung Lee", "Mayank Jobanputra", "Vera Demberg"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Post-hoc explanation methods for black-box models often struggle with\nfaithfulness and human interpretability due to the lack of explainability in\ncurrent neural architectures. Meanwhile, B-cos networks have been introduced to\nimprove model explainability by proposing an architecture that removes bias\nterms and promotes input-weight alignment. Although B-cos networks have shown\nsuccess in building explainable systems, their application has so far been\nlimited to computer vision models and their associated training pipelines. In\nthis work, we introduce B-cos LMs, i.e., B-cos language models (LMs) empowered\nfor natural language processing (NLP) tasks. Our approach directly transforms\npre-trained language models into B-cos LMs by combining B-cos conversion and\ntask fine-tuning, improving efficiency compared to previous methods. Our\nautomatic and human evaluation results demonstrate that B-cos LMs produce more\nfaithful and human interpretable explanations than post-hoc methods, while\nmaintaining task performance comparable to conventional fine-tuning. Our\nin-depth analysis explores how B-cos LMs differ from conventionally fine-tuned\nmodels in their learning processes and explanation patterns. Finally, we are\nalso the first to explore the transformation of decoder-only models to B-cos\nLMs for generation tasks."}
{"id": "2502.13640", "pdf": "https://arxiv.org/pdf/2502.13640.pdf", "abs": "https://arxiv.org/abs/2502.13640", "title": "Qorgau: Evaluating LLM Safety in Kazakh-Russian Bilingual Contexts", "authors": ["Maiya Goloburda", "Nurkhan Laiyk", "Diana Turmakhan", "Yuxia Wang", "Mukhammed Togmanov", "Jonibek Mansurov", "Askhat Sametov", "Nurdaulet Mukhituly", "Minghan Wang", "Daniil Orel", "Zain Muhammad Mujahid", "Fajri Koto", "Timothy Baldwin", "Preslav Nakov"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are known to have the potential to generate\nharmful content, posing risks to users. While significant progress has been\nmade in developing taxonomies for LLM risks and safety evaluation prompts, most\nstudies have focused on monolingual contexts, primarily in English. However,\nlanguage- and region-specific risks in bilingual contexts are often overlooked,\nand core findings can diverge from those in monolingual settings. In this\npaper, we introduce Qorgau, a novel dataset specifically designed for safety\nevaluation in Kazakh and Russian, reflecting the unique bilingual context in\nKazakhstan, where both Kazakh (a low-resource language) and Russian (a\nhigh-resource language) are spoken. Experiments with both multilingual and\nlanguage-specific LLMs reveal notable differences in safety performance,\nemphasizing the need for tailored, region-specific datasets to ensure the\nresponsible and safe deployment of LLMs in countries like Kazakhstan. Warning:\nthis paper contains example data that may be offensive, harmful, or biased."}
{"id": "2502.18448", "pdf": "https://arxiv.org/pdf/2502.18448.pdf", "abs": "https://arxiv.org/abs/2502.18448", "title": "Disambiguate First, Parse Later: Generating Interpretations for Ambiguity Resolution in Semantic Parsing", "authors": ["Irina Saparina", "Mirella Lapata"], "categories": ["cs.CL", "cs.AI"], "comment": "Findings of ACL 2025", "summary": "Handling ambiguity and underspecification is an important challenge in\nnatural language interfaces, particularly for tasks like text-to-SQL semantic\nparsing. We propose a modular approach that resolves ambiguity using natural\nlanguage interpretations before mapping these to logical forms (e.g., SQL\nqueries). Although LLMs excel at parsing unambiguous utterances, they show\nstrong biases for ambiguous ones, typically predicting only preferred\ninterpretations. We constructively exploit this bias to generate an initial set\nof preferred disambiguations and then apply a specialized infilling model to\nidentify and generate missing interpretations. To train the infilling model, we\nintroduce an annotation method that uses SQL execution to validate different\nmeanings. Our approach improves interpretation coverage and generalizes across\ndatasets with different annotation styles, database structures, and ambiguity\ntypes."}
{"id": "2502.18746", "pdf": "https://arxiv.org/pdf/2502.18746.pdf", "abs": "https://arxiv.org/abs/2502.18746", "title": "A Survey of Automatic Prompt Optimization with Instruction-focused Heuristic-based Search Algorithm", "authors": ["Wendi Cui", "Zhuohang Li", "Hao Sun", "Damien Lopez", "Kamalika Das", "Bradley A. Malin", "Sricharan Kumar", "Jiaxin Zhang"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025", "summary": "Recent advances in Large Language Models have led to remarkable achievements\nacross a variety of Natural Language Processing tasks, making prompt\nengineering increasingly central to guiding model outputs. While manual methods\ncan be effective, they typically rely on intuition and do not automatically\nrefine prompts over time. In contrast, automatic prompt optimization employing\nheuristic-based search algorithms can systematically explore and improve\nprompts with minimal human oversight. This survey proposes a comprehensive\ntaxonomy of these methods, categorizing them by where optimization occurs, what\nis optimized, what criteria drive the optimization, which operators generate\nnew prompts, and which iterative search algorithms are applied. We further\nhighlight specialized datasets and tools that support and accelerate automated\nprompt refinement. We conclude by discussing key open challenges pointing\ntoward future opportunities for more robust and versatile LLM applications."}
{"id": "2503.02240", "pdf": "https://arxiv.org/pdf/2503.02240.pdf", "abs": "https://arxiv.org/abs/2503.02240", "title": "OmniSQL: Synthesizing High-quality Text-to-SQL Data at Scale", "authors": ["Haoyang Li", "Shang Wu", "Xiaokang Zhang", "Xinmei Huang", "Jing Zhang", "Fuxin Jiang", "Shuai Wang", "Tieying Zhang", "Jianjun Chen", "Rui Shi", "Hong Chen", "Cuiping Li"], "categories": ["cs.CL", "cs.DB"], "comment": null, "summary": "Text-to-SQL, the task of translating natural language questions into SQL\nqueries, plays a crucial role in enabling non-experts to interact with\ndatabases. While recent advancements in large language models (LLMs) have\nsignificantly enhanced text-to-SQL performance, existing approaches face\nnotable limitations in real-world text-to-SQL applications. Prompting-based\nmethods often depend on closed-source LLMs, which are expensive, raise privacy\nconcerns, and lack customization. Fine-tuning-based methods, on the other hand,\nsuffer from poor generalizability due to the limited coverage of publicly\navailable training data. To overcome these challenges, we propose a novel and\nscalable text-to-SQL data synthesis framework for automatically synthesizing\nlarge-scale, high-quality, and diverse datasets without extensive human\nintervention. Using this framework, we introduce SynSQL-2.5M, the first\nmillion-scale text-to-SQL dataset, containing 2.5 million samples spanning over\n16,000 synthetic databases. Each sample includes a database, SQL query, natural\nlanguage question, and chain-of-thought (CoT) solution. Leveraging SynSQL-2.5M,\nwe develop OmniSQL, a powerful open-source text-to-SQL model available in three\nsizes: 7B, 14B, and 32B. Extensive evaluations across nine datasets demonstrate\nthat OmniSQL achieves state-of-the-art performance, matching or surpassing\nleading closed-source and open-source LLMs, including GPT-4o and DeepSeek-V3,\ndespite its smaller size. We release all code, datasets, and models to support\nfurther research."}
{"id": "2503.22362", "pdf": "https://arxiv.org/pdf/2503.22362.pdf", "abs": "https://arxiv.org/abs/2503.22362", "title": "Supposedly Equivalent Facts That Aren't? Entity Frequency in Pre-training Induces Asymmetry in LLMs", "authors": ["Yuan He", "Bailan He", "Zifeng Ding", "Alisia Lupidi", "Yuqicheng Zhu", "Shuo Chen", "Caiqi Zhang", "Jiaoyan Chen", "Yunpu Ma", "Volker Tresp", "Ian Horrocks"], "categories": ["cs.CL"], "comment": "Accepted at COLM 2025", "summary": "Understanding and mitigating hallucinations in Large Language Models (LLMs)\nis crucial for ensuring reliable content generation. While previous research\nhas primarily focused on \"when\" LLMs hallucinate, our work explains \"why\" and\ndirectly links model behaviour to the pre-training data that forms their prior\nknowledge. Specifically, we demonstrate that an asymmetry exists in the\nrecognition of logically equivalent facts, which can be attributed to frequency\ndiscrepancies of entities appearing as subjects versus objects. Given that most\npre-training datasets are inaccessible, we leverage the fully open-source OLMo\nseries by indexing its Dolma dataset to estimate entity frequencies. Using\nrelational facts (represented as triples) from Wikidata5M, we construct probing\ndatasets to isolate this effect. Our experiments reveal that facts with a\nhigh-frequency subject and a low-frequency object are better recognised than\ntheir inverse, despite their logical equivalence. The pattern reverses in\nlow-to-high frequency settings, and no statistically significant asymmetry\nemerges when both entities are high-frequency. These findings highlight the\ninfluential role of pre-training data in shaping model predictions and provide\ninsights for inferring the characteristics of pre-training data in closed or\npartially closed LLMs."}
{"id": "2504.02882", "pdf": "https://arxiv.org/pdf/2504.02882.pdf", "abs": "https://arxiv.org/abs/2504.02882", "title": "DiaTool-DPO: Multi-Turn Direct Preference Optimization for Tool-Augmented Large Language Models", "authors": ["Sunghee Jung", "Donghun Lee", "Shinbok Lee", "Gaeun Seo", "Daniel Lee", "Byeongil Ko", "Junrae Cho", "Kihyun Kim", "Eunggyun Kim", "Myeongcheol Shin"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to SIGDIAL 2025", "summary": "Tool-Augmented Larage Language Models (TA-LLMs) have shown promise in\nreal-world applications, but face challenges in handling incomplete queries and\nout-of-scope requests. While existing approaches rely mainly on Supervised\nFine-Tuning with expert trajectories, we propose DiaTool-DPO, a novel method\nthat enhances TA-LLM's dialogue capabilities through Direct Preference\nOptimization. We model TA-LLM interactions as a Markov Decision Process with 5\ndistinct dialogue states and categorize user queries into 3 types based on\ntheir state transition trajectories. We automatically construct paired\ntrajectory datasets of correct and incorrect dialogue flows and introduce a\nspecialized objective loss for dialogue control. Our comprehensive evaluation\ndemonstrates that DiaTool-DPO approaches GPT-4o's performance (94.8% in\ninformation gathering, 91% in tool call rejection) with substantial\nimprovements over baseline (44% and 9.6% respectively) while maintaining core\nfunctionality. Our approach opens new possibilities for developing TA-LLMs that\ncan handle diverse real-world scenarios without requiring additional expert\ndemonstrations or human labeling."}
{"id": "2504.11183", "pdf": "https://arxiv.org/pdf/2504.11183.pdf", "abs": "https://arxiv.org/abs/2504.11183", "title": "Bias Beyond English: Evaluating Social Bias and Debiasing Methods in a Low-Resource Setting", "authors": ["Ej Zhou", "Weiming Lu"], "categories": ["cs.CL"], "comment": null, "summary": "Social bias in language models can potentially exacerbate social\ninequalities. Despite it having garnered wide attention, most research focuses\non English data. In a low-resource scenario, the models often perform worse due\nto insufficient training data. This study aims to leverage high-resource\nlanguage corpora to evaluate bias and experiment with debiasing methods in\nlow-resource languages. We evaluated the performance of recent multilingual\nmodels in five languages: English, Chinese, Russian, Indonesian and Thai, and\nanalyzed four bias dimensions: gender, religion, nationality, and race-color.\nBy constructing multilingual bias evaluation datasets, this study allows fair\ncomparisons between models across languages. We have further investigated three\ndebiasing methods-CDA, Dropout, SenDeb-and demonstrated that debiasing methods\nfrom high-resource languages can be effectively transferred to low-resource\nones, providing actionable insights for fairness research in multilingual NLP."}
{"id": "2504.12355", "pdf": "https://arxiv.org/pdf/2504.12355.pdf", "abs": "https://arxiv.org/abs/2504.12355", "title": "Leveraging Large Language Models for Multi-Class and Multi-Label Detection of Drug Use and Overdose Symptoms on Social Media", "authors": ["Muhammad Ahmad", "Fida Ullah", "Ummhy Habiba", "ldar Batyrshin", "Grigori Sidorov"], "categories": ["cs.CL", "cs.AI", "cs.SI"], "comment": null, "summary": "Drug overdose remains a critical global health issue, often driven by misuse\nof opioids, painkillers, and psychiatric medications. Traditional research\nmethods face limitations, whereas social media offers real-time insights into\nself-reported substance use and overdose symptoms. This study proposes an\nAI-driven NLP framework trained on annotated social media data to detect\ncommonly used drugs and associated overdose symptoms. Using a hybrid annotation\nstrategy with LLMs and human annotators, we applied traditional ML models,\nneural networks, and advanced transformer-based models. Our framework achieved\n98% accuracy in multi-class and 97% in multi-label classification,\noutperforming baseline models by up to 8%. These findings highlight the\npotential of AI for supporting public health surveillance and personalized\nintervention strategies."}
{"id": "2504.21018", "pdf": "https://arxiv.org/pdf/2504.21018.pdf", "abs": "https://arxiv.org/abs/2504.21018", "title": "HYPEROFA: Expanding LLM Vocabulary to New Languages via Hypernetwork-Based Embedding Initialization", "authors": ["Enes Ãzeren", "Yihong Liu", "Hinrich SchÃ¼tze"], "categories": ["cs.CL", "cs.LG", "I.2.7"], "comment": "18 pages, 3 figures, 15 tables. After ACL reviews: Corrected typos,\n  Table 4 caption updated and the order of the results changed, numbers are\n  unchanged. This paper will appear in ACL SRW 2025", "summary": "Many pre-trained language models (PLMs) exhibit suboptimal performance on\nmid- and low-resource languages, largely due to limited exposure to these\nlanguages during pre-training. A common strategy to address this is to\nintroduce new tokens specific to the target languages, initialize their\nembeddings, and apply continual pre-training on target-language data. Among\nsuch methods, OFA (Liu et al., 2024a) proposes a similarity-based subword\nembedding initialization heuristic that is both effective and efficient.\nHowever, OFA restricts target-language token embeddings to be convex\ncombinations of a fixed number of source-language embeddings, which may limit\nexpressiveness. To overcome this limitation, we propose HYPEROFA, a\nhypernetwork-based approach for more adaptive token embedding initialization.\nThe hypernetwork is trained to map from an external multilingual word vector\nspace to the PLMs token embedding space using source-language tokens. Once\ntrained, it can generate flexible embeddings for target-language tokens,\nserving as a good starting point for continual pretraining. Experiments\ndemonstrate that HYPEROFA consistently outperforms random initialization\nbaseline and matches or exceeds the performance of OFA in both continual\npre-training convergence and downstream task performance. We make the code\npublicly available."}
{"id": "2505.00268", "pdf": "https://arxiv.org/pdf/2505.00268.pdf", "abs": "https://arxiv.org/abs/2505.00268", "title": "Consistency in Language Models: Current Landscape, Challenges, and Future Directions", "authors": ["Jekaterina Novikova", "Carol Anderson", "Borhane Blili-Hamelin", "Domenic Rosati", "Subhabrata Majumdar"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted in ICML 2025 Workshop on Reliable and Responsible Foundation\n  Models", "summary": "The hallmark of effective language use lies in consistency: expressing\nsimilar meanings in similar contexts and avoiding contradictions. While human\ncommunication naturally demonstrates this principle, state-of-the-art language\nmodels (LMs) struggle to maintain reliable consistency across task- and\ndomain-specific applications. Here we examine the landscape of consistency\nresearch in LMs, analyze current approaches to measure aspects of consistency,\nand identify critical research gaps. Our findings point to an urgent need for\nquality benchmarks to measure and interdisciplinary approaches to ensure\nconsistency while preserving utility."}
{"id": "2505.08245", "pdf": "https://arxiv.org/pdf/2505.08245.pdf", "abs": "https://arxiv.org/abs/2505.08245", "title": "Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement", "authors": ["Haoran Ye", "Jing Jin", "Yuhang Xie", "Xin Zhang", "Guojie Song"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "474 references", "summary": "The advancement of large language models (LLMs) has outpaced traditional\nevaluation methodologies. This progress presents novel challenges, such as\nmeasuring human-like psychological constructs, moving beyond static and\ntask-specific benchmarks, and establishing human-centered evaluation. These\nchallenges intersect with psychometrics, the science of quantifying the\nintangible aspects of human psychology, such as personality, values, and\nintelligence. This review paper introduces and synthesizes the emerging\ninterdisciplinary field of LLM Psychometrics, which leverages psychometric\ninstruments, theories, and principles to evaluate, understand, and enhance\nLLMs. The reviewed literature systematically shapes benchmarking principles,\nbroadens evaluation scopes, refines methodologies, validates results, and\nadvances LLM capabilities. Diverse perspectives are integrated to provide a\nstructured framework for researchers across disciplines, enabling a more\ncomprehensive understanding of this nascent field. Ultimately, the review\nprovides actionable insights for developing future evaluation paradigms that\nalign with human-level AI and promote the advancement of human-centered AI\nsystems for societal benefit. A curated repository of LLM psychometric\nresources is available at\nhttps://github.com/valuebyte-ai/Awesome-LLM-Psychometrics."}
{"id": "2505.12864", "pdf": "https://arxiv.org/pdf/2505.12864.pdf", "abs": "https://arxiv.org/abs/2505.12864", "title": "LEXam: Benchmarking Legal Reasoning on 340 Law Exams", "authors": ["Yu Fan", "Jingwei Ni", "Jakob Merane", "Etienne Salimbeni", "Yang Tian", "Yoan HermstrÃ¼wer", "Yinya Huang", "Mubashara Akhtar", "Florian Geering", "Oliver Dreyer", "Daniel Brunner", "Markus Leippold", "Mrinmaya Sachan", "Alexander Stremitzer", "Christoph Engel", "Elliott Ash", "Joel Niklaus"], "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2"], "comment": null, "summary": "Long-form legal reasoning remains a key challenge for large language models\n(LLMs) in spite of recent advances in test-time scaling. We introduce LEXam, a\nnovel benchmark derived from 340 law exams spanning 116 law school courses\nacross a range of subjects and degree levels. The dataset comprises 4,886 law\nexam questions in English and German, including 2,841 long-form, open-ended\nquestions and 2,045 multiple-choice questions. Besides reference answers, the\nopen questions are also accompanied by explicit guidance outlining the expected\nlegal reasoning approach such as issue spotting, rule recall, or rule\napplication. Our evaluation on both open-ended and multiple-choice questions\npresent significant challenges for current LLMs; in particular, they notably\nstruggle with open questions that require structured, multi-step legal\nreasoning. Moreover, our results underscore the effectiveness of the dataset in\ndifferentiating between models with varying capabilities. Adopting an\nLLM-as-a-Judge paradigm with rigorous human expert validation, we demonstrate\nhow model-generated reasoning steps can be evaluated consistently and\naccurately. Our evaluation setup provides a scalable method to assess legal\nreasoning quality beyond simple accuracy metrics. Project page:\nhttps://lexam-benchmark.github.io/"}
{"id": "2505.15634", "pdf": "https://arxiv.org/pdf/2505.15634.pdf", "abs": "https://arxiv.org/abs/2505.15634", "title": "Feature Extraction and Steering for Enhanced Chain-of-Thought Reasoning in Language Models", "authors": ["Zihao Li", "Xu Wang", "Yuzhe Yang", "Ziyu Yao", "Haoyi Xiong", "Mengnan Du"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) demonstrate the ability to solve reasoning and\nmathematical problems using the Chain-of-Thought (CoT) technique. Expanding CoT\nlength, as seen in models such as DeepSeek-R1, significantly enhances this\nreasoning for complex problems, but requires costly and high-quality long CoT\ndata and fine-tuning. This work, inspired by the deep thinking paradigm of\nDeepSeek-R1, utilizes a steering technique to enhance the reasoning ability of\nan LLM without external datasets. Our method first employs Sparse Autoencoders\n(SAEs) to extract interpretable features from vanilla CoT. These features are\nthen used to steer the LLM's internal states during generation. Recognizing\nthat many LLMs do not have corresponding pre-trained SAEs, we further introduce\na novel SAE-free steering algorithm, which directly computes steering\ndirections from the residual activations of an LLM, obviating the need for an\nexplicit SAE. Experimental results demonstrate that both our SAE-based and\nsubsequent SAE-free steering algorithms significantly enhance the reasoning\ncapabilities of LLMs."}
{"id": "2505.17086", "pdf": "https://arxiv.org/pdf/2505.17086.pdf", "abs": "https://arxiv.org/abs/2505.17086", "title": "Reinforcing Question Answering Agents with Minimalist Policy Gradient Optimization", "authors": ["Yihong Wu", "Liheng Ma", "Muzhi Li", "Jiaming Zhou", "Jianye Hao", "Ho-fung Leung", "Irwin King", "Yingxue Zhang", "Jian-Yun Nie"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable versatility, due to\nthe lack of factual knowledge, their application to Question Answering (QA)\ntasks remains hindered by hallucination. While Retrieval-Augmented Generation\nmitigates these issues by integrating external knowledge, existing approaches\nrely heavily on in-context learning, whose performance is constrained by the\nfundamental reasoning capabilities of LLMs. In this paper, we propose Mujica, a\nMulti-hop Joint Intelligence for Complex Question Answering, comprising a\nplanner that decomposes questions into a directed acyclic graph of subquestions\nand a worker that resolves questions via retrieval and reasoning. Additionally,\nwe introduce MyGO (Minimalist policy Gradient Optimization), a novel\nreinforcement learning method that replaces traditional policy gradient updates\nwith Maximum Likelihood Estimation (MLE) by sampling trajectories from an\nasymptotically optimal policy. MyGO eliminates the need for gradient rescaling\nand reference models, ensuring stable and efficient training. Empirical results\nacross multiple datasets demonstrate the effectiveness of Mujica-MyGO in\nenhancing multi-hop QA performance for various LLMs, offering a scalable and\nresource-efficient solution for complex QA tasks."}
{"id": "2505.20564", "pdf": "https://arxiv.org/pdf/2505.20564.pdf", "abs": "https://arxiv.org/abs/2505.20564", "title": "The NaijaVoices Dataset: Cultivating Large-Scale, High-Quality, Culturally-Rich Speech Data for African Languages", "authors": ["Chris Emezue", "NaijaVoices Community", "Busayo Awobade", "Abraham Owodunni", "Handel Emezue", "Gloria Monica Tobechukwu Emezue", "Nefertiti Nneoma Emezue", "Sewade Ogun", "Bunmi Akinremi", "David Ifeoluwa Adelani", "Chris Pal"], "categories": ["cs.CL"], "comment": "Accepted for publication at Interspeech 2025", "summary": "The development of high-performing, robust, and reliable speech technologies\ndepends on large, high-quality datasets. However, African languages --\nincluding our focus, Igbo, Hausa, and Yoruba -- remain under-represented due to\ninsufficient data. Popular voice-enabled technologies do not support any of the\n2000+ African languages, limiting accessibility for circa one billion people.\nWhile previous dataset efforts exist for the target languages, they lack the\nscale and diversity needed for robust speech models. To bridge this gap, we\nintroduce the NaijaVoices dataset, a 1,800-hour speech-text dataset with 5,000+\nspeakers. We outline our unique data collection approach, analyze its acoustic\ndiversity, and demonstrate its impact through finetuning experiments on\nautomatic speech recognition, averagely achieving 75.86% (Whisper), 52.06%\n(MMS), and 42.33% (XLSR) WER improvements. These results highlight NaijaVoices'\npotential to advance multilingual speech processing for African languages."}
{"id": "2506.00200", "pdf": "https://arxiv.org/pdf/2506.00200.pdf", "abs": "https://arxiv.org/abs/2506.00200", "title": "Structuring Radiology Reports: Challenging LLMs with Lightweight Models", "authors": ["Johannes Moll", "Louisa Fay", "Asfandyar Azhar", "Sophie Ostmeier", "Tim Lueth", "Sergios Gatidis", "Curtis Langlotz", "Jean-Benoit Delbrouck"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Radiology reports are critical for clinical decision-making but often lack a\nstandardized format, limiting both human interpretability and machine learning\n(ML) applications. While large language models (LLMs) have shown strong\ncapabilities in reformatting clinical text, their high computational\nrequirements, lack of transparency, and data privacy concerns hinder practical\ndeployment. To address these challenges, we explore lightweight encoder-decoder\nmodels (<300M parameters)-specifically T5 and BERT2BERT-for structuring\nradiology reports from the MIMIC-CXR and CheXpert Plus datasets. We benchmark\nthese models against eight open-source LLMs (1B-70B), adapted using prefix\nprompting, in-context learning (ICL), and low-rank adaptation (LoRA)\nfinetuning. Our best-performing lightweight model outperforms all LLMs adapted\nusing prompt-based techniques on a human-annotated test set. While some\nLoRA-finetuned LLMs achieve modest gains over the lightweight model on the\nFindings section (BLEU 6.4%, ROUGE-L 4.8%, BERTScore 3.6%, F1-RadGraph 1.1%,\nGREEN 3.6%, and F1-SRR-BERT 4.3%), these improvements come at the cost of\nsubstantially greater computational resources. For example, LLaMA-3-70B\nincurred more than 400 times the inference time, cost, and carbon emissions\ncompared to the lightweight model. These results underscore the potential of\nlightweight, task-specific models as sustainable and privacy-preserving\nsolutions for structuring clinical text in resource-constrained healthcare\nsettings."}
{"id": "2506.04462", "pdf": "https://arxiv.org/pdf/2506.04462.pdf", "abs": "https://arxiv.org/abs/2506.04462", "title": "Watermarking Degrades Alignment in Language Models: Analysis and Mitigation", "authors": ["Apurv Verma", "NhatHai Phan", "Shubhendu Trivedi"], "categories": ["cs.CL", "cs.CR", "cs.LG", "I.2.7"], "comment": "Published at the 1st Workshop on GenAI Watermarking (ICLR 2025).\n  Code: https://github.com/dapurv5/alignmark", "summary": "Watermarking techniques for large language models (LLMs) can significantly\nimpact output quality, yet their effects on truthfulness, safety, and\nhelpfulness remain critically underexamined. This paper presents a systematic\nanalysis of how two popular watermarking approaches-Gumbel and KGW-affect these\ncore alignment properties across four aligned LLMs. Our experiments reveal two\ndistinct degradation patterns: guard attenuation, where enhanced helpfulness\nundermines model safety, and guard amplification, where excessive caution\nreduces model helpfulness. These patterns emerge from watermark-induced shifts\nin token distribution, surfacing the fundamental tension that exists between\nalignment objectives.\n  To mitigate these degradations, we propose Alignment Resampling (AR), an\ninference-time sampling method that uses an external reward model to restore\nalignment. We establish a theoretical lower bound on the improvement in\nexpected reward score as the sample size is increased and empirically\ndemonstrate that sampling just 2-4 watermarked generations effectively recovers\nor surpasses baseline (unwatermarked) alignment scores. To overcome the limited\nresponse diversity of standard Gumbel watermarking, our modified implementation\nsacrifices strict distortion-freeness while maintaining robust detectability,\nensuring compatibility with AR. Experimental results confirm that AR\nsuccessfully recovers baseline alignment in both watermarking approaches, while\nmaintaining strong watermark detectability. This work reveals the critical\nbalance between watermark strength and model alignment, providing a simple\ninference-time solution to responsibly deploy watermarked LLMs in practice."}
{"id": "2506.06955", "pdf": "https://arxiv.org/pdf/2506.06955.pdf", "abs": "https://arxiv.org/abs/2506.06955", "title": "BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for Belief-Inconsistent Syllogistic Reasoning", "authors": ["Ha-Thanh Nguyen", "Chaoran Liu", "Qianying Liu", "Hideyuki Tachibana", "Su Myat Noe", "Yusuke Miyao", "Koichi Takeda", "Sadao Kurohashi"], "categories": ["cs.CL", "cs.AI"], "comment": "This version includes minor typo corrections in the example image", "summary": "We present BIS Reasoning 1.0, the first large-scale Japanese dataset of\nsyllogistic reasoning problems explicitly designed to evaluate\nbelief-inconsistent reasoning in large language models (LLMs). Unlike prior\ndatasets such as NeuBAROCO and JFLD, which focus on general or belief-aligned\nreasoning, BIS Reasoning 1.0 introduces logically valid yet belief-inconsistent\nsyllogisms to uncover reasoning biases in LLMs trained on human-aligned\ncorpora. We benchmark state-of-the-art models - including GPT models, Claude\nmodels, and leading Japanese LLMs - revealing significant variance in\nperformance, with GPT-4o achieving 79.54% accuracy. Our analysis identifies\ncritical weaknesses in current LLMs when handling logically valid but\nbelief-conflicting inputs. These findings have important implications for\ndeploying LLMs in high-stakes domains such as law, healthcare, and scientific\nliterature, where truth must override intuitive belief to ensure integrity and\nsafety."}
{"id": "2506.18421", "pdf": "https://arxiv.org/pdf/2506.18421.pdf", "abs": "https://arxiv.org/abs/2506.18421", "title": "TReB: A Comprehensive Benchmark for Evaluating Table Reasoning Capabilities of Large Language Models", "authors": ["Ce Li", "Xiaofan Liu", "Zhiyan Song", "Ce Chi", "Chen Zhao", "Jingjing Yang", "Zhendong Wang", "Kexin Yang", "Boshen Shi", "Xing Wang", "Chao Deng", "Junlan Feng"], "categories": ["cs.CL", "cs.AI"], "comment": "Benmark report v1.1", "summary": "The majority of data in businesses and industries is stored in tables,\ndatabases, and data warehouses. Reasoning with table-structured data poses\nsignificant challenges for large language models (LLMs) due to its hidden\nsemantics, inherent complexity, and structured nature. One of these challenges\nis lacking an effective evaluation benchmark fairly reflecting the performances\nof LLMs on broad table reasoning abilities. In this paper, we fill in this gap,\npresenting a comprehensive table reasoning evolution benchmark, TReB, which\nmeasures both shallow table understanding abilities and deep table reasoning\nabilities, a total of 26 sub-tasks. We construct a high quality dataset through\nan iterative data processing procedure. We create an evaluation framework to\nrobustly measure table reasoning capabilities with three distinct inference\nmodes, TCoT, PoT and ICoT. Further, we benchmark over 20 state-of-the-art LLMs\nusing this frame work and prove its effectiveness. Experimental results reveal\nthat existing LLMs still have significant room for improvement in addressing\nthe complex and real world Table related tasks. Both the dataset and evaluation\nframework are publicly available, with the dataset hosted on\nhuggingface.co/datasets/JT-LM/JIUTIAN-TReB and the framework on\ngithub.com/JT-LM/jiutian-treb."}
{"id": "2506.22777", "pdf": "https://arxiv.org/pdf/2506.22777.pdf", "abs": "https://arxiv.org/abs/2506.22777", "title": "Teaching Models to Verbalize Reward Hacking in Chain-of-Thought Reasoning", "authors": ["Miles Turpin", "Andy Arditi", "Marvin Li", "Joe Benton", "Julian Michael"], "categories": ["cs.CL", "cs.AI"], "comment": "Published at ICML 2025 Workshop on Reliable and Responsible\n  Foundation Models", "summary": "Language models trained with reinforcement learning (RL) can engage in reward\nhacking--the exploitation of unintended strategies for high reward--without\nrevealing this behavior in their chain-of-thought reasoning. This makes the\ndetection of reward hacking difficult, posing risks for high-stakes\napplications. We propose verbalization fine-tuning (VFT), a pre-RL fine-tuning\nintervention that trains models to explicitly acknowledge when they are\ninfluenced by prompt cues--hints which point to incorrect answers (e.g., \"a\nStanford professor thinks the answer is A\"). To evaluate VFT, we subsequently\ntrain models with RL on environments where held-out prompt cues signal which\nincorrect answers will receive high reward, incentivizing models to exploit\nthese cues instead of reasoning correctly. We measure how often models exploit\nthese cues without verbalizing it. After RL, only 6% of the VFT-trained model's\nresponses consist of undetected reward hacks. In comparison, when we perform RL\nwithout VFT, the rate of undetected reward hacks goes up to 88%; with a\ndebiasing baseline intervention, this increases further to 99%. VFT achieves\nthis by substantially increasing how often models verbalize the influence of\ncues, from 8% to 43% after VFT, and up to 94% after RL. Baselines remain low\neven after RL (11% and 1%). Our results show that teaching models to explicitly\nverbalize reward hacking behavior before RL significantly improves their\ndetection, offering a practical path toward more transparent and safe AI\nsystems."}
{"id": "2506.22791", "pdf": "https://arxiv.org/pdf/2506.22791.pdf", "abs": "https://arxiv.org/abs/2506.22791", "title": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in Large Language Models", "authors": ["Jianxin Yan", "Wangze Ni", "Lei Chen", "Xuemin Lin", "Peng Cheng", "Zhan Qin", "Kui Ren"], "categories": ["cs.CL", "cs.DB"], "comment": null, "summary": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications."}
{"id": "2506.23146", "pdf": "https://arxiv.org/pdf/2506.23146.pdf", "abs": "https://arxiv.org/abs/2506.23146", "title": "Learning-to-Context Slope: Evaluating In-Context Learning Effectiveness Beyond Performance Illusions", "authors": ["Dingzriui Wang", "Xuanliang Zhang", "Keyan Xu", "Qingfu Zhu", "Wanxiang Che", "Yang Deng"], "categories": ["cs.CL"], "comment": null, "summary": "In-context learning (ICL) has emerged as an effective approach to enhance the\nperformance of large language models (LLMs). However, its effectiveness varies\nsignificantly across models and tasks, posing challenges for practitioners to\ndetermine when ICL reliably improves performance. Current evaluation\napproaches, reliant on performance change after applying ICL, suffer from low\nreliability, poor attribution, and impracticality in data-insufficient\nscenarios. We propose the Learning-to-Context Slope (LCS), a novel metric that\nquantifies ICL effectiveness by modeling the slope between learning gain (loss\ndecrease from demonstrations) and contextual relevance (demonstration-input\nrelevance). LCS addresses key limitations of performance-based metrics: (1) it\ncaptures continuous loss changes even when outputs are incorrect, improving\nreliability; (2) its formulation attributes ICL failures to weak contextual\nalignment (inability to adapt inputs to demonstrations) or strong output\ncalibration (self-verification of correctness); and (3) it minimizes reliance\non labeled data via synthetic evaluation. Extensive experiments demonstrate\nthat LCS strongly correlates with performance improvements in labeled settings\nand reliably reflects true effectiveness in biased or data-scarce scenarios.\nFurther analysis reveals actionable thresholds for LCS and identifies model\ncapabilities critical to ICL success."}
{"id": "2506.23377", "pdf": "https://arxiv.org/pdf/2506.23377.pdf", "abs": "https://arxiv.org/abs/2506.23377", "title": "Perspective Dial: Measuring Perspective of Text and Guiding LLM Outputs", "authors": ["Taejin Kim", "Siun-Chuon Mau", "Konrad Vesey"], "categories": ["cs.CL", "cs.AI"], "comment": "7 pages, 5 main pages of text, 5 figures, 2 tables. Research work\n  performed at CACI INTL INC", "summary": "Large language models (LLMs) are used in a variety of mission-critical roles.\nDue to the rapidly developing nature of LLMs, there is a lack of quantifiable\nunderstanding of the bias and perspective associated with LLM output. Inspired\nby this need, this paper considers the broader issue of perspective or\nviewpoint of general text and perspective control of large-language model (LLM)\noutput. Perspective-Dial consists of two main components: a (1) metric space,\ndubbed Perspective Space, that enables quantitative measurements of different\nperspectives regarding a topic, and the use of (2) Systematic Prompt\nEngineering that utilizes greedy-coordinate descent to control LLM output\nperspective based on measurement feedback from the Perspective Space. The\nempirical nature of the approach allows progress to side step a principled\nunderstanding of perspective or bias -- effectively quantifying and adjusting\noutputs for a variety of topics. Potential applications include detection,\ntracking and mitigation of LLM bias, narrative detection, sense making and\ntracking in public discourse, and debate bot advocating given perspective."}
{"id": "2507.01853", "pdf": "https://arxiv.org/pdf/2507.01853.pdf", "abs": "https://arxiv.org/abs/2507.01853", "title": "Eka-Eval : A Comprehensive Evaluation Framework for Large Language Models in Indian Languages", "authors": ["Samridhi Raj Sinha", "Rajvee Sheth", "Abhishek Upperwal", "Mayank Singh"], "categories": ["cs.CL"], "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has intensified the\nneed for evaluation frameworks that address the requirements of linguistically\ndiverse regions, such as India, and go beyond English-centric benchmarks. We\nintroduce EKA-EVAL, a unified evaluation framework that integrates over 35+\nbenchmarks (including 10 Indic benchmarks) across nine major evaluation\ncategories. The framework provides broader coverage than existing Indian\nlanguage evaluation tools, offering 11 core capabilities through a modular\narchitecture, seamless integration with Hugging Face and proprietary models,\nand plug-and-play usability. As the first end-to-end suite for scalable,\nmultilingual LLM benchmarking, the framework combines extensive benchmarks,\nmodular workflows, and dedicated support for low-resource Indian languages to\nenable inclusive assessment of LLM capabilities across diverse domains. We\nconducted extensive comparisons against five existing baselines, demonstrating\nthat EKA-EVAL achieves the highest participant ratings in four out of five\ncategories. The framework is open-source and publicly available at:\nhttps://github.com/lingo-iitgn/eka-eval."}
{"id": "2507.02679", "pdf": "https://arxiv.org/pdf/2507.02679.pdf", "abs": "https://arxiv.org/abs/2507.02679", "title": "Exploring Gender Bias Beyond Occupational Titles", "authors": ["Ahmed Sabir", "Rajesh Sharma"], "categories": ["cs.CL"], "comment": "Work in progress", "summary": "In this work, we investigate the correlation between gender and contextual\nbiases, focusing on elements such as action verbs, object nouns, and\nparticularly on occupations. We introduce a novel dataset, GenderLexicon, and a\nframework that can estimate contextual bias and its related gender bias. Our\nmodel can interpret the bias with a score and thus improve the explainability\nof gender bias. Also, our findings confirm the existence of gender biases\nbeyond occupational stereotypes. To validate our approach and demonstrate its\neffectiveness, we conduct evaluations on five diverse datasets, including a\nJapanese dataset."}
{"id": "2507.03152", "pdf": "https://arxiv.org/pdf/2507.03152.pdf", "abs": "https://arxiv.org/abs/2507.03152", "title": "Expert-level validation of AI-generated medical text with scalable language models", "authors": ["Asad Aali", "Vasiliki Bikia", "Maya Varma", "Nicole Chiou", "Sophie Ostmeier", "Arnav Singhvi", "Magdalini Paschali", "Ashwin Kumar", "Andrew Johnston", "Karimar Amador-Martinez", "Eduardo Juan Perez Guerrero", "Paola Naovi Cruz Rivera", "Sergios Gatidis", "Christian Bluethgen", "Eduardo Pontes Reis", "Eddy D. Zandee van Rilland", "Poonam Laxmappa Hosamani", "Kevin R Keet", "Minjoung Go", "Evelyn Ling", "David B. Larson", "Curtis Langlotz", "Roxana Daneshjou", "Jason Hom", "Sanmi Koyejo", "Emily Alsentzer", "Akshay S. Chaudhari"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "With the growing use of language models (LMs) in clinical environments, there\nis an immediate need to evaluate the accuracy and safety of LM-generated\nmedical text. Currently, such evaluation relies solely on manual physician\nreview. However, detecting errors in LM-generated text is challenging because\n1) manual review is costly and 2) expert-composed reference outputs are often\nunavailable in real-world settings. While the \"LM-as-judge\" paradigm (a LM\nevaluating another LM) offers scalable evaluation, even frontier LMs can miss\nsubtle but clinically significant errors. To address these challenges, we\npropose MedVAL, a self-supervised framework that leverages synthetic data to\ntrain evaluator LMs to assess whether LM-generated medical outputs are\nfactually consistent with inputs, without requiring physician labels or\nreference outputs. To evaluate LM performance, we introduce MedVAL-Bench, a\ndataset containing 840 outputs annotated by physicians, following a\nphysician-defined taxonomy of risk levels and error categories. Across 6\ndiverse medical tasks and 10 state-of-the-art LMs spanning open-source,\nproprietary, and medically adapted models, MedVAL fine-tuning significantly\nimproves (p < 0.001) alignment with physicians on both seen and unseen tasks,\nincreasing average F1 scores from 66% to 83%, with per-sample safety\nclassification scores up to 86%. MedVAL improves the performance of even the\nbest-performing proprietary LM (GPT-4o) by 8%. To support a scalable,\nrisk-aware pathway towards clinical integration, we open-source the 1) codebase\n(https://github.com/StanfordMIMI/MedVAL), 2) MedVAL-Bench\n(https://huggingface.co/datasets/stanfordmimi/MedVAL-Bench), and 3) MedVAL-4B\n(https://huggingface.co/stanfordmimi/MedVAL-4B), the best-performing\nopen-source LM. Our research provides the first evidence of LMs approaching\nexpert-level validation ability for medical text."}
{"id": "2507.04189", "pdf": "https://arxiv.org/pdf/2507.04189.pdf", "abs": "https://arxiv.org/abs/2507.04189", "title": "SymbolicThought: Integrating Language Models and Symbolic Reasoning for Consistent and Interpretable Human Relationship Understanding", "authors": ["Runcong Zhao", "Qinglin Zhu", "Hainiu Xu", "Bin Liang", "Lin Gui", "Yulan He"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Understanding character relationships is essential for interpreting complex\nnarratives and conducting socially grounded AI research. However, manual\nannotation is time-consuming and low in coverage, while large language models\n(LLMs) often produce hallucinated or logically inconsistent outputs. We present\nSymbolicThought, a human-in-the-loop framework that combines LLM-based\nextraction with symbolic reasoning. The system constructs editable character\nrelationship graphs, refines them using seven types of logical constraints, and\nenables real-time validation and conflict resolution through an interactive\ninterface. To support logical supervision and explainable social analysis, we\nrelease a dataset of 160 interpersonal relationships with corresponding logical\nstructures. Experiments show that SymbolicThought improves annotation accuracy\nand consistency while significantly reducing time cost, offering a practical\ntool for narrative understanding, explainable AI, and LLM evaluation."}
{"id": "2507.04607", "pdf": "https://arxiv.org/pdf/2507.04607.pdf", "abs": "https://arxiv.org/abs/2507.04607", "title": "PRIME: Large Language Model Personalization with Cognitive Memory and Thought Processes", "authors": ["Xinliang Frederick Zhang", "Nick Beauchamp", "Lu Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language model (LLM) personalization aims to align model outputs with\nindividuals' unique preferences and opinions. While recent efforts have\nimplemented various personalization methods, a unified theoretical framework\nthat can systematically understand the drivers of effective personalization is\nstill lacking. In this work, we integrate the well-established cognitive\ndual-memory model into LLM personalization, by mirroring episodic memory to\nhistorical user engagements and semantic memory to long-term, evolving user\nbeliefs. Specifically, we systematically investigate memory instantiations and\nintroduce a unified framework, PRIME, using episodic and semantic memory\nmechanisms. We further augment PRIME with a novel personalized thinking\ncapability inspired by the slow thinking strategy. Moreover, recognizing the\nabsence of suitable benchmarks, we introduce a dataset using Change My View\n(CMV) from Reddit, specifically designed to evaluate long-context\npersonalization. Extensive experiments validate PRIME's effectiveness across\nboth long- and short-context scenarios. Further analysis confirms that PRIME\neffectively captures dynamic personalization beyond mere popularity biases."}
{"id": "2507.05179", "pdf": "https://arxiv.org/pdf/2507.05179.pdf", "abs": "https://arxiv.org/abs/2507.05179", "title": "From Fragments to Facts: A Curriculum-Driven DPO Approach for Generating Hindi News Veracity Explanations", "authors": ["Pulkit Bansal", "Raghvendra Kumar", "Shakti Singh", "Sriparna Saha", "Adam Jatowt"], "categories": ["cs.CL"], "comment": null, "summary": "In an era of rampant misinformation, generating reliable news explanations is\nvital, especially for under-represented languages like Hindi. Lacking robust\nautomated tools, Hindi faces challenges in scaling misinformation detection. To\nbridge this gap, we propose a novel framework integrating Direct Preference\nOptimization (DPO) with curriculum learning to align machine-generated\nexplanations with human reasoning. Fact-checked explanations from credible\nsources serve as preferred responses, while LLM outputs highlight system\nlimitations and serve as non-preferred responses. To refine task-specific\nalignment, we introduce two key parameters -- Actuality and Finesse -- into the\nDPO loss function, enhancing explanation quality and consistency. Experiments\nwith LLMs (Mistral, Llama, Gemma) and PLMs (mBART, mT5) confirm the framework's\neffectiveness in generating coherent, contextually relevant explanations. This\nscalable approach combats misinformation and extends automated explanation\ngeneration to low-resource languages."}
{"id": "2507.05285", "pdf": "https://arxiv.org/pdf/2507.05285.pdf", "abs": "https://arxiv.org/abs/2507.05285", "title": "Beyond classical and contemporary models: a transformative AI framework for student dropout prediction in distance learning using RAG, Prompt engineering, and Cross-modal fusion", "authors": ["Miloud Mihoubi", "Meriem Zerkouk", "Belkacem Chikhaoui"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.IR", "I.2.7; I.2.1; K.3.1"], "comment": "13 pages, 8 figures, 1 Algorithms, 17th International Conference on\n  Education and New Learning Technologies,: 30 June-2 July, 2025 Location:\n  Palma, Spain", "summary": "Student dropout in distance learning remains a critical challenge, with\nprofound societal and economic consequences. While classical machine learning\nmodels leverage structured socio-demographic and behavioral data, they often\nfail to capture the nuanced emotional and contextual factors embedded in\nunstructured student interactions. This paper introduces a transformative AI\nframework that redefines dropout prediction through three synergistic\ninnovations: Retrieval-Augmented Generation (RAG) for domain-specific sentiment\nanalysis, prompt engineering to decode academic stressors,and cross-modal\nattention fusion to dynamically align textual, behavioral, and\nsocio-demographic insights. By grounding sentiment analysis in a curated\nknowledge base of pedagogical content, our RAG-enhanced BERT model interprets\nstudent comments with unprecedented contextual relevance, while optimized\nprompts isolate indicators of academic distress (e.g., \"isolation,\" \"workload\nanxiety\"). A cross-modal attention layer then fuses these insights with\ntemporal engagement patterns, creating holistic risk pro-files. Evaluated on a\nlongitudinal dataset of 4 423 students, the framework achieves 89% accuracy and\nan F1-score of 0.88, outperforming conventional models by 7% and reducing false\nnegatives by 21%. Beyond prediction, the system generates interpretable\ninterventions by retrieving contextually aligned strategies (e.g., mentorship\nprograms for isolated learners). This work bridges the gap between predictive\nanalytics and actionable pedagogy, offering a scalable solution to mitigate\ndropout risks in global education systems"}
{"id": "2507.06448", "pdf": "https://arxiv.org/pdf/2507.06448.pdf", "abs": "https://arxiv.org/abs/2507.06448", "title": "Perception-Aware Policy Optimization for Multimodal Reasoning", "authors": ["Zhenhailong Wang", "Xuehang Guo", "Sofia Stoica", "Haiyang Xu", "Hongru Wang", "Hyeonjeong Ha", "Xiusi Chen", "Yangyi Chen", "Ming Yan", "Fei Huang", "Heng Ji"], "categories": ["cs.CL"], "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be a\nhighly effective strategy for endowing Large Language Models (LLMs) with robust\nmulti-step reasoning abilities. However, its design and optimizations remain\ntailored to purely textual domains, resulting in suboptimal performance when\napplied to multimodal reasoning tasks. In particular, we observe that a major\nsource of error in current multimodal reasoning lies in the perception of\nvisual inputs. To address this bottleneck, we propose Perception-Aware Policy\nOptimization (PAPO), a simple yet effective extension of GRPO that encourages\nthe model to learn to perceive while learning to reason, entirely from internal\nsupervision signals. Notably, PAPO does not rely on additional data curation,\nexternal reward models, or proprietary models. Specifically, we introduce the\nImplicit Perception Loss in the form of a KL divergence term to the GRPO\nobjective, which, despite its simplicity, yields significant overall\nimprovements (4.4%) on diverse multimodal benchmarks. The improvements are more\npronounced, approaching 8.0%, on tasks with high vision dependency. We also\nobserve a substantial reduction (30.5%) in perception errors, indicating\nimproved perceptual capabilities with PAPO. We conduct comprehensive analysis\nof PAPO and identify a unique loss hacking issue, which we rigorously analyze\nand mitigate through a Double Entropy Loss. Overall, our work introduces a\ndeeper integration of perception-aware supervision into RLVR learning\nobjectives and lays the groundwork for a new RL framework that encourages\nvisually grounded reasoning. Project page: https://mikewangwzhl.github.io/PAPO."}
{"id": "2507.07024", "pdf": "https://arxiv.org/pdf/2507.07024.pdf", "abs": "https://arxiv.org/abs/2507.07024", "title": "FlexOlmo: Open Language Models for Flexible Data Use", "authors": ["Weijia Shi", "Akshita Bhagia", "Kevin Farhat", "Niklas Muennighoff", "Pete Walsh", "Jacob Morrison", "Dustin Schwenk", "Shayne Longpre", "Jake Poznanski", "Allyson Ettinger", "Daogao Liu", "Margaret Li", "Dirk Groeneveld", "Mike Lewis", "Wen-tau Yih", "Luca Soldaini", "Kyle Lo", "Noah A. Smith", "Luke Zettlemoyer", "Pang Wei Koh", "Hannaneh Hajishirzi", "Ali Farhadi", "Sewon Min"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We introduce FlexOlmo, a new class of language models (LMs) that supports (1)\ndistributed training without data sharing, where different model parameters are\nindependently trained on closed datasets, and (2) data-flexible inference,\nwhere these parameters along with their associated data can be flexibly\nincluded or excluded from model inferences with no further training. FlexOlmo\nemploys a mixture-of-experts (MoE) architecture where each expert is trained\nindependently on closed datasets and later integrated through a new\ndomain-informed routing without any joint training. FlexOlmo is trained on\nFlexMix, a corpus we curate comprising publicly available datasets alongside\nseven domain-specific sets, representing realistic approximations of closed\nsets. We evaluate models with up to 37 billion parameters (20 billion active)\non 31 diverse downstream tasks. We show that a general expert trained on public\ndata can be effectively combined with independently trained experts from other\ndata owners, leading to an average 41% relative improvement while allowing\nusers to opt out of certain data based on data licensing or permission\nrequirements. Our approach also outperforms prior model merging methods by\n10.1% on average and surpasses the standard MoE trained without data\nrestrictions using the same training FLOPs. Altogether, this research presents\na solution for both data owners and researchers in regulated industries with\nsensitive or protected data. FlexOlmo enables benefiting from closed data while\nrespecting data owners' preferences by keeping their data local and supporting\nfine-grained control of data access during inference."}
{"id": "2507.07186", "pdf": "https://arxiv.org/pdf/2507.07186.pdf", "abs": "https://arxiv.org/abs/2507.07186", "title": "Planted in Pretraining, Swayed by Finetuning: A Case Study on the Origins of Cognitive Biases in LLMs", "authors": ["Itay Itzhak", "Yonatan Belinkov", "Gabriel Stanovsky"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "CoLM 2025", "summary": "Large language models (LLMs) exhibit cognitive biases -- systematic\ntendencies of irrational decision-making, similar to those seen in humans.\nPrior work has found that these biases vary across models and can be amplified\nby instruction tuning. However, it remains unclear if these differences in\nbiases stem from pretraining, finetuning, or even random noise due to training\nstochasticity. We propose a two-step causal experimental approach to\ndisentangle these factors. First, we finetune models multiple times using\ndifferent random seeds to study how training randomness affects over $30$\ncognitive biases. Second, we introduce \\emph{cross-tuning} -- swapping\ninstruction datasets between models to isolate bias sources. This swap uses\ndatasets that led to different bias patterns, directly testing whether biases\nare dataset-dependent. Our findings reveal that while training randomness\nintroduces some variability, biases are mainly shaped by pretraining: models\nwith the same pretrained backbone exhibit more similar bias patterns than those\nsharing only finetuning data. These insights suggest that understanding biases\nin finetuned models requires considering their pretraining origins beyond\nfinetuning effects. This perspective can guide future efforts to develop\nprincipled strategies for evaluating and mitigating bias in LLMs."}
{"id": "2507.07498", "pdf": "https://arxiv.org/pdf/2507.07498.pdf", "abs": "https://arxiv.org/abs/2507.07498", "title": "Teaching LLM to Reason: Reinforcement Learning from Algorithmic Problems without Code", "authors": ["Keqin Bao", "Nuo Chen", "Xiaoyuan Li", "Binyuan Hui", "Bowen Yu", "Fuli Feng", "Xiangnan He", "Dayiheng Liu"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Enhancing reasoning capabilities remains a central focus in the LLM reasearch\ncommunity. A promising direction involves requiring models to simulate code\nexecution step-by-step to derive outputs for given inputs. However, as code is\noften designed for large-scale systems, direct application leads to\nover-reliance on complex data structures and algorithms, even for simple cases,\nresulting in overfitting to algorithmic patterns rather than core reasoning\nstructures. To address this, we propose TeaR, which aims at teaching LLMs to\nreason better. TeaR leverages careful data curation and reinforcement learning\nto guide models in discovering optimal reasoning paths through code-related\ntasks, thereby improving general reasoning abilities. We conduct extensive\nexperiments using two base models and three long-CoT distillation models, with\nmodel sizes ranging from 1.5 billion to 32 billion parameters, and across 17\nbenchmarks spanning Math, Knowledge, Code, and Logical Reasoning. The results\nconsistently show significant performance improvements. Notably, TeaR achieves\na 35.9% improvement on Qwen2.5-7B and 5.9% on R1-Distilled-7B."}
{"id": "2507.07586", "pdf": "https://arxiv.org/pdf/2507.07586.pdf", "abs": "https://arxiv.org/abs/2507.07586", "title": "Your Absorbing Discrete Diffusion Secretly Models the Bayesian Posterior", "authors": ["Cooper Doyle"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "12 pages, 2 figures, 2 tables", "summary": "Discrete diffusion language models learn to reconstruct text from randomly\nmasked inputs, yet under mild assumptions their denoiser already implements the\nexact Bayesian posterior over the original tokens. We prove that the expected\ndenoiser output under the forward corruption distribution recovers the true\nposterior, and that a simple Monte Carlo estimator converges to this posterior\nat rate O(1/sqrt(K)) with finite-sample concentration bounds. Building on this\ninsight, we introduce an inference-time ensemble that runs K independent\ndenoising passes and aggregates both posterior means and variances without any\nextra training. On WikiText-2, our MC-marginal sampler recovers the analytic\nlambda-DCE zero-shot perplexity (approximately 39) to within a few points at\nK=128, and its per-token variance shows a strong rank correlation with\nreconstruction error (Spearman rho = 0.996). This cost-proportional procedure\nyields calibrated uncertainty estimates and a direct trade-off between compute\nand posterior fidelity in discrete diffusion LMs."}
{"id": "2507.07803", "pdf": "https://arxiv.org/pdf/2507.07803.pdf", "abs": "https://arxiv.org/abs/2507.07803", "title": "StreamUni: Achieving Streaming Speech Translation with a Unified Large Speech-Language Model", "authors": ["Shoutao Guo", "Xiang Li", "Mengge Liu", "Wei Chen", "Yang Feng"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "The code is at https://github.com/ictnlp/StreamUni; The model is at\n  https://huggingface.co/ICTNLP/StreamUni-Phi4", "summary": "Streaming speech translation (StreamST) requires determining appropriate\ntiming, known as policy, to generate translations while continuously receiving\nsource speech inputs, balancing low latency with high translation quality.\nHowever, existing StreamST methods typically operate on sentence-level speech\nsegments, referred to as simultaneous speech translation (SimulST). In\npractice, they require collaboration with segmentation models to accomplish\nStreamST, where the truncated speech segments constrain SimulST models to make\npolicy decisions and generate translations based on limited contextual\ninformation. Moreover, SimulST models struggle to learn effective policies due\nto the complexity of speech inputs and cross-lingual generation. To address\nthese challenges, we propose StreamUni, which achieves StreamST through a\nunified Large Speech-Language Model (LSLM). Specifically, StreamUni\nincorporates speech Chain-of-Thought (CoT) in guiding the LSLM to generate\nmulti-stage outputs. Leveraging these multi-stage outputs, StreamUni\nsimultaneously accomplishes speech segmentation, policy decision, and\ntranslation generation, completing StreamST without requiring massive\npolicy-specific training. Additionally, we propose a streaming CoT training\nmethod that enhances low-latency policy decisions and generation capabilities\nusing limited CoT data. Experiments demonstrate that our approach achieves\nstate-of-the-art performance on StreamST tasks."}
{"id": "2507.07910", "pdf": "https://arxiv.org/pdf/2507.07910.pdf", "abs": "https://arxiv.org/abs/2507.07910", "title": "DTECT: Dynamic Topic Explorer & Context Tracker", "authors": ["Suman Adhya", "Debarshi Kumar Sanyal"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Code: https://github.com/AdhyaSuman/DTECT | Demo:\n  https://huggingface.co/spaces/AdhyaSuman/DTECT | Video:\n  https://youtu.be/B8nNfxFoJAU", "summary": "The explosive growth of textual data over time presents a significant\nchallenge in uncovering evolving themes and trends. Existing dynamic topic\nmodeling techniques, while powerful, often exist in fragmented pipelines that\nlack robust support for interpretation and user-friendly exploration. We\nintroduce DTECT (Dynamic Topic Explorer & Context Tracker), an end-to-end\nsystem that bridges the gap between raw textual data and meaningful temporal\ninsights. DTECT provides a unified workflow that supports data preprocessing,\nmultiple model architectures, and dedicated evaluation metrics to analyze the\ntopic quality of temporal topic models. It significantly enhances\ninterpretability by introducing LLM-driven automatic topic labeling, trend\nanalysis via temporally salient words, interactive visualizations with\ndocument-level summarization, and a natural language chat interface for\nintuitive data querying. By integrating these features into a single, cohesive\nplatform, DTECT empowers users to more effectively track and understand\nthematic dynamics. DTECT is open-source and available at\nhttps://github.com/AdhyaSuman/DTECT."}
{"id": "2507.07998", "pdf": "https://arxiv.org/pdf/2507.07998.pdf", "abs": "https://arxiv.org/abs/2507.07998", "title": "PyVision: Agentic Vision with Dynamic Tooling", "authors": ["Shitian Zhao", "Haoquan Zhang", "Shaoheng Lin", "Ming Li", "Qilong Wu", "Kaipeng Zhang", "Chen Wei"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "26 Pages, 10 Figures, Technical report", "summary": "LLMs are increasingly deployed as agents, systems capable of planning,\nreasoning, and dynamically calling external tools. However, in visual\nreasoning, prior approaches largely remain limited by predefined workflows and\nstatic toolsets. In this report, we present PyVision, an interactive,\nmulti-turn framework that enables MLLMs to autonomously generate, execute, and\nrefine Python-based tools tailored to the task at hand, unlocking flexible and\ninterpretable problem-solving. We develop a taxonomy of the tools created by\nPyVision and analyze their usage across a diverse set of benchmarks.\nQuantitatively, PyVision achieves consistent performance gains, boosting\nGPT-4.1 by +7.8% on V* and Claude-4.0-Sonnet by +31.1% on VLMsAreBlind-mini.\nThese results point to a broader shift: dynamic tooling allows models not just\nto use tools, but to invent them, advancing toward more agentic visual\nreasoning."}
{"id": "2507.08017", "pdf": "https://arxiv.org/pdf/2507.08017.pdf", "abs": "https://arxiv.org/abs/2507.08017", "title": "Mechanistic Indicators of Understanding in Large Language Models", "authors": ["Pierre Beckmann", "Matthieu Queloz"], "categories": ["cs.CL", "cs.AI"], "comment": "32 pages", "summary": "Recent findings in mechanistic interpretability (MI), the field probing the\ninner workings of Large Language Models (LLMs), challenge the view that these\nmodels rely solely on superficial statistics. We offer an accessible synthesis\nof these findings that doubles as an introduction to MI while integrating these\nfindings within a novel theoretical framework for thinking about machine\nunderstanding. We argue that LLMs develop internal structures that are\nfunctionally analogous to the kind of understanding that consists in seeing\nconnections. To sharpen this idea, we propose a three-tiered conception of\nunderstanding. First, conceptual understanding emerges when a model forms\n\"features\" as directions in latent space, learning the connections between\ndiverse manifestations of something. Second, state-of-the-world understanding\nemerges when a model learns contingent factual connections between features and\ndynamically tracks changes in the world. Third, principled understanding\nemerges when a model ceases to rely on a collection of memorized facts and\ndiscovers a \"circuit\" connecting these facts. However, these forms of\nunderstanding remain radically different from human understanding, as the\nphenomenon of \"parallel mechanisms\" shows. We conclude that the debate should\nmove beyond the yes-or-no question of whether LLMs understand to investigate\nhow their strange minds work and forge conceptions that fit them."}
{"id": "2507.08031", "pdf": "https://arxiv.org/pdf/2507.08031.pdf", "abs": "https://arxiv.org/abs/2507.08031", "title": "Beyond Scale: Small Language Models are Comparable to GPT-4 in Mental Health Understanding", "authors": ["Hong Jia", "Shiya Fu", "Feng Xia", "Vassilis Kostakos", "Ting Dang"], "categories": ["cs.CL"], "comment": null, "summary": "The emergence of Small Language Models (SLMs) as privacy-preserving\nalternatives for sensitive applications raises a fundamental question about\ntheir inherent understanding capabilities compared to Large Language Models\n(LLMs). This paper investigates the mental health understanding capabilities of\ncurrent SLMs through systematic evaluation across diverse classification tasks.\nEmploying zero-shot and few-shot learning paradigms, we benchmark their\nperformance against established LLM baselines to elucidate their relative\nstrengths and limitations in this critical domain. We assess five\nstate-of-the-art SLMs (Phi-3, Phi-3.5, Qwen2.5, Llama-3.2, Gemma2) against\nthree LLMs (GPT-4, FLAN-T5-XXL, Alpaca-7B) on six mental health understanding\ntasks. Our findings reveal that SLMs achieve mean performance within 2\\% of\nLLMs on binary classification tasks (F1 scores of 0.64 vs 0.66 in zero-shot\nsettings), demonstrating notable competence despite orders of magnitude fewer\nparameters. Both model categories experience similar degradation on multi-class\nseverity tasks (a drop of over 30\\%), suggesting that nuanced clinical\nunderstanding challenges transcend model scale. Few-shot prompting provides\nsubstantial improvements for SLMs (up to 14.6\\%), while LLM gains are more\nvariable. Our work highlights the potential of SLMs in mental health\nunderstanding, showing they can be effective privacy-preserving tools for\nanalyzing sensitive online text data. In particular, their ability to quickly\nadapt and specialize with minimal data through few-shot learning positions them\nas promising candidates for scalable mental health screening tools."}
{"id": "2507.08036", "pdf": "https://arxiv.org/pdf/2507.08036.pdf", "abs": "https://arxiv.org/abs/2507.08036", "title": "Barriers in Integrating Medical Visual Question Answering into Radiology Workflows: A Scoping Review and Clinicians' Insights", "authors": ["Deepali Mishra", "Chaklam Silpasuwanchai", "Ashutosh Modi", "Madhumita Sushil", "Sorayouth Chumnanvej"], "categories": ["cs.CL", "cs.CV"], "comment": "29 pages, 5 figures (1 in supplementary), 3 tables (1 in main text, 2\n  in supplementary). Scoping review and clinician survey", "summary": "Medical Visual Question Answering (MedVQA) is a promising tool to assist\nradiologists by automating medical image interpretation through question\nanswering. Despite advances in models and datasets, MedVQA's integration into\nclinical workflows remains limited. This study systematically reviews 68\npublications (2018-2024) and surveys 50 clinicians from India and Thailand to\nexamine MedVQA's practical utility, challenges, and gaps. Following the Arksey\nand O'Malley scoping review framework, we used a two-pronged approach: (1)\nreviewing studies to identify key concepts, advancements, and research gaps in\nradiology workflows, and (2) surveying clinicians to capture their perspectives\non MedVQA's clinical relevance. Our review reveals that nearly 60% of QA pairs\nare non-diagnostic and lack clinical relevance. Most datasets and models do not\nsupport multi-view, multi-resolution imaging, EHR integration, or domain\nknowledge, features essential for clinical diagnosis. Furthermore, there is a\nclear mismatch between current evaluation metrics and clinical needs. The\nclinician survey confirms this disconnect: only 29.8% consider MedVQA systems\nhighly useful. Key concerns include the absence of patient history or domain\nknowledge (87.2%), preference for manually curated datasets (51.1%), and the\nneed for multi-view image support (78.7%). Additionally, 66% favor models\nfocused on specific anatomical regions, and 89.4% prefer dialogue-based\ninteractive systems. While MedVQA shows strong potential, challenges such as\nlimited multimodal analysis, lack of patient context, and misaligned evaluation\napproaches must be addressed for effective clinical integration."}
{"id": "2306.11341", "pdf": "https://arxiv.org/pdf/2306.11341.pdf", "abs": "https://arxiv.org/abs/2306.11341", "title": "MSVD-Indonesian: A Benchmark for Multimodal Video-Text Tasks in Indonesian", "authors": ["Willy Fitra Hendria"], "categories": ["cs.MM", "cs.CL", "cs.CV", "cs.LG", "eess.IV"], "comment": "10 pages, 5 figures, 5 tables", "summary": "Multimodal learning on video and text has seen significant progress,\nparticularly in tasks like text-to-video retrieval, video-to-text retrieval,\nand video captioning. However, most existing methods and datasets focus\nexclusively on English. Despite Indonesian being one of the most widely spoken\nlanguages, multimodal research in Indonesian remains under-explored, largely\ndue to the lack of benchmark datasets. To address this gap, we introduce the\nfirst public Indonesian video-text dataset by translating the English captions\nin the MSVD dataset into Indonesian. Using this dataset, we evaluate neural\nnetwork models which were developed for the English video-text dataset on three\ntasks, i.e., text-to-video retrieval, video-to-text retrieval, and video\ncaptioning. Most existing models rely on feature extractors pretrained on\nEnglish vision-language datasets, raising concerns about their applicability to\nIndonesian, given the scarcity of large-scale pretraining resources in the\nlanguage. We apply a cross-lingual transfer learning approach by leveraging\nEnglish-pretrained extractors and fine-tuning models on our Indonesian dataset.\nExperimental results demonstrate that this strategy improves performance across\nall tasks and metrics. We release our dataset publicly to support future\nresearch and hope it will inspire further progress in Indonesian multimodal\nlearning."}
{"id": "2312.11462", "pdf": "https://arxiv.org/pdf/2312.11462.pdf", "abs": "https://arxiv.org/abs/2312.11462", "title": "Cascade Speculative Drafting for Even Faster LLM Inference", "authors": ["Ziyi Chen", "Xiaocong Yang", "Jiacheng Lin", "Chenkai Sun", "Kevin Chen-Chuan Chang", "Jie Huang"], "categories": ["cs.LG", "cs.CL"], "comment": "NeurIPS 2024", "summary": "Introduced to enhance the efficiency of large language model (LLM) inference,\nspeculative decoding operates by having a smaller model generate a draft. A\nlarger target model then reviews this draft to align with its output, and any\nacceptance by the target model results in a reduction of the number of the\ntarget model runs, ultimately improving efficiency. However, the drafting\nprocess in speculative decoding includes slow autoregressive generation and\nallocates equal time to generating tokens, irrespective of their importance.\nThese inefficiencies collectively contribute to the suboptimal performance of\nspeculative decoding. To further improve LLM inference, we introduce Cascade\nSpeculative Drafting (CS Drafting), a speculative execution algorithm that\nincorporates two types of cascades. The Vertical Cascade eliminates\nautoregressive generation from neural models, while the Horizontal Cascade\noptimizes time allocation in drafting for improved efficiency. Combining both\ncascades, CS Drafting achieves greater speedup compared to the baselines in our\nexperiments, while preserving the same output distribution as the target model."}
{"id": "2409.06377", "pdf": "https://arxiv.org/pdf/2409.06377.pdf", "abs": "https://arxiv.org/abs/2409.06377", "title": "MoRE: A Mixture of Reflectors Framework for Large Language Model-Based Sequential Recommendation", "authors": ["Weicong Qin", "Yi Xu", "Weijie Yu", "Chenglei Shen", "Xiao Zhang", "Ming He", "Jianping Fan", "Jun Xu"], "categories": ["cs.IR", "cs.CL"], "comment": "First 2 authors contributes equally to this work, accepted by\n  RecSys'25 spotlight oral. Corresponding author is Weijie Yu(yu@uibe.edu.cn)", "summary": "Large language models (LLMs) have emerged as a cutting-edge approach in\nsequential recommendation, leveraging historical interactions to model dynamic\nuser preferences. Current methods mainly focus on learning processed\nrecommendation data in the form of sequence-to-sequence text. While effective,\nthey exhibit three key limitations: 1) failing to decouple intra-user explicit\nfeatures (e.g., product titles) from implicit behavioral patterns (e.g., brand\nloyalty) within interaction histories; 2) underutilizing cross-user\ncollaborative filtering (CF) signals; and 3) relying on inefficient reflection\nupdate strategies. To address this, We propose MoRE (Mixture of REflectors),\nwhich introduces three perspective-aware offline reflection processes to\naddress these gaps. This decomposition directly resolves Challenges 1\n(explicit/implicit ambiguity) and 2 (CF underutilization). Furthermore, MoRE's\nmeta-reflector employs a self-improving strategy and a dynamic selection\nmechanism (Challenge 3) to adapt to evolving user preferences. First, two\nintra-user reflectors decouple explicit and implicit patterns from a user's\ninteraction sequence, mimicking traditional recommender systems' ability to\ndistinguish surface-level and latent preferences. A third cross-user reflector\ncaptures CF signals by analyzing user similarity patterns from multiple users'\ninteractions. To optimize reflection quality, MoRE's meta-reflector employs a\noffline self-improving strategy that evaluates reflection impacts through\ncomparisons of presence/absence and iterative refinement of old/new versions,\nwith a online contextual bandit mechanism dynamically selecting the optimal\nperspective for recommendation for each user. Code:\nhttps://github.com/E-qin/MoRE-Rec."}
{"id": "2410.06238", "pdf": "https://arxiv.org/pdf/2410.06238.pdf", "abs": "https://arxiv.org/abs/2410.06238", "title": "EVOLvE: Evaluating and Optimizing LLMs For In-Context Exploration", "authors": ["Allen Nie", "Yi Su", "Bo Chang", "Jonathan N. Lee", "Ed H. Chi", "Quoc V. Le", "Minmin Chen"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "28 pages. Published at ICML 2025", "summary": "Despite their success in many domains, large language models (LLMs) remain\nunder-studied in scenarios requiring optimal decision-making under uncertainty.\nThis is crucial as many real-world applications, ranging from personalized\nrecommendations to healthcare interventions, demand that LLMs not only predict\nbut also actively learn to make optimal decisions through exploration. In this\nwork, we measure LLMs' (in)ability to make optimal decisions in bandits, a\nstate-less reinforcement learning setting relevant to many applications. We\ndevelop a comprehensive suite of environments, including both context-free and\ncontextual bandits with varying task difficulties, to benchmark LLMs'\nperformance. Motivated by the existence of optimal exploration algorithms, we\npropose efficient ways to integrate this algorithmic knowledge into LLMs: by\nproviding explicit algorithm-guided support during inference; and through\nalgorithm distillation via in-context demonstrations and fine-tuning, using\nsynthetic data generated from these algorithms. Impressively, these techniques\nallow us to achieve superior exploration performance with smaller models,\nsurpassing larger models on various tasks. We conducted an extensive ablation\nstudy to shed light on various factors, such as task difficulty and data\nrepresentation, that influence the efficiency of LLM exploration. Additionally,\nwe conduct a rigorous analysis of the LLM's exploration efficiency using the\nconcept of regret, linking its ability to explore to the model size and\nunderlying algorithm."}
{"id": "2410.15595", "pdf": "https://arxiv.org/pdf/2410.15595.pdf", "abs": "https://arxiv.org/abs/2410.15595", "title": "A Comprehensive Survey of Direct Preference Optimization: Datasets, Theories, Variants, and Applications", "authors": ["Wenyi Xiao", "Zechuan Wang", "Leilei Gan", "Shuai Zhao", "Zongrui Li", "Ruirui Lei", "Wanggui He", "Luu Anh Tuan", "Long Chen", "Hao Jiang", "Zhou Zhao", "Fei Wu"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "45 pages, 12 Figures. Project page:\n  https://github.com/Mr-Loevan/DPO-Survey", "summary": "With the rapid advancement of large language models (LLMs), aligning policy\nmodels with human preferences has become increasingly critical. Direct\nPreference Optimization (DPO) has emerged as a promising approach for\nalignment, acting as an RL-free alternative to Reinforcement Learning from\nHuman Feedback (RLHF). Despite DPO's various advancements and inherent\nlimitations, an in-depth review of these aspects is currently lacking in the\nliterature. In this work, we present a comprehensive review of the challenges\nand opportunities in DPO, covering theoretical analyses, variants, relevant\npreference datasets, and applications. Specifically, we categorize recent\nstudies on DPO based on key research questions to provide a thorough\nunderstanding of DPO's current landscape. Additionally, we propose several\nfuture research directions to offer insights on model alignment for the\nresearch community. An updated collection of relevant papers can be found on\nhttps://github.com/Mr-Loevan/DPO-Survey."}
{"id": "2411.03493", "pdf": "https://arxiv.org/pdf/2411.03493.pdf", "abs": "https://arxiv.org/abs/2411.03493", "title": "LASER: Attention with Exponential Transformation", "authors": ["Sai Surya Duvvuri", "Inderjit S. Dhillon"], "categories": ["cs.LG", "cs.CL"], "comment": "ICML 2025", "summary": "Transformers have had tremendous impact for several sequence related tasks,\nlargely due to their ability to retrieve from any part of the sequence via\nsoftmax based dot-product attention. This mechanism plays a crucial role in\nTransformer's performance. We analyze the gradients backpropagated through the\nsoftmax operation in the attention mechanism and observe that these gradients\ncan often be small. This poor gradient signal backpropagation can lead to\ninefficient learning of parameters preceeding the attention operations. To this\nend, we introduce a new attention mechanism called LASER, which we analytically\nshow to admit a larger gradient signal. We show that LASER attention can be\nimplemented by making small modifications to existing attention\nimplementations. We conduct experiments on autoregressive large language models\n(LLMs) with upto 7.7 billion parameters with an average improvement of upto\n1.44% over standard attention on downstream evaluations and 1.65% finetuning\nimprovements. Additionally, LASER demonstrates generalization performance\nimprovement across a variety of tasks (vision, text and speech):Vision\nTransformer (ViT) on Imagenet, Conformer on the Librispeech speech-to-text and\nBERT with 2.2 billion parameters."}
{"id": "2412.17739", "pdf": "https://arxiv.org/pdf/2412.17739.pdf", "abs": "https://arxiv.org/abs/2412.17739", "title": "Fourier Position Embedding: Enhancing Attention's Periodic Extension for Length Generalization", "authors": ["Ermo Hua", "Che Jiang", "Xingtai Lv", "Kaiyan Zhang", "Youbang Sun", "Yuchen Fan", "Xuekai Zhu", "Biqing Qi", "Ning Ding", "Bowen Zhou"], "categories": ["cs.AI", "cs.CL"], "comment": "Accepted to ICML 2025", "summary": "Extending the context length of Language Models (LMs) by improving Rotary\nPosition Embedding (RoPE) has become a trend. While prior works mainly address\nRoPE's limitations within attention, this paper uncovers the adverse effects on\nlength generalization from nearly all parts of LMs. Using Discrete Signal\nProcessing theory, we show that RoPE enables periodic attention by implicitly\nachieving Non-Uniform Discrete Fourier Transform. However, this periodicity is\nundermined by the spectrum damage caused by: 1) linear layers and activation\nfunctions; 2) insufficiently trained frequency components brought by\ntime-domain truncation. Building on our observations, we propose Fourier\nPosition Embedding (FoPE), which enhances attention's frequency-domain\nproperties to improve both its periodic extension and length generalization.\nFoPE constructs \\textit{Fourier Series} and zero-outs the destructive frequency\ncomponents, increasing model robustness against the spectrum damage.\nExperiments across various model scales and benchmarks show that, within\nvarying context windows, FoPE maintains a more stable performance compared to\nother baselines. Several analyses and ablations bring further support to our\nmethod and theoretical modeling."}
{"id": "2501.06848", "pdf": "https://arxiv.org/pdf/2501.06848.pdf", "abs": "https://arxiv.org/abs/2501.06848", "title": "A General Framework for Inference-time Scaling and Steering of Diffusion Models", "authors": ["Raghav Singhal", "Zachary Horvitz", "Ryan Teehan", "Mengye Ren", "Zhou Yu", "Kathleen McKeown", "Rajesh Ranganath"], "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Diffusion models produce impressive results in modalities ranging from images\nand video to protein design and text. However, generating samples with\nuser-specified properties remains a challenge. Recent research proposes\nfine-tuning models to maximize rewards that capture desired properties, but\nthese methods require expensive training and are prone to mode collapse. In\nthis work, we present Feynman-Kac (FK) steering, an inference-time framework\nfor steering diffusion models with reward functions. FK steering works by\nsampling a system of multiple interacting diffusion processes, called\nparticles, and resampling particles at intermediate steps based on scores\ncomputed using functions called potentials. Potentials are defined using\nrewards for intermediate states and are selected such that a high value\nindicates that the particle will yield a high-reward sample. We explore various\nchoices of potentials, intermediate rewards, and samplers. We evaluate FK\nsteering on text-to-image and text diffusion models. For steering text-to-image\nmodels with a human preference reward, we find that FK steering a 0.8B\nparameter model outperforms a 2.6B parameter fine-tuned model on prompt\nfidelity, with faster sampling and no training. For steering text diffusion\nmodels with rewards for text quality and specific text attributes, we find that\nFK steering generates lower perplexity, more linguistically acceptable outputs\nand enables gradient-free control of attributes like toxicity. Our results\ndemonstrate that inference-time scaling and steering of diffusion models - even\nwith off-the-shelf rewards - can provide significant sample quality gains and\ncontrollability benefits. Code is available at\nhttps://github.com/zacharyhorvitz/Fk-Diffusion-Steering ."}
{"id": "2502.06806", "pdf": "https://arxiv.org/pdf/2502.06806.pdf", "abs": "https://arxiv.org/abs/2502.06806", "title": "Logits are All We Need to Adapt Closed Models", "authors": ["Gaurush Hiranandani", "Haolun Wu", "Subhojyoti Mukherjee", "Sanmi Koyejo"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "29 pages, 8 figures", "summary": "Many commercial Large Language Models (LLMs) are often closed-source,\nlimiting developers to prompt tuning for aligning content generation with\nspecific applications. While these models currently do not provide access to\ntoken logits, we argue that if such access were available, it would enable more\npowerful adaptation techniques beyond prompt engineering. In this paper, we\npropose a token-level probability reweighting framework that, given access to\nlogits and a small amount of task-specific data, can effectively steer\nblack-box LLMs toward application-specific content generation. Our approach\nviews next-token prediction through the lens of supervised classification. We\nshow that aligning black-box LLMs with task-specific data can be formulated as\na label noise correction problem, leading to Plugin model -- an autoregressive\nprobability reweighting model that operates solely on logits. We provide\ntheoretical justification for why reweighting logits alone is sufficient for\ntask adaptation. Extensive experiments with multiple datasets, LLMs, and\nreweighting models demonstrate the effectiveness of our method, advocating for\nbroader access to token logits in closed-source models."}
{"id": "2502.15902", "pdf": "https://arxiv.org/pdf/2502.15902.pdf", "abs": "https://arxiv.org/abs/2502.15902", "title": "IPAD: Inverse Prompt for AI Detection -- A Robust and Explainable LLM-Generated Text Detector", "authors": ["Zheng Chen", "Yushi Feng", "Changyang He", "Yue Deng", "Hongxi Pu", "Bo Li"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have attained human-level fluency in text\ngeneration, which complicates the distinction between human-written and\nLLM-generated texts. This increases the risk of misuse and highlights the need\nfor reliable detectors. Yet, existing detectors exhibit poor robustness on\nout-of-distribution (OOD) data and attacked data, which is critical for\nreal-world scenarios. Also, they struggle to provide interpretable evidence to\nsupport their decisions, thus undermining the reliability. In light of these\nchallenges, we propose IPAD (Inverse Prompt for AI Detection), a novel\nframework consisting of a Prompt Inverter that identifies predicted prompts\nthat could have generated the input text, and two Distinguishers that examine\nthe probability that the input texts align with the predicted prompts.\nEmpirical evaluations demonstrate that IPAD outperforms the strongest baselines\nby 9.05% (Average Recall) on in-distribution data, 12.93% (AUROC) on\nout-of-distribution (OOD) data, and 5.48% (AUROC) on attacked data. IPAD also\nperforms robustly on structured datasets. Furthermore, an interpretability\nassessment is conducted to illustrate that IPAD enhances the AI detection\ntrustworthiness by allowing users to directly examine the decision-making\nevidence, which provides interpretable support for its state-of-the-art\ndetection results."}
{"id": "2503.02951", "pdf": "https://arxiv.org/pdf/2503.02951.pdf", "abs": "https://arxiv.org/abs/2503.02951", "title": "KodCode: A Diverse, Challenging, and Verifiable Synthetic Dataset for Coding", "authors": ["Zhangchen Xu", "Yang Liu", "Yueqin Yin", "Mingyuan Zhou", "Radha Poovendran"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted by ACL 2025. Codes and Data: https://kodcode-ai.github.io/", "summary": "We introduce KodCode, a synthetic dataset that addresses the persistent\nchallenge of acquiring high-quality, verifiable training data across diverse\ndifficulties and domains for training Large Language Models for coding.\nExisting code-focused resources typically fail to ensure either the breadth of\ncoverage (e.g., spanning simple coding tasks to advanced algorithmic problems)\nor verifiable correctness (e.g., unit tests). In contrast, KodCode comprises\nquestion-solution-test triplets that are systematically validated via a\nself-verification procedure. Our pipeline begins by synthesizing a broad range\nof coding questions, then generates solutions and test cases with additional\nattempts allocated to challenging problems. Finally, post-training data\nsynthesis is done by rewriting questions into diverse formats and generating\nresponses under a test-based reject sampling procedure from a reasoning model\n(DeepSeek R1). This pipeline yields a large-scale, robust and diverse coding\ndataset. KodCode is suitable for supervised fine-tuning and the paired unit\ntests also provide great potential for RL tuning. Fine-tuning experiments on\ncoding benchmarks (HumanEval(+), MBPP(+), BigCodeBench, and LiveCodeBench)\ndemonstrate that KodCode-tuned models achieve state-of-the-art performance,\nsurpassing models like Qwen2.5-Coder-32B-Instruct and\nDeepSeek-R1-Distill-Llama-70B."}
{"id": "2503.06241", "pdf": "https://arxiv.org/pdf/2503.06241.pdf", "abs": "https://arxiv.org/abs/2503.06241", "title": "A Noise-Robust Turn-Taking System for Real-World Dialogue Robots: A Field Experiment", "authors": ["Koji Inoue", "Yuki Okafuji", "Jun Baba", "Yoshiki Ohira", "Katsuya Hyodo", "Tatsuya Kawahara"], "categories": ["cs.RO", "cs.CL", "cs.SD"], "comment": "This paper has been accepted for presentation at IEEE/RSJ\n  International Conference on Intelligent Robots and Systems 2025 (IROS 2025)\n  and represents the author's version of the work", "summary": "Turn-taking is a crucial aspect of human-robot interaction, directly\ninfluencing conversational fluidity and user engagement. While previous\nresearch has explored turn-taking models in controlled environments, their\nrobustness in real-world settings remains underexplored. In this study, we\npropose a noise-robust voice activity projection (VAP) model, based on a\nTransformer architecture, to enhance real-time turn-taking in dialogue robots.\nTo evaluate the effectiveness of the proposed system, we conducted a field\nexperiment in a shopping mall, comparing the VAP system with a conventional\ncloud-based speech recognition system. Our analysis covered both subjective\nuser evaluations and objective behavioral analysis. The results showed that the\nproposed system significantly reduced response latency, leading to a more\nnatural conversation where both the robot and users responded faster. The\nsubjective evaluations suggested that faster responses contribute to a better\ninteraction experience."}
{"id": "2503.09639", "pdf": "https://arxiv.org/pdf/2503.09639.pdf", "abs": "https://arxiv.org/abs/2503.09639", "title": "Can A Society of Generative Agents Simulate Human Behavior and Inform Public Health Policy? A Case Study on Vaccine Hesitancy", "authors": ["Abe Bohan Hou", "Hongru Du", "Yichen Wang", "Jingyu Zhang", "Zixiao Wang", "Paul Pu Liang", "Daniel Khashabi", "Lauren Gardner", "Tianxing He"], "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.CY", "cs.HC"], "comment": "Accepted to COLM 2025", "summary": "Can we simulate a sandbox society with generative agents to model human\nbehavior, thereby reducing the over-reliance on real human trials for assessing\npublic policies? In this work, we investigate the feasibility of simulating\nhealth-related decision-making, using vaccine hesitancy, defined as the delay\nin acceptance or refusal of vaccines despite the availability of vaccination\nservices (MacDonald, 2015), as a case study. To this end, we introduce the\nVacSim framework with 100 generative agents powered by Large Language Models\n(LLMs). VacSim simulates vaccine policy outcomes with the following steps: 1)\ninstantiate a population of agents with demographics based on census data; 2)\nconnect the agents via a social network and model vaccine attitudes as a\nfunction of social dynamics and disease-related information; 3) design and\nevaluate various public health interventions aimed at mitigating vaccine\nhesitancy. To align with real-world results, we also introduce simulation\nwarmup and attitude modulation to adjust agents' attitudes. We propose a series\nof evaluations to assess the reliability of various LLM simulations.\nExperiments indicate that models like Llama and Qwen can simulate aspects of\nhuman behavior but also highlight real-world alignment challenges, such as\ninconsistent responses with demographic profiles. This early exploration of\nLLM-driven simulations is not meant to serve as definitive policy guidance;\ninstead, it serves as a call for action to examine social simulation for policy\ndevelopment."}
{"id": "2504.01450", "pdf": "https://arxiv.org/pdf/2504.01450.pdf", "abs": "https://arxiv.org/abs/2504.01450", "title": "CASCADE Your Datasets for Cross-Mode Knowledge Retrieval of Language Models", "authors": ["Runlong Zhou", "Yi Zhang"], "categories": ["cs.LG", "cs.CL"], "comment": "COLM 2025", "summary": "Language models often struggle with cross-mode knowledge retrieval -- the\nability to access knowledge learned in one format (mode) when queried in\nanother. We demonstrate that models trained on multiple data sources (e.g.,\nWikipedia and TinyStories) exhibit significantly reduced accuracy when\nretrieving knowledge in a format different from its original training mode.\nThis paper quantitatively investigates this phenomenon through a controlled\nstudy of random token sequence memorization across different modes. We first\nexplore dataset rewriting as a solution, revealing that effective cross-mode\nretrieval requires prohibitively extensive rewriting efforts that follow a\nsigmoid-like relationship. As an alternative, we propose CASCADE, a novel\npretraining algorithm that uses cascading datasets with varying sequence\nlengths and computing losses on only the second half of each training sequence\nto capture knowledge at different scales. Our experiments demonstrate that\nCASCADE outperforms dataset rewriting approaches, even when compressed into a\nsingle model with a unified loss function. This work provides both qualitative\nevidence of cross-mode retrieval limitations and a practical solution to\nenhance language models' ability to access knowledge independently of its\npresentational format."}
{"id": "2504.11393", "pdf": "https://arxiv.org/pdf/2504.11393.pdf", "abs": "https://arxiv.org/abs/2504.11393", "title": "DataDecide: How to Predict Best Pretraining Data with Small Experiments", "authors": ["Ian Magnusson", "Nguyen Tai", "Ben Bogin", "David Heineman", "Jena D. Hwang", "Luca Soldaini", "Akshita Bhagia", "Jiacheng Liu", "Dirk Groeneveld", "Oyvind Tafjord", "Noah A. Smith", "Pang Wei Koh", "Jesse Dodge"], "categories": ["cs.LG", "cs.CL"], "comment": "ICML 2025", "summary": "Because large language models are expensive to pretrain on different\ndatasets, using smaller-scale experiments to decide on data is crucial for\nreducing costs. Which benchmarks and methods of making decisions from observed\nperformance at small scale most accurately predict the datasets that yield the\nbest large models? To empower open exploration of this question, we release\nmodels, data, and evaluations in DataDecide -- the most extensive open suite of\nmodels over differences in data and scale. We conduct controlled pretraining\nexperiments across 25 corpora with differing sources, deduplication, and\nfiltering up to 100B tokens, model sizes up to 1B parameters, and 3 random\nseeds. We find that the ranking of models at a single, small size (e.g., 150M\nparameters) is a strong baseline for predicting best models at our larger\ntarget scale (1B) (~80% of com parisons correct). No scaling law methods among\n8 baselines exceed the compute-decision frontier of single-scale predictions,\nbut DataDecide can measure improvement in future scaling laws. We also identify\nthat using continuous likelihood metrics as proxies in small experiments makes\nbenchmarks including MMLU, ARC, HellaSwag, MBPP, and HumanEval >80% predictable\nat the target 1B scale with just 0.01% of the compute."}
{"id": "2504.15266", "pdf": "https://arxiv.org/pdf/2504.15266.pdf", "abs": "https://arxiv.org/abs/2504.15266", "title": "Roll the dice & look before you leap: Going beyond the creative limits of next-token prediction", "authors": ["Vaishnavh Nagarajan", "Chen Henry Wu", "Charles Ding", "Aditi Raghunathan"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "ICML 2025 (oral)", "summary": "We design a suite of minimal algorithmic tasks that are a loose abstraction\nof open-ended real-world tasks. This allows us to cleanly and controllably\nquantify the creative limits of the present-day language model. Much like\nreal-world tasks that require a creative, far-sighted leap of thought, our\ntasks require an implicit, open-ended stochastic planning step that either (a)\ndiscovers new connections in an abstract knowledge graph (like in wordplay,\ndrawing analogies, or research) or (b) constructs new patterns (like in\ndesigning math problems or new proteins). In these tasks, we empirically and\nconceptually argue how next-token learning is myopic; multi-token approaches,\nnamely teacherless training and diffusion models, comparatively excel in\nproducing diverse and original output. Secondly, to elicit randomness without\nhurting coherence, we find that injecting noise at the input layer (dubbed\nseed-conditioning) works surprisingly as well as (and in some conditions,\nbetter than) temperature sampling from the output layer. Thus, our work offers\na principled, minimal test-bed for analyzing open-ended creative skills, and\noffers new arguments for going beyond next-token learning and temperature\nsampling. We make part of the code available under\nhttps://github.com/chenwu98/algorithmic-creativity"}
{"id": "2505.12185", "pdf": "https://arxiv.org/pdf/2505.12185.pdf", "abs": "https://arxiv.org/abs/2505.12185", "title": "EVALOOP: Assessing LLM Robustness in Programming from a Self-consistency Perspective", "authors": ["Sen Fang", "Weiyuan Ding", "Bowen Xu"], "categories": ["cs.SE", "cs.CL", "cs.LG"], "comment": "20 pages, 11 figures", "summary": "Assessing the programming capabilities of Large Language Models (LLMs) is\ncrucial for their effective use in software engineering. Current evaluations,\nhowever, predominantly measure the accuracy of generated code on static\nbenchmarks, neglecting the critical aspect of model robustness during\nprogramming tasks. While adversarial attacks offer insights on model\nrobustness, their effectiveness is limited and evaluation could be constrained.\nCurrent adversarial attack methods for robustness evaluation yield inconsistent\nresults, struggling to provide a unified evaluation across different LLMs. We\nintroduce EVALOOP, a novel assessment framework that evaluate the robustness\nfrom a self-consistency perspective, i.e., leveraging the natural duality\ninherent in popular software engineering tasks, e.g., code generation and code\nsummarization. EVALOOP initiates a self-contained feedback loop: an LLM\ngenerates output (e.g., code) from an input (e.g., natural language\nspecification), and then use the generated output as the input to produce a new\noutput (e.g., summarizes that code into a new specification). EVALOOP repeats\nthe process to assess the effectiveness of EVALOOP in each loop. This cyclical\nstrategy intrinsically evaluates robustness without rely on any external attack\nsetups, providing a unified metric to evaluate LLMs' robustness in programming.\nWe evaluate 16 prominent LLMs (e.g., GPT-4.1, O4-mini) on EVALOOP and found\nthat EVALOOP typically induces a 5.01%-19.31% absolute drop in pass@1\nperformance within ten loops. Intriguingly, robustness does not always align\nwith initial performance (i.e., one-time query); for instance, GPT-3.5-Turbo,\ndespite superior initial code generation compared to DeepSeek-V2, demonstrated\nlower robustness over repeated evaluation loop."}
{"id": "2505.17826", "pdf": "https://arxiv.org/pdf/2505.17826.pdf", "abs": "https://arxiv.org/abs/2505.17826", "title": "Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement Fine-Tuning of Large Language Models", "authors": ["Xuchen Pan", "Yanxi Chen", "Yushuo Chen", "Yuchang Sun", "Daoyuan Chen", "Wenhao Zhang", "Yuexiang Xie", "Yilun Huang", "Yilei Zhang", "Dawei Gao", "Weijie Shi", "Yaliang Li", "Bolin Ding", "Jingren Zhou"], "categories": ["cs.LG", "cs.CL", "cs.DC"], "comment": "This technical report will be continuously updated as the codebase\n  evolves. GitHub: https://github.com/modelscope/Trinity-RFT", "summary": "Trinity-RFT is a general-purpose, unified and easy-to-use framework designed\nfor reinforcement fine-tuning (RFT) of large language models. It is built with\na modular and decoupled design, consisting of (1) an RFT-core that unifies and\ngeneralizes synchronous/asynchronous, on-policy/off-policy, and online/offline\nmodes of RFT; (2) seamless integration for agent-environment interaction with\nhigh efficiency and robustness; and (3) systematic data pipelines optimized for\nRFT. Trinity-RFT can be easily adapted for diverse application scenarios, and\nserves as a unified platform for development and research of advanced\nreinforcement learning paradigms at both macroscopic and microscopic levels.\nThis technical report outlines the vision, features, design and implementations\nof Trinity-RFT, accompanied by extensive examples, applications and experiments\nthat demonstrate its functionalities and user-friendliness."}
{"id": "2505.24859", "pdf": "https://arxiv.org/pdf/2505.24859.pdf", "abs": "https://arxiv.org/abs/2505.24859", "title": "Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive Free-Form Summarization", "authors": ["Joschka Braun", "Carsten Eickhoff", "Seyed Ali Bahrainian"], "categories": ["cs.LG", "cs.CL"], "comment": "29 pages, 21 figures, published at ICML 2025 Workshop on Reliable and\n  Responsible Foundation Models", "summary": "Steering vectors are a lightweight method for controlling text properties by\nadding a learned bias to language model activations at inference time. So far,\nsteering vectors have predominantly been evaluated in multiple-choice settings,\nwhile their effectiveness in free-form generation tasks remains understudied.\nMoving \"Beyond Multiple Choice,\" we thoroughly evaluate the effectiveness of\nsteering vectors in adaptively controlling topical focus, sentiment, toxicity,\nand readability in abstractive summaries of the NEWTS dataset. We find that\nsteering effectively controls the targeted summary properties, but high\nsteering strengths consistently degrade both intrinsic and extrinsic text\nquality. Compared to steering, prompting offers weaker control, while\npreserving text quality. Combining steering and prompting yields the strongest\ncontrol over text properties and offers the most favorable efficacy-quality\ntrade-off at moderate steering strengths. Our results underscore the practical\ntrade-off between control strength and text quality preservation when applying\nsteering vectors to free-form generation tasks."}
{"id": "2506.10521", "pdf": "https://arxiv.org/pdf/2506.10521.pdf", "abs": "https://arxiv.org/abs/2506.10521", "title": "Scientists' First Exam: Probing Cognitive Abilities of MLLM via Perception, Understanding, and Reasoning", "authors": ["Yuhao Zhou", "Yiheng Wang", "Xuming He", "Ruoyao Xiao", "Zhiwei Li", "Qiantai Feng", "Zijie Guo", "Yuejin Yang", "Hao Wu", "Wenxuan Huang", "Jiaqi Wei", "Dan Si", "Xiuqi Yao", "Jia Bu", "Haiwen Huang", "Tianfan Fu", "Shixiang Tang", "Ben Fei", "Dongzhan Zhou", "Fenghua Ling", "Yan Lu", "Siqi Sun", "Chenhui Li", "Guanjie Zheng", "Jiancheng Lv", "Wenlong Zhang", "Lei Bai"], "categories": ["cs.AI", "cs.CL"], "comment": "82 pages", "summary": "Scientific discoveries increasingly rely on complex multimodal reasoning\nbased on information-intensive scientific data and domain-specific expertise.\nEmpowered by expert-level scientific benchmarks, scientific Multimodal Large\nLanguage Models (MLLMs) hold the potential to significantly enhance this\ndiscovery process in realistic workflows. However, current scientific\nbenchmarks mostly focus on evaluating the knowledge understanding capabilities\nof MLLMs, leading to an inadequate assessment of their perception and reasoning\nabilities. To address this gap, we present the Scientists' First Exam (SFE)\nbenchmark, designed to evaluate the scientific cognitive capacities of MLLMs\nthrough three interconnected levels: scientific signal perception, scientific\nattribute understanding, scientific comparative reasoning. Specifically, SFE\ncomprises 830 expert-verified VQA pairs across three question types, spanning\n66 multimodal tasks across five high-value disciplines. Extensive experiments\nreveal that current state-of-the-art GPT-o3 and InternVL-3 achieve only 34.08%\nand 26.52% on SFE, highlighting significant room for MLLMs to improve in\nscientific realms. We hope the insights obtained in SFE will facilitate further\ndevelopments in AI-enhanced scientific discoveries."}
{"id": "2506.12981", "pdf": "https://arxiv.org/pdf/2506.12981.pdf", "abs": "https://arxiv.org/abs/2506.12981", "title": "SymRAG: Efficient Neuro-Symbolic Retrieval Through Adaptive Query Routing", "authors": ["Safayat Bin Hakim", "Muhammad Adil", "Alvaro Velasquez", "Houbing Herbert Song"], "categories": ["cs.AI", "cs.CL", "cs.IR"], "comment": "Accepted at 19th International Conference on Neurosymbolic Learning\n  and Reasoning (NeSy 2025)", "summary": "Current Retrieval-Augmented Generation systems use uniform processing,\ncausing inefficiency as simple queries consume resources similar to complex\nmulti-hop tasks. We present SymRAG, a framework that introduces adaptive query\nrouting via real-time complexity and load assessment to select symbolic,\nneural, or hybrid pathways. SymRAG's neuro-symbolic approach adjusts\ncomputational pathways based on both query characteristics and system load,\nenabling efficient resource allocation across diverse query types. By combining\nlinguistic and structural query properties with system load metrics, SymRAG\nallocates resources proportional to reasoning requirements. Evaluated on 2,000\nqueries across HotpotQA (multi-hop reasoning) and DROP (discrete reasoning)\nusing Llama-3.2-3B and Mistral-7B models, SymRAG achieves competitive accuracy\n(97.6--100.0% exact match) with efficient resource utilization (3.6--6.2% CPU\nutilization, 0.985--3.165s processing). Disabling adaptive routing increases\nprocessing time by 169--1151%, showing its significance for complex models.\nThese results suggest adaptive computation strategies are more sustainable and\nscalable for hybrid AI systems that use dynamic routing and neuro-symbolic\nframeworks."}
{"id": "2506.23978", "pdf": "https://arxiv.org/pdf/2506.23978.pdf", "abs": "https://arxiv.org/abs/2506.23978", "title": "LLM Agents Are the Antidote to Walled Gardens", "authors": ["Samuele Marro", "Philip Torr"], "categories": ["cs.LG", "cs.CL", "cs.CY", "cs.SI", "68T50, 68M10, 91B26", "I.2.11; I.2.7; H.4.5"], "comment": null, "summary": "While the Internet's core infrastructure was designed to be open and\nuniversal, today's application layer is dominated by closed, proprietary\nplatforms. Open and interoperable APIs require significant investment, and\nmarket leaders have little incentive to enable data exchange that could erode\ntheir user lock-in. We argue that LLM-based agents fundamentally disrupt this\nstatus quo. Agents can automatically translate between data formats and\ninteract with interfaces designed for humans: this makes interoperability\ndramatically cheaper and effectively unavoidable. We name this shift universal\ninteroperability: the ability for any two digital services to exchange data\nseamlessly using AI-mediated adapters. Universal interoperability undermines\nmonopolistic behaviours and promotes data portability. However, it can also\nlead to new security risks and technical debt. Our position is that the ML\ncommunity should embrace this development while building the appropriate\nframeworks to mitigate the downsides. By acting now, we can harness AI to\nrestore user freedom and competitive markets without sacrificing security."}
{"id": "2507.01504", "pdf": "https://arxiv.org/pdf/2507.01504.pdf", "abs": "https://arxiv.org/abs/2507.01504", "title": "Following the Clues: Experiments on Person Re-ID using Cross-Modal Intelligence", "authors": ["Robert AufschlÃ¤ger", "Youssef Shoeb", "Azarm Nowzad", "Michael Heigl", "Fabian Bally", "Martin Schramm"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "accepted for publication at the 2025 IEEE 28th International\n  Conference on Intelligent Transportation Systems (ITSC 2025), taking place\n  during November 18-21, 2025 in Gold Coast, Australia", "summary": "The collection and release of street-level recordings as Open Data play a\nvital role in advancing autonomous driving systems and AI research. However,\nthese datasets pose significant privacy risks, particularly for pedestrians,\ndue to the presence of Personally Identifiable Information (PII) that extends\nbeyond biometric traits such as faces. In this paper, we present cRID, a novel\ncross-modal framework combining Large Vision-Language Models, Graph Attention\nNetworks, and representation learning to detect textual describable clues of\nPII and enhance person re-identification (Re-ID). Our approach focuses on\nidentifying and leveraging interpretable features, enabling the detection of\nsemantically meaningful PII beyond low-level appearance cues. We conduct a\nsystematic evaluation of PII presence in person image datasets. Our experiments\nshow improved performance in practical cross-dataset Re-ID scenarios, notably\nfrom Market-1501 to CUHK03-np (detected), highlighting the framework's\npractical utility. Code is available at https://github.com/RAufschlaeger/cRID."}
{"id": "2507.03147", "pdf": "https://arxiv.org/pdf/2507.03147.pdf", "abs": "https://arxiv.org/abs/2507.03147", "title": "DeepGesture: A conversational gesture synthesis system based on emotions and semantics", "authors": ["Thanh Hoang-Minh"], "categories": ["cs.HC", "cs.CL", "cs.LG", "cs.SD", "eess.AS"], "comment": "Project page: https://deepgesture.github.io", "summary": "Along with the explosion of large language models, improvements in speech\nsynthesis, advancements in hardware, and the evolution of computer graphics,\nthe current bottleneck in creating digital humans lies in generating character\nmovements that correspond naturally to text or speech inputs.\n  In this work, we present DeepGesture, a diffusion-based gesture synthesis\nframework for generating expressive co-speech gestures conditioned on\nmultimodal signals - text, speech, emotion, and seed motion. Built upon the\nDiffuseStyleGesture model, DeepGesture introduces novel architectural\nenhancements that improve semantic alignment and emotional expressiveness in\ngenerated gestures. Specifically, we integrate fast text transcriptions as\nsemantic conditioning and implement emotion-guided classifier-free diffusion to\nsupport controllable gesture generation across affective states. To visualize\nresults, we implement a full rendering pipeline in Unity based on BVH output\nfrom the model. Evaluation on the ZeroEGGS dataset shows that DeepGesture\nproduces gestures with improved human-likeness and contextual appropriateness.\nOur system supports interpolation between emotional states and demonstrates\ngeneralization to out-of-distribution speech, including synthetic voices -\nmarking a step forward toward fully multimodal, emotionally aware digital\nhumans.\n  Project page: https://deepgesture.github.io"}
{"id": "2507.04295", "pdf": "https://arxiv.org/pdf/2507.04295.pdf", "abs": "https://arxiv.org/abs/2507.04295", "title": "LearnLens: LLM-Enabled Personalised, Curriculum-Grounded Feedback with Educators in the Loop", "authors": ["Runcong Zhao", "Artem Bobrov", "Jiazheng Li", "Yulan He"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Effective feedback is essential for student learning but is time-intensive\nfor teachers. We present LearnLens, a modular, LLM-based system that generates\npersonalised, curriculum-aligned feedback in science education. LearnLens\ncomprises three components: (1) an error-aware assessment module that captures\nnuanced reasoning errors; (2) a curriculum-grounded generation module that uses\na structured, topic-linked memory chain rather than traditional\nsimilarity-based retrieval, improving relevance and reducing noise; and (3) an\neducator-in-the-loop interface for customisation and oversight. LearnLens\naddresses key challenges in existing systems, offering scalable, high-quality\nfeedback that empowers both teachers and students."}
{"id": "2507.05201", "pdf": "https://arxiv.org/pdf/2507.05201.pdf", "abs": "https://arxiv.org/abs/2507.05201", "title": "MedGemma Technical Report", "authors": ["Andrew Sellergren", "Sahar Kazemzadeh", "Tiam Jaroensri", "Atilla Kiraly", "Madeleine Traverse", "Timo Kohlberger", "Shawn Xu", "Fayaz Jamil", "CÃ­an Hughes", "Charles Lau", "Justin Chen", "Fereshteh Mahvar", "Liron Yatziv", "Tiffany Chen", "Bram Sterling", "Stefanie Anna Baby", "Susanna Maria Baby", "Jeremy Lai", "Samuel Schmidgall", "Lu Yang", "Kejia Chen", "Per Bjornsson", "Shashir Reddy", "Ryan Brush", "Kenneth Philbrick", "Mercy Asiedu", "Ines Mezerreg", "Howard Hu", "Howard Yang", "Richa Tiwari", "Sunny Jansen", "Preeti Singh", "Yun Liu", "Shekoofeh Azizi", "Aishwarya Kamath", "Johan Ferret", "Shreya Pathak", "Nino Vieillard", "Ramona Merhej", "Sarah Perrin", "Tatiana Matejovicova", "Alexandre RamÃ©", "Morgane Riviere", "Louis Rouillard", "Thomas Mesnard", "Geoffrey Cideron", "Jean-bastien Grill", "Sabela Ramos", "Edouard Yvinec", "Michelle Casbon", "Elena Buchatskaya", "Jean-Baptiste Alayrac", "Dmitry Lepikhin", "Vlad Feinberg", "Sebastian Borgeaud", "Alek Andreev", "Cassidy Hardin", "Robert Dadashi", "LÃ©onard Hussenot", "Armand Joulin", "Olivier Bachem", "Yossi Matias", "Katherine Chou", "Avinatan Hassidim", "Kavi Goel", "Clement Farabet", "Joelle Barral", "Tris Warkentin", "Jonathon Shlens", "David Fleet", "Victor Cotruta", "Omar Sanseviero", "Gus Martins", "Phoebe Kirk", "Anand Rao", "Shravya Shetty", "David F. Steiner", "Can Kirmizibayrak", "Rory Pilgrim", "Daniel Golden", "Lin Yang"], "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Artificial intelligence (AI) has significant potential in healthcare\napplications, but its training and deployment faces challenges due to\nhealthcare's diverse data, complex tasks, and the need to preserve privacy.\nFoundation models that perform well on medical tasks and require less\ntask-specific tuning data are critical to accelerate the development of\nhealthcare AI applications. We introduce MedGemma, a collection of medical\nvision-language foundation models based on Gemma 3 4B and 27B. MedGemma\ndemonstrates advanced medical understanding and reasoning on images and text,\nsignificantly exceeding the performance of similar-sized generative models and\napproaching the performance of task-specific models, while maintaining the\ngeneral capabilities of the Gemma 3 base models. For out-of-distribution tasks,\nMedGemma achieves 2.6-10% improvement on medical multimodal question answering,\n15.5-18.1% improvement on chest X-ray finding classification, and 10.8%\nimprovement on agentic evaluations compared to the base models. Fine-tuning\nMedGemma further improves performance in subdomains, reducing errors in\nelectronic health record information retrieval by 50% and reaching comparable\nperformance to existing specialized state-of-the-art methods for pneumothorax\nclassification and histopathology patch classification. We additionally\nintroduce MedSigLIP, a medically-tuned vision encoder derived from SigLIP.\nMedSigLIP powers the visual understanding capabilities of MedGemma and as an\nencoder achieves comparable or better performance than specialized medical\nimage encoders. Taken together, the MedGemma collection provides a strong\nfoundation of medical image and text capabilities, with potential to\nsignificantly accelerate medical research and development of downstream\napplications. The MedGemma collection, including tutorials and model weights,\ncan be found at https://goo.gle/medgemma."}
{"id": "2507.07610", "pdf": "https://arxiv.org/pdf/2507.07610.pdf", "abs": "https://arxiv.org/abs/2507.07610", "title": "SpatialViz-Bench: Automatically Generated Spatial Visualization Reasoning Tasks for MLLMs", "authors": ["Siting Wang", "Luoyang Sun", "Cheng Deng", "Kun Shao", "Minnan Pei", "Zheng Tian", "Haifeng Zhang", "Jun Wang"], "categories": ["cs.CV", "cs.CL", "cs.HC"], "comment": null, "summary": "Humans can directly imagine and manipulate visual images in their minds, a\ncapability known as spatial visualization. While multi-modal Large Language\nModels (MLLMs) support imagination-based reasoning, spatial visualization\nremains insufficiently evaluated, typically embedded within broader\nmathematical and logical assessments. Existing evaluations often rely on IQ\ntests or math competitions that may overlap with training data, compromising\nassessment reliability. To this end, we introduce SpatialViz-Bench, a\ncomprehensive multi-modal benchmark for spatial visualization with 12 tasks\nacross 4 sub-abilities, comprising 1,180 automatically generated problems. Our\nevaluation of 33 state-of-the-art MLLMs not only reveals wide performance\nvariations and demonstrates the benchmark's strong discriminative power, but\nalso uncovers counter-intuitive findings: models exhibit unexpected behaviors\nby showing difficulty perception that misaligns with human intuition,\ndisplaying dramatic 2D-to-3D performance cliffs, and defaulting to formula\nderivation despite spatial tasks requiring visualization alone. SpatialVizBench\nempirically demonstrates that state-of-the-art MLLMs continue to exhibit\ndeficiencies in spatial visualization tasks, thereby addressing a significant\nlacuna in the field. The benchmark is publicly available."}
