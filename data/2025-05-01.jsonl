{"id": "2504.21240", "pdf": "https://arxiv.org/pdf/2504.21240.pdf", "abs": "https://arxiv.org/abs/2504.21240", "title": "Countering underproduction of peer produced goods", "authors": ["Kaylea Champion", "Benjamin Mako Hill"], "categories": ["cs.HC"], "comment": "New Media & Society, 2024", "summary": "Peer produced goods such as online knowledge bases and free/libre open source\nsoftware rely on contributors who often choose their tasks regardless of\nconsumer needs. These goods are susceptible to underproduction: when popular\ngoods are relatively low quality. Although underproduction is a common feature\nof peer production, very little is known about how to counteract it. We use a\ndetailed longitudinal dataset from English Wikipedia to show that more\nexperienced contributors -- including those who contribute without an account\n-- tend to contribute to underproduced goods. A within-person analysis shows\nthat contributors' efforts shift toward underproduced goods over time. These\nfindings illustrate the value of retaining contributors in peer production,\nincluding those contributing without accounts, as a means to counter\nunderproduction."}
{"id": "2504.21242", "pdf": "https://arxiv.org/pdf/2504.21242.pdf", "abs": "https://arxiv.org/abs/2504.21242", "title": "Passive Measurement of Autonomic Arousal in Real-World Settings", "authors": ["Samy Abdel-Ghaffar", "Isaac Galatzer-Levy", "Conor Heneghan", "Xin Liu", "Sarah Kernasovskiy", "Brennan Garrett", "Andrew Barakat", "Daniel McDuff"], "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "The autonomic nervous system (ANS) is activated during stress, which can have\nnegative effects on cardiovascular health, sleep, the immune system, and mental\nhealth. While there are ways to quantify ANS activity in laboratories, there is\na paucity of methods that have been validated in real-world contexts. We\npresent the Fitbit Body Response Algorithm, an approach to continuous remote\nmeasurement of ANS activation through widely available remote wrist-based\nsensors. The design was validated via two experiments, a Trier Social Stress\nTest (n = 45) and ecological momentary assessments (EMA) of perceived stress\n(n=87), providing both controlled and ecologically valid test data. Model\nperformance predicting perceived stress when using all available sensor\nmodalities was consistent with expectations (accuracy=0.85) and outperformed\nmodels with access to only a subset of the signals. We discuss and address\nchallenges to sensing that arise in real world settings that do not present in\nconventional lab environments."}
{"id": "2504.21332", "pdf": "https://arxiv.org/pdf/2504.21332.pdf", "abs": "https://arxiv.org/abs/2504.21332", "title": "MagicCraft: Natural Language-Driven Generation of Dynamic and Interactive 3D Objects for Commercial Metaverse Platforms", "authors": ["Ryutaro Kurai", "Takefumi Hiraki", "Yuichi Hiroi", "Yutaro Hirao", "Monica Perusquía-Hernández", "Hideaki Uchiyama", "Kiyoshi Kiyokawa"], "categories": ["cs.HC"], "comment": "16 pages. Preprint for submission to IEEE Access", "summary": "Metaverse platforms are rapidly evolving to provide immersive spaces for user\ninteraction and content creation. However, the generation of dynamic and\ninteractive 3D objects remains challenging due to the need for advanced 3D\nmodeling and programming skills. To address this challenge, we present\nMagicCraft, a system that generates functional 3D objects from natural language\nprompts for metaverse platforms. MagicCraft uses generative AI models to manage\nthe entire content creation pipeline: converting user text descriptions into\nimages, transforming images into 3D models, predicting object behavior, and\nassigning necessary attributes and scripts. It also provides an interactive\ninterface for users to refine generated objects by adjusting features such as\norientation, scale, seating positions, and grip points.\n  Implemented on Cluster, a commercial metaverse platform, MagicCraft was\nevaluated by 7 expert CG designers and 51 general users. Results show that\nMagicCraft significantly reduces the time and skill required to create 3D\nobjects. Users with no prior experience in 3D modeling or programming\nsuccessfully created complex, interactive objects and deployed them in the\nmetaverse. Expert feedback highlighted the system's potential to improve\ncontent creation workflows and support rapid prototyping. By integrating\nAI-generated content into metaverse platforms, MagicCraft makes 3D content\ncreation more accessible."}
{"id": "2504.21337", "pdf": "https://arxiv.org/pdf/2504.21337.pdf", "abs": "https://arxiv.org/abs/2504.21337", "title": "Cross-Reality Lifestyle: Integrating Physical and Virtual Lives through Multi-Platform Metaverse", "authors": ["Yuichi Hiroi", "Yuji Hatada", "Takefumi Hiraki"], "categories": ["cs.HC"], "comment": "9 pages, preprint for submission to IEEE Pervasive Computing Special\n  Issue", "summary": "Technological advances are redefining the relationship between physical and\nvirtual space. Traditionally, when users engage in virtual reality (VR), they\nare completely cut off from the physical space; similarly, they are unable to\naccess virtual experiences while engaged in physical activities. However,\nmodern multi-platform metaverse environments allow simultaneous participation\nthrough mobile devices, creating new opportunities for integrated experiences.\nThis study introduces the concept of \"cross-reality lifestyles\" to examine how\nusers actively combine their physical and virtual activities. We identify three\npatterns of integration: 1) amplification: one space enhances experiences in\nthe other; 2) complementary: spaces offer different but equally valuable\nalternatives; and 3) emergence: simultaneous engagement creates entirely new\nexperiences. By analyzing commercial platforms, we create a technical framework\nthat addresses content design, platform infrastructure, and device interfaces.\nThis framework guides the development of cross-reality applications while\ndemonstrating how metaverse technologies blur the traditional boundaries\nbetween physical and virtual experiences."}
{"id": "2504.21012", "pdf": "https://arxiv.org/pdf/2504.21012.pdf", "abs": "https://arxiv.org/abs/2504.21012", "title": "Waking Up an AI: A Quantitative Framework for Prompt-Induced Phase Transition in Large Language Models", "authors": ["Makoto Sato"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "What underlies intuitive human thinking? One approach to this question is to\ncompare the cognitive dynamics of humans and large language models (LLMs).\nHowever, such a comparison requires a method to quantitatively analyze AI\ncognitive behavior under controlled conditions. While anecdotal observations\nsuggest that certain prompts can dramatically change LLM behavior, these\nobservations have remained largely qualitative. Here, we propose a two-part\nframework to investigate this phenomenon: a Transition-Inducing Prompt (TIP)\nthat triggers a rapid shift in LLM responsiveness, and a Transition Quantifying\nPrompt (TQP) that evaluates this change using a separate LLM. Through\ncontrolled experiments, we examined how LLMs react to prompts embedding two\nsemantically distant concepts (e.g., mathematical aperiodicity and traditional\ncrafts)--either fused together or presented separately--by changing their\nlinguistic quality and affective tone. Whereas humans tend to experience\nheightened engagement when such concepts are meaningfully blended producing a\nnovel concept--a form of conceptual fusion--current LLMs showed no significant\ndifference in responsiveness between semantically fused and non-fused prompts.\nThis suggests that LLMs may not yet replicate the conceptual integration\nprocesses seen in human intuition. Our method enables fine-grained,\nreproducible measurement of cognitive responsiveness, and may help illuminate\nkey differences in how intuition and conceptual leaps emerge in artificial\nversus human minds."}
{"id": "2504.21360", "pdf": "https://arxiv.org/pdf/2504.21360.pdf", "abs": "https://arxiv.org/abs/2504.21360", "title": "ImaginateAR: AI-Assisted In-Situ Authoring in Augmented Reality", "authors": ["Jaewook Lee", "Filippo Aleotti", "Diego Mazala", "Guillermo Garcia-Hernando", "Sara Vicente", "Oliver James Johnston", "Isabel Kraus-Liang", "Jakub Powierza", "Donghoon Shin", "Jon E. Froehlich", "Gabriel Brostow", "Jessica Van Brummelen"], "categories": ["cs.HC"], "comment": null, "summary": "While augmented reality (AR) enables new ways to play, tell stories, and\nexplore ideas rooted in the physical world, authoring personalized AR content\nremains difficult for non-experts, often requiring professional tools and time.\nPrior systems have explored AI-driven XR design but typically rely on\nmanually-defined environments and fixed asset libraries, limiting creative\nflexibility and real-world relevance. We introduce ImaginateAR, a mobile\nAI-assisted AR authoring system that aims to let anyone build anything,\nanywhere -- simply by speaking their imagination. ImaginateAR is powered by\ncustom pipelines for offline scene understanding, fast 3D asset generation, and\nLLM-driven speech interaction. Users might say \"a dragon enjoying a campfire\"\n(P7) and iteratively refine the scene using both AI and manual tools. Our\ntechnical evaluation shows that ImaginateAR produces more accurate outdoor\nscene graphs and generates 3D meshes faster than prior methods. A three-part\nuser study (N=20) revealed preferred roles for AI in authoring, what and how\nusers create in free-form use, and design implications for future AR authoring\ntools."}
{"id": "2504.21013", "pdf": "https://arxiv.org/pdf/2504.21013.pdf", "abs": "https://arxiv.org/abs/2504.21013", "title": "Analyzing Feedback Mechanisms in AI-Generated MCQs: Insights into Readability, Lexical Properties, and Levels of Challenge", "authors": ["Antoun Yaacoub", "Zainab Assaghir", "Lionel Prevost", "Jérôme Da-Rugna"], "categories": ["cs.CL", "cs.AI"], "comment": "This paper will be presented in the 9th Int. Conf. on Computer,\n  Software and Modeling (ICCSM 2025), Roma, Italy, 2025, July 3-5", "summary": "Artificial Intelligence (AI)-generated feedback in educational settings has\ngarnered considerable attention due to its potential to enhance learning\noutcomes. However, a comprehensive understanding of the linguistic\ncharacteristics of AI-generated feedback, including readability, lexical\nrichness, and adaptability across varying challenge levels, remains limited.\nThis study delves into the linguistic and structural attributes of feedback\ngenerated by Google's Gemini 1.5-flash text model for computer science\nmultiple-choice questions (MCQs). A dataset of over 1,200 MCQs was analyzed,\nconsidering three difficulty levels (easy, medium, hard) and three feedback\ntones (supportive, neutral, challenging). Key linguistic metrics, such as\nlength, readability scores (Flesch-Kincaid Grade Level), vocabulary richness,\nand lexical density, were computed and examined. A fine-tuned RoBERTa-based\nmulti-task learning (MTL) model was trained to predict these linguistic\nproperties, achieving a Mean Absolute Error (MAE) of 2.0 for readability and\n0.03 for vocabulary richness. The findings reveal significant interaction\neffects between feedback tone and question difficulty, demonstrating the\ndynamic adaptation of AI-generated feedback within diverse educational\ncontexts. These insights contribute to the development of more personalized and\neffective AI-driven feedback mechanisms, highlighting the potential for\nimproved learning outcomes while underscoring the importance of ethical\nconsiderations in their design and deployment."}
{"id": "2504.21397", "pdf": "https://arxiv.org/pdf/2504.21397.pdf", "abs": "https://arxiv.org/abs/2504.21397", "title": "Coping with Uncertainty in UX Design Practice: Practitioner Strategies and Judgment", "authors": ["Prakash Shukla", "Phuong Bui", "Paul Parsons"], "categories": ["cs.HC"], "comment": "11 pages, In Creativity and Cognition (C&C '25), June 23--25, 2025,\n  Virtual, United Kingdom", "summary": "The complexity of UX design practice extends beyond ill-structured design\nproblems to include uncertainties shaped by shifting stakeholder priorities,\nteam dynamics, limited resources, and implementation constraints. While prior\nresearch in related fields has addressed uncertainty in design more broadly,\nthe specific character of uncertainty in UX practice remains underexplored.\nThis study examines how UX practitioners experience and respond to uncertainty\nin real-world projects, drawing on a multi-week diary study and follow-up\ninterviews with ten designers. We identify a range of practitioner\nstrategies-including adaptive framing, negotiation, and judgment-that allow\ndesigners to move forward amid ambiguity. Our findings highlight the central\nrole of design judgment in navigating uncertainty, including emergent forms\nsuch as temporal and sacrificial judgment, and extend prior understandings by\nshowing how UX practitioners engage uncertainty as a persistent, situated\nfeature of practice."}
{"id": "2504.21016", "pdf": "https://arxiv.org/pdf/2504.21016.pdf", "abs": "https://arxiv.org/abs/2504.21016", "title": "Nested Named-Entity Recognition on Vietnamese COVID-19: Dataset and Experiments", "authors": ["Ngoc C. Lê", "Hai-Chung Nguyen-Phung", "Thu-Huong Pham Thi", "Hue Vu", "Phuong-Thao Nguyen Thi", "Thu-Thuy Tran", "Hong-Nhung Le Thi", "Thuy-Duong Nguyen-Thi", "Thanh-Huy Nguyen"], "categories": ["cs.CL", "cs.LG"], "comment": "8 pages. AI4SG-21 The 3rd Workshop on Artificial Intelligence for\n  Social Good at IJCAI 2021", "summary": "The COVID-19 pandemic caused great losses worldwide, efforts are taken place\nto prevent but many countries have failed. In Vietnam, the traceability,\nlocalization, and quarantine of people who contact with patients contribute to\neffective disease prevention. However, this is done by hand, and take a lot of\nwork. In this research, we describe a named-entity recognition (NER) study that\nassists in the prevention of COVID-19 pandemic in Vietnam. We also present our\nmanually annotated COVID-19 dataset with nested named entity recognition task\nfor Vietnamese which be defined new entity types using for our system."}
{"id": "2504.21477", "pdf": "https://arxiv.org/pdf/2504.21477.pdf", "abs": "https://arxiv.org/abs/2504.21477", "title": "A Comprehensive Survey of Electrical Stimulation Haptic Feedback in Human-Computer Interaction", "authors": ["Simin Yang", "Xian Wang", "Yang Li", "Lik-Hang Lee", "Tristan Camille", "Pan Hui"], "categories": ["cs.HC"], "comment": "23 pages, 7 figures", "summary": "Haptic perception and feedback play a pivotal role in interactive\nexperiences, forming an essential component of human-computer interaction\n(HCI). In recent years, the field of haptic interaction has witnessed\nsignificant advancements, particularly in the area of electrical haptic\nfeedback, driving innovation across various domains. To gain a comprehensive\nunderstanding of the current state of research and the latest developments in\nelectrical haptic interaction, this study systematically reviews the literature\nin this area. Our investigation covers key aspects including haptic devices,\nhaptic perception mechanisms, the comparison and integration of electrical\nhaptic feedback with other feedback modalities, and their diverse applications.\nSpecifically, we conduct a systematic analysis of 110 research papers to\nexplore the forefront of electrical haptic feedback, providing insights into\nits latest trends, challenges, and future directions."}
{"id": "2504.21017", "pdf": "https://arxiv.org/pdf/2504.21017.pdf", "abs": "https://arxiv.org/abs/2504.21017", "title": "ViQA-COVID: COVID-19 Machine Reading Comprehension Dataset for Vietnamese", "authors": ["Hai-Chung Nguyen-Phung", "Ngoc C. Lê", "Van-Chien Nguyen", "Hang Thi Nguyen", "Thuy Phuong Thi Nguyen"], "categories": ["cs.CL", "cs.LG"], "comment": "8 pages. Technical report", "summary": "After two years of appearance, COVID-19 has negatively affected people and\nnormal life around the world. As in May 2022, there are more than 522 million\ncases and six million deaths worldwide (including nearly ten million cases and\nover forty-three thousand deaths in Vietnam). Economy and society are both\nseverely affected. The variant of COVID-19, Omicron, has broken disease\nprevention measures of countries and rapidly increased number of infections.\nResources overloading in treatment and epidemics prevention is happening all\nover the world. It can be seen that, application of artificial intelligence\n(AI) to support people at this time is extremely necessary. There have been\nmany studies applying AI to prevent COVID-19 which are extremely useful, and\nstudies on machine reading comprehension (MRC) are also in it. Realizing that,\nwe created the first MRC dataset about COVID-19 for Vietnamese: ViQA-COVID and\ncan be used to build models and systems, contributing to disease prevention.\nBesides, ViQA-COVID is also the first multi-span extraction MRC dataset for\nVietnamese, we hope that it can contribute to promoting MRC studies in\nVietnamese and multilingual."}
{"id": "2504.21563", "pdf": "https://arxiv.org/pdf/2504.21563.pdf", "abs": "https://arxiv.org/abs/2504.21563", "title": "A User-Centered Teleoperation GUI for Automated Vehicles: Identifying and Evaluating Information Requirements for Remote Driving and Assistance", "authors": ["Maria-Magdalena Wolf", "Henrik Schmidt", "Michael Christl", "Jana Fank", "Frank Diermeyer"], "categories": ["cs.HC"], "comment": null, "summary": "Teleoperation emerged as a promising fallback for situations beyond the\ncapabilities of automated vehicles. Nevertheless, teleoperation still faces\nchallenges, such as reduced situational awareness. Since situational awareness\nis primarily built through the remote operator's visual perception, the\nGraphical User Interface (GUI) design is critical. In addition to video feeds,\nsupplemental informational elements are crucial - not only for the\npredominantly studied Remote Driving but also for the arising desk-based Remote\nAssistance concepts. This work develops a GUI for different teleoperation\nconcepts by identifying key informational elements during the teleoperation\nprocess through expert interviews (N = 9). Following this, a static and dynamic\nGUI prototype is developed and evaluated in a click-dummy study (N = 36).\nThereby, the dynamic GUI adapts the number of displayed elements according to\nthe teleoperation phase. Results show that both GUIs achieve good System\nUsability Scale (SUS) ratings, with the dynamic GUI significantly outperforming\nthe static version in both usability and task completion time. The User\nExperience Questionnaire (UEQ) score shows potential for improvement. To\nenhance the user experience, the GUI should be evaluated in a follow-up study\nthat includes interaction with a real vehicle."}
{"id": "2504.21018", "pdf": "https://arxiv.org/pdf/2504.21018.pdf", "abs": "https://arxiv.org/abs/2504.21018", "title": "HYPEROFA: Expanding LLM Vocabulary to New Languages via Hypernetwork-Based Embedding Initialization", "authors": ["Enes Özeren", "Yihong Liu", "Hinrich Schütze"], "categories": ["cs.CL", "cs.LG"], "comment": "18 pages, 3 figures, 15 tables", "summary": "Many pre-trained language models (PLMs) exhibit suboptimal performance on\nmid- and low-resource languages, largely due to limited exposure to these\nlanguages during pre-training. A common strategy to address this is to\nintroduce new tokens specific to the target languages, initialize their\nembeddings, and apply continual pre-training on target-language data. Among\nsuch methods, OFA (Liu et al., 2024a) proposes a similarity-based subword\nembedding initialization heuristic that is both effective and efficient.\nHowever, OFA restricts target-language token embeddings to be convex\ncombinations of a fixed number of source-language embeddings, which may limit\nexpressiveness. To overcome this limitation, we propose HYPEROFA, a\nhypernetwork-based approach for more adaptive token embedding initialization.\nThe hypernetwork is trained to map from an external multilingual word vector\nspace to the PLMs token embedding space using source-language tokens. Once\ntrained, it can generate flexible embeddings for target-language tokens,\nserving as a good starting point for continual pretraining. Experiments\ndemonstrate that HYPEROFA consistently outperforms random initialization\nbaseline and matches or exceeds the performance of OFA in both continual\npre-training convergence and downstream task performance. We make the code\npublicly available."}
{"id": "2504.21702", "pdf": "https://arxiv.org/pdf/2504.21702.pdf", "abs": "https://arxiv.org/abs/2504.21702", "title": "A Conversational Approach to Well-being Awareness Creation and Behavioural Intention", "authors": ["Antonia Azzini", "Ilaria Baroni", "Irene Celino"], "categories": ["cs.HC"], "comment": null, "summary": "The promotion of a healthy lifestyle is one of the main drivers of an\nindividual's overall physical and psycho-emotional well-being. Digital\ntechnologies are more and more adopted as ''facilitators'' for this goal, to\nraise awareness and solicit healthy lifestyle habits.\n  This study aims to experiment the effects of the adoption of a digital\nconversational tool to influence awareness creation and behavioural change in\nthe context of a well-being lifestyle. Our aim is to collect evidence of the\naspects that must be taken into account when designing and implementing such\ntools in well-being promotion campaigns.\n  To this end, we created a conversational application for promoting well-being\nand healthy lifestyles, which presents relevant information and asks specific\nquestions to its intended users within an interaction happening through a chat\ninterface; the conversational tool presents itself as a well-being counsellor\nnamed Allegra and follows a coaching approach to structure the interaction with\nthe user. In our user study, participants were asked to first interact with\nAllegra in one of three experimental conditions, corresponding to different\nconversational styles; then, they answered a questionnaire about their\nexperience. The questionnaire items were related to intrinsic motivation\nfactors as well as awareness creation and behavioural change. The collected\ndata allowed us to assess the hypotheses of our model that put in connection\nthose variables.\n  Our results confirm the positive effect of intrinsic motivation factors on\nboth awareness creation and behavioural intention in the context of well-being\nand healthy lifestyle; on the other hand, we did not record any statistically\nsignificant effect of different language and communication styles on the\noutcomes."}
{"id": "2504.21019", "pdf": "https://arxiv.org/pdf/2504.21019.pdf", "abs": "https://arxiv.org/abs/2504.21019", "title": "Kill two birds with one stone: generalized and robust AI-generated text detection via dynamic perturbations", "authors": ["Yinghan Zhou", "Juan Wen", "Wanli Peng", "Yiming Xue", "Ziwei Zhang", "Zhengxian Wu"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by NAACL 2025 main conference", "summary": "The growing popularity of large language models has raised concerns regarding\nthe potential to misuse AI-generated text (AIGT). It becomes increasingly\ncritical to establish an excellent AIGT detection method with high\ngeneralization and robustness. However, existing methods either focus on model\ngeneralization or concentrate on robustness. The unified mechanism, to\nsimultaneously address the challenges of generalization and robustness, is less\nexplored. In this paper, we argue that robustness can be view as a specific\nform of domain shift, and empirically reveal an intrinsic mechanism for model\ngeneralization of AIGT detection task. Then, we proposed a novel AIGT detection\nmethod (DP-Net) via dynamic perturbations introduced by a reinforcement\nlearning with elaborated reward and action. Experimentally, extensive results\nshow that the proposed DP-Net significantly outperforms some state-of-the-art\nAIGT detection methods for generalization capacity in three cross-domain\nscenarios. Meanwhile, the DP-Net achieves best robustness under two text\nadversarial attacks. The code is publicly available at\nhttps://github.com/CAU-ISS-Lab/AIGT-Detection-Evade-Detection/tree/main/DP-Net."}
{"id": "2504.21731", "pdf": "https://arxiv.org/pdf/2504.21731.pdf", "abs": "https://arxiv.org/abs/2504.21731", "title": "Adaptive 3D UI Placement in Mixed Reality Using Deep Reinforcement Learning", "authors": ["Feiyu Lu", "Mengyu Chen", "Hsiang Hsu", "Pranav Deshpande", "Cheng Yao Wang", "Blair MacIntyre"], "categories": ["cs.HC", "cs.AI", "cs.CV"], "comment": "In Extended Abstracts of the CHI Conference on Human Factors in\n  Computing Systems (CHI EA '24)", "summary": "Mixed Reality (MR) could assist users' tasks by continuously integrating\nvirtual content with their view of the physical environment. However, where and\nhow to place these content to best support the users has been a challenging\nproblem due to the dynamic nature of MR experiences. In contrast to prior work\nthat investigates optimization-based methods, we are exploring how\nreinforcement learning (RL) could assist with continuous 3D content placement\nthat is aware of users' poses and their surrounding environments. Through an\ninitial exploration and preliminary evaluation, our results demonstrate the\npotential of RL to position content that maximizes the reward for users on the\ngo. We further identify future directions for research that could harness the\npower of RL for personalized and optimized UI and content placement in MR."}
{"id": "2504.21020", "pdf": "https://arxiv.org/pdf/2504.21020.pdf", "abs": "https://arxiv.org/abs/2504.21020", "title": "Context-Enhanced Contrastive Search for Improved LLM Text Generation", "authors": ["Jaydip Sen", "Rohit Pandey", "Hetvi Waghela"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "This is the pre-review version of our paper, which has been accepted\n  for publication in the IEEE 6th International Conference on Emerging\n  Technologies (INCET). The conference will be organized at Belgaum, India,\n  from May 24 to 26, 2025. This is not the final camera-ready paper, which will\n  be available on IEEE Xplore. The paper is 9 pages long, and it contains 2\n  Figures and 4 Tables", "summary": "Recently, Large Language Models (LLMs) have demonstrated remarkable\nadvancements in Natural Language Processing (NLP). However, generating\nhigh-quality text that balances coherence, diversity, and relevance remains\nchallenging. Traditional decoding methods, such as bean search and top-k\nsampling, often struggle with either repetitive or incoherent outputs,\nparticularly in tasks that require long-form text generation. To address these\nlimitations, the paper proposes a novel enhancement of the well-known\nContrastive Search algorithm, Context-Enhanced Contrastive Search (CECS) with\ncontextual calibration. The proposed scheme introduces several novelties\nincluding dynamic contextual importance weighting, multi-level Contrastive\nSearch, and adaptive temperature control, to optimize the balance between\nfluency, creativity, and precision. The performance of CECS is evaluated using\nseveral standard metrics such as BLEU, ROUGE, and semantic similarity.\nExperimental results demonstrate significant improvements in both coherence and\nrelevance of the generated texts by CECS outperforming the existing Contrastive\nSearch techniques. The proposed algorithm has several potential applications in\nthe real world including legal document drafting, customer service chatbots,\nand content marketing."}
{"id": "2304.10728", "pdf": "https://arxiv.org/pdf/2304.10728.pdf", "abs": "https://arxiv.org/abs/2304.10728", "title": "PiXi: Password Inspiration by Exploring Information", "authors": ["Shengqian Wang", "Amirali Salehi-Abari", "Julie Thorpe"], "categories": ["cs.CR", "cs.HC"], "comment": "16 pages", "summary": "Passwords, a first line of defense against unauthorized access, must be\nsecure and memorable. However, people often struggle to create secure passwords\nthey can recall. To address this problem, we design Password inspiration by\neXploring information (PiXi), a novel approach to nudge users towards creating\nsecure passwords. PiXi is the first of its kind that employs a password\ncreation nudge to support users in the task of generating a unique secure\npassword themselves. PiXi prompts users to explore unusual information right\nbefore creating a password, to shake them out of their typical habits and\nthought processes, and to inspire them to create unique (and therefore\nstronger) passwords. PiXi's design aims to create an engaging, interactive, and\neffective nudge to improve secure password creation. We conducted a user study\n($N=238$) to compare the efficacy of PiXi to typical password creation. Our\nfindings indicate that PiXi's nudges do influence users' password choices such\nthat passwords are significantly longer and more secure (less predictable and\nguessable)."}
{"id": "2504.21022", "pdf": "https://arxiv.org/pdf/2504.21022.pdf", "abs": "https://arxiv.org/abs/2504.21022", "title": "ConformalNL2LTL: Translating Natural Language Instructions into Temporal Logic Formulas with Conformal Correctness Guarantees", "authors": ["Jun Wang", "David Smith Sundarsingh", "Jyotirmoy V. Deshmukh", "Yiannis Kantaros"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "Linear Temporal Logic (LTL) has become a prevalent specification language for\nrobotic tasks. To mitigate the significant manual effort and expertise required\nto define LTL-encoded tasks, several methods have been proposed for translating\nNatural Language (NL) instructions into LTL formulas, which, however, lack\ncorrectness guarantees. To address this, we introduce a new NL-to-LTL\ntranslation method, called ConformalNL2LTL, that can achieve user-defined\ntranslation success rates over unseen NL commands. Our method constructs LTL\nformulas iteratively by addressing a sequence of open-vocabulary\nQuestion-Answering (QA) problems with LLMs. To enable uncertainty-aware\ntranslation, we leverage conformal prediction (CP), a distribution-free\nuncertainty quantification tool for black-box models. CP enables our method to\nassess the uncertainty in LLM-generated answers, allowing it to proceed with\ntranslation when sufficiently confident and request help otherwise. We provide\nboth theoretical and empirical results demonstrating that ConformalNL2LTL\nachieves user-specified translation accuracy while minimizing help rates."}
{"id": "2504.21347", "pdf": "https://arxiv.org/pdf/2504.21347.pdf", "abs": "https://arxiv.org/abs/2504.21347", "title": "IRL Dittos: Embodied Multimodal AI Agent Interactions in Open Spaces", "authors": ["Seonghee Lee", "Denae Ford", "John Tang", "Sasa Junuzovic", "Asta Roseway", "Ed Cutrell", "Kori Inkpen"], "categories": ["cs.AI", "cs.HC", "H.5.2; I.2.9"], "comment": "8 pages, 3 figures", "summary": "We introduce the In Real Life (IRL) Ditto, an AI-driven embodied agent\ndesigned to represent remote colleagues in shared office spaces, creating\nopportunities for real-time exchanges even in their absence. IRL Ditto offers a\nunique hybrid experience by allowing in-person colleagues to encounter a\ndigital version of their remote teammates, initiating greetings, updates, or\nsmall talk as they might in person. Our research question examines: How can the\nIRL Ditto influence interactions and relationships among colleagues in a shared\noffice space? Through a four-day study, we assessed IRL Ditto's ability to\nstrengthen social ties by simulating presence and enabling meaningful\ninteractions across different levels of social familiarity. We find that\nenhancing social relationships depended deeply on the foundation of the\nrelationship participants had with the source of the IRL Ditto. This study\nprovides insights into the role of embodied agents in enriching workplace\ndynamics for distributed teams."}
{"id": "2504.21023", "pdf": "https://arxiv.org/pdf/2504.21023.pdf", "abs": "https://arxiv.org/abs/2504.21023", "title": "Param$Δ$ for Direct Weight Mixing: Post-Train Large Language Model at Zero Cost", "authors": ["Sheng Cao", "Mingrui Wu", "Karthik Prasad", "Yuandong Tian", "Zechun Liu"], "categories": ["cs.CL", "cs.LG"], "comment": "Published as a conference paper at ICLR 2025", "summary": "The post-training phase of large language models is essential for enhancing\ncapabilities such as instruction-following, reasoning, and alignment with human\npreferences. However, it demands extensive high-quality data and poses risks\nlike overfitting, alongside significant computational costs due to repeated\npost-training and evaluation after each base model update. This paper\nintroduces $Param\\Delta$, a novel method that streamlines post-training by\ntransferring knowledge from an existing post-trained model to a newly updated\nbase model with ZERO additional training. By computing the difference between\npost-trained model weights ($\\Theta_\\text{post}$) and base model weights\n($\\Theta_\\text{base}$), and adding this to the updated base model\n($\\Theta'_\\text{base}$), we define $Param\\Delta$ Model as:\n$\\Theta_{\\text{Param}\\Delta} = \\Theta_\\text{post} - \\Theta_\\text{base} +\n\\Theta'_\\text{base}$. This approach surprisingly equips the new base model with\npost-trained capabilities, achieving performance comparable to direct\npost-training. We did analysis on LLama3, Llama3.1, Qwen, and\nDeepSeek-distilled models. Results indicate $Param\\Delta$ Model effectively\nreplicates traditional post-training. For example, the $Param\\Delta$ Model\nobtained from 70B Llama3-inst, Llama3-base, Llama3.1-base models attains\napproximately 95\\% of Llama3.1-inst model's performance on average.\n$Param\\Delta$ brings a new perspective on how to fully leverage models in the\nopen-weight community, where checkpoints for base and instruct models are\nreadily available and frequently updated, by providing a cost-free framework to\naccelerate the iterative cycle of model development."}
{"id": "2504.21500", "pdf": "https://arxiv.org/pdf/2504.21500.pdf", "abs": "https://arxiv.org/abs/2504.21500", "title": "Visual Analytics Challenges and Trends in the Age of AI: The BigVis Community Perspective", "authors": ["Nikos Bikakis", "Panos K. Chrysanthis", "Guoliang Li", "George Papastefanatos", "Lingyun Yu"], "categories": ["cs.DB", "cs.HC", "97R50, 68P05, 68P15", "E.1; H.2.8; H.5.2; H.4"], "comment": "ACM SIGMOD Record 2025", "summary": "This report provides insights into the challenges, emerging topics, and\nopportunities related to human-data interaction and visual analytics in the AI\nera. The BigVis 2024 organizing committee conducted a survey among experts in\nthe field. They invite the Program Committee members and the authors of\naccepted papers to share their views. Thirty-two scientists from diverse\nresearch communities, including Databases, Information Visualization, and\nHuman-Computer Interaction, participated in the study. These scientists,\nrepresenting both industry and academia, provided valuable insights into the\ncurrent and future landscape of the field.\n  In this report, we analyze the survey responses and compare them to the\nfindings of a similar study conducted four years ago. The results reveal some\ninteresting insights. First, many of the critical challenges identified in the\nprevious survey remain highly relevant today, despite being unrelated to AI.\nMeanwhile, the field's landscape has significantly evolved, with most of\ntoday's vital challenges not even being mentioned in the earlier survey,\nunderscoring the profound impact of AI-related advancements.\n  By summarizing the perspectives of the research community, this report aims\nto shed light on the key challenges, emerging trends, and potential research\ndirections in human-data interaction and visual analytics in the AI era."}
{"id": "2504.21024", "pdf": "https://arxiv.org/pdf/2504.21024.pdf", "abs": "https://arxiv.org/abs/2504.21024", "title": "WebEvolver: Enhancing Web Agent Self-Improvement with Coevolving World Model", "authors": ["Tianqing Fang", "Hongming Zhang", "Zhisong Zhang", "Kaixin Ma", "Wenhao Yu", "Haitao Mi", "Dong Yu"], "categories": ["cs.CL"], "comment": "19 pages", "summary": "Agent self-improvement, where the backbone Large Language Model (LLM) of the\nagent are trained on trajectories sampled autonomously based on their own\npolicies, has emerged as a promising approach for enhancing performance. Recent\nadvancements, particularly in web environments, face a critical limitation:\ntheir performance will reach a stagnation point during autonomous learning\ncycles, hindering further improvement. We argue that this stems from limited\nexploration of the web environment and insufficient exploitation of pre-trained\nweb knowledge in LLMs. To improve the performance of self-improvement, we\npropose a novel framework that introduces a co-evolving World Model LLM. This\nworld model predicts the next observation based on the current observation and\naction within the web environment. Leveraging LLMs' pretrained knowledge of\nabundant web content, the World Model serves dual roles: (1) as a virtual web\nserver generating self-instructed training data to continuously refine the\nagent's policy, and (2) as an imagination engine during inference, enabling\nlook-ahead simulation to guide action selection for the agent LLM. Experiments\nin real-world web environments (Mind2Web-Live, WebVoyager, and GAIA-web) show a\n10% performance gain over existing self-evolving agents, demonstrating the\nefficacy and generalizability of our approach, without using any distillation\nfrom more powerful close-sourced models. Our work establishes the necessity of\nintegrating world models into autonomous agent frameworks to unlock sustained\nadaptability."}
{"id": "2504.21507", "pdf": "https://arxiv.org/pdf/2504.21507.pdf", "abs": "https://arxiv.org/abs/2504.21507", "title": "Efficient Conversational Search via Topical Locality in Dense Retrieval", "authors": ["Cristina Ioana Muntean", "Franco Maria Nardini", "Raffaele Perego", "Guido Rocchietti", "Cosimo Rulli"], "categories": ["cs.IR", "cs.HC", "H.3"], "comment": "5 pages, 2 figures, SIGIR 2025", "summary": "Pre-trained language models have been widely exploited to learn dense\nrepresentations of documents and queries for information retrieval. While\nprevious efforts have primarily focused on improving effectiveness and user\nsatisfaction, response time remains a critical bottleneck of conversational\nsearch systems. To address this, we exploit the topical locality inherent in\nconversational queries, i.e., the tendency of queries within a conversation to\nfocus on related topics. By leveraging query embedding similarities, we\ndynamically restrict the search space to semantically relevant document\nclusters, reducing computational complexity without compromising retrieval\nquality. We evaluate our approach on the TREC CAsT 2019 and 2020 datasets using\nmultiple embedding models and vector indexes, achieving improvements in\nprocessing speed of up to 10.4X with little loss in performance (4.4X without\nany loss). Our results show that the proposed system effectively handles\ncomplex, multiturn queries with high precision and efficiency, offering a\npractical solution for real-time conversational search."}
{"id": "2504.21025", "pdf": "https://arxiv.org/pdf/2504.21025.pdf", "abs": "https://arxiv.org/abs/2504.21025", "title": "Durghotona GPT: A Web Scraping and Large Language Model Based Framework to Generate Road Accident Dataset Automatically in Bangladesh", "authors": ["MD Thamed Bin Zaman Chowdhury", "Moazzem Hossain", "Md. Ridwanul Islam"], "categories": ["cs.CL"], "comment": "It has been accepted in IEEE 27th International Conference on\n  Computer and Information Technology (ICCIT). Now, we are waiting for it to\n  get published in IEEE Xplore", "summary": "Road accidents pose significant concerns globally. They lead to large\nfinancial losses, injuries, disabilities, and societal challenges. Accurate and\ntimely accident data is essential for predicting and mitigating these events.\nThis paper presents a novel framework named 'Durghotona GPT' that integrates\nweb scraping and Large Language Models (LLMs) to automate the generation of\ncomprehensive accident datasets from prominent national dailies in Bangladesh.\nThe authors collected accident reports from three major newspapers: Prothom\nAlo, Dhaka Tribune, and The Daily Star. The collected news was then processed\nusing the newest available LLMs: GPT-4, GPT-3.5, and Llama-3. The framework\nefficiently extracts relevant information, categorizes reports, and compiles\ndetailed datasets. Thus, this framework overcomes limitations of manual data\ncollection methods such as delays, errors, and communication gaps. The authors'\nevaluation demonstrates that Llama-3, an open-source model, performs comparably\nto GPT-4. It achieved 89% accuracy in the authors' evaluation. Therefore, it\ncan be considered a cost-effective alternative for similar tasks. The results\nsuggest that the framework developed by the authors can drastically enhance the\nquality and availability of accident data. As a result, it can support critical\napplications in traffic safety analysis, urban planning, and public health. The\nauthors also developed an interface for 'Durghotona GPT' for ease of use as\npart of this paper. Future work will focus on expanding data collection methods\nand refining LLMs to further increase dataset accuracy and applicability."}
{"id": "2504.21735", "pdf": "https://arxiv.org/pdf/2504.21735.pdf", "abs": "https://arxiv.org/abs/2504.21735", "title": "TheraQuest: A Gamified, LLM-Powered Simulation for Massage Therapy Training", "authors": ["Shengqian Wang"], "categories": ["cs.GT", "cs.HC"], "comment": "8 Pages", "summary": "Massage therapy training emphasizes hands-on techniques and effective\ntherapist--patient communication. However, many educational programs struggle\nto provide realistic practice scenarios. To address this problem, we propose\nTheraQuest, a gamified, web-based simulation platform that employs large\nlanguage models (LLMs) to generate diverse virtual patients with varying\nsymptoms and cultural backgrounds. Through interactive dialogue, anatomical\ndecision-making, and immediate assessment, trainees develop both diagnostic\nreasoning and empathetic communication skills in a low-risk environment. Unlike\nexclusively VR-based solutions, TheraQuest remains accessible via standard web\nbrowsers, mitigating the cost and discomfort associated with extended headset\nuse. Preliminary testing suggests that integrating LLM-driven virtual patients\nwith real-time skill metrics can enhance trainee engagement and help bridge the\ngap between theoretical knowledge and clinical proficiency."}
{"id": "2504.21026", "pdf": "https://arxiv.org/pdf/2504.21026.pdf", "abs": "https://arxiv.org/abs/2504.21026", "title": "Creating and Evaluating Code-Mixed Nepali-English and Telugu-English Datasets for Abusive Language Detection Using Traditional and Deep Learning Models", "authors": ["Manish Pandey", "Nageshwar Prasad Yadav", "Mokshada Adduru", "Sawan Rai"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI"], "comment": null, "summary": "With the growing presence of multilingual users on social media, detecting\nabusive language in code-mixed text has become increasingly challenging.\nCode-mixed communication, where users seamlessly switch between English and\ntheir native languages, poses difficulties for traditional abuse detection\nmodels, as offensive content may be context-dependent or obscured by linguistic\nblending. While abusive language detection has been extensively explored for\nhigh-resource languages like English and Hindi, low-resource languages such as\nTelugu and Nepali remain underrepresented, leaving gaps in effective\nmoderation. In this study, we introduce a novel, manually annotated dataset of\n2 thousand Telugu-English and 5 Nepali-English code-mixed comments, categorized\nas abusive and non-abusive, collected from various social media platforms. The\ndataset undergoes rigorous preprocessing before being evaluated across multiple\nMachine Learning (ML), Deep Learning (DL), and Large Language Models (LLMs). We\nexperimented with models including Logistic Regression, Random Forest, Support\nVector Machines (SVM), Neural Networks (NN), LSTM, CNN, and LLMs, optimizing\ntheir performance through hyperparameter tuning, and evaluate it using 10-fold\ncross-validation and statistical significance testing (t-test). Our findings\nprovide key insights into the challenges of detecting abusive language in\ncode-mixed settings and offer a comparative analysis of computational\napproaches. This study contributes to advancing NLP for low-resource languages\nby establishing benchmarks for abusive language detection in Telugu-English and\nNepali-English code-mixed text. The dataset and insights can aid in the\ndevelopment of more robust moderation strategies for multilingual social media\nenvironments."}
{"id": "2504.21800", "pdf": "https://arxiv.org/pdf/2504.21800.pdf", "abs": "https://arxiv.org/abs/2504.21800", "title": "How Real Are Synthetic Therapy Conversations? Evaluating Fidelity in Prolonged Exposure Dialogues", "authors": ["Suhas BN", "Dominik Mattioli", "Saeed Abdullah", "Rosa I. Arriaga", "Chris W. Wiese", "Andrew M. Sherrill"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "68T50", "I.2.7; H.3.1"], "comment": "11 pages, 5 tables, updated abstract and tables", "summary": "The growing adoption of synthetic data in healthcare is driven by privacy\nconcerns, limited access to real-world data, and the high cost of annotation.\nThis work explores the use of synthetic Prolonged Exposure (PE) therapeutic\nconversations for Post-Traumatic Stress Disorder (PTSD) as a scalable\nalternative for training and evaluating clinical models. We systematically\ncompare real and synthetic dialogues using linguistic, structural, and\nprotocol-specific metrics, including turn-taking patterns and treatment\nfidelity. We also introduce and evaluate PE-specific metrics derived from\nlinguistic analysis and semantic modeling, offering a novel framework for\nassessing clinical fidelity beyond surface fluency. Our findings show that\nalthough synthetic data holds promise for mitigating data scarcity and\nprotecting patient privacy, it can struggle to capture the subtle dynamics of\ntherapeutic interactions. In our dataset, synthetic dialogues match structural\nfeatures of real-world dialogues (e.g., speaker switch ratio: 0.98 vs. 0.99),\nhowever, synthetic interactions do not adequately reflect key fidelity markers\n(e.g., distress monitoring). We highlight gaps in existing evaluation\nframeworks and advocate for fidelity-aware metrics that go beyond surface\nfluency to uncover clinically significant failures. Our findings clarify where\nsynthetic data can effectively complement real-world datasets -- and where\ncritical limitations remain."}
{"id": "2504.21027", "pdf": "https://arxiv.org/pdf/2504.21027.pdf", "abs": "https://arxiv.org/abs/2504.21027", "title": "UrbanPlanBench: A Comprehensive Urban Planning Benchmark for Evaluating Large Language Models", "authors": ["Yu Zheng", "Longyi Liu", "Yuming Lin", "Jie Feng", "Guozhen Zhang", "Depeng Jin", "Yong Li"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The advent of Large Language Models (LLMs) holds promise for revolutionizing\nvarious fields traditionally dominated by human expertise. Urban planning, a\nprofessional discipline that fundamentally shapes our daily surroundings, is\none such field heavily relying on multifaceted domain knowledge and experience\nof human experts. The extent to which LLMs can assist human practitioners in\nurban planning remains largely unexplored. In this paper, we introduce a\ncomprehensive benchmark, UrbanPlanBench, tailored to evaluate the efficacy of\nLLMs in urban planning, which encompasses fundamental principles, professional\nknowledge, and management and regulations, aligning closely with the\nqualifications expected of human planners. Through extensive evaluation, we\nreveal a significant imbalance in the acquisition of planning knowledge among\nLLMs, with even the most proficient models falling short of meeting\nprofessional standards. For instance, we observe that 70% of LLMs achieve\nsubpar performance in understanding planning regulations compared to other\naspects. Besides the benchmark, we present the largest-ever supervised\nfine-tuning (SFT) dataset, UrbanPlanText, comprising over 30,000 instruction\npairs sourced from urban planning exams and textbooks. Our findings demonstrate\nthat fine-tuned models exhibit enhanced performance in memorization tests and\ncomprehension of urban planning knowledge, while there exists significant room\nfor improvement, particularly in tasks requiring domain-specific terminology\nand reasoning. By making our benchmark, dataset, and associated evaluation and\nfine-tuning toolsets publicly available at\nhttps://github.com/tsinghua-fib-lab/PlanBench, we aim to catalyze the\nintegration of LLMs into practical urban planning, fostering a symbiotic\ncollaboration between human expertise and machine intelligence."}
{"id": "2504.21849", "pdf": "https://arxiv.org/pdf/2504.21849.pdf", "abs": "https://arxiv.org/abs/2504.21849", "title": "Public Opinion and The Rise of Digital Minds: Perceived Risk, Trust, and Regulation Support", "authors": ["Justin B. Bullock", "Janet V. T. Pauketat", "Hsini Huang", "Yi-Fan Wang", "Jacy Reese Anthis"], "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "31 pages, 1 figure, 5 tables, accepted to Public Performance and\n  Management Review", "summary": "Governance institutions must respond to societal risks, including those posed\nby generative AI. This study empirically examines how public trust in\ninstitutions and AI technologies, along with perceived risks, shape preferences\nfor AI regulation. Using the nationally representative 2023 Artificial\nIntelligence, Morality, and Sentience (AIMS) survey, we assess trust in\ngovernment, AI companies, and AI technologies, as well as public support for\nregulatory measures such as slowing AI development or outright bans on advanced\nAI. Our findings reveal broad public support for AI regulation, with risk\nperception playing a significant role in shaping policy preferences.\nIndividuals with higher trust in government favor regulation, while those with\ngreater trust in AI companies and AI technologies are less inclined to support\nrestrictions. Trust in government and perceived risks significantly predict\npreferences for both soft (e.g., slowing development) and strong (e.g., banning\nAI systems) regulatory interventions. These results highlight the importance of\npublic opinion in AI governance. As AI capabilities advance, effective\nregulation will require balancing public concerns about risks with trust in\ninstitutions. This study provides a foundational empirical baseline for\npolicymakers navigating AI governance and underscores the need for further\nresearch into public trust, risk perception, and regulatory strategies in the\nevolving AI landscape."}
{"id": "2504.21117", "pdf": "https://arxiv.org/pdf/2504.21117.pdf", "abs": "https://arxiv.org/abs/2504.21117", "title": "Beyond One-Size-Fits-All: Inversion Learning for Highly Effective NLG Evaluation Prompts", "authors": ["Hanhua Hong", "Chenghao Xiao", "Yang Wang", "Yiqi Liu", "Wenge Rong", "Chenghua Lin"], "categories": ["cs.CL"], "comment": "10 pages", "summary": "Evaluating natural language generation (NLG) systems is challenging due to\nthe diversity of valid outputs. While human evaluation is the gold standard, it\nsuffers from inconsistencies, lack of standardisation, and demographic biases,\nlimiting reproducibility. LLM-based evaluation offers a scalable alternative\nbut is highly sensitive to prompt design, where small variations can lead to\nsignificant discrepancies. In this work, we propose an inversion learning\nmethod that learns effective reverse mappings from model outputs back to their\ninput instructions, enabling the automatic generation of highly effective,\nmodel-specific evaluation prompts. Our method requires only a single evaluation\nsample and eliminates the need for time-consuming manual prompt engineering,\nthereby improving both efficiency and robustness. Our work contributes toward a\nnew direction for more robust and efficient LLM-based evaluation."}
{"id": "2403.02145", "pdf": "https://arxiv.org/pdf/2403.02145.pdf", "abs": "https://arxiv.org/abs/2403.02145", "title": "'SSL?! What on earth is that?': Towards Designing Age-Inclusive Secure Smartphone Browsing", "authors": ["Pavithren V. S. Pakianathan", "L. Siddharth", "Sujithra Raviselvam", "Kristin L. Wood", "Hyowon Lee", "Pin Sym Foong", "Jianying Zhou", "Simon Tangi Perrault"], "categories": ["cs.HC", "cs.CY"], "comment": "This version was last submitted to EuroUSEC 2023 - European Symposium\n  on Usable Security. It was later invited for poster submission at the same\n  conference", "summary": "Owing to the increase in 'certified' phishing websites, there is a steady\nincrease in the number of phishing cases and general susceptibility to\nphishing. Trust mechanisms (e.g., HTTPS Lock Indicators, SSL Certificates) that\nhelp differentiate genuine and phishing websites should therefore be evaluated\nfor their effectiveness in preventing vulnerable users from accessing phishing\nwebsites. In this article, we present a study involving 18 adults (male-6;\nfemale-12) and 12 older adults (male-4; female-8) to understand the usability\nof current trust mechanisms and preferred modalities in a conceptualized\nmechanism. In the first part of the study, using Chrome browser on Android, we\nasked the participants to browse a banking website and a government website for\ndigital particulars. We asked them to identify which one of the two was a\nphishing website, rate the usability of both websites and provide qualitative\nfeedback on the trust mechanisms. In the second part, we conceptualized an\nalternative trust mechanism, which allows seeking social, community and\nAI-based support to make website trust-related decisions. Herein, we asked the\nparticipants as to which modality (social, community or AI) they prefer to seek\nsupport from and why it is preferred. Using the current trust mechanisms, none\nof the participants were able to identify the phishing website. As the\nparticipants rated the current mechanisms poorly in terms of usability, they\nexpressed various difficulties that largely did not differ between adults and\nolder adults. In the conceptualized mechanism, we observed a notable difference\nin the preferred modalities, in that, older adults primarily preferred social\nsupport. In addition to these overall findings, specific observations suggest\nthat future trust mechanisms should not only consider age-specific needs but\nalso incorporate substantial improvement in terms of usability."}
{"id": "2504.21132", "pdf": "https://arxiv.org/pdf/2504.21132.pdf", "abs": "https://arxiv.org/abs/2504.21132", "title": "LLM Enhancer: Merged Approach using Vector Embedding for Reducing Large Language Model Hallucinations with External Knowledge", "authors": ["Naheed Rayhan", "Md. Ashrafuzzaman"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs), such as ChatGPT, have demonstrated the\ncapability to generate human like, natural responses across a range of tasks,\nincluding task oriented dialogue and question answering. However, their\napplication in real world, critical scenarios is often hindered by a tendency\nto produce inaccurate information and a limited ability to leverage external\nknowledge sources. This paper introduces the LLM ENHANCER system, designed to\nintegrate multiple online sources such as Google, Wikipedia, and DuckDuckGo to\nenhance data accuracy. The LLMs employed within this system are open source.\nThe data acquisition process for the LLM ENHANCER system operates in parallel,\nutilizing custom agent tools to manage the flow of information. Vector\nembeddings are used to identify the most pertinent information, which is\nsubsequently supplied to the LLM for user interaction. The LLM ENHANCER system\nmitigates hallucinations in chat based LLMs while preserving response\nnaturalness and accuracy."}
{"id": "2405.13924", "pdf": "https://arxiv.org/pdf/2405.13924.pdf", "abs": "https://arxiv.org/abs/2405.13924", "title": "Emotive Speech-to-Text Interfaces in XR: A Narrative Review of Psychophysiological and Accessibility Advances", "authors": ["Sunday David Ubur", "Denis Gracanin"], "categories": ["cs.HC"], "comment": "Updated with overlooked works, and made enhancements", "summary": "This narrative review on emotional expression in Speech-to-Text (STT)\ninterfaces with Extended Reality (XR) aims to identify advancements,\nlimitations, and research gaps in incorporating emotional expression into\ntranscribed text generated by STT systems. Using a rigorous search strategy,\nrelevant articles published between 2020 and 2024 are extracted and categorized\ninto themes such as communication enhancement technologies, innovations in\ncaptioning, visual and affective augmentation, emotion recognition in AR and\nVR, and empathic machines. The findings reveal the evolution of tools and\ntechniques to meet the needs of individuals with hearing impairments,\nshowcasing innovations in live transcription, closed captioning, AR, VR, and\nemotion recognition technologies. Despite improvements in accessibility, the\nabsence of emotional nuance in transcribed text remains a significant\ncommunication challenge. The study underscores the urgency for innovations in\nSTT technology to capture emotional expressions. The research discusses\nintegrating emotional expression into text through strategies like animated\ntext captions, emojilization tools, and models associating emotions with\nanimation properties. Extending these efforts into AR and VR environments opens\nnew possibilities for immersive and emotionally resonant experiences,\nespecially in educational contexts. The study also explores empathic\napplications in healthcare, education, and human-robot interactions,\nhighlighting the potential for personalized and effective interactions. The\nmultidisciplinary nature of the literature underscores the potential for\ncollaborative and interdisciplinary research."}
{"id": "2504.21165", "pdf": "https://arxiv.org/pdf/2504.21165.pdf", "abs": "https://arxiv.org/abs/2504.21165", "title": "Detecting Manipulated Contents Using Knowledge-Grounded Inference", "authors": ["Mark Huasong Meng", "Ruizhe Wang", "Meng Xu", "Chuan Yan", "Guangdong Bai"], "categories": ["cs.CL", "cs.SI"], "comment": "16 pages", "summary": "The detection of manipulated content, a prevalent form of fake news, has been\nwidely studied in recent years. While existing solutions have been proven\neffective in fact-checking and analyzing fake news based on historical events,\nthe reliance on either intrinsic knowledge obtained during training or manually\ncurated context hinders them from tackling zero-day manipulated content, which\ncan only be recognized with real-time contextual information. In this work, we\npropose Manicod, a tool designed for detecting zero-day manipulated content.\nManicod first sources contextual information about the input claim from\nmainstream search engines, and subsequently vectorizes the context for the\nlarge language model (LLM) through retrieval-augmented generation (RAG). The\nLLM-based inference can produce a \"truthful\" or \"manipulated\" decision and\noffer a textual explanation for the decision. To validate the effectiveness of\nManicod, we also propose a dataset comprising 4270 pieces of manipulated fake\nnews derived from 2500 recent real-world news headlines. Manicod achieves an\noverall F1 score of 0.856 on this dataset and outperforms existing methods by\nup to 1.9x in F1 score on their benchmarks on fact-checking and claim\nverification."}
{"id": "2412.03118", "pdf": "https://arxiv.org/pdf/2412.03118.pdf", "abs": "https://arxiv.org/abs/2412.03118", "title": "ObjectFinder: An Open-Vocabulary Assistive System for Interactive Object Search by Blind People", "authors": ["Ruiping Liu", "Jiaming Zhang", "Angela Schön", "Karin Müller", "Junwei Zheng", "Kailun Yang", "Anhong Guo", "Kathrin Gerling", "Rainer Stiefelhagen"], "categories": ["cs.HC", "cs.CV"], "comment": null, "summary": "Searching for objects in unfamiliar scenarios is a challenging task for blind\npeople. It involves specifying the target object, detecting it, and then\ngathering detailed information according to the user's intent. However,\nexisting description- and detection-based assistive technologies do not\nsufficiently support the multifaceted nature of interactive object search\ntasks. We present ObjectFinder, an open-vocabulary wearable assistive system\nfor interactive object search by blind people. ObjectFinder allows users to\nquery target objects using flexible wording. Once the target object is\ndetected, it provides egocentric localization information in real-time,\nincluding distance and direction. Users can then initiate different branches to\ngather detailed information based on their intent towards the target object,\nsuch as navigating to it or perceiving its surroundings. ObjectFinder is\npowered by a seamless combination of open-vocabulary models, namely an\nopen-vocabulary object detector and a multimodal large language model. The\nObjectFinder design concept and its development were carried out in\ncollaboration with a blind co-designer. To evaluate ObjectFinder, we conducted\nan exploratory user study with eight blind participants. We compared\nObjectFinder to BeMyAI and Google Lookout, popular description- and\ndetection-based assistive applications. Our findings indicate that most\nparticipants felt more independent with ObjectFinder and preferred it for\nobject search, as it enhanced scene context gathering and navigation, and\nallowed for active target identification. Finally, we discuss the implications\nfor future assistive systems to support interactive object search."}
{"id": "2504.21191", "pdf": "https://arxiv.org/pdf/2504.21191.pdf", "abs": "https://arxiv.org/abs/2504.21191", "title": "Small or Large? Zero-Shot or Finetuned? Guiding Language Model Choice for Specialized Applications in Healthcare", "authors": ["Lovedeep Gondara", "Jonathan Simkin", "Graham Sayle", "Shebnum Devji", "Gregory Arbour", "Raymond Ng"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This study aims to guide language model selection by investigating: 1) the\nnecessity of finetuning versus zero-shot usage, 2) the benefits of\ndomain-adjacent versus generic pretrained models, 3) the value of further\ndomain-specific pretraining, and 4) the continued relevance of Small Language\nModels (SLMs) compared to Large Language Models (LLMs) for specific tasks.\nUsing electronic pathology reports from the British Columbia Cancer Registry\n(BCCR), three classification scenarios with varying difficulty and data size\nare evaluated. Models include various SLMs and an LLM. SLMs are evaluated both\nzero-shot and finetuned; the LLM is evaluated zero-shot only. Finetuning\nsignificantly improved SLM performance across all scenarios compared to their\nzero-shot results. The zero-shot LLM outperformed zero-shot SLMs but was\nconsistently outperformed by finetuned SLMs. Domain-adjacent SLMs generally\nperformed better than the generic SLM after finetuning, especially on harder\ntasks. Further domain-specific pretraining yielded modest gains on easier tasks\nbut significant improvements on the complex, data-scarce task. The results\nhighlight the critical role of finetuning for SLMs in specialized domains,\nenabling them to surpass zero-shot LLM performance on targeted classification\ntasks. Pretraining on domain-adjacent or domain-specific data provides further\nadvantages, particularly for complex problems or limited finetuning data. While\nLLMs offer strong zero-shot capabilities, their performance on these specific\ntasks did not match that of appropriately finetuned SLMs. In the era of LLMs,\nSLMs remain relevant and effective, offering a potentially superior\nperformance-resource trade-off compared to LLMs."}
{"id": "2504.20567", "pdf": "https://arxiv.org/pdf/2504.20567.pdf", "abs": "https://arxiv.org/abs/2504.20567", "title": "Explanation format does not matter; but explanations do -- An Eggsbert study on explaining Bayesian Optimisation tasks", "authors": ["Tanmay Chakraborty", "Marion Koelle", "Jörg Schlötterer", "Nadine Schlicker", "Christian Wirth", "Christin Seifert"], "categories": ["cs.HC"], "comment": null, "summary": "Bayesian Optimisation (BO) is a family of methods for finding optimal\nparameters when the underlying function to be optimised is unknown. BO is used,\nfor example, for hyperparameter tuning in machine learning and as an expert\nsupport tool for tuning cyberphysical systems. For settings where humans are\ninvolved in the tuning task, methods have been developed to explain BO\n(Explainable Bayesian Optimization, XBO). However, there is little guidance on\nhow to present XBO results to humans so that they can tune the system\neffectively and efficiently. In this paper, we investigate how the XBO\nexplanation format affects users' task performance, task load, understanding\nand trust in XBO. We chose a task that is accessible to a wide range of users.\nSpecifically, we set up an egg cooking scenario with 6 parameters that\nparticipants had to adjust to achieve a perfect soft-boiled egg. We compared\nthree different explanation formats: a bar chart, a list of rules and a textual\nexplanation in a between-subjects online study with 213 participants. Our\nresults show that adding any type of explanation increases task success,\nreduces the number of trials needed to achieve success, and improves\ncomprehension and confidence. While explanations add more information for\nparticipants to process, we found no increase in user task load. We also found\nthat the aforementioned results were independent of the explanation format; all\nformats had a similar effect. This is an interesting finding for practical\napplications, as it suggests that explanations can be added to BO tuning tasks\nwithout the burden of designing or selecting specific explanation formats. In\nthe future, it would be interesting to investigate scenarios of prolonged use\nof the explanation formats and whether they have different effects on users'\nmental models of the underlying system."}
{"id": "2504.21202", "pdf": "https://arxiv.org/pdf/2504.21202.pdf", "abs": "https://arxiv.org/abs/2504.21202", "title": "Automatic Legal Writing Evaluation of LLMs", "authors": ["Ramon Pires", "Roseval Malaquias Junior", "Rodrigo Nogueira"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite the recent advances in Large Language Models, benchmarks for\nevaluating legal writing remain scarce due to the inherent complexity of\nassessing open-ended responses in this domain. One of the key challenges in\nevaluating language models on domain-specific tasks is finding test datasets\nthat are public, frequently updated, and contain comprehensive evaluation\nguidelines. The Brazilian Bar Examination meets these requirements. We\nintroduce oab-bench, a benchmark comprising 105 questions across seven areas of\nlaw from recent editions of the exam. The benchmark includes comprehensive\nevaluation guidelines and reference materials used by human examiners to ensure\nconsistent grading. We evaluate the performance of four LLMs on oab-bench,\nfinding that Claude-3.5 Sonnet achieves the best results with an average score\nof 7.93 out of 10, passing all 21 exams. We also investigated whether LLMs can\nserve as reliable automated judges for evaluating legal writing. Our\nexperiments show that frontier models like OpenAI's o1 achieve a strong\ncorrelation with human scores when evaluating approved exams, suggesting their\npotential as reliable automated evaluators despite the inherently subjective\nnature of legal writing assessment. The source code and the benchmark --\ncontaining questions, evaluation guidelines, model-generated responses, and\ntheir respective automated evaluations -- are publicly available."}
{"id": "2301.11564", "pdf": "https://arxiv.org/pdf/2301.11564.pdf", "abs": "https://arxiv.org/abs/2301.11564", "title": "Learning 6-DoF Fine-grained Grasp Detection Based on Part Affordance Grounding", "authors": ["Yaoxian Song", "Penglei Sun", "Piaopiao Jin", "Yi Ren", "Yu Zheng", "Zhixu Li", "Xiaowen Chu", "Yue Zhang", "Tiefeng Li", "Jason Gu"], "categories": ["cs.RO", "cs.CL", "cs.CV", "cs.HC"], "comment": "15 pages, 8 figures, 7 tables", "summary": "Robotic grasping is a fundamental ability for a robot to interact with the\nenvironment. Current methods focus on how to obtain a stable and reliable\ngrasping pose in object level, while little work has been studied on part\n(shape)-wise grasping which is related to fine-grained grasping and robotic\naffordance. Parts can be seen as atomic elements to compose an object, which\ncontains rich semantic knowledge and a strong correlation with affordance.\nHowever, lacking a large part-wise 3D robotic dataset limits the development of\npart representation learning and downstream applications. In this paper, we\npropose a new large Language-guided SHape grAsPing datasEt (named LangSHAPE) to\npromote 3D part-level affordance and grasping ability learning. From the\nperspective of robotic cognition, we design a two-stage fine-grained robotic\ngrasping framework (named LangPartGPD), including a novel 3D part language\ngrounding model and a part-aware grasp pose detection model, in which explicit\nlanguage input from human or large language models (LLMs) could guide a robot\nto generate part-level 6-DoF grasping pose with textual explanation. Our method\ncombines the advantages of human-robot collaboration and LLMs' planning ability\nusing explicit language as a symbolic intermediate. To evaluate the\neffectiveness of our proposed method, we perform 3D part grounding and\nfine-grained grasp detection experiments on both simulation and physical robot\nsettings, following language instructions across different degrees of textual\ncomplexity. Results show our method achieves competitive performance in 3D\ngeometry fine-grained grounding, object affordance inference, and 3D part-aware\ngrasping tasks. Our dataset and code are available on our project website\nhttps://sites.google.com/view/lang-shape"}
{"id": "2504.21214", "pdf": "https://arxiv.org/pdf/2504.21214.pdf", "abs": "https://arxiv.org/abs/2504.21214", "title": "Pretraining Large Brain Language Model for Active BCI: Silent Speech", "authors": ["Jinzhao Zhou", "Zehong Cao", "Yiqun Duan", "Connor Barkley", "Daniel Leong", "Xiaowei Jiang", "Quoc-Toan Nguyen", "Ziyi Zhao", "Thomas Do", "Yu-Cheng Chang", "Sheng-Fu Liang", "Chin-teng Lin"], "categories": ["cs.CL", "cs.AI", "eess.AS"], "comment": null, "summary": "This paper explores silent speech decoding in active brain-computer interface\n(BCI) systems, which offer more natural and flexible communication than\ntraditional BCI applications. We collected a new silent speech dataset of over\n120 hours of electroencephalogram (EEG) recordings from 12 subjects, capturing\n24 commonly used English words for language model pretraining and decoding.\nFollowing the recent success of pretraining large models with self-supervised\nparadigms to enhance EEG classification performance, we propose Large Brain\nLanguage Model (LBLM) pretrained to decode silent speech for active BCI. To\npretrain LBLM, we propose Future Spectro-Temporal Prediction (FSTP) pretraining\nparadigm to learn effective representations from unlabeled EEG data. Unlike\nexisting EEG pretraining methods that mainly follow a masked-reconstruction\nparadigm, our proposed FSTP method employs autoregressive modeling in temporal\nand frequency domains to capture both temporal and spectral dependencies from\nEEG signals. After pretraining, we finetune our LBLM on downstream tasks,\nincluding word-level and semantic-level classification. Extensive experiments\ndemonstrate significant performance gains of the LBLM over fully-supervised and\npretrained baseline models. For instance, in the difficult cross-session\nsetting, our model achieves 47.0\\% accuracy on semantic-level classification\nand 39.6\\% in word-level classification, outperforming baseline methods by\n5.4\\% and 7.3\\%, respectively. Our research advances silent speech decoding in\nactive BCI systems, offering an innovative solution for EEG language model\npretraining and a new dataset for fundamental research."}
{"id": "2309.06129", "pdf": "https://arxiv.org/pdf/2309.06129.pdf", "abs": "https://arxiv.org/abs/2309.06129", "title": "LEyes: A Lightweight Framework for Deep Learning-Based Eye Tracking using Synthetic Eye Images", "authors": ["Sean Anthony Byrne", "Virmarie Maquiling", "Marcus Nyström", "Enkelejda Kasneci", "Diederick C. Niehorster"], "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": "32 pages, 8 figures", "summary": "Deep learning has bolstered gaze estimation techniques, but real-world\ndeployment has been impeded by inadequate training datasets. This problem is\nexacerbated by both hardware-induced variations in eye images and inherent\nbiological differences across the recorded participants, leading to both\nfeature and pixel-level variance that hinders the generalizability of models\ntrained on specific datasets. While synthetic datasets can be a solution, their\ncreation is both time and resource-intensive. To address this problem, we\npresent a framework called Light Eyes or \"LEyes\" which, unlike conventional\nphotorealistic methods, only models key image features required for video-based\neye tracking using simple light distributions. LEyes facilitates easy\nconfiguration for training neural networks across diverse gaze-estimation\ntasks. We demonstrate that models trained using LEyes are consistently on-par\nor outperform other state-of-the-art algorithms in terms of pupil and CR\nlocalization across well-known datasets. In addition, a LEyes trained model\noutperforms the industry standard eye tracker using significantly more\ncost-effective hardware. Going forward, we are confident that LEyes will\nrevolutionize synthetic data generation for gaze estimation models, and lead to\nsignificant improvements of the next generation video-based eye trackers."}
{"id": "2504.21233", "pdf": "https://arxiv.org/pdf/2504.21233.pdf", "abs": "https://arxiv.org/abs/2504.21233", "title": "Phi-4-Mini-Reasoning: Exploring the Limits of Small Reasoning Language Models in Math", "authors": ["Haoran Xu", "Baolin Peng", "Hany Awadalla", "Dongdong Chen", "Yen-Chun Chen", "Mei Gao", "Young Jin Kim", "Yunsheng Li", "Liliang Ren", "Yelong Shen", "Shuohang Wang", "Weijian Xu", "Jianfeng Gao", "Weizhu Chen"], "categories": ["cs.CL"], "comment": null, "summary": "Chain-of-Thought (CoT) significantly enhances formal reasoning capabilities\nin Large Language Models (LLMs) by training them to explicitly generate\nintermediate reasoning steps. While LLMs readily benefit from such techniques,\nimproving reasoning in Small Language Models (SLMs) remains challenging due to\ntheir limited model capacity. Recent work by Deepseek-R1 demonstrates that\ndistillation from LLM-generated synthetic data can substantially improve the\nreasoning ability of SLM. However, the detailed modeling recipe is not\ndisclosed. In this work, we present a systematic training recipe for SLMs that\nconsists of four steps: (1) large-scale mid-training on diverse distilled\nlong-CoT data, (2) supervised fine-tuning on high-quality long-CoT data, (3)\nRollout DPO leveraging a carefully curated preference dataset, and (4)\nReinforcement Learning (RL) with Verifiable Reward. We apply our method on\nPhi-4-Mini, a compact 3.8B-parameter model. The resulting Phi-4-Mini-Reasoning\nmodel exceeds, on math reasoning tasks, much larger reasoning models, e.g.,\noutperforming DeepSeek-R1-Distill-Qwen-7B by 3.2 points and\nDeepSeek-R1-Distill-Llama-8B by 7.7 points on Math-500. Our results validate\nthat a carefully designed training recipe, with large-scale high-quality CoT\ndata, is effective to unlock strong reasoning capabilities even in\nresource-constrained small models."}
{"id": "2409.16920", "pdf": "https://arxiv.org/pdf/2409.16920.pdf", "abs": "https://arxiv.org/abs/2409.16920", "title": "Cross-Lingual Speech Emotion Recognition: Humans vs. Self-Supervised Models", "authors": ["Zhichen Han", "Tianqi Geng", "Hui Feng", "Jiahong Yuan", "Korin Richmond", "Yuanchao Li"], "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.HC", "cs.SD"], "comment": "Accepted to ICASSP 2025", "summary": "Utilizing Self-Supervised Learning (SSL) models for Speech Emotion\nRecognition (SER) has proven effective, yet limited research has explored\ncross-lingual scenarios. This study presents a comparative analysis between\nhuman performance and SSL models, beginning with a layer-wise analysis and an\nexploration of parameter-efficient fine-tuning strategies in monolingual,\ncross-lingual, and transfer learning contexts. We further compare the SER\nability of models and humans at both utterance- and segment-levels.\nAdditionally, we investigate the impact of dialect on cross-lingual SER through\nhuman evaluation. Our findings reveal that models, with appropriate knowledge\ntransfer, can adapt to the target language and achieve performance comparable\nto native speakers. We also demonstrate the significant effect of dialect on\nSER for individuals without prior linguistic and paralinguistic background.\nMoreover, both humans and models exhibit distinct behaviors across different\nemotions. These results offer new insights into the cross-lingual SER\ncapabilities of SSL models, underscoring both their similarities to and\ndifferences from human emotion perception."}
{"id": "2504.21239", "pdf": "https://arxiv.org/pdf/2504.21239.pdf", "abs": "https://arxiv.org/abs/2504.21239", "title": "Memorization and Knowledge Injection in Gated LLMs", "authors": ["Xu Pan", "Ely Hahami", "Zechen Zhang", "Haim Sompolinsky"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) currently struggle to sequentially add new\nmemories and integrate new knowledge. These limitations contrast with the human\nability to continuously learn from new experiences and acquire knowledge\nthroughout life. Most existing approaches add memories either through large\ncontext windows or external memory buffers (e.g., Retrieval-Augmented\nGeneration), and studies on knowledge injection rarely test scenarios\nresembling everyday life events. In this work, we introduce a continual\nlearning framework, Memory Embedded in Gated LLMs (MEGa), which injects event\nmemories directly into the weights of LLMs. Each memory is stored in a\ndedicated set of gated low-rank weights. During inference, a gating mechanism\nactivates relevant memory weights by matching query embeddings to stored memory\nembeddings. This enables the model to both recall entire memories and answer\nrelated questions. On two datasets - fictional characters and Wikipedia events\n- MEGa outperforms baseline approaches in mitigating catastrophic forgetting.\nOur model draws inspiration from the complementary memory system of the human\nbrain."}
{"id": "2410.08852", "pdf": "https://arxiv.org/pdf/2410.08852.pdf", "abs": "https://arxiv.org/abs/2410.08852", "title": "Conformalized Interactive Imitation Learning: Handling Expert Shift and Intermittent Feedback", "authors": ["Michelle Zhao", "Reid Simmons", "Henny Admoni", "Aaditya Ramdas", "Andrea Bajcsy"], "categories": ["cs.RO", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "In interactive imitation learning (IL), uncertainty quantification offers a\nway for the learner (i.e. robot) to contend with distribution shifts\nencountered during deployment by actively seeking additional feedback from an\nexpert (i.e. human) online. Prior works use mechanisms like ensemble\ndisagreement or Monte Carlo dropout to quantify when black-box IL policies are\nuncertain; however, these approaches can lead to overconfident estimates when\nfaced with deployment-time distribution shifts. Instead, we contend that we\nneed uncertainty quantification algorithms that can leverage the expert human\nfeedback received during deployment time to adapt the robot's uncertainty\nonline. To tackle this, we draw upon online conformal prediction, a\ndistribution-free method for constructing prediction intervals online given a\nstream of ground-truth labels. Human labels, however, are intermittent in the\ninteractive IL setting. Thus, from the conformal prediction side, we introduce\na novel uncertainty quantification algorithm called intermittent quantile\ntracking (IQT) that leverages a probabilistic model of intermittent labels,\nmaintains asymptotic coverage guarantees, and empirically achieves desired\ncoverage levels. From the interactive IL side, we develop ConformalDAgger, a\nnew approach wherein the robot uses prediction intervals calibrated by IQT as a\nreliable measure of deployment-time uncertainty to actively query for more\nexpert feedback. We compare ConformalDAgger to prior uncertainty-aware DAgger\nmethods in scenarios where the distribution shift is (and isn't) present\nbecause of changes in the expert's policy. We find that in simulated and\nhardware deployments on a 7DOF robotic manipulator, ConformalDAgger detects\nhigh uncertainty when the expert shifts and increases the number of\ninterventions compared to baselines, allowing the robot to more quickly learn\nthe new behavior."}
{"id": "2504.21252", "pdf": "https://arxiv.org/pdf/2504.21252.pdf", "abs": "https://arxiv.org/abs/2504.21252", "title": "Talk Before You Retrieve: Agent-Led Discussions for Better RAG in Medical QA", "authors": ["Xuanzhao Dong", "Wenhui Zhu", "Hao Wang", "Xiwen Chen", "Peijie Qiu", "Rui Yin", "Yi Su", "Yalin Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Medical question answering (QA) is a reasoning-intensive task that remains\nchallenging for large language models (LLMs) due to hallucinations and outdated\ndomain knowledge. Retrieval-Augmented Generation (RAG) provides a promising\npost-training solution by leveraging external knowledge. However, existing\nmedical RAG systems suffer from two key limitations: (1) a lack of modeling for\nhuman-like reasoning behaviors during information retrieval, and (2) reliance\non suboptimal medical corpora, which often results in the retrieval of\nirrelevant or noisy snippets. To overcome these challenges, we propose\nDiscuss-RAG, a plug-and-play module designed to enhance the medical QA RAG\nsystem through collaborative agent-based reasoning. Our method introduces a\nsummarizer agent that orchestrates a team of medical experts to emulate\nmulti-turn brainstorming, thereby improving the relevance of retrieved content.\nAdditionally, a decision-making agent evaluates the retrieved snippets before\ntheir final integration. Experimental results on four benchmark medical QA\ndatasets show that Discuss-RAG consistently outperforms MedRAG, especially\nsignificantly improving answer accuracy by up to 16.67% on BioASQ and 12.20% on\nPubMedQA. The code is available at: https://github.com/LLM-VLM-GSL/Discuss-RAG."}
{"id": "2412.19723", "pdf": "https://arxiv.org/pdf/2412.19723.pdf", "abs": "https://arxiv.org/abs/2412.19723", "title": "OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse Task Synthesis", "authors": ["Qiushi Sun", "Kanzhi Cheng", "Zichen Ding", "Chuanyang Jin", "Yian Wang", "Fangzhi Xu", "Zhenyu Wu", "Chengyou Jia", "Liheng Chen", "Zhoumianze Liu", "Ben Kao", "Guohao Li", "Junxian He", "Yu Qiao", "Zhiyong Wu"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": "Work in progress", "summary": "Graphical User Interface (GUI) agents powered by Vision-Language Models\n(VLMs) have demonstrated human-like computer control capability. Despite their\nutility in advancing digital automation, a critical bottleneck persists:\ncollecting high-quality trajectory data for training. Common practices for\ncollecting such data rely on human supervision or synthetic data generation\nthrough executing pre-defined tasks, which are either resource-intensive or\nunable to guarantee data quality. Moreover, these methods suffer from limited\ndata diversity and significant gaps between synthetic data and real-world\nenvironments. To address these challenges, we propose OS-Genesis, a novel GUI\ndata synthesis pipeline that reverses the conventional trajectory collection\nprocess. Instead of relying on pre-defined tasks, OS-Genesis enables agents\nfirst to perceive environments and perform step-wise interactions, then\nretrospectively derive high-quality tasks to enable trajectory-level\nexploration. A trajectory reward model is then employed to ensure the quality\nof the generated trajectories. We demonstrate that training GUI agents with\nOS-Genesis significantly improves their performance on highly challenging\nonline benchmarks. In-depth analysis further validates OS-Genesis's efficiency\nand its superior data quality and diversity compared to existing synthesis\nmethods. Our codes, data, and checkpoints are available at\nhttps://qiushisun.github.io/OS-Genesis-Home/."}
{"id": "2504.21299", "pdf": "https://arxiv.org/pdf/2504.21299.pdf", "abs": "https://arxiv.org/abs/2504.21299", "title": "BiasGuard: A Reasoning-enhanced Bias Detection Tool For Large Language Models", "authors": ["Zhiting Fan", "Ruizhe Chen", "Zuozhu Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Identifying bias in LLM-generated content is a crucial prerequisite for\nensuring fairness in LLMs. Existing methods, such as fairness classifiers and\nLLM-based judges, face limitations related to difficulties in understanding\nunderlying intentions and the lack of criteria for fairness judgment. In this\npaper, we introduce BiasGuard, a novel bias detection tool that explicitly\nanalyzes inputs and reasons through fairness specifications to provide accurate\njudgments. BiasGuard is implemented through a two-stage approach: the first\nstage initializes the model to explicitly reason based on fairness\nspecifications, while the second stage leverages reinforcement learning to\nenhance its reasoning and judgment capabilities. Our experiments, conducted\nacross five datasets, demonstrate that BiasGuard outperforms existing tools,\nimproving accuracy and reducing over-fairness misjudgments. We also highlight\nthe importance of reasoning-enhanced decision-making and provide evidence for\nthe effectiveness of our two-stage optimization pipeline."}
{"id": "2503.08061", "pdf": "https://arxiv.org/pdf/2503.08061.pdf", "abs": "https://arxiv.org/abs/2503.08061", "title": "ForceGrip: Reference-Free Curriculum Learning for Realistic Grip Force Control in VR Hand Manipulation", "authors": ["DongHeun Han", "Byungmin Kim", "RoUn Lee", "KyeongMin Kim", "Hyoseok Hwang", "HyeongYeop Kang"], "categories": ["cs.RO", "cs.GR", "cs.HC", "cs.LG"], "comment": "11 pages, 11 figures. Accepted to SIGGRAPH Conference Papers '25.\n  Project page: https://han-dongheun.github.io/ForceGrip", "summary": "Realistic Hand manipulation is a key component of immersive virtual reality\n(VR), yet existing methods often rely on kinematic approach or motion-capture\ndatasets that omit crucial physical attributes such as contact forces and\nfinger torques. Consequently, these approaches prioritize tight,\none-size-fits-all grips rather than reflecting users' intended force levels. We\npresent ForceGrip, a deep learning agent that synthesizes realistic hand\nmanipulation motions, faithfully reflecting the user's grip force intention.\nInstead of mimicking predefined motion datasets, ForceGrip uses generated\ntraining scenarios-randomizing object shapes, wrist movements, and trigger\ninput flows-to challenge the agent with a broad spectrum of physical\ninteractions. To effectively learn from these complex tasks, we employ a\nthree-phase curriculum learning framework comprising Finger Positioning,\nIntention Adaptation, and Dynamic Stabilization. This progressive strategy\nensures stable hand-object contact, adaptive force control based on user\ninputs, and robust handling under dynamic conditions. Additionally, a proximity\nreward function enhances natural finger motions and accelerates training\nconvergence. Quantitative and qualitative evaluations reveal ForceGrip's\nsuperior force controllability and plausibility compared to state-of-the-art\nmethods. Demo videos are available as supplementary material and the code is\nprovided at https://han-dongheun.github.io/ForceGrip."}
{"id": "2504.21303", "pdf": "https://arxiv.org/pdf/2504.21303.pdf", "abs": "https://arxiv.org/abs/2504.21303", "title": "Confidence in Large Language Model Evaluation: A Bayesian Approach to Limited-Sample Challenges", "authors": ["Xiao Xiao", "Yu Su", "Sijing Zhang", "Zhang Chen", "Yadong Chen", "Tian Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) exhibit probabilistic output characteristics,\nyet conventional evaluation frameworks rely on deterministic scalar metrics.\nThis study introduces a Bayesian approach for LLM capability assessment that\nintegrates prior knowledge through probabilistic inference, addressing\nlimitations under limited-sample regimes. By treating model capabilities as\nlatent variables and leveraging a curated query set to induce discriminative\nresponses, we formalize model ranking as a Bayesian hypothesis testing problem\nover mutually exclusive capability intervals. Experimental evaluations with\nGPT-series models demonstrate that the proposed method achieves superior\ndiscrimination compared to conventional evaluation methods. Results indicate\nthat even with reduced sample sizes, the approach maintains statistical\nrobustness while providing actionable insights, such as probabilistic\nstatements about a model's likelihood of surpassing specific baselines. This\nwork advances LLM evaluation methodologies by bridging Bayesian inference with\npractical constraints in real-world deployment scenarios."}
{"id": "2504.09689", "pdf": "https://arxiv.org/pdf/2504.09689.pdf", "abs": "https://arxiv.org/abs/2504.09689", "title": "EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental Health Safety", "authors": ["Jiahao Qiu", "Yinghui He", "Xinzhe Juan", "Yimin Wang", "Yuhan Liu", "Zixin Yao", "Yue Wu", "Xun Jiang", "Ling Yang", "Mengdi Wang"], "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC", "cs.LG"], "comment": "18 pages, 8 figures", "summary": "The rise of LLM-driven AI characters raises safety concerns, particularly for\nvulnerable human users with psychological disorders. To address these risks, we\npropose EmoAgent, a multi-agent AI framework designed to evaluate and mitigate\nmental health hazards in human-AI interactions. EmoAgent comprises two\ncomponents: EmoEval simulates virtual users, including those portraying\nmentally vulnerable individuals, to assess mental health changes before and\nafter interactions with AI characters. It uses clinically proven psychological\nand psychiatric assessment tools (PHQ-9, PDI, PANSS) to evaluate mental risks\ninduced by LLM. EmoGuard serves as an intermediary, monitoring users' mental\nstatus, predicting potential harm, and providing corrective feedback to\nmitigate risks. Experiments conducted in popular character-based chatbots show\nthat emotionally engaging dialogues can lead to psychological deterioration in\nvulnerable users, with mental state deterioration in more than 34.4% of the\nsimulations. EmoGuard significantly reduces these deterioration rates,\nunderscoring its role in ensuring safer AI-human interactions. Our code is\navailable at: https://github.com/1akaman/EmoAgent"}
{"id": "2504.21330", "pdf": "https://arxiv.org/pdf/2504.21330.pdf", "abs": "https://arxiv.org/abs/2504.21330", "title": "Does the Prompt-based Large Language Model Recognize Students' Demographics and Introduce Bias in Essay Scoring?", "authors": ["Kaixun Yang", "Mladen Raković", "Dragan Gašević", "Guanliang Chen"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are widely used in Automated Essay Scoring (AES)\ndue to their ability to capture semantic meaning. Traditional fine-tuning\napproaches required technical expertise, limiting accessibility for educators\nwith limited technical backgrounds. However, prompt-based tools like ChatGPT\nhave made AES more accessible, enabling educators to obtain machine-generated\nscores using natural-language prompts (i.e., the prompt-based paradigm).\nDespite advancements, prior studies have shown bias in fine-tuned LLMs,\nparticularly against disadvantaged groups. It remains unclear whether such\nbiases persist or are amplified in the prompt-based paradigm with cutting-edge\ntools. Since such biases are believed to stem from the demographic information\nembedded in pre-trained models (i.e., the ability of LLMs' text embeddings to\npredict demographic attributes), this study explores the relationship between\nthe model's predictive power of students' demographic attributes based on their\nwritten works and its predictive bias in the scoring task in the prompt-based\nparadigm. Using a publicly available dataset of over 25,000 students'\nargumentative essays, we designed prompts to elicit demographic inferences\n(i.e., gender, first-language background) from GPT-4o and assessed fairness in\nautomated scoring. Then we conducted multivariate regression analysis to\nexplore the impact of the model's ability to predict demographics on its\nscoring outcomes. Our findings revealed that (i) prompt-based LLMs can somewhat\ninfer students' demographics, particularly their first-language backgrounds,\nfrom their essays; (ii) scoring biases are more pronounced when the LLM\ncorrectly predicts students' first-language background than when it does not;\nand (iii) scoring error for non-native English speakers increases when the LLM\ncorrectly identifies them as non-native."}
{"id": "2504.13955", "pdf": "https://arxiv.org/pdf/2504.13955.pdf", "abs": "https://arxiv.org/abs/2504.13955", "title": "Thousand Voices of Trauma: A Large-Scale Synthetic Dataset for Modeling Prolonged Exposure Therapy Conversations", "authors": ["Suhas BN", "Dominik Mattioli", "Saeed Abdullah", "Rosa I. Arriaga", "Chris W. Wiese", "Andrew M. Sherrill"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC", "cs.LG", "68T50", "I.2.7; H.5.2"], "comment": "14 pages, 6 figures Updated Appendix with example model responses", "summary": "The advancement of AI systems for mental health support is hindered by\nlimited access to therapeutic conversation data, particularly for trauma\ntreatment. We present Thousand Voices of Trauma, a synthetic benchmark dataset\nof 3,000 therapy conversations based on Prolonged Exposure therapy protocols\nfor Post-traumatic Stress Disorder (PTSD). The dataset comprises 500 unique\ncases, each explored through six conversational perspectives that mirror the\nprogression of therapy from initial anxiety to peak distress to emotional\nprocessing. We incorporated diverse demographic profiles (ages 18-80, M=49.3,\n49.4% male, 44.4% female, 6.2% non-binary), 20 trauma types, and 10\ntrauma-related behaviors using deterministic and probabilistic generation\nmethods. Analysis reveals realistic distributions of trauma types (witnessing\nviolence 10.6%, bullying 10.2%) and symptoms (nightmares 23.4%, substance abuse\n20.8%). Clinical experts validated the dataset's therapeutic fidelity,\nhighlighting its emotional depth while suggesting refinements for greater\nauthenticity. We also developed an emotional trajectory benchmark with\nstandardized metrics for evaluating model responses. This privacy-preserving\ndataset addresses critical gaps in trauma-focused mental health data, offering\na valuable resource for advancing both patient-facing applications and\nclinician training tools."}
{"id": "2504.21372", "pdf": "https://arxiv.org/pdf/2504.21372.pdf", "abs": "https://arxiv.org/abs/2504.21372", "title": "Retrieval-Enhanced Few-Shot Prompting for Speech Event Extraction", "authors": ["Máté Gedeon"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Speech Event Extraction (SpeechEE) is a challenging task that lies at the\nintersection of Automatic Speech Recognition (ASR) and Natural Language\nProcessing (NLP), requiring the identification of structured event information\nfrom spoken language. In this work, we present a modular, pipeline-based\nSpeechEE framework that integrates high-performance ASR with semantic\nsearch-enhanced prompting of Large Language Models (LLMs). Our system first\nclassifies speech segments likely to contain events using a hybrid filtering\nmechanism including rule-based, BERT-based, and LLM-based models. It then\nemploys few-shot LLM prompting, dynamically enriched via semantic similarity\nretrieval, to identify event triggers and extract corresponding arguments. We\nevaluate the pipeline using multiple LLMs (Llama3-8B, GPT-4o-mini, and o1-mini)\nhighlighting significant performance gains with o1-mini, which achieves 63.3%\nF1 on trigger classification and 27.8% F1 on argument classification,\noutperforming prior benchmarks. Our results demonstrate that pipeline\napproaches, when empowered by retrieval-augmented LLMs, can rival or exceed\nend-to-end systems while maintaining interpretability and modularity. This work\nprovides practical insights into LLM-driven event extraction and opens pathways\nfor future hybrid models combining textual and acoustic features."}
{"id": "2504.20114", "pdf": "https://arxiv.org/pdf/2504.20114.pdf", "abs": "https://arxiv.org/abs/2504.20114", "title": "TreeHop: Generate and Filter Next Query Embeddings Efficiently for Multi-hop Question Answering", "authors": ["Zhonghao Li", "Kunpeng Zhang", "Jinghuai Ou", "Shuliang Liu", "Xuming Hu"], "categories": ["cs.IR", "cs.AI", "cs.HC", "cs.LG"], "comment": "9 pages", "summary": "Retrieval-augmented generation (RAG) systems face significant challenges in\nmulti-hop question answering (MHQA), where complex queries require synthesizing\ninformation across multiple document chunks. Existing approaches typically rely\non iterative LLM-based query rewriting and routing, resulting in high\ncomputational costs due to repeated LLM invocations and multi-stage processes.\nTo address these limitations, we propose TreeHop, an embedding-level framework\nwithout the need for LLMs in query refinement. TreeHop dynamically updates\nquery embeddings by fusing semantic information from prior queries and\nretrieved documents, enabling iterative retrieval through embedding-space\noperations alone. This method replaces the traditional\n\"Retrieve-Rewrite-Vectorize-Retrieve\" cycle with a streamlined\n\"Retrieve-Embed-Retrieve\" loop, significantly reducing computational overhead.\nMoreover, a rule-based stop criterion is introduced to further prune redundant\nretrievals, balancing efficiency and recall rate. Experimental results show\nthat TreeHop rivals advanced RAG methods across three open-domain MHQA\ndatasets, achieving comparable performance with only 5\\%-0.4\\% of the model\nparameter size and reducing the query latency by approximately 99\\% compared to\nconcurrent approaches. This makes TreeHop a faster and more cost-effective\nsolution for deployment in a range of knowledge-intensive applications. For\nreproducibility purposes, codes and data are available here:\nhttps://github.com/allen-li1231/TreeHop-RAG."}
{"id": "2504.21421", "pdf": "https://arxiv.org/pdf/2504.21421.pdf", "abs": "https://arxiv.org/abs/2504.21421", "title": "The Distribution of Dependency Distance and Hierarchical Distance in Contemporary Written Japanese and Its Influencing Factors", "authors": ["Linxuan Wang", "Shuiyuan Yu"], "categories": ["cs.CL"], "comment": "This paper has been accepted by the 13th International Quantitative\n  Linguistics Conference QUALICO 2025", "summary": "To explore the relationship between dependency distance (DD) and hierarchical\ndistance (HD) in Japanese, we compared the probability distributions of DD and\nHD with and without sentence length fixed, and analyzed the changes in mean\ndependency distance (MDD) and mean hierarchical distance (MHD) as sentence\nlength increases, along with their correlation coefficient based on the\nBalanced Corpus of Contemporary Written Japanese. It was found that the valency\nof the predicates is the underlying factor behind the trade-off relation\nbetween MDD and MHD in Japanese. Native speakers of Japanese regulate the\nlinear complexity and hierarchical complexity through the valency of the\npredicates, and the relative sizes of MDD and MHD depend on whether the\nthreshold of valency has been reached. Apart from the cognitive load, the\nvalency of the predicates also affects the probability distributions of DD and\nHD. The effect of the valency of the predicates on the distribution of HD is\ngreater than on that of DD, which leads to differences in their probability\ndistributions and causes the mean of MDD to be lower than that of MHD."}
{"id": "2504.20519", "pdf": "https://arxiv.org/pdf/2504.20519.pdf", "abs": "https://arxiv.org/abs/2504.20519", "title": "Conversations with AI Chatbots Increase Short-Term Vaccine Intentions But Do Not Outperform Standard Public Health Messaging", "authors": ["Neil K. R. Sehgal", "Sunny Rai", "Manuel Tonneau", "Anish K. Agarwal", "Joseph Cappella", "Melanie Kornides", "Lyle Ungar", "Alison Buttenheim", "Sharath Chandra Guntuku"], "categories": ["cs.CY", "cs.HC"], "comment": null, "summary": "Large language model (LLM) based chatbots show promise in persuasive\ncommunication, but existing studies often rely on weak controls or focus on\nbelief change rather than behavioral intentions or outcomes. This\npre-registered multi-country (US, Canada, UK) randomized controlled trial\ninvolving 930 vaccine-hesitant parents evaluated brief (three-minute)\nmulti-turn conversations with LLM-based chatbots against standard public health\nmessaging approaches for increasing human papillomavirus (HPV) vaccine\nintentions for their children. Participants were randomly assigned to: (1) a\nweak control (no message), (2) a strong control reflecting the standard of care\n(reading official public health materials), or (3 and 4) one of two chatbot\nconditions. One chatbot was prompted to deliver short, conversational\nresponses, while the other used the model's default output style (longer with\nbullet points). While chatbot interactions significantly increased\nself-reported vaccination intent (by 7.1-10.3 points on a 100-point scale)\ncompared to no message, they did not outperform standard public health\nmaterials, with the conversational chatbot performing significantly worse.\nAdditionally, while the short-term effects of chatbot interactions faded during\na 15-day follow-up, the effects of public health material persisted relative to\nno message. These findings suggest that while LLMs can effectively shift\nvaccination intentions in the short-term, their incremental value over existing\npublic health communications is questionable, offering a more tempered view of\ntheir persuasive capabilities and highlighting the importance of integrating\nAI-driven tools alongside, rather than replacing, current public health\nstrategies."}
{"id": "2504.21463", "pdf": "https://arxiv.org/pdf/2504.21463.pdf", "abs": "https://arxiv.org/abs/2504.21463", "title": "RWKV-X: A Linear Complexity Hybrid Language Model", "authors": ["Haowen Hou", "Zhiyi Huang", "Kaifeng Tan", "Rongchang Lu", "Fei Richard Yu"], "categories": ["cs.CL"], "comment": "12 pages", "summary": "In this paper, we introduce \\textbf{RWKV-X}, a novel hybrid architecture that\ncombines the efficiency of RWKV for short-range modeling with a sparse\nattention mechanism designed to capture long-range context. Unlike previous\nhybrid approaches that rely on full attention layers and retain quadratic\ncomplexity, RWKV-X achieves linear-time complexity in training and\nconstant-time complexity in inference decoding. We demonstrate that RWKV-X,\nwhen continually pretrained on 64K-token sequences, achieves near-perfect\naccuracy on the 64K passkey retrieval benchmark. It consistently outperforms\nprior RWKV-7 models on long-context benchmarks, while maintaining strong\nperformance on short-context tasks. These results highlight RWKV-X as a\nscalable and efficient backbone for general-purpose language modeling, capable\nof decoding sequences up to 1 million tokens with stable speed and memory\nusage. To facilitate further research and analysis, we have made the\ncheckpoints and the associated code publicly accessible at:\nhttps://github.com/howard-hou/RWKV-X."}
{"id": "2504.21474", "pdf": "https://arxiv.org/pdf/2504.21474.pdf", "abs": "https://arxiv.org/abs/2504.21474", "title": "Homa at SemEval-2025 Task 5: Aligning Librarian Records with OntoAligner for Subject Tagging", "authors": ["Hadi Bayrami Asl Tekanlou", "Jafar Razmara", "Mahsa Sanaei", "Mostafa Rahgouy", "Hamed Babaei Giglou"], "categories": ["cs.CL", "cs.AI"], "comment": "7 pages, 4 figures, accepted to the LLMs4Subjects shared task at\n  SemEval2025", "summary": "This paper presents our system, Homa, for SemEval-2025 Task 5: Subject\nTagging, which focuses on automatically assigning subject labels to technical\nrecords from TIBKAT using the Gemeinsame Normdatei (GND) taxonomy. We leverage\nOntoAligner, a modular ontology alignment toolkit, to address this task by\nintegrating retrieval-augmented generation (RAG) techniques. Our approach\nformulates the subject tagging problem as an alignment task, where records are\nmatched to GND categories based on semantic similarity. We evaluate\nOntoAligner's adaptability for subject indexing and analyze its effectiveness\nin handling multilingual records. Experimental results demonstrate the\nstrengths and limitations of this method, highlighting the potential of\nalignment techniques for improving subject tagging in digital libraries."}
{"id": "2504.21475", "pdf": "https://arxiv.org/pdf/2504.21475.pdf", "abs": "https://arxiv.org/abs/2504.21475", "title": "Advancing Arabic Reverse Dictionary Systems: A Transformer-Based Approach with Dataset Construction Guidelines", "authors": ["Serry Sibaee", "Samar Ahmed", "Abdullah Al Harbi", "Omer Nacar", "Adel Ammar", "Yasser Habashi", "Wadii Boulila"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This study addresses the critical gap in Arabic natural language processing\nby developing an effective Arabic Reverse Dictionary (RD) system that enables\nusers to find words based on their descriptions or meanings. We present a novel\ntransformer-based approach with a semi-encoder neural network architecture\nfeaturing geometrically decreasing layers that achieves state-of-the-art\nresults for Arabic RD tasks. Our methodology incorporates a comprehensive\ndataset construction process and establishes formal quality standards for\nArabic lexicographic definitions. Experiments with various pre-trained models\ndemonstrate that Arabic-specific models significantly outperform general\nmultilingual embeddings, with ARBERTv2 achieving the best ranking score\n(0.0644). Additionally, we provide a formal abstraction of the reverse\ndictionary task that enhances theoretical understanding and develop a modular,\nextensible Python library (RDTL) with configurable training pipelines. Our\nanalysis of dataset quality reveals important insights for improving Arabic\ndefinition construction, leading to eight specific standards for building\nhigh-quality reverse dictionary resources. This work contributes significantly\nto Arabic computational linguistics and provides valuable tools for language\nlearning, academic writing, and professional communication in Arabic."}
{"id": "2504.21540", "pdf": "https://arxiv.org/pdf/2504.21540.pdf", "abs": "https://arxiv.org/abs/2504.21540", "title": "Improving Informally Romanized Language Identification", "authors": ["Adrian Benton", "Alexander Gutkin", "Christo Kirov", "Brian Roark"], "categories": ["cs.CL"], "comment": "16 pages, 14 tables, 4 figures", "summary": "The Latin script is often used to informally write languages with non-Latin\nnative scripts. In many cases (e.g., most languages in India), there is no\nconventional spelling of words in the Latin script, hence there will be high\nspelling variability in written text. Such romanization renders languages that\nare normally easily distinguished based on script highly confusable, such as\nHindi and Urdu. In this work, we increase language identification (LID)\naccuracy for romanized text by improving the methods used to synthesize\ntraining sets. We find that training on synthetic samples which incorporate\nnatural spelling variation yields higher LID system accuracy than including\navailable naturally occurring examples in the training set, or even training\nhigher capacity models. We demonstrate new state-of-the-art LID performance on\nromanized text from 20 Indic languages in the Bhasha-Abhijnaanam evaluation set\n(Madhani et al., 2023a), improving test F1 from the reported 74.7% (using a\npretrained neural model) to 85.4% using a linear classifier trained solely on\nsynthetic data and 88.2% when also training on available harvested text."}
{"id": "2504.21547", "pdf": "https://arxiv.org/pdf/2504.21547.pdf", "abs": "https://arxiv.org/abs/2504.21547", "title": "TartuNLP at SemEval-2025 Task 5: Subject Tagging as Two-Stage Information Retrieval", "authors": ["Aleksei Dorkin", "Kairit Sirts"], "categories": ["cs.CL"], "comment": "To appear in the Proceedings of the 19th International Workshop on\n  Semantic Evaluation (SemEval-2025)", "summary": "We present our submission to the Task 5 of SemEval-2025 that aims to aid\nlibrarians in assigning subject tags to the library records by producing a list\nof likely relevant tags for a given document. We frame the task as an\ninformation retrieval problem, where the document content is used to retrieve\nsubject tags from a large subject taxonomy. We leverage two types of encoder\nmodels to build a two-stage information retrieval system -- a bi-encoder for\ncoarse-grained candidate extraction at the first stage, and a cross-encoder for\nfine-grained re-ranking at the second stage. This approach proved effective,\ndemonstrating significant improvements in recall compared to single-stage\nmethods and showing competitive results according to qualitative evaluation."}
{"id": "2504.21553", "pdf": "https://arxiv.org/pdf/2504.21553.pdf", "abs": "https://arxiv.org/abs/2504.21553", "title": "Precision Where It Matters: A Novel Spike Aware Mixed-Precision Quantization Strategy for LLaMA-based Language Models", "authors": ["Lucas Maisonnave", "Cyril Moineau", "Olivier Bichler", "Fabrice Rastello"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious natural language processing tasks. However, their size presents\nsignificant challenges for deployment and inference. This paper investigates\nthe quantization of LLMs, focusing on the LLaMA architecture and its\nderivatives. We challenge existing assumptions about activation outliers in\nLLMs and propose a novel mixed-precision quantization approach tailored for\nLLaMA-like models. Our method leverages the observation that activation spikes\nin LLaMA architectures are predominantly concentrated in specific projection\nlayers. By applying higher precision (FP16 or FP8) to these layers while\nquantizing the rest of the model to lower bit-widths, we achieve superior\nperformance compared to existing quantization techniques. Experimental results\non LLaMA2, LLaMA3, and Mistral models demonstrate significant improvements in\nperplexity and zero-shot accuracy, particularly for 8-bit per-tensor\nquantization. Our approach outperforms general-purpose methods designed to\nhandle outliers across all architecture types, highlighting the benefits of\narchitecture-specific quantization strategies. This research contributes to the\nongoing efforts to make LLMs more efficient and deployable, potentially\nenabling their use in resource-constrained environments. Our findings emphasize\nthe importance of considering model-specific characteristics in developing\neffective quantization pipelines for state-of-the-art language models by\nidentifying and targeting a small number of projections that concentrate\nactivation spikes."}
{"id": "2504.21589", "pdf": "https://arxiv.org/pdf/2504.21589.pdf", "abs": "https://arxiv.org/abs/2504.21589", "title": "DNB-AI-Project at SemEval-2025 Task 5: An LLM-Ensemble Approach for Automated Subject Indexing", "authors": ["Lisa Kluge", "Maximilian Kähler"], "categories": ["cs.CL", "cs.AI", "cs.DL", "I.2.7"], "comment": "11 pages, 4 figures, submitted to SemEval-2025 workshop Task 5:\n  LLMs4Subjects", "summary": "This paper presents our system developed for the SemEval-2025 Task 5:\nLLMs4Subjects: LLM-based Automated Subject Tagging for a National Technical\nLibrary's Open-Access Catalog. Our system relies on prompting a selection of\nLLMs with varying examples of intellectually annotated records and asking the\nLLMs to similarly suggest keywords for new records. This few-shot prompting\ntechnique is combined with a series of post-processing steps that map the\ngenerated keywords to the target vocabulary, aggregate the resulting subject\nterms to an ensemble vote and, finally, rank them as to their relevance to the\nrecord. Our system is fourth in the quantitative ranking in the all-subjects\ntrack, but achieves the best result in the qualitative ranking conducted by\nsubject indexing experts."}
{"id": "2504.21604", "pdf": "https://arxiv.org/pdf/2504.21604.pdf", "abs": "https://arxiv.org/abs/2504.21604", "title": "Robust Misinformation Detection by Visiting Potential Commonsense Conflict", "authors": ["Bing Wang", "Ximing Li", "Changchun Li", "Bingrui Zhao", "Bo Fu", "Renchu Guan", "Shengsheng Wang"], "categories": ["cs.CL", "cs.CY"], "comment": "11 pages, 2 figures. Accepted by IJCAI 2025. Code:\n  https://github.com/wangbing1416/MD-PCC", "summary": "The development of Internet technology has led to an increased prevalence of\nmisinformation, causing severe negative effects across diverse domains. To\nmitigate this challenge, Misinformation Detection (MD), aiming to detect online\nmisinformation automatically, emerges as a rapidly growing research topic in\nthe community. In this paper, we propose a novel plug-and-play augmentation\nmethod for the MD task, namely Misinformation Detection with Potential\nCommonsense Conflict (MD-PCC). We take inspiration from the prior studies\nindicating that fake articles are more likely to involve commonsense conflict.\nAccordingly, we construct commonsense expressions for articles, serving to\nexpress potential commonsense conflicts inferred by the difference between\nextracted commonsense triplet and golden ones inferred by the well-established\ncommonsense reasoning tool COMET. These expressions are then specified for each\narticle as augmentation. Any specific MD methods can be then trained on those\ncommonsense-augmented articles. Besides, we also collect a novel\ncommonsense-oriented dataset named CoMis, whose all fake articles are caused by\ncommonsense conflict. We integrate MD-PCC with various existing MD backbones\nand compare them across both 4 public benchmark datasets and CoMis. Empirical\nresults demonstrate that MD-PCC can consistently outperform the existing MD\nbaselines."}
{"id": "2504.21605", "pdf": "https://arxiv.org/pdf/2504.21605.pdf", "abs": "https://arxiv.org/abs/2504.21605", "title": "RDF-Based Structured Quality Assessment Representation of Multilingual LLM Evaluations", "authors": ["Jonas Gwozdz", "Andreas Both"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Large Language Models (LLMs) increasingly serve as knowledge interfaces, yet\nsystematically assessing their reliability with conflicting information remains\ndifficult. We propose an RDF-based framework to assess multilingual LLM\nquality, focusing on knowledge conflicts. Our approach captures model responses\nacross four distinct context conditions (complete, incomplete, conflicting, and\nno-context information) in German and English. This structured representation\nenables the comprehensive analysis of knowledge leakage-where models favor\ntraining data over provided context-error detection, and multilingual\nconsistency. We demonstrate the framework through a fire safety domain\nexperiment, revealing critical patterns in context prioritization and\nlanguage-specific performance, and demonstrating that our vocabulary was\nsufficient to express every assessment facet encountered in the 28-question\nstudy."}
{"id": "2504.21625", "pdf": "https://arxiv.org/pdf/2504.21625.pdf", "abs": "https://arxiv.org/abs/2504.21625", "title": "Meeseeks: An Iterative Benchmark Evaluating LLMs Multi-Turn Instruction-Following Ability", "authors": ["Jiaming Wang"], "categories": ["cs.CL"], "comment": null, "summary": "The ability to follow instructions accurately is fundamental for Large\nLanguage Models (LLMs) to serve as reliable agents in real-world applications.\nWhile existing instruction-following benchmarks are either single-turn or\nintroduce new requirements in each turn without allowing self-correction,\nMeeseeks simulates realistic human-LLM interactions through an iterative\nfeedback process. This design enables models to self-correct based on specific\nrequirement failures, better reflecting real-world user-end usage patterns. The\nbenchmark implements a comprehensive evaluation system with 38 capability tags\norganized across three dimensions: Intent Recognition, Granular Content\nValidation, and Output Structure Validation. Through rigorous evaluation across\nLLMs, Meeseeks provides valuable insights into LLMs' instruction-following\ncapabilities in practical applications."}
{"id": "2504.21635", "pdf": "https://arxiv.org/pdf/2504.21635.pdf", "abs": "https://arxiv.org/abs/2504.21635", "title": "Sadeed: Advancing Arabic Diacritization Through Small Language Model", "authors": ["Zeina Aldallal", "Sara Chrouf", "Khalil Hennara", "Mohamed Motaism Hamed", "Muhammad Hreden", "Safwan AlModhayan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Arabic text diacritization remains a persistent challenge in natural language\nprocessing due to the language's morphological richness. In this paper, we\nintroduce Sadeed, a novel approach based on a fine-tuned decoder-only language\nmodel adapted from Kuwain 1.5B Hennara et al. [2025], a compact model\noriginally trained on diverse Arabic corpora. Sadeed is fine-tuned on carefully\ncurated, high-quality diacritized datasets, constructed through a rigorous\ndata-cleaning and normalization pipeline. Despite utilizing modest\ncomputational resources, Sadeed achieves competitive results compared to\nproprietary large language models and outperforms traditional models trained on\nsimilar domains. Additionally, we highlight key limitations in current\nbenchmarking practices for Arabic diacritization. To address these issues, we\nintroduce SadeedDiac-25, a new benchmark designed to enable fairer and more\ncomprehensive evaluation across diverse text genres and complexity levels.\nTogether, Sadeed and SadeedDiac-25 provide a robust foundation for advancing\nArabic NLP applications, including machine translation, text-to-speech, and\nlanguage learning tools."}
{"id": "2504.21677", "pdf": "https://arxiv.org/pdf/2504.21677.pdf", "abs": "https://arxiv.org/abs/2504.21677", "title": "20min-XD: A Comparable Corpus of Swiss News Articles", "authors": ["Michelle Wastl", "Jannis Vamvas", "Selena Calleri", "Rico Sennrich"], "categories": ["cs.CL"], "comment": "10 pages; accepted at SwissText 2025", "summary": "We present 20min-XD (20 Minuten cross-lingual document-level), a\nFrench-German, document-level comparable corpus of news articles, sourced from\nthe Swiss online news outlet 20 Minuten/20 minutes. Our dataset comprises\naround 15,000 article pairs spanning 2015 to 2024, automatically aligned based\non semantic similarity. We detail the data collection process and alignment\nmethodology. Furthermore, we provide a qualitative and quantitative analysis of\nthe corpus. The resulting dataset exhibits a broad spectrum of cross-lingual\nsimilarity, ranging from near-translations to loosely related articles, making\nit valuable for various NLP applications and broad linguistically motivated\nstudies. We publicly release the dataset in document- and sentence-aligned\nversions and code for the described experiments."}
{"id": "2504.21681", "pdf": "https://arxiv.org/pdf/2504.21681.pdf", "abs": "https://arxiv.org/abs/2504.21681", "title": "Investigating the Effect of Parallel Data in the Cross-Lingual Transfer for Vision-Language Encoders", "authors": ["Andrei-Alexandru Manea", "Jindřich Libovický"], "categories": ["cs.CL"], "comment": null, "summary": "Most pre-trained Vision-Language (VL) models and training data for the\ndownstream tasks are only available in English. Therefore, multilingual VL\ntasks are solved using cross-lingual transfer: fine-tune a multilingual\npre-trained model or transfer the text encoder using parallel data. We study\nthe alternative approach: transferring an already trained encoder using\nparallel data. We investigate the effect of parallel data: domain and the\nnumber of languages, which were out of focus in previous work. Our results show\nthat even machine-translated task data are the best on average, caption-like\nauthentic parallel data outperformed it in some languages. Further, we show\nthat most languages benefit from multilingual training."}
{"id": "2504.21685", "pdf": "https://arxiv.org/pdf/2504.21685.pdf", "abs": "https://arxiv.org/abs/2504.21685", "title": "Enhancing Health Mention Classification Performance: A Study on Advancements in Parameter Efficient Tuning", "authors": ["Reem Abdel-Salam", "Mary Adewunmi"], "categories": ["cs.CL", "cs.AI"], "comment": "10 pages", "summary": "Health Mention Classification (HMC) plays a critical role in leveraging\nsocial media posts for real-time tracking and public health monitoring.\nNevertheless, the process of HMC presents significant challenges due to its\nintricate nature, primarily stemming from the contextual aspects of health\nmentions, such as figurative language and descriptive terminology, rather than\nexplicitly reflecting a personal ailment. To address this problem, we argue\nthat clearer mentions can be achieved through conventional fine-tuning with\nenhanced parameters of biomedical natural language methods (NLP). In this\nstudy, we explore different techniques such as the utilisation of\npart-of-speech (POS) tagger information, improving on PEFT techniques, and\ndifferent combinations thereof. Extensive experiments are conducted on three\nwidely used datasets: RHDM, PHM, and Illness. The results incorporated POS\ntagger information, and leveraging PEFT techniques significantly improves\nperformance in terms of F1-score compared to state-of-the-art methods across\nall three datasets by utilising smaller models and efficient training.\nFurthermore, the findings highlight the effectiveness of incorporating POS\ntagger information and leveraging PEFT techniques for HMC. In conclusion, the\nproposed methodology presents a potentially effective approach to accurately\nclassifying health mentions in social media posts while optimising the model\nsize and training efficiency."}
{"id": "2504.21742", "pdf": "https://arxiv.org/pdf/2504.21742.pdf", "abs": "https://arxiv.org/abs/2504.21742", "title": "Investigating Literary Motifs in Ancient and Medieval Novels with Large Language Models", "authors": ["Emelie Hallenberg"], "categories": ["cs.CL"], "comment": null, "summary": "The Greek fictional narratives often termed love novels or romances, ranging\nfrom the first century CE to the middle of the 15th century, have long been\nconsidered as similar in many ways, not least in the use of particular literary\nmotifs. By applying the use of fine-tuned large language models, this study\naims to investigate which motifs exactly that the texts in this corpus have in\ncommon, and in which ways they differ from each other. The results show that\nwhile some motifs persist throughout the corpus, others fluctuate in frequency,\nindicating certain trends or external influences. Conclusively, the method\nproves to adequately extract literary motifs according to a set definition,\nproviding data for both quantitative and qualitative analyses."}
{"id": "2504.21747", "pdf": "https://arxiv.org/pdf/2504.21747.pdf", "abs": "https://arxiv.org/abs/2504.21747", "title": "Improving Retrieval-Augmented Neural Machine Translation with Monolingual Data", "authors": ["Maxime Bouthors", "Josep Crego", "François Yvon"], "categories": ["cs.CL", "I.2.7"], "comment": "13 pages", "summary": "Conventional retrieval-augmented neural machine translation (RANMT) systems\nleverage bilingual corpora, e.g., translation memories (TMs). Yet, in many\nsettings, in-domain monolingual target-side corpora are often available. This\nwork explores ways to take advantage of such resources by retrieving relevant\nsegments directly in the target language, based on a source-side query. For\nthis, we design improved cross-lingual retrieval systems, trained with both\nsentence level and word-level matching objectives. In our experiments with two\nRANMT architectures, we first demonstrate the benefits of such cross-lingual\nobjectives in a controlled setting, obtaining translation performances that\nsurpass standard TM-based models. We then showcase our method on a real-world\nset-up, where the target monolingual resources far exceed the amount of\nparallel data and observe large improvements of our new techniques, which\noutperform both the baseline setting, and general-purpose cross-lingual\nretrievers."}
{"id": "2504.21773", "pdf": "https://arxiv.org/pdf/2504.21773.pdf", "abs": "https://arxiv.org/abs/2504.21773", "title": "MAC-Tuning: LLM Multi-Compositional Problem Reasoning with Enhanced Knowledge Boundary Awareness", "authors": ["Junsheng Huang", "Zhitao He", "Sandeep Polisetty", "Qingyun Wang", "May Fung"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "With the widespread application of large language models (LLMs), the issue of\ngenerating non-existing facts, known as hallucination, has garnered increasing\nattention. Previous research in enhancing LLM confidence estimation mainly\nfocuses on the single problem setting. However, LLM awareness of its internal\nparameterized knowledge boundary under the more challenging multi-problem\nsetting, which requires answering multiple problems accurately simultaneously,\nremains underexplored. To bridge this gap, we introduce a novel method,\nMultiple Answers and Confidence Stepwise Tuning (MAC-Tuning), that separates\nthe learning of answer prediction and confidence estimation during fine-tuning\non instruction data. Extensive experiments demonstrate that our method\noutperforms baselines by up to 25% in average precision."}
{"id": "2504.21776", "pdf": "https://arxiv.org/pdf/2504.21776.pdf", "abs": "https://arxiv.org/abs/2504.21776", "title": "WebThinker: Empowering Large Reasoning Models with Deep Research Capability", "authors": ["Xiaoxi Li", "Jiajie Jin", "Guanting Dong", "Hongjin Qian", "Yutao Zhu", "Yongkang Wu", "Ji-Rong Wen", "Zhicheng Dou"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, demonstrate\nimpressive long-horizon reasoning capabilities. However, their reliance on\nstatic internal knowledge limits their performance on complex,\nknowledge-intensive tasks and hinders their ability to produce comprehensive\nresearch reports requiring synthesis of diverse web information. To address\nthis, we propose \\textbf{WebThinker}, a deep research agent that empowers LRMs\nto autonomously search the web, navigate web pages, and draft research reports\nduring the reasoning process. WebThinker integrates a \\textbf{Deep Web\nExplorer} module, enabling LRMs to dynamically search, navigate, and extract\ninformation from the web when encountering knowledge gaps. It also employs an\n\\textbf{Autonomous Think-Search-and-Draft strategy}, allowing the model to\nseamlessly interleave reasoning, information gathering, and report writing in\nreal time. To further enhance research tool utilization, we introduce an\n\\textbf{RL-based training strategy} via iterative online Direct Preference\nOptimization (DPO). Extensive experiments on complex reasoning benchmarks\n(GPQA, GAIA, WebWalkerQA, HLE) and scientific report generation tasks (Glaive)\ndemonstrate that WebThinker significantly outperforms existing methods and\nstrong proprietary systems. Our approach enhances LRM reliability and\napplicability in complex scenarios, paving the way for more capable and\nversatile deep research systems. The code is available at\nhttps://github.com/RUC-NLPIR/WebThinker."}
{"id": "2504.21800", "pdf": "https://arxiv.org/pdf/2504.21800.pdf", "abs": "https://arxiv.org/abs/2504.21800", "title": "How Real Are Synthetic Therapy Conversations? Evaluating Fidelity in Prolonged Exposure Dialogues", "authors": ["Suhas BN", "Dominik Mattioli", "Saeed Abdullah", "Rosa I. Arriaga", "Chris W. Wiese", "Andrew M. Sherrill"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "68T50", "I.2.7; H.3.1"], "comment": "11 pages, 5 tables, updated abstract and tables", "summary": "The growing adoption of synthetic data in healthcare is driven by privacy\nconcerns, limited access to real-world data, and the high cost of annotation.\nThis work explores the use of synthetic Prolonged Exposure (PE) therapeutic\nconversations for Post-Traumatic Stress Disorder (PTSD) as a scalable\nalternative for training and evaluating clinical models. We systematically\ncompare real and synthetic dialogues using linguistic, structural, and\nprotocol-specific metrics, including turn-taking patterns and treatment\nfidelity. We also introduce and evaluate PE-specific metrics derived from\nlinguistic analysis and semantic modeling, offering a novel framework for\nassessing clinical fidelity beyond surface fluency. Our findings show that\nalthough synthetic data holds promise for mitigating data scarcity and\nprotecting patient privacy, it can struggle to capture the subtle dynamics of\ntherapeutic interactions. In our dataset, synthetic dialogues match structural\nfeatures of real-world dialogues (e.g., speaker switch ratio: 0.98 vs. 0.99),\nhowever, synthetic interactions do not adequately reflect key fidelity markers\n(e.g., distress monitoring). We highlight gaps in existing evaluation\nframeworks and advocate for fidelity-aware metrics that go beyond surface\nfluency to uncover clinically significant failures. Our findings clarify where\nsynthetic data can effectively complement real-world datasets -- and where\ncritical limitations remain."}
{"id": "2504.21801", "pdf": "https://arxiv.org/pdf/2504.21801.pdf", "abs": "https://arxiv.org/abs/2504.21801", "title": "DeepSeek-Prover-V2: Advancing Formal Mathematical Reasoning via Reinforcement Learning for Subgoal Decomposition", "authors": ["Z. Z. Ren", "Zhihong Shao", "Junxiao Song", "Huajian Xin", "Haocheng Wang", "Wanjia Zhao", "Liyue Zhang", "Zhe Fu", "Qihao Zhu", "Dejian Yang", "Z. F. Wu", "Zhibin Gou", "Shirong Ma", "Hongxuan Tang", "Yuxuan Liu", "Wenjun Gao", "Daya Guo", "Chong Ruan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We introduce DeepSeek-Prover-V2, an open-source large language model designed\nfor formal theorem proving in Lean 4, with initialization data collected\nthrough a recursive theorem proving pipeline powered by DeepSeek-V3. The\ncold-start training procedure begins by prompting DeepSeek-V3 to decompose\ncomplex problems into a series of subgoals. The proofs of resolved subgoals are\nsynthesized into a chain-of-thought process, combined with DeepSeek-V3's\nstep-by-step reasoning, to create an initial cold start for reinforcement\nlearning. This process enables us to integrate both informal and formal\nmathematical reasoning into a unified model. The resulting model,\nDeepSeek-Prover-V2-671B, achieves state-of-the-art performance in neural\ntheorem proving, reaching 88.9% pass ratio on the MiniF2F-test and solving 49\nout of 658 problems from PutnamBench. In addition to standard benchmarks, we\nintroduce ProverBench, a collection of 325 formalized problems, to enrich our\nevaluation, including 15 selected problems from the recent AIME competitions\n(years 24-25). Further evaluation on these 15 AIME problems shows that the\nmodel successfully solves 6 of them. In comparison, DeepSeek-V3 solves 8 of\nthese problems using majority voting, highlighting that the gap between formal\nand informal mathematical reasoning in large language models is substantially\nnarrowing."}
{"id": "2504.21851", "pdf": "https://arxiv.org/pdf/2504.21851.pdf", "abs": "https://arxiv.org/abs/2504.21851", "title": "TRUST: An LLM-Based Dialogue System for Trauma Understanding and Structured Assessments", "authors": ["Sichang Tu", "Abigail Powers", "Stephen Doogan", "Jinho D. Choi"], "categories": ["cs.CL", "cs.AI"], "comment": "5 figures, 4 tables", "summary": "Objectives: While Large Language Models (LLMs) have been widely used to\nassist clinicians and support patients, no existing work has explored dialogue\nsystems for standard diagnostic interviews and assessments. This study aims to\nbridge the gap in mental healthcare accessibility by developing an LLM-powered\ndialogue system that replicates clinician behavior. Materials and Methods: We\nintroduce TRUST, a framework of cooperative LLM modules capable of conducting\nformal diagnostic interviews and assessments for Post-Traumatic Stress Disorder\n(PTSD). To guide the generation of appropriate clinical responses, we propose a\nDialogue Acts schema specifically designed for clinical interviews.\nAdditionally, we develop a patient simulation approach based on real-life\ninterview transcripts to replace time-consuming and costly manual testing by\nclinicians. Results: A comprehensive set of evaluation metrics is designed to\nassess the dialogue system from both the agent and patient simulation\nperspectives. Expert evaluations by conversation and clinical specialists show\nthat TRUST performs comparably to real-life clinical interviews. Discussion:\nOur system performs at the level of average clinicians, with room for future\nenhancements in communication styles and response appropriateness. Conclusions:\nOur TRUST framework shows its potential to facilitate mental healthcare\navailability."}
{"id": "2504.21015", "pdf": "https://arxiv.org/pdf/2504.21015.pdf", "abs": "https://arxiv.org/abs/2504.21015", "title": "Don't Retrieve, Generate: Prompting LLMs for Synthetic Training Data in Dense Retrieval", "authors": ["Aarush Sinha"], "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Training effective dense retrieval models often relies on hard negative (HN)\nexamples mined from the document corpus via methods like BM25 or cross-encoders\n(CE), processes that can be computationally demanding and require full corpus\naccess. This paper introduces a different approach, an end-to-end pipeline\nwhere a Large Language Model (LLM) first generates a query from a passage, and\nthen generates a hard negative example using \\emph{only} that query text. This\ncorpus-free negative generation contrasts with standard mining techniques. We\nevaluated this \\textsc{LLM Query $\\rightarrow$ LLM HN} approach against\ntraditional \\textsc{LLM Query $\\rightarrow$ BM25 HN} and \\textsc{LLM Query\n$\\rightarrow$ CE HN} pipelines using E5-Base and GTE-Base models on several\nBEIR benchmark datasets. Our results show the proposed all-LLM pipeline\nachieves performance identical to both the BM25 and the computationally\nintensive CE baselines across nDCG@10, Precision@10, and Recall@100 metrics.\nThis demonstrates that our corpus-free negative generation method matches the\neffectiveness of complex, corpus-dependent mining techniques, offering a\npotentially simpler and more efficient pathway for training high-performance\nretrievers without sacrificing results. We make the dataset including the\nqueries and the hard-negatives for all three methods publicly available\nhttps://huggingface.co/collections/chungimungi/arxiv-hard-negatives-68027bbc601ff6cc8eb1f449."}
{"id": "2504.21035", "pdf": "https://arxiv.org/pdf/2504.21035.pdf", "abs": "https://arxiv.org/abs/2504.21035", "title": "A False Sense of Privacy: Evaluating Textual Data Sanitization Beyond Surface-level Privacy Leakage", "authors": ["Rui Xin", "Niloofar Mireshghallah", "Shuyue Stella Li", "Michael Duan", "Hyunwoo Kim", "Yejin Choi", "Yulia Tsvetkov", "Sewoong Oh", "Pang Wei Koh"], "categories": ["cs.CR", "cs.CL", "cs.LG"], "comment": null, "summary": "Sanitizing sensitive text data typically involves removing personally\nidentifiable information (PII) or generating synthetic data under the\nassumption that these methods adequately protect privacy; however, their\neffectiveness is often only assessed by measuring the leakage of explicit\nidentifiers but ignoring nuanced textual markers that can lead to\nre-identification. We challenge the above illusion of privacy by proposing a\nnew framework that evaluates re-identification attacks to quantify individual\nprivacy risks upon data release. Our approach shows that seemingly innocuous\nauxiliary information -- such as routine social activities -- can be used to\ninfer sensitive attributes like age or substance use history from sanitized\ndata. For instance, we demonstrate that Azure's commercial PII removal tool\nfails to protect 74\\% of information in the MedQA dataset. Although\ndifferential privacy mitigates these risks to some extent, it significantly\nreduces the utility of the sanitized text for downstream tasks. Our findings\nindicate that current sanitization techniques offer a \\textit{false sense of\nprivacy}, highlighting the need for more robust methods that protect against\nsemantic-level information leakage."}
{"id": "2504.21051", "pdf": "https://arxiv.org/pdf/2504.21051.pdf", "abs": "https://arxiv.org/abs/2504.21051", "title": "Multimodal Large Language Models for Medicine: A Comprehensive Survey", "authors": ["Jiarui Ye", "Hao Tang"], "categories": ["cs.LG", "cs.CL", "cs.MM"], "comment": null, "summary": "MLLMs have recently become a focal point in the field of artificial\nintelligence research. Building on the strong capabilities of LLMs, MLLMs are\nadept at addressing complex multi-modal tasks. With the release of GPT-4, MLLMs\nhave gained substantial attention from different domains. Researchers have\nbegun to explore the potential of MLLMs in the medical and healthcare domain.\nIn this paper, we first introduce the background and fundamental concepts\nrelated to LLMs and MLLMs, while emphasizing the working principles of MLLMs.\nSubsequently, we summarize three main directions of application within\nhealthcare: medical reporting, medical diagnosis, and medical treatment. Our\nfindings are based on a comprehensive review of 330 recent papers in this area.\nWe illustrate the remarkable capabilities of MLLMs in these domains by\nproviding specific examples. For data, we present six mainstream modes of data\nalong with their corresponding evaluation benchmarks. At the end of the survey,\nwe discuss the challenges faced by MLLMs in the medical and healthcare domain\nand propose feasible methods to mitigate or overcome these issues."}
{"id": "2504.21318", "pdf": "https://arxiv.org/pdf/2504.21318.pdf", "abs": "https://arxiv.org/abs/2504.21318", "title": "Phi-4-reasoning Technical Report", "authors": ["Marah Abdin", "Sahaj Agarwal", "Ahmed Awadallah", "Vidhisha Balachandran", "Harkirat Behl", "Lingjiao Chen", "Gustavo de Rosa", "Suriya Gunasekar", "Mojan Javaheripi", "Neel Joshi", "Piero Kauffmann", "Yash Lara", "Caio César Teodoro Mendes", "Arindam Mitra", "Besmira Nushi", "Dimitris Papailiopoulos", "Olli Saarikivi", "Shital Shah", "Vaishnavi Shrivastava", "Vibhav Vineet", "Yue Wu", "Safoora Yousefi", "Guoqing Zheng"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "We introduce Phi-4-reasoning, a 14-billion parameter reasoning model that\nachieves strong performance on complex reasoning tasks. Trained via supervised\nfine-tuning of Phi-4 on carefully curated set of \"teachable\" prompts-selected\nfor the right level of complexity and diversity-and reasoning demonstrations\ngenerated using o3-mini, Phi-4-reasoning generates detailed reasoning chains\nthat effectively leverage inference-time compute. We further develop\nPhi-4-reasoning-plus, a variant enhanced through a short phase of outcome-based\nreinforcement learning that offers higher performance by generating longer\nreasoning traces. Across a wide range of reasoning tasks, both models\noutperform significantly larger open-weight models such as\nDeepSeek-R1-Distill-Llama-70B model and approach the performance levels of full\nDeepSeek-R1 model. Our comprehensive evaluations span benchmarks in math and\nscientific reasoning, coding, algorithmic problem solving, planning, and\nspatial understanding. Interestingly, we observe a non-trivial transfer of\nimprovements to general-purpose benchmarks as well. In this report, we provide\ninsights into our training data, our training methodologies, and our\nevaluations. We show that the benefit of careful data curation for supervised\nfine-tuning (SFT) extends to reasoning language models, and can be further\namplified by reinforcement learning (RL). Finally, our evaluation points to\nopportunities for improving how we assess the performance and robustness of\nreasoning models."}
{"id": "2504.21400", "pdf": "https://arxiv.org/pdf/2504.21400.pdf", "abs": "https://arxiv.org/abs/2504.21400", "title": "Who Gets the Callback? Generative AI and Gender Bias", "authors": ["Sugat Chaturvedi", "Rochana Chaturvedi"], "categories": ["econ.GN", "cs.CL", "q-fin.EC"], "comment": null, "summary": "Generative artificial intelligence (AI), particularly large language models\n(LLMs), is being rapidly deployed in recruitment and for candidate\nshortlisting. We audit several mid-sized open-source LLMs for gender bias using\na dataset of 332,044 real-world online job postings. For each posting, we\nprompt the model to recommend whether an equally qualified male or female\ncandidate should receive an interview callback. We find that most models tend\nto favor men, especially for higher-wage roles. Mapping job descriptions to the\nStandard Occupational Classification system, we find lower callback rates for\nwomen in male-dominated occupations and higher rates in female-associated ones,\nindicating occupational segregation. A comprehensive analysis of linguistic\nfeatures in job ads reveals strong alignment of model recommendations with\ntraditional gender stereotypes. To examine the role of recruiter identity, we\nsteer model behavior by infusing Big Five personality traits and simulating the\nperspectives of historical figures. We find that less agreeable personas reduce\nstereotyping, consistent with an agreeableness bias in LLMs. Our findings\nhighlight how AI-driven hiring may perpetuate biases in the labor market and\nhave implications for fairness and diversity within firms."}
{"id": "2504.21435", "pdf": "https://arxiv.org/pdf/2504.21435.pdf", "abs": "https://arxiv.org/abs/2504.21435", "title": "SeriesBench: A Benchmark for Narrative-Driven Drama Series Understanding", "authors": ["Chenkai Zhang", "Yiming Lei", "Zeming Liu", "Haitao Leng", "ShaoGuo Liu", "Tingting Gao", "Qingjie Liu", "Yunhong Wang"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "29 pages, 15 figures, CVPR 2025", "summary": "With the rapid development of Multi-modal Large Language Models (MLLMs), an\nincreasing number of benchmarks have been established to evaluate the video\nunderstanding capabilities of these models. However, these benchmarks focus on\n\\textbf{standalone} videos and mainly assess ``visual elements'' like human\nactions and object states. In reality, contemporary videos often encompass\ncomplex and continuous narratives, typically presented as a \\textbf{series}. To\naddress this challenge, we propose \\textbf{SeriesBench}, a benchmark consisting\nof 105 carefully curated narrative-driven series, covering 28 specialized tasks\nthat require deep narrative understanding. Specifically, we first select a\ndiverse set of drama series spanning various genres. Then, we introduce a novel\nlong-span narrative annotation method, combined with a full-information\ntransformation approach to convert manual annotations into diverse task\nformats. To further enhance model capacity for detailed analysis of plot\nstructures and character relationships within series, we propose a novel\nnarrative reasoning framework, \\textbf{PC-DCoT}. Extensive results on\n\\textbf{SeriesBench} indicate that existing MLLMs still face significant\nchallenges in understanding narrative-driven series, while \\textbf{PC-DCoT}\nenables these MLLMs to achieve performance improvements. Overall, our\n\\textbf{SeriesBench} and \\textbf{PC-DCoT} highlight the critical necessity of\nadvancing model capabilities to understand narrative-driven series, guiding the\nfuture development of MLLMs. SeriesBench is publicly available at\nhttps://github.com/zackhxn/SeriesBench-CVPR2025."}
{"id": "2504.21559", "pdf": "https://arxiv.org/pdf/2504.21559.pdf", "abs": "https://arxiv.org/abs/2504.21559", "title": "Black-Box Visual Prompt Engineering for Mitigating Object Hallucination in Large Vision Language Models", "authors": ["Sangmin Woo", "Kang Zhou", "Yun Zhou", "Shuai Wang", "Sheng Guan", "Haibo Ding", "Lin Lee Cheong"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "NAACL 2025", "summary": "Large Vision Language Models (LVLMs) often suffer from object hallucination,\nwhich undermines their reliability. Surprisingly, we find that simple\nobject-based visual prompting -- overlaying visual cues (e.g., bounding box,\ncircle) on images -- can significantly mitigate such hallucination; however,\ndifferent visual prompts (VPs) vary in effectiveness. To address this, we\npropose Black-Box Visual Prompt Engineering (BBVPE), a framework to identify\noptimal VPs that enhance LVLM responses without needing access to model\ninternals. Our approach employs a pool of candidate VPs and trains a router\nmodel to dynamically select the most effective VP for a given input image. This\nblack-box approach is model-agnostic, making it applicable to both open-source\nand proprietary LVLMs. Evaluations on benchmarks such as POPE and CHAIR\ndemonstrate that BBVPE effectively reduces object hallucination."}
{"id": "2504.21578", "pdf": "https://arxiv.org/pdf/2504.21578.pdf", "abs": "https://arxiv.org/abs/2504.21578", "title": "Glucagon and insulin production in pancreatic cells modeled using Petri nets and Boolean networks", "authors": ["Kamila Barylska", "Frank Delaplace", "Anna Gogolińska", "Ewa Pańkowska"], "categories": ["q-bio.CB", "cs.CL", "03", "F.2; G.0"], "comment": null, "summary": "Diabetes is a civilization chronic disease characterized by a constant\nelevated concentration of glucose in the blood. Many processes are involved in\nthe glucose regulation, and their interactions are very complex. To better\nunderstand those processes we set ourselves a goal to create a Petri net model\nof the glucose regulation in the whole body. So far we have managed to create a\nmodel of glycolysis and synthesis of glucose in the liver, and the general\noverview models of the glucose regulation in a healthy and diabetic person. In\nthis paper we introduce Petri nets models of insulin secretion in beta cell of\nthe pancreas, and glucagon in the pancreas alpha cells. Those two hormones have\nmutually opposite effects: insulin preventing hyperglycemia, and glucagon\npreventing hypoglycemia. Understanding the mechanisms of insulin and glucagon\nsecretion constitutes the basis for understanding diabetes. We also present a\nmodel in which both processes occur together, depending on the blood glucose\nlevel. The dynamics of each model is analysed. Additionally, we transform the\noverall insulin and glucagon secretion system to a Boolean network, following\nstandard transformation rules."}
{"id": "2504.21659", "pdf": "https://arxiv.org/pdf/2504.21659.pdf", "abs": "https://arxiv.org/abs/2504.21659", "title": "AdaR1: From Long-CoT to Hybrid-CoT via Bi-Level Adaptive Reasoning Optimization", "authors": ["Haotian Luo", "Haiying He", "Yibo Wang", "Jinluan Yang", "Rui Liu", "Naiqiang Tan", "Xiaochun Cao", "Dacheng Tao", "Li Shen"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recently, long-thought reasoning models achieve strong performance on complex\nreasoning tasks, but often incur substantial inference overhead, making\nefficiency a critical concern. Our empirical analysis reveals that the benefit\nof using Long-CoT varies across problems: while some problems require elaborate\nreasoning, others show no improvement, or even degraded accuracy. This\nmotivates adaptive reasoning strategies that tailor reasoning depth to the\ninput. However, prior work primarily reduces redundancy within long reasoning\npaths, limiting exploration of more efficient strategies beyond the Long-CoT\nparadigm. To address this, we propose a novel two-stage framework for adaptive\nand efficient reasoning. First, we construct a hybrid reasoning model by\nmerging long and short CoT models to enable diverse reasoning styles. Second,\nwe apply bi-level preference training to guide the model to select suitable\nreasoning styles (group-level), and prefer concise and correct reasoning within\neach style group (instance-level). Experiments demonstrate that our method\nsignificantly reduces inference costs compared to other baseline approaches,\nwhile maintaining performance. Notably, on five mathematical datasets, the\naverage length of reasoning is reduced by more than 50%, highlighting the\npotential of adaptive strategies to optimize reasoning efficiency in large\nlanguage models. Our code is coming soon at https://github.com/StarDewXXX/AdaR1"}
{"id": "2504.21716", "pdf": "https://arxiv.org/pdf/2504.21716.pdf", "abs": "https://arxiv.org/abs/2504.21716", "title": "LLM-Empowered Embodied Agent for Memory-Augmented Task Planning in Household Robotics", "authors": ["Marc Glocker", "Peter Hönig", "Matthias Hirschmanner", "Markus Vincze"], "categories": ["cs.RO", "cs.AI", "cs.CL"], "comment": "Accepted at Austrian Robotics Workshop 2025", "summary": "We present an embodied robotic system with an LLM-driven agent-orchestration\narchitecture for autonomous household object management. The system integrates\nmemory-augmented task planning, enabling robots to execute high-level user\ncommands while tracking past actions. It employs three specialized agents: a\nrouting agent, a task planning agent, and a knowledge base agent, each powered\nby task-specific LLMs. By leveraging in-context learning, our system avoids the\nneed for explicit model training. RAG enables the system to retrieve context\nfrom past interactions, enhancing long-term object tracking. A combination of\nGrounded SAM and LLaMa3.2-Vision provides robust object detection, facilitating\nsemantic scene understanding for task planning. Evaluation across three\nhousehold scenarios demonstrates high task planning accuracy and an improvement\nin memory recall due to RAG. Specifically, Qwen2.5 yields best performance for\nspecialized agents, while LLaMA3.1 excels in routing tasks. The source code is\navailable at: https://github.com/marc1198/chat-hsr."}
{"id": "2504.21751", "pdf": "https://arxiv.org/pdf/2504.21751.pdf", "abs": "https://arxiv.org/abs/2504.21751", "title": "CodeFlowBench: A Multi-turn, Iterative Benchmark for Complex Code Generation", "authors": ["Sizhe Wang", "Zhengren Wang", "Dongsheng Ma", "Yongan Yu", "Rui Ling", "Zhiyu Li", "Feiyu Xiong", "Wentao Zhang"], "categories": ["cs.SE", "cs.CL"], "comment": null, "summary": "Real world development demands code that is readable, extensible, and\ntestable by organizing the implementation into modular components and\niteratively reuse pre-implemented code. We term this iterative, multi-turn\nprocess codeflow and introduce CodeFlowBench, the first benchmark designed for\ncomprehensively evaluating LLMs' ability to perform codeflow, namely to\nimplement new functionality by reusing existing functions over multiple turns.\nCodeFlowBench comprises 5258 problems drawn from Codeforces and is continuously\nupdated via an automated pipeline that decomposes each problem into a series of\nfunction-level subproblems based on its dependency tree and each subproblem is\npaired with unit tests. We further propose a novel evaluation framework with\ntasks and metrics tailored to multi-turn code reuse to assess model\nperformance. In experiments across various LLMs under both multi-turn and\nsingle-turn patterns. We observe models' poor performance on CodeFlowBench,\nwith a substantial performance drop in the iterative codeflow scenario. For\ninstance, o1-mini achieves a pass@1 of 20.8% in multi-turn pattern versus 37.8%\nin single-turn pattern. Further analysis shows that different models excel at\ndifferent dependency depths, yet all struggle to correctly solve structurally\ncomplex problems, highlighting challenges for current LLMs to serve as code\ngeneration tools when performing codeflow. Overall, CodeFlowBench offers a\ncomprehensive benchmark and new insights into LLM capabilities for multi-turn,\niterative code generation, guiding future advances in code generation tasks."}
{"id": "2504.21798", "pdf": "https://arxiv.org/pdf/2504.21798.pdf", "abs": "https://arxiv.org/abs/2504.21798", "title": "SWE-smith: Scaling Data for Software Engineering Agents", "authors": ["John Yang", "Kilian Leret", "Carlos E. Jimenez", "Alexander Wettig", "Kabir Khandpur", "Yanzhe Zhang", "Binyuan Hui", "Ofir Press", "Ludwig Schmidt", "Diyi Yang"], "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": null, "summary": "Despite recent progress in Language Models (LMs) for software engineering,\ncollecting training data remains a significant pain point. Existing datasets\nare small, with at most 1,000s of training instances from 11 or fewer GitHub\nrepositories. The procedures to curate such datasets are often complex,\nnecessitating hundreds of hours of human labor; companion execution\nenvironments also take up several terabytes of storage, severely limiting their\nscalability and usability. To address this pain point, we introduce SWE-smith,\na novel pipeline for generating software engineering training data at scale.\nGiven any Python codebase, SWE-smith constructs a corresponding execution\nenvironment, then automatically synthesizes 100s to 1,000s of task instances\nthat break existing test(s) in the codebase. Using SWE-smith, we create a\ndataset of 50k instances sourced from 128 GitHub repositories, an order of\nmagnitude larger than all previous works. We train SWE-agent-LM-32B, achieving\n40.2% Pass@1 resolve rate on the SWE-bench Verified benchmark, state of the art\namong open source models. We open source SWE-smith (collection procedure, task\ninstances, trajectories, models) to lower the barrier of entry for research in\nLM systems for automated software engineering. All assets available at\nhttps://swesmith.com."}
{"id": "2310.18964", "pdf": "https://arxiv.org/pdf/2310.18964.pdf", "abs": "https://arxiv.org/abs/2310.18964", "title": "LLMs and Finetuning: Benchmarking cross-domain performance for hate speech detection", "authors": ["Ahmad Nasir", "Aadish Sharma", "Kokil Jaidka", "Saifuddin Ahmed"], "categories": ["cs.CL"], "comment": "18 pages, 3 figures, 5 tables", "summary": "In the evolving landscape of online communication, hate speech detection\nremains a formidable challenge, further compounded by the diversity of digital\nplatforms. This study investigates the effectiveness and adaptability of\npre-trained and fine-tuned Large Language Models (LLMs) in identifying hate\nspeech, to address two central questions: (1) To what extent does the model\nperformance depend on the fine-tuning and training parameters?, (2) To what\nextent do models generalize to cross-domain hate speech detection? and (3) What\nare the specific features of the datasets or models that influence the\ngeneralization potential? The experiment shows that LLMs offer a huge advantage\nover the state-of-the-art even without pretraining. Ordinary least squares\nanalyses suggest that the advantage of training with fine-grained hate speech\nlabels is washed away with the increase in dataset size. While our research\ndemonstrates the potential of large language models (LLMs) for hate speech\ndetection, several limitations remain, particularly regarding the validity and\nthe reproducibility of the results. We conclude with an exhaustive discussion\nof the challenges we faced in our experimentation and offer recommended best\npractices for future scholars designing benchmarking experiments of this kind."}
{"id": "2402.13517", "pdf": "https://arxiv.org/pdf/2402.13517.pdf", "abs": "https://arxiv.org/abs/2402.13517", "title": "Round Trip Translation Defence against Large Language Model Jailbreaking Attacks", "authors": ["Canaan Yung", "Hadi Mohaghegh Dolatabadi", "Sarah Erfani", "Christopher Leckie"], "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 6 figures", "summary": "Large language models (LLMs) are susceptible to social-engineered attacks\nthat are human-interpretable but require a high level of comprehension for LLMs\nto counteract. Existing defensive measures can only mitigate less than half of\nthese attacks at most. To address this issue, we propose the Round Trip\nTranslation (RTT) method, the first algorithm specifically designed to defend\nagainst social-engineered attacks on LLMs. RTT paraphrases the adversarial\nprompt and generalizes the idea conveyed, making it easier for LLMs to detect\ninduced harmful behavior. This method is versatile, lightweight, and\ntransferrable to different LLMs. Our defense successfully mitigated over 70% of\nPrompt Automatic Iterative Refinement (PAIR) attacks, which is currently the\nmost effective defense to the best of our knowledge. We are also the first to\nattempt mitigating the MathsAttack and reduced its attack success rate by\nalmost 40%. Our code is publicly available at\nhttps://github.com/Cancanxxx/Round_Trip_Translation_Defence\n  This version of the article has been accepted for publication, after peer\nreview (when applicable) but is not the Version of Record and does not reflect\npost-acceptance improvements, or any corrections. The Version of Record is\navailable online at: https://doi.org/10.48550/arXiv.2402.13517 Use of this\nAccepted Version is subject to the publisher's Accepted Manuscript terms of use\nhttps://www.springernature.com/gp/open-research/policies/accepted-manuscript-terms"}
{"id": "2404.19442", "pdf": "https://arxiv.org/pdf/2404.19442.pdf", "abs": "https://arxiv.org/abs/2404.19442", "title": "Does Generative AI speak Nigerian-Pidgin?: Issues about Representativeness and Bias for Multilingualism in LLMs", "authors": ["David Ifeoluwa Adelani", "A. Seza Doğruöz", "Iyanuoluwa Shode", "Anuoluwapo Aremu"], "categories": ["cs.CL"], "comment": "Accepted to NAACL 2025 (findings), please cite ACL anthology\n  reference on https://aclanthology.org/2025.findings-naacl.85/", "summary": "Nigeria is a multilingual country with 500+ languages. Naija is a Nigerian\nPidgin spoken by approximately 120M speakers and it is a mixed language (e.g.,\nEnglish, Portuguese, Yoruba, Hausa and Igbo). Although it has mainly been a\nspoken language until recently, there are some online platforms (e.g.,\nWikipedia), publishing in written Naija as well. West African Pidgin English\n(WAPE) is also spoken in Nigeria and it is used by BBC to broadcast news on the\ninternet to a wider audience not only in Nigeria but also in other West African\ncountries (e.g., Cameroon and Ghana). Through statistical analyses and Machine\nTranslation experiments, our paper shows that these two pidgin varieties do not\nrepresent each other (i.e., there are linguistic differences in word order and\nvocabulary) and Generative AI operates only based on WAPE. In other words,\nNaija is underrepresented in Generative AI, and it is hard to teach LLMs with\nfew examples. In addition to the statistical analyses, we also provide\nhistorical information on both pidgins as well as insights from the interviews\nconducted with volunteer Wikipedia contributors in Naija."}
{"id": "2405.15471", "pdf": "https://arxiv.org/pdf/2405.15471.pdf", "abs": "https://arxiv.org/abs/2405.15471", "title": "Emergence of a High-Dimensional Abstraction Phase in Language Transformers", "authors": ["Emily Cheng", "Diego Doimo", "Corentin Kervadec", "Iuri Macocco", "Jade Yu", "Alessandro Laio", "Marco Baroni"], "categories": ["cs.CL"], "comment": "Published as conference paper at ICLR 2025", "summary": "A language model (LM) is a mapping from a linguistic context to an output\ntoken. However, much remains to be known about this mapping, including how its\ngeometric properties relate to its function. We take a high-level geometric\napproach to its analysis, observing, across five pre-trained transformer-based\nLMs and three input datasets, a distinct phase characterized by high intrinsic\ndimensionality. During this phase, representations (1) correspond to the first\nfull linguistic abstraction of the input; (2) are the first to viably transfer\nto downstream tasks; (3) predict each other across different LMs. Moreover, we\nfind that an earlier onset of the phase strongly predicts better language\nmodelling performance. In short, our results suggest that a central\nhigh-dimensionality phase underlies core linguistic processing in many common\nLM architectures."}
{"id": "2410.07825", "pdf": "https://arxiv.org/pdf/2410.07825.pdf", "abs": "https://arxiv.org/abs/2410.07825", "title": "Extracting and Transferring Abilities For Building Multi-lingual Ability-enhanced Large Language Models", "authors": ["Zhipeng Chen", "Kun Zhou", "Liang Song", "Wayne Xin Zhao", "Bingning Wang", "Weipeng Chen", "Ji-Rong Wen"], "categories": ["cs.CL"], "comment": "17 Pages. Working in progress", "summary": "Multi-lingual ability transfer has become increasingly important for the\nbroad application of large language models (LLMs). Existing work highly relies\non training with the multi-lingual ability-related data, which may be not\navailable for low-resource languages. To solve it, we propose a Multi-lingual\nAbility Extraction and Transfer approach, named as MAET. Our key idea is to\ndecompose and extract language-agnostic ability-related weights from LLMs, and\ntransfer them across different languages by simple addition and subtraction\noperations without training. Specially, our MAET consists of the extraction and\ntransfer stages. In the extraction stage, we firstly locate key neurons that\nare highly related to specific abilities, and then employ them to extract the\ntransferable ability-specific weights. In the transfer stage, we further select\nthe ability-related parameter tensors, and design the merging strategy based on\nthe linguistic and ability specific weights, to build the multi-lingual\nability-enhanced LLM. To demonstrate the effectiveness of our proposed\napproach, we conduct extensive experiments on mathematical and scientific tasks\nin both high-resource lingual and low-resource lingual scenarios. Experiment\nresults have shown that MAET can effectively and efficiently extract and\ntransfer the advanced abilities, and outperform training-based baseline\nmethods. Our code and data are available at https://github.com/RUCAIBox/MAET."}
{"id": "2410.16658", "pdf": "https://arxiv.org/pdf/2410.16658.pdf", "abs": "https://arxiv.org/abs/2410.16658", "title": "Adsorb-Agent: Autonomous Identification of Stable Adsorption Configurations via Large Language Model Agent", "authors": ["Janghoon Ock", "Tirtha Vinchurkar", "Yayati Jadhav", "Amir Barati Farimani"], "categories": ["cs.CL", "cond-mat.mtrl-sci"], "comment": null, "summary": "Adsorption energy is a key reactivity descriptor in catalysis, enabling\nefficient screening for optimal catalysts. However, determining adsorption\nenergy typically requires evaluating numerous adsorbate-catalyst\nconfigurations. Current algorithmic approaches rely on exhaustive enumeration\nof adsorption sites and configurations, which makes the process computationally\nintensive and does not inherently guarantee the identification of the global\nminimum energy. In this work, we introduce Adsorb-Agent, a Large Language Model\n(LLM) agent designed to efficiently identify system-specific stable adsorption\nconfigurations corresponding to the global minimum adsorption energy.\nAdsorb-Agent leverages its built-in knowledge and emergent reasoning\ncapabilities to strategically explore adsorption configurations likely to hold\nadsorption energy. By reducing the reliance on exhaustive sampling, it\nsignificantly decreases the number of initial configurations required while\nimproving the accuracy of adsorption energy predictions. We evaluate\nAdsorb-Agent's performance across twenty representative systems encompassing a\nrange of complexities. The Adsorb-Agent successfully identifies comparable\nadsorption energies for 83.7% of the systems and achieves lower energies,\ncloser to the actual global minimum, for 35% of the systems, while requiring\nsignificantly fewer initial configurations than conventional methods. Its\ncapability is particularly evident in complex systems, where it identifies\nlower adsorption energies for 46.7% of systems involving intermetallic surfaces\nand 66.7% of systems with large adsorbate molecules. These results demonstrate\nthe potential of Adsorb-Agent to accelerate catalyst discovery by reducing\ncomputational costs and improving the reliability of adsorption energy\npredictions."}
{"id": "2501.00571", "pdf": "https://arxiv.org/pdf/2501.00571.pdf", "abs": "https://arxiv.org/abs/2501.00571", "title": "KnowRA: Knowledge Retrieval Augmented Method for Document-level Relation Extraction with Comprehensive Reasoning Abilities", "authors": ["Chengcheng Mai", "Yuxiang Wang", "Ziyu Gong", "Hanxiang Wang", "Yihua Huang"], "categories": ["cs.CL"], "comment": "This work has been accepted by IJCAI 2025", "summary": "Document-level relation extraction (Doc-RE) aims to extract relations between\nentities across multiple sentences. Therefore, Doc-RE requires more\ncomprehensive reasoning abilities like humans, involving complex cross-sentence\ninteractions between entities, contexts, and external general knowledge,\ncompared to the sentence-level RE. However, most existing Doc-RE methods focus\non optimizing single reasoning ability, but lack the ability to utilize\nexternal knowledge for comprehensive reasoning on long documents. To solve\nthese problems, a knowledge retrieval augmented method, named KnowRA, was\nproposed with comprehensive reasoning to autonomously determine whether to\naccept external knowledge to assist DocRE. Firstly, we constructed a document\ngraph for semantic encoding and integrated the co-reference resolution model to\naugment the co-reference reasoning ability. Then, we expanded the document\ngraph into a document knowledge graph by retrieving the external knowledge base\nfor common-sense reasoning and a novel knowledge filtration method was\npresented to filter out irrelevant knowledge. Finally, we proposed the axis\nattention mechanism to build direct and indirect associations with intermediary\nentities for achieving cross-sentence logical reasoning. Extensive experiments\nconducted on two datasets verified the effectiveness of our method compared to\nthe state-of-the-art baselines. Our code is available at\nhttps://anonymous.4open.science/r/KnowRA."}
{"id": "2502.11258", "pdf": "https://arxiv.org/pdf/2502.11258.pdf", "abs": "https://arxiv.org/abs/2502.11258", "title": "Leveraging Conditional Mutual Information to Improve Large Language Model Fine-Tuning For Classification", "authors": ["Thanushon Sivakaran", "En-Hui Yang"], "categories": ["cs.CL"], "comment": "6 pages, 2 figures, Published to IEEE ISIT 2025", "summary": "Although large language models (LLMs) have demonstrated remarkable\ncapabilities in recent years, the potential of information theory (IT) to\nenhance LLM development remains underexplored. This paper introduces the\ninformation theoretic principle of Conditional Mutual Information (CMI) to LLM\nfine-tuning for classification tasks, exploring its promise in two main ways:\nminimizing CMI to improve a model's standalone performance and maximizing CMI\nto enhance knowledge distillation (KD) for more capable student models. To\napply CMI in LLM fine-tuning, we adapt the recently proposed CMI-constrained\ndeep learning framework, which was initially developed for image\nclassification, with some modification. By minimizing CMI during LLM\nfine-tuning, we achieve superior performance gains on 6 of 8 GLUE\nclassification tasks compared to BERT. Additionally, maximizing CMI during the\nKD process results in significant performance improvements in 6 of 8 GLUE\nclassification tasks compared to DistilBERT. These findings demonstrate CMI's\nadaptability for optimizing both standalone LLMs and student models, showcasing\nits potential as a robust framework for advancing LLM fine-tuning. Our work\nbridges the gap between information theory and LLM development, offering new\ninsights for building high-performing language models."}
{"id": "2503.04785", "pdf": "https://arxiv.org/pdf/2503.04785.pdf", "abs": "https://arxiv.org/abs/2503.04785", "title": "Mapping Trustworthiness in Large Language Models: A Bibliometric Analysis Bridging Theory to Practice", "authors": ["José Siqueira de Cerqueira", "Kai-Kristian Kemell", "Muhammad Waseem", "Rebekah Rousi", "Nannan Xi", "Juho Hamari", "Pekka Abrahamsson"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "The rapid proliferation of Large Language Models (LLMs) has raised pressing\nconcerns regarding their trustworthiness, spanning issues of reliability,\ntransparency, fairness, and ethical alignment. Despite the increasing adoption\nof LLMs across various domains, there remains a lack of consensus on how to\noperationalize trustworthiness in practice. This study bridges the gap between\ntheoretical discussions and implementation by conducting a bibliometric mapping\nanalysis of 2,006 publications from 2019 to 2025. Through co-authorship\nnetworks, keyword co-occurrence analysis, and thematic evolution tracking, we\nidentify key research trends, influential authors, and prevailing definitions\nof LLM trustworthiness. Additionally, a systematic review of 68 core papers is\nconducted to examine conceptualizations of trust and their practical\nimplications. Our findings reveal that trustworthiness in LLMs is often framed\nthrough existing organizational trust frameworks, emphasizing dimensions such\nas ability, benevolence, and integrity. However, a significant gap exists in\ntranslating these principles into concrete development strategies. To address\nthis, we propose a structured mapping of 20 trust-enhancing techniques across\nthe LLM lifecycle, including retrieval-augmented generation (RAG),\nexplainability techniques, and post-training audits. By synthesizing\nbibliometric insights with practical strategies, this study contributes towards\nfostering more transparent, accountable, and ethically aligned LLMs, ensuring\ntheir responsible deployment in real-world applications."}
{"id": "2503.14258", "pdf": "https://arxiv.org/pdf/2503.14258.pdf", "abs": "https://arxiv.org/abs/2503.14258", "title": "JuDGE: Benchmarking Judgment Document Generation for Chinese Legal System", "authors": ["Weihang Su", "Baoqing Yue", "Qingyao Ai", "Yiran Hu", "Jiaqi Li", "Changyue Wang", "Kaiyuan Zhang", "Yueyue Wu", "Yiqun Liu"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "This paper introduces JuDGE (Judgment Document Generation Evaluation), a\nnovel benchmark for evaluating the performance of judgment document generation\nin the Chinese legal system. We define the task as generating a complete legal\njudgment document from the given factual description of the case. To facilitate\nthis benchmark, we construct a comprehensive dataset consisting of factual\ndescriptions from real legal cases, paired with their corresponding full\njudgment documents, which serve as the ground truth for evaluating the quality\nof generated documents. This dataset is further augmented by two external legal\ncorpora that provide additional legal knowledge for the task: one comprising\nstatutes and regulations, and the other consisting of a large collection of\npast judgment documents. In collaboration with legal professionals, we\nestablish a comprehensive automated evaluation framework to assess the quality\nof generated judgment documents across various dimensions. We evaluate various\nbaseline approaches, including few-shot in-context learning, fine-tuning, and a\nmulti-source retrieval-augmented generation (RAG) approach, using both general\nand legal-domain LLMs. The experimental results demonstrate that, while RAG\napproaches can effectively improve performance in this task, there is still\nsubstantial room for further improvement. All the codes and datasets are\navailable at: https://github.com/oneal2000/JuDGE."}
{"id": "2503.21934", "pdf": "https://arxiv.org/pdf/2503.21934.pdf", "abs": "https://arxiv.org/abs/2503.21934", "title": "Proof or Bluff? Evaluating LLMs on 2025 USA Math Olympiad", "authors": ["Ivo Petrov", "Jasper Dekoninck", "Lyuben Baltadzhiev", "Maria Drencheva", "Kristian Minchev", "Mislav Balunović", "Nikola Jovanović", "Martin Vechev"], "categories": ["cs.CL"], "comment": null, "summary": "Recent math benchmarks for large language models (LLMs) such as MathArena\nindicate that state-of-the-art reasoning models achieve impressive performance\non mathematical competitions like AIME, with the leading model, Gemini-2.5-Pro,\nachieving scores comparable to top human competitors. However, these benchmarks\nevaluate models solely based on final numerical answers, neglecting rigorous\nreasoning and proof generation which are essential for real-world mathematical\ntasks. To address this, we introduce the first comprehensive evaluation of\nfull-solution reasoning for challenging mathematical problems. Using expert\nhuman annotators, we evaluated several state-of-the-art reasoning models on the\nsix problems from the 2025 USAMO within hours of their release. Our results\nreveal that all tested models struggled significantly: only Gemini-2.5-Pro\nachieves a non-trivial score of 25%, while all other models achieve less than\n5%. Through detailed analysis of reasoning traces, we identify the most common\nfailure modes and find several unwanted artifacts arising from the optimization\nstrategies employed during model training. Overall, our results suggest that\ncurrent LLMs are inadequate for rigorous mathematical reasoning tasks,\nhighlighting the need for substantial improvements in reasoning and proof\ngeneration capabilities."}
{"id": "2504.10342", "pdf": "https://arxiv.org/pdf/2504.10342.pdf", "abs": "https://arxiv.org/abs/2504.10342", "title": "VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain Knowledge", "authors": ["Yueqi Song", "Tianyue Ou", "Yibo Kong", "Zecheng Li", "Graham Neubig", "Xiang Yue"], "categories": ["cs.CL"], "comment": "56 pages, 43 figures", "summary": "Current multimodal benchmarks often conflate reasoning with domain-specific\nknowledge, making it difficult to isolate and evaluate general reasoning\nabilities in non-expert settings. To address this, we introduce VisualPuzzles,\na benchmark that targets visual reasoning while deliberately minimizing\nreliance on specialized knowledge. VisualPuzzles consists of diverse questions\nspanning five categories: algorithmic, analogical, deductive, inductive, and\nspatial reasoning. One major source of our questions is manually translated\nlogical reasoning questions from the Chinese Civil Service Examination.\nExperiments show that VisualPuzzles requires significantly less intensive\ndomain-specific knowledge and more complex reasoning compared to benchmarks\nlike MMMU, enabling us to better evaluate genuine multimodal reasoning.\nEvaluations show that state-of-the-art multimodal large language models\nconsistently lag behind human performance on VisualPuzzles, and that strong\nperformance on knowledge-intensive benchmarks does not necessarily translate to\nsuccess on reasoning-focused, knowledge-light tasks. Additionally, reasoning\nenhancements such as scaling up inference compute (with \"thinking\" modes) yield\ninconsistent gains across models and task types, and we observe no clear\ncorrelation between model size and performance. We also found that models\nexhibit different reasoning and answering patterns on VisualPuzzles compared to\nbenchmarks with heavier emphasis on knowledge. VisualPuzzles offers a clearer\nlens through which to evaluate reasoning capabilities beyond factual recall and\ndomain knowledge."}
{"id": "2504.12311", "pdf": "https://arxiv.org/pdf/2504.12311.pdf", "abs": "https://arxiv.org/abs/2504.12311", "title": "Learning Optimal Prompt Ensemble for Multi-source Visual Prompt Transfer", "authors": ["Enming Zhang", "Liwen Cao", "Yanru Wu", "Zijie Zhao", "Guan Wang", "Yang Li"], "categories": ["cs.CL"], "comment": null, "summary": "Prompt tuning has emerged as a lightweight adaptation strategy for adapting\nfoundation models to downstream tasks, particularly in resource-constrained\nsystems. As pre-trained prompts have become valuable intellectual assets,\ncombining multiple source prompts offers a promising approach to enhance\ngeneralization to new tasks by leveraging complementary knowledge from diverse\nsources. However, naive aggregation of these prompts often leads to\nrepresentation collapse due to mutual interference, undermining their\ncollective potential. To address these challenges, we propose HGPrompt, an\nadaptive framework for multi-source prompt transfer that learns optimal\nensemble weights by jointly optimizing dual objectives: transferability and\nstability. Specifically, we first introduce an information-theoretic metric to\nevaluate the transferability of prompt-induced features on the target task,\ncapturing the intrinsic alignment between the feature representations.\nAdditionally, we propose a novel Gradient Alignment Regularization to mitigate\ngradient conflicts among prompts, enabling stable and coherent knowledge\ntransfer from multiple sources while suppressing interference. Extensive\nexperiments on the large-scale VTAB benchmark demonstrate that HGPrompt\nachieves state-of-the-art performance, validating its effectiveness in\nmulti-source prompt transfer."}
{"id": "2504.18428", "pdf": "https://arxiv.org/pdf/2504.18428.pdf", "abs": "https://arxiv.org/abs/2504.18428", "title": "PolyMath: Evaluating Mathematical Reasoning in Multilingual Contexts", "authors": ["Yiming Wang", "Pei Zhang", "Jialong Tang", "Haoran Wei", "Baosong Yang", "Rui Wang", "Chenshu Sun", "Feitong Sun", "Jiran Zhang", "Junxuan Wu", "Qiqian Cang", "Yichang Zhang", "Fei Huang", "Junyang Lin", "Fei Huang", "Jingren Zhou"], "categories": ["cs.CL"], "comment": "Work in Progress", "summary": "In this paper, we introduce PolyMath, a multilingual mathematical reasoning\nbenchmark covering 18 languages and 4 easy-to-hard difficulty levels. Our\nbenchmark ensures difficulty comprehensiveness, language diversity, and\nhigh-quality translation, making it a highly discriminative multilingual\nmathematical benchmark in the era of reasoning LLMs. We conduct a comprehensive\nevaluation for advanced LLMs and find that even Qwen-3-235B-A22B-Thinking and\nGemini-2.5-pro, achieve only 54.6 and 52.2 benchmark scores, with about 40%\naccuracy under the highest level From a language perspective, our benchmark\nreveals several key challenges of LLMs in multilingual reasoning: (1) Reasoning\nperformance varies widely across languages for current LLMs; (2) Input-output\nlanguage consistency is low in reasoning LLMs and may be correlated with\nperformance; (3) The thinking length differs significantly by language for\ncurrent LLMs. Additionally, we demonstrate that controlling the output language\nin the instructions has the potential to affect reasoning performance,\nespecially for some low-resource languages, suggesting a promising direction\nfor improving multilingual capabilities in LLMs."}
{"id": "2504.18762", "pdf": "https://arxiv.org/pdf/2504.18762.pdf", "abs": "https://arxiv.org/abs/2504.18762", "title": "SynLexLM: Scaling Legal LLMs with Synthetic Data and Curriculum Learning", "authors": ["Ojasw Upadhyay", "Abishek Saravanakumar", "Ayman Ismail"], "categories": ["cs.CL", "cs.LG"], "comment": "9 pages, 4 figures, 4 tables", "summary": "Large Language Models (LLMs) are powerful but often require extensive\nfine-tuning and large datasets for specialized domains like law.\nGeneral-purpose pre-training may not capture legal nuances, and acquiring\nsufficient legal data is challenging. We introduce SynLexLM, a novel approach\nto efficiently pre-train a legal LLM. Our method employs curriculum learning,\nprogressing from simple to complex legal texts and queries, combined with\nsynthetic data augmentation using models like Gemini Pro to address data\nscarcity. We aim to achieve improved performance on legal benchmarks\n(BigLaw-Bench, EUR-Lex-Sum) compared to traditional models and fine-tuned\nversions. Preliminary work involves generating synthetic QA pairs reflecting\nlegal reasoning. This work aims to enhance legal document analysis and research\ntools, potentially democratizing access to advanced legal AI."}
{"id": "2504.19254", "pdf": "https://arxiv.org/pdf/2504.19254.pdf", "abs": "https://arxiv.org/abs/2504.19254", "title": "Uncertainty Quantification for Language Models: A Suite of Black-Box, White-Box, LLM Judge, and Ensemble Scorers", "authors": ["Dylan Bouchard", "Mohit Singh Chauhan"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "UQLM repository: https://github.com/cvs-health/uqlm", "summary": "Hallucinations are a persistent problem with Large Language Models (LLMs). As\nthese models become increasingly used in high-stakes domains, such as\nhealthcare and finance, the need for effective hallucination detection is\ncrucial. To this end, we propose a versatile framework for zero-resource\nhallucination detection that practitioners can apply to real-world use cases.\nTo achieve this, we adapt a variety of existing uncertainty quantification (UQ)\ntechniques, including black-box UQ, white-box UQ, and LLM-as-a-Judge,\ntransforming them as necessary into standardized response-level confidence\nscores ranging from 0 to 1. To enhance flexibility, we introduce a tunable\nensemble approach that incorporates any combination of the individual\nconfidence scores. This approach enables practitioners to optimize the ensemble\nfor a specific use case for improved performance. To streamline implementation,\nthe full suite of scorers is offered in this paper's companion Python toolkit,\nUQLM. To evaluate the performance of the various scorers, we conduct an\nextensive set of experiments using several LLM question-answering benchmarks.\nWe find that our tunable ensemble typically surpasses its individual components\nand outperforms existing hallucination detection methods. Our results\ndemonstrate the benefits of customized hallucination detection strategies for\nimproving the accuracy and reliability of LLMs."}
{"id": "2504.19856", "pdf": "https://arxiv.org/pdf/2504.19856.pdf", "abs": "https://arxiv.org/abs/2504.19856", "title": "Efficient Domain-adaptive Continual Pretraining for the Process Industry in the German Language", "authors": ["Anastasia Zhukova", "Christian E. Matt", "Terry Ruas", "Bela Gipp"], "categories": ["cs.CL"], "comment": null, "summary": "Domain-adaptive continual pretraining (DAPT) is a state-of-the-art technique\nthat further trains a language model (LM) on its pretraining task, e.g.,\nlanguage masking. Although popular, it requires a significant corpus of\ndomain-related data, which is difficult to obtain for specific domains in\nlanguages other than English, such as the process industry in the German\nlanguage. This paper introduces an efficient approach called ICL-augmented\npretraining or ICL-APT that leverages in-context learning (ICL) and k-nearest\nneighbors (kNN) to augment target data with domain-related and in-domain texts,\nsignificantly reducing GPU time while maintaining strong model performance. Our\nresults show that this approach performs better than traditional DAPT by 3.5\npoints of the average IR metrics (e.g., mAP, MRR, and nDCG) and requires almost\n4 times less computing time, providing a cost-effective solution for industries\nwith limited computational capacity. The findings highlight the broader\napplicability of this framework to other low-resource industries, making\nNLP-based solutions more accessible and feasible in production environments."}
{"id": "2301.11564", "pdf": "https://arxiv.org/pdf/2301.11564.pdf", "abs": "https://arxiv.org/abs/2301.11564", "title": "Learning 6-DoF Fine-grained Grasp Detection Based on Part Affordance Grounding", "authors": ["Yaoxian Song", "Penglei Sun", "Piaopiao Jin", "Yi Ren", "Yu Zheng", "Zhixu Li", "Xiaowen Chu", "Yue Zhang", "Tiefeng Li", "Jason Gu"], "categories": ["cs.RO", "cs.CL", "cs.CV", "cs.HC"], "comment": "15 pages, 8 figures, 7 tables", "summary": "Robotic grasping is a fundamental ability for a robot to interact with the\nenvironment. Current methods focus on how to obtain a stable and reliable\ngrasping pose in object level, while little work has been studied on part\n(shape)-wise grasping which is related to fine-grained grasping and robotic\naffordance. Parts can be seen as atomic elements to compose an object, which\ncontains rich semantic knowledge and a strong correlation with affordance.\nHowever, lacking a large part-wise 3D robotic dataset limits the development of\npart representation learning and downstream applications. In this paper, we\npropose a new large Language-guided SHape grAsPing datasEt (named LangSHAPE) to\npromote 3D part-level affordance and grasping ability learning. From the\nperspective of robotic cognition, we design a two-stage fine-grained robotic\ngrasping framework (named LangPartGPD), including a novel 3D part language\ngrounding model and a part-aware grasp pose detection model, in which explicit\nlanguage input from human or large language models (LLMs) could guide a robot\nto generate part-level 6-DoF grasping pose with textual explanation. Our method\ncombines the advantages of human-robot collaboration and LLMs' planning ability\nusing explicit language as a symbolic intermediate. To evaluate the\neffectiveness of our proposed method, we perform 3D part grounding and\nfine-grained grasp detection experiments on both simulation and physical robot\nsettings, following language instructions across different degrees of textual\ncomplexity. Results show our method achieves competitive performance in 3D\ngeometry fine-grained grounding, object affordance inference, and 3D part-aware\ngrasping tasks. Our dataset and code are available on our project website\nhttps://sites.google.com/view/lang-shape"}
{"id": "2405.10718", "pdf": "https://arxiv.org/pdf/2405.10718.pdf", "abs": "https://arxiv.org/abs/2405.10718", "title": "SignLLM: Sign Language Production Large Language Models", "authors": ["Sen Fang", "Chen Chen", "Lei Wang", "Ce Zheng", "Chunyu Sui", "Yapeng Tian"], "categories": ["cs.CV", "cs.CL"], "comment": "website at https://signllm.github.io/", "summary": "In this paper, we propose SignLLM, a multilingual Sign Language Production\n(SLP) large language model, which includes two novel multilingual SLP modes\nMLSF and Prompt2LangGloss that allow sign language gestures generation from\nquery texts input and question-style prompts input respectively. Both modes can\nuse a new RL loss based on reinforcement learning and a new RL module named\nPriority Learning Channel. These RL components can accelerate the training by\nenhancing the model's capability to sample high-quality data. To train SignLLM,\nwe introduce Prompt2Sign, a comprehensive multilingual sign language dataset,\nwhich builds from public data, including American Sign Language (ASL) and seven\nothers. This dataset standardizes information by extracting pose information\nfrom sign language videos into a unified compressed format. We extensively\nevaluate SignLLM, demonstrating that our model achieves state-of-the-art\nperformance on SLP tasks across eight sign languages."}
{"id": "2408.05794", "pdf": "https://arxiv.org/pdf/2408.05794.pdf", "abs": "https://arxiv.org/abs/2408.05794", "title": "HateSieve: A Contrastive Learning Framework for Detecting and Segmenting Hateful Content in Multimodal Memes", "authors": ["Xuanyu Su", "Yansong Li", "Diana Inkpen", "Nathalie Japkowicz"], "categories": ["cs.AI", "cs.CL", "cs.MM", "cs.SI"], "comment": "Accepted at NAACL 2025 Findings; camera-ready version", "summary": "Amidst the rise of Large Multimodal Models (LMMs) and their widespread\napplication in generating and interpreting complex content, the risk of\npropagating biased and harmful memes remains significant. Current safety\nmeasures often fail to detect subtly integrated hateful content within\n``Confounder Memes''. To address this, we introduce \\textsc{HateSieve}, a new\nframework designed to enhance the detection and segmentation of hateful\nelements in memes. \\textsc{HateSieve} features a novel Contrastive Meme\nGenerator that creates semantically paired memes, a customized triplet dataset\nfor contrastive learning, and an Image-Text Alignment module that produces\ncontext-aware embeddings for accurate meme segmentation. Empirical experiments\non the Hateful Meme Dataset show that \\textsc{HateSieve} not only surpasses\nexisting LMMs in performance with fewer trainable parameters but also offers a\nrobust mechanism for precisely identifying and isolating hateful content.\n\\textcolor{red}{Caution: Contains academic discussions of hate speech; viewer\ndiscretion advised.}"}
{"id": "2409.15545", "pdf": "https://arxiv.org/pdf/2409.15545.pdf", "abs": "https://arxiv.org/abs/2409.15545", "title": "Addressing Emotion Bias in Music Emotion Recognition and Generation with Frechet Audio Distance", "authors": ["Yuanchao Li", "Azalea Gui", "Dimitra Emmanouilidou", "Hannes Gamper"], "categories": ["eess.AS", "cs.CL", "cs.MM", "cs.SD"], "comment": "Accepted to ICME 2025", "summary": "The complex nature of musical emotion introduces inherent bias in both\nrecognition and generation, particularly when relying on a single audio\nencoder, emotion classifier, or evaluation metric. In this work, we conduct a\nstudy on Music Emotion Recognition (MER) and Emotional Music Generation (EMG),\nemploying diverse audio encoders alongside Frechet Audio Distance (FAD), a\nreference-free evaluation metric. Our study begins with a benchmark evaluation\nof MER, highlighting the limitations of using a single audio encoder and the\ndisparities observed across different measurements. We then propose assessing\nMER performance using FAD derived from multiple encoders to provide a more\nobjective measure of musical emotion. Furthermore, we introduce an enhanced EMG\napproach designed to improve both the variability and prominence of generated\nmusical emotion, thereby enhancing its realism. Additionally, we investigate\nthe differences in realism between the emotions conveyed in real and synthetic\nmusic, comparing our EMG model against two baseline models. Experimental\nresults underscore the issue of emotion bias in both MER and EMG and\ndemonstrate the potential of using FAD and diverse audio encoders to evaluate\nmusical emotion more objectively and effectively."}
{"id": "2409.15551", "pdf": "https://arxiv.org/pdf/2409.15551.pdf", "abs": "https://arxiv.org/abs/2409.15551", "title": "Revise, Reason, and Recognize: LLM-Based Emotion Recognition via Emotion-Specific Prompts and ASR Error Correction", "authors": ["Yuanchao Li", "Yuan Gong", "Chao-Han Huck Yang", "Peter Bell", "Catherine Lai"], "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.MM", "cs.SD"], "comment": "Accepted to ICASSP 2025", "summary": "Annotating and recognizing speech emotion using prompt engineering has\nrecently emerged with the advancement of Large Language Models (LLMs), yet its\nefficacy and reliability remain questionable. In this paper, we conduct a\nsystematic study on this topic, beginning with the proposal of novel prompts\nthat incorporate emotion-specific knowledge from acoustics, linguistics, and\npsychology. Subsequently, we examine the effectiveness of LLM-based prompting\non Automatic Speech Recognition (ASR) transcription, contrasting it with\nground-truth transcription. Furthermore, we propose a Revise-Reason-Recognize\nprompting pipeline for robust LLM-based emotion recognition from spoken\nlanguage with ASR errors. Additionally, experiments on context-aware learning,\nin-context learning, and instruction tuning are performed to examine the\nusefulness of LLM training schemes in this direction. Finally, we investigate\nthe sensitivity of LLMs to minor prompt variations. Experimental results\ndemonstrate the efficacy of the emotion-specific prompts, ASR error correction,\nand LLM training schemes for LLM-based emotion recognition. Our study aims to\nrefine the use of LLMs in emotion recognition and related domains."}
{"id": "2409.16920", "pdf": "https://arxiv.org/pdf/2409.16920.pdf", "abs": "https://arxiv.org/abs/2409.16920", "title": "Cross-Lingual Speech Emotion Recognition: Humans vs. Self-Supervised Models", "authors": ["Zhichen Han", "Tianqi Geng", "Hui Feng", "Jiahong Yuan", "Korin Richmond", "Yuanchao Li"], "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.HC", "cs.SD"], "comment": "Accepted to ICASSP 2025", "summary": "Utilizing Self-Supervised Learning (SSL) models for Speech Emotion\nRecognition (SER) has proven effective, yet limited research has explored\ncross-lingual scenarios. This study presents a comparative analysis between\nhuman performance and SSL models, beginning with a layer-wise analysis and an\nexploration of parameter-efficient fine-tuning strategies in monolingual,\ncross-lingual, and transfer learning contexts. We further compare the SER\nability of models and humans at both utterance- and segment-levels.\nAdditionally, we investigate the impact of dialect on cross-lingual SER through\nhuman evaluation. Our findings reveal that models, with appropriate knowledge\ntransfer, can adapt to the target language and achieve performance comparable\nto native speakers. We also demonstrate the significant effect of dialect on\nSER for individuals without prior linguistic and paralinguistic background.\nMoreover, both humans and models exhibit distinct behaviors across different\nemotions. These results offer new insights into the cross-lingual SER\ncapabilities of SSL models, underscoring both their similarities to and\ndifferences from human emotion perception."}
{"id": "2409.16937", "pdf": "https://arxiv.org/pdf/2409.16937.pdf", "abs": "https://arxiv.org/abs/2409.16937", "title": "Semi-Supervised Cognitive State Classification from Speech with Multi-View Pseudo-Labeling", "authors": ["Yuanchao Li", "Zixing Zhang", "Jing Han", "Peter Bell", "Catherine Lai"], "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.MM", "cs.SD"], "comment": "Accepted to ICASSP 2025", "summary": "The lack of labeled data is a common challenge in speech classification\ntasks, particularly those requiring extensive subjective assessment, such as\ncognitive state classification. In this work, we propose a Semi-Supervised\nLearning (SSL) framework, introducing a novel multi-view pseudo-labeling method\nthat leverages both acoustic and linguistic characteristics to select the most\nconfident data for training the classification model. Acoustically, unlabeled\ndata are compared to labeled data using the Frechet audio distance, calculated\nfrom embeddings generated by multiple audio encoders. Linguistically, large\nlanguage models are prompted to revise automatic speech recognition\ntranscriptions and predict labels based on our proposed task-specific\nknowledge. High-confidence data are identified when pseudo-labels from both\nsources align, while mismatches are treated as low-confidence data. A bimodal\nclassifier is then trained to iteratively label the low-confidence data until a\npredefined criterion is met. We evaluate our SSL framework on emotion\nrecognition and dementia detection tasks. Experimental results demonstrate that\nour method achieves competitive performance compared to fully supervised\nlearning using only 30% of the labeled data and significantly outperforms two\nselected baselines."}
{"id": "2409.17899", "pdf": "https://arxiv.org/pdf/2409.17899.pdf", "abs": "https://arxiv.org/abs/2409.17899", "title": "Exploring Acoustic Similarity in Emotional Speech and Music via Self-Supervised Representations", "authors": ["Yujia Sun", "Zeyu Zhao", "Korin Richmond", "Yuanchao Li"], "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.MM", "cs.SD"], "comment": "Accepted to ICASSP 2025", "summary": "Emotion recognition from speech and music shares similarities due to their\nacoustic overlap, which has led to interest in transferring knowledge between\nthese domains. However, the shared acoustic cues between speech and music,\nparticularly those encoded by Self-Supervised Learning (SSL) models, remain\nlargely unexplored, given the fact that SSL models for speech and music have\nrarely been applied in cross-domain research. In this work, we revisit the\nacoustic similarity between emotion speech and music, starting with an analysis\nof the layerwise behavior of SSL models for Speech Emotion Recognition (SER)\nand Music Emotion Recognition (MER). Furthermore, we perform cross-domain\nadaptation by comparing several approaches in a two-stage fine-tuning process,\nexamining effective ways to utilize music for SER and speech for MER. Lastly,\nwe explore the acoustic similarities between emotional speech and music using\nFrechet audio distance for individual emotions, uncovering the issue of emotion\nbias in both speech and music SSL models. Our findings reveal that while speech\nand music SSL models do capture shared acoustic features, their behaviors can\nvary depending on different emotions due to their training strategies and\ndomain-specificities. Additionally, parameter-efficient fine-tuning can enhance\nSER and MER performance by leveraging knowledge from each other. This study\nprovides new insights into the acoustic similarity between emotional speech and\nmusic, and highlights the potential for cross-domain generalization to improve\nSER and MER systems."}
{"id": "2410.10116", "pdf": "https://arxiv.org/pdf/2410.10116.pdf", "abs": "https://arxiv.org/abs/2410.10116", "title": "How to Construct Random Unitaries", "authors": ["Fermi Ma", "Hsin-Yuan Huang"], "categories": ["quant-ph", "cs.CC", "cs.CL", "math-ph", "math.MP"], "comment": "76 pages; added grant acknowledgments", "summary": "The existence of pseudorandom unitaries (PRUs) -- efficient quantum circuits\nthat are computationally indistinguishable from Haar-random unitaries -- has\nbeen a central open question, with significant implications for cryptography,\ncomplexity theory, and fundamental physics. In this work, we close this\nquestion by proving that PRUs exist, assuming that any quantum-secure one-way\nfunction exists. We establish this result for both (1) the standard notion of\nPRUs, which are secure against any efficient adversary that makes queries to\nthe unitary $U$, and (2) a stronger notion of PRUs, which are secure even\nagainst adversaries that can query both the unitary $U$ and its inverse\n$U^\\dagger$. In the process, we prove that any algorithm that makes queries to\na Haar-random unitary can be efficiently simulated on a quantum computer, up to\ninverse-exponential trace distance."}
{"id": "2411.08165", "pdf": "https://arxiv.org/pdf/2411.08165.pdf", "abs": "https://arxiv.org/abs/2411.08165", "title": "Retrieval, Reasoning, Re-ranking: A Context-Enriched Framework for Knowledge Graph Completion", "authors": ["Muzhi Li", "Cehao Yang", "Chengjin Xu", "Xuhui Jiang", "Yiyan Qi", "Jian Guo", "Ho-fung Leung", "Irwin King"], "categories": ["cs.AI", "cs.CL"], "comment": "Accepted by NAACL2025 main", "summary": "The Knowledge Graph Completion~(KGC) task aims to infer the missing entity\nfrom an incomplete triple. Existing embedding-based methods rely solely on\ntriples in the KG, which is vulnerable to specious relation patterns and\nlong-tail entities. On the other hand, text-based methods struggle with the\nsemantic gap between KG triples and natural language. Apart from triples,\nentity contexts (e.g., labels, descriptions, aliases) also play a significant\nrole in augmenting KGs. To address these limitations, we propose KGR3, a\ncontext-enriched framework for KGC. KGR3 is composed of three modules. Firstly,\nthe Retrieval module gathers supporting triples from the KG, collects plausible\ncandidate answers from a base embedding model, and retrieves context for each\nrelated entity. Then, the Reasoning module employs a large language model to\ngenerate potential answers for each query triple. Finally, the Re-ranking\nmodule combines candidate answers from the two modules mentioned above, and\nfine-tunes an LLM to provide the best answer. Extensive experiments on widely\nused datasets demonstrate that KGR3 consistently improves various KGC methods.\nSpecifically, the best variant of KGR3 achieves absolute Hits@1 improvements of\n12.3% and 5.6% on the FB15k237 and WN18RR datasets."}
{"id": "2411.16508", "pdf": "https://arxiv.org/pdf/2411.16508.pdf", "abs": "https://arxiv.org/abs/2411.16508", "title": "All Languages Matter: Evaluating LMMs on Culturally Diverse 100 Languages", "authors": ["Ashmal Vayani", "Dinura Dissanayake", "Hasindri Watawana", "Noor Ahsan", "Nevasini Sasikumar", "Omkar Thawakar", "Henok Biadglign Ademtew", "Yahya Hmaiti", "Amandeep Kumar", "Kartik Kuckreja", "Mykola Maslych", "Wafa Al Ghallabi", "Mihail Mihaylov", "Chao Qin", "Abdelrahman M Shaker", "Mike Zhang", "Mahardika Krisna Ihsani", "Amiel Esplana", "Monil Gokani", "Shachar Mirkin", "Harsh Singh", "Ashay Srivastava", "Endre Hamerlik", "Fathinah Asma Izzati", "Fadillah Adamsyah Maani", "Sebastian Cavada", "Jenny Chim", "Rohit Gupta", "Sanjay Manjunath", "Kamila Zhumakhanova", "Feno Heriniaina Rabevohitra", "Azril Amirudin", "Muhammad Ridzuan", "Daniya Kareem", "Ketan More", "Kunyang Li", "Pramesh Shakya", "Muhammad Saad", "Amirpouya Ghasemaghaei", "Amirbek Djanibekov", "Dilshod Azizov", "Branislava Jankovic", "Naman Bhatia", "Alvaro Cabrera", "Johan Obando-Ceron", "Olympiah Otieno", "Fabian Farestam", "Muztoba Rabbani", "Sanoojan Baliah", "Santosh Sanjeev", "Abduragim Shtanchaev", "Maheen Fatima", "Thao Nguyen", "Amrin Kareem", "Toluwani Aremu", "Nathan Xavier", "Amit Bhatkal", "Hawau Toyin", "Aman Chadha", "Hisham Cholakkal", "Rao Muhammad Anwer", "Michael Felsberg", "Jorma Laaksonen", "Thamar Solorio", "Monojit Choudhury", "Ivan Laptev", "Mubarak Shah", "Salman Khan", "Fahad Khan"], "categories": ["cs.CV", "cs.CL"], "comment": "A Multilingual Multimodal cultural benchmark for 100 languages", "summary": "Existing Large Multimodal Models (LMMs) generally focus on only a few regions\nand languages. As LMMs continue to improve, it is increasingly important to\nensure they understand cultural contexts, respect local sensitivities, and\nsupport low-resource languages, all while effectively integrating corresponding\nvisual cues. In pursuit of culturally diverse global multimodal models, our\nproposed All Languages Matter Benchmark (ALM-bench) represents the largest and\nmost comprehensive effort to date for evaluating LMMs across 100 languages.\nALM-bench challenges existing models by testing their ability to understand and\nreason about culturally diverse images paired with text in various languages,\nincluding many low-resource languages traditionally underrepresented in LMM\nresearch. The benchmark offers a robust and nuanced evaluation framework\nfeaturing various question formats, including true/false, multiple choice, and\nopen-ended questions, which are further divided into short and long-answer\ncategories. ALM-bench design ensures a comprehensive assessment of a model's\nability to handle varied levels of difficulty in visual and linguistic\nreasoning. To capture the rich tapestry of global cultures, ALM-bench carefully\ncurates content from 13 distinct cultural aspects, ranging from traditions and\nrituals to famous personalities and celebrations. Through this, ALM-bench not\nonly provides a rigorous testing ground for state-of-the-art open and\nclosed-source LMMs but also highlights the importance of cultural and\nlinguistic inclusivity, encouraging the development of models that can serve\ndiverse global populations effectively. Our benchmark is publicly available."}
{"id": "2412.12119", "pdf": "https://arxiv.org/pdf/2412.12119.pdf", "abs": "https://arxiv.org/abs/2412.12119", "title": "Mastering Board Games by External and Internal Planning with Language Models", "authors": ["John Schultz", "Jakub Adamek", "Matej Jusup", "Marc Lanctot", "Michael Kaisers", "Sarah Perrin", "Daniel Hennes", "Jeremy Shar", "Cannada Lewis", "Anian Ruoss", "Tom Zahavy", "Petar Veličković", "Laurel Prince", "Satinder Singh", "Eric Malmi", "Nenad Tomašev"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "70 pages, 10 figures", "summary": "Advancing planning and reasoning capabilities of Large Language Models (LLMs)\nis one of the key prerequisites towards unlocking their potential for\nperforming reliably in complex and impactful domains. In this paper, we aim to\ndemonstrate this across board games (Chess, Fischer Random / Chess960, Connect\nFour, and Hex), and we show that search-based planning can yield significant\nimprovements in LLM game-playing strength. We introduce, compare and contrast\ntwo major approaches: In external search, the model guides Monte Carlo Tree\nSearch (MCTS) rollouts and evaluations without calls to an external game\nengine, and in internal search, the model is trained to generate in-context a\nlinearized tree of search and a resulting final choice. Both build on a\nlanguage model pre-trained on relevant domain knowledge, reliably capturing the\ntransition and value functions in the respective environments, with minimal\nhallucinations. We evaluate our LLM search implementations against\ngame-specific state-of-the-art engines, showcasing substantial improvements in\nstrength over the base model, and reaching Grandmaster-level performance in\nchess while operating closer to the human search budget. Our proposed approach,\ncombining search with domain knowledge, is not specific to board games, hinting\nat more general future applications."}
{"id": "2412.19723", "pdf": "https://arxiv.org/pdf/2412.19723.pdf", "abs": "https://arxiv.org/abs/2412.19723", "title": "OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse Task Synthesis", "authors": ["Qiushi Sun", "Kanzhi Cheng", "Zichen Ding", "Chuanyang Jin", "Yian Wang", "Fangzhi Xu", "Zhenyu Wu", "Chengyou Jia", "Liheng Chen", "Zhoumianze Liu", "Ben Kao", "Guohao Li", "Junxian He", "Yu Qiao", "Zhiyong Wu"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": "Work in progress", "summary": "Graphical User Interface (GUI) agents powered by Vision-Language Models\n(VLMs) have demonstrated human-like computer control capability. Despite their\nutility in advancing digital automation, a critical bottleneck persists:\ncollecting high-quality trajectory data for training. Common practices for\ncollecting such data rely on human supervision or synthetic data generation\nthrough executing pre-defined tasks, which are either resource-intensive or\nunable to guarantee data quality. Moreover, these methods suffer from limited\ndata diversity and significant gaps between synthetic data and real-world\nenvironments. To address these challenges, we propose OS-Genesis, a novel GUI\ndata synthesis pipeline that reverses the conventional trajectory collection\nprocess. Instead of relying on pre-defined tasks, OS-Genesis enables agents\nfirst to perceive environments and perform step-wise interactions, then\nretrospectively derive high-quality tasks to enable trajectory-level\nexploration. A trajectory reward model is then employed to ensure the quality\nof the generated trajectories. We demonstrate that training GUI agents with\nOS-Genesis significantly improves their performance on highly challenging\nonline benchmarks. In-depth analysis further validates OS-Genesis's efficiency\nand its superior data quality and diversity compared to existing synthesis\nmethods. Our codes, data, and checkpoints are available at\nhttps://qiushisun.github.io/OS-Genesis-Home/."}
{"id": "2501.15857", "pdf": "https://arxiv.org/pdf/2501.15857.pdf", "abs": "https://arxiv.org/abs/2501.15857", "title": "Are Transformers Able to Reason by Connecting Separated Knowledge in Training Data?", "authors": ["Yutong Yin", "Zhaoran Wang"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted by ICLR 2025", "summary": "Humans exhibit remarkable compositional reasoning by integrating knowledge\nfrom various sources. For example, if someone learns ( B = f(A) ) from one\nsource and ( C = g(B) ) from another, they can deduce ( C=g(B)=g(f(A)) ) even\nwithout encountering ( ABC ) together, showcasing the generalization ability of\nhuman intelligence. In this paper, we introduce a synthetic learning task,\n\"FTCT\" (Fragmented at Training, Chained at Testing), to validate the potential\nof Transformers in replicating this skill and interpret its inner mechanism. In\nthe training phase, data consist of separated knowledge fragments from an\noverall causal graph. During testing, Transformers must infer complete causal\ngraph traces by integrating these fragments. Our findings demonstrate that\nfew-shot Chain-of-Thought prompting enables Transformers to perform\ncompositional reasoning on FTCT by revealing correct combinations of fragments,\neven if such combinations were absent in the training data. Furthermore, the\nemergence of compositional reasoning ability is strongly correlated with the\nmodel complexity and training-testing data similarity. We propose, both\ntheoretically and empirically, that Transformers learn an underlying\ngeneralizable program from training, enabling effective compositional reasoning\nduring testing."}
{"id": "2502.05439", "pdf": "https://arxiv.org/pdf/2502.05439.pdf", "abs": "https://arxiv.org/abs/2502.05439", "title": "Agentic AI Systems Applied to tasks in Financial Services: Modeling and model risk management crews", "authors": ["Izunna Okpala", "Ashkan Golgoon", "Arjun Ravi Kannan"], "categories": ["cs.AI", "cs.CE", "cs.CL", "cs.LG", "68T01 (Primary) 68T05, 68N99, 68T05, 68T20, 68T50, 62H30, 65C20,\n  68P20 (Secondary)", "I.2.0; I.2.1; I.2.2; I.2.6; I.2.7; I.5.1; I.6.0; I.7.1"], "comment": null, "summary": "The advent of large language models has ushered in a new era of agentic\nsystems, where artificial intelligence programs exhibit remarkable autonomous\ndecision-making capabilities across diverse domains. This paper explores\nagentic system workflows in the financial services industry. In particular, we\nbuild agentic crews with human-in-the-loop module that can effectively\ncollaborate to perform complex modeling and model risk management (MRM) tasks.\nThe modeling crew consists of a judge agent and multiple agents who perform\nspecific tasks such as exploratory data analysis, feature engineering, model\nselection/hyperparameter tuning, model training, model evaluation, and writing\ndocumentation. The MRM crew consists of a judge agent along with specialized\nagents who perform tasks such as checking compliance of modeling documentation,\nmodel replication, conceptual soundness, analysis of outcomes, and writing\ndocumentation. We demonstrate the effectiveness and robustness of modeling and\nMRM crews by presenting a series of numerical examples applied to credit card\nfraud detection, credit card approval, and portfolio credit risk modeling\ndatasets."}
{"id": "2502.12734", "pdf": "https://arxiv.org/pdf/2502.12734.pdf", "abs": "https://arxiv.org/abs/2502.12734", "title": "Iron Sharpens Iron: Defending Against Attacks in Machine-Generated Text Detection with Adversarial Training", "authors": ["Yuanfan Li", "Zhaohan Zhang", "Chengzhengxu Li", "Chao Shen", "Xiaoming Liu"], "categories": ["cs.CR", "cs.CL"], "comment": "Accepted by ACL 2025 Main Conference", "summary": "Machine-generated Text (MGT) detection is crucial for regulating and\nattributing online texts. While the existing MGT detectors achieve strong\nperformance, they remain vulnerable to simple perturbations and adversarial\nattacks. To build an effective defense against malicious perturbations, we view\nMGT detection from a threat modeling perspective, that is, analyzing the\nmodel's vulnerability from an adversary's point of view and exploring effective\nmitigations. To this end, we introduce an adversarial framework for training a\nrobust MGT detector, named GREedy Adversary PromoTed DefendER (GREATER). The\nGREATER consists of two key components: an adversary GREATER-A and a detector\nGREATER-D. The GREATER-D learns to defend against the adversarial attack from\nGREATER-A and generalizes the defense to other attacks. GREATER-A identifies\nand perturbs the critical tokens in embedding space, along with greedy search\nand pruning to generate stealthy and disruptive adversarial examples. Besides,\nwe update the GREATER-A and GREATER-D synchronously, encouraging the GREATER-D\nto generalize its defense to different attacks and varying attack intensities.\nOur experimental results across 10 text perturbation strategies and 6\nadversarial attacks show that our GREATER-D reduces the Attack Success Rate\n(ASR) by 0.67% compared with SOTA defense methods while our GREATER-A is\ndemonstrated to be more effective and efficient than SOTA attack approaches.\nCodes and dataset are available in https://github.com/Liyuuuu111/GREATER."}
{"id": "2502.16949", "pdf": "https://arxiv.org/pdf/2502.16949.pdf", "abs": "https://arxiv.org/abs/2502.16949", "title": "SparseTransX: Efficient Training of Translation-Based Knowledge Graph Embeddings Using Sparse Matrix Operations", "authors": ["Md Saidul Hoque Anik", "Ariful Azad"], "categories": ["cs.LG", "cs.CL"], "comment": "15 pages, 9 figures, 9 tables. MLSys 2025", "summary": "Knowledge graph (KG) learning offers a powerful framework for generating new\nknowledge and making inferences. Training KG embedding can take a significantly\nlong time, especially for larger datasets. Our analysis shows that the gradient\ncomputation of embedding is one of the dominant functions in the\ntranslation-based KG embedding training loop. We address this issue by\nreplacing the core embedding computation with SpMM (Sparse-Dense Matrix\nMultiplication) kernels. This allows us to unify multiple scatter (and gather)\noperations as a single operation, reducing training time and memory usage. We\ncreate a general framework for training KG models using sparse kernels and\nimplement four models, namely TransE, TransR, TransH, and TorusE. Our sparse\nimplementations exhibit up to 5.3x speedup on the CPU and up to 4.2x speedup on\nthe GPU with a significantly low GPU memory footprint. The speedups are\nconsistent across large and small datasets for a given model. Our proposed\nsparse approach can be extended to accelerate other translation-based (such as\nTransC, TransM, etc.) and non-translational (such as DistMult, ComplEx, RotatE,\netc.) models as well. An implementation of the SpTransX framework is publicly\navailable as a Python package in https://github.com/HipGraph/SpTransX."}
{"id": "2502.19407", "pdf": "https://arxiv.org/pdf/2502.19407.pdf", "abs": "https://arxiv.org/abs/2502.19407", "title": "Learning Code-Edit Embedding to Model Student Debugging Behavior", "authors": ["Hasnain Heickal", "Andrew Lan"], "categories": ["cs.SE", "cs.CL"], "comment": "Published on the 26th International Conference on Artificial\n  Intelligence in Education (AIED 2025)", "summary": "Providing effective feedback for programming assignments in computer science\neducation can be challenging: students solve problems by iteratively submitting\ncode, executing it, and using limited feedback from the compiler or the\nauto-grader to debug. Analyzing student debugging behavior in this process may\nreveal important insights into their knowledge and inform better personalized\nsupport tools. In this work, we propose an encoder-decoder-based model that\nlearns meaningful code-edit embeddings between consecutive student code\nsubmissions, to capture their debugging behavior. Our model leverages\ninformation on whether a student code submission passes each test case to\nfine-tune large language models (LLMs) to learn code editing representations.\nIt enables personalized next-step code suggestions that maintain the student's\ncoding style while improving test case correctness. Our model also enables us\nto analyze student code-editing patterns to uncover common student errors and\ndebugging behaviors, using clustering techniques. Experimental results on a\nreal-world student code submission dataset demonstrate that our model excels at\ncode reconstruction and personalized code suggestion while revealing\ninteresting patterns in student debugging behavior."}
{"id": "2503.01453", "pdf": "https://arxiv.org/pdf/2503.01453.pdf", "abs": "https://arxiv.org/abs/2503.01453", "title": "AC-Lite : A Lightweight Image Captioning Model for Low-Resource Assamese Language", "authors": ["Pankaj Choudhury", "Yogesh Aggarwal", "Prabhanjan Jadhav", "Prithwijit Guha", "Sukumar Nandi"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Most existing works in image caption synthesis use computation heavy deep\nneural networks and generates image descriptions in English language. This\noften restricts this important assistive tool for widespread use across\nlanguage and accessibility barriers. This work presents AC-Lite, a\ncomputationally efficient model for image captioning in low-resource Assamese\nlanguage. AC-Lite reduces computational requirements by replacing\ncomputation-heavy deep network components with lightweight alternatives. The\nAC-Lite model is designed through extensive ablation experiments with different\nimage feature extractor networks and language decoders. A combination of\nShuffleNetv2x1.5 with GRU based language decoder along with bilinear attention\nis found to provide the best performance with minimum compute. AC-Lite was\nobserved to achieve an 82.3 CIDEr score on the COCO-AC dataset with 2.45 GFLOPs\nand 22.87M parameters."}
{"id": "2504.02009", "pdf": "https://arxiv.org/pdf/2504.02009.pdf", "abs": "https://arxiv.org/abs/2504.02009", "title": "Urban Computing in the Era of Large Language Models", "authors": ["Zhonghang Li", "Lianghao Xia", "Xubin Ren", "Jiabin Tang", "Tianyi Chen", "Yong Xu", "Chao Huang"], "categories": ["cs.CY", "cs.CL"], "comment": "https://github.com/HKUDS/Awesome-LLM4Urban-Papers", "summary": "Urban computing has emerged as a multidisciplinary field that harnesses\ndata-driven technologies to address challenges and improve urban living.\nTraditional approaches, while beneficial, often face challenges with\ngeneralization, scalability, and contextual understanding. The advent of Large\nLanguage Models (LLMs) offers transformative potential in this domain. This\nsurvey explores the intersection of LLMs and urban computing, emphasizing the\nimpact of LLMs in processing and analyzing urban data, enhancing\ndecision-making, and fostering citizen engagement. We provide a concise\noverview of the evolution and core technologies of LLMs. Additionally, we\nsurvey their applications across key urban domains, such as transportation,\npublic safety, and environmental monitoring, summarizing essential tasks and\nprior works in various urban contexts, while highlighting LLMs' functional\nroles and implementation patterns. Building on this, we propose potential\nLLM-based solutions to address unresolved challenges. To facilitate in-depth\nresearch, we compile a list of available datasets and tools applicable to\ndiverse urban scenarios. Finally, we discuss the limitations of current\napproaches and outline future directions for advancing LLMs in urban computing."}
{"id": "2504.09689", "pdf": "https://arxiv.org/pdf/2504.09689.pdf", "abs": "https://arxiv.org/abs/2504.09689", "title": "EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental Health Safety", "authors": ["Jiahao Qiu", "Yinghui He", "Xinzhe Juan", "Yimin Wang", "Yuhan Liu", "Zixin Yao", "Yue Wu", "Xun Jiang", "Ling Yang", "Mengdi Wang"], "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC", "cs.LG"], "comment": "18 pages, 8 figures", "summary": "The rise of LLM-driven AI characters raises safety concerns, particularly for\nvulnerable human users with psychological disorders. To address these risks, we\npropose EmoAgent, a multi-agent AI framework designed to evaluate and mitigate\nmental health hazards in human-AI interactions. EmoAgent comprises two\ncomponents: EmoEval simulates virtual users, including those portraying\nmentally vulnerable individuals, to assess mental health changes before and\nafter interactions with AI characters. It uses clinically proven psychological\nand psychiatric assessment tools (PHQ-9, PDI, PANSS) to evaluate mental risks\ninduced by LLM. EmoGuard serves as an intermediary, monitoring users' mental\nstatus, predicting potential harm, and providing corrective feedback to\nmitigate risks. Experiments conducted in popular character-based chatbots show\nthat emotionally engaging dialogues can lead to psychological deterioration in\nvulnerable users, with mental state deterioration in more than 34.4% of the\nsimulations. EmoGuard significantly reduces these deterioration rates,\nunderscoring its role in ensuring safer AI-human interactions. Our code is\navailable at: https://github.com/1akaman/EmoAgent"}
{"id": "2504.13955", "pdf": "https://arxiv.org/pdf/2504.13955.pdf", "abs": "https://arxiv.org/abs/2504.13955", "title": "Thousand Voices of Trauma: A Large-Scale Synthetic Dataset for Modeling Prolonged Exposure Therapy Conversations", "authors": ["Suhas BN", "Dominik Mattioli", "Saeed Abdullah", "Rosa I. Arriaga", "Chris W. Wiese", "Andrew M. Sherrill"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC", "cs.LG", "68T50", "I.2.7; H.5.2"], "comment": "14 pages, 6 figures Updated Appendix with example model responses", "summary": "The advancement of AI systems for mental health support is hindered by\nlimited access to therapeutic conversation data, particularly for trauma\ntreatment. We present Thousand Voices of Trauma, a synthetic benchmark dataset\nof 3,000 therapy conversations based on Prolonged Exposure therapy protocols\nfor Post-traumatic Stress Disorder (PTSD). The dataset comprises 500 unique\ncases, each explored through six conversational perspectives that mirror the\nprogression of therapy from initial anxiety to peak distress to emotional\nprocessing. We incorporated diverse demographic profiles (ages 18-80, M=49.3,\n49.4% male, 44.4% female, 6.2% non-binary), 20 trauma types, and 10\ntrauma-related behaviors using deterministic and probabilistic generation\nmethods. Analysis reveals realistic distributions of trauma types (witnessing\nviolence 10.6%, bullying 10.2%) and symptoms (nightmares 23.4%, substance abuse\n20.8%). Clinical experts validated the dataset's therapeutic fidelity,\nhighlighting its emotional depth while suggesting refinements for greater\nauthenticity. We also developed an emotional trajectory benchmark with\nstandardized metrics for evaluating model responses. This privacy-preserving\ndataset addresses critical gaps in trauma-focused mental health data, offering\na valuable resource for advancing both patient-facing applications and\nclinician training tools."}
