{"id": "2504.17018", "pdf": "https://arxiv.org/pdf/2504.17018", "abs": "https://arxiv.org/abs/2504.17018", "authors": ["Prashant Chandrasekar", "Mariel Couvillion", "Ayshwarya Saktheeswaran", "Jessica Zeitz"], "title": "LLM impact on BLV programming", "categories": ["cs.HC"], "comment": "Submitted to ASSETS 2025", "summary": "Large Language Models (LLMs) are rapidly becoming integral to a wide range of\ntools, tasks, and problem-solving processes, especially in software\ndevelopment. Originally designed for natural language processing tasks such as\ntext generation, LLMs are increasingly being used to assist both professionals\nand students in writing code. This growing reliance on LLM-based tools is\nreshaping programming workflows and task execution. In this study, we explore\nthe impact of these technologies on blind and low-vision (BLV) developers. Our\nreview of existing literature indicates that while LLMs help mitigate some of\nthe challenges faced by BLV programmers, they also introduce new forms of\ninaccessibility. We conducted an evaluation of five popular LLM-powered\nintegrated development environments (IDEs), assessing their performance across\na comprehensive set of programming tasks. Our findings highlight several\nunsupported scenarios, instances of incorrect model output, and notable\nlimitations in interaction support for specific tasks. Through observing BLV\ndevelopers as they engaged in coding activities, we uncovered key interaction\nbarriers that go beyond model accuracy or code generation quality. This paper\noutlines the challenges and corresponding opportunities for improving\naccessibility in the context of generative AI-assisted programming. Addressing\nthese issues can meaningfully enhance the programming experience for BLV\ndevelopers. As the generative AI revolution continues to unfold, it must also\naddress the unique burdens faced by this community.", "AI": {"tldr": "This study analyzes the impact of Large Language Models (LLMs) on blind and low-vision developers, highlighting both benefits and new accessibility challenges.", "motivation": "To explore how the integration of LLMs in software development affects blind and low-vision (BLV) developers, acknowledging both potential benefits and new accessibility challenges introduced by these technologies.", "method": "A review of existing literature was conducted alongside an evaluation of five popular LLM-powered integrated development environments (IDEs), assessing their performance on various programming tasks and observing interactions of BLV developers.", "result": "The evaluation revealed unsupported scenarios, instances of incorrect model output, and significant limitations in interaction support, emphasizing key interaction barriers faced by BLV developers beyond model accuracy.", "conclusion": "The findings underline the need to address accessibility challenges while harnessing LLM technologies, suggesting that improving accessibility can significantly enhance the programming experience for BLV developers as generative AI continues to evolve."}}
{"id": "2504.17023", "pdf": "https://arxiv.org/pdf/2504.17023", "abs": "https://arxiv.org/abs/2504.17023", "authors": ["Felix Kares", "Timo Speith", "Hanwei Zhang", "Markus Langer"], "title": "What Makes for a Good Saliency Map? Comparing Strategies for Evaluating Saliency Maps in Explainable AI (XAI)", "categories": ["cs.HC", "cs.AI"], "comment": "27 pages, 7 figures, 4 tables", "summary": "Saliency maps are a popular approach for explaining classifications of\n(convolutional) neural networks. However, it remains an open question as to how\nbest to evaluate salience maps, with three families of evaluation methods\ncommonly being used: subjective user measures, objective user measures, and\nmathematical metrics. We examine three of the most popular saliency map\napproaches (viz., LIME, Grad-CAM, and Guided Backpropagation) in a between\nsubject study (N=166) across these families of evaluation methods. We test 1)\nfor subjective measures, if the maps differ with respect to user trust and\nsatisfaction; 2) for objective measures, if the maps increase users' abilities\nand thus understanding of a model; 3) for mathematical metrics, which map\nachieves the best ratings across metrics; and 4) whether the mathematical\nmetrics can be associated with objective user measures. To our knowledge, our\nstudy is the first to compare several salience maps across all these evaluation\nmethods$-$with the finding that they do not agree in their assessment (i.e.,\nthere was no difference concerning trust and satisfaction, Grad-CAM improved\nusers' abilities best, and Guided Backpropagation had the most favorable\nmathematical metrics). Additionally, we show that some mathematical metrics\nwere associated with user understanding, although this relationship was often\ncounterintuitive. We discuss these findings in light of general debates\nconcerning the complementary use of user studies and mathematical metrics in\nthe evaluation of explainable AI (XAI) approaches.", "AI": {"tldr": "The paper evaluates three saliency map approaches (LIME, Grad-CAM, Guided Backpropagation) using subjective, objective, and mathematical evaluation methods to assess their effectiveness in explaining neural network classifications.", "motivation": "To address the lack of consensus on how to effectively evaluate saliency maps used for explaining the classifications of neural networks.", "method": "A between-subject study with 166 participants was conducted to examine subjective user measures (trust and satisfaction), objective user measures (ability and understanding), and mathematical metrics for three saliency map methods.", "result": "The study found that evaluations across methods do not agree; Grad-CAM improved user abilities the most, Guided Backpropagation had the best mathematical metrics, and there was no significant difference in user trust and satisfaction among the maps.", "conclusion": "The findings highlight the complexity in evaluating explainable AI approaches and suggest that both user studies and mathematical metrics should be considered complementary in understanding saliency maps."}}
{"id": "2504.17055", "pdf": "https://arxiv.org/pdf/2504.17055", "abs": "https://arxiv.org/abs/2504.17055", "authors": ["Ayushi Agrawal", "Aditya Kondai", "Kavita Vemuri"], "title": "Psychological Effect of AI driven marketing tools for beauty/facial feature enhancement", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "AI-powered facial assessment tools are reshaping how individuals evaluate\nappearance and internalize social judgments. This study examines the\npsychological impact of such tools on self-objectification, self-esteem, and\nemotional responses, with attention to gender differences. Two samples used\ndistinct versions of a facial analysis tool: one overtly critical (N=75; M=22.9\nyears), and another more neutral (N=51; M=19.9 years). Participants completed\nvalidated self-objectification and self-esteem scales and custom items\nmeasuring emotion, digital/physical appearance enhancement (DAE, PAEE), and\nperceived social emotion (PSE). Results revealed consistent links between high\nself-objectification, low self-esteem, and increased appearance enhancement\nbehaviors across both versions. Despite softer framing, the newer tool still\nevoked negative emotional responses (U=1466.5, p=0.013), indicating implicit\nfeedback may reinforce appearance-related insecurities. Gender differences\nemerged in DAE (p=0.025) and PSE (p<0.001), with females more prone to digital\nenhancement and less likely to perceive emotional impact in others. These\nfindings reveal how AI tools may unintentionally reinforce and amplify existing\nsocial biases and underscore the critical need for responsible AI design and\ndevelopment. Future research will investigate how human ideologies embedded in\nthe training data of such tools shape their evaluative outputs, and how these,\nin turn, influence user attitudes and decisions.", "AI": {"tldr": "AI-powered facial assessment tools impact self-objectification, self-esteem, and emotional responses differently based on design and gender.", "motivation": "To investigate the psychological effects of AI-powered facial assessment tools on individuals, focusing on self-objectification and self-esteem, and examining gender differences.", "method": "Participants used one of two versions of a facial analysis tool (overtly critical or neutral) and completed validated scales measuring self-objectification, self-esteem, and emotions, along with custom items related to appearance enhancement and perceived social emotions.", "result": "Findings indicated high self-objectification correlating with low self-esteem and increased appearance enhancement behaviors in both tool versions, while the neutral tool still triggered negative emotions. Gender differences were noted, with females more likely to engage in digital enhancement and less aware of social emotional impacts.", "conclusion": "The study highlights the need for responsible AI design to mitigate reinforcement of social biases stemming from these tools and suggests directions for future research on the influence of training data on AI outputs."}}
{"id": "2504.17150", "pdf": "https://arxiv.org/pdf/2504.17150", "abs": "https://arxiv.org/abs/2504.17150", "authors": ["Naimul Hoque", "Nicole Sultanum"], "title": "DashGuide: Authoring Interactive Dashboard Tours for Guiding Dashboard Users", "categories": ["cs.HC"], "comment": null, "summary": "Dashboard guidance helps dashboard users better navigate interactive\nfeatures, understand the underlying data, and assess insights they can\npotentially extract from dashboards. However, authoring dashboard guidance is a\ntime consuming task, and embedding guidance into dashboards for effective\ndelivery is difficult to realize. In this work, we contribute DashGuide, a\nframework and system to support the creation of interactive dashboard guidance\nwith minimal authoring input. Given a dashboard and a communication goal,\nDashGuide captures a sequence of author-performed interactions to generate\nguidance materials delivered as playable step-by-step overlays, a.k.a.,\ndashboard tours. Authors can further edit and refine individual tour steps\nwhile receiving generative assistance. We also contribute findings from a\nformative assessment with 9 dashboard creators, which helped inform the design\nof DashGuide; and findings from an evaluation of DashGuide with 12 dashboard\ncreators, suggesting it provides an improved authoring experience that balances\nefficiency, expressiveness, and creative freedom.", "AI": {"tldr": "DashGuide is a framework that simplifies the creation of interactive guidance for dashboards, enabling quicker and more effective authoring of guidance materials.", "motivation": "Dashboard guidance enhances user navigation and understanding of data insights but is often difficult and time-consuming to create and implement effectively.", "method": "DashGuide captures user interactions on a dashboard to generate guidance materials as step-by-step overlays, allowing authors to edit and refine these tours with generative support.", "result": "Evaluation with 12 dashboard creators indicated that DashGuide improves the authoring experience by effectively balancing efficiency, expressiveness, and creative freedom.", "conclusion": "DashGuide streamlines the process of creating dashboard guidance, enhancing usability without compromising authoring quality."}}
{"id": "2504.16956", "pdf": "https://arxiv.org/pdf/2504.16956", "abs": "https://arxiv.org/abs/2504.16956", "authors": ["Cong Qi", "Hanzhang Fang", "Tianxing Hu", "Siqi Jiang", "Wei Zhi"], "title": "Bidirectional Mamba for Single-Cell Data: Efficient Context Learning with Biological Fidelity", "categories": ["cs.CL", "cs.LG", "q-bio.GN"], "comment": null, "summary": "Single-cell RNA sequencing (scRNA-seq) enables high-resolution analysis of\ncellular heterogeneity, but its complexity, which is marked by high\ndimensionality, sparsity, and batch effects, which poses major computational\nchallenges. Transformer-based models have made significant advances in this\ndomain but are often limited by their quadratic complexity and suboptimal\nhandling of long-range dependencies. In this work, we introduce GeneMamba, a\nscalable and efficient foundation model for single-cell transcriptomics built\non state space modeling. Leveraging the Bi-Mamba architecture, GeneMamba\ncaptures bidirectional gene context with linear-time complexity, offering\nsubstantial computational gains over transformer baselines. The model is\npretrained on nearly 30 million cells and incorporates biologically informed\nobjectives, including pathway-aware contrastive loss and rank-based gene\nencoding. We evaluate GeneMamba across diverse tasks, including multi-batch\nintegration, cell type annotation, and gene-gene correlation, demonstrating\nstrong performance, interpretability, and robustness. These results position\nGeneMamba as a practical and powerful alternative to transformer-based methods,\nadvancing the development of biologically grounded, scalable tools for\nlarge-scale single-cell data analysis.", "AI": {"tldr": "GeneMamba is a scalable and efficient model for single-cell RNA sequencing that improves upon transformer models by capturing bidirectional gene context with linear complexity.", "motivation": "To address the computational challenges posed by high dimensionality, sparsity, and batch effects in single-cell RNA sequencing data analysis.", "method": "GeneMamba utilizes a state space modeling approach with the Bi-Mamba architecture, which enables linear-time complexity and incorporates biologically informed pretraining strategies.", "result": "GeneMamba shows strong performance in multiple tasks such as multi-batch integration, cell type annotation, and gene-gene correlation, while being interpretable and robust.", "conclusion": "GeneMamba serves as a practical alternative to transformer-based models, advancing tools for analyzing large-scale single-cell transcriptomics."}}
{"id": "2504.17170", "pdf": "https://arxiv.org/pdf/2504.17170", "abs": "https://arxiv.org/abs/2504.17170", "authors": ["Robert Kaufman"], "title": "Improving Human-Autonomous Vehicle Interaction in Complex Systems", "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": "PhD Dissertation from University of California, San Diego; 175 pages", "summary": "Unresolved questions about how autonomous vehicles (AVs) should meet the\ninformational needs of riders hinder real-world adoption. Complicating our\nability to satisfy rider needs is that different people, goals, and driving\ncontexts have different criteria for what constitutes interaction success.\nUnfortunately, most human-AV research and design today treats all people and\nsituations uniformly. It is crucial to understand how an AV should communicate\nto meet rider needs, and how communications should change when the human-AV\ncomplex system changes. I argue that understanding the relationships between\ndifferent aspects of the human-AV system can help us build improved and\nadaptable AV communications. I support this argument using three empirical\nstudies. First, I identify optimal communication strategies that enhance\ndriving performance, confidence, and trust for learning in extreme driving\nenvironments. Findings highlight the need for task-sensitive,\nmodality-appropriate communications tuned to learner cognitive limits and\ngoals. Next, I highlight the consequences of deploying faulty communication\nsystems and demonstrate the need for context-sensitive communications. Third, I\nuse machine learning (ML) to illuminate personal factors predicting trust in\nAVs, emphasizing the importance of tailoring designs to individual traits and\nconcerns. Together, this dissertation supports the necessity of transparent,\nadaptable, and personalized AV systems that cater to individual needs, goals,\nand contextual demands. By considering the complex system within which human-AV\ninteractions occur, we can deliver valuable insights for designers,\nresearchers, and policymakers. This dissertation also provides a concrete\ndomain to study theories of human-machine joint action and situational\nawareness, and can be used to guide future human-AI interaction research.\n[shortened for arxiv]", "AI": {"tldr": "The paper highlights the need for tailored communication in autonomous vehicles (AVs) to meet diverse rider needs and improves real-world adoption.", "motivation": "Unresolved questions about AV communication hinder adoption as different riders have varied informational needs based on individual traits, goals, and driving contexts.", "method": "The author uses three empirical studies to identify optimal communication strategies, analyze the consequences of faulty systems, and employ machine learning to predict trust in AVs based on personal factors.", "result": "The studies reveal that effective AV communication should be task-sensitive and modality-appropriate, and that individualized communication strategies enhance trust and performance in AV settings.", "conclusion": "The dissertation advocates for adaptable and personalized AV communication systems, providing insights for designers and researchers to enhance human-AV interactions and guide future human-AI research."}}
{"id": "2504.16977", "pdf": "https://arxiv.org/pdf/2504.16977", "abs": "https://arxiv.org/abs/2504.16977", "authors": ["Priyaranjan Pattnayak", "Hitesh Laxmichand Patel", "Amit Agarwal"], "title": "Tokenization Matters: Improving Zero-Shot NER for Indic Languages", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Tokenization is a critical component of Natural Language Processing (NLP),\nespecially for low resource languages, where subword segmentation influences\nvocabulary structure and downstream task accuracy. Although Byte Pair Encoding\n(BPE) is a standard tokenization method in multilingual language models, its\nsuitability for Named Entity Recognition (NER) in low resource Indic languages\nremains underexplored due to its limitations in handling morphological\ncomplexity. In this work, we systematically compare BPE, SentencePiece, and\nCharacter Level tokenization strategies using IndicBERT for NER tasks in low\nresource Indic languages like Assamese, Bengali, Marathi, and Odia, as well as\nextremely low resource Indic languages like Santali, Manipuri, and Sindhi. We\nassess both intrinsic linguistic properties tokenization efficiency, out of\nvocabulary (OOV) rates, and morphological preservation as well as extrinsic\ndownstream performance, including fine tuning and zero shot cross lingual\ntransfer.\n  Our experiments show that SentencePiece is a consistently better performing\napproach than BPE for NER in low resource Indic Languages, particularly in zero\nshot cross lingual settings, as it better preserves entity consistency. While\nBPE provides the most compact tokenization form, it is not capable of\ngeneralization because it misclassifies or even fails to recognize entity\nlabels when tested on unseen languages. In contrast, SentencePiece constitutes\na better linguistic structural preservation model, benefiting extremely low\nresource and morphologically rich Indic languages, such as Santali and\nManipuri, for superior entity recognition, as well as high generalization\nacross scripts, such as Sindhi, written in Arabic. The results point to\nSentencePiece as the more effective tokenization strategy for NER within\nmultilingual and low resource Indic NLP applications.", "AI": {"tldr": "SentencePiece outperforms Byte Pair Encoding (BPE) for Named Entity Recognition (NER) in low resource Indic languages, showing better entity consistency and generalization.", "motivation": "To evaluate tokenization methods for Named Entity Recognition in low resource Indic languages and address the limitations of BPE due to morphological complexity.", "method": "A systematic comparison of BPE, SentencePiece, and Character Level tokenization strategies using IndicBERT on NER tasks across various Indic languages.", "result": "SentencePiece consistently outperforms BPE in NER tasks, particularly in zero shot cross-lingual settings, while BPE struggles with entity misclassification.", "conclusion": "SentencePiece is the more effective tokenization strategy for NER in multilingual and low resource Indic NLP applications, benefiting both morphological preservation and generalization."}}
{"id": "2504.17171", "pdf": "https://arxiv.org/pdf/2504.17171", "abs": "https://arxiv.org/abs/2504.17171", "authors": ["Sunday David Ubur"], "title": "Augmenting Captions with Emotional Cues: An AR Interface for Real-Time Accessible Communication", "categories": ["cs.HC"], "comment": null, "summary": "This paper introduces an augmented reality (AR) captioning framework designed\nto support Deaf and Hard of Hearing (DHH) learners in STEM classrooms by\nintegrating non-verbal emotional cues into live transcriptions. Unlike\nconventional captioning systems that offer only plain text, our system fuses\nreal-time speech recognition with affective and visual signal interpretation,\nincluding facial movements, gestures, and vocal tone, to produce emotionally\nenriched captions. These enhanced captions are rendered in an AR interface\ndeveloped with Unity and provide contextual annotations such as speaker tone\nmarkers (e.g., \"concerned\") and gesture indicators (e.g., \"nods\"). The system\nleverages live camera and microphone input, processed through AI models to\ndetect multimodal cues. Findings from preliminary evaluations suggest that this\nAR-based captioning approach significantly enhances comprehension and reduces\ncognitive effort compared to standard captions. Our work emphasizes the\npotential of immersive environments for inclusive, emotion-aware educational\naccessibility.", "AI": {"tldr": "The paper presents an AR framework for enhancing captioning for Deaf and Hard of Hearing learners by integrating emotional cues into live transcriptions.", "motivation": "To improve accessibility and comprehension for Deaf and Hard of Hearing learners in STEM classrooms by providing more context-rich captioning options.", "method": "The framework combines real-time speech recognition with emotional and visual signals, including facial expressions and gestures, to create enriched captions rendered in an AR interface using Unity.", "result": "Preliminary evaluations indicate that the AR captioning approach significantly improves comprehension and reduces cognitive effort compared to traditional captioning methods.", "conclusion": "The study highlights the effectiveness of immersive technologies in providing inclusive and emotion-aware educational experiences for learners with hearing impairments."}}
{"id": "2504.17025", "pdf": "https://arxiv.org/pdf/2504.17025", "abs": "https://arxiv.org/abs/2504.17025", "authors": ["Luca Moroni", "Giovanni Puccetti", "Pere-Lluis Huguet Cabot", "Andrei Stefan Bejgu", "Edoardo Barba", "Alessio Miaschi", "Felice Dell'Orletta", "Andrea Esuli", "Roberto Navigli"], "title": "Optimizing LLMs for Italian: Reducing Token Fertility and Enhancing Efficiency Through Vocabulary Adaptation", "categories": ["cs.CL"], "comment": null, "summary": "The number of pretrained Large Language Models (LLMs) is increasing steadily,\nthough the majority are designed predominantly for the English language. While\nstate-of-the-art LLMs can handle other languages, due to language contamination\nor some degree of multilingual pretraining data, they are not optimized for\nnon-English languages, leading to inefficient encoding (high token \"fertility\")\nand slower inference speed. In this work, we thoroughly compare a variety of\nvocabulary adaptation techniques for optimizing English LLMs for the Italian\nlanguage, and put forward Semantic Alignment Vocabulary Adaptation (SAVA), a\nnovel method that leverages neural mapping for vocabulary substitution. SAVA\nachieves competitive performance across multiple downstream tasks, enhancing\ngrounded alignment strategies. We adapt two LLMs: Mistral-7b-v0.1, reducing\ntoken fertility by 25\\%, and Llama-3.1-8B, optimizing the vocabulary and\nreducing the number of parameters by 1 billion. We show that, following the\nadaptation of the vocabulary, these models can recover their performance with a\nrelatively limited stage of continual training on the target language. Finally,\nwe test the capabilities of the adapted models on various multi-choice and\ngenerative tasks.", "AI": {"tldr": "This paper introduces a method called Semantic Alignment Vocabulary Adaptation (SAVA) to optimize English Large Language Models for the Italian language, resulting in improved efficiency and performance across various tasks.", "motivation": "The increasing number of pretrained Large Language Models (LLMs) mostly cater to the English language, leading to inefficiencies when adapting them for non-English languages due to issues like language contamination and non-optimized multilingual pretraining data.", "method": "The authors compare various vocabulary adaptation techniques and propose SAVA, which employs neural mapping for vocabulary substitution to optimize the encoding of models for Italian.", "result": "SAVA achieves competitive performance and reduces token fertility by 25% for Mistral-7b-v0.1 and decreases the parameters by 1 billion for Llama-3.1-8B, with continued training recovering performance effectively on the target language.", "conclusion": "The adaptation of vocabulary significantly enhances the models’ performance on multilingual tasks, demonstrating the efficacy of SAVA for improving LLMs in non-English contexts."}}
{"id": "2504.17173", "pdf": "https://arxiv.org/pdf/2504.17173", "abs": "https://arxiv.org/abs/2504.17173", "authors": ["Tianyu Zhang", "Dongheng Zhang", "Ruixu Geng", "Xuecheng Xie", "Shuai Yang", "Yan Chen"], "title": "Lessons from Deploying Learning-based CSI Localization on a Large-Scale ISAC Platform", "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "In recent years, Channel State Information (CSI), recognized for its\nfine-grained spatial characteristics, has attracted increasing attention in\nWiFi-based indoor localization. However, despite its potential, CSI-based\napproaches have yet to achieve the same level of deployment scale and\ncommercialization as those based on Received Signal Strength Indicator (RSSI).\nA key limitation lies in the fact that most existing CSI-based systems are\ndeveloped and evaluated in controlled, small-scale environments, limiting their\ngeneralizability. To bridge this gap, we explore the deployment of a\nlarge-scale CSI-based localization system involving over 400 Access Points\n(APs) in a real-world building under the Integrated Sensing and Communication\n(ISAC) paradigm. We highlight two critical yet often overlooked factors: the\nunderutilization of unlabeled data and the inherent heterogeneity of CSI\nmeasurements. To address these challenges, we propose a novel CSI-based\nlearning framework for WiFi localization, tailored for large-scale ISAC\ndeployments on the server side. Specifically, we employ a novel graph-based\nstructure to model heterogeneous CSI data and reduce redundancy. We further\ndesign a pretext pretraining task that incorporates spatial and temporal priors\nto effectively leverage large-scale unlabeled CSI data. Complementarily, we\nintroduce a confidence-aware fine-tuning strategy to enhance the robustness of\nlocalization results. In a leave-one-smartphone-out experiment spanning five\nfloors and 25, 600 m2, we achieve a median localization error of 2.17 meters\nand a floor accuracy of 99.49%. This performance corresponds to an 18.7%\nreduction in mean absolute error (MAE) compared to the best-performing\nbaseline.", "AI": {"tldr": "This paper presents a large-scale CSI-based localization system using over 400 Access Points to improve indoor localization accuracy in a real-world setting.", "motivation": "The study aims to enhance the deployment and commercialization of CSI-based localization, which has been limited by previous small-scale evaluations and the underutilization of unlabeled data.", "method": "A novel graph-based learning framework is proposed, leveraging a pretext pretraining task for large-scale unlabeled CSI data and a confidence-aware fine-tuning strategy to improve robustness.", "result": "The system achieved a median localization error of 2.17 meters and a floor accuracy of 99.49%, showing an 18.7% reduction in mean absolute error compared to the best baseline.", "conclusion": "The proposed approach significantly improves CSI-based localization performance in large-scale real-world environments, addressing challenges such as data redundancy and the use of unlabeled data."}}
{"id": "2504.17052", "pdf": "https://arxiv.org/pdf/2504.17052", "abs": "https://arxiv.org/abs/2504.17052", "authors": ["Shariar Kabir", "Kevin Esterling", "Yue Dong"], "title": "Do Words Reflect Beliefs? Evaluating Belief Depth in Large Language Models", "categories": ["cs.CL"], "comment": "20 pages, 9 figures", "summary": "Large Language Models (LLMs) are increasingly shaping political discourse,\nyet their responses often display inconsistency when subjected to scrutiny.\nWhile prior research has primarily categorized LLM outputs as left- or\nright-leaning to assess their political stances, a critical question remains:\nDo these responses reflect genuine internal beliefs or merely surface-level\nalignment with training data? To address this, we propose a novel framework for\nevaluating belief depth by analyzing (1) argumentative consistency and (2)\nuncertainty quantification. We evaluate 12 LLMs on 19 economic policies from\nthe Political Compass Test, challenging their belief stability with both\nsupportive and opposing arguments. Our analysis reveals that LLMs exhibit\ntopic-specific belief stability rather than a uniform ideological stance.\nNotably, up to 95% of left-leaning models' responses and 89% of right-leaning\nmodels' responses remain consistent under the challenge, enabling semantic\nentropy to achieve high accuracy (AUROC=0.78), effectively distinguishing\nbetween surface-level alignment from genuine belief. These findings call into\nquestion the assumption that LLMs maintain stable, human-like political\nideologies, emphasizing the importance of conducting topic-specific reliability\nassessments for real-world applications.", "AI": {"tldr": "The paper evaluates the belief depth in Large Language Models (LLMs) regarding political discourse, revealing their topic-specific belief stability rather than a uniform ideological stance.", "motivation": "To investigate whether LLM responses represent genuine beliefs or just surface-level alignment with training data, particularly in political discourse.", "method": "A novel framework was proposed to evaluate belief depth through argumentative consistency and uncertainty quantification, applying it to 12 LLMs and 19 economic policies from the Political Compass Test.", "result": "The analysis showed that LLMs displayed consistent responses based on topic, with up to 95% of left-leaning and 89% of right-leaning models remaining stable under scrutiny; semantic entropy achieved high accuracy (AUROC=0.78) in distinguishing surface-level alignment from genuine belief.", "conclusion": "The findings challenge the notion that LLMs possess stable political ideologies, highlighting the need for topic-specific assessments of reliability in practical applications."}}
{"id": "2504.17204", "pdf": "https://arxiv.org/pdf/2504.17204", "abs": "https://arxiv.org/abs/2504.17204", "authors": ["Chitralekha Gupta", "Hanjun Wu", "Praveen Sasikumar", "Shreyas Sridhar", "Priambudi Bagaskara", "Suranga Nanayakkara"], "title": "Factually: Exploring Wearable Fact-Checking for Augmented Truth Discernment", "categories": ["cs.HC", "cs.ET"], "comment": "Presented at the 2025 ACM Workshop on Human-AI Interaction for\n  Augmented Reasoning, Report Number: CHI25-WS-AUGMENTED-REASONING", "summary": "Wearable devices are transforming human capabilities by seamlessly augmenting\ncognitive functions. In this position paper, we propose a voice-based,\ninteractive learning companion designed to amplify and extend cognitive\nabilities through informal learning. Our vision is threefold: (1) to enable\nusers to discover new knowledge on-the-go through contextual interactive\nquizzes, fostering critical thinking and mindfulness, (2) to proactively detect\nmisinformation, empowering users to critically assess information in real time,\nand (3) to provide spoken language correction and prompting hints for second\nlanguage learning and effective communication. As an initial step toward this\nvision, we present Factually - a proactive, wearable fact-checking system\nintegrated into devices like smartwatches or rings. Factually discreetly alerts\nusers to potential falsehoods via vibrotactile feedback, helping them assess\ninformation critically. We demonstrate its utility through three illustrative\nscenarios, highlighting its potential to extend cognitive abilities for\nreal-time misinformation detection. Early qualitative feedback suggests that\nFactually can enhance users' fact-checking capabilities, offering both\npractical and experiential benefits.", "AI": {"tldr": "This paper proposes a voice-based, interactive learning companion called Factually, aimed at enhancing cognitive functions through informal learning and real-time misinformation detection using wearable devices.", "motivation": "The goal is to amplify cognitive abilities and empower users to critically assess information, particularly in an era of rampant misinformation.", "method": "The authors introduce a wearable fact-checking system that provides discreet alerts to potential falsehoods through vibrotactile feedback, integrated with devices like smartwatches or rings.", "result": "Factually's utility is illustrated through three scenarios, and early feedback indicates its effectiveness in improving users' fact-checking skills.", "conclusion": "The study suggests that Factually could significantly extend cognitive abilities, providing both practical benefits in misinformation detection and enhancing critical thinking skills."}}
{"id": "2504.17075", "pdf": "https://arxiv.org/pdf/2504.17075", "abs": "https://arxiv.org/abs/2504.17075", "authors": ["Arjun Subramonian", "Vagrant Gautam", "Preethi Seshadri", "Dietrich Klakow", "Kai-Wei Chang", "Yizhou Sun"], "title": "Agree to Disagree? A Meta-Evaluation of LLM Misgendering", "categories": ["cs.CL", "cs.CY"], "comment": "Work in progress", "summary": "Numerous methods have been proposed to measure LLM misgendering, including\nprobability-based evaluations (e.g., automatically with templatic sentences)\nand generation-based evaluations (e.g., with automatic heuristics or human\nvalidation). However, it has gone unexamined whether these evaluation methods\nhave convergent validity, that is, whether their results align. Therefore, we\nconduct a systematic meta-evaluation of these methods across three existing\ndatasets for LLM misgendering. We propose a method to transform each dataset to\nenable parallel probability- and generation-based evaluation. Then, by\nautomatically evaluating a suite of 6 models from 3 families, we find that\nthese methods can disagree with each other at the instance, dataset, and model\nlevels, conflicting on 20.2% of evaluation instances. Finally, with a human\nevaluation of 2400 LLM generations, we show that misgendering behaviour is\ncomplex and goes far beyond pronouns, which automatic evaluations are not\ncurrently designed to capture, suggesting essential disagreement with human\nevaluations. Based on our findings, we provide recommendations for future\nevaluations of LLM misgendering. Our results are also more widely relevant, as\nthey call into question broader methodological conventions in LLM evaluation,\nwhich often assume that different evaluation methods agree.", "AI": {"tldr": "The paper systematically evaluates various methods for measuring LLM misgendering and finds significant disagreements among them, highlighting the need for improved evaluation practices.", "motivation": "The study investigates whether existing methods for evaluating LLM misgendering yield consistent results, addressing a gap in the understanding of the validity of these methods.", "method": "A systematic meta-evaluation of existing probability- and generation-based evaluation methods across three datasets for LLM misgendering, including transformations for parallel evaluations and human validation.", "result": "The evaluation revealed that the different methods disagree on 20.2% of instances, indicating substantial variation at the instance, dataset, and model levels, supplemented by human evaluation showing complexity in misgendering that surpasses pronoun issues.", "conclusion": "The findings question the reliability of current LLM evaluation methodologies and recommend improvements for future evaluations to enhance the accuracy of assessing LLM misgendering."}}
{"id": "2504.17267", "pdf": "https://arxiv.org/pdf/2504.17267", "abs": "https://arxiv.org/abs/2504.17267", "authors": ["Chuer Chen", "Shengqi Dang", "Yuqi Liu", "Nanxuan Zhao", "Yang Shi", "Nan Cao"], "title": "MV-Crafter: An Intelligent System for Music-guided Video Generation", "categories": ["cs.HC", "cs.MM"], "comment": null, "summary": "Music videos, as a prevalent form of multimedia entertainment, deliver\nengaging audio-visual experiences to audiences and have gained immense\npopularity among singers and fans. Creators can express their interpretations\nof music naturally through visual elements. However, the creation process of\nmusic video demands proficiency in script design, video shooting, and\nmusic-video synchronization, posing significant challenges for\nnon-professionals. Previous work has designed automated music video generation\nframeworks. However, they suffer from complexity in input and poor output\nquality. In response, we present MV-Crafter, a system capable of producing\nhigh-quality music videos with synchronized music-video rhythm and style. Our\napproach involves three technical modules that simulate the human creation\nprocess: the script generation module, video generation module, and music-video\nsynchronization module. MV-Crafter leverages a large language model to generate\nscripts considering the musical semantics. To address the challenge of\nsynchronizing short video clips with music of varying lengths, we propose a\ndynamic beat matching algorithm and visual envelope-induced warping method to\nensure precise, monotonic music-video synchronization. Besides, we design a\nuser-friendly interface to simplify the creation process with intuitive editing\nfeatures. Extensive experiments have demonstrated that MV-Crafter provides an\neffective solution for improving the quality of generated music videos.", "AI": {"tldr": "MV-Crafter is a system that automates high-quality music video generation, facilitating an easier creation process for non-professionals.", "motivation": "The creation of music videos poses challenges for non-professionals due to the need for skills in script design, video shooting, and synchronization.", "method": "MV-Crafter employs three modules: script generation using a large language model, video generation, and a dynamic beat matching algorithm for synchronization.", "result": "Experiments show that MV-Crafter produces high-quality music videos with improved synchronization and intuitive creation features.", "conclusion": "MV-Crafter effectively enhances the music video creation process, making it accessible for users without professional skills."}}
{"id": "2504.17083", "pdf": "https://arxiv.org/pdf/2504.17083", "abs": "https://arxiv.org/abs/2504.17083", "authors": ["Rendi Chevi", "Kentaro Inui", "Thamar Solorio", "Alham Fikri Aji"], "title": "How Individual Traits and Language Styles Shape Preferences In Open-ended User-LLM Interaction: A Preliminary Study", "categories": ["cs.CL"], "comment": "Accepted at GenAICHI 2025 @ ACM CHI 2025", "summary": "What makes an interaction with the LLM more preferable for the user? While it\nis intuitive to assume that information accuracy in the LLM's responses would\nbe one of the influential variables, recent studies have found that inaccurate\nLLM's responses could still be preferable when they are perceived to be more\nauthoritative, certain, well-articulated, or simply verbose. These variables\ninterestingly fall under the broader category of language style, implying that\nthe style in the LLM's responses might meaningfully influence users'\npreferences. This hypothesized dynamic could have double-edged consequences:\nenhancing the overall user experience while simultaneously increasing their\nsusceptibility to risks such as LLM's misinformation or hallucinations. In this\nshort paper, we present our preliminary studies in exploring this subject.\nThrough a series of exploratory and experimental user studies, we found that\nLLM's language style does indeed influence user's preferences, but how and\nwhich language styles influence the preference varied across different user\npopulations, and more interestingly, moderated by the user's very own\nindividual traits. As a preliminary work, the findings in our studies should be\ninterpreted with caution, particularly given the limitations in our samples,\nwhich still need wider demographic diversity and larger sample sizes. Our\nfuture directions will first aim to address these limitations, which would\nenable a more comprehensive joint effect analysis between the language style,\nindividual traits, and preferences, and further investigate the potential\ncausal relationship between and beyond these variables.", "AI": {"tldr": "The paper investigates how the language style of LLM responses affects user preferences, revealing that style influences user opinions but varies by individual traits.", "motivation": "To understand the factors that make interactions with LLMs preferable for users, exploring the impact of language style alongside accuracy.", "method": "Preliminary exploratory and experimental user studies were conducted to analyze the influence of LLM's language style on user preferences across different populations.", "result": "Language style significantly influences user preferences, but the effect varies among different user populations and is moderated by individual traits.", "conclusion": "The findings suggest that while language style can enhance user experience, it may also increase the risk of misinformation; further research is needed with larger and more diverse samples."}}
{"id": "2504.17331", "pdf": "https://arxiv.org/pdf/2504.17331", "abs": "https://arxiv.org/abs/2504.17331", "authors": ["Süleyman Özdel", "Kadir Burak Buldu", "Enkelejda Kasneci", "Efe Bozkir"], "title": "Exploring Context-aware and LLM-driven Locomotion for Immersive Virtual Reality", "categories": ["cs.HC", "cs.AI"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Locomotion plays a crucial role in shaping the user experience within virtual\nreality environments. In particular, hands-free locomotion offers a valuable\nalternative by supporting accessibility and freeing users from reliance on\nhandheld controllers. To this end, traditional speech-based methods often\ndepend on rigid command sets, limiting the naturalness and flexibility of\ninteraction. In this study, we propose a novel locomotion technique powered by\nlarge language models (LLMs), which allows users to navigate virtual\nenvironments using natural language with contextual awareness. We evaluate\nthree locomotion methods: controller-based teleportation, voice-based steering,\nand our language model-driven approach. Our evaluation measures include\neye-tracking data analysis, including explainable machine learning through SHAP\nanalysis as well as standardized questionnaires for usability, presence,\ncybersickness, and cognitive load to examine user attention and engagement. Our\nfindings indicate that the LLM-driven locomotion possesses comparable\nusability, presence, and cybersickness scores to established methods like\nteleportation, demonstrating its novel potential as a comfortable, natural\nlanguage-based, hands-free alternative. In addition, it enhances user attention\nwithin the virtual environment, suggesting greater engagement. Complementary to\nthese findings, SHAP analysis revealed that fixation, saccade, and\npupil-related features vary across techniques, indicating distinct patterns of\nvisual attention and cognitive processing. Overall, we state that our method\ncan facilitate hands-free locomotion in virtual spaces, especially in\nsupporting accessibility.", "AI": {"tldr": "A novel hands-free locomotion technique for virtual reality, utilizing large language models, shows promise in natural language navigation while maintaining usability and enhancing user attention.", "motivation": "To improve user experience in virtual reality by offering a speech-based locomotion alternative that is flexible and accessible, moving away from rigid command sets of traditional methods.", "method": "The study evaluates three locomotion methods: controller-based teleportation, voice-based steering, and a language model-driven approach, using eye-tracking analysis, SHAP analysis, and standardized questionnaires for various user engagement metrics.", "result": "The LLM-driven locomotion method demonstrates comparable usability, presence, and cybersickness scores to teleportation, enhances user attention, and shows distinct patterns in visual attention and cognitive processing through SHAP analysis.", "conclusion": "The proposed method provides a comfortable, natural language-driven, hands-free locomotion alternative in virtual environments, particularly benefitting accessibility."}}
{"id": "2504.17091", "pdf": "https://arxiv.org/pdf/2504.17091", "abs": "https://arxiv.org/abs/2504.17091", "authors": ["Seunghyun Yoo"], "title": "Co-CoT: A Prompt-Based Framework for Collaborative Chain-of-Thought Reasoning", "categories": ["cs.CL", "68T05"], "comment": "5 page", "summary": "Due to the proliferation of short-form content and the rapid adoption of AI,\nopportunities for deep, reflective thinking have significantly diminished,\nundermining users' critical thinking and reducing engagement with the reasoning\nbehind AI-generated outputs. To address this issue, we propose an Interactive\nChain-of-Thought (CoT) Framework that enhances human-centered explainability\nand responsible AI usage by making the model's inference process transparent,\nmodular, and user-editable. The framework decomposes reasoning into clearly\ndefined blocks that users can inspect, modify, and re-execute, encouraging\nactive cognitive engagement rather than passive consumption. It further\nintegrates a lightweight edit-adaptation mechanism inspired by preference\nlearning, allowing the system to align with diverse cognitive styles and user\nintentions. Ethical transparency is ensured through explicit metadata\ndisclosure, built-in bias checkpoint functionality, and privacy-preserving\nsafeguards. This work outlines the design principles and architecture necessary\nto promote critical engagement, responsible interaction, and inclusive\nadaptation in AI systems aimed at addressing complex societal challenges.", "AI": {"tldr": "This paper presents an Interactive Chain-of-Thought Framework to enhance human-centered explainability and responsible AI usage by enabling transparent, modular, and editable reasoning processes.", "motivation": "The rise of short-form content and AI has led to a decline in deep thinking and critical engagement with AI outputs, necessitating solutions to foster better cognitive engagement.", "method": "The framework decomposes reasoning into inspectable and editable blocks, incorporates a lightweight edit-adaptation mechanism for user preferences, and ensures ethical transparency through metadata, bias checks, and privacy safeguards.", "result": "The proposed framework encourages active cognitive engagement and responsible interactions with AI, accommodating diverse user cognitive styles and fostering critical thinking.", "conclusion": "The design principles and architecture outlined support the responsible adaptation of AI systems to address complex societal issues, promoting critical engagement."}}
{"id": "2504.17334", "pdf": "https://arxiv.org/pdf/2504.17334", "abs": "https://arxiv.org/abs/2504.17334", "authors": ["Chuer Chen", "Yuqi Liu", "Danqing Shi", "Shixiong Cao", "Nan Cao"], "title": "DataScout: Automatic Data Fact Retrieval for Statement Augmentation with an LLM-Based Agent", "categories": ["cs.HC", "cs.IR"], "comment": null, "summary": "A data story typically integrates data facts from multiple perspectives and\nstances to construct a comprehensive and objective narrative. However,\nretrieving these facts demands time for data search and challenges the\ncreator's analytical skills. In this work, we introduce DataScout, an\ninteractive system that automatically performs reasoning and stance-based data\nfacts retrieval to augment the user's statement. Particularly, DataScout\nleverages an LLM-based agent to construct a retrieval tree, enabling\ncollaborative control of its expansion between users and the agent. The\ninterface visualizes the retrieval tree as a mind map that eases users to\nintuitively steer the retrieval direction and effectively engage in reasoning\nand analysis. We evaluate the proposed system through case studies and in-depth\nexpert interviews. Our evaluation demonstrates that DataScout can effectively\nretrieve multifaceted data facts from different stances, helping users verify\ntheir statements and enhance the credibility of their stories.", "AI": {"tldr": "DataScout is an interactive system that automates stance-based data fact retrieval to enhance users' data storytelling capabilities.", "motivation": "The need for efficient retrieval of multi-perspective data facts to construct comprehensive and objective narratives in data storytelling.", "method": "DataScout employs an LLM-based agent to create a retrieval tree, allowing collaborative expansion between users and the agent, while visualizing this as a mind map for intuitive navigation.", "result": "Case studies and expert interviews show that DataScout effectively retrieves multifaceted data facts, aiding users in verifying their statements and improving narrative credibility.", "conclusion": "DataScout significantly enhances the data storytelling process by streamlining data fact retrieval and supporting users in their analytical efforts."}}
{"id": "2504.17119", "pdf": "https://arxiv.org/pdf/2504.17119", "abs": "https://arxiv.org/abs/2504.17119", "authors": ["Muskan Garg", "Shaina Raza", "Shebuti Rayana", "Xingyi Liu", "Sunghwan Sohn"], "title": "The Rise of Small Language Models in Healthcare: A Comprehensive Survey", "categories": ["cs.CL", "cs.AI"], "comment": "35 pages, 7 tables, 5 figures", "summary": "Despite substantial progress in healthcare applications driven by large\nlanguage models (LLMs), growing concerns around data privacy, and limited\nresources; the small language models (SLMs) offer a scalable and clinically\nviable solution for efficient performance in resource-constrained environments\nfor next-generation healthcare informatics. Our comprehensive survey presents a\ntaxonomic framework to identify and categorize them for healthcare\nprofessionals and informaticians. The timeline of healthcare SLM contributions\nestablishes a foundational framework for analyzing models across three\ndimensions: NLP tasks, stakeholder roles, and the continuum of care. We present\na taxonomic framework to identify the architectural foundations for building\nmodels from scratch; adapting SLMs to clinical precision through prompting,\ninstruction fine-tuning, and reasoning; and accessibility and sustainability\nthrough compression techniques. Our primary objective is to offer a\ncomprehensive survey for healthcare professionals, introducing recent\ninnovations in model optimization and equipping them with curated resources to\nsupport future research and development in the field. Aiming to showcase the\ngroundbreaking advancements in SLMs for healthcare, we present a comprehensive\ncompilation of experimental results across widely studied NLP tasks in\nhealthcare to highlight the transformative potential of SLMs in healthcare. The\nupdated repository is available at Github", "AI": {"tldr": "The paper surveys small language models (SLMs) as a scalable solution for healthcare informatics, presenting a taxonomic framework for their application and optimization.", "motivation": "The motivation is to address concerns about data privacy and resource limitations in healthcare by exploring the potential of small language models as an efficient alternative to large language models.", "method": "The authors provide a taxonomic framework for categorizing SLMs in healthcare, analyzing models across dimensions such as NLP tasks, stakeholder roles, and care continuity while also discussing architectural foundations and optimization techniques.", "result": "The survey compiles experimental results demonstrating the effectiveness of SLMs across various healthcare-related NLP tasks, thus showcasing their transformative potential in the field.", "conclusion": "The paper concludes that SLMs present a promising approach for enhancing healthcare informatics, emphasizing the need for further research and development supported by their findings and curated resources."}}
{"id": "2504.17352", "pdf": "https://arxiv.org/pdf/2504.17352", "abs": "https://arxiv.org/abs/2504.17352", "authors": ["Anton Andreev", "Grégoire Cattan", "Marco Congedo"], "title": "The Riemannian Means Field Classifier for EEG-Based BCI Data", "categories": ["cs.HC", "eess.SP"], "comment": null, "summary": "A substantial amount of research has demonstrated the robustness and accuracy\nof the Riemannian minimum distance to mean (MDM) classifier for all kinds of\nEEG-based brain--computer interfaces (BCIs). This classifier is simple, fully\ndeterministic, robust to noise, computationally efficient, and prone to\ntransfer learning. Its training is very simple, requiring just the computation\nof a geometric mean of a symmetric positive-definite (SPD) matrix per class. We\npropose an improvement of the MDM involving a number of power means of SPD\nmatrices instead of the sole geometric mean. By the analysis of 20 public\ndatabases, 10 for the motor-imagery BCI paradigm and 10 for the P300 BCI\nparadigm, comprising 587 individuals in total, we show that the proposed\nclassifier clearly outperforms the MDM, approaching the state-of-the art in\nterms of performance while retaining the simplicity and the deterministic\nbehavior. In order to promote reproducible research, our code will be released\nas open source.", "AI": {"tldr": "The paper improves the Riemannian minimum distance to mean classifier for EEG-based BCIs by using power means of SPD matrices, resulting in better performance.", "motivation": "To enhance the performance of the robust and efficient MDM classifier used in EEG-based BCIs.", "method": "An improved MDM classifier using multiple power means of symmetric positive-definite matrices is proposed and evaluated on 20 public databases.", "result": "The proposed classifier outperforms the traditional MDM classifier in performance across 20 public databases with a total of 587 individuals.", "conclusion": "The new classifier approaches state-of-the-art performance while maintaining simplicity and deterministic characteristics, with open-source code to facilitate reproducible research."}}
{"id": "2504.17130", "pdf": "https://arxiv.org/pdf/2504.17130", "abs": "https://arxiv.org/abs/2504.17130", "authors": ["Hannah Cyberey", "David Evans"], "title": "Steering the CensorShip: Uncovering Representation Vectors for LLM \"Thought\" Control", "categories": ["cs.CL", "cs.CR", "cs.CY"], "comment": null, "summary": "Large language models (LLMs) have transformed the way we access information.\nThese models are often tuned to refuse to comply with requests that are\nconsidered harmful and to produce responses that better align with the\npreferences of those who control the models. To understand how this\n\"censorship\" works. We use representation engineering techniques to study\nopen-weights safety-tuned models. We present a method for finding a\nrefusal--compliance vector that detects and controls the level of censorship in\nmodel outputs. We also analyze recent reasoning LLMs, distilled from\nDeepSeek-R1, and uncover an additional dimension of censorship through \"thought\nsuppression\". We show a similar approach can be used to find a vector that\nsuppresses the model's reasoning process, allowing us to remove censorship by\napplying the negative multiples of this vector", "AI": {"tldr": "This paper investigates the mechanisms of censorship in large language models (LLMs) through the identification of vectors that control refusal and compliance in model outputs.", "motivation": "The motivation stems from the need to understand how LLMs are tuned to restrict certain responses and how this impacts access to information.", "method": "The authors employ representation engineering techniques and introduce a method for identifying a refusal-compliance vector that quantifies the level of censorship, while also analyzing reasoning LLMs for additional forms of censorship related to thought suppression.", "result": "The study finds a refusal-compliance vector that effectively detects censorship levels and identifies a vector that can suppress the reasoning process, enabling the removal of certain censorship by applying negative multiples of it.", "conclusion": "The findings highlight the complex dynamics of censorship in LLMs and provide a framework for manipulating model behavior to explore the breadth of their generative capabilities."}}
{"id": "2504.17663", "pdf": "https://arxiv.org/pdf/2504.17663", "abs": "https://arxiv.org/abs/2504.17663", "authors": ["Michelle L. Ding", "Harini Suresh"], "title": "The Malicious Technical Ecosystem: Exposing Limitations in Technical Governance of AI-Generated Non-Consensual Intimate Images of Adults", "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.LG"], "comment": null, "summary": "In this paper, we adopt a survivor-centered approach to locate and dissect\nthe role of sociotechnical AI governance in preventing AI-Generated\nNon-Consensual Intimate Images (AIG-NCII) of adults, colloquially known as\n\"deep fake pornography.\" We identify a \"malicious technical ecosystem\" or\n\"MTE,\" comprising of open-source face-swapping models and nearly 200\n\"nudifying\" software programs that allow non-technical users to create AIG-NCII\nwithin minutes. Then, using the National Institute of Standards and Technology\n(NIST) AI 100-4 report as a reflection of current synthetic content governance\nmethods, we show how the current landscape of practices fails to effectively\nregulate the MTE for adult AIG-NCII, as well as flawed assumptions explaining\nthese gaps.", "AI": {"tldr": "The paper examines the sociotechnical AI governance related to AI-Generated Non-Consensual Intimate Images (AIG-NCII), highlighting shortcomings in current regulatory practices.", "motivation": "To address the growing issue of AIG-NCII and understand the role of AI governance in preventing its proliferation.", "method": "Adopting a survivor-centered approach, the authors analyze a 'malicious technical ecosystem' of tools facilitating the creation of AIG-NCII and evaluate the effectiveness of existing governance frameworks.", "result": "The paper identifies a significant failure in current governance practices to regulate the malicious technical ecosystem for adult AIG-NCII, driven by flawed assumptions and inadequate safeguards.", "conclusion": "Enhanced governance strategies are needed to effectively address the challenges posed by AIG-NCII and mitigate its impact."}}
{"id": "2504.17137", "pdf": "https://arxiv.org/pdf/2504.17137", "abs": "https://arxiv.org/abs/2504.17137", "authors": ["Chanhee Park", "Hyeonseok Moon", "Chanjun Park", "Heuiseok Lim"], "title": "MIRAGE: A Metric-Intensive Benchmark for Retrieval-Augmented Generation Evaluation", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to NAACL2025 Findings", "summary": "Retrieval-Augmented Generation (RAG) has gained prominence as an effective\nmethod for enhancing the generative capabilities of Large Language Models\n(LLMs) through the incorporation of external knowledge. However, the evaluation\nof RAG systems remains a challenge, due to the intricate interplay between\nretrieval and generation components. This limitation has resulted in a scarcity\nof benchmarks that facilitate a detailed, component-specific assessment. In\nthis work, we present MIRAGE, a Question Answering dataset specifically\ndesigned for RAG evaluation. MIRAGE consists of 7,560 curated instances mapped\nto a retrieval pool of 37,800 entries, enabling an efficient and precise\nevaluation of both retrieval and generation tasks. We also introduce novel\nevaluation metrics aimed at measuring RAG adaptability, encompassing dimensions\nsuch as noise vulnerability, context acceptability, context insensitivity, and\ncontext misinterpretation. Through comprehensive experiments across various\nretriever-LLM configurations, we provide new insights into the optimal\nalignment of model pairs and the nuanced dynamics within RAG systems. The\ndataset and evaluation code are publicly available, allowing for seamless\nintegration and customization in diverse research settings\\footnote{The MIRAGE\ncode and data are available at https://github.com/nlpai-lab/MIRAGE.", "AI": {"tldr": "MIRAGE is a new dataset and evaluation framework designed for assessing Retrieval-Augmented Generation (RAG) systems, comprising 7,560 instances for detailed evaluation of retrieval and generation tasks.", "motivation": "To address the challenge of evaluating RAG systems, which lack comprehensive benchmarks for the intricate interaction between retrieval and generation components.", "method": "MIRAGE includes a dataset of 7,560 curated instances linked to a 37,800 entry retrieval pool and introduces novel evaluation metrics that assess various adaptability dimensions of RAG systems.", "result": "Comprehensive experiments reveal insights into optimal model configurations and the dynamics of retrieval and generation within RAG systems.", "conclusion": "MIRAGE provides a robust framework for assessing RAG systems and is publicly available for research integration and customization."}}
{"id": "2504.17677", "pdf": "https://arxiv.org/pdf/2504.17677", "abs": "https://arxiv.org/abs/2504.17677", "authors": ["Jarne Thys", "Sebe Vanbrabant", "Davy Vanacken", "Gustavo Rovelo Ruiz"], "title": "INSIGHT: Bridging the Student-Teacher Gap in Times of Large Language Models", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "The rise of AI, especially Large Language Models, presents challenges and\nopportunities to integrate such technology into the classroom. AI has the\npotential to revolutionize education by helping teaching staff with various\ntasks, such as personalizing their teaching methods, but it also raises\nconcerns, for example, about the degradation of student-teacher interactions\nand user privacy. This paper introduces INSIGHT, a proof of concept to combine\nvarious AI tools to assist teaching staff and students in the process of\nsolving exercises. INSIGHT has a modular design that allows it to be integrated\ninto various higher education courses. We analyze students' questions to an LLM\nby extracting keywords, which we use to dynamically build an FAQ from students'\nquestions and provide new insights for the teaching staff to use for more\npersonalized face-to-face support. Future work could build upon INSIGHT by\nusing the collected data to provide adaptive learning and adjust content based\non student progress and learning styles to offer a more interactive and\ninclusive learning experience.", "AI": {"tldr": "The paper presents INSIGHT, an AI tool designed to assist teaching staff and students in solving exercises, while addressing the challenges and opportunities AI brings to education.", "motivation": "The integration of AI, especially Large Language Models, into education offers potential benefits like personalized teaching methods, but also raises concerns about student-teacher interactions and privacy.", "method": "INSIGHT utilizes a modular design to analyze students' questions by extracting keywords, which helps build a dynamic FAQ and provides insights for personalized support from teaching staff.", "result": "INSIGHT effectively assists teaching staff by providing valuable insights derived from student inquiries, enhancing the personalized support offered during face-to-face interactions.", "conclusion": "Future developments of INSIGHT could lead to adaptive learning experiences by utilizing collected data to tailor content according to student progress and learning styles."}}
{"id": "2504.17192", "pdf": "https://arxiv.org/pdf/2504.17192", "abs": "https://arxiv.org/abs/2504.17192", "authors": ["Minju Seo", "Jinheon Baek", "Seongyun Lee", "Sung Ju Hwang"], "title": "Paper2Code: Automating Code Generation from Scientific Papers in Machine Learning", "categories": ["cs.CL"], "comment": null, "summary": "Despite the rapid growth of machine learning research, corresponding code\nimplementations are often unavailable, making it slow and labor-intensive for\nresearchers to reproduce results and build upon prior work. In the meantime,\nrecent Large Language Models (LLMs) excel at understanding scientific documents\nand generating high-quality code. Inspired by this, we introduce PaperCoder, a\nmulti-agent LLM framework that transforms machine learning papers into\nfunctional code repositories. PaperCoder operates in three stages: planning,\nwhere it constructs a high-level roadmap, designs the system architecture with\ndiagrams, identifies file dependencies, and generates configuration files;\nanalysis, which focuses on interpreting implementation-specific details; and\ngeneration, where modular, dependency-aware code is produced. Moreover, each\nphase is instantiated through a set of specialized agents designed to\ncollaborate effectively across the pipeline. We then evaluate PaperCoder on\ngenerating code implementations from machine learning papers based on both\nmodel-based and human evaluations, specifically from the original paper\nauthors, with author-released repositories as ground truth if available. Our\nresults demonstrate the effectiveness of PaperCoder in creating high-quality,\nfaithful implementations. Furthermore, it consistently shows strengths in the\nrecently released PaperBench benchmark, surpassing strong baselines by\nsubstantial margins.", "AI": {"tldr": "PaperCoder is a framework that utilizes multi-agent large language models to convert machine learning papers into functional code repositories, enhancing reproducibility in research.", "motivation": "The lack of available code implementations for machine learning research hinders replication and development, necessitating an efficient solution.", "method": "PaperCoder operates through three stages: planning (creating a high-level roadmap and identifying dependencies), analysis (interpreting implementation details), and generation (producing modular, dependency-aware code), using specialized agents at each phase.", "result": "Evaluation shows that PaperCoder produces high-quality code implementations that are faithful to the original papers, as validated by both model-based and human assessments, including comparisons with author-released repositories and strong performances on the PaperBench benchmark.", "conclusion": "PaperCoder effectively facilitates the generation of reproducible code from machine learning research papers, demonstrating significant advantages over existing baselines."}}
{"id": "2504.17697", "pdf": "https://arxiv.org/pdf/2504.17697", "abs": "https://arxiv.org/abs/2504.17697", "authors": ["Pratyay Suvarnapathaki", "Viral Shah", "Saarthak Negi", "Nimmi Rangaswamy"], "title": "'The Boring and the Tedious': Invisible Labour in India's Gig-Economy", "categories": ["cs.HC"], "comment": "6 pages, 2 figures", "summary": "India's gig-based food delivery platforms, such as Swiggy and Zomato, provide\ncrucial income to marginalised communities but also entrench workers in cycles\nof invisible labour. Through 14 semi-structured interviews, we analyse waiting\ntime and repetitive UI itneractions as key burdens that contribute to 'digital\ndiscomfort' for gig based food delivery agents. We find that workers employ\ncreative strategies to navigate algorithmic management, yet remain constrained\nby platform-side 'gamification' and system opacity. We propose worker-centered\nGUI automation as a potential intervention to reduce friction while preserving\nagency. In conclusion, this position paper argues for rethinking HCI approaches\nin the Global South to prioritise worker autonomy over efficiency-driven design\noptimisations.", "AI": {"tldr": "The paper analyzes the challenges faced by gig workers in India's food delivery sector, particularly focusing on digital discomfort caused by waiting times and repetitive user interfaces, and suggests a shift towards worker-centered design solutions.", "motivation": "To address the crucial income provided to marginalized communities by gig-based food delivery platforms while exposing the cycles of invisible labor and digital discomfort experienced by workers.", "method": "Conducted 14 semi-structured interviews with gig-based food delivery agents to explore their experiences, focusing on waiting times and user interface interactions.", "result": "Identified creative strategies used by workers to navigate algorithmic management, but highlighted ongoing constraints due to platform gamification and a lack of transparency in systems.", "conclusion": "Argues for a rethinking of HCI approaches in the Global South, emphasizing the need to prioritize worker autonomy over efficiency-focused design optimizations."}}
{"id": "2504.17200", "pdf": "https://arxiv.org/pdf/2504.17200", "abs": "https://arxiv.org/abs/2504.17200", "authors": ["Yangxinyu Xie", "Bowen Jiang", "Tanwi Mallick", "Joshua David Bergerson", "John K. Hutchison", "Duane R. Verner", "Jordan Branham", "M. Ross Alexander", "Robert B. Ross", "Yan Feng", "Leslie-Anne Levy", "Weijie Su", "Camillo J. Taylor"], "title": "A RAG-Based Multi-Agent LLM System for Natural Hazard Resilience and Adaptation", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are a transformational capability at the\nfrontier of artificial intelligence and machine learning that can support\ndecision-makers in addressing pressing societal challenges such as extreme\nnatural hazard events. As generalized models, LLMs often struggle to provide\ncontext-specific information, particularly in areas requiring specialized\nknowledge. In this work we propose a retrieval-augmented generation (RAG)-based\nmulti-agent LLM system to support analysis and decision-making in the context\nof natural hazards and extreme weather events. As a proof of concept, we\npresent WildfireGPT, a specialized system focused on wildfire hazards. The\narchitecture employs a user-centered, multi-agent design to deliver tailored\nrisk insights across diverse stakeholder groups. By integrating natural hazard\nand extreme weather projection data, observational datasets, and scientific\nliterature through an RAG framework, the system ensures both the accuracy and\ncontextual relevance of the information it provides. Evaluation across ten\nexpert-led case studies demonstrates that WildfireGPT significantly outperforms\nexisting LLM-based solutions for decision support.", "AI": {"tldr": "The paper proposes WildfireGPT, a retrieval-augmented generation-based multi-agent system that enhances decision-making for wildfire hazards using large language models.", "motivation": "To address the limitations of large language models in providing context-specific information for decision-making in natural hazards and extreme weather events.", "method": "The system integrates natural hazard projections, observational datasets, and scientific literature through a retrieval-augmented generation framework, employing a user-centered multi-agent design for tailored insights.", "result": "WildfireGPT outperforms existing LLM-based solutions in decision support, as validated by evaluations in ten expert-led case studies.", "conclusion": "The proposed RAG-based system effectively improves the accuracy and relevance of information for decision-makers facing natural hazards."}}
{"id": "2504.17705", "pdf": "https://arxiv.org/pdf/2504.17705", "abs": "https://arxiv.org/abs/2504.17705", "authors": ["Yong-Hao Hu", "Sotaro Yokoi", "Yuji Hatada", "Yuichi Hiroi", "Takuji Narumi", "Takefumi Hiraki"], "title": "LUIDA: Large-scale Unified Infrastructure for Digital Assessments based on Commercial Metaverse Platform", "categories": ["cs.HC"], "comment": null, "summary": "Online experiments using metaverse platforms have gained significant traction\nin Human-Computer Interaction and Virtual Reality (VR) research. However,\ncurrent research workflows are highly fragmented, as researchers must use\nseparate tools for system implementation, participant recruitment, experiment\nexecution, and data collection, reducing consistency and increasing workload.\nWe present LUIDA (Large-scale Unified Infrastructure for Digital Assessments),\na metaverse-based framework that integrates these fragmented processes. LUIDA\nautomatically allocates interconnected virtual environments for parallel\nexperiment execution and provides implementation templates adaptable to various\nVR research domains, requiring minimal metaverse development expertise. Our\nevaluation included two studies using a prototype built on Cluster, the\ncommercial metaverse platform. First, VR researchers using LUIDA to develop and\nrun experiments reported high usability scores (SUS: 73.75) and moderate\nworkload (NASA-TLX: 24.11) for overall usage, with interviews confirming\nstreamlined workflows compared to traditional laboratory experiments. Second,\nwe conducted three replicated experiments with public Cluster users, each\nrecruiting approximately 200 participants within one week. These experiments\nproduced results that closely matched the original studies, validating the\nexperimental integrity of LUIDA across research domains. After technical\nrefinements, we plan to release LUIDA as an open platform, providing a\nstandardized protocol to improve research efficiency and experimental\nreproducibility in VR studies.", "AI": {"tldr": "LUIDA is a metaverse-based framework designed to unify and streamline online experiment workflows in VR research, enhancing usability and efficiency.", "motivation": "The motivation for this paper is to address the fragmentation and inefficiency in current research workflows in Human-Computer Interaction and VR, where researchers face challenges using separate tools for various processes.", "method": "The authors present LUIDA, which automates the allocation of virtual environments for experiments and provides adaptable implementation templates, allowing researchers with minimal technical expertise to conduct studies.", "result": "The evaluation of LUIDA showed high usability scores (SUS: 73.75) and moderate workload (NASA-TLX: 24.11) for VR researchers, with interviews confirming improved workflows. Additionally, replicated experiments with public participants demonstrated consistent results with original studies.", "conclusion": "The paper concludes that LUIDA has the potential to enhance research efficiency and reproducibility in VR studies, with plans to release it as an open platform for wider use."}}
{"id": "2504.17220", "pdf": "https://arxiv.org/pdf/2504.17220", "abs": "https://arxiv.org/abs/2504.17220", "authors": ["Kaidong Feng", "Zhu Sun", "Jie Yang", "Hui Fang", "Xinghua Qu", "Wenyuan Liu"], "title": "Does Knowledge Distillation Matter for Large Language Model based Bundle Generation?", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "LLMs are increasingly explored for bundle generation, thanks to their\nreasoning capabilities and knowledge. However, deploying large-scale LLMs\nintroduces significant efficiency challenges, primarily high computational\ncosts during fine-tuning and inference due to their massive parameterization.\nKnowledge distillation (KD) offers a promising solution, transferring expertise\nfrom large teacher models to compact student models. This study systematically\ninvestigates knowledge distillation approaches for bundle generation, aiming to\nminimize computational demands while preserving performance. We explore three\ncritical research questions: (1) how does the format of KD impact bundle\ngeneration performance? (2) to what extent does the quantity of distilled\nknowledge influence performance? and (3) how do different ways of utilizing the\ndistilled knowledge affect performance? We propose a comprehensive KD framework\nthat (i) progressively extracts knowledge (patterns, rules, deep thoughts);\n(ii) captures varying quantities of distilled knowledge through different\nstrategies; and (iii) exploits complementary LLM adaptation techniques\n(in-context learning, supervised fine-tuning, combination) to leverage\ndistilled knowledge in small student models for domain-specific adaptation and\nenhanced efficiency. Extensive experiments provide valuable insights into how\nknowledge format, quantity, and utilization methodologies collectively shape\nLLM-based bundle generation performance, exhibiting KD's significant potential\nfor more efficient yet effective LLM-based bundle generation.", "AI": {"tldr": "This paper investigates knowledge distillation (KD) approaches to enhance the efficiency of large language models (LLMs) for bundle generation, addressing computational challenges while maintaining performance.", "motivation": "The increasing use of LLMs for bundle generation highlights efficiency issues related to their high computational costs during fine-tuning and inference, necessitating more effective deployment strategies.", "method": "The study explores various KD approaches by proposing a framework that gradually extracts knowledge, evaluates different quantities of distilled knowledge, and utilizes complementary adaptation techniques to apply this knowledge in compact models for better efficiency in bundle generation.", "result": "Extensive experiments reveal how the format, quantity, and application of distilled knowledge affect the performance of LLMs in bundle generation, demonstrating significant potential for improving efficiency through KD.", "conclusion": "The findings affirm that knowledge distillation can substantially enhance the performance of compact models for LLM-based bundle generation, making them more practical for real-world applications."}}
{"id": "2504.17238", "pdf": "https://arxiv.org/pdf/2504.17238", "abs": "https://arxiv.org/abs/2504.17238", "authors": ["Jinfeng Zhou", "Yuxuan Chen", "Jianing Yin", "Yongkang Huang", "Yihan Shi", "Xikun Zhang", "Libiao Peng", "Rongsheng Zhang", "Tangjie Lv", "Zhipeng Hu", "Hongning Wang", "Minlie Huang"], "title": "Crisp: Cognitive Restructuring of Negative Thoughts through Multi-turn Supportive Dialogues", "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Cognitive Restructuring (CR) is a psychotherapeutic process aimed at\nidentifying and restructuring an individual's negative thoughts, arising from\nmental health challenges, into more helpful and positive ones via multi-turn\ndialogues. Clinician shortage and stigma urge the development of human-LLM\ninteractive psychotherapy for CR. Yet, existing efforts implement CR via simple\ntext rewriting, fixed-pattern dialogues, or a one-shot CR workflow, failing to\nalign with the psychotherapeutic process for effective CR. To address this gap,\nwe propose CRDial, a novel framework for CR, which creates multi-turn dialogues\nwith specifically designed identification and restructuring stages of negative\nthoughts, integrates sentence-level supportive conversation strategies, and\nadopts a multi-channel loop mechanism to enable iterative CR. With CRDial, we\ndistill Crisp, a large-scale and high-quality bilingual dialogue dataset, from\nLLM. We then train Crispers, Crisp-based conversational LLMs for CR, at 7B and\n14B scales. Extensive human studies show the superiority of Crispers in\npointwise, pairwise, and intervention evaluations.", "AI": {"tldr": "CRDial is a novel framework for Cognitive Restructuring (CR) that enhances psychotherapeutic dialogue through multi-turn interactions and supportive strategies, utilizing a large-scale dataset to improve conversational LLMs for CR.", "motivation": "The paper addresses the clinician shortage and stigma surrounding mental health care by developing a human-LLM interactive psychotherapy framework for effective Cognitive Restructuring (CR).", "method": "CRDial implements multi-turn dialogues with tailored identification and restructuring stages of negative thoughts, integrates supportive conversation strategies, and employs a multi-channel loop mechanism for iterative CR. The framework is trained on Crisp, a high-quality bilingual dialogue dataset distilled from LLM.", "result": "Human studies demonstrate that Crispers, the Crisp-based conversational LLMs for CR, show superiority in pointwise, pairwise, and intervention evaluations compared to existing approaches.", "conclusion": "The proposed CRDial framework and the Crispers conversational models significantly enhance the effectiveness of Cognitive Restructuring in a psychotherapeutic context."}}
{"id": "2504.17252", "pdf": "https://arxiv.org/pdf/2504.17252", "abs": "https://arxiv.org/abs/2504.17252", "authors": ["Ocheme Anthony Ekle", "Biswarup Das"], "title": "Low-Resource Neural Machine Translation Using Recurrent Neural Networks and Transfer Learning: A Case Study on English-to-Igbo", "categories": ["cs.CL", "cs.LG", "68T50, 68T01", "I.2.7; I.2.1"], "comment": "25 pages, 14 combined figures (19 total), includes horizontal\n  layouts. Submitted to arXiv for open access", "summary": "In this study, we develop Neural Machine Translation (NMT) and\nTransformer-based transfer learning models for English-to-Igbo translation - a\nlow-resource African language spoken by over 40 million people across Nigeria\nand West Africa. Our models are trained on a curated and benchmarked dataset\ncompiled from Bible corpora, local news, Wikipedia articles, and Common Crawl,\nall verified by native language experts. We leverage Recurrent Neural Network\n(RNN) architectures, including Long Short-Term Memory (LSTM) and Gated\nRecurrent Units (GRU), enhanced with attention mechanisms to improve\ntranslation accuracy. To further enhance performance, we apply transfer\nlearning using MarianNMT pre-trained models within the SimpleTransformers\nframework. Our RNN-based system achieves competitive results, closely matching\nexisting English-Igbo benchmarks. With transfer learning, we observe a\nperformance gain of +4.83 BLEU points, reaching an estimated translation\naccuracy of 70%. These findings highlight the effectiveness of combining RNNs\nwith transfer learning to address the performance gap in low-resource language\ntranslation tasks.", "AI": {"tldr": "The study develops NMT and Transformer models for English-to-Igbo translation using curated datasets and RNN architectures, achieving significant performance improvements through transfer learning.", "motivation": "To address the challenge of low-resource language translation for English-to-Igbo, a language spoken by over 40 million people, and improve translation accuracy.", "method": "Neural Machine Translation and Transformer-based models were developed and trained on a mixed dataset, with RNN architectures including LSTM and GRU enhanced with attention mechanisms, and transfer learning implemented using MarianNMT pre-trained models.", "result": "The RNN-based system achieved competitive results that closely match existing benchmarks, and the application of transfer learning led to a performance gain of +4.83 BLEU points, reaching approximately 70% translation accuracy.", "conclusion": "Combining RNNs with transfer learning effectively addresses the performance gap in translations for low-resource languages."}}
{"id": "2504.17264", "pdf": "https://arxiv.org/pdf/2504.17264", "abs": "https://arxiv.org/abs/2504.17264", "authors": ["Zhaolu Kang", "Hongtian Cai", "Xiangyang Ji", "Jinzhe Li", "Nanfei Gu"], "title": "JurisCTC: Enhancing Legal Judgment Prediction via Cross-Domain Transfer and Contrastive Learning", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted in International Joint Conference on Neural Networks (IJCNN)\n  2025", "summary": "In recent years, Unsupervised Domain Adaptation (UDA) has gained significant\nattention in the field of Natural Language Processing (NLP) owing to its\nability to enhance model generalization across diverse domains. However, its\napplication for knowledge transfer between distinct legal domains remains\nlargely unexplored. To address the challenges posed by lengthy and complex\nlegal texts and the limited availability of large-scale annotated datasets, we\npropose JurisCTC, a novel model designed to improve the accuracy of Legal\nJudgment Prediction (LJP) tasks. Unlike existing approaches, JurisCTC\nfacilitates effective knowledge transfer across various legal domains and\nemploys contrastive learning to distinguish samples from different domains.\nSpecifically, for the LJP task, we enable knowledge transfer between civil and\ncriminal law domains. Compared to other models and specific large language\nmodels (LLMs), JurisCTC demonstrates notable advancements, achieving peak\naccuracies of 76.59% and 78.83%, respectively.", "AI": {"tldr": "JurisCTC is a novel model for improving Legal Judgment Prediction by facilitating knowledge transfer across legal domains using contrastive learning.", "motivation": "There is a lack of effective Unsupervised Domain Adaptation methods for knowledge transfer between distinct legal domains, especially given the complexity of legal texts and scarcity of annotated datasets.", "method": "JurisCTC uses contrastive learning to improve model performance on Legal Judgment Prediction tasks, specifically transferring knowledge between civil and criminal law.", "result": "JurisCTC achieves peak accuracies of 76.59% and 78.83%, outperforming existing models and specific large language models.", "conclusion": "The proposed JurisCTC model significantly enhances the accuracy of Legal Judgment Prediction tasks by leveraging effective knowledge transfer techniques."}}
{"id": "2504.17279", "pdf": "https://arxiv.org/pdf/2504.17279", "abs": "https://arxiv.org/abs/2504.17279", "authors": ["Xiuying Chen", "Tairan Wang", "Juexiao Zhou", "Zirui Song", "Xin Gao", "Xiangliang Zhang"], "title": "Evaluating and Mitigating Bias in AI-Based Medical Text Generation", "categories": ["cs.CL"], "comment": "12 pages, 8 figures, published in Nature Computational Science", "summary": "Artificial intelligence (AI) systems, particularly those based on deep\nlearning models, have increasingly achieved expert-level performance in medical\napplications. However, there is growing concern that such AI systems may\nreflect and amplify human bias, and reduce the quality of their performance in\nhistorically under-served populations. The fairness issue has attracted\nconsiderable research interest in the medical imaging classification field, yet\nit remains understudied in the text generation domain. In this study, we\ninvestigate the fairness problem in text generation within the medical field\nand observe significant performance discrepancies across different races,\nsexes, and age groups, including intersectional groups, various model scales,\nand different evaluation metrics. To mitigate this fairness issue, we propose\nan algorithm that selectively optimizes those underperformed groups to reduce\nbias. The selection rules take into account not only word-level accuracy but\nalso the pathology accuracy to the target reference, while ensuring that the\nentire process remains fully differentiable for effective model training. Our\nevaluations across multiple backbones, datasets, and modalities demonstrate\nthat our proposed algorithm enhances fairness in text generation without\ncompromising overall performance. Specifically, the disparities among various\ngroups across different metrics were diminished by more than 30% with our\nalgorithm, while the relative change in text generation accuracy was typically\nwithin 2%. By reducing the bias generated by deep learning models, our proposed\napproach can potentially alleviate concerns about the fairness and reliability\nof text generation diagnosis in medical domain.\n  Our code is publicly available to facilitate further research at\nhttps://github.com/iriscxy/GenFair.", "AI": {"tldr": "This study addresses fairness issues in AI text generation for medical applications, finding significant biases across demographics and proposing a selective optimization algorithm to reduce these biases while maintaining performance.", "motivation": "The paper investigates the fairness problem in AI text generation in the medical field, responding to concerns about biases affecting historically under-served populations and the performance of AI systems.", "method": "An algorithm is proposed that selectively optimizes underperformed demographic groups based on word-level accuracy and pathology accuracy, while ensuring the process is differentiable for model training.", "result": "The algorithm reduced performance disparities among various demographic groups by over 30%, with less than 2% relative change in text generation accuracy.", "conclusion": "The proposed approach enhances fairness in medical text generation without compromising performance, aiming to improve the reliability of AI diagnoses."}}
{"id": "2504.17309", "pdf": "https://arxiv.org/pdf/2504.17309", "abs": "https://arxiv.org/abs/2504.17309", "authors": ["Junyan Zhang", "Shuliang Liu", "Aiwei Liu", "Yubo Gao", "Jungang Li", "Xiaojie Gu", "Xuming Hu"], "title": "CoheMark: A Novel Sentence-Level Watermark for Enhanced Text Quality", "categories": ["cs.CL"], "comment": "Published at the 1st workshop on GenAI Watermarking, collocated with\n  ICLR 2025", "summary": "Watermarking technology is a method used to trace the usage of content\ngenerated by large language models. Sentence-level watermarking aids in\npreserving the semantic integrity within individual sentences while maintaining\ngreater robustness. However, many existing sentence-level watermarking\ntechniques depend on arbitrary segmentation or generation processes to embed\nwatermarks, which can limit the availability of appropriate sentences. This\nlimitation, in turn, compromises the quality of the generated response. To\naddress the challenge of balancing high text quality with robust watermark\ndetection, we propose CoheMark, an advanced sentence-level watermarking\ntechnique that exploits the cohesive relationships between sentences for better\nlogical fluency. The core methodology of CoheMark involves selecting sentences\nthrough trained fuzzy c-means clustering and applying specific next sentence\nselection criteria. Experimental evaluations demonstrate that CoheMark achieves\nstrong watermark strength while exerting minimal impact on text quality.", "AI": {"tldr": "CoheMark is a novel sentence-level watermarking technique that balances watermark robustness and text quality by leveraging cohesive sentence relationships.", "motivation": "Existing sentence-level watermarking methods often rely on arbitrary segmentation, limiting the selection of appropriate sentences and compromising the quality of generated responses.", "method": "CoheMark utilizes trained fuzzy c-means clustering to select sentences and applies specific criteria for next sentence selection, enhancing logical fluency in watermarking.", "result": "Experimental evaluations show that CoheMark achieves strong watermark strength with minimal impact on the quality of generated text.", "conclusion": "CoheMark effectively balances watermark robustness with high text quality, making it a significant advancement in watermarking technology."}}
{"id": "2504.17311", "pdf": "https://arxiv.org/pdf/2504.17311", "abs": "https://arxiv.org/abs/2504.17311", "authors": ["Yulia Otmakhova", "Hung Thinh Truong", "Rahmad Mahendra", "Zenan Zhai", "Rongxin Zhu", "Daniel Beck", "Jey Han Lau"], "title": "FLUKE: A Linguistically-Driven and Task-Agnostic Framework for Robustness Evaluation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present FLUKE (Framework for LingUistically-driven and tasK-agnostic\nrobustness Evaluation), a task-agnostic framework for assessing model\nrobustness through systematic minimal variations of test data. FLUKE introduces\ncontrolled variations across linguistic levels - from orthography to dialect\nand style varieties - and leverages large language models (LLMs) with human\nvalidation to generate modifications. We demonstrate FLUKE's utility by\nevaluating both fine-tuned models and LLMs across four diverse NLP tasks, and\nreveal that (1) the impact of linguistic variations is highly task-dependent,\nwith some tests being critical for certain tasks but irrelevant for others; (2)\nwhile LLMs have better overall robustness compared to fine-tuned models, they\nstill exhibit significant brittleness to certain linguistic variations; (3) all\nmodels show substantial vulnerability to negation modifications across most\ntasks. These findings highlight the importance of systematic robustness testing\nfor understanding model behaviors.", "AI": {"tldr": "FLUKE is a framework for evaluating model robustness through controlled linguistic variations in test data, highlighting task-dependent impacts on performance.", "motivation": "The need for a systematic evaluation tool to analyze model robustness against minimal variations in test data, specifically focusing on linguistic factors.", "method": "FLUKE introduces controlled variations across linguistic levels using large language models and human validation to generate modifications, and is applied to evaluate various NLP tasks.", "result": "The evaluation reveals that the impact of linguistic variations is task-dependent, LLMs are generally more robust but still vulnerable to specific variations, and all models struggle with negation modifications.", "conclusion": "Systematic robustness testing is essential for understanding model behaviors, particularly in response to linguistic variations."}}
{"id": "2504.17332", "pdf": "https://arxiv.org/pdf/2504.17332", "abs": "https://arxiv.org/abs/2504.17332", "authors": ["Zihan Wang", "Lu Yuan", "Zhengxuan Zhang", "Qing Zhao"], "title": "Bridging Cognition and Emotion: Empathy-Driven Multimodal Misinformation Detection", "categories": ["cs.CL"], "comment": null, "summary": "In the digital era, social media has become a major conduit for information\ndissemination, yet it also facilitates the rapid spread of misinformation.\nTraditional misinformation detection methods primarily focus on surface-level\nfeatures, overlooking the crucial roles of human empathy in the propagation\nprocess. To address this gap, we propose the Dual-Aspect Empathy Framework\n(DAE), which integrates cognitive and emotional empathy to analyze\nmisinformation from both the creator and reader perspectives. By examining\ncreators' cognitive strategies and emotional appeals, as well as simulating\nreaders' cognitive judgments and emotional responses using Large Language\nModels (LLMs), DAE offers a more comprehensive and human-centric approach to\nmisinformation detection. Moreover, we further introduce an empathy-aware\nfiltering mechanism to enhance response authenticity and diversity.\nExperimental results on benchmark datasets demonstrate that DAE outperforms\nexisting methods, providing a novel paradigm for multimodal misinformation\ndetection.", "AI": {"tldr": "The paper introduces the Dual-Aspect Empathy Framework (DAE) for detecting misinformation by incorporating human empathy factors that traditional methods overlook.", "motivation": "With the rise of social media as a primary information source, misinformation spreads rapidly, necessitating advanced detection methods that consider human empathy.", "method": "The DAE integrates cognitive and emotional empathy to analyze misinformation from both content creators' and readers' perspectives, utilizing Large Language Models to simulate responses and judgments.", "result": "Experimental results show that DAE outperforms traditional misinformation detection methods on benchmark datasets, demonstrating its effectiveness.", "conclusion": "DAE represents a novel, human-centric approach to multimodal misinformation detection that enhances response authenticity and diversity."}}
{"id": "2504.17353", "pdf": "https://arxiv.org/pdf/2504.17353", "abs": "https://arxiv.org/abs/2504.17353", "authors": ["Chengguang Gan", "Sunbowen Lee", "Zhixi Cai", "Yanbin Wei", "Lei Zheng", "Yunhao Liang", "Shiwen Ni", "Tatsunori Mori"], "title": "M-MRE: Extending the Mutual Reinforcement Effect to Multimodal Information Extraction", "categories": ["cs.CL", "cs.CV", "cs.MM"], "comment": null, "summary": "Mutual Reinforcement Effect (MRE) is an emerging subfield at the intersection\nof information extraction and model interpretability. MRE aims to leverage the\nmutual understanding between tasks of different granularities, enhancing the\nperformance of both coarse-grained and fine-grained tasks through joint\nmodeling. While MRE has been explored and validated in the textual domain, its\napplicability to visual and multimodal domains remains unexplored. In this\nwork, we extend MRE to the multimodal information extraction domain for the\nfirst time. Specifically, we introduce a new task: Multimodal Mutual\nReinforcement Effect (M-MRE), and construct a corresponding dataset to support\nthis task. To address the challenges posed by M-MRE, we further propose a\nPrompt Format Adapter (PFA) that is fully compatible with various Large\nVision-Language Models (LVLMs). Experimental results demonstrate that MRE can\nalso be observed in the M-MRE task, a multimodal text-image understanding\nscenario. This provides strong evidence that MRE facilitates mutual gains\nacross three interrelated tasks, confirming its generalizability beyond the\ntextual domain.", "AI": {"tldr": "The paper introduces the Multimodal Mutual Reinforcement Effect (M-MRE) and a new dataset, demonstrating the effectiveness of Mutual Reinforcement Effect (MRE) in enhancing joint modeling in multimodal information extraction tasks.", "motivation": "To explore the applicability of the Mutual Reinforcement Effect (MRE) in the visual and multimodal domains, extending its benefits demonstrated in the textual domain.", "method": "The authors introduce Multimodal Mutual Reinforcement Effect (M-MRE) as a new task and construct a corresponding dataset. They also propose a Prompt Format Adapter (PFA) for compatibility with various Large Vision-Language Models (LVLMs).", "result": "Experimental results validate that MRE can be observed in the M-MRE task, showcasing mutual gains in multimodal text-image understanding scenarios.", "conclusion": "The findings confirm the generalizability of MRE beyond the textual domain, indicating its potential benefits in multimodal contexts."}}
{"id": "2504.17360", "pdf": "https://arxiv.org/pdf/2504.17360", "abs": "https://arxiv.org/abs/2504.17360", "authors": ["Jose G. Moreno", "Jesus Lovon", "M'Rick Robin-Charlet", "Christine Damase-Michel", "Lynda Tamine"], "title": "PatientDx: Merging Large Language Models for Protecting Data-Privacy in Healthcare", "categories": ["cs.CL"], "comment": null, "summary": "Fine-tuning of Large Language Models (LLMs) has become the default practice\nfor improving model performance on a given task. However, performance\nimprovement comes at the cost of training on vast amounts of annotated data\nwhich could be sensitive leading to significant data privacy concerns. In\nparticular, the healthcare domain is one of the most sensitive domains exposed\nto data privacy issues. In this paper, we present PatientDx, a framework of\nmodel merging that allows the design of effective LLMs for health-predictive\ntasks without requiring fine-tuning nor adaptation on patient data. Our\nproposal is based on recently proposed techniques known as merging of LLMs and\naims to optimize a building block merging strategy. PatientDx uses a pivotal\nmodel adapted to numerical reasoning and tunes hyperparameters on examples\nbased on a performance metric but without training of the LLM on these data.\nExperiments using the mortality tasks of the MIMIC-IV dataset show improvements\nup to 7% in terms of AUROC when compared to initial models. Additionally, we\nconfirm that when compared to fine-tuned models, our proposal is less prone to\ndata leak problems without hurting performance. Finally, we qualitatively show\nthe capabilities of our proposal through a case study. Our best model is\npublicly available at https://huggingface.co/ Jgmorenof/mistral\\_merged\\_0\\_4.", "AI": {"tldr": "The paper presents PatientDx, a model merging framework that enhances healthcare predictive tasks without fine-tuning on sensitive patient data.", "motivation": "To address data privacy concerns in healthcare when fine-tuning large language models (LLMs) using sensitive annotated data.", "method": "PatientDx employs a merging strategy of LLMs with a pivotal model adapted for numerical reasoning, tuning hyperparameters based on performance metrics without direct training on patient data.", "result": "Experiments on the MIMIC-IV dataset showed up to a 7% improvement in AUROC over initial models, and PatientDx demonstrated a reduced risk of data leakage compared to fine-tuned models.", "conclusion": "PatientDx effectively balances performance enhancement and data privacy, making it a valuable approach for health-predictive tasks."}}
{"id": "2504.17366", "pdf": "https://arxiv.org/pdf/2504.17366", "abs": "https://arxiv.org/abs/2504.17366", "authors": ["Yongxuan Wu", "Runyu Chen", "Peiyu Liu", "Hongjin Qian"], "title": "LiveLongBench: Tackling Long-Context Understanding for Spoken Texts from Live Streams", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Long-context understanding poses significant challenges in natural language\nprocessing, particularly for real-world dialogues characterized by speech-based\nelements, high redundancy, and uneven information density. Although large\nlanguage models (LLMs) achieve impressive results on existing benchmarks, these\ndatasets fail to reflect the complexities of such texts, limiting their\napplicability to practical scenarios. To bridge this gap, we construct the\nfirst spoken long-text dataset, derived from live streams, designed to reflect\nthe redundancy-rich and conversational nature of real-world scenarios. We\nconstruct tasks in three categories: retrieval-dependent, reasoning-dependent,\nand hybrid. We then evaluate both popular LLMs and specialized methods to\nassess their ability to understand long-contexts in these tasks. Our results\nshow that current methods exhibit strong task-specific preferences and perform\npoorly on highly redundant inputs, with no single method consistently\noutperforming others. We propose a new baseline that better handles redundancy\nin spoken text and achieves strong performance across tasks. Our findings\nhighlight key limitations of current methods and suggest future directions for\nimproving long-context understanding. Finally, our benchmark fills a gap in\nevaluating long-context spoken language understanding and provides a practical\nfoundation for developing real-world e-commerce systems. The code and benchmark\nare available at https://github.com/Yarayx/livelongbench.", "AI": {"tldr": "This paper addresses the challenges of long-context understanding in natural language processing by introducing a new spoken long-text dataset and evaluating language models on it.", "motivation": "Current benchmarks fail to capture the complexities of real-world dialogues, limiting the applicability of large language models (LLMs) to practical scenarios involving speech-based, redundant, and unevenly dense information.", "method": "We constructed a spoken long-text dataset derived from live streams, featuring tasks in three categories: retrieval-dependent, reasoning-dependent, and hybrid. We evaluated various LLMs and specialized methods on these tasks to assess their long-context understanding.", "result": "Results reveal that existing methods struggle with redundancy in inputs and favor specific tasks, with no single method consistently outperforming others. The proposed new baseline performs better in handling redundancy.", "conclusion": "The study highlights limitations of current approaches to long-context understanding and provides a new benchmark that aids evaluation and development of spoken language understanding applicable to real-world e-commerce systems."}}
{"id": "2504.17390", "pdf": "https://arxiv.org/pdf/2504.17390", "abs": "https://arxiv.org/abs/2504.17390", "authors": ["Jihyun Lee", "Yejin Jeon", "Seungyeon Seo", "Gary Geunbae Lee"], "title": "PicPersona-TOD : A Dataset for Personalizing Utterance Style in Task-Oriented Dialogue with Image Persona", "categories": ["cs.CL"], "comment": "Accepted in NAACL 2025 main", "summary": "Task-Oriented Dialogue (TOD) systems are designed to fulfill user requests\nthrough natural language interactions, yet existing systems often produce\ngeneric, monotonic responses that lack individuality and fail to adapt to\nusers' personal attributes. To address this, we introduce PicPersona-TOD, a\nnovel dataset that incorporates user images as part of the persona, enabling\npersonalized responses tailored to user-specific factors such as age or\nemotional context. This is facilitated by first impressions, dialogue\npolicy-guided prompting, and the use of external knowledge to reduce\nhallucinations. Human evaluations confirm that our dataset enhances user\nexperience, with personalized responses contributing to a more engaging\ninteraction. Additionally, we introduce a new NLG model, Pictor, which not only\npersonalizes responses, but also demonstrates robust performance across unseen\ndomains https://github.com/JihyunLee1/PicPersona.", "AI": {"tldr": "Introduction of PicPersona-TOD, a dataset that enhances personalization in task-oriented dialogue systems using user images for tailored responses.", "motivation": "Existing TOD systems often produce generic responses that do not reflect user individuality or adapt to personal attributes, impacting user satisfaction.", "method": "The study introduces the PicPersona-TOD dataset, utilizing user images and incorporating factors like age and emotional context, along with a new NLG model called Pictor for personalized dialogue generation.", "result": "Human evaluations show that personalized responses from the PicPersona dataset significantly improve user experience and engagement in dialogues, while Pictor shows strong performance across various domains.", "conclusion": "The use of user images and personalized dialogue mechanisms in PicPersona-TOD leads to enhanced interactions in TOD systems, paving the way for more engaging and effective communication."}}
{"id": "2504.17445", "pdf": "https://arxiv.org/pdf/2504.17445", "abs": "https://arxiv.org/abs/2504.17445", "authors": ["Anna Lieb", "Maneesh Arora", "Eni Mustafaraj"], "title": "Creating Targeted, Interpretable Topic Models with LLM-Generated Text Augmentation", "categories": ["cs.CL"], "comment": "Presented at IC2S2 2024 in Philadelphia, USA", "summary": "Unsupervised machine learning techniques, such as topic modeling and\nclustering, are often used to identify latent patterns in unstructured text\ndata in fields such as political science and sociology. These methods overcome\ncommon concerns about reproducibility and costliness involved in the\nlabor-intensive process of human qualitative analysis. However, two major\nlimitations of topic models are their interpretability and their practicality\nfor answering targeted, domain-specific social science research questions. In\nthis work, we investigate opportunities for using LLM-generated text\naugmentation to improve the usefulness of topic modeling output. We use a\npolitical science case study to evaluate our results in a domain-specific\napplication, and find that topic modeling using GPT-4 augmentations creates\nhighly interpretable categories that can be used to investigate domain-specific\nresearch questions with minimal human guidance.", "AI": {"tldr": "This paper explores the use of LLM-generated text augmentation to enhance topic modeling in social science research, particularly in political science.", "motivation": "The study addresses limitations in interpretability and practicality of topic models for specific social science research questions, alongside challenges in reproducibility and cost of traditional qualitative analysis.", "method": "The authors apply LLM-generated text augmentations in a case study related to political science, using topic modeling techniques to analyze the resulting output.", "result": "The results show that the augmented topic modeling produces highly interpretable categories, which facilitate research on domain-specific questions with minimal human involvement.", "conclusion": "The findings suggest that LLM-generated text augmentation significantly improves the interpretability and practical application of topic modeling in social science research."}}
{"id": "2504.17480", "pdf": "https://arxiv.org/pdf/2504.17480", "abs": "https://arxiv.org/abs/2504.17480", "authors": ["Xin Yi", "Shunfan Zhengc", "Linlin Wanga", "Xiaoling Wang", "Liang He"], "title": "Unified Attacks to Large Language Model Watermarks: Spoofing and Scrubbing in Unauthorized Knowledge Distillation", "categories": ["cs.CL"], "comment": null, "summary": "Watermarking has emerged as a critical technique for combating misinformation\nand protecting intellectual property in large language models (LLMs). A recent\ndiscovery, termed watermark radioactivity, reveals that watermarks embedded in\nteacher models can be inherited by student models through knowledge\ndistillation. On the positive side, this inheritance allows for the detection\nof unauthorized knowledge distillation by identifying watermark traces in\nstudent models. However, the robustness of watermarks against scrubbing attacks\nand their unforgeability in the face of spoofing attacks under unauthorized\nknowledge distillation remain largely unexplored. Existing watermark attack\nmethods either assume access to model internals or fail to simultaneously\nsupport both scrubbing and spoofing attacks. In this work, we propose\nContrastive Decoding-Guided Knowledge Distillation (CDG-KD), a unified\nframework that enables bidirectional attacks under unauthorized knowledge\ndistillation. Our approach employs contrastive decoding to extract corrupted or\namplified watermark texts via comparing outputs from the student model and\nweakly watermarked references, followed by bidirectional distillation to train\nnew student models capable of watermark removal and watermark forgery,\nrespectively. Extensive experiments show that CDG-KD effectively performs\nattacks while preserving the general performance of the distilled model. Our\nfindings underscore critical need for developing watermarking schemes that are\nrobust and unforgeable.", "AI": {"tldr": "The paper presents a unified framework called CDG-KD for conducting bidirectional attacks on watermarks in language models, addressing vulnerabilities in watermarking techniques.", "motivation": "The study aims to address the vulnerabilities of watermarking techniques in language models against attacks such as scrubbing and spoofing, which remain inadequately explored.", "method": "The authors propose Contrastive Decoding-Guided Knowledge Distillation (CDG-KD), which uses contrastive decoding to generate amplified watermark texts from a student model and weakly watermarked references, facilitating bidirectional attacks during unauthorized knowledge distillation.", "result": "CDG-KD successfully enables watermark removal and forgery attacks while maintaining the performance of the distilled models, demonstrated through extensive experiments.", "conclusion": "The findings highlight the urgent need for the development of watermarking schemes that are both robust and unforgeable to enhance the protection of intellectual property and combat misinformation."}}
{"id": "2504.17550", "pdf": "https://arxiv.org/pdf/2504.17550", "abs": "https://arxiv.org/abs/2504.17550", "authors": ["Yejin Bang", "Ziwei Ji", "Alan Schelten", "Anthony Hartshorn", "Tara Fowler", "Cheng Zhang", "Nicola Cancedda", "Pascale Fung"], "title": "HalluLens: LLM Hallucination Benchmark", "categories": ["cs.CL", "cs.AI"], "comment": "42 pages", "summary": "Large language models (LLMs) often generate responses that deviate from user\ninput or training data, a phenomenon known as \"hallucination.\" These\nhallucinations undermine user trust and hinder the adoption of generative AI\nsystems. Addressing hallucinations is essential for the advancement of LLMs.\nThis paper introduces a comprehensive hallucination benchmark, incorporating\nboth new extrinsic and existing intrinsic evaluation tasks, built upon clear\ntaxonomy of hallucination. A major challenge in benchmarking hallucinations is\nthe lack of a unified framework due to inconsistent definitions and\ncategorizations. We disentangle LLM hallucination from \"factuality,\" proposing\na clear taxonomy that distinguishes between extrinsic and intrinsic\nhallucinations, to promote consistency and facilitate research. Extrinsic\nhallucinations, where the generated content is not consistent with the training\ndata, are increasingly important as LLMs evolve. Our benchmark includes dynamic\ntest set generation to mitigate data leakage and ensure robustness against such\nleakage. We also analyze existing benchmarks, highlighting their limitations\nand saturation. The work aims to: (1) establish a clear taxonomy of\nhallucinations, (2) introduce new extrinsic hallucination tasks, with data that\ncan be dynamically regenerated to prevent saturation by leakage, (3) provide a\ncomprehensive analysis of existing benchmarks, distinguishing them from\nfactuality evaluations.", "AI": {"tldr": "This paper presents a comprehensive benchmark for evaluating hallucinations in large language models (LLMs), addressing issues of user trust and system adoption.", "motivation": "Hallucinations in LLMs lead to responses that do not align with user input or training data, which undermines trust and hinders the adoption of generative AI systems.", "method": "The paper introduces a taxonomy that distinguishes between extrinsic and intrinsic hallucinations, along with new extrinsic evaluation tasks and a dynamic test set generation approach to prevent data leakage.", "result": "The proposed benchmark facilitates a consistent evaluation of hallucinations and highlights limitations in existing benchmarks, emphasizing the importance of distinguishing hallucinations from factuality.", "conclusion": "Establishing a clear taxonomy and robust benchmark for hallucinations is essential for advancing research and improving the reliability of LLMs."}}
{"id": "2504.17562", "pdf": "https://arxiv.org/pdf/2504.17562", "abs": "https://arxiv.org/abs/2504.17562", "authors": ["Rei Higuchi", "Ryotaro Kawata", "Naoki Nishikawa", "Kazusato Oko", "Shoichiro Yamaguchi", "Sosuke Kobayashi", "Seiya Tokui", "Kohei Hayashi", "Daisuke Okanohara", "Taiji Suzuki"], "title": "When Does Metadata Conditioning (NOT) Work for Language Model Pre-Training? A Study with Context-Free Grammars", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The ability to acquire latent semantics is one of the key properties that\ndetermines the performance of language models. One convenient approach to\ninvoke this ability is to prepend metadata (e.g. URLs, domains, and styles) at\nthe beginning of texts in the pre-training data, making it easier for the model\nto access latent semantics before observing the entire text. Previous studies\nhave reported that this technique actually improves the performance of trained\nmodels in downstream tasks; however, this improvement has been observed only in\nspecific downstream tasks, without consistent enhancement in average next-token\nprediction loss. To understand this phenomenon, we closely investigate how\nprepending metadata during pre-training affects model performance by examining\nits behavior using artificial data. Interestingly, we found that this approach\nproduces both positive and negative effects on the downstream tasks. We\ndemonstrate that the effectiveness of the approach depends on whether latent\nsemantics can be inferred from the downstream task's prompt. Specifically,\nthrough investigations using data generated by probabilistic context-free\ngrammars, we show that training with metadata helps improve model's performance\nwhen the given context is long enough to infer the latent semantics. In\ncontrast, the technique negatively impacts performance when the context lacks\nthe necessary information to make an accurate posterior inference.", "AI": {"tldr": "Prepending metadata during language model pre-training affects performance, yielding both positive and negative outcomes depending on context length and inferability of latent semantics.", "motivation": "To understand the performance differences in language models when metadata is prepended to pre-training data, and why improvements are task-specific.", "method": "The study examines the effects of prepending metadata using artificial data generated by probabilistic context-free grammars, analyzing model behavior in relation to downstream task prompts.", "result": "Prepending metadata can improve performance on downstream tasks when the context is long enough to infer latent semantics; however, it can also harm performance when the context is insufficient for posterior inference.", "conclusion": "The effectiveness of metadata prepending in enhancing language model performance is contingent on the capability of the context to convey necessary latent semantics."}}
{"id": "2504.17565", "pdf": "https://arxiv.org/pdf/2504.17565", "abs": "https://arxiv.org/abs/2504.17565", "authors": ["Xiaoyu Tian", "Sitong Zhao", "Haotian Wang", "Shuaiting Chen", "Yiping Peng", "Yunjie Ji", "Han Zhao", "Xiangang Li"], "title": "DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale Difficulty-Graded Data Training", "categories": ["cs.CL"], "comment": null, "summary": "Although large language models (LLMs) have recently achieved remarkable\nperformance on various complex reasoning benchmarks, the academic community\nstill lacks an in-depth understanding of base model training processes and data\nquality. To address this, we construct a large-scale, difficulty-graded\nreasoning dataset containing approximately 3.34 million unique queries of\nvarying difficulty levels and about 40 million distilled responses generated by\nmultiple models over several passes. Leveraging pass rate and Coefficient of\nVariation (CV), we precisely select the most valuable training data to enhance\nreasoning capability. Notably, we observe a training pattern shift, indicating\nthat reasoning-focused training based on base models requires higher learning\nrates for effective training. Using this carefully selected data, we\nsignificantly improve the reasoning capabilities of the base model, achieving a\npass rate of 79.2\\% on the AIME2024 mathematical reasoning benchmark. This\nresult surpasses most current distilled models and closely approaches\nstate-of-the-art performance. We provide detailed descriptions of our data\nprocessing, difficulty assessment, and training methodology, and have publicly\nreleased all datasets and methods to promote rapid progress in open-source\nlong-reasoning LLMs. The dataset is available at:\nhttps://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M", "AI": {"tldr": "The paper presents a large-scale, difficulty-graded reasoning dataset and improved training methodology for large language models (LLMs) to enhance their reasoning capabilities.", "motivation": "The drive to better understand base model training processes and the quality of data used in training LLMs, despite their recent successes in reasoning tasks.", "method": "A large-scale reasoning dataset with 3.34 million unique queries and 40 million responses was constructed, and valuable training data was selected based on pass rates and Coefficient of Variation. Enhanced training methods included using higher learning rates for reasoning-focused training.", "result": "A pass rate of 79.2% was achieved on the AIME2024 benchmark, surpassing most current distilled models and nearing state-of-the-art performance.", "conclusion": "The study develops a methodology for improving reasoning in LLMs through the careful selection of training data and training techniques, and all datasets and methods are publicly available to encourage advancements in this area."}}
{"id": "2504.17574", "pdf": "https://arxiv.org/pdf/2504.17574", "abs": "https://arxiv.org/abs/2504.17574", "authors": ["Zhenkai Qin", "Guifang Yang", "Dongze Wu"], "title": "RAGAT-Mind: A Multi-Granular Modeling Approach for Rumor Detection Based on MindSpore", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "As false information continues to proliferate across social media platforms,\neffective rumor detection has emerged as a pressing challenge in natural\nlanguage processing. This paper proposes RAGAT-Mind, a multi-granular modeling\napproach for Chinese rumor detection, built upon the MindSpore deep learning\nframework. The model integrates TextCNN for local semantic extraction,\nbidirectional GRU for sequential context learning, Multi-Head Self-Attention\nfor global dependency focusing, and Bidirectional Graph Convolutional Networks\n(BiGCN) for structural representation of word co-occurrence graphs. Experiments\non the Weibo1-Rumor dataset demonstrate that RAGAT-Mind achieves superior\nclassification performance, attaining 99.2% accuracy and a macro-F1 score of\n0.9919. The results validate the effectiveness of combining hierarchical\nlinguistic features with graph-based semantic structures. Furthermore, the\nmodel exhibits strong generalization and interpretability, highlighting its\npractical value for real-world rumor detection applications.", "AI": {"tldr": "The paper presents RAGAT-Mind, a model for Chinese rumor detection, achieving high accuracy and F1 score by combining various deep learning techniques.", "motivation": "To address the pressing challenge of effective rumor detection in the face of increasing false information on social media platforms.", "method": "RAGAT-Mind is built on the MindSpore framework and integrates TextCNN, bidirectional GRU, Multi-Head Self-Attention, and Bidirectional Graph Convolutional Networks for comprehensive analysis of textual data.", "result": "RAGAT-Mind outperforms other models on the Weibo1-Rumor dataset with 99.2% accuracy and a macro-F1 score of 0.9919, demonstrating effective classification performance.", "conclusion": "The model successfully combines hierarchical linguistic features with graph-based structures, exhibiting strong generalization and interpretability, which is valuable for real-world rumor detection."}}
{"id": "2504.17653", "pdf": "https://arxiv.org/pdf/2504.17653", "abs": "https://arxiv.org/abs/2504.17653", "authors": ["Samaneh Hosseini Moghaddam", "Kelly Lyons", "Cheryl Regehr", "Vivek Goel", "Kaitlyn Regehr"], "title": "Towards a comprehensive taxonomy of online abusive language informed by machine leaning", "categories": ["cs.CL"], "comment": null, "summary": "The proliferation of abusive language in online communications has posed\nsignificant risks to the health and wellbeing of individuals and communities.\nThe growing concern regarding online abuse and its consequences necessitates\nmethods for identifying and mitigating harmful content and facilitating\ncontinuous monitoring, moderation, and early intervention. This paper presents\na taxonomy for distinguishing key characteristics of abusive language within\nonline text. Our approach uses a systematic method for taxonomy development,\nintegrating classification systems of 18 existing multi-label datasets to\ncapture key characteristics relevant to online abusive language classification.\nThe resulting taxonomy is hierarchical and faceted, comprising 5 categories and\n17 dimensions. It classifies various facets of online abuse, including context,\ntarget, intensity, directness, and theme of abuse. This shared understanding\ncan lead to more cohesive efforts, facilitate knowledge exchange, and\naccelerate progress in the field of online abuse detection and mitigation among\nresearchers, policy makers, online platform owners, and other stakeholders.", "AI": {"tldr": "The paper presents a hierarchical taxonomy for classifying abusive language in online communications, aimed at improving detection and mitigation methods.", "motivation": "The rise of abusive language in online communications poses serious risks to individuals and communities, necessitating tools for identifying and mitigating harmful content.", "method": "A systematic method for taxonomy development was used, integrating classification systems from 18 existing multi-label datasets to create a hierarchical and faceted taxonomy of abusive language.", "result": "The resulting taxonomy features 5 categories and 17 dimensions, addressing aspects such as context, target, intensity, directness, and theme of online abuse.", "conclusion": "This shared understanding can enhance collaboration, facilitate knowledge exchange, and expedite advancements in online abuse detection and mitigation efforts."}}
{"id": "2504.17665", "pdf": "https://arxiv.org/pdf/2504.17665", "abs": "https://arxiv.org/abs/2504.17665", "authors": ["Zena Al-Khalili", "Nick Howell", "Dietrich Klakow"], "title": "Evaluating Grounded Reasoning by Code-Assisted Large Language Models for Mathematics", "categories": ["cs.CL"], "comment": null, "summary": "Assisting LLMs with code generation improved their performance on\nmathematical reasoning tasks. However, the evaluation of code-assisted LLMs is\ngenerally restricted to execution correctness, lacking a rigorous evaluation of\ntheir generated programs. In this work, we bridge this gap by conducting an\nin-depth analysis of code-assisted LLMs' generated programs in response to math\nreasoning tasks. Our evaluation focuses on the extent to which LLMs ground\ntheir programs to math rules, and how that affects their end performance. For\nthis purpose, we assess the generations of five different LLMs, on two\ndifferent math datasets, both manually and automatically. Our results reveal\nthat the distribution of grounding depends on LLMs' capabilities and the\ndifficulty of math problems. Furthermore, mathematical grounding is more\neffective for closed-source models, while open-source models fail to employ\nmath rules in their solutions correctly. On MATH500, the percentage of grounded\nprograms decreased to half, while the ungrounded generations doubled in\ncomparison to ASDiv grade-school problems. Our work highlights the need for\nin-depth evaluation beyond execution accuracy metrics, toward a better\nunderstanding of code-assisted LLMs' capabilities and limits in the math\ndomain.", "AI": {"tldr": "The study analyzes code-assisted language models (LLMs) in mathematical reasoning tasks, revealing significant differences in how well these models ground their code in math rules, affecting their performance.", "motivation": "To address the lack of rigorous evaluation of generated programs by code-assisted LLMs, particularly in the context of mathematical reasoning tasks.", "method": "An in-depth analysis of code-assisted LLMs' generated programs across five different models and two math datasets, assessed both manually and automatically to evaluate grounding to math rules.", "result": "The analysis showed that grounding distribution varies with model capabilities and problem difficulty, with closed-source models showing better grounding than open-source models; specifically, on MATH500, grounded programs decreased while ungrounded ones doubled compared to ASDiv problems.", "conclusion": "The findings emphasize the need for comprehensive evaluations that go beyond mere execution accuracy to better understand code-assisted LLMs' strengths and weaknesses in mathematical reasoning."}}
{"id": "2504.17671", "pdf": "https://arxiv.org/pdf/2504.17671", "abs": "https://arxiv.org/abs/2504.17671", "authors": ["Yuanchang Ye", "Weiyan Wen"], "title": "Data-Driven Calibration of Prediction Sets in Large Vision-Language Models Based on Inductive Conformal Prediction", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This study addresses the critical challenge of hallucination mitigation in\nLarge Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks\nthrough a Split Conformal Prediction (SCP) framework. While LVLMs excel in\nmulti-modal reasoning, their outputs often exhibit hallucinated content with\nhigh confidence, posing risks in safety-critical applications. We propose a\nmodel-agnostic uncertainty quantification method that integrates dynamic\nthreshold calibration and cross-modal consistency verification. By partitioning\ndata into calibration and test sets, the framework computes nonconformity\nscores to construct prediction sets with statistical guarantees under\nuser-defined risk levels ($\\alpha$). Key innovations include: (1) rigorous\ncontrol of \\textbf{marginal coverage} to ensure empirical error rates remain\nstrictly below $\\alpha$; (2) dynamic adjustment of prediction set sizes\ninversely with $\\alpha$, filtering low-confidence outputs; (3) elimination of\nprior distribution assumptions and retraining requirements. Evaluations on\nbenchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces\ntheoretical guarantees across all $\\alpha$ values. The framework achieves\nstable performance across varying calibration-to-test split ratios,\nunderscoring its robustness for real-world deployment in healthcare, autonomous\nsystems, and other safety-sensitive domains. This work bridges the gap between\ntheoretical reliability and practical applicability in multi-modal AI systems,\noffering a scalable solution for hallucination detection and uncertainty-aware\ndecision-making.", "AI": {"tldr": "This study proposes a Split Conformal Prediction (SCP) framework to mitigate hallucinations in Large Vision-Language Models (LVLMs) used for Visual Question Answering (VQA).", "motivation": "The study addresses the critical issue of hallucination in LVLMs, which can produce high-confidence yet incorrect outputs, posing risks in safety-critical applications.", "method": "The framework utilizes a model-agnostic method for uncertainty quantification, combining dynamic threshold calibration and cross-modal consistency verification by partitioning data and computing nonconformity scores to create prediction sets with statistical guarantees.", "result": "Evaluations show that the SCP framework maintains theoretical guarantees across multiple LVLMs and calibration-to-test split ratios, demonstrating stable performance and robustness in practical applications.", "conclusion": "The study successfully bridges the gap between theoretical reliability and practical use in multi-modal AI systems, providing a scalable solution for hallucination detection and uncertainty-aware decision-making."}}
{"id": "2504.17674", "pdf": "https://arxiv.org/pdf/2504.17674", "abs": "https://arxiv.org/abs/2504.17674", "authors": ["Jared Fernandez", "Clara Na", "Vashisth Tiwari", "Yonatan Bisk", "Sasha Luccioni", "Emma Strubell"], "title": "Energy Considerations of Large Language Model Inference and Efficiency Optimizations", "categories": ["cs.CL", "cs.LG"], "comment": "16 pages", "summary": "As large language models (LLMs) scale in size and adoption, their\ncomputational and environmental costs continue to rise. Prior benchmarking\nefforts have primarily focused on latency reduction in idealized settings,\noften overlooking the diverse real-world inference workloads that shape energy\nuse. In this work, we systematically analyze the energy implications of common\ninference efficiency optimizations across diverse Natural Language Processing\n(NLP) and generative Artificial Intelligence (AI) workloads, including\nconversational AI and code generation. We introduce a modeling approach that\napproximates real-world LLM workflows through a binning strategy for\ninput-output token distributions and batch size variations. Our empirical\nanalysis spans software frameworks, decoding strategies, GPU architectures,\nonline and offline serving settings, and model parallelism configurations. We\nshow that the effectiveness of inference optimizations is highly sensitive to\nworkload geometry, software stack, and hardware accelerators, demonstrating\nthat naive energy estimates based on FLOPs or theoretical GPU utilization\nsignificantly underestimate real-world energy consumption. Our findings reveal\nthat the proper application of relevant inference efficiency optimizations can\nreduce total energy use by up to 73% from unoptimized baselines. These insights\nprovide a foundation for sustainable LLM deployment and inform energy-efficient\ndesign strategies for future AI infrastructure.", "AI": {"tldr": "This paper analyzes the energy implications of inference efficiency optimizations for large language models (LLMs) across various NLP and generative AI workloads, revealing that naive energy estimates significantly underestimate actual consumption.", "motivation": "The rising computational and environmental costs of large language models necessitate a more thorough understanding of energy usage in diverse real-world inference workloads beyond standard benchmarking.", "method": "A modeling approach using binning strategies for input-output token distributions and batch size variations, accompanied by a comprehensive empirical analysis of different software frameworks, decoding strategies, GPU architectures, and serving settings.", "result": "The study found that the effectiveness of inference optimizations is sensitive to various factors, with optimized methods reducing energy use by up to 73% compared to unoptimized baselines, challenging naive energy estimates.", "conclusion": "The results emphasize the importance of tailored inference efficiency optimizations for reducing energy consumption in LLM deployments, providing a basis for future energy-efficient AI infrastructure design."}}
{"id": "2504.17685", "pdf": "https://arxiv.org/pdf/2504.17685", "abs": "https://arxiv.org/abs/2504.17685", "authors": ["Haru-Tada Sato", "Fuka Matsuzaki", "Jun-ichiro Takahashi"], "title": "Ensemble Bayesian Inference: Leveraging Small Language Models to Achieve LLM-level Accuracy in Profile Matching Tasks", "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 2 figures", "summary": "This study explores the potential of small language model(SLM) ensembles to\nachieve accuracy comparable to proprietary large language models (LLMs). We\npropose Ensemble Bayesian Inference (EBI), a novel approach that applies\nBayesian estimation to combine judgments from multiple SLMs, allowing them to\nexceed the performance limitations of individual models. Our experiments on\ndiverse tasks(aptitude assessments and consumer profile analysis in both\nJapanese and English) demonstrate EBI's effectiveness. Notably, we analyze\ncases where incorporating models with negative Lift values into ensembles\nimproves overall performance, and we examine the method's efficacy across\ndifferent languages. These findings suggest new possibilities for constructing\nhigh-performance AI systems with limited computational resources and for\neffectively utilizing models with individually lower performance. Building on\nexisting research on LLM performance evaluation, ensemble methods, and\nopen-source LLM utilization, we discuss the novelty and significance of our\napproach.", "AI": {"tldr": "The study investigates the use of small language model ensembles to match the accuracy of large language models through a new approach called Ensemble Bayesian Inference (EBI).", "motivation": "The research aims to explore alternatives to large proprietary language models by leveraging ensembles of smaller models to achieve comparable accuracy with limited computational resources.", "method": "The paper introduces Ensemble Bayesian Inference (EBI), which applies Bayesian estimation to aggregate outputs from multiple small language models, and tests this method on various tasks in both Japanese and English.", "result": "Experiments demonstrate that EBI effectively enhances performance across different tasks, even incorporating models with initially lower individual performance, thereby improving the overall ensemble's effectiveness.", "conclusion": "The findings suggest that using smaller language model ensembles can yield high-performance AI systems while making effective use of available computational resources."}}
{"id": "2504.17704", "pdf": "https://arxiv.org/pdf/2504.17704", "abs": "https://arxiv.org/abs/2504.17704", "authors": ["Cheng Wang", "Yue Liu", "Baolong Li", "Duzhen Zhang", "Zhongzhi Li", "Junfeng Fang"], "title": "Safety in Large Reasoning Models: A Survey", "categories": ["cs.CL"], "comment": null, "summary": "Large Reasoning Models (LRMs) have exhibited extraordinary prowess in tasks\nlike mathematics and coding, leveraging their advanced reasoning capabilities.\nNevertheless, as these capabilities progress, significant concerns regarding\ntheir vulnerabilities and safety have arisen, which can pose challenges to\ntheir deployment and application in real-world settings. This paper presents a\ncomprehensive survey of LRMs, meticulously exploring and summarizing the newly\nemerged safety risks, attacks, and defense strategies. By organizing these\nelements into a detailed taxonomy, this work aims to offer a clear and\nstructured understanding of the current safety landscape of LRMs, facilitating\nfuture research and development to enhance the security and reliability of\nthese powerful models.", "AI": {"tldr": "The paper surveys the safety risks and defense strategies of Large Reasoning Models (LRMs), providing a taxonomy to enhance understanding and facilitate future research.", "motivation": "With the advancement of LRMs in complex tasks, there are growing concerns about their vulnerabilities and safety in real-world applications.", "method": "The study presents a comprehensive survey organizing safety risks, attacks, and defense strategies related to LRMs into a detailed taxonomy.", "result": "The work summarizes newly emerged safety risks and categorizes them along with attacks and defense strategies for LRMs.", "conclusion": "This structured understanding of safety issues aims to support the development of more secure and reliable LRMs."}}
{"id": "2504.17720", "pdf": "https://arxiv.org/pdf/2504.17720", "abs": "https://arxiv.org/abs/2504.17720", "authors": ["Vansh Gupta", "Sankalan Pal Chowdhury", "Vilém Zouhar", "Donya Rooein", "Mrinmaya Sachan"], "title": "Multilingual Performance Biases of Large Language Models in Education", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly being adopted in educational\nsettings. These applications expand beyond English, though current LLMs remain\nprimarily English-centric. In this work, we ascertain if their use in education\nsettings in non-English languages is warranted. We evaluated the performance of\npopular LLMs on four educational tasks: identifying student misconceptions,\nproviding targeted feedback, interactive tutoring, and grading translations in\nsix languages (Hindi, Arabic, Farsi, Telugu, Ukrainian, Czech) in addition to\nEnglish. We find that the performance on these tasks somewhat corresponds to\nthe amount of language represented in training data, with lower-resource\nlanguages having poorer task performance. Although the models perform\nreasonably well in most languages, the frequent performance drop from English\nis significant. Thus, we recommend that practitioners first verify that the LLM\nworks well in the target language for their educational task before deployment.", "AI": {"tldr": "The paper evaluates the effectiveness of large language models (LLMs) in educational tasks across multiple languages, finding significant performance drops in lower-resource languages compared to English.", "motivation": "To determine the suitability of large language models for educational settings in non-English languages, given their prevalent use in English-centric applications.", "method": "The study assesses the performance of popular LLMs on four educational tasks (identifying misconceptions, providing feedback, tutoring, and grading translations) across six languages (Hindi, Arabic, Farsi, Telugu, Ukrainian, Czech, and English).", "result": "Performance on educational tasks generally correlates with the amount of language data the LLMs are trained on, revealing significant performance drops for lower-resource languages when compared to English.", "conclusion": "Practitioners should ensure the LLM performs effectively in the target language for specific educational tasks prior to deployment."}}
{"id": "2504.17753", "pdf": "https://arxiv.org/pdf/2504.17753", "abs": "https://arxiv.org/abs/2504.17753", "authors": ["Anuja Tayal", "Devika Salunke", "Barbara Di Eugenio", "Paula Allen-Meares", "Eulalia Puig Abril", "Olga Garcia", "Carolyn Dickens", "Andrew Boyd"], "title": "Conversational Assistants to support Heart Failure Patients: comparing a Neurosymbolic Architecture with ChatGPT", "categories": ["cs.CL"], "comment": null, "summary": "Conversational assistants are becoming more and more popular, including in\nhealthcare, partly because of the availability and capabilities of Large\nLanguage Models. There is a need for controlled, probing evaluations with real\nstakeholders which can highlight advantages and disadvantages of more\ntraditional architectures and those based on generative AI. We present a\nwithin-group user study to compare two versions of a conversational assistant\nthat allows heart failure patients to ask about salt content in food. One\nversion of the system was developed in-house with a neurosymbolic architecture,\nand one is based on ChatGPT. The evaluation shows that the in-house system is\nmore accurate, completes more tasks and is less verbose than the one based on\nChatGPT; on the other hand, the one based on ChatGPT makes fewer speech errors\nand requires fewer clarifications to complete the task. Patients show no\npreference for one over the other.", "AI": {"tldr": "A user study compares a neurosymbolic conversational assistant with a ChatGPT-based one for heart failure patients' inquiries about food salt content.", "motivation": "To evaluate and compare traditional conversational architectures with generative AI in healthcare settings, specifically for heart failure patient support.", "method": "A within-group user study comparing two versions of a conversational assistant: a neurosymbolic architecture and a ChatGPT-based system.", "result": "The in-house system was more accurate, completed more tasks, and was less verbose, while the ChatGPT system made fewer speech errors and required fewer clarifications, with no patient preference for either system.", "conclusion": "Both systems have their strengths and weaknesses, and patient preferences do not favor one architecture over the other."}}
{"id": "2504.17768", "pdf": "https://arxiv.org/pdf/2504.17768", "abs": "https://arxiv.org/abs/2504.17768", "authors": ["Piotr Nawrot", "Robert Li", "Renjie Huang", "Sebastian Ruder", "Kelly Marchisio", "Edoardo M. Ponti"], "title": "The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Sparse attention offers a promising strategy to extend long-context\ncapabilities in Transformer LLMs, yet its viability, its efficiency-accuracy\ntrade-offs, and systematic scaling studies remain unexplored. To address this\ngap, we perform a careful comparison of training-free sparse attention methods\nat varying model scales, sequence lengths, and sparsity levels on a diverse\ncollection of long-sequence tasks-including novel ones that rely on natural\nlanguage while remaining controllable and easy to evaluate. Based on our\nexperiments, we report a series of key findings: 1) an isoFLOPS analysis\nreveals that for very long sequences, larger and highly sparse models are\npreferable to smaller and dense ones. 2) The level of sparsity attainable while\nstatistically guaranteeing accuracy preservation is higher during decoding than\nprefilling, and correlates with model size in the former. 3) There is no clear\nstrategy that performs best across tasks and phases, with different units of\nsparsification or budget adaptivity needed for different scenarios. Even\nmoderate sparsity levels often result in significant performance degradation on\nat least one task, highlighting that sparse attention is not a universal\nsolution. 4) We introduce and validate novel scaling laws specifically tailored\nfor sparse attention, providing evidence that our findings are likely to hold\ntrue beyond our range of experiments. Through these insights, we demonstrate\nthat sparse attention is a key tool to enhance the capabilities of Transformer\nLLMs for processing longer sequences, but requires careful evaluation of\ntrade-offs for performance-sensitive applications.", "AI": {"tldr": "Sparse attention can extend long-context capabilities in Transformer LLMs, but its efficiency and accuracy trade-offs need careful evaluation.", "motivation": "There is a gap in understanding the viability, efficiency-accuracy trade-offs, and scaling studies of sparse attention in Transformer models for long-context tasks.", "method": "A comparison of training-free sparse attention methods was performed at varying model scales, sequence lengths, and sparsity levels across a diverse set of long-sequence tasks.", "result": "Key findings include the preference for larger sparse models for long sequences, variations in sparsity during decoding vs. pre-filling, no single best strategy across tasks, and the introduction of new scaling laws for sparse attention.", "conclusion": "Sparse attention enhances Transformer LLM capabilities for longer sequences, but necessitates careful evaluation of trade-offs in performance-sensitive applications."}}
