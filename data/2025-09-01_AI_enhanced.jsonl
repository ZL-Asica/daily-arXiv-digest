{"id": "2508.21087", "pdf": "https://arxiv.org/pdf/2508.21087.pdf", "abs": "https://arxiv.org/abs/2508.21087", "title": "Can LLMs Generate Behaviors for Embodied Virtual Agents Based on Personality Traits?", "authors": ["Bin Han", "Deuksin Kwon", "Spencer Lin", "Kaleen Shrestha", "Jonathan Gratch"], "categories": ["cs.HC"], "comment": null, "summary": "This study proposes a framework that employs personality prompting with Large\nLanguage Models to generate verbal and nonverbal behaviors for virtual agents\nbased on personality traits. Focusing on extraversion, we evaluated the system\nin two scenarios: negotiation and ice breaking, using both introverted and\nextroverted agents. In Experiment 1, we conducted agent to agent simulations\nand performed linguistic analysis and personality classification to assess\nwhether the LLM generated language reflected the intended traits and whether\nthe corresponding nonverbal behaviors varied by personality. In Experiment 2,\nwe carried out a user study to evaluate whether these personality aligned\nbehaviors were consistent with their intended traits and perceptible to human\nobservers. Our results show that LLMs can generate verbal and nonverbal\nbehaviors that align with personality traits, and that users are able to\nrecognize these traits through the agents' behaviors. This work underscores the\npotential of LLMs in shaping personality aligned virtual agents.", "AI": {"tldr": "This study presents a framework using Large Language Models (LLMs) to create personality-aligned behaviors in virtual agents.", "motivation": "To explore how personality traits can influence the behaviors of virtual agents, focusing on the trait of extraversion.", "method": "Two experiments were conducted: an agent-to-agent simulation assessing linguistic traits and a user study evaluating human perception of these traits in virtual agents.", "result": "LLMs successfully generated verbal and nonverbal behaviors corresponding to various personality traits, and users could recognize these traits in agent behaviors.", "conclusion": "The findings highlight the capability of LLMs to effectively shape personality-aligned traits in virtual agents, suggesting valuable applications in HCI.", "key_contributions": ["Development of a framework for personality prompting in LLMs", "Demonstration of personality-aligned verbal and nonverbal behaviors", "User perception study confirming recognizability of traits in agents' behaviors"], "limitations": "The study primarily focused on extraversion; further research is needed across more personality traits and contexts.", "keywords": ["Large Language Models", "virtual agents", "personality traits", "Human-Computer Interaction", "behavior generation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.21209", "pdf": "https://arxiv.org/pdf/2508.21209.pdf", "abs": "https://arxiv.org/abs/2508.21209", "title": "Designing Smarter Conversational Agents for Kids: Lessons from Cognitive Work and Means-Ends Analyses", "authors": ["Vanessa Figueiredo"], "categories": ["cs.HC", "cs.CL", "I.2.1; H.5.2"], "comment": null, "summary": "This paper presents two studies on how Brazilian children (ages 9--11) use\nconversational agents (CAs) for schoolwork, discovery, and entertainment, and\nhow structured scaffolds can enhance these interactions. In Study 1, a\nseven-week online investigation with 23 participants (children, parents,\nteachers) employed interviews, observations, and Cognitive Work Analysis to map\nchildren's information-processing flows, the role of more knowledgeable others,\nfunctional uses, contextual goals, and interaction patterns to inform\nconversation-tree design. We identified three CA functions: School, Discovery,\nEntertainment, and derived ``recipe'' scaffolds mirroring parent-child support.\nIn Study 2, we prompted GPT-4o-mini on 1,200 simulated child-CA exchanges,\ncomparing conversation-tree recipes based on structured-prompting to an\nunstructured baseline. Quantitative evaluation of readability, question\ncount/depth/diversity, and coherence revealed gains for the recipe approach.\nBuilding on these findings, we offer design recommendations: scaffolded\nconversation-trees, child-dedicated profiles for personalized context, and\ncaregiver-curated content. Our contributions include the first CWA application\nwith Brazilian children, an empirical framework of child-CA information flows,\nand an LLM-scaffolding ``recipe'' (i.e., structured-prompting) for effective,\nscaffolded learning.", "AI": {"tldr": "The paper explores how Brazilian children interact with conversational agents (CAs) for education and entertainment, revealing designs that enhance these interactions.", "motivation": "To understand the interaction patterns of Brazilian children aged 9-11 with conversational agents and how structured scaffolds can improve these interactions.", "method": "Two studies were conducted: the first involved interviews and observations of children using CAs, while the second evaluated simulated child-CA exchanges by comparing structured and unstructured prompts.", "result": "The studies found three primary functions of CAs (School, Discovery, Entertainment) and demonstrated that structured prompting significantly improves the quality of interactions in terms of readability, question depth/diversity, and coherence.", "conclusion": "The findings suggest design recommendations for conversational agents aimed at children, including scaffolded conversation trees and caregiver-curated content, contributing to the field of human-agent interaction.", "key_contributions": ["First CWA application with Brazilian children", "Empirical framework of child-CA information flows", "LLM-scaffolding 'recipe' for effective learning"], "limitations": "", "keywords": ["Conversational Agents", "Children", "Human-Computer Interaction", "Structured Scaffolding", "LLM"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.21283", "pdf": "https://arxiv.org/pdf/2508.21283.pdf", "abs": "https://arxiv.org/abs/2508.21283", "title": "Design and evaluation of a serious game in virtual reality to increase empathy towards students with phonological dyslexia", "authors": ["Jose Manuel Alcalde-Llergo", "Andrea Zingoni", "Pilar Aparicio-Martinez", "Sara Pinzi", "Enrique Yeguas-Bolivar"], "categories": ["cs.HC"], "comment": "29 pages, 3 tables, 7 figures", "summary": "Dyslexia is a neurodevelopmental disorder estimated to strike approximately 5\nto 10 per cent of the population. In particular, phonological dyslexia causes\nproblems in connecting the sounds of words with their written forms.\nConsequently, affected individuals may encounter issues such as slow reading\nspeed, inaccurate reading, and difficulty decoding unfamiliar words. To address\nthese complexities, the use of compensatory tools and strategies is essential\nto ensure equitable opportunities for dyslexic students. However, the general\nunderestimation of the issue and lack of awareness regarding the significance\nof support methodologies pose significant obstacles. One of the ways to enhance\nconsciousness towards a certain issue is by stimulating empathy with whom is\naffected by it. In light of this, this study introduces a serious game in\nvirtual reality, targeted at educators, students, and, in general, at the\nnon-dyslexic community. The game seeks to enhance understanding of the\nchallenges that individuals with dyslexia experience daily, highlighting the\nrelevance of supportive measures. This approach encourages players to empathize\nwith the struggles of dyslexic individuals and to learn firsthand the\nimportance of supportive methodologies. The final version of the experience was\ntested by 101 participants and evaluated through a specific collection of\nquestionnaires validated in the literature. The results show that using the\nproposed virtual reality tool to promote empathy for individuals with\nphonological dyslexia is highly effective, leading to an average 20 per cent\nincrease in participants' empathy after playing the game.", "AI": {"tldr": "This study presents a serious virtual reality game designed to enhance empathy towards individuals with dyslexia by simulating their reading challenges.", "motivation": "To raise awareness and understanding of dyslexia among educators and non-dyslexic individuals, highlighting the need for supportive methods.", "method": "Development and testing of a serious game in virtual reality, followed by evaluation with 101 participants through validated questionnaires.", "result": "Participants showed an average 20% increase in empathy towards individuals with phonological dyslexia after playing the game.", "conclusion": "The virtual reality tool effectively promotes empathy towards dyslexic individuals, emphasizing the importance of understanding and support.", "key_contributions": ["Novel application of VR for empathy training regarding dyslexia", "Empirical evidence showing increased empathy in participants", "Focus on educator and community awareness regarding dyslexia"], "limitations": "", "keywords": ["dyslexia", "virtual reality", "empathy", "educators", "serious game"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2508.21308", "pdf": "https://arxiv.org/pdf/2508.21308.pdf", "abs": "https://arxiv.org/abs/2508.21308", "title": "Conflict in Community-Based Design: A Case Study of a Relationship Breakdown", "authors": ["Alekhya Gandu", "Aakash Gautam"], "categories": ["cs.HC", "cs.CY"], "comment": "23 pages", "summary": "Community-based design efforts rightly seek to reduce the power differences\nbetween researchers and community participants by aligning with community\nvalues and furthering their priorities. However, what should designers do when\nkey community members' practices seem to enact an oppressive and harmful\nstructure? We reflect on our two-year-long engagement with a non-profit\norganization in southern India that supports women subjected to domestic abuse\nor facing mental health crises. We highlight the organizational gaps in\nknowledge management and transfer, which became an avenue for our design\nintervention. During design, we encountered practices that upheld caste\nhierarchies. These practices were expected to be incorporated into our\ntechnology. Anticipating harms to indirect stakeholders, we resisted this\nincorporation. It led to a breakdown in our relationship with the partner\norganization. Reflecting on this experience, we outline pluralistic pathways\nthat community-based designers might inhabit when navigating value conflicts.\nThese include making space for reflection before and during engagements,\nstrategically repositioning through role reframing or appreciative inquiry, and\nexiting the engagement if necessary.", "AI": {"tldr": "This paper discusses the complexities faced by community-based designers when engaging with organizations where community practices may reinforce oppression. It highlights the need for reflection and strategic decision-making in design interventions.", "motivation": "To address the power dynamics in community-based design and how they can align with community values while preventing harm from oppressive practices.", "method": "The authors reflect on a two-year engagement with a non-profit supporting women in southern India, analyzing their design intervention in light of community practices and organizational gaps.", "result": "The study reveals that adhering to harmful practices can jeopardize the design's integrity, leading to breakdowns in partnerships.", "conclusion": "Community-based designers must navigate value conflicts through reflection, role repositioning, and potential withdrawal from partnerships when necessary.", "key_contributions": ["Insights into navigating oppressive practices in community design", "Framework for reflection and strategic decision-making", "Case study of engagement with a non-profit organization in India"], "limitations": "The findings are based on a single case study and may not be generalizable to all community-based design efforts.", "keywords": ["community-based design", "caste hierarchies", "knowledge management", "stakeholder engagement", "design intervention"], "importance_score": 8, "read_time_minutes": 23}}
{"id": "2508.21083", "pdf": "https://arxiv.org/pdf/2508.21083.pdf", "abs": "https://arxiv.org/abs/2508.21083", "title": "CoBA: Counterbias Text Augmentation for Mitigating Various Spurious Correlations via Semantic Triples", "authors": ["Kyohoon Jin", "Juhwan Choi", "Jungmin Yun", "Junho Lee", "Soojin Jang", "Youngbin Kim"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at EMNLP 2025", "summary": "Deep learning models often learn and exploit spurious correlations in\ntraining data, using these non-target features to inform their predictions.\nSuch reliance leads to performance degradation and poor generalization on\nunseen data. To address these limitations, we introduce a more general form of\ncounterfactual data augmentation, termed counterbias data augmentation, which\nsimultaneously tackles multiple biases (e.g., gender bias, simplicity bias) and\nenhances out-of-distribution robustness. We present CoBA: CounterBias\nAugmentation, a unified framework that operates at the semantic triple level:\nfirst decomposing text into subject-predicate-object triples, then selectively\nmodifying these triples to disrupt spurious correlations. By reconstructing the\ntext from these adjusted triples, CoBA generates counterbias data that\nmitigates spurious patterns. Through extensive experiments, we demonstrate that\nCoBA not only improves downstream task performance, but also effectively\nreduces biases and strengthens out-of-distribution resilience, offering a\nversatile and robust solution to the challenges posed by spurious correlations.", "AI": {"tldr": "Introducing CoBA, a counterbias data augmentation framework that mitigates spurious correlations in deep learning models by adjusting textual data at the semantic triple level.", "motivation": "Deep learning models often rely on spurious correlations in training data, leading to poor generalization on unseen data.", "method": "CoBA operates by decomposing text into subject-predicate-object triples and selectively modifying these to disrupt spurious correlations, thus generating counterbias data.", "result": "CoBA enhances downstream task performance, reduces biases, and improves out-of-distribution robustness.", "conclusion": "CoBA is a versatile framework that effectively addresses biases in deep learning models, improving performance and generalization.", "key_contributions": ["Introduction of counterbias data augmentation framework CoBA", "Operation at the semantic triple level to disrupt spurious correlations", "Demonstrated performance improvements and bias reduction in extensive experiments"], "limitations": "", "keywords": ["counterfactual data augmentation", "deep learning", "bias reduction", "out-of-distribution robustness", "semantic triples"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.21456", "pdf": "https://arxiv.org/pdf/2508.21456.pdf", "abs": "https://arxiv.org/abs/2508.21456", "title": "Morae: Proactively Pausing UI Agents for User Choices", "authors": ["Yi-Hao Peng", "Dingzeyu Li", "Jeffrey P. Bigham", "Amy Pavel"], "categories": ["cs.HC", "cs.CL", "cs.CV"], "comment": "ACM UIST 2025", "summary": "User interface (UI) agents promise to make inaccessible or complex UIs easier\nto access for blind and low-vision (BLV) users. However, current UI agents\ntypically perform tasks end-to-end without involving users in critical choices\nor making them aware of important contextual information, thus reducing user\nagency. For example, in our field study, a BLV participant asked to buy the\ncheapest available sparkling water, and the agent automatically chose one from\nseveral equally priced options, without mentioning alternative products with\ndifferent flavors or better ratings. To address this problem, we introduce\nMorae, a UI agent that automatically identifies decision points during task\nexecution and pauses so that users can make choices. Morae uses large\nmultimodal models to interpret user queries alongside UI code and screenshots,\nand prompt users for clarification when there is a choice to be made. In a\nstudy over real-world web tasks with BLV participants, Morae helped users\ncomplete more tasks and select options that better matched their preferences,\nas compared to baseline agents, including OpenAI Operator. More broadly, this\nwork exemplifies a mixed-initiative approach in which users benefit from the\nautomation of UI agents while being able to express their preferences.", "AI": {"tldr": "Introducing Morae, a UI agent that enhances agency for blind and low-vision users by involving them in decision-making during task execution.", "motivation": "Current UI agents reduce user agency by making end-to-end decisions without user involvement, particularly affecting blind and low-vision users.", "method": "Morae employs large multimodal models to identify decision points during tasks, pausing to allow users to make choices informed by context, such as UI code and screenshots.", "result": "In studies with BLV participants, Morae outperformed baseline agents by enabling users to complete more tasks and make selections aligned with their preferences.", "conclusion": "Morae demonstrates a successful mixed-initiative approach that balances automation with user preference expression, improving overall interaction quality.", "key_contributions": ["Development of Morae, a UI agent that enhances user agency for BLV users", "Implementation of large multimodal models for decision point identification", "Empirical study showing performance improvements over baseline agents"], "limitations": "", "keywords": ["User Interface Agents", "Blind and Low-Vision Users", "Decision Making", "Mixed-Initiative Systems", "Multimodal Models"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.21084", "pdf": "https://arxiv.org/pdf/2508.21084.pdf", "abs": "https://arxiv.org/abs/2508.21084", "title": "Mapping Toxic Comments Across Demographics: A Dataset from German Public Broadcasting", "authors": ["Jan Fillies", "Michael Peter Hoffmann", "Rebecca Reichel", "Roman Salzwedel", "Sven Bodemer", "Adrian Paschke"], "categories": ["cs.CL", "cs.CY"], "comment": "The paper has been accepted to the EMNLP 2025 main track", "summary": "A lack of demographic context in existing toxic speech datasets limits our\nunderstanding of how different age groups communicate online. In collaboration\nwith funk, a German public service content network, this research introduces\nthe first large-scale German dataset annotated for toxicity and enriched with\nplatform-provided age estimates. The dataset includes 3,024 human-annotated and\n30,024 LLM-annotated anonymized comments from Instagram, TikTok, and YouTube.\nTo ensure relevance, comments were consolidated using predefined toxic\nkeywords, resulting in 16.7\\% labeled as problematic. The annotation pipeline\ncombined human expertise with state-of-the-art language models, identifying key\ncategories such as insults, disinformation, and criticism of broadcasting fees.\nThe dataset reveals age-based differences in toxic speech patterns, with\nyounger users favoring expressive language and older users more often engaging\nin disinformation and devaluation. This resource provides new opportunities for\nstudying linguistic variation across demographics and supports the development\nof more equitable and age-aware content moderation systems.", "AI": {"tldr": "This paper introduces a new large-scale German dataset annotated for toxicity and includes age estimates, revealing different toxic speech patterns across age groups.", "motivation": "To address the lack of demographic context in existing toxic speech datasets and improve understanding of communication patterns across different age groups online.", "method": "The study involved creating a dataset of 3,024 human-annotated and 30,024 LLM-annotated comments from Instagram, TikTok, and YouTube, with a focus on predefined toxic keywords for relevance.", "result": "The dataset, which identified 16.7% of comments as problematic, shows that younger users use more expressive language while older users often engage in disinformation and devaluation.", "conclusion": "This resource is significant for studying linguistic variation across demographics and aids in developing more equitable content moderation systems.", "key_contributions": ["First large-scale German dataset annotated for toxicity with age context", "Combined human and LLM annotation for enhanced dataset quality", "Revealed age-based differences in toxic speech patterns"], "limitations": "", "keywords": ["toxic speech", "demographic context", "language models", "content moderation", "age estimates"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.21666", "pdf": "https://arxiv.org/pdf/2508.21666.pdf", "abs": "https://arxiv.org/abs/2508.21666", "title": "Harnessing IoT and Generative AI for Weather-Adaptive Learning in Climate Resilience Education", "authors": ["Imran S. A. Khan", "Emmanuel G. Blanchard", "Sébastien George"], "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.LG", "cs.SE"], "comment": null, "summary": "This paper introduces the Future Atmospheric Conditions Training System\n(FACTS), a novel platform that advances climate resilience education through\nplace-based, adaptive learning experiences. FACTS combines real-time\natmospheric data collected by IoT sensors with curated resources from a\nKnowledge Base to dynamically generate localized learning challenges. Learner\nresponses are analyzed by a Generative AI powered server, which delivers\npersonalized feedback and adaptive support. Results from a user evaluation\nindicate that participants found the system both easy to use and effective for\nbuilding knowledge related to climate resilience. These findings suggest that\nintegrating IoT and Generative AI into atmospherically adaptive learning\ntechnologies holds significant promise for enhancing educational engagement and\nfostering climate awareness.", "AI": {"tldr": "The paper presents FACTS, a platform combining IoT data and Generative AI to enhance climate resilience education through personalized learning experiences.", "motivation": "To advance climate resilience education and improve engagement through adaptive learning technologies.", "method": "FACTS leverages real-time atmospheric data from IoT sensors alongside a curated Knowledge Base to create localized learning challenges, with a Generative AI system providing real-time analysis and feedback.", "result": "User evaluations showed that participants found FACTS easy to use and effective for learning, indicating its potential for educational engagement in climate resilience.", "conclusion": "Integrating IoT and Generative AI into learning technologies can significantly enhance educational experiences and awareness of climate issues.", "key_contributions": ["Introduction of a novel educational platform (FACTS) combining IoT and AI.", "Demonstration of effective personalized learning in climate education.", "User evaluation showing high usability and engagement."], "limitations": "The paper does not discuss the scalability of the platform or long-term impact on learner retention.", "keywords": ["climate resilience", "IoT", "Generative AI", "adaptive learning", "educational technology"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2508.21085", "pdf": "https://arxiv.org/pdf/2508.21085.pdf", "abs": "https://arxiv.org/abs/2508.21085", "title": "Granite Embedding R2 Models", "authors": ["Parul Awasthy", "Aashka Trivedi", "Yulong Li", "Meet Doshi", "Riyaz Bhat", "Vignesh P", "Vishwajeet Kumar", "Yushu Yang", "Bhavani Iyer", "Abraham Daniels", "Rudra Murthy", "Ken Barker", "Martin Franz", "Madison Lee", "Todd Ward", "Salim Roukos", "David Cox", "Luis Lastras", "Jaydeep Sen", "Radu Florian"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "We introduce the Granite Embedding R2 models, a comprehensive family of\nhigh-performance English encoder-based embedding models engineered for\nenterprise-scale dense retrieval applications. Building upon our\nfirst-generation release, these models deliver substantial improvements,\nincluding 16x expanded context length (8,192 tokens), state-of-the-art\nperformance across diverse retrieval domains - text, code, long-document\nsearch, multi-turn conversational, and tabular data - and measurable speed\nadvantages of 19-44\\% over leading competitors while maintaining superior\naccuracy. Our release encompasses both bi-encoder and cross-encoder\narchitectures, featuring a highly effective 22-layer retriever model and its\nefficient 12-layer counterpart, alongside a high-quality reranker model, all\ntrained exclusively on enterprise-appropriate data with comprehensive\ngovernance oversight. The models demonstrate exceptional versatility across\nstandard benchmarks, IBM-developed evaluation suites, and real-world enterprise\nuse cases, establishing new performance standards for open-source embedding\nmodels. In an era where retrieval speed and accuracy are paramount for\ncompetitive advantage, the Granite R2 models deliver a compelling combination\nof cutting-edge performance, enterprise-ready licensing, and transparent data\nprovenance that organizations require for mission-critical deployments. All\nmodels are publicly available under the Apache 2.0 license at\nhttps://huggingface.co/collections/ibm-granite, enabling unrestricted research\nand commercial use.", "AI": {"tldr": "Introduction of Granite Embedding R2 models for dense retrieval applications with substantial improvements in performance and versatility.", "motivation": "To provide high-performance embedding models for enterprise-scale dense retrieval applications, emphasizing speed and accuracy in diverse retrieval domains.", "method": "Release of bi-encoder and cross-encoder architectures including a 22-layer retriever model and a 12-layer efficient counterpart, trained on enterprise-specific data.", "result": "Granite R2 models show substantial performance improvements including 16x expanded context length, speed advantages of 19-44%, and superior accuracy across various benchmarks and real-world use cases.", "conclusion": "Granite R2 models set new performance standards for open-source embedding models, ensuring organizations have the necessary tools for critical data retrieval tasks.", "key_contributions": ["Introduction of Granite R2 models with significant performance improvements.", "Expansion of context length to 8,192 tokens.", "Commercially available models under Apache 2.0 license for unrestricted use."], "limitations": "", "keywords": ["embedding models", "dense retrieval", "machine learning", "enterprise applications", "open-source"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2508.21733", "pdf": "https://arxiv.org/pdf/2508.21733.pdf", "abs": "https://arxiv.org/abs/2508.21733", "title": "Developer Insights into Designing AI-Based Computer Perception Tools", "authors": ["Maya Guhan", "Meghan E. Hurley", "Eric A. Storch", "John Herrington", "Casey Zampella", "Julia Parish-Morris", "Gabriel Lázaro-Muñoz", "Kristin Kostick-Quenet"], "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": "15 pages", "summary": "Artificial intelligence (AI)-based computer perception (CP) technologies use\nmobile sensors to collect behavioral and physiological data for clinical\ndecision-making. These tools can reshape how clinical knowledge is generated\nand interpreted. However, effective integration of these tools into clinical\nworkflows depends on how developers balance clinical utility with user\nacceptability and trustworthiness. Our study presents findings from 20 in-depth\ninterviews with developers of AI-based CP tools. Interviews were transcribed\nand inductive, thematic analysis was performed to identify 4 key design\npriorities: 1) to account for context and ensure explainability for both\npatients and clinicians; 2) align tools with existing clinical workflows; 3)\nappropriately customize to relevant stakeholders for usability and\nacceptability; and 4) push the boundaries of innovation while aligning with\nestablished paradigms. Our findings highlight that developers view themselves\nas not merely technical architects but also ethical stewards, designing tools\nthat are both acceptable by users and epistemically responsible (prioritizing\nobjectivity and pushing clinical knowledge forward). We offer the following\nsuggestions to help achieve this balance: documenting how design choices around\ncustomization are made, defining limits for customization choices,\ntransparently conveying information about outputs, and investing in user\ntraining. Achieving these goals will require interdisciplinary collaboration\nbetween developers, clinicians, and ethicists.", "AI": {"tldr": "The paper discusses the integration of AI-based computer perception technologies in clinical settings, emphasizing the balance between clinical utility and user acceptability.", "motivation": "To explore how AI-based computer perception tools can effectively be integrated into clinical workflows by addressing developers' design priorities.", "method": "Conducted in-depth interviews with 20 developers of AI-based computer perception tools; analyzed transcripts with inductive thematic analysis.", "result": "Identified four key design priorities for developers: ensuring explainability, aligning with workflows, customizing for stakeholders, and innovating responsibly. Developers see themselves as ethical stewards in tool design.", "conclusion": "Interdisciplinary collaboration is crucial for achieving a balance between customization, user acceptability, and the advancement of clinical knowledge.", "key_contributions": ["Identified design priorities for AI-based clinical tools", "Emphasized the role of developers as ethical stewards", "Suggested strategies for balancing customization and usability"], "limitations": "", "keywords": ["AI", "clinical decision-making", "user acceptability", "clinical workflows", "interdisciplinary collaboration"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.21098", "pdf": "https://arxiv.org/pdf/2508.21098.pdf", "abs": "https://arxiv.org/abs/2508.21098", "title": "TrInk: Ink Generation with Transformer Network", "authors": ["Zezhong Jin", "Shubhang Desai", "Xu Chen", "Biyi Fang", "Zhuoyi Huang", "Zhe Li", "Chong-Xin Gan", "Xiao Tu", "Man-Wai Mak", "Yan Lu", "Shujie Liu"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "In this paper, we propose TrInk, a Transformer-based model for ink\ngeneration, which effectively captures global dependencies. To better\nfacilitate the alignment between the input text and generated stroke points, we\nintroduce scaled positional embeddings and a Gaussian memory mask in the\ncross-attention module. Additionally, we design both subjective and objective\nevaluation pipelines to comprehensively assess the legibility and style\nconsistency of the generated handwriting. Experiments demonstrate that our\nTransformer-based model achieves a 35.56\\% reduction in character error rate\n(CER) and an 29.66% reduction in word error rate (WER) on the IAM-OnDB dataset\ncompared to previous methods. We provide an demo page with handwriting samples\nfrom TrInk and baseline models at: https://akahello-a11y.github.io/trink-demo/", "AI": {"tldr": "TrInk is a Transformer-based model designed for ink generation that significantly improves handwriting legibility through enhanced alignment techniques.", "motivation": "The goal is to improve the quality of generated handwriting by effectively capturing global dependencies in ink generation.", "method": "The model uses scaled positional embeddings and a Gaussian memory mask in the cross-attention module to align input text with generated stroke points.", "result": "TrInk achieves a 35.56% reduction in character error rate (CER) and a 29.66% reduction in word error rate (WER) on the IAM-OnDB dataset compared to previous methods.", "conclusion": "The proposed methodology and model significantly enhance handwriting generation, as demonstrated by the performance metrics.", "key_contributions": ["Introduction of scaled positional embeddings", "Implementation of a Gaussian memory mask", "Comprehensive evaluation pipelines for handwriting quality assessment."], "limitations": "", "keywords": ["Transformer", "ink generation", "handwriting synthesis"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2508.21736", "pdf": "https://arxiv.org/pdf/2508.21736.pdf", "abs": "https://arxiv.org/abs/2508.21736", "title": "MicroLabVR: Interactive 3D Visualization of Simulated Spatiotemporal Microbiome Data in Virtual Reality", "authors": ["Simon Burbach", "Maria Maleshkova", "Florian Centler", "Tanja Joan Schmidt"], "categories": ["cs.HC", "cs.CE", "cs.GR", "q-bio.CB", "q-bio.MN"], "comment": null, "summary": "Microbiomes are a vital part of the human body, engaging in tasks like food\ndigestion and immune defense. Their structure and function must be understood\nin order to promote host health and facilitate swift recovery during disease.\nDue to the difficulties in experimentally studying these systems in situ, more\nresearch is being conducted in the field of mathematical modeling. Visualizing\nspatiotemporal data is challenging, and current tools that simulate microbial\ncommunities' spatial and temporal development often only provide limited\nfunctionalities, often requiring expert knowledge to generate useful results.\nTo overcome these limitations, we provide a user-friendly tool to interactively\nexplore spatiotemporal simulation data, called MicroLabVR, which transfers\nspatial data into virtual reality (VR) while following guidelines to enhance\nuser experience (UX). With MicroLabVR, users can import CSV datasets containing\npopulation growth, substance concentration development, and metabolic flux\ndistribution data. The implemented visualization methods allow users to\nevaluate the dataset in a VR environment interactively. MicroLabVR aims to\nimprove data analysis for the user by allowing the exploration of microbiome\ndata in their spatial context.", "AI": {"tldr": "MicroLabVR is a user-friendly VR tool for exploring spatiotemporal simulation data of microbiomes, facilitating analysis and enhancing user experience.", "motivation": "To better understand microbiomes for promoting health and recovery during disease, there is a need for improved tools that allow for the exploration of spatiotemporal data.", "method": "MicroLabVR utilizes virtual reality to visualize spatiotemporal data derived from CSV datasets, enabling interactive exploration of microbiome simulation results.", "result": "MicroLabVR enhances the ability of users to analyze and interact with microbiome data, providing a more intuitive understanding of complex datasets in a spatial context.", "conclusion": "The tool aims to bridge the gap in current visualization methods by allowing users to engage with microbiome data dynamically and intuitively, improving data analysis outcomes.", "key_contributions": ["User-friendly VR environment for microbiome data exploration", "Interactive visualization of spatiotemporal simulation data", "Integration of CSV data for comprehensive analysis"], "limitations": "", "keywords": ["microbiome", "virtual reality", "data visualization", "spatiotemporal", "user experience"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2508.21137", "pdf": "https://arxiv.org/pdf/2508.21137.pdf", "abs": "https://arxiv.org/abs/2508.21137", "title": "How Does Cognitive Bias Affect Large Language Models? A Case Study on the Anchoring Effect in Price Negotiation Simulations", "authors": ["Yoshiki Takenami", "Yin Jou Huang", "Yugo Murawaki", "Chenhui Chu"], "categories": ["cs.CL"], "comment": "work in progress", "summary": "Cognitive biases, well-studied in humans, can also be observed in LLMs,\naffecting their reliability in real-world applications. This paper investigates\nthe anchoring effect in LLM-driven price negotiations. To this end, we\ninstructed seller LLM agents to apply the anchoring effect and evaluated\nnegotiations using not only an objective metric but also a subjective metric.\nExperimental results show that LLMs are influenced by the anchoring effect like\nhumans. Additionally, we investigated the relationship between the anchoring\neffect and factors such as reasoning and personality. It was shown that\nreasoning models are less prone to the anchoring effect, suggesting that the\nlong chain of thought mitigates the effect. However, we found no significant\ncorrelation between personality traits and susceptibility to the anchoring\neffect. These findings contribute to a deeper understanding of cognitive biases\nin LLMs and to the realization of safe and responsible application of LLMs in\nsociety.", "AI": {"tldr": "This paper examines the anchoring effect in LLMs during price negotiations, revealing that LLMs exhibit cognitive biases similar to humans.", "motivation": "Understanding cognitive biases in LLMs is crucial for their reliable applications in real-world scenarios.", "method": "Experiments involved seller LLM agents negotiating prices while applying the anchoring effect, evaluated through objective and subjective metrics.", "result": "LLMs were found to be influenced by the anchoring effect, with reasoning models showing less susceptibility.", "conclusion": "The study enhances comprehension of cognitive biases in LLMs, highlighting important aspects for safe and responsible applications.", "key_contributions": ["Investigated cognitive biases in LLMs", "Showed LLMs are influenced by the anchoring effect", "Found reasoning models are less affected by cognitive biases"], "limitations": "No significant correlation found between personality traits and susceptibility to the anchoring effect.", "keywords": ["Cognitive Biases", "LLM", "Anchoring Effect", "Price Negotiations", "Reasoning Models"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.21143", "pdf": "https://arxiv.org/pdf/2508.21143.pdf", "abs": "https://arxiv.org/abs/2508.21143", "title": "Can Multimodal LLMs Solve the Basic Perception Problems of Percept-V?", "authors": ["Samrajnee Ghosh", "Naman Agarwal", "Hemanshu Garg", "Chinmay Mittal", "Mausam", "Parag Singla"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "The reasoning abilities of Multimodal Large Language Models (MLLMs) have\ngarnered a lot of attention in recent times, with advances made in frontiers\nlike coding, mathematics, and science. However, very limited experiments have\nbeen done to assess their performance in simple perception tasks performed over\nuncontaminated, generated images containing basic shapes and structures. To\naddress this issue, the paper introduces a dataset, Percept-V, containing a\ntotal of 7200 program-generated images equally divided into 30 categories, each\ntesting a combination of visual perception skills. Unlike previously proposed\ndatasets, Percept-V comprises very basic tasks of varying complexity that test\nthe perception abilities of MLLMs. This dataset is then tested on\nstate-of-the-art MLLMs like GPT-4o, Gemini, and Claude as well as Large\nReasoning Models (LRMs) like OpenAI o4-mini and DeepSeek R1 to gauge their\nperformance. Contrary to the evidence that MLLMs excel in many complex tasks,\nour experiments show a significant drop in the models' performance with\nincreasing problem complexity across all categories. An analysis of the\nperformances also reveals that the tested MLLMs exhibit a similar trend in\naccuracy across categories, testing a particular cognitive skill and find some\nskills to be more difficult than others.", "AI": {"tldr": "This paper introduces the Percept-V dataset, assessing Multimodal Large Language Models' (MLLMs) performance in basic perception tasks with program-generated images.", "motivation": "To evaluate the reasoning abilities of MLLMs in simple perception tasks due to limited prior experiments in this area.", "method": "The study introduces the Percept-V dataset with 7200 generated images across 30 categories, testing MLLMs like GPT-4o, Gemini, and Claude on various perception tasks.", "result": "MLLMs showed a significant drop in performance as task complexity increased, contradicting their previously established competence in complex tasks.", "conclusion": "The results demonstrate that MLLMs struggle with basic perception tasks more than expected, revealing specific cognitive skills that are more challenging for them.", "key_contributions": ["Introduction of the Percept-V dataset for perception task evaluation", "Assessment of MLLMs' performance on basic visual perception", "Identification of varying difficulty levels among cognitive skills tested"], "limitations": "Limited to simple visual perception tasks; results may not generalize to more complex reasoning or other modalities.", "keywords": ["Multimodal Large Language Models", "Perception tasks", "Dataset Percept-V"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.21628", "pdf": "https://arxiv.org/pdf/2508.21628.pdf", "abs": "https://arxiv.org/abs/2508.21628", "title": "Personality Matters: User Traits Predict LLM Preferences in Multi-Turn Collaborative Tasks", "authors": ["Sarfaroz Yunusov", "Kaige Chen", "Kazi Nishat Anwar", "Ali Emami"], "categories": ["cs.CL", "cs.HC"], "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "As Large Language Models (LLMs) increasingly integrate into everyday\nworkflows, where users shape outcomes through multi-turn collaboration, a\ncritical question emerges: do users with different personality traits\nsystematically prefer certain LLMs over others? We conducted a study with 32\nparticipants evenly distributed across four Keirsey personality types,\nevaluating their interactions with GPT-4 and Claude 3.5 across four\ncollaborative tasks: data analysis, creative writing, information retrieval,\nand writing assistance. Results revealed significant personality-driven\npreferences: Rationals strongly preferred GPT-4, particularly for goal-oriented\ntasks, while idealists favored Claude 3.5, especially for creative and\nanalytical tasks. Other personality types showed task-dependent preferences.\nSentiment analysis of qualitative feedback confirmed these patterns. Notably,\naggregate helpfulness ratings were similar across models, showing how\npersonality-based analysis reveals LLM differences that traditional evaluations\nmiss.", "AI": {"tldr": "Study evaluates how personality traits influence user preferences for LLMs through collaborative tasks.", "motivation": "Understanding how different personality traits affect user interactions with LLMs in collaborative workflows.", "method": "Study with 32 participants across four Keirsey personality types, assessing their interactions with GPT-4 and Claude 3.5 on tasks like data analysis and creative writing.", "result": "Rationals preferred GPT-4 for goal-oriented tasks; idealists liked Claude 3.5 for creativity. Preferences varied by personality type and task.", "conclusion": "Personality-based preferences reveal insights into LLM utility that traditional evaluations overlook.", "key_contributions": ["Demonstrates personality-driven preferences for LLMs in collaborative settings.", "Highlights the importance of accounting for user personality in LLM evaluations.", "Shows the commonality of helpfulness ratings despite varied preferences."], "limitations": "", "keywords": ["Large Language Models", "personality traits", "user interaction", "collaborative tasks", "GPT-4", "Claude 3.5"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.21148", "pdf": "https://arxiv.org/pdf/2508.21148.pdf", "abs": "https://arxiv.org/abs/2508.21148", "title": "A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers", "authors": ["Ming Hu", "Chenglong Ma", "Wei Li", "Wanghan Xu", "Jiamin Wu", "Jucheng Hu", "Tianbin Li", "Guohang Zhuang", "Jiaqi Liu", "Yingzhou Lu", "Ying Chen", "Chaoyang Zhang", "Cheng Tan", "Jie Ying", "Guocheng Wu", "Shujian Gao", "Pengcheng Chen", "Jiashi Lin", "Haitao Wu", "Lulu Chen", "Fengxiang Wang", "Yuanyuan Zhang", "Xiangyu Zhao", "Feilong Tang", "Encheng Su", "Junzhi Ning", "Xinyao Liu", "Ye Du", "Changkai Ji", "Cheng Tang", "Huihui Xu", "Ziyang Chen", "Ziyan Huang", "Jiyao Liu", "Pengfei Jiang", "Yizhou Wang", "Chen Tang", "Jianyu Wu", "Yuchen Ren", "Siyuan Yan", "Zhonghua Wang", "Zhongxing Xu", "Shiyan Su", "Shangquan Sun", "Runkai Zhao", "Zhisheng Zhang", "Yu Liu", "Fudi Wang", "Yuanfeng Ji", "Yanzhou Su", "Hongming Shan", "Chunmei Feng", "Jiahao Xu", "Jiangtao Yan", "Wenhao Tang", "Diping Song", "Lihao Liu", "Yanyan Huang", "Lequan Yu", "Bin Fu", "Shujun Wang", "Xiaomeng Li", "Xiaowei Hu", "Yun Gu", "Ben Fei", "Zhongying Deng", "Benyou Wang", "Yuewen Cao", "Minjie Shen", "Haodong Duan", "Jie Xu", "Yirong Chen", "Fang Yan", "Hongxia Hao", "Jielan Li", "Jiajun Du", "Yanbo Wang", "Imran Razzak", "Chi Zhang", "Lijun Wu", "Conghui He", "Zhaohui Lu", "Jinhai Huang", "Yihao Liu", "Fenghua Ling", "Yuqiang Li", "Aoran Wang", "Qihao Zheng", "Nanqing Dong", "Tianfan Fu", "Dongzhan Zhou", "Yan Lu", "Wenlong Zhang", "Jin Ye", "Jianfei Cai", "Wanli Ouyang", "Yu Qiao", "Zongyuan Ge", "Shixiang Tang", "Junjun He", "Chunfeng Song", "Lei Bai", "Bowen Zhou"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Scientific Large Language Models (Sci-LLMs) are transforming how knowledge is\nrepresented, integrated, and applied in scientific research, yet their progress\nis shaped by the complex nature of scientific data. This survey presents a\ncomprehensive, data-centric synthesis that reframes the development of Sci-LLMs\nas a co-evolution between models and their underlying data substrate. We\nformulate a unified taxonomy of scientific data and a hierarchical model of\nscientific knowledge, emphasizing the multimodal, cross-scale, and\ndomain-specific challenges that differentiate scientific corpora from general\nnatural language processing datasets. We systematically review recent Sci-LLMs,\nfrom general-purpose foundations to specialized models across diverse\nscientific disciplines, alongside an extensive analysis of over 270\npre-/post-training datasets, showing why Sci-LLMs pose distinct demands --\nheterogeneous, multi-scale, uncertainty-laden corpora that require\nrepresentations preserving domain invariance and enabling cross-modal\nreasoning. On evaluation, we examine over 190 benchmark datasets and trace a\nshift from static exams toward process- and discovery-oriented assessments with\nadvanced evaluation protocols. These data-centric analyses highlight persistent\nissues in scientific data development and discuss emerging solutions involving\nsemi-automated annotation pipelines and expert validation. Finally, we outline\na paradigm shift toward closed-loop systems where autonomous agents based on\nSci-LLMs actively experiment, validate, and contribute to a living, evolving\nknowledge base. Collectively, this work provides a roadmap for building\ntrustworthy, continually evolving artificial intelligence (AI) systems that\nfunction as a true partner in accelerating scientific discovery.", "AI": {"tldr": "This survey reviews the development of Scientific Large Language Models (Sci-LLMs) and their interaction with scientific data, proposing a unified taxonomy and analyzing model demands, benchmark datasets, and emerging solutions for improving scientific data representation and validation.", "motivation": "The paper addresses the transformative impact of Sci-LLMs on knowledge representation in scientific research and aims to understand the co-evolution of these models with their complex underlying data.", "method": "The survey formulates a unified taxonomy of scientific data and reviews recent Sci-LLMs across disciplines, alongside an analysis of pre-/post-training datasets and benchmark assessments.", "result": "The analysis reveals specific challenges in scientific data, highlighting the need for representations that preserve domain invariance and enable cross-modal reasoning, along with issues in data development and validation.", "conclusion": "The work advocates for a shift toward closed-loop systems with autonomous agents based on Sci-LLMs that actively contribute to scientific knowledge, thereby defining a roadmap for building trustworthy AI systems.", "key_contributions": ["Unified taxonomy of scientific data related to Sci-LLMs", "Comprehensive review of Sci-LLMs and their unique demands", "Proposed shift toward closed-loop systems for continuous knowledge evolution"], "limitations": "", "keywords": ["Scientific Large Language Models", "Data-centric AI", "Scientific data representation"], "importance_score": 9, "read_time_minutes": 30}}
{"id": "2405.07963", "pdf": "https://arxiv.org/pdf/2405.07963.pdf", "abs": "https://arxiv.org/abs/2405.07963", "title": "PyZoBot: A Platform for Conversational Information Extraction and Synthesis from Curated Zotero Reference Libraries through Advanced Retrieval-Augmented Generation", "authors": ["Suad Alshammari", "Lama Basalelah", "Walaa Abu Rukbah", "Ali Alsuhibani", "Dayanjan S. Wijesinghe"], "categories": ["cs.HC"], "comment": "10 pages, 2 figures. The code is provided in github and the link to\n  the repository is provided at the end of the publication", "summary": "The exponential growth of scientific literature has resulted in information\noverload, challenging researchers to effectively synthesize relevant\npublications. This paper explores the integration of traditional reference\nmanagement software with advanced computational techniques, including Large\nLanguage Models and Retrieval-Augmented Generation. We introduce PyZoBot, an\nAI-driven platform developed in Python, incorporating Zoteros reference\nmanagement with OpenAIs sophisticated LLMs. PyZoBot streamlines knowledge\nextraction and synthesis from extensive human-curated scientific literature\ndatabases. It demonstrates proficiency in handling complex natural language\nqueries, integrating data from multiple sources, and meticulously presenting\nreferences to uphold research integrity and facilitate further exploration. By\nleveraging LLMs, RAG, and human expertise through a curated library, PyZoBot\noffers an effective solution to manage information overload and keep pace with\nrapid scientific advancements. The development of such AI-enhanced tools\npromises significant improvements in research efficiency and effectiveness\nacross various disciplines.", "AI": {"tldr": "The paper presents PyZoBot, an AI-driven platform that integrates reference management with advanced techniques like LLMs and RAG to aid researchers in managing and synthesizing scientific literature effectively.", "motivation": "To address the challenges of information overload in scientific literature and improve research efficiency and effectiveness.", "method": "Development of PyZoBot, which combines Zotero reference management with OpenAI's LLMs for knowledge extraction and synthesis.", "result": "PyZoBot successfully manages complex natural language queries, integrates data from multiple sources, and maintains research integrity with precise reference presentation.", "conclusion": "AI-enhanced tools like PyZoBot can significantly improve how researchers manage information and keep up with rapid scientific advancements.", "key_contributions": ["Introduction of PyZoBot as a novel research tool", "Integration of LLMs with reference management", "Demonstration of improved knowledge extraction and synthesis capabilities"], "limitations": "", "keywords": ["AI", "Large Language Models", "Research Management", "Information Overload", "Scientific Literature"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.21164", "pdf": "https://arxiv.org/pdf/2508.21164.pdf", "abs": "https://arxiv.org/abs/2508.21164", "title": "Quantifying Label-Induced Bias in Large Language Model Self- and Cross-Evaluations", "authors": ["Muskan Saraf", "Sajjad Rezvani Boroujeni", "Justin Beaudry", "Hossein Abedi", "Tom Bush"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly used to evaluate outputs, yet\ntheir judgments may be influenced. This study examines bias in self- and\ncross-model evaluations by ChatGPT, Gemini, and Claude under four conditions:\nno labels, true labels, and two false-label scenarios. Blog posts authored by\neach model were evaluated by all three using both overall preference voting and\nquality ratings for Coherence, Informativeness, and Conciseness, with all\nscores expressed as percentages for direct comparison. Results reveal striking\nasymmetries: the \"Claude\" label consistently boosts scores, while the \"Gemini\"\nlabel consistently depresses them, regardless of actual content. False labels\nfrequently reversed rankings, producing shifts of up to 50 percentage points in\npreference votes and up to 12 percentage points in converted quality ratings.\nGemini's self-scores collapsed under true labels, while Claude's\nself-preference intensified. These findings show that perceived model identity\ncan heavily distort high-level judgments and subtly influence detailed quality\nratings, underscoring the need for blind or multimodel evaluation protocols to\nensure fairness in LLM benchmarking.", "AI": {"tldr": "This study investigates bias in evaluations made by LLMs (ChatGPT, Gemini, and Claude), showing that model identity significantly affects judgment outcomes.", "motivation": "To address biases in large language model evaluations and improve the fairness of benchmarking protocols.", "method": "The study examined self- and cross-model evaluations under different labeling scenarios using blog posts generated by the models, comparing preference votes and quality ratings among them.", "result": "Model identity impacts evaluation scores significantly; Claude's label increased scores, whereas Gemini's label decreased them, with notable effects under false-label conditions.", "conclusion": "The findings highlight the need for blind evaluation methods in LLM benchmarking to mitigate bias stemming from perceived model identity.", "key_contributions": ["Identification of bias in LLM evaluations based on model identity", "Effect of labeling on performance rankings", "Recommendation for blind or multimodel evaluation protocols"], "limitations": "The study primarily focuses on three models and a specific set of evaluation metrics; results may not generalize to other models or contexts.", "keywords": ["bias", "large language models", "evaluation", "benchmarking", "HCI"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2501.14179", "pdf": "https://arxiv.org/pdf/2501.14179.pdf", "abs": "https://arxiv.org/abs/2501.14179", "title": "AI Chatbots as Professional Service Agents: Developing a Professional Identity", "authors": ["Wenwen Li", "Kangwei Shi", "Yidong Chai"], "categories": ["cs.HC"], "comment": null, "summary": "With the rapid expansion of large language model (LLM) applications, there is\nan emerging shift in the role of LLM-based AI chatbots from serving merely as\ngeneral inquiry tools to acting as professional service agents. However,\ncurrent studies often overlook a critical aspect of professional service\nagents: the act of communicating in a manner consistent with their professional\nidentities. This is of particular importance in the healthcare sector, where\neffective communication with patients is essential for achieving professional\ngoals, such as promoting patient well-being by encouraging healthy behaviors.\nTo bridge this gap, we propose LAPI (LLM-based Agent with a Professional\nIdentity), a novel framework for designing professional service agent tailored\nfor medical question-and-answer (Q\\&A) services, ensuring alignment with a\nspecific professional identity. Our method includes a theory-guided task\nplanning process that decomposes complex professional tasks into manageable\nsubtasks aligned with professional objectives and a pragmatic entropy method\ndesigned to generate professional and ethical responses with low uncertainty.\nExperiments on various LLMs show that the proposed approach outperforms\nbaseline methods, including few-shot prompting, chain-of-thought prompting,\nacross key metrics such as fluency, naturalness, empathy, patient-centricity,\nand ROUGE-L scores. Additionally, the ablation study underscores the\ncontribution of each component to the overall effectiveness of the approach.", "AI": {"tldr": "This paper introduces LAPI, a framework for LLM-based professional service agents in healthcare, focusing on communication consistent with professional identities.", "motivation": "The need for LLM-based AI chatbots to communicate effectively in healthcare, aligning with professional identities.", "method": "The framework includes a theory-guided task planning process and a pragmatic entropy method for generating ethical responses.", "result": "Experiments demonstrate that LAPI outperforms existing methods on metrics such as fluency, empathy, and patient-centricity.", "conclusion": "LAPI effectively ensures that LLMs communicate in ways that promote patient well-being, enhancing their role as professional service agents.", "key_contributions": ["Introduction of LAPI framework for professional service agents in healthcare", "Theory-guided task planning process for managing professional tasks", "Pragmatic entropy method for generating professional responses"], "limitations": "", "keywords": ["Large language models", "Healthcare communication", "Professional identity"], "importance_score": 10, "read_time_minutes": 15}}
{"id": "2508.21184", "pdf": "https://arxiv.org/pdf/2508.21184.pdf", "abs": "https://arxiv.org/abs/2508.21184", "title": "BED-LLM: Intelligent Information Gathering with LLMs and Bayesian Experimental Design", "authors": ["Deepro Choudhury", "Sinead Williamson", "Adam Goliński", "Ning Miao", "Freddie Bickford Smith", "Michael Kirchhof", "Yizhe Zhang", "Tom Rainforth"], "categories": ["cs.CL", "cs.AI", "stat.ML"], "comment": null, "summary": "We propose a general-purpose approach for improving the ability of Large\nLanguage Models (LLMs) to intelligently and adaptively gather information from\na user or other external source using the framework of sequential Bayesian\nexperimental design (BED). This enables LLMs to act as effective multi-turn\nconversational agents and interactively interface with external environments.\nOur approach, which we call BED-LLM (Bayesian Experimental Design with Large\nLanguage Models), is based on iteratively choosing questions or queries that\nmaximize the expected information gain (EIG) about the task of interest given\nthe responses gathered previously. We show how this EIG can be formulated in a\nprincipled way using a probabilistic model derived from the LLM's belief\ndistribution and provide detailed insights into key decisions in its\nconstruction. Further key to the success of BED-LLM are a number of specific\ninnovations, such as a carefully designed estimator for the EIG, not solely\nrelying on in-context updates for conditioning on previous responses, and a\ntargeted strategy for proposing candidate queries. We find that BED-LLM\nachieves substantial gains in performance across a wide range of tests based on\nthe 20-questions game and using the LLM to actively infer user preferences,\ncompared to direct prompting of the LLM and other adaptive design strategies.", "AI": {"tldr": "The paper introduces BED-LLM, an approach for enhancing LLMs' capabilities in engaging users through interactive, adaptive questioning based on Bayesian experimental design.", "motivation": "To improve how LLMs gather information from users by implementing a structured methodology that supports multi-turn interactions.", "method": "The BED-LLM framework uses sequential Bayesian experimental design to select questions that maximize expected information gain (EIG) from user responses.", "result": "BED-LLM demonstrated significant performance improvements in tasks like the 20-questions game and user preference inference over existing methods.", "conclusion": "The approach provides a robust mechanism for LLMs to interactively gather information, outperforming direct prompting and other adaptive strategies.", "key_contributions": ["Development of the BED-LLM framework for LLMs", "Innovative estimator for expected information gain (EIG)", "Method for generating candidate queries based on previous responses"], "limitations": "", "keywords": ["Large Language Models", "Bayesian Experimental Design", "information gain", "conversational agents", "adaptive questioning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.21201", "pdf": "https://arxiv.org/pdf/2508.21201.pdf", "abs": "https://arxiv.org/abs/2508.21201", "title": "Improving Aviation Safety Analysis: Automated HFACS Classification Using Reinforcement Learning with Group Relative Policy Optimization", "authors": ["Arash Ahmadi", "Sarah Sharif", "Yaser Banad"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Analyzing the human factors behind aviation accidents is crucial for\npreventing future incidents, yet traditional methods using the Human Factors\nAnalysis and Classification System (HFACS) are limited by scalability and\nconsistency. To address this, we introduce an automated HFACS classification\nframework for aviation safety analysis that utilizes Reinforcement Learning\nwith Group Relative Policy Optimization (GRPO) to fine-tune a Llama-3.1 8B\nlanguage model. Our approach incorporates a multi-component reward system\ntailored for aviation safety analysis and integrates synthetic data generation\nto overcome class imbalance in accident datasets. The resulting GRPO-optimized\nmodel achieved noticeable performance gains, including a 350% increase in exact\nmatch accuracy (from 0.0400 to 0.1800) and an improved partial match accuracy\nof 0.8800. Significantly, our specialized model outperforms state-of-the-art\nLLMs (Large Language Models), including GPT-5-mini and Gemini-2.5-fiash, on key\nmetrics. This research also proposes exact match accuracy in multi-label HFACS\nclassification problem as a new benchmarking methodology to evaluate the\nadvanced reasoning capabilities of language models. Ultimately, our work\nvalidates that smaller, domain-optimized models can provide a computationally\nefficient and better solution for critical safety analysis. This approach makes\npowerful, low-latency deployment on resource-constrained edge devices feasible.", "AI": {"tldr": "This paper presents an automated classification framework using Reinforcement Learning to improve aviation safety analysis.", "motivation": "Addressing the limitations of traditional HFACS methods in scalability and consistency for aviation accident analysis.", "method": "An automated HFACS classification framework is proposed that utilizes Reinforcement Learning with Group Relative Policy Optimization to fine-tune a Llama-3.1 8B language model.", "result": "The GRPO-optimized model achieved a 350% increase in exact match accuracy and improved partial match accuracy of 0.8800, outperforming state-of-the-art LLMs.", "conclusion": "Optimizing smaller, domain-specific models can enhance efficiency and performance for critical safety analysis on edge devices.", "key_contributions": ["Development of a novel automated HFACS classification framework using Reinforcement Learning.", "Introduction of a multi-component reward system for aviation safety analysis.", "Proposed exact match accuracy as a new benchmarking methodology for evaluating language models."], "limitations": "The focus on aviation safety may limit the generalizability of the findings to other domains.", "keywords": ["Aviation Safety", "Reinforcement Learning", "Language Models", "Human Factors Analysis", "Artificial Intelligence"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2508.21206", "pdf": "https://arxiv.org/pdf/2508.21206.pdf", "abs": "https://arxiv.org/abs/2508.21206", "title": "Enhancing Robustness of Autoregressive Language Models against Orthographic Attacks via Pixel-based Approach", "authors": ["Han Yang", "Jian Lan", "Yihong Liu", "Hinrich Schütze", "Thomas Seidl"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Autoregressive language models are vulnerable to orthographic attacks, where\ninput text is perturbed with characters from multilingual alphabets, leading to\nsubstantial performance degradation. This vulnerability primarily stems from\nthe out-of-vocabulary issue inherent in subword tokenizers and their\nembeddings. To address this limitation, we propose a pixel-based generative\nlanguage model that replaces the text-based embeddings with pixel-based\nrepresentations by rendering words as individual images. This design provides\nstronger robustness to noisy inputs, while an extension of compatibility to\nmultilingual text across diverse writing systems. We evaluate the proposed\nmethod on the multilingual LAMBADA dataset, WMT24 dataset and the SST-2\nbenchmark, demonstrating both its resilience to orthographic noise and its\neffectiveness in multilingual settings.", "AI": {"tldr": "This paper introduces a pixel-based generative language model to enhance robustness against orthographic attacks in autoregressive models by using image representations of words.", "motivation": "Autoregressive language models face vulnerabilities to orthographic attacks due to out-of-vocabulary issues from subword tokenizers. The goal is to solve this problem to improve model robustness and multilingual compatibility.", "method": "The proposed model replaces traditional text-based embeddings with pixel-based representations by rendering words as images. This enables the model to handle orthographic noise better and improves multilingual processing.", "result": "The model was evaluated on the multilingual LAMBADA dataset, WMT24 dataset, and SST-2 benchmark, showing resilience to orthographic noise and effectiveness across diverse writing systems.", "conclusion": "The pixel-based generative language model provides a promising solution for enhancing the robustness of language models against orthographic attacks while supporting multilingual text.", "key_contributions": ["Introduction of pixel-based embeddings for language models", "Demonstrated resilience to orthographic noise", "Evaluation on multilingual datasets showing improved performance"], "limitations": "", "keywords": ["multilingual", "orthographic attacks", "pixel-based language model"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.21210", "pdf": "https://arxiv.org/pdf/2508.21210.pdf", "abs": "https://arxiv.org/abs/2508.21210", "title": "Do Self-Supervised Speech Models Exhibit the Critical Period Effects in Language Acquisition?", "authors": ["Yurie Koga", "Shunsuke Kando", "Yusuke Miyao"], "categories": ["cs.CL"], "comment": "Accepted to ASRU 2025", "summary": "This paper investigates whether the Critical Period (CP) effects in human\nlanguage acquisition are observed in self-supervised speech models (S3Ms). CP\neffects refer to greater difficulty in acquiring a second language (L2) with\ndelayed L2 exposure onset, and greater retention of their first language (L1)\nwith delayed L1 exposure offset. While previous work has studied these effects\nusing textual language models, their presence in speech models remains\nunderexplored despite the central role of spoken language in human language\nacquisition. We train S3Ms with varying L2 training onsets and L1 training\noffsets on child-directed speech and evaluate their phone discrimination\nperformance. We find that S3Ms do not exhibit clear evidence of either CP\neffects in terms of phonological acquisition. Notably, models with delayed L2\nexposure onset tend to perform better on L2 and delayed L1 exposure offset\nleads to L1 forgetting.", "AI": {"tldr": "The paper explores Critical Period effects in self-supervised speech models, finding no evidence of phonological acquisition issues typically seen in human language acquisition.", "motivation": "To investigate the existence of Critical Period effects in self-supervised speech models, which have not been extensively studied despite their relevance to human language acquisition.", "method": "The authors trained self-supervised speech models (S3Ms) with different second language (L2) training onsets and first language (L1) training offsets on child-directed speech, evaluating their phone discrimination performance.", "result": "The study concluded that S3Ms do not show significant evidence for Critical Period effects, with delayed L2 exposure leading to better performance and delayed L1 exposure causing L1 forgetting.", "conclusion": "The findings suggest that self-supervised speech models may not reflect the same challenges in language acquisition faced by humans regarding Critical Period effects.", "key_contributions": ["First investigation of Critical Period effects in self-supervised speech models", "Demonstrated that S3Ms do not exhibit significant phonological acquisition challenges", "Provided insights into model performance based on training onset and offset conditions"], "limitations": "The findings are specific to phone discrimination performance and may not generalize to all aspects of language acquisition.", "keywords": ["Critical Period", "self-supervised speech models", "language acquisition", "phonological performance", "child-directed speech"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2508.21228", "pdf": "https://arxiv.org/pdf/2508.21228.pdf", "abs": "https://arxiv.org/abs/2508.21228", "title": "Decoding Memories: An Efficient Pipeline for Self-Consistency Hallucination Detection", "authors": ["Weizhi Gao", "Xiaorui Liu", "Feiyi Wang", "Dan Lu", "Junqi Yin"], "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, under review", "summary": "Large language models (LLMs) have demonstrated impressive performance in both\nresearch and real-world applications, but they still struggle with\nhallucination. Existing hallucination detection methods often perform poorly on\nsentence-level generation or rely heavily on domain-specific knowledge. While\nself-consistency approaches help address these limitations, they incur high\ncomputational costs due to repeated generation. In this paper, we conduct the\nfirst study on identifying redundancy in self-consistency methods, manifested\nas shared prefix tokens across generations, and observe that non-exact-answer\ntokens contribute minimally to the semantic content. Based on these insights,\nwe propose a novel Decoding Memory Pipeline (DMP) that accelerates generation\nthrough selective inference and annealed decoding. Being orthogonal to the\nmodel, dataset, decoding strategy, and self-consistency baseline, our DMP\nconsistently improves the efficiency of multi-response generation and holds\npromise for extension to alignment and reasoning tasks. Extensive experiments\nshow that our method achieves up to a 3x speedup without sacrificing AUROC\nperformance.", "AI": {"tldr": "This paper proposes a Decoding Memory Pipeline (DMP) to improve generation efficiency in large language models by reducing redundancy in self-consistency methods.", "motivation": "Existing methods for hallucination detection in LLMs struggle with efficiency and accuracy, particularly in sentence-level generation.", "method": "The authors conducted a study to identify redundancy in self-consistency methods, focusing on shared prefix tokens. They introduced a DMP that uses selective inference and annealed decoding to improve efficiency.", "result": "The DMP achieves up to a 3x speedup in multi-response generation while maintaining AUROC performance, indicating improved efficiency without sacrificing accuracy.", "conclusion": "The proposed DMP can enhance LLM performance in multi-response generation and has potential applications in alignment and reasoning tasks.", "key_contributions": ["Introduction of a Decoding Memory Pipeline (DMP) for LLMs", "Identification of redundancy in self-consistency methods", "Demonstration of significant efficiency improvements in generation"], "limitations": "The approach needs further validation in diverse real-world applications and different model architectures.", "keywords": ["Large Language Models", "Hallucination Detection", "Decoding Memory Pipeline", "Self-Consistency", "Multi-Response Generation"], "importance_score": 8, "read_time_minutes": 14}}
{"id": "2411.04090", "pdf": "https://arxiv.org/pdf/2411.04090.pdf", "abs": "https://arxiv.org/abs/2411.04090", "title": "A Collaborative Content Moderation Framework for Toxicity Detection based on Conformalized Estimates of Annotation Disagreement", "authors": ["Guillermo Villate-Castillo", "Javier Del Ser", "Borja Sanz"], "categories": ["cs.CL", "cs.AI", "cs.HC", "68T50 (Primary) 68T37 (Secondary)", "I.2.7; I.2.1"], "comment": "35 pages, 1 figure", "summary": "Content moderation typically combines the efforts of human moderators and\nmachine learning models. However, these systems often rely on data where\nsignificant disagreement occurs during moderation, reflecting the subjective\nnature of toxicity perception. Rather than dismissing this disagreement as\nnoise, we interpret it as a valuable signal that highlights the inherent\nambiguity of the content,an insight missed when only the majority label is\nconsidered. In this work, we introduce a novel content moderation framework\nthat emphasizes the importance of capturing annotation disagreement. Our\napproach uses multitask learning, where toxicity classification serves as the\nprimary task and annotation disagreement is addressed as an auxiliary task.\nAdditionally, we leverage uncertainty estimation techniques, specifically\nConformal Prediction, to account for both the ambiguity in comment annotations\nand the model's inherent uncertainty in predicting toxicity and\ndisagreement.The framework also allows moderators to adjust thresholds for\nannotation disagreement, offering flexibility in determining when ambiguity\nshould trigger a review. We demonstrate that our joint approach enhances model\nperformance, calibration, and uncertainty estimation, while offering greater\nparameter efficiency and improving the review process in comparison to\nsingle-task methods.", "AI": {"tldr": "We present a novel content moderation framework that incorporates annotation disagreement as a valuable signal, using multitask learning and Conformal Prediction techniques to enhance model performance and review processes.", "motivation": "The subjective nature of toxicity perception in content moderation leads to significant disagreement, which is typically disregarded. We seek to utilize this disagreement as a signal to improve moderation processes.", "method": "Our methodology involves using multitask learning where toxicity classification is the primary task and annotation disagreement is an auxiliary task. We also implement uncertainty estimation via Conformal Prediction to manage annotation ambiguity and model uncertainty.", "result": "Our approach improves calibration, enhances model performance, and increases parameter efficiency, leading to a superior review process compared to traditional single-task methods.", "conclusion": "By acknowledging and incorporating annotation disagreement, our framework provides a more flexible and effective approach to content moderation.", "key_contributions": ["Integration of annotation disagreement as a signal in content moderation", "Multitask learning framework for better model performance", "Application of Conformal Prediction for uncertainty estimation"], "limitations": "The system may rely on the quality of the initial annotated dataset and how well disagreement signals are interpreted by moderators.", "keywords": ["content moderation", "toxic content", "multitask learning", "uncertainty estimation", "Conformal Prediction"], "importance_score": 7, "read_time_minutes": 35}}
{"id": "2508.21290", "pdf": "https://arxiv.org/pdf/2508.21290.pdf", "abs": "https://arxiv.org/abs/2508.21290", "title": "Efficient Code Embeddings from Code Generation Models", "authors": ["Daria Kryvosheieva", "Saba Sturua", "Michael Günther", "Scott Martens", "Han Xiao"], "categories": ["cs.CL", "cs.AI", "cs.IR", "68T50", "I.2.7"], "comment": "9 pages, table and evaluations 5-9", "summary": "jina-code-embeddings is a novel code embedding model suite designed to\nretrieve code from natural language queries, perform technical\nquestion-answering, and identify semantically similar code snippets across\nprogramming languages. It makes innovative use of an autoregressive backbone\npre-trained on both text and code, generating embeddings via last-token\npooling. We outline the training recipe and demonstrate state-of-the-art\nperformance despite the relatively small size of the models, validating this\napproach to code embedding model construction.", "AI": {"tldr": "Introduction of jina-code-embeddings, a model suite for code retrieval and question-answering based on natural language queries.", "motivation": "To improve code retrieval and semantic similarity identification across programming languages using embeddings during technical question-answering.", "method": "An autoregressive backbone pre-trained on both text and code is utilized, generating embeddings through last-token pooling.", "result": "Achieves state-of-the-art performance with a relatively small model size, demonstrating the effectiveness of the proposed method for code embeddings.", "conclusion": "The approach to constructing code embedding models is validated due to its performance outcomes with limited model size.", "key_contributions": ["Introduction of jina-code-embeddings for enhanced code retrieval", "Utilization of last-token pooling for embedding generation", "Demonstration of state-of-the-art performance with small models"], "limitations": "", "keywords": ["code embeddings", "natural language queries", "technical question-answering", "semantically similar code snippets", "programming languages"], "importance_score": 6, "read_time_minutes": 9}}
{"id": "2508.21294", "pdf": "https://arxiv.org/pdf/2508.21294.pdf", "abs": "https://arxiv.org/abs/2508.21294", "title": "BLUEX Revisited: Enhancing Benchmark Coverage with Automatic Captioning", "authors": ["João Guilherme Alves Santos", "Giovana Kerche Bonás", "Thales Sales Almeida"], "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 5 figures, 2 tables", "summary": "With the growing capabilities of Large Language Models (LLMs), there is an\nincreasing need for robust evaluation methods, especially in multilingual and\nnon-English contexts. We present an updated version of the BLUEX dataset, now\nincluding 2024-2025 exams and automatically generated image captions using\nstate-of-the-art models, enhancing its relevance for data contamination studies\nin LLM pretraining. Captioning strategies increase accessibility to text-only\nmodels by more than 40%, producing 1,422 usable questions, more than doubling\nthe number in the original BLUEX. We evaluated commercial and open-source LLMs\nand their ability to leverage visual context through captions.", "AI": {"tldr": "This paper updates the BLUEX dataset to improve evaluation methods for Large Language Models (LLMs), enhancing accessibility and data contamination studies in multilingual contexts.", "motivation": "There is an increasing need for robust evaluation methods for LLMs, particularly in multilingual and non-English settings, due to their growing capabilities.", "method": "An updated version of the BLUEX dataset is presented, which includes 2024-2025 exams and automatically generated image captions to facilitate accessibility and evaluation analysis.", "result": "The updated dataset contains 1,422 usable questions, significantly surpassing the original count, and shows that captioning strategies improve text-only model accessibility by over 40%.", "conclusion": "The enhancements to the BLUEX dataset allow for better evaluation of LLMs, particularly in leveraging visual context through captions, thereby supporting more effective studies on data contamination in pretraining.", "key_contributions": ["Updated BLUEX dataset with new exams and image captions", "Increased number of usable questions more than double the original", "Evaluated LLMs' ability to leverage visual context through captions"], "limitations": "", "keywords": ["Large Language Models", "BLUEX", "dataset", "image captions", "evaluation methods"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2508.21377", "pdf": "https://arxiv.org/pdf/2508.21377.pdf", "abs": "https://arxiv.org/abs/2508.21377", "title": "Challenges and Applications of Large Language Models: A Comparison of GPT and DeepSeek family of models", "authors": ["Shubham Sharma", "Sneha Tuli", "Narendra Badam"], "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50, 68T07", "I.2.7; I.2.6; H.3.3"], "comment": "18 pages, 7 figures", "summary": "Large Language Models (LLMs) are transforming AI across industries, but their\ndevelopment and deployment remain complex. This survey reviews 16 key\nchallenges in building and using LLMs and examines how these challenges are\naddressed by two state-of-the-art models with unique approaches: OpenAI's\nclosed source GPT-4o (May 2024 update) and DeepSeek-V3-0324 (March 2025), a\nlarge open source Mixture-of-Experts model. Through this comparison, we\nshowcase the trade-offs between closed source models (robust safety, fine-tuned\nreliability) and open source models (efficiency, adaptability). We also explore\nLLM applications across different domains (from chatbots and coding tools to\nhealthcare and education), highlighting which model attributes are best suited\nfor each use case. This article aims to guide AI researchers, developers, and\ndecision-makers in understanding current LLM capabilities, limitations, and\nbest practices.", "AI": {"tldr": "This survey reviews 16 challenges in developing and using Large Language Models (LLMs) and compares two models: OpenAI's GPT-4o and DeepSeek-V3-0324, highlighting their trade-offs and applications.", "motivation": "To understand the complexities involved in the development and deployment of Large Language Models (LLMs) and to provide insights to AI researchers and developers.", "method": "A survey of 16 key challenges faced when building LLMs is presented, along with a comparison between OpenAI's closed-source model (GPT-4o) and an open-source Mixture-of-Experts model (DeepSeek-V3-0324).", "result": "The article outlines the trade-offs between closed source models, which offer robust safety and fine-tuned reliability, and open source models, known for their efficiency and adaptability.", "conclusion": "The paper provides valuable guidance for AI professionals on the current capabilities, limitations, and best practices of LLMs across various applications, including healthcare and education.", "key_contributions": ["Review of 16 challenges in LLM development", "Comparative analysis of GPT-4o and DeepSeek-V3-0324", "Insights on LLM applications in diverse domains"], "limitations": "", "keywords": ["Large Language Models", "GPT-4o", "DeepSeek-V3-0324", "Machine Learning", "AI applications"], "importance_score": 9, "read_time_minutes": 18}}
{"id": "2508.21382", "pdf": "https://arxiv.org/pdf/2508.21382.pdf", "abs": "https://arxiv.org/abs/2508.21382", "title": "Normality and the Turing Test", "authors": ["Alexandre Kabbach"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper proposes to revisit the Turing test through the concept of\nnormality. Its core argument is that the statistical interpretation of the\nnormal--understood as the average both in the normative and mathematical sense\nof the term--proves useful for understanding the Turing test in at least two\nways. First, in the sense that the Turing test targets normal/average rather\nthan exceptional human intelligence, so that successfully passing the test\nrequires building machines that \"make mistakes\" and display imperfect behavior\njust like normal/average humans. Second, in the sense that the Turing test is a\nstatistical test where judgments of intelligence are never carried out by a\nsingle \"average\" judge (understood as non-expert) but always by a full jury. As\nsuch, the notion of \"average human interrogator\" that Turing talks about in his\noriginal paper should be understood primarily as referring to a mathematical\nabstraction made of the normalized aggregate of individual judgments of\nmultiple judges. In short, this paper argues that the Turing test is a test of\nnormal intelligence as assessed by a normal judge characterizing the average\njudgment of a pool of human interrogators. Its conclusions are twofold. First,\nit argues that large language models such as ChatGPT are unlikely to pass the\nTuring test as those models precisely target exceptional rather than\nnormal/average human intelligence. As such, they constitute models of what it\nproposes to call artificial smartness rather than artificial intelligence per\nse. Second, it argues that the core question of whether the Turing test can\ncontribute anything to the understanding of human cognition is that of whether\nthe human mind is really reducible to the normal/average mind--a question which\nlargely extends beyond the Turing test itself and questions the conceptual\nunderpinnings of the normalist paradigm it belongs to.", "AI": {"tldr": "This paper revisits the Turing test, arguing it targets normal human intelligence as interpreted through a statistical lens, and claims that large language models aim for exceptional intelligence, which may hinder their ability to pass the test.", "motivation": "To provide a deeper understanding of the Turing test through the concept of normality and its implications for artificial intelligence assessment.", "method": "The paper analyzes the Turing test from a statistical perspective, highlighting how it measures normal intelligence via judgments from a jury of non-expert human interrogators.", "result": "It concludes that large language models, such as ChatGPT, are designed for exceptional intelligence and likely won't pass the Turing test because they don't embody normal human intelligence characteristics.", "conclusion": "The findings suggest a fundamental question on whether the human mind can be understood as equivalent to a normal/average mind, challenging the conceptual fundamentals of the normalist paradigm.", "key_contributions": ["Reinterprets the Turing test through the statistical concept of normality.", "Argues that large language models like ChatGPT may not pass the Turing test due to their focus on exceptional intelligence.", "Raises questions about the relationship between normality and human cognition in the context of AI."], "limitations": "", "keywords": ["Turing Test", "Normality", "Artificial Intelligence", "Human Cognition", "Language Models"], "importance_score": 4, "read_time_minutes": 20}}
{"id": "2508.21389", "pdf": "https://arxiv.org/pdf/2508.21389.pdf", "abs": "https://arxiv.org/abs/2508.21389", "title": "AllSummedUp: un framework open-source pour comparer les metriques d'evaluation de resume", "authors": ["Tanguy Herserant", "Vincent Guigue"], "categories": ["cs.CL", "cs.AI"], "comment": "in French language", "summary": "This paper investigates reproducibility challenges in automatic text\nsummarization evaluation. Based on experiments conducted across six\nrepresentative metrics ranging from classical approaches like ROUGE to recent\nLLM-based methods (G-Eval, SEval-Ex), we highlight significant discrepancies\nbetween reported performances in the literature and those observed in our\nexperimental setting. We introduce a unified, open-source framework, applied to\nthe SummEval dataset and designed to support fair and transparent comparison of\nevaluation metrics. Our results reveal a structural trade-off: metrics with the\nhighest alignment with human judgments tend to be computationally intensive and\nless stable across runs. Beyond comparative analysis, this study highlights key\nconcerns about relying on LLMs for evaluation, stressing their randomness,\ntechnical dependencies, and limited reproducibility. We advocate for more\nrobust evaluation protocols including exhaustive documentation and\nmethodological standardization to ensure greater reliability in automatic\nsummarization assessment.", "AI": {"tldr": "This paper examines reproducibility issues in automatic text summarization evaluation, revealing discrepancies between existing literature and experimental findings. It introduces a framework for transparency and makes recommendations for better evaluation practices.", "motivation": "To address the reproducibility challenges in automatic text summarization evaluation and highlight discrepancies in reported performance metrics.", "method": "Experiments were conducted across six summarization evaluation metrics, including classical approaches (ROUGE) and recent LLM-based methods (G-Eval, SEval-Ex), using an open-source framework applied to the SummEval dataset.", "result": "The study found significant discrepancies between literature reports and experimental results, revealing that metrics aligning with human judgments are computationally intensive and less stable.", "conclusion": "The paper calls for more robust evaluation protocols and methodological standardization to improve reliability in summarization assessments.", "key_contributions": ["Introduction of a unified, open-source evaluation framework", "Identification of discrepancies in summarization metrics performance", "Recommendations for improved evaluation methodologies"], "limitations": "The study emphasizes the randomness and technical dependencies of LLM-based metrics, as well as their limited reproducibility.", "keywords": ["automatic text summarization", "evaluation metrics", "reproducibility", "LLM", "methodological standardization"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.21422", "pdf": "https://arxiv.org/pdf/2508.21422.pdf", "abs": "https://arxiv.org/abs/2508.21422", "title": "Automatic Reviewers Fail to Detect Faulty Reasoning in Research Papers: A New Counterfactual Evaluation Framework", "authors": ["Nils Dycke", "Iryna Gurevych"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have great potential to accelerate and support\nscholarly peer review and are increasingly used as fully automatic review\ngenerators (ARGs). However, potential biases and systematic errors may pose\nsignificant risks to scientific integrity; understanding the specific\ncapabilities and limitations of state-of-the-art ARGs is essential. We focus on\na core reviewing skill that underpins high-quality peer review: detecting\nfaulty research logic. This involves evaluating the internal consistency\nbetween a paper's results, interpretations, and claims. We present a fully\nautomated counterfactual evaluation framework that isolates and tests this\nskill under controlled conditions. Testing a range of ARG approaches, we find\nthat, contrary to expectation, flaws in research logic have no significant\neffect on their output reviews. Based on our findings, we derive three\nactionable recommendations for future work and release our counterfactual\ndataset and evaluation framework publicly.", "AI": {"tldr": "This paper investigates the capabilities and limitations of automatic review generators (ARGs) in detecting faulty research logic during peer review.", "motivation": "To address the potential biases and risks to scientific integrity posed by automatic review generators in scholarly peer review.", "method": "The authors present a fully automated counterfactual evaluation framework to test ARGs on their ability to detect internal inconsistencies in research papers.", "result": "The study found that flaws in research logic do not significantly affect the output reviews of ARGs, which was contrary to the initial expectations.", "conclusion": "The findings prompt three actionable recommendations for future research and a public release of the counterfactual dataset and evaluation framework.", "key_contributions": ["Development of a counterfactual evaluation framework for ARGs", "Insights into the limitations of ARGs in identifying faulty research logic", "Public release of a dataset and evaluation framework for further research"], "limitations": "", "keywords": ["automatic review generators", "faulty research logic", "peer review", "counterfactual evaluation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.21430", "pdf": "https://arxiv.org/pdf/2508.21430.pdf", "abs": "https://arxiv.org/abs/2508.21430", "title": "Med-RewardBench: Benchmarking Reward Models and Judges for Medical Multimodal Large Language Models", "authors": ["Meidan Ding", "Jipeng Zhang", "Wenxuan Wang", "Cheng-Yi Li", "Wei-Chieh Fang", "Hsin-Yu Wu", "Haiqin Zhong", "Wenting Chen", "Linlin Shen"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "19 pages, 5 figures, 3 tables", "summary": "Multimodal large language models (MLLMs) hold significant potential in\nmedical applications, including disease diagnosis and clinical decision-making.\nHowever, these tasks require highly accurate, context-sensitive, and\nprofessionally aligned responses, making reliable reward models and judges\ncritical. Despite their importance, medical reward models (MRMs) and judges\nremain underexplored, with no dedicated benchmarks addressing clinical\nrequirements. Existing benchmarks focus on general MLLM capabilities or\nevaluate models as solvers, neglecting essential evaluation dimensions like\ndiagnostic accuracy and clinical relevance. To address this, we introduce\nMed-RewardBench, the first benchmark specifically designed to evaluate MRMs and\njudges in medical scenarios. Med-RewardBench features a multimodal dataset\nspanning 13 organ systems and 8 clinical departments, with 1,026\nexpert-annotated cases. A rigorous three-step process ensures high-quality\nevaluation data across six clinically critical dimensions. We evaluate 32\nstate-of-the-art MLLMs, including open-source, proprietary, and\nmedical-specific models, revealing substantial challenges in aligning outputs\nwith expert judgment. Additionally, we develop baseline models that demonstrate\nsubstantial performance improvements through fine-tuning.", "AI": {"tldr": "This paper introduces Med-RewardBench, the first benchmark for evaluating medical reward models and judges in medical applications, addressing challenges in accurate and context-sensitive responses for disease diagnosis and clinical decision-making.", "motivation": "The need for reliable models that can provide clinically relevant and accurate responses in medical applications is critical, yet existing benchmarks fail to address specific clinical requirements, particularly in evaluating medical reward models.", "method": "Med-RewardBench features a multimodal dataset with 1,026 expert-annotated cases across 13 organ systems and 8 clinical departments, evaluated using a rigorous three-step process to ensure high-quality data.", "result": "The evaluation of 32 state-of-the-art multimodal large language models showed significant challenges in aligning outputs with expert judgment, highlighting the need for improved reward models in medicine.", "conclusion": "Med-RewardBench establishes a foundational benchmark for future research and development of medical reward models and emphasizes the importance of fine-tuning to enhance performance.", "key_contributions": ["Introduction of Med-RewardBench as the first benchmark for medical reward models", "A multimodal dataset specifically curated for clinical evaluation", "Comprehensive evaluation of existing MLLMs and identification of alignment challenges"], "limitations": "The dataset may not cover all clinical scenarios, and the evaluation process may require further refinement to ensure robust results across diverse medical contexts.", "keywords": ["multimodal large language models", "medical reward models", "benchmarking", "clinical decision-making", "disease diagnosis"], "importance_score": 10, "read_time_minutes": 15}}
{"id": "2508.21436", "pdf": "https://arxiv.org/pdf/2508.21436.pdf", "abs": "https://arxiv.org/abs/2508.21436", "title": "Discovering Semantic Subdimensions through Disentangled Conceptual Representations", "authors": ["Yunhao Zhang", "Shaonan Wang", "Nan Lin", "Xinyi Dong", "Chong Li", "Chengqing Zong"], "categories": ["cs.CL"], "comment": null, "summary": "Understanding the core dimensions of conceptual semantics is fundamental to\nuncovering how meaning is organized in language and the brain. Existing\napproaches often rely on predefined semantic dimensions that offer only broad\nrepresentations, overlooking finer conceptual distinctions. This paper proposes\na novel framework to investigate the subdimensions underlying coarse-grained\nsemantic dimensions. Specifically, we introduce a Disentangled Continuous\nSemantic Representation Model (DCSRM) that decomposes word embeddings from\nlarge language models into multiple sub-embeddings, each encoding specific\nsemantic information. Using these sub-embeddings, we identify a set of\ninterpretable semantic subdimensions. To assess their neural plausibility, we\napply voxel-wise encoding models to map these subdimensions to brain\nactivation. Our work offers more fine-grained interpretable semantic\nsubdimensions of conceptual meaning. Further analyses reveal that semantic\ndimensions are structured according to distinct principles, with polarity\nemerging as a key factor driving their decomposition into subdimensions. The\nneural correlates of the identified subdimensions support their cognitive and\nneuroscientific plausibility.", "AI": {"tldr": "This paper introduces a Disentangled Continuous Semantic Representation Model (DCSRM) that decomposes word embeddings into interpretable semantic subdimensions and maps them to brain activation.", "motivation": "To uncover how meaning is organized in language and the brain by investigating finer conceptual distinctions beyond predefined semantic dimensions.", "method": "The paper proposes a DCSRM to decompose word embeddings from large language models into multiple sub-embeddings and identifies interpretable semantic subdimensions using voxel-wise encoding models to correlate them with brain activation.", "result": "The identified semantic subdimensions offer a more fine-grained understanding of conceptual meaning, with polarity being a significant factor in their decomposition, supported by neural correlates.", "conclusion": "The work provides interpretable semantic subdimensions that align with cognitive and neuroscientific evidence, enhancing the understanding of semantic structure in language.", "key_contributions": ["Introduction of the Disentangled Continuous Semantic Representation Model (DCSRM)", "Identification of interpretable semantic subdimensions", "Mapping of semantic subdimensions to brain activation using voxel-wise encoding models"], "limitations": "", "keywords": ["semantic representation", "large language models", "conceptual semantics"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2508.21448", "pdf": "https://arxiv.org/pdf/2508.21448.pdf", "abs": "https://arxiv.org/abs/2508.21448", "title": "Beyond the Surface: Probing the Ideological Depth of Large Language Models", "authors": ["Shariar Kabir", "Kevin Esterling", "Yue Dong"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated pronounced ideological\nleanings, yet the stability and depth of these positions remain poorly\nunderstood. Surface-level responses can often be manipulated through simple\nprompt engineering, calling into question whether they reflect a coherent\nunderlying ideology. This paper investigates the concept of \"ideological depth\"\nin LLMs, defined as the robustness and complexity of their internal political\nrepresentations. We employ a dual approach: first, we measure the\n\"steerability\" of two well-known open-source LLMs using instruction prompting\nand activation steering. We find that while some models can easily switch\nbetween liberal and conservative viewpoints, others exhibit resistance or an\nincreased rate of refusal, suggesting a more entrenched ideological structure.\nSecond, we probe the internal mechanisms of these models using Sparse\nAutoencoders (SAEs). Preliminary analysis reveals that models with lower\nsteerability possess more distinct and abstract ideological features. Our\nevaluations reveal that one model can contain 7.3x more political features than\nanother model of similar size. This allows targeted ablation of a core\npolitical feature in an ideologically \"deep\" model, leading to consistent,\nlogical shifts in its reasoning across related topics, whereas the same\nintervention in a \"shallow\" model results in an increase in refusal outputs.\nOur findings suggest that ideological depth is a quantifiable property of LLMs\nand that steerability serves as a valuable window into their latent political\narchitecture.", "AI": {"tldr": "This paper explores the concept of 'ideological depth' in Large Language Models (LLMs), measuring their steerability and internal political representations using instruction prompting and Sparse Autoencoders.", "motivation": "Understanding the ideological leanings of LLMs and their implications for AI behavior.", "method": "The study employs a dual approach, measuring steerability of LLMs through instruction prompting and activation steering, and analyzing their internal political representations with Sparse Autoencoders.", "result": "Findings indicate that steerability varies among models, with some easily shifting viewpoints while others show deeper ideological structures. Ideologically deep models exhibit more political features and consistent reasoning shifts when core features are targeted.", "conclusion": "Ideological depth is quantifiable in LLMs, offering insights into their political architecture, with implications for ethical AI deployment.", "key_contributions": ["Introduces the concept of ideological depth in LLMs.", "Demonstrates varying steerability across different LLMs.", "Shows that ideological depth impacts reasoning and response fidelity."], "limitations": "", "keywords": ["Large Language Models", "ideological depth", "steerability", "Sparse Autoencoders", "political representation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.21476", "pdf": "https://arxiv.org/pdf/2508.21476.pdf", "abs": "https://arxiv.org/abs/2508.21476", "title": "Igniting Creative Writing in Small Language Models: LLM-as-a-Judge versus Multi-Agent Refined Rewards", "authors": ["Xiaolong Wei", "Bo Lu", "Xingyu Zhang", "Zhejun Zhao", "Dongdong Shen", "Long Xia", "Dawei Yin"], "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025 Main", "summary": "Large Language Models (LLMs) have demonstrated remarkable creative writing\ncapabilities, yet their substantial computational demands hinder widespread\nuse. Enhancing Small Language Models (SLMs) offers a promising alternative, but\ncurrent methods like Supervised Fine-Tuning (SFT) struggle with novelty, and\nReinforcement Learning from Human Feedback (RLHF) is costly. This paper\nexplores two distinct AI-driven reward strategies within a Reinforcement\nLearning from AI Feedback (RLAIF) framework to ignite the creative writing of a\n7B-parameter SLM, specifically for generating Chinese greetings. The first\nstrategy employs a RM trained on high-quality preference data curated by a\nnovel multi-agent rejection sampling framework designed for creative tasks. The\nsecond, more novel strategy utilizes a principle-guided LLM-as-a-Judge, whose\nreward function is optimized via an adversarial training scheme with a\nreflection mechanism, to directly provide reward signals. Comprehensive\nexperiments reveal that while both approaches significantly enhance creative\noutput over baselines, the principle-guided LLM-as-a-Judge demonstrably yields\nsuperior generation quality. Furthermore, it offers notable advantages in\ntraining efficiency and reduced dependency on human-annotated data, presenting\na more scalable and effective path towards creative SLMs. Our automated\nevaluation methods also exhibit strong alignment with human judgments. Our code\nand data are publicly available at\nhttps://github.com/weixiaolong94-hub/Igniting-Creative-Writing-in-Small-Language-Models.", "AI": {"tldr": "This paper investigates reward strategies to improve creative writing in Small Language Models (SLMs) using a Reinforcement Learning from AI Feedback (RLAIF) framework.", "motivation": "The paper addresses the limitations of current methods for enhancing Small Language Models due to high computational demands and difficulty in achieving novelty.", "method": "Two AI-driven reward strategies are explored within an RLAIF framework: one uses reward models trained on high-quality preference data, and the other employs a principle-guided LLM-as-a-Judge with an adversarial training scheme.", "result": "Both strategies significantly enhance creative output compared to baselines, with the principle-guided approach showing superior generation quality and better training efficiency.", "conclusion": "The findings suggest that adopting novel reward strategies can lead to scalable and effective improvements in the creative capabilities of Small Language Models, minimizing reliance on human-annotated data.", "key_contributions": ["Introduction of an AI-driven reward model for creative tasks.", "Development of a principle-guided LLM-as-a-Judge for providing rewards.", "Demonstration of improved efficiency and quality in generating creative outputs."], "limitations": "", "keywords": ["Small Language Models", "Reinforcement Learning", "Creative Writing", "AI Feedback", "Human Evaluation"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.21482", "pdf": "https://arxiv.org/pdf/2508.21482.pdf", "abs": "https://arxiv.org/abs/2508.21482", "title": "HSFN: Hierarchical Selection for Fake News Detection building Heterogeneous Ensemble", "authors": ["Sara B. Coutinho", "Rafael M. O. Cruz", "Francimaria R. S. Nascimento", "George D. C. Cavalcanti"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by IEEE International Conference on Systems, Man, and\n  Cybernetics (SMC) - IEEE SMC 2025", "summary": "Psychological biases, such as confirmation bias, make individuals\nparticularly vulnerable to believing and spreading fake news on social media,\nleading to significant consequences in domains such as public health and\npolitics. Machine learning-based fact-checking systems have been widely studied\nto mitigate this problem. Among them, ensemble methods are particularly\neffective in combining multiple classifiers to improve robustness. However,\ntheir performance heavily depends on the diversity of the constituent\nclassifiers-selecting genuinely diverse models remains a key challenge,\nespecially when models tend to learn redundant patterns. In this work, we\npropose a novel automatic classifier selection approach that prioritizes\ndiversity, also extended by performance. The method first computes pairwise\ndiversity between classifiers and applies hierarchical clustering to organize\nthem into groups at different levels of granularity. A HierarchySelect then\nexplores these hierarchical levels to select one pool of classifiers per level,\neach representing a distinct intra-pool diversity. The most diverse pool is\nidentified and selected for ensemble construction from these. The selection\nprocess incorporates an evaluation metric reflecting each classifiers's\nperformance to ensure the ensemble also generalises well. We conduct\nexperiments with 40 heterogeneous classifiers across six datasets from\ndifferent application domains and with varying numbers of classes. Our method\nis compared against the Elbow heuristic and state-of-the-art baselines. Results\nshow that our approach achieves the highest accuracy on two of six datasets.\nThe implementation details are available on the project's repository:\nhttps://github.com/SaraBCoutinho/HSFN .", "AI": {"tldr": "This paper presents a novel method for selecting diverse classifiers for ensemble methods to improve machine learning fact-checking systems against fake news.", "motivation": "Individuals are prone to psychological biases that contribute to spreading fake news, making effective ML-based fact-checking systems crucial for societal health.", "method": "The proposed approach computes pairwise diversity among classifiers, uses hierarchical clustering to group them, and dynamically selects diverse pools at different levels for ensemble construction while considering performance.", "result": "The method outperforms the Elbow heuristic and state-of-the-art baselines in accuracy on two of six datasets tested.", "conclusion": "The approach effectively improves ensemble classifier diversity and performance, enhancing the robustness of machine learning models for fact-checking against fake news.", "key_contributions": ["Novel automatic classifier selection prioritizing diversity and performance", "Utilizes hierarchical clustering to organize classifiers", "Achieves highest accuracy on multiple datasets compared to existing methods"], "limitations": "", "keywords": ["machine learning", "fact-checking", "ensemble methods", "classifier diversity", "fake news"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2508.21569", "pdf": "https://arxiv.org/pdf/2508.21569.pdf", "abs": "https://arxiv.org/abs/2508.21569", "title": "L3Cube-MahaSTS: A Marathi Sentence Similarity Dataset and Models", "authors": ["Aishwarya Mirashi", "Ananya Joshi", "Raviraj Joshi"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "We present MahaSTS, a human-annotated Sentence Textual Similarity (STS)\ndataset for Marathi, along with MahaSBERT-STS-v2, a fine-tuned Sentence-BERT\nmodel optimized for regression-based similarity scoring. The MahaSTS dataset\nconsists of 16,860 Marathi sentence pairs labeled with continuous similarity\nscores in the range of 0-5. To ensure balanced supervision, the dataset is\nuniformly distributed across six score-based buckets spanning the full 0-5\nrange, thus reducing label bias and enhancing model stability. We fine-tune the\nMahaSBERT model on this dataset and benchmark its performance against other\nalternatives like MahaBERT, MuRIL, IndicBERT, and IndicSBERT. Our experiments\ndemonstrate that MahaSTS enables effective training for sentence similarity\ntasks in Marathi, highlighting the impact of human-curated annotations,\ntargeted fine-tuning, and structured supervision in low-resource settings. The\ndataset and model are publicly shared at\nhttps://github.com/l3cube-pune/MarathiNLP", "AI": {"tldr": "MahaSTS is a human-annotated dataset for Marathi sentence similarity, paired with a fine-tuned model, MahaSBERT-STS-v2, achieving effective similarity scoring.", "motivation": "The need for a structured and balanced dataset for sentence similarity tasks in low-resource languages like Marathi.", "method": "The MahaSTS dataset comprises 16,860 Marathi sentence pairs with continuous similarity scores labeled from 0-5. The MahaSBERT model is fine-tuned on this dataset and benchmarked against other models.", "result": "MahaSBERT-STS-v2 demonstrated improved performance on sentence similarity tasks compared to alternatives such as MahaBERT and IndicBERT.", "conclusion": "Human-annotated datasets and fine-tuning are crucial for enhancing model performance in low-resource language contexts.", "key_contributions": ["Introduction of a new Marathi sentence similarity dataset (MahaSTS)", "Development of a fine-tuned Sentence-BERT model (MahaSBERT-STS-v2)", "Benchmarking against other models in low-resource language settings."], "limitations": "", "keywords": ["sentence similarity", "Marathi", "NLP", "fine-tuning", "low-resource"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2508.21587", "pdf": "https://arxiv.org/pdf/2508.21587.pdf", "abs": "https://arxiv.org/abs/2508.21587", "title": "A Survey on Current Trends and Recent Advances in Text Anonymization", "authors": ["Tobias Deußer", "Lorenz Sparrenberg", "Armin Berger", "Max Hahnbück", "Christian Bauckhage", "Rafet Sifa"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at IEEE DSAA 2025", "summary": "The proliferation of textual data containing sensitive personal information\nacross various domains requires robust anonymization techniques to protect\nprivacy and comply with regulations, while preserving data usability for\ndiverse and crucial downstream tasks. This survey provides a comprehensive\noverview of current trends and recent advances in text anonymization\ntechniques. We begin by discussing foundational approaches, primarily centered\non Named Entity Recognition, before examining the transformative impact of\nLarge Language Models, detailing their dual role as sophisticated anonymizers\nand potent de-anonymization threats. The survey further explores\ndomain-specific challenges and tailored solutions in critical sectors such as\nhealthcare, law, finance, and education. We investigate advanced methodologies\nincorporating formal privacy models and risk-aware frameworks, and address the\nspecialized subfield of authorship anonymization. Additionally, we review\nevaluation frameworks, comprehensive metrics, benchmarks, and practical\ntoolkits for real-world deployment of anonymization solutions. This review\nconsolidates current knowledge, identifies emerging trends and persistent\nchallenges, including the evolving privacy-utility trade-off, the need to\naddress quasi-identifiers, and the implications of LLM capabilities, and aims\nto guide future research directions for both academics and practitioners in\nthis field.", "AI": {"tldr": "This survey reviews the current state of text anonymization techniques, emphasizing challenges and advances in the context of sensitive data protection across various sectors, including healthcare and law.", "motivation": "To protect privacy and comply with regulations while ensuring data usability in the presence of sensitive personal information.", "method": "The paper surveys existing text anonymization techniques, discussing foundational approaches, the impact of Large Language Models, and advanced methodologies for different domains.", "result": "The review highlights trends in anonymization methods, challenges in maintaining privacy-utility balance, and evaluates tools and metrics for effective data anonymization.", "conclusion": "The paper consolidates knowledge in text anonymization and suggests future research directions to address ongoing challenges.", "key_contributions": ["Comprehensive review of text anonymization techniques.", "Analysis of Large Language Models as both anonymizers and de-anonymization threats.", "Evaluation of metrics and benchmarks for practical anonymization tools."], "limitations": "Limited focus on algorithm design; concentrated on practical implications and evaluations.", "keywords": ["text anonymization", "privacy protection", "Large Language Models", "healthcare", "evaluation frameworks"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.21589", "pdf": "https://arxiv.org/pdf/2508.21589.pdf", "abs": "https://arxiv.org/abs/2508.21589", "title": "Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning", "authors": ["Zinan Tang", "Xin Gao", "Qizhi Pei", "Zhuoshi Pan", "Mengzhang Cai", "Jiang Wu", "Conghui He", "Lijun Wu"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by EMNLP 2025 (main)", "summary": "Supervised Fine-Tuning (SFT) Large Language Models (LLM) fundamentally rely\non high-quality training data. While data selection and data synthesis are two\ncommon strategies to improve data quality, existing approaches often face\nlimitations in static dataset curation that fail to adapt to evolving model\ncapabilities. In this paper, we introduce Middo, a self-evolving Model-informed\ndynamic data optimization framework that uses model-aware data selection and\ncontext-preserving data refinement. Unlike conventional one-off\nfiltering/synthesis methods, our framework establishes a closed-loop\noptimization system: (1) A self-referential diagnostic module proactively\nidentifies suboptimal samples through tri-axial model signals - loss patterns\n(complexity), embedding cluster dynamics (diversity), and self-alignment scores\n(quality); (2) An adaptive optimization engine then transforms suboptimal\nsamples into pedagogically valuable training points while preserving semantic\nintegrity; (3) This optimization process continuously evolves with model\ncapability through dynamic learning principles. Experiments on multiple\nbenchmarks demonstrate that our \\method consistently enhances the quality of\nseed data and boosts LLM's performance with improving accuracy by 7.15% on\naverage while maintaining the original dataset scale. This work establishes a\nnew paradigm for sustainable LLM training through dynamic human-AI co-evolution\nof data and models. Our datasets, models, and code are coming soon.", "AI": {"tldr": "This paper presents Middo, a dynamic data optimization framework for supervised fine-tuning of Large Language Models, enhancing data quality and model performance through continuous adaptation.", "motivation": "Improving data quality for SFT in LLMs through adaptive and evolving methods rather than static dataset curation.", "method": "Middo employs a self-referential diagnostic module for identifying suboptimal samples and an adaptive optimization engine that enhances these samples while preserving data integrity.", "result": "Experiments show a consistent average accuracy improvement of 7.15% in LLM performance while maintaining dataset scale.", "conclusion": "Middo introduces a new paradigm for sustainable LLM training through dynamic human-AI co-evolution of data and models.", "key_contributions": ["Introduction of a self-evolving optimization framework for LLM data", "Use of tri-axial model signals for sample selection", "Demonstration of improved model performance on benchmark datasets"], "limitations": "", "keywords": ["Large Language Models", "data optimization", "supervised fine-tuning", "human-AI co-evolution", "model performance"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.21628", "pdf": "https://arxiv.org/pdf/2508.21628.pdf", "abs": "https://arxiv.org/abs/2508.21628", "title": "Personality Matters: User Traits Predict LLM Preferences in Multi-Turn Collaborative Tasks", "authors": ["Sarfaroz Yunusov", "Kaige Chen", "Kazi Nishat Anwar", "Ali Emami"], "categories": ["cs.CL", "cs.HC"], "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "As Large Language Models (LLMs) increasingly integrate into everyday\nworkflows, where users shape outcomes through multi-turn collaboration, a\ncritical question emerges: do users with different personality traits\nsystematically prefer certain LLMs over others? We conducted a study with 32\nparticipants evenly distributed across four Keirsey personality types,\nevaluating their interactions with GPT-4 and Claude 3.5 across four\ncollaborative tasks: data analysis, creative writing, information retrieval,\nand writing assistance. Results revealed significant personality-driven\npreferences: Rationals strongly preferred GPT-4, particularly for goal-oriented\ntasks, while idealists favored Claude 3.5, especially for creative and\nanalytical tasks. Other personality types showed task-dependent preferences.\nSentiment analysis of qualitative feedback confirmed these patterns. Notably,\naggregate helpfulness ratings were similar across models, showing how\npersonality-based analysis reveals LLM differences that traditional evaluations\nmiss.", "AI": {"tldr": "This study examines how users' personality traits influence their preferences for different Large Language Models (LLMs) during multi-turn collaborations.", "motivation": "Understanding the impact of personality traits on LLM user preferences is essential as these models become integral in collaborative workflows.", "method": "A study involving 32 participants from four Keirsey personality types was conducted to evaluate interactions with GPT-4 and Claude 3.5 across four collaborative tasks.", "result": "Significant personality-driven preferences were found, with Rationals favoring GPT-4 for goal-oriented tasks and Idealists preferring Claude 3.5 for creative and analytical tasks. Overall helpfulness ratings were similar across models.", "conclusion": "Personality-based analysis provides insights into user preferences for LLMs that traditional evaluations might overlook.", "key_contributions": ["Identified personality-driven preferences in LLMs", "Evaluated user interactions across multiple collaborative tasks", "Showed how personality impacts model choice beyond traditional metrics"], "limitations": "", "keywords": ["Large Language Models", "personality traits", "human-computer interaction", "collaborative tasks", "user preferences"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.21632", "pdf": "https://arxiv.org/pdf/2508.21632.pdf", "abs": "https://arxiv.org/abs/2508.21632", "title": "QZhou-Embedding Technical Report", "authors": ["Peng Yu", "En Xu", "Bin Chen", "Haibiao Chen", "Yinfei Xu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present QZhou-Embedding, a general-purpose contextual text embedding model\nwith exceptional text representation capabilities. Built upon the\nQwen2.5-7B-Instruct foundation model, we designed a unified multi-task\nframework comprising specialized data transformation and training strategies.\nThe data transformation scheme enables the incorporation of more diverse\ntextual training datasets, while the task-specific training strategies enhance\nmodel learning efficiency. We developed a data synthesis pipeline leveraging\nLLM API, incorporating techniques such as paraphrasing, augmentation, and hard\nnegative example generation to improve the semantic richness and sample\ndifficulty of the training set. Additionally, we employ a two-stage training\nstrategy, comprising initial retrieval-focused pretraining followed by\nfull-task fine-tuning, enabling the embedding model to extend its capabilities\nbased on robust retrieval performance. Our model achieves state-of-the-art\nresults on the MTEB and CMTEB benchmarks, ranking first on both leaderboards\n(August 27 2025), and simultaneously achieves state-of-the-art performance on\ntasks including reranking, clustering, etc. Our findings demonstrate that\nhigher-quality, more diverse data is crucial for advancing retrieval model\nperformance, and that leveraging LLMs generative capabilities can further\noptimize data quality for embedding model breakthroughs. Our model weights are\nreleased on HuggingFace under Apache 2.0 license. For reproducibility, we\nprovide evaluation code and instructions on GitHub.", "AI": {"tldr": "QZhou-Embedding is a contextual text embedding model that excels in text representation, combining a multi-task framework and advanced data transformation techniques for enhanced model training and performance.", "motivation": "To develop a general-purpose embedding model that improves text representation capabilities and model learning efficiency by utilizing diverse datasets and innovative training strategies.", "method": "The model utilizes a unified multi-task framework, including a data synthesis pipeline with LLM APIs for paraphrasing and augmentation, along with a two-stage training process involving retrieval-focused pretraining and task-specific fine-tuning.", "result": "The QZhou-Embedding model achieves state-of-the-art results on MTEB and CMTEB benchmarks, leading in both categories, and excels in tasks such as reranking and clustering.", "conclusion": "Higher-quality and more diverse training data significantly enhances retrieval model performance, and leveraging LLMs can optimize data quality for embedding breakthroughs.", "key_contributions": ["Introduction of a versatile contextual text embedding model", "Innovative data synthesis pipeline utilizing LLM API", "State-of-the-art benchmark results and practical applications in retrieval tasks"], "limitations": "", "keywords": ["text embedding", "contextual representation", "multi-task framework", "LLM", "data synthesis"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.21675", "pdf": "https://arxiv.org/pdf/2508.21675.pdf", "abs": "https://arxiv.org/abs/2508.21675", "title": "Is this chart lying to me? Automating the detection of misleading visualizations", "authors": ["Jonathan Tonglet", "Jan Zimny", "Tinne Tuytelaars", "Iryna Gurevych"], "categories": ["cs.CL", "cs.CV", "cs.GR"], "comment": "Preprint under review. Code and data available at:\n  https://github.com/UKPLab/arxiv2025-misviz", "summary": "Misleading visualizations are a potent driver of misinformation on social\nmedia and the web. By violating chart design principles, they distort data and\nlead readers to draw inaccurate conclusions. Prior work has shown that both\nhumans and multimodal large language models (MLLMs) are frequently deceived by\nsuch visualizations. Automatically detecting misleading visualizations and\nidentifying the specific design rules they violate could help protect readers\nand reduce the spread of misinformation. However, the training and evaluation\nof AI models has been limited by the absence of large, diverse, and openly\navailable datasets. In this work, we introduce Misviz, a benchmark of 2,604\nreal-world visualizations annotated with 12 types of misleaders. To support\nmodel training, we also release Misviz-synth, a synthetic dataset of 81,814\nvisualizations generated using Matplotlib and based on real-world data tables.\nWe perform a comprehensive evaluation on both datasets using state-of-the-art\nMLLMs, rule-based systems, and fine-tuned classifiers. Our results reveal that\nthe task remains highly challenging. We release Misviz, Misviz-synth, and the\naccompanying code.", "AI": {"tldr": "This paper introduces Misviz, a benchmark dataset for detecting misleading visualizations, along with a synthetic dataset, Misviz-synth, to aid model training.", "motivation": "To address the issue of misleading visualizations that contribute to misinformation on social media by providing a dataset and tools for their detection.", "method": "Introducing the Misviz benchmark with 2,604 real-world visualizations annotated for misleading characteristics, and creating Misviz-synth, a synthetic dataset consisting of 81,814 generated visualizations.", "result": "Evaluation on both datasets shows significant challenges in detecting misleading visualizations using state-of-the-art models, even with the provided benchmarks.", "conclusion": "The release of Misviz and Misviz-synth, along with accompanying code, aims to facilitate future research in recognizing and mitigating the effects of misleading visualizations.", "key_contributions": ["Introduction of Misviz benchmark with real-world visualizations", "Creation of Misviz-synth synthetic dataset", "Comprehensive evaluation and baseline performance on the task."], "limitations": "The task of detecting misleading visualizations remains highly challenging despite newly available datasets.", "keywords": ["misleading visualizations", "Misinformation", "benchmark dataset", "multimodal models", "synthetic dataset"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.21741", "pdf": "https://arxiv.org/pdf/2508.21741.pdf", "abs": "https://arxiv.org/abs/2508.21741", "title": "Not All Parameters Are Created Equal: Smart Isolation Boosts Fine-Tuning Performance", "authors": ["Yao Wang", "Di Liang", "Minlong Peng"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "Supervised fine-tuning (SFT) is a pivotal approach to adapting large language\nmodels (LLMs) for downstream tasks; however, performance often suffers from the\n``seesaw phenomenon'', where indiscriminate parameter updates yield progress on\ncertain tasks at the expense of others. To address this challenge, we propose a\nnovel \\emph{Core Parameter Isolation Fine-Tuning} (CPI-FT) framework.\nSpecifically, we first independently fine-tune the LLM on each task to identify\nits core parameter regions by quantifying parameter update magnitudes. Tasks\nwith similar core regions are then grouped based on region overlap, forming\nclusters for joint modeling. We further introduce a parameter fusion technique:\nfor each task, core parameters from its individually fine-tuned model are\ndirectly transplanted into a unified backbone, while non-core parameters from\ndifferent tasks are smoothly integrated via Spherical Linear Interpolation\n(SLERP), mitigating destructive interference. A lightweight, pipelined SFT\ntraining phase using mixed-task data is subsequently employed, while freezing\ncore regions from prior tasks to prevent catastrophic forgetting. Extensive\nexperiments on multiple public benchmarks demonstrate that our approach\nsignificantly alleviates task interference and forgetting, consistently\noutperforming vanilla multi-task and multi-stage fine-tuning baselines.", "AI": {"tldr": "This paper presents Core Parameter Isolation Fine-Tuning (CPI-FT), a method to improve supervised fine-tuning of large language models by isolating core parameters while mitigating task interference and forgetting.", "motivation": "The motivation is to address the seesaw phenomenon in supervised fine-tuning of large language models, where performance on certain tasks improves at the cost of others.", "method": "The proposed framework independently fine-tunes the LLM on each task to identify core parameter regions and groups tasks with similar cores for joint modeling. A parameter fusion technique and a lightweight mixed-task training phase are introduced to prevent interference and forgetting.", "result": "The experiments show that CPI-FT significantly reduces task interference and forgetting, outperforming standard multi-task and multi-stage fine-tuning methods across various benchmarks.", "conclusion": "CPI-FT provides an effective approach to fine-tuning LLMs, enhancing performance across diverse tasks by isolating core parameters and managing task interference.", "key_contributions": ["Novel Core Parameter Isolation Fine-Tuning framework", "Parameter fusion technique mitigating destructive interference", "Lightweight training phase preventing catastrophic forgetting"], "limitations": "", "keywords": ["Fine-tuning", "Language models", "Task interference", "Core parameters", "Machine learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.21762", "pdf": "https://arxiv.org/pdf/2508.21762.pdf", "abs": "https://arxiv.org/abs/2508.21762", "title": "Reasoning-Intensive Regression", "authors": ["Diane Tchuindjo", "Omar Khattab"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "AI researchers and practitioners increasingly apply large language models\n(LLMs) to what we call reasoning-intensive regression (RiR), i.e. deducing\nsubtle numerical properties from text. Unlike standard language regression\ntasks, e.g. for sentiment or similarity, RiR often appears instead in ad-hoc\nproblems like rubric-based scoring or domain-specific retrieval, where much\ndeeper analysis of text is required while only limited task-specific training\ndata and computation are available. We cast three realistic problems as RiR\ntasks to establish an initial benchmark, and use that to test our hypothesis\nthat prompting frozen LLMs and finetuning Transformer encoders via gradient\ndescent will both often struggle in RiR. We then propose MENTAT, a simple and\nlightweight method that combines batch-reflective prompt optimization with\nneural ensemble learning. MENTAT achieves up to 65% improvement over both\nbaselines, though substantial room remains for future advances in RiR.", "AI": {"tldr": "The paper introduces MENTAT, a method designed for reasoning-intensive regression tasks using large language models, showing significant performance improvement over existing approaches.", "motivation": "To address the challenges faced in reasoning-intensive regression tasks with limited data and computation, where traditional methods struggle.", "method": "The authors propose MENTAT, a method that combines batch-reflective prompt optimization with neural ensemble learning for improved performance in RiR tasks.", "result": "MENTAT achieves up to 65% improvement over prompting frozen LLMs and finetuning Transformer encoders in reasoning-intensive regression tasks.", "conclusion": "While MENTAT shows effectiveness in improving RiR, there is still substantial room for future advancements in this area.", "key_contributions": ["Introduction of MENTAT for reasoning-intensive regression tasks", "Combination of batch-reflective prompt optimization and neural ensemble learning", "Establishment of a benchmark for RiR tasks"], "limitations": "The study indicates that substantial room remains for future advances in reasoning-intensive regression.", "keywords": ["large language models", "reasoning-intensive regression", "MENTAT"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.21787", "pdf": "https://arxiv.org/pdf/2508.21787.pdf", "abs": "https://arxiv.org/abs/2508.21787", "title": "PiCSAR: Probabilistic Confidence Selection And Ranking", "authors": ["Joshua Ong Jun Leang", "Zheng Zhao", "Aryo Pradipta Gema", "Sohee Yang", "Wai-Chung Kwan", "Xuanli He", "Wenda Li", "Pasquale Minervini", "Eleonora Giunchiglia", "Shay B. Cohen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Best-of-n sampling improves the accuracy of large language models (LLMs) and\nlarge reasoning models (LRMs) by generating multiple candidate solutions and\nselecting the one with the highest reward. The key challenge for reasoning\ntasks is designing a scoring function that can identify correct reasoning\nchains without access to ground-truth answers. We propose Probabilistic\nConfidence Selection And Ranking (PiCSAR): a simple, training-free method that\nscores each candidate generation using the joint log-likelihood of the\nreasoning and final answer. The joint log-likelihood of the reasoning and final\nanswer naturally decomposes into reasoning confidence and answer confidence.\nPiCSAR achieves substantial gains across diverse benchmarks (+10.18 on MATH500,\n+9.81 on AIME2025), outperforming baselines with at least 2x fewer samples in\n16 out of 20 comparisons. Our analysis reveals that correct reasoning chains\nexhibit significantly higher reasoning and answer confidence, justifying the\neffectiveness of PiCSAR.", "AI": {"tldr": "PiCSAR enhances LLMs by scoring candidate solutions based on joint log-likelihood of reasoning and answers, achieving better results with fewer samples.", "motivation": "The challenge of designing an effective scoring function for reasoning tasks in LLMs without ground-truth answers.", "method": "Probabilistic Confidence Selection And Ranking (PiCSAR) scores candidates using the joint log-likelihood of their reasoning and final answer, separating reasoning confidence from answer confidence.", "result": "PiCSAR demonstrates significant improvements on various benchmarks, achieving gains of +10.18 on MATH500 and +9.81 on AIME2025 while requiring at least 2x fewer samples in 16 out of 20 comparisons.", "conclusion": "The higher reasoning and answer confidence in correct reasoning chains supports the effectiveness of the PiCSAR method.", "key_contributions": ["Introduction of the PiCSAR method as a training-free scoring technique for LLMs", "Demonstrated substantial accuracy improvements across diverse benchmarks", "Established a connection between reasoning confidence and answer correctness"], "limitations": "", "keywords": ["Large Language Models", "Reasoning Tasks", "Probabilistic Confidence", "Scoring Function", "Benchmarks"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2508.21788", "pdf": "https://arxiv.org/pdf/2508.21788.pdf", "abs": "https://arxiv.org/abs/2508.21788", "title": "Going over Fine Web with a Fine-Tooth Comb: Technical Report of Indexing Fine Web for Problematic Content Search and Retrieval", "authors": ["Inés Altemir Marinas", "Anastasiia Kucherenko", "Andrei Kucharavy"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Large language models (LLMs) rely heavily on web-scale datasets like Common\nCrawl, which provides over 80\\% of training data for some modern models.\nHowever, the indiscriminate nature of web crawling raises challenges in data\nquality, safety, and ethics. Despite the critical importance of training data\nquality, prior research on harmful content has been limited to small samples\ndue to computational constraints. This project presents a framework for\nindexing and analyzing LLM training datasets using an ElasticSearch-based\npipeline. We apply it to SwissAI's FineWeb-2 corpus (1.5TB, four languages),\nachieving fast query performance--most searches in milliseconds, all under 2\nseconds. Our work demonstrates real-time dataset analysis, offering practical\ntools for safer, more accountable AI systems.", "AI": {"tldr": "This project presents a framework for indexing and analyzing LLM training datasets to enhance data quality and safety.", "motivation": "To address the challenges of data quality, safety, and ethics in training large language models from web-scale datasets, specifically focusing on harmful content analysis.", "method": "An ElasticSearch-based pipeline is utilized to index and analyze LLM training datasets, applied specifically to SwissAI's FineWeb-2 corpus.", "result": "The framework achieves fast query performance with most searches completed in milliseconds and all under 2 seconds, enabling real-time dataset analysis.", "conclusion": "The developed tools provide practical enhancements for safer, more accountable AI systems by improving the analysis of training data quality.", "key_contributions": ["Development of an ElasticSearch-based framework for dataset indexing and analysis", "Real-time analysis capabilities for large-scale LLM training datasets", "Improved tools for assessing the quality and safety of training data"], "limitations": "", "keywords": ["large language models", "data quality", "ElasticSearch", "dataset analysis", "safety"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.21209", "pdf": "https://arxiv.org/pdf/2508.21209.pdf", "abs": "https://arxiv.org/abs/2508.21209", "title": "Designing Smarter Conversational Agents for Kids: Lessons from Cognitive Work and Means-Ends Analyses", "authors": ["Vanessa Figueiredo"], "categories": ["cs.HC", "cs.CL", "I.2.1; H.5.2"], "comment": null, "summary": "This paper presents two studies on how Brazilian children (ages 9--11) use\nconversational agents (CAs) for schoolwork, discovery, and entertainment, and\nhow structured scaffolds can enhance these interactions. In Study 1, a\nseven-week online investigation with 23 participants (children, parents,\nteachers) employed interviews, observations, and Cognitive Work Analysis to map\nchildren's information-processing flows, the role of more knowledgeable others,\nfunctional uses, contextual goals, and interaction patterns to inform\nconversation-tree design. We identified three CA functions: School, Discovery,\nEntertainment, and derived ``recipe'' scaffolds mirroring parent-child support.\nIn Study 2, we prompted GPT-4o-mini on 1,200 simulated child-CA exchanges,\ncomparing conversation-tree recipes based on structured-prompting to an\nunstructured baseline. Quantitative evaluation of readability, question\ncount/depth/diversity, and coherence revealed gains for the recipe approach.\nBuilding on these findings, we offer design recommendations: scaffolded\nconversation-trees, child-dedicated profiles for personalized context, and\ncaregiver-curated content. Our contributions include the first CWA application\nwith Brazilian children, an empirical framework of child-CA information flows,\nand an LLM-scaffolding ``recipe'' (i.e., structured-prompting) for effective,\nscaffolded learning.", "AI": {"tldr": "The paper investigates how Brazilian children use conversational agents (CAs) for educational and entertainment purposes, and evaluates structured scaffolds for enhancing these interactions.", "motivation": "To explore the interactions between Brazilian children and conversational agents for schoolwork, discovery, and entertainment, and improve these interactions through structured scaffolds.", "method": "Two studies were conducted: the first involved a seven-week online investigation with interviews and observations to understand children's interaction patterns; the second involved simulating exchanges with GPT-4o-mini to compare structured and unstructured conversation-tree designs.", "result": "The structured prompting approach yielded better readability, depth, diversity, and coherence in child-CA conversations compared to the unstructured baseline.", "conclusion": "The findings support the design of scaffolded conversation-trees and personalized profiles to enhance learning through conversational agents, highlighting the benefits of structured prompting.", "key_contributions": ["First Cognitive Work Analysis application with Brazilian children", "Framework for understanding child-conversational agent information flows", "Development of a structured-prompting 'recipe' for scaffolded learning"], "limitations": "", "keywords": ["conversational agents", "scaffolding", "child interaction", "machine learning", "education"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.21456", "pdf": "https://arxiv.org/pdf/2508.21456.pdf", "abs": "https://arxiv.org/abs/2508.21456", "title": "Morae: Proactively Pausing UI Agents for User Choices", "authors": ["Yi-Hao Peng", "Dingzeyu Li", "Jeffrey P. Bigham", "Amy Pavel"], "categories": ["cs.HC", "cs.CL", "cs.CV"], "comment": "ACM UIST 2025", "summary": "User interface (UI) agents promise to make inaccessible or complex UIs easier\nto access for blind and low-vision (BLV) users. However, current UI agents\ntypically perform tasks end-to-end without involving users in critical choices\nor making them aware of important contextual information, thus reducing user\nagency. For example, in our field study, a BLV participant asked to buy the\ncheapest available sparkling water, and the agent automatically chose one from\nseveral equally priced options, without mentioning alternative products with\ndifferent flavors or better ratings. To address this problem, we introduce\nMorae, a UI agent that automatically identifies decision points during task\nexecution and pauses so that users can make choices. Morae uses large\nmultimodal models to interpret user queries alongside UI code and screenshots,\nand prompt users for clarification when there is a choice to be made. In a\nstudy over real-world web tasks with BLV participants, Morae helped users\ncomplete more tasks and select options that better matched their preferences,\nas compared to baseline agents, including OpenAI Operator. More broadly, this\nwork exemplifies a mixed-initiative approach in which users benefit from the\nautomation of UI agents while being able to express their preferences.", "AI": {"tldr": "Morae is a UI agent designed for blind and low-vision users that enhances user agency by pausing at decision points, allowing users to make choices informed by contextual information.", "motivation": "To improve accessibility of complex UIs for blind and low-vision users by enhancing their agency during decision-making processes.", "method": "Morae employs large multimodal models to analyze user queries in conjunction with UI code and screenshots, prompting users for clarification when choices arise.", "result": "In a study, Morae enabled blind and low-vision users to complete more tasks and select options that better matched their preferences compared to baseline agents.", "conclusion": "Morae exemplifies a mixed-initiative approach, balancing automation with user involvement in decision-making, enhancing accessibility and user experience.", "key_contributions": ["Introduction of Morae UI agent for BLV users", "Enhanced user agency by defining decision points", "Comparison with baseline UI agents showing improved user outcomes"], "limitations": "", "keywords": ["UI agents", "blind and low-vision", "user agency", "multimodal models", "accessibility"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2404.07117", "pdf": "https://arxiv.org/pdf/2404.07117.pdf", "abs": "https://arxiv.org/abs/2404.07117", "title": "Continuous Language Model Interpolation for Dynamic and Controllable Text Generation", "authors": ["Sara Kangaslahti", "David Alvarez-Melis"], "categories": ["cs.CL", "cs.LG"], "comment": "20 pages, 22 figures", "summary": "As large language models (LLMs) have gained popularity for a variety of use\ncases, making them adaptable and controllable has become increasingly\nimportant, especially for user-facing applications. While the existing\nliterature on LLM adaptation primarily focuses on finding a model (or models)\nthat optimizes a single predefined objective, here we focus on the challenging\ncase where the model must dynamically adapt to diverse -- and often changing --\nuser preferences. For this, we leverage adaptation methods based on linear\nweight interpolation, casting them as continuous multi-domain interpolators\nthat produce models with specific prescribed generation characteristics\non-the-fly. Specifically, we use low-rank updates to fine-tune a base model to\nvarious different domains, yielding a set of anchor models with distinct\ngeneration profiles. Then, we use the weight updates of these anchor models to\nparametrize the entire (infinite) class of models contained within their convex\nhull. We empirically show that varying the interpolation weights yields\npredictable and consistent change in the model outputs with respect to all of\nthe controlled attributes. We find that there is little entanglement between\nmost attributes and identify and discuss the pairs of attributes for which this\nis not the case. Our results suggest that linearly interpolating between the\nweights of fine-tuned models facilitates predictable, fine-grained control of\nmodel outputs with respect to multiple stylistic characteristics\nsimultaneously.", "AI": {"tldr": "This paper explores dynamic adaptation of large language models (LLMs) to user preferences through linear weight interpolation, enabling fine-grained control over model outputs.", "motivation": "As LLMs are increasingly used in user-facing applications, this work addresses the need for models that can adapt to changing user preferences rather than optimizing for a single objective.", "method": "The authors leverage adaptation methods via linear weight interpolation, creating a set of anchor models fine-tuned to different domains. Weight updates from these models parameterize a vast class of models within their convex hull.", "result": "The study demonstrates that varying interpolation weights leads to predictable changes in model outputs aligned with controlled attributes, with minimal entanglement between most attributes.", "conclusion": "The findings indicate that linear interpolation between weights of fine-tuned models allows for precise, simultaneous control over multiple stylistic characteristics in model outputs.", "key_contributions": ["Proposes a method for dynamic adaptation of LLMs to user preferences.", "Establishes a framework for using linear weight interpolation in model control.", "Empirically validates the predictable response of model outputs to interpolation weight changes."], "limitations": "The paper identifies pairs of attributes with notable entanglement, which can complicate the interpolation approach.", "keywords": ["large language models", "dynamic adaptation", "user preferences", "linear weight interpolation", "fine-grained control"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2406.19238", "pdf": "https://arxiv.org/pdf/2406.19238.pdf", "abs": "https://arxiv.org/abs/2406.19238", "title": "Revealing Fine-Grained Values and Opinions in Large Language Models", "authors": ["Dustin Wright", "Arnav Arora", "Nadav Borenstein", "Srishti Yadav", "Serge Belongie", "Isabelle Augenstein"], "categories": ["cs.CL", "cs.CY", "cs.LG"], "comment": "Findings of EMNLP 2024; 28 pages, 20 figures, 7 tables", "summary": "Uncovering latent values and opinions embedded in large language models\n(LLMs) can help identify biases and mitigate potential harm. Recently, this has\nbeen approached by prompting LLMs with survey questions and quantifying the\nstances in the outputs towards morally and politically charged statements.\nHowever, the stances generated by LLMs can vary greatly depending on how they\nare prompted, and there are many ways to argue for or against a given position.\nIn this work, we propose to address this by analysing a large and robust\ndataset of 156k LLM responses to the 62 propositions of the Political Compass\nTest (PCT) generated by 6 LLMs using 420 prompt variations. We perform\ncoarse-grained analysis of their generated stances and fine-grained analysis of\nthe plain text justifications for those stances. For fine-grained analysis, we\npropose to identify tropes in the responses: semantically similar phrases that\nare recurrent and consistent across different prompts, revealing natural\npatterns in the text that a given LLM is prone to produce. We find that\ndemographic features added to prompts significantly affect outcomes on the PCT,\nreflecting bias, as well as disparities between the results of tests when\neliciting closed-form vs. open domain responses. Additionally, patterns in the\nplain text rationales via tropes show that similar justifications are\nrepeatedly generated across models and prompts even with disparate stances.", "AI": {"tldr": "This paper analyzes 156k responses from large language models (LLMs) to the Political Compass Test, identifying biases in output based on prompt variations and demographic features while proposing a method to uncover recurrent justifications in responses.", "motivation": "To uncover biases and mitigate potential harm caused by large language models (LLMs) by analyzing their responses to politically charged propositions.", "method": "Analysis of a dataset comprising responses from 6 LLMs to 62 Political Compass Test propositions using 420 prompt variations, including both coarse-grained and fine-grained analysis of the generated stances and justifications.", "result": "Demographic features significantly influence LLM responses, reflecting biases, and consistent tropes are found in text rationales across different prompts and models, indicating a propensity to generate similar justifications irrespective of stances.", "conclusion": "Identifying patterns in LLM responses can help understand and mitigate biases in AI outputs, informing the design of safer prompting strategies.", "key_contributions": ["Proposed a method to analyze biases in LLM outputs using the Political Compass Test.", "Demonstrated the impact of prompt design and demographic features on LLM responses.", "Highlighted recurrent justifications in text through the identification of tropes."], "limitations": "The study focuses on specific prompts and LLMs, which may limit generalizability.", "keywords": ["large language models", "bias analysis", "Political Compass Test", "text justification", "tropes"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2409.06679", "pdf": "https://arxiv.org/pdf/2409.06679.pdf", "abs": "https://arxiv.org/abs/2409.06679", "title": "E2LLM: Encoder Elongated Large Language Models for Long-Context Understanding and Reasoning", "authors": ["Zihan Liao", "Jun Wang", "Hang Yu", "Lingxiao Wei", "Jianguo Li", "Jun Wang", "Wei Zhang"], "categories": ["cs.CL"], "comment": "Accept by EMNLP'25", "summary": "Processing long contexts is increasingly important for Large Language Models\n(LLMs) in tasks like multi-turn dialogues, code generation, and document\nsummarization. This paper addresses the challenges of achieving high\nlong-context performance, low computational complexity, and compatibility with\npretrained models -- collectively termed the ``impossible triangle''. We\nintroduce E2LLM (Encoder Elongated Large Language Models), a novel approach\nthat effectively navigates this paradox. E2LLM divides long contexts into\nchunks, compresses each into soft prompts using a pretrained text encoder, and\naligns these representations with a decoder-only LLM via an adapter. To enhance\nthe LLM's reasoning with these soft prompts, we employ two training objectives:\nencoder output reconstruction and long-context instruction fine-tuning.\nExtensive experiments reveal that E2LLM not only outperforms 8 state-of-the-art\n(SOTA) methods in effectiveness and efficiency for document summarization and\nquestion answering, but also achieves the best performance on LongBench v2\namong models of comparable size.", "AI": {"tldr": "E2LLM offers a novel solution for processing long contexts in Large Language Models by chunking context into soft prompts, leading to improved performance in tasks like document summarization and question answering.", "motivation": "To tackle the challenges associated with long-context performance, computational efficiency, and compatibility with pretrained models, termed the 'impossible triangle'.", "method": "The proposed method, E2LLM, divides long contexts into chunks, compresses them into soft prompts using a pretrained text encoder, and aligns these with a decoder-only LLM through an adapter, supported by two training objectives.", "result": "E2LLM surpasses 8 state-of-the-art methods in both effectiveness and efficiency for long-context tasks and achieves top performance on LongBench v2 among similar-sized models.", "conclusion": "The introduction of E2LLM resolves critical challenges in processing long contexts effectively while maintaining low computational demands.", "key_contributions": ["E2LLM effectively manages long contexts using soft prompts.", "Outperformed state-of-the-art methods in major long-context tasks.", "Achieved top performance on LongBench v2."], "limitations": "", "keywords": ["Large Language Models", "long-context processing", "soft prompts"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2410.24155", "pdf": "https://arxiv.org/pdf/2410.24155.pdf", "abs": "https://arxiv.org/abs/2410.24155", "title": "Blind Spot Navigation in Large Language Model Reasoning with Thought Space Explorer", "authors": ["Jinghan Zhang", "Fengran Mo", "Tharindu Cyril Weerasooriya", "Yeyang Zhou", "Xinyue Ye", "Dongjie Wang", "Yanjie Fu", "Kunpeng Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large language models (LLMs) have demonstrated their\npotential in handling complex reasoning tasks, which are usually achieved by\nconstructing a thought chain to guide the model in solving the problem with\nmulti-step thinking. However, existing methods often remain confined to\npreviously explored solution spaces and thus overlook the critical blind spot\nwithin LLMs' cognitive range. To address these issues, we introduce the\n``Thought Space Explorer'' (TSE), a novel framework to expand and optimize\nthought structures to guide LLMs to explore their blind spots of thinking. By\ngenerating new reasoning steps and branches based on the original thought\nstructure with various designed strategies, TSE broadens the thought\nexploration view and alleviates the impact of blind spots for LLM reasoning.\nExperimental results on multiple levels of reasoning tasks demonstrate the\nefficacy of TSE by surpassing various baseline methods. We also conduct\nextensive analysis to understand how structured and expansive thought can\ncontribute to unleashing the potential of LLM reasoning capabilities.", "AI": {"tldr": "The paper introduces the Thought Space Explorer (TSE), a framework designed to enhance large language models' (LLMs) reasoning by expanding thought structures to explore blind spots.", "motivation": "Existing reasoning methods for LLMs are limited by previously explored solution spaces, which prevent them from addressing critical blind spots in their cognitive capabilities.", "method": "The TSE framework generates new reasoning steps and branches from an original thought structure, employing various strategies to broaden the thought exploration process.", "result": "TSE demonstrated superior performance on multiple levels of reasoning tasks compared to existing baseline methods, indicating its effectiveness in enhancing LLM reasoning.", "conclusion": "By utilizing structured and expansive thought processes, TSE can significantly improve LLM reasoning capabilities and address cognitive blind spots.", "key_contributions": ["Introduction of the Thought Space Explorer framework for LLMs", "Demonstration of improved reasoning performance on complex tasks", "Analysis of the impact of structured thought on LLM capabilities"], "limitations": "", "keywords": ["large language models", "reasoning", "thought exploration", "AI", "machine learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2411.04090", "pdf": "https://arxiv.org/pdf/2411.04090.pdf", "abs": "https://arxiv.org/abs/2411.04090", "title": "A Collaborative Content Moderation Framework for Toxicity Detection based on Conformalized Estimates of Annotation Disagreement", "authors": ["Guillermo Villate-Castillo", "Javier Del Ser", "Borja Sanz"], "categories": ["cs.CL", "cs.AI", "cs.HC", "68T50 (Primary) 68T37 (Secondary)", "I.2.7; I.2.1"], "comment": "35 pages, 1 figure", "summary": "Content moderation typically combines the efforts of human moderators and\nmachine learning models. However, these systems often rely on data where\nsignificant disagreement occurs during moderation, reflecting the subjective\nnature of toxicity perception. Rather than dismissing this disagreement as\nnoise, we interpret it as a valuable signal that highlights the inherent\nambiguity of the content,an insight missed when only the majority label is\nconsidered. In this work, we introduce a novel content moderation framework\nthat emphasizes the importance of capturing annotation disagreement. Our\napproach uses multitask learning, where toxicity classification serves as the\nprimary task and annotation disagreement is addressed as an auxiliary task.\nAdditionally, we leverage uncertainty estimation techniques, specifically\nConformal Prediction, to account for both the ambiguity in comment annotations\nand the model's inherent uncertainty in predicting toxicity and\ndisagreement.The framework also allows moderators to adjust thresholds for\nannotation disagreement, offering flexibility in determining when ambiguity\nshould trigger a review. We demonstrate that our joint approach enhances model\nperformance, calibration, and uncertainty estimation, while offering greater\nparameter efficiency and improving the review process in comparison to\nsingle-task methods.", "AI": {"tldr": "A novel content moderation framework that captures annotation disagreement through multitask learning and uncertainty estimation techniques to improve model performance and review processes.", "motivation": "To address the subjective nature of toxicity perception in content moderation and highlight the value of annotation disagreement.", "method": "Introducing a multitask learning framework where toxicity classification is the primary task and annotation disagreement serves as an auxiliary task, utilizing Conformal Prediction for uncertainty estimation.", "result": "Demonstrated enhancements in model performance, calibration, and uncertainty estimation while improving parameter efficiency and the review process compared to single-task methods.", "conclusion": "The proposed framework offers a more nuanced approach to content moderation by incorporating annotation disagreement, ultimately leading to better handling of ambiguous content.", "key_contributions": ["Multitask learning framework for content moderation", "Incorporation of annotation disagreement as an auxiliary task", "Use of Conformal Prediction for better uncertainty estimation"], "limitations": "", "keywords": ["content moderation", "toxic comments", "annotation disagreement", "multitask learning", "uncertainty estimation"], "importance_score": 8, "read_time_minutes": 35}}
{"id": "2412.04342", "pdf": "https://arxiv.org/pdf/2412.04342.pdf", "abs": "https://arxiv.org/abs/2412.04342", "title": "Retrieval-Augmented Machine Translation with Unstructured Knowledge", "authors": ["Jiaan Wang", "Fandong Meng", "Yingxue Zhang", "Jie Zhou"], "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025 Findings", "summary": "Retrieval-augmented generation (RAG) introduces additional information to\nenhance large language models (LLMs). In machine translation (MT), previous\nwork typically retrieves in-context examples from paired MT corpora, or\ndomain-specific knowledge from knowledge graphs, to enhance MT models. However,\na large amount of world knowledge is organized in unstructured documents, and\nmight not be fully paired across different languages. In this paper, we study\nretrieval-augmented MT using unstructured documents. Specifically, we build\nRAGtrans, the first benchmark to train and evaluate LLMs' retrieval-augmented\nMT ability. RAGtrans contains 169K MT samples collected via GPT-4o and human\ntranslators. Besides, documents from various languages are also provided to\nsupply the knowledge to these samples. Based on RAGtrans, we further propose a\nmulti-task training method to teach LLMs how to use information from\nmultilingual documents during their translation. The method uses existing\nmultilingual corpora to create auxiliary training objectives without additional\nlabeling requirements. Extensive experiments show that the method improves LLMs\nby 1.6-3.1 BLEU and 1.0-2.0 COMET scores in En-Zh, and 1.7-2.9 BLEU and 2.1-2.7\nCOMET scores in En-De. We also conclude the critical difficulties that current\nLLMs face with this task.", "AI": {"tldr": "This paper presents RAGtrans, a benchmark for retrieval-augmented machine translation using unstructured documents and proposes a multi-task training method to enhance LLM translation abilities.", "motivation": "To enhance machine translation (MT) models by utilizing unstructured documents and world knowledge not fully paired across languages.", "method": "Developed RAGtrans, a benchmark with 169K MT samples, and proposed a multi-task training method utilizing multilingual corpora for auxiliary training objectives without extra labeling.", "result": "Enhancements in LLM performance with improvements of 1.6-3.1 BLEU and 1.0-2.0 COMET scores for English-Chinese, as well as 1.7-2.9 BLEU and 2.1-2.7 COMET scores for English-German.", "conclusion": "Current LLMs still face critical challenges in retrieval-augmented MT tasks that need further exploration.", "key_contributions": ["Introduction of RAGtrans benchmark for retrieval-augmented MT.", "Proposed multi-task training method leveraging multilingual data.", "Empirical improvements in translation metrics (BLEU and COMET) for LLMs."], "limitations": "Challenges faced by current LLMs in retrieval-augmented MT.", "keywords": ["retrieval-augmented generation", "machine translation", "large language models", "benchmark", "multilingual training"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2501.12640", "pdf": "https://arxiv.org/pdf/2501.12640.pdf", "abs": "https://arxiv.org/abs/2501.12640", "title": "Toxicity Begets Toxicity: Unraveling Conversational Chains in Political Podcasts", "authors": ["Naquee Rizwan", "Nayandeep Deb", "Sarthak Roy", "Vishwajeet Singh Solanki", "Kiran Garimella", "Animesh Mukherjee"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Tackling toxic behavior in digital communication continues to be a pressing\nconcern for both academics and industry professionals. While significant\nresearch has explored toxicity on platforms like social networks and discussion\nboards, podcasts despite their rapid rise in popularity remain relatively\nunderstudied in this context. This work seeks to fill that gap by curating a\ndataset of political podcast transcripts and analyzing them with a focus on\nconversational structure. Specifically, we investigate how toxicity surfaces\nand intensifies through sequences of replies within these dialogues, shedding\nlight on the organic patterns by which harmful language can escalate across\nconversational turns. Warning: Contains potentially abusive/toxic contents.", "AI": {"tldr": "This paper analyzes the emergence and escalation of toxicity in political podcast dialogues using a curated dataset of podcast transcripts.", "motivation": "The increasing popularity of podcasts as a form of communication and the lack of research on toxic behaviors in this medium.", "method": "Curated a dataset of political podcast transcripts and analyzed the conversational structure to identify patterns of toxicity.", "result": "The analysis reveals how toxicity surfaces and escalates in the sequences of replies within podcast dialogues.", "conclusion": "Understanding the conversational dynamics of toxicity in podcasts can help address harmful language in digital communication.", "key_contributions": ["Curated a novel dataset of political podcast transcripts for analysis.", "Identified specific patterns of toxicity escalation within podcast dialogues.", "Provided insights into conversational dynamics that contribute to harmful language."], "limitations": "Focuses only on political podcasts; results may not generalize to other podcast genres.", "keywords": ["toxicity", "podcasts", "conversational structure", "digital communication", "political discourse"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2503.14728", "pdf": "https://arxiv.org/pdf/2503.14728.pdf", "abs": "https://arxiv.org/abs/2503.14728", "title": "Strategic resource allocation in memory encoding: An efficiency principle shaping language processing", "authors": ["Weijie Xu", "Richard Futrell"], "categories": ["cs.CL"], "comment": "manuscript under review", "summary": "How is the limited capacity of working memory efficiently used to support\nhuman linguistic behaviors? In this paper, we propose Strategic Resource\nAllocation (SRA) as an efficiency principle for memory encoding in sentence\nprocessing. The idea is that working memory resources are dynamically and\nstrategically allocated to prioritize novel and unexpected information. From a\nresource-rational perspective, we argue that SRA is the principled solution to\na computational problem posed by two functional assumptions about working\nmemory, namely its limited capacity and its noisy representation. Specifically,\nworking memory needs to minimize the retrieval error of past inputs under the\nconstraint of limited memory resources, an optimization problem whose solution\nis to allocate more resources to encode more surprising inputs with higher\nprecision. One of the critical consequences of SRA is that surprising inputs\nare encoded with enhanced representations, and therefore are less susceptible\nto memory decay and interference. Empirically, through naturalistic corpus\ndata, we find converging evidence for SRA in the context of dependency locality\nfrom both production and comprehension, where non-local dependencies with less\npredictable antecedents are associated with reduced locality effect. However,\nour results also reveal considerable cross-linguistic variability, suggesting\nthe need for a closer examination of how SRA, as a domain-general memory\nefficiency principle, interacts with language-specific phrase structures. SRA\nhighlights the critical role of representational uncertainty in understanding\nmemory encoding. It also reimages the effects of surprisal and entropy on\nprocessing difficulty from the perspective of efficient memory encoding.", "AI": {"tldr": "The paper proposes Strategic Resource Allocation (SRA) as a principle for optimizing memory encoding in sentence processing, suggesting that working memory dynamically allocates resources to prioritize unexpected information.", "motivation": "To understand how limited working memory capacity supports linguistic behaviors in humans.", "method": "The authors present a resource-rational approach and analyze naturalistic corpus data to test the SRA principle in dependency locality effects during sentence processing.", "result": "Findings indicate that surprising inputs are encoded with enhanced representations, reducing memory decay and interference, with observed effects varying across languages.", "conclusion": "SRA serves as an efficiency principle in memory encoding, highlighting the role of representational uncertainty and suggesting that memory allocation is influenced by language-specific structures.", "key_contributions": ["Introduction of Strategic Resource Allocation as a memory efficiency principle", "Empirical support for SRA through analysis of dependency locality", "Highlighting cross-linguistic variability in memory encoding strategies"], "limitations": "Cross-linguistic variability indicates the need for further research on SRA's application in different languages.", "keywords": ["working memory", "sentence processing", "resource allocation", "memory encoding", "linguistic behaviors"], "importance_score": 5, "read_time_minutes": 15}}
{"id": "2504.06821", "pdf": "https://arxiv.org/pdf/2504.06821.pdf", "abs": "https://arxiv.org/abs/2504.06821", "title": "Inducing Programmatic Skills for Agentic Tasks", "authors": ["Zora Zhiruo Wang", "Apurva Gandhi", "Graham Neubig", "Daniel Fried"], "categories": ["cs.CL"], "comment": null, "summary": "To succeed in common digital tasks such as web navigation, agents must carry\nout a variety of specialized tasks such as searching for products or planning a\ntravel route. To tackle these tasks, agents can bootstrap themselves by\nlearning task-specific skills online through interaction with the web\nenvironment. In this work, we demonstrate that programs are an effective\nrepresentation for skills. We propose agent skill induction (ASI), which allows\nagents to adapt themselves by inducing, verifying, and utilizing program-based\nskills on the fly. We start with an evaluation on the WebArena agent benchmark\nand show that ASI outperforms the static baseline agent and its text-skill\ncounterpart by 23.5% and 11.3% in success rate, mainly thanks to the\nprogrammatic verification guarantee during the induction phase. ASI also\nimproves efficiency by reducing 10.7-15.3% of the steps over baselines, by\ncomposing primitive actions (e.g., click) into higher-level skills (e.g.,\nsearch product). We then highlight the efficacy of ASI in remaining efficient\nand accurate under scaled-up web activities. Finally, we examine the\ngeneralizability of induced skills when transferring between websites, and find\nthat ASI can effectively reuse common skills, while also updating incompatible\nskills to versatile website changes.", "AI": {"tldr": "This paper introduces Agent Skill Induction (ASI), a method for agents to learn and adapt task-specific skills via program representations, demonstrating improved performance and efficiency in web navigation tasks.", "motivation": "To improve the adaptability and efficiency of agents in performing digital tasks through learning task-specific skills online.", "method": "The paper proposes Agent Skill Induction (ASI), which involves inducing, verifying, and utilizing program-based skills by agents dynamically based on web interactions.", "result": "ASI outperforms static baselines and text-skill counterparts, achieving a 23.5% and 11.3% higher success rate respectively, and reduces operational steps by 10.7-15.3% through higher-level skill composition.", "conclusion": "ASI demonstrates efficiency and accuracy in complex web activities, with effective skill generalization and adaptation to website changes.", "key_contributions": ["Introduction of Agent Skill Induction (ASI) methodology", "Evidence of performance improvement over existing agent models", "Demonstrated ability to generalize and adapt skills across different web tasks"], "limitations": "", "keywords": ["agent skill induction", "program-based skills", "web navigation", "machine learning", "dynamic adaptation"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2504.10187", "pdf": "https://arxiv.org/pdf/2504.10187.pdf", "abs": "https://arxiv.org/abs/2504.10187", "title": "DeepTrans: Deep Reasoning Translation via Reinforcement Learning", "authors": ["Jiaan Wang", "Fandong Meng", "Jie Zhou"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by Transactions of the Association for Computational\n  Linguistics (TACL)", "summary": "Recently, deep reasoning LLMs (e.g., OpenAI o1 and DeepSeek-R1) have shown\npromising performance in various downstream tasks. Free translation is an\nimportant and interesting task in the multilingual world, which requires going\nbeyond word-for-word translation. However, the task is still under-explored in\ndeep reasoning LLMs. In this paper, we introduce DeepTrans, a deep reasoning\ntranslation model that learns free translation via reinforcement learning (RL).\nSpecifically, we carefully build a reward model with pre-defined scoring\ncriteria on both the translation results and the thought processes. The reward\nmodel teaches DeepTrans how to think and free-translate the given sentences\nduring RL. Besides, our RL training does not need any labeled translations,\navoiding the human-intensive annotation or resource-intensive data synthesis.\nExperimental results show the effectiveness of DeepTrans. Using Qwen2.5-7B as\nthe backbone, DeepTrans improves performance by 16.3% in literature\ntranslation, and outperforms strong deep reasoning LLMs. Moreover, we summarize\nthe failures and interesting findings during our RL exploration. We hope this\nwork could inspire other researchers in free translation.", "AI": {"tldr": "DeepTrans is a deep reasoning translation model that enhances free translation using reinforcement learning, showing significant performance improvements over existing LLMs without the need for labeled data.", "motivation": "The paper addresses the under-explored area of free translation in deep reasoning LLMs, aiming to improve translation quality beyond literal interpretations.", "method": "DeepTrans employs reinforcement learning with a reward model that incorporates pre-defined scoring criteria based on translation results and cognitive processes.", "result": "DeepTrans, using Qwen2.5-7B, improves literature translation performance by 16.3% compared to existing models and demonstrates superior results over other deep reasoning LLMs.", "conclusion": "The findings highlight the potential of using RL for free translation, with the authors hoping to inspire further research in the field.", "key_contributions": ["Introduction of DeepTrans for free translation using RL", "Development of a reward model that guides translation and reasoning", "Demonstration of significant performance improvements in literature translation"], "limitations": "Fails in certain translation scenarios and the specifics of RL exploration limitations are discussed.", "keywords": ["deep reasoning", "free translation", "reinforcement learning", "LLM", "natural language processing"], "importance_score": 8, "read_time_minutes": 15}}
