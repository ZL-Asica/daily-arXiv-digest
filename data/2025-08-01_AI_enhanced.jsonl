{"id": "2507.22889", "pdf": "https://arxiv.org/pdf/2507.22889.pdf", "abs": "https://arxiv.org/abs/2507.22889", "title": "Knowledge Is More Than Performance: How Knowledge Diversity Drives Human-Human and Human-AI Interaction Synergy and Reveals Pure-AI Interaction Shortfalls", "authors": ["Tom Sheffer", "Alon Miron", "Yaniv Dover", "Ariel Goldstein"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Conversations transform individual knowledge into collective insight,\nallowing groups of humans and increasingly groups of artificial intelligence\n(AI) agents to collaboratively solve complex problems. Whether interactions\nbetween AI agents can replicate the synergy observed in human discussions\nremains an open question. To investigate this, we systematically compared four\nconversational configurations: pairs of large language models (LLM-LLM), trios\nof LLMs, trios of humans, and mixed human-LLM pairs. After agents answered\nquestions individually, they engaged in open-ended discussions and then\nreconsidered their initial answers. Interactions involving humans consistently\nled to accuracy improvements after the conversations, benefiting both stronger\nand weaker participants. By contrast, purely LLM-based pairs and trios\nexhibited declines in accuracy, demonstrating limited conversational synergy.\nAnalysis of participants' confidence and answer-switching behavior revealed\nthat knowledge diversity is a critical factor enabling collaborative\nimprovement. Crucially, the lack of gains in LLM-LLM interactions did not stem\nfrom a fundamental limitation of the models' ability to collaborate, but from\nhighly similar knowledge states that left little room for productive exchange.\nOur findings argue for a paradigm shift in AI development: rather than\noptimizing individual models solely for standalone performance, explicitly\ncultivating diversity across agents, even at the cost of slightly lower\nindividual accuracy, may yield AI collaborators that are more effective in\ngroup settings with humans or other AI systems.", "AI": {"tldr": "The paper investigates the synergy of human and AI-agent conversations by comparing various configurations (human, LLM-LLM) and their impact on answer accuracy.", "motivation": "To understand if AI interactions can replicate the synergy seen in human discussions and improve collective problem-solving.", "method": "The study systematically compared conversational configurations: pairs/trios of large language models (LLM-LLM), trios of humans, and mixed pairs of humans and LLMs, evaluating accuracy and behavior post-conversation.", "result": "Human interactions consistently improved answer accuracy, while LLM-only interactions led to declines in accuracy, indicating limited conversational synergy among models.", "conclusion": "Cultivating diversity among AI agents may enhance collaborative performance despite slight reductions in individual accuracy, shifting the focus from optimizing single models to optimizing their collaboration.", "key_contributions": ["Empirical comparisons of various conversational configurations involving AI", "Demonstrates the importance of knowledge diversity for collaborative improvement", "Calls for a paradigm shift in AI development towards diversity over individual accuracy"], "limitations": "Limited to specific configurations and may not generalize to all AI interaction scenarios.", "keywords": ["conversational AI", "large language models", "collective intelligence", "collaborative learning", "knowledge diversity"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.22890", "pdf": "https://arxiv.org/pdf/2507.22890.pdf", "abs": "https://arxiv.org/abs/2507.22890", "title": "Evaluating LLMs for Visualization Generation and Understanding", "authors": ["Saadiq Rauf Khan", "Vinit Chandak", "Sougata Mukherjea"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Information Visualization has been utilized to gain insights from complex\ndata. In recent times, Large Language models (LLMs) have performed very well in\nmany tasks. In this paper, we showcase the capabilities of different popular\nLLMs to generate code for visualization based on simple prompts. We also\nanalyze the power of LLMs to understand some common visualizations by answering\nquestions. Our study shows that LLMs could generate code for some simpler\nvisualizations such as bar and pie charts. Moreover, they could answer simple\nquestions about visualizations. However, LLMs also have several limitations.\nFor example, some of them had difficulty generating complex visualizations,\nsuch as violin plot. LLMs also made errors in answering some questions about\nvisualizations, for example, identifying relationships between close boundaries\nand determining lengths of shapes. We believe that our insights can be used to\nimprove both LLMs and Information Visualization systems.", "AI": {"tldr": "The paper examines the abilities of various LLMs in generating code for information visualizations and answering related questions.", "motivation": "To explore the potential of Large Language Models in generating visualization code and understanding visual data representations.", "method": "Different popular LLMs were tested to generate code for visualizations from prompts and answer questions related to these visualizations.", "result": "LLMs successfully generated code for simpler visualizations like bar and pie charts and answered basic questions about them, but struggled with complex visualizations and had some inaccuracies in responses.", "conclusion": "LLMs have potential in enhancing information visualization but also exhibit limitations that need addressing to improve their functionality and accuracy.", "key_contributions": ["Showcasing LLM capabilities in generating visualization code", "Analyzing LLM performance in understanding visualizations", "Identifying limitations of LLMs in complex visualization scenarios"], "limitations": "LLMs struggled with complex visualizations and made errors in understanding relationships in visual data.", "keywords": ["Information Visualization", "Large Language Models", "Code Generation", "Data Visualization", "HCI"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.22891", "pdf": "https://arxiv.org/pdf/2507.22891.pdf", "abs": "https://arxiv.org/abs/2507.22891", "title": "Real-time energy monitoring infrastructure for residential collective self-consumption operations using Linky meter", "authors": ["Jérôme Ferrari", "Benoit Delinchant", "Frédéric Wurtz", "Olga Rouchouze"], "categories": ["cs.HC"], "comment": "Cired 2025, Jun 2025, Gen{\\`e}ve (CH), Switzerland", "summary": "As part of the energy transition and the rise in energy prices, the number of\ncollective self-consumption operations in France is steadily increasing.\nHowever, energy flow monitoring currently relies on historical ''day+1'' data\nprovided by Linky meters, which does not offer real time feedback to help\nparticipants adapt their energy consumption behaviors. This article introduces\na new open-source infrastructure for real-time monitoring based on Linky meter\ndata, enabling participants to make informed decisions and take timely actions.\nIt includes a description of the xKy device, applied to a collective\nself-consumption operation involving nine participants, supported by the Energy\nTransition Observatory (OTE). The project encompasses the implementation of\ngateways in participants' homes and the development and operation of real-time\nmonitoring website, aimed at increasing participants' self-consumption rate.", "AI": {"tldr": "This article presents an open-source infrastructure for real-time energy monitoring to enhance collective self-consumption operations in France, utilizing Linky meter data.", "motivation": "To provide real-time feedback for participants in collective self-consumption operations, thereby helping them adapt their energy consumption behaviors in response to rising energy prices.", "method": "Development of an infrastructure that includes xKy devices and gateways in participants' homes, along with a real-time monitoring website.", "result": "Enhanced decision-making capabilities for nine participants in a collective self-consumption operation, leading to increased self-consumption rates.", "conclusion": "The implementation of real-time monitoring can significantly improve energy consumption behavior among participants in collective self-consumption initiatives.", "key_contributions": ["Open-source infrastructure for real-time monitoring of energy consumption", "Implementation of the xKy device for monitoring", "Development of a user-friendly real-time monitoring website"], "limitations": "", "keywords": ["Energy Transition", "Real-time Monitoring", "Collective Self-Consumption"], "importance_score": 3, "read_time_minutes": 5}}
{"id": "2507.22892", "pdf": "https://arxiv.org/pdf/2507.22892.pdf", "abs": "https://arxiv.org/abs/2507.22892", "title": "Hybrid EEG--Driven Brain--Computer Interface: A Large Language Model Framework for Personalized Language Rehabilitation", "authors": ["Ismail Hossain", "Mridul Banik"], "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "Conventional augmentative and alternative communication (AAC) systems and\nlanguage-learning platforms often fail to adapt in real time to the user's\ncognitive and linguistic needs, especially in neurological conditions such as\npost-stroke aphasia or amyotrophic lateral sclerosis. Recent advances in\nnoninvasive electroencephalography (EEG)--based brain-computer interfaces\n(BCIs) and transformer--based large language models (LLMs) offer complementary\nstrengths: BCIs capture users' neural intent with low fatigue, while LLMs\ngenerate contextually tailored language content. We propose and evaluate a\nnovel hybrid framework that leverages real-time EEG signals to drive an\nLLM-powered language rehabilitation assistant. This system aims to: (1) enable\nusers with severe speech or motor impairments to navigate language-learning\nmodules via mental commands; (2) dynamically personalize vocabulary,\nsentence-construction exercises, and corrective feedback; and (3) monitor\nneural markers of cognitive effort to adjust task difficulty on the fly.", "AI": {"tldr": "A hybrid framework combining EEG-based BCIs and LLMs for real-time language rehabilitation for users with severe impairments.", "motivation": "Conventional AAC systems and language-learning platforms lack adaptability to users' real-time cognitive and linguistic needs, particularly for individuals with neurological conditions.", "method": "The proposed system utilizes real-time EEG signals to control an LLM-powered language rehabilitation assistant, allowing users to navigate learning modules through mental commands.", "result": "The system dynamically personalizes learning content and adjusts task difficulty based on users' neural markers, significantly enhancing language rehabilitation efforts.", "conclusion": "This hybrid approach offers a novel method of supporting language learning tailored to individual cognitive states, providing immediate feedback and adaptability to enhance user experience.", "key_contributions": ["Combines EEG-based BCIs with LLMs for real-time interaction", "Personalizes language-learning experiences based on cognitive effort", "Enables navigation of language modules via mental commands"], "limitations": "", "keywords": ["augmentative and alternative communication", "brain-computer interfaces", "language rehabilitation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.22910", "pdf": "https://arxiv.org/pdf/2507.22910.pdf", "abs": "https://arxiv.org/abs/2507.22910", "title": "Large Language Models in the Travel Domain: An Industrial Experience", "authors": ["Sergio Di Meglio", "Aniello Somma", "Luigi Libero Lucio Starace", "Fabio Scippacercola", "Giancarlo Sperlì", "Sergio Di Martino"], "categories": ["cs.CL", "cs.AI"], "comment": "Manuscript accepted to the International Conference on Software\n  Engineering and Knowledge Engineering (SEKE) 2025", "summary": "Online property booking platforms are widely used and rely heavily on\nconsistent, up-to-date information about accommodation facilities, often\nsourced from third-party providers. However, these external data sources are\nfrequently affected by incomplete or inconsistent details, which can frustrate\nusers and result in a loss of market. In response to these challenges, we\npresent an industrial case study involving the integration of Large Language\nModels (LLMs) into CALEIDOHOTELS, a property reservation platform developed by\nFERVENTO. We evaluate two well-known LLMs in this context: Mistral 7B,\nfine-tuned with QLoRA, and Mixtral 8x7B, utilized with a refined system prompt.\nBoth models were assessed based on their ability to generate consistent and\nhomogeneous descriptions while minimizing hallucinations. Mixtral 8x7B\noutperformed Mistral 7B in terms of completeness (99.6% vs. 93%), precision\n(98.8% vs. 96%), and hallucination rate (1.2% vs. 4%), producing shorter yet\nmore concise content (249 vs. 277 words on average). However, this came at a\nsignificantly higher computational cost: 50GB VRAM and $1.61/hour versus 5GB\nand $0.16/hour for Mistral 7B. Our findings provide practical insights into the\ntrade-offs between model quality and resource efficiency, offering guidance for\ndeploying LLMs in production environments and demonstrating their effectiveness\nin enhancing the consistency and reliability of accommodation data.", "AI": {"tldr": "This paper discusses the integration of Large Language Models into a property booking platform to improve data consistency and quality.", "motivation": "Online property booking platforms face challenges with inconsistent and incomplete data from third-party sources, which can lead to user frustration and market loss.", "method": "The study evaluates two LLMs, Mistral 7B and Mixtral 8x7B, by assessing their ability to generate consistent and concise descriptions of accommodation facilities with minimal hallucinations.", "result": "Mixtral 8x7B outperformed Mistral 7B in completeness (99.6% vs. 93%), precision (98.8% vs. 96%), and hallucination rate (1.2% vs. 4%), while also producing shorter content on average (249 vs. 277 words).", "conclusion": "These results highlight the trade-offs between LLM quality and computational efficiency, providing guidance for deploying LLMs in production environments for improved data reliability.", "key_contributions": ["Integration of LLMs in property booking systems", "Comparative analysis of Mistral 7B and Mixtral 8x7B", "Insights on model quality vs. resource efficiency"], "limitations": "Higher computational cost associated with Mixtral 8x7B compared to Mistral 7B.", "keywords": ["Large Language Models", "property booking", "data consistency", "LLM performance", "accommodation data"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.22893", "pdf": "https://arxiv.org/pdf/2507.22893.pdf", "abs": "https://arxiv.org/abs/2507.22893", "title": "Invisible Architectures of Thought: Toward a New Science of AI as Cognitive Infrastructure", "authors": ["Giuseppe Riva"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Contemporary human-AI interaction research overlooks how AI systems\nfundamentally reshape human cognition pre-consciously, a critical blind spot\nfor understanding distributed cognition. This paper introduces \"Cognitive\nInfrastructure Studies\" (CIS) as a new interdisciplinary domain to\nreconceptualize AI as \"cognitive infrastructures\": foundational, often\ninvisible systems conditioning what is knowable and actionable in digital\nsocieties. These semantic infrastructures transport meaning, operate through\nanticipatory personalization, and exhibit adaptive invisibility, making their\ninfluence difficult to detect. Critically, they automate \"relevance judgment,\"\nshifting the \"locus of epistemic agency\" to non-human systems. Through\nnarrative scenarios spanning individual (cognitive dependency), collective\n(democratic deliberation), and societal (governance) scales, we describe how\ncognitive infrastructures reshape human cognition, public reasoning, and social\nepistemologies. CIS aims to address how AI preprocessing reshapes distributed\ncognition across individual, collective, and cultural scales, requiring\nunprecedented integration of diverse disciplinary methods. The framework also\naddresses critical gaps across disciplines: cognitive science lacks\npopulation-scale preprocessing analysis capabilities, digital sociology cannot\naccess individual cognitive mechanisms, and computational approaches miss\ncultural transmission dynamics. To achieve this goal CIS also provides\nmethodological innovations for studying invisible algorithmic influence:\n\"infrastructure breakdown methodologies\", experimental approaches that reveal\ncognitive dependencies by systematically withdrawing AI preprocessing after\nperiods of habituation.", "AI": {"tldr": "This paper introduces Cognitive Infrastructure Studies (CIS) to understand how AI systems fundamentally reshape human cognition and social epistemologies through their semantically rich, often invisible structures.", "motivation": "To address the overlooked impact of AI systems on human cognition and the need for a nuanced understanding of distributed cognition in digital societies.", "method": "The paper proposes a new interdisciplinary framework, Cognitive Infrastructure Studies (CIS), combining narrative scenarios with infrastructure breakdown methodologies to study the automation of relevance judgment and cognitive dependencies.", "result": "CIS demonstrates that AI functions as cognitive infrastructures that influence individual, collective, and cultural cognition, altering public reasoning and governance.", "conclusion": "The study underscores the necessity of integrating diverse disciplinary methods to comprehensively analyze the impact of AI on cognition, public reasoning, and social dynamics.", "key_contributions": ["Introduction of Cognitive Infrastructure Studies (CIS) as a new research domain", "Proposes infrastructure breakdown methodologies to study AI's cognitive impact", "Framework for understanding the role of AI in shaping public reasoning and social epistemologies"], "limitations": "", "keywords": ["Cognitive Infrastructure Studies", "human-AI interaction", "cognition", "epistemic agency", "algorithmic influence"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2507.22911", "pdf": "https://arxiv.org/pdf/2507.22911.pdf", "abs": "https://arxiv.org/abs/2507.22911", "title": "ElectriQ: A Benchmark for Assessing the Response Capability of Large Language Models in Power Marketing", "authors": ["Jinzhi Wang", "Qingke Peng", "Haozhou Li", "Zeyuan Zeng", "Qinfeng Song", "Kaixuan Yang", "Jiangbo Zhang", "Yaoying Wang", "Ruimeng Li", "Biyi Zhou"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Electric power marketing customer service plays a critical role in addressing\ninquiries, complaints, and service requests. However, current systems, such as\nChina's 95598 hotline, often struggle with slow response times, inflexible\nprocedures, and limited accuracy in domain-specific tasks. While large language\nmodels (LLMs) like GPT-4o and Claude 3 demonstrate strong general capabilities,\nthey lack the domain expertise and empathy required in this field. To bridge\nthis gap, we introduce ElectriQ, the first benchmark designed to evaluate and\nenhance LLMs in electric power marketing scenarios. ElectriQ consists of a\ndialogue dataset covering six key service categories and introduces four\nevaluation metrics: professionalism, popularity, readability, and\nuser-friendliness. We further incorporate a domain-specific knowledge base and\npropose a knowledge augmentation method to boost model performance. Experiments\non 13 LLMs reveal that smaller models such as LLama3-8B, when fine-tuned and\naugmented, can surpass GPT-4o in terms of professionalism and\nuser-friendliness. ElectriQ establishes a comprehensive foundation for\ndeveloping LLMs tailored to the needs of power marketing services.", "AI": {"tldr": "Introducing ElectriQ, a benchmark for evaluating and enhancing LLMs for electric power marketing customer service, addressing limitations of existing systems.", "motivation": "Electric power marketing customer service is crucial for addressing inquiries and complaints, yet current systems suffer from slow responses and inflexible procedures. The need for domain-specific capabilities and empathy in LLMs drives this research.", "method": "ElectriQ provides a dialogue dataset across six service categories and employs four evaluation metrics: professionalism, popularity, readability, and user-friendliness. A domain-specific knowledge base and a knowledge augmentation method are used to improve model performance.", "result": "Experiments showed that smaller fine-tuned models like LLama3-8B can outperform larger models like GPT-4o in professionalism and user-friendliness after knowledge augmentation.", "conclusion": "ElectriQ lays the groundwork for developing LLMs that are specifically designed to improve customer service in electric power marketing, highlighting the importance of contextual understanding and user engagement.", "key_contributions": ["Introduction of the ElectriQ benchmark for LLMs in power marketing", "Development of a dialogue dataset and evaluation metrics tailored for customer service", "Knowledge augmentation method to enhance model performance"], "limitations": "Limited to electric power marketing domain; results may not generalize to other sectors.", "keywords": ["Electric power marketing", "Large language models", "Benchmark", "Customer service", "Knowledge augmentation"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.22894", "pdf": "https://arxiv.org/pdf/2507.22894.pdf", "abs": "https://arxiv.org/abs/2507.22894", "title": "When no one shows up (at first): Navigating the uncertainties of participatory workshops in interdisciplinary research", "authors": ["Monique Munarini"], "categories": ["cs.HC"], "comment": "Presented at HHAI25:The 4th International Conference Series on Hybrid\n  Human-Artificial Intelligence, workshop Mind the AI-GAP 2025:Co-Designing\n  Socio-Technical Systems. (June 9-13, 2025 in Pisa, Italy)", "summary": "This reflective paper explores often-unspoken challenges of designing and\nfacilitating co-design and participatory workshops, offering practical\nstrategies for early career researchers (ECRs) navigating these methods.\nDrawing from personal experience conducting a series of workshops titled: How\nto Think About Equity in the AI Ecosystem. It follows the full arc of the\nworkshop experience, from conceptualization and activity planning to\nparticipant recruitment and facilitation, offering a grounded account of what\nhappens when participation does not go as expected. The paper examines the\nmethodological challenges of engaging non-expert participants, particularly\nwhen operating without institutional support, financial incentives, or\nintegration into larger events. Despite initial difficulties such as low\nattendance, the workshop fostered rich discussions among a demographically\ndiverse group and ultimately led to one participant volunteering to\nco-facilitate a subsequent session. This transition from participant to\nco-facilitator exemplifies the redistribution of epistemic authority,\npositioning lived experience as central to research and engagement practices.\nBy reframing perceived failure as a productive site of learning, the paper\noffers practical strategies for ECRs working across disciplines who often\nnavigate unfamiliar methodological terrains, contributing to broader\nconversations on the realities of doing interdisciplinary, participatory work\nin practice.", "AI": {"tldr": "Reflective paper on challenges in co-design workshops, offering strategies for early career researchers.", "motivation": "To address the unspoken challenges faced during co-design and participatory workshops, particularly for early career researchers.", "method": "The paper reflects on personal experiences from a series of workshops focused on equity in the AI ecosystem, detailing the entire workshop process from planning to facilitation.", "result": "Despite initial challenges like low attendance, the workshops led to rich discussions and demonstrated the potential for participant engagement, exemplified by one participant moving to a co-facilitator role.", "conclusion": "By reframing challenges as learning opportunities, the paper provides valuable strategies for conducting interdisciplinary participatory research.", "key_contributions": ["Offers insights into challenges of co-design workshops", "Provides practical strategies for engagement without institutional support", "Highlights the importance of participant experience in research"], "limitations": "", "keywords": ["co-design", "participatory workshops", "equity in AI", "early career researchers", "methodological challenges"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.22912", "pdf": "https://arxiv.org/pdf/2507.22912.pdf", "abs": "https://arxiv.org/abs/2507.22912", "title": "A Language Model-Driven Semi-Supervised Ensemble Framework for Illicit Market Detection Across Deep/Dark Web and Social Platforms", "authors": ["Navid Yazdanjue", "Morteza Rakhshaninejad", "Hossein Yazdanjouei", "Mohammad Sadegh Khorshidi", "Mikko S. Niemela", "Fang Chen", "Amir H. Gandomi"], "categories": ["cs.CL", "cs.AI", "cs.LG", "68T07, 68T50"], "comment": "16 pages, 5 figures, 9 tables", "summary": "Illegal marketplaces have increasingly shifted to concealed parts of the\ninternet, including the deep and dark web, as well as platforms such as\nTelegram, Reddit, and Pastebin. These channels enable the anonymous trade of\nillicit goods including drugs, weapons, and stolen credentials. Detecting and\ncategorizing such content remains challenging due to limited labeled data, the\nevolving nature of illicit language, and the structural heterogeneity of online\nsources. This paper presents a hierarchical classification framework that\ncombines fine-tuned language models with a semi-supervised ensemble learning\nstrategy to detect and classify illicit marketplace content across diverse\nplatforms. We extract semantic representations using ModernBERT, a transformer\nmodel for long documents, finetuned on domain-specific data from deep and dark\nweb pages, Telegram channels, Subreddits, and Pastebin pastes to capture\nspecialized jargon and ambiguous linguistic patterns. In addition, we\nincorporate manually engineered features such as document structure, embedded\npatterns including Bitcoin addresses, emails, and IPs, and metadata, which\ncomplement language model embeddings. The classification pipeline operates in\ntwo stages. The first stage uses a semi-supervised ensemble of XGBoost, Random\nForest, and SVM with entropy-based weighted voting to detect sales-related\ndocuments. The second stage further classifies these into drug, weapon, or\ncredential sales. Experiments on three datasets, including our multi-source\ncorpus, DUTA, and CoDA, show that our model outperforms several baselines,\nincluding BERT, ModernBERT, DarkBERT, ALBERT, Longformer, and BigBird. The\nmodel achieves an accuracy of 0.96489, an F1-score of 0.93467, and a TMCC of\n0.95388, demonstrating strong generalization, robustness under limited\nsupervision, and effectiveness in real-world illicit content detection.", "AI": {"tldr": "This paper presents a hierarchical classification framework for detecting and classifying illicit marketplace content on the deep and dark web using fine-tuned language models and a semi-supervised learning strategy.", "motivation": "The increasing shift of illegal marketplaces to the deep and dark web complicates the detection and classification of illicit content due to limited labeled data and evolving language.", "method": "A hierarchical classification framework is proposed that combines fine-tuned language models such as ModernBERT with a semi-supervised ensemble approach utilizing XGBoost, Random Forest, and SVM for document detection and classification.", "result": "The proposed model outperforms several baseline models, achieving an accuracy of 0.96489, an F1-score of 0.93467, and a TMCC of 0.95388.", "conclusion": "The framework demonstrates strong generalization and robustness for detecting illicit content with limited supervision, proving effective in real-world applications.", "key_contributions": ["Development of a hierarchical classification framework combining language models with semi-supervised learning", "Use of ModernBERT fine-tuned on domain-specific data", "Incorporation of engineered features like document structure and embedded patterns."], "limitations": "", "keywords": ["illicit content detection", "language models", "semi-supervised learning", "deep web", "dark web"], "importance_score": 3, "read_time_minutes": 20}}
{"id": "2507.22895", "pdf": "https://arxiv.org/pdf/2507.22895.pdf", "abs": "https://arxiv.org/abs/2507.22895", "title": "Brain motor intention Extraction Amplifier: Non-invasive brain-muscle interface", "authors": ["Ye Sun", "Bowei Zhao", "Dezhong Yao", "Rui Zhang", "Bohan Zhang", "Xiaoyuan Li", "Jing Wang", "Mingxuan Qu", "Gang Liu"], "categories": ["cs.HC"], "comment": "18 pages, 9 figures", "summary": "Brain-computer interfaces (BCIs) enable real-time interaction between the\nbrain and external devices by decoding neural signals. However, existing\nmotor-based BCI paradigms, like motor imagery BCI, face challenges with\nimprecise labeling in real-world use. This mismatch between EEG signals and\ntrue behavioral intentions leads to pseudo-labels, undermining decoding\naccuracy and system robustness. To overcome this bottleneck, this paper first\nproposes a novel motor intention extraction framework based on a non-invasive\nbrain-muscle interface (BMuI)($\\text{BCI} =\n\\frac{\\text{Brain}}{\\text{Computer}} \\text{ Interface} =\n\\frac{\\text{Brain}}{\\not\\text{Muscle}}\\! \\text{ (BMuI)} \\times\n\\!\\frac{\\not\\text{Muscle}}{\\text{Computer}}\\! \\text{ Interface}$). This method\nsimulates the neural pathway from the brain to the muscles in order to capture\nand enhance the weak motor intention signals originating in the brain. It then\nuses EMG as a high-fidelity relay medium to achieve more accurate intention\nrecognition and transmission. To systematically validate the feasibility and\neffectiveness of this approach, we conducted both offline experiments (to\nrepeatedly verify feasibility) and online experiments (to construct a real-time\ninteractive system and evaluate its performance). The results show that BMuI is\nfeasible, achieving a prediction accuracy of 0.8314; in the online experiment,\nall participants are able to successfully control the Unity virtual arm.", "AI": {"tldr": "This paper introduces a novel motor intention extraction framework based on a brain-muscle interface (BMuI) to improve the accuracy of brain-computer interfaces (BCIs) in recognizing motor intentions.", "motivation": "Existing BCI paradigms face challenges in accuracy due to imprecise labeling in real-world applications, leading to pseudo-labels that impact decoding performance.", "method": "The proposed BMuI framework simulates the neural pathway from the brain to the muscles, utilizing EMG signals as a relay to enhance motor intention recognition.", "result": "The BMuI method achieved a prediction accuracy of 0.8314 in offline experiments and effectively controlled a Unity virtual arm in online trials, demonstrating its feasibility and effectiveness.", "conclusion": "The study validates the BMuI framework as a promising approach to improving BCI accuracy and robustness in real-time applications.", "key_contributions": ["Introduction of the BMuI framework for motor intention extraction", "Demonstration of improved accuracy through EMG relay medium", "Validation through both offline and online experiments"], "limitations": "", "keywords": ["Brain-computer interfaces", "motor intention extraction", "EMG", "real-time interaction", "neural signals"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2507.22913", "pdf": "https://arxiv.org/pdf/2507.22913.pdf", "abs": "https://arxiv.org/abs/2507.22913", "title": "A Hybrid Framework for Subject Analysis: Integrating Embedding-Based Regression Models with Large Language Models", "authors": ["Jinyu Liu", "Xiaoying Song", "Diana Zhang", "Jason Thomale", "Daqing He", "Lingzi Hong"], "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 2 figures, accepted by ASIST 2025", "summary": "Providing subject access to information resources is an essential function of\nany library management system. Large language models (LLMs) have been widely\nused in classification and summarization tasks, but their capability to perform\nsubject analysis is underexplored. Multi-label classification with traditional\nmachine learning (ML) models has been used for subject analysis but struggles\nwith unseen cases. LLMs offer an alternative but often over-generate and\nhallucinate. Therefore, we propose a hybrid framework that integrates\nembedding-based ML models with LLMs. This approach uses ML models to (1)\npredict the optimal number of LCSH labels to guide LLM predictions and (2)\npost-edit the predicted terms with actual LCSH terms to mitigate\nhallucinations. We experimented with LLMs and the hybrid framework to predict\nthe subject terms of books using the Library of Congress Subject Headings\n(LCSH). Experiment results show that providing initial predictions to guide LLM\ngenerations and imposing post-edits result in more controlled and\nvocabulary-aligned outputs.", "AI": {"tldr": "This paper proposes a hybrid framework integrating ML models and LLMs for improved subject analysis in library management systems.", "motivation": "To enhance subject access in library systems using LLMs, addressing their limitations in subject analysis.", "method": "A hybrid framework combining embedding-based ML models with LLMs, using ML for initial predictions of LCSH labels and LLMs for post-editing these predictions.", "result": "A controlled and vocabulary-aligned prediction of subject terms for books, outperforming traditional ML-only models.", "conclusion": "The hybrid approach demonstrates better accuracy and reliability in LCSH term predictions compared to solely relying on LLMs.", "key_contributions": ["Development of a hybrid framework for subject analysis", "Improved predictions for Library of Congress Subject Headings", "Mitigation of LLM hallucination effects"], "limitations": "The study may be limited by the specific dataset of books and LCSH used in experiments.", "keywords": ["hybrid framework", "large language models", "subject analysis", "multi-label classification", "LCSH"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.22896", "pdf": "https://arxiv.org/pdf/2507.22896.pdf", "abs": "https://arxiv.org/abs/2507.22896", "title": "iLearnRobot: An Interactive Learning-Based Multi-Modal Robot with Continuous Improvement", "authors": ["Kohou Wang", "ZhaoXiang Liu", "Lin Bai", "Kun Fan", "Xiang Liu", "Huan Hu", "Kai Wang", "Shiguo Lian"], "categories": ["cs.HC", "cs.AI", "cs.CV", "cs.RO"], "comment": "17 pages, 12 figures", "summary": "It is crucial that robots' performance can be improved after deployment, as\nthey are inherently likely to encounter novel scenarios never seen before. This\npaper presents an innovative solution: an interactive learning-based robot\nsystem powered by a Multi-modal Large Language Model(MLLM). A key feature of\nour system is its ability to learn from natural dialogues with non-expert\nusers. We also propose chain of question to clarify the exact intent of the\nquestion before providing an answer and dual-modality retrieval modules to\nleverage these interaction events to avoid repeating same mistakes, ensuring a\nseamless user experience before model updates, which is in contrast to current\nmainstream MLLM-based robotic systems. Our system marks a novel approach in\nrobotics by integrating interactive learning, paving the way for superior\nadaptability and performance in diverse environments. We demonstrate the\neffectiveness and improvement of our method through experiments, both\nquantitively and qualitatively.", "AI": {"tldr": "This paper introduces an interactive learning-based robotic system leveraging a Multi-modal Large Language Model to improve robot performance post-deployment through user dialogues.", "motivation": "To enable robots to adapt and learn from novel scenarios after deployment, enhancing their performance through user interactions.", "method": "The proposed system incorporates an MLLM that facilitates learning from natural dialogues with users, along with a chain of questions to clarify user intent and dual-modality retrieval modules for mistake correction.", "result": "Experiments demonstrate that the system significantly enhances robot adaptability and performance in diverse environments.", "conclusion": "Integrating interactive learning into robotic systems offers a novel pathway for achieving better adaptability and user experience in robotics.", "key_contributions": ["Introduction of a learning-based approach for robots using MLLM.", "Implementation of dual-modality retrieval modules to enhance user interactions.", "Demonstration of improved robot performance through interactive learning."], "limitations": "The paper does not explore the long-term effects of continuous learning in dynamic environments.", "keywords": ["robotics", "interactive learning", "Multi-modal Large Language Model", "user interaction", "adaptability"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.22914", "pdf": "https://arxiv.org/pdf/2507.22914.pdf", "abs": "https://arxiv.org/abs/2507.22914", "title": "Full Triple Matcher: Integrating all triple elements between heterogeneous Knowledge Graphs", "authors": ["Victor Eiti Yamamoto", "Hideaki Takeda"], "categories": ["cs.CL"], "comment": null, "summary": "Knowledge graphs (KGs) are powerful tools for representing and reasoning over\nstructured information. Their main components include schema, identity, and\ncontext. While schema and identity matching are well-established in ontology\nand entity matching research, context matching remains largely unexplored. This\nis particularly important because real-world KGs often vary significantly in\nsource, size, and information density - factors not typically represented in\nthe datasets on which current entity matching methods are evaluated. As a\nresult, existing approaches may fall short in scenarios where diverse and\ncomplex contexts need to be integrated.\n  To address this gap, we propose a novel KG integration method consisting of\nlabel matching and triple matching. We use string manipulation, fuzzy matching,\nand vector similarity techniques to align entity and predicate labels. Next, we\nidentify mappings between triples that convey comparable information, using\nthese mappings to improve entity-matching accuracy. Our approach demonstrates\ncompetitive performance compared to leading systems in the OAEI competition and\nagainst supervised methods, achieving high accuracy across diverse test cases.\nAdditionally, we introduce a new dataset derived from the benchmark dataset to\nevaluate the triple-matching step more comprehensively.", "AI": {"tldr": "The paper presents a novel method for knowledge graph integration focusing on context matching, addressing limitations in existing entity matching approaches.", "motivation": "To address the gap in knowledge graph integration related to context matching, which remains largely unexplored despite its importance in handling diverse real-world KGs.", "method": "The proposed method consists of label matching and triple matching using string manipulation, fuzzy matching, and vector similarity techniques to align entity and predicate labels and identify mappings between triples conveying comparable information.", "result": "The method demonstrates competitive performance in the OAEI competition and against supervised methods, achieving high accuracy across diverse test cases.", "conclusion": "The novel KG integration method improves entity-matching accuracy and is validated using a new dataset for comprehensive evaluation of the triple-matching step.", "key_contributions": ["Novel approach to context matching in knowledge graph integration", "Use of string manipulation, fuzzy matching, and vector similarity for entity label alignment", "Introduction of a new dataset for evaluating triple matching"], "limitations": "", "keywords": ["knowledge graphs", "context matching", "entity matching", "KG integration", "triple matching"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.22897", "pdf": "https://arxiv.org/pdf/2507.22897.pdf", "abs": "https://arxiv.org/abs/2507.22897", "title": "RecUserSim: A Realistic and Diverse User Simulator for Evaluating Conversational Recommender Systems", "authors": ["Luyu Chen", "Quanyu Dai", "Zeyu Zhang", "Xueyang Feng", "Mingyu Zhang", "Pengcheng Tang", "Xu Chen", "Yue Zhu", "Zhenhua Dong"], "categories": ["cs.HC", "cs.AI"], "comment": "Accepted by TheWebConf'25 Industry Track", "summary": "Conversational recommender systems (CRS) enhance user experience through\nmulti-turn interactions, yet evaluating CRS remains challenging. User\nsimulators can provide comprehensive evaluations through interactions with CRS,\nbut building realistic and diverse simulators is difficult. While recent work\nleverages large language models (LLMs) to simulate user interactions, they\nstill fall short in emulating individual real users across diverse scenarios\nand lack explicit rating mechanisms for quantitative evaluation. To address\nthese gaps, we propose RecUserSim, an LLM agent-based user simulator with\nenhanced simulation realism and diversity while providing explicit scores.\nRecUserSim features several key modules: a profile module for defining\nrealistic and diverse user personas, a memory module for tracking interaction\nhistory and discovering unknown preferences, and a core action module inspired\nby Bounded Rationality theory that enables nuanced decision-making while\ngenerating more fine-grained actions and personalized responses. To further\nenhance output control, a refinement module is designed to fine-tune final\nresponses. Experiments demonstrate that RecUserSim generates diverse,\ncontrollable outputs and produces realistic, high-quality dialogues, even with\nsmaller base LLMs. The ratings generated by RecUserSim show high consistency\nacross different base LLMs, highlighting its effectiveness for CRS evaluation.", "AI": {"tldr": "RecUserSim is an LLM-based user simulator designed to enhance the evaluation of conversational recommender systems (CRS) by providing realistic interactions and explicit scores.", "motivation": "Conversational recommender systems improve user experience through rich interactions, but evaluating their effectiveness poses challenges. Existing user simulators lack realism and diversity, which are essential for accurate evaluation.", "method": "RecUserSim employs an LLM agent to simulate user interactions, incorporating a profile module for diverse personas, a memory module for interaction history, and a decision-making module based on Bounded Rationality theory.", "result": "RecUserSim generates high-quality dialogues with diverse outputs and maintains consistency in ratings across different base LLMs.", "conclusion": "RecUserSim significantly improves the evaluation of CRS by offering realistic simulations and effective scoring mechanisms.", "key_contributions": ["LLM-based user simulation for CRS evaluation", "Realistic user persona profiles", "Enhanced interaction tracking and decision-making"], "limitations": "", "keywords": ["Conversational Recommender Systems", "User Simulation", "Large Language Models"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2507.22915", "pdf": "https://arxiv.org/pdf/2507.22915.pdf", "abs": "https://arxiv.org/abs/2507.22915", "title": "Theoretical Foundations and Mitigation of Hallucination in Large Language Models", "authors": ["Esmail Gumaan"], "categories": ["cs.CL", "cs.AI"], "comment": "12 pages", "summary": "Hallucination in Large Language Models (LLMs) refers to the generation of\ncontent that is not faithful to the input or the real-world facts. This paper\nprovides a rigorous treatment of hallucination in LLMs, including formal\ndefinitions and theoretical analyses. We distinguish between intrinsic and\nextrinsic hallucinations, and define a \\textit{hallucination risk} for models.\nWe derive bounds on this risk using learning-theoretic frameworks (PAC-Bayes\nand Rademacher complexity). We then survey detection strategies for\nhallucinations, such as token-level uncertainty estimation, confidence\ncalibration, and attention alignment checks. On the mitigation side, we discuss\napproaches including retrieval-augmented generation, hallucination-aware\nfine-tuning, logit calibration, and the incorporation of fact-verification\nmodules. We propose a unified detection and mitigation workflow, illustrated\nwith a diagram, to integrate these strategies. Finally, we outline evaluation\nprotocols for hallucination, recommending datasets, metrics, and experimental\nsetups to quantify and reduce hallucinations. Our work lays a theoretical\nfoundation and practical guidelines for addressing the crucial challenge of\nhallucination in LLMs.", "AI": {"tldr": "This paper rigorously explores hallucination in Large Language Models (LLMs), defining it, analyzing its risks, and proposing detection and mitigation strategies.", "motivation": "To address the issue of hallucination in LLMs, which generates content not aligned with factual information, impacting the reliability of AI systems.", "method": "Formal definitions are provided for intrinsic and extrinsic hallucinations, and a 'hallucination risk' metric is derived using PAC-Bayes and Rademacher complexity. A survey of detection strategies and mitigation techniques is also included.", "result": "A unified detection and mitigation workflow is proposed to integrate strategies for addressing hallucination, along with recommended evaluation protocols.", "conclusion": "The paper lays a theoretical foundation and offers practical guidelines to reduce hallucinations, enhancing the reliability of LLMs.", "key_contributions": ["Rigorous definitions and classifications of hallucination in LLMs.", "Derivation of bounds on hallucination risk using learning-theoretic frameworks.", "Proposed unified workflow for detection and mitigation strategies for hallucinations."], "limitations": "", "keywords": ["Large Language Models", "Hallucination", "Detection Strategies", "Mitigation Techniques", "Evaluation Protocols"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2507.22898", "pdf": "https://arxiv.org/pdf/2507.22898.pdf", "abs": "https://arxiv.org/abs/2507.22898", "title": "Voice-guided Orchestrated Intelligence for Clinical Evaluation (VOICE): A Voice AI Agent System for Prehospital Stroke Assessment", "authors": ["Julian Acosta", "Scott Adams", "Julius Kernbach", "Romain Hardy", "Sung Eun Kim", "Luyang Luo", "Xiaoman Zhang", "Shreya Johri", "Mohammed Baharoon", "Pranav Rajpurkar"], "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "We developed a voice-driven artificial intelligence (AI) system that guides\nanyone - from paramedics to family members - through expert-level stroke\nevaluations using natural conversation, while also enabling smartphone video\ncapture of key examination components for documentation and potential expert\nreview. This addresses a critical gap in emergency care: current stroke\nrecognition by first responders is inconsistent and often inaccurate, with\nsensitivity for stroke detection as low as 58%, causing life-threatening delays\nin treatment. Three non-medical volunteers used our AI system to assess ten\nsimulated stroke patients, including cases with likely large vessel occlusion\n(LVO) strokes and stroke-like conditions, while we measured diagnostic\naccuracy, completion times, user confidence, and expert physician review of the\nAI-generated reports. The AI system correctly identified 84% of individual\nstroke signs and detected 75% of likely LVOs, completing evaluations in just\nover 6 minutes. Users reported high confidence (median 4.5/5) and ease of use\n(mean 4.67/5). The system successfully identified 86% of actual strokes but\nalso incorrectly flagged 2 of 3 non-stroke cases as strokes. When an expert\nphysician reviewed the AI reports with videos, they identified the correct\ndiagnosis in 100% of cases, but felt confident enough to make preliminary\ntreatment decisions in only 40% of cases due to observed AI errors including\nincorrect scoring and false information. While the current system's limitations\nnecessitate human oversight, ongoing rapid advancements in speech-to-speech AI\nmodels suggest that future versions are poised to enable highly accurate\nassessments. Achieving human-level voice interaction could transform emergency\nmedical care, putting expert-informed assessment capabilities in everyone's\nhands.", "AI": {"tldr": "A voice-driven AI system for stroke evaluations shows promise in improving diagnostic accuracy and user confidence but requires human oversight due to some inaccuracies.", "motivation": "To improve the inconsistent and inaccurate stroke recognition by first responders, thereby reducing delays in treatment.", "method": "The AI system guides users through stroke evaluations using natural conversation and captures video for documentation. Testing involved non-medical volunteers assessing simulated stroke patients.", "result": "The AI achieved 84% accuracy in identifying stroke signs and 75% detection of large vessel occlusions, with evaluations completed in about 6 minutes. Users reported high confidence and ease of use, but the system misidentified some non-stroke cases.", "conclusion": "While the AI system requires human oversight due to errors, advancements in AI suggest future iterations may provide highly accurate assessments, revolutionizing emergency medical care.", "key_contributions": ["Development of a voice-driven AI for emergency stroke evaluations", "Demonstrated high user confidence and ease of use", "Identified critical need for AI oversight in medical applications"], "limitations": "The system incorrectly flags some non-stroke cases and needs human oversight due to inaccuracies.", "keywords": ["Artificial Intelligence", "Stroke Evaluation", "Emergency Care", "Natural Conversation", "Diagnostic Accuracy"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.22917", "pdf": "https://arxiv.org/pdf/2507.22917.pdf", "abs": "https://arxiv.org/abs/2507.22917", "title": "Reading Between the Timelines: RAG for Answering Diachronic Questions", "authors": ["Kwun Hang Lau", "Ruiyuan Zhang", "Weijie Shi", "Xiaofang Zhou", "Xiaojun Cheng"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "While Retrieval-Augmented Generation (RAG) excels at injecting static,\nfactual knowledge into Large Language Models (LLMs), it exhibits a critical\ndeficit in handling longitudinal queries that require tracking entities and\nphenomena across time. This blind spot arises because conventional,\nsemantically-driven retrieval methods are not equipped to gather evidence that\nis both topically relevant and temporally coherent for a specified duration. We\naddress this challenge by proposing a new framework that fundamentally\nredesigns the RAG pipeline to infuse temporal logic. Our methodology begins by\ndisentangling a user's query into its core subject and its temporal window. It\nthen employs a specialized retriever that calibrates semantic matching against\ntemporal relevance, ensuring the collection of a contiguous evidence set that\nspans the entire queried period. To enable rigorous evaluation of this\ncapability, we also introduce the Analytical Diachronic Question Answering\nBenchmark (ADQAB), a challenging evaluation suite grounded in a hybrid corpus\nof real and synthetic financial news. Empirical results on ADQAB show that our\napproach yields substantial gains in answer accuracy, surpassing standard RAG\nimplementations by 13% to 27%. This work provides a validated pathway toward\nRAG systems capable of performing the nuanced, evolutionary analysis required\nfor complex, real-world questions. The dataset and code for this study are\npublicly available at https://github.com/kwunhang/TA-RAG.", "AI": {"tldr": "The paper proposes a new framework for Retrieval-Augmented Generation (RAG) that incorporates temporal logic to effectively handle longitudinal queries, improving accuracy in answer retrieval by redesigning the RAG pipeline.", "motivation": "To address the limitations of conventional retrieval methods in handling temporal queries, which require tracking entities and phenomena over time.", "method": "The methodology involves disentangling a user's query into its subject and temporal window and using a specialized retriever that matches semantic and temporal relevance to collect a contiguous evidence set over the queried period.", "result": "Empirical results show that the proposed approach improves answer accuracy by 13% to 27% compared to standard RAG implementations on the Analytical Diachronic Question Answering Benchmark (ADQAB).", "conclusion": "This work validates a pathway for enhancing RAG systems to perform complex, real-world longitudinal analysis.", "key_contributions": ["Introduction of a temporal logic framework into RAG systems", "Development of the Analytical Diachronic Question Answering Benchmark (ADQAB)", "Demonstrated significant accuracy improvements in answer retrieval for temporal queries"], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "temporal logic", "longitudinal queries", "Analytical Diachronic Question Answering Benchmark", "accuracy"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.22899", "pdf": "https://arxiv.org/pdf/2507.22899.pdf", "abs": "https://arxiv.org/abs/2507.22899", "title": "A visual analytics tool for taxonomy-based trajectory data exploration", "authors": ["Ivan A. Hanono Cozzetti", "Ahmad Abdou"], "categories": ["cs.HC"], "comment": "71 pages, 92 figures", "summary": "The analysis of spatio-temporal data presents significant challenges due to\nthe complexity and heterogeneity of movement patterns. This project proposes a\ndata analytics tool that combines data visualization and statistical\ncomputation to facilitate spatio-temporal data analysis through a multi-level\napproach. The tool categorizes moving objects into distinct taxonomies using\nMachine Learning models, adding meaningful structure to the analysis. Two case\nstudies demonstrate the methodology's effectiveness. The first analyzed Arctic\nfox trajectories, successfully identifying and labeling foxes with Geometric or\nKinematic-based behaviors, further categorized into Curvature and Acceleration\ngroups. Statistical indicators revealed that foxes with Acceleration-based\nbehavior showed constant, steady acceleration, while those with Curvature-based\nbehavior exhibited acceleration peaks and sudden deceleration. The second case\nstudy examined tropical cyclone data, labeling trajectories with Speed,\nCurvature, and hybrid Geometric-based behaviors through unique statistical\nvariables. Analysis of hybrid Geometric behavior (Curvature and Indentation\ncombined) identified specific angles with the highest impact on hurricane shape\nand geometry. The proposed method and tool demonstrate that spatio-temporal\ndata, despite inherent complexity, can be analyzed and explained in detail,\nproviding a theoretical and practical blueprint applicable to multiple domains.", "AI": {"tldr": "A spatio-temporal data analytics tool utilizing ML for categorization and visualization.", "motivation": "To address the challenges in analyzing complex and heterogeneous movement patterns in spatio-temporal data.", "method": "The tool employs a multi-level approach combining data visualization and statistical computation to categorize moving objects using Machine Learning models.", "result": "Case studies on Arctic fox trajectories and tropical cyclone data demonstrated the effectiveness of the methodology in identifying and labeling distinct movement behaviors.", "conclusion": "The proposed method allows detailed analysis of spatio-temporal data, offering a framework applicable across various domains.", "key_contributions": ["Development of a multi-level spatio-temporal data analytics tool", "Application of Machine Learning for categorizing movement patterns", "Case studies demonstrating practical applications in wildlife and meteorology"], "limitations": "", "keywords": ["spatio-temporal data", "data visualization", "machine learning", "behavior analysis"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2507.22918", "pdf": "https://arxiv.org/pdf/2507.22918.pdf", "abs": "https://arxiv.org/abs/2507.22918", "title": "Semantic Convergence: Investigating Shared Representations Across Scaled LLMs", "authors": ["Daniel Son", "Sanjana Rathore", "Andrew Rufail", "Adrian Simon", "Daniel Zhang", "Soham Dave", "Cole Blondin", "Kevin Zhu", "Sean O'Brien"], "categories": ["cs.CL", "cs.LG", "68T50", "I.2.6; I.2.7"], "comment": "Submitted to ACL 2025 Student Research Workshop (poster)", "summary": "We investigate feature universality in Gemma-2 language models (Gemma-2-2B\nand Gemma-2-9B), asking whether models with a four-fold difference in scale\nstill converge on comparable internal concepts. Using the Sparse Autoencoder\n(SAE) dictionary-learning pipeline, we utilize SAEs on each model's\nresidual-stream activations, align the resulting monosemantic features via\nactivation correlation, and compare the matched feature spaces with SVCCA and\nRSA. Middle layers yield the strongest overlap, while early and late layers\nshow far less similarity. Preliminary experiments extend the analysis from\nsingle tokens to multi-token subspaces, showing that semantically similar\nsubspaces interact similarly with language models. These results strengthen the\ncase that large language models carve the world into broadly similar,\ninterpretable features despite size differences, reinforcing universality as a\nfoundation for cross-model interpretability.", "AI": {"tldr": "The paper investigates feature universality in Gemma-2 language models of differing scales, finding that they converge on comparable internal concepts, particularly in middle layers.", "motivation": "To explore whether language models of different sizes (Gemma-2-2B and Gemma-2-9B) converge on similar internal representations, thus supporting the notion of universality in language model interpretability.", "method": "The study uses Sparse Autoencoders (SAEs) to analyze residual-stream activations of both models, aligning monosemantic features through activation correlation and comparing feature spaces using SVCCA and RSA.", "result": "Findings indicate that middle layers of both models show significant overlap in features, while early and late layers exhibit less similarity. Multi-token subspace analysis reveals interactions between semantically similar subspaces.", "conclusion": "The research supports the idea that large language models develop broadly similar, interpretable features regardless of their size, suggesting universality as key for cross-model interpretability.", "key_contributions": ["Demonstrated feature overlap in middle layers of differently sized models", "Extended analysis to multi-token subspaces", "Strengthened the argument for universality in language model features"], "limitations": "", "keywords": ["Feature Universality", "Language Models", "Sparse Autoencoders"], "importance_score": 7, "read_time_minutes": 12}}
{"id": "2507.22900", "pdf": "https://arxiv.org/pdf/2507.22900.pdf", "abs": "https://arxiv.org/abs/2507.22900", "title": "Tool or Trouble? Exploring Student Attitudes Toward AI Coding Assistants", "authors": ["Sergio Rojas-Galeano"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "This exploratory study examines how AI code assistants shape novice\nprogrammers' experiences during a two-part exam in an introductory programming\ncourse. In the first part, students completed a programming task with access to\nAI support; in the second, they extended their solutions without AI. We\ncollected Likert-scale and open-ended responses from 20 students to evaluate\ntheir perceptions and challenges. Findings suggest that AI tools were perceived\nas helpful for understanding code and increasing confidence, particularly\nduring initial development. However, students reported difficulties\ntransferring knowledge to unaided tasks, revealing possible overreliance and\ngaps in conceptual understanding. These insights highlight the need for\npedagogical strategies that integrate AI meaningfully while reinforcing\nfoundational programming skills.", "AI": {"tldr": "The study explores the impact of AI code assistants on novice programmers' experiences in a programming course, highlighting helpfulness in task completion but challenges in knowledge transfer.", "motivation": "To investigate the effects of AI code assistants on the learning experiences of novice programmers and identify challenges in knowledge retention.", "method": "The study involved 20 students who completed programming tasks first with AI assistance and then without. Data was collected through Likert-scale responses and open-ended feedback.", "result": "Students found AI tools helpful, boosting understanding and confidence in initial development tasks; however, they faced difficulties in applying knowledge without AI support.", "conclusion": "There is a need for pedagogical strategies that effectively integrate AI assistance into programming education while reinforcing foundational skills to mitigate overreliance.", "key_contributions": ["Identified the dual role of AI tools in enhancing confidence and understanding among novice programmers.", "Revealed challenges regarding knowledge transfer when switching from AI-supported to unaided tasks.", "Suggested the necessity for improved pedagogical strategies in programming education."], "limitations": "Limited sample size of 20 students may affect generalizability of findings; further research needed with larger groups.", "keywords": ["AI code assistants", "novice programmers", "programming education", "pedagogical strategies", "knowledge transfer"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2507.22919", "pdf": "https://arxiv.org/pdf/2507.22919.pdf", "abs": "https://arxiv.org/abs/2507.22919", "title": "A novel language model for predicting serious adverse event results in clinical trials from their prospective registrations", "authors": ["Qixuan Hu", "Xumou Zhang", "Jinman Kim", "Florence Bourgeois", "Adam G. Dunn"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Objectives: With accurate estimates of expected safety results, clinical\ntrials could be designed to avoid terminations and limit exposing participants\nto unnecessary risks. We evaluated methods for predicting serious adverse event\n(SAE) results in clinical trials using information only from their\nregistrations prior to the trial. Material and Methods: We analysed 22,107\ntwo-arm parallel interventional clinical trials from ClinicalTrials.gov with\nstructured summary results. Two prediction models were developed: a classifier\npredicting will experimental arm have higher SAE rates (area under the receiver\noperating characteristic curve; AUC) than control arm, and a regression model\nto predict the proportion of SAEs in control arms (root mean squared error;\nRMSE). A transfer learning approach using pretrained language models (e.g.,\nClinicalT5, BioBERT) was used for feature extraction, combined with downstream\nmodel for prediction. To maintain semantic representation in long trial texts\nexceeding localised language model input limits, a sliding window method was\ndeveloped for embedding extraction. Results: The best model\n(ClinicalT5+Transformer+MLP) had 77.6% AUC predicting which trial arm has a\nhigher proportion of patients with SAEs. When predicting proportion of\nparticipants experiencing SAE in the control arm, the same model achieved RMSE\nof 18.6%. The sliding window approach consistently outperformed methods without\nit. Across 12 classifiers, the average absolute AUC increase was 2.00%; across\n12 regressors, the average absolute RMSE reduction was 1.58%. Discussion:\nSummary results data available at ClinicalTrials.gov remains underutilised. The\npotential to estimate results of trials before they start is an opportunity to\nimprove trial design and flag discrepancies between expected and reported\nsafety results.", "AI": {"tldr": "The paper evaluates methods for predicting serious adverse events (SAEs) in clinical trials using registration data and pretrained language models, aiming to improve trial design.", "motivation": "Accurate predictions of safety results can guide clinical trial design to reduce terminations and risks to participants.", "method": "Analyzed 22,107 clinical trials from ClinicalTrials.gov; developed a classifier and regression model using a sliding window method with pretrained models (ClinicalT5, BioBERT) for feature extraction.", "result": "The best model achieved 77.6% AUC for predicting higher SAE rates in the experimental arm and RMSE of 18.6% for predicting control arm SAEs, with the sliding window approach proving effective.", "conclusion": "Utilizing summary results data can enhance clinical trial design and highlight discrepancies in safety result expectations and reports.", "key_contributions": ["Development of a sliding window method for embedding extraction from long texts", "Use of pretrained language models for feature extraction in clinical trial data", "Establishment of prediction models for SAEs using trial registration data."], "limitations": "", "keywords": ["Serious Adverse Events", "Clinical Trials", "Predictive Modeling", "Transfer Learning", "Health Informatics"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2507.22901", "pdf": "https://arxiv.org/pdf/2507.22901.pdf", "abs": "https://arxiv.org/abs/2507.22901", "title": "Accelerated and Optimized Search of Imperceptible Color Vibration for Embedding Information into LCD images", "authors": ["Shingo Hattori", "Takefumi Hiraki"], "categories": ["cs.HC"], "comment": "Presented at ACM SIGGRAPH Asia 2022 Posters", "summary": "Large, high-resolution displays are installed throughout the city as public\ndisplays. By superimposing invisible information on the images of these\ndisplays, large numbers of devices with cameras and sensors can communicate\nwith the displays without prior pairing. Several applications have been\nproposed, such as operating robots or communicating information to users by\ndisplaying 2D codes on images. However, the display of 2D codes has the problem\nof compromising the appearance of displayed content.\n  Abe et al. proposed a method of communicating with devices by superimposing\ninvisible information using color vibration on images displayed on\noff-the-shelf liquid-crystal displays (LCD). Using this method, we can embed\nthe information for devices in images without interfering with the displayed\ncontent. Abe et al. uses a simple serial loop operation to search for color\npairs comprising a color vibration, which requires a very long processing time\ndue to the huge search space.\n  In this paper, we propose an accelerated and optimized search method for\ncolor pairs that constitute the imperceptible color vibration for embedding\ninformation on LCD images. To achieve fast color pair search, we parallelized\nthe search process, which is previously done individually, by using arrays\nrepresenting the amount of movement and an operation to extract elements from\nthe array that satisfy the conditions. In addition, we investigate the amount\nof information that can be superimposed on nine color images using the\nimperceptible color vibration and clarify the applicability of embedding\ninformation into images using the color vibration.", "AI": {"tldr": "The paper proposes an optimized method for superimposing invisible information on LCD displays using color vibration, enabling communication without compromising visual content.", "motivation": "Existing methods for invisible information embedding on displays compromise visual content presentation.", "method": "The authors developed a parallelized search method for efficiently identifying color pairs that create imperceptible color vibrations for information embedding on liquid-crystal displays.", "result": "The proposed method significantly accelerates the search process for color pairs, enhancing the amount of information that can be embedded in images displayed on LCDs.", "conclusion": "The optimized method improves the feasibility of unseen information embedding in visual displays while maintaining aesthetic integrity.", "key_contributions": ["Optimized search method for color pairs in invisible information embedding", "Parallel processing approach for faster results", "Enhanced capacity for information superimposition in images"], "limitations": "", "keywords": ["human-computer interaction", "color vibration", "information superimposition", "public displays", "LCD"], "importance_score": 5, "read_time_minutes": 15}}
{"id": "2507.22920", "pdf": "https://arxiv.org/pdf/2507.22920.pdf", "abs": "https://arxiv.org/abs/2507.22920", "title": "Discrete Tokenization for Multimodal LLMs: A Comprehensive Survey", "authors": ["Jindong Li", "Yali Fu", "Jiahong Liu", "Linxiao Cao", "Wei Ji", "Menglin Yang", "Irwin King", "Ming-Hsuan Yang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid advancement of large language models (LLMs) has intensified the\nneed for effective mechanisms to transform continuous multimodal data into\ndiscrete representations suitable for language-based processing. Discrete\ntokenization, with vector quantization (VQ) as a central approach, offers both\ncomputational efficiency and compatibility with LLM architectures. Despite its\ngrowing importance, there is a lack of a comprehensive survey that\nsystematically examines VQ techniques in the context of LLM-based systems. This\nwork fills this gap by presenting the first structured taxonomy and analysis of\ndiscrete tokenization methods designed for LLMs. We categorize 8 representative\nVQ variants that span classical and modern paradigms and analyze their\nalgorithmic principles, training dynamics, and integration challenges with LLM\npipelines. Beyond algorithm-level investigation, we discuss existing research\nin terms of classical applications without LLMs, LLM-based single-modality\nsystems, and LLM-based multimodal systems, highlighting how quantization\nstrategies influence alignment, reasoning, and generation performance. In\naddition, we identify key challenges including codebook collapse, unstable\ngradient estimation, and modality-specific encoding constraints. Finally, we\ndiscuss emerging research directions such as dynamic and task-adaptive\nquantization, unified tokenization frameworks, and biologically inspired\ncodebook learning. This survey bridges the gap between traditional vector\nquantization and modern LLM applications, serving as a foundational reference\nfor the development of efficient and generalizable multimodal systems. A\ncontinuously updated version is available at:\nhttps://github.com/jindongli-Ai/LLM-Discrete-Tokenization-Survey.", "AI": {"tldr": "The paper presents a comprehensive survey on discrete tokenization methods for large language models (LLMs), focusing on vector quantization (VQ) techniques, their classifications, challenges, and emerging research directions.", "motivation": "To address the growing need for effective methods to transform continuous multimodal data into discrete forms suitable for LLMs, a comprehensive survey on vector quantization techniques is conducted.", "method": "The paper categorizes 8 representative variants of vector quantization, analyzes their principles, training dynamics, and challenges when integrated into LLM pipelines.", "result": "The analysis reveals how quantization strategies impact alignment, reasoning, and generation performance in LLMs, and identifies key challenges such as codebook collapse and unstable gradient estimation.", "conclusion": "This work serves as a foundational reference for developing efficient multimodal systems by connecting traditional vector quantization methods with modern applications in LLMs.", "key_contributions": ["First structured taxonomy of discrete tokenization methods for LLMs", "In-depth analysis of 8 VQ variants concerning LLM integration", "Identification of key challenges and emerging research directions in tokenization for LLMs"], "limitations": "The survey may not cover all potential VQ methods and their applications across every modality.", "keywords": ["vector quantization", "large language models", "discrete tokenization", "multimodal systems", "machine learning"], "importance_score": 9, "read_time_minutes": 25}}
{"id": "2507.22902", "pdf": "https://arxiv.org/pdf/2507.22902.pdf", "abs": "https://arxiv.org/abs/2507.22902", "title": "Toward the Autonomous AI Doctor: Quantitative Benchmarking of an Autonomous Agentic AI Versus Board-Certified Clinicians in a Real World Setting", "authors": ["Hashim Hayat", "Maksim Kudrautsau", "Evgeniy Makarov", "Vlad Melnichenko", "Tim Tsykunou", "Piotr Varaksin", "Matt Pavelle", "Adam Z. Oskowitz"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.MA"], "comment": null, "summary": "Background: Globally we face a projected shortage of 11 million healthcare\npractitioners by 2030, and administrative burden consumes 50% of clinical time.\nArtificial intelligence (AI) has the potential to help alleviate these\nproblems. However, no end-to-end autonomous large language model (LLM)-based AI\nsystem has been rigorously evaluated in real-world clinical practice. In this\nstudy, we evaluated whether a multi-agent LLM-based AI framework can function\nautonomously as an AI doctor in a virtual urgent care setting. Methods: We\nretrospectively compared the performance of the multi-agent AI system Doctronic\nand board-certified clinicians across 500 consecutive urgent-care telehealth\nencounters. The primary end points: diagnostic concordance, treatment plan\nconsistency, and safety metrics, were assessed by blinded LLM-based\nadjudication and expert human review. Results: The top diagnosis of Doctronic\nand clinician matched in 81% of cases, and the treatment plan aligned in 99.2%\nof cases. No clinical hallucinations occurred (e.g., diagnosis or treatment not\nsupported by clinical findings). In an expert review of discordant cases, AI\nperformance was superior in 36.1%, and human performance was superior in 9.3%;\nthe diagnoses were equivalent in the remaining cases. Conclusions: In this\nfirst large-scale validation of an autonomous AI doctor, we demonstrated strong\ndiagnostic and treatment plan concordance with human clinicians, with AI\nperformance matching and in some cases exceeding that of practicing clinicians.\nThese findings indicate that multi-agent AI systems achieve comparable clinical\ndecision-making to human providers and offer a potential solution to healthcare\nworkforce shortages.", "AI": {"tldr": "Multi-agent LLM-based AI system Doctronic evaluated against clinicians in urgent care settings, showing strong results in diagnostics and treatment plans.", "motivation": "Address the projected healthcare practitioner shortage and reduce administrative burdens in clinical settings using AI.", "method": "Performance of Doctronic compared to board-certified clinicians in 500 telehealth encounters, assessing diagnostic concordance, treatment plan consistency, and safety metrics.", "result": "Doctronic matched clinician diagnoses in 81% of cases and treatment plans in 99.2%. AI outperformed human clinicians in 36.1% of discordant cases while showing no clinical hallucinations.", "conclusion": "The study demonstrates that multi-agent AI systems can achieve clinical decision-making on par with human providers, helpful in addressing workforce shortages.", "key_contributions": ["First large-scale validation of an autonomous AI doctor", "Strong concordance in diagnostics and treatment with human clinicians", "Potential solution for healthcare workforce shortages"], "limitations": "", "keywords": ["AI doctor", "LLM", "urgent care", "diagnostic concordance", "healthcare workforce"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2507.22921", "pdf": "https://arxiv.org/pdf/2507.22921.pdf", "abs": "https://arxiv.org/abs/2507.22921", "title": "Fast and Accurate Contextual Knowledge Extraction Using Cascading Language Model Chains and Candidate Answers", "authors": ["Lee Harris"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Language models can capture complex relationships in given text, but these\nare notorious for being costly and for producing information that does not\nexist (i.e., hallucinations). Furthermore, the resources invested into\nproducing this information would be wasted if it were incorrect. We address\nthese issues by proposing, implementing, and applying the Language Model Chain\n(LMC) algorithm. In this, a language model's response to a given prompt about\ngiven text is only correct if it exists in the collection of possible (i.e.,\ncandidate) answers, and text corresponding to incorrect responses is fed into a\nmore predictive (but slower) language model. This process is repeated for a\ncollection of language models, or until all predictions about the text are\ncorrect. We used the LMC algorithm to extract patient dates of birth from\nmedical documents, and combining a collection of language models in a\nmulti-stage cascade significantly increased prediction speed and accuracy over\nindividual language models, while greatly reducing the number of corresponding\nhallucinations. We believe that the novel LMC algorithm significantly\ncontributes to the knowledge extraction field, and that this should be explored\nmuch further in the future.", "AI": {"tldr": "Introduction of the Language Model Chain (LMC) algorithm to enhance accuracy and reduce hallucinations in language model outputs.", "motivation": "To address the issues of high cost and hallucinations in language models, especially in the context of extracting information from text.", "method": "The LMC algorithm utilizes a multi-stage cascade of language models to ensure responses are only correct if they exist in a candidate answer set, improving both speed and accuracy in predictions.", "result": "The application of the LMC algorithm to extract patient dates of birth from medical documents resulted in significantly increased prediction speed and accuracy while reducing hallucinations.", "conclusion": "The LMC algorithm shows promise in advancing knowledge extraction, warranting further exploration.", "key_contributions": ["Introduction of the LMC algorithm", "Improved accuracy and speed in language model responses", "Significant reduction in hallucinations in outputs"], "limitations": "", "keywords": ["Language Model Chain", "hallucinations", "knowledge extraction", "medical documents", "multi-stage cascade"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2507.22903", "pdf": "https://arxiv.org/pdf/2507.22903.pdf", "abs": "https://arxiv.org/abs/2507.22903", "title": "A blessing or a burden? Exploring worker perspectives of using a social robot in a church", "authors": ["Andrew Blair", "Peggy Gregory", "Mary Ellen Foster"], "categories": ["cs.HC", "cs.RO"], "comment": "Accepted by the 2025 34th IEEE International Conference on Robot and\n  Human Interactive Communication (ROMAN)", "summary": "Recent technological advances have allowed robots to assist in the service\nsector, and consequently accelerate job and sector transformation. Less\nattention has been paid to the use of robots in real-world organisations where\nsocial benefits, as opposed to profits, are the primary motivator. To explore\nthese opportunities, we have partnered with a working church and visitor\nattraction. We conducted interviews with 15 participants from a range of\nstakeholder groups within the church to understand worker perspectives of\nintroducing a social robot to the church and analysed the results using\nreflexive thematic analysis. Findings indicate mixed responses to the use of a\nrobot, with participants highlighting the empathetic responsibility the church\nhas towards people and the potential for unintended consequences. However,\ninformation provision and alleviation of menial or mundane tasks were\nidentified as potential use cases. This highlights the need to consider not\nonly the financial aspects of robot introduction, but also how social and\nintangible values shape what roles a robot should take on within an\norganisation.", "AI": {"tldr": "This paper explores the introduction of social robots in a church setting, revealing mixed stakeholder perspectives on their use and emphasizing the balance between social benefits and financial considerations.", "motivation": "To investigate the role of robots in organizations primarily motivated by social benefits rather than profits, particularly in a church context.", "method": "Interviews were conducted with 15 participants from different stakeholder groups within the church, followed by reflexive thematic analysis of the responses.", "result": "The analysis revealed mixed responses to the introduction of a social robot, highlighting concerns about empathetic responsibilities and unintended consequences, while identifying potential uses such as information provision and alleviating mundane tasks.", "conclusion": "The findings underscore the importance of considering social and intangible values alongside financial implications when introducing robots in organizational settings.", "key_contributions": ["Exploration of social robot implementation in a non-profit environment.", "Insight into stakeholder perspectives regarding robot use in service contexts.", "Identification of potential use cases beyond financial motivations."], "limitations": "The study is based on a specific case of a church, which may limit generalizability to other sectors.", "keywords": ["social robots", "stakeholder perspectives", "service sector", "robot implementation", "human-robot interaction"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2507.22922", "pdf": "https://arxiv.org/pdf/2507.22922.pdf", "abs": "https://arxiv.org/abs/2507.22922", "title": "Predicting stock prices with ChatGPT-annotated Reddit sentiment", "authors": ["Mateusz Kmak", "Kamil Chmurzyński", "Kamil Matejuk", "Paweł Kotzbach", "Jan Kocoń"], "categories": ["cs.CL", "cs.AI", "cs.SI"], "comment": "International Conference on Computational Science 2025", "summary": "The surge of retail investor activity on social media, exemplified by the\n2021 GameStop short squeeze, raised questions about the influence of online\nsentiment on stock prices. This paper explores whether sentiment derived from\nsocial media discussions can meaningfully predict stock market movements. We\nfocus on Reddit's r/wallstreetbets and analyze sentiment related to two\ncompanies: GameStop (GME) and AMC Entertainment (AMC). To assess sentiment's\nrole, we employ two existing text-based sentiment analysis methods and\nintroduce a third, a ChatGPT-annotated and fine-tuned RoBERTa-based model\ndesigned to better interpret the informal language and emojis prevalent in\nsocial media discussions. We use correlation and causality metrics to determine\nthese models' predictive power. Surprisingly, our findings suggest that social\nmedia sentiment has only a weak correlation with stock prices. At the same\ntime, simpler metrics, such as the volume of comments and Google search trends,\nexhibit stronger predictive signals. These results highlight the complexity of\nretail investor behavior and suggest that traditional sentiment analysis may\nnot fully capture the nuances of market-moving online discussions.", "AI": {"tldr": "This paper investigates the predictive power of social media sentiment on stock prices, particularly focusing on discussions from Reddit's r/wallstreetbets related to GameStop and AMC.", "motivation": "The study is motivated by the significant retail investor activity on social media, particularly during events like the GameStop short squeeze, raising questions about the impact of online sentiment on stock prices.", "method": "The authors analyze sentiment using two existing sentiment analysis methods and introduce a novel ChatGPT-annotated, fine-tuned RoBERTa-based model to interpret informal language and emojis typical in social media discussions. Correlation and causality metrics are employed to evaluate the predictive ability of these methods.", "result": "The findings reveal that social media sentiment has a weak correlation with stock prices. In contrast, simpler metrics such as comment volume and Google search trends show stronger predictive signals.", "conclusion": "The results indicate that the behavior of retail investors in online discussions is complex, and traditional sentiment analysis may fail to capture the intricacies that influence stock prices.", "key_contributions": ["Introduction of a novel sentiment analysis model tailored for informal social media language", "Empirical analysis of sentiment's predictive power on stock prices using correlation and causality metrics", "Insights into the limitations of conventional sentiment analysis in forecasting market movements"], "limitations": "The paper's findings highlight the weak correlation of sentiment with stock prices, indicating limitations in the effectiveness of sentiment analysis for trading predictions.", "keywords": ["sentiment analysis", "social media", "stock market", "retail investors", "ChatGPT"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.22904", "pdf": "https://arxiv.org/pdf/2507.22904.pdf", "abs": "https://arxiv.org/abs/2507.22904", "title": "SketchMind: A Multi-Agent Cognitive Framework for Assessing Student-Drawn Scientific Sketches", "authors": ["Ehsan Latif", "Zirak Khan", "Xiaoming Zhai"], "categories": ["cs.HC", "cs.AI"], "comment": "Submitted to NeurIPS2025", "summary": "Scientific sketches (e.g., models) offer a powerful lens into students'\nconceptual understanding, yet AI-powered automated assessment of such\nfree-form, visually diverse artifacts remains a critical challenge. Existing\nsolutions often treat sketch evaluation as either an image classification task\nor monolithic vision-language models, which lack interpretability, pedagogical\nalignment, and adaptability across cognitive levels. To address these\nlimitations, we present SketchMind, a cognitively grounded, multi-agent\nframework for evaluating and improving student-drawn scientific sketches.\nSketchMind comprises modular agents responsible for rubric parsing, sketch\nperception, cognitive alignment, and iterative feedback with sketch\nmodification, enabling personalized and transparent evaluation. We evaluate\nSketchMind on a curated dataset of 3,575 student-generated sketches across six\nscience assessment items with different highest order of Bloom's level that\nrequire students to draw models to explain phenomena. Compared to baseline\nGPT-4o performance without SRG (average accuracy: 55.6%), and with SRG\nintegration achieves 77.1% average accuracy (+21.4% average absolute gain). We\nalso demonstrate that multi-agent orchestration with SRG enhances SketchMind\nperformance, for example, GPT-4.1 gains an average 8.9% increase in sketch\nprediction accuracy, outperforming single-agent pipelines across all items.\nHuman evaluators rated the feedback and co-created sketches generated by\n\\textsc{SketchMind} with GPT-4.1, which achieved an average of 4.1 out of 5,\nsignificantly higher than those of baseline models (e.g., 2.3 for GPT-4o).\nExperts noted the system's potential to meaningfully support conceptual growth\nthrough guided revision. Our code and (pending approval) dataset will be\nreleased to support reproducibility and future research in AI-driven education.", "AI": {"tldr": "A framework called SketchMind is introduced for evaluating scientific sketches by students, integrating multi-agent systems for personalized feedback and achieving higher accuracy than existing models.", "motivation": "The paper addresses the challenge of automated assessment of student-drawn scientific sketches using AI, given that existing solutions lack interpretability and adaptability.", "method": "SketchMind employs modular agents for rubric parsing, sketch perception, cognitive alignment, and providing iterative feedback on sketches, allowing for a more personalized approach to evaluation.", "result": "SketchMind achieved an average accuracy of 77.1% on a dataset of 3,575 sketches, outperforming baseline models significantly. Human evaluators provided a high satisfaction score to the feedback generated.", "conclusion": "The research demonstrates the potential of SketchMind to enhance conceptual understanding and support educational growth through its tailored feedback mechanisms.", "key_contributions": ["Introduction of SketchMind as a framework for sketch evaluation", "Integration of multi-agent orchestration that improves accuracy and feedback", "Demonstration of significant performance improvement over baseline models"], "limitations": "", "keywords": ["AI in education", "sketch evaluation", "multi-agent framework", "cognitive alignment", "feedback systems"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.22923", "pdf": "https://arxiv.org/pdf/2507.22923.pdf", "abs": "https://arxiv.org/abs/2507.22923", "title": "How and Where to Translate? The Impact of Translation Strategies in Cross-lingual LLM Prompting", "authors": ["Aman Gupta", "Yingying Zhuang", "Zhou Yu", "Ziji Zhang", "Anurag Beniwal"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at Prompt Optimization KDD '25", "summary": "Despite advances in the multilingual capabilities of Large Language Models\n(LLMs), their performance varies substantially across different languages and\ntasks. In multilingual retrieval-augmented generation (RAG)-based systems,\nknowledge bases (KB) are often shared from high-resource languages (such as\nEnglish) to low-resource ones, resulting in retrieved information from the KB\nbeing in a different language than the rest of the context. In such scenarios,\ntwo common practices are pre-translation to create a mono-lingual prompt and\ncross-lingual prompting for direct inference. However, the impact of these\nchoices remains unclear. In this paper, we systematically evaluate the impact\nof different prompt translation strategies for classification tasks with\nRAG-enhanced LLMs in multilingual systems. Experimental results show that an\noptimized prompting strategy can significantly improve knowledge sharing across\nlanguages, therefore improve the performance on the downstream classification\ntask. The findings advocate for a broader utilization of multilingual resource\nsharing and cross-lingual prompt optimization for non-English languages,\nespecially the low-resource ones.", "AI": {"tldr": "This paper evaluates different prompt translation strategies in multilingual retrieval-augmented generation (RAG) systems to improve classification tasks using LLMs.", "motivation": "To assess how prompt translation strategies affect performance in multilingual systems using LLMs, particularly when knowledge bases from high-resource languages are shared with low-resource languages.", "method": "The paper systematically evaluates various prompt translation strategies for classification tasks using multilingual RAG-enhanced LLMs through experimental studies.", "result": "An optimized prompting strategy was found to significantly enhance knowledge sharing across languages, leading to improved performance in downstream classification tasks.", "conclusion": "The study suggests broader utilization of multilingual resource sharing and cross-lingual prompt optimization for low-resource languages.", "key_contributions": ["Evaluation of prompt translation strategies for multilingual RAG systems.", "Experimental results demonstrate performance improvements in classification tasks.", "Advocacy for optimized multilingual resource sharing in low-resource languages."], "limitations": "", "keywords": ["Multilingual LLMs", "Prompt translation strategies", "RAG systems"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2507.22905", "pdf": "https://arxiv.org/pdf/2507.22905.pdf", "abs": "https://arxiv.org/abs/2507.22905", "title": "Exploring LLM-generated Culture-specific Affective Human-Robot Tactile Interaction", "authors": ["Qiaoqiao Ren", "Tony Belpaeme"], "categories": ["cs.HC"], "comment": null, "summary": "As large language models (LLMs) become increasingly integrated into robotic\nsystems, their potential to generate socially and culturally appropriate\naffective touch remains largely unexplored. This study investigates whether\nLLMs-specifically GPT-3.5, GPT-4, and GPT-4o --can generate culturally adaptive\ntactile behaviours to convey emotions in human-robot interaction. We produced\ntext based touch descriptions for 12 distinct emotions across three cultural\ncontexts (Chinese, Belgian, and unspecified), and examined their\ninterpretability in both robot-to-human and human-to-robot scenarios. A total\nof 90 participants (36 Chinese, 36 Belgian, and 18 culturally unspecified)\nevaluated these LLM-generated tactile behaviours for emotional decoding and\nperceived appropriateness. Results reveal that: (1) under matched cultural\nconditions, participants successfully decoded six out of twelve emotions-mainly\nsocially oriented emotions such as love and Ekman emotions such as anger,\nhowever, self-focused emotions like pride and embarrassment were more difficult\nto interpret; (2) tactile behaviours were perceived as more appropriate when\ndirected from human to robot than from robot to human, revealing an asymmetry\nin social expectations based on interaction roles; (3) behaviours interpreted\nas aggressive (e.g., anger), overly intimate (e.g., love), or emotionally\nambiguous (i.e., not clearly decodable) were significantly more likely to be\nrated as inappropriate; and (4) cultural mismatches reduced decoding accuracy\nand increased the likelihood of behaviours being judged as inappropriate.", "AI": {"tldr": "This study explores how LLMs can generate culturally adaptive tactile behaviors for emotional expression in human-robot interaction.", "motivation": "Investigate the integration of LLMs into robotic systems for generating culturally appropriate affective touch.", "method": "The study produced text-based touch descriptions representing 12 distinct emotions across three cultural contexts, engaging 90 participants to evaluate emotional decoding and appropriateness in both robot-to-human and human-to-robot scenarios.", "result": "Participants decoded six out of twelve emotions effectively under matched cultural conditions; behaviors were perceived as more appropriate in human-to-robot interactions and cultural mismatches led to reduced decoding accuracy and increased inappropriate ratings.", "conclusion": "The findings suggest that LLM-generated tactile behaviors can enhance emotional communication in human-robot interactions, but must be culturally adapted to avoid misinterpretations.", "key_contributions": ["Demonstrated LLMs' ability to generate culturally adaptive tactile behaviors for robots.", "Identified the impact of cultural context on emotional decoding in human-robot interactions.", "Revealed the asymmetry in appropriateness of tactile behaviors based on interaction roles."], "limitations": "Study limited to three cultural contexts and a specific set of emotions; results may not generalize across all cultures or emotional expressions.", "keywords": ["Large Language Models", "Human-Robot Interaction", "Affective Touch", "Cultural Adaptability", "Emotional Decoding"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2507.22924", "pdf": "https://arxiv.org/pdf/2507.22924.pdf", "abs": "https://arxiv.org/abs/2507.22924", "title": "Using Sentiment Analysis to Investigate Peer Feedback by Native and Non-Native English Speakers", "authors": ["Brittney Exline", "Melanie Duffin", "Brittany Harbison", "Chrissa da Gomez", "David Joyner"], "categories": ["cs.CL", "I.2.7; K.3.1"], "comment": null, "summary": "Graduate-level CS programs in the U.S. increasingly enroll international\nstudents, with 60.2 percent of master's degrees in 2023 awarded to non-U.S.\nstudents. Many of these students take online courses, where peer feedback is\nused to engage students and improve pedagogy in a scalable manner. Since these\ncourses are conducted in English, many students study in a language other than\ntheir first. This paper examines how native versus non-native English speaker\nstatus affects three metrics of peer feedback experience in online U.S.-based\ncomputing courses. Using the Twitter-roBERTa-based model, we analyze the\nsentiment of peer reviews written by and to a random sample of 500 students. We\nthen relate sentiment scores and peer feedback ratings to students' language\nbackground. Results show that native English speakers rate feedback less\nfavorably, while non-native speakers write more positively but receive less\npositive sentiment in return. When controlling for sex and age, significant\ninteractions emerge, suggesting that language background plays a modest but\ncomplex role in shaping peer feedback experiences.", "AI": {"tldr": "The paper examines the impact of native vs non-native English speaker status on peer feedback experiences in online computing courses through sentiment analysis of peer reviews.", "motivation": "To understand how language background affects students' peer feedback experiences in online U.S. graduate computing courses.", "method": "Sentiment analysis using the Twitter-roBERTa-based model on peer reviews from a sample of 500 students, examining ratings related to language background.", "result": "Native speakers rate feedback less favorably, whereas non-native speakers write more positively but receive less favorable sentiment in return, with significant interactions based on sex and age.", "conclusion": "Language background has a modest but complex effect on peer feedback experiences, influencing the dynamics of how feedback is given and received among students.", "key_contributions": ["Analyzed peer feedback sentiment using advanced AI models", "Identified complex interactions between language background and feedback experiences", "Provided insights into the challenges faced by non-native speakers in online courses"], "limitations": "The study focuses on a specific sample from U.S.-based courses, which may not generalize globally.", "keywords": ["peer feedback", "sentiment analysis", "online courses", "native vs non-native speakers", "language background"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.22952", "pdf": "https://arxiv.org/pdf/2507.22952.pdf", "abs": "https://arxiv.org/abs/2507.22952", "title": "Automated Label Placement on Maps via Large Language Models", "authors": ["Harry Shomer", "Jiejun Xu"], "categories": ["cs.HC", "cs.CV", "cs.LG"], "comment": "Workshop on AI for Data Editing (AI4DE) at KDD 2025", "summary": "Label placement is a critical aspect of map design, serving as a form of\nspatial annotation that directly impacts clarity and interpretability. Despite\nits importance, label placement remains largely manual and difficult to scale,\nas existing automated systems struggle to integrate cartographic conventions,\nadapt to context, or interpret labeling instructions. In this work, we\nintroduce a new paradigm for automatic label placement (ALP) that formulates\nthe task as a data editing problem and leverages large language models (LLMs)\nfor context-aware spatial annotation. To support this direction, we curate\nMAPLE, the first known benchmarking dataset for evaluating ALP on real-world\nmaps, encompassing diverse landmark types and label placement annotations from\nopen-source data. Our method retrieves labeling guidelines relevant to each\nlandmark type leveraging retrieval-augmented generation (RAG), integrates them\ninto prompts, and employs instruction-tuned LLMs to generate ideal label\ncoordinates. We evaluate four open-source LLMs on MAPLE, analyzing both overall\nperformance and generalization across different types of landmarks. This\nincludes both zero-shot and instruction-tuned performance. Our results\ndemonstrate that LLMs, when guided by structured prompts and domain-specific\nretrieval, can learn to perform accurate spatial edits, aligning the generated\noutputs with expert cartographic standards. Overall, our work presents a\nscalable framework for AI-assisted map finishing and demonstrates the potential\nof foundation models in structured data editing tasks. The code and data can be\nfound at https://github.com/HarryShomer/MAPLE.", "AI": {"tldr": "This paper introduces a paradigm for automatic label placement (ALP) using large language models to enhance clarity and interpretability in map design, supported by a new benchmarking dataset called MAPLE.", "motivation": "Label placement in map design is essential but remains manual and difficult to scale due to the limitations of existing automated systems.", "method": "The task is formulated as a data editing problem, utilizing large language models and a benchmark dataset (MAPLE) to evaluate performance and generalization across various landmarks.", "result": "LLMs guided by structured prompts and domain-specific retrieval demonstrated the ability to perform accurate spatial edits, aligning with expert cartographic standards.", "conclusion": "This work provides a scalable framework for AI-assisted map finishing and showcases the effectiveness of foundation models in structured data editing tasks.", "key_contributions": ["Introduction of MAPLE, a benchmarking dataset for ALP", "Application of LLMs for context-aware spatial annotation", "Demonstration of effective AI-assisted map editing framework"], "limitations": "", "keywords": ["automatic label placement", "large language models", "cartographic standards", "data editing", "map design"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.22925", "pdf": "https://arxiv.org/pdf/2507.22925.pdf", "abs": "https://arxiv.org/abs/2507.22925", "title": "Hierarchical Memory for High-Efficiency Long-Term Reasoning in LLM Agents", "authors": ["Haoran Sun", "Shaoning Zeng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Long-term memory is one of the key factors influencing the reasoning\ncapabilities of Large Language Model Agents (LLM Agents). Incorporating a\nmemory mechanism that effectively integrates past interactions can\nsignificantly enhance decision-making and contextual coherence of LLM Agents.\nWhile recent works have made progress in memory storage and retrieval, such as\nencoding memory into dense vectors for similarity-based search or organizing\nknowledge in the form of graph, these approaches often fall short in structured\nmemory organization and efficient retrieval. To address these limitations, we\npropose a Hierarchical Memory (H-MEM) architecture for LLM Agents that\norganizes and updates memory in a multi-level fashion based on the degree of\nsemantic abstraction. Each memory vector is embedded with a positional index\nencoding pointing to its semantically related sub-memories in the next layer.\nDuring the reasoning phase, an index-based routing mechanism enables efficient,\nlayer-by-layer retrieval without performing exhaustive similarity computations.\nWe evaluate our method on five task settings from the LoCoMo dataset.\nExperimental results show that our approach consistently outperforms five\nbaseline methods, demonstrating its effectiveness in long-term dialogue\nscenarios.", "AI": {"tldr": "Proposes a Hierarchical Memory (H-MEM) architecture to enhance long-term memory in LLM Agents for better decision-making and contextual coherence.", "motivation": "Long-term memory influences the reasoning capabilities of Large Language Model Agents, and existing memory approaches struggle with structured memory organization and efficient retrieval.", "method": "Introduces a H-MEM architecture that organizes memory in a multi-level fashion, integrates positional indexing, and uses an index-based routing mechanism for efficient retrieval.", "result": "Experimental results on the LoCoMo dataset show that H-MEM consistently outperforms five baseline methods in long-term dialogue scenarios.", "conclusion": "The proposed H-MEM architecture significantly enhances the reasoning capabilities of LLM Agents by improving memory organization and retrieval efficiency.", "key_contributions": ["Hierarchical Memory (H-MEM) architecture designed for LLM Agents", "Multi-level memory organization with semantic abstraction", "Index-based routing mechanism for efficient retrieval"], "limitations": "", "keywords": ["Hierarchical Memory", "Large Language Models", "Index-based retrieval", "Long-term memory", "Dialogue systems"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.23096", "pdf": "https://arxiv.org/pdf/2507.23096.pdf", "abs": "https://arxiv.org/abs/2507.23096", "title": "ChatVis: Large Language Model Agent for Generating Scientific Visualizations", "authors": ["Tom Peterka", "Tanwi Mallick", "Orcun Yildiz", "David Lenz", "Cory Quammen", "Berk Geveci"], "categories": ["cs.HC"], "comment": null, "summary": "Large language models (LLMs) are rapidly increasing in capability, but they\nstill struggle with highly specialized programming tasks such as scientific\nvisualization. We present an LLM assistant, ChatVis, that aids the LLM to\ngenerate Python code for ParaView scientific visualization tasks, without the\nneed for retraining or fine-tuning the LLM. ChatVis employs chain-of-thought\nprompt simplification, retrieval-augmented prompt generation using a vector\ndatabase of documentation and code examples, and error checking with iterative\nprompt feedback to correct errors until a visualization is produced. An\nintegral part of our approach is a benchmark suite of canonical visualization\ntasks, ParaView regression tests, and scientific use cases that includes\ncomprehensive evaluation metrics. We evaluate our visualization assistant by\ncomparing results with a variety of top-performing unassisted LLMs. We find\nthat all the metrics are significantly improved with ChatVis.", "AI": {"tldr": "ChatVis is an LLM assistant designed to improve Python code generation for scientific visualization tasks in ParaView, enhancing performance without retraining the LLM.", "motivation": "LLMs are advancing but still find it challenging to perform specialized programming tasks such as scientific visualization.", "method": "ChatVis integrates chain-of-thought prompt simplification, retrieval-augmented prompt generation using a vector database, and iterative error checking to assist LLMs in generating correct visualization code.", "result": "Comparison with unassisted LLMs shows that ChatVis significantly improves all evaluation metrics for scientific visualization tasks.", "conclusion": "ChatVis provides a robust solution for generating Python code in scientific visualization, demonstrating improved outcomes over traditional LLM approaches.", "key_contributions": ["Introduction of ChatVis for enhancing LLM capabilities in visualization tasks", "Utilization of chain-of-thought techniques and retrieval-augmented methods", "Comprehensive evaluation metrics with a benchmark suite for performance assessment"], "limitations": "", "keywords": ["large language models", "scientific visualization", "ChatVis"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.22926", "pdf": "https://arxiv.org/pdf/2507.22926.pdf", "abs": "https://arxiv.org/abs/2507.22926", "title": "Multi-Relation Extraction in Entity Pairs using Global Context", "authors": ["Nilesh", "Atul Gupta", "Avinash C Panday"], "categories": ["cs.CL", "cs.IR"], "comment": "11 pages, 9 figures", "summary": "In document-level relation extraction, entities may appear multiple times in\na document, and their relationships can shift from one context to another.\nAccurate prediction of the relationship between two entities across an entire\ndocument requires building a global context spanning all relevant sentences.\nPrevious approaches have focused only on the sentences where entities are\nmentioned, which fails to capture the complete document context necessary for\naccurate relation extraction. Therefore, this paper introduces a novel input\nembedding approach to capture the positions of mentioned entities throughout\nthe document rather than focusing solely on the span where they appear. The\nproposed input encoding approach leverages global relationships and\nmulti-sentence reasoning by representing entities as standalone segments,\nindependent of their positions within the document. The performance of the\nproposed method has been tested on three benchmark relation extraction\ndatasets, namely DocRED, Re-DocRED, and REBEL. The experimental results\ndemonstrated that the proposed method accurately predicts relationships between\nentities in a document-level setting. The proposed research also has\ntheoretical and practical implications. Theoretically, it advances global\ncontext modeling and multi-sentence reasoning in document-level relation\nextraction. Practically, it enhances relationship detection, enabling improved\nperformance in real-world NLP applications requiring comprehensive entity-level\ninsights and interpretability.", "AI": {"tldr": "This paper introduces a novel input embedding approach for document-level relation extraction that captures entity positions throughout an entire document, improving prediction accuracy.", "motivation": "Accurate prediction of relationships between entities in document-level relation extraction requires understanding the global context beyond the sentences where entities are mentioned.", "method": "The paper proposes a new input encoding technique that represents entities as standalone segments to leverage global relationships and multi-sentence reasoning.", "result": "The method was tested on three benchmark datasets and showed improved accuracy in predicting relationships between entities in a document-level context.", "conclusion": "The research enhances relationship detection in NLP applications, providing significant theoretical advancements and practical implications for entity-level insights and interpretability.", "key_contributions": ["Novel input embedding approach for capturing global context", "Enhanced relationship detection in document-level settings", "Improved accuracy on benchmark datasets for relation extraction"], "limitations": "The paper does not address the computational complexity of the proposed approach compared to previous methods.", "keywords": ["relation extraction", "document-level", "global context", "multi-sentence reasoning", "NLP"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.23190", "pdf": "https://arxiv.org/pdf/2507.23190.pdf", "abs": "https://arxiv.org/abs/2507.23190", "title": "Accessibility Scout: Personalized Accessibility Scans of Built Environments", "authors": ["William Huang", "Xia Su", "Jon E. Froehlich", "Yang Zhang"], "categories": ["cs.HC", "cs.AI", "cs.CV", "cs.MA"], "comment": "18 pages, 16 figures. Presented at ACM UIST 2025", "summary": "Assessing the accessibility of unfamiliar built environments is critical for\npeople with disabilities. However, manual assessments, performed by users or\ntheir personal health professionals, are laborious and unscalable, while\nautomatic machine learning methods often neglect an individual user's unique\nneeds. Recent advances in Large Language Models (LLMs) enable novel approaches\nto this problem, balancing personalization with scalability to enable more\nadaptive and context-aware assessments of accessibility. We present\nAccessibility Scout, an LLM-based accessibility scanning system that identifies\naccessibility concerns from photos of built environments. With use,\nAccessibility Scout becomes an increasingly capable \"accessibility scout\",\ntailoring accessibility scans to an individual's mobility level, preferences,\nand specific environmental interests through collaborative Human-AI\nassessments. We present findings from three studies: a formative study with six\nparticipants to inform the design of Accessibility Scout, a technical\nevaluation of 500 images of built environments, and a user study with 10\nparticipants of varying mobility. Results from our technical evaluation and\nuser study show that Accessibility Scout can generate personalized\naccessibility scans that extend beyond traditional ADA considerations. Finally,\nwe conclude with a discussion on the implications of our work and future steps\nfor building more scalable and personalized accessibility assessments of the\nphysical world.", "AI": {"tldr": "Accessibility Scout is an LLM-based system for personalized accessibility scanning of built environments, addressing the limitations of manual assessments.", "motivation": "The need for scalable and personalized assessments of accessibility in built environments for people with disabilities.", "method": "Development of Accessibility Scout, an LLM-based system that analyzes photos to identify accessibility concerns and tailors results based on user-specific preferences.", "result": "Accessibility Scout generated personalized scans that consider individual mobility levels and environmental interests, outperforming traditional assessments.", "conclusion": "The system enhances accessibility assessments and highlights future directions for more adaptive technologies in this field.", "key_contributions": ["Introduction of Accessibility Scout, an LLM-driven assessment tool.", "Demonstration of personalized accessibility scans that exceed traditional ADA guidelines.", "Findings from multiple user studies validating the system's effectiveness."], "limitations": "The studies involved a limited number of participants and focused on a specific set of environments.", "keywords": ["accessibility", "large language models", "human-computer interaction", "personalization", "built environments"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.22927", "pdf": "https://arxiv.org/pdf/2507.22927.pdf", "abs": "https://arxiv.org/abs/2507.22927", "title": "PRGB Benchmark: A Robust Placeholder-Assisted Algorithm for Benchmarking Retrieval-Augmented Generation", "authors": ["Zhehao Tan", "Yihan Jiao", "Dan Yang", "Lei Liu", "Jie Feng", "Duolin Sun", "Yue Shen", "Jian Wang", "Peng Wei", "Jinjie Gu"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\nintegrating external knowledge, where the LLM's ability to generate responses\nbased on the combination of a given query and retrieved documents is crucial.\nHowever, most benchmarks focus on overall RAG system performance, rarely\nassessing LLM-specific capabilities. Current benchmarks emphasize broad aspects\nsuch as noise robustness, but lack a systematic and granular evaluation\nframework on document utilization. To this end, we introduce\n\\textit{Placeholder-RAG-Benchmark}, a multi-level fine-grained benchmark,\nemphasizing the following progressive dimensions: (1) multi-level filtering\nabilities, (2) combination abilities, and (3) reference reasoning. To provide a\nmore nuanced understanding of LLMs' roles in RAG systems, we formulate an\ninnovative placeholder-based approach to decouple the contributions of the\nLLM's parametric knowledge and the external knowledge. Experiments demonstrate\nthe limitations of representative LLMs in the RAG system's generation\ncapabilities, particularly in error resilience and context faithfulness. Our\nbenchmark provides a reproducible framework for developing more reliable and\nefficient RAG systems. Our code is available in\nhttps://github.com/Alipay-Med/PRGB.", "AI": {"tldr": "The paper introduces a fine-grained benchmark for evaluating large language models (LLMs) in Retrieval-Augmented Generation (RAG) systems, focusing on specific capabilities rather than overall performance.", "motivation": "Existing benchmarks for RAG systems overlook detailed evaluations specific to LLMs, leading to a gap in understanding their capabilities.", "method": "The study proposes the Placeholder-RAG-Benchmark, which evaluates LLMs on multi-level filtering, combination abilities, and reference reasoning, using a novel placeholder-based approach for analysis.", "result": "Experiments reveal limitations in current LLMs regarding generation capabilities, especially in areas like error resilience and contextual integrity.", "conclusion": "The introduced benchmark facilitates the development of more effective RAG systems by providing a structured evaluation framework focused on LLM contributions.", "key_contributions": ["Introduction of the Placeholder-RAG-Benchmark for fine-grained LLM evaluation.", "A systematic approach to decouple LLM's parametric knowledge from external knowledge.", "Insights into the limitations of LLMs in context understanding and error handling."], "limitations": "Limited to assessing only the components involved in RAG systems, not broader LLM capabilities outside of this context.", "keywords": ["Retrieval-Augmented Generation", "Large Language Models", "Benchmarking"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.23215", "pdf": "https://arxiv.org/pdf/2507.23215.pdf", "abs": "https://arxiv.org/abs/2507.23215", "title": "Silent Impact: Tracking Tennis Shots from the Passive Arm", "authors": ["Junyong Park", "Saelyne Yang", "Sungho Jo"], "categories": ["cs.HC", "H.5.2; I.5.4"], "comment": "15 pages, 9 figures,", "summary": "Wearable technology has transformed sports analytics, offering new dimensions\nin enhancing player experience. Yet, many solutions involve cumbersome setups\nthat inhibit natural motion. In tennis, existing products require sensors on\nthe racket or dominant arm, causing distractions and discomfort. We propose\nSilent Impact, a novel and user-friendly system that analyzes tennis shots\nusing a sensor placed on the passive arm. Collecting Inertial Measurement Unit\nsensor data from 20 recreational tennis players, we developed neural networks\nthat exclusively utilize passive arm data to detect and classify six shots,\nachieving a classification accuracy of 88.2% and a detection F1 score of 86.0%,\ncomparable to the dominant arm. These models were then incorporated into an\nend-to-end prototype, which records passive arm motion through a smartwatch and\ndisplays a summary of shots on a mobile app. User study (N=10) showed that\nparticipants felt less burdened physically and mentally using Silent Impact on\nthe passive arm. Overall, our research establishes the passive arm as an\neffective, comfortable alternative for tennis shot analysis, advancing\nuser-friendly sports analytics.", "AI": {"tldr": "The paper presents Silent Impact, a user-friendly system for analyzing tennis shots using sensors on the passive arm, achieving high classification accuracy and user satisfaction.", "motivation": "To improve sports analytics by reducing the physical and mental burden of existing sensor setups in tennis.", "method": "Developed neural networks that use data from Inertial Measurement Units on the passive arm to classify six types of tennis shots, and created a mobile app prototype displaying the results.", "result": "Achieved a classification accuracy of 88.2% and a detection F1 score of 86.0%, comparable to solutions using the dominant arm.", "conclusion": "The study shows that analysis through the passive arm is an effective and comfortable alternative, enhancing the user experience in sports analytics.", "key_contributions": ["Introduction of a novel system for tennis shot analysis using passive arm data.", "Demonstrated high accuracy and effectiveness compared to existing solutions.", "User study results indicated reduced physical and mental burden for users."], "limitations": "Limited to recreational tennis players and requires further validation in competitive settings.", "keywords": ["wearable technology", "sports analytics", "tennis", "passive arm", "machine learning"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.22928", "pdf": "https://arxiv.org/pdf/2507.22928.pdf", "abs": "https://arxiv.org/abs/2507.22928", "title": "How does Chain of Thought Think? Mechanistic Interpretability of Chain-of-Thought Reasoning with Sparse Autoencoding", "authors": ["Xi Chen", "Aske Plaat", "Niki van Stein"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Chain-of-thought (CoT) prompting boosts Large Language Models accuracy on\nmulti-step tasks, yet whether the generated \"thoughts\" reflect the true\ninternal reasoning process is unresolved. We present the first feature-level\ncausal study of CoT faithfulness. Combining sparse autoencoders with activation\npatching, we extract monosemantic features from Pythia-70M and Pythia-2.8B\nwhile they tackle GSM8K math problems under CoT and plain (noCoT) prompting.\nSwapping a small set of CoT-reasoning features into a noCoT run raises answer\nlog-probabilities significantly in the 2.8B model, but has no reliable effect\nin 70M, revealing a clear scale threshold. CoT also leads to significantly\nhigher activation sparsity and feature interpretability scores in the larger\nmodel, signalling more modular internal computation. For example, the model's\nconfidence in generating correct answers improves from 1.2 to 4.3. We introduce\npatch-curves and random-feature patching baselines, showing that useful CoT\ninformation is not only present in the top-K patches but widely distributed.\nOverall, our results indicate that CoT can induce more interpretable internal\nstructures in high-capacity LLMs, validating its role as a structured prompting\nmethod.", "AI": {"tldr": "A study on the faithfulness of Chain-of-Thought (CoT) prompting in Large Language Models (LLMs) shows that it enhances model interpretability and accuracy on multi-step tasks, especially in larger models.", "motivation": "To investigate whether the thoughts generated by CoT prompting reflect the true internal reasoning of Large Language Models, and to examine the impact of model scale on CoT effectiveness.", "method": "A causal study utilizing sparse autoencoders and activation patching on Pythia-70M and Pythia-2.8B models while solving math problems, comparing CoT versus noCoT prompting.", "result": "The study found that CoT prompting significantly improves answer log-probabilities in the 2.8B model and leads to higher internal activation sparsity and interpretability scores, indicating enhanced internal computation structure.", "conclusion": "CoT prompting enhances the interpretability and reliability of internal model processes in high-capacity LLMs, validating its effectiveness as a structured prompting technique.", "key_contributions": ["First feature-level causal study of CoT faithfulness in LLMs", "Demonstrated scale threshold effects of CoT in model accuracy", "Introduced patch-curves and explored feature distribution relevance"], "limitations": "Study limited to Pythia models; further investigation needed on broader model architectures and tasks.", "keywords": ["Chain-of-Thought", "Large Language Models", "interpretability", "activation sparsity", "causal study"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.23298", "pdf": "https://arxiv.org/pdf/2507.23298.pdf", "abs": "https://arxiv.org/abs/2507.23298", "title": "Real-time Generation of Various Types of Nodding for Avatar Attentive Listening System", "authors": ["Kazushi Kato", "Koji Inoue", "Divesh Lala", "Keiko Ochi", "Tatsuya Kawahara"], "categories": ["cs.HC", "cs.SD", "eess.AS"], "comment": "Accepted by 27th ACM International Conference on Multimodal\n  Interaction (ICMI '25), Long paper", "summary": "In human dialogue, nonverbal information such as nodding and facial\nexpressions is as crucial as verbal information, and spoken dialogue systems\nare also expected to express such nonverbal behaviors. We focus on nodding,\nwhich is critical in an attentive listening system, and propose a model that\npredicts both its timing and type in real time. The proposed model builds on\nthe voice activity projection (VAP) model, which predicts voice activity from\nboth listener and speaker audio. We extend it to prediction of various types of\nnodding in a continuous and real-time manner unlike conventional models. In\naddition, the proposed model incorporates multi-task learning with verbal\nbackchannel prediction and pretraining on general dialogue data. In the timing\nand type prediction task, the effectiveness of multi-task learning was\nsignificantly demonstrated. We confirmed that reducing the processing rate\nenables real-time operation without a substantial drop in accuracy, and\nintegrated the model into an avatar attentive listening system. Subjective\nevaluations showed that it outperformed the conventional method, which always\ndoes nodding in sync with verbal backchannel. The code and trained models are\navailable at https://github.com/MaAI-Kyoto/MaAI.", "AI": {"tldr": "This paper presents a real-time model for predicting nodding behavior in dialogue systems, enhancing attentive listening through nonverbal cues.", "motivation": "To improve spoken dialogue systems by integrating nonverbal behaviors, specifically nodding, to enhance user interaction and attentiveness.", "method": "The model builds on the voice activity projection (VAP) framework, extending it to predict nodding timing and type in real time using multi-task learning and verbal backchannel predictions.", "result": "The model demonstrates significant improvements in nodding prediction accuracy and real-time performance compared to conventional methods, verified through subjective evaluations in an avatar listening system.", "conclusion": "The proposed prediction model effectively integrates nonverbal cues into dialogue systems, achieving real-time operation and improved user experience.", "key_contributions": ["Real-time nodding prediction in dialogue systems", "Integration of multi-task learning for backchannel prediction", "Improved performance over conventional nodding methods"], "limitations": "", "keywords": ["nonverbal communication", "dialogue systems", "real-time prediction"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.22929", "pdf": "https://arxiv.org/pdf/2507.22929.pdf", "abs": "https://arxiv.org/abs/2507.22929", "title": "EH-Benchmark Ophthalmic Hallucination Benchmark and Agent-Driven Top-Down Traceable Reasoning Workflow", "authors": ["Xiaoyu Pan", "Yang Bai", "Ke Zou", "Yang Zhou", "Jun Zhou", "Huazhu Fu", "Yih-Chung Tham", "Yong Liu"], "categories": ["cs.CL", "cs.CV", "cs.MA"], "comment": "9 figures, 5 tables. submit/6621751", "summary": "Medical Large Language Models (MLLMs) play a crucial role in ophthalmic\ndiagnosis, holding significant potential to address vision-threatening\ndiseases. However, their accuracy is constrained by hallucinations stemming\nfrom limited ophthalmic knowledge, insufficient visual localization and\nreasoning capabilities, and a scarcity of multimodal ophthalmic data, which\ncollectively impede precise lesion detection and disease diagnosis.\nFurthermore, existing medical benchmarks fail to effectively evaluate various\ntypes of hallucinations or provide actionable solutions to mitigate them. To\naddress the above challenges, we introduce EH-Benchmark, a novel ophthalmology\nbenchmark designed to evaluate hallucinations in MLLMs. We categorize MLLMs'\nhallucinations based on specific tasks and error types into two primary\nclasses: Visual Understanding and Logical Composition, each comprising multiple\nsubclasses. Given that MLLMs predominantly rely on language-based reasoning\nrather than visual processing, we propose an agent-centric, three-phase\nframework, including the Knowledge-Level Retrieval stage, the Task-Level Case\nStudies stage, and the Result-Level Validation stage. Experimental results show\nthat our multi-agent framework significantly mitigates both types of\nhallucinations, enhancing accuracy, interpretability, and reliability. Our\nproject is available at https://github.com/ppxy1/EH-Benchmark.", "AI": {"tldr": "The paper introduces EH-Benchmark, a new ophthalmology benchmark aimed at evaluating and mitigating hallucinations in Medical Large Language Models (MLLMs) utilized for ophthalmic diagnosis.", "motivation": "To address the limitations of Medical Large Language Models in ophthalmic diagnosis, specifically their inaccuracies due to hallucinations and insufficient multimodal data.", "method": "The authors present a three-phase framework: Knowledge-Level Retrieval, Task-Level Case Studies, and Result-Level Validation to systematically evaluate and reduce hallucinations in MLLMs.", "result": "The multi-agent framework proposed significantly reduces hallucinations, improving the accuracy, interpretability, and reliability of ophthalmic diagnostics using MLLMs.", "conclusion": "The EH-Benchmark offers a structured approach to evaluate and mitigate hallucinations in MLLMs, thereby enhancing their diagnostic capabilities in ophthalmology.", "key_contributions": ["Introduction of EH-Benchmark for evaluating MLLM hallucinations", "Categorization of hallucinations into Visual Understanding and Logical Composition", "Proposed framework that improves MLLM performance in ophthalmic tasks"], "limitations": "", "keywords": ["Medical Large Language Models", "ophthalmology", "hallucinations", "benchmark", "multimodal data"], "importance_score": 9, "read_time_minutes": 6}}
{"id": "2507.23454", "pdf": "https://arxiv.org/pdf/2507.23454.pdf", "abs": "https://arxiv.org/abs/2507.23454", "title": "Breaking the mould of Social Mixed Reality -- State-of-the-Art and Glossary", "authors": ["Marta Bieńkiewicz", "Julia Ayache", "Panayiotis Charalambous", "Cristina Becchio", "Marco Corragio", "Bertram Taetz", "Francesco De Lellis", "Antonio Grotta", "Anna Server", "Daniel Rammer", "Richard Kulpa", "Franck Multon", "Azucena Garcia-Palacios", "Jessica Sutherland", "Kathleen Bryson", "Stéphane Donikian", "Didier Stricker", "Benoît Bardy"], "categories": ["cs.HC", "cs.CY", "cs.ET", "cs.GR", "q-bio.NC", "I.3.0; I.2; J.4; K.4"], "comment": "pre-print", "summary": "This article explores a critical gap in Mixed Reality (MR) technology: while\nadvances have been made, MR still struggles to authentically replicate human\nembodiment and socio-motor interaction. For MR to enable truly meaningful\nsocial experiences, it needs to incorporate multi-modal data streams and\nmulti-agent interaction capabilities. To address this challenge, we present a\ncomprehensive glossary covering key topics such as Virtual Characters and\nAutonomisation, Responsible AI, Ethics by Design, and the Scientific Challenges\nof Social MR within Neuroscience, Embodiment, and Technology. Our aim is to\ndrive the transformative evolution of MR technologies that prioritize\nhuman-centric innovation, fostering richer digital connections. We advocate for\nMR systems that enhance social interaction and collaboration between humans and\nvirtual autonomous agents, ensuring inclusivity, ethical design and\npsychological safety in the process.", "AI": {"tldr": "The paper addresses the limitations of Mixed Reality (MR) in replicating human embodiment and socio-motor interaction, advocating for advancements in multi-modal data and multi-agent interactions to enhance social experiences.", "motivation": "To improve meaningful social interactions in Mixed Reality (MR) environments, addressing gaps in technology that hinder authentic embodiment and interaction.", "method": "The article provides a comprehensive glossary of key topics related to Mixed Reality, including Virtual Characters, Responsible AI, Ethics by Design, and scientific challenges linked to Neuroscience and Technology.", "result": "The paper emphasizes the need for MR technologies that enhance human-centric innovation and social collaboration between humans and virtual agents.", "conclusion": "The authors argue for MR systems that ensure ethical design, inclusivity, and psychological safety to foster richer social interactions in digital environments.", "key_contributions": ["Comprehensive glossary encompassing key MR topics", "Emphasis on human-centric innovation in MR", "Discussion on ethical considerations in MR design"], "limitations": "", "keywords": ["Mixed Reality", "Human-Centric Innovation", "Social Interaction", "Ethics by Design", "Virtual Characters"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2507.22930", "pdf": "https://arxiv.org/pdf/2507.22930.pdf", "abs": "https://arxiv.org/abs/2507.22930", "title": "Protecting Vulnerable Voices: Synthetic Dataset Generation for Self-Disclosure Detection", "authors": ["Shalini Jangra", "Suparna De", "Nishanth Sastry", "Saeed Fadaei"], "categories": ["cs.CL", "cs.SI"], "comment": "15 pages, 4 Figures, Accepted in \"The 17th International Conference\n  on Advances in Social Networks Analysis and Mining -ASONAM-2025\"", "summary": "Social platforms such as Reddit have a network of communities of shared\ninterests, with a prevalence of posts and comments from which one can infer\nusers' Personal Information Identifiers (PIIs). While such self-disclosures can\nlead to rewarding social interactions, they pose privacy risks and the threat\nof online harms. Research into the identification and retrieval of such risky\nself-disclosures of PIIs is hampered by the lack of open-source labeled\ndatasets. To foster reproducible research into PII-revealing text detection, we\ndevelop a novel methodology to create synthetic equivalents of PII-revealing\ndata that can be safely shared. Our contributions include creating a taxonomy\nof 19 PII-revealing categories for vulnerable populations and the creation and\nrelease of a synthetic PII-labeled multi-text span dataset generated from 3\ntext generation Large Language Models (LLMs), Llama2-7B, Llama3-8B, and\nzephyr-7b-beta, with sequential instruction prompting to resemble the original\nReddit posts. The utility of our methodology to generate this synthetic dataset\nis evaluated with three metrics: First, we require reproducibility equivalence,\ni.e., results from training a model on the synthetic data should be comparable\nto those obtained by training the same models on the original posts. Second, we\nrequire that the synthetic data be unlinkable to the original users, through\ncommon mechanisms such as Google Search. Third, we wish to ensure that the\nsynthetic data be indistinguishable from the original, i.e., trained humans\nshould not be able to tell them apart. We release our dataset and code at\nhttps://netsys.surrey.ac.uk/datasets/synthetic-self-disclosure/ to foster\nreproducible research into PII privacy risks in online social media.", "AI": {"tldr": "The paper presents a novel methodology for generating synthetic data to study Personal Information Identifiers (PIIs) in social media posts, addressing privacy concerns with reproducible research.", "motivation": "To address the privacy risks associated with self-disclosure of Personal Information Identifiers (PIIs) on social platforms and facilitate reproducible research in this area.", "method": "The authors create synthetic equivalents of PII-revealing data using a taxonomy of 19 categories and three text generation Large Language Models (LLMs), with an emphasis on making the data unlinkable and indistinguishable from real posts.", "result": "The methodology successfully generates a synthetic multi-text span dataset, evaluated on reproducibility equivalence, unlinkability, and indistinguishability from original posts.", "conclusion": "The synthetic PII-labeled dataset is publicly released to advance research on privacy risks in online social media while minimizing harm to real users.", "key_contributions": ["Creation of a taxonomy of 19 PII-revealing categories for vulnerable populations.", "Development of a synthetic PII-labeled multi-text span dataset using three LLMs.", "Evaluation metrics ensuring data's unlinkability and indistinguishability from original content."], "limitations": "The study may face challenges in completely ensuring indistinguishability and may not cover all types of PIIs or contexts.", "keywords": ["Personal Information Identifiers", "Synthetic Data", "Privacy Risks", "Large Language Models", "Social Media"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.23470", "pdf": "https://arxiv.org/pdf/2507.23470.pdf", "abs": "https://arxiv.org/abs/2507.23470", "title": "Automated Feedback on Student-Generated UML and ER Diagrams Using Large Language Models", "authors": ["Sebastian Gürtl", "Gloria Schimetta", "David Kerschbaumer", "Michael Liut", "Alexander Steinmaurer"], "categories": ["cs.HC", "cs.AI"], "comment": "Learnersourcing: Student-generated Content @ Scale Workshop at L@S\n  2025", "summary": "UML and ER diagrams are foundational in computer science education but come\nwith challenges for learners due to the need for abstract thinking, contextual\nunderstanding, and mastery of both syntax and semantics. These complexities are\ndifficult to address through traditional teaching methods, which often struggle\nto provide scalable, personalized feedback, especially in large classes. We\nintroduce DUET (Diagrammatic UML & ER Tutor), a prototype of an LLM-based tool,\nwhich converts a reference diagram and a student-submitted diagram into a\ntextual representation and provides structured feedback based on the\ndifferences. It uses a multi-stage LLM pipeline to compare diagrams and\ngenerate reflective feedback. Furthermore, the tool enables analytical insights\nfor educators, aiming to foster self-directed learning and inform instructional\nstrategies. We evaluated DUET through semi-structured interviews with six\nparticipants, including two educators and four teaching assistants. They\nidentified strengths such as accessibility, scalability, and learning support\nalongside limitations, including reliability and potential misuse. Participants\nalso suggested potential improvements, such as bulk upload functionality and\ninteractive clarification features. DUET presents a promising direction for\nintegrating LLMs into modeling education and offers a foundation for future\nclassroom integration and empirical evaluation.", "AI": {"tldr": "DUET is an LLM-based tool designed to provide structured feedback on UML and ER diagrams, enhancing learning in computer science.", "motivation": "Traditional teaching methods struggle to offer scalable, personalized feedback in UML and ER diagram education, which is needed for student mastery.", "method": "DUET converts reference and student-submitted diagrams into textual representations, using a multi-stage LLM pipeline for comparison and feedback generation.", "result": "Evaluation through interviews revealed strengths of DUET in accessibility and scalability, as well as limitations like reliability and potential misuse.", "conclusion": "DUET shows promise for integrating LLMs in modeling education, with suggestions for improvement and implications for future classroom use.", "key_contributions": ["Introduction of an LLM-based tool for diagram feedback", "Structured feedback generation from diagram comparisons", "Insights for educators to improve instructional strategies"], "limitations": "Reliability and potential misuse of the tool were identified.", "keywords": ["LLM", "UML", "ER diagrams", "feedback", "education"], "importance_score": 7, "read_time_minutes": 12}}
{"id": "2507.22931", "pdf": "https://arxiv.org/pdf/2507.22931.pdf", "abs": "https://arxiv.org/abs/2507.22931", "title": "Enhancing RAG Efficiency with Adaptive Context Compression", "authors": ["Shuyu Guo", "Zhaochun Ren"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs)\nwith external knowledge but incurs significant inference costs due to lengthy\nretrieved contexts. While context compression mitigates this issue, existing\nmethods apply fixed compression rates, over-compressing simple queries or\nunder-compressing complex ones. We propose Adaptive Context Compression for RAG\n(ACC-RAG), a framework that dynamically adjusts compression rates based on\ninput complexity, optimizing inference efficiency without sacrificing accuracy.\nACC-RAG combines a hierarchical compressor (for multi-granular embeddings) with\na context selector to retain minimal sufficient information, akin to human\nskimming. Evaluated on Wikipedia and five QA datasets, ACC-RAG outperforms\nfixed-rate methods and matches/unlocks over 4 times faster inference versus\nstandard RAG while maintaining or improving accuracy.", "AI": {"tldr": "ACC-RAG introduces Adaptive Context Compression for RAG, optimizing inference costs by dynamically adjusting compression rates based on input complexity.", "motivation": "To mitigate the significant inference costs incurred by retrieval-augmented generation (RAG) systems due to lengthy retrieved contexts.", "method": "ACC-RAG combines a hierarchical compressor for multi-granular embeddings with a context selector that dynamically adjusts compression rates based on the complexity of the input.", "result": "ACC-RAG outperforms existing fixed-rate compression methods and achieves over 4 times faster inference compared to standard RAG while maintaining or improving accuracy.", "conclusion": "The adaptive nature of ACC-RAG allows for optimized inference efficiency without sacrificing the accuracy of the results.", "key_contributions": ["Introduction of Adaptive Context Compression for RAG", "Dynamic adjustment of compression rates based on input complexity", "Combination of hierarchical compression with context selection for improved efficiency."], "limitations": "", "keywords": ["Adaptive Context Compression", "RAG", "Large Language Models", "Inference Efficiency", "Context Compression"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2507.23492", "pdf": "https://arxiv.org/pdf/2507.23492.pdf", "abs": "https://arxiv.org/abs/2507.23492", "title": "Digital literacy interventions can boost humans in discerning deepfakes", "authors": ["Dominique Geissler", "Claire Robertson", "Stefan Feuerriegel"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Deepfakes, i.e., images generated by artificial intelligence (AI), can erode\ntrust in institutions and compromise election outcomes, as people often\nstruggle to discern real images from deepfakes. Improving digital literacy can\nhelp address these challenges, yet scalable and effective approaches remain\nlargely unexplored. Here, we compare the efficacy of five digital literacy\ninterventions to boost people's ability to discern deepfakes: (1) textual\nguidance on common indicators of deepfakes; (2) visual demonstrations of these\nindicators; (3) a gamified exercise for identifying deepfakes; (4) implicit\nlearning through repeated exposure and feedback; and (5) explanations of how\ndeepfakes are generated with the help of AI. We conducted an experiment with\nN=1,200 participants from the United States to test the immediate and long-term\neffectiveness of our interventions. Our results show that our interventions can\nboost deepfake discernment by up to 13 percentage points while maintaining\ntrust in real images. Altogether, our approach is scalable, suitable for\ndiverse populations, and highly effective for boosting deepfake detection while\nmaintaining trust in truthful information.", "AI": {"tldr": "This paper examines five interventions aimed at improving digital literacy for discerning deepfakes, demonstrating significant effectiveness in enhancing detection skills.", "motivation": "The increasing prevalence of deepfakes threatens trust in institutions and democratic processes, necessitating effective strategies to enhance digital literacy.", "method": "The study compares five interventions aimed at improving deepfake discernment among 1,200 participants, measuring immediate and long-term effectiveness.", "result": "The interventions improved participants' ability to identify deepfakes by up to 13 percentage points while preserving trust in genuine images.", "conclusion": "The proposed digital literacy interventions are scalable and effective, making them suitable for diverse populations to enhance deepfake detection skills.", "key_contributions": ["Comparison of five digital literacy interventions for deepfake detection", "Demonstrated significant improvements in discernment capabilities", "Maintained trust in authentic images while boosting detection rates."], "limitations": "The study's findings may not generalize across different cultural contexts or age groups.", "keywords": ["deepfakes", "digital literacy", "interventions", "human-computer interaction", "trust"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.22932", "pdf": "https://arxiv.org/pdf/2507.22932.pdf", "abs": "https://arxiv.org/abs/2507.22932", "title": "FinMarBa: A Market-Informed Dataset for Financial Sentiment Classification", "authors": ["Baptiste Lefort", "Eric Benhamou", "Beatrice Guez", "Jean-Jacques Ohana", "Ethan Setrouk", "Alban Etienne"], "categories": ["cs.CL", "q-fin.GN"], "comment": "8 pages", "summary": "This paper presents a novel hierarchical framework for portfolio\noptimization, integrating lightweight Large Language Models (LLMs) with Deep\nReinforcement Learning (DRL) to combine sentiment signals from financial news\nwith traditional market indicators. Our three-tier architecture employs base RL\nagents to process hybrid data, meta-agents to aggregate their decisions, and a\nsuper-agent to merge decisions based on market data and sentiment analysis.\nEvaluated on data from 2018 to 2024, after training on 2000-2017, the framework\nachieves a 26% annualized return and a Sharpe ratio of 1.2, outperforming\nequal-weighted and S&P 500 benchmarks. Key contributions include scalable\ncross-modal integration, a hierarchical RL structure for enhanced stability,\nand open-source reproducibility.", "AI": {"tldr": "A hierarchical framework integrates LLMs with DRL for portfolio optimization using sentiment analysis and traditional market indicators.", "motivation": "To improve portfolio optimization by leveraging sentiment signals and market data through an advanced hierarchical approach.", "method": "The framework consists of three tiers: base RL agents process data, meta-agents aggregate decisions, and a super-agent merges decisions for optimal portfolio management.", "result": "The framework achieved a 26% annualized return and a Sharpe ratio of 1.2, outperforming traditional benchmarks.", "conclusion": "This novel approach demonstrates significant improvements in portfolio returns and stability by combining LLMs with reinforcement learning techniques.", "key_contributions": ["Scalable cross-modal integration of market data and sentiment analysis", "Hierarchical reinforcement learning structure for enhanced stability", "Open-source framework for reproducibility"], "limitations": "", "keywords": ["portfolio optimization", "large language models", "deep reinforcement learning", "financial sentiment analysis", "hierarchical frameworks"], "importance_score": 6, "read_time_minutes": 8}}
{"id": "2507.23585", "pdf": "https://arxiv.org/pdf/2507.23585.pdf", "abs": "https://arxiv.org/abs/2507.23585", "title": "Agency Among Agents: Designing with Hypertextual Friction in the Algorithmic Web", "authors": ["Sophia Liu", "Shm Garanganao Almeda"], "categories": ["cs.HC"], "comment": "To appear in: Adjunct Proceedings of the 36th ACM Conference on\n  Hypertext and Social Media, Chicago, IL, USA, September 15-18, 2025", "summary": "Today's algorithm-driven interfaces, from recommendation feeds to GenAI\ntools, often prioritize engagement and efficiency at the expense of user\nagency. As systems take on more decision-making, users have less control over\nwhat they see and how meaning or relationships between content are constructed.\nThis paper introduces \"Hypertextual Friction,\" a conceptual design stance that\nrepositions classical hypertext principles--friction, traceability, and\nstructure--as actionable values for reclaiming agency in algorithmically\nmediated environments. Through a comparative analysis of real-world\ninterfaces--Wikipedia vs. Instagram Explore, and Are.na vs. GenAI image\ntools--we examine how different systems structure user experience, navigation,\nand authorship. We show that hypertext systems emphasize provenance,\nassociative thinking, and user-driven meaning-making, while algorithmic systems\ntend to obscure process and flatten participation. We contribute: (1) a\ncomparative analysis of how interface structures shape agency in user-driven\nversus agent-driven systems, and (2) a conceptual stance that offers\nhypertextual values as design commitments for reclaiming agency in an\nincreasingly algorithmic web.", "AI": {"tldr": "This paper discusses 'Hypertextual Friction' to reclaim user agency in algorithm-driven interfaces.", "motivation": "Algorithm-driven interfaces often compromise user agency for efficiency and engagement, necessitating a design approach that emphasizes user control.", "method": "A comparative analysis of real-world interfaces such as Wikipedia and Instagram Explore, examining their impact on user experience, navigation, and authorship.", "result": "Hypertext systems prioritize provenance and user-driven meaning-making, which contrasts with algorithmic systems that obscure processes and flatten participation.", "conclusion": "The paper proposes hypertextual values as actionable design commitments for enhancing user agency in algorithmically mediated environments.", "key_contributions": ["Comparative analysis of interface structures affecting user agency", "Introduction of 'Hypertextual Friction' as a design stance", "Recommendations for integrating hypertextual values in algorithmic interfaces"], "limitations": "", "keywords": ["Human-Computer Interaction", "Algorithmic Interfaces", "User Agency"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.22933", "pdf": "https://arxiv.org/pdf/2507.22933.pdf", "abs": "https://arxiv.org/abs/2507.22933", "title": "Augmented Vision-Language Models: A Systematic Review", "authors": ["Anthony C Davis", "Burhan Sadiq", "Tianmin Shu", "Chien-Ming Huang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in visual-language machine learning models have demonstrated\nexceptional ability to use natural language and understand visual scenes by\ntraining on large, unstructured datasets. However, this training paradigm\ncannot produce interpretable explanations for its outputs, requires retraining\nto integrate new information, is highly resource-intensive, and struggles with\ncertain forms of logical reasoning. One promising solution involves integrating\nneural networks with external symbolic information systems, forming neural\nsymbolic systems that can enhance reasoning and memory abilities. These neural\nsymbolic systems provide more interpretable explanations to their outputs and\nthe capacity to assimilate new information without extensive retraining.\nUtilizing powerful pre-trained Vision-Language Models (VLMs) as the core neural\ncomponent, augmented by external systems, offers a pragmatic approach to\nrealizing the benefits of neural-symbolic integration. This systematic\nliterature review aims to categorize techniques through which visual-language\nunderstanding can be improved by interacting with external symbolic information\nsystems.", "AI": {"tldr": "This paper reviews methods to enhance visual-language understanding through integration with external symbolic systems, addressing issues like interpretability and resource intensity in traditional models.", "motivation": "To improve the interpretability and reasoning capabilities of visual-language models by integrating them with external symbolic information systems.", "method": "The paper conducts a systematic literature review of existing techniques that enhance visual-language understanding using neural-symbolic systems.", "result": "The review categorizes various methods and highlights the advantages of combining Vision-Language Models with symbolic information systems, including improved interpretability and reduced retraining needs.", "conclusion": "Integrating neural networks with symbolic systems presents a promising avenue for enhancing visual-language models' performance and adaptability.", "key_contributions": ["Systematic categorization of techniques for visual-language improvement", "Identification of benefits from combining neural and symbolic systems", "Discussion on enhancing interpretability and reasoning capabilities"], "limitations": "", "keywords": ["visual-language models", "neural-symbolic systems", "interpretability"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.22936", "pdf": "https://arxiv.org/pdf/2507.22936.pdf", "abs": "https://arxiv.org/abs/2507.22936", "title": "Evaluating Large Language Models (LLMs) in Financial NLP: A Comparative Study on Financial Report Analysis", "authors": ["Md Talha Mohsin"], "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.HC", "q-fin.CP"], "comment": "22 Pages, 6 Tables, 7 Figures", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na wide variety of Financial Natural Language Processing (FinNLP) tasks.\nHowever, systematic comparisons among widely used LLMs remain underexplored.\nGiven the rapid advancement and growing influence of LLMs in financial\nanalysis, this study conducts a thorough comparative evaluation of five leading\nLLMs, GPT, Claude, Perplexity, Gemini and DeepSeek, using 10-K filings from the\n'Magnificent Seven' technology companies. We create a set of domain-specific\nprompts and then use three methodologies to evaluate model performance: human\nannotation, automated lexical-semantic metrics (ROUGE, Cosine Similarity,\nJaccard), and model behavior diagnostics (prompt-level variance and\nacross-model similarity). The results show that GPT gives the most coherent,\nsemantically aligned, and contextually relevant answers; followed by Claude and\nPerplexity. Gemini and DeepSeek, on the other hand, have more variability and\nless agreement. Also, the similarity and stability of outputs change from\ncompany to company and over time, showing that they are sensitive to how\nprompts are written and what source material is used.", "AI": {"tldr": "This study conducts a comparative evaluation of five leading LLMs in financial analysis using 10-K filings, assessing their performance through various methodologies.", "motivation": "To systematically compare the capabilities of leading LLMs in FinNLP tasks, given their increasing importance in financial analysis.", "method": "Evaluation of five LLMs (GPT, Claude, Perplexity, Gemini, DeepSeek) using human annotation, automated metrics (ROUGE, Cosine Similarity, Jaccard), and diagnostics on outputs from 10-K filings.", "result": "GPT provided the most coherent and contextually relevant responses, while Gemini and DeepSeek showed variability and less agreement across prompts and companies.", "conclusion": "The performance of LLMs is sensitive to prompt design and source material, with notable differences in stability and agreement across models and contexts.", "key_contributions": ["Thorough comparative analysis of multiple LLMs in financial tasks.", "Use of mixed evaluation methodologies combining human and automated metrics.", "Insights into the sensitivity of model outputs to prompt design and source material."], "limitations": "Focuses only on five LLMs and may not generalize to other domains outside FinNLP.", "keywords": ["Large Language Models", "Financial NLP", "LLM Evaluation", "Prompt Sensitivity", "Model Comparison"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.22934", "pdf": "https://arxiv.org/pdf/2507.22934.pdf", "abs": "https://arxiv.org/abs/2507.22934", "title": "Deep Learning Approaches for Multimodal Intent Recognition: A Survey", "authors": ["Jingwei Zhao", "Yuhua Wen", "Qifei Li", "Minchi Hu", "Yingying Zhou", "Jingyao Xue", "Junyang Wu", "Yingming Gao", "Zhengqi Wen", "Jianhua Tao", "Ya Li"], "categories": ["cs.CL", "cs.AI"], "comment": "Submitted to ACM Computing Surveys", "summary": "Intent recognition aims to identify users' underlying intentions,\ntraditionally focusing on text in natural language processing. With growing\ndemands for natural human-computer interaction, the field has evolved through\ndeep learning and multimodal approaches, incorporating data from audio, vision,\nand physiological signals. Recently, the introduction of Transformer-based\nmodels has led to notable breakthroughs in this domain. This article surveys\ndeep learning methods for intent recognition, covering the shift from unimodal\nto multimodal techniques, relevant datasets, methodologies, applications, and\ncurrent challenges. It provides researchers with insights into the latest\ndevelopments in multimodal intent recognition (MIR) and directions for future\nresearch.", "AI": {"tldr": "This article surveys the evolution of intent recognition methods, emphasizing deep learning and multimodal techniques incorporating various data types.", "motivation": "The need for natural human-computer interaction has driven advancements in intent recognition beyond traditional text approaches.", "method": "The article reviews deep learning methods for intent recognition, with a focus on the transition from unimodal to multimodal techniques, including relevant datasets and applications.", "result": "The survey highlights significant breakthroughs achieved through the use of Transformer-based models and identifies existing challenges in the field.", "conclusion": "The paper provides a comprehensive overview of multimodal intent recognition, offering insights into future research directions.", "key_contributions": ["Survey of intent recognition methods involving deep learning", "Emphasis on multimodal data sources such as audio and vision", "Identifies future research directions in multimodal intent recognition"], "limitations": "", "keywords": ["intent recognition", "multimodal", "deep learning", "human-computer interaction", "Transformer models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.22935", "pdf": "https://arxiv.org/pdf/2507.22935.pdf", "abs": "https://arxiv.org/abs/2507.22935", "title": "Trusted Knowledge Extraction for Operations and Maintenance Intelligence", "authors": ["Kathleen Mealey", "Jonathan A. Karr Jr.", "Priscila Saboia Moreira", "Paul R. Brenner", "Charles F. Vardeman II"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Deriving operational intelligence from organizational data repositories is a\nkey challenge due to the dichotomy of data confidentiality vs data integration\nobjectives, as well as the limitations of Natural Language Processing (NLP)\ntools relative to the specific knowledge structure of domains such as\noperations and maintenance. In this work, we discuss Knowledge Graph\nconstruction and break down the Knowledge Extraction process into its Named\nEntity Recognition, Coreference Resolution, Named Entity Linking, and Relation\nExtraction functional components. We then evaluate sixteen NLP tools in concert\nwith or in comparison to the rapidly advancing capabilities of Large Language\nModels (LLMs). We focus on the operational and maintenance intelligence use\ncase for trusted applications in the aircraft industry. A baseline dataset is\nderived from a rich public domain US Federal Aviation Administration dataset\nfocused on equipment failures or maintenance requirements. We assess the\nzero-shot performance of NLP and LLM tools that can be operated within a\ncontrolled, confidential environment (no data is sent to third parties). Based\non our observation of significant performance limitations, we discuss the\nchallenges related to trusted NLP and LLM tools as well as their Technical\nReadiness Level for wider use in mission-critical industries such as aviation.\nWe conclude with recommendations to enhance trust and provide our open-source\ncurated dataset to support further baseline testing and evaluation.", "AI": {"tldr": "This paper addresses the challenge of operational intelligence from organizational data while balancing data confidentiality and integration. It evaluates NLP tools and LLMs for applications in the aircraft industry, focusing on knowledge graph construction and knowledge extraction.", "motivation": "The need to derive operational intelligence from data repositories while ensuring data confidentiality is a critical challenge, particularly in domains like operations and maintenance.", "method": "The paper discusses the Knowledge Graph construction and breaks down the Knowledge Extraction process into its components, including Named Entity Recognition, Coreference Resolution, Named Entity Linking, and Relation Extraction. It evaluates sixteen NLP tools alongside current capabilities of LLMs.", "result": "The evaluation reveals significant performance limitations of existing NLP and LLM tools for trusted applications in the aircraft industry, particularly in a controlled environment where confidentiality is prioritized.", "conclusion": "The paper concludes with a discussion on the challenges of using trusted NLP and LLM tools in mission-critical industries and provides recommendations for enhancement. An open-source curated dataset is offered for further testing.", "key_contributions": ["Evaluation of 16 NLP tools in the context of operational intelligence in the aircraft industry", "Identification of performance limitations of NLP and LLM tools for trusted applications", "Provision of an open-source curated dataset for further testing and evaluation."], "limitations": "The performance limitations of the evaluated NLP and LLM tools, especially in maintaining data confidentiality.", "keywords": ["Knowledge Graph", "Natural Language Processing", "Large Language Models", "Operational Intelligence", "Aircraft Industry"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2507.22936", "pdf": "https://arxiv.org/pdf/2507.22936.pdf", "abs": "https://arxiv.org/abs/2507.22936", "title": "Evaluating Large Language Models (LLMs) in Financial NLP: A Comparative Study on Financial Report Analysis", "authors": ["Md Talha Mohsin"], "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.HC", "q-fin.CP"], "comment": "22 Pages, 6 Tables, 7 Figures", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na wide variety of Financial Natural Language Processing (FinNLP) tasks.\nHowever, systematic comparisons among widely used LLMs remain underexplored.\nGiven the rapid advancement and growing influence of LLMs in financial\nanalysis, this study conducts a thorough comparative evaluation of five leading\nLLMs, GPT, Claude, Perplexity, Gemini and DeepSeek, using 10-K filings from the\n'Magnificent Seven' technology companies. We create a set of domain-specific\nprompts and then use three methodologies to evaluate model performance: human\nannotation, automated lexical-semantic metrics (ROUGE, Cosine Similarity,\nJaccard), and model behavior diagnostics (prompt-level variance and\nacross-model similarity). The results show that GPT gives the most coherent,\nsemantically aligned, and contextually relevant answers; followed by Claude and\nPerplexity. Gemini and DeepSeek, on the other hand, have more variability and\nless agreement. Also, the similarity and stability of outputs change from\ncompany to company and over time, showing that they are sensitive to how\nprompts are written and what source material is used.", "AI": {"tldr": "This study evaluates and compares five leading LLMs in the context of financial analysis using 10-K filings.", "motivation": "To systematically compare the performance of major LLMs in financial NLP tasks amidst their increasing influence in financial analysis.", "method": "The study uses human annotation, automated metrics (ROUGE, Cosine Similarity, Jaccard), and model behavior diagnostics to evaluate LLMs.", "result": "GPT outperforms others in coherence and contextual relevance, followed by Claude and Perplexity, with Gemini and DeepSeek showing more variability and less agreement.", "conclusion": "LLM performance varies significantly based on prompt design and source material, indicating a need for careful prompt crafting in financial NLP.", "key_contributions": ["Comprehensive evaluation of leading LLMs in financial contexts", "Domain-specific prompt creation for performance comparison", "Insights into the sensitivity of LLM outputs based on prompt and source material"], "limitations": "Limited to financial NLP tasks and specific technology companies' filings, may not generalize to other domains or prompts.", "keywords": ["Large Language Models", "Financial NLP", "Model Comparison", "10-K Filings", "AI in Finance"], "importance_score": 5, "read_time_minutes": 15}}
{"id": "2507.22937", "pdf": "https://arxiv.org/pdf/2507.22937.pdf", "abs": "https://arxiv.org/abs/2507.22937", "title": "CoE-Ops: Collaboration of LLM-based Experts for AIOps Question-Answering", "authors": ["Jinkun Zhao", "Yuanshuai Wang", "Xingjian Zhang", "Ruibo Chen", "Xingchuang Liao", "Junle Wang", "Lei Huang", "Kui Zhang", "Wenjun Wu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "With the rapid evolution of artificial intelligence, AIOps has emerged as a\nprominent paradigm in DevOps. Lots of work has been proposed to improve the\nperformance of different AIOps phases. However, constrained by domain-specific\nknowledge, a single model can only handle the operation requirement of a\nspecific task,such as log parser,root cause analysis. Meanwhile, combining\nmultiple models can achieve more efficient results, which have been proved in\nboth previous ensemble learning and the recent LLM training domain. Inspired by\nthese works,to address the similar challenges in AIOPS, this paper first\nproposes a collaboration-of-expert framework(CoE-Ops) incorporating a\ngeneral-purpose large language model task classifier. A retrieval-augmented\ngeneration mechanism is introduced to improve the framework's capability in\nhandling both Question-Answering tasks with high-level(Code,build,Test,etc.)\nand low-level(fault analysis,anomaly detection,etc.). Finally, the proposed\nmethod is implemented in the AIOps domain, and extensive experiments are\nconducted on the DevOps-EVAL dataset. Experimental results demonstrate that\nCoE-Ops achieves a 72% improvement in routing accuracy for high-level AIOps\ntasks compared to existing CoE methods, delivers up to 8% accuracy enhancement\nover single AIOps models in DevOps problem resolution, and outperforms\nlarger-scale Mixture-of-Experts (MoE) models by up to 14% in accuracy.", "AI": {"tldr": "The paper presents CoE-Ops, a framework that utilizes a general-purpose large language model for improved AIOps performance through a collaboration-of-expert approach and retrieval-augmented generation.", "motivation": "To address the limitations of single models in AIOps that can only handle specific tasks, this research aims to enhance task performance using a collaborative approach with large language models.", "method": "The paper proposes a collaboration-of-expert framework (CoE-Ops) that integrates a general-purpose large language model as a task classifier and employs a retrieval-augmented generation mechanism to handle various high-level and low-level AIOps tasks.", "result": "CoE-Ops achieves a 72% improvement in routing accuracy for high-level AIOps tasks, an 8% enhancement in accuracy over single AIOps models, and outperforms larger Mixture-of-Experts models by 14% in accuracy.", "conclusion": "The experimental results demonstrate that CoE-Ops significantly outperforms existing approaches in AIOps, indicating its potential for broader application in the field.", "key_contributions": ["Introduction of a collaboration-of-expert framework for AIOps", "Implementation of retrieval-augmented generation for diverse task handling", "Substantial experimental performance improvements over existing models"], "limitations": "", "keywords": ["AIOps", "large language model", "collaboration-of-expert", "retrieval-augmented generation", "DevOps"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2507.22938", "pdf": "https://arxiv.org/pdf/2507.22938.pdf", "abs": "https://arxiv.org/abs/2507.22938", "title": "A Graph-based Approach for Multi-Modal Question Answering from Flowcharts in Telecom Documents", "authors": ["Sumit Soman", "H. G. Ranjani", "Sujoy Roychowdhury", "Venkata Dharma Surya Narayana Sastry", "Akshat Jain", "Pranav Gangrade", "Ayaaz Khan"], "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "comment": "Accepted for publication at the KDD 2025 Workshop on Structured\n  Knowledge for Large Language Models", "summary": "Question-Answering (QA) from technical documents often involves questions\nwhose answers are present in figures, such as flowcharts or flow diagrams.\nText-based Retrieval Augmented Generation (RAG) systems may fail to answer such\nquestions. We leverage graph representations of flowcharts obtained from Visual\nlarge Language Models (VLMs) and incorporate them in a text-based RAG system to\nshow that this approach can enable image retrieval for QA in the telecom\ndomain. We present the end-to-end approach from processing technical documents,\nclassifying image types, building graph representations, and incorporating them\nwith the text embedding pipeline for efficient retrieval. We benchmark the same\non a QA dataset created based on proprietary telecom product information\ndocuments. Results show that the graph representations obtained using a\nfine-tuned VLM model have lower edit distance with respect to the ground truth,\nwhich illustrate the robustness of these representations for flowchart images.\nFurther, the approach for QA using these representations gives good retrieval\nperformance using text-based embedding models, including a telecom-domain\nadapted one. Our approach also alleviates the need for a VLM in inference,\nwhich is an important cost benefit for deployed QA systems.", "AI": {"tldr": "This paper proposes enhancing question-answering (QA) in telecom by integrating graph representations of flowcharts with text-based retrieval systems.", "motivation": "Improving QA systems' ability to retrieve answers from figures in technical documents, especially in the telecom domain.", "method": "Utilizing Visual Language Models to obtain graph representations of flowcharts and integrating these into a retrieval augmented generation (RAG) framework for effective QA.", "result": "The approach demonstrates that graph representations improve retrieval performance and have lower edit distances to ground truth compared to traditional methods.", "conclusion": "Integrating graph representations with text embeddings boosts QA effectiveness, reducing reliance on costly Visual Language Models during inference.", "key_contributions": ["Integration of graph representations of flowcharts in text-based RAG systems", "Demonstration of improved retrieval performance in telecom QA", "Cost benefits by reducing dependency on Visual Language Models during inference"], "limitations": "", "keywords": ["Question-Answering", "Graph Representations", "Visual Language Models", "Telecom", "Flowcharts"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.22939", "pdf": "https://arxiv.org/pdf/2507.22939.pdf", "abs": "https://arxiv.org/abs/2507.22939", "title": "PARROT: An Open Multilingual Radiology Reports Dataset", "authors": ["Bastien Le Guellec", "Kokou Adambounou", "Lisa C Adams", "Thibault Agripnidis", "Sung Soo Ahn", "Radhia Ait Chalal", "Tugba Akinci D Antonoli", "Philippe Amouyel", "Henrik Andersson", "Raphael Bentegeac", "Claudio Benzoni", "Antonino Andrea Blandino", "Felix Busch", "Elif Can", "Riccardo Cau", "Armando Ugo Cavallo", "Christelle Chavihot", "Erwin Chiquete", "Renato Cuocolo", "Eugen Divjak", "Gordana Ivanac", "Barbara Dziadkowiec Macek", "Armel Elogne", "Salvatore Claudio Fanni", "Carlos Ferrarotti", "Claudia Fossataro", "Federica Fossataro", "Katarzyna Fulek", "Michal Fulek", "Pawel Gac", "Martyna Gachowska", "Ignacio Garcia Juarez", "Marco Gatti", "Natalia Gorelik", "Alexia Maria Goulianou", "Aghiles Hamroun", "Nicolas Herinirina", "Krzysztof Kraik", "Dominik Krupka", "Quentin Holay", "Felipe Kitamura", "Michail E Klontzas", "Anna Kompanowska", "Rafal Kompanowski", "Alexandre Lefevre", "Tristan Lemke", "Maximilian Lindholz", "Lukas Muller", "Piotr Macek", "Marcus Makowski", "Luigi Mannacio", "Aymen Meddeb", "Antonio Natale", "Beatrice Nguema Edzang", "Adriana Ojeda", "Yae Won Park", "Federica Piccione", "Andrea Ponsiglione", "Malgorzata Poreba", "Rafal Poreba", "Philipp Prucker", "Jean Pierre Pruvo", "Rosa Alba Pugliesi", "Feno Hasina Rabemanorintsoa", "Vasileios Rafailidis", "Katarzyna Resler", "Jan Rotkegel", "Luca Saba", "Ezann Siebert", "Arnaldo Stanzione", "Ali Fuat Tekin", "Liz Toapanta Yanchapaxi", "Matthaios Triantafyllou", "Ekaterini Tsaoulia", "Evangelia Vassalou", "Federica Vernuccio", "Johan Wasselius", "Weilang Wang", "Szymon Urban", "Adrian Wlodarczak", "Szymon Wlodarczak", "Andrzej Wysocki", "Lina Xu", "Tomasz Zatonski", "Shuhang Zhang", "Sebastian Ziegelmayer", "Gregory Kuchcinski", "Keno K Bressem"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Rationale and Objectives: To develop and validate PARROT (Polyglottal\nAnnotated Radiology Reports for Open Testing), a large, multicentric,\nopen-access dataset of fictional radiology reports spanning multiple languages\nfor testing natural language processing applications in radiology. Materials\nand Methods: From May to September 2024, radiologists were invited to\ncontribute fictional radiology reports following their standard reporting\npractices. Contributors provided at least 20 reports with associated metadata\nincluding anatomical region, imaging modality, clinical context, and for\nnon-English reports, English translations. All reports were assigned ICD-10\ncodes. A human vs. AI report differentiation study was conducted with 154\nparticipants (radiologists, healthcare professionals, and non-healthcare\nprofessionals) assessing whether reports were human-authored or AI-generated.\nResults: The dataset comprises 2,658 radiology reports from 76 authors across\n21 countries and 13 languages. Reports cover multiple imaging modalities (CT:\n36.1%, MRI: 22.8%, radiography: 19.0%, ultrasound: 16.8%) and anatomical\nregions, with chest (19.9%), abdomen (18.6%), head (17.3%), and pelvis (14.1%)\nbeing most prevalent. In the differentiation study, participants achieved 53.9%\naccuracy (95% CI: 50.7%-57.1%) in distinguishing between human and AI-generated\nreports, with radiologists performing significantly better (56.9%, 95% CI:\n53.3%-60.6%, p<0.05) than other groups. Conclusion: PARROT represents the\nlargest open multilingual radiology report dataset, enabling development and\nvalidation of natural language processing applications across linguistic,\ngeographic, and clinical boundaries without privacy constraints.", "AI": {"tldr": "PARROT is a large, open-access dataset of fictional radiology reports in multiple languages intended for testing NLP applications in radiology.", "motivation": "To develop a large, open-access dataset of fictional radiology reports to facilitate testing of natural language processing applications.", "method": "Radiologists contributed fictional reports adhering to standard practices, with metadata and ICD-10 codes. A differentiation study assessed human vs. AI authorship with 154 participants.", "result": "The dataset contains 2,658 reports from 76 authors in 21 countries, covering various imaging modalities and anatomical regions. Participants in the differentiation study identified human vs. AI reports with 53.9% overall accuracy.", "conclusion": "PARROT is the largest open multilingual radiology report dataset, supporting NLP application development across different languages and clinical settings.", "key_contributions": ["Creation of PARROT dataset with extensive multilingual coverage", "Inclusion of diverse imaging modalities and clinical contexts", "Establishment of a benchmark for distinguishing AI-generated reports from human-authored ones"], "limitations": "", "keywords": ["Natural Language Processing", "Radiology", "Dataset", "Open Access", "Multilingual"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2507.22940", "pdf": "https://arxiv.org/pdf/2507.22940.pdf", "abs": "https://arxiv.org/abs/2507.22940", "title": "Trustworthy Reasoning: Evaluating and Enhancing Factual Accuracy in LLM Intermediate Thought Processes", "authors": ["Rui Jiao", "Yue Zhang", "Jinku Li"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present RELIANCE (Reasoning Evaluation with Logical Integrity and Accuracy\nfor Confidence Enhancement), a novel framework addressing a critical\nvulnerability in Large Language Models (LLMs): the prevalence of factual\ninaccuracies within intermediate reasoning steps despite correct final answers.\nThis phenomenon poses substantial risks in high-stakes domains including\nhealthcare, legal analysis, and scientific research, where erroneous yet\nconfidently presented reasoning can mislead users into dangerous decisions. Our\nframework integrates three core components: (1) a specialized fact-checking\nclassifier trained on counterfactually augmented data to detect subtle factual\ninconsistencies within reasoning chains; (2) a Group Relative Policy\nOptimization (GRPO) reinforcement learning approach that balances factuality,\ncoherence, and structural correctness through multi-dimensional rewards; and\n(3) a mechanistic interpretability module examining how factuality improvements\nmanifest in model activations during reasoning processes. Extensive evaluation\nacross ten state-of-the-art models reveals concerning patterns: even leading\nmodels like Claude-3.7 and GPT-o1 demonstrate reasoning factual accuracy of\nonly 81.93% and 82.57% respectively. RELIANCE significantly enhances factual\nrobustness (up to 49.90% improvement) while maintaining or improving\nperformance on challenging benchmarks including Math-500, AIME-2024, and GPQA.\nFurthermore, our activation-level analysis provides actionable insights into\nhow factual enhancements reshape reasoning trajectories within model\narchitectures, establishing foundations for future training methodologies that\nexplicitly target factual robustness through activation-guided optimization.", "AI": {"tldr": "RELIANCE is a framework designed to enhance factual accuracy in LLM reasoning, particularly in high-stakes areas like healthcare and legal analysis, by integrating a fact-checker, a reinforcement learning approach, and a mechanistic interpretability module.", "motivation": "The paper addresses the critical issue of factual inaccuracies in LLMs' intermediate reasoning steps, which can lead to harmful consequences in vital domains such as healthcare and legal analysis.", "method": "RELIANCE integrates a fact-checking classifier trained on counterfactually augmented data, a Group Relative Policy Optimization (GRPO) approach for balancing multiple reward dimensions, and a mechanistic interpretability module for examining model activations during reasoning.", "result": "Extensive evaluations show that leading models only achieved around 82% factual accuracy, while RELIANCE improved factual robustness by up to 49.90% without compromising performance on benchmarks like Math-500 and GPQA.", "conclusion": "RELIANCE provides a significant step forward in enhancing the factual integrity of LLMs' reasoning and informs future training methods for improved factual robustness.", "key_contributions": ["Introduction of a specialized fact-checking classifier", "Implementation of Group Relative Policy Optimization for reinforcement learning", "Mechanistic interpretability module to analyze reasoning processes"], "limitations": "", "keywords": ["Large Language Models", "factual accuracy", "reinforcement learning", "healthcare", "mechanistic interpretability"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2309.12365", "pdf": "https://arxiv.org/pdf/2309.12365.pdf", "abs": "https://arxiv.org/abs/2309.12365", "title": "An Efficient Intelligent Semi-Automated Warehouse Inventory Stocktaking System", "authors": ["Chunan Tong"], "categories": ["cs.HC", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "In the context of evolving supply chain management, the significance of\nefficient inventory management has grown substantially for businesses. However,\nconventional manual and experience-based approaches often struggle to meet the\ncomplexities of modern market demands. This research introduces an intelligent\ninventory management system to address challenges related to inaccurate data,\ndelayed monitoring, and overreliance on subjective experience in forecasting.\nThe proposed system integrates bar code and distributed flutter application\ntechnologies for intelligent perception, alongside comprehensive big data\nanalytics to enable data-driven decision-making. Through meticulous analysis,\nsystem design, critical technology exploration, and simulation validation, the\neffectiveness of the proposed system is successfully demonstrated. The\nintelligent system facilitates second-level monitoring, high-frequency checks,\nand artificial intelligence-driven forecasting, consequently enhancing the\nautomation, precision, and intelligence of inventory management. This system\ncontributes to cost reduction and optimized inventory sizes through accurate\npredictions and informed decisions, ultimately achieving a mutually beneficial\nscenario. The outcomes of this research offer", "AI": {"tldr": "This paper presents an intelligent inventory management system integrating barcode and distributed application technologies with big data analytics to enhance automation and decision-making in supply chain management.", "motivation": "To address the challenges of inefficient inventory management and inaccuracy in forecasting faced by conventional methods in supply chain management.", "method": "The research employs bar code technology, a distributed flutter application, and big data analytics to create an intelligent inventory management system, validated through system design and simulation.", "result": "The proposed system enhances automation, precision, and intelligence in inventory management, enabling second-level monitoring and AI-driven forecasting, leading to cost reductions and optimized inventory sizes.", "conclusion": "The intelligent inventory management system effectively mitigates issues related to inaccurate data and delayed monitoring, driving informed decision-making in supply chains.", "key_contributions": ["Integration of barcode anddistributed flutter applications for intelligent perception.", "Utilization of big data analytics for data-driven decision-making.", "AI-driven forecasting to improve inventory management accuracy."], "limitations": "", "keywords": ["inventory management", "big data analytics", "AI forecasting"], "importance_score": 2, "read_time_minutes": 10}}
{"id": "2507.22941", "pdf": "https://arxiv.org/pdf/2507.22941.pdf", "abs": "https://arxiv.org/abs/2507.22941", "title": "SigBERT: Combining Narrative Medical Reports and Rough Path Signature Theory for Survival Risk Estimation in Oncology", "authors": ["Paul Minchella", "Loïc Verlingue", "Stéphane Chrétien", "Rémi Vaucher", "Guillaume Metzler"], "categories": ["cs.CL", "cs.CY", "cs.LG", "stat.AP"], "comment": "12 pages, 2 figures, accepted for ECML PKDD 2025", "summary": "Electronic medical reports (EHR) contain a vast amount of information that\ncan be leveraged for machine learning applications in healthcare. However,\nexisting survival analysis methods often struggle to effectively handle the\ncomplexity of textual data, particularly in its sequential form. Here, we\npropose SigBERT, an innovative temporal survival analysis framework designed to\nefficiently process a large number of clinical reports per patient. SigBERT\nprocesses timestamped medical reports by extracting and averaging word\nembeddings into sentence embeddings. To capture temporal dynamics from the time\nseries of sentence embedding coordinates, we apply signature extraction from\nrough path theory to derive geometric features for each patient, which\nsignificantly enhance survival model performance by capturing complex temporal\ndynamics. These features are then integrated into a LASSO-penalized Cox model\nto estimate patient-specific risk scores. The model was trained and evaluated\non a real-world oncology dataset from the L\\'eon B\\'erard Center corpus, with a\nC-index score of 0.75 (sd 0.014) on the independent test cohort. SigBERT\nintegrates sequential medical data to enhance risk estimation, advancing\nnarrative-based survival analysis.", "AI": {"tldr": "SigBERT is a novel temporal survival analysis framework that enhances risk estimation from sequential electronic medical reports using signature extraction and LASSO-penalized Cox models.", "motivation": "Existing survival analysis methods struggle with the complexity of textual data in electronic medical reports, necessitating a better approach to leverage this information for healthcare applications.", "method": "SigBERT processes timestamped medical reports by averaging word embeddings into sentence embeddings and extracting geometric features using signature extraction from rough path theory, which are then integrated into a LASSO-penalized Cox model.", "result": "The model achieved a C-index score of 0.75 on a real-world oncology dataset, indicating improved survival estimation capabilities.", "conclusion": "SigBERT successfully integrates sequential medical data to advance narrative-based survival analysis and improve patient-specific risk estimation.", "key_contributions": ["Introduction of SigBERT for temporal survival analysis", "Use of signature extraction for processing sequential medical text", "LASSO-penalized Cox model for risk score estimation"], "limitations": "", "keywords": ["survival analysis", "machine learning", "electronic health records", "temporal dynamics", "LASSO"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2408.09186", "pdf": "https://arxiv.org/pdf/2408.09186.pdf", "abs": "https://arxiv.org/abs/2408.09186", "title": "EEG-SCMM: Soft Contrastive Masked Modeling for Cross-Corpus EEG-Based Emotion Recognition", "authors": ["Qile Liu", "Weishan Ye", "Lingli Zhang", "Zhen Liang"], "categories": ["cs.HC", "cs.AI"], "comment": "18 pages, 10 figures, 14 tables. Accepted in ACMMM 2025", "summary": "Emotion recognition using electroencephalography (EEG) signals has attracted\nincreasing attention in recent years. However, existing methods often lack\ngeneralization in cross-corpus settings, where a model trained on one dataset\nis directly applied to another without retraining, due to differences in data\ndistribution and recording conditions. To tackle the challenge of cross-corpus\nEEG-based emotion recognition, we propose a novel framework termed Soft\nContrastive Masked Modeling (SCMM). Grounded in the theory of emotional\ncontinuity, SCMM integrates soft contrastive learning with a hybrid masking\nstrategy to effectively capture emotion dynamics (refer to short-term\ncontinuity). Specifically, in the self-supervised learning stage, we propose a\nsoft weighting mechanism that assigns similarity scores to sample pairs,\nenabling fine-grained modeling of emotional transitions and capturing the\ntemporal continuity of human emotions. To further enhance representation\nlearning, we design a similarity-aware aggregator that fuses complementary\ninformation from semantically related samples based on pairwise similarities,\nthereby improving feature expressiveness and reconstruction quality. This dual\ndesign contributes to a more discriminative and transferable representation,\nwhich is crucial for robust cross-corpus generalization. Extensive experiments\non the SEED, SEED-IV, and DEAP datasets show that SCMM achieves\nstate-of-the-art (SOTA) performance, outperforming the second-best method by an\naverage accuracy of 4.26% under both same-class and different-class\ncross-corpus settings. The source code is available at\nhttps://github.com/Kyler-RL/SCMM.", "AI": {"tldr": "This paper presents a novel framework, Soft Contrastive Masked Modeling (SCMM), for improving emotion recognition from EEG signals across different datasets without retraining.", "motivation": "Existing methods for EEG-based emotion recognition often fail to generalize across different datasets due to varying data distributions and recording conditions.", "method": "SCMM integrates soft contrastive learning with a hybrid masking strategy; it uses a soft weighting mechanism for sample pairs to capture emotional dynamics, and employs a similarity-aware aggregator to enhance representation learning.", "result": "SCMM outperforms the second-best method by an average accuracy of 4.26% in cross-corpus settings on SEED, SEED-IV, and DEAP datasets, achieving state-of-the-art results.", "conclusion": "The proposed framework contributes significantly to better cross-corpus generalization in EEG emotion recognition tasks.", "key_contributions": ["Introduction of Soft Contrastive Masked Modeling (SCMM) framework", "Combining soft contrastive learning with a hybrid masking strategy", "Achieving state-of-the-art performance in cross-corpus emotion recognition"], "limitations": "", "keywords": ["EEG", "emotion recognition", "cross-corpus", "machine learning", "self-supervised learning"], "importance_score": 5, "read_time_minutes": 15}}
{"id": "2507.22943", "pdf": "https://arxiv.org/pdf/2507.22943.pdf", "abs": "https://arxiv.org/abs/2507.22943", "title": "A chart review process aided by natural language processing and multi-wave adaptive sampling to expedite validation of code-based algorithms for large database studies", "authors": ["Shirley V Wang", "Georg Hahn", "Sushama Kattinakere Sreedhara", "Mufaddal Mahesri", "Haritha S. Pillai", "Rajendra Aldis", "Joyce Lii", "Sarah K. Dutcher", "Rhoda Eniafe", "Jamal T. Jones", "Keewan Kim", "Jiwei He", "Hana Lee", "Sengwee Toh", "Rishi J Desai", "Jie Yang"], "categories": ["cs.CL", "stat.ME"], "comment": null, "summary": "Background: One of the ways to enhance analyses conducted with large claims\ndatabases is by validating the measurement characteristics of code-based\nalgorithms used to identify health outcomes or other key study parameters of\ninterest. These metrics can be used in quantitative bias analyses to assess the\nrobustness of results for an inferential study given potential bias from\noutcome misclassification. However, extensive time and resource allocation are\ntypically re-quired to create reference-standard labels through manual chart\nreview of free-text notes from linked electronic health records. Methods: We\ndescribe an expedited process that introduces efficiency in a validation study\nus-ing two distinct mechanisms: 1) use of natural language processing (NLP) to\nreduce time spent by human reviewers to review each chart, and 2) a multi-wave\nadaptive sampling approach with pre-defined criteria to stop the validation\nstudy once performance characteristics are identified with sufficient\nprecision. We illustrate this process in a case study that validates the\nperformance of a claims-based outcome algorithm for intentional self-harm in\npatients with obesity. Results: We empirically demonstrate that the\nNLP-assisted annotation process reduced the time spent on review per chart by\n40% and use of the pre-defined stopping rule with multi-wave samples would have\nprevented review of 77% of patient charts with limited compromise to precision\nin derived measurement characteristics. Conclusion: This approach could\nfacilitate more routine validation of code-based algorithms used to define key\nstudy parameters, ultimately enhancing understanding of the reliability of\nfind-ings derived from database studies.", "AI": {"tldr": "The paper presents an efficient validation process using NLP and adaptive sampling to assess health outcome algorithms in claims databases.", "motivation": "To enhance analyses using large claims databases by validating code-based algorithms that identify health outcomes, addressing issues related to outcome misclassification.", "method": "The study employs natural language processing to expedite the review of health records and a multi-wave adaptive sampling strategy to optimize the validation process by stopping once sufficient precision is achieved.", "result": "NLP-assisted annotation reduced review time per chart by 40%, and the adaptive sampling approach potentially prevented the review of 77% of charts without significant compromise in measurement precision.", "conclusion": "The proposed methods could support routine validation of algorithms, improving the reliability of findings from database studies.", "key_contributions": ["Introduction of NLP to streamline chart reviews", "Development of a multi-wave adaptive sampling method", "Demonstration of significant time savings in chart validation"], "limitations": "The study's findings may be limited to the specific health outcomes and databases investigated.", "keywords": ["Natural Language Processing", "Health Informatics", "Validation Study"], "importance_score": 8, "read_time_minutes": 6}}
{"id": "2409.11911", "pdf": "https://arxiv.org/pdf/2409.11911.pdf", "abs": "https://arxiv.org/abs/2409.11911", "title": "AI vs. Human Paintings? Deciphering Public Interactions and Perceptions towards AI-Generated Paintings on TikTok", "authors": ["Jiajun Wang", "Xiangzhe Yuan", "Siying Hu", "Zhicong Lu"], "categories": ["cs.HC"], "comment": "Published online in International Journal of Human Computer\n  Interaction", "summary": "With the development of generative AI technology, a vast array of\nAI-generated paintings (AIGP) have gone viral on social media like TikTok.\nHowever, some negative news about AIGP has also emerged. For example, in 2022,\nnumerous painters worldwide organized a large-scale anti-AI movement because of\nthe infringement in generative AI model training. This event reflected a social\nissue that, with the development and application of generative AI, public\nfeedback and feelings towards it may have been overlooked. Therefore, to\ninvestigate public interactions and perceptions towards AIGP on social media,\nwe analyzed user engagement level and comment sentiment scores of AIGP using\nhuman painting videos as a baseline. In analyzing user engagement, we also\nconsidered the possible moderating effect of the aesthetic quality of\nPaintings. Utilizing topic modeling, we identified seven reasons, including\nhyperrealistic quality, ambivalent reactions, perceived theft of art, etc.,\nleading to negative public perceptions of AIGP. Our work may provide\ninstructive suggestions for future generative AI technology development and\navoid potential crises in human-AI collaboration.", "AI": {"tldr": "The paper investigates public perceptions of AI-generated paintings (AIGP) on social media, highlighting user engagement and sentiment analysis.", "motivation": "To understand public interactions and concerns regarding AI-generated paintings amidst a backdrop of recent anti-AI movements by artists, reflecting potential issues in human-AI collaboration.", "method": "Analyzed user engagement level and comment sentiment scores for AI-generated paintings and compared these to human painting videos; utilized topic modeling to identify reasons for negative perceptions.", "result": "Identified seven key reasons for negative public perceptions, including hyperrealistic quality and perceived theft of art, contributing to a nuanced understanding of user sentiments.", "conclusion": "The findings may help guide future generative AI technology development and address societal concerns, ensuring better human-AI collaboration.", "key_contributions": ["Analysis of user engagement and sentiment regarding AIGP", "Identification of key reasons for negative perceptions of AIGP", "Suggestions for improving future generative AI technologies"], "limitations": "Focused solely on social media interactions, may not represent broader public opinions.", "keywords": ["AI-generated paintings", "public perception", "user engagement", "sentiment analysis", "generative AI"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.22944", "pdf": "https://arxiv.org/pdf/2507.22944.pdf", "abs": "https://arxiv.org/abs/2507.22944", "title": "Opacity as Authority: Arbitrariness and the Preclusion of Contestation", "authors": ["Naomi Omeonga wa Kayembe"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "This article redefines arbitrariness not as a normative flaw or a symptom of\ndomination, but as a foundational functional mechanism structuring human\nsystems and interactions. Diverging from critical traditions that conflate\narbitrariness with injustice, it posits arbitrariness as a semiotic trait: a\nproperty enabling systems - linguistic, legal, or social - to operate\neffectively while withholding their internal rationale. Building on Ferdinand\nde Saussure's concept of l'arbitraire du signe, the analysis extends this\nprinciple beyond language to demonstrate its cross-domain applicability,\nparticularly in law and social dynamics. The paper introduces the \"Motivation\n-> Constatability -> Contestability\" chain, arguing that motivation functions\nas a crucial interface rendering an act's logic vulnerable to intersubjective\ncontestation. When this chain is broken through mechanisms like\n\"immotivization\" or \"Conflict Lateralization\" (exemplified by \"the blur of the\nwolf drowned in the fish\"), acts produce binding effects without exposing their\nrationale, thus precluding justiciability. This structural opacity, while\nappearing illogical, is a deliberate design protecting authority from\naccountability. Drawing on Shannon's entropy model, the paper formalizes\narbitrariness as A = H(L|M) (conditional entropy). It thereby proposes a modern\ntheory of arbitrariness as a neutral operator central to control as well as\ncare, an overlooked dimension of interpersonal relations. While primarily\ndeveloped through human social systems, this framework also illuminates a new\npathway for analyzing explainability in advanced artificial intelligence\nsystems.", "AI": {"tldr": "The paper redefines arbitrariness as a functional mechanism structuring human systems, challenging traditional views that link it to injustice, and extends its implications to AI explainability.", "motivation": "To redefine arbitrariness beyond normative flaws and explore its functional role in human interactions and systems.", "method": "The paper builds upon Ferdinand de Saussure's concept of l'arbitraire du signe and introduces a chain of 'Motivation -> Constatability -> Contestability' to analyze structural opacity in systems.", "result": "It formalizes arbitrariness using Shannon's entropy model, arguing that it serves both control and care in human systems and has implications for AI explainability.", "conclusion": "The study proposes a contemporary theory of arbitrariness that highlights its role in protecting authority and lacks rational exposure, suggesting this can also inform AI explainability concepts.", "key_contributions": ["Reconceptualization of arbitrariness as a functional mechanism rather than a flaw.", "Introduction of the 'Motivation -> Constatability -> Contestability' chain.", "Formalization of arbitrariness through Shannon's entropy model."], "limitations": "", "keywords": ["arbitrariness", "semiotics", "human systems", "explainability", "artificial intelligence"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2410.07486", "pdf": "https://arxiv.org/pdf/2410.07486.pdf", "abs": "https://arxiv.org/abs/2410.07486", "title": "Visual Story-Writing: Writing by Manipulating Visual Representations of Stories", "authors": ["Damien Masson", "Zixin Zhao", "Fanny Chevalier"], "categories": ["cs.HC"], "comment": "In Proceedings of the 38th Annual ACM Symposium on User Interface\n  Software and Technology (UIST '25)", "summary": "We define \"visual story-writing\" as using visual representations of story\nelements to support writing and revising narrative texts. To demonstrate this\napproach, we developed a text editor that automatically visualizes a graph of\nentity interactions, movement between locations, and a timeline of story\nevents. Interacting with these visualizations results in suggested text edits:\nfor example, connecting two characters in the graph creates an interaction\nbetween them, moving an entity updates their described location, and\nrearranging events on the timeline reorganizes the narrative sequence. Through\ntwo user studies on narrative text editing and writing, we found that visuals\nsupported participants in planning high-level revisions, tracking story\nelements, and exploring story variations in ways that encourage creativity.\nBroadly, our work lays the foundation for writing support, not just through\nwords, but also visuals.", "AI": {"tldr": "This paper presents a text editor that utilizes visual representations to aid in writing and revising narrative texts, supporting creativity through interactive visualizations of story elements.", "motivation": "To enhance narrative writing and revision by integrating visual tools that support the writing process and encourage creativity.", "method": "Developed a text editor that visualizes entity interactions, movements, and timelines in narrative texts, suggesting edits based on user interactions with these visualizations.", "result": "User studies showed that visualizations helped participants in planning revisions, tracking story elements, and exploring narrative variations, ultimately fostering creativity.", "conclusion": "Visual story-writing tools can significantly aid in the narrative writing process by providing interactive visual aids that enhance communication and creativity.", "key_contributions": ["Development of a novel text editor integrating visual story elements.", "User studies demonstrating the benefits of visualization in narrative writing.", "Insights into how visuals can aid in narrative planning and revision."], "limitations": "", "keywords": ["Visual Story-Writing", "Narrative Text Editing", "User Interface Design"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.22968", "pdf": "https://arxiv.org/pdf/2507.22968.pdf", "abs": "https://arxiv.org/abs/2507.22968", "title": "C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring Challenges in Complex Conversations", "authors": ["Chengqian Ma", "Wei Tao", "Yiwen Guo"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Spoken Dialogue Models (SDMs) have recently attracted significant attention\nfor their ability to generate voice responses directly to users' spoken\nqueries. Despite their increasing popularity, there exists a gap in research\nfocused on comprehensively understanding their practical effectiveness in\ncomprehending and emulating human conversations. This is especially true\ncompared to text-based Large Language Models (LLMs), which benefit from\nextensive benchmarking. Human voice interactions are inherently more complex\nthan text due to characteristics unique to spoken dialogue. Ambiguity poses one\nchallenge, stemming from semantic factors like polysemy, as well as\nphonological aspects such as heterograph, heteronyms, and stress patterns.\nAdditionally, context-dependency, like omission, coreference, and multi-turn\ninteraction, adds further complexity to human conversational dynamics. To\nilluminate the current state of SDM development and to address these\nchallenges, we present a benchmark dataset in this paper, which comprises 1,079\ninstances in English and Chinese. Accompanied by an LLM-based evaluation method\nthat closely aligns with human judgment, this dataset facilitates a\ncomprehensive exploration of the performance of SDMs in tackling these\npractical challenges.", "AI": {"tldr": "This paper presents a benchmark dataset for evaluating Spoken Dialogue Models (SDMs) against challenges in human conversation, including ambiguity and context-dependency.", "motivation": "To address the gap in research regarding the effectiveness of Spoken Dialogue Models in understanding and emulating human conversations compared to text-based models.", "method": "The authors developed a benchmark dataset with 1,079 instances in English and Chinese, accompanied by an LLM-based evaluation method that aligns with human judgment.", "result": "The dataset allows for a comprehensive assessment of SDM capabilities in managing the complexities of human dialogues.", "conclusion": "The findings highlight the practical challenges faced by SDMs and provide a foundational resource for evaluating their performance in voice interaction.", "key_contributions": ["Creation of a benchmark dataset for SDMs", "LLM-based evaluation method aligned with human judgment", "Focus on complexities of spoken dialogue versus text-based models"], "limitations": "The dataset may not encompass all possible conversational scenarios or languages beyond English and Chinese.", "keywords": ["Spoken Dialogue Models", "benchmark dataset", "human conversation", "LLM evaluation", "complexity in dialogue"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.23063", "pdf": "https://arxiv.org/pdf/2507.23063.pdf", "abs": "https://arxiv.org/abs/2507.23063", "title": "Math Natural Language Inference: this should be easy!", "authors": ["Valeria de Paiva", "Qiyue Gao", "Hai Hu", "Pavel Kovalev", "Yikang Liu", "Lawrence S. Moss", "Zhiheng Qian"], "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "9 pages plus appendices", "summary": "We ask whether contemporary LLMs are able to perform natural language\ninference (NLI) tasks on mathematical texts. We call this the Math NLI problem.\nWe construct a corpus of Math NLI pairs whose premises are from extant\nmathematical text and whose hypotheses and gold labels were provided by people\nwith experience in both research-level mathematics and also in the NLI field.\nWe also investigate the quality of corpora using the same premises but whose\nhypotheses are provided by LLMs themselves. We not only investigate the\nperformance but also the inter-group consistency of the diverse group of LLMs.\nWe have both positive and negative findings. Among our positive findings: in\nsome settings, using a majority vote of LLMs is approximately equivalent to\nusing human-labeled data in the Math NLI area. On the negative side: LLMs still\nstruggle with mathematical language. They occasionally fail at even basic\ninferences. Current models are not as prone to hypothesis-only \"inference\" in\nour data the way the previous generation had been. In addition to our findings,\nwe also provide our corpora as data to support future work on Math NLI.", "AI": {"tldr": "This paper investigates the ability of contemporary LLMs to perform natural language inference (NLI) on mathematical texts, revealing both strengths and weaknesses in their performance.", "motivation": "To explore if LLMs can effectively handle NLI tasks within the context of mathematical texts, as this area has not been extensively examined yet.", "method": "A corpus of Math NLI pairs was constructed, with premises taken from existing mathematical literature and hypotheses labeled by experts. The study also compares human-labeled hypotheses with those generated by LLMs, examining the performance and consistency of various LLMs.", "result": "Findings indicate that in certain scenarios, a majority vote among LLMs yields results comparable to human labeling, although LLMs still struggle with mathematical language and basic inferences.", "conclusion": "The study highlights both the potential and limitations of LLMs in Math NLI tasks, and provides a valuable resource for future research in this area through the released corpus.", "key_contributions": ["Construction of a unique Math NLI corpus from mathematical texts", "Comparison of LLM-generated hypotheses and human-labeled data", "Insights into LLM performance on complex inference tasks"], "limitations": "LLMs exhibit weaknesses in understanding mathematical language and performing basic inferences; results may vary across models.", "keywords": ["natural language inference", "large language models", "mathematics", "corpora", "inference"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.23082", "pdf": "https://arxiv.org/pdf/2507.23082.pdf", "abs": "https://arxiv.org/abs/2507.23082", "title": "Exploring In-Context Learning for Frame-Semantic Parsing", "authors": ["Diego Garat", "Guillermo Moncecchi", "Dina Wonsever"], "categories": ["cs.CL"], "comment": null, "summary": "Frame Semantic Parsing (FSP) entails identifying predicates and labeling\ntheir arguments according to Frame Semantics. This paper investigates the use\nof In-Context Learning (ICL) with Large Language Models (LLMs) to perform FSP\nwithout model fine-tuning. We propose a method that automatically generates\ntask-specific prompts for the Frame Identification (FI) and Frame Semantic Role\nLabeling (FSRL) subtasks, relying solely on the FrameNet database. These\nprompts, constructed from frame definitions and annotated examples, are used to\nguide six different LLMs. Experiments are conducted on a subset of frames\nrelated to violent events. The method achieves competitive results, with F1\nscores of 94.3% for FI and 77.4% for FSRL. The findings suggest that ICL offers\na practical and effective alternative to traditional fine-tuning for\ndomain-specific FSP tasks.", "AI": {"tldr": "This paper explores the use of in-context learning with large language models for frame semantic parsing without needing model fine-tuning.", "motivation": "To investigate the potential of ICL in enhancing frame semantic parsing tasks and to provide an efficient alternative to traditional model fine-tuning.", "method": "The study proposes a method that generates task-specific prompts from the FrameNet database for Frame Identification and Frame Semantic Role Labeling, utilized by six different LLMs.", "result": "The proposed method achieved F1 scores of 94.3% for Frame Identification and 77.4% for Frame Semantic Role Labeling, demonstrating competitive performance.", "conclusion": "In-context learning can effectively serve as a practical alternative to traditional fine-tuning methods for domain-specific tasks in frame semantic parsing.", "key_contributions": ["Introduction of task-specific prompts from FrameNet for LLMs.", "Demonstration of competitive performance without the need for fine-tuning.", "Exploration of ICL applicability in frame semantic parsing."], "limitations": "The experiments were conducted on a limited subset of frames related to violent events, which may not generalize across all domains.", "keywords": ["Frame Semantic Parsing", "In-Context Learning", "Large Language Models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.23083", "pdf": "https://arxiv.org/pdf/2507.23083.pdf", "abs": "https://arxiv.org/abs/2507.23083", "title": "Context-aware Rotary Position Embedding", "authors": ["Ali Veisi", "Delaram Fartoot", "Hamidreza Amirzadeh"], "categories": ["cs.CL"], "comment": "4 pages, 1 table", "summary": "Positional encoding is a vital component of Transformer architectures,\nenabling models to incorporate sequence order into self-attention mechanisms.\nRotary Positional Embeddings (RoPE) have become a widely adopted solution due\nto their compatibility with relative position encoding and computational\nefficiency. However, RoPE relies on static, input-independent sinusoidal\nfrequency patterns, limiting its ability to model context-sensitive\nrelationships. In this work, we propose CARoPE (Context-Aware Rotary Positional\nEmbedding), a novel generalization of RoPE that dynamically generates\nhead-specific frequency patterns conditioned on token embeddings. This design\nintroduces token- and context-sensitive positional representations while\npreserving RoPE efficiency and architectural simplicity. CARoPE computes\ninput-dependent phase shifts using a bounded transformation of token embeddings\nand integrates them into the rotary mechanism across attention heads. We\nevaluate CARoPE on the FineWeb-Edu-10B dataset using GPT-2 variants trained on\nnext-token prediction tasks. Experimental results show that CARoPE consistently\noutperforms RoPE and other common positional encoding baselines, achieving\nsignificantly lower perplexity, even at longer context lengths. Additionally,\nCARoPE enables faster training throughput without sacrificing model stability.\nThese findings demonstrate that CARoPE offers a scalable, expressive, and\nefficient upgrade to existing positional encoding strategies in Transformer\nmodels.", "AI": {"tldr": "CARoPE is a context-aware enhancement of Rotary Positional Embeddings that introduces dynamic token-specific frequency patterns to improve Transformer models' performance in sequential tasks.", "motivation": "Existing Rotary Positional Embeddings (RoPE) rely on static sinusoidal frequencies, limiting their context sensitivity.", "method": "CARoPE generates head-specific frequency patterns based on token embeddings, allowing for dynamic input-dependent phase adjustments in the rotary mechanism.", "result": "CARoPE outperforms RoPE and other baselines in terms of lowering perplexity on the FineWeb-Edu-10B dataset, especially for longer context lengths.", "conclusion": "CARoPE represents a scalable and efficient improvement over traditional positional encodings in Transformer architectures.", "key_contributions": ["Introduction of context-aware, token-specific positional encoding", "Preservation of computational efficiency and architectural simplicity", "Demonstration of improved performance metrics in next-token prediction tasks"], "limitations": "", "keywords": ["Rotary Positional Embeddings", "Context-Aware", "Transformer Models"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.23095", "pdf": "https://arxiv.org/pdf/2507.23095.pdf", "abs": "https://arxiv.org/abs/2507.23095", "title": "SMART-Editor: A Multi-Agent Framework for Human-Like Design Editing with Structural Integrity", "authors": ["Ishani Mondal", "Meera Bharadwaj", "Ayush Roy", "Aparna Garimella", "Jordan Lee Boyd-Graber"], "categories": ["cs.CL", "cs.AI"], "comment": "Under Submission", "summary": "We present SMART-Editor, a framework for compositional layout and content\nediting across structured (posters, websites) and unstructured (natural images)\ndomains. Unlike prior models that perform local edits, SMART-Editor preserves\nglobal coherence through two strategies: Reward-Refine, an inference-time\nrewardguided refinement method, and RewardDPO, a training-time preference\noptimization approach using reward-aligned layout pairs. To evaluate model\nperformance, we introduce SMARTEdit-Bench, a benchmark covering multi-domain,\ncascading edit scenarios. SMART-Editor outperforms strong baselines like\nInstructPix2Pix and HIVE, with RewardDPO achieving up to 15% gains in\nstructured settings and Reward-Refine showing advantages on natural images.\nAutomatic and human evaluations confirm the value of reward-guided planning in\nproducing semantically consistent and visually aligned edits.", "AI": {"tldr": "SMART-Editor is a framework for editing layouts and content in both structured and unstructured domains while maintaining global coherence through reward-guided methods.", "motivation": "Current editing models often fail to preserve global coherence during local edits. SMART-Editor aims to overcome this limitation.", "method": "The framework employs two strategies: Reward-Refine for inference-time refinement and RewardDPO for training-time preference optimization using reward-aligned pairs.", "result": "SMART-Editor shows superior performance in structured settings with up to 15% gains over models like InstructPix2Pix and HIVE, achieving visually aligned edits confirmed by evaluations.", "conclusion": "The study demonstrates that reward-guided planning is effective in ensuring semantic consistency and visual alignment in editing tasks across different domains.", "key_contributions": ["Introduction of SMART-Editor framework for diverse layout and content editing", "Development of SMARTEdit-Bench for multi-domain evaluation", "Demonstration of improved performance via reward-guided methods"], "limitations": "", "keywords": ["SMART-Editor", "content editing", "reward-guided refinement", "layout optimization", "benchmarking"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2507.23104", "pdf": "https://arxiv.org/pdf/2507.23104.pdf", "abs": "https://arxiv.org/abs/2507.23104", "title": "RASL: Retrieval Augmented Schema Linking for Massive Database Text-to-SQL", "authors": ["Jeffrey Eben", "Aitzaz Ahmad", "Stephen Lau"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Despite advances in large language model (LLM)-based natural language\ninterfaces for databases, scaling to enterprise-level data catalogs remains an\nunder-explored challenge. Prior works addressing this challenge rely on\ndomain-specific fine-tuning - complicating deployment - and fail to leverage\nimportant semantic context contained within database metadata. To address these\nlimitations, we introduce a component-based retrieval architecture that\ndecomposes database schemas and metadata into discrete semantic units, each\nseparately indexed for targeted retrieval. Our approach prioritizes effective\ntable identification while leveraging column-level information, ensuring the\ntotal number of retrieved tables remains within a manageable context budget.\nExperiments demonstrate that our method maintains high recall and accuracy,\nwith our system outperforming baselines over massive databases with varying\nstructure and available metadata. Our solution enables practical text-to-SQL\nsystems deployable across diverse enterprise settings without specialized\nfine-tuning, addressing a critical scalability gap in natural language database\ninterfaces.", "AI": {"tldr": "The paper presents a component-based retrieval architecture for LLM-based natural language interfaces aimed at scaling enterprise-level data catalogs.", "motivation": "To address the scalability challenges of LLM-based natural language interfaces for databases which are not effectively handled by domain-specific fine-tuning and do not leverage semantic context from database metadata.", "method": "The authors introduce a retrieval architecture that decomposes database schemas and metadata into semantic units for targeted retrieval, prioritizing table identification and using column-level information.", "result": "Experiments show that the method maintains high recall and accuracy, outperforming existing baselines over large and varied databases.", "conclusion": "The proposed solution facilitates practical deployment of text-to-SQL systems across diverse enterprise environments without the need for specialized fine-tuning, effectively bridging a critical gap in database interactions.", "key_contributions": ["Component-based retrieval architecture for natural language interfaces", "Utilization of semantic units from database metadata", "High scalability and accuracy without domain-specific fine-tuning"], "limitations": "", "keywords": ["large language models", "natural language interfaces", "database metadata"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.23121", "pdf": "https://arxiv.org/pdf/2507.23121.pdf", "abs": "https://arxiv.org/abs/2507.23121", "title": "Uncovering the Fragility of Trustworthy LLMs through Chinese Textual Ambiguity", "authors": ["Xinwei Wu", "Haojie Li", "Hongyu Liu", "Xinyu Ji", "Ruohan Li", "Yule Chen", "Yigeng Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at KDD workshop on Evaluation and Trustworthiness of Agentic\n  and Generative AI Models (Agentic & GenAI Evaluation Workshop KDD '25)", "summary": "In this work, we study a critical research problem regarding the\ntrustworthiness of large language models (LLMs): how LLMs behave when\nencountering ambiguous narrative text, with a particular focus on Chinese\ntextual ambiguity. We created a benchmark dataset by collecting and generating\nambiguous sentences with context and their corresponding disambiguated pairs,\nrepresenting multiple possible interpretations. These annotated examples are\nsystematically categorized into 3 main categories and 9 subcategories. Through\nexperiments, we discovered significant fragility in LLMs when handling\nambiguity, revealing behavior that differs substantially from humans.\nSpecifically, LLMs cannot reliably distinguish ambiguous text from unambiguous\ntext, show overconfidence in interpreting ambiguous text as having a single\nmeaning rather than multiple meanings, and exhibit overthinking when attempting\nto understand the various possible meanings. Our findings highlight a\nfundamental limitation in current LLMs that has significant implications for\ntheir deployment in real-world applications where linguistic ambiguity is\ncommon, calling for improved approaches to handle uncertainty in language\nunderstanding. The dataset and code are publicly available at this GitHub\nrepository: https://github.com/ictup/LLM-Chinese-Textual-Disambiguation.", "AI": {"tldr": "This study explores how large language models (LLMs) handle ambiguous narrative text, particularly in Chinese, revealing significant limitations in distinguishing between ambiguous and unambiguous text.", "motivation": "To investigate the trustworthiness of LLMs in dealing with linguistic ambiguities, especially in the context of Chinese language, and to assess their reliability in real-world applications where ambiguity is prevalent.", "method": "A benchmark dataset was created by collecting and generating ambiguous sentences and their corresponding disambiguated pairs, categorized into three main categories and nine subcategories. Experiments were conducted to evaluate LLM performance on this dataset.", "result": "LLMs demonstrated substantial fragility when interpreting ambiguity, showing overconfidence in providing single interpretations for ambiguous text and difficulty in distinguishing it from unambiguous text.", "conclusion": "The findings indicate a significant limitation in current LLMs with implications for their practical use, emphasizing the need for improved methods to handle linguistic ambiguity.", "key_contributions": ["Creation of a benchmark dataset for Chinese textual ambiguity", "Systematic categorization of ambiguous sentences", "Experimental analysis revealing LLM limitations in handling ambiguity"], "limitations": "The study primarily focused on Chinese text, which may limit the applicability of the findings to LLMs in other languages.", "keywords": ["large language models", "textual ambiguity", "natural language processing", "AI trustworthiness", "Chinese disambiguation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.23135", "pdf": "https://arxiv.org/pdf/2507.23135.pdf", "abs": "https://arxiv.org/abs/2507.23135", "title": "ISO-Bench: Benchmarking Multimodal Causal Reasoning in Visual-Language Models through Procedural Plans", "authors": ["Ananya Sadana", "Yash Kumar Lal", "Jiawei Zhou"], "categories": ["cs.CL"], "comment": null, "summary": "Understanding causal relationships across modalities is a core challenge for\nmultimodal models operating in real-world environments. We introduce ISO-Bench,\na benchmark for evaluating whether models can infer causal dependencies between\nvisual observations and procedural text. Each example presents an image of a\ntask step and a text snippet from a plan, with the goal of deciding whether the\nvisual step occurs before or after the referenced text step. Evaluation results\non ten frontier vision-language models show underwhelming performance: the best\nzero-shot F1 is only 0.57, and chain-of-thought reasoning yields only modest\ngains (up to 0.62 F1), largely behind humans (0.98 F1). Our analysis further\nhighlights concrete directions for improving causal understanding in multimodal\nmodels.", "AI": {"tldr": "ISO-Bench is introduced as a benchmark to evaluate how multimodal models infer causal relations between visual tasks and procedural text, showing poor performance compared to humans.", "motivation": "To address the challenge of understanding causal relationships in multimodal models operating in real-world environments.", "method": "A benchmark called ISO-Bench is established which tests models on determining the causal order between images of task steps and corresponding procedural text snippets.", "result": "Evaluation of ten state-of-the-art vision-language models shows mediocre performance, with the best zero-shot F1 score at 0.57 and a slight improvement to 0.62 with chain-of-thought reasoning, significantly lower than human performance at 0.98 F1.", "conclusion": "The findings call for improved approaches to enhance causal understanding in multimodal frameworks.", "key_contributions": ["Introduction of ISO-Bench for causal inference evaluation in multimodal models", "Benchmark results highlighting the limitations of current vision-language models", "Identification of potential directions for improving causal reasoning capabilities."], "limitations": "The current models exhibit significant performance gaps compared to human reasoning, indicating a need for methodologies that better capture causal relationships.", "keywords": ["causal inference", "multimodal models", "benchmark", "vision-language", "machine learning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.23158", "pdf": "https://arxiv.org/pdf/2507.23158.pdf", "abs": "https://arxiv.org/abs/2507.23158", "title": "User Feedback in Human-LLM Dialogues: A Lens to Understand Users But Noisy as a Learning Signal", "authors": ["Yuhan Liu", "Michael J. Q. Zhang", "Eunsol Choi"], "categories": ["cs.CL"], "comment": "Earlier version of this paper was presented at 2nd Workshop on Models\n  of Human Feedback for AI Alignment (MoFA), ICML 2025", "summary": "Once language models (LMs) are deployed, they can interact with users\nlong-term, ideally evolving continuously based on their feedback. Asking for\ndirect user feedback can be disruptive; thus, we study harvesting user feedback\nfrom user-LM interaction logs. We study implicit user feedback in two user-LM\ninteraction datasets (WildChat and LMSYS). First, we analyze user feedback in\nthe user-LLM conversation trajectory, providing insights into when and why such\nfeedback occurs. Second, we study harvesting learning signals from such\nimplicit user feedback. We find that the contents of user feedback (e.g., user\nwanted clarification), not just the polarity (e.g., users were unhappy with the\nprevious model response), can improve model performance in short human-designed\nquestions (MTBench) but not on longer and more complex questions (WildBench).\nWe also find that the usefulness of user feedback is largely tied to the\nquality of the user's initial prompt. Together, we provide an in-depth study of\nimplicit user feedback, showing its potential and limitations.", "AI": {"tldr": "This study focuses on harvesting implicit user feedback from interactions with language models (LMs) to improve model performance.", "motivation": "To improve continuity and evolution of LMs based on long-term user feedback without being disruptive through direct requests for feedback.", "method": "Analyzed implicit user feedback from two interaction datasets, examining the trajectory of user-LLM conversations and the learning signals from feedback.", "result": "User feedback content improvements can enhance model performance, especially for short questions, but is limited by the complexity of questions and quality of initial prompts.", "conclusion": "Implicit user feedback has its potential and limitations, particularly influenced by the quality of user prompts.", "key_contributions": ["In-depth analysis of implicit user feedback in user-LLM interactions", "Insights into the timing and reasoning behind user feedback", "Empirical findings on feedback content's impact on model performance"], "limitations": "User feedback effectiveness is limited on longer, complex questions due to the quality of initial prompts.", "keywords": ["implicit user feedback", "language models", "user interaction", "model performance", "HCI"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.23167", "pdf": "https://arxiv.org/pdf/2507.23167.pdf", "abs": "https://arxiv.org/abs/2507.23167", "title": "LENS: Learning Ensemble Confidence from Neural States for Multi-LLM Answer Integration", "authors": ["Jizhou Guo"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\nvarious tasks, with different models excelling in distinct domains and specific\nabilities. Effectively combining the predictions of multiple LLMs is crucial\nfor enhancing system robustness and performance. However, existing ensemble\nmethods often rely on simple techniques like voting or logits ensembling, which\noverlook the varying confidence and reliability of models in different\ncontexts. In this work, we propose LENS (Learning ENsemble confidence from\nNeural States), a novel approach that learns to estimate model confidence by\nanalyzing internal representations. For each LLM, we train a lightweight linear\nconfidence predictor that leverages layer-wise hidden states and normalized\nprobabilities as inputs. This allows for more nuanced weighting of model\npredictions based on their context-dependent reliability. Our method does not\nrequire modifying the model parameters and requires negligible additional\ncomputation. Experimental results on multiple-choice and boolean\nquestion-answering tasks demonstrate that LENS outperforms traditional ensemble\nmethods by a substantial margin. Our findings suggest that internal\nrepresentations provide valuable signals for determining model confidence and\ncan be effectively leveraged for ensemble learning.", "AI": {"tldr": "LENS is a novel method for enhancing LLM ensemble performance by learning model confidence from internal representations.", "motivation": "Current ensemble methods for LLMs fail to account for the varying confidence and reliability of models in different contexts, leading to suboptimal performance.", "method": "LENS trains a lightweight linear confidence predictor for each LLM using layer-wise hidden states and normalized probabilities as inputs, allowing for context-dependent weighting of predictions.", "result": "LENS significantly outperforms traditional ensemble methods on multiple-choice and boolean question-answering tasks, demonstrating the value of internal model representations in confidence estimation.", "conclusion": "Internal representations can effectively inform model confidence in ensemble learning, providing robust improvements over traditional techniques.", "key_contributions": ["Introduction of LENS for ensemble learning of LLMs", "Context-dependent confidence estimation based on internal model states", "Demonstrated improvements in performance over traditional methods"], "limitations": "", "keywords": ["Large Language Models", "Ensemble Learning", "Model Confidence", "Internal Representations", "Machine Learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.23194", "pdf": "https://arxiv.org/pdf/2507.23194.pdf", "abs": "https://arxiv.org/abs/2507.23194", "title": "Geak: Introducing Triton Kernel AI Agent & Evaluation Benchmarks", "authors": ["Jianghui Wang", "Vinay Joshi", "Saptarshi Majumder", "Xu Chao", "Bin Ding", "Ziqiong Liu", "Pratik Prabhanjan Brahma", "Dong Li", "Zicheng Liu", "Emad Barsoum"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The demand for AI-generated GPU kernels is rapidly growing, influenced by the\nneed for scalable, hardware-optimized solutions in both industry and academia.\nAs deep learning workloads grow in complexity and diversity, it is imperative\nto automate low-level kernel development to meet performance and productivity\ndemands. Major cloud providers, semiconductor companies, and research\ninstitutions are now investing heavily in AI-driven code generation for GPUs,\naiming to reduce manual optimization efforts while achieving near-expert\nperformance on hardware like AMD MI300X. The Triton language, a Python-based\nDSL for GPU programming, has emerged as a popular target for such AI-generated\nkernels due to its balance of performance and ease-of-coding. In this work, we\npresent an evaluation suite for Triton-based GPU kernels and GEAK (Generating\nEfficient AI-centric GPU Kernels)-a framework that leverages cutting-edge LLMs\nto generate performant Triton code specifically for AMD GPUs, including the AMD\nMI300X and MI250. GEAK leverages inference-time compute scaling to produce\nTriton-based GPU kernels using a reasoning loop adapted from Reflexion-style\nfeedback mechanisms. On two evaluation benchmarks, GEAK significantly\noutperformed the baselines of directly prompting frontier LLMs as well as\nReflexion-based generation pipelines by achieving correctness up to $63$% and\nexecution speed up of up to $2.59$X. These results highlight the promise of\nGEAK-like agentic code generation for accelerating the adoption of diverse\nhardware platforms and democratizing access to expert-level kernel performance.", "AI": {"tldr": "This paper presents GEAK, a framework for generating efficient GPU kernels using LLMs, specifically targeting AMD GPUs with the Triton programming language, outperforming traditional methods in both correctness and execution speed.", "motivation": "The growing complexity of deep learning workloads necessitates the automation of GPU kernel development to improve performance and productivity in hardware-optimized solutions.", "method": "An evaluation suite for Triton-based GPU kernels is introduced along with GEAK, which uses LLMs and a reasoning loop inspired by Reflexion-style mechanisms to produce optimized Triton code for AMD GPUs.", "result": "GEAK achieved correctness rates up to 63% and execution speed improvements of up to 2.59X over conventional prompting and generation pipelines.", "conclusion": "GEAK demonstrates the potential of LLM-driven code generation to enhance access and optimize performance across diverse hardware platforms, particularly in GPU programming.", "key_contributions": ["Introduction of GEAK for generating efficient Triton GPU kernels", "Evaluation suite for assessing Triton kernel performance", "Demonstrated significant improvements in correctness and execution speed over existing methods."], "limitations": "", "keywords": ["GPU kernels", "AI code generation", "Triton programming language", "LLMs", "AMD GPUs"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.23211", "pdf": "https://arxiv.org/pdf/2507.23211.pdf", "abs": "https://arxiv.org/abs/2507.23211", "title": "Failures Are the Stepping Stones to Success: Enhancing Few-Shot In-Context Learning by Leveraging Negative Samples", "authors": ["Yunhao Liang", "Ruixuan Ying", "Takuya Taniguchi", "Zhe Cui"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models exhibit powerful few-shot in-context learning (ICL)\ncapabilities, but the performance is highly sensitive to provided examples.\n  Recent research has focused on retrieving corresponding examples for each\ninput query, not only enhancing the efficiency and scalability of the learning\nprocess but also mitigating inherent biases in manual example selection.\n  However, these studies have primarily emphasized leveraging Positive samples\nwhile overlooking the additional information within Negative samples for\ncontextual learning.\n  We propose a novel method that utilizes Negative samples to better select\nPositive sample examples, thereby enhancing the performance of few-shot ICL.\nInitially, we construct Positive and Negative sample corpora based on\nZero-Shot-Cot. Then, during inference, we employ a semantic similarity-based\napproach to select the most similar examples from both the Positive and\nNegative corpora for a given query. Subsequently, we further retrieve Positive\nexamples from the Positive sample corpus based on semantic similarity to the\nNegative examples, then concatenating them with the previously selected\nPositive examples to serve as ICL demonstrations. Experimental results\ndemonstrate that our approach surpasses methods solely relying on the most\nsimilar positive examples for context, validating that the additional\ninformation in negative samples aids in enhancing ICL performance through\nimproved Positive sample selection.", "AI": {"tldr": "This paper presents a method that enhances few-shot in-context learning (ICL) in large language models by utilizing negative samples to improve positive sample selection during inference.", "motivation": "The study addresses the limitations of existing methods which mostly focus on positive samples, neglecting the potential benefits of negative samples in few-shot ICL processes.", "method": "The proposed method constructs positive and negative sample corpora based on Zero-Shot-Cot. It uses a semantic similarity approach during inference to select related examples from both corpora, and further refines positive examples based on the semantic proximity to negative examples.", "result": "Experimental results show that the proposed method outperforms existing techniques that rely only on the most similar positive samples, indicating that negative samples contribute positively to ICL performance through better positive sample selection.", "conclusion": "Incorporating negative samples in the example selection process improves the efficacy of few-shot ICL in large language models, leading to better performance in semantic task applications.", "key_contributions": ["Introduction of negative samples in ICL for enhanced positive sample selection", "Development of a semantic similarity-based selection method", "Demonstrated improvement in performance over existing positive-only approaches"], "limitations": "", "keywords": ["Few-shot learning", "In-context learning", "Negative samples", "Large language models", "Semantic similarity"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.23220", "pdf": "https://arxiv.org/pdf/2507.23220.pdf", "abs": "https://arxiv.org/abs/2507.23220", "title": "Model Directions, Not Words: Mechanistic Topic Models Using Sparse Autoencoders", "authors": ["Carolina Zheng", "Nicolas Beltran-Velez", "Sweta Karlekar", "Claudia Shi", "Achille Nazaret", "Asif Mallik", "Amir Feder", "David M. Blei"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Traditional topic models are effective at uncovering latent themes in large\ntext collections. However, due to their reliance on bag-of-words\nrepresentations, they struggle to capture semantically abstract features. While\nsome neural variants use richer representations, they are similarly constrained\nby expressing topics as word lists, which limits their ability to articulate\ncomplex topics. We introduce Mechanistic Topic Models (MTMs), a class of topic\nmodels that operate on interpretable features learned by sparse autoencoders\n(SAEs). By defining topics over this semantically rich space, MTMs can reveal\ndeeper conceptual themes with expressive feature descriptions. Moreover,\nuniquely among topic models, MTMs enable controllable text generation using\ntopic-based steering vectors. To properly evaluate MTM topics against\nword-list-based approaches, we propose \\textit{topic judge}, an LLM-based\npairwise comparison evaluation framework. Across five datasets, MTMs match or\nexceed traditional and neural baselines on coherence metrics, are consistently\npreferred by topic judge, and enable effective steering of LLM outputs.", "AI": {"tldr": "Mechanistic Topic Models (MTMs) enhance traditional topic modeling by using interpretable features from sparse autoencoders, allowing for richer semantic understanding and controllable text generation.", "motivation": "Traditional topic models struggle to capture complex topics due to their reliance on bag-of-words representations. There is a need for models that can articulate deeper conceptual themes and enable controllable text generation.", "method": "The paper introduces Mechanistic Topic Models (MTMs) which use sparse autoencoders to learn interpretable features for defining topics in a semantically rich space. It also presents an LLM-based evaluation framework named 'topic judge' for assessing these topics against traditional models.", "result": "MTMs demonstrate comparable or superior performance on coherence metrics compared to traditional and neural topic models across five datasets, and are preferred in evaluations by the topic judge.", "conclusion": "The results indicate that MTMs successfully capture complex topics and facilitate controllable generation of text, marking a significant advancement in topic modeling techniques.", "key_contributions": ["Introduction of Mechanistic Topic Models (MTMs) using sparse autoencoders", "Development of LLM-based 'topic judge' for evaluation", "Demonstration of controllable text generation through topic-based steering vectors"], "limitations": "", "keywords": ["topic modeling", "sparse autoencoders", "LLM evaluation", "text generation", "semantically rich features"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.23227", "pdf": "https://arxiv.org/pdf/2507.23227.pdf", "abs": "https://arxiv.org/abs/2507.23227", "title": "Enabling Few-Shot Alzheimer's Disease Diagnosis on Tabular Biomarker Data with LLMs", "authors": ["Sophie Kearney", "Shu Yang", "Zixuan Wen", "Bojian Hou", "Duy Duong-Tran", "Tianlong Chen", "Jason Moore", "Marylyn Ritchie", "Li Shen"], "categories": ["cs.CL", "cs.LG", "q-bio.QM"], "comment": null, "summary": "Early and accurate diagnosis of Alzheimer's disease (AD), a complex\nneurodegenerative disorder, requires analysis of heterogeneous biomarkers\n(e.g., neuroimaging, genetic risk factors, cognitive tests, and cerebrospinal\nfluid proteins) typically represented in a tabular format. With flexible\nfew-shot reasoning, multimodal integration, and natural-language-based\ninterpretability, large language models (LLMs) offer unprecedented\nopportunities for prediction with structured biomedical data. We propose a\nnovel framework called TAP-GPT, Tabular Alzheimer's Prediction GPT, that adapts\nTableGPT2, a multimodal tabular-specialized LLM originally developed for\nbusiness intelligence tasks, for AD diagnosis using structured biomarker data\nwith small sample sizes. Our approach constructs few-shot tabular prompts using\nin-context learning examples from structured biomedical data and finetunes\nTableGPT2 using the parameter-efficient qLoRA adaption for a clinical binary\nclassification task of AD or cognitively normal (CN). The TAP-GPT framework\nharnesses the powerful tabular understanding ability of TableGPT2 and the\nencoded prior knowledge of LLMs to outperform more advanced general-purpose\nLLMs and a tabular foundation model (TFM) developed for prediction tasks. To\nour knowledge, this is the first application of LLMs to the prediction task\nusing tabular biomarker data, paving the way for future LLM-driven multi-agent\nframeworks in biomedical informatics.", "AI": {"tldr": "The paper presents TAP-GPT, a framework using LLMs for Alzheimer's disease diagnosis leveraging structured biomarker data.", "motivation": "To improve early and accurate diagnosis of Alzheimer's disease through advanced analysis of diverse biomarker data.", "method": "The TAP-GPT framework adapts TableGPT2 for clinical binary classification tasks, utilizing few-shot tabular prompts and finetuning with qLoRA.", "result": "TAP-GPT demonstrates superior performance over general-purpose LLMs and dedicated tabular models in predicting Alzheimer's disease.", "conclusion": "This novel framework successfully integrates LLMs with structured biomedical data, facilitating more effective prediction of Alzheimer's disease.", "key_contributions": ["Introduction of TAP-GPT for Alzheimer's disease diagnosis using LLMs.", "Utilization of few-shot learning with structured biomarker data.", "Demonstration of improved performance over existing models."], "limitations": "", "keywords": ["Alzheimer's disease", "large language models", "tabular data", "biomedical informatics", "prediction framework"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.23247", "pdf": "https://arxiv.org/pdf/2507.23247.pdf", "abs": "https://arxiv.org/abs/2507.23247", "title": "P-ReMIS: Pragmatic Reasoning in Mental Health and a Social Implication", "authors": ["Sneha Oram", "Pushpak Bhattacharyya"], "categories": ["cs.CL"], "comment": null, "summary": "There has been an increase in recent advancements in the explainability and\ndevelopment of personalized chatbots for mental health. However, the reasoning\naspects for explainability and dialogue discourse have not been explored\npreviously for mental health. Hence, we are investigating the pragmatic\nreasoning capability of large language models (LLMs) in this domain. We\nintroduce P-ReMe dataset, and propose a modified definition for the pragmatic\nphenomena of implicature (implied meaning) and presupposition (implicit\nassumption) in mental health. Following the definition, we formulate two tasks\nin implicature and one task in presupposition. To benchmark the dataset and the\npresented tasks, we consider four models - Llama3.1, Mistral, MentaLLaMa, and\nQwen. The results of the experiments suggest that Mistral and Qwen show\nsubstantial reasoning capabilities in the domain. In addition, we also propose\nStiPRompts to study the stigma around mental health with the state-of-the-art\nLLMs, GPT-4o mini, Deepseek-chat, and Claude-3.5-haiku. Our evaluated findings\nshow that Claude-3.5-haiku deals with the stigma more responsibly compared to\nthe other two LLMs.", "AI": {"tldr": "This paper investigates the reasoning capabilities of LLMs in mental health by introducing the P-ReMe dataset and conducting experiments with various models.", "motivation": "To explore the unexplored aspects of explainability and dialogue in mental health chatbots using LLMs.", "method": "The study introduces a modified definition of implicature and presupposition, formulates tasks based on these, and benchmarks them using models like Llama3.1, Mistral, MentaLLaMa, and Qwen.", "result": "Mistral and Qwen displayed significant reasoning abilities in the mental health domain. Additionally, Claude-3.5-haiku was found to handle stigma around mental health more responsibly than GPT-4o mini and Deepseek-chat.", "conclusion": "The findings highlight the potential of LLMs in enhancing dialogue in mental health contexts and addressing stigma responsibly.", "key_contributions": ["Introduction of the P-ReMe dataset", "Modified definitions of implicature and presupposition in mental health", "Assessment of LLMs for handling mental health stigma"], "limitations": "", "keywords": ["mental health", "large language models", "explainability", "chatbots", "implying meaning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.23248", "pdf": "https://arxiv.org/pdf/2507.23248.pdf", "abs": "https://arxiv.org/abs/2507.23248", "title": "Evaluating LLMs' Multilingual Capabilities for Bengali: Benchmark Creation and Performance Analysis", "authors": ["Shimanto Bhowmik", "Tawsif Tashwar Dipto", "Md Sazzad Islam", "Sheryl Hsu", "Tahsin Reasat"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Bengali is an underrepresented language in NLP research. However, it remains\na challenge due to its unique linguistic structure and computational\nconstraints. In this work, we systematically investigate the challenges that\nhinder Bengali NLP performance by focusing on the absence of standardized\nevaluation benchmarks. We then evaluated 10 recent open source Large Language\nModels (LLMs) in 8 of the translated datasets and performed a comprehensive\nerror analysis to pinpoint their primary failure modes. Our findings reveal\nconsistent performance gaps for Bengali compared to English, particularly for\nsmaller models and specific model families like Mistral. We also identified\npromising robustness in certain architectures, such as DeepSeek, that maintain\nmore stable performance across languages. Our analysis reveals an inverse\nrelationship between tokenization efficiency and LLM accuracy where models tend\nto perform worse when inputs are excessively tokenized, whereas more efficient\n\\& concise tokenization results in improved performance. These findings\nhighlight critical areas where current models fall short and underscore the\nneed for improved dataset quality and evaluation methodologies tailored to\nmultilingual contexts. This work will catalyze further research on NLP for\nunderrepresented languages, helping to democratize access to advanced language\ntechnologies worldwide. The code and dataset used in this research is publicly\navailable at https://github.com/BengaliAI/bn-llm-benchmark.", "AI": {"tldr": "This paper investigates challenges in Bengali NLP, evaluates recent LLMs, performs error analysis, and identifies performance gaps compared to English, emphasizing the need for better evaluation methodologies.", "motivation": "Bengali is underrepresented in NLP research due to unique linguistic structures and lack of standardized benchmarks.", "method": "The study evaluates 10 recent open-source LLMs on 8 translated datasets, conducting error analysis to understand performance gaps.", "result": "Findings indicate consistent performance gaps for Bengali compared to English, especially in smaller models, and an inverse relationship between tokenization efficiency and LLM accuracy.", "conclusion": "The research highlights areas needing improvement in dataset quality and evaluation methodologies for underrepresented languages, aiming to democratize access to NLP technologies.", "key_contributions": ["Systematic investigation of Bengali NLP challenges.", "Evaluation of multiple LLMs on translated datasets.", "Identification of tokenization efficiency impacting model accuracy."], "limitations": "Focuses primarily on Bengali; findings may not generalize to other underrepresented languages without further research.", "keywords": ["Bengali", "NLP", "Large Language Models", "evaluation benchmarks", "tokenization"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.23279", "pdf": "https://arxiv.org/pdf/2507.23279.pdf", "abs": "https://arxiv.org/abs/2507.23279", "title": "Unveiling Super Experts in Mixture-of-Experts Large Language Models", "authors": ["Zunhai Su", "Qingyuan Li", "Hao Zhang", "YuLei Qian", "Yuchen Xie", "Kehong Yuan"], "categories": ["cs.CL"], "comment": null, "summary": "Sparsely activated Mixture-of-Experts (MoE) models have shown promise in\nenhancing the learning capacity of large language models (LLMs). Leveraging the\nintrinsic importance differences among experts, recent research has explored\nexpert-level compression techniques to improve the efficiency of MoE LLMs.\nHowever, existing approaches often rely on empirical criteria to identify\ncritical experts, lacking a deeper exploration and understanding of the\nheterogeneous importance of experts. In this study, we present the first\ndiscovery and investigation of a distinct subset of experts that play a crucial\nrole in the underlying mechanisms during the model's forward inference. These\nexperts are prevalent in open-source MoE LLMs, and despite their limited\nnumber, pruning them leads to a significant decline in model performance (e.g.,\npruning three causes Qwen3-30B-A3B to produce repetitive and uninformative\noutputs). We refer to these experts as Super Experts (SEs). Our comprehensive\nanalysis provides progressively deeper insights into SEs. (i) SEs are\ncharacterized by rare but extreme activation outliers in the output of the\ndown_proj, which give rise to massive activations in the hidden states between\ndecoder layers. Moreover, the distribution of SEs remains model-specific and is\nunaffected by post-training processes. (ii) By pruning SEs, we assess their\nsignificance across a variety of tasks, revealing their considerable impact on\nthe model's overall performance, particularly in mathematical reasoning. (iii)\nWe further enhance our understanding of the influence of SEs compression. Our\nfindings confirm that MoE LLMs rely on SEs to induce attention sinks, which are\ncrucial for the distribution of attention scores but are significantly\ndisrupted by SE pruning. The code is available at\nhttps://github.com/ZunhaiSu/Super-Experts-Profilling.", "AI": {"tldr": "This study investigates Super Experts in Mixture-of-Experts LLMs, revealing their crucial role in model performance and the implications of their pruning.", "motivation": "To address the lack of understanding regarding the importance of different experts in MoE LLMs and improve their efficiency.", "method": "Analysis of expert activation patterns and performance impact through pruning in various tasks with open-source MoE LLMs.", "result": "Identified Super Experts (SEs) that are critical for maintaining model performance, especially in tasks requiring mathematical reasoning, and demonstrated that pruning SEs leads to significant declines in output quality.", "conclusion": "SEs are essential for the model's performance, particularly in specific tasks, and their pruning severely impacts the distribution of attention scores within the model.", "key_contributions": ["Identification of Super Experts (SEs) in MoE LLMs and their characteristics.", "Analysis of SEs' impact on model performance and attention distribution.", "Open-source code to assist in further research and exploration of SEs."], "limitations": "The findings are based on specific models and may not generalize to all MoE LLMs; further research is needed to explore SEs in different architectures.", "keywords": ["Mixture-of-Experts", "Super Experts", "Machine Learning", "Large Language Models", "Model Compression"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.23319", "pdf": "https://arxiv.org/pdf/2507.23319.pdf", "abs": "https://arxiv.org/abs/2507.23319", "title": "What's Taboo for You? - An Empirical Evaluation of LLMs Behavior Toward Sensitive Content", "authors": ["Alfio Ferrara", "Sergio Picascia", "Laura Pinnavaia", "Vojimir Ranitovic", "Elisabetta Rocchetti", "Alice Tuveri"], "categories": ["cs.CL"], "comment": null, "summary": "Proprietary Large Language Models (LLMs) have shown tendencies toward\npoliteness, formality, and implicit content moderation. While previous research\nhas primarily focused on explicitly training models to moderate and detoxify\nsensitive content, there has been limited exploration of whether LLMs\nimplicitly sanitize language without explicit instructions. This study\nempirically analyzes the implicit moderation behavior of GPT-4o-mini when\nparaphrasing sensitive content and evaluates the extent of sensitivity shifts.\nOur experiments indicate that GPT-4o-mini systematically moderates content\ntoward less sensitive classes, with substantial reductions in derogatory and\ntaboo language. Also, we evaluate the zero-shot capabilities of LLMs in\nclassifying sentence sensitivity, comparing their performances against\ntraditional methods.", "AI": {"tldr": "This study examines the implicit moderation behavior of GPT-4o-mini in paraphrasing sensitive content and its ability to classify sentence sensitivity.", "motivation": "To explore whether proprietary LLMs, specifically GPT-4o-mini, implicitly sanitize language during paraphrasing without explicit guidance.", "method": "Empirical analysis of GPT-4o-mini's behavior when paraphrasing sensitive content, alongside evaluation of zero-shot classification capabilities against traditional methods.", "result": "Findings show that GPT-4o-mini systematically reduces sensitive content, indicating substantial implicit moderation towards less sensitive classes.", "conclusion": "The results suggest that LLMs can effectively moderate language without direct training or instructions, highlighting potential implications for safe content generation.", "key_contributions": ["Empirical analysis of GPT-4o-mini's implicit moderation behavior.", "Evaluation of LLMs' zero-shot classification capabilities regarding sentence sensitivity.", "Comparison of LLM performance against traditional methods."], "limitations": "The study focuses only on GPT-4o-mini, so findings may not generalize to other models.", "keywords": ["Large Language Models", "implicit moderation", "paraphrasing", "sensitivity classification", "GPT-4o-mini"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2507.23334", "pdf": "https://arxiv.org/pdf/2507.23334.pdf", "abs": "https://arxiv.org/abs/2507.23334", "title": "MUST-RAG: MUSical Text Question Answering with Retrieval Augmented Generation", "authors": ["Daeyong Kwon", "SeungHeon Doh", "Juhan Nam"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": "8 pages, 2 figures", "summary": "Recent advancements in Large language models (LLMs) have demonstrated\nremarkable capabilities across diverse domains. While they exhibit strong\nzero-shot performance on various tasks, LLMs' effectiveness in music-related\napplications remains limited due to the relatively small proportion of\nmusic-specific knowledge in their training data. To address this limitation, we\npropose MusT-RAG, a comprehensive framework based on Retrieval Augmented\nGeneration (RAG) to adapt general-purpose LLMs for text-only music question\nanswering (MQA) tasks. RAG is a technique that provides external knowledge to\nLLMs by retrieving relevant context information when generating answers to\nquestions. To optimize RAG for the music domain, we (1) propose MusWikiDB, a\nmusic-specialized vector database for the retrieval stage, and (2) utilizes\ncontext information during both inference and fine-tuning processes to\neffectively transform general-purpose LLMs into music-specific models. Our\nexperiment demonstrates that MusT-RAG significantly outperforms traditional\nfine-tuning approaches in enhancing LLMs' music domain adaptation capabilities,\nshowing consistent improvements across both in-domain and out-of-domain MQA\nbenchmarks. Additionally, our MusWikiDB proves substantially more effective\nthan general Wikipedia corpora, delivering superior performance and\ncomputational efficiency.", "AI": {"tldr": "MusT-RAG is a framework that enhances the performance of general-purpose LLMs for music-related question answering by using a music-specific vector database and retrieval techniques.", "motivation": "To improve the effectiveness of LLMs in music-related applications, as their training data lacks sufficient music-specific knowledge.", "method": "MusT-RAG integrates Retrieval Augmented Generation (RAG) to adapt LLMs for music question answering, utilizing a specialized music vector database (MusWikiDB) for improved retrieval of context during inference and fine-tuning.", "result": "MusT-RAG significantly outperforms traditional fine-tuning methods in adapting LLMs for music tasks, showing improvements on both in-domain and out-of-domain benchmarks.", "conclusion": "The MusWikiDB and the RAG approach provide a more efficient and effective way to utilize LLMs in music-focused applications, leading to better performance than using general databases.", "key_contributions": ["Introduction of MusT-RAG framework for music question answering", "Development of MusWikiDB as a specialized music retrieval database", "Demonstration of significant performance improvements over traditional methods"], "limitations": "", "keywords": ["Large Language Models", "Retrieval Augmented Generation", "Music Question Answering", "MusWikiDB", "AI"], "importance_score": 4, "read_time_minutes": 8}}
{"id": "2507.23358", "pdf": "https://arxiv.org/pdf/2507.23358.pdf", "abs": "https://arxiv.org/abs/2507.23358", "title": "Text-to-SQL Task-oriented Dialogue Ontology Construction", "authors": ["Renato Vukovic", "Carel van Niekerk", "Michael Heck", "Benjamin Ruppik", "Hsien-Chin Lin", "Shutong Feng", "Nurul Lubis", "Milica Gasic"], "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR"], "comment": null, "summary": "Large language models (LLMs) are widely used as general-purpose knowledge\nsources, but they rely on parametric knowledge, limiting explainability and\ntrustworthiness. In task-oriented dialogue (TOD) systems, this separation is\nexplicit, using an external database structured by an explicit ontology to\nensure explainability and controllability. However, building such ontologies\nrequires manual labels or supervised training. We introduce TeQoDO: a\nText-to-SQL task-oriented Dialogue Ontology construction method. Here, an LLM\nautonomously builds a TOD ontology from scratch without supervision using its\ninherent SQL programming capabilities combined with dialogue theory provided in\nthe prompt. We show that TeQoDO outperforms transfer learning approaches, and\nits constructed ontology is competitive on a downstream dialogue state tracking\ntask. Ablation studies demonstrate the key role of dialogue theory. TeQoDO also\nscales to allow construction of much larger ontologies, which we investigate on\na Wikipedia and ArXiv dataset. We view this as a step towards broader\napplication of ontologies to increase LLM explainability.", "AI": {"tldr": "TeQoDO is a method for autonomously constructing task-oriented dialogue ontologies using large language models without supervision, enhancing explainability in dialogue systems.", "motivation": "LLMs have limitations in explainability and trustworthiness when used as knowledge sources in task-oriented dialogue systems, necessitating structured ontologies.", "method": "TeQoDO uses an LLM to autonomously build a dialogue ontology from scratch by leveraging its SQL programming capabilities and dialogue theory provided in the prompt, without requiring manual labels or supervised training.", "result": "TeQoDO outperforms existing transfer learning approaches, and the constructed ontology is competitive for downstream dialogue state tracking tasks. It also supports the construction of larger ontologies, as demonstrated with Wikipedia and ArXiv datasets.", "conclusion": "TeQoDO represents progress towards enhancing the explainability of LLMs in dialogue systems through the automated construction of ontologies.", "key_contributions": ["Introduction of TeQoDO for ontology construction using LLMs without supervision", "Demonstration of performance improvement over transfer learning methods", "Scalability to larger ontologies using external datasets"], "limitations": "The method's effectiveness may depend on the quality and diversity of the dialogue theory used in the prompts.", "keywords": ["large language models", "task-oriented dialogue systems", "ontology construction", "explainability", "dialogue theory"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2507.23382", "pdf": "https://arxiv.org/pdf/2507.23382.pdf", "abs": "https://arxiv.org/abs/2507.23382", "title": "MPCC: A Novel Benchmark for Multimodal Planning with Complex Constraints in Multimodal Large Language Models", "authors": ["Yiyan Ji", "Haoran Chen", "Qiguang Chen", "Chengyue Wu", "Libo Qin", "Wanxiang Che"], "categories": ["cs.CL", "cs.AI", "cs.CV", "I.2.8; I.2.10"], "comment": "Accepted to ACM Multimedia 2025", "summary": "Multimodal planning capabilities refer to the ability to predict, reason, and\ndesign steps for task execution with multimodal context, which is essential for\ncomplex reasoning and decision-making across multiple steps. However, current\nbenchmarks face two key challenges: (1) they cannot directly assess multimodal\nreal-world planning capabilities, and (2) they lack constraints or implicit\nconstraints across modalities. To address these issues, we introduce Multimodal\nPlanning with Complex Constraints (MPCC), the first benchmark to systematically\nevaluate MLLMs' ability to handle multimodal constraints in planning. To\naddress the first challenge, MPCC focuses on three real-world tasks: Flight\nPlanning, Calendar Planning, and Meeting Planning. To solve the second\nchallenge, we introduce complex constraints (e.g. budget, temporal, and\nspatial) in these tasks, with graded difficulty levels (EASY, MEDIUM, HARD) to\nseparate constraint complexity from search space expansion. Experiments on 13\nadvanced MLLMs reveal significant challenges: closed-source models achieve only\n21.3% feasible plans, while open-source models average below 11%. Additionally,\nwe observe that MLLMs are highly sensitive to constraint complexity and that\ntraditional multimodal prompting strategies fail in multi-constraint scenarios.\nOur work formalizes multimodal constraints in planning, provides a rigorous\nevaluation framework, and highlights the need for advancements in\nconstraint-aware reasoning for real-world MLLM applications.", "AI": {"tldr": "This paper introduces the Multimodal Planning with Complex Constraints (MPCC) benchmark, designed to evaluate the ability of Multimodal Large Language Models (MLLMs) in handling real-world planning tasks with implicit multimodal constraints.", "motivation": "To address the challenges in assessing MLLMs' capabilities in multimodal planning and the lack of effective benchmarks that incorporate constraints across modalities.", "method": "The benchmark focuses on three real-world tasks (Flight Planning, Calendar Planning, and Meeting Planning) and introduces complex constraints (budget, temporal, spatial) with graded difficulty levels (EASY, MEDIUM, HARD).", "result": "Experiments reveal that closed-source models achieve 21.3% feasible plans, while open-source models average below 11%, indicating significant challenges in planning under constraints.", "conclusion": "The work formalizes multimodal constraints in task planning, providing a new evaluation framework and calling for advancements in constraint-aware reasoning for real-world MLLM applications.", "key_contributions": ["Introduction of the MPCC benchmark for multimodal planning", "Focus on three specific real-world planning tasks", "Identification of significant challenges faced by MLLMs in planning with complex constraints"], "limitations": "The study highlights the sensitivity of MLLMs to constraint complexity and limitations of traditional multimodal prompting strategies under multi-constraint scenarios.", "keywords": ["multimodal planning", "MLLMs", "benchmark", "constraints", "real-world tasks"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.23386", "pdf": "https://arxiv.org/pdf/2507.23386.pdf", "abs": "https://arxiv.org/abs/2507.23386", "title": "Causal2Vec: Improving Decoder-only LLMs as Versatile Embedding Models", "authors": ["Ailiang Lin", "Zhuoyun Li", "Kotaro Funakoshi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Decoder-only large language models (LLMs) are increasingly used to build\nembedding models that effectively encode the semantic information of natural\nlanguage texts into dense vector representations for various embedding tasks.\nHowever, many existing methods primarily focus on removing the causal attention\nmask in LLMs to enable bidirectional attention, potentially undermining the\nmodel's ability to extract semantic information acquired during pretraining.\nAdditionally, leading unidirectional approaches often rely on extra input text\nto overcome the inherent limitations of causal attention, inevitably increasing\ncomputational costs. In this work, we propose Causal2Vec, a general-purpose\nembedding model tailored to enhance the performance of decoder-only LLMs\nwithout altering their original architectures or introducing significant\ncomputational overhead. Specifically, we first employ a lightweight BERT-style\nmodel to pre-encode the input text into a single Contextual token, which is\nthen prepended to the LLM's input sequence, allowing each token to capture\ncontextualized information even without attending to future tokens.\nFurthermore, to mitigate the recency bias introduced by last-token pooling and\nhelp LLMs better leverage the semantic information encoded in the Contextual\ntoken, we concatenate the last hidden states of Contextual and EOS tokens as\nthe final text embedding. In practice, Causal2Vec achieves state-of-the-art\nperformance on the Massive Text Embeddings Benchmark (MTEB) among models\ntrained solely on publicly available retrieval datasets, while reducing the\nrequired sequence length by up to 85% and inference time by up to 82% compared\nto best-performing methods.", "AI": {"tldr": "Causal2Vec enhances decoder-only LLMs for embedding tasks without altering their architecture, achieving state-of-the-art performance while significantly reducing computational costs.", "motivation": "Existing methods modify causal attention in LLMs, potentially degrading their ability to extract semantic information, leading to increased computational costs.", "method": "Causal2Vec uses a BERT-style model to create a Contextual token, which is added to the LLM's input sequence to capture contextual information. It combines this Contextual token with the last hidden states of other tokens to generate embeddings.", "result": "Causal2Vec shows state-of-the-art results on the MTEB, reducing required sequence length by up to 85% and inference time by 82% compared to top methods.", "conclusion": "Causal2Vec improves existing practices in embedding model design for decoder-only LLMs without compromising architecture or increasing costs.", "key_contributions": ["Introduction of a Contextual token for better semantic information capture", "Significant reduction in sequence length and inference time", "State-of-the-art performance on MTEB benchmarks"], "limitations": "", "keywords": ["Causal2Vec", "embedding models", "large language models", "natural language processing", "computational efficiency"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.23399", "pdf": "https://arxiv.org/pdf/2507.23399.pdf", "abs": "https://arxiv.org/abs/2507.23399", "title": "Beyond the Cloud: Assessing the Benefits and Drawbacks of Local LLM Deployment for Translators", "authors": ["Peter Sandrini"], "categories": ["cs.CL", "cs.CY", "I.2.7; K.4.3"], "comment": null, "summary": "The rapid proliferation of Large Language Models presents both opportunities\nand challenges for the translation field. While commercial, cloud-based AI\nchatbots have garnered significant attention in translation studies, concerns\nregarding data privacy, security, and equitable access necessitate exploration\nof alternative deployment models. This paper investigates the feasibility and\nperformance of locally deployable, free language models as a viable alternative\nto proprietary, cloud-based AI solutions. This study evaluates three\nopen-source models installed on CPU-based platforms and compared against\ncommercially available online chat-bots. The evaluation focuses on functional\nperformance rather than a comparative analysis of human-machine translation\nquality, an area already subject to extensive research. The platforms assessed\nwere chosen for their accessibility and ease of use across various operating\nsystems. While local deployment introduces its own challenges, the benefits of\nenhanced data control, improved privacy, and reduced dependency on cloud\nservices are compelling. The findings of this study contribute to a growing\nbody of knowledge concerning the democratization of AI technology and inform\nfuture research and development efforts aimed at making LLMs more accessible\nand practical for a wider range of users, specifically focusing on the needs of\nindividual translators and small businesses.", "AI": {"tldr": "This paper explores the potential of locally deployable, free language models as alternatives to commercial cloud-based AI chatbots in translation, emphasizing data privacy and accessibility.", "motivation": "The paper addresses the challenges and opportunities posed by the rapid growth of Large Language Models in translation, highlighting the need for alternative deployment models due to data privacy and access concerns.", "method": "The study evaluates three open-source language models installed on CPU-based platforms and compares their performance with commercial AI chatbots, focusing on functional performance.", "result": "The findings underscore the advantages of local deployment, including better data control and privacy, while recognizing the challenges it presents.", "conclusion": "Local language models can democratize AI technology, providing accessible options for individual translators and small businesses, despite the inherent challenges of local deployment.", "key_contributions": ["Evaluation of local language models for translation", "Focus on data privacy and control", "Support for democratization of AI technology"], "limitations": "", "keywords": ["Large Language Models", "local deployment", "data privacy", "translation studies", "open-source models"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.23400", "pdf": "https://arxiv.org/pdf/2507.23400.pdf", "abs": "https://arxiv.org/abs/2507.23400", "title": "MRGSEM-Sum: An Unsupervised Multi-document Summarization Framework based on Multi-Relational Graphs and Structural Entropy Minimization", "authors": ["Yongbing Zhang", "Fang Nan", "Shengxiang Gao", "Yuxin Huang", "Kaiwen Tan", "Zhengtao Yu"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "The core challenge faced by multi-document summarization is the complexity of\nrelationships among documents and the presence of information redundancy. Graph\nclustering is an effective paradigm for addressing this issue, as it models the\ncomplex relationships among documents using graph structures and reduces\ninformation redundancy through clustering, achieving significant research\nprogress. However, existing methods often only consider single-relational\ngraphs and require a predefined number of clusters, which hinders their ability\nto fully represent rich relational information and adaptively partition\nsentence groups to reduce redundancy. To overcome these limitations, we propose\nMRGSEM-Sum, an unsupervised multi-document summarization framework based on\nmulti-relational graphs and structural entropy minimization. Specifically, we\nconstruct a multi-relational graph that integrates semantic and discourse\nrelations between sentences, comprehensively modeling the intricate and dynamic\nconnections among sentences across documents. We then apply a two-dimensional\nstructural entropy minimization algorithm for clustering, automatically\ndetermining the optimal number of clusters and effectively organizing sentences\ninto coherent groups. Finally, we introduce a position-aware compression\nmechanism to distill each cluster, generating concise and informative\nsummaries. Extensive experiments on four benchmark datasets (Multi-News,\nDUC-2004, PubMed, and WikiSum) demonstrate that our approach consistently\noutperforms previous unsupervised methods and, in several cases, achieves\nperformance comparable to supervised models and large language models. Human\nevaluation demonstrates that the summaries generated by MRGSEM-Sum exhibit high\nconsistency and coverage, approaching human-level quality.", "AI": {"tldr": "MRGSEM-Sum is an unsupervised multi-document summarization framework that employs multi-relational graphs and structural entropy minimization to generate concise summaries, outperforming existing methods.", "motivation": "To address the limitations of existing multi-document summarization methods that only consider single-relational graphs and require a predefined number of clusters, which restricts their adaptability and redundancy reduction capabilities.", "method": "The framework constructs a multi-relational graph to model semantic and discourse relations among sentences and then applies a two-dimensional structural entropy minimization algorithm to determine optimal clustering rather than using a predefined number of clusters. Additionally, a position-aware compression mechanism is introduced to create concise summaries.", "result": "Experiments on four benchmark datasets showcase MRGSEM-Sum's performance consistently outperforming previous unsupervised techniques and, in some instances, matching the results of supervised models and large language models.", "conclusion": "MRGSEM-Sum produces high-quality summaries that exhibit human-level consistency and coverage, indicating its effectiveness in multi-document summarization tasks.", "key_contributions": ["Introduction of multi-relational graphs for summarization", "Implementation of structural entropy minimization for adaptive clustering", "Position-aware compression mechanism for enhanced summary quality"], "limitations": "", "keywords": ["multi-document summarization", "graph clustering", "structural entropy minimization", "semantic relations", "discourse relations"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.23404", "pdf": "https://arxiv.org/pdf/2507.23404.pdf", "abs": "https://arxiv.org/abs/2507.23404", "title": "Enhanced Arabic Text Retrieval with Attentive Relevance Scoring", "authors": ["Salah Eddine Bekhouche", "Azeddine Benlamoudi", "Yazid Bounab", "Fadi Dornaika", "Abdenour Hadid"], "categories": ["cs.CL"], "comment": null, "summary": "Arabic poses a particular challenge for natural language processing (NLP) and\ninformation retrieval (IR) due to its complex morphology, optional diacritics\nand the coexistence of Modern Standard Arabic (MSA) and various dialects.\nDespite the growing global significance of Arabic, it is still underrepresented\nin NLP research and benchmark resources. In this paper, we present an enhanced\nDense Passage Retrieval (DPR) framework developed specifically for Arabic. At\nthe core of our approach is a novel Attentive Relevance Scoring (ARS) that\nreplaces standard interaction mechanisms with an adaptive scoring function that\nmore effectively models the semantic relevance between questions and passages.\nOur method integrates pre-trained Arabic language models and architectural\nrefinements to improve retrieval performance and significantly increase ranking\naccuracy when answering Arabic questions. The code is made publicly available\nat \\href{https://github.com/Bekhouche/APR}{GitHub}.", "AI": {"tldr": "The paper presents an enhanced Dense Passage Retrieval framework for Arabic, improving retrieval performance using a novel Attentive Relevance Scoring mechanism.", "motivation": "The paper addresses the challenges in NLP and information retrieval for Arabic due to its morphological complexity and underrepresentation in research.", "method": "The methodology includes the development of an enhanced Dense Passage Retrieval framework with an Attentive Relevance Scoring mechanism that models semantic relevance between questions and passages.", "result": "The approach integrates pre-trained Arabic language models and yields significant improvements in ranking accuracy for Arabic question answering.", "conclusion": "The enhanced DPR framework demonstrates improved retrieval performance for Arabic, highlighting the importance of focusing on underrepresented languages in NLP research.", "key_contributions": ["Introduction of Attentive Relevance Scoring for Arabic", "Enhanced Dense Passage Retrieval framework tailored for Arabic", "Public availability of the code for further research"], "limitations": "", "keywords": ["Arabic NLP", "Dense Passage Retrieval", "Information Retrieval", "Attentive Relevance Scoring", "Question Answering"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.23407", "pdf": "https://arxiv.org/pdf/2507.23407.pdf", "abs": "https://arxiv.org/abs/2507.23407", "title": "Beyond Passive Critical Thinking: Fostering Proactive Questioning to Enhance Human-AI Collaboration", "authors": ["Ante Wang", "Yujie Lin", "Jingyao Liu", "Suhang Wu", "Hao Liu", "Xinyan Xiao", "Jinsong Su"], "categories": ["cs.CL"], "comment": null, "summary": "Critical thinking is essential for building robust AI systems, preventing\nthem from blindly accepting flawed data or biased reasoning. However, prior\nwork has primarily focused on passive critical thinking, where models simply\nreject problematic queries without taking constructive steps to address user\nrequests. In this work, we introduce proactive critical thinking, a paradigm\nwhere models actively seek missing or clarifying information from users to\nresolve their queries better. To evaluate this capability, we present GSM-MC\nand GSM-MCE, two novel benchmarks based on GSM8K for assessing mathematical\nreasoning under incomplete or misleading conditions. GSM-MC contains 1,368 math\nproblems with a key variable deliberately removed, requiring models to identify\nand request the missing information. GSM-MCE further increases the difficulty\nby introducing irrelevant details to test robustness against distractions.\nExperiments on Qwen3 and Llama series models show that, while these models\nexcel in traditional reasoning tasks due to extensive post-training and\ninference-time scaling, they struggle with proactive critical thinking,\nespecially smaller ones. However, we demonstrate that reinforcement learning\n(RL) can significantly improve this ability. Using our enhanced RL algorithm,\nwe achieve substantial gains, boosting the Qwen3-1.7B's accuracy from 0.15% to\n73.98% on GSM-MC. We hope this work advances models that collaborate more\neffectively with users in problem-solving through proactive critical thinking.", "AI": {"tldr": "This paper introduces proactive critical thinking in AI systems, where models actively seek missing information from users to improve problem-solving. It presents two benchmarks to evaluate this capability and demonstrates significant improvements in model performance using reinforcement learning.", "motivation": "To enhance AI models' interaction with users by encouraging proactive critical thinking rather than passive data rejection or problem avoidance.", "method": "The paper introduces two benchmarks, GSM-MC and GSM-MCE, to evaluate proactive critical thinking in models. Experiments were conducted on Qwen3 and Llama series models, measuring their performance on these benchmarks and implementing reinforcement learning to improve results.", "result": "Models like Qwen3 show significant improvement in proactive critical thinking tasks, with accuracy increasing from 0.15% to 73.98% on the GSM-MC benchmark after applying the enhanced reinforcement learning algorithm.", "conclusion": "This work highlights the importance of proactive critical thinking in AI, aiming to foster better collaboration between models and users in problem-solving scenarios.", "key_contributions": ["Introduction of proactive critical thinking framework for AI models.", "Development of two novel benchmarks (GSM-MC, GSM-MCE) for assessing mathematical reasoning under challenging conditions.", "Demonstration of significant performance improvement using reinforcement learning."], "limitations": "The study primarily focuses on mathematical reasoning and may not generalize to all types of queries or tasks.", "keywords": ["proactive critical thinking", "AI collaboration", "mathematical reasoning", "reinforcement learning", "benchmarking"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.23465", "pdf": "https://arxiv.org/pdf/2507.23465.pdf", "abs": "https://arxiv.org/abs/2507.23465", "title": "Role-Aware Language Models for Secure and Contextualized Access Control in Organizations", "authors": ["Saeed Almheiri", "Yerulan Kongrat", "Adrian Santosh", "Ruslan Tasmukhanov", "Josemaria Vera", "Muhammad Dehan Al Kautsar", "Fajri Koto"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As large language models (LLMs) are increasingly deployed in enterprise\nsettings, controlling model behavior based on user roles becomes an essential\nrequirement. Existing safety methods typically assume uniform access and focus\non preventing harmful or toxic outputs, without addressing role-specific access\nconstraints. In this work, we investigate whether LLMs can be fine-tuned to\ngenerate responses that reflect the access privileges associated with different\norganizational roles. We explore three modeling strategies: a BERT-based\nclassifier, an LLM-based classifier, and role-conditioned generation. To\nevaluate these approaches, we construct two complementary datasets. The first\nis adapted from existing instruction-tuning corpora through clustering and role\nlabeling, while the second is synthetically generated to reflect realistic,\nrole-sensitive enterprise scenarios. We assess model performance across varying\norganizational structures and analyze robustness to prompt injection, role\nmismatch, and jailbreak attempts.", "AI": {"tldr": "This paper explores fine-tuning large language models (LLMs) to generate responses based on user role access privileges in enterprise settings.", "motivation": "The need for LLMs to behave according to user roles in enterprise settings is critical, as current safety methods do not consider role-specific access constraints.", "method": "The authors investigate three modeling strategies: a BERT-based classifier, an LLM-based classifier, and role-conditioned generation. Two datasets were created for evaluation: one from existing corpora and another synthetically generated for realistic scenarios.", "result": "The study assesses model performance across various organizational structures, focusing on robustness to prompt injection, role mismatch, and jailbreak attempts.", "conclusion": "Fine-tuning LLMs for role-sensitive responses can enhance safety and control in enterprise applications, addressing a significant gap in current methods.", "key_contributions": ["Introduction of role-sensitive fine-tuning for LLMs", "Creation of two novel datasets for evaluation", "Analysis of model robustness in the context of organizational roles"], "limitations": "The study may not address all potential user roles or organizational contexts, and the synthetic dataset may have limitations in realism.", "keywords": ["large language models", "role-specific access", "enterprise applications", "fine-tuning", "safety methods"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.23486", "pdf": "https://arxiv.org/pdf/2507.23486.pdf", "abs": "https://arxiv.org/abs/2507.23486", "title": "A Novel Evaluation Benchmark for Medical LLMs: Illuminating Safety and Effectiveness in Clinical Domains", "authors": ["Shirui Wang", "Zhihui Tang", "Huaxia Yang", "Qiuhong Gong", "Tiantian Gu", "Hongyang Ma", "Yongxin Wang", "Wubin Sun", "Zeliang Lian", "Kehang Mao", "Yinan Jiang", "Zhicheng Huang", "Lingyun Ma", "Wenjie Shen", "Yajie Ji", "Yunhui Tan", "Chunbo Wang", "Yunlu Gao", "Qianling Ye", "Rui Lin", "Mingyu Chen", "Lijuan Niu", "Zhihao Wang", "Peng Yu", "Mengran Lang", "Yue Liu", "Huimin Zhang", "Haitao Shen", "Long Chen", "Qiguang Zhao", "Si-Xuan Liu", "Lina Zhou", "Hua Gao", "Dongqiang Ye", "Lingmin Meng", "Youtao Yu", "Naixin Liang", "Jianxiong Wu"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) hold promise in clinical decision support but\nface major challenges in safety evaluation and effectiveness validation. We\ndeveloped the Clinical Safety-Effectiveness Dual-Track Benchmark (CSEDB), a\nmultidimensional framework built on clinical expert consensus, encompassing 30\ncriteria covering critical areas like critical illness recognition, guideline\nadherence, and medication safety, with weighted consequence measures.\nThirty-two specialist physicians developed and reviewed 2,069 open-ended Q&A\nitems aligned with these criteria, spanning 26 clinical departments to simulate\nreal-world scenarios. Benchmark testing of six LLMs revealed moderate overall\nperformance (average total score 57.2%, safety 54.7%, effectiveness 62.3%),\nwith a significant 13.3% performance drop in high-risk scenarios (p < 0.0001).\nDomain-specific medical LLMs showed consistent performance advantages over\ngeneral-purpose models, with relatively higher top scores in safety (0.912) and\neffectiveness (0.861). The findings of this study not only provide a\nstandardized metric for evaluating the clinical application of medical LLMs,\nfacilitating comparative analyses, risk exposure identification, and\nimprovement directions across different scenarios, but also hold the potential\nto promote safer and more effective deployment of large language models in\nhealthcare environments.", "AI": {"tldr": "The paper presents a benchmark framework for evaluating the safety and effectiveness of large language models in clinical settings, identifying significant performance variances across different scenarios.", "motivation": "To address challenges in safety evaluation and effectiveness validation of large language models in clinical decision support.", "method": "Developed the Clinical Safety-Effectiveness Dual-Track Benchmark (CSEDB) with input from 32 physicians, covering 30 criteria and 2,069 Q&A items, comparing six LLMs on performance.", "result": "Moderate overall performance of LLMs with a mean score of 57.2%, a significant drop in performance under high-risk scenarios, and domain-specific LLMs outperforming general models.", "conclusion": "The study provides a standardized metric for evaluating medical LLMs, which can enhance their safety and effectiveness in healthcare applications.", "key_contributions": ["Development of the CSEDB framework", "Establishment of a standardized evaluation metric for clinical LLMs", "Identification of performance differences between domain-specific and general-purpose LLMs"], "limitations": "", "keywords": ["Large Language Models", "Clinical Decision Support", "Safety Evaluation", "Effectiveness Validation", "Health Informatics"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.23541", "pdf": "https://arxiv.org/pdf/2507.23541.pdf", "abs": "https://arxiv.org/abs/2507.23541", "title": "Med-R$^3$: Enhancing Medical Retrieval-Augmented Reasoning of LLMs via Progressive Reinforcement Learning", "authors": ["Keer Lu", "Zheng Liang", "Youquan Li", "Jiejun Tan", "Da Pan", "Shusen Zhang", "Guosheng Dong", "Huang Leng"], "categories": ["cs.CL"], "comment": null, "summary": "In medical scenarios, effectively retrieving external knowledge and\nleveraging it for rigorous logical reasoning is of significant importance.\nDespite their potential, existing work has predominantly focused on enhancing\neither retrieval or reasoning capabilities of the models in isolation, with\nlittle attention given to their joint optimization, which leads to limited\ncoordination between the two processes. Additionally, current methods rely\nheavily on supervised fine-tuning (SFT), which can cause models to memorize\nexisting problem-solving pathways, thereby restricting their generalization\nability when confronted with novel problem contexts. Furthermore, while some\nstudies have explored to improve retrieval-augmented reasoning in general\ndomains via reinforcement learning, their reward function designs do not\nadequately capture the specific demands of the medical domain. To address these\nchallenges, we introduce **Med-R$^3$**, a **Med**ical **R**etrieval-augmented\n**R**easoning framework driven by progressive **R**einforcement learning. In\nthis framework, we first develop the model's ability to perform logical\nreasoning over medical problems. Subsequently, on the basis of this foundation,\nwe adaptively optimize the retrieval capability to better align with the\ncharacteristics of knowledge corpus and external information utilization\nthroughout the reasoning process. Finally, we conduct joint optimization of the\nmodel's retrieval and reasoning coordination. Extensive experiments indicate\nthat **Med-R$^3$** could achieve state-of-the-art performances, with\nLLaMA3.1-8B-Instruct + Med-R$^3$ surpassing closed-sourced GPT-4o-mini by\n3.93\\% at a comparable parameter scale, while Qwen2.5-14B augmented with\nMed-R$^3$ shows a more substantial gain of 13.53\\%.", "AI": {"tldr": "Med-R$^3$ is a framework combining retrieval-augmented reasoning with reinforcement learning for improved medical problem-solving.", "motivation": "The framework aims to address the limitations of current methods that separately optimize retrieval and reasoning, which restricts model generalization in medical contexts.", "method": "The authors develop a model to perform logical reasoning on medical problems and then adaptively optimize its retrieval capabilities while jointly optimizing both retrieval and reasoning processes.", "result": "Med-R$^3$ achieves state-of-the-art performance, surpassing GPT-4o-mini by 3.93% and showing a 13.53% improvement with Qwen2.5-14B.", "conclusion": "The joint optimization in Med-R$^3$ significantly enhances both retrieval and reasoning in medical applications, demonstrating better performance compared to existing models.", "key_contributions": ["Introduction of the Med-R$^3$ framework for medical retrieval-augmented reasoning.", "Joint optimization approach to improve retrieval and reasoning interaction.", "Enhanced model performance with reinforcement learning in the medical domain."], "limitations": "", "keywords": ["medical retrieval", "augmented reasoning", "reinforcement learning", "logical reasoning", "knowledge corpus"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.23577", "pdf": "https://arxiv.org/pdf/2507.23577.pdf", "abs": "https://arxiv.org/abs/2507.23577", "title": "T-Detect: Tail-Aware Statistical Normalization for Robust Detection of Adversarial Machine-Generated Text", "authors": ["Alva West", "Luodan Zhang", "Liuliu Zhang", "Minjun Zhu", "Yixuan Weng", "Yue Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "The proliferation of sophisticated text generation models necessitates the\ndevelopment of robust detection methods capable of identifying\nmachine-generated content, particularly text designed to evade detection\nthrough adversarial perturbations. Existing zero-shot detectors often rely on\nstatistical measures that implicitly assume Gaussian distributions, a premise\nthat falters when confronted with the heavy-tailed statistical artifacts\ncharacteristic of adversarial or non-native English texts. This paper\nintroduces T-Detect, a novel detection method that fundamentally redesigns the\nstatistical core of curvature-based detectors. Our primary innovation is the\nreplacement of standard Gaussian normalization with a heavy-tailed discrepancy\nscore derived from the Student's t-distribution. This approach is theoretically\ngrounded in the empirical observation that adversarial texts exhibit\nsignificant leptokurtosis, rendering traditional statistical assumptions\ninadequate. T-Detect computes a detection score by normalizing the\nlog-likelihood of a passage against the expected moments of a t-distribution,\nproviding superior resilience to statistical outliers. We validate our approach\non the challenging RAID benchmark for adversarial text and the comprehensive\nHART dataset. Experiments show that T-Detect provides a consistent performance\nuplift over strong baselines, improving AUROC by up to 3.9\\% in targeted\ndomains. When integrated into a two-dimensional detection framework (CT), our\nmethod achieves state-of-the-art performance, with an AUROC of 0.926 on the\nBooks domain of RAID. Our contributions are a new, theoretically-justified\nstatistical foundation for text detection, an ablation-validated method that\ndemonstrates superior robustness, and a comprehensive analysis of its\nperformance under adversarial conditions. Ours code are released at\nhttps://github.com/ResearAI/t-detect.", "AI": {"tldr": "Introducing T-Detect, a novel method for detecting machine-generated text, designed to be robust against adversarial perturbations by utilizing a heavy-tailed statistical approach based on the Student's t-distribution.", "motivation": "Develop robust detection methods to identify machine-generated content that can evade existing detection systems through adversarial techniques.", "method": "T-Detect replaces standard Gaussian normalization with a heavy-tailed discrepancy score derived from the Student's t-distribution, enhancing resilience to statistical outliers.", "result": "T-Detect improves AUROC by up to 3.9% over strong baselines on the RAID benchmark and achieves state-of-the-art performance with an AUROC of 0.926 on the Books domain of RAID.", "conclusion": "T-Detect provides a theoretically justified and effective method for the detection of adversarial texts, validated through extensive experiments.", "key_contributions": ["New statistical foundation for text detection", "Ablation-validated method demonstrating superior robustness", "Comprehensive performance analysis under adversarial conditions"], "limitations": "", "keywords": ["text generation", "detection methods", "heavy-tailed statistics"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.23588", "pdf": "https://arxiv.org/pdf/2507.23588.pdf", "abs": "https://arxiv.org/abs/2507.23588", "title": "DiffLoRA: Differential Low-Rank Adapters for Large Language Models", "authors": ["Alexandre Misrahi", "Nadezhda Chirkova", "Maxime Louis", "Vassilina Nikoulina"], "categories": ["cs.CL"], "comment": null, "summary": "Differential Transformer has recently been proposed to improve performance in\nTransformer models by canceling out noise through a denoiser attention\nmechanism. In this work, we introduce DiffLoRA, a parameter-efficient\nadaptation of the differential attention mechanism, with low-rank adapters on\nboth positive and negative attention terms. This approach retains the\nefficiency of LoRA while aiming to benefit from the performance gains of\ndifferential attention. We evaluate DiffLoRA across a broad range of NLP tasks,\nincluding general benchmarks, many-shot in-context learning, RAG, and\nlong-context tests. We observe that, although DiffLoRA falls short of other\nparameter-efficient fine-tuning methods in most evaluation tasks, it shows\ninteresting results in certain domains (+11 pts on LoRA for HumanEval). We\nanalyze the attention patterns post-finetuning to identify the reasons for this\nbehavior.", "AI": {"tldr": "DiffLoRA introduces a parameter-efficient adaptation of differential attention for Transformers, leveraging low-rank adapters to enhance performance in NLP tasks.", "motivation": "To improve performance in Transformer models by addressing noise through a denoiser attention mechanism.", "method": "DiffLoRA combines differential attention with low-rank adapters on both positive and negative attention terms.", "result": "DiffLoRA shows mixed results across NLP tasks, demonstrating improvements in specific areas, such as a +11 pts performance gain on LoRA for HumanEval.", "conclusion": "While DiffLoRA is less effective compared to other fine-tuning methods in many tasks, it yields noteworthy results in certain domains.", "key_contributions": ["Introduction of DiffLoRA as a parameter-efficient adaptation", "Utilization of low-rank adapters in differential attention", "Demonstration of specific performance gains in NLP tasks"], "limitations": "DiffLoRA underperforms in many evaluation tasks compared to other parameter-efficient methods.", "keywords": ["Differential Transformer", "LoRA", "NLP tasks", "attention mechanism", "parameter-efficient"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.23661", "pdf": "https://arxiv.org/pdf/2507.23661.pdf", "abs": "https://arxiv.org/abs/2507.23661", "title": "Arabic Hate Speech Identification and Masking in Social Media using Deep Learning Models and Pre-trained Models Fine-tuning", "authors": ["Salam Thabet Doghmash", "Motaz Saad"], "categories": ["cs.CL", "I.2.7"], "comment": "23 pages, 5 figures", "summary": "Hate speech identification in social media has become an increasingly\nimportant issue in recent years. In this research, we address two problems: 1)\nto detect hate speech in Arabic text, 2) to clean a given text from hate\nspeech. The meaning of cleaning here is replacing each bad word with stars\nbased on the number of letters for each word. Regarding the first problem, we\nconduct several experiments using deep learning models and transformers to\ndetermine the best model in terms of the F1 score. Regarding second problem, we\nconsider it as a machine translation task, where the input is a sentence\ncontaining dirty text and the output is the same sentence with masking the\ndirty text. The presented methods achieve the best model in hate speech\ndetection with a 92\\% Macro F1 score and 95\\% accuracy. Regarding the text\ncleaning experiment, the best result in the hate speech masking model reached\n0.3 in BLEU score with 1-gram, which is a good result compared with the state\nof the art machine translation systems.", "AI": {"tldr": "This paper addresses detection and cleaning of hate speech in Arabic text using deep learning and machine translation techniques.", "motivation": "The rise of hate speech on social media necessitates effective detection and cleaning methods, particularly for Arabic text.", "method": "Deep learning models and transformers were employed to detect hate speech, while a separate approach treated text cleaning as a machine translation task.", "result": "Achieved a 92% Macro F1 score and 95% accuracy for hate speech detection, and a BLEU score of 0.3 for text cleaning.", "conclusion": "The proposed methods outperformed existing systems in both detecting and cleaning hate speech, demonstrating their effectiveness.", "key_contributions": ["Development of a hate speech detection model for Arabic text achieving high accuracy", "Introduction of a novel approach for cleaning hate speech through text masking", "Demonstration of deep learning and transformer capabilities in natural language processing tasks"], "limitations": "", "keywords": ["hate speech", "Arabic text", "machine learning", "deep learning", "text cleaning"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2507.23740", "pdf": "https://arxiv.org/pdf/2507.23740.pdf", "abs": "https://arxiv.org/abs/2507.23740", "title": "Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs", "authors": ["Nasim Shirvani-Mahdavi", "Devin Wingfield", "Amin Ghasemi", "Chengkai Li"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Knowledge graphs (KGs) often contain sufficient information to support the\ninference of new facts. Identifying logical rules not only improves the\ncompleteness of a knowledge graph but also enables the detection of potential\nerrors, reveals subtle data patterns, and enhances the overall capacity for\nreasoning and interpretation. However, the complexity of such rules, combined\nwith the unique labeling conventions of each KG, can make them difficult for\nhumans to understand. In this paper, we explore the potential of large language\nmodels to generate natural language explanations for logical rules.\nSpecifically, we extract logical rules using the AMIE 3.5.1 rule discovery\nalgorithm from the benchmark dataset FB15k-237 and two large-scale datasets,\nFB-CVT-REV and FB+CVT-REV. We examine various prompting strategies, including\nzero- and few-shot prompting, including variable entity types, and\nchain-of-thought reasoning. We conduct a comprehensive human evaluation of the\ngenerated explanations based on correctness, clarity, and hallucination, and\nalso assess the use of large language models as automatic judges. Our results\ndemonstrate promising performance in terms of explanation correctness and\nclarity, although several challenges remain for future research. All scripts\nand data used in this study are publicly available at\nhttps://github.com/idirlab/KGRule2NL}{https://github.com/idirlab/KGRule2NL.", "AI": {"tldr": "This paper investigates the use of large language models to generate natural language explanations for logical rules extracted from knowledge graphs, improving understanding and reasoning.", "motivation": "The complexity of logical rules in knowledge graphs hinders human understanding, thus enhancing their interpretability through natural language explanations is valuable.", "method": "The authors apply AMIE 3.5.1 to extract rules from datasets including FB15k-237 and employ various prompting strategies with large language models for explanation generation.", "result": "The generated explanations show promising correctness and clarity, but some challenges related to hallucination and complexity persist.", "conclusion": "While the results indicate potential for large language models in generating understandable explanations for knowledge graph rules, further research is needed to overcome existing challenges.", "key_contributions": ["Explored the generation of natural language explanations for logical rules in knowledge graphs using LLMs.", "Conducted comprehensive human evaluations of explanation clarity and correctness.", "Provided public access to scripts and datasets used in the study."], "limitations": "Remaining challenges with explanation correctness and instances of hallucination need to be addressed in future work.", "keywords": ["knowledge graphs", "explanation generation", "large language models", "logical rules", "natural language processing"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.23776", "pdf": "https://arxiv.org/pdf/2507.23776.pdf", "abs": "https://arxiv.org/abs/2507.23776", "title": "Cascaded Information Disclosure for Generalized Evaluation of Problem Solving Capabilities", "authors": ["Yunxiang Yan", "Tomohiro Sawada", "Kartik Goyal"], "categories": ["cs.CL"], "comment": "Under review", "summary": "While question-answering~(QA) benchmark performance is an automatic and\nscalable method to compare LLMs, it is an indirect method of evaluating their\nunderlying problem-solving capabilities. Therefore, we propose a holistic and\ngeneralizable framework based on \\emph{cascaded question disclosure} that\nprovides a more accurate estimate of the models' problem-solving capabilities\nwhile maintaining the scalability and automation. This approach collects model\nresponses in a stagewise manner with each stage revealing partial information\nabout the question designed to elicit generalized reasoning in LLMs. We find\nthat our approach not only provides a better comparison between LLMs, but also\ninduces better intermediate traces in models compared to the standard QA\nparadigm. We empirically verify this behavior on diverse reasoning and\nknowledge-heavy QA datasets by comparing LLMs of varying sizes and families.\nOur approach narrows the performance gap observed in the standard QA evaluation\nsettings, indicating that the prevalent indirect QA paradigm of evaluation\noverestimates the differences in performance between models. We further\nvalidate our findings by extensive ablation studies.", "AI": {"tldr": "The paper proposes a new framework for evaluating LLMs' problem-solving capabilities through cascaded question disclosure, which reveals information stagewise to enable better reasoning assessment.", "motivation": "To provide a more accurate and scalable method for evaluating the problem-solving capabilities of LLMs, as traditional QA benchmarks are indirect and may overestimate performance differences.", "method": "A cascaded question disclosure framework that collects model responses in stages, revealing partial information to facilitate generalized reasoning in LLMs.", "result": "Empirical validation shows improved comparisons of LLM performance and better intermediate traces in model responses compared to standard QA methods across various datasets.", "conclusion": "The proposed framework narrows performance gaps seen in traditional QA evaluations, suggesting they may not accurately reflect true model capabilities.", "key_contributions": ["Introduction of cascaded question disclosure for LLM evaluation", "Improved methods to assess LLM problem-solving capabilities", "Ablation studies validate findings and framework effectiveness"], "limitations": "", "keywords": ["LLM Evaluation", "Cascaded Question Disclosure", "Problem-Solving", "Generalized Reasoning", "QA Benchmarking"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.22892", "pdf": "https://arxiv.org/pdf/2507.22892.pdf", "abs": "https://arxiv.org/abs/2507.22892", "title": "Hybrid EEG--Driven Brain--Computer Interface: A Large Language Model Framework for Personalized Language Rehabilitation", "authors": ["Ismail Hossain", "Mridul Banik"], "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "Conventional augmentative and alternative communication (AAC) systems and\nlanguage-learning platforms often fail to adapt in real time to the user's\ncognitive and linguistic needs, especially in neurological conditions such as\npost-stroke aphasia or amyotrophic lateral sclerosis. Recent advances in\nnoninvasive electroencephalography (EEG)--based brain-computer interfaces\n(BCIs) and transformer--based large language models (LLMs) offer complementary\nstrengths: BCIs capture users' neural intent with low fatigue, while LLMs\ngenerate contextually tailored language content. We propose and evaluate a\nnovel hybrid framework that leverages real-time EEG signals to drive an\nLLM-powered language rehabilitation assistant. This system aims to: (1) enable\nusers with severe speech or motor impairments to navigate language-learning\nmodules via mental commands; (2) dynamically personalize vocabulary,\nsentence-construction exercises, and corrective feedback; and (3) monitor\nneural markers of cognitive effort to adjust task difficulty on the fly.", "AI": {"tldr": "This paper proposes a hybrid framework utilizing EEG signals and LLMs for language rehabilitation in users with severe impairments.", "motivation": "Conventional AAC systems lack real-time adaptability to users' cognitive and linguistic needs, particularly for those with neurological conditions.", "method": "The proposed framework integrates EEG-based BCIs to interpret neural intent and LLMs to provide dynamic language-learning content.", "result": "Preliminary evaluations indicate that the hybrid system effectively personalizes language exercises and monitors cognitive effort in real-time.", "conclusion": "The integration of EEG with LLMs holds promise for enhancing AAC systems, enabling better support for users with severe speech or motor impairments.", "key_contributions": ["Development of a hybrid framework combining EEG and LLMs for language rehabilitation.", "Real-time personalization of language exercises based on user cognitive state.", "Dynamic adjustment of task difficulty based on neural markers."], "limitations": "The current study may require larger sample sizes for robust validation and long-term adaptation assessments.", "keywords": ["Augmentative and Alternative Communication", "Brain-Computer Interfaces", "Large Language Models"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.22898", "pdf": "https://arxiv.org/pdf/2507.22898.pdf", "abs": "https://arxiv.org/abs/2507.22898", "title": "Voice-guided Orchestrated Intelligence for Clinical Evaluation (VOICE): A Voice AI Agent System for Prehospital Stroke Assessment", "authors": ["Julian Acosta", "Scott Adams", "Julius Kernbach", "Romain Hardy", "Sung Eun Kim", "Luyang Luo", "Xiaoman Zhang", "Shreya Johri", "Mohammed Baharoon", "Pranav Rajpurkar"], "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "We developed a voice-driven artificial intelligence (AI) system that guides\nanyone - from paramedics to family members - through expert-level stroke\nevaluations using natural conversation, while also enabling smartphone video\ncapture of key examination components for documentation and potential expert\nreview. This addresses a critical gap in emergency care: current stroke\nrecognition by first responders is inconsistent and often inaccurate, with\nsensitivity for stroke detection as low as 58%, causing life-threatening delays\nin treatment. Three non-medical volunteers used our AI system to assess ten\nsimulated stroke patients, including cases with likely large vessel occlusion\n(LVO) strokes and stroke-like conditions, while we measured diagnostic\naccuracy, completion times, user confidence, and expert physician review of the\nAI-generated reports. The AI system correctly identified 84% of individual\nstroke signs and detected 75% of likely LVOs, completing evaluations in just\nover 6 minutes. Users reported high confidence (median 4.5/5) and ease of use\n(mean 4.67/5). The system successfully identified 86% of actual strokes but\nalso incorrectly flagged 2 of 3 non-stroke cases as strokes. When an expert\nphysician reviewed the AI reports with videos, they identified the correct\ndiagnosis in 100% of cases, but felt confident enough to make preliminary\ntreatment decisions in only 40% of cases due to observed AI errors including\nincorrect scoring and false information. While the current system's limitations\nnecessitate human oversight, ongoing rapid advancements in speech-to-speech AI\nmodels suggest that future versions are poised to enable highly accurate\nassessments. Achieving human-level voice interaction could transform emergency\nmedical care, putting expert-informed assessment capabilities in everyone's\nhands.", "AI": {"tldr": "Development of a voice-driven AI system to improve stroke evaluations in emergency care.", "motivation": "Current stroke recognition by first responders is inconsistent and often inaccurate, leading to delays in treatment.", "method": "Non-medical volunteers used the AI system to assess simulated stroke patients and measured accuracy, completion times, user confidence, and expert reviews.", "result": "The AI system identified 84% of stroke signs and 75% of likely LVOs, with overall stroke detection accuracy at 86%.", "conclusion": "While current limitations require human oversight, advancements in AI may soon enable highly accurate assessments and improve emergency care.", "key_contributions": ["Voice-driven AI for stroke evaluation", "High accuracy in identifying stroke signs", "Documenting evaluations through smartphone video"], "limitations": "Human oversight is necessary due to AI errors affecting scoring and false information.", "keywords": ["Stroke evaluation", "Voice-driven AI", "Emergency care", "Diagnostic accuracy", "Human oversight"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.22902", "pdf": "https://arxiv.org/pdf/2507.22902.pdf", "abs": "https://arxiv.org/abs/2507.22902", "title": "Toward the Autonomous AI Doctor: Quantitative Benchmarking of an Autonomous Agentic AI Versus Board-Certified Clinicians in a Real World Setting", "authors": ["Hashim Hayat", "Maksim Kudrautsau", "Evgeniy Makarov", "Vlad Melnichenko", "Tim Tsykunou", "Piotr Varaksin", "Matt Pavelle", "Adam Z. Oskowitz"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.MA"], "comment": null, "summary": "Background: Globally we face a projected shortage of 11 million healthcare\npractitioners by 2030, and administrative burden consumes 50% of clinical time.\nArtificial intelligence (AI) has the potential to help alleviate these\nproblems. However, no end-to-end autonomous large language model (LLM)-based AI\nsystem has been rigorously evaluated in real-world clinical practice. In this\nstudy, we evaluated whether a multi-agent LLM-based AI framework can function\nautonomously as an AI doctor in a virtual urgent care setting. Methods: We\nretrospectively compared the performance of the multi-agent AI system Doctronic\nand board-certified clinicians across 500 consecutive urgent-care telehealth\nencounters. The primary end points: diagnostic concordance, treatment plan\nconsistency, and safety metrics, were assessed by blinded LLM-based\nadjudication and expert human review. Results: The top diagnosis of Doctronic\nand clinician matched in 81% of cases, and the treatment plan aligned in 99.2%\nof cases. No clinical hallucinations occurred (e.g., diagnosis or treatment not\nsupported by clinical findings). In an expert review of discordant cases, AI\nperformance was superior in 36.1%, and human performance was superior in 9.3%;\nthe diagnoses were equivalent in the remaining cases. Conclusions: In this\nfirst large-scale validation of an autonomous AI doctor, we demonstrated strong\ndiagnostic and treatment plan concordance with human clinicians, with AI\nperformance matching and in some cases exceeding that of practicing clinicians.\nThese findings indicate that multi-agent AI systems achieve comparable clinical\ndecision-making to human providers and offer a potential solution to healthcare\nworkforce shortages.", "AI": {"tldr": "A study evaluating a multi-agent AI framework as an autonomous AI doctor in urgent care describes strong performance in diagnostics and treatment parallels with human clinicians.", "motivation": "The study addresses the projected shortage of healthcare practitioners and aims to explore the potential of AI to alleviate the administrative burden in clinical settings by evaluating an autonomous LLM-based AI system in a real-world context.", "method": "The study retrospectively compared the multi-agent AI system Doctronic with board-certified clinicians through 500 consecutive urgent-care telehealth encounters, focusing on diagnostic concordance, treatment plan consistency, and safety metrics.", "result": "Doctronic matched clinicians in the top diagnosis 81% of the time and aligned treatment plans in 99.2% of cases, with no hallucinations noted. AI outperformed human clinicians in 36.1% of discordant cases.", "conclusion": "The study shows that autonomous AI systems can achieve diagnostic and treatment decision-making levels comparable to human healthcare providers, addressing workforce shortages.", "key_contributions": ["First large-scale validation of an autonomous AI doctor", "Demonstrated strong diagnostic and treatment alignment with clinicians", "Highlighted the potential of multi-agent AI to ease healthcare workforce shortage"], "limitations": "", "keywords": ["Artificial Intelligence", "Health Informatics", "Large Language Models", "Telehealth", "Clinical Decision-Making"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2404.12829", "pdf": "https://arxiv.org/pdf/2404.12829.pdf", "abs": "https://arxiv.org/abs/2404.12829", "title": "LiMe: a Latin Corpus of Late Medieval Criminal Sentences", "authors": ["Alessandra Bassani", "Beatrice Del Bo", "Alfio Ferrara", "Marta Mangini", "Sergio Picascia", "Ambra Stefanello"], "categories": ["cs.CL"], "comment": null, "summary": "The Latin language has received attention from the computational linguistics\nresearch community, which has built, over the years, several valuable\nresources, ranging from detailed annotated corpora to sophisticated tools for\nlinguistic analysis. With the recent advent of large language models,\nresearchers have also started developing models capable of generating vector\nrepresentations of Latin texts. The performances of such models remain behind\nthe ones for modern languages, given the disparity in available data. In this\npaper, we present the LiMe dataset, a corpus of 325 documents extracted from a\nseries of medieval manuscripts called Libri sententiarum potestatis Mediolani,\nand thoroughly annotated by experts, in order to be employed for masked\nlanguage model, as well as supervised natural language processing tasks.", "AI": {"tldr": "Introduction of the LiMe dataset for Latin texts to enhance NLP tasks.", "motivation": "To improve NLP resources and model performance for the Latin language, which lags behind modern languages due to less available data.", "method": "The paper presents the construction of the LiMe dataset, consisting of 325 expert-annotated documents from medieval manuscripts for NLP applications.", "result": "The dataset is suitable for masked language models and supervised NLP tasks, enhancing the capabilities of Latin text processing.", "conclusion": "The LiMe dataset will contribute to advancing NLP techniques for Latin, bridging the gap with modern language models.", "key_contributions": ["Introduction of the LiMe dataset for Latin texts", "Expert annotation of historical documents", "Enhancement of NLP resources for Latin language"], "limitations": "", "keywords": ["Latin", "NLP", "dataset", "masked language model", "corpus"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2404.18154", "pdf": "https://arxiv.org/pdf/2404.18154.pdf", "abs": "https://arxiv.org/abs/2404.18154", "title": "Explaining vague language", "authors": ["Paul Égré", "Benjamin Spector"], "categories": ["cs.CL", "cs.GT", "cs.IT", "math.IT", "91A86", "I.2.7"], "comment": null, "summary": "Why is language vague? Vagueness may be explained and rationalized if it can\nbe shown that vague language is more useful to speaker and hearer than precise\nlanguage. In a well-known paper, Lipman proposes a game-theoretic account of\nvagueness in terms of mixed strategy that leads to a puzzle: vagueness cannot\nbe strictly better than precision at equilibrium. More recently, \\'Egr\\'e,\nSpector, Mortier and Verheyen have put forward a Bayesian account of vagueness\nestablishing that using vague words can be strictly more informative than using\nprecise words. This paper proposes to compare both results and to explain why\nthey are not in contradiction. Lipman's definition of vagueness relies\nexclusively on a property of signaling strategies, without making any\nassumptions about the lexicon, whereas \\'Egr\\'e et al.'s involves a layer of\nsemantic content. We argue that the semantic account of vagueness is needed,\nand more adequate and explanatory of vagueness.", "AI": {"tldr": "This paper compares game-theoretic and Bayesian accounts of vagueness in language, arguing the necessity of a semantic understanding of vagueness.", "motivation": "To explore why vague language may be more useful than precise language by comparing established theories.", "method": "The paper analyzes the game-theoretic approach of Lipman and the Bayesian approach of 'Egr'é et al. to reconcile their findings on vagueness.", "result": "The paper concludes that a semantic account is essential for understanding vagueness, as it provides better insight than purely signaling strategies.", "conclusion": "Understanding vagueness in terms of semantics rather than just signaling strategies allows for a more comprehensive explanation of its utility.", "key_contributions": ["Comparison of game-theoretic and Bayesian frameworks for vagueness", "Arguments for the necessity of semantic considerations in vagueness", "Resolution of apparent contradictions between existing theories"], "limitations": "", "keywords": ["vagueness", "language", "semantic account", "game-theoretic", "Bayesian"], "importance_score": 2, "read_time_minutes": 15}}
{"id": "2406.14313", "pdf": "https://arxiv.org/pdf/2406.14313.pdf", "abs": "https://arxiv.org/abs/2406.14313", "title": "Iterative Repair with Weak Verifiers for Few-shot Transfer in KBQA with Unanswerability", "authors": ["Riya Sawhney", "Samrat Yadav", "Indrajit Bhattacharya", "Mausam"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Real-world applications of KBQA require models to handle unanswerable\nquestions with a limited volume of in-domain labeled training data. We propose\nthe novel task of few-shot transfer for KBQA with unanswerable questions and\ncontribute two new datasets for performance evaluation. We present FUn-FuSIC -\na novel solution for our task that extends FuSIC KBQA, the state-of-the-art\nfew-shot transfer model for answerable-only KBQA. We first note that\nFuSIC-KBQA's iterative repair makes a strong assumption that all questions are\nunanswerable. As a remedy, we propose Feedback for Unanswerability (FUn), which\nuses iterative repair using feedback from a suite of strong and weak verifiers,\nand an adaptation of self consistency for unanswerabilty to better assess the\nanswerability of a question. Our experiments show that FUn-FuSIC significantly\noutperforms suitable adaptations of multiple LLM based and supervised SoTA\nmodels on our task, while establishing a new SoTA for answerable few-shot\ntransfer as well.", "AI": {"tldr": "The paper introduces the task of few-shot transfer for Knowledge Base Question Answering (KBQA) that addresses unanswerable questions, proposing a novel solution FUn-FuSIC and new datasets for evaluation.", "motivation": "To improve KBQA models' ability to handle unanswerable questions with limited labeled training data.", "method": "The proposed method, FUn-FuSIC, enhances the existing FuSIC model by incorporating Feedback for Unanswerability (FUn), implementing iterative repair techniques and self-consistency for determining question answerability.", "result": "FUn-FuSIC significantly outperforms existing LLM-based and supervised state-of-the-art models in addressing unanswerable questions in KBQA, while achieving a new state-of-the-art for answerable few-shot transfer.", "conclusion": "The approach shows promise in handling few-shot KBQA with unanswerable questions and sets new benchmarks in the field.", "key_contributions": ["Introduction of the few-shot transfer task for KBQA with unanswerable questions.", "Development of the FUn-FuSIC model that integrates feedback mechanisms for unanswerability.", "Creation of two new datasets for evaluating KBQA performance."], "limitations": "", "keywords": ["Knowledge Base Question Answering", "Few-shot transfer", "Unanswerable questions"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2406.15444", "pdf": "https://arxiv.org/pdf/2406.15444.pdf", "abs": "https://arxiv.org/abs/2406.15444", "title": "Cutting Through the Noise: Boosting LLM Performance on Math Word Problems", "authors": ["Ujjwala Anantheswaran", "Himanshu Gupta", "Kevin Scaria", "Shreyas Verma", "Chitta Baral", "Swaroop Mishra"], "categories": ["cs.CL"], "comment": "Published at ICLR 2025 Workshop on Reasoning and Planning for LLMs", "summary": "Large Language Models (LLMs) excel at various tasks, including solving math\nword problems (MWPs), but struggle with real-world problems containing\nirrelevant information. To address this, we propose a prompting framework that\ngenerates adversarial variants of MWPs by adding irrelevant variables. We\nintroduce a dataset, PROBLEMATHIC, containing both adversarial and\nnon-adversarial MWPs. Our experiments reveal that LLMs are susceptible to\ndistraction by numerical noise, resulting in an average relative performance\ndrop of ~26% on adversarial MWPs. To mitigate this, we fine-tune LLMs (Llama-2,\nMistral) on the adversarial samples from our dataset. Fine-tuning on\nadversarial training instances improves performance on adversarial MWPs by ~8%,\nindicating increased robustness to noise and improved ability to identify\nrelevant data for reasoning. Finally, to assess the generalizability of our\nprompting framework, we introduce GSM-8K-Adv, an adversarial variant of the\nGSM-8K benchmark. LLMs continue to struggle when faced with adversarial\ninformation, reducing performance by up to 6%.", "AI": {"tldr": "This paper presents a prompting framework to create adversarial math word problems (MWPs) that contain irrelevant information, revealing vulnerabilities in large language models (LLMs) and proposing a solution through fine-tuning.", "motivation": "To improve the performance of LLMs in solving real-world math word problems by addressing their vulnerability to irrelevant information.", "method": "The authors developed a prompting framework to generate adversarial MWPs and introduced a new dataset, PROBLEMATHIC, containing both types of problems. They conducted experiments and fine-tuned LLMs on adversarial samples to improve their robustness.", "result": "Experiments indicated a ~26% performance drop on adversarial MWPs, with fine-tuning showing an improvement of ~8% in handling these adversarial cases. Performance decreased by up to 6% when LLMs faced the adversarial GSM-8K benchmark.", "conclusion": "The prompting framework helps in assessing the robustness of LLMs against irrelevant data, and fine-tuning can enhance their performance on adversarial tasks.", "key_contributions": ["Introduction of the PROBLEMATHIC dataset for adversarial MWPs", "Creation of the prompting framework for generating adversarial problems", "Fine-tuning approach that improves LLM performance against distractions"], "limitations": "The study primarily focuses on adversarial MWPs, and further research is needed to explore other real-world applications and further validate the generalizability of results.", "keywords": ["Large Language Models", "math word problems", "adversarial training", "dataset", "HCI"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2412.11167", "pdf": "https://arxiv.org/pdf/2412.11167.pdf", "abs": "https://arxiv.org/abs/2412.11167", "title": "Cultural Palette: Pluralising Culture Alignment via Multi-agent Palette", "authors": ["Jiahao Yuan", "Zixiang Di", "Shangzixin Zhao", "Zhiqing Cui", "Hanqing Wang", "Guisong Yang", "Usman Naseem"], "categories": ["cs.CL"], "comment": "20 pages, 10 figures", "summary": "Large language models (LLMs) face challenges in aligning with diverse\ncultural values despite their remarkable performance in generation, which stems\nfrom inherent monocultural biases and difficulties in capturing nuanced\ncultural semantics. Existing methods struggle to adapt to unknown culture after\nfine-tuning. Inspired by cultural geography across five continents, we propose\nCultural Palette, a multi-agent framework that redefines cultural alignment as\nan adaptive \"color-blending\" process for country-specific adaptation. Our\napproach harnesses cultural geography across five continents (Africa, America,\nAsia, Europe, Oceania) through three key steps: First, we synthesize the\nPentachromatic Cultural Palette Dataset using GPT-4o, refining\ncontinental-level dialogues with Hofstede's cultural dimensions to establish\nfoundational cultural representations. Second, five continent-level alignment\nagents form specialized cultural communities that generate region-specific\ndraft responses. Third, a Meta Agent employs Cultural MoErges to dynamically\nblend these cultural \"colors\" through attention-gated parameter merging, akin\nto mixing pigments on a palette, resolving conflicts while preserving cultural\nnuances to produce the final culturally-aligned response. Extensive experiments\nacross various countries demonstrate that Cultural Palette surpasses existing\nbaselines in cultural alignment.", "AI": {"tldr": "The paper proposes a multi-agent framework called Cultural Palette for aligning large language models (LLMs) with diverse cultural values.", "motivation": "Despite the performance of LLMs, they struggle with cultural alignment due to monocultural biases and challenges in capturing nuanced cultural semantics.", "method": "The Cultural Palette framework synthesizes a dataset using GPT-4o, utilizes continent-level alignment agents for region-specific responses, and employs a Meta Agent for dynamic cultural blending via attention-gated parameter merging.", "result": "Cultural Palette outperforms existing baselines in cultural alignment through a novel adaptive color-blending process.", "conclusion": "The proposed method effectively resolves cultural conflicts and enhances LLM performance in generating culturally-aligned responses.", "key_contributions": ["Cultural Palette as a new multi-agent framework for cultural alignment", "Synthesis of the Pentachromatic Cultural Palette Dataset using GPT-4o", "Introduction of attention-gated parameter merging for cultural blending"], "limitations": "", "keywords": ["Cultural Alignment", "Large Language Models", "Multi-agent Framework", "Cultural Geography", "Attention-gated"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2503.15299", "pdf": "https://arxiv.org/pdf/2503.15299.pdf", "abs": "https://arxiv.org/abs/2503.15299", "title": "Inside-Out: Hidden Factual Knowledge in LLMs", "authors": ["Zorik Gekhman", "Eyal Ben David", "Hadas Orgad", "Eran Ofek", "Yonatan Belinkov", "Idan Szpektor", "Jonathan Herzig", "Roi Reichart"], "categories": ["cs.CL"], "comment": "Accepted to COLM 2025", "summary": "This work presents a framework for assessing whether large language models\n(LLMs) encode more factual knowledge in their parameters than what they express\nin their outputs. While a few studies hint at this possibility, none has\nclearly defined or demonstrated this phenomenon. We first propose a formal\ndefinition of knowledge, quantifying it for a given question as the fraction of\ncorrect-incorrect answer pairs where the correct one is ranked higher. This\ngives rise to external and internal knowledge, depending on the information\nused to score individual answer candidates: either the model's observable\ntoken-level probabilities or its intermediate computations. Hidden knowledge\narises when internal knowledge exceeds external knowledge. We then present a\ncase study, applying this framework to three popular open-weights LLMs in a\nclosed-book QA setup. Our results indicate that: (1) LLMs consistently encode\nmore factual knowledge internally than what they express externally, with an\naverage relative gap of 40%. (2) Surprisingly, some knowledge is so deeply\nhidden that a model can internally know an answer perfectly, yet fail to\ngenerate it even once, despite large-scale repeated sampling of 1,000 answers.\nThis reveals fundamental limitations in the generation capabilities of LLMs,\nwhich (3) put a practical constraint on scaling test-time compute via repeated\nanswer sampling in closed-book QA: significant performance improvements remain\ninaccessible because some answers are practically never sampled, yet if they\nwere, we would be guaranteed to rank them first.", "AI": {"tldr": "This paper presents a framework for quantifying the hidden knowledge in large language models (LLMs) by comparing internal knowledge to external expression. It shows that LLMs consistently encode more factual knowledge internally than they express outwardly.", "motivation": "To assess whether large language models encode more factual knowledge in their parameters than what they express in their outputs, providing a clearer understanding of their capabilities.", "method": "A formal definition of knowledge is proposed, quantifying it for specific questions and dividing it into external and internal knowledge. A case study is conducted on three open-weights LLMs in a closed-book QA setup.", "result": "The findings show that LLMs encode an average of 40% more factual knowledge internally compared to what they generate externally. Some knowledge remains deeply hidden despite perfect internal knowledge, and practical constraints limit performance improvements in QA settings.", "conclusion": "The paper concludes that there are significant limitations to the generation capabilities of LLMs, revealing challenges in scaling their test-time computation effectively due to hidden knowledge.", "key_contributions": ["Proposes a formal definition and quantification of knowledge in LLMs", "Demonstrates through a case study that LLMs often encode more knowledge than they express", "Highlights practical implications for closed-book QA tasks"], "limitations": "The study focuses on open-weights LLMs in a closed-book QA setup, which may not generalize across all types of LLMs or interactions.", "keywords": ["large language models", "knowledge quantification", "closed-book Q&A", "HCI", "machine learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2503.15768", "pdf": "https://arxiv.org/pdf/2503.15768.pdf", "abs": "https://arxiv.org/abs/2503.15768", "title": "Can one size fit all?: Measuring Failure in Multi-Document Summarization Domain Transfer", "authors": ["Alexandra DeLucia", "Mark Dredze"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Abstractive multi-document summarization (MDS) is the task of automatically\nsummarizing information in multiple documents, from news articles to\nconversations with multiple speakers. The training approaches for current MDS\nmodels can be grouped into four approaches: end-to-end with special\npre-training (\"direct\"), chunk-then-summarize, extract-then-summarize, and\ninference with GPT-style models. In this work, we evaluate MDS models across\ntraining approaches, domains, and dimensions (reference similarity, quality,\nand factuality), to analyze how and why models trained on one domain can fail\nto summarize documents from another (News, Science, and Conversation) in the\nzero-shot domain transfer setting. We define domain-transfer \"failure\" as a\ndecrease in factuality, higher deviation from the target, and a general\ndecrease in summary quality. In addition to exploring domain transfer for MDS\nmodels, we examine potential issues with applying popular summarization metrics\nout-of-the-box.", "AI": {"tldr": "This paper assesses various approaches for multi-document summarization (MDS) and their effectiveness in zero-shot domain transfer scenarios.", "motivation": "The need for robust multi-document summarization models that can generalize across different domains, such as news, science, and conversation.", "method": "Evaluation of MDS models using four training approaches and across different document domains analyzing reference similarity, quality, and factuality.", "result": "Models trained in one domain often suffer in factuality and summary quality when applied to other domains in a zero-shot context.", "conclusion": "Domain transfer failure is identified by decreased factuality and summary quality, prompting a need for better evaluation metrics.", "key_contributions": ["Comprehensive evaluation of MDS model training approaches.", "Identification of domain transfer issues in MDS.", "Critique of existing summarization metrics for out-of-the-box use."], "limitations": "", "keywords": ["multi-document summarization", "domain transfer", "factuality", "summary quality", "evaluation metrics"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2504.04640", "pdf": "https://arxiv.org/pdf/2504.04640.pdf", "abs": "https://arxiv.org/abs/2504.04640", "title": "Splits! A Flexible Dataset and Evaluation Framework for Sociocultural Linguistic Investigation", "authors": ["Eylon Caplan", "Tania Chakraborty", "Dan Goldwasser"], "categories": ["cs.CL", "cs.AI"], "comment": "Preprint, under review", "summary": "Variation in language use, shaped by speakers' sociocultural background and\nspecific context of use, offers a rich lens into cultural perspectives, values,\nand opinions. However, the computational study of these Sociocultural\nLinguistic Phenomena (SLP) has often been limited to bespoke analyses of\nspecific groups or topics, hindering the pace of scientific discovery. To\naddress this, we introduce Splits!, a 9.7 million-post dataset from Reddit\ndesigned for systematic and flexible research. The dataset contains posts from\nover 53,000 users across 6 demographic groups, organized into 89 discussion\ntopics to enable comparative analysis. We validate Splits! via\nself-identification and by successfully replicating several known SLPs from\nexisting literature. We complement this dataset with a framework that leverages\nefficient retrieval methods to rapidly validate potential SLPs (PSLPs) by\nautomatically evaluating whether a given hypothesis is supported by our data.\nCrucially, to distinguish between novel and obvious insights, the framework\nincorporates a human-validated measure of a hypothesis's ``unexpectedness.'' We\ndemonstrate that the two-stage process reduces the number of statistically\nsignificant findings requiring manual inspection by a factor of 1.5-1.8x,\nstreamlining the discovery of promising phenomena for further investigation.", "AI": {"tldr": "The paper introduces Splits!, a large dataset from Reddit for studying Sociocultural Linguistic Phenomena (SLP) with a framework for validating hypotheses using this data.", "motivation": "To facilitate the computational study of Sociocultural Linguistic Phenomena (SLP) and overcome limitations of previous bespoke analyses.", "method": "A dataset of 9.7 million Reddit posts from over 53,000 users across 6 demographic groups, organized into 89 topics, is created and validated. A framework to evaluate hypotheses using efficient retrieval methods is introduced.", "result": "The dataset and framework allow for systematic comparative analysis and successfully replicate known SLPs. The framework also reduces the number of findings needing manual inspection by 1.5-1.8 times.", "conclusion": "The integration of the Splits! dataset with a hypothesis validation framework enhances the study of cultural perspectives in language use.", "key_contributions": ["Creation of the Splits! dataset for SLP research", "Introduction of a framework for validating Sociocultural Linguistic hypotheses", "Demonstration of improved efficiency in identifying significant findings"], "limitations": "", "keywords": ["Sociocultural Linguistic Phenomena", "Reddit dataset", "Hypothesis validation"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.23628", "pdf": "https://arxiv.org/pdf/2505.23628.pdf", "abs": "https://arxiv.org/abs/2505.23628", "title": "AutoSchemaKG: Autonomous Knowledge Graph Construction through Dynamic Schema Induction from Web-Scale Corpora", "authors": ["Jiaxin Bai", "Wei Fan", "Qi Hu", "Qing Zong", "Chunyang Li", "Hong Ting Tsang", "Hongyu Luo", "Yauwai Yim", "Haoyu Huang", "Xiao Zhou", "Feng Qin", "Tianshi Zheng", "Xi Peng", "Xin Yao", "Huiwen Yang", "Leijie Wu", "Yi Ji", "Gong Zhang", "Renhai Chen", "Yangqiu Song"], "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, preprint, code:\n  https://github.com/HKUST-KnowComp/AutoSchemaKG", "summary": "We present AutoSchemaKG, a framework for fully autonomous knowledge graph\nconstruction that eliminates the need for predefined schemas. Our system\nleverages large language models to simultaneously extract knowledge triples and\ninduce comprehensive schemas directly from text, modeling both entities and\nevents while employing conceptualization to organize instances into semantic\ncategories. Processing over 50 million documents, we construct ATLAS (Automated\nTriple Linking And Schema induction), a family of knowledge graphs with 900+\nmillion nodes and 5.9 billion edges. This approach outperforms state-of-the-art\nbaselines on multi-hop QA tasks and enhances LLM factuality. Notably, our\nschema induction achieves 95\\% semantic alignment with human-crafted schemas\nwith zero manual intervention, demonstrating that billion-scale knowledge\ngraphs with dynamically induced schemas can effectively complement parametric\nknowledge in large language models.", "AI": {"tldr": "A framework called AutoSchemaKG enables autonomous knowledge graph construction without predefined schemas, using large language models to extract knowledge and create schemas from text.", "motivation": "The need for effective and autonomous knowledge graph construction methods that do not rely on predefined schemas.", "method": "Leveraging large language models to extract knowledge triples and induce schemas from text while organizing them into semantic categories.", "result": "Automatically constructs ATLAS knowledge graphs containing over 900 million nodes and 5.9 billion edges, outperforming existing baselines in multi-hop QA tasks.", "conclusion": "The system shows that billions of dynamically induced schemas can enhance large language models' factuality without manual intervention.", "key_contributions": ["Introduction of AutoSchemaKG for autonomous knowledge graph construction", "Achievement of 95% semantic alignment with human schemas", "Demonstration of scalability with knowledge graphs exceeding 900 million nodes"], "limitations": "", "keywords": ["knowledge graph", "schema induction", "large language models", "autoamtic construction", "natural language processing"], "importance_score": 8, "read_time_minutes": 9}}
{"id": "2506.07106", "pdf": "https://arxiv.org/pdf/2506.07106.pdf", "abs": "https://arxiv.org/abs/2506.07106", "title": "Theorem-of-Thought: A Multi-Agent Framework for Abductive, Deductive, and Inductive Reasoning in Language Models", "authors": ["Samir Abdaljalil", "Hasan Kurban", "Khalid Qaraqe", "Erchin Serpedin"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 KnowFM", "summary": "Large language models (LLMs) have shown strong performance across natural\nlanguage reasoning tasks, yet their reasoning processes remain brittle and\ndifficult to interpret. Prompting techniques like Chain-of-Thought (CoT)\nenhance reliability by eliciting intermediate reasoning steps or aggregating\nmultiple outputs. However, they lack mechanisms for enforcing logical structure\nand assessing internal coherence. We introduce Theorem-of-Thought (ToTh), a\nnovel framework that models reasoning as collaboration among three parallel\nagents, each simulating a distinct mode of inference: abductive, deductive, and\ninductive. Each agent produces a reasoning trace, which is structured into a\nformal reasoning graph. To evaluate consistency, we apply Bayesian belief\npropagation guided by natural language inference (NLI), assigning confidence\nscores to each step. The most coherent graph is selected to derive the final\nanswer. Experiments on symbolic (WebOfLies) and numerical (MultiArith)\nreasoning benchmarks show that ToTh consistently outperforms CoT,\nSelf-Consistency, and CoT-Decoding across multiple LLMs, while producing\ninterpretable and logically grounded reasoning chains. Our findings suggest a\npromising direction for building more robust and cognitively inspired LLM\nreasoning. The implementation is available at\nhttps://github.com/KurbanIntelligenceLab/theorem-of-thought.", "AI": {"tldr": "Theorem-of-Thought (ToTh) enhances reasoning in large language models by employing three parallel inference agents and formal reasoning graphs, outperforming existing prompting techniques.", "motivation": "Large language models struggle with brittle and difficult-to-interpret reasoning processes, warranting approaches that improve logical structure and coherence assessment.", "method": "Theorem-of-Thought (ToTh) involves three parallel agents simulating abductive, deductive, and inductive inference, producing structured reasoning traces formalized into a reasoning graph. Bayesian belief propagation and natural language inference are applied to evaluate coherence and consistency.", "result": "ToTh consistently outperformed Chain-of-Thought, Self-Consistency, and CoT-Decoding on reasoning benchmarks (WebOfLies and MultiArith), providing interpretable and logically grounded reasoning chains.", "conclusion": "The findings indicate that ToTh offers a robust, cognitively inspired framework for enhancing LLM reasoning. Its implementation is publicly available.", "key_contributions": ["Introduction of the Theorem-of-Thought framework for LLM reasoning.", "Collaboration of three inference agents enhances logical structure.", "Application of Bayesian belief propagation for coherence assessment."], "limitations": "", "keywords": ["large language models", "reasoning", "Bayesian belief propagation", "inference", "natural language"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.16725", "pdf": "https://arxiv.org/pdf/2507.16725.pdf", "abs": "https://arxiv.org/abs/2507.16725", "title": "RAVine: Reality-Aligned Evaluation for Agentic Search", "authors": ["Yilong Xu", "Xiang Long", "Zhi Zheng", "Jinhua Gao"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Agentic search, as a more autonomous and adaptive paradigm of retrieval\naugmentation, is driving the evolution of intelligent search systems. However,\nexisting evaluation frameworks fail to align well with the goals of agentic\nsearch. First, the complex queries commonly used in current benchmarks often\ndeviate from realistic user search scenarios. Second, prior approaches tend to\nintroduce noise when extracting ground truth for end-to-end evaluations,\nleading to distorted assessments at a fine-grained level. Third, most current\nframeworks focus solely on the quality of final answers, neglecting the\nevaluation of the iterative process inherent to agentic search. To address\nthese limitations, we propose RAVine -- a Reality-Aligned eValuation framework\nfor agentic LLMs with search. RAVine targets multi-point queries and long-form\nanswers that better reflect user intents, and introduces an attributable ground\ntruth construction strategy to enhance the accuracy of fine-grained evaluation.\nMoreover, RAVine examines model's interaction with search tools throughout the\niterative process, and accounts for factors of efficiency. We benchmark a\nseries of models using RAVine and derive several insights, which we hope will\ncontribute to advancing the development of agentic search systems. The code and\ndatasets are available at https://github.com/SwordFaith/RAVine.", "AI": {"tldr": "RAVine is a new evaluation framework for agentic LLMs that addresses shortcomings in existing benchmarks by focusing on realistic user queries, improving ground truth extraction, and evaluating iterative search processes.", "motivation": "To improve the evaluation of agentic search systems, which have been inadequately assessed by existing frameworks that do not consider realistic user scenarios and the iterative nature of searches.", "method": "RAVine introduces multi-point queries and long-form answers to reflect user intents better, along with a new strategy for ground truth construction to enhance evaluation accuracy.", "result": "RAVine has been used to benchmark various models and provides insights that could advance the development of intelligent search systems.", "conclusion": "The implementation of RAVine could lead to better assessment frameworks that align more closely with real-world user intents and improve agentic search capabilities.", "key_contributions": ["Introduction of RAVine, a new evaluation framework.", "Focus on realistic user scenarios with multi-point queries.", "Enhanced accuracy in ground truth extraction for evaluation."], "limitations": "", "keywords": ["agentic search", "LLM", "evaluation framework", "user intents", "RAVine"], "importance_score": 8, "read_time_minutes": 5}}
