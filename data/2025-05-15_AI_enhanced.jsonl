{"id": "2505.08894", "pdf": "https://arxiv.org/pdf/2505.08894.pdf", "abs": "https://arxiv.org/abs/2505.08894", "title": "WaLLM -- Insights from an LLM-Powered Chatbot deployment via WhatsApp", "authors": ["Hiba Eltigani", "Rukhshan Haroon", "Asli Kocak", "Abdullah Bin Faisal", "Noah Martin", "Fahad Dogar"], "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": null, "summary": "Recent advances in generative AI, such as ChatGPT, have transformed access to\ninformation in education, knowledge-seeking, and everyday decision-making.\nHowever, in many developing regions, access remains a challenge due to the\npersistent digital divide. To help bridge this gap, we developed WaLLM - a\ncustom AI chatbot over WhatsApp, a widely used communication platform in\ndeveloping regions. Beyond answering queries, WaLLM offers several features to\nenhance user engagement: a daily top question, suggested follow-up questions,\ntrending and recent queries, and a leaderboard-based reward system. Our service\nhas been operational for over 6 months, amassing over 14.7K queries from\napproximately 100 users. In this paper, we present WaLLM's design and a\nsystematic analysis of logs to understand user interactions. Our results show\nthat 55% of user queries seek factual information. \"Health and well-being\" was\nthe most popular topic (28%), including queries about nutrition and disease,\nsuggesting users view WaLLM as a reliable source. Two-thirds of users' activity\noccurred within 24 hours of the daily top question. Users who accessed the\n\"Leaderboard\" interacted with WaLLM 3x as those who did not. We conclude by\ndiscussing implications for culture-based customization, user interface design,\nand appropriate calibration of users' trust in AI systems for developing\nregions.", "AI": {"tldr": "WaLLM is a custom AI chatbot over WhatsApp designed to improve access to information in developing regions, focusing on user engagement and health topics.", "motivation": "To address the digital divide in education and decision-making in developing regions through a widely used communication platform.", "method": "Developed a WhatsApp-based chatbot (WaLLM) and analyzed over 6 months of user interaction logs to understand engagement and information-seeking behavior.", "result": "55% of queries sought factual information, with 'Health and well-being' being the most popular category (28%), and users engaged more with features like the leaderboard.", "conclusion": "The study highlights the importance of culture-based customization and the need for careful calibration of user trust in AI systems in developing areas.", "key_contributions": ["Design and implementation of WaLLM for WhatsApp", "Analysis of user interaction logs to derive insights on engagement", "Recommendations for UI design and trust calibration in AI systems"], "limitations": "", "keywords": ["WaLLM", "WhatsApp chatbot", "digital divide", "user engagement", "health informatics"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.08902", "pdf": "https://arxiv.org/pdf/2505.08902.pdf", "abs": "https://arxiv.org/abs/2505.08902", "title": "Performance Gains of LLMs With Humans in a World of LLMs Versus Humans", "authors": ["Lucas McCullum", "Pelagie Ami Agassi", "Leo Anthony Celi", "Daniel K. Ebner", "Chrystinne Oliveira Fernandes", "Rachel S. Hicklen", "Mkliwa Koumbia", "Lisa Soleymani Lehmann", "David Restrepo"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "Currently, a considerable research effort is devoted to comparing LLMs to a\ngroup of human experts, where the term \"expert\" is often ill-defined or\nvariable, at best, in a state of constantly updating LLM releases. Without\nproper safeguards in place, LLMs will threaten to cause harm to the established\nstructure of safe delivery of patient care which has been carefully developed\nthroughout history to keep the safety of the patient at the forefront. A key\ndriver of LLM innovation is founded on community research efforts which, if\ncontinuing to operate under \"humans versus LLMs\" principles, will expedite this\ntrend. Therefore, research efforts moving forward must focus on effectively\ncharacterizing the safe use of LLMs in clinical settings that persist across\nthe rapid development of novel LLM models. In this communication, we\ndemonstrate that rather than comparing LLMs to humans, there is a need to\ndevelop strategies enabling efficient work of humans with LLMs in an almost\nsymbiotic manner.", "AI": {"tldr": "Explores the need for strategies to effectively collaborate between humans and LLMs in clinical settings, moving beyond mere comparison of LLMs to human experts.", "motivation": "To address the potential risks posed by LLMs in patient care due to their evolving nature and the ill-defined concept of 'expert' in this context.", "method": "The paper advocates for a shift in research focus towards creating frameworks for safe human-LLM collaboration in healthcare, rather than direct comparison of their capabilities.", "result": "Proposes that efficient human-LLM partnerships may enhance patient care while mitigating risks associated with LLM deployment.", "conclusion": "Future research must prioritize symbiotic relationships between humans and LLMs in clinical contexts to ensure patient safety and effective care.", "key_contributions": ["Emphasizes the need for a new approach to human-LLM interaction in healthcare.", "Highlights the risks of comparing LLMs to human experts without clear definitions.", "Advocates for frameworks that enable safe collaboration rather than competition."], "limitations": "", "keywords": ["LLMs", "patient care", "human-LLM collaboration", "clinical settings", "health informatics"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.08939", "pdf": "https://arxiv.org/pdf/2505.08939.pdf", "abs": "https://arxiv.org/abs/2505.08939", "title": "Tracing the Invisible: Understanding Students' Judgment in AI-Supported Design Work", "authors": ["Suchismita Naik", "Prakash Shukla", "Ike Obi", "Jessica Backus", "Nancy Rasche", "Paul Parsons"], "categories": ["cs.HC", "cs.AI"], "comment": "5 pages, 2 Tables, In Creativity and Cognition 2025, June 23--25,\n  2025, Virtual, United Kingdom", "summary": "As generative AI tools become integrated into design workflows, students\nincreasingly engage with these tools not just as aids, but as collaborators.\nThis study analyzes reflections from 33 student teams in an HCI design course\nto examine the kinds of judgments students make when using AI tools. We found\nboth established forms of design judgment (e.g., instrumental, appreciative,\nquality) and emergent types: agency-distribution judgment and reliability\njudgment. These new forms capture how students negotiate creative\nresponsibility with AI and assess the trustworthiness of its outputs. Our\nfindings suggest that generative AI introduces new layers of complexity into\ndesign reasoning, prompting students to reflect not only on what AI produces,\nbut also on how and when to rely on it. By foregrounding these judgments, we\noffer a conceptual lens for understanding how students engage in co-creative\nsensemaking with AI in design contexts.", "AI": {"tldr": "The study analyzes reflections from students in an HCI design course on their judgments while using generative AI tools as collaborators in design workflows.", "motivation": "To understand how students incorporate generative AI tools in design and the implications for their creative judgment in HCI.", "method": "Analysis of reflections from 33 student teams engaged in an HCI design course.", "result": "Identified established and emergent forms of design judgment, including agency-distribution judgment and reliability judgment, indicating new complexities in creative responsibility and trust in AI outputs.", "conclusion": "Generative AI complicates design reasoning and encourages students to reflect critically on their co-creative processes with AI.", "key_contributions": ["Introduces new forms of design judgment that encompass the interaction with generative AI.", "Provides a conceptual lens for co-creative sensemaking with AI in design contexts.", "Highlights the complexities in negotiating creative responsibility between students and AI."], "limitations": "", "keywords": ["HCI", "generative AI", "design judgment", "co-creation", "trustworthiness"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.09047", "pdf": "https://arxiv.org/pdf/2505.09047.pdf", "abs": "https://arxiv.org/abs/2505.09047", "title": "Positioning Monocular Optical See Through Head Worn Displays in Glasses for Everyday Wear", "authors": ["Parth Arora", "Ethan Kimmel", "Katherine Huang", "Tyler Kwok", "Yukun Song", "Sofia Vempala", "Georgianna Lin", "Ozan Cakmakci", "Thad Starner"], "categories": ["cs.HC"], "comment": null, "summary": "Head-worn displays for everyday wear in the form of regular eyeglasses are\ntechnically feasible with recent advances in waveguide technology. One major\ndesign decision is determining where in the user's visual field to position the\ndisplay. Centering the display in the principal point of gaze (PPOG) allows the\nuser to switch attentional focus between the virtual and real images quickly,\nand best performance often occurs when the display is centered in PPOG or is\ncentered vertically below PPOG. However, these positions are often undesirable\nin that they are considered interruptive or are associated with negative social\nperceptions by users. Offsetting the virtual image may be preferred when tasks\ninvolve driving, walking, or social interaction. This paper consolidates\nfindings from recent studies on monocular optical see-through HWDs (OST-HWDs),\nfocusing on potential for interruption, comfort, performance, and social\nperception. For text-based tasks, which serve as a proxy for many monocular\nOST-HWD tasks, we recommend a 15{\\deg} horizontal field of view (FOV) with the\nvirtual image in the right lens vertically centered but offset to +8.7{\\deg} to\n+23.7{\\deg} toward the ear. Glanceable content can be offset up to +30{\\deg}\nfor short interactions.", "AI": {"tldr": "This paper explores optimal positioning of head-worn displays (HWDs) in eyeglasses to enhance user experience and reduce social interruption.", "motivation": "To understand the impact of display positioning on user experience with head-worn displays in everyday scenarios, particularly regarding comfort and social perceptions.", "method": "The paper consolidates findings from recent studies on monocular optical see-through head-worn displays, focusing on performance, comfort, and social perception based on different display positions.", "result": "The study recommends specific display placements for optimal interaction, suggesting a horizontal field of view of 15° with an offset towards the ear for minimal interruption during tasks.", "conclusion": "Proper positioning of HWDs can enhance user comfort and performance while mitigating negative social perceptions.", "key_contributions": ["Provides guidelines for effective visual display positioning in smart eyewear.", "Addresses user comfort and societal perceptions of HWDs occupations.", "Synthesizes recent research for practical applications in design of HWDs."], "limitations": "The study primarily focuses on monocular displays and text-based tasks, limiting applicability to wider use cases.", "keywords": ["Head-worn displays", "User experience", "Display positioning", "Social perception", "Monocular optical see-through"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.08828", "pdf": "https://arxiv.org/pdf/2505.08828.pdf", "abs": "https://arxiv.org/abs/2505.08828", "title": "Human-AI Collaboration or Academic Misconduct? Measuring AI Use in Student Writing Through Stylometric Evidence", "authors": ["Eduardo Araujo Oliveira", "Madhavi Mohoni", "Sonsoles López-Pernas", "Mohammed Saqr"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "19 pages, 10 figures, 11 tables", "summary": "As human-AI collaboration becomes increasingly prevalent in educational\ncontexts, understanding and measuring the extent and nature of such\ninteractions pose significant challenges. This research investigates the use of\nauthorship verification (AV) techniques not as a punitive measure, but as a\nmeans to quantify AI assistance in academic writing, with a focus on promoting\ntransparency, interpretability, and student development. Building on prior\nwork, we structured our investigation into three stages: dataset selection and\nexpansion, AV method development, and systematic evaluation. Using three\ndatasets - including a public dataset (PAN-14) and two from University of\nMelbourne students from various courses - we expanded the data to include\nLLM-generated texts, totalling 1,889 documents and 540 authorship problems from\n506 students. We developed an adapted Feature Vector Difference AV methodology\nto construct robust academic writing profiles for students, designed to capture\nmeaningful, individual characteristics of their writing. The method's\neffectiveness was evaluated across multiple scenarios, including distinguishing\nbetween student-authored and LLM-generated texts and testing resilience against\nLLMs' attempts to mimic student writing styles. Results demonstrate the\nenhanced AV classifier's ability to identify stylometric discrepancies and\nmeasure human-AI collaboration at word and sentence levels while providing\neducators with a transparent tool to support academic integrity investigations.\nThis work advances AV technology, offering actionable insights into the\ndynamics of academic writing in an AI-driven era.", "AI": {"tldr": "This paper explores the use of authorship verification techniques to measure AI assistance in academic writing, aimed at promoting transparency and student development.", "motivation": "To understand and quantify human-AI collaboration in educational contexts without punitive measures.", "method": "The research involved dataset selection and expansion, development of AV methods, and systematic evaluation using a new adapted Feature Vector Difference methodology across various datasets.", "result": "The AV classifier effectively identified differences between student-authored and LLM-generated texts and measured collaboration at word and sentence levels.", "conclusion": "The study provides a tool for educators to support academic integrity while advancing AV technology in the context of AI-assisted academic writing.", "key_contributions": ["Adapted AV methodology for academic writing profiles", "Evaluation of AI collaboration in educational settings", "Insights into stylometric discrepancies for educational purposes"], "limitations": "", "keywords": ["authorship verification", "AI collaboration", "academic integrity", "LLM-generated texts", "educational technology"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2505.09054", "pdf": "https://arxiv.org/pdf/2505.09054.pdf", "abs": "https://arxiv.org/abs/2505.09054", "title": "EcoSphere: A Decision-Support Tool for Automated Carbon Emission and Cost Optimization in Sustainable Urban Development", "authors": ["Siavash Ghorbany", "Ming Hu", "Siyuan Yao", "Matthew Sisk", "Chaoli Wang"], "categories": ["cs.HC", "cs.CY", "stat.AP"], "comment": "Proc of the 23rd CIB World Building Congress, 19th to 23rd May 2025,\n  Purdue University, West Lafayette, USA", "summary": "The construction industry is a major contributor to global greenhouse gas\nemissions, with embodied carbon being a key component. This study develops\nEcoSphere, an innovative software designed to evaluate and balance embodied and\noperational carbon emissions with construction and environmental costs in urban\nplanning. Using high-resolution data from the National Structure Inventory,\ncombined with computer vision and natural language processing applied to Google\nStreet View and satellite imagery, EcoSphere categorizes buildings by\nstructural and material characteristics with a bottom-up approach, creating a\nbaseline emissions dataset. By simulating policy scenarios and mitigation\nstrategies, EcoSphere provides policymakers and non-experts with actionable\ninsights for sustainable development in cities and provide them with a vision\nof the environmental and financial results of their decisions. Case studies in\nChicago and Indianapolis showcase how EcoSphere aids in assessing policy\nimpacts on carbon emissions and costs, supporting data-driven progress toward\ncarbon neutrality.", "AI": {"tldr": "EcoSphere software evaluates embodied and operational carbon in urban planning using computer vision and NLP to assist policymakers in sustainable development.", "motivation": "To address the construction industry's contribution to greenhouse gas emissions through the evaluation of carbon emissions.", "method": "Developed EcoSphere software utilizing high-resolution data and applying computer vision along with NLP to classify building materials and emissions.", "result": "EcoSphere produces a baseline emissions dataset and allows the simulation of various policy scenarios for better decision-making in urban planning.", "conclusion": "The software aids in making data-driven policy decisions that can lead to carbon neutrality in urban environments.", "key_contributions": ["EcoSphere categorizes buildings by structural and material characteristics.", "It creates a baseline emissions dataset from high-resolution data.", "The software simulates policy scenarios to provide actionable insights."], "limitations": "", "keywords": ["EcoSphere", "carbon emissions", "urban planning", "sustainable development", "computer vision"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2505.08891", "pdf": "https://arxiv.org/pdf/2505.08891.pdf", "abs": "https://arxiv.org/abs/2505.08891", "title": "Clicking some of the silly options: Exploring Player Motivation in Static and Dynamic Educational Interactive Narratives", "authors": ["Daeun Hwang", "Samuel Shields", "Alex Calderwood", "Shi Johnson-Bey", "Michael Mateas", "Noah Wardrip-Fruin", "Edward F. Melcer"], "categories": ["cs.CL"], "comment": "8 pages, 3 figures, 1 table, 1 appendix. Workshop paper, CHI 2025\n  Augmented Educators and AI", "summary": "Motivation is an important factor underlying successful learning. Previous\nresearch has demonstrated the positive effects that static interactive\nnarrative games can have on motivation. Concurrently, advances in AI have made\ndynamic and adaptive approaches to interactive narrative increasingly\naccessible. However, limited work has explored the impact that dynamic\nnarratives can have on learner motivation. In this paper, we compare two\nversions of Academical, a choice-based educational interactive narrative game\nabout research ethics. One version employs a traditional hand-authored\nbranching plot (i.e., static narrative) while the other dynamically sequences\nplots during play (i.e., dynamic narrative). Results highlight the importance\nof responsive content and a variety of choices for player engagement, while\nalso illustrating the challenge of balancing pedagogical goals with the dynamic\naspects of narrative. We also discuss design implications that arise from these\nfindings. Ultimately, this work provides initial steps to illuminate the\nemerging potential of AI-driven dynamic narrative in educational games.", "AI": {"tldr": "This paper examines the effects of dynamic versus static narratives on learner motivation in educational games, using a comparison of two versions of the game Academical.", "motivation": "The study aims to investigate how dynamic interactive narratives can influence learner motivation, in light of the positive effects of static narrative games established in previous research.", "method": "A comparative study was conducted using two versions of Academical: one with a static narrative and another with a dynamic narrative where plots are sequenced based on player choices.", "result": "Results show that responsive content and diverse choice options enhance player engagement, emphasizing the challenges of integrating educational goals with dynamic narratives.", "conclusion": "The findings suggest that AI-driven dynamic narratives hold significant potential for enhancing engagement in educational contexts, leading to important design implications for future development.", "key_contributions": ["Investigates the impact of dynamic narratives on learner motivation.", "Provides empirical evidence comparing static and dynamic game narratives.", "Highlights design implications for integrating AI in educational games."], "limitations": "Limited scope of research focused on a specific game type and context; future research could explore broader applications.", "keywords": ["dynamic narrative", "learner motivation", "educational games", "AI-driven design", "interactive narrative"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.09065", "pdf": "https://arxiv.org/pdf/2505.09065.pdf", "abs": "https://arxiv.org/abs/2505.09065", "title": "Display Content, Display Methods and Evaluation Methods of the HCI in Explainable Recommender Systems: A Survey", "authors": ["Weiqing Li", "Yue Xu", "Yuefeng Li", "Yinghui Huang"], "categories": ["cs.HC", "cs.IR"], "comment": "2 Tables, 29 figures", "summary": "Explainable Recommender Systems (XRS) aim to provide users with\nunderstandable reasons for the recommendations generated by these systems,\nrepresenting a crucial research direction in artificial intelligence (AI).\nRecent research has increasingly focused on the algorithms, display, and\nevaluation methodologies of XRS. While current research and reviews primarily\nemphasize the algorithmic aspects, with fewer studies addressing the\nHuman-Computer Interaction (HCI) layer of XRS. Additionally, existing reviews\nlack a unified taxonomy for XRS and there is insufficient attention given to\nthe emerging area of short video recommendations. In this study, we synthesize\nexisting literature and surveys on XRS, presenting a unified framework for its\nresearch and development. The main contributions are as follows: 1) We adopt a\nlifecycle perspective to systematically summarize the technologies and methods\nused in XRS, addressing challenges posed by the diversity and complexity of\nalgorithmic models and explanation techniques. 2) For the first time, we\nhighlight the application of multimedia, particularly video-based explanations,\nalong with its potential, technical pathways, and challenges in XRS. 3) We\nprovide a structured overview of evaluation methods from both qualitative and\nquantitative dimensions. These findings provide valuable insights for the\nsystematic design, progress, and testing of XRS.", "AI": {"tldr": "This paper presents a unified framework for Explainable Recommender Systems (XRS), addressing algorithmic challenges and highlighting the role of multimedia in explanations.", "motivation": "To advance the research on Explainable Recommender Systems (XRS) by providing a unified framework that considers algorithmic aspects as well as human-computer interaction layers.", "method": "The study synthesizes existing literature on XRS and adopts a lifecycle perspective to summarize technologies and methods in the field, focusing on multimedia, especially video-based explanations.", "result": "The paper emphasizes the importance of considering video-based explanations and provides a structured overview of evaluation methods for XRS.", "conclusion": "The findings offer insights for systematically designing, progressing, and testing XRS while addressing the HCI layer and challenges in the field.", "key_contributions": ["Unified framework for XRS research and development", "Lifecycle perspective on technologies in XRS", "Focus on video-based explanations and evaluation methods"], "limitations": "", "keywords": ["Explainable Recommender Systems", "Human-Computer Interaction", "Video Recommendations"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.08996", "pdf": "https://arxiv.org/pdf/2505.08996.pdf", "abs": "https://arxiv.org/abs/2505.08996", "title": "A suite of LMs comprehend puzzle statements as well as humans", "authors": ["Adele E Goldberg", "Supantho Rakshit", "Jennifer Hu", "Kyle Mahowald"], "categories": ["cs.CL"], "comment": null, "summary": "Recent claims suggest that large language models (LMs) underperform humans in\ncomprehending minimally complex English statements (Dentella et al., 2024).\nHere, we revisit those findings and argue that human performance was\noverestimated, while LLM abilities were underestimated. Using the same stimuli,\nwe report a preregistered study comparing human responses in two conditions:\none allowed rereading (replicating the original study), and one that restricted\nrereading (a more naturalistic comprehension test). Human accuracy dropped\nsignificantly when rereading was restricted (73%), falling below that of\nFalcon-180B-Chat (76%) and GPT-4 (81%). The newer GPT-o1 model achieves perfect\naccuracy. Results further show that both humans and models are\ndisproportionately challenged by queries involving potentially reciprocal\nactions (e.g., kissing), suggesting shared pragmatic sensitivities rather than\nmodel-specific deficits. Additional analyses using Llama-2-70B log\nprobabilities, a recoding of open-ended model responses, and grammaticality\nratings of other sentences reveal systematic underestimation of model\nperformance. We find that GPT-4o can align with either naive or expert\ngrammaticality judgments, depending on prompt framing. These findings\nunderscore the need for more careful experimental design and coding practices\nin LLM evaluation, and they challenge the assumption that current models are\ninherently weaker than humans at language comprehension.", "AI": {"tldr": "This study argues that large language models (LLMs) perform better than previously assumed in understanding English statements, particularly under conditions that simulate natural reading. It critiques prior assessments of human performance and emphasizes the need for improved experimental designs in LLM evaluations.", "motivation": "To address claims that LLMs underperform compared to humans in comprehending simple English statements and to investigate whether human performance has been overestimated.", "method": "A preregistered study compared human responses under two conditions: one with rereading allowed and another with rereading restricted to better simulate natural comprehension.", "result": "Human accuracy dropped to 73% when rereading was restricted, lower than that of LLMs Falcon-180B-Chat (76%) and GPT-4 (81%); GPT-4o reached perfect accuracy. Both humans and LLMs struggled with queries involving reciprocal actions, indicating similar pragmatic challenges.", "conclusion": "The findings suggest that LLMs may not be inherently weaker than humans in language comprehension and highlight a need for more rigorous experimental design in evaluating LLMs.", "key_contributions": ["Demonstrates higher performance of LLMs than previously reported under restricted comprehension conditions", "Identifies shared challenges between human and model responses to certain types of queries", "Calls for improved experimental design in the evaluation of language models"], "limitations": "The study is based on specific conditions and types of queries, which may not generalize across all language comprehension tasks.", "keywords": ["Language Models", "Human Comprehension", "Pragmatics", "Experimental Design", "LLM Evaluation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.09094", "pdf": "https://arxiv.org/pdf/2505.09094.pdf", "abs": "https://arxiv.org/abs/2505.09094", "title": "PLanet: Formalizing Experimental Design", "authors": ["London Bielicke", "Anna Zhang", "Shruti Tyagi", "Emery Berger", "Adam Chlipala", "Eunice Jun"], "categories": ["cs.HC"], "comment": "14 pages, 4 tables, 6 figures, human-computer interaction, domain\n  specific language, experimental design", "summary": "Carefully constructed experimental designs are essential for drawing valid,\ngeneralizable conclusions from scientific studies. Unfortunately, experimental\ndesign plans can be difficult to specify, communicate clearly, and relate to\nalternatives. In response, we introduce a grammar of experimental design that\nprovides composable operators for constructing assignment procedures (e.g.,\nLatin square). We implement this grammar in PLanet, a domain-specific language\n(DSL) that constructs assignment plans in three stages: experimental unit\nspecification, trial-order construction, and order-to-unit mapping. We evaluate\nPLanet's expressivity by taking a purposive sample of recent CHI and UIST\npublications, representing their experiments as programs in PLanet, and\nidentifying ambiguities and alternatives. In our evaluation, PLanet could\nexpress 11 out of 12 experiments found in sampled papers. Additionally, we\nfound that PLanet constructs helped make complex design choices explicit when\nthe researchers omit technical language describing their study designs.", "AI": {"tldr": "This paper introduces PLanet, a domain-specific language for constructing experimental design plans, enhancing clarity and reducing ambiguities in scientific studies.", "motivation": "The paper addresses the challenges in specifying, communicating, and relating experimental design plans due to their complexity.", "method": "The authors developed a grammar of experimental design that allows for composable operators to create assignment procedures, implemented in the DSL PLanet, which operates in three stages: experimental unit specification, trial-order construction, and order-to-unit mapping.", "result": "PLanet was able to express 11 out of 12 experiments from a sample of recent CHI and UIST publications, revealing ambiguities in the original designs.", "conclusion": "The use of PLanet facilitates explicit communication of design choices, particularly when researchers fail to provide thorough technical descriptions.", "key_contributions": ["Introduction of a grammar of experimental design", "Development of PLanet DSL for assignment planning", "Evaluation demonstrating PLanet's ability to clarify complex experimental designs"], "limitations": "", "keywords": ["human-computer interaction", "domain specific language", "experimental design"], "importance_score": 6, "read_time_minutes": 14}}
{"id": "2505.09005", "pdf": "https://arxiv.org/pdf/2505.09005.pdf", "abs": "https://arxiv.org/abs/2505.09005", "title": "For GPT-4 as with Humans: Information Structure Predicts Acceptability of Long-Distance Dependencies", "authors": ["Nicole Cuneo", "Eleanor Graves", "Supantho Rakshit", "Adele E. Goldberg"], "categories": ["cs.CL"], "comment": null, "summary": "It remains debated how well any LM understands natural language or generates\nreliable metalinguistic judgments. Moreover, relatively little work has\ndemonstrated that LMs can represent and respect subtle relationships between\nform and function proposed by linguists. We here focus on a particular such\nrelationship established in recent work: English speakers' judgments about the\ninformation structure of canonical sentences predicts independently collected\nacceptability ratings on corresponding 'long distance dependency' [LDD]\nconstructions, across a wide array of base constructions and multiple types of\nLDDs. To determine whether any LM captures this relationship, we probe GPT-4 on\nthe same tasks used with humans and new extensions.Results reveal reliable\nmetalinguistic skill on the information structure and acceptability tasks,\nreplicating a striking interaction between the two, despite the zero-shot,\nexplicit nature of the tasks, and little to no chance of contamination [Studies\n1a, 1b]. Study 2 manipulates the information structure of base sentences and\nconfirms a causal relationship: increasing the prominence of a constituent in a\ncontext sentence increases the subsequent acceptability ratings on an LDD\nconstruction. The findings suggest a tight relationship between natural and\nGPT-4 generated English, and between information structure and syntax, which\nbegs for further exploration.", "AI": {"tldr": "The paper investigates whether GPT-4 can understand and generate reliable metalinguistic judgments, particularly in relation to the information structure of sentences and their acceptability ratings for long-distance dependency constructions.", "motivation": "To examine the capability of language models (LMs) in understanding natural language and their ability to reflect the relationship between form and function as described by linguists.", "method": "The study probes GPT-4 using tasks that evaluate metalinguistic skills related to information structure and acceptability ratings, replicating the tasks applied to human subjects.", "result": "Results indicate that GPT-4 exhibits metalinguistic skills, successfully capturing a relationship between information structure and acceptability ratings in long-distance dependency constructions across various base constructions.", "conclusion": "The findings suggest a close connection between natural and GPT-4 generated English, highlighting a need for further exploration into the relationship between information structure and syntax.", "key_contributions": ["Demonstrates GPT-4's ability to reflect human-like understanding of information structure in sentences.", "Establishes a causal link between information prominence and construction acceptability ratings.", "Provides empirical evidence of LMs capturing linguistic relationships previously debated in theoretical linguistics."], "limitations": "The study focuses solely on GPT-4 and may not be generalizable to other LMs; the tasks may not encompass the full complexity of human linguistic judgment.", "keywords": ["GPT-4", "information structure", "long-distance dependency", "metalinguistic skill", "acceptability ratings"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.09115", "pdf": "https://arxiv.org/pdf/2505.09115.pdf", "abs": "https://arxiv.org/abs/2505.09115", "title": "PreCare: Designing AI Assistants for Advance Care Planning (ACP) to Enhance Personal Value Exploration, Patient Knowledge, and Decisional Confidence", "authors": ["Yu Lun Hsu", "Yun-Rung Chou", "Chiao-Ju Chang", "Yu-Cheng Chang", "Zer-Wei Lee", "Rokas Gipiškis", "Rachel Li", "Chih-Yuan Shih", "Jen-Kuei Peng", "Hsien-Liang Huang", "Jaw-Shiun Tsai", "Mike Y. Chen"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Advance Care Planning (ACP) allows individuals to specify their preferred\nend-of-life life-sustaining treatments before they become incapacitated by\ninjury or terminal illness (e.g., coma, cancer, dementia). While online ACP\noffers high accessibility, it lacks key benefits of clinical consultations,\nincluding personalized value exploration, immediate clarification of decision\nconsequences. To bridge this gap, we conducted two formative studies: 1)\nshadowed and interviewed 3 ACP teams consisting of physicians, nurses, and\nsocial workers (18 patients total), and 2) interviewed 14 users of ACP\nwebsites. Building on these insights, we designed PreCare in collaboration with\n6 ACP professionals. PreCare is a website with 3 AI-driven assistants designed\nto guide users through exploring personal values, gaining ACP knowledge, and\nsupporting informed decision-making. A usability study (n=12) showed that\nPreCare achieved a System Usability Scale (SUS) rating of excellent. A\ncomparative evaluation (n=12) showed that PreCare's AI assistants significantly\nimproved exploration of personal values, knowledge, and decisional confidence,\nand was preferred by 92% of participants.", "AI": {"tldr": "The paper presents PreCare, an AI-driven platform for Advance Care Planning (ACP) that enhances user engagement and decision-making compared to traditional online ACP services.", "motivation": "To address the limitations of online Advance Care Planning (ACP) which lacks personalized consultation benefits, this research aims to enhance user understanding and decision making regarding end-of-life treatments.", "method": "Two formative studies were conducted: one involved shadowing and interviewing ACP teams (3 teams, 18 patients), and the other interviewed users of ACP websites (14 users). Based on these insights, PreCare was designed in collaboration with 6 ACP professionals.", "result": "Usability testing of PreCare (n=12) resulted in an excellent System Usability Scale (SUS) rating. A comparative evaluation (n=12) demonstrated that PreCare's AI assistants significantly improved users' exploration of personal values, knowledge, and decisional confidence, with 92% user preference over alternatives.", "conclusion": "PreCare is an effective tool for enhancing user engagement in Advance Care Planning, leveraging AI to improve understanding and comfort in making end-of-life decisions.", "key_contributions": ["Design and implementation of PreCare, an AI-driven ACP platform", "Demonstrated significant improvement in user engagement and decisional confidence", "Collaboration with healthcare professionals to align technology with clinical needs"], "limitations": "", "keywords": ["Advance Care Planning", "AI-driven assistance", "usability study"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2505.09039", "pdf": "https://arxiv.org/pdf/2505.09039.pdf", "abs": "https://arxiv.org/abs/2505.09039", "title": "Atomic Consistency Preference Optimization for Long-Form Question Answering", "authors": ["Jingfeng Chen", "Raghuveer Thirukovalluru", "Junlin Wang", "Kaiwei Luo", "Bhuwan Dhingra"], "categories": ["cs.CL"], "comment": "16 pages, 2 figures", "summary": "Large Language Models (LLMs) frequently produce factoid hallucinations -\nplausible yet incorrect answers. A common mitigation strategy is model\nalignment, which improves factual accuracy by training on curated factual and\nnon-factual pairs. However, this approach often relies on a stronger model\n(e.g., GPT-4) or an external knowledge base to assess factual correctness,\nwhich may not always be accessible. To address this, we propose Atomic\nConsistency Preference Optimization (ACPO), a self-supervised preference-tuning\nmethod that enhances factual accuracy without external supervision. ACPO\nleverages atomic consistency signals, i.e., the agreement of individual facts\nacross multiple stochastic responses, to identify high- and low-quality data\npairs for model alignment. By eliminating the need for costly GPT calls, ACPO\nprovides a scalable and efficient approach to improving factoid\nquestion-answering. Despite being self-supervised, empirical results\ndemonstrate that ACPO outperforms FactAlign, a strong supervised alignment\nbaseline, by 1.95 points on the LongFact and BioGen datasets, highlighting its\neffectiveness in enhancing factual reliability without relying on external\nmodels or knowledge bases.", "AI": {"tldr": "The proposed Atomic Consistency Preference Optimization (ACPO) is a self-supervised method that enhances the factual accuracy of Large Language Models (LLMs) in question-answering without external supervision.", "motivation": "To address the issue of factoid hallucinations in LLMs while avoiding dependencies on external models or knowledge bases for factual correctness assessment.", "method": "ACPO uses atomic consistency signals to find high- and low-quality data pairs for model alignment without the need for costly external calls, making the method scalable and efficient.", "result": "Empirical results show that ACPO outperforms the supervised alignment baseline, FactAlign, by 1.95 points on the LongFact and BioGen datasets.", "conclusion": "ACPO effectively improves the factual reliability of models in a self-supervised manner, eliminating the need for external knowledge sources.", "key_contributions": ["Introduction of Atomic Consistency Preference Optimization (ACPO) for self-supervised factual alignment", "Demonstration of ACPO's superior performance over a state-of-the-art supervised method (FactAlign)", "Establishment of a scalable approach to enhance LLM's factual accuracy without external dependency."], "limitations": "The study's effectiveness may vary depending on the quality of input data used for training and model architecture.", "keywords": ["Large Language Models", "factoid hallucinations", "self-supervised", "factual accuracy", "model alignment"], "importance_score": 9, "read_time_minutes": 16}}
{"id": "2505.09166", "pdf": "https://arxiv.org/pdf/2505.09166.pdf", "abs": "https://arxiv.org/abs/2505.09166", "title": "An Initial Exploration of Default Images in Text-to-Image Generation", "authors": ["Hannu Simonen", "Atte Kiviniemi", "Jonas Oppenlaender"], "categories": ["cs.HC", "cs.AI", "H.5.m; I.2.m"], "comment": "16 pages, 6 figures", "summary": "In the creative practice of text-to-image generation (TTI), images are\ngenerated from text prompts. However, TTI models are trained to always yield an\noutput, even if the prompt contains unknown terms. In this case, the model may\ngenerate what we call \"default images\": images that closely resemble each other\nacross many unrelated prompts. We argue studying default images is valuable for\ndesigning better solutions for TTI and prompt engineering. In this paper, we\nprovide the first investigation into default images on Midjourney, a popular\nimage generator. We describe our systematic approach to create input prompts\ntriggering default images, and present the results of our initial experiments\nand several small-scale ablation studies. We also report on a survey study\ninvestigating how default images affect user satisfaction. Our work lays the\nfoundation for understanding default images in TTI and highlights challenges\nand future research directions.", "AI": {"tldr": "This paper investigates 'default images' generated by text-to-image models when prompts contain unknown terms, focusing on Midjourney.", "motivation": "To understand the impact of default images on user satisfaction and improve text-to-image generation models and prompt engineering.", "method": "A systematic approach to create prompts that trigger default images, followed by experiments and a survey on user satisfaction.", "result": "Findings from user surveys indicating that default images affect user satisfaction; results from initial experiments and ablation studies are presented.", "conclusion": "The study highlights the challenges posed by default images in text-to-image generation and suggests directions for future research.", "key_contributions": ["First investigation into default images in TTI", "Development of systematic prompting approach", "Insights into user satisfaction relating to default images"], "limitations": "Focused on a specific model (Midjourney), limiting generalizability.", "keywords": ["text-to-image generation", "default images", "user satisfaction"], "importance_score": 6, "read_time_minutes": 16}}
{"id": "2505.09056", "pdf": "https://arxiv.org/pdf/2505.09056.pdf", "abs": "https://arxiv.org/abs/2505.09056", "title": "A Comprehensive Analysis of Large Language Model Outputs: Similarity, Diversity, and Bias", "authors": ["Brandon Smith", "Mohamed Reda Bouadjenek", "Tahsin Alamgir Kheya", "Phillip Dawson", "Sunil Aryal"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) represent a major step toward artificial general\nintelligence, significantly advancing our ability to interact with technology.\nWhile LLMs perform well on Natural Language Processing tasks -- such as\ntranslation, generation, code writing, and summarization -- questions remain\nabout their output similarity, variability, and ethical implications. For\ninstance, how similar are texts generated by the same model? How does this\ncompare across different models? And which models best uphold ethical\nstandards? To investigate, we used 5{,}000 prompts spanning diverse tasks like\ngeneration, explanation, and rewriting. This resulted in approximately 3\nmillion texts from 12 LLMs, including proprietary and open-source systems from\nOpenAI, Google, Microsoft, Meta, and Mistral. Key findings include: (1) outputs\nfrom the same LLM are more similar to each other than to human-written texts;\n(2) models like WizardLM-2-8x22b generate highly similar outputs, while GPT-4\nproduces more varied responses; (3) LLM writing styles differ significantly,\nwith Llama 3 and Mistral showing higher similarity, and GPT-4 standing out for\ndistinctiveness; (4) differences in vocabulary and tone underscore the\nlinguistic uniqueness of LLM-generated content; (5) some LLMs demonstrate\ngreater gender balance and reduced bias. These results offer new insights into\nthe behavior and diversity of LLM outputs, helping guide future development and\nethical evaluation.", "AI": {"tldr": "This paper investigates the output similarity, variability, and ethical implications of 12 large language models (LLMs), revealing significant differences in their generated texts and highlighting issues related to similarity and bias.", "motivation": "To understand the similarities and ethical standards of outputs generated by different LLMs and to provide insights for future development.", "method": "Analyzed approximately 3 million texts generated from 5,000 prompts across 12 LLMs, including proprietary and open-source models.", "result": "Outputs from the same LLM are more similar than to human-written texts; some models, like WizardLM-2-8x22b, show high similarity, while GPT-4 is more varied and distinctive in style.", "conclusion": "The study sheds light on the behavior and diversity of LLM outputs, which is essential for guiding ethical evaluations and future improvements in LLMs.", "key_contributions": ["Insights into output similarity and variability across different LLMs", "Identification of linguistic uniqueness and tone differences among LLM-generated texts", "Assessment of gender balance and bias in LLM outputs"], "limitations": "", "keywords": ["Large Language Models", "Natural Language Processing", "Ethics in AI"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.09208", "pdf": "https://arxiv.org/pdf/2505.09208.pdf", "abs": "https://arxiv.org/abs/2505.09208", "title": "Educational impacts of generative artificial intelligence on learning and performance of engineering students in China", "authors": ["Lei Fan", "Kunyang Deng", "Fangxue Liu"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "With the rapid advancement of generative artificial intelligence(AI), its\npotential applications in higher education have attracted significant\nattention. This study investigated how 148 students from diverse engineering\ndisciplines and regions across China used generative AI, focusing on its impact\non their learning experience and the opportunities and challenges it poses in\nengineering education. Based on the surveyed data, we explored four key areas:\nthe frequency and application scenarios of AI use among engineering students,\nits impact on students' learning and performance, commonly encountered\nchallenges in using generative AI, and future prospects for its adoption in\nengineering education. The results showed that more than half of the\nparticipants reported a positive impact of generative AI on their learning\nefficiency, initiative, and creativity, with nearly half believing it also\nenhanced their independent thinking. However, despite acknowledging improved\nstudy efficiency, many felt their actual academic performance remained largely\nunchanged and expressed concerns about the accuracy and domain-specific\nreliability of generative AI. Our findings provide a first-hand insight into\nthe current benefits and challenges generative AI brings to students,\nparticularly Chinese engineering students, while offering several\nrecommendations, especially from the students' perspective, for effectively\nintegrating generative AI into engineering education.", "AI": {"tldr": "This study investigates the impact of generative AI on engineering students' learning experiences across China, revealing both benefits and challenges in its adoption.", "motivation": "To explore the potential applications of generative AI in higher education, particularly in engineering disciplines, and assess its impact on students' learning experiences.", "method": "Surveys were conducted with 148 students from diverse engineering disciplines and regions in China, focusing on usage frequency, learning impact, challenges, and future prospects of generative AI in education.", "result": "More than half of the students reported positive effects on learning efficiency, initiative, and creativity, while concerns were raised about academic performance, accuracy, and reliability of generative AI.", "conclusion": "Generative AI offers both opportunities and challenges for engineering education, necessitating recommendations for effective integration based on student perspectives.", "key_contributions": ["First-hand insights into the benefits and challenges of generative AI for engineering students", "Recommendations for integrating generative AI into engineering education", "Empirical data on student perceptions of generative AI usage"], "limitations": "The study is based on self-reported data from a specific demographic (engineering students in China), which may not be generalizable to other fields or regions.", "keywords": ["Generative AI", "Engineering Education", "Learning Experience", "Higher Education", "Student Perspectives"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.09068", "pdf": "https://arxiv.org/pdf/2505.09068.pdf", "abs": "https://arxiv.org/abs/2505.09068", "title": "S-DAT: A Multilingual, GenAI-Driven Framework for Automated Divergent Thinking Assessment", "authors": ["Jennifer Haase", "Paul H. P. Hanel", "Sebastian Pokutta"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "This paper introduces S-DAT (Synthetic-Divergent Association Task), a\nscalable, multilingual framework for automated assessment of divergent thinking\n(DT) -a core component of human creativity. Traditional creativity assessments\nare often labor-intensive, language-specific, and reliant on subjective human\nratings, limiting their scalability and cross-cultural applicability. In\ncontrast, S-DAT leverages large language models and advanced multilingual\nembeddings to compute semantic distance -- a language-agnostic proxy for DT. We\nevaluate S-DAT across eleven diverse languages, including English, Spanish,\nGerman, Russian, Hindi, and Japanese (Kanji, Hiragana, Katakana), demonstrating\nrobust and consistent scoring across linguistic contexts. Unlike prior DAT\napproaches, the S-DAT shows convergent validity with other DT measures and\ncorrect discriminant validity with convergent thinking. This cross-linguistic\nflexibility allows for more inclusive, global-scale creativity research,\naddressing key limitations of earlier approaches. S-DAT provides a powerful\ntool for fairer, more comprehensive evaluation of cognitive flexibility in\ndiverse populations and can be freely assessed online:\nhttps://sdat.iol.zib.de/.", "AI": {"tldr": "S-DAT is a novel multilingual framework for automated assessment of divergent thinking using large language models to compute semantic distance as a language-agnostic measure.", "motivation": "Traditional assessments of divergent thinking are labor-intensive, language-specific, and subjective, making them less scalable and applicable across cultures. S-DAT aims to address these limitations.", "method": "The framework utilizes large language models and advanced multilingual embeddings to assess divergent thinking by computing semantic distances in a language-agnostic manner across various languages.", "result": "S-DAT was evaluated in eleven languages, demonstrating consistent and robust scoring for divergent thinking and showing convergent and discriminant validity compared to other measures of thinking.", "conclusion": "S-DAT provides an innovative and inclusive approach to assessing cognitive flexibility in diverse populations, enabling global-scale creativity research and fair evaluations.", "key_contributions": ["Introduction of S-DAT for multilingual divergent thinking assessment", "Cross-linguistic evaluation across eleven languages", "Establishment of convergent and discriminant validity with other thinking measures."], "limitations": "", "keywords": ["divergent thinking", "multilingual framework", "creativity assessment", "large language models", "automated evaluation"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2505.09283", "pdf": "https://arxiv.org/pdf/2505.09283.pdf", "abs": "https://arxiv.org/abs/2505.09283", "title": "A Note on Semantic Diffusion", "authors": ["Alexander P. Ryjov", "Alina A. Egorova"], "categories": ["cs.HC", "03E72, 68T37, 94D05", "I.2.8; I.2.10"], "comment": "8 figures", "summary": "This paper provides an in-depth examination of the concept of semantic\ndiffusion as a complementary instrument to large language models (LLMs) for\ndesign applications. Conventional LLMs and diffusion models fail to induce a\nconvergent, iterative refinement process: each invocation of the diffusion\nmechanism spawns a new stochastic cycle, so successive outputs do not relate to\nprior ones and convergence toward a desired design is not guaranteed. The\nproposed hybrid framework - \"LLM + semantic diffusion\" - resolves this\nlimitation by enforcing an approximately convergent search procedure, thereby\nformally addressing the problem of localized design refinement.", "AI": {"tldr": "This paper introduces a hybrid framework combining large language models and semantic diffusion to enhance iterative refinement in design applications.", "motivation": "The limitation of traditional LLMs and diffusion models in producing convergent design outputs motivated the development of this hybrid framework.", "method": "The proposed framework enhances the diffusion mechanism by ensuring a convergent search process for design refinements.", "result": "The hybrid model demonstrated improved capability for localized design refinement by addressing the stochastic nature of output generation in conventional models.", "conclusion": "This work presents a novel approach that formalizes the design refinement process, potentially improving the usability of models in design settings.", "key_contributions": ["Introduction of a hybrid LLM + semantic diffusion framework", "Formalization of the convergence process in design outputs", "Improvement of iterative refinement in design applications"], "limitations": "", "keywords": ["semantic diffusion", "large language models", "design applications"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.09082", "pdf": "https://arxiv.org/pdf/2505.09082.pdf", "abs": "https://arxiv.org/abs/2505.09082", "title": "CEC-Zero: Chinese Error Correction Solution Based on LLM", "authors": ["Sophie Zhang", "Zhiming Lin"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in large language models (LLMs) demonstrate exceptional\nChinese text processing capabilities, particularly in Chinese Spelling\nCorrection (CSC). While LLMs outperform traditional BERT-based models in\naccuracy and robustness, challenges persist in reliability and generalization.\nThis paper proposes CEC-Zero, a novel reinforcement learning (RL) framework\nenabling LLMs to self-correct through autonomous error strategy learning\nwithout external supervision. By integrating RL with LLMs' generative power,\nthe method eliminates dependency on annotated data or auxiliary models.\nExperiments reveal RL-enhanced LLMs achieve industry-viable accuracy and\nsuperior cross-domain generalization, offering a scalable solution for\nreliability optimization in Chinese NLP applications. This breakthrough\nfacilitates LLM deployment in practical Chinese text correction scenarios while\nestablishing a new paradigm for self-improving language models.", "AI": {"tldr": "This paper introduces CEC-Zero, a reinforcement learning framework that allows large language models to self-correct in Chinese text processing without needing external supervision, improving reliability and generalization in applications such as Chinese spelling correction.", "motivation": "To address the reliability and generalization challenges faced by large language models in Chinese NLP, particularly in the context of Chinese Spelling Correction (CSC).", "method": "The paper proposes CEC-Zero, a novel reinforcement learning framework that enables large language models to autonomously learn error correction strategies, eliminating the need for annotated data or auxiliary models.", "result": "Experiments demonstrate that RL-enhanced large language models achieve industry-viable accuracy and better cross-domain generalization for Chinese text correction tasks.", "conclusion": "The proposed framework not only enhances the reliability of LLMs in Chinese NLP applications but also establishes a new paradigm for self-improvement in language models, making them more practical for real-world scenarios.", "key_contributions": ["Introduction of CEC-Zero, a novel RL framework for LLMs.", "The elimination of dependency on annotated data for error correction.", "Demonstrated improvements in cross-domain generalization for Chinese NLP applications."], "limitations": "", "keywords": ["large language models", "reinforcement learning", "Chinese spelling correction"], "importance_score": 6, "read_time_minutes": 12}}
{"id": "2505.09376", "pdf": "https://arxiv.org/pdf/2505.09376.pdf", "abs": "https://arxiv.org/abs/2505.09376", "title": "AfforDance: Personalized AR Dance Learning System with Visual Affordance", "authors": ["Hyunyoung Han", "Jongwon Jang", "Kitaeg Shim", "Sang Ho Yoon"], "categories": ["cs.HC"], "comment": "CHI 2025 Workshop on Beyond Glasses: Future Directions for XR\n  Interactions within the Physical World", "summary": "We propose AfforDance, an augmented reality (AR)-based dance learning system\nthat generates personalized learning content and enhances learning through\nvisual affordances. Our system converts user-selected dance videos into\ninteractive learning experiences by integrating 3D reference avatars, audio\nsynchronization, and adaptive visual cues that guide movement execution. This\nwork contributes to personalized dance education by offering an adaptable,\nuser-centered learning interface.", "AI": {"tldr": "AfforDance is an AR-based dance learning system that personalizes education through 3D avatars and interactive visuals.", "motivation": "To enhance dance learning experiences using augmented reality and personalized content.", "method": "The system converts dance videos into interactive experiences with 3D avatars, audio synchronization, and adaptive visual cues.", "result": "AfforDance offers an improved, adaptable learning interface for personalized dance education.", "conclusion": "The system demonstrates potential for transforming traditional dance education through technology.", "key_contributions": ["Interactive learning through AR", "User-centered design for dance education", "Adaptive visual cues for movement guidance"], "limitations": "", "keywords": ["Augmented Reality", "Dance Education", "Personalized Learning"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2505.09269", "pdf": "https://arxiv.org/pdf/2505.09269.pdf", "abs": "https://arxiv.org/abs/2505.09269", "title": "How an unintended Side Effect of a Research Project led to Boosting the Power of UML", "authors": ["Ulrich Frank", "Pierre Maier"], "categories": ["cs.CL"], "comment": null, "summary": "This paper describes the design, implementation and use of a new UML modeling\ntool that represents a significant advance over conventional tools. Among other\nthings, it allows the integration of class diagrams and object diagrams as well\nas the execution of objects. This not only enables new software architectures\ncharacterized by the integration of software with corresponding object models,\nbut is also ideal for use in teaching, as it provides students with a\nparticularly stimulating learning experience. A special feature of the project\nis that it has emerged from a long-standing international research project,\nwhich is aimed at a comprehensive multi-level architecture. The project is\ntherefore an example of how research can lead to valuable results that arise as\na side effect of other work.", "AI": {"tldr": "The paper presents a novel UML modeling tool that integrates class and object diagrams while allowing object execution, enhancing both software architecture and educational experiences.", "motivation": "To provide an advanced UML modeling tool that integrates various aspects of software modeling, improving both software architecture and educational tools for teaching.", "method": "The design and implementation of a new UML modeling tool that integrates class diagrams with object diagrams and supports object execution.", "result": "The new tool offers a stimulating learning experience for students and demonstrates the potential of research to yield practical outcomes from an international collaborative project.", "conclusion": "The integration of software with object models through this UML tool signifies a progression in the capability of modeling tools in the software development field.", "key_contributions": ["Integration of class and object diagrams", "Execution of objects within the modeling tool", "Enhancement of educational practices in software modeling"], "limitations": "", "keywords": ["UML modeling", "software architecture", "educational tools"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.09402", "pdf": "https://arxiv.org/pdf/2505.09402.pdf", "abs": "https://arxiv.org/abs/2505.09402", "title": "Utilization of Skin Color Change for Image-based Tactile Sensing", "authors": ["Seitaro Kaneko", "Hiroki Ishizuka", "Hidenori Yoshimura", "Hiroyuki Kajimoto"], "categories": ["cs.HC"], "comment": "This is the accepted version of the following article:Medical\n  Engineering & Physics, which has been published in final form at\n  https://doi.org/10.1016/j.medengphy.2025.104357", "summary": "Measurement of pressure distribution applied to a fingertip is crucial for\nthe teleoperation of robots and human computer interface. Previous studies have\nacquired pressure distribution by affixing a sensor array to the fingertip or\nby optically recording the deformation of an object. However, these existing\nmethods inhibit the fingertip from directly contacting the texture, and the\npressure applied to the fingertip is measured indirectly. In this study, we\npropose a method to measure pressure distribution by directly touching a\ntransparent object, focusing on the change in skin color induced by the applied\npressure, caused by blood flow. We evaluated the relationship between pressure\nand skin color change when local pressure is applied, and found a correlation\nbetween the pressure and the color change. However, the contact area and the\ncolor change area did not align perfectly. We further explored the factor\ncausing the spatial non-uniformity of the color change, by accounting for the\nstress distribution using finite element analysis. These results suggest that\nthe proposed measurement method can be utilized to measure the internal stress\ndistribution, and it is anticipated to serve as a simple sensor in the field of\nhuman computer interface.", "AI": {"tldr": "Proposes a direct method to measure fingertip pressure distribution using skin color change due to blood flow.", "motivation": "To overcome limitations of existing pressure measurement methods that do not allow direct contact with textured surfaces.", "method": "A novel technique measuring pressure distribution through direct touch with a transparent object and monitoring skin color changes via blood flow.", "result": "Establishes a correlation between applied pressure and skin color change, using finite element analysis to address spatial non-uniformities in color change.", "conclusion": "The proposed method can effectively measure internal stress distribution and may serve as a simple sensor in human-computer interactions.", "key_contributions": ["Direct measurement of pressure distribution via skin color change", "Analysis of the relationship between applied pressure and color change", "Finite element analysis to understand spatial non-uniformity."], "limitations": "The contact area and the color change area did not align perfectly.", "keywords": ["pressure measurement", "skin color change", "human-computer interaction", "finite element analysis", "sensor technology"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.09286", "pdf": "https://arxiv.org/pdf/2505.09286.pdf", "abs": "https://arxiv.org/abs/2505.09286", "title": "A Scalable Unsupervised Framework for multi-aspect labeling of Multilingual and Multi-Domain Review Data", "authors": ["Jiin Park", "Misuk Kim"], "categories": ["cs.CL"], "comment": "36 pages, 3 figures", "summary": "Effectively analyzing online review data is essential across industries.\nHowever, many existing studies are limited to specific domains and languages or\ndepend on supervised learning approaches that require large-scale labeled\ndatasets. To address these limitations, we propose a multilingual, scalable,\nand unsupervised framework for cross-domain aspect detection. This framework is\ndesigned for multi-aspect labeling of multilingual and multi-domain review\ndata. In this study, we apply automatic labeling to Korean and English review\ndatasets spanning various domains and assess the quality of the generated\nlabels through extensive experiments. Aspect category candidates are first\nextracted through clustering, and each review is then represented as an\naspect-aware embedding vector using negative sampling. To evaluate the\nframework, we conduct multi-aspect labeling and fine-tune several pretrained\nlanguage models to measure the effectiveness of the automatically generated\nlabels. Results show that these models achieve high performance, demonstrating\nthat the labels are suitable for training. Furthermore, comparisons with\npublicly available large language models highlight the framework's superior\nconsistency and scalability when processing large-scale data. A human\nevaluation also confirms that the quality of the automatic labels is comparable\nto those created manually. This study demonstrates the potential of a robust\nmulti-aspect labeling approach that overcomes limitations of supervised methods\nand is adaptable to multilingual, multi-domain environments. Future research\nwill explore automatic review summarization and the integration of artificial\nintelligence agents to further improve the efficiency and depth of review\nanalysis.", "AI": {"tldr": "This paper presents an unsupervised, multilingual framework for cross-domain aspect detection in online reviews, leveraging negative sampling and pretrained language models for high-performance automatic labeling.", "motivation": "To address limitations in existing studies that focus on specific domains, languages, and supervised learning approaches requiring large labeled datasets.", "method": "The proposed framework extracts aspect category candidates through clustering and represents each review as an aspect-aware embedding vector using negative sampling, followed by multi-aspect labeling and fine-tuning of pretrained language models.", "result": "The framework achieves high performance in generating suitable labels for training and demonstrates superior consistency and scalability compared to other large language models. Human evaluations indicate that automatic labels are comparable to manual ones.", "conclusion": "This study showcases a robust multi-aspect labeling approach applicable to multilingual and multi-domain review data, with future research directions including automatic review summarization.", "key_contributions": ["Multilingual and unsupervised framework for aspect detection", "High-performance automatic labeling with pretrained models", "Validation of automatic labels through human evaluation"], "limitations": "", "keywords": ["aspect detection", "multilingual", "unsupervised learning", "review analysis", "language models"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.09478", "pdf": "https://arxiv.org/pdf/2505.09478.pdf", "abs": "https://arxiv.org/abs/2505.09478", "title": "Card Sorting Simulator: Augmenting Design of Logical Information Architectures with Large Language Models", "authors": ["Eduard Kuric", "Peter Demcak", "Matus Krajcovic"], "categories": ["cs.HC"], "comment": null, "summary": "Card sorting is a common ideation technique that elicits information on\nusers' mental organization of content and functionality by having them sort\nitems into categories. For more robust card sorting research, digital card\nsorting tools could benefit from providing quick automated feedback. Our\nobjective of this research is to advance toward an instrument that applies\nartificial intelligence (AI) to augment card sorting. For this purpose, we\ndevelop the Card Sorting Simulator, a prototype tool that leverages Large\nLanguage Models (LLMs) to generate informative categorizations of cards. To\nilluminate how aligned the simulation is with card sorting by actual\nparticipants, and to inform the instrument's design decisions, we conducted a\ngeneralizability-focused comparative study. We obtained 28 pre-existing card\nsorting studies from real practitioners, comprising 1,399 participants, along\nwith diverse contents and origins. With this dataset, we conducted a\ncomprehensive and nuanced analysis of the agreement between actual card sorting\nresults (clusterings of cards) and synthetic clusterings across a multitude of\nLLMs and prompt designs. Mutual information scores indicate a good degree of\nagreement to real result clustering, although similarity matrices also\ndemonstrate inconsistencies from mental models, which can be attributed to\ntheir top-down nature. Furthermore, the number of cards or complexity of their\nlabels impact the accuracy of its simulation. These findings bolster the case\nfor AI augmentation in card sorting research as a source of meaningful\npreliminary feedback and highlight the need for further study for the\ndevelopment and validation of intelligent user research tools.", "AI": {"tldr": "This paper presents the development of a Card Sorting Simulator that uses AI to enhance card sorting methodologies in HCI research by generating categorizations, and evaluates its effectiveness against actual participant data.", "motivation": "To improve digital card sorting tools through automated feedback using AI, aiming to enhance the ideation process for understanding user content organization.", "method": "A prototype tool, the Card Sorting Simulator, harnesses Large Language Models (LLMs) to produce categorizations of cards. A comparative study analyzed data from 28 existing card sorting studies with 1,399 participants to compare AI-generated clusterings with actual participant results.", "result": "The study found a good degree of agreement between AI-generated categorizations and real participant data, though there were noted inconsistencies related to the complexity of card labels and their impact on simulation accuracy.", "conclusion": "AI can augment card sorting research by providing valuable preliminary feedback, but further studies are needed to refine and validate AI-driven tools in user research.", "key_contributions": ["Development of an AI-based Card Sorting Simulator", "Analysis of AI clustering against real participant data", "Provision of insights on improving card sorting methodologies using AI."], "limitations": "The study notes inconsistencies from the use of top-down models and the impact of card complexity on simulation accuracy.", "keywords": ["card sorting", "human-computer interaction", "artificial intelligence", "large language models", "user research tools"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.09316", "pdf": "https://arxiv.org/pdf/2505.09316.pdf", "abs": "https://arxiv.org/abs/2505.09316", "title": "Scent of Knowledge: Optimizing Search-Enhanced Reasoning with Information Foraging", "authors": ["Hongjin Qian", "Zheng Liu"], "categories": ["cs.CL", "cs.IR"], "comment": "16 pages", "summary": "Augmenting large language models (LLMs) with external retrieval has become a\nstandard method to address their inherent knowledge cutoff limitations.\nHowever, traditional retrieval-augmented generation methods employ static,\npre-inference retrieval strategies, making them inadequate for complex tasks\ninvolving ambiguous, multi-step, or evolving information needs. Recent advances\nin test-time scaling techniques have demonstrated significant potential in\nenabling LLMs to dynamically interact with external tools, motivating the shift\ntoward adaptive inference-time retrieval. Inspired by Information Foraging\nTheory (IFT), we propose InForage, a reinforcement learning framework that\nformalizes retrieval-augmented reasoning as a dynamic information-seeking\nprocess. Unlike existing approaches, InForage explicitly rewards intermediate\nretrieval quality, encouraging LLMs to iteratively gather and integrate\ninformation through adaptive search behaviors. To facilitate training, we\nconstruct a human-guided dataset capturing iterative search and reasoning\ntrajectories for complex, real-world web tasks. Extensive evaluations across\ngeneral question answering, multi-hop reasoning tasks, and a newly developed\nreal-time web QA dataset demonstrate InForage's superior performance over\nbaseline methods. These results highlight InForage's effectiveness in building\nrobust, adaptive, and efficient reasoning agents.", "AI": {"tldr": "InForage is a reinforcement learning framework that enhances LLMs by enabling dynamic, adaptive retrieval during inference, outperforming traditional static methods in complex tasks.", "motivation": "Existing retrieval-augmented generation methods are static and inadequate for complex, evolving information needs. The need for adaptive inference-time retrieval is motivating this research.", "method": "InForage formalizes retrieval-augmented reasoning as a dynamic information-seeking process using reinforcement learning, rewarding intermediate retrieval quality.", "result": "InForage shows superior performance in general question answering and multi-hop reasoning tasks compared to baseline methods, demonstrating robust and efficient reasoning capabilities.", "conclusion": "The evaluations underscore InForage's effectiveness in developing LLMs that can adaptively gather and integrate information for complex tasks.", "key_contributions": ["Proposes a novel reinforcement learning framework (InForage) for adaptive retrieval in LLMs.", "Introduces a human-guided dataset for training iterative search and reasoning.", "Demonstrates significant performance improvements in reasoning tasks over traditional methods."], "limitations": "", "keywords": ["large language models", "adaptive retrieval", "reinforcement learning", "information foraging theory", "dynamic reasoning"], "importance_score": 9, "read_time_minutes": 16}}
{"id": "2505.09509", "pdf": "https://arxiv.org/pdf/2505.09509.pdf", "abs": "https://arxiv.org/abs/2505.09509", "title": "Partnership through Play: Investigating How Long-Distance Couples Use Digital Games to Facilitate Intimacy", "authors": ["Nisha Devasia", "Adrian Rodriguez", "Logan Tuttle", "Julie Kientz"], "categories": ["cs.HC"], "comment": "Proceedings of Designing Interactive Systems (DIS '25)", "summary": "Long-distance relationships (LDRs) have become more common in the last few\ndecades, primarily among young adults pursuing educational or employment\nopportunities. A common way for couples in LDRs to spend time together is by\nplaying multiplayer video games, which are often a shared hobby and therefore a\npreferred joint activity. However, games are relatively understudied in the\ncontext of relational maintenance for LDRs. In this work, we used a\nmixed-methods approach to collect data on the experiences of 13 couples in LDRs\nwho frequently play games together. We investigated different values around\nvarious game mechanics and modalities and found significant differences in\ncouple play styles, and also detail how couples appropriate game mechanics to\nexpress affection to each other virtually. We also created prototypes and\ndesign implications based on couples' needs surrounding the lack of physical\nsensation and memorabilia storage in most popular games.", "AI": {"tldr": "This study explores how couples in long-distance relationships use multiplayer video games for relational maintenance and affection expression.", "motivation": "To investigate the role of multiplayer video games in maintaining long-distance relationships, which are increasingly common among young adults.", "method": "A mixed-methods approach was employed, collecting data from 13 couples in long-distance relationships who frequently play games together.", "result": "The study identified significant differences in couple play styles and how game mechanics are appropriated for virtual affection.", "conclusion": "Prototypes and design implications were created to address the needs of couples regarding physical sensation and memorabilia storage in popular games.", "key_contributions": ["Investigated couple play styles in multiplayer gaming among LDRs.", "Highlighted the appropriation of game mechanics for expressing affection.", "Developed prototypes addressing the limitations of physical sensations in gaming."], "limitations": "Limited sample size of 13 couples may not represent all LDR experiences.", "keywords": ["Long-distance relationships", "Multiplayer video games", "Relational maintenance", "Couple play styles", "Game design"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.09338", "pdf": "https://arxiv.org/pdf/2505.09338.pdf", "abs": "https://arxiv.org/abs/2505.09338", "title": "Llama See, Llama Do: A Mechanistic Perspective on Contextual Entrainment and Distraction in LLMs", "authors": ["Jingcheng Niu", "Xingdi Yuan", "Tong Wang", "Hamidreza Saghir", "Amir H. Abdi"], "categories": ["cs.CL"], "comment": null, "summary": "We observe a novel phenomenon, contextual entrainment, across a wide range of\nlanguage models (LMs) and prompt settings, providing a new mechanistic\nperspective on how LMs become distracted by ``irrelevant'' contextual\ninformation in the input prompt. Specifically, LMs assign significantly higher\nlogits (or probabilities) to any tokens that have previously appeared in the\ncontext prompt, even for random tokens. This suggests that contextual\nentrainment is a mechanistic phenomenon, occurring independently of the\nrelevance or semantic relation of the tokens to the question or the rest of the\nsentence. We find statistically significant evidence that the magnitude of\ncontextual entrainment is influenced by semantic factors. Counterfactual\nprompts have a greater effect compared to factual ones, suggesting that while\ncontextual entrainment is a mechanistic phenomenon, it is modulated by semantic\nfactors.\n  We hypothesise that there is a circuit of attention heads -- the entrainment\nheads -- that corresponds to the contextual entrainment phenomenon. Using a\nnovel entrainment head discovery method based on differentiable masking, we\nidentify these heads across various settings. When we ``turn off'' these heads,\ni.e., set their outputs to zero, the effect of contextual entrainment is\nsignificantly attenuated, causing the model to generate output that capitulates\nto what it would produce if no distracting context were provided. Our discovery\nof contextual entrainment, along with our investigation into LM distraction via\nthe entrainment heads, marks a key step towards the mechanistic analysis and\nmitigation of the distraction problem.", "AI": {"tldr": "This paper introduces the phenomenon of contextual entrainment in language models, where irrelevant contextual information influences model outputs. It presents a method to identify attention heads responsible for this effect and provides insights for mitigating distractions in language models.", "motivation": "The study aims to explore how language models are affected by irrelevant contextual information in prompts, termed as contextual entrainment, and its implications for model performance.", "method": "The authors propose a novel method for discovering 'entrainment heads' in attention mechanisms through differentiable masking, examining their impact on model outputs under different prompt conditions.", "result": "The findings indicate that language models assign higher probabilities to tokens previously encountered in the input context, and that the degree of this effect is influenced by the semantic relevance of the prompt's content. Turning off the identified entrainment heads reduces this distraction effect.", "conclusion": "The research contributes to understanding the mechanistic factors behind distractions in language models and lays the groundwork for developing strategies to mitigate these issues in practical applications.", "key_contributions": ["Introduction of the contextual entrainment phenomenon", "Identification of attention heads related to contextual distraction", "Method to reduce the effect of contextually irrelevant information on model outputs"], "limitations": "", "keywords": ["contextual entrainment", "language models", "distraction problem"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.09526", "pdf": "https://arxiv.org/pdf/2505.09526.pdf", "abs": "https://arxiv.org/abs/2505.09526", "title": "Evaluation Metrics for Misinformation Warning Interventions: Challenges and Prospects", "authors": ["Hussaini Zubairu", "Abdelrahaman Abdou", "Ashraf Matrawy"], "categories": ["cs.HC"], "comment": "10 pages, 2 figures", "summary": "Misinformation has become a widespread issue in the 21st century, impacting\nnumerous areas of society and underscoring the need for effective intervention\nstrategies. Among these strategies, user-centered interventions, such as\nwarning systems, have shown promise in reducing the spread of misinformation.\nMany studies have used various metrics to evaluate the effectiveness of these\nwarning interventions. However, no systematic review has thoroughly examined\nthese metrics in all studies. This paper provides a comprehensive review of\nexisting metrics for assessing the effectiveness of misinformation warnings,\ncategorizing them into four main groups: behavioral impact, trust and\ncredulity, usability, and cognitive and psychological effects. Through this\nreview, we identify critical challenges in measuring the effectiveness of\nmisinformation warnings, including inconsistent use of cognitive and\nattitudinal metrics, the lack of standardized metrics for affective and\nemotional impact, variations in user trust, and the need for more inclusive\nwarning designs. We present an overview of these metrics and propose areas for\nfuture research.", "AI": {"tldr": "This paper reviews metrics for evaluating the effectiveness of user-centered interventions against misinformation, identifying challenges and proposing future research directions.", "motivation": "Misinformation is a significant societal issue that necessitates effective intervention strategies, specifically user-centered interventions like warning systems.", "method": "The paper categorizes and reviews existing metrics used to assess the effectiveness of misinformation warnings into four groups: behavioral impact, trust and credulity, usability, and cognitive and psychological effects.", "result": "The review highlights critical challenges in measuring effectiveness, including inconsistency in cognitive metrics, lack of standardized emotional impact metrics, variations in user trust, and the need for inclusive warning designs.", "conclusion": "The paper outlines the current gaps in research and suggests areas for future studies to improve the design and assessment of misinformation warnings.", "key_contributions": ["Comprehensive categorization of metrics for misinformation warning effectiveness", "Identification of key challenges in current approaches", "Proposals for future research directions in the field"], "limitations": "No systematic review has thoroughly examined these metrics until now, indicating gaps in existing research.", "keywords": ["misinformation", "user-centered interventions", "effectiveness metrics", "behavioral impact", "trust"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.09388", "pdf": "https://arxiv.org/pdf/2505.09388.pdf", "abs": "https://arxiv.org/abs/2505.09388", "title": "Qwen3 Technical Report", "authors": ["An Yang", "Anfeng Li", "Baosong Yang", "Beichen Zhang", "Binyuan Hui", "Bo Zheng", "Bowen Yu", "Chang Gao", "Chengen Huang", "Chenxu Lv", "Chujie Zheng", "Dayiheng Liu", "Fan Zhou", "Fei Huang", "Feng Hu", "Hao Ge", "Haoran Wei", "Huan Lin", "Jialong Tang", "Jian Yang", "Jianhong Tu", "Jianwei Zhang", "Jianxin Yang", "Jiaxi Yang", "Jing Zhou", "Jingren Zhou", "Junyang Lin", "Kai Dang", "Keqin Bao", "Kexin Yang", "Le Yu", "Lianghao Deng", "Mei Li", "Mingfeng Xue", "Mingze Li", "Pei Zhang", "Peng Wang", "Qin Zhu", "Rui Men", "Ruize Gao", "Shixuan Liu", "Shuang Luo", "Tianhao Li", "Tianyi Tang", "Wenbiao Yin", "Xingzhang Ren", "Xinyu Wang", "Xinyu Zhang", "Xuancheng Ren", "Yang Fan", "Yang Su", "Yichang Zhang", "Yinger Zhang", "Yu Wan", "Yuqiong Liu", "Zekun Wang", "Zeyu Cui", "Zhenru Zhang", "Zhipeng Zhou", "Zihan Qiu"], "categories": ["cs.CL"], "comment": null, "summary": "In this work, we present Qwen3, the latest version of the Qwen model family.\nQwen3 comprises a series of large language models (LLMs) designed to advance\nperformance, efficiency, and multilingual capabilities. The Qwen3 series\nincludes models of both dense and Mixture-of-Expert (MoE) architectures, with\nparameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is\nthe integration of thinking mode (for complex, multi-step reasoning) and\nnon-thinking mode (for rapid, context-driven responses) into a unified\nframework. This eliminates the need to switch between different models--such as\nchat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g.,\nQwQ-32B)--and enables dynamic mode switching based on user queries or chat\ntemplates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing\nusers to allocate computational resources adaptively during inference, thereby\nbalancing latency and performance based on task complexity. Moreover, by\nleveraging the knowledge from the flagship models, we significantly reduce the\ncomputational resources required to build smaller-scale models, while ensuring\ntheir highly competitive performance. Empirical evaluations demonstrate that\nQwen3 achieves state-of-the-art results across diverse benchmarks, including\ntasks in code generation, mathematical reasoning, agent tasks, etc.,\ncompetitive against larger MoE models and proprietary models. Compared to its\npredecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119\nlanguages and dialects, enhancing global accessibility through improved\ncross-lingual understanding and generation capabilities. To facilitate\nreproducibility and community-driven research and development, all Qwen3 models\nare publicly accessible under Apache 2.0.", "AI": {"tldr": "Qwen3 introduces a series of advanced large language models with enhanced performance, efficiency, and multilingual capabilities, featuring innovative modes of operation and dynamic resource allocation.", "motivation": "To develop a versatile series of large language models that improve upon existing LLMs in terms of performance, efficiency, and multilingual capabilities while facilitating user adaptability.", "method": "The Qwen3 model series implements both dense and Mixture-of-Expert architectures with innovative features such as integrated thinking modes and a thinking budget mechanism for adaptive computational resource allocation.", "result": "Qwen3 achieves state-of-the-art performance in various benchmarks, including code generation and mathematical reasoning, while expanding multilingual support to 119 languages and dialects.", "conclusion": "Qwen3 represents a significant advancement in LLM technology, enabling improved accessibility and competitive performance even at smaller scales; all models are publicly available to promote community research.", "key_contributions": ["Integration of thinking and non-thinking modes for flexible model use", "Introduction of a thinking budget for adaptive resource allocation", "Expansion of multilingual capabilities from 29 to 119 languages"], "limitations": "", "keywords": ["large language models", "multilingual support", "adaptive resource allocation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.09583", "pdf": "https://arxiv.org/pdf/2505.09583.pdf", "abs": "https://arxiv.org/abs/2505.09583", "title": "Beyond Likes: How Normative Feedback Complements Engagement Signals on Social Media", "authors": ["Yuchen Wu", "Mingduo Zhao", "John Canny"], "categories": ["cs.HC"], "comment": null, "summary": "Many online platforms incorporate engagement signals--such as likes and\nupvotes--into their content ranking systems and interface design. These signals\nare designed to boost user engagement. However, they can unintentionally\nelevate content that is less inclusive and may not support normatively\ndesirable behavior. This issue becomes especially concerning when toxic content\ncorrelates strongly with popularity indicators such as likes and upvotes. In\nthis study, we propose structured prosocial feedback as a complementary signal\nto likes and upvotes--one that highlights content quality based on normative\ncriteria to help address the limitations of conventional engagement signals. We\nbegin by designing and implementing a machine learning feedback system powered\nby a large language model (LLM), which evaluates user comments based on\nprinciples of positive psychology, such as individual well-being, constructive\nsocial media use, and character strengths. We then conduct a pre-registered\nuser study to examine how existing peer-based and the new expert-based feedback\ninteract to shape users' selection of comments in a social media setting.\nResults show that peer feedback increases conformity to popularity cues, while\nexpert feedback shifts preferences toward normatively higher-quality content.\nMoreover, incorporating expert feedback alongside peer evaluations improves\nalignment with expert assessments and contributes to a less toxic community\nenvironment. This illustrates the added value of normative cues--such as expert\nscores generated by LLMs using psychological rubrics--and underscores the\npotential benefits of incorporating such signals into platform feedback systems\nto foster healthier online environments.", "AI": {"tldr": "The study proposes structured prosocial feedback using machine learning and LLMs to counteract the negative impact of conventional engagement signals on content quality in online platforms.", "motivation": "Conventional engagement signals like likes and upvotes can promote toxic content and hinder inclusivity. This study aims to find a solution to enhance content quality and user engagement positively.", "method": "A machine learning feedback system powered by a large language model (LLM) was designed to evaluate user comments based on positive psychology principles, followed by a pre-registered user study to assess its impact.", "result": "The study found that peer feedback increases conformity to popularity cues, while expert feedback led users toward higher-quality content. Combining expert feedback with peer evaluations improved both expert alignment and reduced toxicity in the community.", "conclusion": "Incorporating normative signals like expert scores into feedback systems can foster healthier online environments by addressing the limitations of traditional engagement metrics.", "key_contributions": ["Introduction of structured prosocial feedback as a complement to traditional engagement signals.", "Implementation of a machine learning system that evaluates content based on psychological principles.", "Evidence showing expert feedback reduces toxicity and enhances content quality in social media."], "limitations": "The study is limited to the specific social media context tested and may not generalize to all online platforms.", "keywords": ["prosociial feedback", "machine learning", "social media", "toxic content", "expert feedback"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.09407", "pdf": "https://arxiv.org/pdf/2505.09407.pdf", "abs": "https://arxiv.org/abs/2505.09407", "title": "Multilingual Machine Translation with Quantum Encoder Decoder Attention-based Convolutional Variational Circuits", "authors": ["Subrit Dikshit", "Ritu Tiwari", "Priyank Jain"], "categories": ["cs.CL", "cs.AI", "cs.ET"], "comment": "12 pages, 12 figures", "summary": "Cloud-based multilingual translation services like Google Translate and\nMicrosoft Translator achieve state-of-the-art translation capabilities. These\nservices inherently use large multilingual language models such as GRU, LSTM,\nBERT, GPT, T5, or similar encoder-decoder architectures with attention\nmechanisms as the backbone. Also, new age natural language systems, for\ninstance ChatGPT and DeepSeek, have established huge potential in multiple\ntasks in natural language processing. At the same time, they also possess\noutstanding multilingual translation capabilities. However, these models use\nthe classical computing realm as a backend. QEDACVC (Quantum Encoder Decoder\nAttention-based Convolutional Variational Circuits) is an alternate solution\nthat explores the quantum computing realm instead of the classical computing\nrealm to study and demonstrate multilingual machine translation. QEDACVC\nintroduces the quantum encoder-decoder architecture that simulates and runs on\nquantum computing hardware via quantum convolution, quantum pooling, quantum\nvariational circuit, and quantum attention as software alterations. QEDACVC\nachieves an Accuracy of 82% when trained on the OPUS dataset for English,\nFrench, German, and Hindi corpora for multilingual translations.", "AI": {"tldr": "This paper presents QEDACVC, a quantum computing-based multilingual translation model that outperforms traditional methods by utilizing quantum architectures.", "motivation": "To explore multilingual machine translation using quantum computing as an alternative to classical models, which include state-of-the-art techniques like GPT and BERT.", "method": "QEDACVC uses a quantum encoder-decoder architecture that incorporates quantum convolution, pooling, variational circuits, and attention mechanisms, implemented on quantum hardware.", "result": "The model achieved an accuracy of 82% on the OPUS dataset across English, French, German, and Hindi corpora.", "conclusion": "QEDACVC demonstrates the feasibility of using quantum computing in multilingual translation and has the potential to enhance performance over classical methods.", "key_contributions": ["Introduction of a quantum encoder-decoder architecture for multilingual translation.", "Demonstration of quantum convolution and attention mechanisms in NLP tasks.", "Achieved 82% accuracy on multilingual datasets, showcasing the model's effectiveness."], "limitations": "The implementation requires quantum computing hardware, which may not be widely accessible.", "keywords": ["multilingual translation", "quantum computing", "natural language processing"], "importance_score": 7, "read_time_minutes": 12}}
{"id": "2505.09519", "pdf": "https://arxiv.org/pdf/2505.09519.pdf", "abs": "https://arxiv.org/abs/2505.09519", "title": "PT-MoE: An Efficient Finetuning Framework for Integrating Mixture-of-Experts into Prompt Tuning", "authors": ["Zongqian Li", "Yixuan Su", "Nigel Collier"], "categories": ["cs.CL"], "comment": null, "summary": "Parameter-efficient fine-tuning (PEFT) methods have shown promise in adapting\nlarge language models, yet existing approaches exhibit counter-intuitive\nphenomena: integrating router into prompt tuning (PT) increases training\nefficiency yet does not improve performance universally; parameter reduction\nthrough matrix decomposition can improve performance in specific domains.\nMotivated by these observations and the modular nature of PT, we propose\nPT-MoE, a novel framework that integrates matrix decomposition with\nmixture-of-experts (MoE) routing for efficient PT. Results across 17 datasets\ndemonstrate that PT-MoE achieves state-of-the-art performance in both question\nanswering (QA) and mathematical problem solving tasks, improving F1 score by\n1.49 points over PT and 2.13 points over LoRA in QA tasks, while enhancing\nmathematical accuracy by 10.75 points over PT and 0.44 points over LoRA, all\nwhile using 25% fewer parameters than LoRA. Our analysis reveals that while PT\nmethods generally excel in QA tasks and LoRA-based methods in math datasets,\nthe integration of matrix decomposition and MoE in PT-MoE yields complementary\nbenefits: decomposition enables efficient parameter sharing across experts\nwhile MoE provides dynamic adaptation, collectively enabling PT-MoE to\ndemonstrate cross-task consistency and generalization abilities. These\nfindings, along with ablation studies on routing mechanisms and architectural\ncomponents, provide insights for future PEFT methods.", "AI": {"tldr": "PT-MoE integrates matrix decomposition and MoE routing for efficient prompt tuning, achieving state-of-the-art performance on QA and math problem solving tasks with fewer parameters.", "motivation": "The need for efficient mechanisms to adapt large language models while addressing counter-intuitive performance dynamics observed in existing PEFT methods.", "method": "PT-MoE framework combines matrix decomposition with mixture-of-experts routing to enhance training efficiency and performance in prompt tuning.", "result": "PT-MoE outperforms traditional PT and LoRA methods, improving F1 scores in QA tasks and mathematical accuracy while using 25% fewer parameters than LoRA.", "conclusion": "The integration of matrix decomposition and MoE routing in PT-MoE yields improved performance and generalization across tasks, indicating potential for future PEFT methods.", "key_contributions": ["Introduction of the PT-MoE framework for prompt tuning.", "Demonstrated superior performance on 17 datasets compared to existing approaches.", "Insights into the interplay between matrix decomposition and MoE for efficient learning."], "limitations": "", "keywords": ["Parameter-efficient fine-tuning", "Prompt tuning", "Mixture of experts"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.09068", "pdf": "https://arxiv.org/pdf/2505.09068.pdf", "abs": "https://arxiv.org/abs/2505.09068", "title": "S-DAT: A Multilingual, GenAI-Driven Framework for Automated Divergent Thinking Assessment", "authors": ["Jennifer Haase", "Paul H. P. Hanel", "Sebastian Pokutta"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "This paper introduces S-DAT (Synthetic-Divergent Association Task), a\nscalable, multilingual framework for automated assessment of divergent thinking\n(DT) -a core component of human creativity. Traditional creativity assessments\nare often labor-intensive, language-specific, and reliant on subjective human\nratings, limiting their scalability and cross-cultural applicability. In\ncontrast, S-DAT leverages large language models and advanced multilingual\nembeddings to compute semantic distance -- a language-agnostic proxy for DT. We\nevaluate S-DAT across eleven diverse languages, including English, Spanish,\nGerman, Russian, Hindi, and Japanese (Kanji, Hiragana, Katakana), demonstrating\nrobust and consistent scoring across linguistic contexts. Unlike prior DAT\napproaches, the S-DAT shows convergent validity with other DT measures and\ncorrect discriminant validity with convergent thinking. This cross-linguistic\nflexibility allows for more inclusive, global-scale creativity research,\naddressing key limitations of earlier approaches. S-DAT provides a powerful\ntool for fairer, more comprehensive evaluation of cognitive flexibility in\ndiverse populations and can be freely assessed online:\nhttps://sdat.iol.zib.de/.", "AI": {"tldr": "The paper presents S-DAT, a multilingual framework for automated assessment of divergent thinking using language models.", "motivation": "To overcome the limitations of traditional creativity assessments which are labor-intensive, language-specific, and subjective.", "method": "S-DAT leverages large language models and multilingual embeddings to compute a language-agnostic proxy for divergent thinking.", "result": "S-DAT was tested across eleven languages and showed robust, consistent scoring and convergent validation with other divergent thinking measures.", "conclusion": "S-DAT enhances the scalability and inclusivity of creativity research across diverse linguistic contexts.", "key_contributions": ["Multilingual framework for divergent thinking assessment", "Scalable and automated evaluation using language models", "Cross-linguistic validity and flexibility in creativity research"], "limitations": "", "keywords": ["divergent thinking", "creativity assessment", "multilingual", "large language models", "cognitive flexibility"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.09595", "pdf": "https://arxiv.org/pdf/2505.09595.pdf", "abs": "https://arxiv.org/abs/2505.09595", "title": "WorldView-Bench: A Benchmark for Evaluating Global Cultural Perspectives in Large Language Models", "authors": ["Abdullah Mushtaq", "Imran Taj", "Rafay Naeem", "Ibrahim Ghaznavi", "Junaid Qadir"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.MA"], "comment": "Preprint. Submitted to the Journal of Artificial Intelligence\n  Research (JAIR) on April 29, 2025", "summary": "Large Language Models (LLMs) are predominantly trained and aligned in ways\nthat reinforce Western-centric epistemologies and socio-cultural norms, leading\nto cultural homogenization and limiting their ability to reflect global\ncivilizational plurality. Existing benchmarking frameworks fail to adequately\ncapture this bias, as they rely on rigid, closed-form assessments that overlook\nthe complexity of cultural inclusivity. To address this, we introduce\nWorldView-Bench, a benchmark designed to evaluate Global Cultural Inclusivity\n(GCI) in LLMs by analyzing their ability to accommodate diverse worldviews. Our\napproach is grounded in the Multiplex Worldview proposed by Senturk et al.,\nwhich distinguishes between Uniplex models, reinforcing cultural\nhomogenization, and Multiplex models, which integrate diverse perspectives.\nWorldView-Bench measures Cultural Polarization, the exclusion of alternative\nperspectives, through free-form generative evaluation rather than conventional\ncategorical benchmarks. We implement applied multiplexity through two\nintervention strategies: (1) Contextually-Implemented Multiplex LLMs, where\nsystem prompts embed multiplexity principles, and (2) Multi-Agent System\n(MAS)-Implemented Multiplex LLMs, where multiple LLM agents representing\ndistinct cultural perspectives collaboratively generate responses. Our results\ndemonstrate a significant increase in Perspectives Distribution Score (PDS)\nentropy from 13% at baseline to 94% with MAS-Implemented Multiplex LLMs,\nalongside a shift toward positive sentiment (67.7%) and enhanced cultural\nbalance. These findings highlight the potential of multiplex-aware AI\nevaluation in mitigating cultural bias in LLMs, paving the way for more\ninclusive and ethically aligned AI systems.", "AI": {"tldr": "Introducing WorldView-Bench, a benchmark for assessing Global Cultural Inclusivity in LLMs, focusing on diverse worldviews to mitigate cultural bias.", "motivation": "To address the cultural bias in LLMs resulting from Western-centric training methods that standardize epistemologies, leading to cultural homogenization.", "method": "Development of WorldView-Bench to evaluate LLMs on their ability to reflect global cultural diversity, using free-form generative evaluations and multiplexity principles.", "result": "WorldView-Bench showed a significant increase in Perspectives Distribution Score entropy from 13% to 94% with the implementation of Multiplex LLMs, along with improved sentiment and cultural balance.", "conclusion": "Multiplex-aware AI evaluations can effectively reduce cultural bias in LLMs and promote a more inclusive approach to AI system development.", "key_contributions": ["Introduction of WorldView-Bench for assessing Global Cultural Inclusivity in LLMs.", "Implementation of contextually-implemented and multi-agent system strategies for LLMs.", "Demonstrated effectiveness of multiplex-aware approaches in improving cultural diversity metrics in LLMs."], "limitations": "", "keywords": ["Large Language Models", "Cultural Inclusivity", "Bias Mitigation", "Multiplex Worldview", "AI Evaluation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2109.10132", "pdf": "https://arxiv.org/pdf/2109.10132.pdf", "abs": "https://arxiv.org/abs/2109.10132", "title": "Manifesto for Putting 'Chartjunk' in the Trash 2021!", "authors": ["Derya Akbaba", "Jack Wilburn", "Main T. Nance", "Miriah Meyer"], "categories": ["cs.HC"], "comment": "For the associated site, see https://jackwilb.github.io/chart-junk/", "summary": "In this provocation we ask the visualization research community to join us in\nremoving chartjunk from our research lexicon. We present an etymology of\nchartjunk, framing its provocative origins as misaligned, and harmful, to the\nways the term is currently used by visualization researchers. We call on the\ncommunity to dissolve chartjunk from the ways we talk about, write about, and\nthink about the graphical devices we design and study. As a step towards this\ngoal we contribute a performance of maintenance through a trio of acts: editing\nthe Wikipedia page on chartjunk, cutting out chartjunk from IEEE papers, and\nscanning and posting a repository of the pages with chartjunk removed to invite\nthe community to re-imagine how we describe visualizations. This contribution\nblurs the boundaries between research, activism, and maintenance art, and is\nintended to inspire the community to join us in taking out the trash.", "AI": {"tldr": "The paper calls on the visualization research community to eliminate the term 'chartjunk' from their discussions and practices, proposing alternative ways to express and design visualizations.", "motivation": "To challenge and refine the language used in visualization research, particularly the term 'chartjunk', which is seen as misaligned with effective communication and design.", "method": "The authors propose three key actions: editing the Wikipedia page on chartjunk, removing instances of chartjunk from IEEE papers, and creating a repository of visualizations with chartjunk eliminated.", "result": "The authors aim to promote better practices in visualization design by encouraging researchers to reconsider how they talk about and create visualizations, fostering a more constructive dialogue in the community.", "conclusion": "By removing chartjunk from the research lexicon, the authors hope to inspire a more refined understanding and application of visual communication in research.", "key_contributions": ["A historical critique of the term 'chartjunk' and its implications for visualization research.", "A practical proposal to edit existing literature to eliminate chartjunk.", "Creation of a repository enhancing visualization design by showcasing works that discard chartjunk."], "limitations": "", "keywords": ["chartjunk", "visualization", "research", "activism", "maintenance art"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.08902", "pdf": "https://arxiv.org/pdf/2505.08902.pdf", "abs": "https://arxiv.org/abs/2505.08902", "title": "Performance Gains of LLMs With Humans in a World of LLMs Versus Humans", "authors": ["Lucas McCullum", "Pelagie Ami Agassi", "Leo Anthony Celi", "Daniel K. Ebner", "Chrystinne Oliveira Fernandes", "Rachel S. Hicklen", "Mkliwa Koumbia", "Lisa Soleymani Lehmann", "David Restrepo"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "Currently, a considerable research effort is devoted to comparing LLMs to a\ngroup of human experts, where the term \"expert\" is often ill-defined or\nvariable, at best, in a state of constantly updating LLM releases. Without\nproper safeguards in place, LLMs will threaten to cause harm to the established\nstructure of safe delivery of patient care which has been carefully developed\nthroughout history to keep the safety of the patient at the forefront. A key\ndriver of LLM innovation is founded on community research efforts which, if\ncontinuing to operate under \"humans versus LLMs\" principles, will expedite this\ntrend. Therefore, research efforts moving forward must focus on effectively\ncharacterizing the safe use of LLMs in clinical settings that persist across\nthe rapid development of novel LLM models. In this communication, we\ndemonstrate that rather than comparing LLMs to humans, there is a need to\ndevelop strategies enabling efficient work of humans with LLMs in an almost\nsymbiotic manner.", "AI": {"tldr": "This paper discusses the importance of defining the role of LLMs in clinical settings, emphasizing a collaborative approach between humans and LLMs rather than a competitive one.", "motivation": "The rapid evolution of LLMs poses risks to patient care delivery systems, which require careful consideration and safeguards.", "method": "This communication proposes strategies for enabling collaborative interactions between humans and LLMs to ensure patient safety in clinical environments.", "result": "The proposed approach aims to facilitate a symbiotic relationship between healthcare professionals and LLMs, enhancing patient care quality.", "conclusion": "Future research should prioritize the effective integration of LLMs in clinical practice to maintain patient safety and care standards.", "key_contributions": ["Identifies the risks of LLMs in patient care settings.", "Proposes a new framework for human-LLM collaboration.", "Calls for a paradigm shift from competition to collaboration in LLM research."], "limitations": "The paper does not provide empirical evidence for the proposed strategies.", "keywords": ["LLMs", "patient safety", "HCI", "clinical settings", "collaboration"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2402.01383", "pdf": "https://arxiv.org/pdf/2402.01383.pdf", "abs": "https://arxiv.org/abs/2402.01383", "title": "LLM-based NLG Evaluation: Current Status and Challenges", "authors": ["Mingqi Gao", "Xinyu Hu", "Jie Ruan", "Xiao Pu", "Xiaojun Wan"], "categories": ["cs.CL"], "comment": null, "summary": "Evaluating natural language generation (NLG) is a vital but challenging\nproblem in natural language processing. Traditional evaluation metrics mainly\ncapturing content (e.g. n-gram) overlap between system outputs and references\nare far from satisfactory, and large language models (LLMs) such as ChatGPT\nhave demonstrated great potential in NLG evaluation in recent years. Various\nautomatic evaluation methods based on LLMs have been proposed, including\nmetrics derived from LLMs, prompting LLMs, fine-tuning LLMs, and human-LLM\ncollaborative evaluation. In this survey, we first give a taxonomy of LLM-based\nNLG evaluation methods, and discuss their pros and cons, respectively. Lastly,\nwe discuss several open problems in this area and point out future research\ndirections.", "AI": {"tldr": "This paper surveys various LLM-based methods for evaluating natural language generation, discussing their advantages and disadvantages, and suggesting future research directions.", "motivation": "The motivation behind this survey is to address the limitations of traditional evaluation metrics in natural language generation, especially in light of advancements made by large language models.", "method": "The paper presents a taxonomy of evaluation methods that utilize LLMs, including metrics derived from LLMs, prompting, fine-tuning, and collaborative evaluation with humans.", "result": "The survey categorizes different LLM-based evaluation methods and provides a discussion on their effectiveness, identifying both strengths and weaknesses in each approach.", "conclusion": "The authors conclude by highlighting open research questions and proposing future directions for LLM-based natural language generation evaluation.", "key_contributions": ["Taxonomy of LLM-based evaluation methods", "Evaluation of the advantages and challenges of these methods", "Identification of future research directions in NLG evaluation using LLMs."], "limitations": "", "keywords": ["natural language generation", "evaluation metrics", "large language models", "NLG", "machine learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2404.03080", "pdf": "https://arxiv.org/pdf/2404.03080.pdf", "abs": "https://arxiv.org/abs/2404.03080", "title": "Construction and Application of Materials Knowledge Graph in Multidisciplinary Materials Science via Large Language Model", "authors": ["Yanpeng Ye", "Jie Ren", "Shaozhou Wang", "Yuwei Wan", "Imran Razzak", "Bram Hoex", "Haofen Wang", "Tong Xie", "Wenjie Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 7 figures, 3 tables; Accepted by 38th Conference on Neural\n  Information Processing Systems (NeurIPS 2024)", "summary": "Knowledge in materials science is widely dispersed across extensive\nscientific literature, posing significant challenges to the efficient discovery\nand integration of new materials. Traditional methods, often reliant on costly\nand time-consuming experimental approaches, further complicate rapid\ninnovation. Addressing these challenges, the integration of artificial\nintelligence with materials science has opened avenues for accelerating the\ndiscovery process, though it also demands precise annotation, data extraction,\nand traceability of information. To tackle these issues, this article\nintroduces the Materials Knowledge Graph (MKG), which utilizes advanced natural\nlanguage processing techniques integrated with large language models to extract\nand systematically organize a decade's worth of high-quality research into\nstructured triples, contains 162,605 nodes and 731,772 edges. MKG categorizes\ninformation into comprehensive labels such as Name, Formula, and Application,\nstructured around a meticulously designed ontology, thus enhancing data\nusability and integration. By implementing network-based algorithms, MKG not\nonly facilitates efficient link prediction but also significantly reduces\nreliance on traditional experimental methods. This structured approach not only\nstreamlines materials research but also lays the groundwork for more\nsophisticated science knowledge graphs.", "AI": {"tldr": "The paper introduces the Materials Knowledge Graph (MKG), which utilizes advanced NLP and LLM techniques to organize extensive materials science research into structured data, enhancing discovery and integration processes.", "motivation": "To address the challenges in discovering and integrating materials science knowledge due to its dispersion across literature and reliance on traditional experimental methods.", "method": "The MKG employs natural language processing and large language models to extract and organize a decade of research into structured triples, forming a graph with 162,605 nodes and 731,772 edges.", "result": "MKG enhances data usability by categorizing information into structured labels and facilitates efficient link prediction through network-based algorithms while reducing reliance on experimental methods.", "conclusion": "The Materials Knowledge Graph streamlines materials research and sets the stage for more advanced science knowledge graphs.", "key_contributions": ["Development of the Materials Knowledge Graph (MKG)", "Integration of NLP and LLM for data extraction and organization", "Reduction of reliance on traditional methods for materials discovery"], "limitations": "", "keywords": ["Materials Science", "Knowledge Graph", "Natural Language Processing", "Large Language Models", "Data Integration"], "importance_score": 7, "read_time_minutes": 14}}
{"id": "2410.04526", "pdf": "https://arxiv.org/pdf/2410.04526.pdf", "abs": "https://arxiv.org/abs/2410.04526", "title": "FAMMA: A Benchmark for Financial Domain Multilingual Multimodal Question Answering", "authors": ["Siqiao Xue", "Xiaojing Li", "Fan Zhou", "Qingyang Dai", "Zhixuan Chu", "Hongyuan Mei"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this paper, we introduce FAMMA, an open-source benchmark for\n\\underline{f}in\\underline{a}ncial \\underline{m}ultilingual\n\\underline{m}ultimodal question \\underline{a}nswering (QA). Our benchmark aims\nto evaluate the abilities of large language models (LLMs) in answering complex\nreasoning questions that require advanced financial knowledge. The benchmark\nhas two versions: FAMMA-Basic consists of 1,945 questions extracted from\nuniversity textbooks and exams, along with human-annotated answers and\nrationales; FAMMA-LivePro consists of 103 novel questions created by human\ndomain experts, with answers and rationales held out from the public for a\ncontamination-free evaluation. These questions cover advanced knowledge of 8\nmajor subfields in finance (e.g., corporate finance, derivatives, and portfolio\nmanagement). Some are in Chinese or French, while a majority of them are in\nEnglish. Each question has some non-text data such as charts, diagrams, or\ntables. Our experiments reveal that FAMMA poses a significant challenge on\nLLMs, including reasoning models such as GPT-o1 and DeepSeek-R1. Additionally,\nwe curated 1,270 reasoning trajectories of DeepSeek-R1 on the FAMMA-Basic data,\nand fine-tuned a series of open-source Qwen models using this reasoning data.\nWe found that training a model on these reasoning trajectories can\nsignificantly improve its performance on FAMMA-LivePro. We released our\nleaderboard, data, code, and trained models at\nhttps://famma-bench.github.io/famma/.", "AI": {"tldr": "Introduction of FAMMA, a benchmark for evaluating LLMs in financial multilingual multimodal question answering.", "motivation": "Developing a benchmark to assess LLMs' capabilities in complex financial reasoning.", "method": "Two versions of the benchmark: FAMMA-Basic with textbook derived questions and FAMMA-LivePro with expert-created novel questions, both incorporating multimodal data.", "result": "FAMMA is challenging for LLMs such as GPT-o1 and DeepSeek-R1, indicating room for improvement in financial QA performance.", "conclusion": "Fine-tuning models on curated reasoning data enhances performance on the FAMMA-LivePro dataset; resources like leaderboard and data are publicly available.", "key_contributions": ["Introduction of a novel benchmark for financial QA", "Incorporation of multimodal data in questions", "Demonstrated performance improvement through reasoning data fine-tuning"], "limitations": "", "keywords": ["financial QA", "benchmark", "multilingual", "multimodal", "LLMs"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2411.09116", "pdf": "https://arxiv.org/pdf/2411.09116.pdf", "abs": "https://arxiv.org/abs/2411.09116", "title": "P-MMEval: A Parallel Multilingual Multitask Benchmark for Consistent Evaluation of LLMs", "authors": ["Yidan Zhang", "Yu Wan", "Boyi Deng", "Baosong Yang", "Haoran Wei", "Fei Huang", "Bowen Yu", "Junyang Lin", "Fei Huang", "Jingren Zhou"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in large language models (LLMs) showcase varied\nmultilingual capabilities across tasks like translation, code generation, and\nreasoning. Previous assessments often limited their scope to fundamental\nnatural language processing (NLP) or isolated capability-specific tasks. To\nalleviate this drawback, we aim to present a comprehensive multilingual\nmultitask benchmark. First, we introduce P-MMEval, a large-scale benchmark\ncovering effective fundamental and capability-specialized datasets.\nFurthermore, P-MMEval delivers consistent language coverage across various\ndatasets and provides parallel samples. Finally, we conduct extensive\nexperiments on representative multilingual model series to compare performances\nacross models and tasks, explore the relationship between multilingual\nperformances and factors such as tasks, model sizes, languages, and prompts,\nand examine the effectiveness of knowledge transfer from English to other\nlanguages. The resulting insights are intended to offer valuable guidance for\nfuture research. The dataset is available at\nhttps://huggingface.co/datasets/Qwen/P-MMEval.", "AI": {"tldr": "The paper presents P-MMEval, a comprehensive benchmark for evaluating multilingual capabilities across various tasks in large language models.", "motivation": "To address the limits of previous assessments that focused on isolated NLP tasks, providing a broad evaluation of multilingual capabilities in LLMs.", "method": "Introduction of P-MMEval, a large-scale benchmark covering diverse datasets with consistent language coverage and parallel samples, followed by extensive experiments comparing multilingual models.", "result": "The experiments reveal performance comparisons across models and tasks, and insights on the relationship between performance, task types, model sizes, languages, and prompts.", "conclusion": "The findings provide guidance for future research and the dataset aims to improve the evaluation of multilingual capabilities in LLMs.", "key_contributions": ["Introduction of the P-MMEval benchmark", "Consistent language coverage across datasets", "Insights into performance factors for multilingual models"], "limitations": "", "keywords": ["Multilingual Models", "Benchmark", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2502.09650", "pdf": "https://arxiv.org/pdf/2502.09650.pdf", "abs": "https://arxiv.org/abs/2502.09650", "title": "Principled Data Selection for Alignment: The Hidden Risks of Difficult Examples", "authors": ["Chengqian Gao", "Haonan Li", "Liu Liu", "Zeke Xie", "Peilin Zhao", "Zhiqiang Xu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at ICML 2025", "summary": "The alignment of large language models (LLMs) often assumes that using more\nclean data yields better outcomes, overlooking the match between model capacity\nand example difficulty. Challenging this, we propose a new principle:\nPreference data vary in difficulty, and overly difficult examples hinder\nalignment, by exceeding the model's capacity. Through systematic\nexperimentation, we validate this principle with three key findings: (1)\npreference examples vary in difficulty, as evidenced by consistent learning\norders across alignment runs; (2) overly difficult examples significantly\ndegrade performance across four LLMs and two datasets; and (3) the capacity of\na model dictates its threshold for handling difficult examples, underscoring a\ncritical relationship between data selection and model capacity. Building on\nthis principle, we introduce Selective DPO, which filters out overly difficult\nexamples. This simple adjustment improves alignment performance by 9-16% in win\nrates on the AlpacaEval 2 benchmark compared to the DPO baseline, suppressing a\nseries of DPO variants with different algorithmic adjustments. Together, these\nresults illuminate the importance of aligning data difficulty with model\ncapacity, offering a transformative perspective for improving alignment\nstrategies in LLMs. Code is available at\nhttps://github.com/glorgao/SelectiveDPO.", "AI": {"tldr": "This paper proposes a new principle regarding the alignment of large language models (LLMs), focusing on the impact of data difficulty on model performance, and introduces Selective DPO to enhance alignment by filtering challenging examples.", "motivation": "To address the assumption that more clean data leads to better alignment in LLMs, focusing on the match between model capacity and example difficulty.", "method": "Systematic experimentation was conducted to analyze the relationship between data difficulty and model capacity, leading to the development of Selective DPO which filters out overly difficult examples.", "result": "The findings indicate that preference examples vary in difficulty, and the performance of models significantly degrades with overly difficult examples. Selective DPO improves alignment performance by 9-16% in win rates on the AlpacaEval 2 benchmark over traditional DPO.", "conclusion": "Aligning data difficulty with model capacity is essential for improving the alignment strategies in large language models.", "key_contributions": ["Introduction of Selective DPO to filter overly difficult examples", "Demonstrated that challenging examples hinder alignment across multiple LLMs", "Illustrated a critical relationship between model capacity and data difficulty"], "limitations": "The study primarily addresses performance in specific datasets and LLMs, leaving other potential impacts unexplored.", "keywords": ["large language models", "model alignment", "data difficulty", "Selective DPO", "machine learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.10725", "pdf": "https://arxiv.org/pdf/2502.10725.pdf", "abs": "https://arxiv.org/abs/2502.10725", "title": "PropNet: a White-Box and Human-Like Network for Sentence Representation", "authors": ["Fei Yang"], "categories": ["cs.CL", "cs.AI"], "comment": "Clarified some ambiguities in the previous version", "summary": "Transformer-based embedding methods have dominated the field of sentence\nrepresentation in recent years. Although they have achieved remarkable\nperformance on NLP missions, such as semantic textual similarity (STS) tasks,\ntheir black-box nature and large-data-driven training style have raised\nconcerns, including issues related to bias, trust, and safety. Many efforts\nhave been made to improve the interpretability of embedding models, but these\nproblems have not been fundamentally resolved. To achieve inherent\ninterpretability, we propose a purely white-box and human-like sentence\nrepresentation network, PropNet. Inspired by findings from cognitive science,\nPropNet constructs a hierarchical network based on the propositions contained\nin a sentence. While experiments indicate that PropNet has a significant gap\ncompared to state-of-the-art (SOTA) embedding models in STS tasks, case studies\nreveal substantial room for improvement. Additionally, PropNet enables us to\nanalyze and understand the human cognitive processes underlying STS benchmarks.", "AI": {"tldr": "This paper introduces PropNet, a white-box hierarchical network for sentence representation that enhances interpretability compared to traditional black-box transformer models.", "motivation": "The paper addresses issues of bias, trust, and safety in transformer-based embedding methods due to their black-box nature.", "method": "PropNet is proposed as a purely white-box model that constructs a hierarchical network based on propositions in sentences, inspired by cognitive science.", "result": "Experiments show that while PropNet lags behind state-of-the-art models in STS tasks, it offers significant interpretability and insights into human cognitive processes.", "conclusion": "PropNet represents a step towards inherent interpretability in sentence representation models, though further improvements are necessary to enhance its performance in STS tasks.", "key_contributions": ["Introduction of PropNet for interpretable sentence representation", "Basis on cognitive science findings for hierarchical network design", "Insights into human cognitive processes for sentence understanding"], "limitations": "PropNet shows a significant performance gap compared to state-of-the-art models in STS tasks.", "keywords": ["sentence representation", "interpretability", "cognitive science", "natural language processing", "transformer models"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2503.10652", "pdf": "https://arxiv.org/pdf/2503.10652.pdf", "abs": "https://arxiv.org/abs/2503.10652", "title": "Simulating and Analysing Human Survey Responses with Large Language Models: A Case Study in Energy Stated Preference", "authors": ["Han Wang", "Jacek Pawlak", "Aruna Sivakumar"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Survey research plays a crucial role in studies by capturing consumer\npreferences and informing policy decisions. Stated preference (SP) surveys help\nresearchers understand how individuals make trade-offs in hypothetical,\npotentially futuristic, scenarios. However, traditional methods are costly,\ntime-consuming, and affected by respondent fatigue and ethical constraints.\nLarge language models (LLMs) have shown remarkable capabilities in generating\nhuman-like responses, prompting interest in their use in survey research. This\nstudy investigates LLMs for simulating consumer choices in energy-related SP\nsurveys and explores their integration into data collection and analysis\nworkflows. Test scenarios were designed to assess the simulation performance of\nseveral LLMs (LLaMA 3.1, Mistral, GPT-3.5, DeepSeek-R1) at individual and\naggregated levels, considering prompt design, in-context learning (ICL),\nchain-of-thought (CoT) reasoning, model types, integration with traditional\nchoice models, and potential biases. While LLMs achieve accuracy above random\nguessing, performance remains insufficient for practical simulation use.\nCloud-based LLMs do not consistently outperform smaller local models.\nDeepSeek-R1 achieves the highest average accuracy (77%) and outperforms\nnon-reasoning LLMs in accuracy, factor identification, and choice distribution\nalignment. Previous SP choices are the most effective input; longer prompts\nwith more factors reduce accuracy. Mixed logit models can support LLM prompt\nrefinement. Reasoning LLMs show potential in data analysis by indicating factor\nsignificance, offering a qualitative complement to statistical models. Despite\nlimitations, pre-trained LLMs offer scalability and require minimal historical\ndata. Future work should refine prompts, further explore CoT reasoning, and\ninvestigate fine-tuning techniques.", "AI": {"tldr": "This study investigates the use of large language models (LLMs) in simulating consumer choices within stated preference surveys for energy-related scenarios, examining prompt design and integration with traditional choice models.", "motivation": "The development aims to improve the efficiency and effectiveness of survey research in capturing consumer preferences through the integration of LLMs, which may alleviate issues with traditional survey methods.", "method": "The study tests the performance of various LLMs (LLaMA 3.1, Mistral, GPT-3.5, DeepSeek-R1) in simulating consumer choices, analyzing factors such as prompt design and reasoning capabilities, and compares them with traditional choice models.", "result": "DeepSeek-R1 achieved the highest average accuracy (77%) in simulating consumer choices, while LLMs generally showed better accuracy compared to random guessing but were not sufficient for practical simulation use. Mixed logit models helped refine LLM prompts leading to better performance.", "conclusion": "Despite the limitations in LLM performance, they offer a scalable solution for survey research with minimal historical data, indicating future directions for refinement in prompt design and reasoning capabilities.", "key_contributions": ["Integration of LLMs into stated preference surveys for energy-related choices", "Demonstration of mixed logit models aiding in prompt refinement", "Insights into the balance between prompt length and accuracy in LLM simulations"], "limitations": "Performance remains insufficient for practical simulation use, and cloud-based models do not consistently outperform smaller local models.", "keywords": ["Large Language Models", "Stated Preference Surveys", "Consumer Choices", "Data Collection", "Machine Learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2503.17599", "pdf": "https://arxiv.org/pdf/2503.17599.pdf", "abs": "https://arxiv.org/abs/2503.17599", "title": "Evaluating Clinical Competencies of Large Language Models with a General Practice Benchmark", "authors": ["Zheqing Li", "Yiying Yang", "Jiping Lang", "Wenhao Jiang", "Yuhang Zhao", "Shuang Li", "Dingqian Wang", "Zhu Lin", "Xuanna Li", "Yuze Tang", "Jiexian Qiu", "Xiaolin Lu", "Hongji Yu", "Shuang Chen", "Yuhua Bi", "Xiaofei Zeng", "Yixian Chen", "Junrong Chen", "Lin Yao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated considerable potential in\ngeneral practice. However, existing benchmarks and evaluation frameworks\nprimarily depend on exam-style or simplified question-answer formats, lacking a\ncompetency-based structure aligned with the real-world clinical\nresponsibilities encountered in general practice. Consequently, the extent to\nwhich LLMs can reliably fulfill the duties of general practitioners (GPs)\nremains uncertain. In this work, we propose a novel evaluation framework to\nassess the capability of LLMs to function as GPs. Based on this framework, we\nintroduce a general practice benchmark (GPBench), whose data are meticulously\nannotated by domain experts in accordance with routine clinical practice\nstandards. We evaluate ten state-of-the-art LLMs and analyze their\ncompetencies. Our findings indicate that current LLMs are not yet ready for\ndeployment in such settings without human oversight, and further optimization\nspecifically tailored to the daily responsibilities of GPs is essential.", "AI": {"tldr": "This paper introduces a new evaluation framework for assessing Large Language Models (LLMs) in their potential roles as general practitioners (GPs), highlighting the necessity for further development before practical deployment.", "motivation": "To address the lack of competency-based benchmarks for evaluating LLMs in real-world clinical contexts, specifically in general practice.", "method": "A novel evaluation framework is proposed, alongside the creation of a general practice benchmark (GPBench) that includes expert-annotated data representing routine clinical practice.", "result": "Evaluation of ten state-of-the-art LLMs showed that they are not yet ready for deployment in general practice without human supervision and require further adaptation to GP responsibilities.", "conclusion": "Current LLMs need significant optimization to meet the responsibilities of general practitioners effectively.", "key_contributions": ["Developed a new evaluation framework for LLMs in general practice", "Introduced GPBench, a benchmark with expert-annotated data for LLMs", "Evaluated ten LLMs to determine their readiness for clinical use"], "limitations": "Current LLMs require human oversight and optimization before deployment in clinical settings.", "keywords": ["Large Language Models", "general practice", "evaluation framework", "benchmark", "competency"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2503.21696", "pdf": "https://arxiv.org/pdf/2503.21696.pdf", "abs": "https://arxiv.org/abs/2503.21696", "title": "Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for Embodied Interactive Tasks", "authors": ["Wenqi Zhang", "Mengna Wang", "Gangao Liu", "Xu Huixin", "Yiwei Jiang", "Yongliang Shen", "Guiyang Hou", "Zhe Zheng", "Hang Zhang", "Xin Li", "Weiming Lu", "Peng Li", "Yueting Zhuang"], "categories": ["cs.CL", "cs.CV"], "comment": "Code: https://github.com/zwq2018/embodied_reasoner Dataset:\n  https://huggingface.co/datasets/zwq2018/embodied_reasoner", "summary": "Recent advances in deep thinking models have demonstrated remarkable\nreasoning capabilities on mathematical and coding tasks. However, their\neffectiveness in embodied domains which require continuous interaction with\nenvironments through image action interleaved trajectories remains largely\n-unexplored. We present Embodied Reasoner, a model that extends o1 style\nreasoning to interactive embodied search tasks. Unlike mathematical reasoning\nthat relies primarily on logical deduction, embodied scenarios demand spatial\nunderstanding, temporal reasoning, and ongoing self-reflection based on\ninteraction history. To address these challenges, we synthesize 9.3k coherent\nObservation-Thought-Action trajectories containing 64k interactive images and\n90k diverse thinking processes (analysis, spatial reasoning, reflection,\nplanning, and verification). We develop a three-stage training pipeline that\nprogressively enhances the model's capabilities through imitation learning,\nself-exploration via rejection sampling, and self-correction through reflection\ntuning. The evaluation shows that our model significantly outperforms those\nadvanced visual reasoning models, e.g., it exceeds OpenAI o1, o3-mini, and\nClaude-3.7 by +9\\%, 24\\%, and +13\\%. Analysis reveals our model exhibits fewer\nrepeated searches and logical inconsistencies, with particular advantages in\ncomplex long-horizon tasks. Real-world environments also show our superiority\nwhile exhibiting fewer repeated searches and logical inconsistency cases.", "AI": {"tldr": "Embodied Reasoner improves reasoning in interactive tasks requiring spatial and temporal understanding.", "motivation": "To explore the effectiveness of deep reasoning models in embodied domains needing interaction with environments through image action trajectories.", "method": "The paper introduces a model that synthesizes a large dataset of Observation-Thought-Action trajectories and employs a three-stage training pipeline involving imitation learning, self-exploration, and reflection tuning.", "result": "Embodied Reasoner outperforms various advanced visual reasoning models, demonstrating significant improvements in handling long-horizon tasks and reducing logical inconsistencies.", "conclusion": "The model shows promise in enhancing reasoning capabilities in embodied search tasks, with further applications in interactive environments.", "key_contributions": ["Introduction of the Embodied Reasoner model for interactive embodied search tasks", "Creation of a robust dataset of Observation-Thought-Action trajectories for training", "Demonstration of improved reasoning capabilities over existing models"], "limitations": "", "keywords": ["embodied reasoning", "interactive search", "machine learning", "spatial reasoning", "temporal reasoning"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2503.24293", "pdf": "https://arxiv.org/pdf/2503.24293.pdf", "abs": "https://arxiv.org/abs/2503.24293", "title": "Is analogy enough to draw novel adjective-noun inferences?", "authors": ["Hayley Ross", "Kathryn Davidson", "Najoung Kim"], "categories": ["cs.CL"], "comment": "9 pages (17 pages with appendix). Accepted to SCiL 2025", "summary": "Recent work (Ross et al., 2025, 2024) has argued that the ability of humans\nand LLMs respectively to generalize to novel adjective-noun combinations shows\nthat they each have access to a compositional mechanism to determine the\nphrase's meaning and derive inferences. We study whether these inferences can\ninstead be derived by analogy to known inferences, without need for\ncomposition. We investigate this by (1) building a model of analogical\nreasoning using similarity over lexical items, and (2) asking human\nparticipants to reason by analogy. While we find that this strategy works well\nfor a large proportion of the dataset of Ross et al. (2025), there are novel\ncombinations for which both humans and LLMs derive convergent inferences but\nwhich are not well handled by analogy. We thus conclude that the mechanism\nhumans and LLMs use to generalize in these cases cannot be fully reduced to\nanalogy, and likely involves composition.", "AI": {"tldr": "This study examines whether inferences from novel adjective-noun combinations can be derived through analogical reasoning rather than compositional mechanisms in both humans and LLMs.", "motivation": "To explore the mechanisms underlying how humans and LLMs generalize meanings from novel adjective-noun combinations.", "method": "A model of analogical reasoning using lexical similarity was built, and human participants were tasked with reasoning by analogy.", "result": "The analogical reasoning strategy was effective for a large part of the dataset; however, there were novel combinations where both humans and LLMs derived similar inferences that were not adequately handled by analogy.", "conclusion": "The generalization mechanisms for certain novel adjective-noun combinations involve compositional understanding rather than being solely reliant on analogy.", "key_contributions": ["Developed a model for analogical reasoning in lexical items.", "Demonstrated the limits of analogical reasoning in deriving meanings compared to compositional methods.", "Provided empirical evidence from human reasoning aligning with LLM performance."], "limitations": "The study may not cover all possible combinations of adjectives and nouns, limiting the generalization of findings.", "keywords": ["analogical reasoning", "compositionality", "language models"], "importance_score": 7, "read_time_minutes": 9}}
