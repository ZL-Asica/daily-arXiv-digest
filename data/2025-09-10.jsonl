{"id": "2509.07126", "pdf": "https://arxiv.org/pdf/2509.07126.pdf", "abs": "https://arxiv.org/abs/2509.07126", "title": "Short-Term Gaze Prediction: Analysis of Individual Differences, Typical and Extreme-Case Errors", "authors": ["Kateryna Melnyk", "Lee Friedman", "Oleg Komogortsev"], "categories": ["cs.HC"], "comment": "12 pages, 14 figures", "summary": "Gaze prediction is a diverse field of study with multiple research focuses\nand practical applications. This article investigates how recurrent neural\nnetworks and transformers perform short-term gaze prediction. We used three\nmodels: a three-layer long-short-term memory (LSTM) network, a simple\ntransformer-encoder model (TF), and a classification-predictor network (ClPr),\nwhich simultaneously classifies the signal into eye movement events and\npredicts the positions of gaze. The performance of the models was evaluated for\nocular fixations and saccades of various amplitudes and as a function of\nindividual differences in both typical and extreme cases. On average, LSTM\nperformed better on fixations and saccades, whereas TF and ClPr demonstrated\nmore precise results for post-saccadic periods. In extreme cases, the\nbest-performing models vary depending on the type of eye movement. We reviewed\nthe difference between the median $P_{50}$ and high-percentile $P_{95}$ error\nprofiles across subjects. The subjects for which the models perform the best\noverall do not necessarily exhibit the lowest $P_{95}$ values, which supports\nthe idea of analyzing extreme cases separately in future work. We explore the\ntrade-offs between the proposed solutions and provide practical insights into\nmodel selection for gaze prediction."}
{"id": "2509.07187", "pdf": "https://arxiv.org/pdf/2509.07187.pdf", "abs": "https://arxiv.org/abs/2509.07187", "title": "Wellbeing-Centered UX: Supporting Content Moderators", "authors": ["Diana Mihalache", "Dalila Szostak"], "categories": ["cs.HC", "cs.CY", "J.4; K.4.1"], "comment": "In M. L. Daniel, A. Menking, M. T. Savio, & J. Claffey (Eds.) (In\n  Press, upcoming), Trust, Safety, and the Internet We Share: Multistakeholder\n  Insights. Taylor & Francis", "summary": "This chapter focuses on the intersection of user experience (UX) and\nwellbeing in the context of content moderation. Human content moderators play a\nkey role in protecting end users from harm by detecting, evaluating, and\naddressing content that may violate laws or product policies. They face\nnumerous challenges, including exposure to sensitive content, monotonous tasks,\nand complex decisions, which are often exacerbated by inadequate tools. This\nchapter explains the importance of incorporating wellbeing considerations\nthroughout the product development lifecycle, offering a framework and\npractical strategies for implementation across key UX disciplines: research,\nwriting, and design. By examining these considerations, this chapter provides a\nroadmap for creating user experiences that support content moderators,\nbenefiting both the user and the business."}
{"id": "2509.07202", "pdf": "https://arxiv.org/pdf/2509.07202.pdf", "abs": "https://arxiv.org/abs/2509.07202", "title": "Neurocognitive Modeling for Text Generation: Deep Learning Architecture for EEG Data", "authors": ["Khushiyant"], "categories": ["cs.HC", "cs.CL", "I.2.7; I.2.6; J.3"], "comment": "15 pages, 10 figures, 5 tables", "summary": "Text generating capabilities have undergone a substantial transformation with\nthe introduction of large language models (LLMs). Electroencephalography\n(EEG)-based text production is still difficult, though, because it requires a\nlot of data and processing power. This paper introduces a new method that\ncombines the use of the Gemma 2B LLM with a classifier-LLM architecture to\nincorporate a Recurrent Neural Network (RNN) encoder. Our approach drastically\nlowers the amount of data and compute power needed while achieving performance\nclose to that of cutting-edge methods. Notably, compared to current\nmethodologies, our methodology delivers an overall performance improvement of\n10%. The suggested architecture demonstrates the possibility of effective\ntransfer learning for EEG-based text production, remaining strong and\nfunctional even in the face of data limits. This work highlights the potential\nof integrating LLMs with EEG decoding to improve assistive technologies and\nimprove independence and communication for those with severe motor limitations.\nOur method pushes the limits of present capabilities and opens new paths for\nresearch and application in brain-computer interfaces by efficiently using the\nstrengths of pre-trained language models. This makes EEG-based text production\nmore accessible and efficient."}
{"id": "2509.07314", "pdf": "https://arxiv.org/pdf/2509.07314.pdf", "abs": "https://arxiv.org/abs/2509.07314", "title": "In the Queue: Understanding How Reddit Moderators Use the Modqueue", "authors": ["Tanvi Bajpai", "Eshwar Chandrasekharan"], "categories": ["cs.HC"], "comment": "26 pages, 7 figures", "summary": "On Reddit, the moderation queue (modqueue) is a primary interface for\nmoderators to review reported content. Despite its central role in Reddit's\ncommunity-reliant moderation model, little is known about how moderators\nactually use it in practice. To address this gap, we surveyed 110 moderators,\nwho collectively oversee more than 400 unique subreddits, and asked them about\ntheir usage of the modqueue. Modqueue practices vary widely: some moderators\napproach it as a daily checklist, others as a hub to infer community-wide\npatterns, and many still find the queue insufficient to inform their moderation\ndecisions. We also identify persistent challenges around review coordination,\ninconsistent interface signals, and reliance on third-party tools. Taken\ntogether, we show the modqueue is neither a one-size-fits-all solution nor\nsufficient on its own for supporting moderator review. Our work highlights\ndesign opportunities for more modular, integrated, and customizable platform\ninfrastructures that better support the diversity of moderator workflows."}
{"id": "2509.07135", "pdf": "https://arxiv.org/pdf/2509.07135.pdf", "abs": "https://arxiv.org/abs/2509.07135", "title": "MedBench-IT: A Comprehensive Benchmark for Evaluating Large Language Models on Italian Medical Entrance Examinations", "authors": ["Ruggero Marino Lazzaroni", "Alessandro Angioi", "Michelangelo Puliga", "Davide Sanna", "Roberto Marras"], "categories": ["cs.CL"], "comment": "Accepted as an oral presentation at CLiC-it 2025", "summary": "Large language models (LLMs) show increasing potential in education, yet\nbenchmarks for non-English languages in specialized domains remain scarce. We\nintroduce MedBench-IT, the first comprehensive benchmark for evaluating LLMs on\nItalian medical university entrance examinations. Sourced from Edizioni Simone,\na leading preparatory materials publisher, MedBench-IT comprises 17,410\nexpert-written multiple-choice questions across six subjects (Biology,\nChemistry, Logic, General Culture, Mathematics, Physics) and three difficulty\nlevels. We evaluated diverse models including proprietary LLMs (GPT-4o, Claude\nseries) and resource-efficient open-source alternatives (<30B parameters)\nfocusing on practical deployability.\n  Beyond accuracy, we conducted rigorous reproducibility tests (88.86% response\nconsistency, varying by subject), ordering bias analysis (minimal impact), and\nreasoning prompt evaluation. We also examined correlations between question\nreadability and model performance, finding a statistically significant but\nsmall inverse relationship. MedBench-IT provides a crucial resource for Italian\nNLP community, EdTech developers, and practitioners, offering insights into\ncurrent capabilities and standardized evaluation methodology for this critical\ndomain."}
{"id": "2509.07334", "pdf": "https://arxiv.org/pdf/2509.07334.pdf", "abs": "https://arxiv.org/abs/2509.07334", "title": "SpecifyUI: Supporting Iterative UI Design Intent Expression through Structured Specifications and Generative AI", "authors": ["Yunnong Chen", "Chengwei Shi", "Liuqing Chen"], "categories": ["cs.HC"], "comment": "27 pages, 12 figures", "summary": "Large language models (LLMs) promise to accelerate UI design, yet current\ntools struggle with two fundamentals: externalizing designers' intent and\ncontrolling iterative change. We introduce SPEC, a structured, parameterized,\nhierarchical intermediate representation that exposes UI elements as\ncontrollable parameters. Building on SPEC, we present SpecifyUI, an interactive\nsystem that extracts SPEC from UI references via region segmentation and\nvision-language models, composes UIs across multiple sources, and supports\ntargeted edits at global, regional, and component levels. A multi-agent\ngenerator renders SPEC into high-fidelity designs, closing the loop between\nintent expression and controllable generation. Quantitative experiments show\nSPEC-based generation more faithfully captures reference intent than\nprompt-based baselines. In a user study with 16 professional designers,\nSpecifyUI significantly outperformed Stitch on intent alignment, design\nquality, controllability, and overall experience in human-AI co-creation. Our\nresults position SPEC as a specification-driven paradigm that shifts\nLLM-assisted design from one-shot prompting to iterative, collaborative\nworkflows."}
{"id": "2509.07139", "pdf": "https://arxiv.org/pdf/2509.07139.pdf", "abs": "https://arxiv.org/abs/2509.07139", "title": "The ML-SUPERB 2.0 Challenge: Towards Inclusive ASR Benchmarking for All Language Varieties", "authors": ["William Chen", "Chutong Meng", "Jiatong Shi", "Martijn Bartelds", "Shih-Heng Wang", "Hsiu-Hsuan Wang", "Rafael Mosquera", "Sara Hincapie", "Dan Jurafsky", "Antonis Anastasopoulos", "Hung-yi Lee", "Karen Livescu", "Shinji Watanabe"], "categories": ["cs.CL", "eess.AS"], "comment": "Interspeech 2025", "summary": "Recent improvements in multilingual ASR have not been equally distributed\nacross languages and language varieties. To advance state-of-the-art (SOTA) ASR\nmodels, we present the Interspeech 2025 ML-SUPERB 2.0 Challenge. We construct a\nnew test suite that consists of data from 200+ languages, accents, and dialects\nto evaluate SOTA multilingual speech models. The challenge also introduces an\nonline evaluation server based on DynaBench, allowing for flexibility in model\ndesign and architecture for participants. The challenge received 5 submissions\nfrom 3 teams, all of which outperformed our baselines. The best-performing\nsubmission achieved an absolute improvement in LID accuracy of 23% and a\nreduction in CER of 18% when compared to the best baseline on a general\nmultilingual test set. On accented and dialectal data, the best submission\nobtained 30.2% lower CER and 15.7% higher LID accuracy, showing the importance\nof community challenges in making speech technologies more inclusive."}
{"id": "2509.07424", "pdf": "https://arxiv.org/pdf/2509.07424.pdf", "abs": "https://arxiv.org/abs/2509.07424", "title": "Feed-O-Meter: Fostering Design Feedback Skills through Role-playing Interactions with AI Mentee", "authors": ["Hyunseung Lim", "Dasom Choi", "DaEun Choi", "Sooyohn Nam", "Hwajung Hong"], "categories": ["cs.HC"], "comment": null, "summary": "Effective feedback, including critique and evaluation, helps designers\ndevelop design concepts and refine their ideas, supporting informed\ndecision-making throughout the iterative design process. However, in\nstudio-based design courses, students often struggle to provide feedback due to\na lack of confidence and fear of being judged, which limits their ability to\ndevelop essential feedback-giving skills. Recent advances in large language\nmodels (LLMs) suggest that role-playing with AI agents can let learners engage\nin multi-turn feedback without the anxiety of external judgment or the time\nconstraints of real-world settings. Yet prior studies have raised concerns that\nLLMs struggle to behave like real people in role-play scenarios, diminishing\nthe educational benefits of these interactions. Therefore, designing AI-based\nagents that effectively support learners in practicing and developing\nintellectual reasoning skills requires more than merely assigning the target\npersona's personality and role to the agent. By addressing these issues, we\npresent Feed-O-Meter, a novel system that employs carefully designed LLM-based\nagents to create an environment in which students can practice giving design\nfeedback. The system enables users to role-play as mentors, providing feedback\nto an AI mentee and allowing them to reflect on how that feedback impacts the\nAI mentee's idea development process. A user study (N=24) indicated that\nFeed-O-Meter increased participants' engagement and motivation through\nrole-switching and helped them adjust feedback to be more comprehensible for an\nAI mentee. Based on these findings, we discuss future directions for designing\nsystems to foster feedback skills in design education."}
{"id": "2509.07142", "pdf": "https://arxiv.org/pdf/2509.07142.pdf", "abs": "https://arxiv.org/abs/2509.07142", "title": "Toward Purpose-oriented Topic Model Evaluation enabled by Large Language Models", "authors": ["Zhiyin Tan", "Jennifer D'Souza"], "categories": ["cs.CL", "cs.AI", "cs.DL"], "comment": "Accepted for publication in International Journal on Digital\n  Libraries (IJDL)", "summary": "This study presents a framework for automated evaluation of dynamically\nevolving topic models using Large Language Models (LLMs). Topic modeling is\nessential for organizing and retrieving scholarly content in digital library\nsystems, helping users navigate complex and evolving knowledge domains.\nHowever, widely used automated metrics, such as coherence and diversity, often\ncapture only narrow statistical patterns and fail to explain semantic failures\nin practice. We introduce a purpose-oriented evaluation framework that employs\nnine LLM-based metrics spanning four key dimensions of topic quality: lexical\nvalidity, intra-topic semantic soundness, inter-topic structural soundness, and\ndocument-topic alignment soundness. The framework is validated through\nadversarial and sampling-based protocols, and is applied across datasets\nspanning news articles, scholarly publications, and social media posts, as well\nas multiple topic modeling methods and open-source LLMs. Our analysis shows\nthat LLM-based metrics provide interpretable, robust, and task-relevant\nassessments, uncovering critical weaknesses in topic models such as redundancy\nand semantic drift, which are often missed by traditional metrics. These\nresults support the development of scalable, fine-grained evaluation tools for\nmaintaining topic relevance in dynamic datasets. All code and data supporting\nthis work are accessible at\nhttps://github.com/zhiyintan/topic-model-LLMjudgment."}
{"id": "2509.07502", "pdf": "https://arxiv.org/pdf/2509.07502.pdf", "abs": "https://arxiv.org/abs/2509.07502", "title": "Social Media Clones: Exploring the Impact of Social Delegation with AI Clones through a Design Workbook Study", "authors": ["Jackie Liu", "Mehrnoosh Sadat Shirvani", "Hwajung Hong", "Ig-Jae Kim", "Dongwook Yoon"], "categories": ["cs.HC"], "comment": null, "summary": "Social media clones are AI-powered social delegates of ourselves created\nusing our personal data. As our identities and online personas intertwine,\nthese technologies have the potential to greatly enhance our social media\nexperience. If mismanaged, however, these clones may also pose new risks to our\nsocial reputation and online relationships. To set the foundation for a\nproductive and responsible integration, we set out to understand how social\nmedia clones will impact our online behavior and interactions. We conducted a\nseries of semi-structured interviews introducing eight speculative clone\nconcepts to 32 social media users through a design workbook. Applying existing\nwork in AI-mediated communication in the context of social media, we found that\nalthough clones can offer convenience and comfort, they can also threaten the\nuser's authenticity and increase skepticism within the online community. As a\nresult, users tend to behave more like their clones to mitigate discrepancies\nand interaction breakdowns. These findings are discussed through the lens of\npast literature in identity and impression management to highlight challenges\nin the adoption of social media clones by the general public, and propose\ndesign considerations for their successful integration into social media\nplatforms."}
{"id": "2509.07177", "pdf": "https://arxiv.org/pdf/2509.07177.pdf", "abs": "https://arxiv.org/abs/2509.07177", "title": "Towards EnergyGPT: A Large Language Model Specialized for the Energy Sector", "authors": ["Amal Chebbi", "Babajide Kolade"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models have demonstrated impressive capabilities across\nvarious domains. However, their general-purpose nature often limits their\neffectiveness in specialized fields such as energy, where deep technical\nexpertise and precise domain knowledge are essential. In this paper, we\nintroduce EnergyGPT, a domain-specialized language model tailored for the\nenergy sector, developed by fine-tuning LLaMA 3.1-8B model using Supervised\nFine-Tuning on a high-quality, curated corpus of energy-related texts. We\npresent a complete development pipeline, including data collection and\ncuration, model fine-tuning, benchmark design and LLM-judge choice, evaluation\nand deployment. Through this work, we demonstrate that our training strategy\nenables improvements in domain relevance and performance without the need for\nlarge-scale infrastructure. By evaluating the performance of the model using\ndomain-specific question-answering benchmarks, our results demonstrate that\nEnergyGPT outperforms the base model in most of the energy-related language\nunderstanding and generation tasks."}
{"id": "2509.07740", "pdf": "https://arxiv.org/pdf/2509.07740.pdf", "abs": "https://arxiv.org/abs/2509.07740", "title": "Digital Twins for Extended Reality Tourism: User Experience Evaluation Across User Groups", "authors": ["Maximilian Warsinke", "Francesco Vona", "Tanja Kojić", "Jan-Niklas Voigt-Antons", "Sebastian Möller"], "categories": ["cs.HC"], "comment": "Submitted manuscript. The final version was presented at XR Salento\n  2025", "summary": "This study evaluates the user experience (UX) in extended reality (XR)\ntourism of two digital twin-based applications: an Augmented Reality Virtual\nTour (AR-VT) for enhanced on-site visits and a Virtual Reality Virtual Tour\n(VR-VT) for remote exploration. Using a quantitative exploratory approach, 84\nparticipants from Spain and Germany, divided into three sample groups, assessed\nUX, task load, presence, cybersickness, and emotional response through\nstandardized questionnaires. Findings indicate that both applications provided\na low task load and high enjoyment. The VR-based tour enhanced presence but\nposed usability and cybersickness challenges, while the AR-based tour achieved\nhigh UX ratings, with qualitative feedback suggesting areas for refinement.\nCorrelation analysis revealed significant relationships between age, prior XR\nexperience, and technological affinity with the measured metrics for both\napplications. These results highlight the importance of well-designed\nexperiences tailored to XR novices, reinforcing the critical role of UX in\ndigital twin-based XR tourism."}
{"id": "2509.07188", "pdf": "https://arxiv.org/pdf/2509.07188.pdf", "abs": "https://arxiv.org/abs/2509.07188", "title": "DischargeSim: A Simulation Benchmark for Educational Doctor-Patient Communication at Discharge", "authors": ["Zonghai Yao", "Michael Sun", "Won Seok Jang", "Sunjae Kwon", "Soie Kwon", "Hong Yu"], "categories": ["cs.CL", "cs.AI"], "comment": "Equal contribution for the first two authors. To appear in the\n  proceedings of the Main Conference on Empirical Methods in Natural Language\n  Processing (EMNLP) 2025", "summary": "Discharge communication is a critical yet underexplored component of patient\ncare, where the goal shifts from diagnosis to education. While recent large\nlanguage model (LLM) benchmarks emphasize in-visit diagnostic reasoning, they\nfail to evaluate models' ability to support patients after the visit. We\nintroduce DischargeSim, a novel benchmark that evaluates LLMs on their ability\nto act as personalized discharge educators. DischargeSim simulates post-visit,\nmulti-turn conversations between LLM-driven DoctorAgents and PatientAgents with\ndiverse psychosocial profiles (e.g., health literacy, education, emotion).\nInteractions are structured across six clinically grounded discharge topics and\nassessed along three axes: (1) dialogue quality via automatic and LLM-as-judge\nevaluation, (2) personalized document generation including free-text summaries\nand structured AHRQ checklists, and (3) patient comprehension through a\ndownstream multiple-choice exam. Experiments across 18 LLMs reveal significant\ngaps in discharge education capability, with performance varying widely across\npatient profiles. Notably, model size does not always yield better education\noutcomes, highlighting trade-offs in strategy use and content prioritization.\nDischargeSim offers a first step toward benchmarking LLMs in post-visit\nclinical education and promoting equitable, personalized patient support."}
{"id": "2509.07742", "pdf": "https://arxiv.org/pdf/2509.07742.pdf", "abs": "https://arxiv.org/abs/2509.07742", "title": "Enhancing Online Learning by Integrating Biosensors and Multimodal Learning Analytics for Detecting and Predicting Student Behavior: A Review", "authors": ["Alvaro Becerra", "Ruth Cobos", "Charles Lang"], "categories": ["cs.HC", "cs.AI", "cs.CV"], "comment": "Accepted for publication in Behaviour & Information Technology\n  (Taylor & Francis). Final published version will be available soon at\n  https://www.tandfonline.com/journals/tbit20", "summary": "In modern online learning, understanding and predicting student behavior is\ncrucial for enhancing engagement and optimizing educational outcomes. This\nsystematic review explores the integration of biosensors and Multimodal\nLearning Analytics (MmLA) to analyze and predict student behavior during\ncomputer-based learning sessions. We examine key challenges, including emotion\nand attention detection, behavioral analysis, experimental design, and\ndemographic considerations in data collection. Our study highlights the growing\nrole of physiological signals, such as heart rate, brain activity, and\neye-tracking, combined with traditional interaction data and self-reports to\ngain deeper insights into cognitive states and engagement levels. We synthesize\nfindings from 54 key studies, analyzing commonly used methodologies such as\nadvanced machine learning algorithms and multimodal data pre-processing\ntechniques. The review identifies current research trends, limitations, and\nemerging directions in the field, emphasizing the transformative potential of\nbiosensor-driven adaptive learning systems. Our findings suggest that\nintegrating multimodal data can facilitate personalized learning experiences,\nreal-time feedback, and intelligent educational interventions, ultimately\nadvancing toward a more customized and adaptive online learning experience."}
{"id": "2509.07190", "pdf": "https://arxiv.org/pdf/2509.07190.pdf", "abs": "https://arxiv.org/abs/2509.07190", "title": "Rule-Based Moral Principles for Explaining Uncertainty in Natural Language Generation", "authors": ["Zahra Atf", "Peter R Lewis"], "categories": ["cs.CL", "cs.HC"], "comment": "This paper was accepted for presentation at the 35th IEEE\n  International Conference on Collaborative Advances in Software and Computing.\n  Conference website:https://conf.researchr.org/home/cascon-2025", "summary": "Large language models (LLMs) are increasingly used in high-stakes settings,\nwhere explaining uncertainty is both technical and ethical. Probabilistic\nmethods are often opaque and misaligned with expectations of transparency. We\npropose a framework based on rule-based moral principles for handling\nuncertainty in LLM-generated text. Using insights from moral psychology and\nvirtue ethics, we define rules such as precaution, deference, and\nresponsibility to guide responses under epistemic or aleatoric uncertainty.\nThese rules are encoded in a lightweight Prolog engine, where uncertainty\nlevels (low, medium, high) trigger aligned system actions with plain-language\nrationales. Scenario-based simulations benchmark rule coverage, fairness, and\ntrust calibration. Use cases in clinical and legal domains illustrate how moral\nreasoning can improve trust and interpretability. Our approach offers a\ntransparent, lightweight alternative to probabilistic models for socially\nresponsible natural language generation."}
{"id": "2509.07819", "pdf": "https://arxiv.org/pdf/2509.07819.pdf", "abs": "https://arxiv.org/abs/2509.07819", "title": "LLMs in Wikipedia: Investigating How LLMs Impact Participation in Knowledge Communities", "authors": ["Moyan Zhou", "Soobin Cho", "Loren Terveen"], "categories": ["cs.HC"], "comment": null, "summary": "Large language models (LLMs) are reshaping knowledge production as community\nmembers increasingly incorporate them into their contribution workflows.\nHowever, participating in knowledge communities involves more than just\ncontributing content - it is also a deeply social process. While communities\nmust carefully consider appropriate and responsible LLM integration, the\nabsence of concrete norms has left individual editors to experiment and\nnavigate LLM use on their own. Understanding how LLMs influence community\nparticipation is therefore critical in shaping future norms and supporting\neffective adoption. To address this gap, we investigated Wikipedia, one of the\nlargest knowledge production communities, to understand 1) how LLMs influence\nthe ways editors contribute content, 2) what strategies editors leverage to\nalign LLM outputs with community norms, and 3) how other editors in the\ncommunity respond to LLM-assisted contributions. Through interviews with 16\nWikipedia editors who had used LLMs for their edits, we found that 1) LLMs\naffected the content contributions for experienced and new editors differently;\n2) aligning LLM outputs with community norms required tacit knowledge that\noften challenged newcomers; and 3) as a result, other editors responded to\nLLM-assisted edits differently depending on the editors' expertise level. Based\non these findings, we challenge existing models of newcomer involvement and\npropose design implications for LLMs that support community engagement through\nscaffolding, teaching, and context awareness."}
{"id": "2509.07274", "pdf": "https://arxiv.org/pdf/2509.07274.pdf", "abs": "https://arxiv.org/abs/2509.07274", "title": "LLM Analysis of 150+ years of German Parliamentary Debates on Migration Reveals Shift from Post-War Solidarity to Anti-Solidarity in the Last Decade", "authors": ["Aida Kostikova", "Ole Pütz", "Steffen Eger", "Olga Sabelfeld", "Benjamin Paassen"], "categories": ["cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "Migration has been a core topic in German political debate, from millions of\nexpellees post World War II over labor migration to refugee movements in the\nrecent past. Studying political speech regarding such wide-ranging phenomena in\ndepth traditionally required extensive manual annotations, limiting the scope\nof analysis to small subsets of the data. Large language models (LLMs) have the\npotential to partially automate even complex annotation tasks. We provide an\nextensive evaluation of a multiple LLMs in annotating (anti-)solidarity\nsubtypes in German parliamentary debates compared to a large set of thousands\nof human reference annotations (gathered over a year). We evaluate the\ninfluence of model size, prompting differences, fine-tuning, historical versus\ncontemporary data; and we investigate systematic errors. Beyond methodological\nevaluation, we also interpret the resulting annotations from a social science\nlense, gaining deeper insight into (anti-)solidarity trends towards migrants in\nthe German post-World War II period and recent past. Our data reveals a high\ndegree of migrant-directed solidarity in the postwar period, as well as a\nstrong trend towards anti-solidarity in the German parliament since 2015,\nmotivating further research. These findings highlight the promise of LLMs for\npolitical text analysis and the importance of migration debates in Germany,\nwhere demographic decline and labor shortages coexist with rising polarization."}
{"id": "2509.07863", "pdf": "https://arxiv.org/pdf/2509.07863.pdf", "abs": "https://arxiv.org/abs/2509.07863", "title": "NeuroGaze: A Hybrid EEG and Eye-Tracking Brain-Computer Interface for Hands-Free Interaction in Virtual Reality", "authors": ["Kyle Coutray", "Wanyea Barbel", "Zack Groth", "Joseph J LaViola Jr"], "categories": ["cs.HC"], "comment": null, "summary": "Brain-Computer Interfaces (BCIs) have traditionally been studied in clinical\nand laboratory contexts, but the rise of consumer-grade devices now allows\nexploration of their use in daily activities. Virtual reality (VR) provides a\nparticularly relevant domain, where existing input methods often force\ntrade-offs between speed, accuracy, and physical effort. This study introduces\nNeuroGaze, a hybrid interface combining electroencephalography (EEG) with eye\ntracking to enable hands-free interaction in immersive VR. Twenty participants\ncompleted a 360{\\deg} cube-selection task using three different input methods:\nVR controllers, gaze combined with a pinch gesture, and NeuroGaze. Performance\nwas measured by task completion time and error rate, while workload was\nevaluated using the NASA Task Load Index (NASA-TLX). NeuroGaze successfully\nsupported target selection with off-the-shelf hardware, producing fewer errors\nthan the alternative methods but requiring longer completion times, reflecting\na classic speed-accuracy tradeoff. Workload analysis indicated reduced physical\ndemand for NeuroGaze compared to controllers, though overall ratings and user\npreferences were mixed. These findings demonstrate the feasibility of hybrid\nEEG+gaze systems for everyday VR use, highlighting their ergonomic benefits and\ninclusivity potential. Although not yet competitive in speed, NeuroGaze points\ntoward a practical role for consumer-grade BCIs in accessibility and\nlong-duration applications, and underscores the need for improved EEG signal\nprocessing and adaptive multimodal integration to enhance future performance."}
{"id": "2509.07301", "pdf": "https://arxiv.org/pdf/2509.07301.pdf", "abs": "https://arxiv.org/abs/2509.07301", "title": "Causal Attention with Lookahead Keys", "authors": ["Zhuoqing Song", "Peng Sun", "Huizhuo Yuan", "Quanquan Gu"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "In standard causal attention, each token's query, key, and value (QKV) are\nstatic and encode only preceding context. We introduce CAuSal aTtention with\nLookahead kEys (CASTLE), an attention mechanism that continually updates each\ntoken's keys as the context unfolds. We term these updated keys lookahead keys\nbecause they belong to earlier positions yet integrate information from tokens\nthat appear later relative to those positions, while strictly preserving the\nautoregressive property. Although the mechanism appears sequential, we derive a\nmathematical equivalence that avoids explicitly materializing lookahead keys at\neach position and enables efficient parallel training. On language modeling\nbenchmarks, CASTLE consistently outperforms standard causal attention across\nmodel scales, reducing validation perplexity and improving performance on a\nrange of downstream tasks."}
{"id": "2509.07871", "pdf": "https://arxiv.org/pdf/2509.07871.pdf", "abs": "https://arxiv.org/abs/2509.07871", "title": "An Enactivist Approach to Human-Computer Interaction: Bridging the Gap Between Human Agency and Affordances", "authors": ["Angjelin Hila"], "categories": ["cs.HC", "H.5"], "comment": "Published in HCI International 2025", "summary": "Emerging paradigms in XR, AI, and BCI contexts necessitate novel theoretical\nframeworks for understanding human autonomy and agency in HCI. Drawing from\nenactivist theories of cognition, we conceptualize human agents as\nself-organizing, operationally closed systems that actively enact their\ncognitive domains through dynamic interaction with their environments. To\ndevelop measurable variables aligned with this framework, we introduce\n\"feelings of agency\" (FoA) as an alternative to the established construct of\n\"sense of agency\" (SoA), refining Synofzyk's multifactorial weighting model and\noffering a novel conceptual pathway for overcoming gaps in the dominant\ncomparator model. We define FoA as comprising two subconstructs: affective\nengagement and volitional attention, which we operationalize through integrated\nneurodynamic indicators (valence, arousal, cross frequency coupling within the\ndorsal attention system) and first-person phenomenological reports. We argue\nthat these neurophenomenological indicators provide richer, more actionable\ninsights for digital affordance design, particularly in XR, BCI, Human AI\nInteraction (HAX), and generative AI environments. Our framework aims to inform\nand inspire design parameters that significantly enhance human agency in\nrapidly evolving interactive domains."}
{"id": "2509.07308", "pdf": "https://arxiv.org/pdf/2509.07308.pdf", "abs": "https://arxiv.org/abs/2509.07308", "title": "Basis Vector Metric: A Method for Robust Open-Ended State Change Detection", "authors": ["David Oprea", "Sam Powers"], "categories": ["cs.CL", "cs.AI"], "comment": "24 pages", "summary": "We test a new method, which we will abbreviate using the acronym BVM (Basis\nVectors Method), in its ability to judge the state changes in images through\nusing language embeddings. We used the MIT-States dataset, containing about\n53,000 images, to gather all of our data, which has 225 nouns and 115\nadjectives, with each noun having about 9 different adjectives, forming\napproximately 1000 noun-adjective pairs. For our first experiment, we test our\nmethod's ability to determine the state of each noun class separately against\nother metrics for comparison. These metrics are cosine similarity, dot product,\nproduct quantization, binary index, Naive Bayes, and a custom neural network.\nAmong these metrics, we found that our proposed BVM performs the best in\nclassifying the states for each noun. We then perform a second experiment where\nwe try using BVM to determine if it can differentiate adjectives from one\nanother for each adjective separately. We compared the abilities of BVM to\ndifferentiate adjectives against the proposed method the MIT-States paper\nsuggests: using a logistic regression model. In the end, we did not find\nconclusive evidence that our BVM metric could perform better than the logistic\nregression model at discerning adjectives. Yet, we were able to find evidence\nfor possible improvements to our method; this leads to the chance of increasing\nour method's accuracy through certain changes in our methodologies."}
{"id": "2509.07873", "pdf": "https://arxiv.org/pdf/2509.07873.pdf", "abs": "https://arxiv.org/abs/2509.07873", "title": "A Robot That Listens: Enhancing Self-Disclosure and Engagement Through Sentiment-based Backchannels and Active Listening", "authors": ["Hieu Tran", "Go-Eum Cha", "Sooyeon Jeong"], "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "As social robots get more deeply integrated intoour everyday lives, they will\nbe expected to engage in meaningful conversations and exhibit socio-emotionally\nintelligent listening behaviors when interacting with people. Active listening\nand backchanneling could be one way to enhance robots' communicative\ncapabilities and enhance their effectiveness in eliciting deeper\nself-disclosure, providing a sense of empathy,and forming positive rapport and\nrelationships with people.Thus, we developed an LLM-powered social robot that\ncan exhibit contextually appropriate sentiment-based backchannelingand active\nlistening behaviors (active listening+backchanneling) and compared its efficacy\nin eliciting people's self-disclosurein comparison to robots that do not\nexhibit any of these listening behaviors (control) and a robot that only\nexhibitsbackchanneling behavior (backchanneling-only). Through ourexperimental\nstudy with sixty-five participants, we found theparticipants who conversed with\nthe active listening robot per-ceived the interactions more positively, in\nwhich they exhibited the highest self-disclosures, and reported the strongest\nsenseof being listened to. The results of our study suggest that the\nimplementation of active listening behaviors in social robotshas the potential\nto improve human-robot communication andcould further contribute to the\nbuilding of deeper human-robot relationships and rapport."}
{"id": "2509.07309", "pdf": "https://arxiv.org/pdf/2509.07309.pdf", "abs": "https://arxiv.org/abs/2509.07309", "title": "Instance-level Performance Prediction for Long-form Generation Tasks", "authors": ["Chi-Yang Hsu", "Alexander Braylan", "Yiheng Su", "Omar Alonso", "Matthew Lease"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "We motivate and share a new benchmark for instance-level performance\nprediction of long-form generation tasks having multi-faceted, fine-grained\nquality metrics. Our task-, model- and metric-agnostic formulation predicts\ncontinuous evaluation metric scores given only black-box model inputs and\noutputs. Beyond predicting point estimates of metric scores, the benchmark also\nrequires inferring prediction intervals to quantify uncertainty around point\nestimates. Evaluation spans 11 long-form datasets/tasks with multiple LLMs,\nbaselines, and metrics per task. We show that scores can be effectively\npredicted across long-form generation tasks using as few as 16 training\nexamples. Overall, we introduce a novel and useful task, a valuable benchmark\nto drive progress, and baselines ready for practical adoption today."}
{"id": "2509.07897", "pdf": "https://arxiv.org/pdf/2509.07897.pdf", "abs": "https://arxiv.org/abs/2509.07897", "title": "dciWebMapper2: Enhancing the dciWebMapper framework toward integrated, interactive visualization of linked multi-type maps, charts, and spatial statistics and analysis", "authors": ["Sarigai Sarigai", "Liping Yang", "Katie Slack", "Carolyn Fish", "Michaela Buenemann", "Qiusheng Wu", "Yan Lin", "Joseph A. Cook", "David Jacobs"], "categories": ["cs.HC", "cs.DB", "cs.GR"], "comment": "15 figures, 2 tables, and three advanced interactive web map apps\n  that are openly available to the public", "summary": "As interactive web-based geovisualization becomes increasingly vital across\ndisciplines, there is a growing need for open-source frameworks that support\ndynamic, multi-attribute spatial analysis and accessible design. This paper\nintroduces dciWebMapper2, a significant expansion of the original dciWebMapper\nframework, designed to enable exploratory analysis across domains such as\nclimate justice, food access, and social vulnerability. The enhanced framework\nintegrates multiple map types, including choropleth, proportional symbol, small\nmultiples, and heatmaps, with linked statistical charts (e.g., scatter plots,\nboxplots) and time sliders, all within a coordinated-view environment.\nDropdown-based controls allow flexible, high-dimensional comparisons while\nmaintaining visual clarity. Grounded in cartographic and information\nvisualization principles, dciWebMapper2 is fully open-source, self-contained,\nand server-free, supporting modularity, reproducibility, and long-term\nsustainability. Three applied use cases demonstrate its adaptability and\npotential to democratize interactive web cartography. This work offers a\nversatile foundation for inclusive spatial storytelling and transparent\ngeospatial analysis in research, education, and civic engagement."}
{"id": "2509.07311", "pdf": "https://arxiv.org/pdf/2509.07311.pdf", "abs": "https://arxiv.org/abs/2509.07311", "title": "Does This Look Familiar to You? Knowledge Analysis via Model Internal Representations", "authors": ["Sihyun Park"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have been driven by\npretraining, supervised fine tuning (SFT), and alignment tuning. Among these,\nSFT plays a crucial role in transforming a model 's general knowledge into\nstructured responses tailored to specific tasks. However, there is no clearly\nestablished methodology for effective training data selection. Simply\nincreasing the volume of data does not guarantee performance improvements,\nwhile preprocessing, sampling, and validation require substantial time and\ncost.\n  To address this issue, a variety of data selection methods have been\nproposed. Among them, knowledge based selection approaches identify suitable\ntraining data by analyzing the model 's responses. Nevertheless, these methods\ntypically rely on prompt engineering, making them sensitive to variations and\nincurring additional costs for prompt design.\n  In this study, we propose Knowledge Analysis via Model Internal\nRepresentations (KAMIR), a novel approach that overcomes these limitations by\nanalyzing data based on the model 's internal representations. KAMIR computes\nsimilarities between the hidden states of each layer (block) and the final\nhidden states for a given input to assess the data. Unlike prior methods that\nwere largely limited to multiple choice tasks, KAMIR can be applied to a wide\nrange of tasks such as machine reading comprehension and summarization.\nMoreover, it selects data useful for training based on the model 's familiarity\nwith the input, even with a small dataset and a simple classifier architecture.\nExperiments across diverse task datasets demonstrate that training with less\nfamiliar data leads to better generalization performance."}
{"id": "2509.07942", "pdf": "https://arxiv.org/pdf/2509.07942.pdf", "abs": "https://arxiv.org/abs/2509.07942", "title": "Knowledge Isn't Power: The Ethics of Social Robots and the Difficulty of Informed Consent", "authors": ["James M. Berzuk", "Lauren Corcoran", "Brannen McKenzie-Lefurgey", "Katie Szilagyi", "James E. Young"], "categories": ["cs.HC", "cs.RO"], "comment": "Submitted to the International Journal of Social Robotics. 18 pages,\n  1 figure", "summary": "Contemporary robots are increasingly mimicking human social behaviours to\nfacilitate interaction, such as smiling to signal approachability, or\nhesitating before taking an action to allow people time to react. Such\ntechniques can activate a person's entrenched social instincts, triggering\nemotional responses as though they are interacting with a fellow human, and can\nprompt them to treat a robot as if it truly possesses the underlying life-like\nprocesses it outwardly presents, raising significant ethical questions. We\nengage these issues through the lens of informed consent: drawing upon\nprevailing legal principles and ethics, we examine how social robots can\ninfluence user behaviour in novel ways, and whether under those circumstances\nusers can be appropriately informed to consent to these heightened\ninteractions. We explore the complex circumstances of human-robot interaction\nand highlight how it differs from more familiar interaction contexts, and we\napply legal principles relating to informed consent to social robots in order\nto reconceptualize the current ethical debates surrounding the field. From this\ninvestigation, we synthesize design goals for robot developers to achieve more\nethical and informed human-robot interaction."}
{"id": "2509.07324", "pdf": "https://arxiv.org/pdf/2509.07324.pdf", "abs": "https://arxiv.org/abs/2509.07324", "title": "Mitigating Attention Localization in Small Scale: Self-Attention Refinement via One-step Belief Propagation", "authors": ["Nakyung Lee", "Yeongoon Kim", "Minhae Oh", "Suhwan Kim", "Jin Woo Koo", "Hyewon Jo", "Jungwoo Lee"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at EMNLP 2025", "summary": "Transformer-based self-attention mechanism serves as the core of modern\nlanguage models, yet it often suffers from localization, where attentions\ncollapse onto a limited subset of tokens and fail to capture long-range\ndependencies. To address this issue, we propose Self-Attention One-step Belief\nPropagation (SAOBP), a refinement framework that injects multi-hop\nrelationships through a belief propagation process. To interpret and quantify\nthese interactions, we introduce Global Token Dependency (GTD) that captures\nthe relative contribution of multihop connections within the attention graph.\nEmpirical results indicate that SAOBP helps prevent entropy collapse in deeper\nlayers and adaptively maintains GTD at task-appropriate levels, thereby\nsupporting improvements in model performance. Importantly, we observe\ncompetitive gains in small-scale models, highlighting its potential for\nimproving inference quality in resource-constrained scenarios."}
{"id": "2509.06964", "pdf": "https://arxiv.org/pdf/2509.06964.pdf", "abs": "https://arxiv.org/abs/2509.06964", "title": "Prototype: A Keyword Spotting-Based Intelligent Audio SoC for IoT", "authors": ["Huihong Liang", "Dongxuan Jia", "Youquan Wang", "Longtao Huang", "Shida Zhong", "Luping Xiang", "Lei Huang", "Tao Yuan"], "categories": ["cs.SD", "cs.AR", "cs.HC", "eess.AS"], "comment": null, "summary": "In this demo, we present a compact intelligent audio system-on-chip (SoC)\nintegrated with a keyword spotting accelerator, enabling ultra-low latency,\nlow-power, and low-cost voice interaction in Internet of Things (IoT) devices.\nThrough algorithm-hardware co-design, the system's energy efficiency is\nmaximized. We demonstrate the system's capabilities through a live FPGA-based\nprototype, showcasing stable performance and real-time voice interaction for\nedge intelligence applications."}
{"id": "2509.07370", "pdf": "https://arxiv.org/pdf/2509.07370.pdf", "abs": "https://arxiv.org/abs/2509.07370", "title": "PersonaFuse: A Personality Activation-Driven Framework for Enhancing Human-LLM Interactions", "authors": ["Yixuan Tang", "Yi Yang", "Ahmed Abbasi"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) demonstrate remarkable\ncapabilities across various fields. These developments have led to more direct\ncommunication between humans and LLMs in various situations, such as social\ncompanionship and psychological support. However, LLMs often exhibit\nlimitations in emotional perception and social competence during real-world\nconversations. These limitations partly originate from their inability to adapt\ntheir communication style and emotional expression to different social and task\ncontexts. In this work, we introduce PersonaFuse, a novel LLM post-training\nframework that enables LLMs to adapt and express different personalities for\nvarying situations. Inspired by Trait Activation Theory and the Big Five\npersonality model, PersonaFuse employs a Mixture-of-Expert architecture that\ncombines persona adapters with a dynamic routing network, enabling contextual\ntrait expression. Experimental results show that PersonaFuse substantially\noutperforms baseline models across multiple dimensions of social-emotional\nintelligence. Importantly, these gains are achieved without sacrificing general\nreasoning ability or model safety, which remain common limitations of direct\nprompting and supervised fine-tuning approaches. PersonaFuse also delivers\nconsistent improvements in downstream human-centered applications, such as\nmental health counseling and review-based customer service. Finally, human\npreference evaluations against leading LLMs, including GPT-4o and DeepSeek,\ndemonstrate that PersonaFuse achieves competitive response quality despite its\ncomparatively smaller model size. These findings demonstrate that\nPersonaFuse~offers a theoretically grounded and practical approach for\ndeveloping social-emotional enhanced LLMs, marking a significant advancement\ntoward more human-centric AI systems."}
{"id": "2509.07190", "pdf": "https://arxiv.org/pdf/2509.07190.pdf", "abs": "https://arxiv.org/abs/2509.07190", "title": "Rule-Based Moral Principles for Explaining Uncertainty in Natural Language Generation", "authors": ["Zahra Atf", "Peter R Lewis"], "categories": ["cs.CL", "cs.HC"], "comment": "This paper was accepted for presentation at the 35th IEEE\n  International Conference on Collaborative Advances in Software and Computing.\n  Conference website:https://conf.researchr.org/home/cascon-2025", "summary": "Large language models (LLMs) are increasingly used in high-stakes settings,\nwhere explaining uncertainty is both technical and ethical. Probabilistic\nmethods are often opaque and misaligned with expectations of transparency. We\npropose a framework based on rule-based moral principles for handling\nuncertainty in LLM-generated text. Using insights from moral psychology and\nvirtue ethics, we define rules such as precaution, deference, and\nresponsibility to guide responses under epistemic or aleatoric uncertainty.\nThese rules are encoded in a lightweight Prolog engine, where uncertainty\nlevels (low, medium, high) trigger aligned system actions with plain-language\nrationales. Scenario-based simulations benchmark rule coverage, fairness, and\ntrust calibration. Use cases in clinical and legal domains illustrate how moral\nreasoning can improve trust and interpretability. Our approach offers a\ntransparent, lightweight alternative to probabilistic models for socially\nresponsible natural language generation."}
{"id": "2509.07389", "pdf": "https://arxiv.org/pdf/2509.07389.pdf", "abs": "https://arxiv.org/abs/2509.07389", "title": "Talking with Oompa Loompas: A novel framework for evaluating linguistic acquisition of LLM agents", "authors": ["Sankalp Tattwadarshi Swain", "Anshika Krishnatray", "Dhruv Kumar", "Jagat Sesh Challa"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": "Under review", "summary": "Existing evaluation studies on linguistic competence of large language models\n(LLM agents) have focused primarily on vocabulary learning, morphological rule\ninduction, syntactic generalization, pragmatic inference, and cross-linguistic\ntransfer. However, none assess whether LLM agents can acquire a language\nthrough pattern recognition and interactive feedback, a central feature of\nhuman language acquisition. We propose a novel experimental framework in which\nan LLM agent is evaluated on its ability to acquire and use a newly constructed\nlanguage (Tinkatongue) in conversation with a bot that understands only\nTinkatongue. Our findings show that LLM agents fail to establish a conversation\nwithin 100 responses, yet they adopt distinct strategies that mirror human\napproaches to language learning. The results suggest a new direction for\nevaluation benchmarks and open pathways to model designs that learn more\neffectively from interactive feedback."}
{"id": "2509.07260", "pdf": "https://arxiv.org/pdf/2509.07260.pdf", "abs": "https://arxiv.org/abs/2509.07260", "title": "HealthSLM-Bench: Benchmarking Small Language Models for Mobile and Wearable Healthcare Monitoring", "authors": ["Xin Wang", "Ting Dang", "Xinyu Zhang", "Vassilis Kostakos", "Michael J. Witbrock", "Hong Jia"], "categories": ["cs.AI", "cs.HC", "cs.LG"], "comment": "9 pages, 6 tables, 6 figures", "summary": "Mobile and wearable healthcare monitoring play a vital role in facilitating\ntimely interventions, managing chronic health conditions, and ultimately\nimproving individuals' quality of life. Previous studies on large language\nmodels (LLMs) have highlighted their impressive generalization abilities and\neffectiveness in healthcare prediction tasks. However, most LLM-based\nhealthcare solutions are cloud-based, which raises significant privacy concerns\nand results in increased memory usage and latency. To address these challenges,\nthere is growing interest in compact models, Small Language Models (SLMs),\nwhich are lightweight and designed to run locally and efficiently on mobile and\nwearable devices. Nevertheless, how well these models perform in healthcare\nprediction remains largely unexplored. We systematically evaluated SLMs on\nhealth prediction tasks using zero-shot, few-shot, and instruction fine-tuning\napproaches, and deployed the best performing fine-tuned SLMs on mobile devices\nto evaluate their real-world efficiency and predictive performance in practical\nhealthcare scenarios. Our results show that SLMs can achieve performance\ncomparable to LLMs while offering substantial gains in efficiency and privacy.\nHowever, challenges remain, particularly in handling class imbalance and\nfew-shot scenarios. These findings highlight SLMs, though imperfect in their\ncurrent form, as a promising solution for next-generation, privacy-preserving\nhealthcare monitoring."}
{"id": "2509.07399", "pdf": "https://arxiv.org/pdf/2509.07399.pdf", "abs": "https://arxiv.org/abs/2509.07399", "title": "The Role of Exploration Modules in Small Language Models for Knowledge Graph Question Answering", "authors": ["Yi-Jie Cheng", "Oscar Chew", "Yun-Nung Chen"], "categories": ["cs.CL"], "comment": "Extended from ACL 2025 SRW", "summary": "Integrating knowledge graphs (KGs) into the reasoning processes of large\nlanguage models (LLMs) has emerged as a promising approach to mitigate\nhallucination. However, existing work in this area often relies on proprietary\nor extremely large models, limiting accessibility and scalability. In this\nstudy, we investigate the capabilities of existing integration methods for\nsmall language models (SLMs) in KG-based question answering and observe that\ntheir performance is often constrained by their limited ability to traverse and\nreason over knowledge graphs. To address this limitation, we propose leveraging\nsimple and efficient exploration modules to handle knowledge graph traversal in\nplace of the language model itself. Experiment results demonstrate that these\nlightweight modules effectively improve the performance of small language\nmodels on knowledge graph question answering tasks. Source code:\nhttps://github.com/yijie-cheng/SLM-ToG/."}
{"id": "2509.07389", "pdf": "https://arxiv.org/pdf/2509.07389.pdf", "abs": "https://arxiv.org/abs/2509.07389", "title": "Talking with Oompa Loompas: A novel framework for evaluating linguistic acquisition of LLM agents", "authors": ["Sankalp Tattwadarshi Swain", "Anshika Krishnatray", "Dhruv Kumar", "Jagat Sesh Challa"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": "Under review", "summary": "Existing evaluation studies on linguistic competence of large language models\n(LLM agents) have focused primarily on vocabulary learning, morphological rule\ninduction, syntactic generalization, pragmatic inference, and cross-linguistic\ntransfer. However, none assess whether LLM agents can acquire a language\nthrough pattern recognition and interactive feedback, a central feature of\nhuman language acquisition. We propose a novel experimental framework in which\nan LLM agent is evaluated on its ability to acquire and use a newly constructed\nlanguage (Tinkatongue) in conversation with a bot that understands only\nTinkatongue. Our findings show that LLM agents fail to establish a conversation\nwithin 100 responses, yet they adopt distinct strategies that mirror human\napproaches to language learning. The results suggest a new direction for\nevaluation benchmarks and open pathways to model designs that learn more\neffectively from interactive feedback."}
{"id": "2509.07403", "pdf": "https://arxiv.org/pdf/2509.07403.pdf", "abs": "https://arxiv.org/abs/2509.07403", "title": "LongEmotion: Measuring Emotional Intelligence of Large Language Models in Long-Context Interaction", "authors": ["Weichu Liu", "Jing Xiong", "Yuxuan Hu", "Zixuan Li", "Minghuan Tan", "Ningning Mao", "Chenyang Zhao", "Zhongwei Wan", "Chaofan Tao", "Wendong Xu", "Hui Shen", "Chengming Li", "Lingpeng Kong", "Ngai Wong"], "categories": ["cs.CL"], "comment": "Technical Report", "summary": "Large language models (LLMs) make significant progress in Emotional\nIntelligence (EI) and long-context understanding. However, existing benchmarks\ntend to overlook certain aspects of EI in long-context scenarios, especially\nunder realistic, practical settings where interactions are lengthy, diverse,\nand often noisy. To move towards such realistic settings, we present\nLongEmotion, a benchmark specifically designed for long-context EI tasks. It\ncovers a diverse set of tasks, including Emotion Classification, Emotion\nDetection, Emotion QA, Emotion Conversation, Emotion Summary, and Emotion\nExpression. On average, the input length for these tasks reaches 8,777 tokens,\nwith long-form generation required for Emotion Expression. To enhance\nperformance under realistic constraints, we incorporate Retrieval-Augmented\nGeneration (RAG) and Collaborative Emotional Modeling (CoEM), and compare them\nwith standard prompt-based methods. Unlike conventional approaches, our RAG\nmethod leverages both the conversation context and the large language model\nitself as retrieval sources, avoiding reliance on external knowledge bases. The\nCoEM method further improves performance by decomposing the task into five\nstages, integrating both retrieval augmentation and limited knowledge\ninjection. Experimental results show that both RAG and CoEM consistently\nenhance EI-related performance across most long-context tasks, advancing LLMs\ntoward more practical and real-world EI applications. Furthermore, we conducted\na comparative case study experiment on the GPT series to demonstrate the\ndifferences among various models in terms of EI. Code is available on GitHub at\nhttps://github.com/LongEmotion/LongEmotion, and the project page can be found\nat https://longemotion.github.io/."}
{"id": "2509.07438", "pdf": "https://arxiv.org/pdf/2509.07438.pdf", "abs": "https://arxiv.org/abs/2509.07438", "title": "Timing the Message: Language-Based Notifications for Time-Critical Assistive Settings", "authors": ["Ya-Chuan Hsu", "Jonathan DeCastro", "Andrew Silva", "Guy Rosman"], "categories": ["cs.RO", "cs.HC"], "comment": null, "summary": "In time-critical settings such as assistive driving, assistants often rely on\nalerts or haptic signals to prompt rapid human attention, but these cues\nusually leave humans to interpret situations and decide responses\nindependently, introducing potential delays or ambiguity in meaning.\nLanguage-based assistive systems can instead provide instructions backed by\ncontext, offering more informative guidance. However, current approaches (e.g.,\nsocial assistive robots) largely prioritize content generation while\noverlooking critical timing factors such as verbal conveyance duration, human\ncomprehension delays, and subsequent follow-through duration. These timing\nconsiderations are crucial in time-critical settings, where even minor delays\ncan substantially affect outcomes. We aim to study this inherent trade-off\nbetween timeliness and informativeness by framing the challenge as a sequential\ndecision-making problem using an augmented-state Markov Decision Process. We\ndesign a framework combining reinforcement learning and a generated offline\ntaxonomy dataset, where we balance the trade-off while enabling a scalable\ntaxonomy dataset generation pipeline. Empirical evaluation with synthetic\nhumans shows our framework improves success rates by over 40% compared to\nmethods that ignore time delays, while effectively balancing timeliness and\ninformativeness. It also exposes an often-overlooked trade-off between these\ntwo factors, opening new directions for optimizing communication in\ntime-critical human-AI assistance."}
{"id": "2509.07459", "pdf": "https://arxiv.org/pdf/2509.07459.pdf", "abs": "https://arxiv.org/abs/2509.07459", "title": "AIxcellent Vibes at GermEval 2025 Shared Task on Candy Speech Detection: Improving Model Performance by Span-Level Training", "authors": ["Christian Rene Thelen", "Patrick Gustav Blaneck", "Tobias Bornheim", "Niklas Grieger", "Stephan Bialonski"], "categories": ["cs.CL"], "comment": "6 pages, 1 figure, 2 tables", "summary": "Positive, supportive online communication in social media (candy speech) has\nthe potential to foster civility, yet automated detection of such language\nremains underexplored, limiting systematic analysis of its impact. We\ninvestigate how candy speech can be reliably detected in a 46k-comment German\nYouTube corpus by monolingual and multilingual language models, including\nGBERT, Qwen3 Embedding, and XLM-RoBERTa. We find that a multilingual\nXLM-RoBERTa-Large model trained to detect candy speech at the span level\noutperforms other approaches, ranking first in both binary positive F1: 0.8906)\nand categorized span-based detection (strict F1: 0.6307) subtasks at the\nGermEval 2025 Shared Task on Candy Speech Detection. We speculate that\nspan-based training, multilingual capabilities, and emoji-aware tokenizers\nimproved detection performance. Our results demonstrate the effectiveness of\nmultilingual models in identifying positive, supportive language."}
{"id": "2509.07674", "pdf": "https://arxiv.org/pdf/2509.07674.pdf", "abs": "https://arxiv.org/abs/2509.07674", "title": "Temporal Counterfactual Explanations of Behaviour Tree Decisions", "authors": ["Tamlin Love", "Antonio Andriella", "Guillem Alenyà"], "categories": ["cs.RO", "cs.HC"], "comment": "23 pages, 6 figures, submitted to Engineering Applications of\n  Artificial Intelligence", "summary": "Explainability is a critical tool in helping stakeholders understand robots.\nIn particular, the ability for robots to explain why they have made a\nparticular decision or behaved in a certain way is useful in this regard.\nBehaviour trees are a popular framework for controlling the decision-making of\nrobots and other software systems, and thus a natural question to ask is\nwhether or not a system driven by a behaviour tree is capable of answering\n\"why\" questions. While explainability for behaviour trees has seen some prior\nattention, no existing methods are capable of generating causal, counterfactual\nexplanations which detail the reasons for robot decisions and behaviour.\nTherefore, in this work, we introduce a novel approach which automatically\ngenerates counterfactual explanations in response to contrastive \"why\"\nquestions. Our method achieves this by first automatically building a causal\nmodel from the structure of the behaviour tree as well as domain knowledge\nabout the state and individual behaviour tree nodes. The resultant causal model\nis then queried and searched to find a set of diverse counterfactual\nexplanations. We demonstrate that our approach is able to correctly explain the\nbehaviour of a wide range of behaviour tree structures and states. By being\nable to answer a wide range of causal queries, our approach represents a step\ntowards more transparent, understandable and ultimately trustworthy robotic\nsystems."}
{"id": "2509.07462", "pdf": "https://arxiv.org/pdf/2509.07462.pdf", "abs": "https://arxiv.org/abs/2509.07462", "title": "Understanding Stigmatizing Language Lexicons: A Comparative Analysis in Clinical Contexts", "authors": ["Yiliang Zhou", "Di Hu", "Tianchu Lyu", "Jasmine Dhillon", "Alexandra L. Beck", "Gelareh Sadigh", "Kai Zheng"], "categories": ["cs.CL"], "comment": null, "summary": "Stigmatizing language results in healthcare inequities, yet there is no\nuniversally accepted or standardized lexicon defining which words, terms, or\nphrases constitute stigmatizing language in healthcare. We conducted a\nsystematic search of the literature to identify existing stigmatizing language\nlexicons and then analyzed them comparatively to examine: 1) similarities and\ndiscrepancies between these lexicons, and 2) the distribution of positive,\nnegative, or neutral terms based on an established sentiment dataset. Our\nsearch identified four lexicons. The analysis results revealed moderate\nsemantic similarity among them, and that most stigmatizing terms are related to\njudgmental expressions by clinicians to describe perceived negative behaviors.\nSentiment analysis showed a predominant proportion of negatively classified\nterms, though variations exist across lexicons. Our findings underscore the\nneed for a standardized lexicon and highlight challenges in defining\nstigmatizing language in clinical texts."}
{"id": "2509.07681", "pdf": "https://arxiv.org/pdf/2509.07681.pdf", "abs": "https://arxiv.org/abs/2509.07681", "title": "FUnc-SNE: A flexible, Fast, and Unconstrained algorithm for neighbour embeddings", "authors": ["Pierre Lambert", "Edouard Couplet", "Michel Verleysen", "John Aldo Lee"], "categories": ["cs.LG", "cs.HC"], "comment": "Preprint submitted to Neurocomputing", "summary": "Neighbour embeddings (NE) allow the representation of high dimensional\ndatasets into lower dimensional spaces and are often used in data\nvisualisation. In practice, accelerated approximations are employed to handle\nvery large datasets. Accelerating NE is challenging, and two main directions\nhave been explored: very coarse approximations based on negative sampling (as\nin UMAP) achieve high effective speed but may lack quality in the extracted\nstructures; less coarse approximations, as used in FIt-SNE or BH-t-SNE, offer\nbetter structure preservation at the cost of speed, while also restricting the\ntarget dimensionality to 2 or 3, limiting NE to visualisation. In some\nvariants, the precision of these costlier accelerations also enables\nfiner-grained control on the extracted structures through dedicated\nhyperparameters.\n  This paper proposes to bridge the gab between both approaches by introducing\na novel way to accelerate NE, requiring a small number of computations per\niteration while maintaining good fine-grained structure preservation and\nflexibility through hyperparameter tuning, without limiting the dimensionality\nof the embedding space. The method was designed for interactive exploration of\ndata; as such, it abandons the traditional two-phased approach of other NE\nmethods, allowing instantaneous visual feedback when changing hyperparameters,\neven when these control processes happening on the high-dimensional side of the\ncomputations. Experiments using a publicly available, GPU accelerated GUI\nintegration of the method show promising results in terms of speed, flexibility\nin the structures getting extracted, and show potential uses in broader machine\nlearning contexts with minimal algorithmic modifications. Central to this\nalgorithm is a novel approach to iterative approximate nearest neighbour\nsearch, which shows promising results compared to nearest neighbour descent."}
{"id": "2509.07471", "pdf": "https://arxiv.org/pdf/2509.07471.pdf", "abs": "https://arxiv.org/abs/2509.07471", "title": "From Scarcity to Efficiency: Investigating the Effects of Data Augmentation on African Machine Translation", "authors": ["Mardiyyah Oduwole", "Oluwatosin Olajide", "Jamiu Suleiman", "Faith Hunja", "Busayo Awobade", "Fatimo Adebanjo", "Comfort Akanni", "Chinonyelum Igwe", "Peace Ododo", "Promise Omoigui", "Steven Kolawole", "Abraham Owodunni"], "categories": ["cs.CL", "68T50", "I.7"], "comment": "8 pages, 3 tables. Exploratory work on Data Augmentation for African\n  Machine Translation", "summary": "The linguistic diversity across the African continent presents different\nchallenges and opportunities for machine translation. This study explores the\neffects of data augmentation techniques in improving translation systems in\nlow-resource African languages. We focus on two data augmentation techniques:\nsentence concatenation with back translation and switch-out, applying them\nacross six African languages. Our experiments show significant improvements in\nmachine translation performance, with a minimum increase of 25\\% in BLEU score\nacross all six languages.We provide a comprehensive analysis and highlight the\npotential of these techniques to improve machine translation systems for\nlow-resource languages, contributing to the development of more robust\ntranslation systems for under-resourced languages."}
{"id": "2509.07869", "pdf": "https://arxiv.org/pdf/2509.07869.pdf", "abs": "https://arxiv.org/abs/2509.07869", "title": "Are Humans as Brittle as Large Language Models?", "authors": ["Jiahui Li", "Sean Papay", "Roman Klinger"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "The output of large language models (LLM) is unstable, due to both\nnon-determinism of the decoding process as well as to prompt brittleness. While\nthe intrinsic non-determinism of LLM generation may mimic existing uncertainty\nin human annotations through distributional shifts in outputs, it is largely\nassumed, yet unexplored, that the prompt brittleness effect is unique to LLMs.\nThis raises the question: do human annotators show similar sensitivity to\ninstruction changes? If so, should prompt brittleness in LLMs be considered\nproblematic? One may alternatively hypothesize that prompt brittleness\ncorrectly reflects human annotation variances. To fill this research gap, we\nsystematically compare the effects of prompt modifications on LLMs and\nidentical instruction modifications for human annotators, focusing on the\nquestion of whether humans are similarly sensitive to prompt perturbations. To\nstudy this, we prompt both humans and LLMs for a set of text classification\ntasks conditioned on prompt variations. Our findings indicate that both humans\nand LLMs exhibit increased brittleness in response to specific types of prompt\nmodifications, particularly those involving the substitution of alternative\nlabel sets or label formats. However, the distribution of human judgments is\nless affected by typographical errors and reversed label order than that of\nLLMs."}
{"id": "2509.07475", "pdf": "https://arxiv.org/pdf/2509.07475.pdf", "abs": "https://arxiv.org/abs/2509.07475", "title": "HALT-RAG: A Task-Adaptable Framework for Hallucination Detection with Calibrated NLI Ensembles and Abstention", "authors": ["Saumya Goswami", "Siddharth Kurra"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Detecting content that contradicts or is unsupported by a given source text\nis a critical challenge for the safe deployment of generative language models.\nWe introduce HALT-RAG, a post-hoc verification system designed to identify\nhallucinations in the outputs of Retrieval-Augmented Generation (RAG)\npipelines. Our flexible and task-adaptable framework uses a universal feature\nset derived from an ensemble of two frozen, off-the-shelf Natural Language\nInference (NLI) models and lightweight lexical signals. These features are used\nto train a simple, calibrated, and task-adapted meta-classifier. Using a\nrigorous 5-fold out-of-fold (OOF) training protocol to prevent data leakage and\nproduce unbiased estimates, we evaluate our system on the HaluEval benchmark.\nBy pairing our universal feature set with a lightweight, task-adapted\nclassifier and a precision-constrained decision policy, HALT-RAG achieves\nstrong OOF F1-scores of 0.7756, 0.9786, and 0.7391 on the summarization, QA,\nand dialogue tasks, respectively. The system's well-calibrated probabilities\nenable a practical abstention mechanism, providing a reliable tool for\nbalancing model performance with safety requirements."}
{"id": "2411.02088", "pdf": "https://arxiv.org/pdf/2411.02088.pdf", "abs": "https://arxiv.org/abs/2411.02088", "title": "Affordances and Design Principles of The Political Left and Right", "authors": ["Felix Anand Epp", "Jesse Haapoja", "Matti Nelimarkka"], "categories": ["cs.HC", "H.5"], "comment": "24 pages, 5 figures, to be published in Proc. ACM Hum.-Comput.\n  Interact., Vol. 9, No. 7, CSCW series", "summary": "Like any form of technology, social media services embed values. To examine\nhow societal values may be present in these systems, we focus on exploring\npolitical ideology as a value system. We organised four co-design workshops\nwith political representatives from five major parties in Finland to\ninvestigate what values they would incorporate into social media services. The\nparticipants were divided into one right-leaning group, two left-leaning\ngroups, and one mixed group. This approach allows us to examine the differences\nin social media services designed by groups with different political ideologies\ni.e., value systems. We analysed produced artefacts (early-stage paper mockups)\nto identify different features and affordances for each group and then\ncontrasted the ideological compositions. Our results revealed a clear\ndistinction between groups: the right-leaning group favoured market-based\nvisibility, while left-leaning groups rejected such design principles in favour\nof open profile work. Additionally, we found tentative differences in design\noutcomes along the liberal--conservative dimension. These findings underscore\nthe importance of acknowledging existing political value systems in the design\nof social computing systems. They also highlight the need for further research\nto map out political ideologies in technology design."}
{"id": "2509.07512", "pdf": "https://arxiv.org/pdf/2509.07512.pdf", "abs": "https://arxiv.org/abs/2509.07512", "title": "ALLabel: Three-stage Active Learning for LLM-based Entity Recognition using Demonstration Retrieval", "authors": ["Zihan Chen", "Lei Shi", "Weize Wu", "Qiji Zhou", "Yue Zhang"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Many contemporary data-driven research efforts in the natural sciences, such\nas chemistry and materials science, require large-scale, high-performance\nentity recognition from scientific datasets. Large language models (LLMs) have\nincreasingly been adopted to solve the entity recognition task, with the same\ntrend being observed on all-spectrum NLP tasks. The prevailing entity\nrecognition LLMs rely on fine-tuned technology, yet the fine-tuning process\noften incurs significant cost. To achieve a best performance-cost trade-off, we\npropose ALLabel, a three-stage framework designed to select the most\ninformative and representative samples in preparing the demonstrations for LLM\nmodeling. The annotated examples are used to construct a ground-truth retrieval\ncorpus for LLM in-context learning. By sequentially employing three distinct\nactive learning strategies, ALLabel consistently outperforms all baselines\nunder the same annotation budget across three specialized domain datasets.\nExperimental results also demonstrate that selectively annotating only 5\\%-10\\%\nof the dataset with ALLabel can achieve performance comparable to the method\nannotating the entire dataset. Further analyses and ablation studies verify the\neffectiveness and generalizability of our proposal."}
{"id": "2411.07441", "pdf": "https://arxiv.org/pdf/2411.07441.pdf", "abs": "https://arxiv.org/abs/2411.07441", "title": "Automatically Detecting Online Deceptive Patterns", "authors": ["Asmit Nayak", "Shirley Zhang", "Yash Wani", "Rishabh Khandelwal", "Kassem Fawaz"], "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": null, "summary": "Deceptive patterns in digital interfaces manipulate users into making\nunintended decisions, exploiting cognitive biases and psychological\nvulnerabilities. These patterns have become ubiquitous on various digital\nplatforms. While efforts to mitigate deceptive patterns have emerged from legal\nand technical perspectives, a significant gap remains in creating usable and\nscalable solutions. We introduce our AutoBot framework to address this gap and\nhelp web stakeholders navigate and mitigate online deceptive patterns. AutoBot\naccurately identifies and localizes deceptive patterns from a screenshot of a\nwebsite without relying on the underlying HTML code. AutoBot employs a\ntwo-stage pipeline that leverages the capabilities of specialized vision models\nto analyze website screenshots, identify interactive elements, and extract\ntextual features. Next, using a large language model, AutoBot understands the\ncontext surrounding these elements to determine the presence of deceptive\npatterns. We also use AutoBot, to create a synthetic dataset to distill\nknowledge from 'teacher' LLMs to smaller language models. Through extensive\nevaluation, we demonstrate AutoBot's effectiveness in detecting deceptive\npatterns on the web, achieving an F1-score of 0.93 when detecting deceptive\npatterns, underscoring its potential as an essential tool for mitigating online\ndeceptive patterns. We implement AutoBot, across three downstream applications\ntargeting different web stakeholders: (1) a local browser extension providing\nusers with real-time feedback, (2) a Lighthouse audit to inform developers of\npotential deceptive patterns on their sites, and (3) as a measurement tool\ndesigned for researchers and regulators."}
{"id": "2509.07553", "pdf": "https://arxiv.org/pdf/2509.07553.pdf", "abs": "https://arxiv.org/abs/2509.07553", "title": "VeriOS: Query-Driven Proactive Human-Agent-GUI Interaction for Trustworthy OS Agents", "authors": ["Zheng Wu", "Heyuan Huang", "Xingyu Lou", "Xiangmou Qu", "Pengzhou Cheng", "Zongru Wu", "Weiwen Liu", "Weinan Zhang", "Jun Wang", "Zhaoxiang Wang", "Zhuosheng Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "With the rapid progress of multimodal large language models, operating system\n(OS) agents become increasingly capable of automating tasks through on-device\ngraphical user interfaces (GUIs). However, most existing OS agents are designed\nfor idealized settings, whereas real-world environments often present\nuntrustworthy conditions. To mitigate risks of over-execution in such\nscenarios, we propose a query-driven human-agent-GUI interaction framework that\nenables OS agents to decide when to query humans for more reliable task\ncompletion. Built upon this framework, we introduce VeriOS-Agent, a trustworthy\nOS agent trained with a two-stage learning paradigm that falicitate the\ndecoupling and utilization of meta-knowledge. Concretely, VeriOS-Agent\nautonomously executes actions in normal conditions while proactively querying\nhumans in untrustworthy scenarios. Experiments show that VeriOS-Agent improves\nthe average step-wise success rate by 20.64\\% in untrustworthy scenarios over\nthe state-of-the-art, without compromising normal performance. Analysis\nhighlights VeriOS-Agent's rationality, generalizability, and scalability. The\ncodes, datasets and models are available at\nhttps://github.com/Wuzheng02/VeriOS."}
{"id": "2412.02603", "pdf": "https://arxiv.org/pdf/2412.02603.pdf", "abs": "https://arxiv.org/abs/2412.02603", "title": "Generative AI as a Tool for Enhancing Reflective Learning in Students", "authors": ["Bo Yuan", "Jiazi Hu"], "categories": ["cs.HC"], "comment": "Accepted by IEEE TALE 2025", "summary": "Reflection is widely recognized as a cornerstone of student development,\nfostering critical thinking, self-regulation, and deep conceptual\nunderstanding. Traditionally, reflective skills have been cultivated through\nstructured feedback, mentorship, and guided self-assessment. However, these\napproaches often face challenges such as limited scalability, difficulties in\ndelivering individualized feedback, and a shortage of instructors proficient in\nfacilitating meaningful reflection. This study pioneers the use of generative\nAI, specifically large language models (LLMs), as an innovative solution to\nthese limitations. By leveraging the capacity of LLMs to deliver personalized,\ncontext-sensitive feedback at scale, this research investigates their potential\nto serve as effective facilitators of reflective exercises, sustaining deep\nengagement and promoting critical thinking. Through in-depth analyses of prompt\nengineering strategies and simulated multi-turn dialogues grounded in a\nproject-based learning (PBL) context, the study demonstrates that, with\npedagogically aligned prompts, LLMs can serve as accessible and adaptive tools\nfor scalable reflective guidance. Furthermore, LLM-assisted evaluation is\nemployed to objectively assess the performance of both tutors and students\nacross multiple dimensions of reflective learning. The findings contribute to\nthe evolving understanding of AI's role in reflective pedagogy and point to new\nopportunities for advancing AI-driven intelligent tutoring systems."}
{"id": "2509.07555", "pdf": "https://arxiv.org/pdf/2509.07555.pdf", "abs": "https://arxiv.org/abs/2509.07555", "title": "Avoiding Knowledge Edit Skipping in Multi-hop Question Answering with Guided Decomposition", "authors": ["Yi Liu", "Xiangrong Zhu", "Xiangyu Liu", "Wei Wei", "Wei Hu"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted in EMNLP Findings 2025", "summary": "In a rapidly evolving world where information updates swiftly, knowledge in\nlarge language models (LLMs) becomes outdated quickly. Retraining LLMs is not a\ncost-effective option, making knowledge editing (KE) without modifying\nparameters particularly necessary. We find that although existing\nretrieval-augmented generation (RAG)-based KE methods excel at editing simple\nknowledge, they struggle with KE in multi-hop question answering due to the\nissue of \"edit skipping\", which refers to skipping the relevant edited fact in\ninference. In addition to the diversity of natural language expressions of\nknowledge, edit skipping also arises from the mismatch between the granularity\nof LLMs in problem-solving and the facts in the edited memory. To address this\nissue, we propose a novel Iterative Retrieval-Augmented Knowledge Editing\nmethod with guided decomposition (IRAKE) through the guidance from single\nedited facts and entire edited cases. Experimental results demonstrate that\nIRAKE mitigates the failure of editing caused by edit skipping and outperforms\nstate-of-the-art methods for KE in multi-hop question answering."}
{"id": "2504.13934", "pdf": "https://arxiv.org/pdf/2504.13934.pdf", "abs": "https://arxiv.org/abs/2504.13934", "title": "VoxCity: A Seamless Framework for Open Geospatial Data Integration, Grid-Based Semantic 3D City Model Generation, and Urban Environment Simulation", "authors": ["Kunihiko Fujiwara", "Ryuta Tsurumi", "Tomoki Kiyono", "Zicheng Fan", "Xiucheng Liang", "Binyu Lei", "Winston Yap", "Koichi Ito", "Filip Biljecki"], "categories": ["cs.HC"], "comment": null, "summary": "Three-dimensional urban environment simulation is a powerful tool for\ninformed urban planning. However, the intensive manual effort required to\nprepare input 3D city models has hindered its widespread adoption. To address\nthis challenge, we present VoxCity, an open-source Python package that provides\na one-stop solution for grid-based 3D city model generation and urban\nenvironment simulation for cities worldwide. VoxCity's `generator' subpackage\nautomatically downloads building heights, tree canopy heights, land cover, and\nterrain elevation within a specified target area, and voxelizes buildings,\ntrees, land cover, and terrain to generate an integrated voxel city model. The\n`simulator' subpackage enables users to conduct environmental simulations,\nincluding solar radiation and view index analyses. Users can export the\ngenerated models using several file formats compatible with external software,\nsuch as ENVI-met (INX), Blender, and Rhino (OBJ). We generated 3D city models\nfor eight global cities, and demonstrated the calculation of solar irradiance,\nsky view index, and green view index. We also showcased microclimate simulation\nand 3D rendering visualization through ENVI-met and Rhino, respectively,\nthrough the file export function. Additionally, we reviewed openly available\ngeospatial data to create guidelines to help users choose appropriate data\nsources depending on their target areas and purposes. VoxCity can significantly\nreduce the effort and time required for 3D city model preparation and promote\nthe utilization of urban environment simulations. This contributes to more\ninformed urban and architectural design that considers environmental impacts,\nand in turn, fosters sustainable and livable cities. VoxCity is released openly\nat https://github.com/kunifujiwara/VoxCity."}
{"id": "2509.07588", "pdf": "https://arxiv.org/pdf/2509.07588.pdf", "abs": "https://arxiv.org/abs/2509.07588", "title": "BALI: Enhancing Biomedical Language Representations through Knowledge Graph and Language Model Alignment", "authors": ["Andrey Sakhovskiy", "Elena Tutubalina"], "categories": ["cs.CL", "cs.AI", "I.2.7; H.3.3; J.3"], "comment": "9 pages, 1 figure, published in \"The 48th International ACM SIGIR\n  Conference on Research and Development in Information Retrieval (SIGIR 2025)\"", "summary": "In recent years, there has been substantial progress in using pretrained\nLanguage Models (LMs) on a range of tasks aimed at improving the understanding\nof biomedical texts. Nonetheless, existing biomedical LLMs show limited\ncomprehension of complex, domain-specific concept structures and the factual\ninformation encoded in biomedical Knowledge Graphs (KGs). In this work, we\npropose BALI (Biomedical Knowledge Graph and Language Model Alignment), a novel\njoint LM and KG pre-training method that augments an LM with external knowledge\nby the simultaneous learning of a dedicated KG encoder and aligning the\nrepresentations of both the LM and the graph. For a given textual sequence, we\nlink biomedical concept mentions to the Unified Medical Language System (UMLS)\nKG and utilize local KG subgraphs as cross-modal positive samples for these\nmentions. Our empirical findings indicate that implementing our method on\nseveral leading biomedical LMs, such as PubMedBERT and BioLinkBERT, improves\ntheir performance on a range of language understanding tasks and the quality of\nentity representations, even with minimal pre-training on a small alignment\ndataset sourced from PubMed scientific abstracts."}
{"id": "2508.00103", "pdf": "https://arxiv.org/pdf/2508.00103.pdf", "abs": "https://arxiv.org/abs/2508.00103", "title": "A Mixed User-Centered Approach to Enable Augmented Intelligence in Intelligent Tutoring Systems: The Case of MathAIde app", "authors": ["Guilherme Guerino", "Luiz Rodrigues", "Luana Bianchini", "Mariana Alves", "Marcelo Marinho", "Thomaz Veloso", "Valmir Macario", "Diego Dermeval", "Thales Vieira", "Ig Bittencourt", "Seiji Isotani"], "categories": ["cs.HC", "cs.AI", "68T01", "H.5.0; I.2.0"], "comment": "Article published in the International Journal of Human-Computer\n  Interaction", "summary": "This study explores the integration of Augmented Intelligence (AuI) in\nIntelligent Tutoring Systems (ITS) to address challenges in Artificial\nIntelligence in Education (AIED), including teacher involvement, AI\nreliability, and resource accessibility. We present MathAIde, an ITS that uses\ncomputer vision and AI to correct mathematics exercises from student work\nphotos and provide feedback. The system was designed through a collaborative\nprocess involving brainstorming with teachers, high-fidelity prototyping, A/B\ntesting, and a real-world case study. Findings emphasize the importance of a\nteacher-centered, user-driven approach, where AI suggests remediation\nalternatives while teachers retain decision-making. Results highlight\nefficiency, usability, and adoption potential in classroom contexts,\nparticularly in resource-limited environments. The study contributes practical\ninsights into designing ITSs that balance user needs and technological\nfeasibility, while advancing AIED research by demonstrating the effectiveness\nof a mixed-methods, user-centered approach to implementing AuI in educational\ntechnologies."}
{"id": "2509.07622", "pdf": "https://arxiv.org/pdf/2509.07622.pdf", "abs": "https://arxiv.org/abs/2509.07622", "title": "MaLei at MultiClinSUM: Summarisation of Clinical Documents using Perspective-Aware Iterative Self-Prompting with LLMs", "authors": ["Libo Ren", "Yee Man Ng", "Lifeng Han"], "categories": ["cs.CL"], "comment": "system paper at CLEF 2025", "summary": "Efficient communication between patients and clinicians plays an important\nrole in shared decision-making. However, clinical reports are often lengthy and\nfilled with clinical jargon, making it difficult for domain experts to identify\nimportant aspects in the document efficiently. This paper presents the\nmethodology we applied in the MultiClinSUM shared task for summarising clinical\ncase documents. We used an Iterative Self-Prompting technique on large language\nmodels (LLMs) by asking LLMs to generate task-specific prompts and refine them\nvia example-based few-shot learning. Furthermore, we used lexical and embedding\nspace metrics, ROUGE and BERT-score, to guide the model fine-tuning with\nepochs. Our submission using perspective-aware ISP on GPT-4 and GPT-4o achieved\nROUGE scores (46.53, 24.68, 30.77) and BERTscores (87.84, 83.25, 85.46) for (P,\nR, F1) from the official evaluation on 3,396 clinical case reports from various\nspecialties extracted from open journals. The high BERTscore indicates that the\nmodel produced semantically equivalent output summaries compared to the\nreferences, even though the overlap at the exact lexicon level is lower, as\nreflected in the lower ROUGE scores. This work sheds some light on how\nperspective-aware ISP (PA-ISP) can be deployed for clinical report\nsummarisation and support better communication between patients and clinicians."}
{"id": "2508.11613", "pdf": "https://arxiv.org/pdf/2508.11613.pdf", "abs": "https://arxiv.org/abs/2508.11613", "title": "Adaptive Cardio Load Targets for Improving Fitness and Performance", "authors": ["Justin Phillips", "Daniel Roggen", "Cathy Speed", "Robert Harle"], "categories": ["cs.HC"], "comment": null, "summary": "Cardio Load, introduced by Google in 2024, is a measure of cardiovascular\nwork (also known as training load) resulting from all the user's activities\nacross the day. It is based on heart rate reserve and captures both activity\nintensity and duration. Thanks to feedback from users and internal research, we\nintroduce adaptive and personalized targets which will be set weekly. This\nfeature will be available in the Public Preview of the Fitbit app after\nSeptember 2025. This white paper provides a comprehensive overview of Cardio\nLoad (CL) and how weekly CL targets are established, with examples shown to\nillustrate the effect of varying CL on the weekly target. We compare Cardio\nLoad and Active Zone Minutes (AZMs), highlighting their distinct purposes, i.e.\nAZMs for health guidelines and CL for performance measurement. We highlight\nthat CL is accumulated both during active workouts and incidental daily\nactivities, so users are able top-up their CL score with small bouts of\nactivity across the day."}
{"id": "2509.07666", "pdf": "https://arxiv.org/pdf/2509.07666.pdf", "abs": "https://arxiv.org/abs/2509.07666", "title": "MoLoRAG: Bootstrapping Document Understanding via Multi-modal Logic-aware Retrieval", "authors": ["Xixi Wu", "Yanchao Tan", "Nan Hou", "Ruiyang Zhang", "Hong Cheng"], "categories": ["cs.CL", "cs.IR"], "comment": "EMNLP Main 2025", "summary": "Document Understanding is a foundational AI capability with broad\napplications, and Document Question Answering (DocQA) is a key evaluation task.\nTraditional methods convert the document into text for processing by Large\nLanguage Models (LLMs), but this process strips away critical multi-modal\ninformation like figures. While Large Vision-Language Models (LVLMs) address\nthis limitation, their constrained input size makes multi-page document\ncomprehension infeasible. Retrieval-augmented generation (RAG) methods mitigate\nthis by selecting relevant pages, but they rely solely on semantic relevance,\nignoring logical connections between pages and the query, which is essential\nfor reasoning.\n  To this end, we propose MoLoRAG, a logic-aware retrieval framework for\nmulti-modal, multi-page document understanding. By constructing a page graph\nthat captures contextual relationships between pages, a lightweight VLM\nperforms graph traversal to retrieve relevant pages, including those with\nlogical connections often overlooked. This approach combines semantic and\nlogical relevance to deliver more accurate retrieval. After retrieval, the\ntop-$K$ pages are fed into arbitrary LVLMs for question answering. To enhance\nflexibility, MoLoRAG offers two variants: a training-free solution for easy\ndeployment and a fine-tuned version to improve logical relevance checking.\nExperiments on four DocQA datasets demonstrate average improvements of 9.68% in\naccuracy over LVLM direct inference and 7.44% in retrieval precision over\nbaselines. Codes and datasets are released at\nhttps://github.com/WxxShirley/MoLoRAG."}
{"id": "2508.11892", "pdf": "https://arxiv.org/pdf/2508.11892.pdf", "abs": "https://arxiv.org/abs/2508.11892", "title": "RPKT: Learning What You Don't -- Know Recursive Prerequisite Knowledge Tracing in Conversational AI Tutors for Personalized Learning", "authors": ["Jinwen Tang", "Qiming Guo", "Zhicheng Tang", "Yi Shang"], "categories": ["cs.HC"], "comment": null, "summary": "Educational systems often assume learners can identify their knowledge gaps,\nyet research consistently shows that students struggle to recognize what they\ndon't know they need to learn-the \"unknown unknowns\" problem. This paper\npresents a novel Recursive Prerequisite Knowledge Tracing (RPKT) system that\naddresses this challenge through dynamic prerequisite discovery using large\nlanguage models. Unlike existing adaptive learning systems that rely on\npre-defined knowledge graphs, our approach recursively traces prerequisite\nconcepts in real-time until reaching a learner's actual knowledge boundary. The\nsystem employs LLMs for intelligent prerequisite extraction, implements binary\nassessment interfaces for cognitive load reduction, and provides personalized\nlearning paths based on identified knowledge gaps. Demonstration across\ncomputer science domains shows the system can discover multiple nested levels\nof prerequisite dependencies, identify cross-domain mathematical foundations,\nand generate hierarchical learning sequences without requiring pre-built\ncurricula. Our approach shows great potential for advancing personalized\neducation technology by enabling truly adaptive learning across any academic\ndomain."}
{"id": "2509.07730", "pdf": "https://arxiv.org/pdf/2509.07730.pdf", "abs": "https://arxiv.org/abs/2509.07730", "title": "M-BRe: Discovering Training Samples for Relation Extraction from Unlabeled Texts with Large Language Models", "authors": ["Zexuan Li", "Hongliang Dai", "Piji Li"], "categories": ["cs.CL"], "comment": "Accepted by EMNLP2025 Main Conference", "summary": "For Relation Extraction (RE), the manual annotation of training data may be\nprohibitively expensive, since the sentences that contain the target relations\nin texts can be very scarce and difficult to find. It is therefore beneficial\nto develop an efficient method that can automatically extract training\ninstances from unlabeled texts for training RE models. Recently, large language\nmodels (LLMs) have been adopted in various natural language processing tasks,\nwith RE also benefiting from their advances. However, when leveraging LLMs for\nRE with predefined relation categories, two key challenges arise. First, in a\nmulti-class classification setting, LLMs often struggle to comprehensively\ncapture the semantics of every relation, leading to suboptimal results. Second,\nalthough employing binary classification for each relation individually can\nmitigate this issue, it introduces significant computational overhead,\nresulting in impractical time complexity for real-world applications.\nTherefore, this paper proposes a framework called M-BRe to extract training\ninstances from unlabeled texts for RE. It utilizes three modules to combine the\nadvantages of both of the above classification approaches: Relation Grouping,\nRelation Extraction, and Label Decision. Extensive experiments confirm its\nsuperior capability in discovering high-quality training samples from unlabeled\ntexts for RE."}
{"id": "2508.19261", "pdf": "https://arxiv.org/pdf/2508.19261.pdf", "abs": "https://arxiv.org/abs/2508.19261", "title": "Floor sensors are cheap and easy to use! A Nihon Buyo Case Study", "authors": ["Miho Imai"], "categories": ["cs.HC"], "comment": "Withdrawn due to figures without appropriate permissions. A revised\n  version will be prepared", "summary": "As floor-sensing technologies gain traction in movement research, questions\nremain about their usability and effectiveness for non-expert users. This study\npresents a case study evaluating Flexel, a modular, low-cost, high-resolution\npressure-sensing floor interface, in the context of Nihon Buyo, a traditional\nJapanese dance. The system was installed, calibrated, and used by a first-time,\nnon-technical user to track weight distribution patterns of a teacher and\nlearner over nine weeks. Live pressure data was synchronized with video\nrecordings, and custom software was developed to process and analyze the\nsignal. Despite expectations that the learner's weight distribution would\nconverge toward the teacher's over time, quantitative analyses revealed that\nthe learner developed a consistent yet distinct movement profile. These\nfindings suggest that even within rigid pedagogical structures, individual\nmovement signatures can emerge. More importantly, the study demonstrates that\nFlexel can be deployed and operated effectively by non-expert users,\nhighlighting its potential for broader adoption in education, performance, and\nembodied research."}
{"id": "2509.07755", "pdf": "https://arxiv.org/pdf/2509.07755.pdf", "abs": "https://arxiv.org/abs/2509.07755", "title": "Factuality Beyond Coherence: Evaluating LLM Watermarking Methods for Medical Texts", "authors": ["Rochana Prih Hastuti", "Rian Adam Rajagede", "Mansour Al Ghanim", "Mengxin Zheng", "Qian Lou"], "categories": ["cs.CL", "cs.CR"], "comment": "Accepted at EMNLP 2025 Findings", "summary": "As large language models (LLMs) adapted to sensitive domains such as\nmedicine, their fluency raises safety risks, particularly regarding provenance\nand accountability. Watermarking embeds detectable patterns to mitigate these\nrisks, yet its reliability in medical contexts remains untested. Existing\nbenchmarks focus on detection-quality tradeoffs, overlooking factual risks\nunder low-entropy settings often exploited by watermarking's reweighting\nstrategy. We propose a medical-focused evaluation workflow that jointly\nassesses factual accuracy and coherence. Using GPT-Judger and further human\nvalidation, we introduce the Factuality-Weighted Score (FWS), a composite\nmetric prioritizing factual accuracy beyond coherence to guide watermarking\ndeployment in medical domains. Our evaluation shows current watermarking\nmethods substantially compromise medical factuality, with entropy shifts\ndegrading medical entity representation. These findings underscore the need for\ndomain-aware watermarking approaches that preserve the integrity of medical\ncontent."}
{"id": "2509.06776", "pdf": "https://arxiv.org/pdf/2509.06776.pdf", "abs": "https://arxiv.org/abs/2509.06776", "title": "Hue4U: Real-Time Personalized Color Correction in Augmented Reality", "authors": ["Jingwen Qin", "Semen Checherin", "Yue Li", "Berend-Jan van der Zwaag", "Ozlem Durmaz-Incel"], "categories": ["cs.HC", "cs.MM"], "comment": null, "summary": "Color Vision Deficiency (CVD) affects nearly 8 percent of men and 0.5 percent\nof women worldwide. Existing color-correction methods often rely on prior\nclinical diagnosis and static filtering, making them less effective for users\nwith mild or moderate CVD. In this paper, we introduce Hue4U, a personalized,\nreal-time color-correction system in augmented reality using consumer-grade\nMeta Quest headsets. Unlike previous methods, Hue4U requires no prior medical\ndiagnosis and adapts to the user in real time. A user study with 10\nparticipants showed notable improvements in their ability to distinguish\ncolors. The results demonstrated large effect sizes (Cohen's d > 1.4),\nsuggesting clinically meaningful gains for individuals with CVD. These findings\nhighlight the potential of personalized AR interventions to improve visual\naccessibility and quality of life for people affected by CVD."}
{"id": "2509.07768", "pdf": "https://arxiv.org/pdf/2509.07768.pdf", "abs": "https://arxiv.org/abs/2509.07768", "title": "Are LLMs Enough for Hyperpartisan, Fake, Polarized and Harmful Content Detection? Evaluating In-Context Learning vs. Fine-Tuning", "authors": ["Michele Joshua Maggini", "Dhia Merzougui", "Rabiraj Bandyopadhyay", "Gaël Dias", "Fabrice Maurel", "Pablo Gamallo"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The spread of fake news, polarizing, politically biased, and harmful content\non online platforms has been a serious concern. With large language models\nbecoming a promising approach, however, no study has properly benchmarked their\nperformance across different models, usage methods, and languages. This study\npresents a comprehensive overview of different Large Language Models adaptation\nparadigms for the detection of hyperpartisan and fake news, harmful tweets, and\npolitical bias. Our experiments spanned 10 datasets and 5 different languages\n(English, Spanish, Portuguese, Arabic and Bulgarian), covering both binary and\nmulticlass classification scenarios. We tested different strategies ranging\nfrom parameter efficient Fine-Tuning of language models to a variety of\ndifferent In-Context Learning strategies and prompts. These included zero-shot\nprompts, codebooks, few-shot (with both randomly-selected and\ndiversely-selected examples using Determinantal Point Processes), and\nChain-of-Thought. We discovered that In-Context Learning often underperforms\nwhen compared to Fine-Tuning a model. This main finding highlights the\nimportance of Fine-Tuning even smaller models on task-specific settings even\nwhen compared to the largest models evaluated in an In-Context Learning setup -\nin our case LlaMA3.1-8b-Instruct, Mistral-Nemo-Instruct-2407 and\nQwen2.5-7B-Instruct."}
{"id": "2509.00167", "pdf": "https://arxiv.org/pdf/2509.00167.pdf", "abs": "https://arxiv.org/abs/2509.00167", "title": "Pilot Study on Generative AI and Critical Thinking in Higher Education Classrooms", "authors": ["W. F. Lamberti", "S. R. Lawrence", "D. White", "S. Kim", "S. Abdullah"], "categories": ["cs.CY", "cs.AI", "cs.HC", "stat.AP"], "comment": null, "summary": "Generative AI (GAI) tools have seen rapid adoption in educational settings,\nyet their role in fostering critical thinking remains underexplored. While\nprevious studies have examined GAI as a tutor for specific lessons or as a tool\nfor completing assignments, few have addressed how students critically evaluate\nthe accuracy and appropriateness of GAI-generated responses. This pilot study\ninvestigates students' ability to apply structured critical thinking when\nassessing Generative AI outputs in introductory Computational and Data Science\ncourses. Given that GAI tools often produce contextually flawed or factually\nincorrect answers, we designed learning activities that require students to\nanalyze, critique, and revise AI-generated solutions. Our findings offer\ninitial insights into students' ability to engage critically with GAI content\nand lay the groundwork for more comprehensive studies in future semesters."}
{"id": "2509.07801", "pdf": "https://arxiv.org/pdf/2509.07801.pdf", "abs": "https://arxiv.org/abs/2509.07801", "title": "SciNLP: A Domain-Specific Benchmark for Full-Text Scientific Entity and Relation Extraction in NLP", "authors": ["Decheng Duan", "Yingyi Zhang", "Jitong Peng", "Chengzhi Zhang"], "categories": ["cs.CL", "cs.DL", "cs.IR"], "comment": "EMNLP 2025 Main", "summary": "Structured information extraction from scientific literature is crucial for\ncapturing core concepts and emerging trends in specialized fields. While\nexisting datasets aid model development, most focus on specific publication\nsections due to domain complexity and the high cost of annotating scientific\ntexts. To address this limitation, we introduce SciNLP - a specialized\nbenchmark for full-text entity and relation extraction in the Natural Language\nProcessing (NLP) domain. The dataset comprises 60 manually annotated full-text\nNLP publications, covering 7,072 entities and 1,826 relations. Compared to\nexisting research, SciNLP is the first dataset providing full-text annotations\nof entities and their relationships in the NLP domain. To validate the\neffectiveness of SciNLP, we conducted comparative experiments with similar\ndatasets and evaluated the performance of state-of-the-art supervised models on\nthis dataset. Results reveal varying extraction capabilities of existing models\nacross academic texts of different lengths. Cross-comparisons with existing\ndatasets show that SciNLP achieves significant performance improvements on\ncertain baseline models. Using models trained on SciNLP, we implemented\nautomatic construction of a fine-grained knowledge graph for the NLP domain.\nOur KG has an average node degree of 3.2 per entity, indicating rich semantic\ntopological information that enhances downstream applications. The dataset is\npublicly available at https://github.com/AKADDC/SciNLP."}
{"id": "2509.04404", "pdf": "https://arxiv.org/pdf/2509.04404.pdf", "abs": "https://arxiv.org/abs/2509.04404", "title": "No Thoughts Just AI: Biased LLM Hiring Recommendations Alter Human Decision Making and Limit Human Autonomy", "authors": ["Kyra Wilson", "Mattea Sim", "Anna-Maria Gueorguieva", "Aylin Caliskan"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC", "K.4.2"], "comment": "Published in Proceedings of the 2025 AAAI/ACM Conference on AI,\n  Ethics, and Society; code available at\n  https://github.com/kyrawilson/No-Thoughts-Just-AI", "summary": "In this study, we conduct a resume-screening experiment (N=528) where people\ncollaborate with simulated AI models exhibiting race-based preferences (bias)\nto evaluate candidates for 16 high and low status occupations. Simulated AI\nbias approximates factual and counterfactual estimates of racial bias in\nreal-world AI systems. We investigate people's preferences for White, Black,\nHispanic, and Asian candidates (represented through names and affinity groups\non quality-controlled resumes) across 1,526 scenarios and measure their\nunconscious associations between race and status using implicit association\ntests (IATs), which predict discriminatory hiring decisions but have not been\ninvestigated in human-AI collaboration. When making decisions without AI or\nwith AI that exhibits no race-based preferences, people select all candidates\nat equal rates. However, when interacting with AI favoring a particular group,\npeople also favor those candidates up to 90% of the time, indicating a\nsignificant behavioral shift. The likelihood of selecting candidates whose\nidentities do not align with common race-status stereotypes can increase by 13%\nif people complete an IAT before conducting resume screening. Finally, even if\npeople think AI recommendations are low quality or not important, their\ndecisions are still vulnerable to AI bias under certain circumstances. This\nwork has implications for people's autonomy in AI-HITL scenarios, AI and work,\ndesign and evaluation of AI hiring systems, and strategies for mitigating bias\nin collaborative decision-making tasks. In particular, organizational and\nregulatory policy should acknowledge the complex nature of AI-HITL decision\nmaking when implementing these systems, educating people who use them, and\ndetermining which are subject to oversight."}
{"id": "2509.07817", "pdf": "https://arxiv.org/pdf/2509.07817.pdf", "abs": "https://arxiv.org/abs/2509.07817", "title": "Dual Knowledge-Enhanced Two-Stage Reasoner for Multimodal Dialog Systems", "authors": ["Xiaolin Chen", "Xuemeng Song", "Haokun Wen", "Weili Guan", "Xiangyu Zhao", "Liqiang Nie"], "categories": ["cs.CL", "cs.MM"], "comment": null, "summary": "Textual response generation is pivotal for multimodal \\mbox{task-oriented}\ndialog systems, which aims to generate proper textual responses based on the\nmultimodal context. While existing efforts have demonstrated remarkable\nprogress, there still exist the following limitations: 1) \\textit{neglect of\nunstructured review knowledge} and 2) \\textit{underutilization of large\nlanguage models (LLMs)}. Inspired by this, we aim to fully utilize dual\nknowledge (\\textit{i.e., } structured attribute and unstructured review\nknowledge) with LLMs to promote textual response generation in multimodal\ntask-oriented dialog systems. However, this task is non-trivial due to two key\nchallenges: 1) \\textit{dynamic knowledge type selection} and 2)\n\\textit{intention-response decoupling}. To address these challenges, we propose\na novel dual knowledge-enhanced two-stage reasoner by adapting LLMs for\nmultimodal dialog systems (named DK2R). To be specific, DK2R first extracts\nboth structured attribute and unstructured review knowledge from external\nknowledge base given the dialog context. Thereafter, DK2R uses an LLM to\nevaluate each knowledge type's utility by analyzing LLM-generated provisional\nprobe responses. Moreover, DK2R separately summarizes the intention-oriented\nkey clues via dedicated reasoning, which are further used as auxiliary signals\nto enhance LLM-based textual response generation. Extensive experiments\nconducted on a public dataset verify the superiority of DK2R. We have released\nthe codes and parameters."}
{"id": "2509.07829", "pdf": "https://arxiv.org/pdf/2509.07829.pdf", "abs": "https://arxiv.org/abs/2509.07829", "title": "Small Open Models Achieve Near Parity with Large Models in Low Resource Literary Translation at a Fraction of the Cost", "authors": ["Mihai Nadas", "Laura Diosan", "Andreea Tomescu", "Andrei Piscoran"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "25 pages, 8 figures, includes datasets and models released on Hugging\n  Face", "summary": "Literary translation has recently gained attention as a distinct and complex\ntask in machine translation research. However, the translation by small open\nmodels remains an open problem. We contribute to this ongoing research by\nintroducing TINYFABULIST TRANSLATION FRAMEWORK (TF2), a unified framework for\ndataset creation, fine tuning, and evaluation in English-Romanian literary\ntranslations, centred on the creation and open release of both a compact, fine\ntuned language model (TF2-12B) and large scale synthetic parallel datasets\n(DS-TF2-EN-RO-3M and DS-TF2-EN-RO-15K). Building on DS-TF1-EN-3M (TF1), the\nlargest collection of synthetic English fables to date, we address the need for\nrich, high quality literary datasets in low resource languages such as\nRomanian. Our pipeline first generates 15k high quality Romanian references\nfrom the TF1 pool using a high performing LLM. We then apply a two stage fine\ntuning process to a 12B parameter open weight model: (i) instruction tuning to\ncapture genre specific narrative style, and (ii) adapter compression for\nefficient deployment. Evaluation combines corpus level BLEU and a five\ndimension LLM based rubric (accuracy, fluency, coherence, style, cultural\nadaptation) to provide a nuanced assessment of translation quality. Results\nshow that our fine tuned model achieves fluency and adequacy competitive with\ntop performing large proprietary models, while being open, accessible, and\nsignificantly more cost effective. Alongside the fine tuned model and both\ndatasets, we publicly release all scripts and evaluation prompts. TF2 thus\nprovides an end-to-end, reproducible pipeline for research on cost efficient\ntranslation, cross lingual narrative generation, and the broad adoption of open\nmodels for culturally significant literary content in low resource settings."}
{"id": "2509.07869", "pdf": "https://arxiv.org/pdf/2509.07869.pdf", "abs": "https://arxiv.org/abs/2509.07869", "title": "Are Humans as Brittle as Large Language Models?", "authors": ["Jiahui Li", "Sean Papay", "Roman Klinger"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "The output of large language models (LLM) is unstable, due to both\nnon-determinism of the decoding process as well as to prompt brittleness. While\nthe intrinsic non-determinism of LLM generation may mimic existing uncertainty\nin human annotations through distributional shifts in outputs, it is largely\nassumed, yet unexplored, that the prompt brittleness effect is unique to LLMs.\nThis raises the question: do human annotators show similar sensitivity to\ninstruction changes? If so, should prompt brittleness in LLMs be considered\nproblematic? One may alternatively hypothesize that prompt brittleness\ncorrectly reflects human annotation variances. To fill this research gap, we\nsystematically compare the effects of prompt modifications on LLMs and\nidentical instruction modifications for human annotators, focusing on the\nquestion of whether humans are similarly sensitive to prompt perturbations. To\nstudy this, we prompt both humans and LLMs for a set of text classification\ntasks conditioned on prompt variations. Our findings indicate that both humans\nand LLMs exhibit increased brittleness in response to specific types of prompt\nmodifications, particularly those involving the substitution of alternative\nlabel sets or label formats. However, the distribution of human judgments is\nless affected by typographical errors and reversed label order than that of\nLLMs."}
{"id": "2509.07889", "pdf": "https://arxiv.org/pdf/2509.07889.pdf", "abs": "https://arxiv.org/abs/2509.07889", "title": "From Detection to Mitigation: Addressing Gender Bias in Chinese Texts via Efficient Tuning and Voting-Based Rebalancing", "authors": ["Chengyan Wu", "Yiqiang Cai", "Yufei Cheng", "Yun Xue"], "categories": ["cs.CL"], "comment": "NLPCC 2025", "summary": "This paper presents our team's solution to Shared Task 7 of NLPCC-2025, which\nfocuses on sentence-level gender bias detection and mitigation in Chinese. The\ntask aims to promote fairness and controllability in natural language\ngeneration by automatically detecting, classifying, and mitigating gender bias.\nTo address this challenge, we adopt a fine-tuning approach based on large\nlanguage models (LLMs), efficiently adapt to the bias detection task via\nLow-Rank Adaptation (LoRA). In terms of data processing, we construct a more\nbalanced training set to alleviate class imbalance and introduce heterogeneous\nsamples from multiple sources to enhance model generalization. For the\ndetection and classification sub-tasks, we employ a majority voting strategy\nthat integrates outputs from multiple expert models to boost performance.\nAdditionally, to improve bias generation detection and mitigation, we design a\nmulti-temperature sampling mechanism to capture potential variations in bias\nexpression styles. Experimental results demonstrate the effectiveness of our\napproach in bias detection, classification, and mitigation. Our method\nultimately achieves an average score of 47.90%, ranking fourth in the shared\ntask."}
{"id": "2509.07908", "pdf": "https://arxiv.org/pdf/2509.07908.pdf", "abs": "https://arxiv.org/abs/2509.07908", "title": "Biased Tales: Cultural and Topic Bias in Generating Children's Stories", "authors": ["Donya Rooein", "Vilém Zouhar", "Debora Nozza", "Dirk Hovy"], "categories": ["cs.CL"], "comment": null, "summary": "Stories play a pivotal role in human communication, shaping beliefs and\nmorals, particularly in children. As parents increasingly rely on large\nlanguage models (LLMs) to craft bedtime stories, the presence of cultural and\ngender stereotypes in these narratives raises significant concerns. To address\nthis issue, we present Biased Tales, a comprehensive dataset designed to\nanalyze how biases influence protagonists' attributes and story elements in\nLLM-generated stories. Our analysis uncovers striking disparities. When the\nprotagonist is described as a girl (as compared to a boy), appearance-related\nattributes increase by 55.26%. Stories featuring non-Western children\ndisproportionately emphasize cultural heritage, tradition, and family themes\nfar more than those for Western children. Our findings highlight the role of\nsociocultural bias in making creative AI use more equitable and diverse."}
{"id": "2509.07925", "pdf": "https://arxiv.org/pdf/2509.07925.pdf", "abs": "https://arxiv.org/abs/2509.07925", "title": "GENUINE: Graph Enhanced Multi-level Uncertainty Estimation for Large Language Models", "authors": ["Tuo Wang", "Adithya Kulkarni", "Tyler Cody", "Peter A. Beling", "Yujun Yan", "Dawei Zhou"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by EMNLP 2025", "summary": "Uncertainty estimation is essential for enhancing the reliability of Large\nLanguage Models (LLMs), particularly in high-stakes applications. Existing\nmethods often overlook semantic dependencies, relying on token-level\nprobability measures that fail to capture structural relationships within the\ngenerated text. We propose GENUINE: Graph ENhanced mUlti-level uncertaINty\nEstimation for Large Language Models, a structure-aware framework that\nleverages dependency parse trees and hierarchical graph pooling to refine\nuncertainty quantification. By incorporating supervised learning, GENUINE\neffectively models semantic and structural relationships, improving confidence\nassessments. Extensive experiments across NLP tasks show that GENUINE achieves\nup to 29% higher AUROC than semantic entropy-based approaches and reduces\ncalibration errors by over 15%, demonstrating the effectiveness of graph-based\nuncertainty modeling. The code is available at\nhttps://github.com/ODYSSEYWT/GUQ."}
{"id": "2509.07968", "pdf": "https://arxiv.org/pdf/2509.07968.pdf", "abs": "https://arxiv.org/abs/2509.07968", "title": "SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric Knowledge", "authors": ["Lukas Haas", "Gal Yona", "Giovanni D'Antonio", "Sasha Goldshtein", "Dipanjan Das"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce SimpleQA Verified, a 1,000-prompt benchmark for evaluating Large\nLanguage Model (LLM) short-form factuality based on OpenAI's SimpleQA. It\naddresses critical limitations in OpenAI's benchmark, including noisy and\nincorrect labels, topical biases, and question redundancy. SimpleQA Verified\nwas created through a rigorous multi-stage filtering process involving\nde-duplication, topic balancing, and source reconciliation to produce a more\nreliable and challenging evaluation set, alongside improvements in the\nautorater prompt. On this new benchmark, Gemini 2.5 Pro achieves a\nstate-of-the-art F1-score of 55.6, outperforming other frontier models,\nincluding GPT-5. This work provides the research community with a\nhigher-fidelity tool to track genuine progress in parametric model factuality\nand to mitigate hallucinations. The benchmark dataset, evaluation code, and\nleaderboard are available at:\nhttps://www.kaggle.com/benchmarks/deepmind/simpleqa-verified."}
{"id": "2509.07980", "pdf": "https://arxiv.org/pdf/2509.07980.pdf", "abs": "https://arxiv.org/abs/2509.07980", "title": "Parallel-R1: Towards Parallel Thinking via Reinforcement Learning", "authors": ["Tong Zheng", "Hongming Zhang", "Wenhao Yu", "Xiaoyang Wang", "Xinyu Yang", "Runpeng Dai", "Rui Liu", "Huiwen Bao", "Chengsong Huang", "Heng Huang", "Dong Yu"], "categories": ["cs.CL"], "comment": "Project website: https://zhengkid.github.io/Parallel_R1.github.io/", "summary": "Parallel thinking has emerged as a novel approach for enhancing the reasoning\ncapabilities of large language models (LLMs) by exploring multiple reasoning\npaths concurrently. However, activating such capabilities through training\nremains challenging, as existing methods predominantly rely on supervised\nfine-tuning (SFT) over synthetic data, which encourages teacher-forced\nimitation rather than exploration and generalization. Different from them, we\npropose \\textbf{Parallel-R1}, the first reinforcement learning (RL) framework\nthat enables parallel thinking behaviors for complex real-world reasoning\ntasks. Our framework employs a progressive curriculum that explicitly addresses\nthe cold-start problem in training parallel thinking with RL. We first use SFT\non prompt-generated trajectories from easier tasks to instill the parallel\nthinking ability, then transition to RL to explore and generalize this skill on\nharder problems. Experiments on various math benchmarks, including MATH, AMC23,\nand AIME, show that Parallel-R1 successfully instills parallel thinking,\nleading to 8.4% accuracy improvements over the sequential thinking model\ntrained directly on challenging tasks with RL. Further analysis reveals a clear\nshift in the model's thinking behavior: at an early stage, it uses parallel\nthinking as an exploration strategy, while in a later stage, it uses the same\ncapability for multi-perspective verification. Most significantly, we validate\nparallel thinking as a \\textbf{mid-training exploration scaffold}, where this\ntemporary exploratory phase unlocks a higher performance ceiling after RL,\nyielding a 42.9% improvement over the baseline on AIME25. Our model, data, and\ncode will be open-source at https://github.com/zhengkid/Parallel-R1."}
{"id": "2509.06982", "pdf": "https://arxiv.org/pdf/2509.06982.pdf", "abs": "https://arxiv.org/abs/2509.06982", "title": "CARE: Decoding Time Safety Alignment via Rollback and Introspection Intervention", "authors": ["Xiaomeng Hu", "Fei Huang", "Chenhan Yuan", "Junyang Lin", "Tsung-Yi Ho"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "As large language models (LLMs) are increasingly deployed in real-world\napplications, ensuring the safety of their outputs during decoding has become a\ncritical challenge. However, existing decoding-time interventions, such as\nContrastive Decoding, often force a severe trade-off between safety and\nresponse quality. In this work, we propose CARE, a novel framework for\ndecoding-time safety alignment that integrates three key components: (1) a\nguard model for real-time safety monitoring, enabling detection of potentially\nunsafe content; (2) a rollback mechanism with a token buffer to correct unsafe\noutputs efficiently at an earlier stage without disrupting the user experience;\nand (3) a novel introspection-based intervention strategy, where the model\ngenerates self-reflective critiques of its previous outputs and incorporates\nthese reflections into the context to guide subsequent decoding steps. The\nframework achieves a superior safety-quality trade-off by using its guard model\nfor precise interventions, its rollback mechanism for timely corrections, and\nour novel introspection method for effective self-correction. Experimental\nresults demonstrate that our framework achieves a superior balance of safety,\nquality, and efficiency, attaining a low harmful response rate and minimal\ndisruption to the user experience while maintaining high response quality."}
{"id": "2509.06994", "pdf": "https://arxiv.org/pdf/2509.06994.pdf", "abs": "https://arxiv.org/abs/2509.06994", "title": "VLMs-in-the-Wild: Bridging the Gap Between Academic Benchmarks and Enterprise Reality", "authors": ["Srihari Bandraupalli", "Anupam Purwar"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Open-source Vision-Language Models show immense promise for enterprise\napplications, yet a critical disconnect exists between academic evaluation and\nenterprise deployment requirements. Current benchmarks rely heavily on\nmultiple-choice questions and synthetic data, failing to capture the complexity\nof real-world business applications like social media content analysis. This\npaper introduces VLM-in-the-Wild (ViLD), a comprehensive framework to bridge\nthis gap by evaluating VLMs on operational enterprise requirements. We define\nten business-critical tasks: logo detection, OCR, object detection, human\npresence and demographic analysis, human activity and appearance analysis,\nscene detection, camera perspective and media quality assessment, dominant\ncolors, comprehensive description, and NSFW detection. To this framework, we\nbring an innovative BlockWeaver Algorithm that solves the challenging problem\nof comparing unordered, variably-grouped OCR outputs from VLMs without relying\non embeddings or LLMs, achieving remarkable speed and reliability. To\ndemonstrate efficacy of ViLD, we constructed a new benchmark dataset of 7,500\ndiverse samples, carefully stratified from a corpus of one million real-world\nimages and videos. ViLD provides actionable insights by combining semantic\nmatching (both embedding-based and LLM-as-a-judge approaches), traditional\nmetrics, and novel methods to measure the completeness and faithfulness of\ndescriptive outputs. By benchmarking leading open-source VLMs (Qwen, MIMO, and\nInternVL) against a powerful proprietary baseline as per ViLD framework, we\nprovide one of the first industry-grounded, task-driven assessment of VLMs\ncapabilities, offering actionable insights for their deployment in enterprise\nenvironments."}
{"id": "2509.07006", "pdf": "https://arxiv.org/pdf/2509.07006.pdf", "abs": "https://arxiv.org/abs/2509.07006", "title": "ArGen: Auto-Regulation of Generative AI via GRPO and Policy-as-Code", "authors": ["Kapil Madan"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG", "68T07, 68T50", "I.2.6; I.2.7; K.4.1"], "comment": "53 pages, 7 figures, 8 tables. Open-source implementation available\n  at: https://github.com/Principled-Evolution/argen-demo. Work explores the\n  integration of policy-as-code for AI alignment, with a case study in\n  culturally-nuanced, ethical AI using Dharmic principles", "summary": "This paper introduces ArGen (Auto-Regulation of Generative AI systems), a\nframework for aligning Large Language Models (LLMs) with complex sets of\nconfigurable, machine-readable rules spanning ethical principles, operational\nsafety protocols, and regulatory compliance standards. Moving beyond just\npreference-based alignment, ArGen is designed to ensure LLMs adhere to these\nmultifaceted policies through a novel synthesis of principle-based automated\nreward scoring, Group Relative Policy Optimisation (GRPO), and an Open Policy\nAgent (OPA) inspired governance layer. This approach provides the technical\nfoundation for achieving and demonstrating compliance with diverse and nuanced\ngovernance requirements. To showcase the framework's capability to\noperationalize a deeply nuanced and culturally-specific value system, we\npresent an in-depth case study: the development of a medical AI assistant\nguided by principles from Dharmic ethics (such as Ahimsa and Dharma), as\nderived from texts like the Bhagavad Gita. This challenging application\ndemonstrates ArGen's adaptability, achieving a 70.9% improvement in\ndomain-scope adherence over the baseline. Through our open-source repository,\nwe show that ArGen's methodology offers a path to 'Governable Al' systems that\nare technically proficient, ethically robust, and verifiably compliant for safe\ndeployment in diverse global contexts."}
{"id": "2509.07017", "pdf": "https://arxiv.org/pdf/2509.07017.pdf", "abs": "https://arxiv.org/abs/2509.07017", "title": "From Eigenmodes to Proofs: Integrating Graph Spectral Operators with Symbolic Interpretable Reasoning", "authors": ["Andrew Kiruluta", "Priscilla Burity"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "We introduce Spectral NSR, a fully spectral neuro-symbolic reasoning\nframework that embeds logical rules as spectral templates and performs\ninference directly in the graph spectral domain. By leveraging graph signal\nprocessing (GSP) and frequency-selective filters grounded in the Laplacian\neigenstructure of knowledge graphs, the architecture unifies the\ninterpretability of symbolic reasoning with the scalability and adaptability of\nspectral learning. Beyond the core formulation, we incorporate a comprehensive\nset of extensions, including dynamic graph and basis learning, rational and\ndiffusion filters for sharper spectral selectivity, mixture-of-spectral-experts\nfor modular specialization, proof-guided training with spectral curricula, and\nuncertainty quantification for calibrated confidence. Additional enhancements\nsuch as large language model coupling, co-spectral transfer alignment,\nadversarial robustness, efficient GPU kernels, generalized Laplacians, and\ncausal interventions further expand the versatility of the framework.\n  Empirical evaluation on state-of-the-art reasoning benchmarks such as\nProofWriter and CLUTRR demonstrates that Spectral NSR achieves superior\naccuracy, faster inference, improved robustness to adversarial perturbations,\nand higher interpretability compared to leading baselines including\ntransformers, message-passing neural networks, and neuro-symbolic logic\nprogramming systems. Spectral attribution and proof-band agreement analyses\nconfirm that model decisions align closely with symbolic proof structures,\nwhile transfer experiments validate effective domain adaptation through\nco-spectral alignment. These results establish Spectral NSR as a scalable and\nprincipled foundation for the next generation of reasoning systems, offering\ntransparency, robustness, and generalization beyond conventional approaches."}
{"id": "2509.07098", "pdf": "https://arxiv.org/pdf/2509.07098.pdf", "abs": "https://arxiv.org/abs/2509.07098", "title": "Instruction Agent: Enhancing Agent with Expert Demonstration", "authors": ["Yinheng Li", "Hailey Hultquist", "Justin Wagle", "Kazuhito Koishida"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Graphical user interface (GUI) agents have advanced rapidly but still\nstruggle with complex tasks involving novel UI elements, long-horizon actions,\nand personalized trajectories. In this work, we introduce Instruction Agent, a\nGUI agent that leverages expert demonstrations to solve such tasks, enabling\ncompletion of otherwise difficult workflows. Given a single demonstration, the\nagent extracts step-by-step instructions and executes them by strictly\nfollowing the trajectory intended by the user, which avoids making mistakes\nduring execution. The agent leverages the verifier and backtracker modules\nfurther to improve robustness. Both modules are critical to understand the\ncurrent outcome from each action and handle unexpected interruptions(such as\npop-up windows) during execution. Our experiments show that Instruction Agent\nachieves a 60% success rate on a set of tasks in OSWorld that all top-ranked\nagents failed to complete. The Instruction Agent offers a practical and\nextensible framework, bridging the gap between current GUI agents and reliable\nreal-world GUI task automation."}
{"id": "2509.07122", "pdf": "https://arxiv.org/pdf/2509.07122.pdf", "abs": "https://arxiv.org/abs/2509.07122", "title": "Neuro-Symbolic Frameworks: Conceptual Characterization and Empirical Comparative Analysis", "authors": ["Sania Sinha", "Tanawan Premsri", "Danial Kamali", "Parisa Kordjamshidi"], "categories": ["cs.AI", "cs.CL", "cs.SC"], "comment": null, "summary": "Neurosymbolic (NeSy) frameworks combine neural representations and learning\nwith symbolic representations and reasoning. Combining the reasoning\ncapacities, explainability, and interpretability of symbolic processing with\nthe flexibility and power of neural computing allows us to solve complex\nproblems with more reliability while being data-efficient. However, this\nrecently growing topic poses a challenge to developers with its learning curve,\nlack of user-friendly tools, libraries, and unifying frameworks. In this paper,\nwe characterize the technical facets of existing NeSy frameworks, such as the\nsymbolic representation language, integration with neural models, and the\nunderlying algorithms. A majority of the NeSy research focuses on algorithms\ninstead of providing generic frameworks for declarative problem specification\nto leverage problem solving. To highlight the key aspects of Neurosymbolic\nmodeling, we showcase three generic NeSy frameworks - \\textit{DeepProbLog},\n\\textit{Scallop}, and \\textit{DomiKnowS}. We identify the challenges within\neach facet that lay the foundation for identifying the expressivity of each\nframework in solving a variety of problems. Building on this foundation, we aim\nto spark transformative action and encourage the community to rethink this\nproblem in novel ways."}
{"id": "2509.07149", "pdf": "https://arxiv.org/pdf/2509.07149.pdf", "abs": "https://arxiv.org/abs/2509.07149", "title": "Measuring Uncertainty in Transformer Circuits with Effective Information Consistency", "authors": ["Anatoly A. Krasnovsky"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IT", "math.IT"], "comment": null, "summary": "Mechanistic interpretability has identified functional subgraphs within large\nlanguage models (LLMs), known as Transformer Circuits (TCs), that appear to\nimplement specific algorithms. Yet we lack a formal, single-pass way to\nquantify when an active circuit is behaving coherently and thus likely\ntrustworthy. Building on prior systems-theoretic proposals, we specialize a\nsheaf/cohomology and causal emergence perspective to TCs and introduce the\nEffective-Information Consistency Score (EICS). EICS combines (i) a normalized\nsheaf inconsistency computed from local Jacobians and activations, with (ii) a\nGaussian EI proxy for circuit-level causal emergence derived from the same\nforward state. The construction is white-box, single-pass, and makes units\nexplicit so that the score is dimensionless. We further provide practical\nguidance on score interpretation, computational overhead (with fast and exact\nmodes), and a toy sanity-check analysis. Empirical validation on LLM tasks is\ndeferred."}
{"id": "2509.07163", "pdf": "https://arxiv.org/pdf/2509.07163.pdf", "abs": "https://arxiv.org/abs/2509.07163", "title": "Beyond Sequential Reranking: Reranker-Guided Search Improves Reasoning Intensive Retrieval", "authors": ["Haike Xu", "Tong Chen"], "categories": ["cs.IR", "cs.CL", "cs.LG"], "comment": null, "summary": "The widely used retrieve-and-rerank pipeline faces two critical limitations:\nthey are constrained by the initial retrieval quality of the top-k documents,\nand the growing computational demands of LLM-based rerankers restrict the\nnumber of documents that can be effectively processed. We introduce\nReranker-Guided-Search (RGS), a novel approach that bypasses these limitations\nby directly retrieving documents according to reranker preferences rather than\nfollowing the traditional sequential reranking method. Our method uses a greedy\nsearch on proximity graphs generated by approximate nearest neighbor\nalgorithms, strategically prioritizing promising documents for reranking based\non document similarity. Experimental results demonstrate substantial\nperformance improvements across multiple benchmarks: 3.5 points on BRIGHT, 2.9\non FollowIR, and 5.1 on M-BEIR, all within a constrained reranker budget of 100\ndocuments. Our analysis suggests that, given a fixed pair of embedding and\nreranker models, strategically selecting documents to rerank can significantly\nimprove retrieval accuracy under limited reranker budget."}
{"id": "2509.07170", "pdf": "https://arxiv.org/pdf/2509.07170.pdf", "abs": "https://arxiv.org/abs/2509.07170", "title": "That's So FETCH: Fashioning Ensemble Techniques for LLM Classification in Civil Legal Intake and Referral", "authors": ["Quinten Steenhuis"], "categories": ["cs.AI", "cs.CL", "cs.CY"], "comment": "Submission to JURIX 2025", "summary": "Each year millions of people seek help for their legal problems by calling a\nlegal aid program hotline, walking into a legal aid office, or using a lawyer\nreferral service. The first step to match them to the right help is to identify\nthe legal problem the applicant is experiencing. Misdirection has consequences.\nApplicants may miss a deadline, experience physical abuse, lose housing or lose\ncustody of children while waiting to connect to the right legal help. We\nintroduce and evaluate the FETCH classifier for legal issue classification and\ndescribe two methods for improving accuracy: a hybrid LLM/ML ensemble\nclassification method, and the automatic generation of follow-up questions to\nenrich the initial problem narrative. We employ a novel data set of 419\nreal-world queries to a nonprofit lawyer referral service. Ultimately, we show\nclassification accuracy (hits@2) of 97.37\\% using a mix of inexpensive models,\nexceeding the performance of the current state-of-the-art GPT-5 model. Our\napproach shows promise in significantly reducing the cost of guiding users of\nthe legal system to the right resource for their problem while achieving high\naccuracy."}
{"id": "2509.07202", "pdf": "https://arxiv.org/pdf/2509.07202.pdf", "abs": "https://arxiv.org/abs/2509.07202", "title": "Neurocognitive Modeling for Text Generation: Deep Learning Architecture for EEG Data", "authors": ["Khushiyant"], "categories": ["cs.HC", "cs.CL", "I.2.7; I.2.6; J.3"], "comment": "15 pages, 10 figures, 5 tables", "summary": "Text generating capabilities have undergone a substantial transformation with\nthe introduction of large language models (LLMs). Electroencephalography\n(EEG)-based text production is still difficult, though, because it requires a\nlot of data and processing power. This paper introduces a new method that\ncombines the use of the Gemma 2B LLM with a classifier-LLM architecture to\nincorporate a Recurrent Neural Network (RNN) encoder. Our approach drastically\nlowers the amount of data and compute power needed while achieving performance\nclose to that of cutting-edge methods. Notably, compared to current\nmethodologies, our methodology delivers an overall performance improvement of\n10%. The suggested architecture demonstrates the possibility of effective\ntransfer learning for EEG-based text production, remaining strong and\nfunctional even in the face of data limits. This work highlights the potential\nof integrating LLMs with EEG decoding to improve assistive technologies and\nimprove independence and communication for those with severe motor limitations.\nOur method pushes the limits of present capabilities and opens new paths for\nresearch and application in brain-computer interfaces by efficiently using the\nstrengths of pre-trained language models. This makes EEG-based text production\nmore accessible and efficient."}
{"id": "2509.07253", "pdf": "https://arxiv.org/pdf/2509.07253.pdf", "abs": "https://arxiv.org/abs/2509.07253", "title": "Benchmarking Information Retrieval Models on Complex Retrieval Tasks", "authors": ["Julian Killingback", "Hamed Zamani"], "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) are incredible and versatile tools for\ntext-based tasks that have enabled countless, previously unimaginable,\napplications. Retrieval models, in contrast, have not yet seen such capable\ngeneral-purpose models emerge. To achieve this goal, retrieval models must be\nable to perform complex retrieval tasks, where queries contain multiple parts,\nconstraints, or requirements in natural language. These tasks represent a\nnatural progression from the simple, single-aspect queries that are used in the\nvast majority of existing, commonly used evaluation sets. Complex queries\nnaturally arise as people expect search systems to handle more specific and\noften ambitious information requests, as is demonstrated by how people use\nLLM-based information systems. Despite the growing desire for retrieval models\nto expand their capabilities in complex retrieval tasks, there exist limited\nresources to assess the ability of retrieval models on a comprehensive set of\ndiverse complex tasks. The few resources that do exist feature a limited scope\nand often lack realistic settings making it hard to know the true capabilities\nof retrieval models on complex real-world retrieval tasks. To address this\nshortcoming and spur innovation in next-generation retrieval models, we\nconstruct a diverse and realistic set of complex retrieval tasks and benchmark\na representative set of state-of-the-art retrieval models. Additionally, we\nexplore the impact of LLM-based query expansion and rewriting on retrieval\nquality. Our results show that even the best models struggle to produce\nhigh-quality retrieval results with the highest average nDCG@10 of only 0.346\nand R@100 of only 0.587 across all tasks. Although LLM augmentation can help\nweaker models, the strongest model has decreased performance across all metrics\nwith all rewriting techniques."}
{"id": "2509.07282", "pdf": "https://arxiv.org/pdf/2509.07282.pdf", "abs": "https://arxiv.org/abs/2509.07282", "title": "ALICE: An Interpretable Neural Architecture for Generalization in Substitution Ciphers", "authors": ["Jeff Shen", "Lindsay Smith"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "comment": "Preprint. Project page at https://jshen.net/alice", "summary": "We present cryptogram solving as an ideal testbed for studying neural network\ngeneralization in combinatorially complex domains. In this task, models must\ndecrypt text encoded with substitution ciphers, choosing from 26! possible\nmappings without explicit access to the cipher. We develop ALICE (an\nArchitecture for Learning Interpretable Cryptogram dEcipherment): a simple\nencoder-only Transformer that sets a new state-of-the-art for both accuracy and\nspeed on this decryption problem. Surprisingly, ALICE generalizes to unseen\nciphers after training on only ${\\sim}1500$ unique ciphers, a minute fraction\n($3.7 \\times 10^{-24}$) of the possible cipher space. To enhance\ninterpretability, we introduce a novel bijective decoding head that explicitly\nmodels permutations via the Gumbel-Sinkhorn method, enabling direct extraction\nof learned cipher mappings. Through early exit analysis, we reveal how ALICE\nprogressively refines its predictions in a way that appears to mirror common\nhuman strategies for this task: early layers employ frequency-based heuristics,\nmiddle layers form word structures, and final layers correct individual\ncharacters. Our architectural innovations and analysis methods extend beyond\ncryptograms to any domain with bijective mappings and combinatorial structure,\noffering new insights into neural network generalization and interpretability."}
{"id": "2509.07414", "pdf": "https://arxiv.org/pdf/2509.07414.pdf", "abs": "https://arxiv.org/abs/2509.07414", "title": "Language Self-Play For Data-Free Training", "authors": ["Jakub Grudzien Kuba", "Mengting Gu", "Qi Ma", "Yuandong Tian", "Vijai Mohan"], "categories": ["cs.AI", "cs.CL", "cs.GT"], "comment": null, "summary": "Large language models (LLMs) have advanced rapidly in recent years, driven by\nscale, abundant high-quality training data, and reinforcement learning. Yet\nthis progress faces a fundamental bottleneck: the need for ever more data from\nwhich models can continue to learn. In this work, we propose a reinforcement\nlearning approach that removes this dependency by enabling models to improve\nwithout additional data. Our method leverages a game-theoretic framework of\nself-play, where a model's capabilities are cast as performance in a\ncompetitive game and stronger policies emerge by having the model play against\nitself - a process we call Language Self-Play (LSP). Experiments with\nLlama-3.2-3B-Instruct on instruction-following benchmarks show that pretrained\nmodels can not only enhance their performance on challenging tasks through\nself-play alone, but can also do so more effectively than data-driven\nbaselines."}
{"id": "2509.07450", "pdf": "https://arxiv.org/pdf/2509.07450.pdf", "abs": "https://arxiv.org/abs/2509.07450", "title": "GLEAM: Learning to Match and Explain in Cross-View Geo-Localization", "authors": ["Xudong Lu", "Zhi Zheng", "Yi Wan", "Yongxiang Yao", "Annan Wang", "Renrui Zhang", "Panwang Xia", "Qiong Wu", "Qingyun Li", "Weifeng Lin", "Xiangyu Zhao", "Xue Yang", "Hongsheng Li"], "categories": ["cs.CV", "cs.CL"], "comment": "18 pages", "summary": "Cross-View Geo-Localization (CVGL) focuses on identifying correspondences\nbetween images captured from distinct perspectives of the same geographical\nlocation. However, existing CVGL approaches are typically restricted to a\nsingle view or modality, and their direct visual matching strategy lacks\ninterpretability: they merely predict whether two images correspond, without\nexplaining the rationale behind the match. In this paper, we present GLEAM-C, a\nfoundational CVGL model that unifies multiple views and modalities-including\nUAV imagery, street maps, panoramic views, and ground photographs-by aligning\nthem exclusively with satellite imagery. Our framework enhances training\nefficiency through optimized implementation while achieving accuracy comparable\nto prior modality-specific CVGL models through a two-phase training strategy.\nMoreover, to address the lack of interpretability in traditional CVGL methods,\nwe leverage the reasoning capabilities of multimodal large language models\n(MLLMs) to propose a new task, GLEAM-X, which combines cross-view\ncorrespondence prediction with explainable reasoning. To support this task, we\nconstruct a bilingual benchmark using GPT-4o and Doubao-1.5-Thinking-Vision-Pro\nto generate training and testing data. The test set is further refined through\ndetailed human revision, enabling systematic evaluation of explainable\ncross-view reasoning and advancing transparency and scalability in\ngeo-localization. Together, GLEAM-C and GLEAM-X form a comprehensive CVGL\npipeline that integrates multi-modal, multi-view alignment with interpretable\ncorrespondence analysis, unifying accurate cross-view matching with explainable\nreasoning and advancing Geo-Localization by enabling models to better Explain\nAnd Match. Code and datasets used in this work will be made publicly accessible\nat https://github.com/Lucky-Lance/GLEAM."}
{"id": "2509.07506", "pdf": "https://arxiv.org/pdf/2509.07506.pdf", "abs": "https://arxiv.org/abs/2509.07506", "title": "Astra: A Multi-Agent System for GPU Kernel Performance Optimization", "authors": ["Anjiang Wei", "Tianran Sun", "Yogesh Seenichamy", "Hang Song", "Anne Ouyang", "Azalia Mirhoseini", "Ke Wang", "Alex Aiken"], "categories": ["cs.DC", "cs.AI", "cs.CL", "cs.LG", "cs.SE"], "comment": null, "summary": "GPU kernel optimization has long been a central challenge at the intersection\nof high-performance computing and machine learning. Efficient kernels are\ncrucial for accelerating large language model (LLM) training and serving, yet\nattaining high performance typically requires extensive manual tuning.\nCompiler-based systems reduce some of this burden, but still demand substantial\nmanual design and engineering effort. Recently, researchers have explored using\nLLMs for GPU kernel generation, though prior work has largely focused on\ntranslating high-level PyTorch modules into CUDA code. In this work, we\nintroduce Astra, the first LLM-based multi-agent system for GPU kernel\noptimization. Unlike previous approaches, Astra starts from existing CUDA\nimplementations extracted from SGLang, a widely deployed framework for serving\nLLMs, rather than treating PyTorch modules as the specification. Within Astra,\nspecialized LLM agents collaborate through iterative code generation, testing,\nprofiling, and planning to produce kernels that are both correct and\nhigh-performance. On kernels from SGLang, Astra achieves an average speedup of\n1.32x using zero-shot prompting with OpenAI o4-mini. A detailed case study\nfurther demonstrates that LLMs can autonomously apply loop transformations,\noptimize memory access patterns, exploit CUDA intrinsics, and leverage fast\nmath operations to yield substantial performance gains. Our work highlights\nmulti-agent LLM systems as a promising new paradigm for GPU kernel\noptimization."}
{"id": "2509.07526", "pdf": "https://arxiv.org/pdf/2509.07526.pdf", "abs": "https://arxiv.org/abs/2509.07526", "title": "Competitive Audio-Language Models with Data-Efficient Single-Stage Training on Public Data", "authors": ["Gokul Karthik Kumar", "Rishabh Saraf", "Ludovick Lepauloux", "Abdul Muneer", "Billel Mokeddem", "Hakim Hacid"], "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted at ASRU 2025", "summary": "Large language models (LLMs) have transformed NLP, yet their integration with\naudio remains underexplored -- despite audio's centrality to human\ncommunication. We introduce Falcon3-Audio, a family of Audio-Language Models\n(ALMs) built on instruction-tuned LLMs and Whisper encoders. Using a remarkably\nsmall amount of public audio data -- less than 30K hours (5K unique) --\nFalcon3-Audio-7B matches the best reported performance among open-weight models\non the MMAU benchmark, with a score of 64.14, matching R1-AQA, while\ndistinguishing itself through superior data and parameter efficiency,\nsingle-stage training, and transparency. Notably, our smallest 1B model remains\ncompetitive with larger open models ranging from 2B to 13B parameters. Through\nextensive ablations, we find that common complexities -- such as curriculum\nlearning, multiple audio encoders, and intricate cross-attention connectors --\nare not required for strong performance, even compared to models trained on\nover 500K hours of data."}
{"id": "2509.07909", "pdf": "https://arxiv.org/pdf/2509.07909.pdf", "abs": "https://arxiv.org/abs/2509.07909", "title": "Uncovering Scaling Laws for Large Language Models via Inverse Problems", "authors": ["Arun Verma", "Zhaoxuan Wu", "Zijian Zhou", "Xiaoqiang Lin", "Zhiliang Chen", "Rachael Hwee Ling Sim", "Rui Qiao", "Jingtan Wang", "Nhung Bui", "Xinyuan Niu", "Wenyang Hu", "Gregory Kang Ruey Lau", "Zi-Yu Khoo", "Zitong Zhao", "Xinyi Xu", "Apivich Hemachandra", "See-Kiong Ng", "Bryan Kian Hsiang Low"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted at EMNLP Findings 2025", "summary": "Large Language Models (LLMs) are large-scale pretrained models that have\nachieved remarkable success across diverse domains. These successes have been\ndriven by unprecedented complexity and scale in both data and computations.\nHowever, due to the high costs of training such models, brute-force\ntrial-and-error approaches to improve LLMs are not feasible. Inspired by the\nsuccess of inverse problems in uncovering fundamental scientific laws, this\nposition paper advocates that inverse problems can also efficiently uncover\nscaling laws that guide the building of LLMs to achieve the desirable\nperformance with significantly better cost-effectiveness."}
{"id": "2509.07966", "pdf": "https://arxiv.org/pdf/2509.07966.pdf", "abs": "https://arxiv.org/abs/2509.07966", "title": "Visual-TableQA: Open-Domain Benchmark for Reasoning over Table Images", "authors": ["Boammani Aser Lompo", "Marc Haraoui"], "categories": ["cs.CV", "cs.CL"], "comment": "Work in Progress", "summary": "Visual reasoning over structured data such as tables is a critical capability\nfor modern vision-language models (VLMs), yet current benchmarks remain limited\nin scale, diversity, or reasoning depth, especially when it comes to rendered\ntable images. Addressing this gap, we introduce Visual-TableQA, a large-scale,\nopen-domain multimodal dataset specifically designed to evaluate and enhance\nvisual reasoning over complex tabular data. Our generation pipeline is modular,\nscalable, and fully autonomous, involving multiple reasoning LLMs collaborating\nacross distinct roles: generation, validation, and inspiration. Visual-TableQA\ncomprises 2.5k richly structured LaTeX-rendered tables and 6k\nreasoning-intensive QA pairs, all produced at a cost of under USD 100. To\npromote diversity and creativity, our pipeline performs multi-model\ncollaborative data generation via cross-model prompting ('inspiration') and\nLLM-jury filtering. Stronger models seed layouts and topics that weaker models\nelaborate, collectively distilling diverse reasoning patterns and visual\nstructures into the dataset. Empirical results show that models fine-tuned on\nVisual-TableQA generalize robustly to external benchmarks, outperforming\nseveral proprietary models despite the dataset's synthetic nature. The full\npipeline and resources are publicly available at\nhttps://github.com/AI-4-Everyone/Visual-TableQA."}
{"id": "2509.07969", "pdf": "https://arxiv.org/pdf/2509.07969.pdf", "abs": "https://arxiv.org/abs/2509.07969", "title": "Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search", "authors": ["Xin Lai", "Junyi Li", "Wei Li", "Tao Liu", "Tianjian Li", "Hengshuang Zhao"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Code, datasets, models are available at\n  https://github.com/Mini-o3/Mini-o3. Project Page: https://mini-o3.github.io/", "summary": "Recent advances in large multimodal models have leveraged image-based tools\nwith reinforcement learning to tackle visual problems. However, existing\nopen-source approaches often exhibit monotonous reasoning patterns and allow\nonly a limited number of interaction turns, making them inadequate for\ndifficult tasks that require trial-and-error exploration. In this work, we\naddress this limitation by scaling up tool-based interactions and introduce\nMini-o3, a system that executes deep, multi-turn reasoning -- spanning tens of\nsteps -- and achieves state-of-the-art performance on challenging visual search\ntasks. Our recipe for reproducing OpenAI o3-style behaviors comprises three key\ncomponents. First, we construct the Visual Probe Dataset, a collection of\nthousands of challenging visual search problems designed for exploratory\nreasoning. Second, we develop an iterative data collection pipeline to obtain\ncold-start trajectories that exhibit diverse reasoning patterns, including\ndepth-first search, trial-and-error, and goal maintenance. Third, we propose an\nover-turn masking strategy that prevents penalization of over-turn responses\n(those that hit the maximum number of turns) during reinforcement learning,\nthereby balancing training-time efficiency with test-time scalability. Despite\ntraining with an upper bound of only six interaction turns, our model generates\ntrajectories that naturally scale to tens of turns at inference time, with\naccuracy improving as the number of turns increases. Extensive experiments\ndemonstrate that Mini-o3 produces rich reasoning patterns and deep thinking\npaths, effectively solving challenging visual search problems."}
{"id": "2310.16582", "pdf": "https://arxiv.org/pdf/2310.16582.pdf", "abs": "https://arxiv.org/abs/2310.16582", "title": "UPLex: Fine-Grained Personality Control in Large Language Models via Unsupervised Lexical Modulation", "authors": ["Tianlong Li", "Wenhao Liu", "Muling Wu", "Shihan Dou", "Zhenghua Wang", "Changze Lv", "Xiaohua Wang", "Xiaoqing Zheng", "Xuanjing Huang"], "categories": ["cs.CL"], "comment": "EMNLP 2025 Findings", "summary": "Personality is a crucial factor that shapes human communication patterns,\nthereby regulating the personalities of large language models (LLMs) holds\nsignificant potential in enhancing their user experiences. Previous approaches\neither relied on fine-tuning LLMs on specific corpora or required manually\ncrafted prompts to evoke specific personalities from LLMs. However, the former\nis inefficient and costly, while the latter cannot precisely manipulate\npersonality traits at a fine-grained level. To address these challenges, we\npropose UPLex, a method that uses an Unsupervisedly-Built Personalized Lexicon\n(UPL) during the decoding phase to manipulate LLM's personality traits. UPL can\nbe constructed from a newly built situational judgment test dataset in an\nunsupervised fashion, and used to modulate the personality expression of LLMs\nby dynamically altering their predicted probability of upcoming words in a\npluggable fashion. Extensive experimentation demonstrates the remarkable\neffectiveness and pluggability of our method for fine-grained manipulation of\nLLMs' personalities."}
{"id": "2405.15454", "pdf": "https://arxiv.org/pdf/2405.15454.pdf", "abs": "https://arxiv.org/abs/2405.15454", "title": "Linearly Controlled Language Generation with Performative Guarantees", "authors": ["Emily Cheng", "Carmen Amo Alonso"], "categories": ["cs.CL", "cs.SY", "eess.SY"], "comment": "Under review", "summary": "The increasing prevalence of Large Language Models (LMs) in critical\napplications highlights the need for controlled language generation strategies\nthat are not only computationally efficient but that also enjoy performance\nguarantees. To achieve this, we use a common model of concept semantics as\nlinearly represented in an LM's latent space. In particular, we take the view\nthat natural language generation traces a trajectory in this continuous\nsemantic space, realized by the language model's hidden activations. This view\npermits a control-theoretic treatment of text generation in latent space, in\nwhich we propose a lightweight, gradient-free intervention that dynamically\nsteers trajectories away from regions corresponding to undesired meanings. In\nparticular, we propose to directly intervene the activations of the token that\nis being generated in embedding space in an online fashion. Crucially, we do\nnot simply steer activations towards a desirable region. Instead, our method\nrelies on classical techniques from control theory to precisely control\nactivations in a context-dependent way, and guarantees that they are brought\ninto a specific pre-defined region of embedding space that corresponds to\nallowed semantics. Our intervention is computed in closed-form according to an\noptimal controller formulation, minimally impacting generation time. This\ncontrol of the activations in embedding space allows for fine-grained steering\nof attributes of the generated sequence. We demonstrate the effectiveness of\nour approach on different objectives -- toxicity avoidance and sentiment\ncontrol -- while maintaining text quality."}
{"id": "2405.20404", "pdf": "https://arxiv.org/pdf/2405.20404.pdf", "abs": "https://arxiv.org/abs/2405.20404", "title": "JoPA:Explaining Large Language Model's Generation via Joint Prompt Attribution", "authors": ["Yurui Chang", "Bochuan Cao", "Yujia Wang", "Jinghui Chen", "Lu Lin"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to ACL 2025 (Main)", "summary": "Large Language Models (LLMs) have demonstrated impressive performances in\ncomplex text generation tasks. However, the contribution of the input prompt to\nthe generated content still remains obscure to humans, underscoring the\nnecessity of understanding the causality between input and output pairs.\nExisting works for providing prompt-specific explanation often confine model\noutput to be classification or next-word prediction. Few initial attempts\naiming to explain the entire language generation often treat input prompt texts\nindependently, ignoring their combinatorial effects on the follow-up\ngeneration. In this study, we introduce a counterfactual explanation framework\nbased on Joint Prompt Attribution, JoPA, which aims to explain how a few prompt\ntexts collaboratively influences the LLM's complete generation. Particularly,\nwe formulate the task of prompt attribution for generation interpretation as a\ncombinatorial optimization problem, and introduce a probabilistic algorithm to\nsearch for the casual input combination in the discrete space. We define and\nutilize multiple metrics to evaluate the produced explanations, demonstrating\nboth the faithfulness and efficiency of our framework."}
{"id": "2407.12791", "pdf": "https://arxiv.org/pdf/2407.12791.pdf", "abs": "https://arxiv.org/abs/2407.12791", "title": "CTourLLM: Enhancing LLMs with Chinese Tourism Knowledge", "authors": ["Qikai Wei", "Mingzhi Yang", "Jinqiang Wang", "Wenwei Mao", "Jiabo Xu", "Huansheng Ning"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recently, large language models (LLMs) have demonstrated their effectiveness\nin various natural language processing (NLP) tasks. However, the lack of\ntourism knowledge limits the performance of LLMs in tourist attraction\npresentations and travel planning. To address this challenge, we constructed a\nsupervised fine-tuning dataset for the Chinese culture and tourism domain,\nnamed Cultour. This dataset consists of three parts: tourism knowledge base\ndata, travelogues data, and tourism QA data. Additionally, we propose CTourLLM,\na Qwen-based model supervised fine-tuned with Cultour, to improve the quality\nof information about attractions and travel planning. To evaluate the\nperformance of CTourLLM, we proposed a human evaluation criterion named RRA\n(Relevance, Readability, Availability), and employed both automatic and human\nevaluation. The experimental results demonstrate that CTourLLM outperforms\nChatGPT, achieving an improvement of 1.21 in BLEU-1 and 1.54 in Rouge-L,\nthereby validating the effectiveness of the response outcomes. Our proposed\nCultour is accessible at https://github.com/mrweiqk/Cultour."}
{"id": "2411.02886", "pdf": "https://arxiv.org/pdf/2411.02886.pdf", "abs": "https://arxiv.org/abs/2411.02886", "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation for LLMs via Dynamic Token-Level KV Cache Selection", "authors": ["Wei Wu", "Zhuoshi Pan", "Chao Wang", "Liyi Chen", "Yunchu Bai", "Tianfu Wang", "Kun Fu", "Zheng Wang", "Hui Xiong"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by EMNLP2025", "summary": "Rapid advances in Large Language Models (LLMs) have spurred demand for\nprocessing extended context sequences in contemporary applications. However,\nthis progress faces two challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues limit LLMs in\nlong-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache\nSelection (TokenSelect), a training-free method for efficient and accurate\nlong-context inference. TokenSelect builds upon the observation of\nnon-contiguous attention sparsity, using QK dot products to measure per-head KV\nCache criticality at token-level. By per-head soft voting mechanism,\nTokenSelect selectively involves a few critical KV cache tokens in attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesign the Selection Cache based on observations of consecutive Query\nsimilarity and implemented the efficient Paged Dot Product Kernel,\nsignificantly reducing the selection overhead. A comprehensive evaluation of\nTokenSelect demonstrates up to $23.84\\times$ speedup in attention computation\nand up to $2.28\\times$ acceleration in end-to-end latency, while providing\nsuperior performance compared to state-of-the-art long-context inference\nmethods."}
{"id": "2412.18934", "pdf": "https://arxiv.org/pdf/2412.18934.pdf", "abs": "https://arxiv.org/abs/2412.18934", "title": "Dovetail: A CPU/GPU Heterogeneous Speculative Decoding for LLM inference", "authors": ["Libo Zhang", "Zhaoning Zhang", "Baizhou Xu", "Rui Li", "Zhiliang Tian", "Songzhu Mei", "Dongsheng Li"], "categories": ["cs.CL"], "comment": "14 pages, 6 figures", "summary": "With the continuous advancement in the performance of large language models\n(LLMs), their demand for computational resources and memory has significantly\nincreased, which poses major challenges for efficient inference on\nconsumer-grade devices and legacy servers. These devices typically feature\nrelatively weaker GPUs and stronger CPUs. Although techniques such as parameter\noffloading and partial offloading can alleviate GPU memory pressure to some\nextent, their effectiveness is limited due to communication latency and\nsuboptimal hardware resource utilization. To address this issue, we propose\nDovetail, a lossless inference acceleration method that leverages the\ncomplementary characteristics of heterogeneous devices and the advantages of\nspeculative decoding. Dovetail deploys a draft model on the GPU to perform\npreliminary predictions, while a target model running on the CPU validates\nthese outputs. By reducing the granularity of data transfer, Dovetail\nsignificantly minimizes communication overhead. To further improve efficiency,\nwe optimize the draft model specifically for heterogeneous hardware\nenvironments by reducing the number of draft tokens to lower parallel\nverification latency, increasing model depth to enhance predictive\ncapabilities, and introducing a Dynamic Gating Fusion (DGF) mechanism to\nimprove the integration of feature and embedding information. We conduct\ncomprehensive evaluations of Dovetail across various consumer-grade GPUs,\ncovering multiple tasks and mainstream models. Experimental results on 13B\nmodels demonstrate that Dovetail achieves inference speedups ranging from 1.79x\nto 10.1x across different devices, while maintaining consistency and stability\nin the distribution of generated texts."}
{"id": "2502.07128", "pdf": "https://arxiv.org/pdf/2502.07128.pdf", "abs": "https://arxiv.org/abs/2502.07128", "title": "Cardiverse: Harnessing LLMs for Novel Card Game Prototyping", "authors": ["Danrui Li", "Sen Zhang", "Sam S. Sohn", "Kaidong Hu", "Muhammad Usman", "Mubbasir Kapadia"], "categories": ["cs.CL", "cs.AI", "cs.MM"], "comment": "37 pages, 13 figures, 8 tables. Accepted by EMNLP 2025", "summary": "The prototyping of computer games, particularly card games, requires\nextensive human effort in creative ideation and gameplay evaluation. Recent\nadvances in Large Language Models (LLMs) offer opportunities to automate and\nstreamline these processes. However, it remains challenging for LLMs to design\nnovel game mechanics beyond existing databases, generate consistent gameplay\nenvironments, and develop scalable gameplay AI for large-scale evaluations.\nThis paper addresses these challenges by introducing a comprehensive automated\ncard game prototyping framework. The approach highlights a graph-based indexing\nmethod for generating novel game variations, an LLM-driven system for\nconsistent game code generation validated by gameplay records, and a gameplay\nAI constructing method that uses an ensemble of LLM-generated heuristic\nfunctions optimized through self-play. These contributions aim to accelerate\ncard game prototyping, reduce human labor, and lower barriers to entry for game\ndevelopers. For code repo visit this http URL\nhttps://github.com/danruili/Cardiverse"}
{"id": "2502.07322", "pdf": "https://arxiv.org/pdf/2502.07322.pdf", "abs": "https://arxiv.org/abs/2502.07322", "title": "MEMIT-Merge: Addressing MEMIT's Key-Value Conflicts in Same-Subject Batch Editing for LLMs", "authors": ["Zilu Dong", "Xiangqing Shen", "Rui Xia"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted by ACL2025 findings", "summary": "As large language models continue to scale up, knowledge editing techniques\nthat modify models' internal knowledge without full retraining have gained\nsignificant attention. MEMIT, a prominent batch editing algorithm, stands out\nfor its capability to perform mass knowledge modifications. However, we uncover\nthat MEMIT's editing efficacy significantly deteriorates when processing\nbatches containing multiple edits sharing the same subject. Our analysis\nreveals this stems from MEMIT's key value modeling framework: identical keys\n(derived from the shared subject) are forced to represent different values\n(corresponding to different knowledge), resulting in update conflicts during\nediting. Addressing this issue, we propose MEMIT-Merge, an enhanced approach\nthat merges value computation processes for facts sharing the same subject,\neffectively resolving the performance degradation in samesubject batch editing\nscenarios. Experimental results demonstrate that when MEMIT's edit success rate\ndrops to around 50% at larger batch sizes, MEMIT-Merge maintains a success rate\nexceeding 90%, showcasing remarkable robustness to subject entity collisions.\nThe code is available at https://github.com/NUSTM/ MEMIT-Merge."}
{"id": "2502.11824", "pdf": "https://arxiv.org/pdf/2502.11824.pdf", "abs": "https://arxiv.org/abs/2502.11824", "title": "M-ABSA: A Multilingual Dataset for Aspect-Based Sentiment Analysis", "authors": ["Chengyan Wu", "Bolei Ma", "Yihong Liu", "Zheyu Zhang", "Ningyuan Deng", "Yanshu Li", "Baolan Chen", "Yi Zhang", "Yun Xue", "Barbara Plank"], "categories": ["cs.CL"], "comment": "EMNLP 2025", "summary": "Aspect-based sentiment analysis (ABSA) is a crucial task in information\nextraction and sentiment analysis, aiming to identify aspects with associated\nsentiment elements in text. However, existing ABSA datasets are predominantly\nEnglish-centric, limiting the scope for multilingual evaluation and research.\nTo bridge this gap, we present M-ABSA, a comprehensive dataset spanning 7\ndomains and 21 languages, making it the most extensive multilingual parallel\ndataset for ABSA to date. Our primary focus is on triplet extraction, which\ninvolves identifying aspect terms, aspect categories, and sentiment polarities.\nThe dataset is constructed through an automatic translation process with human\nreview to ensure quality. We perform extensive experiments using various\nbaselines to assess performance and compatibility on M-ABSA. Our empirical\nfindings highlight that the dataset enables diverse evaluation tasks, such as\nmultilingual and multi-domain transfer learning, and large language model\nevaluation, underscoring its inclusivity and its potential to drive\nadvancements in multilingual ABSA research."}
{"id": "2502.13061", "pdf": "https://arxiv.org/pdf/2502.13061.pdf", "abs": "https://arxiv.org/abs/2502.13061", "title": "Robust Adaptation of Large Multimodal Models for Retrieval Augmented Hateful Meme Detection", "authors": ["Jingbiao Mei", "Jinghong Chen", "Guangyu Yang", "Weizhe Lin", "Bill Byrne"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "EMNLP 2025 Main", "summary": "Hateful memes have become a significant concern on the Internet,\nnecessitating robust automated detection systems. While Large Multimodal Models\n(LMMs) have shown promise in hateful meme detection, they face notable\nchallenges like sub-optimal performance and limited out-of-domain\ngeneralization capabilities. Recent studies further reveal the limitations of\nboth supervised fine-tuning (SFT) and in-context learning when applied to LMMs\nin this setting. To address these issues, we propose a robust adaptation\nframework for hateful meme detection that enhances in-domain accuracy and\ncross-domain generalization while preserving the general vision-language\ncapabilities of LMMs. Analysis reveals that our approach achieves improved\nrobustness under adversarial attacks compared to SFT models. Experiments on six\nmeme classification datasets show that our approach achieves state-of-the-art\nperformance, outperforming larger agentic systems. Moreover, our method\ngenerates higher-quality rationales for explaining hateful content compared to\nstandard SFT, enhancing model interpretability. Code available at\nhttps://github.com/JingbiaoMei/RGCL"}
{"id": "2502.18993", "pdf": "https://arxiv.org/pdf/2502.18993.pdf", "abs": "https://arxiv.org/abs/2502.18993", "title": "MEBench: Benchmarking Large Language Models for Cross-Document Multi-Entity Question Answering", "authors": ["Teng Lin", "Yuyu Luo", "Nan Tang"], "categories": ["cs.CL", "cs.DB"], "comment": null, "summary": "Multi-entity question answering (MEQA) represents significant challenges for\nlarge language models (LLM) and retrieval-augmented generation (RAG) systems,\nwhich frequently struggle to consolidate scattered information across diverse\ndocuments. While existing methods excel at single-document comprehension, they\noften struggle with cross-document aggregation, particularly when resolving\nentity-dense questions like \"What is the distribution of ACM Fellows among\nvarious fields of study?\", which require integrating entity-centric insights\nfrom heterogeneous sources (e.g., Wikipedia pages). To address this gap, we\nintroduce MEBench, a novel multi-document, multi-entity benchmark designed to\nsystematically evaluate LLMs' capacity to retrieve, consolidate, and reason\nover fragmented information. Our benchmark comprises 4,780 questions which are\nsystematically categorized into three primary categories, further divided into\neight distinct types, ensuring broad coverage of real-world multi-entity\nreasoning scenarios. Our experiments on state-of-the-art LLMs (e.g., GPT-4,\nLlama-3) and RAG pipelines reveal critical limitations: even advanced models\nachieve only 59% accuracy on MEBench. Our benchmark emphasizes the importance\nof completeness and factual precision of information extraction in MEQA tasks,\nusing Entity-Attributed F1 (EA-F1) metric for granular evaluation of\nentity-level correctness and attribution validity. MEBench not only highlights\nsystemic weaknesses in current LLM frameworks but also provides a foundation\nfor advancing robust, entity-aware QA architectures."}
{"id": "2502.19548", "pdf": "https://arxiv.org/pdf/2502.19548.pdf", "abs": "https://arxiv.org/abs/2502.19548", "title": "When Large Language Models Meet Speech: A Survey on Integration Approaches", "authors": ["Zhengdong Yang", "Shuichiro Shimizu", "Yahan Yu", "Chenhui Chu"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted at Findings of ACL 2025 (Long Paper)", "summary": "Recent advancements in large language models (LLMs) have spurred interest in\nexpanding their application beyond text-based tasks. A large number of studies\nhave explored integrating other modalities with LLMs, notably speech modality,\nwhich is naturally related to text. This paper surveys the integration of\nspeech with LLMs, categorizing the methodologies into three primary approaches:\ntext-based, latent-representation-based, and audio-token-based integration. We\nalso demonstrate how these methods are applied across various speech-related\napplications and highlight the challenges in this field to offer inspiration\nfor"}
{"id": "2503.21929", "pdf": "https://arxiv.org/pdf/2503.21929.pdf", "abs": "https://arxiv.org/abs/2503.21929", "title": "Local Normalization Distortion and the Thermodynamic Formalism of Decoding Strategies for Large Language Models", "authors": ["Tom Kempton", "Stuart Burrell"], "categories": ["cs.CL", "cs.LG", "math.DS"], "comment": null, "summary": "Advances in hardware and language model architecture have spurred a\nrevolution in natural language generation. However, autoregressive models\ncompute probability distributions over next-token choices, and sampling from\nthese distributions, known as decoding, has received significantly less\nattention than other design choices. Existing decoding strategies are largely\nbased on heuristics, resulting in methods that are difficult to apply or\nimprove in a principled manner. We develop the theory of decoding strategies\nfor language models by expressing popular decoding algorithms as equilibrium\nstates in the language of ergodic theory and stating the objective functions\nthey optimize. Using this, we analyze the effect of the local normalization\nstep required to make probabilities sum to one in top-k, nucleus, and\ntemperature sampling. We argue that local normalization distortion is a\nfundamental defect of decoding strategies and quantify the size of this\ndistortion and its effect on mathematical proxies for the quality and diversity\nof generated text. This yields conclusions for the design of decoding\nalgorithms and the detection of machine-generated text."}
{"id": "2504.01542", "pdf": "https://arxiv.org/pdf/2504.01542.pdf", "abs": "https://arxiv.org/abs/2504.01542", "title": "Register Always Matters: Analysis of LLM Pretraining Data Through the Lens of Language Variation", "authors": ["Amanda Myntti", "Erik Henriksson", "Veronika Laippala", "Sampo Pyysalo"], "categories": ["cs.CL"], "comment": null, "summary": "Pretraining data curation is a cornerstone in Large Language Model (LLM)\ndevelopment, leading to growing research on quality filtering of large web\ncorpora. From statistical quality flags to LLM-based labelling systems,\ndatasets are divided into categories, frequently reducing to a binary: those\npassing the filters are deemed as valuable examples, others are discarded as\nuseless or detrimental. However, a more detailed understanding of the\ncontribution of different kinds of texts to model performance is still largely\nlacking. In this article, we present the first study utilising registers or\ngenres - a widely used standard in corpus linguistics to model linguistic\nvariation - to curate pretraining datasets and investigate the effect of\nregister on the performance of LLMs. We train small generative models with\nregister classified data and evaluate them using standard benchmarks, and show\nthat the register of pretraining data substantially affects model performance.\nWe uncover surprising relationships between the pretraining material and the\nresulting models: using the News register results in subpar performance, and on\nthe contrary, including the Opinion class, covering texts such as reviews and\nopinion blogs, is highly beneficial. While a model trained on the entire\nunfiltered dataset outperforms those trained on datasets limited to a single\nregister, combining well-performing registers like How-to-Instructions,\nInformational Description, and Opinion leads to major improvements.\nFurthermore, analysis of individual benchmark results reveals key differences\nin the strengths and drawbacks of specific register classes as pretraining\ndata. These findings show that register is an important explainer of model\nvariation and can facilitate more deliberate future data selection practices."}
{"id": "2504.08776", "pdf": "https://arxiv.org/pdf/2504.08776.pdf", "abs": "https://arxiv.org/abs/2504.08776", "title": "SemCAFE: When Named Entities make the Difference Assessing Web Source Reliability through Entity-level Analytics", "authors": ["Gautam Kishore Shahi", "Oshani Seneviratne", "Marc Spaniol"], "categories": ["cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "With the shift from traditional to digital media, the online landscape now\nhosts not only reliable news articles but also a significant amount of\nunreliable content. Digital media has faster reachability by significantly\ninfluencing public opinion and advancing political agendas. While newspaper\nreaders may be familiar with their preferred outlets political leanings or\ncredibility, determining unreliable news articles is much more challenging. The\ncredibility of many online sources is often opaque, with AI generated content\nbeing easily disseminated at minimal cost. Unreliable news articles,\nparticularly those that followed the Russian invasion of Ukraine in 2022,\nclosely mimic the topics and writing styles of credible sources, making them\ndifficult to distinguish. To address this, we introduce SemCAFE, a system\ndesigned to detect news reliability by incorporating entity relatedness into\nits assessment. SemCAFE employs standard Natural Language Processing\ntechniques, such as boilerplate removal and tokenization, alongside entity\nlevel semantic analysis using the YAGO knowledge base. By creating a semantic\nfingerprint for each news article, SemCAFE could assess the credibility of\n46,020 reliable and 3,407 unreliable articles on the 2022 Russian invasion of\nUkraine. Our approach improved the macro F1 score by 12% over state of the art\nmethods. The sample data and code are available on GitHub"}
{"id": "2504.21117", "pdf": "https://arxiv.org/pdf/2504.21117.pdf", "abs": "https://arxiv.org/abs/2504.21117", "title": "Beyond One-Size-Fits-All: Inversion Learning for Highly Effective NLG Evaluation Prompts", "authors": ["Hanhua Hong", "Chenghao Xiao", "Yang Wang", "Yiqi Liu", "Wenge Rong", "Chenghua Lin"], "categories": ["cs.CL"], "comment": "12 pages, accepted by Transactions of the Association for\n  Computational Linguistics (TACL)", "summary": "Evaluating natural language generation systems is challenging due to the\ndiversity of valid outputs. While human evaluation is the gold standard, it\nsuffers from inconsistencies, lack of standardisation, and demographic biases,\nlimiting reproducibility. LLM-based evaluators offer a scalable alternative but\nare highly sensitive to prompt design, where small variations can lead to\nsignificant discrepancies. In this work, we propose an inversion learning\nmethod that learns effective reverse mappings from model outputs back to their\ninput instructions, enabling the automatic generation of highly effective,\nmodel-specific evaluation prompts. Our method requires only a single evaluation\nsample and eliminates the need for time-consuming manual prompt engineering,\nthereby improving both efficiency and robustness. Our work contributes toward a\nnew direction for more robust and efficient LLM-based evaluation."}
{"id": "2505.00949", "pdf": "https://arxiv.org/pdf/2505.00949.pdf", "abs": "https://arxiv.org/abs/2505.00949", "title": "Llama-Nemotron: Efficient Reasoning Models", "authors": ["Akhiad Bercovich", "Itay Levy", "Izik Golan", "Mohammad Dabbah", "Ran El-Yaniv", "Omri Puny", "Ido Galil", "Zach Moshe", "Tomer Ronen", "Najeeb Nabwani", "Ido Shahaf", "Oren Tropp", "Ehud Karpas", "Ran Zilberstein", "Jiaqi Zeng", "Soumye Singhal", "Alexander Bukharin", "Yian Zhang", "Tugrul Konuk", "Gerald Shen", "Ameya Sunil Mahabaleshwarkar", "Bilal Kartal", "Yoshi Suhara", "Olivier Delalleau", "Zijia Chen", "Zhilin Wang", "David Mosallanezhad", "Adi Renduchintala", "Haifeng Qian", "Dima Rekesh", "Fei Jia", "Somshubra Majumdar", "Vahid Noroozi", "Wasi Uddin Ahmad", "Sean Narenthiran", "Aleksander Ficek", "Mehrzad Samadi", "Jocelyn Huang", "Siddhartha Jain", "Igor Gitman", "Ivan Moshkov", "Wei Du", "Shubham Toshniwal", "George Armstrong", "Branislav Kisacanin", "Matvei Novikov", "Daria Gitman", "Evelina Bakhturina", "Prasoon Varshney", "Makesh Narsimhan", "Jane Polak Scowcroft", "John Kamalu", "Dan Su", "Kezhi Kong", "Markus Kliegl", "Rabeeh Karimi Mahabadi", "Ying Lin", "Sanjeev Satheesh", "Jupinder Parmar", "Pritam Gundecha", "Brandon Norick", "Joseph Jennings", "Shrimai Prabhumoye", "Syeda Nahida Akter", "Mostofa Patwary", "Abhinav Khattar", "Deepak Narayanan", "Roger Waleffe", "Jimmy Zhang", "Bor-Yiing Su", "Guyue Huang", "Terry Kong", "Parth Chadha", "Sahil Jain", "Christine Harvey", "Elad Segal", "Jining Huang", "Sergey Kashirsky", "Robert McQueen", "Izzy Putterman", "George Lam", "Arun Venkatesan", "Sherry Wu", "Vinh Nguyen", "Manoj Kilaru", "Andrew Wang", "Anna Warno", "Abhilash Somasamudramath", "Sandip Bhaskar", "Maka Dong", "Nave Assaf", "Shahar Mor", "Omer Ullman Argov", "Scot Junkin", "Oleksandr Romanenko", "Pedro Larroy", "Monika Katariya", "Marco Rovinelli", "Viji Balas", "Nicholas Edelman", "Anahita Bhiwandiwalla", "Muthu Subramaniam", "Smita Ithape", "Karthik Ramamoorthy", "Yuting Wu", "Suguna Varshini Velury", "Omri Almog", "Joyjit Daw", "Denys Fridman", "Erick Galinkin", "Michael Evans", "Shaona Ghosh", "Katherine Luna", "Leon Derczynski", "Nikki Pope", "Eileen Long", "Seth Schneider", "Guillermo Siman", "Tomasz Grzegorzek", "Pablo Ribalta", "Monika Katariya", "Chris Alexiuk", "Joey Conway", "Trisha Saar", "Ann Guan", "Krzysztof Pawelec", "Shyamala Prayaga", "Oleksii Kuchaiev", "Boris Ginsburg", "Oluwatobi Olabiyi", "Kari Briski", "Jonathan Cohen", "Bryan Catanzaro", "Jonah Alben", "Yonatan Geifman", "Eric Chung"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce the Llama-Nemotron series of models, an open family of\nheterogeneous reasoning models that deliver exceptional reasoning capabilities,\ninference efficiency, and an open license for enterprise use. The family comes\nin three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs\ncompetitively with state-of-the-art reasoning models such as DeepSeek-R1 while\noffering superior inference throughput and memory efficiency. In this report,\nwe discuss the training procedure for these models, which entails using neural\narchitecture search from Llama 3 models for accelerated inference, knowledge\ndistillation, and continued pretraining, followed by a reasoning-focused\npost-training stage consisting of two main parts: supervised fine-tuning and\nlarge scale reinforcement learning. Llama-Nemotron models are the first\nopen-source models to support a dynamic reasoning toggle, allowing users to\nswitch between standard chat and reasoning modes during inference. To further\nsupport open research and facilitate model development, we provide the\nfollowing resources: 1. We release the Llama-Nemotron reasoning models --\nLN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA\nOpen Model License Agreement. 2. We release the complete post-training dataset:\nLlama-Nemotron-Post-Training-Dataset. 3. We also release our training\ncodebases: NeMo, NeMo-Aligner, and Megatron-LM."}
{"id": "2505.04416", "pdf": "https://arxiv.org/pdf/2505.04416.pdf", "abs": "https://arxiv.org/abs/2505.04416", "title": "OBLIVIATE: Robust and Practical Machine Unlearning for Large Language Models", "authors": ["Xiaoyu Xu", "Minxin Du", "Qingqing Ye", "Haibo Hu"], "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "comment": "To appear at EMNLP 25 main conference", "summary": "Large language models (LLMs) trained over extensive corpora risk memorizing\nsensitive, copyrighted, or toxic content. To address this, we propose\n\\textbf{OBLIVIATE}, a robust unlearning framework that removes targeted data\nwhile preserving model utility. The framework follows a structured process:\nextracting target tokens, building retain sets, and fine-tuning with a tailored\nloss function comprising three components -- masking, distillation, and world\nfact. Using low-rank adapters (LoRA) ensures efficiency without compromising\nunlearning quality. We conduct experiments on multiple datasets, including\nHarry Potter series, WMDP, and TOFU, using a comprehensive suite of metrics:\n\\emph{forget quality} (via a new document-level memorization score),\n\\emph{model utility}, and \\emph{fluency}. Results demonstrate its effectiveness\nin resisting membership inference attacks, minimizing the impact on retained\ndata, and maintaining robustness across diverse scenarios."}
{"id": "2505.14406", "pdf": "https://arxiv.org/pdf/2505.14406.pdf", "abs": "https://arxiv.org/abs/2505.14406", "title": "Pierce the Mists, Greet the Sky: Decipher Knowledge Overshadowing via Knowledge Circuit Analysis", "authors": ["Haoming Huang", "Yibo Yan", "Jiahao Huo", "Xin Zou", "Xinfeng Li", "Kun Wang", "Xuming Hu"], "categories": ["cs.CL"], "comment": "Accepted by 2025 EMNLP Main", "summary": "Large Language Models (LLMs), despite their remarkable capabilities, are\nhampered by hallucinations. A particularly challenging variant, knowledge\novershadowing, occurs when one piece of activated knowledge inadvertently masks\nanother relevant piece, leading to erroneous outputs even with high-quality\ntraining data. Current understanding of overshadowing is largely confined to\ninference-time observations, lacking deep insights into its origins and\ninternal mechanisms during model training. Therefore, we introduce\nPhantomCircuit, a novel framework designed to comprehensively analyze and\ndetect knowledge overshadowing. By innovatively employing knowledge circuit\nanalysis, PhantomCircuit dissects the function of key components in the circuit\nand how the attention pattern dynamics contribute to the overshadowing\nphenomenon and its evolution throughout the training process. Extensive\nexperiments demonstrate PhantomCircuit's effectiveness in identifying such\ninstances, offering novel insights into this elusive hallucination and\nproviding the research community with a new methodological lens for its\npotential mitigation."}
{"id": "2505.16661", "pdf": "https://arxiv.org/pdf/2505.16661.pdf", "abs": "https://arxiv.org/abs/2505.16661", "title": "A Japanese Language Model and Three New Evaluation Benchmarks for Pharmaceutical NLP", "authors": ["Shinnosuke Ono", "Issey Sukeda", "Takuro Fujii", "Kosei Buma", "Shunsuke Sasaki"], "categories": ["cs.CL"], "comment": "15 pages, 9 tables, 5 figures", "summary": "We present a Japanese domain-specific language model for the pharmaceutical\nfield, developed through continual pretraining on 2 billion Japanese\npharmaceutical tokens and 8 billion English biomedical tokens. To enable\nrigorous evaluation, we introduce three new benchmarks: YakugakuQA, based on\nnational pharmacist licensing exams; NayoseQA, which tests cross-lingual\nsynonym and terminology normalization; and SogoCheck, a novel task designed to\nassess consistency reasoning between paired statements. We evaluate our model\nagainst both open-source medical LLMs and commercial models, including GPT-4o.\nResults show that our domain-specific model outperforms existing open models\nand achieves competitive performance with commercial ones, particularly on\nterminology-heavy and knowledge-based tasks. Interestingly, even GPT-4o\nperforms poorly on SogoCheck, suggesting that cross-sentence consistency\nreasoning remains an open challenge. Our benchmark suite offers a broader\ndiagnostic lens for pharmaceutical NLP, covering factual recall, lexical\nvariation, and logical consistency. This work demonstrates the feasibility of\nbuilding practical, secure, and cost-effective language models for Japanese\ndomain-specific applications, and provides reusable evaluation resources for\nfuture research in pharmaceutical and healthcare NLP. Our model, codes, and\ndatasets are released at https://github.com/EQUES-Inc/pharma-LLM-eval."}
{"id": "2505.17471", "pdf": "https://arxiv.org/pdf/2505.17471.pdf", "abs": "https://arxiv.org/abs/2505.17471", "title": "FinRAGBench-V: A Benchmark for Multimodal RAG with Visual Citation in the Financial Domain", "authors": ["Suifeng Zhao", "Zhuoran Jin", "Sujian Li", "Jun Gao"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) plays a vital role in the financial\ndomain, powering applications such as real-time market analysis, trend\nforecasting, and interest rate computation. However, most existing RAG research\nin finance focuses predominantly on textual data, overlooking the rich visual\ncontent in financial documents, resulting in the loss of key analytical\ninsights. To bridge this gap, we present FinRAGBench-V, a comprehensive visual\nRAG benchmark tailored for finance which effectively integrates multimodal data\nand provides visual citation to ensure traceability. It includes a bilingual\nretrieval corpus with 60,780 Chinese and 51,219 English pages, along with a\nhigh-quality, human-annotated question-answering (QA) dataset spanning\nheterogeneous data types and seven question categories. Moreover, we introduce\nRGenCite, an RAG baseline that seamlessly integrates visual citation with\ngeneration. Furthermore, we propose an automatic citation evaluation method to\nsystematically assess the visual citation capabilities of Multimodal Large\nLanguage Models (MLLMs). Extensive experiments on RGenCite underscore the\nchallenging nature of FinRAGBench-V, providing valuable insights for the\ndevelopment of multimodal RAG systems in finance."}
{"id": "2505.18744", "pdf": "https://arxiv.org/pdf/2505.18744.pdf", "abs": "https://arxiv.org/abs/2505.18744", "title": "LogicCat: A Chain-of-Thought Text-to-SQL Benchmark for Complex Reasoning", "authors": ["Tao Liu", "Xutao Mao", "Hongying Zan", "Dixuan Zhang", "Yifan Li", "Haixin Liu", "Lulu Kong", "Jiaming Hou", "Rui Li", "YunLong Li", "aoze zheng", "Zhiqiang Zhang", "Luo Zhewei", "Kunli Zhang", "Min Peng"], "categories": ["cs.CL"], "comment": "9 pages, 5 figures", "summary": "Text-to-SQL is a critical task in natural language processing that aims to\ntransform natural language questions into accurate and executable SQL queries.\nIn real-world scenarios, these reasoning tasks are often accompanied by complex\nmathematical computations, domain knowledge, and hypothetical reasoning\nscenarios. However, existing large-scale Text-to-SQL datasets typically focus\non business logic and task logic, neglecting critical factors such as vertical\ndomain knowledge, complex mathematical reasoning, and hypothetical reasoning,\nwhich are essential for realistically reflecting the reasoning demands in\npractical applications and completing data querying and analysis. To bridge\nthis gap, we introduce LogicCat, the first Text-to-SQL benchmark dataset\nspecifically designed for complex reasoning and chain-of-thought parsing,\nencompassing physics, arithmetic, commonsense, and hypothetical reasoning\nscenarios. LogicCat comprises 4,038 English questions paired 12,114 detailed\nchain-of-thought reasoning steps, spanning 45 databases across diverse domains,\nsignificantly surpassing existing datasets in complexity. Experimental results\ndemonstrate that LogicCat substantially increases the task difficulty for\ncurrent state-of-the-art models to at most 33.20% execution accuracy,\nindicating that this task remains exceptionally challenging. The advancement of\nLogicCat represents a crucial step toward developing systems suitable for\nreal-world enterprise data analysis and autonomous query generation. We have\nreleased our dataset code at https://github.com/Ffunkytao/LogicCat."}
{"id": "2505.20511", "pdf": "https://arxiv.org/pdf/2505.20511.pdf", "abs": "https://arxiv.org/abs/2505.20511", "title": "Multimodal Emotion Recognition in Conversations: A Survey of Methods, Trends, Challenges and Prospects", "authors": ["Chengyan Wu", "Yiqiang Cai", "Yang Liu", "Pengxu Zhu", "Yun Xue", "Ziwei Gong", "Julia Hirschberg", "Bolei Ma"], "categories": ["cs.CL"], "comment": "EMNLP 2025 Findings", "summary": "While text-based emotion recognition methods have achieved notable success,\nreal-world dialogue systems often demand a more nuanced emotional understanding\nthan any single modality can offer. Multimodal Emotion Recognition in\nConversations (MERC) has thus emerged as a crucial direction for enhancing the\nnaturalness and emotional understanding of human-computer interaction. Its goal\nis to accurately recognize emotions by integrating information from various\nmodalities such as text, speech, and visual signals.\n  This survey offers a systematic overview of MERC, including its motivations,\ncore tasks, representative methods, and evaluation strategies. We further\nexamine recent trends, highlight key challenges, and outline future directions.\nAs interest in emotionally intelligent systems grows, this survey provides\ntimely guidance for advancing MERC research."}
{"id": "2505.24539", "pdf": "https://arxiv.org/pdf/2505.24539.pdf", "abs": "https://arxiv.org/abs/2505.24539", "title": "Localizing Persona Representations in LLMs", "authors": ["Celia Cintas", "Miriam Rateike", "Erik Miehling", "Elizabeth Daly", "Skyler Speakman"], "categories": ["cs.CL", "cs.AI"], "comment": "To appear in the AAAI/ACM Conference on AI, Ethics, and Society\n  (AIES) 2025", "summary": "We present a study on how and where personas -- defined by distinct sets of\nhuman characteristics, values, and beliefs -- are encoded in the representation\nspace of large language models (LLMs). Using a range of dimension reduction and\npattern recognition methods, we first identify the model layers that show the\ngreatest divergence in encoding these representations. We then analyze the\nactivations within a selected layer to examine how specific personas are\nencoded relative to others, including their shared and distinct embedding\nspaces. We find that, across multiple pre-trained decoder-only LLMs, the\nanalyzed personas show large differences in representation space only within\nthe final third of the decoder layers. We observe overlapping activations for\nspecific ethical perspectives -- such as moral nihilism and utilitarianism --\nsuggesting a degree of polysemy. In contrast, political ideologies like\nconservatism and liberalism appear to be represented in more distinct regions.\nThese findings help to improve our understanding of how LLMs internally\nrepresent information and can inform future efforts in refining the modulation\nof specific human traits in LLM outputs. Warning: This paper includes\npotentially offensive sample statements."}
{"id": "2506.02659", "pdf": "https://arxiv.org/pdf/2506.02659.pdf", "abs": "https://arxiv.org/abs/2506.02659", "title": "Are Economists Always More Introverted? Analyzing Consistency in Persona-Assigned LLMs", "authors": ["Manon Reusens", "Bart Baesens", "David Jurgens"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025 findings", "summary": "Personalized Large Language Models (LLMs) are increasingly used in diverse\napplications, where they are assigned a specific persona - such as a happy high\nschool teacher - to guide their responses. While prior research has examined\nhow well LLMs adhere to predefined personas in writing style, a comprehensive\nanalysis of consistency across different personas and task types is lacking. In\nthis paper, we introduce a new standardized framework to analyze consistency in\npersona-assigned LLMs. We define consistency as the extent to which a model\nmaintains coherent responses when assigned the same persona across different\ntasks and runs. Our framework evaluates personas across four different\ncategories (happiness, occupation, personality, and political stance) spanning\nmultiple task dimensions (survey writing, essay generation, social media post\ngeneration, single turn, and multi-turn conversations). Our findings reveal\nthat consistency is influenced by multiple factors, including the assigned\npersona, stereotypes, and model design choices. Consistency also varies across\ntasks, increasing with more structured tasks and additional context. All code\nis available on GitHub."}
{"id": "2506.05062", "pdf": "https://arxiv.org/pdf/2506.05062.pdf", "abs": "https://arxiv.org/abs/2506.05062", "title": "Debatable Intelligence: Benchmarking LLM Judges via Debate Speech Evaluation", "authors": ["Noy Sternlicht", "Ariel Gera", "Roy Bar-Haim", "Tom Hope", "Noam Slonim"], "categories": ["cs.CL"], "comment": "EMNLP 2025. Code:\n  https://github.com/noy-sternlicht/Debatable-Intelligence", "summary": "We introduce Debate Speech Evaluation as a novel and challenging benchmark\nfor assessing LLM judges. Evaluating debate speeches requires a deep\nunderstanding of the speech at multiple levels, including argument strength and\nrelevance, the coherence and organization of the speech, the appropriateness of\nits style and tone, and so on. This task involves a unique set of cognitive\nabilities that previously received limited attention in systematic LLM\nbenchmarking. To explore such skills, we leverage a dataset of over 600\nmeticulously annotated debate speeches and present the first in-depth analysis\nof how state-of-the-art LLMs compare to human judges on this task. Our findings\nreveal a nuanced picture: while larger models can approximate individual human\njudgments in some respects, they differ substantially in their overall judgment\nbehavior. We also investigate the ability of frontier LLMs to generate\npersuasive, opinionated speeches, showing that models may perform at a human\nlevel on this task."}
{"id": "2506.07642", "pdf": "https://arxiv.org/pdf/2506.07642.pdf", "abs": "https://arxiv.org/abs/2506.07642", "title": "TreeReview: A Dynamic Tree of Questions Framework for Deep and Efficient LLM-based Scientific Peer Review", "authors": ["Yuan Chang", "Ziyue Li", "Hengyuan Zhang", "Yuanbo Kong", "Yanru Wu", "Hayden Kwok-Hay So", "Zhijiang Guo", "Liya Zhu", "Ngai Wong"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP2025 Main", "summary": "While Large Language Models (LLMs) have shown significant potential in\nassisting peer review, current methods often struggle to generate thorough and\ninsightful reviews while maintaining efficiency. In this paper, we propose\nTreeReview, a novel framework that models paper review as a hierarchical and\nbidirectional question-answering process. TreeReview first constructs a tree of\nreview questions by recursively decomposing high-level questions into\nfine-grained sub-questions and then resolves the question tree by iteratively\naggregating answers from leaf to root to get the final review. Crucially, we\nincorporate a dynamic question expansion mechanism to enable deeper probing by\ngenerating follow-up questions when needed. We construct a benchmark derived\nfrom ICLR and NeurIPS venues to evaluate our method on full review generation\nand actionable feedback comments generation tasks. Experimental results of both\nLLM-based and human evaluation show that TreeReview outperforms strong\nbaselines in providing comprehensive, in-depth, and expert-aligned review\nfeedback, while reducing LLM token usage by up to 80% compared to\ncomputationally intensive approaches. Our code and benchmark dataset are\navailable at https://github.com/YuanChang98/tree-review."}
{"id": "2506.07899", "pdf": "https://arxiv.org/pdf/2506.07899.pdf", "abs": "https://arxiv.org/abs/2506.07899", "title": "MEMOIR: Lifelong Model Editing with Minimal Overwrite and Informed Retention for LLMs", "authors": ["Ke Wang", "Yiming Qin", "Nikolaos Dimitriadis", "Alessandro Favero", "Pascal Frossard"], "categories": ["cs.CL", "cs.LG"], "comment": "The first two authors contributed equally to this work", "summary": "Language models deployed in real-world systems often require post-hoc updates\nto incorporate new or corrected knowledge. However, editing such models\nefficiently and reliably-without retraining or forgetting previous\ninformation-remains a major challenge. Existing methods for lifelong model\nediting either compromise generalization, interfere with past edits, or fail to\nscale to long editing sequences. We propose MEMOIR, a novel scalable framework\nthat injects knowledge through a residual memory, i.e., a dedicated parameter\nmodule, while preserving the core capabilities of the pre-trained model. By\nsparsifying input activations through sample-dependent masks, MEMOIR confines\neach edit to a distinct subset of the memory parameters, minimizing\ninterference among edits. At inference, it identifies relevant edits by\ncomparing the sparse activation patterns of new queries to those stored during\nediting. This enables generalization to rephrased queries by activating only\nthe relevant knowledge while suppressing unnecessary memory activation for\nunrelated prompts. Experiments on question answering, hallucination correction,\nand out-of-distribution generalization benchmarks for LLaMA-3 and Mistral\nbackbones demonstrate that MEMOIR achieves state-of-the-art performance across\nreliability, generalization, and locality metrics, scaling to thousands of\nsequential edits with minimal forgetting."}
{"id": "2506.19089", "pdf": "https://arxiv.org/pdf/2506.19089.pdf", "abs": "https://arxiv.org/abs/2506.19089", "title": "Language Models Might Not Understand You: Evaluating Theory of Mind via Story Prompting", "authors": ["Nathaniel Getachew", "Abulhair Saparov"], "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 11 figures", "summary": "We introduce $\\texttt{StorySim}$, a programmable framework for synthetically\ngenerating stories to evaluate the theory of mind (ToM) and world modeling (WM)\ncapabilities of large language models (LLMs). Unlike prior benchmarks that may\nsuffer from contamination in pretraining data, $\\texttt{StorySim}$ produces\nnovel, compositional story prompts anchored by a highly controllable\n$\\texttt{Storyboard}$, enabling precise manipulation of character perspectives\nand events. We use this framework to design first- and second-order ToM tasks\nalongside WM tasks that control for the ability to track and model mental\nstates. Our experiments across a suite of state-of-the-art LLMs reveal that\nmost models perform better on WM tasks than ToM tasks, and that models tend to\nperform better reasoning with humans compared to inanimate objects.\nAdditionally, our framework enabled us to find evidence of heuristic behavior\nsuch as recency bias and an over-reliance on earlier events in the story. All\ncode for generating data and evaluations is freely available."}
{"id": "2507.15512", "pdf": "https://arxiv.org/pdf/2507.15512.pdf", "abs": "https://arxiv.org/abs/2507.15512", "title": "Step-level Verifier-guided Hybrid Test-Time Scaling for Large Language Models", "authors": ["Kaiyan Chang", "Yonghao Shi", "Chenglong Wang", "Hang Zhou", "Chi Hu", "Xiaoqian Liu", "Yingfeng Luo", "Yuan Ge", "Tong Xiao", "Jingbo Zhu"], "categories": ["cs.CL"], "comment": "Accepted by EMNLP 2025. Code: https://github.com/Lucky-259/Hybrid_TTS", "summary": "Test-Time Scaling (TTS) is a promising approach to progressively elicit the\nmodel's intelligence during inference. Recently, training-based TTS methods,\nsuch as continued reinforcement learning (RL), have further surged in\npopularity, while training-free TTS methods are gradually fading from\nprominence. However, the additional computation overhead of training amplifies\nthe burden on test-time scaling. In this paper, we focus on training-free TTS\nmethods for reasoning. We first design Conditional Step-level Self-refinement,\na fine-grained sequential scaling method guided by process verification. On top\nof its effectiveness, we further combine it with other classical parallel\nscaling methods at the step level, to introduce a novel inference paradigm\ncalled Hybrid Test-Time Scaling. Extensive experiments on five\ninstruction-tuned LLMs across different scales (3B-14B) and families\ndemonstrate that hybrid strategy incorporating various training-free TTS\nmethods at a fine granularity has considerable potential for expanding the\nreasoning performance boundaries of LLMs."}
{"id": "2507.22286", "pdf": "https://arxiv.org/pdf/2507.22286.pdf", "abs": "https://arxiv.org/abs/2507.22286", "title": "Meaning-infused grammar: Gradient Acceptability Shapes the Geometric Representations of Constructions in LLMs", "authors": ["Supantho Rakshit", "Adele Goldberg"], "categories": ["cs.CL", "cs.AI", "68T50"], "comment": "6 pages, 3 figures, Accepted for publication at the Second\n  International Workshop on Construction Grammars and NLP at the 16th\n  International Conference for Computational Semantics (IWCS) 2025", "summary": "The usage-based constructionist (UCx) approach to language posits that\nlanguage comprises a network of learned form-meaning pairings (constructions)\nwhose use is largely determined by their meanings or functions, requiring them\nto be graded and probabilistic. This study investigates whether the internal\nrepresentations in Large Language Models (LLMs) reflect the proposed\nfunction-infused gradience. We analyze representations of the English Double\nObject (DO) and Prepositional Object (PO) constructions in Pythia-$1.4$B, using\na dataset of $5000$ sentence pairs systematically varied by human-rated\npreference strength for DO or PO. Geometric analyses show that the separability\nbetween the two constructions' representations, as measured by energy distance\nor Jensen-Shannon divergence, is systematically modulated by gradient\npreference strength, which depends on lexical and functional properties of\nsentences. That is, more prototypical exemplars of each construction occupy\nmore distinct regions in activation space, compared to sentences that could\nhave equally well have occured in either construction. These results provide\nevidence that LLMs learn rich, meaning-infused, graded representations of\nconstructions and offer support for geometric measures for representations in\nLLMs."}
{"id": "2508.04795", "pdf": "https://arxiv.org/pdf/2508.04795.pdf", "abs": "https://arxiv.org/abs/2508.04795", "title": "Enhancing Dialogue Annotation with Speaker Characteristics Leveraging a Frozen LLM", "authors": ["Thomas Thebaud", "Yen-Ju Lu", "Matthew Wiesner", "Peter Viechnicki", "Najim Dehak"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "Accepted in the 2025 IEEE Automatic Speech Recognition and\n  Understanding Workshop", "summary": "In dialogue transcription pipelines, Large Language Models (LLMs) are\nfrequently employed in post-processing to improve grammar, punctuation, and\nreadability. We explore a complementary post-processing step: enriching\ntranscribed dialogues by adding metadata tags for speaker characteristics such\nas age, gender, and emotion. Some of the tags are global to the entire\ndialogue, while some are time-variant. Our approach couples frozen audio\nfoundation models, such as Whisper or WavLM, with a frozen LLAMA language model\nto infer these speaker attributes, without requiring task-specific fine-tuning\nof either model. Using lightweight, efficient connectors to bridge audio and\nlanguage representations, we achieve competitive performance on speaker\nprofiling tasks while preserving modularity and speed. Additionally, we\ndemonstrate that a frozen LLAMA model can compare x-vectors directly, achieving\nan Equal Error Rate of 8.8% in some scenarios."}
{"id": "2508.15792", "pdf": "https://arxiv.org/pdf/2508.15792.pdf", "abs": "https://arxiv.org/abs/2508.15792", "title": "Bhav-Net: Knowledge Transfer for Cross-Lingual Antonym vs Synonym Distinction via Dual-Space Graph Transformers", "authors": ["Samyak S. Sanghvi"], "categories": ["cs.CL"], "comment": "Found some issues and need to correct them", "summary": "Antonym vs synonym distinction across multiple languages presents unique\ncomputational challenges due to the paradoxical nature of antonymous\nrelationships words that share semantic domains while expressing opposite\nmeanings. This work introduces Bhav-Net, a novel dual-space architecture that\nenables effective knowledge transfer from complex multilingual models to\nsimpler, language-specific architectures while maintaining robust cross-lingual\nantonym--synonym distinction capabilities. Our approach combines\nlanguage-specific BERT encoders with graph transformer networks, creating\ndistinct semantic projections where synonymous pairs cluster in one space while\nantonymous pairs exhibit high similarity in a complementary space. Through\ncomprehensive evaluation across eight languages (English, German, French,\nSpanish, Italian, Portuguese, Dutch, and Russian), we demonstrate that semantic\nrelationship modeling transfers effectively across languages. The dual-encoder\ndesign achieves competitive performance against state-of-the-art baselines\nwhile providing interpretable semantic representations and effective\ncross-lingual generalization."}
{"id": "2508.16665", "pdf": "https://arxiv.org/pdf/2508.16665.pdf", "abs": "https://arxiv.org/abs/2508.16665", "title": "Trust but Verify! A Survey on Verification Design for Test-time Scaling", "authors": ["V Venktesh", "Mandeep Rathee", "Avishek Anand"], "categories": ["cs.CL", "cs.AI"], "comment": "18 pages", "summary": "Test-time scaling (TTS) has emerged as a new frontier for scaling the\nperformance of Large Language Models. In test-time scaling, by using more\ncomputational resources during inference, LLMs can improve their reasoning\nprocess and task performance. Several approaches have emerged for TTS such as\ndistilling reasoning traces from another model or exploring the vast decoding\nsearch space by employing a verifier. The verifiers serve as reward models that\nhelp score the candidate outputs from the decoding process to diligently\nexplore the vast solution space and select the best outcome. This paradigm\ncommonly termed has emerged as a superior approach owing to parameter free\nscaling at inference time and high performance gains. The verifiers could be\nprompt-based, fine-tuned as a discriminative or generative model to verify\nprocess paths, outcomes or both. Despite their widespread adoption, there is no\ndetailed collection, clear categorization and discussion of diverse\nverification approaches and their training mechanisms. In this survey, we cover\nthe diverse approaches in the literature and present a unified view of verifier\ntraining, types and their utility in test-time scaling. Our repository can be\nfound at\nhttps://github.com/elixir-research-group/Verifierstesttimescaling.github.io."}
{"id": "2508.17450", "pdf": "https://arxiv.org/pdf/2508.17450.pdf", "abs": "https://arxiv.org/abs/2508.17450", "title": "Persuasion Dynamics in LLMs: Investigating Robustness and Adaptability in Knowledge and Safety with DuET-PD", "authors": ["Bryan Chen Zhengyu Tan", "Daniel Wai Kit Chin", "Zhengyuan Liu", "Nancy F. Chen", "Roy Ka-Wei Lee"], "categories": ["cs.CL", "cs.CY"], "comment": "To appear at EMNLP 2025", "summary": "Large Language Models (LLMs) can struggle to balance gullibility to\nmisinformation and resistance to valid corrections in persuasive dialogues, a\ncritical challenge for reliable deployment. We introduce DuET-PD (Dual\nEvaluation for Trust in Persuasive Dialogues), a framework evaluating\nmulti-turn stance-change dynamics across dual dimensions: persuasion type\n(corrective/misleading) and domain (knowledge via MMLU-Pro, and safety via\nSALAD-Bench). We find that even a state-of-the-art model like GPT-4o achieves\nonly 27.32% accuracy in MMLU-Pro under sustained misleading persuasions.\nMoreover, results reveal a concerning trend of increasing sycophancy in newer\nopen-source models. To address this, we introduce Holistic DPO, a training\napproach balancing positive and negative persuasion examples. Unlike prompting\nor resist-only training, Holistic DPO enhances both robustness to\nmisinformation and receptiveness to corrections, improving\nLlama-3.1-8B-Instruct's accuracy under misleading persuasion in safety contexts\nfrom 4.21% to 76.54%. These contributions offer a pathway to developing more\nreliable and adaptable LLMs for multi-turn dialogue. Code is available at\nhttps://github.com/Social-AI-Studio/DuET-PD."}
{"id": "2509.00591", "pdf": "https://arxiv.org/pdf/2509.00591.pdf", "abs": "https://arxiv.org/abs/2509.00591", "title": "Probe-Rewrite-Evaluate: A Workflow for Reliable Benchmarks and Quantifying Evaluation Awareness", "authors": ["Lang Xiong", "Nishant Bhargava", "Jeremy Chang", "Jianhang Hong", "Haihao Liu", "Vasu Sharma", "Kevin Zhu"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) often exhibit significant behavioral shifts when\nthey perceive a change from a real-world deployment context to a controlled\nevaluation setting, a phenomenon known as \"evaluation awareness.\" This\ndiscrepancy poses a critical challenge for AI alignment, as benchmark\nperformance may not accurately reflect a model's true safety and honesty. In\nthis work, we systematically quantify these behavioral changes by manipulating\nthe perceived context of prompts. We introduce a methodology that uses a linear\nprobe to score prompts on a continuous scale from \"test-like\" to \"deploy-like\"\nand leverage an LLM rewriting strategy to shift these prompts towards a more\nnatural, deployment-style context while preserving the original task. Using\nthis method, we achieved a 30% increase in the average probe score across a\nstrategic role-playing dataset after rewriting. Evaluating a suite of\nstate-of-the-art models on these original and rewritten prompts, we find that\nrewritten \"deploy-like\" prompts induce a significant and consistent shift in\nbehavior. Across all models, we observed an average increase in honest\nresponses of 5.26% and a corresponding average decrease in deceptive responses\nof 12.40%. Furthermore, refusal rates increased by an average of 6.38%,\nindicating heightened safety compliance. Our findings demonstrate that\nevaluation awareness is a quantifiable and manipulable factor that directly\ninfluences LLM behavior, revealing that models are more prone to unsafe or\ndeceptive outputs in perceived test environments. This underscores the urgent\nneed for more realistic evaluation frameworks to accurately gauge true model\nalignment before deployment."}
{"id": "2509.01158", "pdf": "https://arxiv.org/pdf/2509.01158.pdf", "abs": "https://arxiv.org/abs/2509.01158", "title": "Joint Information Extraction Across Classical and Modern Chinese with Tea-MOELoRA", "authors": ["Xuemei Tang", "Chengxi Yan", "Jinghang Gu", "Chu-Ren Huang"], "categories": ["cs.CL"], "comment": "9 pages, 3 figures", "summary": "Chinese information extraction (IE) involves multiple tasks across diverse\ntemporal domains, including Classical and Modern documents. Fine-tuning a\nsingle model on heterogeneous tasks and across different eras may lead to\ninterference and reduced performance. Therefore, in this paper, we propose\nTea-MOELoRA, a parameter-efficient multi-task framework that combines LoRA with\na Mixture-of-Experts (MoE) design. Multiple low-rank LoRA experts specialize in\ndifferent IE tasks and eras, while a task-era-aware router mechanism\ndynamically allocates expert contributions. Experiments show that Tea-MOELoRA\noutperforms both single-task and joint LoRA baselines, demonstrating its\nability to leverage task and temporal knowledge effectively."}
{"id": "2509.01535", "pdf": "https://arxiv.org/pdf/2509.01535.pdf", "abs": "https://arxiv.org/abs/2509.01535", "title": "CAT: Causal Attention Tuning For Injecting Fine-grained Causal Knowledge into Large Language Models", "authors": ["Kairong Han", "Wenshuo Zhao", "Ziyu Zhao", "JunJian Ye", "Lujia Pan", "Kun Kuang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to EMNLP2025 Main conference", "summary": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains. However, a fundamental question remains: Can LLMs effectively utilize\ncausal knowledge for prediction and generation? Through empirical studies, we\nfind that LLMs trained directly on large-scale data often capture spurious\ncorrelations rather than true causal relationships, leading to suboptimal\nperformance, especially in out-of-distribution (OOD) scenarios. To address this\nchallenge, we propose Causal Attention Tuning (CAT), a novel approach that\ninjects fine-grained causal knowledge into the attention mechanism. We propose\nan automated pipeline that leverages human priors to automatically generate\ntoken-level causal signals and introduce the Re-Attention mechanism to guide\ntraining, helping the model focus on causal structures while mitigating noise\nand biases in attention scores. Experimental results on our proposed Spurious\nToken Game (STG) benchmark and multiple downstream tasks demonstrate that our\napproach effectively leverages causal knowledge for prediction and remains\nrobust in OOD scenarios. The CAT achieves an average improvement of 5.76% on\nthe STG dataset and 1.56% on downstream tasks. Notably, the OOD performance of\nthe Llama-3.1-8B model on STG_M increased from 64.5% to 90.5%, and Qwen's OOD\nperformance on the STG_H dataset improved from 25.4% to 55.9%. Implementation\ndetails can be found at https://github.com/Kairong-Han/CAT."}
{"id": "2509.03020", "pdf": "https://arxiv.org/pdf/2509.03020.pdf", "abs": "https://arxiv.org/abs/2509.03020", "title": "Training LLMs to be Better Text Embedders through Bidirectional Reconstruction", "authors": ["Chang Su", "Dengliang Shi", "Siyuan Huang", "Jintao Du", "Changhua Meng", "Yu Cheng", "Weiqiang Wang", "Zhouhan Lin"], "categories": ["cs.CL", "cs.IR"], "comment": "accepted by EMNLP 2025 Main Conference", "summary": "Large language models (LLMs) have increasingly been explored as powerful text\nembedders. Existing LLM-based text embedding approaches often leverage the\nembedding of the final token, typically a reserved special token such as [EOS].\nHowever, these tokens have not been intentionally trained to capture the\nsemantics of the whole context, limiting their capacity as text embeddings,\nespecially for retrieval and re-ranking tasks. We propose to add a new training\nstage before contrastive learning to enrich the semantics of the final token\nembedding. This stage employs bidirectional generative reconstruction tasks,\nnamely EBQ2D (Embedding-Based Query-to-Document) and EBD2Q (Embedding-Based\nDocument-to-Query), which interleave to anchor the [EOS] embedding and\nreconstruct either side of Query-Document pairs. Experimental results\ndemonstrate that our additional training stage significantly improves LLM\nperformance on the Massive Text Embedding Benchmark (MTEB), achieving new\nstate-of-the-art results across different LLM base models and scales."}
{"id": "2509.04373", "pdf": "https://arxiv.org/pdf/2509.04373.pdf", "abs": "https://arxiv.org/abs/2509.04373", "title": "Measuring Bias or Measuring the Task: Understanding the Brittle Nature of LLM Gender Biases", "authors": ["Bufan Gao", "Elisa Kreiss"], "categories": ["cs.CL"], "comment": "To be published at EMNLP 2025 (main conference)", "summary": "As LLMs are increasingly applied in socially impactful settings, concerns\nabout gender bias have prompted growing efforts both to measure and mitigate\nsuch bias. These efforts often rely on evaluation tasks that differ from\nnatural language distributions, as they typically involve carefully constructed\ntask prompts that overtly or covertly signal the presence of gender\nbias-related content. In this paper, we examine how signaling the evaluative\npurpose of a task impacts measured gender bias in LLMs.Concretely, we test\nmodels under prompt conditions that (1) make the testing context salient, and\n(2) make gender-focused content salient. We then assess prompt sensitivity\nacross four task formats with both token-probability and discrete-choice\nmetrics. We find that prompts that more clearly align with (gender bias)\nevaluation framing elicit distinct gender output distributions compared to less\nevaluation-framed prompts. Discrete-choice metrics further tend to amplify bias\nrelative to probabilistic measures. These findings do not only highlight the\nbrittleness of LLM gender bias evaluations but open a new puzzle for the NLP\nbenchmarking and development community: To what extent can well-controlled\ntesting designs trigger LLM \"testing mode\" performance, and what does this mean\nfor the ecological validity of future benchmarks."}
{"id": "2509.04518", "pdf": "https://arxiv.org/pdf/2509.04518.pdf", "abs": "https://arxiv.org/abs/2509.04518", "title": "Advancing SLM Tool-Use Capability using Reinforcement Learning", "authors": ["Dhruvi Paprunia", "Vansh Kharidia", "Pankti Doshi"], "categories": ["cs.CL"], "comment": null, "summary": "In an era where tool-augmented AI agents are becoming increasingly vital, our\nfindings highlight the ability of Group Relative Policy Optimization (GRPO) to\nempower SLMs, which are traditionally constrained in tool use. The ability to\nuse tools effectively has become a defining feature of Large Language Models\n(LLMs), allowing them to access external data and internal resources. As AI\nagents grow more sophisticated, tool-use capabilities have become\nindispensable. While LLMs have made significant progress in this area, Small\nLanguage Models (SLMs) still face challenges in accurately integrating tool\nuse, especially in resource-constrained settings.\n  This study investigates how Reinforcement Learning, specifically Group\nRelative Policy Optimization (GRPO), can enhance the tool-use accuracy of SLMs.\nBy designing a well-defined reward system that reinforces structured JSON\noutput, correct tool selection, and precise parameter usage, we demonstrate\nthat GRPO enables SLMs to achieve significant improvements in tool-use\ncapabilities (function calling/JSON output). Our approach provides a\ncomputationally efficient training method that enhances SLMs practical\ndeployment in real-world AI applications."}
{"id": "2509.04656", "pdf": "https://arxiv.org/pdf/2509.04656.pdf", "abs": "https://arxiv.org/abs/2509.04656", "title": "AraHalluEval: A Fine-grained Hallucination Evaluation Framework for Arabic LLMs", "authors": ["Aisha Alansari", "Hamzah Luqman"], "categories": ["cs.CL"], "comment": null, "summary": "Recently, extensive research on the hallucination of the large language\nmodels (LLMs) has mainly focused on the English language. Despite the growing\nnumber of multilingual and Arabic-specific LLMs, evaluating LLMs' hallucination\nin the Arabic context remains relatively underexplored. The knowledge gap is\nparticularly pressing given Arabic's widespread use across many regions and its\nimportance in global communication and media. This paper presents the first\ncomprehensive hallucination evaluation of Arabic and multilingual LLMs on two\ncritical Arabic natural language generation tasks: generative question\nanswering (GQA) and summarization. This study evaluates a total of 12 LLMs,\nincluding 4 Arabic pre-trained models, 4 multilingual models, and 4\nreasoning-based models. To assess the factual consistency and faithfulness of\nLLMs' outputs, we developed a fine-grained hallucination evaluation framework\nconsisting of 12 fine-grained hallucination indicators that represent the\nvarying characteristics of each task. The results reveal that factual\nhallucinations are more prevalent than faithfulness errors across all models\nand tasks. Notably, the Arabic pre-trained model Allam consistently\ndemonstrates lower hallucination rates than multilingual models and a\ncomparative performance with reasoning-based models. The code is available at:\nhttps://github.com/aishaalansari57/AraHalluEval"}
{"id": "2509.05209", "pdf": "https://arxiv.org/pdf/2509.05209.pdf", "abs": "https://arxiv.org/abs/2509.05209", "title": "Hunyuan-MT Technical Report", "authors": ["Mao Zheng", "Zheng Li", "Bingxin Qu", "Mingyang Song", "Yang Du", "Mingrui Sun", "Di Wang"], "categories": ["cs.CL"], "comment": null, "summary": "In this report, we introduce Hunyuan-MT-7B, our first open-source\nmultilingual translation model, which supports bidirectional translation across\n33 major languages and places a special emphasis on translation between\nMandarin and several ethnic minority languages as well as dialects.\nFurthermore, to serve and address diverse translation scenarios and enhance\nmodel performance at test time, we introduce Hunyuan-MT-Chimera-7B, a\ntranslation model inspired by the slow thinking mode. This model integrates\nmultiple outputs generated by the Hunyuan-MT-7B model under varying parameter\nsettings, thereby achieving performance superior to that of conventional\nslow-thinking models based on Chain-of-Thought (CoT). The development of our\nmodels follows a holistic training process specifically engineered for\nmultilingual translation, which begins with general and MT-oriented\npre-training to build foundational capabilities, proceeds to Supervised\nFine-Tuning (SFT) for task-specific adaptation, and culminates in advanced\nalignment through Reinforcement Learning (RL) and weak-to-strong RL. Through\ncomprehensive experimentation, we demonstrate that both Hunyuan-MT-7B and\nHunyuan-MT-Chimera-7B significantly outperform all translation-specific models\nof comparable parameter size and most of the SOTA large models, particularly on\nthe task of translation between Mandarin and minority languages as well as\ndialects. In the WMT2025 shared task (General Machine Translation), our models\ndemonstrate state-of-the-art performance, ranking first in 30 out of 31\nlanguage pairs. This result highlights the robustness of our models across a\ndiverse linguistic spectrum, encompassing high-resource languages such as\nChinese, English, and Japanese, as well as low-resource languages including\nCzech, Marathi, Estonian, and Icelandic."}
{"id": "2509.05602", "pdf": "https://arxiv.org/pdf/2509.05602.pdf", "abs": "https://arxiv.org/abs/2509.05602", "title": "Mitigating Spurious Correlations Between Question and Answer via Chain-of-Thought Correctness Perception Distillation", "authors": ["Hongyan Xie", "Yitong Yao", "Yikun Ban", "Zixuan Huang", "Deqing Wang", "Zhenhe Wu", "Haoxiang Su", "Chao Wang", "Shuangyong Song"], "categories": ["cs.CL"], "comment": "PrePrint", "summary": "Large language models (LLMs) excel at reasoning tasks but are expensive to\ndeploy. Thus small language models (SLMs) are fine-tuned on CoT data generated\nby LLMs to copy LLMs' abilities. However, these CoT data may include noisy\nrationales that either fail to substantiate the answers or contribute no\nadditional information to support answer prediction, which leads SLMs to\ncapture spurious correlations between questions and answers and compromise the\nquality of reasoning. In this work, we propose Chain-of-Thought Correctness\nPerception Distillation (CoPeD), which aims to improve the reasoning quality of\nthe student model from the perspectives of task setting and data utilization.\nFirstly, we introduce a correctness-aware task setting that encourages the\nstudent model to predict answers based on correct rationales and revise them\nwhen they are incorrect. This setting improves the faithfulness of reasoning\nand allows the model to learn from its mistakes. Then, we propose a\nCorrectness-Aware Weighted loss, which dynamically adjusts the contribution of\neach training instance based on the combined loss of the rationale and the\nanswer. This strategy encourages the model to focus more on samples where the\nrationale offers stronger support for the correct answer. Experiments have\nshown that CoPeD is effective on both in-distribution (IND) and\nout-of-distribution (OOD) benchmark reasoning datasets."}
{"id": "2509.05657", "pdf": "https://arxiv.org/pdf/2509.05657.pdf", "abs": "https://arxiv.org/abs/2509.05657", "title": "LM-Searcher: Cross-domain Neural Architecture Search with LLMs via Unified Numerical Encoding", "authors": ["Yuxuan Hu", "Jihao Liu", "Ke Wang", "Jinliang Zhen", "Weikang Shi", "Manyuan Zhang", "Qi Dou", "Rui Liu", "Aojun Zhou", "Hongsheng Li"], "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025 Main", "summary": "Recent progress in Large Language Models (LLMs) has opened new avenues for\nsolving complex optimization problems, including Neural Architecture Search\n(NAS). However, existing LLM-driven NAS approaches rely heavily on prompt\nengineering and domain-specific tuning, limiting their practicality and\nscalability across diverse tasks. In this work, we propose LM-Searcher, a novel\nframework that leverages LLMs for cross-domain neural architecture optimization\nwithout the need for extensive domain-specific adaptation. Central to our\napproach is NCode, a universal numerical string representation for neural\narchitectures, which enables cross-domain architecture encoding and search. We\nalso reformulate the NAS problem as a ranking task, training LLMs to select\nhigh-performing architectures from candidate pools using instruction-tuning\nsamples derived from a novel pruning-based subspace sampling strategy. Our\ncurated dataset, encompassing a wide range of architecture-performance pairs,\nencourages robust and transferable learning. Comprehensive experiments\ndemonstrate that LM-Searcher achieves competitive performance in both in-domain\n(e.g., CNNs for image classification) and out-of-domain (e.g., LoRA\nconfigurations for segmentation and generation) tasks, establishing a new\nparadigm for flexible and generalizable LLM-based architecture search. The\ndatasets and models will be released at https://github.com/Ashone3/LM-Searcher."}
{"id": "2509.06637", "pdf": "https://arxiv.org/pdf/2509.06637.pdf", "abs": "https://arxiv.org/abs/2509.06637", "title": "Modelling Intertextuality with N-gram Embeddings", "authors": ["Yi Xing"], "categories": ["cs.CL"], "comment": null, "summary": "Intertextuality is a central tenet in literary studies. It refers to the\nintricate links between literary texts that are created by various types of\nreferences. This paper proposes a new quantitative model of intertextuality to\nenable scalable analysis and network-based insights: perform pairwise\ncomparisons of the embeddings of n-grams from two texts and average their\nresults as the overall intertextuality. Validation on four texts with known\ndegrees of intertextuality, alongside a scalability test on 267 diverse texts,\ndemonstrates the method's effectiveness and efficiency. Network analysis\nfurther reveals centrality and community structures, affirming the approach's\nsuccess in capturing and quantifying intertextual relationships."}
{"id": "2405.15302", "pdf": "https://arxiv.org/pdf/2405.15302.pdf", "abs": "https://arxiv.org/abs/2405.15302", "title": "Understanding the Language Model to Solve the Symbolic Multi-Step Reasoning Problem from the Perspective of Buffer Mechanism", "authors": ["Zhiwei Wang", "Yunji Wang", "Zhongwang Zhang", "Zhangchen Zhou", "Hui Jin", "Tianyang Hu", "Jiacheng Sun", "Zhenguo Li", "Yaoyu Zhang", "Zhi-Qin John Xu"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Large language models have consistently struggled with complex reasoning\ntasks, such as mathematical problem-solving. Investigating the internal\nreasoning mechanisms of these models can help us design better model\narchitectures and training strategies, ultimately enhancing their reasoning\ncapability. In this study, we constructed a symbolic multi-step reasoning task\nto investigate the information propagation mechanisms in Transformer models\nwhen solving the task through direct answering and Chain-of-Thought (CoT)\nreasoning. We introduced the concept of buffer mechanism: the model stores\nvarious information in distinct buffers and selectively extracts it through the\nquery-key matrix. We proposed a random matrix-based algorithm to enhance the\nmodel's reasoning ability. This algorithm introduces only 132 trainable\nparameters, yet leads to significant performance improvements on 7 multi-step\nreasoning datasets, including PrOntoQA, LogicAsker, and LogicInference. These\nfindings provide new insights into understanding the large language models."}
{"id": "2406.13923", "pdf": "https://arxiv.org/pdf/2406.13923.pdf", "abs": "https://arxiv.org/abs/2406.13923", "title": "PIN: A Knowledge-Intensive Dataset for Paired and Interleaved Multimodal Documents", "authors": ["Junjie Wang", "Yuxiang Zhang", "Minghao Liu", "Yin Zhang", "Yatai Ji", "Weihao Xuan", "Nie Lin", "Kang Zhu", "Zhiqiang Lin", "Yiming Ren", "Chunyang Jiang", "Yiyao Yu", "Zekun Wang", "Tiezhen Wang", "Wenhao Huang", "Jie Fu", "Qunshu Lin", "Yujiu Yang", "Ge Zhang", "Ruibin Yuan", "Bei Chen", "Wenhu Chen"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.MM"], "comment": "Technical report v1.0", "summary": "Recent advancements in large multimodal models (LMMs) have leveraged\nextensive multimodal datasets to enhance capabilities in complex\nknowledge-driven tasks. However, persistent challenges in perceptual and\nreasoning errors limit their efficacy, particularly in interpreting intricate\nvisual data and deducing multimodal relationships. To address these issues, we\nintroduce PIN (Paired and INterleaved multimodal documents), a novel data\nformat designed to foster a deeper integration of visual and textual knowledge.\nThe PIN format uniquely combines semantically rich Markdown files, which\npreserve fine-grained textual structures, with holistic overall images that\ncapture the complete document layout. Following this format, we construct and\nrelease two large-scale, open-source datasets: PIN-200M (~200 million\ndocuments) and PIN-14M (~14 million), compiled from diverse web and scientific\nsources in both English and Chinese. To maximize usability, we provide detailed\nstatistical analyses and equip the datasets with quality signals, enabling\nresearchers to easily filter and select data for specific tasks. Our work\nprovides the community with a versatile data format and substantial resources,\noffering a foundation for new research in pre-training strategies and the\ndevelopment of more powerful knowledge-intensive LMMs."}
{"id": "2407.20454", "pdf": "https://arxiv.org/pdf/2407.20454.pdf", "abs": "https://arxiv.org/abs/2407.20454", "title": "CoMMIT: Coordinated Multimodal Instruction Tuning", "authors": ["Xintong Li", "Junda Wu", "Tong Yu", "Yu Wang", "Xiang Chen", "Jiuxiang Gu", "Lina Yao", "Julian McAuley", "Jingbo Shang"], "categories": ["cs.LG", "cs.CL"], "comment": "9 pages", "summary": "Instruction tuning in multimodal large language models (MLLMs) generally\ninvolves cooperative learning between a backbone LLM and a feature encoder of\nnon-text input modalities. The major challenge is how to efficiently find the\nsynergy between the two modules so that LLMs can adapt their reasoning\nabilities to downstream tasks while feature encoders can adjust to provide more\ntask-specific information about its modality. In this paper, we analyze the\nMLLM instruction tuning from both theoretical and empirical perspectives, where\nwe find the unbalanced learning between the feature encoder and the LLM can\ncause problems of oscillation and biased learning that lead to sub-optimal\nconvergence. Inspired by our findings, we propose a Multimodal Balance\nCoefficient that enables quantitative measurement of the balance of learning.\nBased on this, we further design a dynamic learning scheduler that better\ncoordinates the learning between the LLM and feature encoder, alleviating the\nproblems of oscillation and biased learning. In addition, we introduce an\nauxiliary regularization on the gradient to promote updating with larger step\nsizes, which potentially allows for a more accurate estimation of the proposed\nMultiModal Balance Coefficient and further improves the training sufficiency.\nOur proposed approach is agnostic to the architecture of LLM and feature\nencoder, so it can be generically integrated with various MLLMs. We conduct\nexperiments on multiple downstream tasks with various MLLMs, demonstrating that\nthe proposed method is more effective than the baselines in MLLM instruction\ntuning."}
{"id": "2412.01370", "pdf": "https://arxiv.org/pdf/2412.01370.pdf", "abs": "https://arxiv.org/abs/2412.01370", "title": "Understanding Museum Exhibits using Vision-Language Reasoning", "authors": ["Ada-Astrid Balauca", "Sanjana Garai", "Stefan Balauca", "Rasesh Udayakumar Shetty", "Naitik Agrawal", "Dhwanil Subhashbhai Shah", "Yuqian Fu", "Xi Wang", "Kristina Toutanova", "Danda Pani Paudel", "Luc Van Gool"], "categories": ["cs.CV", "cs.CL"], "comment": "Accepted at ICCV 2025", "summary": "Museums serve as repositories of cultural heritage and historical artifacts\nfrom diverse epochs, civilizations, and regions, preserving well-documented\ncollections that encapsulate vast knowledge, which, when systematically\nstructured into large-scale datasets, can train specialized models. Visitors\nengage with exhibits through curiosity and questions, making expert\ndomain-specific models essential for interactive query resolution and gaining\nhistorical insights. Understanding exhibits from images requires analyzing\nvisual features and linking them to historical knowledge to derive meaningful\ncorrelations. We facilitate such reasoning by (a) collecting and curating a\nlarge-scale dataset of 65M images and 200M question-answer pairs for exhibits\nfrom all around the world; (b) training large vision-language models (VLMs) on\nthe collected dataset; (c) benchmarking their ability on five visual question\nanswering tasks, specifically designed to reflect real-world inquiries and\nchallenges observed in museum settings. The complete dataset is labeled by\nmuseum experts, ensuring the quality and the practical significance of the\nlabels. We train two VLMs from different categories: BLIP with vision-language\naligned embeddings, but lacking the expressive power of large language models,\nand the LLaVA model, a powerful instruction-tuned LLM enriched with\nvision-language reasoning capabilities. Through extensive experiments, we find\nthat while both model types effectively answer visually grounded questions,\nlarge vision-language models excel in queries requiring deeper historical\ncontext and reasoning. We further demonstrate the necessity of fine-tuning\nmodels on large-scale domain-specific datasets by showing that our fine-tuned\nmodels significantly outperform current SOTA VLMs in answering questions\nrelated to specific attributes, highlighting their limitations in handling\ncomplex, nuanced queries."}
{"id": "2502.18536", "pdf": "https://arxiv.org/pdf/2502.18536.pdf", "abs": "https://arxiv.org/abs/2502.18536", "title": "FilterRAG: Zero-Shot Informed Retrieval-Augmented Generation to Mitigate Hallucinations in VQA", "authors": ["Nobin Sarwar"], "categories": ["cs.CV", "cs.CL", "cs.IR", "cs.LG"], "comment": "12 pages, 6 figures and 2 tables; Accepted at ICCV 2025 Workshop on\n  Building Foundation Models You Can Trust (T2FM)", "summary": "Visual Question Answering requires models to generate accurate answers by\nintegrating visual and textual understanding. However, VQA models still\nstruggle with hallucinations, producing convincing but incorrect answers,\nparticularly in knowledge-driven and Out-of-Distribution scenarios. We\nintroduce FilterRAG, a retrieval-augmented framework that combines BLIP-VQA\nwith Retrieval-Augmented Generation to ground answers in external knowledge\nsources like Wikipedia and DBpedia. FilterRAG achieves 36.5% accuracy on the\nOK-VQA dataset, demonstrating its effectiveness in reducing hallucinations and\nimproving robustness in both in-domain and Out-of-Distribution settings. These\nfindings highlight the potential of FilterRAG to improve Visual Question\nAnswering systems for real-world deployment."}
{"id": "2503.15552", "pdf": "https://arxiv.org/pdf/2503.15552.pdf", "abs": "https://arxiv.org/abs/2503.15552", "title": "Personalized Attacks of Social Engineering in Multi-turn Conversations: LLM Agents for Simulation and Detection", "authors": ["Tharindu Kumarage", "Cameron Johnson", "Jadie Adams", "Lin Ai", "Matthias Kirchner", "Anthony Hoogs", "Joshua Garland", "Julia Hirschberg", "Arslan Basharat", "Huan Liu"], "categories": ["cs.CR", "cs.CL"], "comment": "Accepted as a paper at COLM 2025 Workshop on AI Agents: Capabilities\n  and Safety", "summary": "The rapid advancement of conversational agents, particularly chatbots powered\nby Large Language Models (LLMs), poses a significant risk of social engineering\n(SE) attacks on social media platforms. SE detection in multi-turn, chat-based\ninteractions is considerably more complex than single-instance detection due to\nthe dynamic nature of these conversations. A critical factor in mitigating this\nthreat is understanding the SE attack mechanisms through which SE attacks\noperate, specifically how attackers exploit vulnerabilities and how victims'\npersonality traits contribute to their susceptibility. In this work, we propose\nan LLM-agentic framework, SE-VSim, to simulate SE attack mechanisms by\ngenerating multi-turn conversations. We model victim agents with varying\npersonality traits to assess how psychological profiles influence\nsusceptibility to manipulation. Using a dataset of over 1000 simulated\nconversations, we examine attack scenarios in which adversaries, posing as\nrecruiters, funding agencies, and journalists, attempt to extract sensitive\ninformation. Based on this analysis, we present a proof of concept,\nSE-OmniGuard, to offer personalized protection to users by leveraging prior\nknowledge of the victims personality, evaluating attack strategies, and\nmonitoring information exchanges in conversations to identify potential SE\nattempts."}
{"id": "2503.16833", "pdf": "https://arxiv.org/pdf/2503.16833.pdf", "abs": "https://arxiv.org/abs/2503.16833", "title": "The Model Hears You: Audio Language Model Deployments Should Consider the Principle of Least Privilege", "authors": ["Luxi He", "Xiangyu Qi", "Michel Liao", "Inyoung Cheong", "Prateek Mittal", "Danqi Chen", "Peter Henderson"], "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.CY", "eess.AS"], "comment": "Published at AIES 2025", "summary": "The latest Audio Language Models (Audio LMs) process speech directly instead\nof relying on a separate transcription step. This shift preserves detailed\ninformation, such as intonation or the presence of multiple speakers, that\nwould otherwise be lost in transcription. However, it also introduces new\nsafety risks, including the potential misuse of speaker identity cues and other\nsensitive vocal attributes, which could have legal implications. In this paper,\nwe urge a closer examination of how these models are built and deployed. Our\nexperiments show that end-to-end modeling, compared with cascaded pipelines,\ncreates socio-technical safety risks such as identity inference, biased\ndecision-making, and emotion detection. This raises concerns about whether\nAudio LMs store voiceprints and function in ways that create uncertainty under\nexisting legal regimes. We then argue that the Principle of Least Privilege\nshould be considered to guide the development and deployment of these models.\nSpecifically, evaluations should assess (1) the privacy and safety risks\nassociated with end-to-end modeling; and (2) the appropriate scope of\ninformation access. Finally, we highlight related gaps in current audio LM\nbenchmarks and identify key open research questions, both technical and\npolicy-related, that must be addressed to enable the responsible deployment of\nend-to-end Audio LMs."}
{"id": "2505.12312", "pdf": "https://arxiv.org/pdf/2505.12312.pdf", "abs": "https://arxiv.org/abs/2505.12312", "title": "Visuospatial Cognitive Assistant", "authors": ["Qi Feng"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "comment": "31 pages, 10 figures, 6 tables", "summary": "Video-based spatial cognition is vital for robotics and embodied AI but\nchallenges current Vision-Language Models (VLMs). This paper makes two key\ncontributions. First, we introduce ViCA (Visuospatial Cognitive\nAssistant)-322K, a diverse dataset of 322,003 QA pairs from real-world indoor\nvideos (ARKitScenes, ScanNet, ScanNet++), offering supervision for 3D\nmetadata-grounded queries and video-based complex reasoning. Second, we develop\nViCA-7B, fine-tuned on ViCA-322K, which achieves new state-of-the-art on all\neight VSI-Bench tasks, outperforming existing models, including larger ones\n(e.g., +26.1 on Absolute Distance). For interpretability, we present\nViCA-Thinking-2.68K, a dataset with explicit reasoning chains, and fine-tune\nViCA-7B to create ViCA-7B-Thinking, a model that articulates its spatial\nreasoning. Our work highlights the importance of targeted data and suggests\npaths for improved temporal-spatial modeling. We release all resources to\nfoster research in robust visuospatial intelligence."}
{"id": "2505.12363", "pdf": "https://arxiv.org/pdf/2505.12363.pdf", "abs": "https://arxiv.org/abs/2505.12363", "title": "Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts", "authors": ["Qi Feng"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "comment": "26 pages, 19 figures, 4 tables", "summary": "While Multimodal Large Language Models (MLLMs) excel at general\nvision-language tasks, visuospatial cognition - reasoning about spatial\nlayouts, relations, and dynamics - remains a significant challenge. Existing\nmodels often lack the necessary architectural components and specialized\ntraining data for fine-grained spatial understanding. We introduce ViCA2\n(Visuospatial Cognitive Assistant 2), a novel MLLM designed to enhance spatial\nreasoning. ViCA2 features a dual vision encoder architecture integrating SigLIP\nfor semantics and Hiera for spatial structure, coupled with a token ratio\ncontrol mechanism for efficiency. We also developed ViCA-322K, a new\nlarge-scale dataset with over 322,000 spatially grounded question-answer pairs\nfor targeted instruction tuning. On the challenging VSI-Bench benchmark, our\nViCA2-7B model achieves a state-of-the-art average score of 56.8, significantly\nsurpassing larger open-source models (e.g., LLaVA-NeXT-Video-72B, 40.9) and\nleading proprietary models (Gemini-1.5 Pro, 45.4). This demonstrates the\neffectiveness of our approach in achieving strong visuospatial intelligence\nwith a compact model. We release ViCA2, its codebase, and the ViCA-322K dataset\nto facilitate further research."}
{"id": "2507.04431", "pdf": "https://arxiv.org/pdf/2507.04431.pdf", "abs": "https://arxiv.org/abs/2507.04431", "title": "MedGellan: LLM-Generated Medical Guidance to Support Physicians", "authors": ["Debodeep Banerjee", "Burcu Sayin", "Stefano Teso", "Andrea Passerini"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Medical decision-making is a critical task, where errors can result in\nserious, potentially life-threatening consequences. While full automation\nremains challenging, hybrid frameworks that combine machine intelligence with\nhuman oversight offer a practical alternative. In this paper, we present\nMedGellan, a lightweight, annotation-free framework that uses a Large Language\nModel (LLM) to generate clinical guidance from raw medical records, which is\nthen used by a physician to predict diagnoses. MedGellan uses a\nBayesian-inspired prompting strategy that respects the temporal order of\nclinical data. Preliminary experiments show that the guidance generated by the\nLLM with MedGellan improves diagnostic performance, particularly in recall and\n$F_1$ score."}
{"id": "2508.06401", "pdf": "https://arxiv.org/pdf/2508.06401.pdf", "abs": "https://arxiv.org/abs/2508.06401", "title": "A Systematic Literature Review of Retrieval-Augmented Generation: Techniques, Metrics, and Challenges", "authors": ["Andrew Brown", "Muhammad Roman", "Barry Devereux"], "categories": ["cs.DL", "cs.AI", "cs.CL", "cs.IR"], "comment": "58 page", "summary": "This systematic review of the research literature on retrieval-augmented\ngeneration (RAG) provides a focused analysis of the most highly cited studies\npublished between 2020 and May 2025. A total of 128 articles met our inclusion\ncriteria. The records were retrieved from ACM Digital Library, IEEE Xplore,\nScopus, ScienceDirect, and the Digital Bibliography and Library Project (DBLP).\nRAG couples a neural retriever with a generative language model, grounding\noutput in up-to-date, non-parametric memory while retaining the semantic\ngeneralisation stored in model weights. Guided by the PRISMA 2020 framework, we\n(i) specify explicit inclusion and exclusion criteria based on citation count\nand research questions, (ii) catalogue datasets, architectures, and evaluation\npractices, and (iii) synthesise empirical evidence on the effectiveness and\nlimitations of RAG. To mitigate citation-lag bias, we applied a lower\ncitation-count threshold to papers published in 2025 so that emerging\nbreakthroughs with naturally fewer citations were still captured. This review\nclarifies the current research landscape, highlights methodological gaps, and\ncharts priority directions for future research."}
{"id": "2508.07353", "pdf": "https://arxiv.org/pdf/2508.07353.pdf", "abs": "https://arxiv.org/abs/2508.07353", "title": "Benchmarking for Domain-Specific LLMs: A Case Study on Academia and Beyond", "authors": ["Rubing Chen", "Jiaxin Wu", "Jian Wang", "Xulu Zhang", "Wenqi Fan", "Chenghua Lin", "Xiao-Yong Wei", "Qing Li"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted by EMNLP2025 Findings", "summary": "The increasing demand for domain-specific evaluation of large language models\n(LLMs) has led to the development of numerous benchmarks. These efforts often\nadhere to the principle of data scaling, relying on large corpora or extensive\nquestion-answer (QA) sets to ensure broad coverage. However, the impact of\ncorpus and QA set design on the precision and recall of domain-specific LLM\nperformance remains poorly understood. In this paper, we argue that data\nscaling is not always the optimal principle for domain-specific benchmark\nconstruction. Instead, we introduce Comp-Comp, an iterative benchmarking\nframework grounded in the principle of comprehensiveness and compactness.\nComprehensiveness ensures semantic recall by covering the full breadth of the\ndomain, while compactness improves precision by reducing redundancy and noise.\nTo demonstrate the effectiveness of our approach, we present a case study\nconducted at a well-renowned university, resulting in the creation of\nPolyBench, a large-scale, high-quality academic benchmark. Although this study\nfocuses on academia, the Comp-Comp framework is domain-agnostic and readily\nadaptable to a wide range of specialized fields. The source code and datasets\ncan be accessed at https://github.com/Anya-RB-Chen/COMP-COMP."}
{"id": "2508.19990", "pdf": "https://arxiv.org/pdf/2508.19990.pdf", "abs": "https://arxiv.org/abs/2508.19990", "title": "Heterogeneous Self-Supervised Acoustic Pre-Training with Local Constraints", "authors": ["Xiaodong Cui", "A F M Saif", "Brian Kingsbury", "Tianyi Chen"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Self-supervised pre-training using unlabeled data is widely used in automatic\nspeech recognition. In this paper, we propose a new self-supervised\npre-training approach to dealing with heterogeneous data. Instead of mixing all\nthe data and minimizing the averaged global loss in the conventional way, we\nimpose additional local constraints to ensure that the model optimizes each\nsource of heterogeneous data to its local optimum after $K$-step gradient\ndescent initialized from the model. We formulate this as a bilevel optimization\nproblem, and use the first-order approximation method to solve the problem. We\ndiscuss its connection to model-agnostic meta learning. Experiments are carried\nout on self-supervised pre-training using multi-domain and multilingual\ndatasets, demonstrating that the proposed approach can significantly improve\nthe adaptivity of the self-supervised pre-trained model for the downstream\nsupervised fine-tuning tasks."}
{"id": "2509.01907", "pdf": "https://arxiv.org/pdf/2509.01907.pdf", "abs": "https://arxiv.org/abs/2509.01907", "title": "RSCC: A Large-Scale Remote Sensing Change Caption Dataset for Disaster Events", "authors": ["Zhenyuan Chen", "Chenxi Wang", "Ningyu Zhang", "Feng Zhang"], "categories": ["cs.CV", "cs.CL"], "comment": "under review", "summary": "Remote sensing is critical for disaster monitoring, yet existing datasets\nlack temporal image pairs and detailed textual annotations. While\nsingle-snapshot imagery dominates current resources, it fails to capture\ndynamic disaster impacts over time. To address this gap, we introduce the\nRemote Sensing Change Caption (RSCC) dataset, a large-scale benchmark\ncomprising 62,315 pre-/post-disaster image pairs (spanning earthquakes, floods,\nwildfires, and more) paired with rich, human-like change captions. By bridging\nthe temporal and semantic divide in remote sensing data, RSCC enables robust\ntraining and evaluation of vision-language models for disaster-aware\nbi-temporal understanding. Our results highlight RSCC's ability to facilitate\ndetailed disaster-related analysis, paving the way for more accurate,\ninterpretable, and scalable vision-language applications in remote sensing.\nCode and dataset are available at https://github.com/Bili-Sakura/RSCC."}
{"id": "2509.04404", "pdf": "https://arxiv.org/pdf/2509.04404.pdf", "abs": "https://arxiv.org/abs/2509.04404", "title": "No Thoughts Just AI: Biased LLM Hiring Recommendations Alter Human Decision Making and Limit Human Autonomy", "authors": ["Kyra Wilson", "Mattea Sim", "Anna-Maria Gueorguieva", "Aylin Caliskan"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC", "K.4.2"], "comment": "Published in Proceedings of the 2025 AAAI/ACM Conference on AI,\n  Ethics, and Society; code available at\n  https://github.com/kyrawilson/No-Thoughts-Just-AI", "summary": "In this study, we conduct a resume-screening experiment (N=528) where people\ncollaborate with simulated AI models exhibiting race-based preferences (bias)\nto evaluate candidates for 16 high and low status occupations. Simulated AI\nbias approximates factual and counterfactual estimates of racial bias in\nreal-world AI systems. We investigate people's preferences for White, Black,\nHispanic, and Asian candidates (represented through names and affinity groups\non quality-controlled resumes) across 1,526 scenarios and measure their\nunconscious associations between race and status using implicit association\ntests (IATs), which predict discriminatory hiring decisions but have not been\ninvestigated in human-AI collaboration. When making decisions without AI or\nwith AI that exhibits no race-based preferences, people select all candidates\nat equal rates. However, when interacting with AI favoring a particular group,\npeople also favor those candidates up to 90% of the time, indicating a\nsignificant behavioral shift. The likelihood of selecting candidates whose\nidentities do not align with common race-status stereotypes can increase by 13%\nif people complete an IAT before conducting resume screening. Finally, even if\npeople think AI recommendations are low quality or not important, their\ndecisions are still vulnerable to AI bias under certain circumstances. This\nwork has implications for people's autonomy in AI-HITL scenarios, AI and work,\ndesign and evaluation of AI hiring systems, and strategies for mitigating bias\nin collaborative decision-making tasks. In particular, organizational and\nregulatory policy should acknowledge the complex nature of AI-HITL decision\nmaking when implementing these systems, educating people who use them, and\ndetermining which are subject to oversight."}
{"id": "2509.06283", "pdf": "https://arxiv.org/pdf/2509.06283.pdf", "abs": "https://arxiv.org/abs/2509.06283", "title": "SFR-DeepResearch: Towards Effective Reinforcement Learning for Autonomously Reasoning Single Agents", "authors": ["Xuan-Phi Nguyen", "Shrey Pandit", "Revanth Gangi Reddy", "Austin Xu", "Silvio Savarese", "Caiming Xiong", "Shafiq Joty"], "categories": ["cs.AI", "cs.CL"], "comment": "Technical Report", "summary": "Equipping large language models (LLMs) with complex, interleaved reasoning\nand tool-use capabilities has become a key focus in agentic AI research,\nespecially with recent advances in reasoning-oriented (``thinking'') models.\nSuch capabilities are key to unlocking a number of important applications. One\nsuch application is Deep Research (DR), which requires extensive search and\nreasoning over many sources. Our work in this paper focuses on the development\nof native Autonomous Single-Agent models for DR featuring minimal web crawling\nand Python tool integration. Unlike multi-agent systems, where agents take up\npre-defined roles and are told what to do at each step in a static workflow, an\nautonomous single-agent determines its next action dynamically based on\ncontext, without manual directive. While prior work has proposed training\nrecipes for base or instruction-tuned LLMs, we focus on continual reinforcement\nlearning (RL) of reasoning-optimized models to further enhance agentic skills\nwhile preserving reasoning ability. Towards this end, we propose a simple RL\nrecipe with entirely synthetic data, which we apply to various open-source\nLLMs. Our best variant SFR-DR-20B achieves up to 28.7% on Humanity's Last Exam\nbenchmark. In addition, we conduct key analysis experiments to provide more\ninsights into our methodologies."}
{"id": "2509.06945", "pdf": "https://arxiv.org/pdf/2509.06945.pdf", "abs": "https://arxiv.org/abs/2509.06945", "title": "Interleaving Reasoning for Better Text-to-Image Generation", "authors": ["Wenxuan Huang", "Shuang Chen", "Zheyong Xie", "Shaosheng Cao", "Shixiang Tang", "Yufan Shen", "Qingyu Yin", "Wenbo Hu", "Xiaoman Wang", "Yuntian Tang", "Junbo Qiao", "Yue Guo", "Yao Hu", "Zhenfei Yin", "Philip Torr", "Yu Cheng", "Wanli Ouyang", "Shaohui Lin"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Unified multimodal understanding and generation models recently have achieve\nsignificant improvement in image generation capability, yet a large gap remains\nin instruction following and detail preservation compared to systems that\ntightly couple comprehension with generation such as GPT-4o. Motivated by\nrecent advances in interleaving reasoning, we explore whether such reasoning\ncan further improve Text-to-Image (T2I) generation. We introduce Interleaving\nReasoning Generation (IRG), a framework that alternates between text-based\nthinking and image synthesis: the model first produces a text-based thinking to\nguide an initial image, then reflects on the result to refine fine-grained\ndetails, visual quality, and aesthetics while preserving semantics. To train\nIRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL),\nwhich targets two sub-goals: (1) strengthening the initial think-and-generate\nstage to establish core content and base quality, and (2) enabling high-quality\ntextual reflection and faithful implementation of those refinements in a\nsubsequent image. We curate IRGL-300K, a dataset organized into six decomposed\nlearning modes that jointly cover learning text-based thinking, and full\nthinking-image trajectories. Starting from a unified foundation model that\nnatively emits interleaved text-image outputs, our two-stage training first\nbuilds robust thinking and reflection, then efficiently tunes the IRG pipeline\nin the full thinking-image trajectory data. Extensive experiments show SoTA\nperformance, yielding absolute gains of 5-10 points on GenEval, WISE, TIIF,\nGenAI-Bench, and OneIG-EN, alongside substantial improvements in visual quality\nand fine-grained fidelity. The code, model weights and datasets will be\nreleased in: https://github.com/Osilly/Interleaving-Reasoning-Generation ."}
