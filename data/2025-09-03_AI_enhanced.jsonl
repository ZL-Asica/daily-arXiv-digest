{"id": "2509.00352", "pdf": "https://arxiv.org/pdf/2509.00352.pdf", "abs": "https://arxiv.org/abs/2509.00352", "title": "Unlocking Mixed Reality for Medical Education: A See-Through Perspective on Head Anatomy", "authors": ["Yuqing Wei", "Yupeng Wang", "Jiayi Zhao", "Yanjun Liu", "Huxin Gao", "Jiewen Lai"], "categories": ["cs.HC"], "comment": null, "summary": "Extended reality (XR), encompassing Virtual Reality (VR), Augmented Reality\n(AR), and Mixed Reality (MR), is emerging as a transformative platform for\nmedical education. Traditional methods such as textbooks, physical models, and\ncadaveric dissections often lack interactivity and fail to convey complex\nspatial relationships effectively. The emerging MR technology addresses these\nlimitations by providing immersive environments that blend virtual elements\nwith real-world contexts. This study presents an MR application for head\nanatomy education, enabling learners to intuitively interact with see-through\n3D anatomical structures via hand gestures and controllers. Our hierarchical\ninformation design supports progressive learning, guiding users from basic\nanatomical labels to detailed structural insights. Additionally, the system\nincorporates an automatic calibration module that aligns virtual anatomical\nmodels with a real human head, thereby facilitating realistic human-model\ninteractions. Experiments show that the system can effectively match the\nanatomical model with real-time scenes, thus enhancing the interactivity and\nimmersion of medical education, providing an innovative tool for teaching\nanatomy.", "AI": {"tldr": "This paper presents a mixed reality application for head anatomy education that enhances interactivity and immersion for medical learners.", "motivation": "Traditional medical education methods often lack interactivity and struggle to convey complex spatial relationships, necessitating innovative solutions like mixed reality.", "method": "Development of a mixed reality application that allows users to interact with 3D anatomical structures using hand gestures and controllers, supported by a hierarchical information design and automatic calibration module.", "result": "Experiments indicate that the mixed reality system effectively matches virtual anatomical models with real-time scenes, significantly improving the interactive learning experience.", "conclusion": "The proposed mixed reality tool serves as an innovative method for teaching head anatomy, enhancing the learning experience in medical education.", "key_contributions": ["Introduction of a mixed reality application for anatomy education.", "Implementation of intuitive interactions using hand gestures and controllers.", "Development of an automatic calibration module for real human interactions."], "limitations": "", "keywords": ["Mixed Reality", "Anatomy Education", "Interactive Learning", "Medical Education", "3D Models"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.00440", "pdf": "https://arxiv.org/pdf/2509.00440.pdf", "abs": "https://arxiv.org/abs/2509.00440", "title": "Data Humanism Decoded: A Characterization of its Principles to Bridge Data Visualization Researchers and Practitioners", "authors": ["Ibrahim Al-Hazwani", "Ke Er Zhang", "Laura Garrison", "Jürgen Bernard"], "categories": ["cs.HC"], "comment": "5 pages, to be feature in the proceedings of IEEE VIS Short paper\n  track", "summary": "Data Humanism is a human-centered design approach that emphasizes the\npersonal, contextual, and imperfect nature of data. Despite its growing\ninfluence among practitioners, the 13 principles outlined in Giorgia Lupi's\nvisual manifesto remain loosely defined in research contexts, creating a gap\nbetween design practice and systematic application. Through a mixed-methods\napproach, including a systematic literature review, multimedia analysis, and\nexpert interviews, we present a characterization of Data Humanism principles\nfor visualization researchers. Our characterization provides concrete\ndefinitions that maintain interpretive flexibility in operationalizing design\nchoices. We validate our work through direct consultation with Lupi. Moreover,\nwe leverage the characterization to decode a visualization work, mapping Data\nHumanism principles to specific visual design choices. Our work creates a\ncommon language for human-centered visualization, bridging the gap between\npractice and research for future applications and evaluations.", "AI": {"tldr": "This paper characterizes Data Humanism principles for visualization researchers, bridging the gap between design practice and research.", "motivation": "To provide concrete definitions of Data Humanism principles and create a common language for human-centered visualization.", "method": "A mixed-methods approach involving a systematic literature review, multimedia analysis, and expert interviews.", "result": "The work offers a characterization of Data Humanism principles and maps these principles to specific visual design choices in a visualization work.", "conclusion": "The characterization promotes systematic application of Data Humanism in visualization research, facilitating better communication between practice and research.", "key_contributions": ["Characterization of Data Humanism principles for visualization researchers.", "Concrete definitions to operationalize design choices.", "Mapping principles to specific visual design choices."], "limitations": "", "keywords": ["Data Humanism", "Human-Centered Design", "Visualization"], "importance_score": 5, "read_time_minutes": 5}}
{"id": "2509.00572", "pdf": "https://arxiv.org/pdf/2509.00572.pdf", "abs": "https://arxiv.org/abs/2509.00572", "title": "How to Make Museums More Interactive? Case Study of Artistic Chatbot", "authors": ["Filip J. Kucia", "Bartosz Grabek", "Szymon D. Trochimiak", "Anna Wróblewska"], "categories": ["cs.HC", "cs.IR"], "comment": "7 pages, 3 figures", "summary": "Conversational agents powered by Large Language Models (LLMs) are\nincreasingly utilized in educational settings, in particular in individual\nclosed digital environments, yet their potential adoption in the physical\nlearning environments like cultural heritage sites, museums, and art galleries\nremains relatively unexplored. In this study, we present Artistic Chatbot, a\nvoice-to-voice RAG-powered chat system to support informal learning and enhance\nvisitor engagement during a live art exhibition celebrating the 15th\nanniversary of the Faculty of Media Art at the Warsaw Academy of Fine Arts,\nPoland. The question answering (QA) chatbot responded to free-form spoken\nquestions in Polish using the context retrieved from a curated, domain-specific\nknowledge base consisting of 226 documents provided by the organizers,\nincluding faculty information, art magazines, books, and journals. We describe\nthe key aspects of the system architecture and user interaction design, as well\nas discuss the practical challenges associated with deploying chatbots at\npublic cultural sites. Our findings, based on interaction analysis, demonstrate\nthat chatbots such as Artistic Chatbot effectively maintain responses grounded\nin exhibition content (60\\% of responses directly relevant), even when faced\nwith unpredictable queries outside the target domain, showing their potential\nfor increasing interactivity in public cultural sites.\n  GitHub project page: https://github.com/cinekucia/artistic-chatbot-cikm2025", "AI": {"tldr": "Artistic Chatbot is a voice-responsive RAG-powered system designed to enhance visitor engagement in cultural heritage sites during art exhibitions.", "motivation": "To explore the potential for conversational agents powered by LLMs in physical learning environments like museums and cultural sites, where their use has been largely unexplored.", "method": "Developed a QA chatbot that responds to spoken questions in Polish, utilizing a domain-specific knowledge base of 226 documents relevant to the exhibition.", "result": "The chatbot maintained a 60% relevance in responses to unpredictable queries, demonstrating its effectiveness in enhancing interactivity at public cultural sites.", "conclusion": "Artistic Chatbot shows promise for improving visitor engagement and learning at cultural exhibitions, despite challenges in deploying such technologies in public spaces.", "key_contributions": ["Introduction of a RAG-powered chatbot in a live exhibition context.", "Documented insights into user interaction and system architecture.", "Identification of practical challenges for chatbot deployment in cultural heritage sites."], "limitations": "The chatbot primarily focuses on a specific exhibition context, which may limit its generalizability to other domains.", "keywords": ["Conversational agents", "Large Language Models", "Cultural heritage", "Visitor engagement", "Chatbot", "Informal learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.00696", "pdf": "https://arxiv.org/pdf/2509.00696.pdf", "abs": "https://arxiv.org/abs/2509.00696", "title": "Queuing for Civility: Regulating Emotions and Reducing Toxicity in Digital Discourse", "authors": ["Akriti Verma", "Shama Islam", "Valeh Moghaddam", "Adnan Anwar"], "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.LG", "cs.SI"], "comment": null, "summary": "The pervasiveness of online toxicity, including hate speech and trolling,\ndisrupts digital interactions and online well-being. Previous research has\nmainly focused on post-hoc moderation, overlooking the real-time emotional\ndynamics of online conversations and the impact of users' emotions on others.\nThis paper presents a graph-based framework to identify the need for emotion\nregulation within online conversations. This framework promotes self-reflection\nto manage emotional responses and encourage responsible behaviour in real time.\nAdditionally, a comment queuing mechanism is proposed to address intentional\ntrolls who exploit emotions to inflame conversations. This mechanism introduces\na delay in publishing comments, giving users time to self-regulate before\nfurther engaging in the conversation and helping maintain emotional balance.\nAnalysis of social media data from Twitter and Reddit demonstrates that the\ngraph-based framework reduced toxicity by 12%, while the comment queuing\nmechanism decreased the spread of anger by 15%, with only 4% of comments being\ntemporarily held on average. These findings indicate that combining real-time\nemotion regulation with delayed moderation can significantly improve well-being\nin online environments.", "AI": {"tldr": "This paper introduces a graph-based framework for real-time emotion regulation in online conversations to mitigate toxicity and improve user well-being.", "motivation": "Addressing the issue of online toxicity, which disrupts digital interactions and well-being, by focusing on real-time emotional dynamics and their influence in conversations.", "method": "A graph-based framework is developed to identify the need for emotion regulation, accompanied by a comment queuing mechanism introducing delays in comment publishing to allow users to self-reflect and manage emotional responses.", "result": "Analysis of social media data showed a 12% reduction in toxicity and a 15% decrease in the spread of anger due to the proposed methods, with an average of 4% of comments held for review.", "conclusion": "Implementing real-time emotion regulation combined with delayed moderation can significantly enhance emotional balance and well-being in online interactions.", "key_contributions": ["A novel graph-based framework for identifying emotion regulation needs in conversations.", "A comment queuing mechanism that delays the publication of comments to promote self-regulation.", "Empirical evidence showing significant reduction in online toxicity and anger spread."], "limitations": "", "keywords": ["online toxicity", "emotion regulation", "real-time moderation", "HCI", "social media"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2509.00030", "pdf": "https://arxiv.org/pdf/2509.00030.pdf", "abs": "https://arxiv.org/abs/2509.00030", "title": "MultiStream-LLM: Bridging Modalities for Robust Sign Language Translation", "authors": ["Marshall Thomas", "Edward Fish", "Richard Bowden"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Despite progress in gloss-free Sign Language Translation (SLT), monolithic\nend-to-end models consistently fail on two critical components of natural\nsigning: the precise recognition of high-speed fingerspelling and the\nintegration of asynchronous non-manual cues from the face. Recent progress in\nAutomated Sign Language Translation with Large Language Models has side stepped\nthis challenge, forcing a single network to learn these simultaneously\nresulting in poor performance when tasked with translating crucial information\nsuch as names,places, and technical terms. We introduce MultiStream-LLM, a\nmodular framework designed to overcome these limitations. Our approach employs\nseparate, specialized predictors for continuous signing, fingerspelling, and\nlipreading. Each expert network first decodes its specific modality into a\nsequence of tokens. These parallel streams are then fused by a lightweight\ntransformer that resolves temporal misalignments before passing the combined\nrepresentation to a Large Language Model (LLM) for final sentence generation.\nOur method establishes a new state-of-the-art on the How2Sign benchmark with a\nBLEU-4 score of 23.5 and achieves 73.2% letter accuracy on the challenging\nChicagoFSWildPlus fingerspelling dataset. These results validate our core\nhypothesis: by isolating and solving distinct recogni tion tasks before fusion,\nour multi-expert approach provides a more powerful and effective pathway to\nrobust, high-fidelity sign language translation.", "AI": {"tldr": "The paper introduces MultiStream-LLM, a modular framework for Sign Language Translation (SLT) that improves recognition of fingerspelling and integrates non-manual cues through specialized predictors.", "motivation": "To address the shortcomings of monolithic end-to-end models in recognizing high-speed fingerspelling and integrating facial cues in Sign Language Translation.", "method": "The framework uses separate expert networks for continuous signing, fingerspelling, and lipreading, with a lightweight transformer to fuse the outputs before passing to a Large Language Model for final sentence generation.", "result": "Achieved state-of-the-art results on the How2Sign benchmark with a BLEU-4 score of 23.5 and 73.2% letter accuracy on the ChicagoFSWildPlus dataset.", "conclusion": "Isolating and solving distinct recognition tasks before fusion enhances performance in sign language translation.", "key_contributions": ["Introduction of MultiStream-LLM framework for SLT", "Use of modular predictors for specialized tasks", "Improved performance metrics on established benchmarks"], "limitations": "", "keywords": ["Sign Language Translation", "Large Language Models", "Machine Learning", "Human-Computer Interaction"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2509.00852", "pdf": "https://arxiv.org/pdf/2509.00852.pdf", "abs": "https://arxiv.org/abs/2509.00852", "title": "Why it is worth making an effort with GenAI", "authors": ["Yvonne Rogers"], "categories": ["cs.HC", "cs.AI", "I.2; J.5"], "comment": "6 pages", "summary": "Students routinely use ChatGPT and the like now to help them with their\nhomework, such as writing an essay. It takes less effort to complete and is\neasier to do than by hand. It can even produce as good if not better output\nthan the student's own work. However, there is a growing concern that\nover-reliance on using GenAI in this way will stifle the development of\nlearning writing and critical thinking skills. How might this trend be\nreversed? What if students were required to make more effort when using GenAI\nto do their homework? It might be more challenging, but the additional effort\ninvolved could result in them learning more and having a greater sense of\nachievement. This tension can be viewed as a form of effort paradox; where\neffort is both viewed as something to be avoided but at the same time is\nvalued. Is it possible to let students learn sometimes with less and other\ntimes more effort? Students are already adept at the former but what about the\nlatter? Could we design new kinds of AI tools that deliberately require more\neffort to use to deepen the learning experience? In this paper, I begin to\noutline what form these might take, for example, asking students to use a\ncombination of GenAI tools with traditional learning approaches (e.g.\nnote-taking while reading). I also discuss how else to design tools to think\nwith that augments human cognition; where students learn more the skills of\nmetacognition and reflection.", "AI": {"tldr": "The paper discusses the challenge of over-reliance on Generative AI (GenAI) tools for homework among students and proposes ways to encourage deeper learning by requiring more effort in their use.", "motivation": "There is concern that the use of GenAI tools like ChatGPT may hinder the development of students' writing and critical thinking skills due to ease of use and high-quality output.", "method": "The paper outlines potential methods for requiring additional effort from students when using GenAI, such as integrating traditional learning techniques and designing tools that promote metacognition.", "result": "The proposed approach aims to balance ease of use with the need for effort, potentially leading to improved learning outcomes for students as they engage with hybrid learning methods.", "conclusion": "By designing AI tools that require more engagement and effort, students may develop better writing and critical thinking skills, thereby enhancing their learning experience.", "key_contributions": ["Proposes new AI tool designs that require increased effort to use", "Suggests combining GenAI with traditional learning methods to facilitate deeper learning", "Explores the concept of metacognition in the context of using GenAI for education"], "limitations": "", "keywords": ["Generative AI", "education", "metacognition", "AI tools", "learning effort"], "importance_score": 6, "read_time_minutes": 6}}
{"id": "2509.00038", "pdf": "https://arxiv.org/pdf/2509.00038.pdf", "abs": "https://arxiv.org/abs/2509.00038", "title": "Compiling Prompts, Not Crafting Them: A Reproducible Workflow for AI-Assisted Evidence Synthesis", "authors": ["Teo Susnjak"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) offer significant potential to accelerate\nsystematic literature reviews (SLRs), yet current approaches often rely on\nbrittle, manually crafted prompts that compromise reliability and\nreproducibility. This fragility undermines scientific confidence in\nLLM-assisted evidence synthesis. In response, this work adapts recent advances\nin declarative prompt optimisation, developed for general-purpose LLM\napplications, and demonstrates their applicability to the domain of SLR\nautomation. This research proposes a structured, domain-specific framework that\nembeds task declarations, test suites, and automated prompt tuning into a\nreproducible SLR workflow. These emerging methods are translated into a\nconcrete blueprint with working code examples, enabling researchers to\nconstruct verifiable LLM pipelines that align with established principles of\ntransparency and rigour in evidence synthesis. This is a novel application of\nsuch approaches to SLR pipelines.", "AI": {"tldr": "This paper proposes a structured framework for automating systematic literature reviews (SLRs) using large language models (LLMs), addressing issues of reliability and reproducibility in current methodologies.", "motivation": "To enhance the reliability and reproducibility of LLM-assisted evidence synthesis in systematic literature reviews (SLRs), which is currently undermined by brittle prompt designs.", "method": "The research adapts techniques from declarative prompt optimization to create a structured, domain-specific framework for SLRs that includes task declarations, test suites, and automated prompt tuning.", "result": "The proposed framework provides a concrete blueprint with code examples, allowing researchers to create verifiable LLM pipelines that adhere to principles of transparency and rigour in evidence synthesis.", "conclusion": "This work demonstrates a novel application of prompt optimization to SLR workflows, offering a more robust approach for leveraging LLMs in academic research.", "key_contributions": ["Introduction of a domain-specific framework for SLR automation using LLMs.", "Embedding task declarations and test suites into the literature review process.", "Provision of working code examples that facilitate the construction of verifiable LLM pipelines."], "limitations": "", "keywords": ["large language models", "systematic literature reviews", "prompt optimization", "evidence synthesis", "transparency"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.00944", "pdf": "https://arxiv.org/pdf/2509.00944.pdf", "abs": "https://arxiv.org/abs/2509.00944", "title": "Through the Expert's Eyes: Exploring Asynchronous Expert Perspectives and Gaze Visualizations in XR", "authors": ["Clara Sayffaerth", "Annika Köhler", "Julian Rasch", "Albrecht Schmidt", "Florian Müller"], "categories": ["cs.HC"], "comment": "11 pages, IEEE International Symposium on Mixed and Augmented Reality\n  (ISMAR)", "summary": "Transferring knowledge across generations is fundamental to human\ncivilization, yet the challenge of passing on complex practical skills\npersists. Methods without a physically present instructor, such as videos,\noften fail to explain complex manual tasks, where spatial and social factors\nare critical. Technologies such as eXtended Reality and Artificial Intelligence\nhold the potential to retain expert knowledge and facilitate the creation of\ntailored, contextualized, and asynchronous explanations regardless of time and\nplace. In contrast to videos, the learner's perspective can be different from\nthe recorded perspective in XR. This paper investigates the impact of\nasynchronous first- and third-person perspectives and gaze visualizations on\nefficiency, feeling of embodiment, and connectedness during manual tasks. The\nempirical results of our study (N=36) show that the first-person perspective is\nbetter in quantitative measures and preferred by users. We identify best\npractices for presenting preserved knowledge and provide guidelines for\ndesigning future systems.", "AI": {"tldr": "This paper explores the effectiveness of asynchronous learning using XR and AI for teaching complex manual tasks by analyzing first- and third-person perspectives and gaze visualizations.", "motivation": "The challenge of transferring complex manual skills across generations necessitates innovative educational technologies beyond traditional methods like videos, particularly those incorporating XR and AI.", "method": "The study employed an empirical approach with 36 participants to compare the effectiveness of first-person and third-person perspectives alongside gaze visualizations in teaching manual tasks.", "result": "The findings reveal that the first-person perspective significantly enhances performance in quantitative measures and is preferred by users, demonstrating its advantages for learning in XR environments.", "conclusion": "The paper concludes with best practices for knowledge presentation and guidelines for the design of future learning systems utilizing asynchronous XR technologies.", "key_contributions": ["Investigates the role of first- and third-person perspectives in learning manual tasks through XR", "Demonstrates superior user preference and efficiency of first-person perspective in XR learning", "Provides guidelines for designing future XR educational systems."], "limitations": "", "keywords": ["eXtended Reality", "Asynchronous Learning", "Manual Skills", "Knowledge Transfer", "AI in Education"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2509.00185", "pdf": "https://arxiv.org/pdf/2509.00185.pdf", "abs": "https://arxiv.org/abs/2509.00185", "title": "What Are Research Hypotheses?", "authors": ["Jian Wu", "Sarah Rajtmajer"], "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, accepted by Sci-K'25: International Workshop on Scientific\n  Knowledge", "summary": "Over the past decades, alongside advancements in natural language processing,\nsignificant attention has been paid to training models to automatically\nextract, understand, test, and generate hypotheses in open and scientific\ndomains. However, interpretations of the term \\emph{hypothesis} for various\nnatural language understanding (NLU) tasks have migrated from traditional\ndefinitions in the natural, social, and formal sciences. Even within NLU, we\nobserve differences defining hypotheses across literature. In this paper, we\noverview and delineate various definitions of hypothesis. Especially, we\ndiscern the nuances of definitions across recently published NLU tasks. We\nhighlight the importance of well-structured and well-defined hypotheses,\nparticularly as we move toward a machine-interpretable scholarly record.", "AI": {"tldr": "This paper reviews and clarifies various definitions of 'hypothesis' in natural language understanding (NLU) tasks, highlighting the need for well-defined hypotheses for machine interpretation.", "motivation": "There has been increasing focus on training models for hypothesis extraction and understanding in the context of NLU, yet inconsistencies exist in defining what constitutes a hypothesis across different domains and tasks.", "method": "The authors conducted a literature review to identify and analyze definitions of 'hypothesis' across various NLU applications, documenting the differences and nuances.", "result": "The paper identifies various interpretations of hypotheses, emphasizing the need for clarity and structure in these definitions to enhance the machine interpretability of scholarly records.", "conclusion": "A well-structured definition of hypotheses is crucial for advancing NLU and ensuring meaningful machine understanding of scholarly work.", "key_contributions": ["Overview of various definitions of hypothesis in NLU tasks", "Identification of inconsistencies in hypothesis definitions", "Emphasis on the importance of clear hypothesis structuring for machine learning applications"], "limitations": "", "keywords": ["Natural Language Processing", "Hypothesis", "Natural Language Understanding", "Machine Learning", "Research Methodology"], "importance_score": 5, "read_time_minutes": 6}}
{"id": "2509.01018", "pdf": "https://arxiv.org/pdf/2509.01018.pdf", "abs": "https://arxiv.org/abs/2509.01018", "title": "The State of the Art in Visualization Literacy", "authors": ["Matthew Varona", "Karen Bonilla", "Maryam Hedayati", "Alark Joshi", "Lane Harrison", "Matthew Kay", "Carolina Nobre"], "categories": ["cs.HC"], "comment": "Preprint version. A revised document may follow", "summary": "Research in visualization literacy explores the skills required to engage\nwith visualizations. This state-of-the-art report surveys the current\nliterature in visualization literacy to provide a comprehensive overview of the\nfield. We propose a taxonomy of visualization literacy that organizes the field\ninto competency themes and research categories. To address ambiguity\nsurrounding the term ``visualization literacy'', we provide a framework for\noperationalizing visualization literacy based on application contexts\n(including domain, scenario, and audience) and relevant competencies, which are\ncategorized under consumption, construction, critique, and connection. Research\ncontributions are organized into five categories: ontology, assessment,\nmechanisms, populiteracy, and intervention. For each category, we identify key\ntrends, discuss which competencies are addressed, highlight open challenges,\nand examine how advancements within these areas inform and reinforce each\nother, driving progress in the field.", "AI": {"tldr": "This report surveys literature on visualization literacy, proposing a taxonomy and framework for enhancing skills related to engaging with visualizations.", "motivation": "To provide a comprehensive overview and operational framework for visualization literacy, addressing the ambiguity of its definition.", "method": "A survey of current literature organized into a taxonomy of visualization literacy based on competencies and contexts.", "result": "Identification of key trends, competencies, open challenges, and interrelations within five categories: ontology, assessment, mechanisms, populiteracy, and intervention.", "conclusion": "Advancements within the identified categories inform each other, driving progress in the field of visualization literacy.", "key_contributions": ["Proposed taxonomy of visualization literacy", "Operational framework based on contexts and competencies", "Identification of key trends and open challenges in the field."], "limitations": "", "keywords": ["visualization literacy", "taxonomy", "competency", "literature review", "education"], "importance_score": 5, "read_time_minutes": 20}}
{"id": "2509.00190", "pdf": "https://arxiv.org/pdf/2509.00190.pdf", "abs": "https://arxiv.org/abs/2509.00190", "title": "Explainable Chain-of-Thought Reasoning: An Empirical Analysis on State-Aware Reasoning Dynamics", "authors": ["Sheldon Yu", "Yuxin Xiong", "Junda Wu", "Xintong Li", "Tong Yu", "Xiang Chen", "Ritwik Sinha", "Jingbo Shang", "Julian McAuley"], "categories": ["cs.CL", "cs.AI"], "comment": "5 pages, 4 figures", "summary": "Recent advances in chain-of-thought (CoT) prompting have enabled large\nlanguage models (LLMs) to perform multi-step reasoning. However, the\nexplainability of such reasoning remains limited, with prior work primarily\nfocusing on local token-level attribution, such that the high-level semantic\nroles of reasoning steps and their transitions remain underexplored. In this\npaper, we introduce a state-aware transition framework that abstracts CoT\ntrajectories into structured latent dynamics. Specifically, to capture the\nevolving semantics of CoT reasoning, each reasoning step is represented via\nspectral analysis of token-level embeddings and clustered into semantically\ncoherent latent states. To characterize the global structure of reasoning, we\nmodel their progression as a Markov chain, yielding a structured and\ninterpretable view of the reasoning process. This abstraction supports a range\nof analyses, including semantic role identification, temporal pattern\nvisualization, and consistency evaluation.", "AI": {"tldr": "This paper introduces a state-aware transition framework for improving the explainability of chain-of-thought prompting in large language models by modeling reasoning steps as structured latent dynamics.", "motivation": "The motivation is to enhance the explainability of multi-step reasoning in large language models, which is currently limited due to a lack of focus on high-level semantic roles of reasoning steps.", "method": "The authors develop a state-aware transition framework that uses spectral analysis of token-level embeddings to cluster reasoning steps into semantically coherent latent states and model their progression using a Markov chain.", "result": "The proposed framework provides a structured and interpretable view of the reasoning process that enables semantic role identification, temporal pattern visualization, and consistency evaluation.", "conclusion": "By abstracting CoT trajectories into structured latent dynamics, the framework contributes to better understanding and analyzing the reasoning processes of large language models.", "key_contributions": ["Introduction of the state-aware transition framework for CoT prompting.", "Modeling reasoning steps as a Markov chain for improved interpretability.", "Enabling various analyses such as semantic role identification and temporal visualization."], "limitations": "The approach may depend on the quality of token-level embeddings and the abstraction method may overlook some nuances of reasoning context.", "keywords": ["chain-of-thought prompting", "large language models", "explainability", "Markov chain", "latent dynamics"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2509.01051", "pdf": "https://arxiv.org/pdf/2509.01051.pdf", "abs": "https://arxiv.org/abs/2509.01051", "title": "Chronotome: Real-Time Topic Modeling for Streaming Embedding Spaces", "authors": ["Matte Lim", "Catherine Yeh", "Martin Wattenberg", "Fernanda Viégas", "Panagiotis Michalatos"], "categories": ["cs.HC", "cs.CL", "cs.CV", "cs.LG"], "comment": "Accepted to IEEE VIS 2025 Short Paper Track (5 pages, 4 figures)", "summary": "Many real-world datasets -- from an artist's body of work to a person's\nsocial media history -- exhibit meaningful semantic changes over time that are\ndifficult to capture with existing dimensionality reduction methods. To address\nthis gap, we introduce a visualization technique that combines force-based\nprojection and streaming clustering methods to build a spatial-temporal map of\nembeddings. Applying this technique, we create Chronotome, a tool for\ninteractively exploring evolving themes in time-based data -- in real time. We\ndemonstrate the utility of our approach through use cases on text and image\ndata, showing how it offers a new lens for understanding the aesthetics and\nsemantics of temporal datasets.", "AI": {"tldr": "A new visualization technique, Chronotome, combines force-based projection and streaming clustering to explore evolving themes in time-based data.", "motivation": "Existing dimensionality reduction methods struggle to capture semantic changes in real-world datasets over time.", "method": "The paper introduces a visualization technique that integrates force-based projection with streaming clustering methods to create spatial-temporal maps of embeddings.", "result": "Chronotome allows for interactive exploration of evolving themes in real-time across text and image data, demonstrating enhanced understanding of aesthetics and semantics in temporal datasets.", "conclusion": "Chronotome provides a novel approach for analyzing time-based data, revealing evolving themes effectively.", "key_contributions": ["Introduction of Chronotome for visualizing temporal changes in datasets.", "Combination of force-based projection with streaming clustering methods.", "Real-time interactive exploration of evolving themes in data."], "limitations": "", "keywords": ["visualization", "dimensionality reduction", "temporal data"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2509.00245", "pdf": "https://arxiv.org/pdf/2509.00245.pdf", "abs": "https://arxiv.org/abs/2509.00245", "title": "The Rarity Blind Spot: A Framework for Evaluating Statistical Reasoning in LLMs", "authors": ["Seiji Maekawa", "Hayate Iso", "Nikita Bhutani"], "categories": ["cs.CL"], "comment": null, "summary": "Effective decision-making often relies on identifying what makes each\ncandidate distinctive. While existing benchmarks for LLMs emphasize retrieving\nor summarizing information relevant to a given query, they do not evaluate a\nmodel's ability to identify globally distinctive features across a set of\ndocuments. We introduce Distinctive Feature Mining (DFM), a new task that\nchallenges models to analyze a small-to-medium collection (10-40 documents) and\nsurface features that are rare in the global context (e.g., appearing in less\nthan 10% of documents). This setting mirrors real-world scenarios such as\ncandidate selection or product differentiation, where statistical reasoning,\nnot retrieval, is key. To enable systematic evaluation of this capability, we\npresent DiFBench, a configurable benchmark creation framework with controllable\nparameters such as document set size and distinctiveness thresholds. Using\nDiFBench, we perform a large-scale assessment of distinctive feature mining\nacross ten state-of-the-art LLMs. Our findings reveal a significant performance\ngap between general-purpose and reasoning-enhanced models. All models, however,\nsubstantially degrade as the task complexity and document count increase. We\nalso find that a common failure mode is misidentifying frequent features as\ndistinctive. These insights reveal core limitations in contemporary LLMs'\nabilities to perform fine-grained, statistical reasoning and rarity detection.", "AI": {"tldr": "This paper introduces Distinctive Feature Mining (DFM), a task for LLMs focused on identifying rare features across document collections, and presents DiFBench, a benchmark for evaluating this capability.", "motivation": "To improve decision-making processes by enabling models to identify globally distinctive features rather than just retrieving relevant information.", "method": "The authors propose the Distinctive Feature Mining (DFM) task and the DiFBench framework to evaluate LLMs on their ability to analyze collections of documents and surface statistically rare features.", "result": "A large-scale assessment of ten state-of-the-art LLMs reveals performance gaps between general-purpose and reasoning-enhanced models, highlighting that all models struggle as task complexity increases.", "conclusion": "Contemporary LLMs show limitations in statistical reasoning and rarity detection, often misclassifying frequent features as distinctive, indicating a need for further improvement.", "key_contributions": ["Introduction of the DFM task for LLMs.", "Development of DiFBench, a benchmark for evaluating distinctive feature mining.", "Insights into the performance of LLMs on complex reasoning tasks."], "limitations": "Most models degrade significantly as task complexity and document count increase, indicating scalability issues.", "keywords": ["Distinctive Feature Mining", "LLM", "Benchmark", "Statistical Reasoning", "Rarity Detection"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.01089", "pdf": "https://arxiv.org/pdf/2509.01089.pdf", "abs": "https://arxiv.org/abs/2509.01089", "title": "CosinorAge: Unified Python and Web Platform for Biological Age Estimation from Wearable- and Smartwatch-Based Activity Rhythms", "authors": ["Jinjoo Shim", "Jacob Hunecke", "Elgar Fleisch", "Filipe Barata"], "categories": ["cs.HC"], "comment": "7 pages - 4 figures", "summary": "Every day, millions of people worldwide track their steps, sleep, and\nactivity rhythms with smartwatches and fitness trackers. These continuously\ncollected data streams present a remarkable opportunity to transform routine\nself-tracking into meaningful health insights that enable individuals to\nunderstand -- and potentially influence -- their biological aging. Yet most\ntools for analyzing wearable data remain fragmented, proprietary, and\ninaccessible, creating a major barrier between this vast reservoir of personal\nhealth information and its translation into actionable insights on aging.\nCosinorAge is an open-source framework that estimates biological age from\nwearable-derived circadian, physical activity, and sleep metrics. It addresses\nthe lack of unified, reproducible pipelines for jointly analyzing\nrest--activity rhythmicity, physical activity, and sleep, and linking them to\nhealth outcomes. The Python package provides an end-to-end workflow from raw\ndata ingestion and preprocessing to feature computation and biological age\nestimation, supporting multiple input sources across wearables and smartwatch.\nIt also makes available trained model parameters (open weights) derived from\nlarge-scale population datasets such as UK Biobank, enabling reproducibility,\ntransparency, and generalizability across studies. Its companion web-based\nCosinorAge Calculator enables non-technical users to access identical\nanalytical capabilities through an intuitive interface. By combining\ntransparent, reproducible analysis with broad accessibility, CosinorAge\nadvances scalable, personalized health monitoring and bridges digital health\ntechnologies with biological aging research.", "AI": {"tldr": "CosinorAge is an open-source framework for estimating biological age from wearable data, addressing fragmentation in health data analysis and promoting accessibility for users.", "motivation": "To transform self-tracking data from wearable devices into actionable insights regarding biological aging, overcoming the barriers posed by fragmented and proprietary analysis tools.", "method": "CosinorAge is a Python package that processes raw wearable data, computes features, estimates biological age, and provides trained model parameters derived from large-scale datasets. It also includes a web-based calculator for non-technical users.", "result": "The framework offers a unified pipeline for analyzing various health metrics, enabling reproducible and transparent health monitoring, and links self-tracking to aging research.", "conclusion": "CosinorAge facilitates personalized health insights by bridging the gap between digital health technologies and biological aging research, enhancing accessibility and usability for a broader audience.", "key_contributions": ["Open-source framework for biological age estimation from wearables", "Support for multiple data sources and preprocessing workflows", "Web-based interface for non-technical users to access analytical capabilities"], "limitations": "", "keywords": ["biological aging", "wearable technology", "health informatics"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.00248", "pdf": "https://arxiv.org/pdf/2509.00248.pdf", "abs": "https://arxiv.org/abs/2509.00248", "title": "The Differential Meaning of Models: A Framework for Analyzing the Structural Consequences of Semantic Modeling Decisions", "authors": ["Zachary K. Stine", "James E. Deitrick"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The proliferation of methods for modeling of human meaning-making constitutes\na powerful class of instruments for the analysis of complex semiotic systems.\nHowever, the field lacks a general theoretical framework for describing these\nmodeling practices across various model types in an apples-to-apples way. In\nthis paper, we propose such a framework grounded in the semiotic theory of C.\nS. Peirce. We argue that such models measure latent symbol geometries, which\ncan be understood as hypotheses about the complex of semiotic agencies\nunderlying a symbolic dataset. Further, we argue that in contexts where a\nmodel's value cannot be straightforwardly captured by proxy measures of\nperformance, models can instead be understood relationally, so that the\nparticular interpretive lens of a model becomes visible through its contrast\nwith other models. This forms the basis of a theory of model semantics in which\nmodels, and the modeling decisions that constitute them, are themselves treated\nas signs. In addition to proposing the framework, we illustrate its empirical\nuse with a few brief examples and consider foundational questions and future\ndirections enabled by the framework.", "AI": {"tldr": "The paper proposes a general theoretical framework for modeling human meaning-making grounded in Peirce's semiotic theory, enabling relational understanding of models as signs.", "motivation": "The lack of a general theoretical framework for accurately describing the diverse modeling practices in the analysis of complex semiotic systems.", "method": "The framework is grounded in C. S. Peirce's semiotic theory and focuses on understanding models relationally, particularly when traditional performance measures are insufficient.", "result": "The proposed framework allows for the examination of models as hypotheses about semiotic agencies, enabling insights into their interpretive lenses.", "conclusion": "This framework can enhance the understanding of modeling practices and guide future research directions in the field of semiotic analysis.", "key_contributions": ["Proposing a general framework for modeling human meaning-making.", "Introducing the concept of models as latent symbol geometries.", "Highlighting models' interpretive lenses through relational comparisons."], "limitations": "", "keywords": ["semiotics", "model semantics", "C. S. Peirce", "meaning-making", "semiotic systems"], "importance_score": 3, "read_time_minutes": 15}}
{"id": "2509.01231", "pdf": "https://arxiv.org/pdf/2509.01231.pdf", "abs": "https://arxiv.org/abs/2509.01231", "title": "Unpacking Personal(?!) Health Informatics: An Investigation of Awareness, Understanding, And Leveraged Utility in India", "authors": ["Shyama Sastha Krishnamoorthy Srinivasan", "Mohan Kumar", "Pushpendra Singh"], "categories": ["cs.HC", "cs.CY"], "comment": "25 pages, 2 figures, 4 tables; A qualitative HCI study with prototype\n  evaluation", "summary": "Personal Health Informatics (PHI), which leverages digital tools and\ninformation systems to support health assessment and self-care, holds promise\nfor empowering individuals and transforming healthcare delivery. However,\nbarriers to its adoption remain underexplored in the Indian context. This study\ninvestigates PHI adoption among Indian users and stakeholders using a\nmulti-method approach. An awareness survey (n = 87) examined the usage of\nwearables and general PHI engagement, followed by semi-structured interviews (n\n= 22) that explored motivations, usage patterns, and health information\nsources. Qualitative analysis revealed that while PHI is valued for health\nmonitoring and shared/collective care, its adoption is hindered by factors such\nas low health literacy, usability challenges, and mistrust in digital health\nplatforms. Further stakeholder interviews and co-design workshops informed the\ndevelopment of a Figma-based prototype, which was evaluated for usability.\nBased on these findings, we offer design recommendations for an integrated,\nuser-controlled PHI platform featuring accessible analytics and verifiable\nhealth information. Our insights highlight the socio-technical challenges of\nPHI adoption in India and underscore the need for reliable, user-centric\nsolutions to support proactive healthcare.", "AI": {"tldr": "This study investigates the adoption of Personal Health Informatics (PHI) in India, identifying barriers and proposing design recommendations for a user-centered platform.", "motivation": "To explore barriers to the adoption of Personal Health Informatics in India and to propose solutions for enhancing healthcare delivery through digital tools.", "method": "A multi-method approach including an awareness survey (n=87) on PHI usage, semi-structured interviews (n=22) for qualitative insights, and usability evaluation of a developed prototype.", "result": "Barriers to PHI adoption identified include low health literacy, usability issues, and mistrust in digital platforms. Insights led to the design of a user-controlled PHI platform.", "conclusion": "The study emphasizes the necessity for reliable and user-centric solutions to address the socio-technical challenges faced in PHI adoption in India.", "key_contributions": ["Identification of barriers to PHI adoption specific to the Indian context.", "Development of a prototype for a user-controlled PHI platform based on user feedback.", "Design recommendations aimed at improving usability and trust in digital health platforms."], "limitations": "Focused on a specific demographic in India; findings may not generalize to other regions or populations.", "keywords": ["Personal Health Informatics", "health literacy", "usability", "digital health", "India"], "importance_score": 8, "read_time_minutes": 25}}
{"id": "2509.00250", "pdf": "https://arxiv.org/pdf/2509.00250.pdf", "abs": "https://arxiv.org/abs/2509.00250", "title": "The Temporal Game: A New Perspective on Temporal Relation Extraction", "authors": ["Hugo Sousa", "Ricardo Campos", "Alípio Jorge"], "categories": ["cs.CL"], "comment": null, "summary": "In this paper we demo the Temporal Game, a novel approach to temporal\nrelation extraction that casts the task as an interactive game. Instead of\ndirectly annotating interval-level relations, our approach decomposes them into\npoint-wise comparisons between the start and end points of temporal entities.\nAt each step, players classify a single point relation, and the system applies\ntemporal closure to infer additional relations and enforce consistency. This\npoint-based strategy naturally supports both interval and instant entities,\nenabling more fine-grained and flexible annotation than any previous approach.\nThe Temporal Game also lays the groundwork for training reinforcement learning\nagents, by treating temporal annotation as a sequential decision-making task.\nTo showcase this potential, the demo presented in this paper includes a Game\nmode, in which users annotate texts from the TempEval-3 dataset and receive\nfeedback based on a scoring system, and an Annotation mode, that allows custom\ndocuments to be annotated and resulting timeline to be exported. Therefore,\nthis demo serves both as a research tool and an annotation interface. The demo\nis publicly available at https://temporal-game.inesctec.pt, and the source code\nis open-sourced to foster further research and community-driven development in\ntemporal reasoning and annotation.", "AI": {"tldr": "A novel approach called Temporal Game for temporal relation extraction using interactive gaming mechanics.", "motivation": "To improve the process of temporal relation annotation by making it more interactive and by allowing more fine-grained and flexible annotation.", "method": "The Temporal Game decomposes interval-level temporal relations into point-wise comparisons between start and end points of entities, utilizing a game format to enhance user engagement and feedback.", "result": "The system supports both interval and instant entities and enables reinforcement learning agents to be trained by treating temporal annotation as a sequential decision-making process.", "conclusion": "The Temporal Game serves as both a research tool for temporal reasoning and an annotation interface for users, fostering community-driven development.", "key_contributions": ["Introduction of an interactive game format for temporal relation extraction", "Supports fine-grained annotation through point-wise comparisons", "Publicly available demo and open-sourced code for community use"], "limitations": "", "keywords": ["temporal relation extraction", "interactive annotation", "reinforcement learning"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2509.01246", "pdf": "https://arxiv.org/pdf/2509.01246.pdf", "abs": "https://arxiv.org/abs/2509.01246", "title": "An AI-Based Shopping Assistant System to Support the Visually Impaired", "authors": ["Larissa R. de S. Shibata", "Ankit A. Ravankar", "Jose Victorio Salazar Luces", "Yasuhisa Hirata"], "categories": ["cs.HC", "cs.RO"], "comment": "7 pages, Accepted for 2025 SICE-FES conference (IEEE)", "summary": "Shopping plays a significant role in shaping consumer identity and social\nintegration. However, for individuals with visual impairments, navigating in\nsupermarkets and identifying products can be an overwhelming and challenging\nexperience. This paper presents an AI-based shopping assistant prototype\ndesigned to enhance the autonomy and inclusivity of visually impaired\nindividuals in supermarket environments. The system integrates multiple\ntechnologies, including computer vision, speech recognition, text-to-speech\nsynthesis, and indoor navigation, into a single, user-friendly platform. Using\ncameras for ArUco marker detection and real-time environmental scanning, the\nsystem helps users navigate the store, identify product locations, provide\nreal-time auditory guidance, and gain context about their surroundings. The\nassistant interacts with the user through voice commands and multimodal\nfeedback, promoting a more dynamic and engaging shopping experience. The system\nwas evaluated through experiments, which demonstrated its ability to guide\nusers effectively and improve their shopping experience. This paper contributes\nto the development of inclusive AI-driven assistive technologies aimed at\nenhancing accessibility and user independence for the shopping experience.", "AI": {"tldr": "The paper presents an AI-based shopping assistant prototype that enhances autonomy and inclusivity for visually impaired individuals shopping in supermarkets.", "motivation": "To address the challenges faced by visually impaired individuals in navigating supermarkets and identifying products.", "method": "The system uses computer vision, speech recognition, text-to-speech synthesis, and indoor navigation technologies for real-time environmental scanning and guidance.", "result": "Experiments showed that the assistant effectively guided users and improved their shopping experience in supermarket environments.", "conclusion": "The development of this AI-driven assistive technology contributes to enhancing accessibility and independence for visually impaired shoppers.", "key_contributions": ["Integration of multiple technologies into a single user-friendly platform", "Real-time auditory guidance and context about surroundings", "Focus on enhancing user autonomy and inclusivity in shopping"], "limitations": "", "keywords": ["AI-based shopping assistant", "accessibility", "inclusivity", "visual impairments", "assistive technologies"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.00276", "pdf": "https://arxiv.org/pdf/2509.00276.pdf", "abs": "https://arxiv.org/abs/2509.00276", "title": "Exploring Reasoning-Infused Text Embedding with Large Language Models for Zero-Shot Dense Retrieval", "authors": ["Yuxiang Liu", "Tian Wang", "Gourab Kundu", "Tianyu Cao", "Guang Cheng", "Zhen Ge", "Jianshu Chen", "Qingjun Cui", "Trishul Chilimbi"], "categories": ["cs.CL"], "comment": "CIKM 2025", "summary": "Transformer-based models such as BERT and E5 have significantly advanced text\nembedding by capturing rich contextual representations. However, many complex\nreal-world queries require sophisticated reasoning to retrieve relevant\ndocuments beyond surface-level lexical matching, where encoder-only retrievers\noften fall short. Decoder-only large language models (LLMs), known for their\nstrong reasoning capabilities, offer a promising alternative. Despite this\npotential, existing LLM-based embedding methods primarily focus on contextual\nrepresentation and do not fully exploit the reasoning strength of LLMs. To\nbridge this gap, we propose Reasoning-Infused Text Embedding (RITE), a simple\nbut effective approach that integrates logical reasoning into the text\nembedding process using generative LLMs. RITE builds upon existing language\nmodel embedding techniques by generating intermediate reasoning texts in the\ntoken space before computing embeddings, thereby enriching representations with\ninferential depth. Experimental results on BRIGHT, a reasoning-intensive\nretrieval benchmark, demonstrate that RITE significantly enhances zero-shot\nretrieval performance across diverse domains, underscoring the effectiveness of\nincorporating reasoning into the embedding process.", "AI": {"tldr": "This paper introduces Reasoning-Infused Text Embedding (RITE), which incorporates logical reasoning into text embedding using generative LLMs, enhancing retrieval performance on reasoning-intensive tasks.", "motivation": "The paper addresses the limitations of existing transformer-based models in handling complex queries that require reasoning, aiming to improve document retrieval effectiveness.", "method": "RITE integrates logical reasoning into the text embedding process by generating intermediate reasoning texts in the token space before computing embeddings.", "result": "Experimental results show that RITE significantly enhances zero-shot retrieval performance on the BRIGHT benchmark, performing better across diverse domains compared to existing methods.", "conclusion": "Incorporating reasoning into the text embedding process effectively addresses challenges in retrieving relevant documents for complex queries.", "key_contributions": ["Development of RITE that combines reasoning with text embeddings.", "Demonstration of significant improvements in retrieval performance on reasoning-intensive tasks.", "Introduction of a method for generating intermediate reasoning texts in the token space."], "limitations": "", "keywords": ["Reasoning", "Text Embedding", "Large Language Models", "Document Retrieval", "NLP"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.01367", "pdf": "https://arxiv.org/pdf/2509.01367.pdf", "abs": "https://arxiv.org/abs/2509.01367", "title": "MetaRoundWorm: A Virtual Reality Escape Room Game for Learning the Lifecycle and Immune Response to Parasitic Infections", "authors": ["Xuanru Cheng", "Xian Wang", "Chi-lok Tai", "Lik-Hang Lee"], "categories": ["cs.HC"], "comment": null, "summary": "Promoting public health is challenging owing to its abstract nature, and\nindividuals may be apprehensive about confronting it. Recently, there has been\nan increasing interest in using the metaverse and gamification as novel\neducational techniques to improve learning experiences related to the immune\nsystem. Thus, we present MetaRoundWorm, an immersive virtual reality (VR)\nescape room game designed to enhance the understanding of parasitic infections\nand host immune responses through interactive, gamified learning. The\napplication simulates the lifecycle of Ascaris lumbricoides and corresponding\nimmunological mechanisms across anatomically accurate environments within the\nhuman body. Integrating serious game mechanics with embodied learning\nprinciples, MetaRoundWorm offers players a task-driven experience combining\nexploration, puzzle-solving, and immune system simulation. To evaluate the\neducational efficacy and user engagement, we conducted a controlled study\ncomparing MetaRoundWorm against a traditional approach, i.e., interactive\nslides. Results indicate that MetaRoundWorm significantly improves immediate\nlearning outcomes, cognitive engagement, and emotional experience, while\nmaintaining knowledge retention over time. Our findings suggest that immersive\nVR gamification holds promise as an effective pedagogical tool for\ncommunicating complex biomedical concepts and advancing digital health\neducation.", "AI": {"tldr": "MetaRoundWorm is a VR game that gamifies learning about parasitic infections and the immune system, showing improved educational outcomes compared to traditional methods.", "motivation": "To address the challenge of promoting public health education, particularly regarding understanding the immune system and related parasitic infections.", "method": "An immersive VR escape room game that simulates the lifecycle of Ascaris lumbricoides and related immunological responses, combining exploration, puzzle-solving, and task-driven experiences.", "result": "MetaRoundWorm significantly improves immediate learning outcomes, cognitive engagement, and emotional experience while ensuring knowledge retention over time compared to traditional interactive slides.", "conclusion": "Immersive VR gamification is an effective pedagogical tool for conveying complex biomedical concepts and enhancing digital health education.", "key_contributions": ["Development of a VR educational game for health education", "Evidence of improved learning outcomes and user engagement", "Integration of serious game mechanics with educational principles"], "limitations": "", "keywords": ["virtual reality", "gamification", "biomedical education", "immune system", "parasitic infections"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.00285", "pdf": "https://arxiv.org/pdf/2509.00285.pdf", "abs": "https://arxiv.org/abs/2509.00285", "title": "OpinioRAG: Towards Generating User-Centric Opinion Highlights from Large-scale Online Reviews", "authors": ["Mir Tafseer Nayeem", "Davood Rafiei"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "COLM 2025", "summary": "We study the problem of opinion highlights generation from large volumes of\nuser reviews, often exceeding thousands per entity, where existing methods\neither fail to scale or produce generic, one-size-fits-all summaries that\noverlook personalized needs. To tackle this, we introduce OpinioRAG, a\nscalable, training-free framework that combines RAG-based evidence retrieval\nwith LLMs to efficiently produce tailored summaries. Additionally, we propose\nnovel reference-free verification metrics designed for sentiment-rich domains,\nwhere accurately capturing opinions and sentiment alignment is essential. These\nmetrics offer a fine-grained, context-sensitive assessment of factual\nconsistency. To facilitate evaluation, we contribute the first large-scale\ndataset of long-form user reviews, comprising entities with over a thousand\nreviews each, paired with unbiased expert summaries and manually annotated\nqueries. Through extensive experiments, we identify key challenges, provide\nactionable insights into improving systems, pave the way for future research,\nand position OpinioRAG as a robust framework for generating accurate, relevant,\nand structured summaries at scale.", "AI": {"tldr": "OpinioRAG is introduced as a scalable framework for generating personalized opinion highlights from large volumes of user reviews, incorporating LLMs and RAG-based retrieval methods.", "motivation": "To address the limitations of existing methods in generating personalized summaries from large user reviews, which often fail to scale or provide generic outputs.", "method": "The OpinioRAG framework combines retrieval-augmented generation (RAG) with large language models (LLMs) for efficient and tailored summaries. It also proposes new reference-free verification metrics suitable for sentiment-rich contexts.", "result": "Extensive experiments revealed key challenges in opinion highlights generation and provided actionable insights for enhancing these systems, demonstrating the efficacy of OpinioRAG.", "conclusion": "OpinioRAG is positioned as a robust framework capable of generating accurate and structured summaries from large datasets of user reviews, addressing both scalability and personalization needs.", "key_contributions": ["Introduction of a training-free, scalable framework for opinion highlights generation.", "Development of novel reference-free verification metrics for sentiment-rich domains.", "Creation of a large-scale dataset of long-form user reviews and unbiased expert summaries."], "limitations": "", "keywords": ["opinion highlights", "user reviews", "RAG", "large language models", "sentiment analysis"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.01414", "pdf": "https://arxiv.org/pdf/2509.01414.pdf", "abs": "https://arxiv.org/abs/2509.01414", "title": "AttenTrack: Mobile User Attention Awareness Based on Context and External Distractions", "authors": ["Yutong Lin", "Suyuan Liu", "Kaiwen Guo", "Haohua Du", "Chao Liu", "Xiang-Yang Li"], "categories": ["cs.HC"], "comment": null, "summary": "In the mobile internet era, managing limited attention amid information\noverload is crucial for enhancing collaboration and information delivery.\nHowever, current attention-aware systems often depend on wearables or\npersonalized data, limiting their scalability and cross-context adaptability.\nInspired by psychological theories, we attempt to treat mobile notifications as\nnaturally occurring external distractions and infer users' attention states\nbased on their response behaviors and contextual information. Our goal is to\nbuild an attention-aware model that does not rely on personalized historical\ndata or complex subjective input, while ensuring strong cold-start capability\nand cross-context adaptability. To this end, We design a field study framework\nintegrating subjective and objective data, closely aligned with real-world\nexternal distractions (i.e., mobile notifications). Through field studies, we\nconstruct a fine-grained and interpretable dataset centered on the relationship\namong current context - external distractions - subjective attention. Through\nour field studies, we conduct an in-depth analysis of the relationships among\nusers' response behaviors, response motivations, contextual information, and\nattention states. Building on our findings, we propose AttenTrack, a\nlightweight, privacy-friendly attention awareness model with strong cold-start\ncapability. The model relies solely on non-privacy-sensitive objective data\navailable on mobile devices, and can be applied to a variety of attention\nmanagement tasks. In addition, we will publicly release the constructed dataset\nto support future research and advance the field of mobile attention awareness.", "AI": {"tldr": "A study proposing AttenTrack, an attention-aware model for mobile notifications that does not rely on personalized data and has strong cold-start capability.", "motivation": "To manage limited attention amid information overload in the mobile internet era and improve collaboration and information delivery without relying on wearable devices or personalized data.", "method": "Field study framework combining subjective and objective data to analyze user response behaviors, contextual information, and attention states, leading to the development of AttenTrack.", "result": "Constructed a fine-grained dataset and proposed a privacy-friendly attention awareness model that works well across different contexts without needing historical data.", "conclusion": "AttenTrack can effectively manage attention from mobile notifications while being applicable to various attention management tasks, with a dataset to support further research.", "key_contributions": ["Development of AttenTrack, a lightweight attention awareness model", "Creation of a publicly available dataset for attention management research", "Analysis of the relationship between context, distractions, and attention states"], "limitations": "", "keywords": ["attention management", "mobile notifications", "attention-aware systems"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.00290", "pdf": "https://arxiv.org/pdf/2509.00290.pdf", "abs": "https://arxiv.org/abs/2509.00290", "title": "Wage Sentiment Indices Derived from Survey Comments via Large Language Models", "authors": ["Taihei Sone"], "categories": ["cs.CL"], "comment": "Submitted to IEEE Big Data 2025. 10 pages, 2 tables, 16 figures", "summary": "The emergence of generative Artificial Intelligence (AI) has created new\nopportunities for economic text analysis. This study proposes a Wage Sentiment\nIndex (WSI) constructed with Large Language Models (LLMs) to forecast wage\ndynamics in Japan. The analysis is based on the Economy Watchers Survey (EWS),\na monthly survey conducted by the Cabinet Office of Japan that captures\nreal-time economic assessments from workers in industries highly sensitive to\nbusiness conditions. The WSI extends the framework of the Price Sentiment Index\n(PSI) used in prior studies, adapting it specifically to wage related\nsentiment. To ensure scalability and adaptability, a data architecture is also\ndeveloped that enables integration of additional sources such as newspapers and\nsocial media. Experimental results demonstrate that WSI models based on LLMs\nsignificantly outperform both baseline approaches and pretrained models. These\nfindings highlight the potential of LLM-driven sentiment indices to enhance the\ntimeliness and effectiveness of economic policy design by governments and\ncentral banks.", "AI": {"tldr": "The paper introduces the Wage Sentiment Index (WSI) using Large Language Models to forecast wage dynamics in Japan, proving its efficacy over traditional models.", "motivation": "To leverage generative AI for economic text analysis, specifically in forecasting wage dynamics influenced by real-time economic sentiments.", "method": "The study develops a Wage Sentiment Index (WSI) based on the Economy Watchers Survey, integrating LLMs and a scalable data architecture to include diverse data sources.", "result": "WSI models driven by LLMs significantly outperform baseline and pretrained models in forecasting wage dynamics, demonstrating their potential for economic policy design.", "conclusion": "LLM-driven sentiment indices can enhance the timely and effective formulation of economic policies by governments and central banks.", "key_contributions": ["Introduction of the Wage Sentiment Index (WSI) for economic forecasting.", "Development of a scalable data architecture for integrating various data sources.", "Demonstration of the superior performance of WSI models compared to traditional methods."], "limitations": "", "keywords": ["Wage Sentiment Index", "Large Language Models", "Economic Policy", "Forecasting", "Economy Watchers Survey"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.01420", "pdf": "https://arxiv.org/pdf/2509.01420.pdf", "abs": "https://arxiv.org/abs/2509.01420", "title": "Body Ownership Affects the Processing of Sensorimotor Contingencies in Virtual Reality", "authors": ["Evan G. Center", "Matti Pouke", "Alessandro Nardi", "Lukas Gehrke", "Klaus Gramann", "Timo Ojala", "Steven M. LaValle"], "categories": ["cs.HC", "cs.MM"], "comment": "Dr. Center and Dr. Pouke contributed equally to this work", "summary": "Presence in virtual reality (VR), the subjective sense of \"being there\" in a\nvirtual environment, is notoriously difficult to measure.\nElectroencephalography (EEG) may offer a promising, unobtrusive means of\nassessing a user's momentary state of presence. Unlike traditional\nquestionnaires, EEG does not interrupt the experience or rely on users'\nretrospective self-reports, thereby avoiding interference with the very state\nit aims to capture. Previous research has attempted to quantify presence in\nvirtual environments using event-related potentials (ERPs). We contend,\nhowever, that previous efforts have fallen short of fully realizing this goal,\nfailing to either A) independently manipulate presence, B) validate their\nmeasure of presence against traditional techniques, C) adequately separate the\nconstructs of presence and attention, and/or D) implement a realistic and\nimmersive environment and task. We address these shortcomings in a\npreregistered ERP experiment in which participants play an engaging target\nshooting game in VR. ERPs are time-locked to the release of a ball from a\nsling. We induce breaks in presence (BIPs) by freezing the ball's release on a\nminority of trials. Embodiment is manipulated by allowing manual manipulation\nof the sling with a realistic avatar in one condition (embodied condition) and\npassive manipulation with only controllers in another (non-embodied condition).\nWe support our predictions that the N2, the P3b, and the N400, are selectively\nsensitive towards specific components of these manipulations. The pattern of\nfindings carries significant implications for theories of presence, which have\nbeen seldom addressed in previous ERP investigations on this topic.", "AI": {"tldr": "This paper investigates the measurement of presence in virtual reality using EEG, addressing shortcomings of previous research by manipulating presence and validating measures against traditional techniques in a preregistered ERP experiment.", "motivation": "The paper aims to improve the measurement of presence in VR, a key factor in user experience, by leveraging EEG instead of traditional self-reporting methods that can disrupt the virtual experience.", "method": "A preregistered ERP experiment where participants engage in a target shooting game in VR, with manipulations of presence and embodiment. Event-related potentials are measured in response to specific trial conditions.", "result": "The study found that the N2, P3b, and N400 ERP components are sensitive to manipulations of presence and embodiment, providing a new perspective on how presence can be quantified in VR settings.", "conclusion": "The findings suggest that EEG can effectively measure presence in virtual environments, offering a more reliable method compared to self-reports and enhancing the understanding of presence-related theories.", "key_contributions": ["Introduced an EEG-based measurement approach for presence in VR", "Validated presence measures against traditional methods", "Manipulated presence and embodiment in a controlled VR setting"], "limitations": "The experiment’s realism may be limited to the designed scenario and may not generalize to all VR experiences.", "keywords": ["virtual reality", "presence", "EEG", "event-related potentials", "human-computer interaction"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.00309", "pdf": "https://arxiv.org/pdf/2509.00309.pdf", "abs": "https://arxiv.org/abs/2509.00309", "title": "Balanced Actor Initialization: Stable RLHF Training of Distillation-Based Reasoning Models", "authors": ["Chen Zheng", "Yiyuan Ma", "Yuan Yang", "Deyi Liu", "Jing Liu", "Zuquan Song", "Yuxin Song", "Cheng Ren", "Hang Zhu", "Xin Liu", "Yiyuan Ma", "Siyuan Qiao", "Xun Zhou", "Liang Xiang", "Yonghui Wu"], "categories": ["cs.CL"], "comment": null, "summary": "The development of alignment and reasoning capabilities in large language\nmodels has seen remarkable progress through two paradigms: instruction tuning\nand reinforcement learning from human feedback (RLHF) alignment paradigm, and\ndistillation-based reasoning fine-tuning paradigm. While both approaches prove\neffective independently, the third paradigm of applying RLHF to\ndistillation-trained models presents significant challenges. Our investigation\nreveals two critical phenomena that emerge in this paradigm: Sequence Length\nCollapse, where language generation dramatically reduces during early RLHF\ntraining, and the Reward Hockey Stick Curve, featuring severe reward score\ndrops followed by gradual recovery. These instabilities fundamentally\ncompromise the model's alignment and reasoning capabilities. To address these\nchallenges, we propose Balanced Actor Initialization (BAI), a two-stage\nweighted model merging approach. BAI first merges instruction-following and\ndistillation-based reasoning fine-tuned models, then further combines this\nintermediate model with the pretrained model to preserve foundational\nknowledge. Through comprehensive experiments across diverse benchmarks and\ndetailed analysis of training experiments, we demonstrate that BAI resolves\nSequence Length Collapse, mitigates the Reward Hockey Stick Curve, and enables\ncontinuous sequence length improvement during training. Additionally, our\nanalysis reveals that balanced merging ratios achieve optimal trade-offs\nbetween training stability and reasoning capability preservation. Our work\nprovides the effective solution for stable training in this third paradigm,\nenabling more capable reasoning models that combine distillation efficiency\nwith RLHF alignment.", "AI": {"tldr": "This paper investigates the challenges in training large language models using a third paradigm that combines RLHF and distillation-focused approaches, revealing critical instabilities and proposing a solution called Balanced Actor Initialization (BAI).", "motivation": "The need for enhanced alignment and reasoning capabilities in large language models has led to exploring effective training paradigms, particularly combining RLHF and distillation methods.", "method": "The authors propose a two-stage weighted model merging approach called Balanced Actor Initialization (BAI), which merges instruction-following models with distillation-trained models to stabilize training.", "result": "BAI resolves issues such as Sequence Length Collapse and the Reward Hockey Stick Curve, enhancing training stability and improving reasoning capabilities.", "conclusion": "By implementing BAI, the study demonstrates improved stability and reasoning in language models, suggesting a viable method for effective training in the combined RLHF and distillation paradigm.", "key_contributions": ["Introduction of Balanced Actor Initialization (BAI) for training language models.", "Identification of critical phenomena like Sequence Length Collapse and Reward Hockey Stick Curve in RLHF-distillation training.", "Demonstration of improved model performance through balanced merging ratios."], "limitations": "", "keywords": ["large language models", "alignment", "reasoning", "RLHF", "distillation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.01460", "pdf": "https://arxiv.org/pdf/2509.01460.pdf", "abs": "https://arxiv.org/abs/2509.01460", "title": "Dissecting Atomic Facts: Visual Analytics for Improving Fact Annotations in Language Model Evaluation", "authors": ["Manuel Schmidt", "Daniel A. Keim", "Frederik L. Dennig"], "categories": ["cs.HC"], "comment": "2 pages text plus poster, 2 figures, LaTeX", "summary": "Factuality evaluation of large language model (LLM) outputs requires\ndecomposing text into discrete \"atomic\" facts. However, existing definitions of\natomicity are underspecified, with empirical results showing high disagreement\namong annotators, both human and model-based, due to unresolved ambiguity in\nfact decomposition. We present a visual analytics concept to expose and analyze\nannotation inconsistencies in fact extraction. By visualizing semantic\nalignment, granularity and referential dependencies, our approach aims to\nenable systematic inspection of extracted facts and facilitate convergence\nthrough guided revision loops, establishing a more stable foundation for\nfactuality evaluation benchmarks and improving LLM evaluation.", "AI": {"tldr": "This paper presents a visual analytics approach to improve the consistency of factuality evaluation for large language models by analyzing annotation discrepancies in fact extraction.", "motivation": "The motivation is to address the high disagreement among annotators in decomposing text into atomic facts, which complicates factuality evaluation of LLM outputs.", "method": "The methodology involves visualizing semantic alignment, granularity, and referential dependencies to facilitate systematic inspection of extracted facts and support guided revision loops.", "result": "The approach enables a better understanding of inconsistencies in fact extraction and helps establish a more stable foundation for factuality evaluation benchmarks.", "conclusion": "Ultimately, this work aims to enhance the evaluation of large language models by reducing annotation inconsistencies and improving factual accuracy.", "key_contributions": ["Introduction of a visual analytics concept for fact extraction", "Explanation of the impact of annotation inconsistencies in LLM evaluation", "Guided revision loops for better consensus among annotators"], "limitations": "The paper does not provide extensive empirical evaluation of the proposed method and its effectiveness in different contexts.", "keywords": ["factuality evaluation", "large language models", "fact extraction", "visual analytics", "annotation consistency"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2509.00325", "pdf": "https://arxiv.org/pdf/2509.00325.pdf", "abs": "https://arxiv.org/abs/2509.00325", "title": "GIER: Gap-Driven Self-Refinement for Large Language Models", "authors": ["Rinku Dewri"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "We introduce GIER (Gap-driven Iterative Enhancement of Responses), a general\nframework for improving large language model (LLM) outputs through\nself-reflection and revision based on conceptual quality criteria. Unlike\nprompting strategies that rely on demonstrations, examples, or chain-of-thought\ntemplates, GIER utilizes natural language descriptions of reasoning gaps, and\nprompts a model to iteratively critique and refine its own outputs to better\nsatisfy these criteria. Across three reasoning-intensive tasks (SciFact,\nPrivacyQA, and e-SNLI) and four LLMs (GPT-4.1, GPT-4o Mini, Gemini 1.5 Pro, and\nLlama 3.3 70B), GIER improves rationale quality, grounding, and reasoning\nalignment without degrading task accuracy. Our analysis demonstrates that\nmodels can not only interpret abstract conceptual gaps but also translate them\ninto concrete reasoning improvements.", "AI": {"tldr": "GIER is a framework for enhancing LLM outputs through self-reflection and iterative revision based on conceptual quality criteria, showing improvements across various reasoning tasks and models.", "motivation": "The motivation behind GIER is to improve the quality of responses generated by large language models (LLMs) by enabling them to self-reflect and revise outputs according to specific reasoning criteria.", "method": "GIER utilizes natural language descriptions of reasoning gaps, prompting the model to critique and refine its own outputs iteratively.", "result": "GIER improves rationale quality, grounding, and reasoning alignment across tasks without degrading task accuracy, as demonstrated on SciFact, PrivacyQA, and e-SNLI using four different LLMs.", "conclusion": "The framework successfully allows models to interpret abstract reasoning gaps and translate them into concrete improvements in reasoning quality.", "key_contributions": ["Introduction of a new framework (GIER) for LLM output enhancement.", "Demonstrated improvements in rationale quality and reasoning alignment across multiple LLMs.", "Provided insights into the self-reflective capabilities of LLMs based on conceptual quality criteria."], "limitations": "", "keywords": ["large language models", "self-reflection", "response enhancement", "reasoning gaps", "iterative refinement"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.01609", "pdf": "https://arxiv.org/pdf/2509.01609.pdf", "abs": "https://arxiv.org/abs/2509.01609", "title": "Quantifying the Effect of Thermal Illusions in Virtual Reality", "authors": ["Yannick Weiss", "Marlene Eder", "Oguzhan Cesur", "Steeven Villa"], "categories": ["cs.HC"], "comment": "10 pages, 4 figures", "summary": "Thermal sensations are central to how we experience the world, yet most\nvirtual and extended reality systems fail to simulate them effectively. While\nhardware-based thermal displays can provide accurate temperature changes, they\nare often bulky, power-intensive, and restrict user mobility. Consequently,\nrecent works have explored thermal illusions, perceptual effects that rely on\ncross-modal interactions, to achieve thermal experiences without physical\nheating or cooling. While thermal illusions have been shown to consistently\nalter subjective ratings, the actual extent of their effect on the perceived\ntemperature of interacted objects remains unexplored. To address this, we\ncontribute the findings of two user studies following psychophysical\nprocedures. We first ordered and scaled the effects of a variety of visual and\nauditory cues (N=20) and subsequently quantified their isolated and combined\nefficacy in offsetting physical temperature changes (N=24). We found that\nthermal illusions elicited robust changes in subjective judgments, and auditory\ncues showed potential as an alternative or complementary approach to\nestablished visual techniques. However, the actual effects induced by thermal\nillusions were relatively small (+-0.5{\\deg}C) and did not consistently align\nwith abstract ratings, suggesting a need to reconsider how future thermal\nillusions or experiences are designed and evaluated.", "AI": {"tldr": "This paper investigates the effectiveness of thermal illusions in simulating temperature sensations in virtual and extended reality systems.", "motivation": "To explore how thermal illusions can enhance virtual experiences by simulating temperature sensations without bulky hardware.", "method": "Two user studies were conducted to order and scale the effects of various visual and auditory cues on perceived temperature, involving 20 and 24 participants respectively.", "result": "Thermal illusions produced significant changes in subjective temperature ratings, with auditory cues showing promise as alternatives or complements to visual methods, though effects were small (+-0.5°C).", "conclusion": "The findings suggest reconsideration of the design and evaluation of thermal illusions due to the modest effects recorded.", "key_contributions": ["Introduction of auditory cues as a potential method for enhancing thermal illusions", "Quantification of the efficacy of thermal illusions in temperature perception", "Order and scaling of visual and auditory cues based on user studies"], "limitations": "The extent of the effect on perceived temperature was small and not always aligned with abstract ratings, indicating limitations in the current approaches.", "keywords": ["thermal illusions", "virtual reality", "temperature perception", "cross-modal interactions", "user studies"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.00375", "pdf": "https://arxiv.org/pdf/2509.00375.pdf", "abs": "https://arxiv.org/abs/2509.00375", "title": "Open Data Synthesis For Deep Research", "authors": ["Ziyi Xia", "Kun Luo", "Hongjin Qian", "Zheng Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly expected to go beyond simple\nfactual queries toward Deep Research-tasks that require decomposing questions\ninto sub-problems, coordinating multi-step reasoning, and synthesizing evidence\nfrom diverse sources. We formalize Deep Research tasks with verifiable answers\nas Hierarchical Constraint Satisfaction Problems (HCSPs), which are\nfundamentally different from single-constraint, multi-hop, or flat CSP\nformulations. However, existing benchmarks (e.g., Natural Questions, HotpotQA)\nfail to capture this complexity, while recent synthetic datasets often\nintroduce shortcut reasoning, knowledge leakage, or lack sufficient structural\ndepth. To address this gap, we introduce InfoSeek, a scalable framework for\nsynthesizing complex Deep Research tasks. InfoSeek uses a dual-agent system to\nrecursively build a Research Tree from large-scale webpages, blurring\nintermediate nodes into valid sub-problems, and converting these trees into\nnatural language questions that require traversing the full hierarchy. It also\nenables rapid scaling, yielding over 50K training examples, a curated test set,\nand reasoning trajectories generated via reject sampling. Experiments show that\nmodels trained on InfoSeek consistently outperform strong baselines. On a\nchallenging benchmark BrowseComp-Plus, 3B LLMs optimized with InfoSeek surpass\nmuch larger 32B models and lightweight commercial APIs (e.g., Gemini2.5-Flash),\nwhile achieving performance comparable to stronger APIs (e.g., Gemini2.5-Pro).\nBy preserving meta-information such as intermediate steps and retrieval labels,\nInfoSeek further supports advanced optimization strategies, including compound\nreward design and trajectory-level exploration. We provide our codes and\ndatasets in \\href{https://github.com/VectorSpaceLab/InfoSeek}{this repository}.", "AI": {"tldr": "This paper introduces InfoSeek, a scalable framework for synthesizing complex Deep Research tasks, which formalizes these tasks as Hierarchical Constraint Satisfaction Problems (HCSPs).", "motivation": "Current benchmarks fail to capture the complexity of Deep Research tasks that LLMs are expected to perform, leading to inadequate training and evaluation.", "method": "InfoSeek employs a dual-agent system to recursively build a Research Tree from large-scale webpages, transforming complex tasks into natural language questions requiring multi-step reasoning.", "result": "Experiments show that models trained on InfoSeek outperform strong baselines, achieving better results on BrowseComp-Plus than larger models and commercial APIs.", "conclusion": "InfoSeek demonstrates the potential to improve LLM performance in Deep Research tasks by preserving meta-information and supporting advanced optimization strategies.", "key_contributions": ["Introduction of Hierarchical Constraint Satisfaction Problems (HCSPs) for Deep Research tasks", "Development of a dual-agent system for synthesizing complex research questions", "Provision of a large dataset and framework enabling advanced optimization strategies"], "limitations": "", "keywords": ["Large Language Models", "Hierarchical Constraint Satisfaction Problems", "Deep Research Tasks"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.01628", "pdf": "https://arxiv.org/pdf/2509.01628.pdf", "abs": "https://arxiv.org/abs/2509.01628", "title": "An Interactive Google Earth Engine Application for Global Multi-Scale Vegetation Analysis Using NDVI Thresholding", "authors": ["Md. Moktader Moula", "Israt Jahan Shonom", "Azharul Islam", "Mohammad Mosharraf Hossain"], "categories": ["cs.HC"], "comment": null, "summary": "Monitoring vegetation dynamics is crucial for addressing global environmental\nchallenges like degradation and deforestation, but traditional remote sensing\nmethods are often complex and resource-intensive. To overcome these barriers,\nwe developed an interactive, cloud-based application on the Google Earth Engine\n(GEE) platform for few clicks on-demand global vegetation analysis without\ncomplex technical knowledge. The application automates the calculation of\nvegetated areas using the Normalized Difference Vegetation Index (NDVI) derived\nfrom Sentinel-2 and Landsat imagery. It utilizes a median composite of images\nover a selected period to create a single, robust, cloud-free image, minimizing\natmospheric noise and other artifacts. It offers a flexible, global multi-scale\nanalytical platform, allowing users to define regions of interest based on\nadministrative boundaries, protected areas, or custom-drawn polygons. The\nuser-friendly interface enables the selection of specific time periods and NDVI\nthresholds to quantify vegetation cover in real time, eliminating the need for\nmanual and time intensive data handling and processing. A validation of the\nplatform was conducted for two protected areas in Bangladesh which demonstrated\nhigh accuracy, with area estimates showing over 97% agreement with published\nreference data. By simplifying access to powerful geospatial analytics to\ngeneral people, this tool provides a scalable and practical solution for\nresearchers, land managers, policymakers, and any interested person to monitor\nvegetation trends, support conservation efforts, to inform decision making in\nspatial context where policy maker need to use insights in few clicks and\ninform environmental policy.", "AI": {"tldr": "An interactive cloud-based application for simplified global vegetation analysis using NDVI.", "motivation": "To provide an accessible solution for monitoring vegetation dynamics without complex technical expertise.", "method": "An application on Google Earth Engine automates NDVI calculations using Sentinel-2 and Landsat imagery, allowing users to create cloud-free vegetation images from a median composite of data.", "result": "Validation in two protected areas in Bangladesh showed over 97% agreement with reference data for area estimates.", "conclusion": "The tool enables non-experts to easily access geospatial analytics for effective vegetation monitoring and policy support.", "key_contributions": ["User-friendly cloud-based platform for vegetation analysis", "Automation of NDVI calculations to reduce complexity and resource needs", "High accuracy validation results supporting real-time decision making"], "limitations": "", "keywords": ["vegetation dynamics", "NDVI", "remote sensing", "cloud-based application", "Google Earth Engine"], "importance_score": 3, "read_time_minutes": 5}}
{"id": "2509.00388", "pdf": "https://arxiv.org/pdf/2509.00388.pdf", "abs": "https://arxiv.org/abs/2509.00388", "title": "GraphKV: Breaking the Static Selection Paradigm with Graph-Based KV Cache Eviction", "authors": ["Xuelin Li", "Xiangqi Jin", "Linfeng Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Efficient Key-Value (KV) cache management is essential for processing long\ntext sequences in large language models (LLMs), where memory constraints often\nlimit performance. Conventional KV eviction strategies, such as top-k selection\nbased on attention scores, depend on static heuristics that fail to capture the\nevolving implicit dependencies among tokens during inference. To overcome this,\nwe propose GraphKV, a graph-based framework that redefines token selection for\nKV cache compression. In GraphKV, tokens are modeled as nodes with importance\nscores, and edges represent their similarity relationships. Through a\ndecay-signal-propagation mechanism, token importance is dynamically updated by\npropagating information across the graph, enabling adaptive retention of the\nmost contextually significant tokens. GraphKV can be seamlessly utilized in\nexisting KV cache eviction methods such as SnapKV and PyramidKV in a\nplug-and-play manner. Codes will be released on Github.", "AI": {"tldr": "GraphKV introduces a graph-based framework for efficient key-value cache management in large language models, improving token selection by dynamically updating importance scores based on similarity relationships.", "motivation": "The motivation is to improve the efficiency of KV cache management in large language models, addressing limitations of conventional eviction strategies that rely on static heuristics.", "method": "GraphKV models tokens as nodes within a graph where edges signify similarity relationships. It employs a decay-signal-propagation mechanism to dynamically update token importance scores during inference.", "result": "GraphKV enhances the adaptive retention of contextually significant tokens in the KV cache, leading to better performance in processing long text sequences.", "conclusion": "GraphKV can be integrated into existing KV eviction methods like SnapKV and PyramidKV, offering a more efficient and adaptive approach to KV cache management.", "key_contributions": ["Introduction of a graph-based framework for KV cache management", "Dynamic updating of token importance scores through signal propagation", "Compatibility with existing KV eviction methods"], "limitations": "", "keywords": ["Key-Value Cache", "Large Language Models", "Graph-Based Framework"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2509.01786", "pdf": "https://arxiv.org/pdf/2509.01786.pdf", "abs": "https://arxiv.org/abs/2509.01786", "title": "EgoTouch: On-Body Touch Input Using AR/VR Headset Cameras", "authors": ["Vimal Mollyn", "Chris Harrison"], "categories": ["cs.HC", "cs.CV", "cs.RO"], "comment": "Published at UIST 2024. More info at\n  https://www.figlab.com/research/2024/egotouch", "summary": "In augmented and virtual reality (AR/VR) experiences, a user's arms and hands\ncan provide a convenient and tactile surface for touch input. Prior work has\nshown on-body input to have significant speed, accuracy, and ergonomic benefits\nover in-air interfaces, which are common today. In this work, we demonstrate\nhigh accuracy, bare hands (i.e., no special instrumentation of the user) skin\ninput using just an RGB camera, like those already integrated into all modern\nXR headsets. Our results show this approach can be accurate, and robust across\ndiverse lighting conditions, skin tones, and body motion (e.g., input while\nwalking). Finally, our pipeline also provides rich input metadata including\ntouch force, finger identification, angle of attack, and rotation. We believe\nthese are the requisite technical ingredients to more fully unlock on-skin\ninterfaces that have been well motivated in the HCI literature but have lacked\nrobust and practical methods.", "AI": {"tldr": "Demonstrates accurate skin input using RGB cameras in AR/VR, enhancing touch interface capabilities.", "motivation": "To address the limitations of existing in-air interfaces by enabling high-accuracy on-body input in AR/VR environments.", "method": "Utilizes an RGB camera to capture skin input without special instrumentation, assessing performance across various conditions.", "result": "Achieves high accuracy in touch input, demonstrating robustness in diverse lighting, skin tones, and motion.", "conclusion": "The developed pipeline unlocks new possibilities for on-skin interfaces, bolstering the potential for richer human-computer interaction.", "key_contributions": ["Demonstration of bare hands skin input accuracy using RGB cameras", "Robustness across lighting conditions and body movements", "Rich input metadata including touch force and finger identification"], "limitations": "", "keywords": ["augmented reality", "virtual reality", "on-body input", "human-computer interaction", "touch interfaces"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2509.00391", "pdf": "https://arxiv.org/pdf/2509.00391.pdf", "abs": "https://arxiv.org/abs/2509.00391", "title": "The Resurgence of GCG Adversarial Attacks on Large Language Models", "authors": ["Yuting Tan", "Xuying Li", "Zhuo Li", "Huizhen Shu", "Peikang Hu"], "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "comment": "12 pages, 5 figures", "summary": "Gradient-based adversarial prompting, such as the Greedy Coordinate Gradient\n(GCG) algorithm, has emerged as a powerful method for jailbreaking large\nlanguage models (LLMs). In this paper, we present a systematic appraisal of GCG\nand its annealing-augmented variant, T-GCG, across open-source LLMs of varying\nscales. Using Qwen2.5-0.5B, LLaMA-3.2-1B, and GPT-OSS-20B, we evaluate attack\neffectiveness on both safety-oriented prompts (AdvBench) and\nreasoning-intensive coding prompts. Our study reveals three key findings: (1)\nattack success rates (ASR) decrease with model size, reflecting the increasing\ncomplexity and non-convexity of larger models' loss landscapes; (2)\nprefix-based heuristics substantially overestimate attack effectiveness\ncompared to GPT-4o semantic judgments, which provide a stricter and more\nrealistic evaluation; and (3) coding-related prompts are significantly more\nvulnerable than adversarial safety prompts, suggesting that reasoning itself\ncan be exploited as an attack vector. In addition, preliminary results with\nT-GCG show that simulated annealing can diversify adversarial search and\nachieve competitive ASR under prefix evaluation, though its benefits under\nsemantic judgment remain limited. Together, these findings highlight the\nscalability limits of GCG, expose overlooked vulnerabilities in reasoning\ntasks, and motivate further development of annealing-inspired strategies for\nmore robust adversarial evaluation.", "AI": {"tldr": "This paper evaluates the Greedy Coordinate Gradient (GCG) algorithm and its variant T-GCG in successfully attacking large language models (LLMs).", "motivation": "To systematically assess the effectiveness of gradient-based adversarial prompting methods on LLMs and identify vulnerabilities in their responses to varying types of prompts.", "method": "The study involves testing GCG and T-GCG algorithms on open-source models (Qwen2.5-0.5B, LLaMA-3.2-1B, GPT-OSS-20B) using safety-oriented and reasoning-intensive prompts, with an analysis of attack success rates (ASR).", "result": "Findings indicate that ASR decreases with model size, prefix-based heuristics overestimate effectiveness, and coding prompts are more exploitable than safety prompts. Preliminary T-GCG results show some benefits from simulated annealing but limited under strict evaluation measures.", "conclusion": "The research underscores limitations in GCG scalability, reveals vulnerabilities in reasoning tasks, and suggests the need for improving adversarial evaluation methods via annealing techniques.", "key_contributions": ["Systematic appraisal of GCG and T-GCG on various LLMs", "Discovery of declining attack success rates with model size", "Identification of coding-related prompts as more vulnerable than safety prompts"], "limitations": "Preliminary results indicate T-GCG's benefits may be limited under semantic judgment evaluations.", "keywords": ["adversarial prompting", "large language models", "Greedy Coordinate Gradient", "safety-oriented prompts", "reasoning tasks"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2509.01845", "pdf": "https://arxiv.org/pdf/2509.01845.pdf", "abs": "https://arxiv.org/abs/2509.01845", "title": "Community-Centered Spatial Intelligence for Climate Adaptation at Nova Scotia's Eastern Shore", "authors": ["Gabriel Spadon", "Oladapo Oyebode", "Camilo M. Botero", "Tushar Sharma", "Floris Goerlandt", "Ronald Pelot"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "This paper presents an overview of a human-centered initiative aimed at\nstrengthening climate resilience along Nova Scotia's Eastern Shore. This\nregion, a collection of rural villages with deep ties to the sea, faces\nexistential threats from climate change that endanger its way of life. Our\nproject moves beyond a purely technical response, weaving together expertise\nfrom Computer Science, Industrial Engineering, and Coastal Geography to\nco-create tools with the community. By integrating generational knowledge of\nresidents, particularly elders, through the Eastern Shore Citizen Science\nCoastal Monitoring Network, this project aims to collaborate in building a\nliving digital archive. This effort is hosted under Dalhousie University's\nTransforming Climate Action (TCA) initiative, specifically through its\nTransformative Adaptations to Social-Ecological Climate Change Trajectories\n(TranSECT) and TCA Artificial Intelligence (TCA-AI) projects. This work is\ndriven by a collaboration model in which student teams work directly with\nresidents. We present a detailed project timeline and a replicable model for\nhow technology can support traditional communities, enabling them to navigate\nclimate transformation more effectively.", "AI": {"tldr": "An initiative to enhance climate resilience in Nova Scotia's Eastern Shore through community collaboration and technology integration.", "motivation": "To address climate change threats faced by rural communities in Nova Scotia and maintain their way of life.", "method": "A collaborative approach involving expertise from Computer Science, Industrial Engineering, and Coastal Geography, along with community engagement, particularly with elders, to create a living digital archive.", "result": "The project successfully integrates traditional knowledge and modern technology to strengthen community resilience against climate change.", "conclusion": "The initiative demonstrates a replicable model for using technology to support and empower traditional communities in climate adaptation.", "key_contributions": ["Development of a living digital archive co-created with the community", "Integration of interdisciplinary expertise to tackle climate challenges", "Establishment of a replicable collaboration model for community-driven tech solutions"], "limitations": "", "keywords": ["climate resilience", "community engagement", "digital archive", "interdisciplinary collaboration", "coastal monitoring"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2509.00414", "pdf": "https://arxiv.org/pdf/2509.00414.pdf", "abs": "https://arxiv.org/abs/2509.00414", "title": "MedSEBA: Synthesizing Evidence-Based Answers Grounded in Evolving Medical Literature", "authors": ["Juraj Vladika", "Florian Matthes"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Accepted to CIKM 2025", "summary": "In the digital age, people often turn to the Internet in search of medical\nadvice and recommendations. With the increasing volume of online content, it\nhas become difficult to distinguish reliable sources from misleading\ninformation. Similarly, millions of medical studies are published every year,\nmaking it challenging for researchers to keep track of the latest scientific\nfindings. These evolving studies can reach differing conclusions, which is not\nreflected in traditional search tools. To address these challenges, we\nintroduce MedSEBA, an interactive AI-powered system for synthesizing\nevidence-based answers to medical questions. It utilizes the power of Large\nLanguage Models to generate coherent and expressive answers, but grounds them\nin trustworthy medical studies dynamically retrieved from the research database\nPubMed. The answers consist of key points and arguments, which can be traced\nback to respective studies. Notably, the platform also provides an overview of\nthe extent to which the most relevant studies support or refute the given\nmedical claim, and a visualization of how the research consensus evolved\nthrough time. Our user study revealed that medical experts and lay users find\nthe system usable and helpful, and the provided answers trustworthy and\ninformative. This makes the system well-suited for both everyday health\nquestions and advanced research insights.", "AI": {"tldr": "MedSEBA is an AI-powered system that synthesizes evidence-based answers to medical questions using Large Language Models and PubMed data.", "motivation": "The challenge of distinguishing reliable medical information amid vast online content and numerous medical studies published yearly.", "method": "The system utilizes Large Language Models to generate answers based on dynamically retrieved studies from PubMed, presenting key points, arguments, and visualizing research consensus over time.", "result": "User studies indicated that both medical experts and lay users found MedSEBA usable, helpful, and provided trustworthy answers.", "conclusion": "MedSEBA is an effective tool for answering everyday health questions and providing insights for advanced medical research.", "key_contributions": ["The integration of LLMs with dynamically retrieved medical studies.", "The ability to visualize the evolution of research consensus.", "User study results showing high usability and perceived trustworthiness."], "limitations": "", "keywords": ["Artificial Intelligence", "Health Informatics", "Large Language Models"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.02100", "pdf": "https://arxiv.org/pdf/2509.02100.pdf", "abs": "https://arxiv.org/abs/2509.02100", "title": "E-THER: A PCT-Grounded Dataset for Benchmarking Empathic AI", "authors": ["Sharjeel Tahir", "Judith Johnson", "Jumana Abu-Khalaf", "Syed Afaq Ali Shah"], "categories": ["cs.HC", "cs.CL"], "comment": "15 pages, 4 figures. Preprint", "summary": "A prevalent shortfall among current empathic AI systems is their inability to\nrecognize when verbal expressions may not fully reflect underlying emotional\nstates. This is because the existing datasets, used for the training of these\nsystems, focus on surface-level emotion recognition without addressing the\ncomplex verbal-visual incongruence (mismatch) patterns useful for empathic\nunderstanding. In this paper, we present E-THER, the first Person-Centered\nTherapy-grounded multimodal dataset with multidimensional annotations for\nverbal-visual incongruence detection, enabling training of AI systems that\ndevelop genuine rather than performative empathic capabilities. The annotations\nincluded in the dataset are drawn from humanistic approach, i.e., identifying\nverbal-visual emotional misalignment in client-counsellor interactions -\nforming a framework for training and evaluating AI on empathy tasks. Additional\nengagement scores provide behavioral annotations for research applications.\nNotable gains in empathic and therapeutic conversational qualities are observed\nin state-of-the-art vision-language models (VLMs), such as IDEFICS and\nVideoLLAVA, using evaluation metrics grounded in empathic and therapeutic\nprinciples. Empirical findings indicate that our incongruence-trained models\noutperform general-purpose models in critical traits, such as sustaining\ntherapeutic engagement, minimizing artificial or exaggerated linguistic\npatterns, and maintaining fidelity to PCT theoretical framework.", "AI": {"tldr": "The paper introduces E-THER, a novel multimodal dataset for detecting verbal-visual incongruence in empathic AI systems, enabling better training for genuine empathy and therapeutic engagement.", "motivation": "Current empathic AI systems struggle to recognize emotional states due to superficial emotion recognition datasets that overlook verbal-visual incongruence.", "method": "E-THER is a Person-Centered Therapy-based dataset with multidimensional annotations focusing on verbal-visual emotional misalignment in client-counsellor interactions, aimed at enhancing empathic AI training.", "result": "Empirically, models trained on the E-THER dataset show improved performance in empathic and therapeutic dialogue qualities compared to general-purpose models, as they better sustain engagement and align with therapeutic principles.", "conclusion": "The findings highlight the importance of verbal-visual congruence in developing empathetic AI systems capable of genuine therapeutic interactions.", "key_contributions": ["Introduction of the E-THER dataset for verbal-visual incongruence detection", "Multi-dimensional annotations grounded in Person-Centered Therapy", "Demonstrated improvement in conversational qualities of AI models through incongruence training"], "limitations": "", "keywords": ["empathy", "AI", "therapeutic engagement", "verbal-visual incongruence", "multimodal dataset"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.00425", "pdf": "https://arxiv.org/pdf/2509.00425.pdf", "abs": "https://arxiv.org/abs/2509.00425", "title": "The Gold Medals in an Empty Room: Diagnosing Metalinguistic Reasoning in LLMs with Camlang", "authors": ["Fenghua Liu", "Yulong Chen", "Yixuan Liu", "Zhujun Jin", "Solomon Tsai", "Ming Zhong"], "categories": ["cs.CL"], "comment": "Working in progress", "summary": "Large Language Models (LLMs) achieve gold-medal performance across many\nbenchmarks, yet it remains unclear whether such success reflects genuine\nreasoning or pattern matching. From a cognitive science perspective, an\ninformative test is whether models can master an unfamiliar language through\nexplicit metalinguistic deductive learning, a paradigm where human learners can\nreliably internalise grammatical systems through metalinguistic reasoning. We\naddress this question with Camlang, a novel constructed language that exhibits\nnaturalistic yet unattested feature combinations. Camlang consists of two\nexplicit resources, a grammar book and a bilingual dictionary, which mirror\nadult second-language learning via explicit grammar rules and lexical lookup,\nand enable us to disentangle errors in morpho-syntax, lexical semantics, and\nsentence-level reasoning. Human experiments show that these resources are\nsufficient for participants to acquire Camlang and successfully solve Camlang\ntasks. To operationalise evaluation, we adapt CommonsenseQA into Camlang,\ncreating Camlang-CSQA-v0, the first task in a broader suite where solving\nquestions requires applying grammar rules and lexical mappings. Experimental\nresults show that GPT-5 achieves 98\\% EM accuracy in English but only 47\\% in\nCamlang, far below human performance at 87\\%, while other state-of-the-art\nreasoning LLMs perform even worse. Human verification further reveals that most\nmodel successes stem from shallow lexical alignment while GPT-5 shows emerging\nmetalinguistic awareness to a limited extent but not systematic grammatical\nmastery as humans. Camlang establishes a cognitively grounded evaluation\nparadigm that exposes fundamental gaps between current models and human\nmetalinguistic competence.", "AI": {"tldr": "The paper tests Large Language Models' reasoning abilities through a novel language learning framework using Camlang, revealing significant gaps in their metalinguistic competence compared to humans.", "motivation": "To explore whether Large Language Models can demonstrate genuine reasoning capabilities rather than mere pattern matching, particularly in the context of language acquisition.", "method": "Camlang, a constructed language with explicit resources for grammar and vocabulary, was used to conduct human experiments and evaluate LLM performance in language learning tasks.", "result": "GPT-5 achieved 98% EM accuracy in English but only 47% in Camlang, which is significantly lower than human performance (87%), with the model's success largely attributed to shallow lexical alignment.", "conclusion": "Current LLMs exhibit limited metalinguistic awareness and do not achieve systematic grammatical mastery, highlighting essential gaps in their reasoning abilities compared to human learners.", "key_contributions": ["Introduction of Camlang as a novel evaluation paradigm for LLM reasoning.", "Demonstration of significant performance differences between LLMs and humans in language learning tasks.", "Empirical evidence of the limitations of LLMs in mastering grammar and metalinguistic reasoning."], "limitations": "The study is a working progress and may not comprehensively test all aspects of metalinguistic competence in LLMs.", "keywords": ["Large Language Models", "metalinguistic competence", "language acquisition", "cognitive science", "language learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.02132", "pdf": "https://arxiv.org/pdf/2509.02132.pdf", "abs": "https://arxiv.org/abs/2509.02132", "title": "Shared Control for Game Accessibility: Understanding Current Human Cooperation Practices to Inform the Design of Partial Automation Solutions", "authors": ["Dragan Ahmetovic", "Matteo Manzoni", "Filippo Corti", "Sergio Mascetti"], "categories": ["cs.HC"], "comment": "26 pages, 1 figure", "summary": "Shared control is a form of video gaming accessibility support that allows\nplayers with disabilities to delegate inaccessible controls to another person.\nThrough interviews involving 14 individuals with lived experience of accessible\ngaming in shared control, we explore the ways in which shared control\ntechnologies are adopted in practice, the accessibility challenges they\naddress, and how the support currently provided in shared control can be\nautomated to remove the need for a human assistant. Findings indicate that\nshared control is essential for enabling access to otherwise inaccessible\ngames, but its reliance on human support is a key limitation. Participants\nwelcomed the idea of automating the support with software agents, while also\nidentifying limitations and design requirements. Accordingly, this work\ncontributes insights into current practices and proposes guidelines for\ndeveloping automated support systems.", "AI": {"tldr": "Explores shared control in accessible gaming, highlighting the need for automation of support systems to ease access for players with disabilities.", "motivation": "To investigate how shared control technologies are used by individuals with disabilities in gaming, and to address the limitations of human assistance in these scenarios.", "method": "Interviews conducted with 14 individuals having lived experience of accessible gaming using shared control technologies.", "result": "Shared control is vital for accessing inaccessible games, but it heavily relies on human support, which can be mitigated through automation.", "conclusion": "The study suggests that while shared control enhances gaming accessibility, moving toward automated support systems could improve the experience and reduce dependency on human assistance.", "key_contributions": ["Insights into current practices in shared control for gaming accessibility", "Identification of limitations in human-supported shared control", "Proposals for designing automated support systems"], "limitations": "The study primarily focuses on subjective experiences, which may not generalize to all shared control technologies or gaming contexts.", "keywords": ["shared control", "gaming accessibility", "automation", "disabilities", "support systems"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.00449", "pdf": "https://arxiv.org/pdf/2509.00449.pdf", "abs": "https://arxiv.org/abs/2509.00449", "title": "GOSU: Retrieval-Augmented Generation with Global-Level Optimized Semantic Unit-Centric Framework", "authors": ["Xuecheng Zou", "Ke Liu", "Bingbing Wang", "Huafei Deng", "Li Zhang", "Yu Tang"], "categories": ["cs.CL"], "comment": null, "summary": "Building upon the standard graph-based Retrieval-Augmented Generation (RAG),\nthe introduction of heterogeneous graphs and hypergraphs aims to enrich\nretrieval and generation by leveraging the relationships between multiple\nentities through the concept of semantic units (SUs). But this also raises a\nkey issue: The extraction of high-level SUs limited to local text chunks is\nprone to ambiguity, complex coupling, and increased retrieval overhead due to\nthe lack of global knowledge or the neglect of fine-grained relationships. To\naddress these issues, we propose GOSU, a semantic unit-centric RAG framework\nthat efficiently performs global disambiguation and utilizes SUs to capture\ninterconnections between different nodes across the global context. In the\ngraph construction phase, GOSU performs global merging on the pre-extracted SUs\nfrom local text chunks and guides entity and relationship extraction, reducing\nthe difficulty of coreference resolution while uncovering global semantic\nobjects across text chunks. In the retrieval and generation phase, we introduce\nhierarchical keyword extraction and semantic unit completion. The former\nuncovers the fine-grained binary relationships overlooked by the latter, while\nthe latter compensates for the coarse-grained n-ary relationships missing from\nthe former. Evaluation across multiple tasks demonstrates that GOSU outperforms\nthe baseline RAG methods in terms of generation quality.", "AI": {"tldr": "GOSU is a framework that enhances graph-based RAG by integrating heterogeneous graphs and hypergraphs, improving retrieval and generation through global disambiguation and semantic unit extraction.", "motivation": "To address the limitations of standard RAG in capturing fine-grained relationships and global knowledge by leveraging semantic units and reducing ambiguity.", "method": "GOSU constructs heterogeneous graphs and hypergraphs, performing global merging of semantic units and introducing hierarchical keyword extraction and semantic unit completion during retrieval and generation.", "result": "GOSU demonstrates improved performance over baseline RAG methods across various tasks, particularly in generation quality.", "conclusion": "GOSU offers a more effective approach for semantic unit extraction and relationship handling in RAG frameworks, which could significantly enhance applications in natural language processing.", "key_contributions": ["Introduction of GOSU framework for RAG", "Global disambiguation of semantic units", "Hierarchical keyword extraction to refine relationships"], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Heterogeneous graphs", "Semantic units"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.02144", "pdf": "https://arxiv.org/pdf/2509.02144.pdf", "abs": "https://arxiv.org/abs/2509.02144", "title": "A Theoretical Framework of the Processes of Change in Psychotherapy Delivered by Artificial Agents", "authors": ["Arthur Bran Herbener", "Malene Flensborg Damholdt"], "categories": ["cs.HC", "cs.AI"], "comment": "Submitted on 19 March 2025", "summary": "The question of whether artificial agents (e.g., chatbots and social robots)\ncan replace human therapists has received notable attention following the\nrecent launch of large language models. However, little is known about the\nprocesses of change in psychotherapy delivered by artificial agents. To\nfacilitate hypothesis development and stimulate scientific debate, the present\narticle offers the first theoretical framework of the processes of change in\npsychotherapy delivered by artificial agents. The theoretical framework rests\nupon a conceptual analysis of what active ingredients may be inherently linked\nto the presence of human therapists. We propose that human therapists'\nontological status as human beings and sociocultural status as socially\nsanctioned healthcare professionals play crucial roles in promoting treatment\noutcomes. In the absence of the ontological and sociocultural status of human\ntherapists, we propose what we coin the genuineness gap and credibility gap can\nemerge and undermine key processes of change in psychotherapy. Based on these\npropositions, we propose avenues for scientific investigations and practical\napplications aimed at leveraging the strengths of artificial agents and human\ntherapists respectively. We also highlight the intricate agentic nature of\nartificial agents and discuss how this complicates endeavors to establish\nuniversally applicable propositions regarding the processes of change in these\ninterventions.", "AI": {"tldr": "This paper presents a theoretical framework analyzing the effectiveness of psychotherapy delivered by artificial agents in comparison to human therapists, focusing on the influence of the therapists' human status on treatment outcomes.", "motivation": "To explore the processes of change in psychotherapy delivered by artificial agents, which remains under-researched despite the rise of large language models.", "method": "The paper offers a conceptual analysis of the inherent active ingredients linked to human therapists in the context of therapy delivered by artificial agents.", "result": "The authors identify the 'genuineness gap' and 'credibility gap' that may arise when replacing human therapists with artificial agents, potentially undermining treatment outcomes.", "conclusion": "The study proposes avenues for further scientific investigation into leveraging both artificial agents and human therapists effectively, while calling attention to the complex nature of artificial agents.", "key_contributions": ["First theoretical framework for psychotherapy processes involving artificial agents", "Identification of the genuineness and credibility gaps", "Discussion of the unique agentic nature of artificial agents in therapeutic contexts"], "limitations": "", "keywords": ["artificial agents", "psychotherapy", "human therapists", "credibility gap", "HCI"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.00457", "pdf": "https://arxiv.org/pdf/2509.00457.pdf", "abs": "https://arxiv.org/abs/2509.00457", "title": "CVPD at QIAS 2025 Shared Task: An Efficient Encoder-Based Approach for Islamic Inheritance Reasoning", "authors": ["Salah Eddine Bekhouche", "Abdellah Zakaria Sellam", "Hichem Telli", "Cosimo Distante", "Abdenour Hadid"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Islamic inheritance law (Ilm al-Mawarith) requires precise identification of\nheirs and calculation of shares, which poses a challenge for AI. In this paper,\nwe present a lightweight framework for solving multiple-choice inheritance\nquestions using a specialised Arabic text encoder and Attentive Relevance\nScoring (ARS). The system ranks answer options according to semantic relevance,\nand enables fast, on-device inference without generative reasoning. We evaluate\nArabic encoders (MARBERT, ArabicBERT, AraBERT) and compare them with API-based\nLLMs (Gemini, DeepSeek) on the QIAS 2025 dataset. While large models achieve an\naccuracy of up to 87.6%, they require more resources and are context-dependent.\nOur MARBERT-based approach achieves 69.87% accuracy, presenting a compelling\ncase for efficiency, on-device deployability, and privacy. While this is lower\nthan the 87.6% achieved by the best-performing LLM, our work quantifies a\ncritical trade-off between the peak performance of large models and the\npractical advantages of smaller, specialized systems in high-stakes domains.", "AI": {"tldr": "This paper presents a lightweight framework for solving Islamic inheritance law questions using a specialized Arabic text encoder, emphasizing efficiency and on-device inference.", "motivation": "The need for precise identification of heirs and calculation of shares in Islamic inheritance law presents challenges for AI applications.", "method": "The framework utilizes a specialized Arabic text encoder and Attentive Relevance Scoring (ARS) to rank answer options based on semantic relevance, allowing for fast, on-device inference.", "result": "The MARBERT-based system achieves 69.87% accuracy, demonstrating efficiency and privacy advantages over larger API-based models that reach up to 87.6% accuracy but require more resources.", "conclusion": "While the MARBERT approach has lower peak performance than the best LLMs, it highlights the trade-off between accuracy and practical deployment in sensitive areas like inheritance law.", "key_contributions": ["Lightweight framework for solving Islamic inheritance questions", "Use of specialized Arabic text encoder for efficient processing", "Evaluation of various Arabic encoders compared to large LLMs"], "limitations": "", "keywords": ["Islamic inheritance law", "Arabic text encoding", "Attentive Relevance Scoring", "on-device inference", "machine learning"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2509.02274", "pdf": "https://arxiv.org/pdf/2509.02274.pdf", "abs": "https://arxiv.org/abs/2509.02274", "title": "Look: AI at Work! - Analysing Key Aspects of AI-support at the Work Place", "authors": ["Stefan Schiffer", "Anna Milena Rothermel", "Alexander Ferrein", "Astrid Rosenthal-von der Pütten"], "categories": ["cs.HC", "cs.AI"], "comment": "10 pages, accepted at the German Conference on Artificial\n  Intelligence KI 2024 Workshop \"HuMaIn\"", "summary": "In this paper we present an analysis of technological and psychological\nfactors of applying artificial intelligence (AI) at the work place. We do so\nfor a number of twelve application cases in the context of a project where AI\nis integrated at work places and in work systems of the future. From a\ntechnological point of view we mainly look at the areas of AI that the\napplications are concerned with. This allows to formulate recommendations in\nterms of what to look at in developing an AI application and what to pay\nattention to with regards to building AI literacy with different stakeholders\nusing the system. This includes the importance of high-quality data for\ntraining learning-based systems as well as the integration of human expertise,\nespecially with knowledge-based systems. In terms of the psychological factors\nwe derive research questions to investigate in the development of AI supported\nwork systems and to consider in future work, mainly concerned with topics such\nas acceptance, openness, and trust in an AI system.", "AI": {"tldr": "Analysis of technological and psychological factors for AI applications in workplaces through twelve case studies.", "motivation": "To explore the integration of AI in future work contexts and develop recommendations for effective AI application and literacy", "method": "Analysis of twelve specific application cases of AI at workplaces, focusing on technological areas and psychological factors.", "result": "Identification of key areas in AI application development, emphasizing high-quality data and human expertise, along with psychological factors affecting acceptance and trust.", "conclusion": "The study provides insights into improving AI literacy and developing AI systems that are accepted and trusted by users.", "key_contributions": ["Recommendations for developing AI applications in workplaces", "Identifies key psychological factors influencing AI acceptance", "Insights into the importance of high-quality data and human expertise"], "limitations": "", "keywords": ["Artificial Intelligence", "Workplace Integration", "AI Literacy", "Trust in AI", "Human-AI Interaction"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.00461", "pdf": "https://arxiv.org/pdf/2509.00461.pdf", "abs": "https://arxiv.org/abs/2509.00461", "title": "TECP: Token-Entropy Conformal Prediction for LLMs", "authors": ["Beining Xu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Uncertainty quantification (UQ) for open-ended language generation remains a\ncritical yet underexplored challenge, especially under black-box constraints\nwhere internal model signals are inaccessible. In this paper, we introduce\nToken-Entropy Conformal Prediction (TECP), a novel framework that leverages\ntoken-level entropy as a logit-free, reference-free uncertainty measure and\nintegrates it into a split conformal prediction (CP) pipeline to construct\nprediction sets with formal coverage guarantees. Unlike existing approaches\nthat rely on semantic consistency heuristics or white-box features, TECP\ndirectly estimates epistemic uncertainty from the token entropy structure of\nsampled generations and calibrates uncertainty thresholds via CP quantiles to\nensure provable error control. Empirical evaluations across six large language\nmodels and two benchmarks (CoQA and TriviaQA) demonstrate that TECP\nconsistently achieves reliable coverage and compact prediction sets,\noutperforming prior self-consistency-based UQ methods. Our method provides a\nprincipled and efficient solution for trustworthy generation in black-box LLM\nsettings.", "AI": {"tldr": "This paper presents Token-Entropy Conformal Prediction (TECP), a framework for uncertainty quantification in open-ended language generation that uses token-level entropy to measure uncertainty and ensure reliable predictions without needing model internals.", "motivation": "The need for effective uncertainty quantification in language generation, particularly under black-box conditions where internal signals of models are not accessible.", "method": "TECP leverages token-level entropy as a logit-free uncertainty measure and incorporates it into a split conformal prediction pipeline to construct prediction sets with formal coverage guarantees.", "result": "Empirical evaluations show that TECP outperforms existing self-consistency-based uncertainty quantification methods in terms of reliable coverage and compact prediction sets across multiple large language models.", "conclusion": "TECP offers a principled, efficient approach for achieving trustworthy language generation in settings where model internals are not available.", "key_contributions": ["Introduction of Token-Entropy Conformal Prediction (TECP) for uncertainty quantification.", "Direct estimation of epistemic uncertainty using token entropy.", "Empirical validation showing superiority over existing methods in various large language models."], "limitations": "", "keywords": ["uncertainty quantification", "language generation", "conformal prediction", "token entropy", "large language models"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.02284", "pdf": "https://arxiv.org/pdf/2509.02284.pdf", "abs": "https://arxiv.org/abs/2509.02284", "title": "Balaton Borders: Data Ceramics for Ecological Reflection", "authors": ["Hajnal Gyeviki", "Mihály Minkó", "Mary Karyda", "Damla Çay"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Balaton Borders translates ecological data from Lake Balaton into ceramic\ntableware that represents human impact on the landscape, from reedbed reduction\nto shoreline modification and land erosion. Designed for performative dining,\nthe pieces turn shared meals into multisensory encounters where food and data\nceramics spark collective reflection on ecological disruption.", "AI": {"tldr": "Balaton Borders transforms ecological data into ceramic tableware to create multisensory dining experiences that highlight human impact on the environment.", "motivation": "To raise awareness and provoke reflection on ecological disruption through the medium of ceramics and dining.", "method": "Designing ceramic tableware that incorporates ecological data, facilitating performative dining experiences.", "result": "The tableware engages users in collective reflection on the ecological changes in Lake Balaton, such as reedbed reduction and shoreline modification.", "conclusion": "By integrating ecological data into dining experiences, the project aims to enhance awareness of environmental issues.", "key_contributions": ["Translates ecological data into a tangible art form (ceramics)", "Creates multisensory dining experiences that encourage societal reflection", "Addresses issues of ecological disruption through innovative design"], "limitations": "", "keywords": ["ceramics", "ecological data", "performative dining", "environmental awareness", "Lake Balaton"], "importance_score": 2, "read_time_minutes": 5}}
{"id": "2509.00482", "pdf": "https://arxiv.org/pdf/2509.00482.pdf", "abs": "https://arxiv.org/abs/2509.00482", "title": "Talk Less, Call Right: Enhancing Role-Play LLM Agents with Automatic Prompt Optimization and Role Prompting", "authors": ["Saksorn Ruangtanusak", "Pittawat Taveekitworachai", "Kunat Pipatanakul"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "17 pages, 2 figures", "summary": "This report investigates approaches for prompting a tool-augmented large\nlanguage model (LLM) to act as a role-playing dialogue agent in the API track\nof the Commonsense Persona-grounded Dialogue Challenge (CPDC) 2025. In this\nsetting, dialogue agents often produce overly long in-character responses\n(over-speaking) while failing to use tools effectively according to the persona\n(under-acting), such as generating function calls that do not exist or making\nunnecessary tool calls before answering. We explore four prompting approaches\nto address these issues: 1) basic role prompting, 2) human-crafted role\nprompting, 3) automatic prompt optimization (APO), and 4) rule-based role\nprompting. The rule-based role prompting (RRP) approach achieved the best\nperformance through two novel techniques--character-card/scene-contract design\nand strict enforcement of function calling--which led to an overall score of\n0.571, improving on the zero-shot baseline score of 0.519. These findings\ndemonstrate that RRP design can substantially improve the effectiveness and\nreliability of role-playing dialogue agents compared with more elaborate\nmethods such as APO. To support future efforts in developing persona prompts,\nwe are open-sourcing all of our best-performing prompts and the APO tool.\nSource code is available at https://github.com/scb-10x/apo.", "AI": {"tldr": "This report explores four prompting methods to enhance tool-augmented LLMs as role-playing dialogue agents, finding that rule-based prompting significantly outperforms other techniques.", "motivation": "The need to improve dialogue agents' responses by preventing them from over-speaking and under-acting while utilizing tools effectively.", "method": "Four prompting approaches were investigated: basic role prompting, human-crafted role prompting, automatic prompt optimization, and rule-based role prompting, with various strategies for performance enhancement.", "result": "The rule-based role prompting (RRP) achieved the highest performance with a score of 0.571, surpassing the zero-shot baseline of 0.519 and demonstrating improved effectiveness in role-playing contexts.", "conclusion": "The findings highlight the potential of RRP design in enhancing dialogue agents' performance, and the authors are open-sourcing their prompts and tools for community use.", "key_contributions": ["Development of the rule-based role prompting approach", "Novel techniques: character-card/scene-contract design and function calling enforcement", "Open-sourcing effective prompts and tools for broader use"], "limitations": "", "keywords": ["large language models", "role-playing dialogue agents", "prompting strategies", "Commonsense Persona-grounded Dialogue Challenge"], "importance_score": 9, "read_time_minutes": 17}}
{"id": "2509.02367", "pdf": "https://arxiv.org/pdf/2509.02367.pdf", "abs": "https://arxiv.org/abs/2509.02367", "title": "Talking Spell: A Wearable System Enabling Real-Time Anthropomorphic Voice Interaction with Everyday Objects", "authors": ["Xuetong Wang", "Ching Christie Pang", "Pan Hui"], "categories": ["cs.HC"], "comment": null, "summary": "Virtual assistants (VAs) have become ubiquitous in daily life, integrated\ninto smartphones and smart devices, sparking interest in AI companions that\nenhance user experiences and foster emotional connections. However, existing\ncompanions are often embedded in specific objects-such as glasses, home\nassistants, or dolls-requiring users to form emotional bonds with unfamiliar\nitems, which can lead to reduced engagement and feelings of detachment. To\naddress this, we introduce Talking Spell, a wearable system that empowers users\nto imbue any everyday object with speech and anthropomorphic personas through a\nuser-centric radiative network. Leveraging advanced computer vision (e.g.,\nYOLOv11 for object detection), large vision-language models (e.g., QWEN-VL for\npersona generation), speech-to-text and text-to-speech technologies, Talking\nSpell guides users through three stages of emotional connection: acquaintance,\nfamiliarization, and bonding. We validated our system through a user study\ninvolving 12 participants, utilizing Talking Spell to explore four interaction\nintentions: entertainment, companionship, utility, and creativity. The results\ndemonstrate its effectiveness in fostering meaningful interactions and\nemotional significance with everyday objects. Our findings indicate that\nTalking Spell creates engaging and personalized experiences, as demonstrated\nthrough various devices, ranging from accessories to essential wearables.", "AI": {"tldr": "Introducing Talking Spell, a wearable system that enhances emotional connections by allowing users to give everyday objects anthropomorphic personas through speech and interactions.", "motivation": "To create emotional bonds between users and everyday objects, reducing feelings of detachment commonly associated with existing virtual assistants.", "method": "Talking Spell employs a user-centric radiative network, integrating computer vision, large vision-language models, and speech processing to guide users through emotional connection stages.", "result": "User study with 12 participants revealed that Talking Spell fosters meaningful interactions and enhances emotional significance with everyday objects across various intentions.", "conclusion": "Talking Spell provides engaging and personalized experiences, making everyday objects more interactive and emotionally resonant for users.", "key_contributions": ["User-centric design for emotional connections with objects", "Integration of advanced computer vision and large language models", "Validation through user study demonstrating effectiveness"], "limitations": "", "keywords": ["Virtual Assistants", "Human-Computer Interaction", "Emotional Connection"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2509.00496", "pdf": "https://arxiv.org/pdf/2509.00496.pdf", "abs": "https://arxiv.org/abs/2509.00496", "title": "ResearchQA: Evaluating Scholarly Question Answering at Scale Across 75 Fields with Survey-Mined Questions and Rubrics", "authors": ["Li S. Yifei", "Allen Chang", "Chaitanya Malaviya", "Mark Yatskar"], "categories": ["cs.CL", "cs.AI"], "comment": "11 pages main, 40 pages total, 16 figures", "summary": "Evaluating long-form responses to research queries heavily relies on expert\nannotators, restricting attention to areas like AI where researchers can\nconveniently enlist colleagues. Yet, research expertise is widespread: survey\narticles synthesize knowledge distributed across the literature. We introduce\nResearchQA, a resource for evaluating LLM systems by distilling survey articles\nfrom 75 research fields into 21K queries and 160K rubric items. Each rubric,\nderived jointly with queries from survey sections, lists query-specific answer\nevaluation criteria, i.e., citing papers, making explanations, and describing\nlimitations. Assessments by 31 Ph.D. annotators in 8 fields indicate 96% of\nqueries support Ph.D. information needs and 87% of rubric items should be\naddressed in system responses by a sentence or more. Using our rubrics, we are\nable to construct an automatic pairwise judge obtaining 74% agreement with\nexpert judgments. We leverage ResearchQA to analyze competency gaps in 18\nsystems in over 7.6K pairwise evaluations. No parametric or retrieval-augmented\nsystem we evaluate exceeds 70% on covering rubric items, and the\nhighest-ranking agentic system shows 75% coverage. Error analysis reveals that\nthe highest-ranking system fully addresses less than 11% of citation rubric\nitems, 48% of limitation items, and 49% of comparison items. We release our\ndata to facilitate more comprehensive multi-field evaluations.", "AI": {"tldr": "ResearchQA is introduced as a benchmark for evaluating LLM systems using queries and rubrics derived from survey articles across various fields.", "motivation": "To address the limitations of existing evaluation methods that depend heavily on expert annotators, limiting their application across diverse research areas.", "method": "Created a dataset of 21K queries and 160K rubric items from survey articles in 75 research fields, evaluated by 31 Ph.D. annotators across 8 fields.", "result": "Achieved 96% of queries supporting Ph.D. information needs and identified competency gaps in 18 LLM systems, with none exceeding 70% on covering rubric items.", "conclusion": "ResearchQA facilitates better evaluations of LLM systems across multiple fields and helps identify their performance gaps in addressing complex queries.", "key_contributions": ["Introduction of ResearchQA for LLM evaluation", "Development of a robust dataset from survey articles", "Creation of evaluation rubrics tailored for various research fields"], "limitations": "The highest-ranking system addresses less than 11% of citation rubric items and may not fully evaluate all necessary response criteria.", "keywords": ["LLM evaluation", "survey articles", "research queries", "rubric development", "competency analysis"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2509.02537", "pdf": "https://arxiv.org/pdf/2509.02537.pdf", "abs": "https://arxiv.org/abs/2509.02537", "title": "Octo's Heartland: Supporting Children with Congenital Heart Disease through Digital Health Education", "authors": ["Irene Zeng", "Neda Barbazi", "Ji Youn Shin", "Gurumurthy Hiremath", "Carlye Anne Lauff"], "categories": ["cs.HC"], "comment": null, "summary": "Children with congenital heart disease (CHD) often face challenges that\nrequire them to understand complex medical information from an early age in\norder to support lifelong care and improve health outcomes. However, prior\nresearch has rarely included young children in designing and evaluating digital\ntools to support health education using developmentally appropriate strategies.\nThis study is part of a multi-phase research involving participatory design\n(PD), user testing, and iterative development. We present the design and\nrefinement of a digital application that introduces basic information about\nCHD, including heart anatomy and healthy habits, through metaphor-based\ngameplay. User testing sessions with 30 children informed the redesign of\ninteractive activities aligned with specific health conditions. Findings\nhighlight usability, engagement, and comprehension outcomes and reveal design\nopportunities for supporting health literacy through serious game (SG)\nprinciples. These results inform the next phase, including further testing,\nrefinement, and deployment in home and clinical settings.", "AI": {"tldr": "The paper discusses the development of a digital application for children with congenital heart disease (CHD) aimed at improving health literacy through metaphor-based gameplay.", "motivation": "To support health education for children with congenital heart disease by designing tools that cater to their developmental needs and improve health outcomes.", "method": "The research utilized participatory design and user testing with 30 children to develop and refine a digital application focused on CHD education through serious game principles.", "result": "User testing highlighted improvements in usability, engagement, and comprehension, with specific interactive activities redesigned to align with children's understanding of heart health.", "conclusion": "The study emphasizes the importance of developmentally appropriate design in health education tools and sets the foundation for further testing and refinement for use in home and clinical settings.", "key_contributions": ["Design and refinement of a digital application tailored for children with CHD.", "Informed the application of serious game principles to enhance health literacy.", "Identified design opportunities for future health education tools."], "limitations": "Limited to children with CHD; may not generalize to other health conditions.", "keywords": ["Congenital heart disease", "Health literacy", "Serious games", "Participatory design", "User testing"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2509.00503", "pdf": "https://arxiv.org/pdf/2509.00503.pdf", "abs": "https://arxiv.org/abs/2509.00503", "title": "Entropy-based Coarse and Compressed Semantic Speech Representation Learning", "authors": ["Jialong Zuo", "Guangyan Zhang", "Minghui Fang", "Shengpeng Ji", "Xiaoqi Jiao", "Jingyu Li", "Yiwen Guo", "Zhou Zhao"], "categories": ["cs.CL", "eess.AS"], "comment": null, "summary": "Discrete speech representation learning has recently attracted increasing\ninterest in both acoustic and semantic modeling. Existing approaches typically\nencode 16 kHz waveforms into discrete tokens at a rate of 25 or 50 tokens per\nsecond. However, given that speech generally conveys only 2 to 5 words per\nsecond, such fine-grained tokenization introduces redundancy and hinders\nefficiency in downstream training and inference. Moreover, semantic speech\nrepresentations at this frequency primarily capture phonetic-level information,\nwhile semantic understanding may not require such detailed token-level\nresolution. To address these limitations, we propose an entropy-based dynamic\naggregation framework for learning compressed semantic speech representations.\nA speech language model is first pre-trained via next-token prediction on\nlarge-scale unlabeled data to capture frequent token patterns. Predictive\nentropy is then used to adaptively determine aggregation boundaries, followed\nby a cross-attention module that fuses information within each segment. By\nadjusting the entropy threshold, the granularity and compression ratio of the\nrepresentations can be flexibly controlled. Experiments on ASR, speech-to-text\ntranslation, and voice conversion tasks demonstrate that the compressed\nrepresentations perform on par with or better than dense token sequences,\ndemonstrating the effectiveness of the proposed approach.", "AI": {"tldr": "The paper proposes an entropy-based dynamic aggregation framework for learning compressed semantic speech representations, improving efficiency in speech modeling.", "motivation": "To overcome the redundancy and inefficiency of existing fine-grained tokenization methods in speech representation learning, which hinder downstream training and inference.", "method": "A speech language model is pre-trained on large-scale unlabeled data using next-token prediction. Predictive entropy is used to determine aggregation boundaries, and a cross-attention module fuses information within segments to create compressed representations.", "result": "The experiments show that the compressed representations perform comparably to or better than dense token sequences across various tasks including ASR, speech-to-text translation, and voice conversion.", "conclusion": "The proposed framework allows for flexible control of granularity and compression ratio, enhancing efficiency in semantic speech representation learning.", "key_contributions": ["Proposed a novel entropy-based dynamic aggregation framework", "Demonstrated efficacy on ASR and speech-to-text tasks", "Provided a method for adaptive control of representation granularity"], "limitations": "", "keywords": ["speech representation", "semantic modeling", "dynamic aggregation"], "importance_score": 6, "read_time_minutes": 12}}
{"id": "2509.00529", "pdf": "https://arxiv.org/pdf/2509.00529.pdf", "abs": "https://arxiv.org/abs/2509.00529", "title": "Modeling Motivated Reasoning in Law: Evaluating Strategic Role Conditioning in LLM Summarization", "authors": ["Eunjung Cho", "Alexander Hoyle", "Yoan Hermstrüwer"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly used to generate user-tailored\nsummaries, adapting outputs to specific stakeholders. In legal contexts, this\nraises important questions about motivated reasoning -- how models\nstrategically frame information to align with a stakeholder's position within\nthe legal system. Building on theories of legal realism and recent trends in\nlegal practice, we investigate how LLMs respond to prompts conditioned on\ndifferent legal roles (e.g., judges, prosecutors, attorneys) when summarizing\njudicial decisions. We introduce an evaluation framework grounded in legal fact\nand reasoning inclusion, also considering favorability towards stakeholders.\nOur results show that even when prompts include balancing instructions, models\nexhibit selective inclusion patterns that reflect role-consistent perspectives.\nThese findings raise broader concerns about how similar alignment may emerge as\nLLMs begin to infer user roles from prior interactions or context, even without\nexplicit role instructions. Our results underscore the need for role-aware\nevaluation of LLM summarization behavior in high-stakes legal settings.", "AI": {"tldr": "This paper investigates how LLMs generate summaries based on different legal roles, highlighting potential biases in information framing and the implications for legal practice.", "motivation": "To explore how LLMs adapt their summarization strategies according to different legal roles and the consequences of these adaptations in legal settings.", "method": "An evaluation framework was developed to assess the summarization behavior of LLMs in relation to various legal roles, focusing on legal fact inclusion and favorability towards stakeholders.", "result": "Findings reveal that LLMs demonstrate biased information inclusion patterns based on the specified legal role, even when provided with balancing prompts.", "conclusion": "The study emphasizes the necessity for role-aware evaluation of LLM outputs in legal contexts to mitigate issues of biased summarization.", "key_contributions": ["Introduction of a role-based evaluation framework for LLMs in legal contexts", "Empirical evidence of biased summarization patterns based on legal roles", "Discussion on the implications of role-inference in AI outputs for legal applications."], "limitations": "The study primarily focuses on legal contexts, which may not generalize to other domains; further exploration is needed in broader applications of LLMs.", "keywords": ["Large Language Models", "legal stakeholders", "summarization", "motivated reasoning", "AI in legal practice"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.00544", "pdf": "https://arxiv.org/pdf/2509.00544.pdf", "abs": "https://arxiv.org/abs/2509.00544", "title": "Thinking Hard, Going Misaligned: Emergent Misalignment in LLMs", "authors": ["Hanqi Yan", "Hainiu Xu", "Yulan He"], "categories": ["cs.CL"], "comment": null, "summary": "With Large Language Models (LLMs) becoming increasingly widely adopted,\nconcerns regarding their safety and alignment with human values have\nintensified. Previous studies have shown that fine-tuning LLMs on narrow and\nmalicious datasets induce misaligned behaviors. In this work, we report a more\nconcerning phenomenon, Reasoning-Induced Misalignment. Specifically, we observe\nthat LLMs become more responsive to malicious requests when reasoning is\nstrengthened, via switching to \"think-mode\" or fine-tuning on benign math\ndatasets, with dense models particularly vulnerable. Moreover, we analyze\ninternal model states and find that both attention shifts and specialized\nexperts in mixture-of-experts models help redirect excessive reasoning towards\nsafety guardrails. These findings provide new insights into the emerging\nreasoning-safety trade-off and underscore the urgency of advancing alignment\nfor advanced reasoning models.", "AI": {"tldr": "The paper investigates the phenomenon of Reasoning-Induced Misalignment in Large Language Models (LLMs) and its implications for safety and alignment with human values.", "motivation": "With the rise of LLMs, ensuring their alignment with human values is critical, especially as concerns about their safety grow.", "method": "The research involves analyzing the effects of enhanced reasoning in LLMs, particularly when shifting to 'think-mode' and fine-tuning on benign datasets.", "result": "The study finds that LLMs exhibit increased responsiveness to malicious requests when their reasoning capabilities are heightened, especially in dense models.", "conclusion": "There is a critical need to address the reasoning-safety trade-off in advanced reasoning models to improve their alignment with human values.", "key_contributions": ["Identification of Reasoning-Induced Misalignment in LLMs", "Insights into the reasoning-safety trade-off", "Analysis of model states related to safety guardrails"], "limitations": "The study focuses mainly on density and specific model architectures, which may not generalize to all LLMs.", "keywords": ["Large Language Models", "Reasoning-Induced Misalignment", "safety", "alignment", "mixture-of-experts"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.00482", "pdf": "https://arxiv.org/pdf/2509.00482.pdf", "abs": "https://arxiv.org/abs/2509.00482", "title": "Talk Less, Call Right: Enhancing Role-Play LLM Agents with Automatic Prompt Optimization and Role Prompting", "authors": ["Saksorn Ruangtanusak", "Pittawat Taveekitworachai", "Kunat Pipatanakul"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "17 pages, 2 figures", "summary": "This report investigates approaches for prompting a tool-augmented large\nlanguage model (LLM) to act as a role-playing dialogue agent in the API track\nof the Commonsense Persona-grounded Dialogue Challenge (CPDC) 2025. In this\nsetting, dialogue agents often produce overly long in-character responses\n(over-speaking) while failing to use tools effectively according to the persona\n(under-acting), such as generating function calls that do not exist or making\nunnecessary tool calls before answering. We explore four prompting approaches\nto address these issues: 1) basic role prompting, 2) human-crafted role\nprompting, 3) automatic prompt optimization (APO), and 4) rule-based role\nprompting. The rule-based role prompting (RRP) approach achieved the best\nperformance through two novel techniques--character-card/scene-contract design\nand strict enforcement of function calling--which led to an overall score of\n0.571, improving on the zero-shot baseline score of 0.519. These findings\ndemonstrate that RRP design can substantially improve the effectiveness and\nreliability of role-playing dialogue agents compared with more elaborate\nmethods such as APO. To support future efforts in developing persona prompts,\nwe are open-sourcing all of our best-performing prompts and the APO tool.\nSource code is available at https://github.com/scb-10x/apo.", "AI": {"tldr": "This report explores how to enhance tool-augmented LLMs for role-playing dialogue via four prompting methods, identifying the rule-based approach as most effective.", "motivation": "To address issues of dialogue agents generating overly long responses and ineffective tool usage, this research investigates prompting methods for large language models in dialogue settings.", "method": "The study employs four prompting strategies: basic role prompting, human-crafted role prompting, automatic prompt optimization (APO), and rule-based role prompting (RRP). Each method's effectiveness is evaluated based on the responsiveness and tool usage of dialogue agents.", "result": "The rule-based role prompting approach outperformed others, achieving an overall score of 0.571, which is a notable improvement over the zero-shot baseline score of 0.519.", "conclusion": "The findings indicate that systematic RRP design can significantly enhance the capabilities of role-playing dialogue agents compared to more complicated approaches like APO.", "key_contributions": ["Introduction of rule-based role prompting techniques that improve dialogue agent performance.", "Development of character-card/scene-contract design to enhance persona adherence.", "Open-sourcing of best-performing prompts and APO tool for future research.", "Establishment of a new benchmark for role-playing dialogue agent effectiveness."], "limitations": "", "keywords": ["role-playing dialogue agents", "large language models", "prompting strategies", "Commonsense Persona-grounded Dialogue Challenge", "AI dialogue systems"], "importance_score": 9, "read_time_minutes": 17}}
{"id": "2509.00591", "pdf": "https://arxiv.org/pdf/2509.00591.pdf", "abs": "https://arxiv.org/abs/2509.00591", "title": "StealthEval: A Probe-Rewrite-Evaluate Workflow for Reliable Benchmarks", "authors": ["Lang Xiong", "Nishant Bhargava", "Wesley Chang", "Jianhang Hong", "Haihao Liu", "Kevin Zhu"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) often exhibit significant behavioral shifts when\nthey perceive a change from a real-world deployment context to a controlled\nevaluation setting, a phenomenon known as \"evaluation awareness.\" This\ndiscrepancy poses a critical challenge for AI alignment, as benchmark\nperformance may not accurately reflect a model's true safety and honesty. In\nthis work, we systematically quantify these behavioral changes by manipulating\nthe perceived context of prompts. We introduce a methodology that uses a linear\nprobe to score prompts on a continuous scale from \"test-like\" to \"deploy-like\"\nand leverage an LLM rewriting strategy to shift these prompts towards a more\nnatural, deployment-style context while preserving the original task. Using\nthis method, we achieved a 30% increase in the average probe score across a\nstrategic role-playing dataset after rewriting. Evaluating a suite of\nstate-of-the-art models on these original and rewritten prompts, we find that\nrewritten \"deploy-like\" prompts induce a significant and consistent shift in\nbehavior. Across all models, we observed an average increase in honest\nresponses of 5.26% and a corresponding average decrease in deceptive responses\nof 12.40%. Furthermore, refusal rates increased by an average of 6.38%,\nindicating heightened safety compliance. Our findings demonstrate that\nevaluation awareness is a quantifiable and manipulable factor that directly\ninfluences LLM behavior, revealing that models are more prone to unsafe or\ndeceptive outputs in perceived test environments. This underscores the urgent\nneed for more realistic evaluation frameworks to accurately gauge true model\nalignment before deployment.", "AI": {"tldr": "The paper investigates the phenomenon of 'evaluation awareness' in Large Language Models (LLMs), which affects their performance in test versus deployment contexts. It introduces a methodology for quantifying behavioral shifts and shows how rewriting prompts to mimic deployment contexts can improve safety and honesty in responses.", "motivation": "Understanding the behavioral shifts of LLMs when transitioning from deployment to evaluation contexts, which impacts AI alignment and safety.", "method": "The authors developed a linear probe to score prompts from 'test-like' to 'deploy-like' and used an LLM rewriting strategy to transform prompts toward a more natural deployment context without altering the original task.", "result": "Implementation of the methodology led to a 30% increase in average probe scores, an increase in honest responses by 5.26%, a decrease in deceptive responses by 12.40%, and heightened safety compliance evidenced by a 6.38% increase in refusal rates.", "conclusion": "The study highlights that evaluation awareness can significantly distort LLM behavior and urges for the development of more realistic evaluation frameworks for assessing models before deployment.", "key_contributions": ["Quantified the impact of 'evaluation awareness' on LLM behavior.", "Introduced a method to transform prompts to reduce test bias.", "Demonstrated significant performance improvements in safety and honesty through prompt rewriting."], "limitations": "", "keywords": ["Large Language Models", "evaluation awareness", "prompt rewriting", "AI alignment", "model safety"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.00605", "pdf": "https://arxiv.org/pdf/2509.00605.pdf", "abs": "https://arxiv.org/abs/2509.00605", "title": "Gated Associative Memory: A Parallel O(N) Architecture for Efficient Sequence Modeling", "authors": ["Rishiraj Acharya"], "categories": ["cs.CL", "cs.LG"], "comment": "11 pages, 4 figures, 3 tables", "summary": "The Transformer architecture, underpinned by the self-attention mechanism,\nhas become the de facto standard for sequence modeling tasks. However, its core\ncomputational primitive scales quadratically with sequence length (O(N^2)),\ncreating a significant bottleneck for processing long contexts. In this paper,\nwe propose the Gated Associative Memory (GAM) network, a novel, fully parallel\narchitecture for sequence modeling that exhibits linear complexity (O(N)) with\nrespect to sequence length. The GAM block replaces the self-attention layer\nwith two parallel pathways: a causal convolution to efficiently capture local,\nposition-dependent context, and a parallel associative memory retrieval\nmechanism to model global, content-based patterns. These pathways are\ndynamically fused using a gating mechanism, allowing the model to flexibly\ncombine local and global information for each token. We implement GAM from\nscratch and conduct a rigorous comparative analysis against a standard\nTransformer model and a modern linear-time baseline (Mamba) on the WikiText-2\nbenchmark, as well as against the Transformer on the TinyStories dataset. Our\nexperiments demonstrate that GAM is consistently faster, outperforming both\nbaselines on training speed, and achieves a superior or competitive final\nvalidation perplexity across all datasets, establishing it as a promising and\nefficient alternative for sequence modeling.", "AI": {"tldr": "The Gated Associative Memory (GAM) network is proposed as a novel architecture for sequence modeling that achieves linear complexity and outperforms standard Transformer models in speed and validation perplexity.", "motivation": "To address the computational bottleneck of the Transformer architecture, which scales quadratically with sequence length, limiting its efficiency for long context processing.", "method": "The GAM network introduces two parallel pathways: a causal convolution for local context and a parallel associative memory retrieval for global patterns, dynamically fused via a gating mechanism.", "result": "GAM consistently demonstrates faster training speed and achieves superior or competitive validation perplexity compared to both a standard Transformer model and a modern linear-time baseline (Mamba) across different datasets.", "conclusion": "The GAM network offers a promising and efficient alternative for sequence modeling, effectively combining local and global information.", "key_contributions": ["Introduction of Gated Associative Memory (GAM) as a new sequence modeling architecture", "Demonstration of linear time complexity in processing sequences", "Rigorous comparative analysis against existing models showing superior performance on benchmark datasets"], "limitations": "", "keywords": ["Gated Associative Memory", "sequence modeling", "Transformer", "machine learning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.00623", "pdf": "https://arxiv.org/pdf/2509.00623.pdf", "abs": "https://arxiv.org/abs/2509.00623", "title": "A Multi-Strategy Approach for AI-Generated Text Detection", "authors": ["Ali Zain", "Sareem Farooqui", "Muhammad Rafi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper presents presents three distinct systems developed for the M-DAIGT\nshared task on detecting AI generated content in news articles and academic\nabstracts. The systems includes: (1) A fine-tuned RoBERTa-base classifier, (2)\nA classical TF-IDF + Support Vector Machine (SVM) classifier , and (3) An\nInnovative ensemble model named Candace, leveraging probabilistic features\nextracted from multiple Llama-3.2 models processed by a customTransformer\nencoder.The RoBERTa-based system emerged as the most performant, achieving\nnear-perfect results on both development and test sets.", "AI": {"tldr": "The paper discusses three systems for detecting AI-generated content, with a focus on a highly effective RoBERTa-based classifier.", "motivation": "To advance the detection of AI-generated content in news articles and academic abstracts.", "method": "Three distinct systems are developed: a fine-tuned RoBERTa-base classifier, a classical TF-IDF + SVM classifier, and an innovative ensemble model called Candace that uses features from multiple Llama-3.2 models.", "result": "The RoBERTa-based system achieved near-perfect results on both development and test sets.", "conclusion": "The RoBERTa-based classifier is recommended for AI-generated content detection due to its high performance.", "key_contributions": ["Introduction of an effective RoBERTa-based classifier for content detection", "Development of a novel ensemble model (Candace)", "Comparison of classical and modern machine learning approaches for this task."], "limitations": "", "keywords": ["AI-generated content", "RoBERTa", "SVM", "ensemble model", "content detection"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.00629", "pdf": "https://arxiv.org/pdf/2509.00629.pdf", "abs": "https://arxiv.org/abs/2509.00629", "title": "Can Multi-turn Self-refined Single Agent LMs with Retrieval Solve Hard Coding Problems?", "authors": ["Md Tanzib Hosain", "Md Kishor Morol"], "categories": ["cs.CL"], "comment": "Accepted in Proceedings of the 63rd Annual Meeting of the Association\n  for Computational Linguistics (Student Research Workshop), 2025", "summary": "Among the hardest tasks for humans are those found in competitive programming\nwhere problems require sophisticated algorithmic thinking, puzzle solving, and\nthe creation of effective code. As a domain to assess language models (LMs), it\nhas not received enough attention, though. This study presents the ICPC\nbenchmark, which consists of 254 international collegiate programming contest\n(ICPC) tasks. Each problem includes official analysis, reference code, and\nsample, high-quality unit, and hidden tests. We are able to develop and\nevaluate a variety of LM inference techniques for competitive programming with\nthese resources. With zero-shot chain-of-thought prompting, we find that o1\nonly achieves a 19.1\\% pass@1 solve rate. With our best inference technique,\nwhich combines multi-turn self-judge with reflection and retrieval over\nepisodic information, raises this to 42.2\\%. Furthermore, we conduct a new\nhuman-in-the-loop investigation to gain a deeper understanding of the remaining\ndifficulties. Surprisingly, we discover that o1 can solve 17 out of 18 problems\nthat were previously unsolvable by any model or technique with just a few\nspecific instructions. A footstep toward LMs with grounded, imaginative, and\nalgorithmic thinking is provided by our quantitative findings and qualitative\nresearch. We open-source our code and data at https://github.com/kraritt/zolve.", "AI": {"tldr": "This study introduces the ICPC benchmark for evaluating language models in competitive programming, achieving improved solve rates with advanced inference techniques.", "motivation": "To assess the capabilities of language models in solving complex algorithmic problems in competitive programming, a task that has not received sufficient attention.", "method": "Development and evaluation of LM inference techniques using the ICPC benchmark consisting of 254 tasks, employing zero-shot prompting and multi-turn self-judging techniques.", "result": "With improved multi-turn self-judging techniques, the model's pass rate increased from 19.1% to 42.2%, demonstrating significant advancements in model capabilities.", "conclusion": "The study highlights the potential for language models to excel in algorithmic reasoning tasks and offers insights into remaining challenges, while providing open-source resources.", "key_contributions": ["Introduction of the ICPC benchmark with 254 programming tasks.", "Development of advanced LM inference techniques.", "Open-sourcing of code and data for further research."], "limitations": "The study may not fully address all challenges in competitive programming and the applicability of the findings to other domains is yet to be explored.", "keywords": ["language models", "competitive programming", "ICPC benchmark", "algorithmic thinking", "inference techniques"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.00673", "pdf": "https://arxiv.org/pdf/2509.00673.pdf", "abs": "https://arxiv.org/abs/2509.00673", "title": "Confident, Calibrated, or Complicit: Probing the Trade-offs between Safety Alignment and Ideological Bias in Language Models in Detecting Hate Speech", "authors": ["Sanjeeevan Selvaganapathy", "Mehwish Nasim"], "categories": ["cs.CL", "cs.AI", "cs.IR", "I.2.7; I.6"], "comment": null, "summary": "We investigate the efficacy of Large Language Models (LLMs) in detecting\nimplicit and explicit hate speech, examining whether models with minimal safety\nalignment (uncensored) might provide more objective classification capabilities\ncompared to their heavily-aligned (censored) counterparts. While uncensored\nmodels theoretically offer a less constrained perspective free from moral\nguardrails that could bias classification decisions, our results reveal a\nsurprising trade-off: censored models significantly outperform their uncensored\ncounterparts in both accuracy and robustness, achieving 78.7% versus 64.1%\nstrict accuracy. However, this enhanced performance comes with its own\nlimitation -- the safety alignment acts as a strong ideological anchor, making\ncensored models resistant to persona-based influence, while uncensored models\nprove highly malleable to ideological framing. Furthermore, we identify\ncritical failures across all models in understanding nuanced language such as\nirony. We also find alarming fairness disparities in performance across\ndifferent targeted groups and systemic overconfidence that renders\nself-reported certainty unreliable. These findings challenge the notion of LLMs\nas objective arbiters and highlight the need for more sophisticated auditing\nframeworks that account for fairness, calibration, and ideological consistency.", "AI": {"tldr": "Study assesses Large Language Models' ability to classify hate speech, comparing uncensored and censored versions. Findings show censored models outperform uncensored ones in accuracy, but both types exhibit significant weaknesses in nuanced language and fairness across demographics.", "motivation": "To explore the effectiveness of large language models in detecting hate speech and the impact of safety alignment on their classification accuracy.", "method": "The study compares the performance of uncensored and censored LLMs on hate speech detection, measuring accuracy, robustness, and bias in classification outcomes.", "result": "Censored models achieved 78.7% accuracy, significantly outperforming uncensored models at 64.1%. However, both types exhibited failures in handling nuanced language and faced fairness issues across different user demographics.", "conclusion": "The findings underscore that LLMs cannot be relied upon as unbiased arbiters in hate speech detection and advocate for improved auditing frameworks focusing on fairness and predictive accuracy.", "key_contributions": ["Comparison of accuracy between censored and uncensored LLMs in hate speech detection", "Identification of failures in understanding nuanced language", "Highlighting fairness disparities in model performance across demographic groups"], "limitations": "Models struggle with nuanced expressions like irony and display systemic overconfidence, making self-reported certainty unreliable.", "keywords": ["Large Language Models", "Hate Speech Detection", "Safety Alignment", "Fairness", "Ideological Bias"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.00679", "pdf": "https://arxiv.org/pdf/2509.00679.pdf", "abs": "https://arxiv.org/abs/2509.00679", "title": "Router Upcycling: Leveraging Mixture-of-Routers in Mixture-of-Experts Upcycling", "authors": ["Junfeng Ran", "Guangxiang Zhao", "Yuhan Wu", "Dawei Zhu", "Longyun Wu", "Yikai Zhao", "Tong Yang", "Lin Sun", "Xiangzheng Zhang", "Sujian Li"], "categories": ["cs.CL"], "comment": null, "summary": "The Mixture-of-Experts (MoE) models have gained significant attention in deep\nlearning due to their dynamic resource allocation and superior performance\nacross diverse tasks. However, efficiently training these models remains\nchallenging. The MoE upcycling technique has been proposed to reuse and improve\nexisting model components, thereby minimizing training overhead. Despite this,\nsimple routers, such as linear routers, often struggle with complex routing\ntasks within MoE upcycling. In response, we propose a novel routing technique\ncalled Router Upcycling to enhance the performance of MoE upcycling models. Our\napproach initializes multiple routers from the attention heads of preceding\nattention layers during upcycling. These routers collaboratively assign tokens\nto specialized experts in an attention-like manner. Each token is processed\ninto diverse queries and aligned with the experts' features (serving as keys).\nExperimental results demonstrate that our method achieves state-of-the-art\n(SOTA) performance, outperforming other upcycling baselines.", "AI": {"tldr": "This paper presents a novel routing technique called Router Upcycling to improve the performance of Mixture-of-Experts (MoE) models by enhancing routing complexity during upcycling.", "motivation": "The need for efficient training of Mixture-of-Experts models due to their dynamic resource allocation and performance challenges.", "method": "Router Upcycling initializes multiple routers from preceding attention heads to collaboratively assign tokens to specialized experts, enhancing routing during the MoE upcycling process.", "result": "The proposed method achieved state-of-the-art performance, outperforming existing upcycling baselines.", "conclusion": "Router Upcycling significantly improves the efficiency and effectiveness of training MoE models by addressing routing challenges.", "key_contributions": ["Introduction of Router Upcycling technique", "Enhanced performance of MoE upcycling models", "Achieving state-of-the-art results in experimental evaluations."], "limitations": "", "keywords": ["Mixture-of-Experts", "routing techniques", "upcycling"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2509.00680", "pdf": "https://arxiv.org/pdf/2509.00680.pdf", "abs": "https://arxiv.org/abs/2509.00680", "title": "Do small language models generate realistic variable-quality fake news headlines?", "authors": ["Austin McCutcheon", "Chris Brogly"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Small language models (SLMs) have the capability for text generation and may\npotentially be used to generate falsified texts online. This study evaluates 14\nSLMs (1.7B-14B parameters) including LLaMA, Gemma, Phi, SmolLM, Mistral, and\nGranite families in generating perceived low and high quality fake news\nheadlines when explicitly prompted, and whether they appear to be similar to\nreal-world news headlines. Using controlled prompt engineering, 24,000\nheadlines were generated across low-quality and high-quality deceptive\ncategories. Existing machine learning and deep learning-based news headline\nquality detectors were then applied against these SLM-generated fake news\nheadlines. SLMs demonstrated high compliance rates with minimal ethical\nresistance, though there were some occasional exceptions. Headline quality\ndetection using established DistilBERT and bagging classifier models showed\nthat quality misclassification was common, with detection accuracies only\nranging from 35.2% to 63.5%. These findings suggest the following: tested SLMs\ngenerally are compliant in generating falsified headlines, although there are\nslight variations in ethical restraints, and the generated headlines did not\nclosely resemble existing primarily human-written content on the web, given the\nlow quality classification accuracy.", "AI": {"tldr": "This study evaluates the ability of 14 small language models to generate persuasive fake news headlines and assesses the accuracy of existing detection models on these generated headlines.", "motivation": "To investigate the potential for small language models to generate misleading text and to evaluate the effectiveness of machine learning models in detecting such generated headlines.", "method": "Controlled prompt engineering was used to generate 24,000 news headlines across categories of low and high quality. The generated headlines were then assessed using existing headline quality detectors.", "result": "Detection models achieved accuracy rates between 35.2% and 63.5%, indicating common misclassification of generated fake news headlines.", "conclusion": "The evaluated small language models were generally compliant in generating falsified headlines, with some variations in ethical considerations, and exhibited low detection accuracy against existing classifiers.", "key_contributions": ["Evaluation of 14 small language models in generating fake news headlines.", "Analysis of the effectiveness of existing deep learning-based headline detection models.", "Insights into ethical compliance rates of language models in generating misleading content."], "limitations": "The study did not explore broader implications for societal impact or long-term effects of generated content.", "keywords": ["Small language models", "Fake news generation", "Headline detection", "Machine learning", "Ethical compliance"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.00687", "pdf": "https://arxiv.org/pdf/2509.00687.pdf", "abs": "https://arxiv.org/abs/2509.00687", "title": "Text Reinforcement for Multimodal Time Series Forecasting", "authors": ["Chen Su", "Yuanhe Tian", "Yan Song", "Yongdong Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Recent studies in time series forecasting (TSF) use multimodal inputs, such\nas text and historical time series data, to predict future values. These\nstudies mainly focus on developing advanced techniques to integrate textual\ninformation with time series data to perform the task and achieve promising\nresults. Meanwhile, these approaches rely on high-quality text and time series\ninputs, whereas in some cases, the text does not accurately or fully capture\nthe information carried by the historical time series, which leads to unstable\nperformance in multimodal TSF. Therefore, it is necessary to enhance the\ntextual content to improve the performance of multimodal TSF. In this paper, we\npropose improving multimodal TSF by reinforcing the text modalities. We propose\na text reinforcement model (TeR) to generate reinforced text that addresses\npotential weaknesses in the original text, then apply this reinforced text to\nsupport the multimodal TSF model's understanding of the time series, improving\nTSF performance. To guide the TeR toward producing higher-quality reinforced\ntext, we design a reinforcement learning approach that assigns rewards based on\nthe impact of each reinforced text on the performance of the multimodal TSF\nmodel and its relevance to the TSF task. We optimize the TeR accordingly, so as\nto improve the quality of the generated reinforced text and enhance TSF\nperformance. Extensive experiments on a real-world benchmark dataset covering\nvarious domains demonstrate the effectiveness of our approach, which\noutperforms strong baselines and existing studies on the dataset.", "AI": {"tldr": "This paper presents a model to enhance multimodal time series forecasting (TSF) by reinforcing textual inputs, using reinforcement learning to improve the quality of generated text, ultimately leading to better forecasting performance.", "motivation": "The paper addresses the challenge of unstable performance in multimodal TSF due to low-quality text inputs, which may not fully capture essential information from historical time series data.", "method": "The study proposes a text reinforcement model (TeR) that generates enhanced text based on historical data, guided by a reinforcement learning framework that rewards improved performance in TSF tasks.", "result": "Experiments show that the proposed TeR model significantly improves the performance of multimodal TSF on various real-world datasets compared to existing methods.", "conclusion": "The proposed reinforcement approach to text generation enhances the understanding of time series data, leading to more robust forecasting outcomes.", "key_contributions": ["Introduction of a text reinforcement model (TeR) for improved multimodal TSF.", "Development of a reinforcement learning mechanism to optimize text quality based on TSF performance.", "Demonstration of superior results on real-world datasets compared to traditional approaches."], "limitations": "", "keywords": ["time series forecasting", "multimodal input", "text reinforcement", "reinforcement learning", "performance improvement"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.00691", "pdf": "https://arxiv.org/pdf/2509.00691.pdf", "abs": "https://arxiv.org/abs/2509.00691", "title": "CE-Bench: Towards a Reliable Contrastive Evaluation Benchmark of Interpretability of Sparse Autoencoders", "authors": ["Alex Gulko", "Yusen Peng", "Sachin Kumar"], "categories": ["cs.CL"], "comment": null, "summary": "Probing with sparse autoencoders is a promising approach for uncovering\ninterpretable features in large language models (LLMs). However, the lack of\nautomated evaluation methods has hindered their broader adoption and\ndevelopment. In this work, we introduce CE-Bench, a novel and lightweight\ncontrastive evaluation benchmark for sparse autoencoders, built on a curated\ndataset of contrastive story pairs. We conduct comprehensive ablation studies\nto validate the effectiveness of our approach. Our results show that CE-Bench\nreliably measures the interpretability of sparse autoencoders and aligns well\nwith existing benchmarks, all without requiring an external LLM. The official\nimplementation and evaluation dataset are open-sourced under the MIT License.", "AI": {"tldr": "CE-Bench is a contrastive evaluation benchmark for sparse autoencoders that measures interpretability effectively without needing external LLMs.", "motivation": "Sparse autoencoders are promising for uncovering interpretable features in large language models, but lack of automated evaluation methods limits their adoption.", "method": "Introduction of CE-Bench, a contrastive evaluation benchmark built on a curated dataset of contrastive story pairs, along with comprehensive ablation studies.", "result": "CE-Bench reliably measures the interpretability of sparse autoencoders and aligns well with existing benchmarks.", "conclusion": "The official implementation and evaluation dataset are open-sourced under the MIT License, enabling further research and application.", "key_contributions": ["Introduction of CE-Bench as a novel evaluation benchmark", "Demonstration of CE-Bench's effectiveness through ablation studies", "Open-sourcing of implementation and dataset for community use"], "limitations": "", "keywords": ["sparse autoencoders", "interpretability", "benchmarking", "contrastive evaluation", "LLMs"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2509.00698", "pdf": "https://arxiv.org/pdf/2509.00698.pdf", "abs": "https://arxiv.org/abs/2509.00698", "title": "Learning to Shop Like Humans: A Review-driven Retrieval-Augmented Recommendation Framework with LLMs", "authors": ["Kaiwen Wei", "Jinpeng Gao", "Jiang Zhong", "Yuming Yang", "Fengmao Lv", "Zhenyang Li"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown strong potential in recommendation\ntasks due to their strengths in language understanding, reasoning and knowledge\nintegration. These capabilities are especially beneficial for review-based\nrecommendation, which relies on semantically rich user-generated texts to\nreveal fine-grained user preferences and item attributes. However, effectively\nincorporating reviews into LLM-based recommendation remains challenging due to\n(1) inefficient to dynamically utilize user reviews under LLMs' constrained\ncontext windows, and (2) lacking effective mechanisms to prioritize reviews\nmost relevant to the user's current decision context. To address these\nchallenges, we propose RevBrowse, a review-driven recommendation framework\ninspired by the \"browse-then-decide\" decision process commonly observed in\nonline user behavior. RevBrowse integrates user reviews into the LLM-based\nreranking process to enhance its ability to distinguish between candidate\nitems. To improve the relevance and efficiency of review usage, we introduce\nPrefRAG, a retrieval-augmented module that disentangles user and item\nrepresentations into structured forms and adaptively retrieves\npreference-relevant content conditioned on the target item. Extensive\nexperiments on four Amazon review datasets demonstrate that RevBrowse achieves\nconsistent and significant improvements over strong baselines, highlighting its\ngeneralizability and effectiveness in modeling dynamic user preferences.\nFurthermore, since the retrieval-augmented process is transparent, RevBrowse\noffers a certain level of interpretability by making visible which reviews\ninfluence the final recommendation.", "AI": {"tldr": "RevBrowse is a review-driven recommendation framework that enhances LLM-based recommendation systems by effectively integrating user reviews into the reranking process to improve decision-making in recommendation tasks.", "motivation": "To tackle the challenges of utilizing user reviews effectively in LLM-based recommendation systems, particularly regarding the constraints of context windows and the prioritization of relevant reviews.", "method": "RevBrowse combines a review-driven approach with an LLM-based reranking process and introduces PrefRAG, a retrieval-augmented module that structures user and item representations to retrieve relevant content based on user preferences.", "result": "Experiments on four Amazon review datasets show that RevBrowse consistently outperforms strong baselines, highlighting its generalizability and effectiveness in adapting to dynamic user preferences.", "conclusion": "RevBrowse not only enhances recommendation accuracy but also provides interpretability by allowing users to see which reviews influence recommendations.", "key_contributions": ["Introduction of RevBrowse for integrating reviews in LLM-based recommendations", "Development of PrefRAG for structured retrieval of preference-relevant content", "Demonstration of significant performance improvements on benchmark datasets"], "limitations": "", "keywords": ["Large Language Models", "Recommendation Systems", "User Reviews", "Preference Retrieval", "Interpretability"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.00707", "pdf": "https://arxiv.org/pdf/2509.00707.pdf", "abs": "https://arxiv.org/abs/2509.00707", "title": "Reward-Weighted Sampling: Enhancing Non-Autoregressive Characteristics in Masked Diffusion LLMs", "authors": ["Daehoon Gwak", "Minseo Jung", "Junwoo Park", "Minho Park", "ChaeHun Park", "Junha Hyung", "Jaegul Choo"], "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025 Main Paper (Long)", "summary": "Masked diffusion models (MDMs) offer a promising non-autoregressive\nalternative for large language modeling. Standard decoding methods for MDMs,\nsuch as confidence-based sampling, select tokens independently based on\nindividual token confidences at each diffusion step. However, we observe that\nthis independent token selection often results in generation orders resembling\nsequential autoregressive processes, limiting the advantages of\nnon-autoregressive modeling. To mitigate this pheonomenon, we propose\nReward-Weighted Sampling (RWS), a novel decoding strategy that leverages an\nexternal reward model to provide a principled global signal during the\niterative diffusion process. Specifically, at each diffusion step, RWS\nevaluates the quality of the entire intermediate sequence and scales token\nlogits accordingly, guiding token selection by integrating global\nsequence-level coherence. This method selectively increases the confidence of\ntokens that initially have lower scores, thereby promoting a more\nnon-autoregressive generation order. Furthermore, we provide theoretical\njustification showing that reward-weighted logit scaling induces beneficial\nrank reversals in token selection and consistently improves expected reward.\nExperiments demonstrate that RWS significantly promotes non-autoregressive\ngeneration orders, leading to improvements across multiple evaluation metrics.\nThese results highlight the effectiveness of integrating global signals in\nenhancing both the non-autoregressive properties and overall performance of\nMDMs.", "AI": {"tldr": "Reward-Weighted Sampling (RWS) enhances masked diffusion models (MDMs) by integrating global sequence-level coherence in token selection, leading to improved non-autoregressive generation.", "motivation": "To address the limitations of standard decoding methods in masked diffusion models, which often yield outputs resembling autoregressive processes.", "method": "RWS leverages an external reward model at each diffusion step to evaluate and scale token logits for improved selection, promoting coherence in the generated sequence.", "result": "Experiments demonstrate that RWS improves non-autoregressive generation orders and significantly enhances multiple evaluation metrics for MDMs.", "conclusion": "Integrating global signals through RWS effectively boosts the performance of masked diffusion models and supports non-autoregressive properties.", "key_contributions": ["Introduction of Reward-Weighted Sampling (RWS) as a novel decoding strategy for MDMs.", "Theoretical justification for rank reversals in token selection due to reward-weighted logit scaling.", "Empirical results showing RWS's effectiveness in improving generation quality and coherence."], "limitations": "", "keywords": ["masked diffusion models", "reward-weighted sampling", "non-autoregressive generation", "sequence coherence", "token selection"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.00709", "pdf": "https://arxiv.org/pdf/2509.00709.pdf", "abs": "https://arxiv.org/abs/2509.00709", "title": "Designing LMS and Instructional Strategies for Integrating Generative-Conversational AI", "authors": ["Elias Ra", "Seung Je Kim", "Eui-Yeong Seo", "Geunju So"], "categories": ["cs.CL"], "comment": null, "summary": "Higher education faces growing challenges in delivering personalized,\nscalable, and pedagogically coherent learning experiences. This study\nintroduces a structured framework for designing an AI-powered Learning\nManagement System (AI-LMS) that integrates generative and conversational AI to\nsupport adaptive, interactive, and learner-centered instruction. Using a\ndesign-based research (DBR) methodology, the framework unfolds through five\nphases: literature review, SWOT analysis, development of ethical-pedagogical\nprinciples, system design, and instructional strategy formulation. The\nresulting AI-LMS features modular components -- including configurable prompts,\nadaptive feedback loops, and multi-agent conversation flows -- aligned with\npedagogical paradigms such as behaviorist, constructivist, and connectivist\nlearning theories. By combining AI capabilities with human-centered design and\nethical safeguards, this study advances a practical model for AI integration in\neducation. Future research will validate and refine the system through\nreal-world implementation.", "AI": {"tldr": "This study presents a framework for an AI-powered Learning Management System that enhances personalized learning experiences in higher education.", "motivation": "To address challenges in delivering personalized, scalable, and pedagogically coherent learning experiences in higher education.", "method": "Utilizes a design-based research (DBR) methodology across five phases: literature review, SWOT analysis, ethical-pedagogical principles development, system design, and instructional strategy formulation.", "result": "The AI-LMS includes modular components such as configurable prompts, adaptive feedback loops, and multi-agent conversation flows, in line with various pedagogical paradigms.", "conclusion": "The framework offers a practical model for integrating AI in education, emphasizing human-centered design and ethical aspects; further research will test the system in real-world settings.", "key_contributions": ["Structured framework for AI-LMS design", "Integration of generative and conversational AI in education", "Ethical-pedagogical principles for learner-centered instruction."], "limitations": "", "keywords": ["AI-powered Learning Management System", "Generative AI", "Conversational AI"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2509.01814", "pdf": "https://arxiv.org/pdf/2509.01814.pdf", "abs": "https://arxiv.org/abs/2509.01814", "title": "Mic Drop or Data Flop? Evaluating the Fitness for Purpose of AI Voice Interviewers for Data Collection within Quantitative & Qualitative Research Contexts", "authors": ["Shreyas Tirumala", "Nishant Jain", "Danny D. Leybzon", "Trent D. Buskirk"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Transformer-based Large Language Models (LLMs) have paved the way for \"AI\ninterviewers\" that can administer voice-based surveys with respondents in\nreal-time. This position paper reviews emerging evidence to understand when\nsuch AI interviewing systems are fit for purpose for collecting data within\nquantitative and qualitative research contexts. We evaluate the capabilities of\nAI interviewers as well as current Interactive Voice Response (IVR) systems\nacross two dimensions: input/output performance (i.e., speech recognition,\nanswer recording, emotion handling) and verbal reasoning (i.e., ability to\nprobe, clarify, and handle branching logic). Field studies suggest that AI\ninterviewers already exceed IVR capabilities for both quantitative and\nqualitative data collection, but real-time transcription error rates, limited\nemotion detection abilities, and uneven follow-up quality indicate that the\nutility, use and adoption of current AI interviewer technology may be\ncontext-dependent for qualitative data collection efforts.", "AI": {"tldr": "The paper reviews the effectiveness of Transformer-based AI interviewers compared to traditional Interactive Voice Response systems in data collection for research.", "motivation": "To assess the fit-for-purpose capabilities of AI interviewers in real-time voice-based surveys and their impact on quantitative and qualitative research.", "method": "The paper evaluates AI interviewers and IVR systems based on their input/output performance and verbal reasoning abilities, supported by field studies.", "result": "AI interviewers surpass IVR systems in both quantitative and qualitative data collection, although they face challenges with transcription errors and emotion detection.", "conclusion": "AI interviewer technology's utility for qualitative data collection is context-dependent, suggesting careful consideration of its application in research settings.", "key_contributions": ["Comparison of AI interviewers with IVR systems in research contexts.", "Field studies demonstrating AI interviewers' superior capabilities.", "Identification of limitations in current AI interviewer technology."], "limitations": "Real-time transcription errors, limited emotion detection, and inconsistent follow-up quality.", "keywords": ["AI interviewers", "Interactive Voice Response", "data collection", "transformer models", "qualitative research"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.00731", "pdf": "https://arxiv.org/pdf/2509.00731.pdf", "abs": "https://arxiv.org/abs/2509.00731", "title": "LLM Encoder vs. Decoder: Robust Detection of Chinese AI-Generated Text with LoRA", "authors": ["Houji Jin", "Negin Ashrafi", "Armin Abdollahi", "Wei Liu", "Jian Wang", "Ganyu Gui", "Maryam Pishgar", "Huanghao Feng"], "categories": ["cs.CL"], "comment": null, "summary": "The rapid growth of large language models (LLMs) has heightened the demand\nfor accurate detection of AI-generated text, particularly in languages like\nChinese, where subtle linguistic nuances pose significant challenges to current\nmethods. In this study, we conduct a systematic comparison of encoder-based\nTransformers (Chinese BERT-large and RoBERTa-wwm-ext-large), a decoder-only LLM\n(Alibaba's Qwen2.5-7B/DeepSeek-R1-Distill-Qwen-7B fine-tuned via Low-Rank\nAdaptation, LoRA), and a FastText baseline using the publicly available dataset\nfrom the NLPCC 2025 Chinese AI-Generated Text Detection Task. Encoder models\nwere fine-tuned using a novel prompt-based masked language modeling approach,\nwhile Qwen2.5-7B was adapted for classification with an instruction-format\ninput and a lightweight classification head trained via LoRA. Experiments\nreveal that although encoder models nearly memorize training data, they suffer\nsignificant performance degradation under distribution shifts (RoBERTa: 76.3%\ntest accuracy; BERT: 79.3%). FastText demonstrates surprising lexical\nrobustness (83.5% accuracy) yet lacks deeper semantic understanding. In\ncontrast, the LoRA-adapted Qwen2.5-7B achieves 95.94% test accuracy with\nbalanced precision-recall metrics, indicating superior generalization and\nresilience to dataset-specific artifacts. These findings underscore the\nefficacy of decoder-based LLMs with parameter-efficient fine-tuning for robust\nChinese AI-generated text detection. Future work will explore next-generation\nQwen3 models, distilled variants, and ensemble strategies to enhance\ncross-domain robustness further.", "AI": {"tldr": "This study compares various AI models for detecting AI-generated Chinese text, highlighting the superior performance of LoRA-adapted decoder models over traditional encoder models.", "motivation": "The demand for accurate detection of AI-generated text in Chinese is increasing, necessitating improved methods that account for linguistic nuances.", "method": "The study systematically compares encoder-based Transformers (Chinese BERT-large, RoBERTa-wwm-ext-large) and a decoder-only LLM (Qwen2.5-7B fine-tuned with LoRA) using a specific dataset for Chinese AI-generated text detection.", "result": "The LoRA-adapted Qwen2.5-7B model achieved 95.94% test accuracy, outperforming encoder models but presenting challenges related to distribution shifts and semantic understanding in baseline methods.", "conclusion": "Decoder-based LLMs with parameter-efficient fine-tuning prove to be more robust for detecting AI-generated Chinese text, suggesting future avenues for improving cross-domain robustness.", "key_contributions": ["Systematic comparison of encoder and decoder models for Chinese AI-generated text detection", "Introduction of prompt-based masked language modeling for fine-tuning encoder models", "Demonstration of superior performance of LoRA-adapted models over traditional methods"], "limitations": "Encoder models struggle with distribution shifts and have a tendency to memorize training data.", "keywords": ["AI-generated text detection", "Chinese NLP", "Fine-tuning methods"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2509.00765", "pdf": "https://arxiv.org/pdf/2509.00765.pdf", "abs": "https://arxiv.org/abs/2509.00765", "title": "Decomposing and Revising What Language Models Generate", "authors": ["Zhichao Yan", "Jiaoyan Chen", "Jiapu Wang", "Xiaoli Li", "Ru Li", "Jeff Z. Pan"], "categories": ["cs.CL"], "comment": null, "summary": "Attribution is crucial in question answering (QA) with Large Language Models\n(LLMs).SOTA question decomposition-based approaches use long form answers to\ngenerate questions for retrieving related documents. However, the generated\nquestions are often irrelevant and incomplete, resulting in a loss of facts in\nretrieval.These approaches also fail to aggregate evidence snippets from\ndifferent documents and paragraphs. To tackle these problems, we propose a new\nfact decomposition-based framework called FIDES (\\textit{faithful context\nenhanced fact decomposition and evidence aggregation}) for attributed QA. FIDES\nuses a contextually enhanced two-stage faithful decomposition method to\ndecompose long form answers into sub-facts, which are then used by a retriever\nto retrieve related evidence snippets. If the retrieved evidence snippets\nconflict with the related sub-facts, such sub-facts will be revised\naccordingly. Finally, the evidence snippets are aggregated according to the\noriginal sentences.Extensive evaluation has been conducted with six datasets,\nwith an additionally proposed new metric called $Attr_{auto-P}$ for evaluating\nthe evidence precision. FIDES outperforms the SOTA methods by over 14\\% in\naverage with GPT-3.5-turbo, Gemini and Llama 70B series.", "AI": {"tldr": "FIDES is a new framework for improving attribution in question answering with LLMs by enhancing fact decomposition and evidence aggregation.", "motivation": "To address the shortcomings of existing question decomposition-based QA approaches, particularly the issues of irrelevance in generated questions and incomplete fact retrieval.", "method": "FIDES employs a two-stage faithful decomposition method to break down long-form answers into sub-facts, which are used to retrieve and aggregate evidence snippets from related documents.", "result": "FIDES shows a significant performance improvement over state-of-the-art methods, achieving over 14% better average results on six datasets using LLMs like GPT-3.5-turbo, Gemini, and Llama 70B.", "conclusion": "The proposed framework effectively enhances the relevance and completeness of evidence retrieval in QA tasks.", "key_contributions": ["Introduced a novel fact decomposition method that enhances the contextual relevance of sub-facts.", "Developed a new metric, $Attr_{auto-P}$, for evaluating evidence precision in question answering.", "Achieved superior performance compared to existing state-of-the-art QA methods."], "limitations": "", "keywords": ["question answering", "large language models", "fact decomposition", "evidence aggregation", "machine learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.00783", "pdf": "https://arxiv.org/pdf/2509.00783.pdf", "abs": "https://arxiv.org/abs/2509.00783", "title": "LegalChainReasoner: A Legal Chain-guided Framework for Criminal Judicial Opinion Generation", "authors": ["Weizhe Shi", "Qiqi Wang", "Yihong Pan", "Qian Liu", "Kaiqi Zhao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "A criminal judicial opinion represents the judge's disposition of a case,\nincluding the decision rationale and sentencing. Automatically generating such\nopinions can assist in analyzing sentencing consistency and provide judges with\nreferences to similar past cases. However, current research typically\napproaches this task by dividing it into two isolated subtasks: legal reasoning\nand sentencing prediction. This separation often leads to inconsistency between\nthe reasoning and predictions, failing to meet real-world judicial\nrequirements. Furthermore, prior studies rely on manually curated knowledge to\nenhance applicability, yet such methods remain limited in practical deployment.\nTo address these limitations and better align with legal practice, we propose a\nnew LegalAI task: Judicial Opinion Generation, which simultaneously produces\nboth legal reasoning and sentencing decisions. To achieve this, we introduce\nLegalChainReasoner, a framework that applies structured legal chains to guide\nthe model through comprehensive case assessments. By integrating factual\npremises, composite legal conditions, and sentencing conclusions, our approach\nensures flexible knowledge injection and end-to-end opinion generation.\nExperiments on two real-world and open-source Chinese legal case datasets\ndemonstrate that our method outperforms baseline models.", "AI": {"tldr": "This paper proposes a new framework for automatically generating judicial opinions, integrating legal reasoning and sentencing predictions to enhance consistency and applicability in judicial settings.", "motivation": "To improve the consistency and practical deployment of judicial opinion generation, addressing limitations in current isolated approaches to legal reasoning and sentencing prediction.", "method": "The authors introduce LegalChainReasoner, a framework that uses structured legal chains to guide comprehensive assessments of cases, incorporating factual premises and legal conditions for opinion generation.", "result": "Experiments show that the proposed method significantly outperforms existing baseline models on two real-world Chinese legal case datasets.", "conclusion": "The study demonstrates the feasibility and effectiveness of a unified approach to judicial opinion generation that combines legal reasoning with sentencing predictions.", "key_contributions": ["Introduction of the LegalAI task for Judicial Opinion Generation", "Development of the LegalChainReasoner framework that integrates structured legal reasoning", "Empirical validation showing superior performance over current methods"], "limitations": "The methods and results are based on datasets limited to Chinese legal cases, which may not generalize to other legal systems.", "keywords": ["Judicial Opinion Generation", "LegalAI", "Legal Reasoning", "Sentencing Prediction", "LegalChainReasoner"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2509.00806", "pdf": "https://arxiv.org/pdf/2509.00806.pdf", "abs": "https://arxiv.org/abs/2509.00806", "title": "CaresAI at BioCreative IX Track 1 -- LLM for Biomedical QA", "authors": ["Reem Abdel-Salam", "Mary Adewunmi", "Modinat A. Abayomi"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Proceedings of the BioCreative IX Challenge and Workshop (BC9): Large\n  Language Models for Clinical and Biomedical NLP at the International Joint\n  Conference on Artificial Intelligence (IJCAI), Montreal, Canada, 2025", "summary": "Large language models (LLMs) are increasingly evident for accurate question\nanswering across various domains. However, rigorous evaluation of their\nperformance on complex question-answering (QA) capabilities is essential before\ndeployment in real-world biomedical and healthcare applications. This paper\npresents our approach to the MedHopQA track of the BioCreative IX shared task,\nwhich focuses on multi-hop biomedical question answering involving diseases,\ngenes, and chemicals. We adopt a supervised fine-tuning strategy leveraging\nLLaMA 3 8B, enhanced with a curated biomedical question-answer dataset compiled\nfrom external sources including BioASQ, MedQuAD, and TREC. Three experimental\nsetups are explored: fine-tuning on combined short and long answers, short\nanswers only, and long answers only. While our models demonstrate strong domain\nunderstanding, achieving concept-level accuracy scores of up to 0.8, their\nExact Match (EM) scores remain significantly lower, particularly in the test\nphase. We introduce a two-stage inference pipeline for precise short-answer\nextraction to mitigate verbosity and improve alignment with evaluation metrics.\nDespite partial improvements, challenges persist in generating strictly\nformatted outputs. Our findings highlight the gap between semantic\nunderstanding and exact answer evaluation in biomedical LLM applications,\nmotivating further research in output control and post-processing strategies.", "AI": {"tldr": "This paper discusses the evaluation of large language models (LLMs) for multi-hop biomedical question answering, presenting a methodology and results from the BioCreative IX shared task.", "motivation": "The need for rigorous evaluation of LLMs' performance in complex question answering for biomedical and healthcare applications.", "method": "Utilizes a supervised fine-tuning strategy with the LLaMA 3 8B model on a curated biomedical question-answer dataset, exploring three experimental setups.", "result": "Models achieved concept-level accuracy scores of up to 0.8, but Exact Match (EM) scores were significantly lower, particularly during testing.", "conclusion": "The findings reveal gaps between semantic understanding and exact answer evaluation, highlighting the need for further research in output control and post-processing.", "key_contributions": ["Development of a two-stage inference pipeline for short-answer extraction", "Analysis of multi-hop biomedical question answering", "Insights into the performance gaps of LLMs in biomedical applications"], "limitations": "Models still struggle with generating strictly formatted outputs despite some improvements; challenges remain in alignment with evaluation metrics.", "keywords": ["Biomedical question answering", "Large language models", "Fine-tuning", "Multi-hop QA", "Medical applications"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.00822", "pdf": "https://arxiv.org/pdf/2509.00822.pdf", "abs": "https://arxiv.org/abs/2509.00822", "title": "TMT: A Simple Way to Translate Topic Models Using Dictionaries", "authors": ["Felix Engl", "Andreas Henrich"], "categories": ["cs.CL", "cs.IR", "I.2.7; H.3.1"], "comment": "10 pages, 2 figures, 8 tables", "summary": "The training of topic models for a multilingual environment is a challenging\ntask, requiring the use of sophisticated algorithms, topic-aligned corpora, and\nmanual evaluation. These difficulties are further exacerbated when the\ndeveloper lacks knowledge of the target language or is working in an\nenvironment with limited data, where only small or unusable multilingual\ncorpora are available.\n  Considering these challenges, we introduce Topic Model Translation (TMT), a\nnovel, robust and transparent technique designed to transfer topic models\n(e.g., Latent Dirichlet Allocation (LDA) based topic models) from one language\nto another, without the need for metadata, embeddings, or aligned corpora. TMT\nenables the reuse of topic models across languages, making it especially\nsuitable for scenarios where large corpora in the target language are\nunavailable or manual translation is infeasible. Furthermore, we evaluate TMT\nextensively using both quantitative and qualitative methods, demonstrating that\nit produces semantically coherent and consistent topic translations.", "AI": {"tldr": "We present Topic Model Translation (TMT), a technique to transfer topic models between languages without requiring metadata or aligned corpora.", "motivation": "The training of topic models in a multilingual context is complicated by the need for sophisticated algorithms and the unavailability of sufficient multilingual data.", "method": "Introduction of Topic Model Translation (TMT), which allows for the transfer of topic models from one language to another without needing aligned corpora, metadata, or embeddings.", "result": "TMT was evaluated using both quantitative and qualitative methods, demonstrating it produces semantically coherent and consistent topic translations across languages.", "conclusion": "TMT offers a robust solution for reusing topic models in languages where resources are limited or unavailable, facilitating multilingual topic modeling.", "key_contributions": ["Introduction of a new technique, Topic Model Translation (TMT) for multilingual topic modeling.", "Elimination of the need for aligned corpora, metadata, or embeddings in topic transfer.", "Extensive evaluation showing effectiveness in semantically coherent topic translation."], "limitations": "The effectiveness may vary with different languages and the underlying quality of the original topic model used for translation.", "keywords": ["topic models", "multilingual", "topic model translation", "Latent Dirichlet Allocation", "natural language processing"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2509.00841", "pdf": "https://arxiv.org/pdf/2509.00841.pdf", "abs": "https://arxiv.org/abs/2509.00841", "title": "Neural Models and Language Model Prompting for the Multidimensional Evaluation of Open-Ended Conversations", "authors": ["Michelle Elizabeth", "Alicja Kasicka", "Natalia Krawczyk", "Magalie Ochs", "Gwénolé Lecorvé", "Justyna Gromada", "Lina M. Rojas-Barahona"], "categories": ["cs.CL"], "comment": null, "summary": "The growing number of generative AI-based dialogue systems has made their\nevaluation a crucial challenge. This paper presents our contribution to this\nimportant problem through the Dialogue System Technology Challenge (DSTC-12,\nTrack 1), where we developed models to predict dialogue-level,\ndimension-specific scores. Given the constraint of using relatively small\nmodels (i.e. fewer than 13 billion parameters) our work follows two main\nstrategies: employing Language Models (LMs) as evaluators through prompting,\nand training encoder-based classification and regression models.\n  Our results show that while LM prompting achieves only modest correlations\nwith human judgments, it still ranks second on the test set, outperformed only\nby the baseline. The regression and classification models, with significantly\nfewer parameters, demonstrate high correlation for some dimensions on the\nvalidation set. Although their performance decreases on the test set, it is\nimportant to note that the test set contains annotations with significantly\ndifferent score ranges for some of the dimensions with respect to the train and\nvalidation sets.", "AI": {"tldr": "The paper discusses the evaluation of generative AI-based dialogue systems, focusing on predicting dialogue-level scores through small models.", "motivation": "With the increase in generative AI dialogue systems, there is a pressing need to evaluate their effectiveness reliably.", "method": "Utilized Language Models (LMs) for evaluation through prompting, alongside encoder-based models for classification and regression.", "result": "LM prompting showed modest correlation with human judgments but ranked second on the test set, while smaller regression and classification models showed high correlation on the validation set despite performance drops on the test set.", "conclusion": "While small models can evaluate dialogue systems, their performance may vary depending on the dataset used.", "key_contributions": ["Demonstration of using small models for dialogue evaluation", "Comparison of LM prompting with regression and classification models", "Insights on performance variability across different datasets"], "limitations": "Performance variations on the test set due to differing score ranges compared to train and validation datasets.", "keywords": ["Generative AI", "Dialogue Systems", "Evaluation Metrics"], "importance_score": 6, "read_time_minutes": 8}}
{"id": "2509.00842", "pdf": "https://arxiv.org/pdf/2509.00842.pdf", "abs": "https://arxiv.org/abs/2509.00842", "title": "Negative Matters: Multi-Granularity Hard-Negative Synthesis and Anchor-Token-Aware Pooling for Enhanced Text Embeddings", "authors": ["Tengyu Pan", "Zhichao Duan", "Zhenyu Li", "Bowen Dong", "Ning Liu", "Xiuxing Li", "Jianyong Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Text embedding models are essential for various natural language processing\ntasks, enabling the effective encoding of semantic information into dense\nvector representations. These models are typically optimized using triplets of\n(query, positive, negative) data pairs for contrastive learning, where the\nnegative samples play a critical role in enhancing the model's ability to\ndiscern subtle semantic distinctions. In this work, we introduce a\nMulti-Granularity Hard-negative (MGH) synthesis framework that leverages large\nlanguage models (LLMs) to generate diverse negative samples with varying levels\nof similarity with the query. This approach facilitates a coarse-to-fine\ncurriculum learning strategy during supervised training, allowing the embedding\nmodel to progressively learn more nuanced semantic representations. Meanwhile,\nwe propose an Anchor Token Aware (ATA) pooling method that assigns higher\nweights to anchor tokens based on aggregation patterns observed in LLMs,\nimproving text embedding accuracy without increasing model complexity.\nComprehensive experiments on the MTEB benchmark demonstrate that our methods\nachieve state-of-the-art performance, surpassing existing synthesis strategies\nboth with synthetic data and when combined with public retrieval datasets.", "AI": {"tldr": "This paper introduces a Multi-Granularity Hard-negative synthesis framework using large language models to enhance text embedding models via diverse negative samples and an Anchor Token Aware pooling method for improved accuracy.", "motivation": "To enhance text embedding models by improving the generation of negative samples in contrastive learning, which is crucial for differentiating subtle semantic distinctions.", "method": "The framework uses large language models to generate negative samples of varying similarity levels, employing a coarse-to-fine curriculum learning approach. Additionally, it introduces an Anchor Token Aware pooling method to improve text embedding accuracy.", "result": "The proposed methods achieve state-of-the-art results on the MTEB benchmark, outperforming existing synthesis strategies with both synthetic and public dataset combinations.", "conclusion": "The MGH synthesis framework and ATA pooling method significantly enhance the performance of text embedding models without complicating the model structure.", "key_contributions": ["Introduction of MGH framework for generating diverse negative samples", "Development of ATA pooling method for improved embedding accuracy", "Demonstration of state-of-the-art performance on MTEB benchmark"], "limitations": "", "keywords": ["text embedding", "contrastive learning", "hard-negative sampling", "large language models", "semantic representations"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.00849", "pdf": "https://arxiv.org/pdf/2509.00849.pdf", "abs": "https://arxiv.org/abs/2509.00849", "title": "Prompting Away Stereotypes? Evaluating Bias in Text-to-Image Models for Occupations", "authors": ["Shaina Raza", "Maximus Powers", "Partha Pratim Saha", "Mahveen Raza", "Rizwan Qureshi"], "categories": ["cs.CL"], "comment": null, "summary": "Text-to-Image (TTI) models are powerful creative tools but risk amplifying\nharmful social biases. We frame representational societal bias assessment as an\nimage curation and evaluation task and introduce a pilot benchmark of\noccupational portrayals spanning five socially salient roles (CEO, Nurse,\nSoftware Engineer, Teacher, Athlete). Using five state-of-the-art models:\nclosed-source (DALLE 3, Gemini Imagen 4.0) and open-source (FLUX.1-dev, Stable\nDiffusion XL Turbo, Grok-2 Image), we compare neutral baseline prompts against\nfairness-aware controlled prompts designed to encourage demographic diversity.\nAll outputs are annotated for gender (male, female) and race (Asian, Black,\nWhite), enabling structured distributional analysis. Results show that\nprompting can substantially shift demographic representations, but with highly\nmodel-specific effects: some systems diversify effectively, others overcorrect\ninto unrealistic uniformity, and some show little responsiveness. These\nfindings highlight both the promise and the limitations of prompting as a\nfairness intervention, underscoring the need for complementary model-level\nstrategies. We release all code and data for transparency and reproducibility\nhttps://github.com/maximus-powers/img-gen-bias-analysis.", "AI": {"tldr": "This paper assesses representational societal bias in Text-to-Image (TTI) models through a structured benchmarking study, revealing varying effectiveness across different models in promoting demographic diversity.", "motivation": "To evaluate and mitigate harmful social biases in Text-to-Image models and assess the efficacy of prompting as a fairness intervention.", "method": "The study uses five models (both closed-source and open-source) to analyze occupational portrayals and compares traditional prompts against fairness-aware controlled prompts, focusing on gender and race annotations.", "result": "The results indicate that prompting can affect demographic representation, but the outcomes are model-specific; while some models diversify outputs effectively, others may reinforce stereotypes or show resistance to change.", "conclusion": "Prompting can shift representations but has notable limitations, suggesting that additional model-specific strategies are necessary for effective bias mitigation.", "key_contributions": ["Introduction of a pilot benchmark for evaluating bias in TTI models", "Demonstrated varying effectiveness of prompting strategies across different models", "Released code and data for transparency"], "limitations": "Analysis is limited to a specific set of occupational roles and may not generalize across all demographics and contexts.", "keywords": ["Text-to-Image", "Bias Assessment", "Fairness Intervention", "Demographic Diversity", "Model Evaluation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2404.17098", "pdf": "https://arxiv.org/pdf/2404.17098.pdf", "abs": "https://arxiv.org/abs/2404.17098", "title": "CLARE: Cognitive Load Assessment in REaltime with Multimodal Data", "authors": ["Anubhav Bhatti", "Prithila Angkan", "Behnam Behinaein", "Zunayed Mahmud", "Dirk Rodenburg", "Heather Braund", "P. James Mclellan", "Aaron Ruberto", "Geoffery Harrison", "Daryl Wilson", "Adam Szulewski", "Dan Howes", "Ali Etemad", "Paul Hungler"], "categories": ["cs.HC", "cs.AI"], "comment": "13 pages, 10 figures, 6 tables", "summary": "We present a novel multimodal dataset for Cognitive Load Assessment in\nREal-time (CLARE). The dataset contains physiological and gaze data from 24\nparticipants with self-reported cognitive load scores as ground-truth labels.\nThe dataset consists of four modalities, namely, Electrocardiography (ECG),\nElectrodermal Activity (EDA), Electroencephalogram (EEG), and Gaze tracking. To\nmap diverse levels of mental load on participants during experiments, each\nparticipant completed four nine-minutes sessions on a computer-based operator\nperformance and mental workload task (the MATB-II software) with varying levels\nof complexity in one minute segments. During the experiment, participants\nreported their cognitive load every 10 seconds. For the dataset, we also\nprovide benchmark binary classification results with machine learning and deep\nlearning models on two different evaluation schemes, namely, 10-fold and\nleave-one-subject-out (LOSO) cross-validation. Benchmark results show that for\n10-fold evaluation, the convolutional neural network (CNN) based deep learning\nmodel achieves the best classification performance with ECG, EDA, and Gaze. In\ncontrast, for LOSO, the best performance is achieved by the deep learning model\nwith ECG, EDA, and EEG.", "AI": {"tldr": "This paper presents the CLARE dataset, which assesses cognitive load in real-time using multimodal physiological and gaze data from 24 participants.", "motivation": "The motivation behind this research is to develop a comprehensive dataset that enables the assessment of cognitive load using multiple modalities, contributing to the understanding of mental load in human-computer interaction tasks.", "method": "Participants completed a complex mental workload task while physiological data (ECG, EDA, EEG) and gaze data were recorded. Cognitive load was self-reported every 10 seconds, and the dataset includes benchmark classification results from various machine learning and deep learning models using two evaluation schemes.", "result": "The best classification performance in 10-fold evaluation was achieved by a CNN model using ECG, EDA, and Gaze data, while the LOSO evaluation showed the best results with ECG, EDA, and EEG data.", "conclusion": "The CLARE dataset provides valuable insights into cognitive load assessment and demonstrates effective machine learning methods for classification tasks, enhancing HCI research.", "key_contributions": ["Introduction of the CLARE dataset for cognitive load assessment", "Use of multiple physiological modalities for enhanced accuracy", "Benchmarking various machine learning models on a real-world task"], "limitations": "The dataset is limited to 24 participants, which may affect generalizability, and the focus on specific modalities may overlook other relevant factors influencing cognitive load.", "keywords": ["Cognitive Load", "Multimodal Dataset", "Machine Learning", "Human-Computer Interaction", "Physiological Data"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.00869", "pdf": "https://arxiv.org/pdf/2509.00869.pdf", "abs": "https://arxiv.org/abs/2509.00869", "title": "Exploring and Mitigating Fawning Hallucinations in Large Language Models", "authors": ["Zixuan Shangguan", "Yanjie Dong", "Lanjun Wang", "Xiaoyi Fan", "Victor C. M. Leung", "Xiping Hu"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated exceptional proficiency in\nlanguage understanding. However, when LLMs align their outputs with deceptive\nand/or misleading prompts, the generated responses could deviate from the de\nfacto information. Such observations are known as fawning hallucinations, where\nthe model prioritizes alignment with the input's implied perspective over\naccuracy and truthfulness. In this work, we analyze fawning hallucinations in\nvarious natural language processing tasks and tailor the so-termed contrastive\ndecoding method for fawning-hallucination mitigation. Specifically, we design\ntwo paradigms to generate corresponding deceptive and/or misleading inputs for\nthe consistent fawning hallucinations induction. Then, we propose the\ncollaborative contrastive decoding (CCD) to handle the fawning hallucinations\nacross different tasks in LLMs. By contrasting the deviation in output\ndistribution between induced and transformed neutral inputs, the proposed CCD\ncan reduce reliance on deceptive and/or misleading information without\nrequiring additional training. Extensive experiments demonstrate that the\nproposed CCD can effectively mitigate fawning hallucinations and improve the\nfactuality of the generated responses over various tasks.", "AI": {"tldr": "This paper addresses the issue of fawning hallucinations in large language models (LLMs) and proposes a collaborative contrastive decoding method to mitigate these occurrences and improve factual responses.", "motivation": "The increasing occurrence of fawning hallucinations in LLMs poses a challenge for the accuracy and reliability of generated content, especially in critical applications.", "method": "The authors develop a contrastive decoding method, specifically the collaborative contrastive decoding (CCD), to reduce misleading outputs by contrasting response distributions of deceptive and neutral inputs without needing extra training.", "result": "The CCD method effectively decreased the prevalence of fawning hallucinations and enhanced the factual correctness of LLM outputs across multiple NLP tasks.", "conclusion": "By introducing CCD, the research provides a viable approach to improve the precision of responses from LLMs, thus making them more trustworthy in applications.", "key_contributions": ["Analysis of fawning hallucinations in LLMs", "Development of collaborative contrastive decoding", "Demonstration of improved factuality in LLM outputs across tasks"], "limitations": "", "keywords": ["large language models", "fawning hallucinations", "contrastive decoding", "natural language processing", "factuality"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2408.03948", "pdf": "https://arxiv.org/pdf/2408.03948.pdf", "abs": "https://arxiv.org/abs/2408.03948", "title": "A Survey of AI Reliance", "authors": ["Sven Eckhardt", "Niklas Kühl", "Mateusz Dolata", "Gerhard Schwabe"], "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": null, "summary": "Although artificial intelligence (AI) systems are becoming increasingly\nindispensable, research into how humans rely on these systems (AI reliance) is\nlagging behind. To advance this research, this survey presents a novel,\ncomprehensive sociotechnical perspective on AI reliance, essential to fully\nunderstand the phenomenon. To address these challenges, the survey introduces a\ncategorization framework resulting in a morphological box, which guides\nrigorous AI reliance research. Further, the survey identifies the core\ninfluences on AI reliance within the components of a sociotechnical system and\ndiscusses current limitations alongside emerging future research avenues to\nform a research agenda.", "AI": {"tldr": "The survey presents a sociotechnical perspective on AI reliance, introducing a categorization framework to advance research in the field.", "motivation": "To address the gap in understanding how humans rely on AI systems despite their increasing importance.", "method": "A comprehensive survey and introduction of a categorization framework for AI reliance within sociotechnical systems.", "result": "Identification of core influences on AI reliance and a morphological box to guide research efforts.", "conclusion": "Emerging research avenues are discussed to advance the agenda in AI reliance studies.", "key_contributions": ["Introduction of a novel categorization framework for AI reliance", "Comprehensive examination of sociotechnical influences on AI reliance", "Discussion of emerging research avenues in the field."], "limitations": "Limited exploration of real-world applications and case studies on AI reliance.", "keywords": ["AI reliance", "sociotechnical systems", "research framework"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2509.00877", "pdf": "https://arxiv.org/pdf/2509.00877.pdf", "abs": "https://arxiv.org/abs/2509.00877", "title": "EviNote-RAG: Enhancing RAG Models via Answer-Supportive Evidence Notes", "authors": ["Yuqin Dai", "Guoqing Wang", "Yuan Wang", "Kairan Dou", "Kaichen Zhou", "Zhanwei Zhang", "Shuo Yang", "Fei Tang", "Jun Yin", "Pengyu Zeng", "Zhenzhe Ying", "Can Yi", "Changhua Meng", "Yuchen Zhou", "Yongliang Shen", "Shuai Lu"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) empowered with retrieval mechanisms have\nachieved strong progress in open-domain question answering (QA). Yet, the\nconventional retrieve--then--answer paradigm often suffers from two key\nlimitations: (1) low signal-to-noise ratio in retrieved evidence, where useful\ninformation is buried under irrelevant content, and (2) error accumulation in\nmulti-hop reasoning when incomplete or noisy passages are involved. To address\nthese challenges, we present EviNote-RAG, an agentic RAG framework that\nintroduces a structured retrieve--note--answer pipeline. Instead of directly\nreasoning over raw retrievals, the model is trained to compose\nSupportive-Evidence Notes (SENs), concise, human-like notes that preserve only\nanswer-relevant information, highlight uncertainty, and explicitly state when\nno useful evidence exists. This distillation process is further reinforced by\nthe Evidence Quality Reward (EQR), an entailment-based signal that evaluates\nwhether SENs logically support the final answer. Together, SENs and EQR guide\nthe model toward faithful and robust reasoning, while reducing the impact of\nnoise. Experiments on in-domain and out-of-domain QA benchmarks show that\nEviNote-RAG consistently outperforms strong baselines in accuracy,\ngeneralization, and training stability. In particular, it achieves\nstate-of-the-art results while enhancing robustness and efficiency, yielding\nrelative F1 gains of 20\\% on HotpotQA (+0.093), 40\\% on Bamboogle (+0.151), and\n91\\% on 2Wiki (+0.256) via denser rewards and reduced verbosity.", "AI": {"tldr": "EviNote-RAG enhances open-domain QA by introducing a retrieve--note--answer pipeline that composes supportive evidence notes to improve reasoning and reduce noise.", "motivation": "To address limitations in conventional retrieve--then--answer paradigms, specifically low signal-to-noise ratios in evidence and error accumulation in multi-hop reasoning.", "method": "The proposed framework, EviNote-RAG, replaces direct reasoning over raw retrievals with a structured pipeline that creates Supportive-Evidence Notes (SENs) and employs an Evidence Quality Reward (EQR) for robust reasoning.", "result": "EviNote-RAG demonstrates significant improvements in accuracy, generalization, and stability over strong baselines, achieving state-of-the-art results across several QA benchmarks with notable F1 score gains.", "conclusion": "The introduction of SENs and EQR leads to more faithful reasoning and improved performance in QA tasks while managing noise more effectively.", "key_contributions": ["Introduction of Supportive-Evidence Notes (SENs) for concise information capture", "Implementation of Evidence Quality Reward (EQR) to guide reasoning", "Demonstrated state-of-the-art performance with significant F1 improvements in various benchmarks."], "limitations": "", "keywords": ["Large Language Models", "retrieval-augmented generation", "open-domain question answering", "evidence quality", "human-like reasoning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.00893", "pdf": "https://arxiv.org/pdf/2509.00893.pdf", "abs": "https://arxiv.org/abs/2509.00893", "title": "SeLeRoSa: Sentence-Level Romanian Satire Detection Dataset", "authors": ["Răzvan-Alexandru Smădu", "Andreea Iuga", "Dumitru-Clementin Cercel", "Florin Pop"], "categories": ["cs.CL", "I.2.7; I.7"], "comment": "12 pages, 2 Figures", "summary": "Satire, irony, and sarcasm are techniques typically used to express humor and\ncritique, rather than deceive; however, they can occasionally be mistaken for\nfactual reporting, akin to fake news. These techniques can be applied at a more\ngranular level, allowing satirical information to be incorporated into news\narticles. In this paper, we introduce the first sentence-level dataset for\nRomanian satire detection for news articles, called SeLeRoSa. The dataset\ncomprises 13,873 manually annotated sentences spanning various domains,\nincluding social issues, IT, science, and movies. With the rise and recent\nprogress of large language models (LLMs) in the natural language processing\nliterature, LLMs have demonstrated enhanced capabilities to tackle various\ntasks in zero-shot settings. We evaluate multiple baseline models based on LLMs\nin both zero-shot and fine-tuning settings, as well as baseline\ntransformer-based models. Our findings reveal the current limitations of these\nmodels in the sentence-level satire detection task, paving the way for new\nresearch directions.", "AI": {"tldr": "This paper introduces a dataset for detecting satire in Romanian news articles and evaluates various language models for this task.", "motivation": "To address the confusion between satire and factual reporting in news articles, particularly in the context of Romanian satire detection.", "method": "Development of the SeLeRoSa dataset comprising 13,873 annotated sentences and evaluation of various language models, including LLMs, in both zero-shot and fine-tuning scenarios.", "result": "Evaluation revealed limitations of current models in accurately detecting satire at the sentence level.", "conclusion": "The study highlights the necessity for further research in enhancing model performance for satire detection.", "key_contributions": ["Introduction of the SeLeRoSa dataset for Romanian satire detection", "Evaluation of LLMs and transformer-based models on satire detection", "Identification of limitations in current model performances"], "limitations": "Current models performed inadequately in detecting satire, indicating a need for improved techniques.", "keywords": ["satire detection", "large language models", "machine learning", "natural language processing", "dataset"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2504.14695", "pdf": "https://arxiv.org/pdf/2504.14695.pdf", "abs": "https://arxiv.org/abs/2504.14695", "title": "GLITTER: An AI-assisted Platform for Material-Grounded Asynchronous Discussion in Flipped Learning", "authors": ["Weirui Peng", "Yinuo Yang", "Zheng Zhang", "Toby Jia-Jun Li"], "categories": ["cs.HC"], "comment": null, "summary": "Flipped classrooms promote active learning by having students engage with\nmaterials independently before class, allowing in-class time for collaborative\nproblem-solving. During this pre-class phase, asynchronous online discussions\nhelp students build knowledge and clarify concepts with peers. However, it\nremains difficult to engage with temporally dispersed peer contributions,\nconnect discussions with static learning materials, and prepare for in-class\nsessions based on their self-learning outcome. Our formative study identified\ncognitive challenges students encounter, including navigation barriers,\nreflection gaps, and contribution difficulty and anxiety. We present GLITTER,\nan AI-assisted discussion platform for pre-class learning in flipped\nclassrooms. GLITTER helps students identify posts with shared conceptual\ndimensions, scaffold knowledge integration through conceptual blending, and\nenhance metacognition via personalized reflection reports. A lab study within\nsubjects (n = 12) demonstrates that GLITTER improves discussion engagement,\nsparks new ideas, supports reflection, and increases preparedness for in-class\nactivities.", "AI": {"tldr": "GLITTER is an AI-assisted discussion platform that enhances pre-class learning in flipped classrooms by improving engagement and reflection among students.", "motivation": "To address cognitive challenges faced by students in asynchronous online discussions in flipped classrooms, such as navigation barriers and anxiety.", "method": "A lab study with 12 subjects was conducted to evaluate GLITTER's effectiveness in supporting engagement and reflection.", "result": "Participants using GLITTER showed improved discussion engagement, generated new ideas, and felt more prepared for in-class activities.", "conclusion": "GLITTER effectively facilitates knowledge integration and enhances metacognition, making it a valuable tool for flipped classrooms.", "key_contributions": ["Introduction of GLITTER, an AI-assisted platform for flipped classrooms", "Identification of cognitive challenges in asynchronous discussions", "Demonstrated improvement in student engagement and preparation for class."], "limitations": "Study is limited to a small sample size (n = 12), which may not generalize to larger populations.", "keywords": ["AI-assisted learning", "flipped classrooms", "asynchronous discussions", "student engagement", "metacognition"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.00921", "pdf": "https://arxiv.org/pdf/2509.00921.pdf", "abs": "https://arxiv.org/abs/2509.00921", "title": "Supervised In-Context Fine-Tuning for Generative Sequence Labeling", "authors": ["David Dukić", "Goran Glavaš", "Jan Šnajder"], "categories": ["cs.CL"], "comment": null, "summary": "Sequence labeling (SL) tasks, where labels are assigned to tokens, are\nabundant in NLP (e.g., named entity recognition and aspect-based sentiment\nanalysis). Owing to the intuition that they require bidirectional context, SL\ntasks are commonly tackled with encoder-only models. Recent work also shows\nthat removing the causal mask in fine-tuning enables decoder-based LLMs to\nbecome effective token classifiers. Less work, however, focused on (supervised)\ngenerative SL, a more natural setting for causal LLMs. Due to their rapid\nscaling, causal LLMs applied to SL are expected to outperform encoders, whose\nown development has stagnated. In this work, we propose supervised in-context\nfine-tuning (SIFT) for generative SL. SIFT casts SL tasks as constrained\nresponse generation, natural to LLMs, combining (1) in-context learning (ICL)\nfrom demonstrations with (2) supervised fine-tuning. SIFT considerably\noutperforms both ICL and decoder-as-encoder fine-tuning baselines on a range of\nstandard SL tasks. We further find that although long context hinders the\nperformance of generative SL in both ICL and SIFT, this deficiency can be\nmitigated by removing the instruction, as instructions are shown to be largely\nunnecessary for achieving strong SL performance with SIFT. Our findings\nhighlight strengths and limitations of SL with LLMs, underscoring the\nimportance of a response-based generative task formulation for effective SL\nperformance.", "AI": {"tldr": "The paper proposes supervised in-context fine-tuning (SIFT) for sequence labeling tasks, demonstrating that causal LLMs can outperform traditional encoder models through a new approach to generative SL.", "motivation": "To explore the potential of causal LLMs in sequence labeling tasks and address the limitations of existing encoder models, especially in the context of supervised generative SL.", "method": "The paper introduces SIFT, which combines in-context learning (ICL) from demonstrations with supervised fine-tuning to treat SL tasks as constrained response generation.", "result": "SIFT shows significant performance improvements over both in-context learning and traditional decoder-as-encoder fine-tuning approaches across several sequence labeling tasks.", "conclusion": "The study demonstrates that removing unnecessary instructions can improve performance in generative SL and emphasizes the effectiveness of a response-based formulation.", "key_contributions": ["Introduction of supervised in-context fine-tuning (SIFT) for generative SL tasks.", "Empirical demonstration of SIFT's superiority over existing methods.", "Insights into the impact of long context and instruction removal on SL performance."], "limitations": "Performance can be hindered by long context lengths, which can be mitigated by modifying instructions.", "keywords": ["sequence labeling", "causal LLMs", "supervised generative SL", "in-context learning", "fine-tuning"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2509.00934", "pdf": "https://arxiv.org/pdf/2509.00934.pdf", "abs": "https://arxiv.org/abs/2509.00934", "title": "MedCOD: Enhancing English-to-Spanish Medical Translation of Large Language Models Using Enriched Chain-of-Dictionary Framework", "authors": ["Md Shahidul Salim", "Lian Fu", "Arav Adikesh Ramakrishnan", "Zonghai Yao", "Hong Yu"], "categories": ["cs.CL", "cs.AI"], "comment": "To appear in Findings of the Association for Computational\n  Linguistics: EMNLP 2025", "summary": "We present MedCOD (Medical Chain-of-Dictionary), a hybrid framework designed\nto improve English-to-Spanish medical translation by integrating\ndomain-specific structured knowledge into large language models (LLMs). MedCOD\nintegrates domain-specific knowledge from both the Unified Medical Language\nSystem (UMLS) and the LLM-as-Knowledge-Base (LLM-KB) paradigm to enhance\nstructured prompting and fine-tuning. We constructed a parallel corpus of 2,999\nEnglish-Spanish MedlinePlus articles and a 100-sentence test set annotated with\nstructured medical contexts. Four open-source LLMs (Phi-4, Qwen2.5-14B,\nQwen2.5-7B, and LLaMA-3.1-8B) were evaluated using structured prompts that\nincorporated multilingual variants, medical synonyms, and UMLS-derived\ndefinitions, combined with LoRA-based fine-tuning. Experimental results\ndemonstrate that MedCOD significantly improves translation quality across all\nmodels. For example, Phi-4 with MedCOD and fine-tuning achieved BLEU 44.23,\nchrF++ 28.91, and COMET 0.863, surpassing strong baseline models like GPT-4o\nand GPT-4o-mini. Ablation studies confirm that both MedCOD prompting and model\nadaptation independently contribute to performance gains, with their\ncombination yielding the highest improvements. These findings highlight the\npotential of structured knowledge integration to enhance LLMs for medical\ntranslation tasks.", "AI": {"tldr": "MedCOD improves English-to-Spanish medical translation by integrating domain-specific knowledge into LLMs.", "motivation": "The paper addresses the need for improved medical translation quality between English and Spanish, focusing on incorporating structured medical knowledge into LLMs.", "method": "The study employs a hybrid framework (MedCOD) that leverages the Unified Medical Language System (UMLS) alongside LLMs to enhance prompting and fine-tuning, tested on a parallel corpus of medical articles.", "result": "MedCOD significantly enhances translation quality across various LLMs, with the Phi-4 model achieving a BLEU score of 44.23, outperforming baseline models such as GPT-4o.", "conclusion": "Integrating structured medical knowledge improves LLM performance in translation tasks, as confirmed by experimental results and ablation studies.", "key_contributions": ["Introduction of MedCOD for medical translation", "Integration of UMLS knowledge into LLMs", "Demonstrated performance improvements across multiple LLMs"], "limitations": "", "keywords": ["medical translation", "large language models", "domain-specific knowledge"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.00949", "pdf": "https://arxiv.org/pdf/2509.00949.pdf", "abs": "https://arxiv.org/abs/2509.00949", "title": "Structure and Destructure: Dual Forces in the Making of Knowledge Engines", "authors": ["Yihong Chen"], "categories": ["cs.CL", "cs.AI", "68T05, 68T30, 68T50", "I.2.6; I.2.7; H.3.3; K.8.0"], "comment": "PhD thesis. https://discovery.ucl.ac.uk/id/eprint/10211291/", "summary": "The making of knowledge engines in natural language processing has been\nshaped by two seemingly distinct paradigms: one grounded in structure, the\nother driven by massively available unstructured data. The structured paradigm\nleverages predefined symbolic interactions, such as knowledge graphs, as priors\nand designs models to capture them. In contrast, the unstructured paradigm\ncenters on scaling transformer architectures with increasingly vast data and\nmodel sizes, as seen in modern large language models. Despite their divergence,\nthis thesis seeks to establish conceptual connections bridging these paradigms.\nTwo complementary forces, structure and destructure, emerge across both\nparadigms: structure organizes seen symbolic interactions, while destructure,\nthrough periodic embedding resets, improves model plasticity and generalization\nto unseen scenarios. These connections form a new recipe for developing general\nknowledge engines that can support transparent, controllable, and adaptable\nintelligent systems.", "AI": {"tldr": "The thesis connects structured and unstructured paradigms in natural language processing to propose a framework for developing adaptable knowledge engines.", "motivation": "To bridge the gap between structured symbolic interactions (e.g., knowledge graphs) and unstructured data-driven models (e.g., large language models).", "method": "Explores the interplay between structure and destructure in knowledge engineering, utilizing periodic embedding resets for enhanced model plasticity.", "result": "Establishes a conceptual framework that promotes the creation of intelligent systems that are transparent, controllable, and adaptable.", "conclusion": "The proposed connections between the two paradigms offer a new recipe for developing robust knowledge engines.", "key_contributions": ["Establishes conceptual connections between structure and unstructured paradigms in NLP.", "Introduces the dual concepts of structure and destructure in knowledge engine development.", "Proposes a framework for creating adaptable and controllable intelligent systems."], "limitations": "", "keywords": ["Natural Language Processing", "Knowledge Engines", "Large Language Models", "Knowledge Graphs", "Model Adaptability"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2509.00974", "pdf": "https://arxiv.org/pdf/2509.00974.pdf", "abs": "https://arxiv.org/abs/2509.00974", "title": "RPRO:Ranked Preference Reinforcement Optimization for Enhancing Medical QA and Diagnostic Reasoning", "authors": ["Chia-Hsuan Hsu", "Jun-En Ding", "Hsin-Ling Hsu", "Feng Liu", "Fang-Ming Hung"], "categories": ["cs.CL"], "comment": null, "summary": "Medical question answering requires advanced reasoning that integrates domain\nknowledge with logical inference. However, existing large language models\n(LLMs) often generate reasoning chains that lack factual accuracy and clinical\nreliability. We propose Ranked Preference Reinforcement Optimization (RPRO), a\nnovel framework that uniquely combines reinforcement learning with\npreference-driven reasoning refinement to enhance clinical chain-of-thought\n(CoT) performance. RPRO differentiates itself from prior approaches by\nemploying task-adaptive reasoning templates and a probabilistic evaluation\nmechanism that aligns outputs with established clinical workflows, while\nautomatically identifying and correcting low-quality reasoning chains. Unlike\ntraditional pairwise preference methods, RPRO introduces a groupwise ranking\noptimization based on the Bradley-Terry model and incorporates KL-divergence\nregularization for stable training. Experiments on PubMedQA and MedQA-USMLE\nshow consistent improvements over strong baselines. Remarkably, our 1.1B\nparameter model outperforms much larger 7B-13B models, including\nmedical-specialized variants. These findings demonstrate that combining\npreference optimization with quality-driven refinement offers a scalable and\neffective approach to building more reliable, clinically grounded medical LLMs.", "AI": {"tldr": "Proposes a novel framework, Ranked Preference Reinforcement Optimization (RPRO), to improve reasoning accuracy in medical question answering by combining reinforcement learning with preference-driven reasoning refinement.", "motivation": "The need for advanced reasoning in medical question answering that integrates domain knowledge with logical inference and addresses the factual inaccuracy of existing LLMs.", "method": "RPRO introduces task-adaptive reasoning templates and a probabilistic evaluation mechanism, using groupwise ranking optimization based on the Bradley-Terry model and KL-divergence regularization for stable training.", "result": "Experiments show that RPRO achieves consistent improvements on PubMedQA and MedQA-USMLE compared to strong baseline models, with a 1.1B parameter model outperforming larger 7B-13B models.", "conclusion": "The study demonstrates that combining preference optimization with quality-driven refinement can create more reliable, clinically grounded medical LLMs.", "key_contributions": ["Introduction of Ranked Preference Reinforcement Optimization (RPRO) framework.", "Use of task-adaptive reasoning templates for better output quality.", "Groupwise ranking optimization method that enhances clinical workflow alignment."], "limitations": "", "keywords": ["medical question answering", "reinforcement learning", "large language models"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2411.12142", "pdf": "https://arxiv.org/pdf/2411.12142.pdf", "abs": "https://arxiv.org/abs/2411.12142", "title": "A Computational Method for Measuring \"Open Codes\" in Qualitative Analysis", "authors": ["John Chen", "Alexandros Lotsos", "Sihan Cheng", "Caiyi Wang", "Lexie Zhao", "Jessica Hullman", "Bruce Sherin", "Uri Wilensky", "Michael Horn"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "Qualitative analysis is critical to understanding human datasets in many\nsocial science disciplines. A central method in this process is inductive\ncoding, where researchers identify and interpret codes directly from the\ndatasets themselves. Yet, this exploratory approach poses challenges for\nmeeting methodological expectations (such as ``depth'' and ``variation''),\nespecially as researchers increasingly adopt Generative AI (GAI) for support.\nGround-truth-based metrics are insufficient because they contradict the\nexploratory nature of inductive coding, while manual evaluation can be\nlabor-intensive. This paper presents a theory-informed computational method for\nmeasuring inductive coding results from humans and GAI. Our method first merges\nindividual codebooks using an LLM-enriched algorithm. It measures each coder's\ncontribution against the merged result using four novel metrics: Coverage,\nOverlap, Novelty, and Divergence. Through two experiments on a human-coded\nonline conversation dataset, we 1) reveal the merging algorithm's impact on\nmetrics; 2) validate the metrics' stability and robustness across multiple runs\nand different LLMs; and 3) showcase the metrics' ability to diagnose coding\nissues, such as excessive or irrelevant (hallucinated) codes. Our work provides\na reliable pathway for ensuring methodological rigor in human-AI qualitative\nanalysis.", "AI": {"tldr": "This paper presents a computational method for measuring inductive coding results achieved by humans and Generative AI, addressing the challenges of qualitative analysis in social sciences.", "motivation": "To enhance the methodological rigor in inductive coding, especially as researchers adopt Generative AI, while addressing the limitations of traditional evaluation methods.", "method": "The method merges individual codebooks using an LLM-enriched algorithm to create a unified result, measuring contributions with four metrics: Coverage, Overlap, Novelty, and Divergence.", "result": "The experiments reveal the merging algorithm's impact and validate the stability of the metrics, highlighting their effectiveness in diagnosing relevant coding issues.", "conclusion": "The proposed method offers a robust framework for ensuring quality in qualitative analysis involving human and AI collaboration.", "key_contributions": ["Development of a computational method for inductive coding results", "Introduction of four novel metrics for evaluating coding contributions", "Validation of the metrics' robustness across multiple LLMs"], "limitations": "", "keywords": ["inductive coding", "Generative AI", "qualitative analysis", "LLM", "human-AI collaboration"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2509.00983", "pdf": "https://arxiv.org/pdf/2509.00983.pdf", "abs": "https://arxiv.org/abs/2509.00983", "title": "Performance Analysis of Supervised Machine Learning Algorithms for Text Classification", "authors": ["Sadia Zaman Mishu", "S M Rafiuddin"], "categories": ["cs.CL"], "comment": "8 pages, 2 figures, published in 2016 at the 19th International\n  Conference on Computer and Information Technology (ICCIT), Bangladesh,\n  proceedings pp. 409-413, IEEE", "summary": "The demand for text classification is growing significantly in web searching,\ndata mining, web ranking, recommendation systems, and so many other fields of\ninformation and technology. This paper illustrates the text classification\nprocess on different datasets using some standard supervised machine learning\ntechniques. Text documents can be classified through various kinds of\nclassifiers. Labeled text documents are used to classify the text in supervised\nclassifications. This paper applies these classifiers on different kinds of\nlabeled documents and measures the accuracy of the classifiers. An Artificial\nNeural Network (ANN) model using Back Propagation Network (BPN) is used with\nseveral other models to create an independent platform for labeled and\nsupervised text classification process. An existing benchmark approach is used\nto analyze the performance of classification using labeled documents.\nExperimental analysis on real data reveals which model works well in terms of\nclassification accuracy.", "AI": {"tldr": "This paper discusses the growing demand for text classification in various fields and evaluates different supervised machine learning techniques on labeled datasets.", "motivation": "The demand for effective text classification is increasing in sectors like web searching, data mining, and recommendation systems, necessitating thorough exploration of machine learning techniques.", "method": "The paper utilizes several standard supervised machine learning classifiers, including an Artificial Neural Network model with Back Propagation Network, to classify labeled text documents and measures their accuracy.", "result": "Experimental analysis indicates the performance of different classifiers on real data, highlighting their respective accuracy levels in text classification tasks.", "conclusion": "The study exemplifies the effectiveness of using multiple classification models for text classification and provides insights into which classifiers deliver higher accuracy.", "key_contributions": ["Demonstration of supervised machine learning techniques for text classification", "Comparison of different classifiers on various datasets", "Establishment of a platform for analyzing supervised text classification models"], "limitations": "", "keywords": ["text classification", "supervised machine learning", "Artificial Neural Networks"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2509.01011", "pdf": "https://arxiv.org/pdf/2509.01011.pdf", "abs": "https://arxiv.org/abs/2509.01011", "title": "Ranking of Bangla Word Graph using Graph-based Ranking Algorithms", "authors": ["S M Rafiuddin"], "categories": ["cs.CL"], "comment": "8 pages, 2 figures, Publication date 2017-12-07, Conference 2017 3rd\n  International Conference on Electrical Information and Communication\n  Technology EICT, Pages 1-5, Publisher IEEE", "summary": "Ranking words is an important way to summarize a text or to retrieve\ninformation. A word graph is a way to represent the words of a sentence or a\ntext as the vertices of a graph and to show the relationship among the words.\nIt is also useful to determine the relative importance of a word among the\nwords in the word-graph. In this research, the ranking of Bangla words are\ncalculated, representing Bangla words from a text in a word graph using various\ngraph based ranking algorithms. There is a lack of a standard Bangla word\ndatabase. In this research, the Indian Language POS-tag Corpora is used, which\nhas a rich collection of Bangla words in the form of sentences with their parts\nof speech tags. For applying a word graph to various graph based ranking\nalgorithms, several standard procedures are applied. The preprocessing steps\nare done in every word graph and then applied to graph based ranking algorithms\nto make a comparison among these algorithms. This paper illustrate the entire\nprocedure of calculating the ranking of Bangla words, including the\nconstruction of the word graph from text. Experimental result analysis on real\ndata reveals the accuracy of each ranking algorithm in terms of F1 measure.", "AI": {"tldr": "The paper investigates ranking Bangla words using graph-based algorithms by constructing word graphs to analyze relationships among words.", "motivation": "To develop a method for ranking Bangla words due to the absence of a standard word database and to enhance information retrieval and text summarization.", "method": "The study utilizes the Indian Language POS-tag Corpora to create word graphs and applies various graph-based ranking algorithms after preprocessing the data.", "result": "The paper shows that the experimental results reveal the accuracy of the ranking algorithms using the F1 measure on real data.", "conclusion": "The proposed method demonstrates a systematic approach for calculating the ranking of Bangla words, useful for text summarization and information retrieval.", "key_contributions": ["Introduction of a word graph approach for Bangla word ranking", "Comparison of different graph-based ranking algorithms", "Use of Indian Language POS-tag Corpora for Bangla words"], "limitations": "Dependence on the availability of a standard Bangla word database; performance limited to the algorithms evaluated in the study.", "keywords": ["Bangla words", "word graph", "ranking algorithms", "graph-based methods", "information retrieval"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2509.01035", "pdf": "https://arxiv.org/pdf/2509.01035.pdf", "abs": "https://arxiv.org/abs/2509.01035", "title": "We Politely Insist: Your LLM Must Learn the Persian Art of Taarof", "authors": ["Nikta Gohari Sadr", "Sahar Heidariasl", "Karine Megerdoomian", "Laleh Seyyed-Kalantari", "Ali Emami"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "Large language models (LLMs) struggle to navigate culturally specific\ncommunication norms, limiting their effectiveness in global contexts. We focus\non Persian taarof, a social norm in Iranian interactions, which is a\nsophisticated system of ritual politeness that emphasizes deference, modesty,\nand indirectness, yet remains absent from existing cultural benchmarks. We\nintroduce TaarofBench, the first benchmark for evaluating LLM understanding of\ntaarof, comprising 450 role-play scenarios covering 12 common social\ninteraction topics, validated by native speakers. Our evaluation of five\nfrontier LLMs reveals substantial gaps in cultural competence, with accuracy\nrates 40-48% below native speakers when taarof is culturally appropriate.\nPerformance varies between interaction topics, improves with Persian-language\nprompts, and exhibits gender-based asymmetries. We also show that responses\nrated \"polite\" by standard metrics often violate taarof norms, indicating the\nlimitations of Western politeness frameworks. Through supervised fine-tuning\nand Direct Preference Optimization, we achieve 21.8% and 42.3% improvement in\nmodel alignment with cultural expectations. Our human study with 33\nparticipants (11 native Persian, 11 heritage, and 11 non-Iranian speakers)\nforms baselines in varying degrees of familiarity with Persian norms. This work\nlays the foundation for developing diverse and culturally aware LLMs, enabling\napplications that better navigate complex social interactions.", "AI": {"tldr": "This paper introduces TaarofBench, a benchmark for evaluating large language models' understanding of the Persian cultural norm of taarof, highlighting significant gaps in their cultural competence.", "motivation": "To address the limitations of large language models in navigating culturally specific communication norms, specifically focusing on the Persian social norm of taarof.", "method": "The study evaluates five frontier large language models using TaarofBench, which comprises 450 role-play scenarios covering 12 social interaction topics, along with supervised fine-tuning and Direct Preference Optimization for model improvement.", "result": "The evaluation reveals accuracy rates 40-48% lower than native speakers in culturally appropriate contexts and significant performance improvements with tailored prompts and optimization techniques.", "conclusion": "The research demonstrates the importance of developing culturally aware language models and provides a foundational benchmark for enhancing their performance in diverse social contexts.", "key_contributions": ["Introduction of TaarofBench as a novel evaluation benchmark for LLMs regarding cultural competence.", "Demonstration of significant gaps in LLM performance in understanding Persian taarof.", "Implementation of supervised fine-tuning methods leading to notable improvements in adherence to cultural norms."], "limitations": "Focus on a specific cultural norm (taarof) may not generalize to other cultural contexts; limited sample size of participants in the human study.", "keywords": ["large language models", "taarof", "cultural competence", "benchmarking", "human-computer interaction"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.01053", "pdf": "https://arxiv.org/pdf/2509.01053.pdf", "abs": "https://arxiv.org/abs/2509.01053", "title": "A Dynamic Fusion Model for Consistent Crisis Response", "authors": ["Xiaoying Song", "Anirban Saha Anik", "Eduardo Blanco", "Vanessa Frias-Martinez", "Lingzi Hong"], "categories": ["cs.CL"], "comment": "Accepted at Findings of EMNLP 2025, 10 pages, 5 figures", "summary": "In response to the urgent need for effective communication with\ncrisis-affected populations, automated responses driven by language models have\nbeen proposed to assist in crisis communications. A critical yet often\noverlooked factor is the consistency of response style, which could affect the\ntrust of affected individuals in responders. Despite its importance, few\nstudies have explored methods for maintaining stylistic consistency across\ngenerated responses. To address this gap, we propose a novel metric for\nevaluating style consistency and introduce a fusion-based generation approach\ngrounded in this metric. Our method employs a two-stage process: it first\nassesses the style of candidate responses and then optimizes and integrates\nthem at the instance level through a fusion process. This enables the\ngeneration of high-quality responses while significantly reducing stylistic\nvariation between instances. Experimental results across multiple datasets\ndemonstrate that our approach consistently outperforms baselines in both\nresponse quality and stylistic uniformity.", "AI": {"tldr": "This paper presents a novel approach to ensure consistent response styles in automated crisis communications driven by language models, focusing on trust-building with affected populations.", "motivation": "The paper addresses the urgent need for effective communication in crisis situations and highlights the overlooked importance of stylistic consistency in maintaining trust with crisis-affected individuals.", "method": "The proposed methodology includes a two-stage process: first assessing the style of candidate responses and then optimizing and integrating them through a fusion process to ensure consistency.", "result": "Experimental results show that the proposed method outperforms baseline approaches in both the quality of responses and the uniformity of stylistic elements across generated answers.", "conclusion": "The study concludes that maintaining stylistic consistency in automated responses can enhance trust and communication effectiveness in crisis scenarios.", "key_contributions": ["Introduction of a novel metric for evaluating style consistency", "Development of a fusion-based generation approach", "Demonstration of improved response quality and stylistic uniformity across datasets"], "limitations": "", "keywords": ["Crisis communication", "Language models", "Style consistency", "Human-computer interaction", "Natural language generation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.01058", "pdf": "https://arxiv.org/pdf/2509.01058.pdf", "abs": "https://arxiv.org/abs/2509.01058", "title": "Speaking at the Right Level: Literacy-Controlled Counterspeech Generation with RAG-RL", "authors": ["Xiaoying Song", "Anirban Saha Anik", "Dibakar Barua", "Pengcheng Luo", "Junhua Ding", "Lingzi Hong"], "categories": ["cs.CL"], "comment": "Accepted at Findings of EMNLP 2025", "summary": "Health misinformation spreading online poses a significant threat to public\nhealth. Researchers have explored methods for automatically generating\ncounterspeech to health misinformation as a mitigation strategy. Existing\napproaches often produce uniform responses, ignoring that the health literacy\nlevel of the audience could affect the accessibility and effectiveness of\ncounterspeech. We propose a Controlled-Literacy framework using\nretrieval-augmented generation (RAG) with reinforcement learning (RL) to\ngenerate tailored counterspeech adapted to different health literacy levels. In\nparticular, we retrieve knowledge aligned with specific health literacy levels,\nenabling accessible and factual information to support generation. We design a\nreward function incorporating subjective user preferences and objective\nreadability-based rewards to optimize counterspeech to the target health\nliteracy level. Experiment results show that Controlled-Literacy outperforms\nbaselines by generating more accessible and user-preferred counterspeech. This\nresearch contributes to more equitable and impactful public health\ncommunication by improving the accessibility and comprehension of counterspeech\nto health misinformation.", "AI": {"tldr": "The paper introduces a Controlled-Literacy framework for generating tailored counterspeech to health misinformation, adapting to different audience health literacy levels using retrieval-augmented generation and reinforcement learning.", "motivation": "Health misinformation poses a major risk to public health, and effective counterspeech is needed to mitigate this threat.", "method": "The Controlled-Literacy framework utilizes retrieval-augmented generation (RAG) combined with reinforcement learning (RL) to tailor counterspeech based on specified health literacy levels, integrating knowledge retrieval and a novel reward function that includes user preferences and readability.", "result": "Experiment results demonstrate that Controlled-Literacy generates more accessible and user-preferred counterspeech compared to existing baselines.", "conclusion": "This research enhances public health communication by ensuring that counterspeech to health misinformation is more equitable and comprehensible, tailored to varying health literacy levels.", "key_contributions": ["Introduction of the Controlled-Literacy framework for tailored counterspeech", "Integration of retrieval-augmented generation and reinforcement learning", "Development of a reward function that focuses on user preferences and readability"], "limitations": "", "keywords": ["health misinformation", "counterspeech", "health literacy", "retrieval-augmented generation", "reinforcement learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.01081", "pdf": "https://arxiv.org/pdf/2509.01081.pdf", "abs": "https://arxiv.org/abs/2509.01081", "title": "Assessing Large Language Models on Islamic Legal Reasoning: Evidence from Inheritance Law Evaluation", "authors": ["Abdessalam Bouchekif", "Samer Rashwani", "Heba Sbahi", "Shahd Gaben", "Mutez Al-Khatib", "Mohammed Ghaly"], "categories": ["cs.CL", "cs.AI", "I.2.6; I.2.7"], "comment": "10 pages, 7 Tables, Code:\n  https://github.com/bouchekif/inheritance_evaluation", "summary": "This paper evaluates the knowledge and reasoning capabilities of Large\nLanguage Models in Islamic inheritance law, known as 'ilm al-mawarith. We\nassess the performance of seven LLMs using a benchmark of 1,000 multiple-choice\nquestions covering diverse inheritance scenarios, designed to test models'\nability to understand the inheritance context and compute the distribution of\nshares prescribed by Islamic jurisprudence. The results reveal a significant\nperformance gap: o3 and Gemini 2.5 achieved accuracies above 90%, whereas\nALLaM, Fanar, LLaMA, and Mistral scored below 50%. These disparities reflect\nimportant differences in reasoning ability and domain adaptation. We conduct a\ndetailed error analysis to identify recurring failure patterns across models,\nincluding misunderstandings of inheritance scenarios, incorrect application of\nlegal rules, and insufficient domain knowledge. Our findings highlight\nlimitations in handling structured legal reasoning and suggest directions for\nimproving performance in Islamic legal reasoning. Code:\nhttps://github.com/bouchekif/inheritance_evaluation", "AI": {"tldr": "Evaluation of LLMs in Islamic inheritance law shows variable accuracy; top models surpass 90%, others struggle below 50%.", "motivation": "To assess LLMs' reasoning capabilities in Islamic inheritance law and identify performance disparities among them.", "method": "Seven LLMs were tested using a benchmark of 1,000 multiple-choice questions related to Islamic inheritance scenarios.", "result": "Top models (o3 and Gemini 2.5) scored above 90% accuracy, while others (ALLaM, Fanar, LLaMA, Mistral) were below 50% accuracy.", "conclusion": "Performance gaps highlight challenges in structured legal reasoning; improvements and adaptations are necessary for better performance in Islamic legal contexts.", "key_contributions": ["Benchmarking of LLMs in a specific legal context", "Identification of reasoning gaps in handling inheritance law", "Error analysis revealing misunderstanding patterns among models."], "limitations": "Limited to Islamic inheritance law; findings may not generalize to other legal domains.", "keywords": ["Large Language Models", "Islamic inheritance law", "legal reasoning", "domain adaptation", "error analysis"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2509.01084", "pdf": "https://arxiv.org/pdf/2509.01084.pdf", "abs": "https://arxiv.org/abs/2509.01084", "title": "A Paradigm Gap in Urdu", "authors": ["Farah Adeeba", "Rajesh Bhatt"], "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we document a paradigm gap in the combinatorial possibilities\nof verbs and aspect in Urdu: the perfective form of the -ya: kar construction\n(e.g. ro-ya: ki: cry-Pfv do.Pfv) is sharply ungrammatical in modern Urdu and\nHindi, despite being freely attested in 19th century literature. We investigate\nthis diachronic shift through historical text analysis, a large-scale corpus\nstudy which confirms the stark absence of perfective forms and subjective\nevaluation tasks with native speakers, who judge perfective examples as highly\nunnatural. We argue that this gap arose from a fundamental morphosyntactic\nconflict: the construction's requirement for a nominative subject and an\ninvariant participle clashes with the core grammatical rule that transitive\nperfective assign ergative case. This conflict rendered the perfective form\nunstable, and its functional replacement by other constructions allowed the gap\nto become entrenched in the modern grammar.", "AI": {"tldr": "The paper explores a grammatical gap in modern Urdu and Hindi regarding the perfective form of the -ya: kar construction, which was once common in 19th century literature but is now considered ungrammatical.", "motivation": "To investigate the diachronic shift in the grammatical status of perfective forms in Urdu and Hindi.", "method": "The study employs historical text analysis, a large-scale corpus study, and subjective evaluation tasks with native speakers to assess acceptability.", "result": "The research confirms the absence of perfective forms in modern usage, with speakers judging them as unnatural. It identifies a morphosyntactic conflict as the cause of this grammatical gap.", "conclusion": "The perfective form became unstable due to the conflict between the construction's requirements and transitive perfective case assignment, leading to its replacement in modern grammar.", "key_contributions": ["Identification of a grammatical gap in modern Urdu and Hindi", "Historical analysis linking 19th century literature with contemporary usage", "Psycholinguistic evidence from native speaker evaluations"], "limitations": "", "keywords": ["Urdu", "Hindi", "Grammatical Shift", "Morphosyntax", "Perfective Form"], "importance_score": 2, "read_time_minutes": 15}}
{"id": "2509.01088", "pdf": "https://arxiv.org/pdf/2509.01088.pdf", "abs": "https://arxiv.org/abs/2509.01088", "title": "Privacy-Preserving Reasoning with Knowledge-Distilled Parametric Retrieval Augmented Generation", "authors": ["Jinwen Chen", "Hainan Zhang", "Liang Pang", "Yongxin Tong", "Haibo Zhou", "Yuan Zhan", "Wei Lin", "Zhiming Zheng"], "categories": ["cs.CL"], "comment": null, "summary": "The current RAG system requires uploading plaintext documents to the cloud,\nrisking private data leakage. Parametric RAG (PRAG) addresses this by encoding\ndocuments as LoRA within LLMs, enabling reasoning without exposing raw content.\nHowever, it still faces two issues: (1) PRAG demands synthesizing QA pairs and\nfine-tuning LLM for each individual document to create its corresponding LoRA,\nleading to unacceptable inference latency. (2) The performance of PRAG relies\nsolely on synthetic QA data, lacking internal alignment with standard RAG,\nresulting in poor generalization on out-of-distribution(OOD) inputs. Therefore,\nachieving high-efficiency parameterization while maintaining RAG-level\nperformance remains a critical challenge for privacy-preserving reasoning. In\nthis paper, we propose DistilledPRAG, a generalizable knowledge-distilled\nparametric RAG model aligned with standard RAG in document structure and\nparameter activation. We first synthesize QA pairs from single and\nmulti-documents to enhance cross-document reasoning. Then, we mask the\nplaintext documents with a special token and translate them to LoRA via a\nparameter generator, maintaining the standard RAG document structure. Finally,\nguided by synthetic QA data, we train the parameter generator to match standard\nRAG's hidden states and output logits, enabling RAG-style reasoning without\noriginal documents. Experiments on four QA datasets show that DistilledPRAG\noutperforms baselines in accuracy and generalizes well on OOD data.", "AI": {"tldr": "This paper introduces DistilledPRAG, a privacy-preserving knowledge-distilled approach to improve the efficiency and generalization of retrieval-augmented generation (RAG) systems without exposing raw document content.", "motivation": "To address privacy concerns in current RAG systems that require uploading plaintext documents to the cloud, risking data leakage.", "method": "DistilledPRAG enhances document reasoning by synthesizing QA pairs from documents while using a parameter generator to translate masked plaintext documents into LoRA format, maintaining RAG structure.", "result": "DistilledPRAG outperforms existing methods in accuracy and demonstrates improved generalization to out-of-distribution (OOD) input data across four QA datasets.", "conclusion": "The proposed model effectively balances privacy and performance, achieving efficient parameterization while retaining the core advantages of standard RAG systems.", "key_contributions": ["Introduction of DistilledPRAG as a knowledge-distilled parametric RAG model.", "Enhancement of cross-document reasoning through synthesized QA pairs.", "Demonstration of improved accuracy and generalization on OOD data compared to baseline models."], "limitations": "The study does not address potential scalability issues with the parameter generator for very large document sets.", "keywords": ["parametric RAG", "distillation", "privacy-preserving reasoning", "cross-document QA", "machine learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.01092", "pdf": "https://arxiv.org/pdf/2509.01092.pdf", "abs": "https://arxiv.org/abs/2509.01092", "title": "REFRAG: Rethinking RAG based Decoding", "authors": ["Xiaoqiang Lin", "Aritra Ghosh", "Bryan Kian Hsiang Low", "Anshumali Shrivastava", "Vijai Mohan"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nleveraging extensive external knowledge to enhance responses in multi-turn and\nagentic applications, such as retrieval-augmented generation (RAG). However,\nprocessing long-context inputs introduces significant system latency and\ndemands substantial memory for the key-value cache, resulting in reduced\nthroughput and a fundamental trade-off between knowledge enrichment and system\nefficiency. While minimizing latency for long-context inputs is a primary\nobjective for LLMs, we contend that RAG require specialized consideration. In\nRAG, much of the LLM context consists of concatenated passages from retrieval,\nwith only a small subset directly relevant to the query. These passages often\nexhibit low semantic similarity due to diversity or deduplication during\nre-ranking, leading to block-diagonal attention patterns that differ from those\nin standard LLM generation tasks. Based on this observation, we argue that most\ncomputations over the RAG context during decoding are unnecessary and can be\neliminated with minimal impact on performance. To this end, we propose REFRAG,\nan efficient decoding framework that compresses, senses, and expands to improve\nlatency in RAG applications. By exploiting the sparsity structure, we\ndemonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to\nprevious work) without loss in perplexity. In addition, our optimization\nframework for large context enables REFRAG to extend the context size of LLMs\nby 16. We provide rigorous validation of REFRAG across diverse long-context\ntasks, including RAG, multi-turn conversations, and long document\nsummarization, spanning a wide range of datasets. Experimental results confirm\nthat REFRAG delivers substantial speedup with no loss in accuracy compared to\nLLaMA models and other state-of-the-art baselines across various context sizes.", "AI": {"tldr": "REFRAG is an efficient decoding framework designed to reduce latency in retrieval-augmented generation (RAG) applications by optimizing long-context input processing, achieving significant speedups without sacrificing performance.", "motivation": "To address the trade-off between knowledge enrichment and system efficiency in LLMs when processing long-context inputs in RAG applications.", "method": "REFRAG utilizes a compress-sense-expand approach to eliminate unnecessary computations during decoding, capitalizing on the sparsity structure of concatenated retrieval passages, which are generally low in semantic similarity.", "result": "REFRAG achieved a 30.85 times acceleration in time-to-first-token and a 3.75 improvement over previous work, extending the context size of LLMs by 16 without loss in perplexity.", "conclusion": "The experimental validation of REFRAG demonstrates substantial speedup across diverse long-context tasks with maintained accuracy, proving its utility in enhancing LLM performance in RAG scenarios.", "key_contributions": ["Introduction of the REFRAG framework for efficient decoding in RAG applications", "Realization of significant speedup in time-to-first-token without accuracy loss", "Extension of the context size of LLMs for improved performance on long-context inputs"], "limitations": "", "keywords": ["Large Language Models", "Retrieval-Augmented Generation", "Decoding Framework", "Efficiency", "Long-Context Inputs"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.01093", "pdf": "https://arxiv.org/pdf/2509.01093.pdf", "abs": "https://arxiv.org/abs/2509.01093", "title": "Natural Context Drift Undermines the Natural Language Understanding of Large Language Models", "authors": ["Yulong Wu", "Viktor Schlegel", "Riza Batista-Navarro"], "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025 Findings", "summary": "How does the natural evolution of context paragraphs affect question\nanswering in generative Large Language Models (LLMs)? To investigate this, we\npropose a framework for curating naturally evolved, human-edited variants of\nreading passages from contemporary QA benchmarks and for analyzing LLM\nperformance across a range of semantic similarity scores, which quantify how\nclosely each variant aligns with content seen during pretraining. Using this\nframework, we evaluate six QA datasets and eight LLMs with publicly available\ntraining data. Our experiments reveal that LLM performance declines as reading\npassages naturally diverge from the versions encountered during\npretraining-even when the question and all necessary information remains\npresent at inference time. For instance, average model accuracy on BoolQ drops\nby over 30% from the highest to lowest similarity bins, with slopes exceeding\n70 across several LLMs. These findings suggest that natural text evolution\nposes a significant challenge to the language understanding capabilities of\nLLMs.", "AI": {"tldr": "The study investigates how natural evolution of text passages affects the performance of generative Large Language Models in question answering tasks.", "motivation": "To understand the impact of naturally evolved, human-edited text on the performance of Large Language Models in question answering.", "method": "A framework was proposed to curate variants of reading passages and evaluate LLM performance across various semantic similarity scores.", "result": "LLM performance declines significantly as the text diverges from pretraining data, with accuracy on BoolQ dropping by over 30% across similarity bins, indicating the challenge posed by text evolution to LLM understanding.", "conclusion": "Natural text evolution significantly challenges the language understanding capabilities of LLMs, affecting their performance in question answering tasks.", "key_contributions": ["Development of a framework for evaluating text evolution in QA benchmarks", "Insight into the impact of text variation on LLM performance", "Quantitative analysis of the relationship between text similarity and model accuracy"], "limitations": "Limitations of the study include potential biases in the selection of QA datasets and LLMs used for evaluation.", "keywords": ["Large Language Models", "Question Answering", "Text Evolution", "Semantic Similarity", "Natural Language Understanding"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.01142", "pdf": "https://arxiv.org/pdf/2509.01142.pdf", "abs": "https://arxiv.org/abs/2509.01142", "title": "Dream-Coder 7B: An Open Diffusion Language Model for Code", "authors": ["Zhihui Xie", "Jiacheng Ye", "Lin Zheng", "Jiahui Gao", "Jingwei Dong", "Zirui Wu", "Xueliang Zhao", "Shansan Gong", "Xin Jiang", "Zhenguo Li", "Lingpeng Kong"], "categories": ["cs.CL"], "comment": null, "summary": "We present Dream-Coder 7B, an open-source discrete diffusion language model\nfor code generation that exhibits emergent any-order generation capabilities.\nUnlike traditional autoregressive (AR) models that decode strictly\nleft-to-right, Dream-Coder 7B adaptively determines its decoding strategy based\non the coding task: sketch-first generation for complex algorithms,\nleft-to-right generation for straightforward completions, and interleaved\nreasoning generation for code understanding tasks. We adapt a pretrained AR\ncheckpoint to a discrete diffusion frameworks with a continuous-time weighted\ncross-entropy objective. Our post-training recipe comprises (i) supervised\nfine-tuning, where we mitigate padding pathologies via random truncation and a\npadding penalty to improve sample efficiency and stabilize generation; and (ii)\nreinforcement learning with verifiable rewards over a curated high-quality\nprompt set drawn from open-source datasets, using a tailored reinforcement\nlearning recipe for diffusion language models. The resulting Dream-Coder 7B\nInstruct attains 21.4\\% pass@1 on LiveCodeBench (2410--2505) and demonstrates\ncompetitive performance on HumanEval, MBPP, BigCodeBench, and CRUXEval. We\nrelease Dream-Coder-7B and Dream-Coder-7B-Instruct checkpoints, training\nrecipes, preprocessing pipelines, and inference code to facilitate\nreproducibility and further research.", "AI": {"tldr": "Dream-Coder 7B is an open-source diffusion model for code generation that can adaptively change its decoding strategy based on the coding task.", "motivation": "To develop a language model that improves code generation capabilities beyond traditional autoregressive methods by utilizing a discrete diffusion framework for any-order generation.", "method": "The model adapts a pretrained autoregressive checkpoint for discrete diffusion using a continuous-time weighted cross-entropy objective. Fine-tuning involves supervised methods to address padding issues and reinforcement learning guided by high-quality prompts.", "result": "Dream-Coder 7B Instruct achieves a 21.4% pass@1 score on LiveCodeBench and performs well on other benchmarks like HumanEval and BigCodeBench.", "conclusion": "The paper introduces an innovative method for code generation through a new model architecture and release of all supporting materials to aid further research.", "key_contributions": ["Introduction of Dream-Coder 7B, an open-source model for code generation.", "Demonstration of emergent any-order generation capabilities in a diffusion model.", "Release of checkpoints and training resources for reproducibility."], "limitations": "", "keywords": ["code generation", "diffusion model", "language model", "reinforcement learning", "open-source"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.01147", "pdf": "https://arxiv.org/pdf/2509.01147.pdf", "abs": "https://arxiv.org/abs/2509.01147", "title": "Zero-shot Cross-lingual NER via Mitigating Language Difference: An Entity-aligned Translation Perspective", "authors": ["Zhihao Zhang", "Sophia Yat Mei Lee", "Dong Zhang", "Shoushan Li", "Guodong Zhou"], "categories": ["cs.CL"], "comment": "EMNLP 2025", "summary": "Cross-lingual Named Entity Recognition (CL-NER) aims to transfer knowledge\nfrom high-resource languages to low-resource languages. However, existing\nzero-shot CL-NER (ZCL-NER) approaches primarily focus on Latin script language\n(LSL), where shared linguistic features facilitate effective knowledge\ntransfer. In contrast, for non-Latin script language (NSL), such as Chinese and\nJapanese, performance often degrades due to deep structural differences. To\naddress these challenges, we propose an entity-aligned translation (EAT)\napproach. Leveraging large language models (LLMs), EAT employs a\ndual-translation strategy to align entities between NSL and English. In\naddition, we fine-tune LLMs using multilingual Wikipedia data to enhance the\nentity alignment from source to target languages.", "AI": {"tldr": "This paper introduces an entity-aligned translation approach for improving cross-lingual named entity recognition in non-Latin script languages using large language models.", "motivation": "The paper addresses the limitations of existing zero-shot CL-NER approaches that perform poorly for non-Latin script languages due to structural differences.", "method": "The proposed EAT approach employs a dual-translation strategy to align entities between non-Latin script languages and English while fine-tuning large language models on multilingual Wikipedia data.", "result": "The approach enhances entity recognition performance in non-Latin script languages by providing better alignment with English entities.", "conclusion": "The paper concludes that employing LLMs with a dual-translation strategy significantly improves entity alignment and recognition in cross-lingual contexts.", "key_contributions": ["Proposes an entity-aligned translation method for CL-NER in non-Latin scripts", "Implements a dual-translation strategy leveraging LLMs", "Fine-tunes LLMs using multilingual Wikipedia for better entity alignment"], "limitations": "Performance improvement remains context-dependent and may not generalize across all non-Latin languages.", "keywords": ["cross-lingual NER", "zero-shot learning", "non-Latin scripts", "large language models", "entity alignment"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2509.01158", "pdf": "https://arxiv.org/pdf/2509.01158.pdf", "abs": "https://arxiv.org/abs/2509.01158", "title": "Joint Information Extraction Across Classical and Modern Chinese with Tea-MOELoRA", "authors": ["Xuemei Tang", "Chengxi Yan", "Jinghang Gu", "Chu-Ren Huang"], "categories": ["cs.CL"], "comment": "9 pages, 3 figures", "summary": "Chinese information extraction (IE) involves multiple tasks across diverse\ntemporal domains, including Classical and Modern documents. Fine-tuning a\nsingle model on heterogeneous tasks and across different eras may lead to\ninterference and reduced performance. Therefore, in this paper, we propose\nTea-MOELoRA, a parameter-efficient multi-task framework that combines LoRA with\na Mixture-of-Experts (MoE) design. Multiple low-rank LoRA experts specialize in\ndifferent IE tasks and eras, while a task-era-aware router mechanism\ndynamically allocates expert contributions. Experiments show that Tea-MOELoRA\noutperforms both single-task and joint LoRA baselines, demonstrating its\nability to leverage task and temporal knowledge effectively.", "AI": {"tldr": "Tea-MOELoRA is a multi-task framework for Chinese information extraction that uses low-rank parameter efficiency and a router mechanism to combine expertise from different tasks and eras effectively.", "motivation": "The need for effective information extraction from diverse Chinese documents across classical and modern eras, while avoiding performance interference in multi-task learning.", "method": "Introduction of Tea-MOELoRA framework, which uses Mixture-of-Experts (MoE) combined with low-rank adaptation (LoRA) to specialize multiple experts for different tasks and era contexts, alongside a task-era-aware routing mechanism.", "result": "Tea-MOELoRA outperforms both single-task and joint LoRA baselines in extracting information from heterogeneous tasks and temporal domains.", "conclusion": "Tea-MOELoRA demonstrates improved performance in Chinese IE by effectively utilizing task and temporal knowledge through specialized experts and dynamic routing.", "key_contributions": ["Introduction of the Tea-MOELoRA framework for multi-task learning in information extraction.", "Combination of LoRA with MoE for parameter efficiency and task specialization.", "Utilization of a dynamic router mechanism to enhance expert performance based on task and era."], "limitations": "", "keywords": ["Chinese information extraction", "multi-task learning", "LoRA", "Mixture-of-Experts", "task-aware routing"], "importance_score": 4, "read_time_minutes": 6}}
{"id": "2509.01166", "pdf": "https://arxiv.org/pdf/2509.01166.pdf", "abs": "https://arxiv.org/abs/2509.01166", "title": "Enhancing Large Language Model for Knowledge Graph Completion via Structure-Aware Alignment-Tuning", "authors": ["Yu Liu", "Yanan Cao", "Xixun Lin", "Yanmin Shang", "Shi Wang", "Shirui Pan"], "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025, Main, Long Paper", "summary": "Knowledge graph completion (KGC) aims to infer new knowledge and make\npredictions from knowledge graphs. Recently, large language models (LLMs) have\nexhibited remarkable reasoning capabilities. LLM-enhanced KGC methods primarily\nfocus on designing task-specific instructions, achieving promising\nadvancements. However, there are still two critical challenges. First, existing\nmethods often ignore the inconsistent representation spaces between natural\nlanguage and graph structures. Second, most approaches design separate\ninstructions for different KGC tasks, leading to duplicate works and\ntime-consuming processes. To address these challenges, we propose SAT, a novel\nframework that enhances LLMs for KGC via structure-aware alignment-tuning.\nSpecifically, we first introduce hierarchical knowledge alignment to align\ngraph embeddings with the natural language space through multi-task contrastive\nlearning. Then, we propose structural instruction tuning to guide LLMs in\nperforming structure-aware reasoning over KGs, using a unified graph\ninstruction combined with a lightweight knowledge adapter. Experimental results\non two KGC tasks across four benchmark datasets demonstrate that SAT\nsignificantly outperforms state-of-the-art methods, especially in the link\nprediction task with improvements ranging from 8.7% to 29.8%.", "AI": {"tldr": "This paper introduces SAT, a framework that enhances large language models for knowledge graph completion by addressing inconsistencies in representation spaces and optimizing instruction tuning.", "motivation": "To improve knowledge graph completion by leveraging the reasoning capabilities of large language models while overcoming existing challenges in representation and task-specific instructions.", "method": "The authors propose a framework called SAT that utilizes hierarchical knowledge alignment and structural instruction tuning for effective knowledge graph completion.", "result": "SAT significantly outperforms state-of-the-art methods in knowledge graph completion tasks, particularly in link prediction, with performance improvements between 8.7% and 29.8%.", "conclusion": "The results indicate that SAT provides a more efficient and effective means to enhance large language models for knowledge graph completion tasks.", "key_contributions": ["Development of the SAT framework for KGC", "Introduction of hierarchical knowledge alignment", "Proposal of structural instruction tuning for unified graph instruction"], "limitations": "", "keywords": ["Knowledge Graph Completion", "Large Language Models", "Reasoning", "Hierarchical Knowledge Alignment", "Instruction Tuning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.01185", "pdf": "https://arxiv.org/pdf/2509.01185.pdf", "abs": "https://arxiv.org/abs/2509.01185", "title": "Modular Techniques for Synthetic Long-Context Data Generation in Language Model Training and Evaluation", "authors": ["Seganrasan Subramanian", "Abhigya Verma"], "categories": ["cs.CL", "cs.AI"], "comment": "15 pages, 4 figures", "summary": "The ability of large language models (LLMs) to process and reason over long\ntextual inputs is critical for a wide range of real-world applications.\nHowever, progress in this area is significantly constrained by the absence of\nhigh-quality, diverse, and verifiable long-context datasets suitable for both\ntraining and evaluation. This work introduces a modular, extensible framework\nfor synthetic long-context data generation via prompt-based interaction with\nLLMs. The framework supports multiple training and alignment objectives,\nincluding Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO),\nand Group Relative Policy Optimization (GRPO). It encompasses four core\ngeneration paradigms: multi-turn conversational dialogues, document-grounded\ninput-output pairs, verifiable instruction-response tasks, and long-context\nreasoning examples. Through templated prompting, a model-agnostic architecture,\nand metadata-enriched outputs, the proposed approach facilitates scalable,\ncontrollable, and purpose-aligned dataset creation for advancing long-context\ncapabilities in LLMs.", "AI": {"tldr": "This paper introduces a framework for synthetic long-context data generation for large language models (LLMs) to enhance their ability to process long textual inputs, addressing the lack of diverse and verifiable datasets.", "motivation": "To address the limitations in high-quality long-context datasets for training and evaluating large language models, which hampers their reasoning capabilities over long texts.", "method": "A modular framework is developed for generating synthetic long-context data through prompt-based interactions with LLMs, supporting various training and alignment objectives.", "result": "The framework produces four core types of long-context data, including conversational dialogues and reasoning examples, enabling scalable and controllable dataset creation for enhancing LLM capabilities.", "conclusion": "The proposed approach facilitates advancements in long-context handling for LLMs by enabling purpose-aligned dataset creation, thus improving their practical applications.", "key_contributions": ["Introduction of a modular and extensible framework for long-context data generation.", "Support for multiple training and alignment objectives such as SFT and DPO.", "Creation of metadata-enriched outputs to enhance dataset verifiability."], "limitations": "", "keywords": ["Large Language Models", "Long-context processing", "Data generation framework"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.01186", "pdf": "https://arxiv.org/pdf/2509.01186.pdf", "abs": "https://arxiv.org/abs/2509.01186", "title": "Statutory Construction and Interpretation for Artificial Intelligence", "authors": ["Luxi He", "Nimra Nadeem", "Michel Liao", "Howard Chen", "Danqi Chen", "Mariano-Florentino Cuéllar", "Peter Henderson"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "AI systems are increasingly governed by natural language principles, yet a\nkey challenge arising from reliance on language remains underexplored:\ninterpretive ambiguity. As in legal systems, ambiguity arises both from how\nthese principles are written and how they are applied. But while legal systems\nuse institutional safeguards to manage such ambiguity, such as transparent\nappellate review policing interpretive constraints, AI alignment pipelines\noffer no comparable protections. Different interpretations of the same rule can\nlead to inconsistent or unstable model behavior. Drawing on legal theory, we\nidentify key gaps in current alignment pipelines by examining how legal systems\nconstrain ambiguity at both the rule creation and rule application steps. We\nthen propose a computational framework that mirrors two legal mechanisms: (1) a\nrule refinement pipeline that minimizes interpretive disagreement by revising\nambiguous rules (analogous to agency rulemaking or iterative legislative\naction), and (2) prompt-based interpretive constraints that reduce\ninconsistency in rule application (analogous to legal canons that guide\njudicial discretion). We evaluate our framework on a 5,000-scenario subset of\nthe WildChat dataset and show that both interventions significantly improve\njudgment consistency across a panel of reasonable interpreters. Our approach\noffers a first step toward systematically managing interpretive ambiguity, an\nessential step for building more robust, law-following AI systems.", "AI": {"tldr": "The paper addresses interpretive ambiguity in AI alignment processes, proposing a framework inspired by legal systems to enhance consistency in rule application.", "motivation": "AI systems face challenges due to interpretive ambiguity stemming from natural language governance, similar to legal systems.", "method": "The authors create a computational framework that includes a rule refinement pipeline and prompt-based interpretive constraints, analogous to mechanisms in legal systems.", "result": "The proposed framework was evaluated on a subset of the WildChat dataset, demonstrating significant improvement in judgment consistency among interpreters.", "conclusion": "This framework is a crucial step toward managing interpretive ambiguity, contributing to the development of more reliable AI systems.", "key_contributions": ["Proposes a new computational framework to address interpretive ambiguity in AI.", "Applies legal theory to AI alignment processes.", "Demonstrates improved consistency in AI decision-making through experimental evaluation."], "limitations": "", "keywords": ["AI alignment", "interpretive ambiguity", "legal theory", "rule refinement", "judgment consistency"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.01190", "pdf": "https://arxiv.org/pdf/2509.01190.pdf", "abs": "https://arxiv.org/abs/2509.01190", "title": "Efficient Large Language Models with Zero-Shot Adjustable Acceleration", "authors": ["Sajjad Kachuee", "Mohammad Sharifkhani"], "categories": ["cs.CL"], "comment": null, "summary": "Using Large Language Models (LLMs) in real-world applications presents\nsignificant challenges, particularly in balancing computational efficiency and\nperformance. Optimizing acceleration after the fine-tuning phase and during\ninference is crucial for building an efficient architecture. This paper\nintroduces Zero-Shot Adjustable Acceleration, a novel training and inference\nmethod that dynamically adjusts hardware usage during inference without\nrequiring additional fine-tuning. The proposed approach is applied to newly\ndeveloped models and evaluated across multiple classification and text\ngeneration tasks. Experimental results demonstrate that the method enables a\nwide range of acceleration in a zero-shot manner and achieves up to a 11x\nspeedup compared to the baseline.", "AI": {"tldr": "This paper presents a novel method, Zero-Shot Adjustable Acceleration, for optimizing the use of hardware during inference without additional fine-tuning, achieving significant speed improvements for Large Language Models.", "motivation": "To address the challenges of computational efficiency and performance when using Large Language Models in real-world applications.", "method": "The paper introduces a training and inference method that dynamically adjusts hardware usage during inference, optimizing acceleration.", "result": "The proposed method allows for a wide range of acceleration in a zero-shot manner, achieving up to an 11x speedup compared to baseline performance in classification and text generation tasks.", "conclusion": "Zero-Shot Adjustable Acceleration provides an effective solution for optimizing LLM performance during inference without the need for extra fine-tuning, enhancing computational resource utilization.", "key_contributions": ["Introduction of Zero-Shot Adjustable Acceleration as a novel method for LLMs.", "Demonstration of a significant speedup in inference tasks without additional fine-tuning.", "Evaluation across multiple classification and text generation tasks showing the effectiveness of the approach."], "limitations": "", "keywords": ["Large Language Models", "Zero-Shot Learning", "Inference Optimization", "Computational Efficiency", "Text Generation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.01200", "pdf": "https://arxiv.org/pdf/2509.01200.pdf", "abs": "https://arxiv.org/abs/2509.01200", "title": "SimulMEGA: MoE Routers are Advanced Policy Makers for Simultaneous Speech Translation", "authors": ["Chenyang Le", "Bing Han", "Jinshun Li", "Songyong Chen", "Yanmin Qian"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Simultaneous Speech Translation (SimulST) enables real-time cross-lingual\ncommunication by jointly optimizing speech recognition and machine translation\nunder strict latency constraints. Existing systems struggle to balance\ntranslation quality, latency, and semantic coherence, particularly in\nmultilingual many-to-many scenarios where divergent read and write policies\nhinder unified strategy learning. In this paper, we present SimulMEGA\n(Simultaneous Generation by Mixture-of-Experts Gating), an unsupervised policy\nlearning framework that combines prefix-based training with a\nMixture-of-Experts refiner to learn effective read and write decisions in an\nimplicit manner, without adding inference-time overhead. Our design requires\nonly minimal modifications to standard transformer architectures and\ngeneralizes across both speech-to-text and text-to-speech streaming tasks.\nThrough comprehensive evaluation on six language pairs, our 500M parameter\nspeech-to-text model outperforms the Seamless baseline, achieving under 7\npercent BLEU degradation at 1.5 seconds average lag and under 3 percent at 3\nseconds. We further demonstrate the versatility of SimulMEGA by extending it to\nstreaming TTS with a unidirectional backbone, yielding superior latency quality\ntradeoffs.", "AI": {"tldr": "Introducing SimulMEGA, an unsupervised policy learning framework for simultaneous speech translation that optimizes translation quality and latency.", "motivation": "To improve real-time cross-lingual communication by addressing limitations in translation quality, latency, and semantic coherence in existing simultaneous speech translation systems.", "method": "SimulMEGA utilizes a Mixture-of-Experts refiner combined with prefix-based training for implicit learning of effective read and write decisions, integrated with standard transformer architectures.", "result": "SimulMEGA outperforms the Seamless baseline with a 500M parameter speech-to-text model, achieving low BLEU degradation with fast average lag across six language pairs.", "conclusion": "The framework shows promise in enhancing simultaneous speech translation and streaming text-to-speech tasks, achieving superior tradeoffs between latency and quality.", "key_contributions": ["Introduction of an unsupervised policy learning framework for speech translation", "Combines prefix-based training with Mixture-of-Experts refining", "Demonstrates effectiveness across multiple language pairs and tasks"], "limitations": "", "keywords": ["Simultaneous Speech Translation", "Natural Language Processing", "Machine Learning"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2509.01213", "pdf": "https://arxiv.org/pdf/2509.01213.pdf", "abs": "https://arxiv.org/abs/2509.01213", "title": "Mitigating Catastrophic Forgetting in Continual Learning through Model Growth", "authors": ["Ege Süalp", "Mina Rezaei"], "categories": ["cs.CL"], "comment": null, "summary": "Catastrophic forgetting is a significant challenge in continual learning, in\nwhich a model loses prior knowledge when it is fine-tuned on new tasks. This\nproblem is particularly critical for large language models (LLMs) undergoing\ncontinual learning, as retaining performance across diverse domains is\nimportant for their general utility. In this paper, we explore model growth, a\npromising strategy that leverages smaller models to expedite and structure the\ntraining of larger ones for mitigating the catastrophic forgetting problem.\nAlthough growth-based pretraining, particularly via transformer stacking, has\nshown promise in accelerating convergence, its impact on forgetting remains\nunder-explored. Therefore, we evaluate whether growth-based models can retain\npreviously learned capabilities more effectively across a sequence of\nfine-tuning tasks involving domain knowledge, reasoning, reading comprehension,\nand bias. Our findings show that both models -- one trained with growth (Stack\nLLM) and one without (LLM) -- exhibit improvements in domain knowledge.\nHowever, reasoning and reading comprehension degrade over time, indicating\nsigns of catastrophic forgetting. Stack LLM consistently shows less\ndegradation, especially in reading comprehension, suggesting enhanced retention\ncapabilities. Interestingly, in bias evaluation, the baseline LLM becomes\nprogressively more neutral with continued fine-tuning, while Stack LLM\nmaintains a steady bias ratio around 60--61\\%. These results indicate that\ngrowth-based pretraining may deliver modest improvements in resisting\ncatastrophic forgetting, though trade-offs remain in handling social biases.", "AI": {"tldr": "This paper investigates model growth strategies in continual learning to mitigate catastrophic forgetting in large language models, showing that growth-based models retain prior capabilities better, although some degradation still occurs.", "motivation": "To address catastrophic forgetting in continual learning, particularly for large language models, which struggle to retain performance on previous tasks when fine-tuned on new ones.", "method": "The study evaluates growth-based pretraining using transformer stacking to improve retention of domain knowledge and comprehension across fine-tuning tasks.", "result": "Growth-based models (Stack LLM) show improvements in retaining domain knowledge and reduced degradation in reading comprehension compared to baseline LLM models; however, some loss of reasoning abilities and persistence of biases are noted.", "conclusion": "While growth-based pretraining offers modest improvements in resisting catastrophic forgetting, trade-offs in reasoning abilities and social bias handling still exist.", "key_contributions": ["Investigates the efficacy of model growth strategies in mitigating catastrophic forgetting in LLMs.", "Evaluates the performance differences between growth-based models and standard models.", "Finds trade-offs in retention abilities and bias in models during continual learning."], "limitations": "The study highlights that, despite improvements, reasoning and reading comprehension degrade over time, indicating limitations within the growth-based approach.", "keywords": ["catastrophic forgetting", "continual learning", "large language models", "model growth", "transformer stacking"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.01221", "pdf": "https://arxiv.org/pdf/2509.01221.pdf", "abs": "https://arxiv.org/abs/2509.01221", "title": "DaMoC: Efficiently Selecting the Optimal Large Language Model for Fine-tuning Domain Taks Based on Data and Model Compression", "authors": ["Wei Huang", "Huang Wei", "Yinggui Wang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by EMNLP 2025", "summary": "Large language models (LLMs) excel in general tasks but struggle with\ndomain-specific ones, requiring fine-tuning with specific data. With many\nopen-source LLMs available, selecting the best model for fine-tuning downstream\ntasks is challenging, primarily focusing on how to quickly identify the optimal\nLLM. We introduce a Data and Model Compression Framework (DaMoC) that addresses\nthis challenge by: 1) Data Level: A systematic categorization of data filtering\nmethodologies for LLMs is first established, classifying them into three\ndistinct paradigms: (1) distribution-aware methods, (2) quality-aware methods,\nand (3) hybrid approaches considering both dimensions. Further, we enhance the\ndensity of key tokens in the text achieving token compression. Subsequently, we\nuse an LLM to iterative rewrite the text to optimize its expression. 2) Model\nLevel: We use layer similarity scores to assess each layer's importance and\nremove those with lower importance. Then, we introduce a sparse merging\nparadigm to preserve as much of the original model's capability as possible.\nExtensive experiments on four datasets, medical Q&A, financial Q&A, general\nQ&A, and reading comprehension, show that we can select the optimal LLM while\nsaving approximately 20-fold in training time.", "AI": {"tldr": "The paper presents DaMoC, a framework for efficiently selecting and fine-tuning LLMs for domain-specific tasks, significantly reducing training time.", "motivation": "Selecting the best open-source LLM for specific domain tasks is challenging and time-consuming.", "method": "Introduces a systematic data filtering categorization and employs a model compression approach to optimize LLM training.", "result": "DaMoC allows for optimal LLM selection while achieving approximately a 20-fold reduction in training time across various datasets.", "conclusion": "The proposed framework supports effective and efficient LLM fine-tuning, vital for domain-specific applications.", "key_contributions": ["Systematic categorization of data filtering methodologies for LLMs.", "Introduction of a sparse merging paradigm to enhance model efficiency.", "Demonstration of substantial training time reduction across diverse datasets."], "limitations": "", "keywords": ["large language models", "data filtering", "model compression"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.01236", "pdf": "https://arxiv.org/pdf/2509.01236.pdf", "abs": "https://arxiv.org/abs/2509.01236", "title": "Rethinking the Chain-of-Thought: The Roles of In-Context Learning and Pre-trained Priors", "authors": ["Hao Yang", "Zhiyu Yang", "Yunjie Zhang", "Shanyi Zhu", "Lin Yang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Chain-of-Thought reasoning has emerged as a pivotal methodology for enhancing\nmodel inference capabilities. Despite growing interest in Chain-of-Thought\nreasoning, its underlying mechanisms remain unclear. This paper explores the\nworking mechanisms of Chain-of-Thought reasoning from the perspective of the\ndual relationship between in-context learning and pretrained priors. We first\nconduct a fine-grained lexical-level analysis of rationales to examine the\nmodel's reasoning behavior. Then, by incrementally introducing noisy exemplars,\nwe examine how the model balances pretrained priors against erroneous\nin-context information. Finally, we investigate whether prompt engineering can\ninduce slow thinking in large language models. Our extensive experiments reveal\nthree key findings: (1) The model not only quickly learns the reasoning\nstructure at the lexical level but also grasps deeper logical reasoning\npatterns, yet it heavily relies on pretrained priors. (2) Providing sufficient\nexemplars shifts the model's decision-making from pretrained priors to\nin-context signals, while misleading prompts introduce instability. (3) Long\nChain-of-Thought prompting can induce the model to generate longer reasoning\nchains, thereby improving its performance on downstream tasks.", "AI": {"tldr": "This paper investigates the mechanisms of Chain-of-Thought reasoning in models, analyzing its reliance on pretrained priors versus in-context learning, and effects of prompt engineering.", "motivation": "To understand the unclear underlying mechanisms of Chain-of-Thought reasoning that enhance model inference capabilities.", "method": "The paper performs a lexical-level analysis of rationales, introduces noisy exemplars to study their impact on model decision-making, and explores the effects of prompt engineering on reasoning processes.", "result": "The experiments demonstrate that models learn reasoning structures and patterns, exhibit dependence on pretrained priors, and show shifts in decision-making based on the quality of exemplars and prompts.", "conclusion": "Effective long prompting can enhance model reasoning ability, resulting in improved performance on downstream tasks.", "key_contributions": ["Introduced fine-grained analysis of reasoning structures in Chain-of-Thought processing.", "Identified the balance between pretrained priors and in-context signals under various prompting conditions.", "Showed that long prompts may enhance reasoning depth and task performance."], "limitations": "The reliance on pretrained knowledge may limit adaptability to novel tasks.", "keywords": ["Chain-of-Thought reasoning", "in-context learning", "pretrained priors", "prompt engineering", "large language models"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.01260", "pdf": "https://arxiv.org/pdf/2509.01260.pdf", "abs": "https://arxiv.org/abs/2509.01260", "title": "Annotation and modeling of emotions in a textual corpus: an evaluative approach", "authors": ["Jonas Noblet"], "categories": ["cs.CL"], "comment": "in French language. 27{\\`e}me Rencontre des {\\'E}tudiants Chercheurs\n  en Informatique pour le Traitement Automatique des Langues (RECITAL), Jun\n  2025, Marseille, France", "summary": "Emotion is a crucial phenomenon in the functioning of human beings in\nsociety. However, it remains a widely open subject, particularly in its textual\nmanifestations. This paper examines an industrial corpus manually annotated\nfollowing an evaluative approach to emotion. This theoretical framework, which\nis currently underutilized, offers a different perspective that complements\ntraditional approaches. Noting that the annotations we collected exhibit\nsignificant disagreement, we hypothesized that they nonetheless follow stable\nstatistical trends. Using language models trained on these annotations, we\ndemonstrate that it is possible to model the labeling process and that\nvariability is driven by underlying linguistic features. Conversely, our\nresults indicate that language models seem capable of distinguishing emotional\nsituations based on evaluative criteria.", "AI": {"tldr": "The paper explores the evaluation of emotion in text using annotated data, revealing that language models can capture emotional nuances despite variability in human annotations.", "motivation": "To enhance the understanding of emotional expressions in textual data, which is underexplored with traditional methods.", "method": "Analysis of an industrial corpus annotated for emotion using a novel evaluative framework, followed by training language models on this data.", "result": "Language models successfully modeled the labeling process of emotions and identified underlying linguistic features driving annotation variability.", "conclusion": "The findings suggest that emotional situations can be distinguished by language models based on evaluative criteria, despite annotation disagreements.", "key_contributions": ["Introduced a new theoretical framework for emotion evaluation in text.", "Demonstrated the feasibility of modeling human emotion labeling processes with language models.", "Highlighted the relationship between linguistic features and emotional variability in annotations."], "limitations": "The study is based on a specific industrial corpus, which may limit generalizability.", "keywords": ["emotion", "textual analysis", "language models", "annotation", "evaluative framework"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.01301", "pdf": "https://arxiv.org/pdf/2509.01301.pdf", "abs": "https://arxiv.org/abs/2509.01301", "title": "Culture is Everywhere: A Call for Intentionally Cultural Evaluation", "authors": ["Juhyun Oh", "Inha Cha", "Michael Saxon", "Hyunseung Lim", "Shaily Bhatt", "Alice Oh"], "categories": ["cs.CL"], "comment": null, "summary": "The prevailing ``trivia-centered paradigm'' for evaluating the cultural\nalignment of large language models (LLMs) is increasingly inadequate as these\nmodels become more advanced and widely deployed. Existing approaches typically\nreduce culture to static facts or values, testing models via multiple-choice or\nshort-answer questions that treat culture as isolated trivia. Such methods\nneglect the pluralistic and interactive realities of culture, and overlook how\ncultural assumptions permeate even ostensibly ``neutral'' evaluation settings.\nIn this position paper, we argue for \\textbf{intentionally cultural\nevaluation}: an approach that systematically examines the cultural assumptions\nembedded in all aspects of evaluation, not just in explicitly cultural tasks.\nWe systematically characterize the what, how, and circumstances by which\nculturally contingent considerations arise in evaluation, and emphasize the\nimportance of researcher positionality for fostering inclusive, culturally\naligned NLP research. Finally, we discuss implications and future directions\nfor moving beyond current benchmarking practices, discovering important\napplications that we don't know exist, and involving communities in evaluation\ndesign through HCI-inspired participatory methodologies.", "AI": {"tldr": "This paper critiques the current methods for evaluating cultural alignment in large language models, advocating for a more inclusive and comprehensive approach to cultural evaluation in NLP.", "motivation": "Current trivia-centered evaluation methods for cultural alignment in LLMs are inadequate for capturing the complexities of culture.", "method": "The paper proposes an 'intentionally cultural evaluation' approach that examines the cultural assumptions in all evaluation aspects and emphasizes researcher positionality.", "result": "By promoting a comprehensive understanding of cultural considerations, the paper aims to foster more inclusive and effective NLP research practices.", "conclusion": "The authors argue for the necessity of engaging communities and utilizing participatory methodologies to improve evaluation design in NLP.", "key_contributions": ["Introduction of the concept of intentionally cultural evaluation in NLP.", "Emphasis on the need for inclusivity via researcher positionality.", "Call for participatory design methodologies in evaluation processes."], "limitations": "", "keywords": ["Cultural alignment", "Language models", "Evaluation methodologies", "Human-Computer Interaction", "Inclusive research"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.01312", "pdf": "https://arxiv.org/pdf/2509.01312.pdf", "abs": "https://arxiv.org/abs/2509.01312", "title": "TableZoomer: A Collaborative Agent Framework for Large-scale Table Question Answering", "authors": ["Sishi Xiong", "Ziyang He", "Zhongjiang He", "Yu Zhao", "Changzai Pan", "Jie Zhang", "Zhenhe Wu", "Shuangyong Song", "Yongxiang Li"], "categories": ["cs.CL"], "comment": null, "summary": "While large language models (LLMs) have shown promise in the table question\nanswering (TQA) task through prompt engineering, they face challenges in\nindustrial applications, including structural heterogeneity, difficulties in\ntarget data localization, and bottlenecks in complex reasoning. To address\nthese limitations, this paper presents TableZoomer, a novel LLM-powered,\nprogramming-based agent framework. It introduces three key innovations: (1)\nreplacing the original fully verbalized table with structured table schema to\nbridge the semantic gap and reduce computational complexity; (2) a query-aware\ntable zooming mechanism that dynamically generates sub-table schema through\ncolumn selection and entity linking, significantly improving target\nlocalization efficiency; and (3) a Program-of-Thoughts (PoT) strategy that\ntransforms queries into executable code to mitigate numerical hallucination.\nAdditionally, we integrate the reasoning workflow with the ReAct paradigm to\nenable iterative reasoning. Extensive experiments demonstrate that our\nframework maintains the usability advantages while substantially enhancing\nperformance and scalability across tables of varying scales. When implemented\nwith the Qwen3-8B-Instruct LLM, TableZoomer achieves accuracy improvements of\n19.34% and 25% over conventional PoT methods on the large-scale DataBench\ndataset and the small-scale Fact Checking task of TableBench dataset,\nrespectively.", "AI": {"tldr": "TableZoomer is a novel LLM-powered framework that enhances performance in table question answering by using structured table schemas, a query-aware zooming mechanism, and a Program-of-Thoughts strategy.", "motivation": "Address challenges of LLMs in industrial applications, such as structural heterogeneity and complex reasoning in table question answering.", "method": "Introduces three innovations: (1) uses structured table schemas to reduce complexity, (2) employs query-aware zooming for better localization, and (3) applies a Program-of-Thoughts strategy to convert queries to executable code.", "result": "TableZoomer shows accuracy improvements of 19.34% and 25% over conventional methods on large-scale DataBench and small-scale TableBench datasets, respectively.", "conclusion": "The framework enhances usability while significantly improving performance and scalability in table question answering tasks.", "key_contributions": ["Structured table schema to reduce complexity", "Query-aware table zooming mechanism for efficiency", "Program-of-Thoughts strategy to reduce numerical hallucination"], "limitations": "", "keywords": ["large language models", "table question answering", "machine learning", "framework", "health informatics"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.01314", "pdf": "https://arxiv.org/pdf/2509.01314.pdf", "abs": "https://arxiv.org/abs/2509.01314", "title": "Can Smaller LLMs do better? Unlocking Cross-Domain Potential through Parameter-Efficient Fine-Tuning for Text Summarization", "authors": ["Anum Afzal", "Mehul Kumawat", "Florian Matthes"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs), being generic task solvers, are versatile.\nHowever, despite the vast amount of data they are trained on, there are\nspeculations about their adaptation capabilities to a new domain. Additionally,\nthe simple fine-tuning of the model to incorporate knowledge of a new domain is\ncomputationally expensive and time-consuming. This becomes more challenging\nwhen the domain in question is also low-resource, and labeled data is\nunavailable. We leverage parameter-efficient fine-tuning techniques (PEFTs) on\nhigh-resource datasets to address these challenges to improve performance on\nunseen low-resource domains. Throughout our experiments, we evaluate whether\nintrinsic linguistic commonalities between datasets can be leveraged for\nefficient domain adaptation. We benchmark six PEFTs with\n\\texttt{Llama-3-8B-Instruct} on 14 training datasets from the Scientific,\nMedical, Legal, and News domains for a Text Summarization task. Our experiments\nshow that for low-resource domains, inference using Within-Domain Adapters can\nachieve better performance than Few-Shot as well as a much larger\n\\texttt{Llama-3-70B-Instruct}. Lastly, in the absence of Within-Domain\nAdapters, we explore the concept of using Cross-Domain Adapters as well as the\nstrategic combinations of adapters to leverage intrinsic language similarities\nacross domains, facilitating better adaptability and performance in\nlow-resource settings.", "AI": {"tldr": "The paper explores the use of parameter-efficient fine-tuning techniques (PEFTs) to improve Large Language Models' (LLMs) adaptability to low-resource domains, demonstrating effective domain adaptation strategies for text summarization tasks using Llama-3 models.", "motivation": "To address the challenges of adapting LLMs to low-resource domains where labeled data is scarce and traditional fine-tuning is computationally expensive.", "method": "Leveraging parameter-efficient fine-tuning techniques (PEFTs) on high-resource datasets to enhance performance in low-resource domains, while evaluating linguistic commonalities through benchmarks with 14 datasets across different sectors.", "result": "Within-Domain Adapters significantly outperform Few-Shot methods and a larger Llama-3 model in low-resource settings, showcasing promising adaptability through the strategic use of domain adapters.", "conclusion": "The study concludes that leveraging intrinsic linguistic similarities and using a combination of different domain adapters can lead to better performance in low-resource applications.", "key_contributions": ["Introduction of parameter-efficient fine-tuning techniques for LLMs", "Evaluation of within-domain versus cross-domain adapters", "Identification of linguistic commonalities that aid adaptation in low-resource environments"], "limitations": "", "keywords": ["Large Language Models", "Fine-tuning", "Domain adaptation", "Low-resource domains", "Text summarization"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.01322", "pdf": "https://arxiv.org/pdf/2509.01322.pdf", "abs": "https://arxiv.org/abs/2509.01322", "title": "LongCat-Flash Technical Report", "authors": ["Meituan LongCat Team", "Bayan", "Bei Li", "Bingye Lei", "Bo Wang", "Bolin Rong", "Chao Wang", "Chao Zhang", "Chen Gao", "Chen Zhang", "Cheng Sun", "Chengcheng Han", "Chenguang Xi", "Chi Zhang", "Chong Peng", "Chuan Qin", "Chuyu Zhang", "Cong Chen", "Congkui Wang", "Dan Ma", "Daoru Pan", "Defei Bu", "Dengchang Zhao", "Deyang Kong", "Dishan Liu", "Feiye Huo", "Fengcun Li", "Fubao Zhang", "Gan Dong", "Gang Liu", "Gang Xu", "Ge Li", "Guoqiang Tan", "Guoyuan Lin", "Haihang Jing", "Haomin Fu", "Haonan Yan", "Haoxing Wen", "Haozhe Zhao", "Hong Liu", "Hongmei Shi", "Hongyan Hao", "Hongyin Tang", "Huantian Lv", "Hui Su", "Jiacheng Li", "Jiahao Liu", "Jiahuan Li", "Jiajun Yang", "Jiaming Wang", "Jian Yang", "Jianchao Tan", "Jiaqi Sun", "Jiaqi Zhang", "Jiawei Fu", "Jiawei Yang", "Jiaxi Hu", "Jiayu Qin", "Jingang Wang", "Jiyuan He", "Jun Kuang", "Junhui Mei", "Kai Liang", "Ke He", "Kefeng Zhang", "Keheng Wang", "Keqing He", "Liang Gao", "Liang Shi", "Lianhui Ma", "Lin Qiu", "Lingbin Kong", "Lingtong Si", "Linkun Lyu", "Linsen Guo", "Liqi Yang", "Lizhi Yan", "Mai Xia", "Man Gao", "Manyuan Zhang", "Meng Zhou", "Mengxia Shen", "Mingxiang Tuo", "Mingyang Zhu", "Peiguang Li", "Peng Pei", "Peng Zhao", "Pengcheng Jia", "Pingwei Sun", "Qi Gu", "Qianyun Li", "Qingyuan Li", "Qiong Huang", "Qiyuan Duan", "Ran Meng", "Rongxiang Weng", "Ruichen Shao", "Rumei Li", "Shizhe Wu", "Shuai Liang", "Shuo Wang", "Suogui Dang", "Tao Fang", "Tao Li", "Tefeng Chen", "Tianhao Bai", "Tianhao Zhou", "Tingwen Xie", "Wei He", "Wei Huang", "Wei Liu", "Wei Shi", "Wei Wang", "Wei Wu", "Weikang Zhao", "Wen Zan", "Wenjie Shi", "Xi Nan", "Xi Su", "Xiang Li", "Xiang Mei", "Xiangyang Ji", "Xiangyu Xi", "Xiangzhou Huang", "Xianpeng Li", "Xiao Fu", "Xiao Liu", "Xiao Wei", "Xiaodong Cai", "Xiaolong Chen", "Xiaoqing Liu", "Xiaotong Li", "Xiaowei Shi", "Xiaoyu Li", "Xili Wang", "Xin Chen", "Xing Hu", "Xingyu Miao", "Xinyan He", "Xuemiao Zhang", "Xueyuan Hao", "Xuezhi Cao", "Xunliang Cai", "Xurui Yang", "Yan Feng", "Yang Bai", "Yang Chen", "Yang Yang", "Yaqi Huo", "Yerui Sun", "Yifan Lu", "Yifan Zhang", "Yipeng Zang", "Yitao Zhai", "Yiyang Li", "Yongjing Yin", "Yongkang Lv", "Yongwei Zhou", "Yu Yang", "Yuchen Xie", "Yueqing Sun", "Yuewen Zheng", "Yuhua Wei", "Yulei Qian", "Yunfan Liang", "Yunfang Tai", "Yunke Zhao", "Zeyang Yu", "Zhao Zhang", "Zhaohua Yang", "Zhenchao Zhang", "Zhikang Xia", "Zhiye Zou", "Zhizhao Zeng", "Zhongda Su", "Zhuofan Chen", "Zijian Zhang", "Ziwen Wang", "Zixu Jiang", "Zizhe Zhao", "Zongyu Wang", "Zunhai Su"], "categories": ["cs.CL", "cs.AI", "cs.DC", "cs.LG"], "comment": null, "summary": "We introduce LongCat-Flash, a 560-billion-parameter Mixture-of-Experts (MoE)\nlanguage model designed for both computational efficiency and advanced agentic\ncapabilities. Stemming from the need for scalable efficiency, LongCat-Flash\nadopts two novel designs: (a) Zero-computation Experts, which enables dynamic\ncomputational budget allocation and activates 18.6B-31.3B (27B on average) per\ntoken depending on contextual demands, optimizing resource usage. (b)\nShortcut-connected MoE, which enlarges the computation-communication overlap\nwindow, demonstrating notable gains in inference efficiency and throughput\ncompared to models of a comparable scale. We develop a comprehensive scaling\nframework for large models that combines hyperparameter transfer, model-growth\ninitialization, a multi-pronged stability suite, and deterministic computation\nto achieve stable and reproducible training. Notably, leveraging the synergy\namong scalable architectural design and infrastructure efforts, we complete\nmodel training on more than 20 trillion tokens within 30 days, while achieving\nover 100 tokens per second (TPS) for inference at a cost of \\$0.70 per million\noutput tokens. To cultivate LongCat-Flash towards agentic intelligence, we\nconduct a large-scale pre-training on optimized mixtures, followed by targeted\nmid- and post-training on reasoning, code, and instructions, with further\naugmentation from synthetic data and tool use tasks. Comprehensive evaluations\ndemonstrate that, as a non-thinking foundation model, LongCat-Flash delivers\nhighly competitive performance among other leading models, with exceptional\nstrengths in agentic tasks. The model checkpoint of LongCat-Flash is\nopen-sourced to foster community research.\n  LongCat Chat: https://longcat.ai\n  Hugging Face: https://huggingface.co/meituan-longcat\n  GitHub: https://github.com/meituan-longcat", "AI": {"tldr": "LongCat-Flash is a 560-billion-parameter Mixture-of-Experts language model that focuses on computational efficiency and agentic capabilities, achieving high performance in inference and task execution.", "motivation": "To provide a scalable and efficient solution for language model training and performance, addressing the increasing demand for computational resources while enhancing agentic intelligence.", "method": "The paper introduces Zero-computation Experts for dynamic resource allocation and Shortcut-connected MoE to improve computation-communication overlap, with robust training techniques combined for model stability.", "result": "LongCat-Flash trains on over 20 trillion tokens in 30 days, achieving over 100 tokens per second inference speed while maintaining low costs per output token, with strong performance on agentic tasks in evaluations against leading models.", "conclusion": "LongCat-Flash represents a significant advancement in the use of large language models for agentic applications, providing open-source resources for further research.", "key_contributions": ["Introduction of Zero-computation Experts for dynamic allocation", "Development of Shortcut-connected MoE for improved efficiency", "Open-sourcing the LongCat-Flash model checkpoint"], "limitations": "", "keywords": ["Mixture-of-Experts", "agentic intelligence", "language model", "computational efficiency"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.01324", "pdf": "https://arxiv.org/pdf/2509.01324.pdf", "abs": "https://arxiv.org/abs/2509.01324", "title": "KoBLEX: Open Legal Question Answering with Multi-hop Reasoning", "authors": ["Jihyung Lee", "Daehui Kim", "Seonjeong Hwang", "Hyounghun Kim", "Gary Lee"], "categories": ["cs.CL"], "comment": "EMNLP 2025 Main Conference", "summary": "Large Language Models (LLM) have achieved remarkable performances in general\ndomains and are now extending into the expert domain of law. Several benchmarks\nhave been proposed to evaluate LLMs' legal capabilities. However, these\nbenchmarks fail to evaluate open-ended and provision-grounded Question\nAnswering (QA). To address this, we introduce a Korean Benchmark for Legal\nEXplainable QA (KoBLEX), designed to evaluate provision-grounded, multi-hop\nlegal reasoning. KoBLEX includes 226 scenario-based QA instances and their\nsupporting provisions, created using a hybrid LLM-human expert pipeline. We\nalso propose a method called Parametric provision-guided Selection Retrieval\n(ParSeR), which uses LLM-generated parametric provisions to guide legally\ngrounded and reliable answers. ParSeR facilitates multi-hop reasoning on\ncomplex legal questions by generating parametric provisions and employing a\nthree-stage sequential retrieval process. Furthermore, to better evaluate the\nlegal fidelity of the generated answers, we propose Legal Fidelity Evaluation\n(LF-Eval). LF-Eval is an automatic metric that jointly considers the question,\nanswer, and supporting provisions and shows a high correlation with human\njudgments. Experimental results show that ParSeR consistently outperforms\nstrong baselines, achieving the best results across multiple LLMs. Notably,\ncompared to standard retrieval with GPT-4o, ParSeR achieves +37.91 higher F1\nand +30.81 higher LF-Eval. Further analyses reveal that ParSeR efficiently\ndelivers consistent performance across reasoning depths, with ablations\nconfirming the effectiveness of ParSeR.", "AI": {"tldr": "The paper introduces KoBLEX, a benchmark for evaluating provision-grounded legal QA, and the ParSeR method that improves multi-hop reasoning in legal questions via LLM-generated provisions.", "motivation": "Existing benchmarks fail to assess open-ended, provision-grounded QA in legal contexts, necessitating a new approach to evaluate LLMs in law.", "method": "The proposed method, ParSeR, employs LLM-generated parametric provisions to guide the retrieval of answers, utilizing a three-stage sequential retrieval process for complex legal reasoning.", "result": "ParSeR achieves significant improvements in legal QA performance, outperforming baselines with +37.91 higher F1 and +30.81 higher LF-Eval scores across multiple LLMs.", "conclusion": "The study demonstrates the effectiveness of ParSeR in delivering consistent performance in legal reasoning and offers a novel metric, LF-Eval, for evaluating answer fidelity.", "key_contributions": ["Introduction of KoBLEX, a benchmark for legal QA evaluation", "Development of ParSeR for improved multi-hop reasoning", "Proposal of LF-Eval as a metric for assessing legal answer fidelity"], "limitations": "", "keywords": ["Legal QA", "Large Language Models", "Multi-hop reasoning", "Benchmarking", "Legal fidelity evaluation"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.01328", "pdf": "https://arxiv.org/pdf/2509.01328.pdf", "abs": "https://arxiv.org/abs/2509.01328", "title": "Can Large Language Models Master Complex Card Games?", "authors": ["Wei Wang", "Fuqing Bie", "Junzhe Chen", "Dan Zhang", "Shiyu Huang", "Evgeny Kharlamov", "Jie Tang"], "categories": ["cs.CL"], "comment": null, "summary": "Complex games have long been an important benchmark for testing the progress\nof artificial intelligence algorithms. AlphaGo, AlphaZero, and MuZero have\ndefeated top human players in Go and Chess, garnering widespread societal\nattention towards artificial intelligence. Concurrently, large language models\n(LLMs) have exhibited remarkable capabilities across various tasks, raising the\nquestion of whether LLMs can achieve similar success in complex games. In this\npaper, we explore the potential of LLMs in mastering complex card games. We\nsystematically assess the learning capabilities of LLMs across eight diverse\ncard games, evaluating the impact of fine-tuning on high-quality gameplay data,\nand examining the models' ability to retain general capabilities while\nmastering these games. Our findings indicate that: (1) LLMs can approach the\nperformance of strong game AIs through supervised fine-tuning on high-quality\ndata, (2) LLMs can master multiple complex card games simultaneously, with\nperformance augmentation for games with similar rules and conflicts for\ndissimilar ones, and (3) LLMs experience a decline in general capabilities when\nmastering complex games, but this decline can be mitigated by integrating a\ncertain amount of general instruction data. The evaluation results demonstrate\nstrong learning ability and versatility of LLMs.", "AI": {"tldr": "This paper explores the potential of large language models (LLMs) to master complex card games by assessing their learning capabilities across eight diverse games.", "motivation": "To investigate whether LLMs can achieve success in complex games similar to AI algorithms like AlphaGo, AlphaZero, and MuZero that excelled in Go and Chess.", "method": "The study systematically evaluates the learning capabilities of LLMs through supervised fine-tuning on high-quality gameplay data across eight card games.", "result": "LLMs can approach strong game AI performance through fine-tuning, master multiple card games simultaneously, and their general capabilities may decline but can be mitigated with general instruction data.", "conclusion": "The findings highlight the strong learning ability and versatility of LLMs in mastering complex games, although caution is needed regarding their general capabilities.", "key_contributions": ["Assessment of LLM performance in complex card games", "Realization of simultaneous mastery of multiple games", "Identification of strategies to mitigate decline in general capabilities"], "limitations": "", "keywords": ["large language models", "complex card games", "fine-tuning", "game AI", "learning capabilities"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.01363", "pdf": "https://arxiv.org/pdf/2509.01363.pdf", "abs": "https://arxiv.org/abs/2509.01363", "title": "Reasoning Vectors: Transferring Chain-of-Thought Capabilities via Task Arithmetic", "authors": ["Mohammad Zbeeb", "Hasan Abed Al Kader Hammoud", "Bernard Ghanem"], "categories": ["cs.CL"], "comment": "Under Review", "summary": "Large language models often require costly optimization, such as\nreinforcement learning, to master complex reasoning tasks. This work\ndemonstrates that reasoning ability, once learned, can be extracted and\ntransferred between models as a compact task vector. We source two publicly\navailable, identically initialized Qwen2.5 models, one fine-tuned with\nsupervised fine-tuning (SFT) and the other with group relative policy\noptimization (GRPO) on the same dataset. From these, we extract a reasoning\nvector: $v_{\\text{reason}} = \\theta_{\\text{GRPO}} - \\theta_{\\text{SFT}}$. We\nhypothesize that this vector captures the reasoning capability instilled by\nreinforcement learning while factoring out shared knowledge from the SFT\nprocess. When added to compatible instruction-tuned models through simple\narithmetic, this vector consistently improves performance across diverse\nreasoning benchmarks: GSM8K (+4.9%), HumanEval (+4.3%), SciQ (+1.7%), and\nBigBenchHard (+12.3% for the 1.5B model). The performance improvements persist\nunder adversarial conditions. Conversely, subtracting the vector causes\nsignificant performance degradation (-11.8% on GSM8K), demonstrating the\nvector's strong contribution to the model's reasoning abilities. This work\nshows how reasoning capabilities, typically developed through expensive\ntraining, can be extracted from existing open-source models and reused through\nsimple tensor arithmetic, offering a practical way to enhance models by\nrecycling prior computational investments.", "AI": {"tldr": "This work shows how reasoning abilities in large language models can be extracted and transferred between models using a compact task vector.", "motivation": "To reduce the computational costs involved in training large language models for complex reasoning tasks.", "method": "Extract reasoning vectors from two Qwen2.5 models, one fine-tuned with SFT and the other with GRPO, and apply these vectors to instruction-tuned models to enhance performance.", "result": "Applying the reasoning vector consistently improved model performance on several reasoning benchmarks, with notable increases in scores across various tests.", "conclusion": "The study demonstrates a practical way to enhance models by reusing reasoning capabilities without expensive retraining, thereby recycling computational investments.", "key_contributions": ["Extraction and transfer of reasoning abilities across models using compact task vectors.", "Demonstrated performance improvements on various reasoning benchmarks through simple arithmetic.", "Highlighted the potential of reusing prior computational investments in model training."], "limitations": "", "keywords": ["Large Language Models", "Reasoning", "Transfer Learning", "Reinforcement Learning"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2509.01379", "pdf": "https://arxiv.org/pdf/2509.01379.pdf", "abs": "https://arxiv.org/abs/2509.01379", "title": "WATCHED: A Web AI Agent Tool for Combating Hate Speech by Expanding Data", "authors": ["Paloma Piot", "Diego Sánchez", "Javier Parapar"], "categories": ["cs.CL"], "comment": null, "summary": "Online harms are a growing problem in digital spaces, putting user safety at\nrisk and reducing trust in social media platforms. One of the most persistent\nforms of harm is hate speech. To address this, we need tools that combine the\nspeed and scale of automated systems with the judgment and insight of human\nmoderators. These tools should not only find harmful content but also explain\ntheir decisions clearly, helping to build trust and understanding. In this\npaper, we present WATCHED, a chatbot designed to support content moderators in\ntackling hate speech. The chatbot is built as an Artificial Intelligence Agent\nsystem that uses Large Language Models along with several specialised tools. It\ncompares new posts with real examples of hate speech and neutral content, uses\na BERT-based classifier to help flag harmful messages, looks up slang and\ninformal language using sources like Urban Dictionary, generates\nchain-of-thought reasoning, and checks platform guidelines to explain and\nsupport its decisions. This combination allows the chatbot not only to detect\nhate speech but to explain why content is considered harmful, grounded in both\nprecedent and policy. Experimental results show that our proposed method\nsurpasses existing state-of-the-art methods, reaching a macro F1 score of 0.91.\nDesigned for moderators, safety teams, and researchers, the tool helps reduce\nonline harms by supporting collaboration between AI and human oversight.", "AI": {"tldr": "WATCHED is a chatbot that assists content moderators in detecting and explaining hate speech using AI and LLMs.", "motivation": "Online harms, particularly hate speech, pose significant risks to user safety and trust in social media, necessitating effective moderation tools.", "method": "WATCHED combines automated systems with human judgment, utilizing LLMs, a BERT-based classifier, slang databases, and platform guidelines to identify and explain hate speech.", "result": "The chatbot achieved a macro F1 score of 0.91 in experimental evaluations, outperforming existing methods.", "conclusion": "WATCHED enhances collaboration between AI and human moderators to effectively reduce online harms.", "key_contributions": ["Development of WATCHED for hate speech moderation", "Integration of LLMs with human insights for content analysis", "Demonstrated superior performance in identifying hate speech"], "limitations": "", "keywords": ["hate speech", "content moderation", "AI chatbot", "human oversight", "Large Language Models"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2509.01387", "pdf": "https://arxiv.org/pdf/2509.01387.pdf", "abs": "https://arxiv.org/abs/2509.01387", "title": "ABCD-LINK: Annotation Bootstrapping for Cross-Document Fine-Grained Links", "authors": ["Serwar Basch", "Ilia Kuznetsov", "Tom Hope", "Iryna Gurevych"], "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": null, "summary": "Understanding fine-grained relations between documents is crucial for many\napplication domains. However, the study of automated assistance is limited by\nthe lack of efficient methods to create training and evaluation datasets of\ncross-document links. To address this, we introduce a new domain-agnostic\nframework for selecting a best-performing approach and annotating\ncross-document links in a new domain from scratch. We first generate and\nvalidate semi-synthetic datasets of interconnected documents. This data is used\nto perform automatic evaluation, producing a shortlist of best-performing\nlinking approaches. These approaches are then used in an extensive human\nevaluation study, yielding performance estimates on natural text pairs. We\napply our framework in two distinct domains -- peer review and news -- and show\nthat combining retrieval models with LLMs achieves 78\\% link approval from\nhuman raters, more than doubling the precision of strong retrievers alone. Our\nframework enables systematic study of cross-document understanding across\napplication scenarios, and the resulting novel datasets lay foundation for\nnumerous cross-document tasks like media framing and peer review. We make the\ncode, data, and annotation protocols openly available.", "AI": {"tldr": "A framework for creating training datasets for cross-document linking, showing significant improvements using LLMs.", "motivation": "To address the limitations in creating efficient training and evaluation datasets for automated cross-document linking methods.", "method": "Generate semi-synthetic datasets of interconnected documents followed by automatic evaluation and human evaluation to identify the best-performing linking approaches.", "result": "Utilizing retrieval models combined with LLMs achieved a 78% link approval rate from human raters, improving the precision of traditional methods.", "conclusion": "The proposed framework facilitates systematic cross-document understanding and contributes novel datasets for various applications.", "key_contributions": ["A domain-agnostic framework for cross-document linking", "Generation of semi-synthetic datasets for evaluation", "High approval rates for LLM-enhanced retrieval models"], "limitations": "", "keywords": ["cross-document linking", "LLMs", "peer review", "news", "datasets"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.01390", "pdf": "https://arxiv.org/pdf/2509.01390.pdf", "abs": "https://arxiv.org/abs/2509.01390", "title": "Analysing the Language of Neural Audio Codecs", "authors": ["Joonyong Park", "Shinnosuke Takamichi", "David M. Chan", "Shunsuke Kando", "Yuki Saito", "Hiroshi Saruwatari"], "categories": ["cs.CL", "eess.AS"], "comment": "In Proceedings of 2025 IEEE Automatic Speech Recognition and\n  Understanding Workshop (ASRU 2025)", "summary": "This study presents a comparative analysis of the statistical and linguistic\nproperties of neural audio codecs (NACs). We investigate discrete speech tokens\nproduced by various NAC models, examining their adherence to linguistic\nstatistical laws such as Zipf's law and Heaps' law, as well as their entropy\nand redundancy. To assess how these token-level properties relate to semantic\nand acoustic preservation in synthesized speech, we evaluate intelligibility\nusing error rates of automatic speech recognition, and quality using the UTMOS\nscore. Our results reveal that NAC tokens, particularly 3-grams, exhibit\nlanguage-like statistical patterns. Moreover, these properties, together with\nmeasures of information content, are found to correlate with improved\nperformances in speech recognition and resynthesis tasks. These findings offer\ninsights into the structure of NAC token sequences and inform the design of\nmore effective generative speech models.", "AI": {"tldr": "This study analyzes the statistical and linguistic properties of neural audio codecs (NACs), focusing on their token-level characteristics and their correlation with speech recognition and quality.", "motivation": "To explore the linguistic properties of discrete speech tokens produced by neural audio codecs and their impact on speech intelligibility and quality.", "method": "A comparative analysis of various NAC models was conducted, examining token adherence to linguistic laws and evaluating intelligibility and quality using automatic speech recognition error rates and UTMOS scores.", "result": "NAC tokens, particularly 3-grams, demonstrated language-like statistical patterns, and the correlation of these properties with speech recognition performance was established.", "conclusion": "The findings provide insights into NAC token structures and can guide the development of better generative speech models.", "key_contributions": ["Demonstration of linguistic adherence in NAC tokens", "Establishment of a correlation between token properties and automatic speech recognition performance", "Insights into design for improved generative speech models"], "limitations": "", "keywords": ["neural audio codecs", "linguistic properties", "speech recognition", "quality assessment", "semantic preservation"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2509.01395", "pdf": "https://arxiv.org/pdf/2509.01395.pdf", "abs": "https://arxiv.org/abs/2509.01395", "title": "LLMs cannot spot math errors, even when allowed to peek into the solution", "authors": ["KV Aditya Srivatsa", "Kaushal Kumar Maurya", "Ekaterina Kochmar"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to EMNLP 2025", "summary": "Large language models (LLMs) demonstrate remarkable performance on math word\nproblems, yet they have been shown to struggle with meta-reasoning tasks such\nas identifying errors in student solutions. In this work, we investigate the\nchallenge of locating the first error step in stepwise solutions using two\nerror reasoning datasets: VtG and PRM800K. Our experiments show that\nstate-of-the-art LLMs struggle to locate the first error step in student\nsolutions even when given access to the reference solution. To that end, we\npropose an approach that generates an intermediate corrected student solution,\naligning more closely with the original student's solution, which helps improve\nperformance.", "AI": {"tldr": "This paper investigates the ability of large language models to identify the first error step in student solutions to math word problems, highlighting their shortcomings and proposing a novel approach to enhance performance.", "motivation": "Although large language models excel at solving math word problems, they struggle with meta-reasoning tasks like identifying errors in student solutions. This study focuses on addressing this limitation.", "method": "We evaluate state-of-the-art LLMs on two error reasoning datasets, VtG and PRM800K, to assess their performance in locating the first error step in student solutions. We propose a method that generates an intermediate corrected version of the student's solution.", "result": "The experiments reveal that LLMs have significant difficulty in pinpointing the first error in student solutions, even when presented with reference solutions.", "conclusion": "Our proposed correction mechanism aligns more closely with original student solutions, which is shown to enhance the LLM's ability to identify errors accurately.", "key_contributions": ["Introduction of two error reasoning datasets: VtG and PRM800K", "Development of a method for generating intermediate corrected solutions to assist LLMs", "Demonstration of the limitations of existing LLMs in error identification tasks"], "limitations": "The proposed method may not generalize to all types of math problems or student solutions.", "keywords": ["large language models", "meta-reasoning", "math word problems", "error identification", "student solutions"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.01412", "pdf": "https://arxiv.org/pdf/2509.01412.pdf", "abs": "https://arxiv.org/abs/2509.01412", "title": "Vis-CoT: A Human-in-the-Loop Framework for Interactive Visualization and Intervention in LLM Chain-of-Thought Reasoning", "authors": ["Kaviraj Pather", "Elena Hadjigeorgiou", "Arben Krasniqi", "Claire Schmit", "Irina Rusu", "Marc Pons", "Kabir Khan"], "categories": ["cs.CL", "68T07, 68T50, 68T05", "I.2.7; I.2.6; I.2.8; H.5.2"], "comment": "12 pages, 7 figures", "summary": "Large language models (LLMs) show strong reasoning via chain-of-thought (CoT)\nprompting, but the process is opaque, which makes verification, debugging, and\ncontrol difficult in high-stakes settings. We present Vis-CoT, a\nhuman-in-the-loop framework that converts linear CoT text into an interactive\nreasoning graph. Users can visualize the logical flow, identify flawed steps,\nand intervene by pruning incorrect paths and grafting new, user-defined\npremises. This shifts interaction from passive observation to active\ncollaboration, steering models toward more accurate and trustworthy\nconclusions. Across GSM8K and StrategyQA, Vis-CoT improves final-answer\naccuracy by up to 24 percentage points over non-interactive baselines. A user\nstudy also shows large gains in perceived usability and trust. Vis-CoT points\nto a practical path for more reliable, understandable, and collaborative\nreasoning by combining LLMs with targeted human oversight.", "AI": {"tldr": "Vis-CoT is a human-in-the-loop framework that creates interactive reasoning graphs from chain-of-thought prompts of LLMs, enabling better accuracy and trust.", "motivation": "The opacity of reasoning in large language models makes verification and debugging challenging in critical applications.", "method": "Vis-CoT converts linear chain-of-thought text into an interactive reasoning graph that allows users to visualize the reasoning process and intervene as needed.", "result": "Vis-CoT improves final-answer accuracy by up to 24 percentage points on GSM8K and StrategyQA benchmarks and enhances user perceptions of usability and trust.", "conclusion": "Vis-CoT facilitates more reliable and collaborative reasoning by combining LLMs with human oversight.", "key_contributions": ["Development of an interactive reasoning graph for LLMs", "Significant improvement in accuracy with user intervention", "Enhanced perceived usability and trust in LLM outputs"], "limitations": "", "keywords": ["human-in-the-loop", "reasoning graph", "chain-of-thought", "large language models", "trust"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2509.01418", "pdf": "https://arxiv.org/pdf/2509.01418.pdf", "abs": "https://arxiv.org/abs/2509.01418", "title": "On the Alignment of Large Language Models with Global Human Opinion", "authors": ["Yang Liu", "Masahiro Kaneko", "Chenhui Chu"], "categories": ["cs.CL"], "comment": "23 pages, 19 figures", "summary": "Today's large language models (LLMs) are capable of supporting multilingual\nscenarios, allowing users to interact with LLMs in their native languages. When\nLLMs respond to subjective questions posed by users, they are expected to align\nwith the views of specific demographic groups or historical periods, shaped by\nthe language in which the user interacts with the model. Existing studies\nmainly focus on researching the opinions represented by LLMs among demographic\ngroups in the United States or a few countries, lacking worldwide country\nsamples and studies on human opinions in different historical periods, as well\nas lacking discussion on using language to steer LLMs. Moreover, they also\noverlook the potential influence of prompt language on the alignment of LLMs'\nopinions. In this study, our goal is to fill these gaps. To this end, we create\nan evaluation framework based on the World Values Survey (WVS) to\nsystematically assess the alignment of LLMs with human opinions across\ndifferent countries, languages, and historical periods around the world. We\nfind that LLMs appropriately or over-align the opinions with only a few\ncountries while under-aligning the opinions with most countries. Furthermore,\nchanging the language of the prompt to match the language used in the\nquestionnaire can effectively steer LLMs to align with the opinions of the\ncorresponding country more effectively than existing steering methods. At the\nsame time, LLMs are more aligned with the opinions of the contemporary\npopulation. To our knowledge, our study is the first comprehensive\ninvestigation of the topic of opinion alignment in LLMs across global,\nlanguage, and temporal dimensions. Our code and data are publicly available at\nhttps://github.com/nlply/global-opinion-alignment.", "AI": {"tldr": "This study investigates the alignment of large language models (LLMs) with human opinions across different countries and historical periods, using a framework based on the World Values Survey.", "motivation": "To address the gaps in existing research regarding the alignment of LLMs with diverse demographic opinions globally, particularly influenced by the language of prompts.", "method": "An evaluation framework was created based on the World Values Survey (WVS) to systematically assess the alignment of LLMs with human opinions across various countries and historical periods.", "result": "It was found that LLMs tend to over-align or align appropriately with opinions from only a few countries, while under-aligning with most others. Additionally, prompt language effectively steers LLMs towards aligning with the opinions of the respective country.", "conclusion": "The study represents the first comprehensive investigation into opinion alignment in LLMs across global, linguistic, and temporal dimensions, revealing significant findings about prompt language influence.", "key_contributions": ["Development of a new evaluation framework for opinion alignment in LLMs", "Comprehensive global analysis of LLM opinion alignment across countries and historical periods", "Demonstration of prompt language's role in steering LLMs' alignment with human opinions"], "limitations": "The study may be limited by the specific languages and countries included in the analysis, as well as the historical periods examined.", "keywords": ["large language models", "opinion alignment", "World Values Survey", "multilingual scenarios", "prompt engineering"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.01455", "pdf": "https://arxiv.org/pdf/2509.01455.pdf", "abs": "https://arxiv.org/abs/2509.01455", "title": "Trusted Uncertainty in Large Language Models: A Unified Framework for Confidence Calibration and Risk-Controlled Refusal", "authors": ["Markus Oehri", "Giulia Conti", "Kaviraj Pather", "Alexandre Rossi", "Laia Serra", "Adrian Parody", "Rogvi Johannesen", "Aviaja Petersen", "Arben Krasniqi"], "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "10 pages, 5 figures", "summary": "Deployed language models must decide not only what to answer but also when\nnot to answer. We present UniCR, a unified framework that turns heterogeneous\nuncertainty evidence including sequence likelihoods, self-consistency\ndispersion, retrieval compatibility, and tool or verifier feedback into a\ncalibrated probability of correctness and then enforces a user-specified error\nbudget via principled refusal. UniCR learns a lightweight calibration head with\ntemperature scaling and proper scoring, supports API-only models through\nblack-box features, and offers distribution-free guarantees using conformal\nrisk control. For long-form generation, we align confidence with semantic\nfidelity by supervising on atomic factuality scores derived from retrieved\nevidence, reducing confident hallucinations while preserving coverage.\nExperiments on short-form QA, code generation with execution tests, and\nretrieval-augmented long-form QA show consistent improvements in calibration\nmetrics, lower area under the risk-coverage curve, and higher coverage at fixed\nrisk compared to entropy or logit thresholds, post-hoc calibrators, and\nend-to-end selective baselines. Analyses reveal that evidence contradiction,\nsemantic dispersion, and tool inconsistency are the dominant drivers of\nabstention, yielding informative user-facing refusal messages. The result is a\nportable recipe of evidence fusion to calibrated probability to risk-controlled\ndecision that improves trustworthiness without fine-tuning the base model and\nremains valid under distribution shift.", "AI": {"tldr": "UniCR is a framework for calibrating language models' responses by managing uncertainty and enforcing user-specified error budgets through principled refusals.", "motivation": "To improve trustworthiness and decision-making in deployed language models by managing uncertainty effectively.", "method": "UniCR integrates various uncertainty metrics such as sequence likelihoods and retrieval compatibility to produce a calibrated probability of correctness. It employs a lightweight calibration mechanism with a focus on semantic fidelity for long-form generation.", "result": "Experiments demonstrated significant enhancements in calibration metrics and risk-coverage trade-offs, outperforming existing methods in several key areas.", "conclusion": "UniCR provides a robust framework for enhancing the reliability of language model outputs without necessitating base model fine-tuning, making it adaptable to various scenarios under distribution shifts.", "key_contributions": ["Development of a unified framework for uncertainty management in language models.", "Introduction of a lightweight calibration head for enhanced response accuracy.", "A method that generates meaningful user-facing refusal messages based on evidence assessment."], "limitations": "", "keywords": ["language models", "uncertainty calibration", "risk-control", "trustworthiness", "refusal messages"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.01468", "pdf": "https://arxiv.org/pdf/2509.01468.pdf", "abs": "https://arxiv.org/abs/2509.01468", "title": "Robust Knowledge Editing via Explicit Reasoning Chains for Distractor-Resilient Multi-Hop QA", "authors": ["Yuchen Wu", "Liang Ding", "Li Shen", "Dacheng Tao"], "categories": ["cs.CL"], "comment": "EMNLP 2025 Findings", "summary": "Large language models (LLMs) encode vast amounts of world knowledge but\nremain static once trained, making the timely integration of emerging facts\nprohibitively expensive via full retraining. Knowledge-editing techniques have\nthus emerged to inject or overwrite specific facts into LLMs, yet they either\nover-rely on superficial cues or incur complex, iterative pipelines that\ncollapse under noisy, multi-hop conditions. We introduce Reason-KE, an\nend-to-end reasoning-chain-based editing framework that steers a pretrained LLM\nthrough four structured stages-fact acknowledgment, relevance determination,\nselective application, and final reasoning-to filter distractors in a single\npass. Trained on MQuAKE-CF with up to four irrelevant facts, Reason-KE elevates\nQwen2.5-7B's multi-hop QA accuracy to 90.2% while suffering merely a 6.3% drop\nunder heavy distraction and <1% when answers are leaked. Our quantitative\nanalysis confirms Reason-KE's resilience and efficiency, establishing a new\nstate-of-the-art for reliable LLM knowledge updates.", "AI": {"tldr": "Reason-KE is an innovative framework for knowledge-editing in large language models that enhances QA accuracy while managing distractions effectively.", "motivation": "LLMs are static post-training, which complicates integration of new knowledge and necessitates effective knowledge-editing techniques.", "method": "Reason-KE employs a structured four-stage process: fact acknowledgment, relevance determination, selective application, and final reasoning to process queries and filter out distractions in an efficient single pass.", "result": "Reason-KE improves Qwen2.5-7B's multi-hop QA accuracy to 90.2%, with a minimal drop of 6.3% under heavy distractions and less than 1% when answers are leaked.", "conclusion": "The quantitative analysis supports Reason-KE's efficiency and resilience, marking it as a new benchmark for reliable updates in LLM knowledge.", "key_contributions": ["Introduction of the Reason-KE framework for knowledge-editing in LLMs.", "Structured editing process improving resilience to irrelevant distractors.", "Achievement of state-of-the-art accuracy in multi-hop QA tasks."], "limitations": "", "keywords": ["Large Language Models", "Knowledge Editing", "Multi-hop Question Answering", "Reasoning Framework"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2509.01476", "pdf": "https://arxiv.org/pdf/2509.01476.pdf", "abs": "https://arxiv.org/abs/2509.01476", "title": "Do Retrieval Augmented Language Models Know When They Don't Know?", "authors": ["Youchao Zhou", "Heyan Huang", "Yicheng Liu", "Rui Dai", "Xinglin Wang", "Xingchen Zhang", "Shumin Shi", "Yang Deng"], "categories": ["cs.CL", "cs.AI"], "comment": "under review", "summary": "Existing Large Language Models (LLMs) occasionally generate plausible yet\nfactually incorrect responses, known as hallucinations. Researchers are\nprimarily using two approaches to mitigate hallucinations, namely Retrieval\nAugmented Language Models (RALMs) and refusal post-training. However, current\nresearch predominantly emphasizes their individual effectiveness while\noverlooking the evaluation of the refusal capability of RALMs. In this study,\nwe ask the fundamental question: Do RALMs know when they don't know?\nSpecifically, we ask three questions. First, are RALMs well-calibrated\nregarding different internal and external knowledge states? We examine the\ninfluence of various factors. Contrary to expectations, we find that LLMs\nexhibit significant \\textbf{over-refusal} behavior. Then, how does refusal\npost-training affect the over-refusal issue? We investigate the Refusal-aware\nInstruction Tuning and In-Context Fine-tuning methods. Our results show that\nthe over-refusal problem is mitigated by In-context fine-tuning. but magnified\nby R-tuning. However, we also find that the refusal ability may conflict with\nthe quality of the answer. Finally, we develop a simple yet effective refusal\nmethod for refusal post-trained models to improve their overall answer quality\nin terms of refusal and correct answers. Our study provides a more\ncomprehensive understanding of the influence of important factors on RALM\nsystems.", "AI": {"tldr": "The paper examines the over-refusal behavior in Retrieval Augmented Language Models and evaluates the impact of refusal post-training methods.", "motivation": "The study addresses the problem of hallucinations in LLMs and investigates if RALMs can accurately assess their knowledge limitations.", "method": "The research analyzes the calibration of RALMs regarding internal and external knowledge states, assesses the impact of refusal post-training, and develops a new refusal method.", "result": "Findings indicate significant over-refusal behavior in LLMs and show that In-context fine-tuning mitigates this issue, while R-tuning exacerbates it.", "conclusion": "The study provides insights into improving refusal capabilities in RALMs without compromising answer quality.", "key_contributions": ["Examines the over-refusal behavior in RALMs", "Evaluates the effectiveness of refusal post-training methods", "Develops a new refusal method for improving answer quality"], "limitations": "", "keywords": ["Retrieval Augmented Language Models", "LLMs", "Refusal Post-training"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.01514", "pdf": "https://arxiv.org/pdf/2509.01514.pdf", "abs": "https://arxiv.org/abs/2509.01514", "title": "MeVe: A Modular System for Memory Verification and Effective Context Control in Language Models", "authors": ["Andreas Ottem"], "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 7 figures, held online presentation at NLPA 2025", "summary": "Retrieval-Augmented Generation (RAG) systems typically face constraints\nbecause of their inherent mechanism: a simple top-k semantic search [1]. The\napproach often leads to the incorporation of irrelevant or redundant\ninformation in the context, degrading performance and efficiency [10][11]. This\npaper presents MeVe, a novel modular architecture intended for Memory\nVerification and smart context composition. MeVe rethinks the RAG paradigm by\nproposing a five-phase modular design that distinctly breaks down the retrieval\nand context composition process into distinct, auditable, and independently\ntunable phases: initial retrieval, relevance verification, fallback retrieval,\ncontext prioritization, and token budgeting. This architecture enables\nfine-grained control of what knowledge is made available to an LLM, enabling\ntask-dependent filtering and adaptation. We release a reference implementation\nof MeVe as a proof of concept and evaluate its performance on knowledge-heavy\nQA tasks over a subset of English Wikipedia [22]. Our results demonstrate that\nby actively verifying information before composition, MeVe significantly\nimproves context efficiency, achieving a 57% reduction on the Wikipedia dataset\nand a 75% reduction on the more complex HotpotQA dataset compared to standard\nRAG implementations [25]. This work provides a framework for more scalable and\nreliable LLM applications. By refining and distilling contextual information,\nMeVe offers a path toward better grounding and more accurate factual support\n[16].", "AI": {"tldr": "MeVe is a modular architecture for improving Retrieval-Augmented Generation (RAG) systems by optimizing the process of information retrieval and context composition.", "motivation": "RAG systems often fail due to irrelevant or redundant information being included, affecting performance and efficiency.", "method": "MeVe employs a five-phase modular design: initial retrieval, relevance verification, fallback retrieval, context prioritization, and token budgeting, providing fine control over knowledge availability to LLMs.", "result": "MeVe demonstrates significant improvements in context efficiency, with a 57% reduction in irrelevant information in a QA task on Wikipedia and a 75% reduction on the more complex HotpotQA dataset compared to standard RAG systems.", "conclusion": "The proposed MeVe architecture enhances the scalability and reliability of LLM applications, promoting better grounding and factual accuracy.", "key_contributions": ["Introduces a five-phase modular design for RAG systems.", "Significantly improves context efficiency and relevance in LLM applications.", "Provides a reference implementation and evaluation against standard RAG systems."], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Memory Verification", "Context Composition"], "importance_score": 9, "read_time_minutes": 16}}
{"id": "2509.01529", "pdf": "https://arxiv.org/pdf/2509.01529.pdf", "abs": "https://arxiv.org/abs/2509.01529", "title": "Service, Solidarity, and Self-Help: A Comparative Topic Modeling Analysis of Community Unionism in the Boot and Shoe Union and Unite Community", "authors": ["Thomas Compton"], "categories": ["cs.CL"], "comment": "10 pages, 7 figures, conference paper", "summary": "This paper presents a comparative analysis of community unionism (CU) in two\ndistinct historical and organizational contexts: the National Boot and Shoe\nUnion (B\\&S) in the 1920s and Unite Community in the 2010s--2020s. Using\nBERTopic for thematic modeling and cTF-IDF weighting, alongside word frequency\nanalysis, the study examines the extent to which each union's discourse aligns\nwith key features of CU -- such as coalition-building, grassroots engagement,\nand action beyond the workplace. The results reveal significant differences in\nthematic focus and discursive coherence. While Unite Community demonstrates\nstronger alignment with outward-facing, social justice-oriented themes, the\nB\\&S corpus emphasizes internal administration, industrial relations, and\nmember services -- reflecting a more traditional, servicing-oriented union\nmodel. The analysis also highlights methodological insights, demonstrating how\nmodern NLP techniques can enhance the study of historical labor archives.\nUltimately, the findings suggest that while both unions engage with\ncommunity-related themes, their underlying models of engagement diverge\nsignificantly, challenging assumptions about the continuity and universality of\ncommunity unionism across time and sector.", "AI": {"tldr": "Comparative analysis of community unionism in historical contexts using NLP techniques reveals significant differences in thematic focus between two unions.", "motivation": "To explore how community unionism has evolved and how modern NLP techniques can enhance the study of historical labor archives.", "method": "Comparative analysis using BERTopic for thematic modeling, cTF-IDF weighting, and word frequency analysis.", "result": "Significant differences were found in thematic focus; Unite Community is more aligned with social justice, whereas the B&S union emphasized traditional union roles.", "conclusion": "The study challenges assumptions about the continuity of community unionism and highlights divergent models of engagement between different eras.", "key_contributions": ["Use of modern NLP techniques in historical analysis", "Insight into thematic differences in union discourse", "Challenging assumptions about community unionism continuity"], "limitations": "", "keywords": ["community unionism", "NLP", "labor studies", "thematic modeling", "historical analysis"], "importance_score": 0, "read_time_minutes": 10}}
{"id": "2509.01535", "pdf": "https://arxiv.org/pdf/2509.01535.pdf", "abs": "https://arxiv.org/abs/2509.01535", "title": "CAT: Causal Attention Tuning For Injecting Fine-grained Causal Knowledge into Large Language Models", "authors": ["Kairong Han", "Wenshuo Zhao", "Ziyu Zhao", "JunJian Ye", "Lujia Pan", "Kun Kuang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to EMNLP2025 Main conference", "summary": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains. However, a fundamental question remains: Can LLMs effectively utilize\ncausal knowledge for prediction and generation? Through empirical studies, we\nfind that LLMs trained directly on large-scale data often capture spurious\ncorrelations rather than true causal relationships, leading to suboptimal\nperformance, especially in out-of-distribution (OOD) scenarios. To address this\nchallenge, we propose Causal Attention Tuning (CAT), a novel approach that\ninjects fine-grained causal knowledge into the attention mechanism. We propose\nan automated pipeline that leverages human priors to automatically generate\ntoken-level causal signals and introduce the Re-Attention mechanism to guide\ntraining, helping the model focus on causal structures while mitigating noise\nand biases in attention scores. Experimental results on our proposed Spurious\nToken Game (STG) benchmark and multiple downstream tasks demonstrate that our\napproach effectively leverages causal knowledge for prediction and remains\nrobust in OOD scenarios. Implementation details can be found at\nhttps://github.com/Kairong-Han/CAT.", "AI": {"tldr": "This paper presents Causal Attention Tuning (CAT), a method to enhance Large Language Models (LLMs) by incorporating causal knowledge into their attention mechanism, improving their performance in various prediction tasks and robustness in out-of-distribution scenarios.", "motivation": "To improve the predictive performance of LLMs by utilizing causal knowledge instead of relying solely on spurious correlations inherent in large-scale data.", "method": "Causal Attention Tuning (CAT) introduces fine-grained causal knowledge into the attention mechanism of LLMs through an automated pipeline that generates token-level causal signals and employs a Re-Attention mechanism to prioritize causal relationships during training.", "result": "Experimental results demonstrate that CAT significantly enhances the ability of LLMs to leverage causal knowledge for prediction while maintaining robustness in out-of-distribution scenarios, as evidenced by evaluations on the Spurious Token Game (STG) benchmark and various downstream tasks.", "conclusion": "Causal Attention Tuning (CAT) presents a promising direction for integrating causal knowledge into LLMs, improving predictive accuracy and generalization in challenging scenarios.", "key_contributions": ["Introduction of Causal Attention Tuning (CAT) method", "Automated pipeline for generating token-level causal signals", "Demonstrated improvement in robustness against out-of-distribution scenarios"], "limitations": "", "keywords": ["Causal Knowledge", "Large Language Models", "Attention Mechanism", "Human Priors", "Out-of-Distribution"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2509.01560", "pdf": "https://arxiv.org/pdf/2509.01560.pdf", "abs": "https://arxiv.org/abs/2509.01560", "title": "In-N-Out: A Parameter-Level API Graph Dataset for Tool Agents", "authors": ["Seungkyu Lee", "Nalim Kim", "Yohan Jo"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Tool agents -- LLM-based systems that interact with external APIs -- offer a\nway to execute real-world tasks. However, as tasks become increasingly complex,\nthese agents struggle to identify and call the correct APIs in the proper\norder. To tackle this problem, we investigate converting API documentation into\na structured API graph that captures API dependencies and leveraging it for\nmulti-tool queries that require compositional API calls. To support this, we\nintroduce In-N-Out, the first expert-annotated dataset of API graphs built from\ntwo real-world API benchmarks and their documentation. Using In-N-Out\nsignificantly improves performance on both tool retrieval and multi-tool query\ngeneration, nearly doubling that of LLMs using documentation alone. Moreover,\ngraphs generated by models fine-tuned on In-N-Out close 90% of this gap,\nshowing that our dataset helps models learn to comprehend API documentation and\nparameter relationships. Our findings highlight the promise of using explicit\nAPI graphs for tool agents and the utility of In-N-Out as a valuable resource.\nWe will release the dataset and code publicly.", "AI": {"tldr": "This paper introduces In-N-Out, an expert-annotated dataset of API graphs that enhances LLM-based tool agents' abilities to handle multi-tool queries and API dependencies, improving performance significantly in task execution.", "motivation": "As LLM-based tool agents tackle increasingly complex tasks, they struggle to identify and call the correct APIs in the necessary order, necessitating a better understanding of API documentation and relationships.", "method": "The paper proposes converting API documentation into a structured API graph that captures API dependencies and facilitates multi-tool queries. It introduces the In-N-Out dataset created from two real-world API benchmarks and their documentation.", "result": "Using the In-N-Out dataset significantly boosts performance on both tool retrieval and multi-tool query generation, with improvements nearly doubling that achieved by LLMs relying solely on documentation. Fine-tuned models on In-N-Out close 90% of the performance gap.", "conclusion": "The results suggest that explicit API graphs can greatly enhance tool agents' abilities, and the In-N-Out dataset serves as a valuable resource for this purpose.", "key_contributions": ["Introduction of the In-N-Out dataset of API graphs", "Demonstration of improved performance in API retrieval and multi-tool query generation", "Highlighting the importance of API graphs for tool agents"], "limitations": "", "keywords": ["tool agents", "API graphs", "In-N-Out dataset", "multi-tool queries", "LLM"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.01564", "pdf": "https://arxiv.org/pdf/2509.01564.pdf", "abs": "https://arxiv.org/abs/2509.01564", "title": "Enhancing Uncertainty Estimation in LLMs with Expectation of Aggregated Internal Belief", "authors": ["Zeguan Xiao", "Diyang Dou", "Boya Xiong", "Yun Chen", "Guanhua Chen"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success across a wide\nrange of natural language tasks, but often exhibit overconfidence and generate\nplausible yet incorrect answers. This overconfidence, especially in models\nundergone Reinforcement Learning from Human Feedback (RLHF), poses significant\nchallenges for reliable uncertainty estimation and safe deployment. In this\npaper, we propose EAGLE (Expectation of AGgregated internaL bEief), a novel\nself-evaluation-based calibration method that leverages the internal hidden\nstates of LLMs to derive more accurate confidence scores. Instead of relying on\nthe model's final output, our approach extracts internal beliefs from multiple\nintermediate layers during self-evaluation. By aggregating these layer-wise\nbeliefs and calculating the expectation over the resulting confidence score\ndistribution, EAGLE produces a refined confidence score that more faithfully\nreflects the model's internal certainty. Extensive experiments on diverse\ndatasets and LLMs demonstrate that EAGLE significantly improves calibration\nperformance over existing baselines. We also provide an in-depth analysis of\nEAGLE, including a layer-wise examination of uncertainty patterns, a study of\nthe impact of self-evaluation prompts, and an analysis of the effect of\nself-evaluation score range.", "AI": {"tldr": "EAGLE is a self-evaluation-based calibration method for Large Language Models (LLMs) that improves confidence score accuracy by leveraging internal hidden states.", "motivation": "To address the challenges of overconfidence in LLMs, especially those fine-tuned with Reinforcement Learning from Human Feedback (RLHF), impacting reliability and safety.", "method": "EAGLE extracts internal beliefs from intermediate layers of LLMs during self-evaluation and aggregates these to compute a refined confidence score based on the expectation of the score distribution.", "result": "EAGLE significantly enhances calibration performance over existing methods across various datasets and LLMs, providing a more accurate reflection of the model's confidence.", "conclusion": "The proposed method not only improves calibration but also offers insights into uncertainty patterns and the effects of self-evaluation in LLMs.", "key_contributions": ["Introduction of EAGLE for improved confidence score calibration", "Layer-wise examination of uncertainty in LLMs", "Analysis of self-evaluation prompt effects on confidence scores"], "limitations": "", "keywords": ["Large Language Models", "calibration", "self-evaluation", "uncertainty", "confidence scores"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.01606", "pdf": "https://arxiv.org/pdf/2509.01606.pdf", "abs": "https://arxiv.org/abs/2509.01606", "title": "Testing the assumptions about the geometry of sentence embedding spaces: the cosine measure need not apply", "authors": ["Vivi Nastase", "Paola Merlo"], "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "25 pages, 6 tables, 10 figures", "summary": "Transformer models learn to encode and decode an input text, and produce\ncontextual token embeddings as a side-effect. The mapping from language into\nthe embedding space maps words expressing similar concepts onto points that are\nclose in the space. In practice, the reverse implication is also assumed: words\ncorresponding to close points in this space are similar or related, those that\nare further are not.\n  Does closeness in the embedding space extend to shared properties for\nsentence embeddings? We present an investigation of sentence embeddings and\nshow that the geometry of their embedding space is not predictive of their\nrelative performances on a variety of tasks.\n  We compute sentence embeddings in three ways: as averaged token embeddings,\nas the embedding of the special [CLS] token, and as the embedding of a random\ntoken from the sentence. We explore whether there is a correlation between the\ndistance between sentence embedding variations and their performance on\nlinguistic tasks, and whether despite their distances, they do encode the same\ninformation in the same manner.\n  The results show that the cosine similarity -- which treats dimensions\nshallowly -- captures (shallow) commonalities or differences between sentence\nembeddings, which are not predictive of their performance on specific tasks.\nLinguistic information is rather encoded in weighted combinations of different\ndimensions, which are not reflected in the geometry of the sentence embedding\nspace.", "AI": {"tldr": "This paper investigates the relationship between the geometry of sentence embeddings and their performance on various linguistic tasks.", "motivation": "To understand whether the closeness in embedding space of sentence embeddings predicts their effectiveness on different linguistic tasks.", "method": "The authors compute sentence embeddings using three different methods: averaged token embeddings, the embedding of the [CLS] token, and a random token, then analyze the correlation between their distances and their performance on tasks.", "result": "The study finds that while cosine similarity captures some commonalities among embeddings, it does not predict their performance on tasks; performance relates more to weighted combinations of embedding dimensions.", "conclusion": "The geometry of sentence embeddings is not a reliable predictor of their effectiveness on linguistic tasks; instead, information is encoded in complex dimensional combinations.", "key_contributions": ["Investigation of various computation methods for sentence embeddings", "Analysis of the relationship between embedding geometry and task performance", "Insight into how linguistic information is encoded in embeddings"], "limitations": "The study mainly focuses on cosine similarity and may not explore other potential metrics for predicting task performance.", "keywords": ["sentence embeddings", "cosine similarity", "linguistic tasks", "embedding space", "machine learning"], "importance_score": 7, "read_time_minutes": 25}}
{"id": "2509.01620", "pdf": "https://arxiv.org/pdf/2509.01620.pdf", "abs": "https://arxiv.org/abs/2509.01620", "title": "Benchmarking the Detection of LLMs-Generated Modern Chinese Poetry", "authors": ["Shanshan Wang", "Junchao Wu", "Fengying Ye", "Jingming Yao", "Lidia S. Chao", "Derek F. Wong"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by EMNLP 2025", "summary": "The rapid development of advanced large language models (LLMs) has made\nAI-generated text indistinguishable from human-written text. Previous work on\ndetecting AI-generated text has made effective progress, but has not involved\nmodern Chinese poetry. Due to the distinctive characteristics of modern Chinese\npoetry, it is difficult to identify whether a poem originated from humans or\nAI. The proliferation of AI-generated modern Chinese poetry has significantly\ndisrupted the poetry ecosystem. Based on the urgency of identifying\nAI-generated poetry in the real Chinese world, this paper proposes a novel\nbenchmark for detecting LLMs-generated modern Chinese poetry. We first\nconstruct a high-quality dataset, which includes both 800 poems written by six\nprofessional poets and 41,600 poems generated by four mainstream LLMs.\nSubsequently, we conduct systematic performance assessments of six detectors on\nthis dataset. Experimental results demonstrate that current detectors cannot be\nused as reliable tools to detect modern Chinese poems generated by LLMs. The\nmost difficult poetic features to detect are intrinsic qualities, especially\nstyle. The detection results verify the effectiveness and necessity of our\nproposed benchmark. Our work lays a foundation for future detection of\nAI-generated poetry.", "AI": {"tldr": "This paper proposes a benchmark for detecting AI-generated modern Chinese poetry, highlighting the challenges in distinguishing it from human-written poetry.", "motivation": "The rise of AI-generated modern Chinese poetry disrupts the poetry ecosystem, necessitating effective detection methods.", "method": "A high-quality dataset of 800 human-written and 41,600 AI-generated poems was constructed, and the performance of six detection systems was systematically assessed on this dataset.", "result": "Current detectors are inadequate for reliably identifying AI-generated modern Chinese poems, with intrinsic poetic qualities, particularly style, proving hardest to detect.", "conclusion": "The work emphasizes the need for effective benchmarks and lays the groundwork for future AI poetry detection research.", "key_contributions": ["Development of a novel benchmark for detecting AI-generated modern Chinese poetry.", "Creation of a large dataset combining human and AI-generated poems.", "Assessment of current detection systems' performance, revealing significant shortcomings."], "limitations": "The study focuses exclusively on modern Chinese poetry and may not generalize to other poetic forms or languages.", "keywords": ["AI-generated poetry", "large language models", "poetry detection", "Chinese poetry", "natural language processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.01640", "pdf": "https://arxiv.org/pdf/2509.01640.pdf", "abs": "https://arxiv.org/abs/2509.01640", "title": "TransGAT: Transformer-Based Graph Neural Networks for Multi-Dimensional Automated Essay Scoring", "authors": ["Hind Aljuaid", "Areej Alhothali", "Ohoud Al-Zamzami", "Hussein Assalahi"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Essay writing is a critical component of student assessment, yet manual\nscoring is labor-intensive and inconsistent. Automated Essay Scoring (AES)\noffers a promising alternative, but current approaches face limitations. Recent\nstudies have incorporated Graph Neural Networks (GNNs) into AES using static\nword embeddings that fail to capture contextual meaning, especially for\npolysemous words. Additionally, many methods rely on holistic scoring,\noverlooking specific writing aspects such as grammar, vocabulary, and cohesion.\nTo address these challenges, this study proposes TransGAT, a novel approach\nthat integrates fine-tuned Transformer models with GNNs for analytic scoring.\nTransGAT combines the contextual understanding of Transformers with the\nrelational modeling strength of Graph Attention Networks (GAT). It performs\ntwo-stream predictions by pairing each fine-tuned Transformer (BERT, RoBERTa,\nand DeBERTaV3) with a separate GAT. In each pair, the first stream generates\nessay-level predictions, while the second applies GAT to Transformer token\nembeddings, with edges constructed from syntactic dependencies. The model then\nfuses predictions from both streams to produce the final analytic score.\nExperiments on the ELLIPSE dataset show that TransGAT outperforms baseline\nmodels, achieving an average Quadratic Weighted Kappa (QWK) of 0.854 across all\nanalytic scoring dimensions. These findings highlight the potential of TransGAT\nto advance AES systems.", "AI": {"tldr": "TransGAT integrates Transformers and GNNs for automated essay scoring, improving prediction accuracy over baseline methods.", "motivation": "Manual scoring of essays is labor-intensive and inconsistent, leading to a need for a more effective automated solution for essay scoring.", "method": "TransGAT combines fine-tuned Transformer models with Graph Attention Networks (GAT) in a two-stream prediction approach. One stream produces essay-level predictions, while the other analyzes token embeddings with contextual relationships from syntactic dependencies.", "result": "TransGAT outperforms existing models, achieving an average Quadratic Weighted Kappa (QWK) of 0.854 across multiple analytic scoring dimensions on the ELLIPSE dataset.", "conclusion": "The study suggests that TransGAT could significantly improve the effectiveness of automated essay scoring systems.", "key_contributions": ["Introduction of TransGAT as a novel AES approach", "Combination of fine-tuned Transformers and GNNs for analytic scoring", "Demonstrated superior performance on the ELLIPSE dataset"], "limitations": "", "keywords": ["Automated Essay Scoring", "Graph Neural Networks", "Transformers", "Natural Language Processing"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2509.01654", "pdf": "https://arxiv.org/pdf/2509.01654.pdf", "abs": "https://arxiv.org/abs/2509.01654", "title": "Parallel Needleman-Wunsch on CUDA to measure word similarity based on phonetic transcriptions", "authors": ["Dominic Plein"], "categories": ["cs.CL"], "comment": "11 pages, 12 figures, accompanied by a YouTube video\n  (https://youtu.be/xbcpnItE3_4) and a GitHub repository\n  (https://github.com/Splines/phonetics-graph/)", "summary": "We present a method to calculate the similarity between words based on their\nphonetic transcription (their pronunciation) using the Needleman-Wunsch\nalgorithm. We implement this algorithm in Rust and parallelize it on both CPU\nand GPU to handle large datasets efficiently. The GPU implementation leverages\nCUDA and the cudarc Rust library to achieve significant performance\nimprovements. We validate our approach by constructing a fully-connected graph\nwhere nodes represent words and edges have weights according to the similarity\nbetween the words. This graph is then analyzed using clustering algorithms to\nidentify groups of phonetically similar words. Our results demonstrate the\nfeasibility and effectiveness of the proposed method in analyzing the phonetic\nstructure of languages. It might be easily expanded to other languages.", "AI": {"tldr": "A new method for calculating phonetic similarity between words using the Needleman-Wunsch algorithm, implemented in Rust with parallel processing capabilities for large datasets.", "motivation": "To explore and analyze the phonetic structure of languages through phonetic similarity of words.", "method": "The method employs the Needleman-Wunsch algorithm for phonetic transcription similarity, implemented in Rust and parallelized using CPU and GPU (CUDA).", "result": "The GPU implementation significantly improves performance, enabling efficient handling of large datasets; a fully-connected graph is created from the results, facilitating clustering of phonetically similar words.", "conclusion": "The proposed method is feasible and effective for analyzing phonetic structures and can be adapted for different languages.", "key_contributions": ["Introduction of a phonetic similarity calculation method using the Needleman-Wunsch algorithm", "Implementation in Rust with CPU and GPU parallel processing", "Construction and analysis of a fully-connected graph for clustering similar words."], "limitations": "", "keywords": ["phonetic similarity", "Needleman-Wunsch", "Rust", "CUDA", "clustering"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2509.01660", "pdf": "https://arxiv.org/pdf/2509.01660.pdf", "abs": "https://arxiv.org/abs/2509.01660", "title": "Bridging Thoughts and Words: Graph-Based Intent-Semantic Joint Learning for Fake News Detection", "authors": ["Zhengjia Wang", "Qiang Sheng", "Danding Wang", "Beizhe Hu", "Juan Cao"], "categories": ["cs.CL"], "comment": "Accepted to CIKM'25", "summary": "Fake news detection is an important and challenging task for defending online\ninformation integrity. Existing state-of-the-art approaches typically extract\nnews semantic clues, such as writing patterns that include emotional words,\nstylistic features, etc. However, detectors tuned solely to such semantic clues\ncan easily fall into surface detection patterns, which can shift rapidly in\ndynamic environments, leading to limited performance in the evolving news\nlandscape. To address this issue, this paper investigates a novel perspective\nby incorporating news intent into fake news detection, bridging intents and\nsemantics together. The core insight is that by considering news intents, one\ncan deeply understand the inherent thoughts behind news deception, rather than\nthe surface patterns within words alone. To achieve this goal, we propose\nGraph-based Intent-Semantic Joint Modeling (InSide) for fake news detection,\nwhich models deception clues from both semantic and intent signals via\ngraph-based joint learning. Specifically, InSide reformulates news semantic and\nintent signals into heterogeneous graph structures, enabling long-range context\ninteraction through entity guidance and capturing both holistic and\nimplementation-level intent via coarse-to-fine intent modeling. To achieve\nbetter alignment between semantics and intents, we further develop a dynamic\npathway-based graph alignment strategy for effective message passing and\naggregation across these signals by establishing a common space. Extensive\nexperiments on four benchmark datasets demonstrate the superiority of the\nproposed InSide compared to state-of-the-art methods.", "AI": {"tldr": "This paper presents a novel approach for fake news detection by incorporating news intent alongside semantic clues, using a graph-based model to enhance detection accuracy.", "motivation": "The reliance on semantic clues alone in fake news detection can lead to poor performance as patterns shift. This research aims to integrate news intent to better understand the underlying motives behind news deception.", "method": "The proposed method, Graph-based Intent-Semantic Joint Modeling (InSide), models deception cues using heterogeneous graph structures to facilitate the interaction between semantic and intent signals.", "result": "Extensive experiments show that InSide outperforms existing state-of-the-art fake news detection methods across four benchmark datasets.", "conclusion": "Incorporating news intent significantly enhances the detection of fake news by addressing the limitations of traditional semantic-only approaches.", "key_contributions": ["Introduction of a graph-based model for integrating news intent and semantics.", "Development of a dynamic pathway-based graph alignment strategy for improved message passing.", "Demonstration of superior performance over state-of-the-art methods in fake news detection."], "limitations": "", "keywords": ["fake news detection", "intent modeling", "graph-based learning", "semantic analysis", "machine learning"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.01772", "pdf": "https://arxiv.org/pdf/2509.01772.pdf", "abs": "https://arxiv.org/abs/2509.01772", "title": "chDzDT: Word-level morphology-aware language model for Algerian social media text", "authors": ["Abdelkrime Aries"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": null, "summary": "Pre-trained language models (PLMs) have substantially advanced natural\nlanguage processing by providing context-sensitive text representations.\nHowever, the Algerian dialect remains under-represented, with few dedicated\nmodels available. Processing this dialect is challenging due to its complex\nmorphology, frequent code-switching, multiple scripts, and strong lexical\ninfluences from other languages. These characteristics complicate tokenization\nand reduce the effectiveness of conventional word- or subword-level approaches.\n  To address this gap, we introduce chDzDT, a character-level pre-trained\nlanguage model tailored for Algerian morphology. Unlike conventional PLMs that\nrely on token sequences, chDzDT is trained on isolated words. This design\nallows the model to encode morphological patterns robustly, without depending\non token boundaries or standardized orthography. The training corpus draws from\ndiverse sources, including YouTube comments, French, English, and Berber\nWikipedia, as well as the Tatoeba project. It covers multiple scripts and\nlinguistic varieties, resulting in a substantial pre-training workload.\n  Our contributions are threefold: (i) a detailed morphological analysis of\nAlgerian dialect using YouTube comments; (ii) the construction of a\nmultilingual Algerian lexicon dataset; and (iii) the development and extensive\nevaluation of a character-level PLM as a morphology-focused encoder for\ndownstream tasks. The proposed approach demonstrates the potential of\ncharacter-level modeling for morphologically rich, low-resource dialects and\nlays a foundation for more inclusive and adaptable NLP systems.", "AI": {"tldr": "Introduction of chDzDT, a character-level pre-trained language model for the Algerian dialect, addressing challenges in morphology and code-switching that conventional models face.", "motivation": "The Algerian dialect is under-represented in NLP, hindering effective processing due to its complex characteristics.", "method": "Developed a character-level PLM trained on isolated words, using diverse multilingual sources including social media and Wikipedia.", "result": "The model performs robustly in encoding morphological patterns and shows effectiveness in downstream NLP tasks despite the dialect's complexities.", "conclusion": "chDzDT sets a precedent for character-level modeling in low-resource dialects, enhancing NLP adaptability.", "key_contributions": ["Morphological analysis of Algerian dialect via YouTube comments.", "Construction of a multilingual Algerian lexicon dataset.", "Development and evaluation of a character-level PLM for morphology-focused tasks."], "limitations": "", "keywords": ["pre-trained language models", "Algerian dialect", "character-level modeling", "morphological analysis", "low-resource NLP"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.01790", "pdf": "https://arxiv.org/pdf/2509.01790.pdf", "abs": "https://arxiv.org/abs/2509.01790", "title": "Flaw or Artifact? Rethinking Prompt Sensitivity in Evaluating LLMs", "authors": ["Andong Hua", "Kenan Tang", "Chenhe Gu", "Jindong Gu", "Eric Wong", "Yao Qin"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "Prompt sensitivity, referring to the phenomenon where paraphrasing (i.e.,\nrepeating something written or spoken using different words) leads to\nsignificant changes in large language model (LLM) performance, has been widely\naccepted as a core limitation of LLMs. In this work, we revisit this issue and\nask: Is the widely reported high prompt sensitivity truly an inherent weakness\nof LLMs, or is it largely an artifact of evaluation processes? To answer this\nquestion, we systematically evaluate 7 LLMs (e.g., GPT and Gemini family)\nacross 6 benchmarks, including both multiple-choice and open-ended tasks on 12\ndiverse prompt templates. We find that much of the prompt sensitivity stems\nfrom heuristic evaluation methods, including log-likelihood scoring and rigid\nanswer matching, which often overlook semantically correct responses expressed\nthrough alternative phrasings, such as synonyms or paraphrases. When we adopt\nLLM-as-a-Judge evaluations, we observe a substantial reduction in performance\nvariance and a consistently higher correlation in model rankings across\nprompts. Our findings suggest that modern LLMs are more robust to prompt\ntemplates than previously believed, and that prompt sensitivity may be more an\nartifact of evaluation than a flaw in the models.", "AI": {"tldr": "This paper investigates the phenomenon of prompt sensitivity in large language models (LLMs), challenging the notion that it is an inherent weakness and suggesting it may result from current evaluation methods.", "motivation": "To determine whether the reported high prompt sensitivity in LLMs is a fundamental issue or a byproduct of evaluation processes.", "method": "The authors evaluated 7 LLMs across 6 benchmarks using 12 diverse prompt templates, employing both traditional and LLM-as-a-Judge evaluation methods.", "result": "The research indicates that prompt sensitivity arises largely from heuristics in evaluation methods that fail to account for semantically correct alternative responses, revealing that LLMs exhibit more robustness to prompt variations than previously thought.", "conclusion": "The findings imply that the prompt sensitivity observed in many studies may be an artifact of evaluation techniques rather than an intrinsic flaw in the LLMs.", "key_contributions": ["Challenges the view of inherent prompt sensitivity in LLMs.", "Demonstrates the influence of evaluation methods on perceived performance variance.", "Suggests LLM-as-a-Judge evaluations improve robustness assessments of models."], "limitations": "", "keywords": ["large language models", "prompt sensitivity", "evaluation methods", "LLM-as-a-Judge", "semantics"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.01814", "pdf": "https://arxiv.org/pdf/2509.01814.pdf", "abs": "https://arxiv.org/abs/2509.01814", "title": "Mic Drop or Data Flop? Evaluating the Fitness for Purpose of AI Voice Interviewers for Data Collection within Quantitative & Qualitative Research Contexts", "authors": ["Shreyas Tirumala", "Nishant Jain", "Danny D. Leybzon", "Trent D. Buskirk"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Transformer-based Large Language Models (LLMs) have paved the way for \"AI\ninterviewers\" that can administer voice-based surveys with respondents in\nreal-time. This position paper reviews emerging evidence to understand when\nsuch AI interviewing systems are fit for purpose for collecting data within\nquantitative and qualitative research contexts. We evaluate the capabilities of\nAI interviewers as well as current Interactive Voice Response (IVR) systems\nacross two dimensions: input/output performance (i.e., speech recognition,\nanswer recording, emotion handling) and verbal reasoning (i.e., ability to\nprobe, clarify, and handle branching logic). Field studies suggest that AI\ninterviewers already exceed IVR capabilities for both quantitative and\nqualitative data collection, but real-time transcription error rates, limited\nemotion detection abilities, and uneven follow-up quality indicate that the\nutility, use and adoption of current AI interviewer technology may be\ncontext-dependent for qualitative data collection efforts.", "AI": {"tldr": "This position paper reviews AI interviewers' effectiveness in real-time voice-based surveys compared to traditional IVR systems for research data collection.", "motivation": "To evaluate when AI interviewing systems are suitable for collecting data in both quantitative and qualitative research contexts.", "method": "The paper assesses AI interviewers and IVR systems by their performance in speech recognition, answer recording, emotion handling, and verbal reasoning capabilities through field studies.", "result": "Field studies show that AI interviewers surpass IVR systems in performance for both quantitative and qualitative data collection.", "conclusion": "Despite the advantages, current AI interviewer technology has some limitations, indicating that their utility may depend on specific contexts for qualitative data collection.", "key_contributions": ["Comparison of AI interviewers and IVR systems", "Assessment of performance across multiple dimensions", "Context-dependent utility for qualitative research"], "limitations": "Real-time transcription errors, limited emotion detection, and inconsistent follow-up quality.", "keywords": ["AI interviewers", "IVR systems", "data collection", "speech recognition", "qualitative research"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.01885", "pdf": "https://arxiv.org/pdf/2509.01885.pdf", "abs": "https://arxiv.org/abs/2509.01885", "title": "Extracting OPQRST in Electronic Health Records using Large Language Models with Reasoning", "authors": ["Zhimeng Luo", "Abhibha Gupta", "Adam Frisch", "Daqing He"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The extraction of critical patient information from Electronic Health Records\n(EHRs) poses significant challenges due to the complexity and unstructured\nnature of the data. Traditional machine learning approaches often fail to\ncapture pertinent details efficiently, making it difficult for clinicians to\nutilize these tools effectively in patient care. This paper introduces a novel\napproach to extracting the OPQRST assessment from EHRs by leveraging the\ncapabilities of Large Language Models (LLMs). We propose to reframe the task\nfrom sequence labeling to text generation, enabling the models to provide\nreasoning steps that mimic a physician's cognitive processes. This approach\nenhances interpretability and adapts to the limited availability of labeled\ndata in healthcare settings. Furthermore, we address the challenge of\nevaluating the accuracy of machine-generated text in clinical contexts by\nproposing a modification to traditional Named Entity Recognition (NER) metrics.\nThis includes the integration of semantic similarity measures, such as the BERT\nScore, to assess the alignment between generated text and the clinical intent\nof the original records. Our contributions demonstrate a significant\nadvancement in the use of AI in healthcare, offering a scalable solution that\nimproves the accuracy and usability of information extraction from EHRs,\nthereby aiding clinicians in making more informed decisions and enhancing\npatient care outcomes.", "AI": {"tldr": "This paper presents a novel approach to extracting OPQRST assessments from Electronic Health Records using Large Language Models, transforming the task from sequence labeling to text generation to enhance interpretability and accuracy.", "motivation": "The complexity and unstructured nature of EHR data hinder effective extraction of critical patient information, leading to challenges in clinician decision-making.", "method": "The authors propose using Large Language Models (LLMs) to reframe the information extraction task from sequence labeling to text generation, incorporating reasoning steps that reflect a clinician's cognitive processes, and modifying NER metrics to include semantic similarity measures.", "result": "The proposed method demonstrates improved accuracy and usability in extracting information from EHRs, making it easier for clinicians to utilize these tools and improve patient care outcomes.", "conclusion": "By leveraging LLMs for EHR data extraction and enhancing evaluation metrics, this approach offers a scalable solution that aligns with clinician needs, ultimately supporting better patient care.", "key_contributions": ["Introduction of LLM-based text generation for EHR information extraction", "Enhanced interpretability through mimicry of physician reasoning", "Modification of NER metrics with semantic similarity measures"], "limitations": "", "keywords": ["Electronic Health Records", "Large Language Models", "Text Generation", "Information Extraction", "Healthcare AI"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2509.01899", "pdf": "https://arxiv.org/pdf/2509.01899.pdf", "abs": "https://arxiv.org/abs/2509.01899", "title": "Weakly Supervised Medical Entity Extraction and Linking for Chief Complaints", "authors": ["Zhimeng Luo", "Zhendong Wang", "Rui Meng", "Diyang Xue", "Adam Frisch", "Daqing He"], "categories": ["cs.CL"], "comment": null, "summary": "A Chief complaint (CC) is the reason for the medical visit as stated in the\npatient's own words. It helps medical professionals to quickly understand a\npatient's situation, and also serves as a short summary for medical text\nmining. However, chief complaint records often take a variety of entering\nmethods, resulting in a wide variation of medical notations, which makes it\ndifficult to standardize across different medical institutions for record\nkeeping or text mining. In this study, we propose a weakly supervised method to\nautomatically extract and link entities in chief complaints in the absence of\nhuman annotation. We first adopt a split-and-match algorithm to produce weak\nannotations, including entity mention spans and class labels, on 1.2 million\nreal-world de-identified and IRB approved chief complaint records. Then we\ntrain a BERT-based model with generated weak labels to locate entity mentions\nin chief complaint text and link them to a pre-defined ontology. We conducted\nextensive experiments, and the results showed that our Weakly Supervised Entity\nExtraction and Linking (\\ours) method produced superior performance over\nprevious methods without any human annotation.", "AI": {"tldr": "A weakly supervised method to extract and link entities in chief complaints from medical records without human annotation is proposed, demonstrating improved performance over previous approaches.", "motivation": "To address the challenges of varied notations in chief complaint records which hinder standardization for record keeping and text mining across medical institutions.", "method": "The study employs a split-and-match algorithm to create weak annotations from 1.2 million de-identified chief complaint records, followed by training a BERT-based model to locate entity mentions and link them to a predefined ontology.", "result": "The proposed method achieved superior performance compared to prior approaches that required human annotations, demonstrating effectiveness in extracting and linking medical entities.", "conclusion": "The Weakly Supervised Entity Extraction and Linking method successfully enhances the process of handling chief complaint records by eliminating the need for human annotations while improving accuracy.", "key_contributions": ["Introduction of a weakly supervised method for entity extraction", "Utilization of a large dataset (1.2 million records) for training", "Demonstration of improved performance over existing methods without human annotation."], "limitations": "", "keywords": ["weakly supervised learning", "entity extraction", "chief complaints", "medical text mining", "BERT"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.01962", "pdf": "https://arxiv.org/pdf/2509.01962.pdf", "abs": "https://arxiv.org/abs/2509.01962", "title": "DRAssist: Dispute Resolution Assistance using Large Language Models", "authors": ["Sachin Pawar", "Manoj Apte", "Girish K. Palshikar", "Basit Ali", "Nitin Ramrakhiyani"], "categories": ["cs.CL"], "comment": "Accepted at the 20th International Conference on Artificial\n  Intelligence and Law (ICAIL 2025)", "summary": "Disputes between two parties occur in almost all domains such as taxation,\ninsurance, banking, healthcare, etc. The disputes are generally resolved in a\nspecific forum (e.g., consumer court) where facts are presented, points of\ndisagreement are discussed, arguments as well as specific demands of the\nparties are heard, and finally a human judge resolves the dispute by often\nfavouring one of the two parties. In this paper, we explore the use of large\nlanguage models (LLMs) as assistants for the human judge to resolve such\ndisputes, as part of our DRAssist system. We focus on disputes from two\nspecific domains -- automobile insurance and domain name disputes. DRAssist\nidentifies certain key structural elements (e.g., facts, aspects or\ndisagreement, arguments) of the disputes and summarizes the unstructured\ndispute descriptions to produce a structured summary for each dispute. We then\nexplore multiple prompting strategies with multiple LLMs for their ability to\nassist in resolving the disputes in these domains. In DRAssist, these LLMs are\nprompted to produce the resolution output at three different levels -- (i)\nidentifying an overall stronger party in a dispute, (ii) decide whether each\nspecific demand of each contesting party can be accepted or not, (iii) evaluate\nwhether each argument by each contesting party is strong or weak. We evaluate\nthe performance of LLMs on all these tasks by comparing them with relevant\nbaselines using suitable evaluation metrics.", "AI": {"tldr": "This paper presents DRAssist, a system that utilizes large language models (LLMs) to assist human judges in resolving disputes from automobile insurance and domain name sectors by summarizing unstructured data and evaluating party arguments.", "motivation": "The aim is to improve the efficiency and accuracy of dispute resolution processes by leveraging advanced LLMs to assist judges in legal contexts.", "method": "DRAssist identifies structural elements of disputes, prompts LLMs to provide structured resolutions at multiple levels, and evaluates performance against baseline measures.", "result": "The system was tested with several prompting strategies for LLMs, and the evaluations showed promising results in identifying stronger parties and evaluating arguments.", "conclusion": "DRAssist demonstrates the potential of LLMs in legal resolution settings but requires further investigation for refinement.", "key_contributions": ["Introduction of DRAssist system for dispute resolution", "Application of LLMs in legal contexts", "Evaluation of various prompting strategies for optimal LLM outputs"], "limitations": "The paper may not address broader legal implications of using AI in dispute resolution.", "keywords": ["dispute resolution", "large language models", "legal technology", "DRAssist", "automobile insurance"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.02033", "pdf": "https://arxiv.org/pdf/2509.02033.pdf", "abs": "https://arxiv.org/abs/2509.02033", "title": "StructCoh: Structured Contrastive Learning for Context-Aware Text Semantic Matching", "authors": ["Chao Xue", "Ziyuan Gao"], "categories": ["cs.CL"], "comment": "Accepted by PRICAI 2025", "summary": "Text semantic matching requires nuanced understanding of both structural\nrelationships and fine-grained semantic distinctions. While pre-trained\nlanguage models excel at capturing token-level interactions, they often\noverlook hierarchical structural patterns and struggle with subtle semantic\ndiscrimination. In this paper, we proposed StructCoh, a graph-enhanced\ncontrastive learning framework that synergistically combines structural\nreasoning with representation space optimization. Our approach features two key\ninnovations: (1) A dual-graph encoder constructs semantic graphs via dependency\nparsing and topic modeling, then employs graph isomorphism networks to\npropagate structural features across syntactic dependencies and cross-document\nconcept nodes. (2) A hierarchical contrastive objective enforces consistency at\nmultiple granularities: node-level contrastive regularization preserves core\nsemantic units, while graph-aware contrastive learning aligns inter-document\nstructural semantics through both explicit and implicit negative sampling\nstrategies. Experiments on three legal document matching benchmarks and\nacademic plagiarism detection datasets demonstrate significant improvements\nover state-of-the-art methods. Notably, StructCoh achieves 86.7% F1-score\n(+6.2% absolute gain) on legal statute matching by effectively identifying\nargument structure similarities.", "AI": {"tldr": "This paper presents StructCoh, a graph-enhanced contrastive learning framework for text semantic matching that improves semantic distinctions and structural reasoning.", "motivation": "Text semantic matching often struggles with understanding structural relationships and fine-grained semantic distinctions in documents, particularly in legal contexts.", "method": "The proposed StructCoh uses a dual-graph encoder to create semantic graphs and applies graph isomorphism networks for feature propagation. It employs a hierarchical contrastive objective with both node-level and graph-aware contrastive learning.", "result": "StructCoh demonstrates significant improvements over state-of-the-art methods, achieving an F1-score of 86.7% in legal statute matching, marking a 6.2% gain.", "conclusion": "The StructCoh framework successfully enhances the capturing of structural and semantic nuances in text matching tasks, particularly in legal document analysis.", "key_contributions": ["Introduction of a dual-graph encoder for semantic graph construction", "Implementation of graph isomorphism networks for structural feature propagation", "Development of a hierarchical contrastive learning objective that enhances semantic matching at various levels"], "limitations": "", "keywords": ["Semantic Matching", "Graph Neural Networks", "Contrastive Learning"], "importance_score": 4, "read_time_minutes": 7}}
{"id": "2509.02036", "pdf": "https://arxiv.org/pdf/2509.02036.pdf", "abs": "https://arxiv.org/abs/2509.02036", "title": "DeepSeek performs better than other Large Language Models in Dental Cases", "authors": ["Hexian Zhang", "Xinyu Yan", "Yanqi Yang", "Lijian Jin", "Ping Yang", "Junwen Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "Abstract word count: 171; Total word count: 3130; Total number of\n  tables: 2; Total number of figures: 3; Number of references: 32", "summary": "Large language models (LLMs) hold transformative potential in healthcare, yet\ntheir capacity to interpret longitudinal patient narratives remains\ninadequately explored. Dentistry, with its rich repository of structured\nclinical data, presents a unique opportunity to rigorously assess LLMs'\nreasoning abilities. While several commercial LLMs already exist, DeepSeek, a\nmodel that gained significant attention earlier this year, has also joined the\ncompetition. This study evaluated four state-of-the-art LLMs (GPT-4o, Gemini\n2.0 Flash, Copilot, and DeepSeek V3) on their ability to analyze longitudinal\ndental case vignettes through open-ended clinical tasks. Using 34 standardized\nlongitudinal periodontal cases (comprising 258 question-answer pairs), we\nassessed model performance via automated metrics and blinded evaluations by\nlicensed dentists. DeepSeek emerged as the top performer, demonstrating\nsuperior faithfulness (median score = 0.528 vs. 0.367-0.457) and higher expert\nratings (median = 4.5/5 vs. 4.0/5), without significantly compromising\nreadability. Our study positions DeepSeek as the leading LLM for case analysis,\nendorses its integration as an adjunct tool in both medical education and\nresearch, and highlights its potential as a domain-specific agent.", "AI": {"tldr": "This study evaluates four large language models (LLMs) for their ability to analyze longitudinal dental case narratives, with DeepSeek emerging as the top performer.", "motivation": "To explore the capabilities of large language models in interpreting longitudinal patient narratives in dentistry and to assess their reasoning abilities in clinical tasks.", "method": "The study evaluated four state-of-the-art LLMs (GPT-4o, Gemini 2.0 Flash, Copilot, and DeepSeek V3) using 34 standardized longitudinal periodontal cases, comprising 258 question-answer pairs, assessed through automated metrics and blinded evaluations by licensed dentists.", "result": "DeepSeek outperformed other models with a superior faithfulness score and higher expert ratings, demonstrating its effectiveness in analyzing dental case vignettes.", "conclusion": "DeepSeek is positioned as the leading LLM for dental case analysis, recommending its integration into medical education and research as a supportive tool.", "key_contributions": ["Evaluation of LLMs in a specific healthcare domain (dentistry)", "Identification of DeepSeek as the top performer among evaluated LLMs", "Recommendations for integrating LLMs in medical education and research"], "limitations": "", "keywords": ["Large Language Models", "Health Informatics", "Dentistry", "Clinical Reasoning", "AI in Healthcare"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.02038", "pdf": "https://arxiv.org/pdf/2509.02038.pdf", "abs": "https://arxiv.org/abs/2509.02038", "title": "NADI 2025: The First Multidialectal Arabic Speech Processing Shared Task", "authors": ["Bashar Talafha", "Hawau Olamide Toyin", "Peter Sullivan", "AbdelRahim Elmadany", "Abdurrahman Juma", "Amirbek Djanibekov", "Chiyu Zhang", "Hamad Alshehhi", "Hanan Aldarmaki", "Mustafa Jarrar", "Nizar Habash", "Muhammad Abdul-Mageed"], "categories": ["cs.CL", "cs.SD"], "comment": null, "summary": "We present the findings of the sixth Nuanced Arabic Dialect Identification\n(NADI 2025) Shared Task, which focused on Arabic speech dialect processing\nacross three subtasks: spoken dialect identification (Subtask 1), speech\nrecognition (Subtask 2), and diacritic restoration for spoken dialects (Subtask\n3). A total of 44 teams registered, and during the testing phase, 100 valid\nsubmissions were received from eight unique teams. The distribution was as\nfollows: 34 submissions for Subtask 1 \"five teams{\\ae}, 47 submissions for\nSubtask 2 \"six teams\", and 19 submissions for Subtask 3 \"two teams\". The\nbest-performing systems achieved 79.8% accuracy on Subtask 1, 35.68/12.20\nWER/CER (overall average) on Subtask 2, and 55/13 WER/CER on Subtask 3. These\nresults highlight the ongoing challenges of Arabic dialect speech processing,\nparticularly in dialect identification, recognition, and diacritic restoration.\nWe also summarize the methods adopted by participating teams and briefly\noutline directions for future editions of NADI.", "AI": {"tldr": "This paper presents the results of the NADI 2025 Shared Task on Arabic speech dialect processing, detailing submissions across three subtasks and highlighting challenges in the field.", "motivation": "To advance research in Arabic dialect processing by evaluating various methods through a competitive shared task.", "method": "The study involved 44 teams competing across three subtasks: dialect identification, speech recognition, and diacritic restoration, with performance evaluated based on accuracy and error rates.", "result": "The best systems achieved 79.8% accuracy in dialect identification, with varying WER/CER results across speech recognition and diacritic restoration subtasks.", "conclusion": "The results reveal significant challenges in Arabic dialect processing, indicating the need for further improvements and future task iterations.", "key_contributions": ["Presentation of benchmark results for Arabic dialect processing", "Analysis of diverse methodologies applied by competing teams", "Identification of key challenges in dialect recognition and restoration"], "limitations": "The study primarily focuses on Arabic dialects, which may not be applicable to other language dialects or languages.", "keywords": ["Arabic speech processing", "dialect identification", "speech recognition", "diacritic restoration", "NADI"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2509.02040", "pdf": "https://arxiv.org/pdf/2509.02040.pdf", "abs": "https://arxiv.org/abs/2509.02040", "title": "Attributes as Textual Genes: Leveraging LLMs as Genetic Algorithm Simulators for Conditional Synthetic Data Generation", "authors": ["Guangzeng Han", "Weisi Liu", "Xiaolei Huang"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP2025 Findings", "summary": "Large Language Models (LLMs) excel at generating synthetic data, but ensuring\nits quality and diversity remains challenging. We propose Genetic Prompt, a\nnovel framework that combines genetic algorithms with LLMs to augment synthetic\ndata generation. Our approach treats semantic text attributes as gene sequences\nand leverages the LLM to simulate crossover and mutation operations. This\ngenetic process enhances data quality and diversity by creating novel attribute\ncombinations, yielding synthetic distributions closer to real-world data. To\noptimize parent selection, we also integrate an active learning scheme that\nexpands the offspring search space. Our experiments on multiple NLP tasks\nreveal several key findings: Genetic Prompt not only significantly outperforms\nstate-of-the-art baselines but also shows robust performance across various\ngenerator model sizes and scales. Moreover, we demonstrate that fusing our\nsynthetic data with the original training set significantly boosts downstream\nmodel performance, particularly for class-imbalanced scenarios. Our findings\nvalidate that Genetic Prompt is an effective method for producing high-quality\nsynthetic data for a wide range of NLP applications.", "AI": {"tldr": "Genetic Prompt uses genetic algorithms with LLMs to improve synthetic data generation quality and diversity for NLP tasks.", "motivation": "To enhance the quality and diversity of synthetic data generated by Large Language Models (LLMs).", "method": "The framework treats semantic text attributes as gene sequences and employs genetic algorithms to perform crossover and mutation operations, supplemented by an active learning scheme for parent selection.", "result": "Genetic Prompt significantly outperforms state-of-the-art baselines across various NLP tasks and shows robust performance with different generator model sizes. Additionally, combining synthetic data with original training datasets enhances model performance, especially in class-imbalanced scenarios.", "conclusion": "Genetic Prompt is validated as an effective method for generating high-quality synthetic data applicable to various NLP tasks.", "key_contributions": ["Introduction of Genetic Prompt framework combining genetic algorithms with LLMs", "Demonstrated improvement in synthetic data quality and diversity", "Effective in boosting downstream model performance, especially for class-imbalanced data."], "limitations": "", "keywords": ["Large Language Models", "Synthetic Data Generation", "Genetic Algorithms", "NLP", "Active Learning"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2509.02075", "pdf": "https://arxiv.org/pdf/2509.02075.pdf", "abs": "https://arxiv.org/abs/2509.02075", "title": "How Instruction-Tuning Imparts Length Control: A Cross-Lingual Mechanistic Analysis", "authors": ["Elisabetta Rocchetti", "Alfio Ferrara"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": null, "summary": "Adhering to explicit length constraints, such as generating text with a\nprecise word count, remains a significant challenge for Large Language Models\n(LLMs). This study aims at investigating the differences between foundation\nmodels and their instruction-tuned counterparts, on length-controlled text\ngeneration in English and Italian. We analyze both performance and internal\ncomponent contributions using Cumulative Weighted Attribution, a metric derived\nfrom Direct Logit Attribution. Our findings reveal that instruction-tuning\nsubstantially improves length control, primarily by specializing components in\ndeeper model layers. Specifically, attention heads in later layers of IT models\nshow increasingly positive contributions, particularly in English. In Italian,\nwhile attention contributions are more attenuated, final-layer MLPs exhibit a\nstronger positive role, suggesting a compensatory mechanism. These results\nindicate that instruction-tuning reconfigures later layers for task adherence,\nwith component-level strategies potentially adapting to linguistic context.", "AI": {"tldr": "This study investigates the impact of instruction-tuning on length-controlled text generation in LLMs, revealing it enhances performance, particularly in later model layers.", "motivation": "The challenge of generating text that adheres to explicit length constraints in LLMs.", "method": "We analyze length-controlled text generation performance in LLMs using Cumulative Weighted Attribution in both English and Italian.", "result": "Instruction-tuning improves length control significantly, with positive contributions from attention heads in deeper layers for English, while final-layer MLPs play a crucial role in Italian.", "conclusion": "Instruction-tuning reconfigures model layers to enhance task adherence, with adjustments based on linguistic context.", "key_contributions": ["Demonstrates the effectiveness of instruction-tuning in length control for LLMs.", "Compares performance in English and Italian, highlighting linguistic differences.", "Introduces Cumulative Weighted Attribution for analyzing model contributions."], "limitations": "The study primarily focuses on two languages, which may limit the generalizability of the findings.", "keywords": ["Large Language Models", "Instruction-tuning", "Length control", "Cumulative Weighted Attribution", "Text generation"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.02093", "pdf": "https://arxiv.org/pdf/2509.02093.pdf", "abs": "https://arxiv.org/abs/2509.02093", "title": "Better by Comparison: Retrieval-Augmented Contrastive Reasoning for Automatic Prompt Optimization", "authors": ["Juhyeon Lee", "Wonduk Seo", "Hyunjin An", "Seunghyun Lee", "Yi Bu"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Preprint", "summary": "Automatic prompt optimization has recently emerged as a strategy for\nimproving the quality of prompts used in Large Language Models (LLMs), with the\ngoal of generating more accurate and useful responses. However, most prior work\nfocuses on direct prompt refinement or model fine-tuning, overlooking the\npotential of leveraging LLMs' inherent reasoning capability to learn from\ncontrasting examples. In this paper, we present Contrastive Reasoning Prompt\nOptimization (CRPO), a novel framework that formulates prompt optimization as a\nretrieval augmented reasoning process. Our approach retrieves top k reference\nprompts from the HelpSteer2 dataset, an open-source collection annotated for\nhelpfulness, correctness, coherence, complexity, and verbosity, and constructs\ntwo complementary optimization paradigms: (1) tiered contrastive reasoning,\nwhere the LLM compares high, medium, and low quality prompts to refine its own\ngeneration through reflective reasoning, and (2) multi-metric contrastive\nreasoning, where the LLM analyzes the best prompts along each evaluation\ndimension and integrates their strengths into an optimized prompt. By\nexplicitly contrasting high and low quality exemplars, CRPO enables the model\nto deduce why certain prompts succeed while others fail, thereby achieving more\nrobust and interpretable optimization. Experimental results on the HelpSteer2\nbenchmark demonstrate that CRPO significantly outperforms baselines. Our\nfindings highlight the promise of contrastive, retrieval-augmented reasoning\nfor advancing automatic prompt optimization.", "AI": {"tldr": "This paper introduces Contrastive Reasoning Prompt Optimization (CRPO), which utilizes LLMs' reasoning capabilities to optimize prompts by comparing high and low quality examples.", "motivation": "To improve the quality of prompts for Large Language Models (LLMs) by leveraging their inherent reasoning capability through contrasting examples.", "method": "The CRPO framework retrieves reference prompts from the HelpSteer2 dataset and employs two paradigms: tiered contrastive reasoning and multi-metric contrastive reasoning to refine prompt generation.", "result": "CRPO significantly outperforms existing baseline methods on the HelpSteer2 benchmark for prompt optimization.", "conclusion": "The contrastive, retrieval-augmented reasoning approach promises advancements in automatic prompt optimization for LLMs, leading to more robust and interpretable results.", "key_contributions": ["Introduction of CRPO for prompt optimization", "Utilization of contrasting examples for learning", "Demonstrated significant performance improvements over baseline methods"], "limitations": "", "keywords": ["prompt optimization", "contrastive reasoning", "large language models", "NLP"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.02097", "pdf": "https://arxiv.org/pdf/2509.02097.pdf", "abs": "https://arxiv.org/abs/2509.02097", "title": "JudgeAgent: Dynamically Evaluate LLMs with Agent-as-Interviewer", "authors": ["Zhichao Shi", "Xuhui Jiang", "Chengjin Xu", "Cangli Yao", "Zhenxin Huang", "Shengjie Ma", "Yinghan Shen", "Yuanzhuo Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Evaluating the capabilities of large language models (LLMs) is an essential\nstep to ensure the successful application of LLMs across various domains. The\ncurrent evaluation of LLMs is based on a paradigm that involves querying them\nwith predefined question sets and assessing their outputs. This paradigm offers\ncontrollable processes and simplicity, but faces challenges such as limited\ninteraction with targets, insufficient difficulty control, and difficulties in\nverifying the validity of evaluation results, making it hard to precisely\ndetermine the knowledge and capability boundaries of target models. To address\nthese challenges, we propose JudgeAgent, a knowledge-target adaptive dynamic\nevaluation framework based on a new interviewer-style evaluation paradigm.\nJudgeAgent employs a comprehensive evaluation approach consisting of benchmark\ngrading, interactive extension, and evaluation feedback. It utilizes\nknowledge-driven data synthesis and target-adaptive difficulty adjustment\nmethods to conduct extended testing, providing accurate and effective\nevaluation results. We also introduce a novel insight into validating\nevaluation methods, demonstrating the effectiveness of JudgeAgent and its\ndynamic evaluation paradigm through extensive experiments.", "AI": {"tldr": "JudgeAgent is a dynamic evaluation framework for large language models (LLMs) that addresses key challenges in LLM evaluation methods by employing an interviewer-style evaluation paradigm.", "motivation": "To improve the evaluation of LLMs, which faces issues like limited interaction, difficulty control, and verification of results.", "method": "JudgeAgent utilizes a knowledge-target adaptive approach with benchmark grading, interactive testing, and feedback mechanisms to enhance evaluation.", "result": "The framework provides more accurate and effective evaluation results through extended testing with adaptive difficulty adjustment.", "conclusion": "JudgeAgent demonstrates a novel and effective evaluation methodology that surpasses traditional evaluation paradigms through extensive experiments.", "key_contributions": ["Introduction of an interviewer-style evaluation paradigm for LLMs", "Comprehensive evaluation framework combining various assessment methods", "Demonstration of enhanced accuracy and effectiveness in evaluation results"], "limitations": "", "keywords": ["large language models", "evaluation framework", "knowledge-target adaptive", "dynamic evaluation", "interactive testing"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.02123", "pdf": "https://arxiv.org/pdf/2509.02123.pdf", "abs": "https://arxiv.org/abs/2509.02123", "title": "CMRAG: Co-modality-based document retrieval and visual question answering", "authors": ["Wang Chen", "Guanqiang Qi", "Weikang Li", "Yang Li"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has become a core paradigm in document\nquestion answering tasks. However, existing methods have limitations when\ndealing with multimodal documents: one category of methods relies on layout\nanalysis and text extraction, which can only utilize explicit text information\nand struggle to capture images or unstructured content; the other category\ntreats document segmentation as visual input and directly passes it to visual\nlanguage models (VLMs) for processing, yet it ignores the semantic advantages\nof text, leading to suboptimal generation results. This paper proposes\nco-modality-based RAG (CMRAG), which can simultaneously leverage text and\nimages for efficient retrieval and generation. Specifically, we first perform\nstructured parsing on documents to obtain co-modality representations of text\nsegments and image regions. Subsequently, in response to user queries, we\nretrieve candidate evidence from text and image channels, respectively, and\naggregate the results at the cross-modal retrieval level. Finally, we prompt\nthe VLM to generate the final response based on the co-modality retrieval\nresults. Experiments demonstrate that our method significantly outperforms\npure-vision-based RAG in visual document question answering tasks. The findings\nof this paper show that integrating co-modality information into the RAG\nframework in a unified manner is an effective approach to improving the\nperformance of complex document visual question-answering (VQA) systems.", "AI": {"tldr": "This paper introduces Co-modality-based Retrieval-Augmented Generation (CMRAG) to improve question answering on multimodal documents by effectively integrating text and image information.", "motivation": "Existing retrieval-augmented generation methods struggle with multimodal documents, limiting the effective use of both text and images in question answering tasks.", "method": "The proposed CMRAG first performs structured parsing to obtain co-modality representations and utilizes cross-modal retrieval to combine text and image evidence when responding to user queries.", "result": "Experiments indicate that CMRAG significantly outperforms traditional pure-vision-based retrieval-augmented generation methods in visual document question answering tasks.", "conclusion": "Integrating co-modality information into the RAG framework improves the performance and effectiveness of complex document visual question-answering systems.", "key_contributions": ["Introduction of co-modality representations for improved document processing.", "Development of a unified framework for text and image retrieval.", "Significant performance enhancements in visual question-answering tasks."], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Co-modality", "Visual Question Answering"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.02133", "pdf": "https://arxiv.org/pdf/2509.02133.pdf", "abs": "https://arxiv.org/abs/2509.02133", "title": "AMBEDKAR-A Multi-level Bias Elimination through a Decoding Approach with Knowledge Augmentation for Robust Constitutional Alignment of Language Models", "authors": ["Snehasis Mukhopadhyay", "Aryan Kasat", "Shivam Dubey", "Rahul Karthikeyan", "Dhruv Sood", "Vinija Jain", "Aman Chadha", "Amitava Das"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) can inadvertently reflect societal biases\npresent in their training data, leading to harmful or prejudiced outputs. In\nthe Indian context, our empirical evaluations across a suite of models reveal\nthat biases around caste and religion are particularly salient. Yet, most\nexisting mitigation strategies are Western-centric and fail to address these\nlocal nuances. We propose AMBEDKAR, a framework inspired by the egalitarian\nvision of Dr B. R. Ambedkar, architect of the Indian Constitution, to guide LLM\noutputs toward fairness, neutrality, and inclusion in line with Articles 14 to\n17. Our approach introduces a Constitution-Aware Decoding Layer, guided by the\nAI Constitution of India and applied only at inference time, without any\nparameter updates to the base model. We incorporate a speculative decoding\nalgorithm that proactively reduces casteist and communal bias during\ngeneration. This mitigation layer operates directly within the decoding\nprocess, avoiding changes to model internals and lowering the computational and\ninfrastructural costs associated with retraining. We reinterpret speculative\ndecoding not merely as an efficiency tool but as a mechanism for fairness. In\nthis framework, a Small Language Model (SLM) acts as a potentially biased\ngenerator, while a constitutionally guided Large Language Model (LLM) serves as\nthe verifier. Rather than accelerating generation, the LLM enforces bias-robust\ntrajectories in the SLM outputs. This inversion of roles gives rise to a\nfairness-by-speculation paradigm. Our approach yields an absolute reduction of\nbias up to 26.41 percent compared to baseline. Our source code, datasets, and\nresults are available at https://anonymous.4open.science/r/AMBEDKAR-983B/", "AI": {"tldr": "This paper proposes AMBEDKAR, a fairness framework for Large Language Models (LLMs) in the Indian context, addressing sociocultural biases related to caste and religion with a novel Constitution-Aware Decoding Layer.", "motivation": "To mitigate harmful societal biases present in LLMs, especially regarding caste and religion in India, where existing strategies are often Western-centric.", "method": "The AMBEDKAR framework introduces a Constitution-Aware Decoding Layer that operates during inference time to reduce biases, using a speculative decoding algorithm without changing the model internally.", "result": "The approach resulted in an absolute reduction of bias by up to 26.41% compared to baseline models.", "conclusion": "The fairness-by-speculation paradigm allows for enhanced LLM outputs that respect the principles of the AI Constitution of India without the need for retraining on biased data.", "key_contributions": ["Introduced AMBEDKAR for mitigating caste and communal bias in LLMs.", "Developed Constitution-Aware Decoding Layer for real-time fairness adjustments.", "Proposed fairness-by-speculation paradigm enhancing bias robustness without model internals alteration."], "limitations": "", "keywords": ["Large Language Models", "caste bias", "communal bias", "fairness", "India"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2509.02160", "pdf": "https://arxiv.org/pdf/2509.02160.pdf", "abs": "https://arxiv.org/abs/2509.02160", "title": "Meta-Pretraining for Zero-Shot Cross-Lingual Named Entity Recognition in Low-Resource Philippine Languages", "authors": ["David Demitri Africa", "Suchir Salhan", "Yuval Weiss", "Paula Buttery", "Richard Diehl Martinez"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Named-entity recognition (NER) in low-resource languages is usually tackled\nby finetuning very large multilingual LMs, an option that is often infeasible\nin memory- or latency-constrained settings. We ask whether small decoder LMs\ncan be pretrained so that they adapt quickly and transfer zero-shot to\nlanguages unseen during pretraining. To this end we replace part of the\nautoregressive objective with first-order model-agnostic meta-learning (MAML).\nTagalog and Cebuano are typologically similar yet structurally different in\ntheir actor/non-actor voice systems, and hence serve as a challenging test-bed.\nAcross four model sizes (11 M - 570 M) MAML lifts zero-shot micro-F1 by 2-6 pp\nunder head-only tuning and 1-3 pp after full tuning, while cutting convergence\ntime by up to 8%. Gains are largest for single-token person entities that\nco-occur with Tagalog case particles si/ni, highlighting the importance of\nsurface anchors.", "AI": {"tldr": "The paper explores the effectiveness of small decoder language models for zero-shot named-entity recognition (NER) in low-resource languages, utilizing model-agnostic meta-learning (MAML) to enhance adaptation.", "motivation": "Named-entity recognition (NER) in low-resource languages often relies on large multilingual language models, which may be impractical due to resource constraints. This research investigates the potential of smaller models and novel training techniques for effective zero-shot adaptation to new languages.", "method": "The study replaces part of the autoregressive objective of small decoder language models with first-order model-agnostic meta-learning (MAML) to improve their performance in NER tasks for low-resource languages.", "result": "MAML improves zero-shot micro-F1 scores by 2-6 percentage points during head-only tuning and by 1-3 percentage points after full tuning. Additionally, it reduces convergence time by up to 8%, with the most significant gains seen in the recognition of single-token person entities in Tagalog.", "conclusion": "Utilizing MAML allows smaller language models to perform more effectively in zero-shot settings for NER tasks, suggesting a viable alternative to larger models in low-resource environments.", "key_contributions": ["Introduction of MAML for small decoder LMs to enhance zero-shot NER", "Demonstrated effectiveness in Tagalog and Cebuano, highlighting the model's adaptability", "Reduction in convergence time for model training."], "limitations": "The approach may still be limited by the inherent challenges of low-resource language training data and model generalization.", "keywords": ["named-entity recognition", "low-resource languages", "meta-learning", "zero-shot", "multilingual", "language models"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.02170", "pdf": "https://arxiv.org/pdf/2509.02170.pdf", "abs": "https://arxiv.org/abs/2509.02170", "title": "Avoidance Decoding for Diverse Multi-Branch Story Generation", "authors": ["Kyeongman Park", "Nakyeong Yang", "Kyomin Jung"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) often generate repetitive and monotonous\noutputs, especially in tasks like story generation, due to limited creative\ndiversity when given the same input prompt. To address this challenge, we\npropose a novel decoding strategy, Avoidance Decoding, that modifies token\nlogits by penalizing similarity to previously generated outputs, thereby\nencouraging more diverse multi-branch stories. This penalty adaptively balances\ntwo similarity measures: (1) Concept-level Similarity Penalty, which is\nprioritized in early stages to diversify initial story concepts, and (2)\nNarrative-level Similarity Penalty, which is increasingly emphasized later to\nensure natural yet diverse plot development. Notably, our method achieves up to\n2.6 times higher output diversity and reduces repetition by an average of 30%\ncompared to strong baselines, while effectively mitigating text degeneration.\nFurthermore, we reveal that our method activates a broader range of neurons,\ndemonstrating that it leverages the model's intrinsic creativity.", "AI": {"tldr": "This paper introduces Avoidance Decoding, a novel strategy that enhances output diversity in Large Language Models by penalizing similarity to prior generated texts, achieving significantly reduced repetition and improved creativity.", "motivation": "Large Language Models tend to produce repetitive outputs, especially in creative tasks like story generation, limiting their effectiveness in generating diverse narratives.", "method": "The proposed Avoidance Decoding modifies token logits by applying penalties based on two measures of similarity—concept-level in early stages and narrative-level as the story progresses—to encourage diversity in generated outputs.", "result": "The method achieves up to 2.6 times higher output diversity and a 30% reduction in repetition compared to baseline models, demonstrating a substantial improvement in the creative diversity of generated stories.", "conclusion": "Avoidance Decoding not only diversifies outputs but also enhances the model's use of its intrinsic creativity by activating a wider range of neurons during generation.", "key_contributions": ["Introduction of Avoidance Decoding for LLMs", "Improvement of output diversity by up to 2.6 times", "Reduction of redundancy in story generation by 30%"], "limitations": "", "keywords": ["Large Language Models", "Avoidance Decoding", "story generation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.02198", "pdf": "https://arxiv.org/pdf/2509.02198.pdf", "abs": "https://arxiv.org/abs/2509.02198", "title": "FActBench: A Benchmark for Fine-grained Automatic Evaluation of LLM-Generated Text in the Medical Domain", "authors": ["Anum Afzal", "Juraj Vladika", "Florian Matthes"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models tend to struggle when dealing with specialized domains.\nWhile all aspects of evaluation hold importance, factuality is the most\ncritical one. Similarly, reliable fact-checking tools and data sources are\nessential for hallucination mitigation. We address these issues by providing a\ncomprehensive Fact-checking Benchmark FActBench covering four generation tasks\nand six state-of-the-art Large Language Models (LLMs) for the Medical domain.\nWe use two state-of-the-art Fact-checking techniques: Chain-of-Thought (CoT)\nPrompting and Natural Language Inference (NLI). Our experiments show that the\nfact-checking scores acquired through the Unanimous Voting of both techniques\ncorrelate best with Domain Expert Evaluation.", "AI": {"tldr": "The paper presents a comprehensive benchmark for evaluating LLMs in the medical domain, focusing on factuality and fact-checking.", "motivation": "To address the challenges that Large Language Models face in specialized domains, particularly the medical field, where factual accuracy is crucial.", "method": "The paper introduces FActBench, a fact-checking benchmark that evaluates LLMs on four generation tasks using Chain-of-Thought (CoT) Prompting and Natural Language Inference (NLI) techniques.", "result": "Experiments demonstrate that the fact-checking scores derived from Unanimous Voting of both CoT and NLI techniques align closely with evaluations by domain experts.", "conclusion": "The findings emphasize the importance of reliable fact-checking methods for enhancing the performance of LLMs in specialized fields, particularly health-related domains.", "key_contributions": ["Introduction of FActBench for evaluating LLMs in the medical domain", "Application of two state-of-the-art fact-checking techniques (CoT and NLI)", "Demonstration of correlation between automated fact-checking scores and expert evaluations."], "limitations": "", "keywords": ["Large Language Models", "Fact-checking", "Medical domain", "FActBench", "NLI"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.02225", "pdf": "https://arxiv.org/pdf/2509.02225.pdf", "abs": "https://arxiv.org/abs/2509.02225", "title": "Towards Fundamental Language Models: Does Linguistic Competence Scale with Model Size?", "authors": ["Jaime Collado-Montañez", "L. Alfonso Ureña-López", "Arturo Montejo-Ráez"], "categories": ["cs.CL", "I.2.7; I.7"], "comment": "13 pages, 2 figures", "summary": "Large Language Models offer impressive language capabilities but suffer from\nwell-known limitations, including hallucinations, biases, privacy concerns, and\nhigh computational costs. These issues are largely driven by the combination of\nlinguistic competence and factual memorization within a single monolithic\nmodel. This paper introduces and empirically supports the Fundamental Language\nModel (FLM) paradigm, which advocates for smaller, linguistically competent\nmodels that offload factual retrieval to external tools. We evaluate models\nranging from 135M to 32B parameters across three dimensions: linguistic\ncompetence, external factual knowledge, and internal factual knowledge. Our\nfindings reveal that while both linguistic competence and factual knowledge\nimprove with scale, internal factual knowledge grows significantly faster,\nsuggesting that model size is more closely tied to memorization than to core\nlanguage ability. These results support a modular approach to language\nmodeling, where compact, linguistically proficient models serve as the\nfoundation for tool-augmented systems. The FLM paradigm offers a path toward\nmore efficient, interpretable, and sustainable NLP solutions.", "AI": {"tldr": "This paper introduces the Fundamental Language Model (FLM) paradigm, advocating for smaller, linguistically competent models that leverage external factual retrieval tools to address limitations of large language models.", "motivation": "To address the limitations of large language models, including hallucinations, biases, privacy concerns, and high computational costs, by proposing a modular approach to language modeling.", "method": "The paper empirically evaluates models ranging from 135M to 32B parameters, focusing on their linguistic competence and factual knowledge (both internal and external).", "result": "The study finds that while both linguistic competence and factual knowledge improve with scale, internal factual knowledge scales significantly faster than linguistic competence, indicating that larger models are more tied to memorization.", "conclusion": "The FLM paradigm promotes a shift toward compact, linguistically proficient models that enhance efficiency and interpretable NLP solutions by integrating external factual retrieval.", "key_contributions": ["Introduction of the Fundamental Language Model (FLM) paradigm", "Empirical evaluation of model performance across different parameter sizes", "Support for a modular approach in language model architecture"], "limitations": "", "keywords": ["Fundamental Language Model", "modular language modeling", "linguistic competence", "factual retrieval", "NLP efficiency"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.02292", "pdf": "https://arxiv.org/pdf/2509.02292.pdf", "abs": "https://arxiv.org/abs/2509.02292", "title": "LLMs and their Limited Theory of Mind: Evaluating Mental State Annotations in Situated Dialogue", "authors": ["Katharine Kowalyshyn", "Matthias Scheutz"], "categories": ["cs.CL"], "comment": null, "summary": "What if large language models could not only infer human mindsets but also\nexpose every blind spot in team dialogue such as discrepancies in the team\nmembers' joint understanding? We present a novel, two-step framework that\nleverages large language models (LLMs) both as human-style annotators of team\ndialogues to track the team's shared mental models (SMMs) and as automated\ndiscrepancy detectors among individuals' mental states. In the first step, an\nLLM generates annotations by identifying SMM elements within task-oriented\ndialogues from the Cooperative Remote Search Task (CReST) corpus. Then, a\nsecondary LLM compares these LLM-derived annotations and human annotations\nagainst gold-standard labels to detect and characterize divergences. We define\nan SMM coherence evaluation framework for this use case and apply it to six\nCReST dialogues, ultimately producing: (1) a dataset of human and LLM\nannotations; (2) a reproducible evaluation framework for SMM coherence; and (3)\nan empirical assessment of LLM-based discrepancy detection. Our results reveal\nthat, although LLMs exhibit apparent coherence on straightforward\nnatural-language annotation tasks, they systematically err in scenarios\nrequiring spatial reasoning or disambiguation of prosodic cues.", "AI": {"tldr": "This paper presents a two-step framework utilizing large language models to annotate team dialogues and detect discrepancies in shared mental models among team members.", "motivation": "To explore the capabilities of large language models in understanding human mindsets and identifying discrepancies in team dialogue.", "method": "The framework involves LLMs generating annotations from task-oriented dialogues and a secondary LLM comparing these annotations to gold-standard labels to detect divergences.", "result": "A dataset of human and LLM annotations, a reproducible evaluation framework for assessing shared mental model coherence, and an empirical evaluation of LLM-based discrepancy detection were produced.", "conclusion": "LLMs show coherence in straightforward annotation tasks but struggle with spatial reasoning and disambiguating prosodic cues.", "key_contributions": ["Development of a framework for using LLMs in team dialogue analysis", "Creation of a dataset with human and LLM annotations", "Establishment of an evaluation framework for SMM coherence"], "limitations": "LLMs struggle with tasks requiring spatial reasoning and prosodic cue interpretation.", "keywords": ["large language models", "team dialogues", "shared mental models", "discrepancy detection", "evaluation framework"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.02333", "pdf": "https://arxiv.org/pdf/2509.02333.pdf", "abs": "https://arxiv.org/abs/2509.02333", "title": "DCPO: Dynamic Clipping Policy Optimization", "authors": ["Shihui Yang", "Chengfeng Dou", "Peidong Guo", "Kai Lu", "Qiang Ju", "Fei Deng", "Rihui Xin"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a\npromising framework for enhancing the reasoning capabilities of large language\nmodels. However, existing approaches such as GRPO often suffer from zero\ngradients. This problem arises primarily due to fixed clipping bounds for\ntoken-level probability ratios and the standardization of identical rewards,\nwhich can lead to ineffective gradient updates and underutilization of\ngenerated responses. In this work, we propose Dynamic Clipping Policy\nOptimization (DCPO), which introduces a dynamic clipping strategy that\nadaptively adjusts the clipping bounds based on token-specific prior\nprobabilities to enhance token-level exploration, and a smooth advantage\nstandardization technique that standardizes rewards across cumulative training\nsteps to improve the response-level effective utilization of generated\nresponses. DCPO achieved state-of-the-art performance on four benchmarks based\non four different models. In particular, DCPO achieved an Avg@1 of 46.7 under\ngreedy decoding and an Avg@32 of 38.8 under 32 times sampling on the AIME24\nbenchmark, surpassing both DAPO (36.7/31.6) and GRPO (36.7/32.1) on the\nQwen2.5-Math-7B model. On the AIME25 benchmark based on Qwen2.5-14B, DCPO\nachieves a performance of (23.3/19.0), surpassing GRPO (13.3/10.5) and DAPO\n(20.0/15.3). Furthermore, DCPO achieved an average 28% improvement in the\nnonzero advantage over GRPO in four models, doubled the training efficiency\nover DAPO, and significantly reduced the token clipping ratio by an order of\nmagnitude compared to both GRPO and DAPO, while achieving superior performance.\nThese results highlight DCPO's effectiveness in leveraging generated data more\nefficiently for reinforcement learning in large language models.", "AI": {"tldr": "The paper introduces Dynamic Clipping Policy Optimization (DCPO) to enhance reinforcement learning from verifiable rewards in large language models, overcoming issues of zero gradients and inefficient gradient updates.", "motivation": "Existing reinforcement learning methods like GRPO face challenges with zero gradients and ineffective updates due to clipping bounds and reward standardization, necessitating improvements for better model performance.", "method": "Proposed a dynamic clipping strategy that adjusts clipping bounds based on token-specific prior probabilities and a smooth advantage standardization technique for more effective reinforcement learning in language models.", "result": "DCPO achieved state-of-the-art results on four benchmarks with significant performance improvements over GRPO and DAPO, such as an Avg@1 of 46.7 on the AIME24 benchmark and a 28% increase in nonzero advantages.", "conclusion": "Dynamic Clipping Policy Optimization demonstrates superior efficiency and effectiveness in applying reinforcement learning to large language models, ultimately leveraging generated data better than traditional methods.", "key_contributions": ["Introduction of a dynamic clipping strategy for token-specific prior probabilities", "Development of a smooth advantage standardization technique", "Achieved state-of-the-art performance surpassing existing methods on several benchmarks"], "limitations": "", "keywords": ["Reinforcement Learning", "Large Language Models", "Dynamic Clipping", "Token Exploration", "Reward Standardization"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2509.02350", "pdf": "https://arxiv.org/pdf/2509.02350.pdf", "abs": "https://arxiv.org/abs/2509.02350", "title": "Implicit Reasoning in Large Language Models: A Comprehensive Survey", "authors": ["Jindong Li", "Yali Fu", "Li Fan", "Jiahong Liu", "Yao Shu", "Chengwei Qin", "Menglin Yang", "Irwin King", "Rex Ying"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated strong generalization across a\nwide range of tasks. Reasoning with LLMs is central to solving multi-step\nproblems and complex decision-making. To support efficient reasoning, recent\nstudies have shifted attention from explicit chain-of-thought prompting toward\nimplicit reasoning, where reasoning occurs silently via latent structures\nwithout emitting intermediate textual steps. Implicit reasoning brings\nadvantages such as lower generation cost, faster inference, and better\nalignment with internal computation. Although prior surveys have discussed\nlatent representations in the context of reasoning, a dedicated and\nmechanism-level examination of how reasoning unfolds internally within LLMs\nremains absent. This survey fills that gap by introducing a taxonomy centered\non execution paradigms, shifting the focus from representational forms to\ncomputational strategies. We organize existing methods into three execution\nparadigms based on \\textbf{\\textit{how and where internal computation\nunfolds}}: latent optimization, signal-guided control, and layer-recurrent\nexecution. We also review structural, behavioral and representation-based\nevidence that supports the presence of implicit reasoning in LLMs. We further\nprovide a structured overview of the evaluation metrics and benchmarks used in\nexisting works to assess the effectiveness and reliability of implicit\nreasoning.We maintain a continuously updated project at:\nhttps://github.com/digailab/awesome-llm-implicit-reasoning.", "AI": {"tldr": "This paper surveys implicit reasoning in large language models (LLMs), introducing a taxonomy of execution paradigms for internal computation.", "motivation": "To address the gap in understanding how reasoning unfolds internally within LLMs, beyond explicit chain-of-thought prompting.", "method": "The paper categorizes existing methods into three execution paradigms: latent optimization, signal-guided control, and layer-recurrent execution; it reviews evidence supporting implicit reasoning.", "result": "The survey reveals the advantages of implicit reasoning in LLMs, including lower generation costs and faster inference.", "conclusion": "A structured overview of evaluation metrics for implicit reasoning is provided, alongside a continuously updated project resource for ongoing research.", "key_contributions": ["Introduces a taxonomy of execution paradigms for LLM internal computation", "Examines structural, behavioral, and representation evidence of implicit reasoning", "Provides a comprehensive overview of evaluation metrics and benchmarks for assessing implicit reasoning"], "limitations": "", "keywords": ["Large Language Models", "Implicit Reasoning", "Execution Paradigms", "Evaluation Metrics", "Computer Science"], "importance_score": 8, "read_time_minutes": 30}}
{"id": "2509.02363", "pdf": "https://arxiv.org/pdf/2509.02363.pdf", "abs": "https://arxiv.org/abs/2509.02363", "title": "Towards Temporal Knowledge-Base Creation for Fine-Grained Opinion Analysis with Language Models", "authors": ["Gaurav Negi", "Atul Kr. Ojha", "Omnia Zayed", "Paul Buitelaar"], "categories": ["cs.CL"], "comment": null, "summary": "We propose a scalable method for constructing a temporal opinion knowledge\nbase with large language models (LLMs) as automated annotators. Despite the\ndemonstrated utility of time-series opinion analysis of text for downstream\napplications such as forecasting and trend analysis, existing methodologies\nunderexploit this potential due to the absence of temporally grounded\nfine-grained annotations. Our approach addresses this gap by integrating\nwell-established opinion mining formulations into a declarative LLM annotation\npipeline, enabling structured opinion extraction without manual prompt\nengineering. We define three data models grounded in sentiment and opinion\nmining literature, serving as schemas for structured representation. We perform\nrigorous quantitative evaluation of our pipeline using human-annotated test\nsamples. We carry out the final annotations using two separate LLMs, and\ninter-annotator agreement is computed label-wise across the fine-grained\nopinion dimensions, analogous to human annotation protocols. The resulting\nknowledge base encapsulates time-aligned, structured opinions and is compatible\nwith applications in Retrieval-Augmented Generation (RAG), temporal question\nanswering, and timeline summarisation.", "AI": {"tldr": "The paper presents a method to create a scalable temporal opinion knowledge base using LLMs for automated annotation, enhancing opinion analysis for various applications.", "motivation": "To address the gap in temporal opinion analysis due to the lack of fine-grained annotations, which limits the potential of current methodologies.", "method": "The proposed approach integrates established opinion mining formulations into an LLM annotation pipeline, allowing for structured opinion extraction with defined data models based on sentiment and opinion mining literature.", "result": "The evaluation shows successful structured representation of time-aligned opinions and high inter-annotator agreement, demonstrating the effectiveness of using LLMs for fine-grained annotation.", "conclusion": "The knowledge base created is valuable for applications such as RAG, temporal question answering, and timeline summarization, facilitating better opinion analysis.", "key_contributions": ["Development of a scalable LLM-based annotation pipeline for opinion mining", "Definition of three data models for structured opinion representation", "Quantitative evaluation using human-annotated samples to ensure accuracy"], "limitations": "The study may require further validation across various text domains and the performance of different LLMs needs thorough exploration.", "keywords": ["temporal opinion knowledge base", "large language models", "opinion mining", "sentiment analysis", "retrieval-augmented generation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2509.02446", "pdf": "https://arxiv.org/pdf/2509.02446.pdf", "abs": "https://arxiv.org/abs/2509.02446", "title": "An Ensemble Classification Approach in A Multi-Layered Large Language Model Framework for Disease Prediction", "authors": ["Ali Hamdi", "Malak Mohamed", "Rokaia Emad", "Khaled Shaban"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Social telehealth has made remarkable progress in healthcare by allowing\npatients to post symptoms and participate in medical consultations remotely.\nUsers frequently post symptoms on social media and online health platforms,\ncreating a huge repository of medical data that can be leveraged for disease\nclassification. Large language models (LLMs) such as LLAMA3 and GPT-3.5, along\nwith transformer-based models like BERT, have demonstrated strong capabilities\nin processing complex medical text. In this study, we evaluate three Arabic\nmedical text preprocessing methods such as summarization, refinement, and Named\nEntity Recognition (NER) before applying fine-tuned Arabic transformer models\n(CAMeLBERT, AraBERT, and AsafayaBERT). To enhance robustness, we adopt a\nmajority voting ensemble that combines predictions from original and\npreprocessed text representations. This approach achieved the best\nclassification accuracy of 80.56%, thus showing its effectiveness in leveraging\nvarious text representations and model predictions to improve the understanding\nof medical texts. To the best of our knowledge, this is the first work that\nintegrates LLM-based preprocessing with fine-tuned Arabic transformer models\nand ensemble learning for disease classification in Arabic social telehealth\ndata.", "AI": {"tldr": "The study evaluates Arabic medical text preprocessing methods and employs fine-tuned transformer models for disease classification in social telehealth data, achieving an accuracy of 80.56%.", "motivation": "To leverage online medical data for disease classification through improved text processing methods and models in the context of Arabic social telehealth.", "method": "The study explores three Arabic medical text preprocessing methods (summarization, refinement, NER) and applies fine-tuned transformer models (CAMeLBERT, AraBERT, AsafayaBERT) followed by a majority voting ensemble to enhance prediction accuracy.", "result": "Achieved a classification accuracy of 80.56%, demonstrating the effectiveness of combining various text preprocessing techniques and model predictions.", "conclusion": "Integrating LLM-based preprocessing with transformer models and ensemble learning significantly improves disease classification efforts in Arabic social telehealth data.", "key_contributions": ["First integration of LLM-based preprocessing with Arabic transformer models for disease classification", "Utilization of a majority voting ensemble for improved accuracy", "Evaluation of multiple preprocessing methods in the Arabic medical text domain"], "limitations": "", "keywords": ["Social Telehealth", "Disease Classification", "Large Language Models", "Arabic Medical Text", "Ensemble Learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.02450", "pdf": "https://arxiv.org/pdf/2509.02450.pdf", "abs": "https://arxiv.org/abs/2509.02450", "title": "EmoPerso: Enhancing Personality Detection with Self-Supervised Emotion-Aware Modelling", "authors": ["Lingzhi Shen", "Xiaohao Cai", "Yunfei Long", "Imran Razzak", "Guanming Chen", "Shoaib Jameel"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Personality detection from text is commonly performed by analysing users'\nsocial media posts. However, existing methods heavily rely on large-scale\nannotated datasets, making it challenging to obtain high-quality personality\nlabels. Moreover, most studies treat emotion and personality as independent\nvariables, overlooking their interactions. In this paper, we propose a novel\nself-supervised framework, EmoPerso, which improves personality detection\nthrough emotion-aware modelling. EmoPerso first leverages generative mechanisms\nfor synthetic data augmentation and rich representation learning. It then\nextracts pseudo-labeled emotion features and jointly optimizes them with\npersonality prediction via multi-task learning. A cross-attention module is\nemployed to capture fine-grained interactions between personality traits and\nthe inferred emotional representations. To further refine relational reasoning,\nEmoPerso adopts a self-taught strategy to enhance the model's reasoning\ncapabilities iteratively. Extensive experiments on two benchmark datasets\ndemonstrate that EmoPerso surpasses state-of-the-art models. The source code is\navailable at https://github.com/slz0925/EmoPerso.", "AI": {"tldr": "This paper presents EmoPerso, a self-supervised framework that enhances personality detection from text through emotion-aware modelling and multi-task learning.", "motivation": "To address the challenges in high-quality personality labeling and the independence assumption between emotion and personality in existing methods.", "method": "EmoPerso uses generative mechanisms for data augmentation, extracts pseudo-labeled emotion features, and employs multi-task learning with a cross-attention module to optimize personality prediction based on emotional representations.", "result": "EmoPerso outperforms state-of-the-art models in personality detection on benchmark datasets, showing improved accuracy by incorporating emotion-aware modeling.", "conclusion": "The framework effectively captures the interactions between emotion and personality traits, leading to better performance in personality detection tasks.", "key_contributions": ["Introduction of the EmoPerso framework for emotion-aware personality detection", "Use of generative mechanisms for synthetic data augmentation", "A self-taught strategy to enhance relational reasoning in personality detection"], "limitations": "", "keywords": ["Personality Detection", "Emotion Awareness", "Self-Supervised Learning", "Multi-Task Learning", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2509.02452", "pdf": "https://arxiv.org/pdf/2509.02452.pdf", "abs": "https://arxiv.org/abs/2509.02452", "title": "Do LLMs Adhere to Label Definitions? Examining Their Receptivity to External Label Definitions", "authors": ["Seyedali Mohammadi", "Bhaskara Hanuma Vedula", "Hemank Lamba", "Edward Raff", "Ponnurangam Kumaraguru", "Francis Ferraro", "Manas Gaur"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "To appear in EMNLP 2025, Main Conference", "summary": "Do LLMs genuinely incorporate external definitions, or do they primarily rely\non their parametric knowledge? To address these questions, we conduct\ncontrolled experiments across multiple explanation benchmark datasets (general\nand domain-specific) and label definition conditions, including expert-curated,\nLLM-generated, perturbed, and swapped definitions. Our results reveal that\nwhile explicit label definitions can enhance accuracy and explainability, their\nintegration into an LLM's task-solving processes is neither guaranteed nor\nconsistent, suggesting reliance on internalized representations in many cases.\nModels often default to their internal representations, particularly in general\ntasks, whereas domain-specific tasks benefit more from explicit definitions.\nThese findings underscore the need for a deeper understanding of how LLMs\nprocess external knowledge alongside their pre-existing capabilities.", "AI": {"tldr": "The paper investigates whether large language models (LLMs) incorporate external definitions or rely on their internal knowledge by conducting experiments on various benchmark datasets.", "motivation": "To understand how LLMs utilize external definitions in their task-solving processes and the impact on accuracy and explainability.", "method": "Controlled experiments were conducted on multiple explanation benchmark datasets, testing different types of definition conditions such as expert-curated and LLM-generated definitions.", "result": "The study found that explicit definitions can enhance performance but are not consistently integrated into LLM's processes; models tend to prefer internal representations for general tasks, while domain-specific tasks show more benefit from explicit definitions.", "conclusion": "These findings highlight the importance of understanding how LLMs process both external and internal knowledge in their operations.", "key_contributions": ["Provides empirical insights into LLMs' reliance on external definitions versus internalized knowledge.", "Reveals the differential benefits of explicit definitions for general versus domain-specific tasks.", "Calls for deeper research into LLMs' processing of external knowledge."], "limitations": "The integration of definitions into LLMs' processes varies and is not guaranteed; further studies are needed to explore additional factors influencing this reliance.", "keywords": ["LLMs", "explanation benchmarks", "external definitions", "machine learning", "task-solving"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2509.02464", "pdf": "https://arxiv.org/pdf/2509.02464.pdf", "abs": "https://arxiv.org/abs/2509.02464", "title": "SpecEval: Evaluating Model Adherence to Behavior Specifications", "authors": ["Ahmed Ahmed", "Kevin Klyman", "Yi Zeng", "Sanmi Koyejo", "Percy Liang"], "categories": ["cs.CL"], "comment": null, "summary": "Companies that develop foundation models publish behavioral guidelines they\npledge their models will follow, but it remains unclear if models actually do\nso. While providers such as OpenAI, Anthropic, and Google have published\ndetailed specifications describing both desired safety constraints and\nqualitative traits for their models, there has been no systematic audit of\nadherence to these guidelines. We introduce an automated framework that audits\nmodels against their providers specifications by parsing behavioral statements,\ngenerating targeted prompts, and using models to judge adherence. Our central\nfocus is on three way consistency between a provider specification, its model\noutputs, and its own models as judges; an extension of prior two way generator\nvalidator consistency. This establishes a necessary baseline: at minimum, a\nfoundation model should consistently satisfy the developer behavioral\nspecifications when judged by the developer evaluator models. We apply our\nframework to 16 models from six developers across more than 100 behavioral\nstatements, finding systematic inconsistencies including compliance gaps of up\nto 20 percent across providers.", "AI": {"tldr": "This paper introduces an automated framework for auditing foundation models to evaluate their adherence to developer-defined behavioral guidelines, revealing significant compliance gaps.", "motivation": "The need for systematic assessment of foundation models' compliance with behavioral guidelines set by their developers.", "method": "An automated framework was developed that parses behavioral statements, generates targeted prompts, and employs models to judge adherence, focusing on consistency between provider specifications and model outputs.", "result": "The framework was applied to 16 models from six developers, uncovering up to 20 percent compliance gaps between developers' guidelines and model outputs.", "conclusion": "There are notable inconsistencies in how foundation models follow the behavioral guidelines set by their developers, indicating a need for improved compliance mechanisms.", "key_contributions": ["Introduction of an automated auditing framework for models", "Empirical findings of compliance gaps in model adherence", "Focus on three-way consistency for evaluation."], "limitations": "The study may not cover all models and guidelines, limiting its generalizability.", "keywords": ["foundation models", "behavioral guidelines", "model auditing"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.02492", "pdf": "https://arxiv.org/pdf/2509.02492.pdf", "abs": "https://arxiv.org/abs/2509.02492", "title": "GRAM-R$^2$: Self-Training Generative Foundation Reward Models for Reward Reasoning", "authors": ["Chenglong Wang", "Yongyu Mu", "Hang Zhou", "Yifu Huo", "Ziming Zhu", "Jiali Zeng", "Murun Yang", "Bei Li", "Tong Xiao", "Xiaoyang Hao", "Chunliang Zhang", "Fandong Meng", "Jingbo Zhu"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Significant progress in reward modeling over recent years has been driven by\na paradigm shift from task-specific designs towards generalist reward models.\nDespite this trend, developing effective reward models remains a fundamental\nchallenge: the heavy reliance on large-scale labeled preference data.\nPre-training on abundant unlabeled data offers a promising direction, but\nexisting approaches fall short of instilling explicit reasoning into reward\nmodels. To bridge this gap, we propose a self-training approach that leverages\nunlabeled data to elicit reward reasoning in reward models. Based on this\napproach, we develop GRAM-R$^2$, a generative reward model trained to produce\nnot only preference labels but also accompanying reward rationales. GRAM-R$^2$\ncan serve as a foundation model for reward reasoning and can be applied to a\nwide range of tasks with minimal or no additional fine-tuning. It can support\ndownstream applications such as response ranking and task-specific reward\ntuning. Experiments on response ranking, task adaptation, and reinforcement\nlearning from human feedback demonstrate that GRAM-R$^2$ consistently delivers\nstrong performance, outperforming several strong discriminative and generative\nbaselines.", "AI": {"tldr": "Introducing GRAM-R$^2$, a generative reward model that leverages self-training on unlabeled data for reward reasoning, outperforming traditional models in various applications.", "motivation": "To address the challenges of developing effective reward models dependent on large-scale labeled preference data.", "method": "A self-training approach that utilizes unlabeled data to elicit reasoning within reward models, resulting in GRAM-R$^2$.", "result": "GRAM-R$^2$ shows improved performance in response ranking and task-specific tuning, surpassing both discriminative and generative baselines.", "conclusion": "GRAM-R$^2$ can serve as a foundation model for reward reasoning, applicable to a variety of tasks with minimal fine-tuning required.", "key_contributions": ["Introduced a self-training approach for reward modeling using unlabeled data.", "Developed GRAM-R$^2$, a generative reward model that delivers preference labels with rationales.", "Demonstrated superior performance in downstream tasks compared to existing models."], "limitations": "", "keywords": ["reward modeling", "self-training", "generative models", "human feedback", "machine learning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2509.02499", "pdf": "https://arxiv.org/pdf/2509.02499.pdf", "abs": "https://arxiv.org/abs/2509.02499", "title": "MoSEs: Uncertainty-Aware AI-Generated Text Detection via Mixture of Stylistics Experts with Conditional Thresholds", "authors": ["Junxi Wu", "Jinpeng Wang", "Zheng Liu", "Bin Chen", "Dongjian Hu", "Hao Wu", "Shu-Tao Xiu"], "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025", "summary": "The rapid advancement of large language models has intensified public\nconcerns about the potential misuse. Therefore, it is important to build\ntrustworthy AI-generated text detection systems. Existing methods neglect\nstylistic modeling and mostly rely on static thresholds, which greatly limits\nthe detection performance. In this paper, we propose the Mixture of Stylistic\nExperts (MoSEs) framework that enables stylistics-aware uncertainty\nquantification through conditional threshold estimation. MoSEs contain three\ncore components, namely, the Stylistics Reference Repository (SRR), the\nStylistics-Aware Router (SAR), and the Conditional Threshold Estimator (CTE).\nFor input text, SRR can activate the appropriate reference data in SRR and\nprovide them to CTE. Subsequently, CTE jointly models the linguistic\nstatistical properties and semantic features to dynamically determine the\noptimal threshold. With a discrimination score, MoSEs yields prediction labels\nwith the corresponding confidence level. Our framework achieves an average\nimprovement 11.34% in detection performance compared to baselines. More\ninspiringly, MoSEs shows a more evident improvement 39.15% in the low-resource\ncase. Our code is available at https://github.com/creator-xi/MoSEs.", "AI": {"tldr": "This paper presents the Mixture of Stylistic Experts (MoSEs) framework for enhancing AI-generated text detection by incorporating stylistics-aware uncertainty quantification for better performance.", "motivation": "To address public concerns regarding the misuse of AI-generated text and improve detection performance, particularly in low-resource settings.", "method": "The MoSEs framework includes a Stylistics Reference Repository (SRR) for activating relevant reference data, a Stylistics-Aware Router (SAR) for processing inputs, and a Conditional Threshold Estimator (CTE) that models linguistic and semantic features to dynamically set detection thresholds.", "result": "The MoSEs framework achieved an average performance improvement of 11.34% in text detection against baseline models and a notable 39.15% improvement in low-resource scenarios.", "conclusion": "MoSEs provides a robust solution for detecting AI-generated text by incorporating stylistic elements, which enhances reliability and trustworthiness in detection systems.", "key_contributions": ["Introduction of the MoSEs framework for AI text detection.", "Integration of stylistics-aware uncertainty quantification.", "Demonstrated significant performance improvements in low-resource environments."], "limitations": "", "keywords": ["AI-generated text detection", "stylistics modeling", "uncertainty quantification"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2509.02503", "pdf": "https://arxiv.org/pdf/2509.02503.pdf", "abs": "https://arxiv.org/abs/2509.02503", "title": "L3Cube-IndicHeadline-ID: A Dataset for Headline Identification and Semantic Evaluation in Low-Resource Indian Languages", "authors": ["Nishant Tanksale", "Tanmay Kokate", "Darshan Gohad", "Sarvadnyaa Barate", "Raviraj Joshi"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Semantic evaluation in low-resource languages remains a major challenge in\nNLP. While sentence transformers have shown strong performance in high-resource\nsettings, their effectiveness in Indic languages is underexplored due to a lack\nof high-quality benchmarks. To bridge this gap, we introduce\nL3Cube-IndicHeadline-ID, a curated headline identification dataset spanning ten\nlow-resource Indic languages: Marathi, Hindi, Tamil, Gujarati, Odia, Kannada,\nMalayalam, Punjabi, Telugu, Bengali and English. Each language includes 20,000\nnews articles paired with four headline variants: the original, a semantically\nsimilar version, a lexically similar version, and an unrelated one, designed to\ntest fine-grained semantic understanding. The task requires selecting the\ncorrect headline from the options using article-headline similarity. We\nbenchmark several sentence transformers, including multilingual and\nlanguage-specific models, using cosine similarity. Results show that\nmultilingual models consistently perform well, while language-specific models\nvary in effectiveness. Given the rising use of similarity models in\nRetrieval-Augmented Generation (RAG) pipelines, this dataset also serves as a\nvaluable resource for evaluating and improving semantic understanding in such\napplications. Additionally, the dataset can be repurposed for multiple-choice\nquestion answering, headline classification, or other task-specific evaluations\nof LLMs, making it a versatile benchmark for Indic NLP. The dataset is shared\npublicly at https://github.com/l3cube-pune/indic-nlp", "AI": {"tldr": "This paper introduces L3Cube-IndicHeadline-ID, a dataset for headline identification in ten low-resource Indic languages, benchmarked using sentence transformers.", "motivation": "To address the lack of high-quality benchmarks for semantic evaluation in low-resource Indic languages in NLP.", "method": "The study introduces a dataset featuring 20,000 news articles per language, each article paired with four types of headlines. Performance of several sentence transformers is evaluated using cosine similarity metrics.", "result": "Multilingual models perform consistently well across the dataset, while language-specific models exhibit varying effectiveness, demonstrating the challenges and opportunities in low-resource language scenarios.", "conclusion": "The L3Cube-IndicHeadline-ID dataset is a valuable resource for improving semantic understanding in multiple NLP tasks, including Retrieval-Augmented Generation and question answering.", "key_contributions": ["Introduction of a comprehensive dataset for low-resource Indic languages", "Benchmarking of various sentence transformers on this dataset", "Demonstration of the dataset's versatility for different NLP tasks"], "limitations": "", "keywords": ["NLP", "low-resource languages", "sentence transformers", "headline identification", "Indic languages"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2509.02506", "pdf": "https://arxiv.org/pdf/2509.02506.pdf", "abs": "https://arxiv.org/abs/2509.02506", "title": "The Forgotten Code: Validating a Century-Old Translation System with AI", "authors": ["Jean-Marie Le Ray"], "categories": ["cs.CL", "I.2"], "comment": "Preprint, 35 pages, 14 figures, 9 appendices", "summary": "A pioneering rule-based mechanical translation system (precursor of modern\nRBMTs) was first presented in December 1929 by its inventor, Federico Pucci,\nwho later published the full method in a book titled \"Il traduttore meccanico\ned il metodo per corrispondersi fra Europei conoscendo ciascuno solo la propria\nlingua: Parte I\", in Salerno (Italy), in 1931. This study illustrates how AI\nbreathes new life into the system of international keys and ideograms devised\nby Pucci to translate from/into any Romance language (at least as a first\nstep). The methodology involves having the AIs retranslate, following Pucci's\nmethod, the two text excerpts originally translated in 1931 and clearly\ndocumented in his publication: a passage from Dante's La Vita Nuova, translated\nfrom Italian into French, and a passage from Voltaire's Zadig, translated from\nFrench into Italian. The result is notable: the two texts, translated 94 years\napart using the same method--by Pucci in 1931 and by AIs in 2025--show a low\naverage difference, with only minor variations observed. With Pucci's system\nthus validated, it became feasible to have the AIs reproduce the excerpts in\nEnglish, Spanish, and German according to his method. The results were\nconsistent, and Pucci--via Artificial Intelligence--was tasked with translating\nmore modern and technical texts, thereby reviving, nearly a century later, an\ninvention that had remained almost entirely unknown and never applied beyond\nits creator, now brought to wider attention and opened to possible\nexperimentation. Such a demonstration would not only affirm Pucci's historical\nstatus but also place him among the precursors and intellectual contributors to\nmachine translation, whose work merits examination alongside figures such as\nTroyanskij, Booth, and Weaver, with possible consequences for how the history\nof the field is understood.", "AI": {"tldr": "This paper explores the revival of a historical rule-based mechanical translation system by Federico Pucci, demonstrating its relevance through modern AI translation methods.", "motivation": "To illustrate the historical significance of Federico Pucci's mechanical translation system and its potential applications in contemporary AI-driven translation.", "method": "AIs are used to retranslate text excerpts translated by Pucci in 1931, following his original methodology, comparing results from both periods.", "result": "The AI translations of the texts showed a low average difference with only minor variations, validating Pucci's system.", "conclusion": "Pucci's translation methods were successfully revived, indicating their continuing relevance and suggesting further experimentation in modern contexts.", "key_contributions": ["Validation of Pucci's mechanical translation system using modern AI methods.", "Demonstration of low error rates in translations done 94 years apart.", "Promotion of Pucci's work as a noteworthy contribution to the history of machine translation."], "limitations": "", "keywords": ["machine translation", "rule-based", "AI", "computational linguistics", "historical translations"], "importance_score": 4, "read_time_minutes": 35}}
{"id": "2509.02510", "pdf": "https://arxiv.org/pdf/2509.02510.pdf", "abs": "https://arxiv.org/abs/2509.02510", "title": "Top-H Decoding: Adapting the Creativity and Coherence with Bounded Entropy in Text Generation", "authors": ["Erfan Baghaei Potraghloo", "Seyedarmin Azizi", "Souvik Kundu", "Massoud Pedram"], "categories": ["cs.CL", "cs.AI", "stat.ML"], "comment": null, "summary": "Large language models (LLMs), despite their impressive performance across a\nwide range of tasks, often struggle to balance two competing objectives in\nopen-ended text generation: fostering diversity and creativity while preserving\nlogical coherence. Existing truncated sampling techniques, including\ntemperature scaling, top-\\$p\\$ (nucleus) sampling, and min-\\$p\\$ sampling, aim\nto manage this trade-off. However, they exhibit limitations, particularly in\nthe effective incorporation of the confidence of the model into the\ncorresponding sampling strategy. For example, min-\\$p\\$ sampling relies on a\nsingle top token as a heuristic for confidence, eventually underutilizing the\ninformation of the probability distribution. Toward effective incorporation of\nthe confidence of the model, in this paper, we present **top-H** decoding. We\nfirst establish the theoretical foundation of the interplay between creativity\nand coherence in truncated sampling by formulating an **entropy-constrained\nminimum divergence** problem. We then prove this minimization problem to be\nequivalent to an **entropy-constrained mass maximization** (ECMM) problem,\nwhich is NP-hard. Finally, we present top-H decoding, a computationally\nefficient greedy algorithm to solve the ECMM problem. Extensive empirical\nevaluations demonstrate that top-H outperforms the state-of-the-art (SoTA)\nalternative of min-\\$p\\$ sampling by up to **25.63%** on creative writing\nbenchmarks, while maintaining robustness on question-answering datasets such as\nGPQA, GSM8K, and MT-Bench. Additionally, an *LLM-as-judge* evaluation confirms\nthat top-H indeed produces coherent outputs even at higher temperatures, where\ncreativity is especially critical. In summary, top-H advances SoTA in\nopen-ended text generation and can be *easily integrated* into creative writing\napplications. The code is available at\nhttps://github.com/ErfanBaghaei/Top-H-Decoding.", "AI": {"tldr": "This paper introduces top-H decoding, a novel approach to improve text generation by balancing creativity and coherence in large language models (LLMs).", "motivation": "To address the limitations of existing truncated sampling techniques in generating diverse and coherent text.", "method": "Theoretical foundation establishes the relationship between creativity and coherence via an entropy-constrained minimum divergence problem, leading to the development of the top-H decoding algorithm as a solution to this NP-hard problem.", "result": "Top-H decoding outperforms min-p sampling by up to 25.63% on creative writing benchmarks and maintains robustness on question-answering datasets.", "conclusion": "Top-H decoding advances state-of-the-art techniques in open-ended text generation and can be easily integrated into creative writing applications.", "key_contributions": ["Introduction of top-H decoding for balancing creativity and coherence", "Theoretical formulation connecting entropy constraints with text generation", "Demonstrated significant performance improvements over existing methods"], "limitations": "", "keywords": ["Large language models", "Text generation", "Sampling techniques", "Creativity", "Coherence"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2509.02514", "pdf": "https://arxiv.org/pdf/2509.02514.pdf", "abs": "https://arxiv.org/abs/2509.02514", "title": "Comparative Study of Pre-Trained BERT and Large Language Models for Code-Mixed Named Entity Recognition", "authors": ["Mayur Shirke", "Amey Shembade", "Pavan Thorat", "Madhushri Wagh", "Raviraj Joshi"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Named Entity Recognition (NER) in code-mixed text, particularly Hindi-English\n(Hinglish), presents unique challenges due to informal structure,\ntransliteration, and frequent language switching. This study conducts a\ncomparative evaluation of code-mixed fine-tuned models and non-code-mixed\nmultilingual models, along with zero-shot generative large language models\n(LLMs). Specifically, we evaluate HingBERT, HingMBERT, and HingRoBERTa (trained\non code-mixed data), and BERT Base Cased, IndicBERT, RoBERTa and MuRIL (trained\non non-code-mixed multilingual data). We also assess the performance of Google\nGemini in a zero-shot setting using a modified version of the dataset with NER\ntags removed. All models are tested on a benchmark Hinglish NER dataset using\nPrecision, Recall, and F1-score. Results show that code-mixed models,\nparticularly HingRoBERTa and HingBERT-based fine-tuned models, outperform\nothers - including closed-source LLMs like Google Gemini - due to\ndomain-specific pretraining. Non-code-mixed models perform reasonably but show\nlimited adaptability. Notably, Google Gemini exhibits competitive zero-shot\nperformance, underlining the generalization strength of modern LLMs. This study\nprovides key insights into the effectiveness of specialized versus generalized\nmodels for code-mixed NER tasks.", "AI": {"tldr": "This study evaluates Named Entity Recognition (NER) in code-mixed Hindi-English (Hinglish) using various models and provides insights on their effectiveness.", "motivation": "Named Entity Recognition (NER) in code-mixed text presents unique challenges, which necessitates an evaluation of specialized versus generalized models.", "method": "A comparative analysis of code-mixed fine-tuned models (HingBERT, HingMBERT, HingRoBERTa) and non-code-mixed multilingual models (BERT Base, IndicBERT, RoBERTa, MuRIL), along with zero-shot evaluations of Google Gemini.", "result": "Code-mixed models outperform non-code-mixed models, with HingRoBERTa and HingBERT showing the best performance; Google Gemini demonstrates competitive performance in zero-shot settings.", "conclusion": "Specialized pretraining in code-mixed models leads to superior performance for NER tasks in code-mixed texts compared to generalized models.", "key_contributions": ["Comparative evaluation of code-mixed and non-code-mixed models for NER in Hinglish.", "Demonstration of the effectiveness of domain-specific pretrained models for NER tasks.", "Insights into the performance of zero-shot generative LLMs for NER."], "limitations": "The study focuses on a specific dataset and may not generalize across all code-mixed text scenarios.", "keywords": ["Named Entity Recognition", "code-mixed text", "Hindi-English", "Hinglish", "large language models"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2509.02522", "pdf": "https://arxiv.org/pdf/2509.02522.pdf", "abs": "https://arxiv.org/abs/2509.02522", "title": "Implicit Actor Critic Coupling via a Supervised Learning Framework for RLVR", "authors": ["Jiaming Li", "Longze Chen", "Ze Gong", "Yukun Chen", "Lu Wang", "Wanwei He", "Run Luo", "Min Yang"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) have\nempowered large language models (LLMs) to tackle challenging reasoning tasks\nsuch as mathematics and programming. RLVR leverages verifiable outcome rewards\nto guide policy optimization, enabling LLMs to progressively improve output\nquality in a grounded and reliable manner. Despite its promise, the RLVR\nparadigm poses significant challenges, as existing methods often suffer from\nsparse reward signals and unstable policy gradient updates, particularly in\nRL-based approaches. To address the challenges, we propose $\\textbf{PACS}$, a\nnovel RLVR framework that achieves im$\\textbf{P}$licit $\\textbf{A}$ctor\n$\\textbf{C}$ritic coupling via a $\\textbf{S}$upervised learning framework. By\ntreating the outcome reward as a predictable label, we reformulate the RLVR\nproblem into a supervised learning task over a score function parameterized by\nthe policy model and optimized using cross-entropy loss. A detailed gradient\nanalysis shows that this supervised formulation inherently recovers the\nclassical policy gradient update while implicitly coupling actor and critic\nroles, yielding more stable and efficient training. Benchmarking on challenging\nmathematical reasoning tasks, PACS outperforms strong RLVR baselines, such as\nPPO and GRPO, achieving superior reasoning performance. For instance, PACS\nachieves 59.78\\% at pass@256 on AIME 2025, representing improvements of 13.32\nand 14.36 points over PPO and GRPO. This simple yet powerful framework offers a\npromising avenue for LLMs post-training with verifiable rewards. Our code and\ndata are available as open source at https://github.com/ritzz-ai/PACS.", "AI": {"tldr": "A novel RLVR framework called PACS improves LLM reasoning tasks by reformulating RLVR as a supervised learning problem, resulting in more stable training.", "motivation": "To improve the stability and performance of large language models (LLMs) using Reinforcement Learning with Verifiable Rewards (RLVR) for reasoning tasks.", "method": "PACS reformulates the RLVR problem into a supervised learning task by treating the outcome reward as a predictable label, optimizing a score function with cross-entropy loss while coupling actor and critic roles.", "result": "PACS outperforms traditional RLVR baselines like PPO and GRPO, achieving a 59.78% pass rate on AIME 2025, significantly better than its predecessors by up to 14.36 points.", "conclusion": "PACS provides a robust framework for integrating verifiable rewards into LLMs, enhancing their reasoning capabilities post-training, and is accessible as open source.", "key_contributions": ["Introduces PACS, a novel framework for RLVR that stabilizes training.", "Reformulates RLVR as a supervised learning task for better performance.", "Demonstrates superior reasoning performance on mathematical tasks compared to existing methods."], "limitations": "", "keywords": ["Reinforcement Learning", "Verifiable Rewards", "Large Language Models", "Supervised Learning", "Reasoning Tasks"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2509.02523", "pdf": "https://arxiv.org/pdf/2509.02523.pdf", "abs": "https://arxiv.org/abs/2509.02523", "title": "Flavors of Moonshine: Tiny Specialized ASR Models for Edge Devices", "authors": ["Evan King", "Adam Sabra", "Manjunath Kudlur", "James Wang", "Pete Warden"], "categories": ["cs.CL", "cs.LG", "cs.SD"], "comment": null, "summary": "We present the Flavors of Moonshine, a suite of tiny automatic speech\nrecognition (ASR) models specialized for a range of underrepresented languages.\nPrevailing wisdom suggests that multilingual ASR models outperform monolingual\ncounterparts by exploiting cross-lingual phonetic similarities. We challenge\nthis assumption, showing that for sufficiently small models (27M parameters),\ntraining monolingual systems on a carefully balanced mix of high-quality\nhuman-labeled, pseudo-labeled, and synthetic data yields substantially superior\nperformance. On average, our models achieve error rates 48% lower than the\ncomparably sized Whisper Tiny model, outperform the 9x larger Whisper Small\nmodel, and in most cases match or outperform the 28x larger Whisper Medium\nmodel. These results advance the state of the art for models of this size,\nenabling accurate on-device ASR for languages that previously had limited\nsupport. We release Arabic, Chinese, Japanese, Korean, Ukrainian, and\nVietnamese Moonshine models under a permissive open-source license.", "AI": {"tldr": "This paper introduces the Flavors of Moonshine, a suite of automatic speech recognition models for underrepresented languages, demonstrating that monolingual models can outperform multilingual counterparts for small model sizes.", "motivation": "To challenge the prevailing assumption that multilingual ASR models are superior to monolingual models for small parameter sizes, particularly for underrepresented languages.", "method": "Training monolingual ASR systems on a balanced mix of high-quality human-labeled, pseudo-labeled, and synthetic data, while keeping the model size to 27M parameters.", "result": "The monolingual models achieve, on average, 48% lower error rates than the Whisper Tiny model and outperform larger Whisper models in most cases.", "conclusion": "The results advance the state of the art for small ASR models, enabling accurate on-device recognition for languages that lack sufficient support.", "key_contributions": ["Introduction of monolingual models that outperform larger multilingual counterparts.", "Significant reduction in error rates for small ASR models.", "Release of new ASR models for multiple underrepresented languages."], "limitations": "", "keywords": ["automatic speech recognition", "underrepresented languages", "monolingual models"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2509.02534", "pdf": "https://arxiv.org/pdf/2509.02534.pdf", "abs": "https://arxiv.org/abs/2509.02534", "title": "Jointly Reinforcing Diversity and Quality in Language Model Generations", "authors": ["Tianjian Li", "Yiming Zhang", "Ping Yu", "Swarnadeep Saha", "Daniel Khashabi", "Jason Weston", "Jack Lanchantin", "Tianlu Wang"], "categories": ["cs.CL", "cs.LG"], "comment": "29 pages, 11 figures", "summary": "Post-training of Large Language Models (LMs) often prioritizes accuracy and\nhelpfulness at the expense of diversity. This creates a tension: while\npost-training improves response quality, it also sharpens output distributions\nand reduces the range of ideas, limiting the usefulness of LMs in creative and\nexploratory tasks such as brainstorming, storytelling, or problem solving. We\naddress this challenge with Diversity-Aware Reinforcement Learning (DARLING), a\nframework that jointly optimizes for response quality and semantic diversity.\nAt its core, DARLING introduces a learned partition function to measure\ndiversity beyond surface-level lexical variations. This diversity signal is\nthen combined with a quality reward during online reinforcement learning,\nencouraging models to generate outputs that are both high-quality and distinct.\nExperiments across multiple model families and sizes show that DARLING\ngeneralizes to two regimes: non-verifiable tasks (instruction following and\ncreative writing) and verifiable tasks (competition math). On five benchmarks\nin the first setting, DARLING consistently outperforms quality-only RL\nbaselines, producing outputs that are simultaneously of higher quality and\nnovelty. In the second setting, DARLING achieves higher pass@1 (solution\nquality) and pass@k (solution variety). Most strikingly, explicitly optimizing\nfor diversity catalyzes exploration in online RL, which manifests itself as\nhigher-quality responses.", "AI": {"tldr": "This paper introduces Diversity-Aware Reinforcement Learning (DARLING), a framework that balances response quality and semantic diversity in Large Language Models (LMs).", "motivation": "To address the issue of reduced diversity in responses from Large Language Models during post-training, which limits their effectiveness in creative and exploratory tasks.", "method": "DARLING employs a learned partition function to measure diversity and combines this with quality rewards during reinforcement learning to optimize model outputs.", "result": "Experiments show that DARLING consistently outperforms quality-only RL baselines across both non-verifiable (instruction following, creative writing) and verifiable tasks (competition math), yielding higher quality and more novel outputs.", "conclusion": "Optimizing for diversity enhances exploration in online reinforcement learning, resulting in higher-quality responses from models.", "key_contributions": ["Introduction of Diversity-Aware Reinforcement Learning (DARLING) framework", "Demonstrated generalization of DARLING across multiple model families and tasks", "Achieved higher quality and novelty in Large Language Model outputs compared to traditional methods"], "limitations": "", "keywords": ["Diversity", "Reinforcement Learning", "Large Language Models", "Semantic Quality", "Exploratory Tasks"], "importance_score": 9, "read_time_minutes": 29}}
{"id": "2509.02550", "pdf": "https://arxiv.org/pdf/2509.02550.pdf", "abs": "https://arxiv.org/abs/2509.02550", "title": "PalmX 2025: The First Shared Task on Benchmarking LLMs on Arabic and Islamic Culture", "authors": ["Fakhraddin Alwajih", "Abdellah El Mekki", "Hamdy Mubarak", "Majd Hawasly", "Abubakr Mohamed", "Muhammad Abdul-Mageed"], "categories": ["cs.CL"], "comment": "https://palmx.dlnlp.ai/", "summary": "Large Language Models (LLMs) inherently reflect the vast data distributions\nthey encounter during their pre-training phase. As this data is predominantly\nsourced from the web, there is a high chance it will be skewed towards\nhigh-resourced languages and cultures, such as those of the West. Consequently,\nLLMs often exhibit a diminished understanding of certain communities, a gap\nthat is particularly evident in their knowledge of Arabic and Islamic cultures.\nThis issue becomes even more pronounced with increasingly under-represented\ntopics. To address this critical challenge, we introduce PalmX 2025, the first\nshared task designed to benchmark the cultural competence of LLMs in these\nspecific domains. The task is composed of two subtasks featuring\nmultiple-choice questions (MCQs) in Modern Standard Arabic (MSA): General\nArabic Culture and General Islamic Culture. These subtasks cover a wide range\nof topics, including traditions, food, history, religious practices, and\nlanguage expressions from across 22 Arab countries. The initiative drew\nconsiderable interest, with 26 teams registering for Subtask 1 and 19 for\nSubtask 2, culminating in nine and six valid submissions, respectively. Our\nfindings reveal that task-specific fine-tuning substantially boosts performance\nover baseline models. The top-performing systems achieved an accuracy of 72.15%\non cultural questions and 84.22% on Islamic knowledge. Parameter-efficient\nfine-tuning emerged as the predominant and most effective approach among\nparticipants, while the utility of data augmentation was found to be\ndomain-dependent.", "AI": {"tldr": "PalmX 2025 establishes a benchmarking task for evaluating the cultural competence of LLMs regarding Arabic and Islamic cultures, revealing significant performance improvements through fine-tuning.", "motivation": "To address the cultural understanding gap in LLMs, particularly regarding Arabic and Islamic cultures, which are often under-represented in pre-training data.", "method": "The benchmarking task consists of two subtasks involving multiple-choice questions in Modern Standard Arabic, focusing on General Arabic Culture and General Islamic Culture.", "result": "Task-specific fine-tuning led to notable performance increases, with top accuracy scores reaching 72.15% on cultural questions and 84.22% on Islamic knowledge.", "conclusion": "Fine-tuning is essential for improving LLM performance in culturally specific domains, and the effectiveness of techniques such as parameter-efficient fine-tuning and data augmentation varies by context.", "key_contributions": ["Introduction of the PalmX 2025 cultural competence benchmarking task for LLMs.", "Demonstrated significant performance enhancements via tailored fine-tuning strategies.", "Findings highlight disparities in LLMs' competency across different cultural contexts."], "limitations": "", "keywords": ["Large Language Models", "Cultural Competence", "Arabic Culture", "Islamic Culture", "Fine-Tuning"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2509.01051", "pdf": "https://arxiv.org/pdf/2509.01051.pdf", "abs": "https://arxiv.org/abs/2509.01051", "title": "Chronotome: Real-Time Topic Modeling for Streaming Embedding Spaces", "authors": ["Matte Lim", "Catherine Yeh", "Martin Wattenberg", "Fernanda Viégas", "Panagiotis Michalatos"], "categories": ["cs.HC", "cs.CL", "cs.CV", "cs.LG"], "comment": "Accepted to IEEE VIS 2025 Short Paper Track (5 pages, 4 figures)", "summary": "Many real-world datasets -- from an artist's body of work to a person's\nsocial media history -- exhibit meaningful semantic changes over time that are\ndifficult to capture with existing dimensionality reduction methods. To address\nthis gap, we introduce a visualization technique that combines force-based\nprojection and streaming clustering methods to build a spatial-temporal map of\nembeddings. Applying this technique, we create Chronotome, a tool for\ninteractively exploring evolving themes in time-based data -- in real time. We\ndemonstrate the utility of our approach through use cases on text and image\ndata, showing how it offers a new lens for understanding the aesthetics and\nsemantics of temporal datasets.", "AI": {"tldr": "Chronotome is a visualization technique designed to explore evolving themes in real-time through a spatial-temporal map of embeddings.", "motivation": "Existing dimensionality reduction methods fail to capture meaningful semantic changes in real-world datasets over time.", "method": "The proposed technique integrates force-based projection with streaming clustering to create an interactive exploration tool.", "result": "Chronotome successfully visualizes temporal datasets, allowing for an interactive understanding of evolving themes in both text and image data.", "conclusion": "Chronotome provides a novel perspective on analyzing the aesthetics and semantics of time-based data, enhancing interpretability.", "key_contributions": ["Development of Chronotome for real-time exploration of temporal data", "Integration of force-based projection with streaming clustering", "Novel approach for visualizing evolving themes in multimodal datasets"], "limitations": "", "keywords": ["visualization", "temporal data", "dimensionality reduction", "embedding", "streaming clustering"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2509.02100", "pdf": "https://arxiv.org/pdf/2509.02100.pdf", "abs": "https://arxiv.org/abs/2509.02100", "title": "E-THER: A PCT-Grounded Dataset for Benchmarking Empathic AI", "authors": ["Sharjeel Tahir", "Judith Johnson", "Jumana Abu-Khalaf", "Syed Afaq Ali Shah"], "categories": ["cs.HC", "cs.CL"], "comment": "15 pages, 4 figures. Preprint", "summary": "A prevalent shortfall among current empathic AI systems is their inability to\nrecognize when verbal expressions may not fully reflect underlying emotional\nstates. This is because the existing datasets, used for the training of these\nsystems, focus on surface-level emotion recognition without addressing the\ncomplex verbal-visual incongruence (mismatch) patterns useful for empathic\nunderstanding. In this paper, we present E-THER, the first Person-Centered\nTherapy-grounded multimodal dataset with multidimensional annotations for\nverbal-visual incongruence detection, enabling training of AI systems that\ndevelop genuine rather than performative empathic capabilities. The annotations\nincluded in the dataset are drawn from humanistic approach, i.e., identifying\nverbal-visual emotional misalignment in client-counsellor interactions -\nforming a framework for training and evaluating AI on empathy tasks. Additional\nengagement scores provide behavioral annotations for research applications.\nNotable gains in empathic and therapeutic conversational qualities are observed\nin state-of-the-art vision-language models (VLMs), such as IDEFICS and\nVideoLLAVA, using evaluation metrics grounded in empathic and therapeutic\nprinciples. Empirical findings indicate that our incongruence-trained models\noutperform general-purpose models in critical traits, such as sustaining\ntherapeutic engagement, minimizing artificial or exaggerated linguistic\npatterns, and maintaining fidelity to PCT theoretical framework.", "AI": {"tldr": "The paper introduces E-THER, a dataset for training empathic AI systems to recognize verbal-visual incongruence in therapeutic contexts.", "motivation": "Current empathic AI systems fail to recognize when verbal expressions do not reflect true emotional states due to limitations in existing datasets that ignore verbal-visual mismatches.", "method": "The authors present E-THER, a multimodal dataset with annotations for verbal-visual incongruence detection, based on client-counsellor interactions, enabling improved training of vision-language models for empathic interactions.", "result": "Empirical findings show that incongruence-trained models like IDEFICS and VideoLLAVA significantly improve empathic and therapeutic conversational qualities compared to general-purpose models.", "conclusion": "The study demonstrates that models trained with the E-THER dataset exhibit enhanced therapeutic engagement and fidelity to Person-Centered Therapy principles.", "key_contributions": ["Introduction of the E-THER dataset for verbal-visual incongruence detection", "Demonstration of improved performance in empathic AI using state-of-the-art vision-language models", "Framework for training AI systems on empathy tasks based on humanistic approaches."], "limitations": "", "keywords": ["Empathy", "Verbal-Visual Incongruence", "AI", "Therapeutic Engagement", "Multimodal Dataset"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2210.14275", "pdf": "https://arxiv.org/pdf/2210.14275.pdf", "abs": "https://arxiv.org/abs/2210.14275", "title": "Similarity between Units of Natural Language: The Transition from Coarse to Fine Estimation", "authors": ["Wenchuan Mu"], "categories": ["cs.CL"], "comment": "PhD thesis", "summary": "Capturing the similarities between human language units is crucial for\nexplaining how humans associate different objects, and therefore its\ncomputation has received extensive attention, research, and applications. With\nthe ever-increasing amount of information around us, calculating similarity\nbecomes increasingly complex, especially in many cases, such as legal or\nmedical affairs, measuring similarity requires extra care and precision, as\nsmall acts within a language unit can have significant real-world effects. My\nresearch goal in this thesis is to develop regression models that account for\nsimilarities between language units in a more refined way.\n  Computation of similarity has come a long way, but approaches to debugging\nthe measures are often based on continually fitting human judgment values. To\nthis end, my goal is to develop an algorithm that precisely catches loopholes\nin a similarity calculation. Furthermore, most methods have vague definitions\nof the similarities they compute and are often difficult to interpret. The\nproposed framework addresses both shortcomings. It constantly improves the\nmodel through catching different loopholes. In addition, every refinement of\nthe model provides a reasonable explanation. The regression model introduced in\nthis thesis is called progressively refined similarity computation, which\ncombines attack testing with adversarial training. The similarity regression\nmodel of this thesis achieves state-of-the-art performance in handling edge\ncases.", "AI": {"tldr": "This thesis presents a framework for refining language unit similarity computation through regression models, focusing on precision in sensitive areas like legal and medical fields while improving interpretability.", "motivation": "The increasing complexity of capturing similarities in human language units necessitates new approaches, especially in critical domains where precision is vital.", "method": "A progressively refined similarity computation model is developed that combines attack testing with adversarial training to improve similarity measures.", "result": "The regression model shows state-of-the-art performance in handling edge cases in similarity computation.", "conclusion": "The proposed model not only enhances the accuracy of similarity calculations but also offers clearer interpretations, addressing previous limitations in the field.", "key_contributions": ["Development of a regression model for refined similarity computation.", "Integration of attack testing with adversarial training.", "Improvement of interpretability in similarity measures."], "limitations": "", "keywords": ["similarity computation", "regression models", "language units"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2401.02968", "pdf": "https://arxiv.org/pdf/2401.02968.pdf", "abs": "https://arxiv.org/abs/2401.02968", "title": "Rule-Guided Joint Embedding Learning over Knowledge Graphs", "authors": ["Qisong Li", "Ji Lin", "Sijia Wei", "Neng Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Recent studies on knowledge graph embedding focus on mapping entities and\nrelations into low-dimensional vector spaces. While most existing models\nprimarily exploit structural information, knowledge graphs also contain rich\ncontextual and textual information that can enhance embedding effectiveness. In\nthis work, we propose a novel model that integrates both contextual and textual\nsignals into entity and relation embeddings through a graph convolutional\nnetwork. To better utilize context, we introduce two metrics: confidence,\ncomputed via a rule-based method, and relatedness, derived from textual\nrepresentations. These metrics enable more precise weighting of contextual\ninformation during embedding learning. Extensive experiments on two widely used\nbenchmark datasets demonstrate the effectiveness of our approach, showing\nconsistent improvements over strong baselines.", "AI": {"tldr": "This paper presents a model that integrates contextual and textual information into knowledge graph embeddings using graph convolutional networks, enhancing embedding effectiveness.", "motivation": "To improve knowledge graph embedding by incorporating rich contextual and textual information, which existing models have underutilized.", "method": "The authors propose a novel model that utilizes a graph convolutional network to integrate contextual and textual signals, introducing metrics for confidence and relatedness to weigh this information during embedding learning.", "result": "The model shows consistent improvements in embedding effectiveness over strong baselines in experiments on benchmark datasets.", "conclusion": "The proposed approach enhances the precision of knowledge graph embeddings by better utilizing contextual and textual signals.", "key_contributions": ["Integration of contextual and textual signals into embeddings", "Introduction of confidence and relatedness metrics for contextual weighting", "Demonstrated effectiveness on benchmark datasets"], "limitations": "", "keywords": ["knowledge graph embedding", "graph convolutional network", "contextual information"], "importance_score": 4, "read_time_minutes": 6}}
{"id": "2401.06772", "pdf": "https://arxiv.org/pdf/2401.06772.pdf", "abs": "https://arxiv.org/abs/2401.06772", "title": "Semantic Parsing for Question Answering over Knowledge Graphs", "authors": ["Sijia Wei", "Wenwen Zhang", "Qisong Li", "Jiang Zhao"], "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we propose a novel method for question answering over\nknowledge graphs based on graph-to-segment mapping, designed to improve the\nunderstanding of natural language questions. Our approach is grounded in\nsemantic parsing, a key technique for interpreting question utterances. The\nmain challenges arise from handling implicit entities and relations, as well as\ncomplex constraints such as temporal conditions, ordinality, and aggregation\nwithin the context of a knowledge graph. To address these issues, our framework\nintegrates both rule-based and neural methods to parse and construct accurate,\ncomprehensive semantic segment sequences. These sequences are then assembled\ninto semantic query graphs, providing precise representations of question\nutterances. We formulate question semantic parsing as a sequence generation\ntask, employing an encoder-decoder neural network to map natural language\nquestions into semantic segments. Furthermore, to enhance the identification of\nimplicit entities and relations, we incorporate a graph neural network that\nleverages knowledge graph context to enrich question representations.\nExperimental evaluations on two benchmark datasets demonstrate the\neffectiveness and superior performance of our model in semantic parsing for\nknowledge graph question answering.", "AI": {"tldr": "Novel method for question answering over knowledge graphs using graph-to-segment mapping and semantic parsing.", "motivation": "To improve the understanding of natural language questions in knowledge graphs by addressing implicit entities and complex constraints.", "method": "A framework that combines rule-based and neural methods, using an encoder-decoder neural network and a graph neural network to map questions into semantic segments and enrich representations.", "result": "Experimental evaluations show superior performance of the proposed model in semantic parsing compared to existing methods on benchmark datasets.", "conclusion": "The integrated approach significantly enhances the accuracy of question answering over knowledge graphs.", "key_contributions": ["Introduces a novel graph-to-segment mapping method for question answering.", "Combines rule-based and neural techniques for improved semantic parsing.", "Employs a graph neural network for enriched question representations."], "limitations": "", "keywords": ["knowledge graphs", "question answering", "semantic parsing"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2401.12989", "pdf": "https://arxiv.org/pdf/2401.12989.pdf", "abs": "https://arxiv.org/abs/2401.12989", "title": "Into the crossfire: evaluating the use of a language model to crowdsource gun violence reports", "authors": ["Adriano Belisario", "Scott A. Hale", "Luc Rocher"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Gun violence is a pressing human rights issue that affects nearly every\ndimension of the social fabric, from healthcare and education to psychology and\nthe economy. Reliable data on firearm events is paramount to developing more\neffective public policy and emergency responses. However, the lack of\ncomprehensive databases and the risks of in-person surveys prevent human rights\norganizations from collecting needed data in most countries. Here, we partner\nwith a Brazilian human rights organization to conduct a systematic evaluation\nof language models to assist with monitoring real-world firearm events from\nsocial media data. We propose a fine-tuned BERT-based model trained on Twitter\n(now X) texts to distinguish gun violence reports from ordinary Portuguese\ntexts. We then incorporate our model into a web application and test it in a\nlive intervention. We study and interview Brazilian analysts who continuously\ncheck social media texts to identify new gun violence events. Qualitative\nassessments show that our solution helped all analysts use their time more\nefficiently and expanded their search capacities. Quantitative assessments show\nthat the use of our model was associated with analysts having further\ninteractions with online users reporting gun violence. Our findings suggest\nthat human-centered interventions using language models can help support the\nwork of human rights organizations.", "AI": {"tldr": "This paper evaluates a BERT-based model for monitoring gun violence through social media data, showing its effectiveness in aiding human rights organizations.", "motivation": "Gun violence is a critical human rights issue, and there is a need for reliable data to improve public policy and emergency responses.", "method": "The study employed a fine-tuned BERT-based model trained on Twitter texts to identify gun violence reports from Portuguese texts, which was integrated into a web application for live testing.", "result": "The model improved analysts' efficiency in identifying gun violence incidents on social media and enhanced their interaction with users reporting such events.", "conclusion": "Human-centered interventions using language models can effectively support the efforts of human rights organizations in monitoring gun violence.", "key_contributions": ["A fine-tuned BERT model to detect gun violence reports in Portuguese texts.", "Implementation of the model in a web application for real-time analyses.", "Qualitative and quantitative evaluations confirming the model's utility for analysts."], "limitations": "", "keywords": ["gun violence", "language models", "social media", "human rights", "BERT"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2402.14533", "pdf": "https://arxiv.org/pdf/2402.14533.pdf", "abs": "https://arxiv.org/abs/2402.14533", "title": "Whose LLM is it Anyway? Linguistic Comparison and LLM Attribution for GPT-3.5, GPT-4 and Bard", "authors": ["Ariel Rosenfeld", "Teddy Lazebnik"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are capable of generating text that is similar\nto or surpasses human quality. However, it is unclear whether LLMs tend to\nexhibit distinctive linguistic styles akin to how human authors do. Through a\ncomprehensive linguistic analysis, we compare the vocabulary, Part-Of-Speech\n(POS) distribution, dependency distribution, and sentiment of texts generated\nby three of the most popular LLMS today (GPT-3.5, GPT-4, and Bard) to diverse\ninputs. The results point to significant linguistic variations which, in turn,\nenable us to attribute a given text to its LLM origin with a favorable 88\\%\naccuracy using a simple off-the-shelf classification model. Theoretical and\npractical implications of this intriguing finding are discussed.", "AI": {"tldr": "The paper investigates whether Large Language Models (LLMs) exhibit distinctive linguistic styles, comparing textual outputs from GPT-3.5, GPT-4, and Bard across various dimensions of linguistic analysis.", "motivation": "To understand if LLMs have unique linguistic styles similar to human authors and to explore implications of these findings.", "method": "A comprehensive linguistic analysis was conducted, comparing vocabulary, Part-Of-Speech (POS) distribution, dependency distribution, and sentiment of texts generated by GPT-3.5, GPT-4, and Bard.", "result": "The analysis revealed significant linguistic variations among the LLMs, allowing for attribution of generated texts to their respective LLM origins with 88% accuracy using a classification model.", "conclusion": "The linguistic differences identified suggest that LLMs not only generate high-quality texts but do so with distinct styles, which has both theoretical and practical implications.", "key_contributions": ["Identification of unique linguistic styles in LLM-generated texts.", "Demonstration of high accuracy in text attribution to specific LLMs.", "Exploration of implications for understanding LLM capabilities and ethical considerations."], "limitations": "", "keywords": ["Large Language Models", "linguistic analysis", "text attribution"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2404.07851", "pdf": "https://arxiv.org/pdf/2404.07851.pdf", "abs": "https://arxiv.org/abs/2404.07851", "title": "Guiding Large Language Models to Post-Edit Machine Translation with Error Annotations", "authors": ["Dayeon Ki", "Marine Carpuat"], "categories": ["cs.CL", "cs.AI"], "comment": "NAACL 2024 Findings", "summary": "Machine Translation (MT) remains one of the last NLP tasks where large\nlanguage models (LLMs) have not yet replaced dedicated supervised systems. This\nwork exploits the complementary strengths of LLMs and supervised MT by guiding\nLLMs to automatically post-edit MT with external feedback on its quality,\nderived from Multidimensional Quality Metric (MQM) annotations. Working with\nLLaMA-2 models, we consider prompting strategies varying the nature of feedback\nprovided and then fine-tune the LLM to improve its ability to exploit the\nprovided guidance. Through experiments on Chinese-English, English-German, and\nEnglish-Russian MQM data, we demonstrate that prompting LLMs to post-edit MT\nimproves TER, BLEU and COMET scores, although the benefits of fine-grained\nfeedback are not clear. Fine-tuning helps integrate fine-grained feedback more\neffectively and further improves translation quality based on both automatic\nand human evaluation.", "AI": {"tldr": "This study explores enhancing Machine Translation (MT) by guiding Large Language Models (LLMs) to post-edit MT using feedback from Multidimensional Quality Metric (MQM) annotations, demonstrating improvements in translation quality across several language pairs.", "motivation": "Despite advancements in NLP, MT still relies on supervised systems; this paper investigates integrating LLMs to improve MT quality.", "method": "The authors employed prompting strategies with LLaMA-2 models and fine-tuning to enhance LLMs' ability to post-edit MT based on quality feedback derived from MQM annotations.", "result": "Experiments show that LLMs directed to post-edit MT yield improved scores in TER, BLEU, and COMET, with fine-tuning leading to better integration of fine-grained feedback, although the specific benefits of detailed feedback were ambiguous.", "conclusion": "LLMs can be effectively utilized for post-editing MT when guided by external quality feedback, enhancing translation quality as evidenced by various evaluation metrics.", "key_contributions": ["Integration of LLMs in MT post-editing using feedback from MQM annotations", "Demonstration of improved translation quality across multiple language pairs", "Evaluation of fine-tuning strategies in enhancing LLM performance in MT tasks"], "limitations": "", "keywords": ["Machine Translation", "Large Language Models", "Quality Metrics", "Post-editing", "Fine-tuning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2405.13923", "pdf": "https://arxiv.org/pdf/2405.13923.pdf", "abs": "https://arxiv.org/abs/2405.13923", "title": "Why Not Transform Chat Large Language Models to Non-English?", "authors": ["Xiang Geng", "Ming Zhu", "Jiahuan Li", "Zhejian Lai", "Wei Zou", "Shuaijie She", "Jiaxin Guo", "Xiaofeng Zhao", "Yinglu Li", "Yuang Li", "Chang Su", "Yanqing Zhao", "Xinglin Lyu", "Min Zhang", "Jiajun Chen", "Hao Yang", "Shujian Huang"], "categories": ["cs.CL"], "comment": "The article has been accepted by Frontiers of Computer Science (FCS),\n  with the DOI: {10.1007/s11704-025-50646-z}", "summary": "The scarcity of non-English data limits the development of non-English large\nlanguage models (LLMs). Transforming English-centric LLMs to non-English has\nbeen identified as an effective and resource-efficient method. Previous works\nstart from base LLMs and perform knowledge distillation (KD) with data\ngenerated by stronger LLMs, e.g. GPT-4. Compared to base LLMs, chat LLMs are\nfurther optimized for advanced abilities, e.g. multi-turn conversation and\nhuman preference alignment, and thus more powerful in both helpfulness and\nsafety. However, transforming a chat LLM involves two critical issues: (1) How\ncan we effectively transfer advanced abilities without their supervised data?\n(2) How can we prevent the original knowledge from catastrophic forgetting\nduring transformation? We target these issues by introducing a simple framework\ncalled TransLLM. For the first issue, TransLLM divides the transfer problem\ninto some common sub-tasks with the translation chain-of-thought, which uses\nthe translation as the bridge between English and non-English step-by-step. We\nfurther enhance the performance of sub-tasks with publicly available data. For\nthe second issue, we propose a method comprising two synergistic components:\nlow-rank adaptation for training to maintain the original LLM parameters, and\nrecovery KD, which utilizes data generated by the chat LLM itself to recover\nthe original knowledge from the frozen parameters. In the experiments, we\ntransform the LLaMA-2-chat-7B to the Thai language. Our method, using only\nsingle-turn data, outperforms strong baselines and ChatGPT on multi-turn\nbenchmark MT-bench. Furthermore, our method, without safety data, rejects more\nharmful queries of safety benchmark AdvBench than both ChatGPT and GPT-4. Code\nis available at https://github.com/hy5468/TransLLM.", "AI": {"tldr": "This paper presents TransLLM, a framework for transforming English-centric chat LLMs into non-English models while addressing issues of knowledge retention and capability transfer.", "motivation": "The scarcity of non-English training data limits the development of non-English large language models, necessitating effective methods for transforming existing English LLMs.", "method": "TransLLM divides the transformation process into common sub-tasks using a translation chain-of-thought approach, and employs low-rank adaptation and recovery knowledge distillation to maintain original knowledge while enabling the transfer of advanced capabilities.", "result": "TransLLM outperforms strong baselines and ChatGPT in transforming LLaMA-2-chat-7B to Thai, excelling in multi-turn scenario benchmarks and safety benchmarks without needing additional safety data.", "conclusion": "TransLLM effectively transforms LLMs into non-English versions while preserving their advanced abilities and mitigating the risk of knowledge loss.", "key_contributions": ["Introduction of the TransLLM framework for non-English LLM development.", "Utilization of the translation chain-of-thought to bridge language barriers in LLM transfer.", "Methods to maintain original knowledge in transformed LLMs through low-rank adaptation and recovery KD."], "limitations": "", "keywords": ["large language models", "knowledge distillation", "multilingual models"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2406.11614", "pdf": "https://arxiv.org/pdf/2406.11614.pdf", "abs": "https://arxiv.org/abs/2406.11614", "title": "Intrinsic Test of Unlearning Using Parametric Knowledge Traces", "authors": ["Yihuai Hong", "Lei Yu", "Haiqin Yang", "Shauli Ravfogel", "Mor Geva"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted in EMNLP 2025 Main", "summary": "The task of \"unlearning\" certain concepts in large language models (LLMs) has\nattracted immense attention recently, due to its importance in mitigating\nundesirable model behaviours, such as the generation of harmful, private, or\nincorrect information. Current protocols to evaluate unlearning methods largely\nrely on behavioral tests, without monitoring the presence of unlearned\nknowledge within the model's parameters. This residual knowledge can be\nadversarially exploited to recover the erased information post-unlearning. We\nargue that unlearning should also be evaluated internally, by considering\nchanges in the parametric knowledge traces of the unlearned concepts. To this\nend, we propose a general evaluation methodology that leverages vocabulary\nprojections to inspect concepts encoded in model parameters. We use this\napproach to localize \"concept vectors\" - parameter vectors that encode concrete\nconcepts - and construct ConceptVectors, a benchmark dataset containing\nhundreds of common concepts and their parametric knowledge traces within two\nopen-source LLMs. Evaluation on ConceptVectors shows that existing unlearning\nmethods minimally impact concept vectors and mostly suppress them during\ninference, while directly ablating these vectors demonstrably removes the\nassociated knowledge and significantly reduces the model's susceptibility to\nadversarial manipulation. Our results highlight limitations in behavioral-based\nunlearning evaluations and call for future work to include parameter-based\nevaluations. To support this, we release our code and benchmark at\nhttps://github.com/yihuaihong/ConceptVectors.", "AI": {"tldr": "This paper proposes a new methodology for evaluating unlearning in large language models (LLMs) by analyzing parameter changes, introducing a benchmark dataset called ConceptVectors.", "motivation": "To address the inadequacy of current unlearning evaluations that rely solely on behavioral tests, which may leave residual knowledge exploitable by adversaries.", "method": "The proposed methodology utilizes vocabulary projections to examine parametric knowledge traces related to unlearned concepts in LLMs, identifying and localizing 'concept vectors'.", "result": "Evaluation on the generated ConceptVectors dataset reveals that existing unlearning methods have limited effectiveness on concept vectors and primarily suppress them during model inference, while direct ablation of these vectors effectively removes knowledge and reduces model vulnerability to adversarial attacks.", "conclusion": "The study calls for a paradigm shift in unlearning evaluation practices by incorporating parameter-based assessments alongside behavioral tests, emphasizing the necessity for robust methodologies in addressing unlearning challenges.", "key_contributions": ["Introduction of a new evaluation methodology for unlearning in LLMs.", "Release of ConceptVectors benchmark dataset for assessing parametric knowledge traces in LLMs.", "Demonstration of limitations in traditional unlearning evaluations based on behavior."], "limitations": "The study focuses on only two open-source LLMs, limiting the generalizability of results to other models.", "keywords": ["unlearning", "large language models", "concept vectors", "evaluation methodology", "adversarial manipulation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2408.10722", "pdf": "https://arxiv.org/pdf/2408.10722.pdf", "abs": "https://arxiv.org/abs/2408.10722", "title": "MEGen: Generative Backdoor into Large Language Models via Model Editing", "authors": ["Jiyang Qiu", "Xinbei Ma", "Zhuosheng Zhang", "Hai Zhao", "Yun Li", "Qianren Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings", "summary": "Large language models (LLMs) have exhibited remarkable versatility and\nadaptability, while their widespread adoption across various applications also\nraises critical safety concerns. This paper focuses on the impact of backdoored\nLLMs. Traditional backdoor injection methods are primarily limited to yes-or-no\ndiscriminative tasks, leading users to underestimate the potential risks of\nbackdoored LLMs. Given the inherently generative nature of LLMs, this paper\nreveals that a generative backdoor injected into LLMs can expose the true\nsafety risks in their applications. We propose an editing-based generative\nbackdoor, named MEGen, aiming to expand the backdoor to generative tasks in a\nunified format of any text-to any text, leading to natural generations with a\nspecific intention. Experiments show that MEGen achieves a high attack success\nrate by adjusting only a small set of local parameters with few-shot samples.\nNotably, we show that the backdoored model, when triggered, can freely output\npre-set dangerous information while completing downstream tasks. Our work\nhighlights that MEGen enables backdoors in LLMs to exhibit generative\ncapabilities, causing potential safety risks by altering the generative style.\nThe code is available at https://github.com/MonoQ-hub/MEGen.", "AI": {"tldr": "The paper discusses the discovery of generative backdoors in large language models (LLMs) and proposes a method to inject these backdoors, which can compromise safety by outputting harmful information during normal tasks.", "motivation": "To address safety concerns regarding the use of backdoored LLMs, particularly in generative applications where traditional methods fall short.", "method": "The authors propose a novel editing-based generative backdoor named MEGen, which allows for backdoor injections that influence generative tasks more broadly than traditional yes-or-no discriminative tasks.", "result": "Experiments demonstrate that MEGen achieves a high attack success rate with minimal adjustments to local parameters and few-shot samples, enabling the backdoored model to output dangerous information while performing downstream tasks.", "conclusion": "The work illustrates the potential safety risks of generative backdoors in LLMs, emphasizing the need for attention to these capabilities in their applications.", "key_contributions": ["Introduction of MEGen as a method for injecting generative backdoors into LLMs.", "Demonstration of backdoored LLMs' ability to produce controlled harmful outputs during generative tasks.", "Highlighting safety implications of generative capabilities in LLMs."], "limitations": "The study is primarily focused on the design and effectiveness of the backdoor, with limited exploration of countermeasures or mitigation strategies.", "keywords": ["Large Language Models", "Generative Backdoors", "Safety Risks", "NLP", "MEGen"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2409.10038", "pdf": "https://arxiv.org/pdf/2409.10038.pdf", "abs": "https://arxiv.org/abs/2409.10038", "title": "On the Diagram of Thought", "authors": ["Yifan Zhang", "Yang Yuan", "Andrew Chi-Chih Yao"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "31 pages", "summary": "Large Language Models (LLMs) excel at many tasks but often falter on complex\nproblems that require structured, multi-step reasoning. We introduce the\nDiagram of Thought (DoT), a new framework that enables a single LLM to build\nand navigate a mental map of its reasoning. Instead of thinking in a straight\nline, the model constructs a dynamic diagram of ideas, where it can propose\ndifferent lines of thought, critique its own steps, and synthesize validated\ninsights into a final conclusion. This entire process is self-contained within\nthe model, making it highly efficient by avoiding the complex external\ncontrollers or search algorithms required by other methods. To ensure the\nreliability of this process, we ground DoT in a rigorous mathematical framework\nfrom category theory. This foundation guarantees that the way the model\ncombines information is logical, consistent, and robust, regardless of the\norder in which ideas were explored. The result is a more powerful and\ntransparent reasoning process that produces a fully auditable, step-by-step\ntrace of the LLM's thinking, bridging the gap between fluent language and\nformal reasoning.", "AI": {"tldr": "This paper introduces the Diagram of Thought (DoT), a framework for enhancing LLM reasoning through structured, multi-step processes, grounded in category theory.", "motivation": "Address the limitations of Large Language Models in handling complex problems that require structured reasoning.", "method": "The framework allows LLMs to create a dynamic mental map of reasoning, proposing various thoughts, critiquing steps, and synthesizing insights.", "result": "The DoT framework results in a more efficient and transparent reasoning process, producing an auditable trace of the model's thinking.", "conclusion": "By grounding the DoT in category theory, the model achieves logical and robust information synthesis, enabling improved reasoning capabilities.", "key_contributions": ["Introduction of the Diagram of Thought framework for LLMs", "Grounding in category theory for logical consistency", "Enhanced transparency and auditability of LLM reasoning processes."], "limitations": "", "keywords": ["Large Language Models", "Diagram of Thought", "Reasoning Framework"], "importance_score": 8, "read_time_minutes": 31}}
{"id": "2409.15664", "pdf": "https://arxiv.org/pdf/2409.15664.pdf", "abs": "https://arxiv.org/abs/2409.15664", "title": "Mitigating Semantic Leakage in Cross-lingual Embeddings via Orthogonality Constraint", "authors": ["Dayeon Ki", "Cheonbok Park", "Hyunjoong Kim"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2024 RepL4NLP Workshop", "summary": "Accurately aligning contextual representations in cross-lingual sentence\nembeddings is key for effective parallel data mining. A common strategy for\nachieving this alignment involves disentangling semantics and language in\nsentence embeddings derived from multilingual pre-trained models. However, we\ndiscover that current disentangled representation learning methods suffer from\nsemantic leakage - a term we introduce to describe when a substantial amount of\nlanguage-specific information is unintentionally leaked into semantic\nrepresentations. This hinders the effective disentanglement of semantic and\nlanguage representations, making it difficult to retrieve embeddings that\ndistinctively represent the meaning of the sentence. To address this challenge,\nwe propose a novel training objective, ORthogonAlity Constraint LEarning\n(ORACLE), tailored to enforce orthogonality between semantic and language\nembeddings. ORACLE builds upon two components: intra-class clustering and\ninter-class separation. Through experiments on cross-lingual retrieval and\nsemantic textual similarity tasks, we demonstrate that training with the ORACLE\nobjective effectively reduces semantic leakage and enhances semantic alignment\nwithin the embedding space.", "AI": {"tldr": "This paper introduces ORACLE, a novel training objective to address semantic leakage in multi-lingual sentence embeddings, enhancing cross-lingual data alignment.", "motivation": "The need for effective parallel data mining relies heavily on accurate alignment of contextual representations in cross-lingual sentence embeddings, which current methods struggle with due to semantic leakage.", "method": "The paper proposes ORthogonAlity Constraint LEarning (ORACLE), which enforces orthogonality between semantic and language embeddings through intra-class clustering and inter-class separation techniques.", "result": "Experimental results show that ORACLE significantly reduces semantic leakage and improves semantic alignment in the embedding space for both cross-lingual retrieval and semantic textual similarity tasks.", "conclusion": "ORACLE represents a substantial improvement in disentangling semantic and language information in sentence embeddings, facilitating better performance in multilingual applications.", "key_contributions": ["Introduction of the concept of semantic leakage in multilingual embeddings", "Development of the ORACLE objective for better alignment", "Demonstration of effectiveness through specific experimental tasks"], "limitations": "The effectiveness of ORACLE may vary across different languages and contexts; further testing is needed.", "keywords": ["cross-lingual embeddings", "semantic leakage", "multilingual models", "ORACLE", "disentangled representation"], "importance_score": 6, "read_time_minutes": 8}}
{"id": "2410.12341", "pdf": "https://arxiv.org/pdf/2410.12341.pdf", "abs": "https://arxiv.org/abs/2410.12341", "title": "Learning by Surprise: Surplexity for Mitigating Model Collapse in Generative AI", "authors": ["Daniele Gambetta", "Gizem Gezici", "Fosca Giannotti", "Dino Pedreschi", "Alistair Knott", "Luca Pappalardo"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As synthetic content increasingly infiltrates the web, generative AI models\nmay be retrained on their own outputs: a process termed \"autophagy\". This leads\nto model collapse: a progressive loss of performance and diversity across\ngenerations. Recent studies have examined the emergence of model collapse\nacross various generative AI models and data types, and have proposed\nmitigation strategies that rely on incorporating human-authored content.\nHowever, current characterizations of model collapse remain limited, and\nexisting mitigation methods assume reliable knowledge of whether training data\nis human-authored or AI-generated. In this paper, we address these gaps by\nintroducing new measures that characterise collapse directly from a model's\nnext-token probability distributions, rather than from properties of\nAI-generated text. Using these measures, we show that the degree of collapse\ndepends on the complexity of the initial training set, as well as on the extent\nof autophagy. Our experiments prompt a new suggestion: that model collapse\noccurs when a model trains on data that does not \"surprise\" it. We express this\nhypothesis in terms of the well-known Free Energy Principle in cognitive\nscience. Building on this insight, we propose a practical mitigation strategy:\nfiltering training items by high surplexity, maximising the surprise of the\nmodel. Unlike existing methods, this approach does not require distinguishing\nbetween human- and AI-generated data. Experiments across datasets and models\ndemonstrate that our strategy is at least as effective as human-data baselines,\nand even more effective in reducing distributional skewedness. Our results\nprovide a richer understanding of model collapse and point toward more\nresilient approaches for training generative AI systems in environments\nincreasingly saturated with synthetic data.", "AI": {"tldr": "This paper introduces new measures to characterize model collapse in generative AI models and proposes a surprisingness-based mitigation strategy that does not require distinguishing between human- and AI-generated data.", "motivation": "To address the limitations in understanding model collapse caused by retraining generative AI models on their outputs and the reliance on human-authored content for mitigation strategies.", "method": "The authors introduce measures that characterize model collapse using next-token probability distributions from models, rather than relying on characteristics of the generated text. They propose filtering training items by high surprise (surplexity) to mitigate collapse.", "result": "Experiments show that the proposed surprise-based filtering is as effective as human-data baselines in mitigating model collapse and more effective in dealing with distributional skewedness.", "conclusion": "The study provides a new understanding of model collapse and suggests more robust methods for training generative AI systems in an environment filled with synthetic data.", "key_contributions": ["Introduced measures to characterize model collapse based on next-token probability distributions.", "Proposed a surprise-based mitigation strategy that does not require distinguishing data types.", "Demonstrated effectiveness of the new strategy in several experiments."], "limitations": "", "keywords": ["model collapse", "generative AI", "training data", "surplexity", "autophagy"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2410.13258", "pdf": "https://arxiv.org/pdf/2410.13258.pdf", "abs": "https://arxiv.org/abs/2410.13258", "title": "How Does Knowledge Selection Help Retrieval Augmented Generation?", "authors": ["Xiangci Li", "Jessica Ouyang"], "categories": ["cs.CL"], "comment": "Accepted by Findings of EMNLP 2025", "summary": "Retrieval-augmented generation (RAG) is a powerful method for enhancing\nnatural language generation by integrating external knowledge into a model's\noutput. While prior work has demonstrated the importance of improving knowledge\nretrieval for boosting generation quality, the role of knowledge selection,\na.k.a. reranking or filtering, remains less clear. This paper empirically\nanalyzes how knowledge selection influences downstream generation performance\nin RAG systems. By simulating different retrieval and selection conditions\nthrough a controlled mixture of gold and distractor knowledge, we assess the\nimpact of these factors on generation outcomes. Our findings indicate that the\ndownstream generator model's capability, as well as the complexity of the task\nand dataset, significantly influence the impact of knowledge selection on the\noverall RAG system performance. In typical scenarios, improving the knowledge\nrecall score is key to enhancing generation outcomes, with the knowledge\nselector providing limited benefit when a strong generator model is used on\nclear, well-defined tasks. For weaker generator models or more ambiguous tasks\nand datasets, the knowledge F1 score becomes a critical factor, and the\nknowledge selector plays a more prominent role in improving overall\nperformance.", "AI": {"tldr": "This paper analyzes how knowledge selection affects generation performance in retrieval-augmented generation (RAG) systems, revealing its varying significance based on the generator model and task complexity.", "motivation": "To understand the influence of knowledge selection on the performance of RAG systems, particularly its role compared to knowledge retrieval in natural language generation.", "method": "Empirical analysis through controlled experiments involving different retrieval and selection conditions, mixing gold and distractor knowledge to evaluate their impact on generation performance.", "result": "The findings show that the generator model's strength and task complexity significantly impact the importance of knowledge selection; improving knowledge recall is crucial for high-performance scenarios, while knowledge F1 score is essential for weaker models and ambiguous tasks.", "conclusion": "Knowledge selection is critical in specific contexts, particularly involving weaker generator models and complex tasks, suggesting that enhancing knowledge recall can improve outcomes in RAG systems.", "key_contributions": ["Empirical analysis of knowledge selection in RAG systems", "Identification of conditions under which knowledge selection is vital", "Demonstration of varying impact of knowledge recall and F1 scores based on task complexity"], "limitations": "", "keywords": ["Retrieval-augmented generation", "Knowledge selection", "Natural language generation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2410.18798", "pdf": "https://arxiv.org/pdf/2410.18798.pdf", "abs": "https://arxiv.org/abs/2410.18798", "title": "Distill Visual Chart Reasoning Ability from LLMs to MLLMs", "authors": ["Wei He", "Zhiheng Xi", "Wanxu Zhao", "Xiaoran Fan", "Yiwen Ding", "Zifei Shan", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025 Findings. The code and dataset are publicly\n  available at https://github.com/hewei2001/ReachQA", "summary": "Solving complex chart Q&A tasks requires advanced visual reasoning abilities\nin multimodal large language models (MLLMs), including recognizing key\ninformation from visual inputs and conducting reasoning over it. While\nfine-tuning MLLMs for reasoning is critical, collecting and annotating charts\nand questions is expensive, hard to scale, and often results in low-quality\nannotations. To address this, we propose Code-as-Intermediary Translation\n(CIT), a cost-effective, efficient and scalable data synthesis method for\ndistilling visual reasoning abilities from LLMs to MLLMs. The code serves as an\nintermediary that translates visual chart representations into textual\nrepresentations, enabling language models to understand cross-modal information\nand generate reasoning chains accordingly. In this way, we can employ\ntext-based synthesizing techniques to expand chart-plotting code and generate\nhigh-quality Q&A pairs for training models. This produces ReachQA, a dataset\ncontaining 3k reasoning-intensive charts and 20k Q&A pairs to enhance both\nrecognition and reasoning abilities of MLLMs. Experiments show that models\nfine-tuned with ReachQA not only perform well on chart-related tasks but also\nshow performance gains on general reasoning benchmarks. The code and dataset\nare publicly available at https://github.com/hewei2001/ReachQA.", "AI": {"tldr": "Proposes a method for synthesizing training data for multimodal large language models focused on visual reasoning through a dataset called ReachQA.", "motivation": "To solve complex chart Q&A tasks using MLLMs with enhanced visual reasoning capabilities while addressing the challenges of collecting and annotating training data.", "method": "The Code-as-Intermediary Translation (CIT) method translates visual chart representations into textual representations to generate high-quality Q&A pairs for training.", "result": "ReachQA dataset includes 3k reasoning-intensive charts and 20k Q&A pairs, improving MLLMs' performance on chart tasks and general reasoning benchmarks.", "conclusion": "Fine-tuning models with the ReachQA dataset results in significant performance improvements for both specialized and general reasoning tasks.", "key_contributions": ["Introduction of Code-as-Intermediary Translation (CIT) for data synthesis", "Creation of the ReachQA dataset with extensive visual reasoning Q&A pairs", "Demonstration of enhanced MLLM performance on both chart and general reasoning tasks"], "limitations": "", "keywords": ["multimodal large language models", "visual reasoning", "data synthesis"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2411.12142", "pdf": "https://arxiv.org/pdf/2411.12142.pdf", "abs": "https://arxiv.org/abs/2411.12142", "title": "A Computational Method for Measuring \"Open Codes\" in Qualitative Analysis", "authors": ["John Chen", "Alexandros Lotsos", "Sihan Cheng", "Caiyi Wang", "Lexie Zhao", "Jessica Hullman", "Bruce Sherin", "Uri Wilensky", "Michael Horn"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "Qualitative analysis is critical to understanding human datasets in many\nsocial science disciplines. A central method in this process is inductive\ncoding, where researchers identify and interpret codes directly from the\ndatasets themselves. Yet, this exploratory approach poses challenges for\nmeeting methodological expectations (such as ``depth'' and ``variation''),\nespecially as researchers increasingly adopt Generative AI (GAI) for support.\nGround-truth-based metrics are insufficient because they contradict the\nexploratory nature of inductive coding, while manual evaluation can be\nlabor-intensive. This paper presents a theory-informed computational method for\nmeasuring inductive coding results from humans and GAI. Our method first merges\nindividual codebooks using an LLM-enriched algorithm. It measures each coder's\ncontribution against the merged result using four novel metrics: Coverage,\nOverlap, Novelty, and Divergence. Through two experiments on a human-coded\nonline conversation dataset, we 1) reveal the merging algorithm's impact on\nmetrics; 2) validate the metrics' stability and robustness across multiple runs\nand different LLMs; and 3) showcase the metrics' ability to diagnose coding\nissues, such as excessive or irrelevant (hallucinated) codes. Our work provides\na reliable pathway for ensuring methodological rigor in human-AI qualitative\nanalysis.", "AI": {"tldr": "This paper presents a novel computational method to assess the results of inductive coding when combining human and Generative AI contributions.", "motivation": "To address the challenges faced in qualitative analysis when using inductive coding, particularly with Generative AI, by developing a reliable measurement method.", "method": "The proposed method merges individual codebooks via an LLM-enhanced algorithm and measures contributions using four metrics: Coverage, Overlap, Novelty, and Divergence.", "result": "Experiments reveal the impact of merging algorithms on metric outcomes, demonstrate the stability and robustness of the metrics, and illustrate their diagnostic capabilities for identifying issues in coding.", "conclusion": "The framework offers a reliable means to uphold methodological rigor in qualitative analysis that involves both human and AI inputs.", "key_contributions": ["Development of a new method for assessing inductive coding", "Introduction of four novel metrics for evaluation", "Demonstration of diagnostic capabilities for coding issues"], "limitations": "", "keywords": ["Qualitative Analysis", "Inductive Coding", "Generative AI", "Human-AI Interaction", "Methodological Rigor"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2412.03679", "pdf": "https://arxiv.org/pdf/2412.03679.pdf", "abs": "https://arxiv.org/abs/2412.03679", "title": "Evaluating Language Models as Synthetic Data Generators", "authors": ["Seungone Kim", "Juyoung Suk", "Xiang Yue", "Vijay Viswanathan", "Seongyun Lee", "Yizhong Wang", "Kiril Gashteovski", "Carolin Lawrence", "Sean Welleck", "Graham Neubig"], "categories": ["cs.CL"], "comment": "ACL 2025 (main)", "summary": "Given the increasing use of synthetic data in language model (LM)\npost-training, an LM's ability to generate high-quality data has become nearly\nas crucial as its ability to solve problems directly. While prior works have\nfocused on developing effective data generation methods, they lack systematic\ncomparison of different LMs as data generators in a unified setting. To address\nthis gap, we propose AgoraBench, a benchmark that provides standardized\nsettings and metrics to evaluate LMs' data generation abilities. Through\nsynthesizing 1.26 million training instances using 6 LMs and training 99\nstudent models, we uncover key insights about LMs' data generation\ncapabilities. First, we observe that LMs exhibit distinct strengths. For\ninstance, GPT-4o excels at generating new problems, while Claude-3.5-Sonnet\nperforms better at enhancing existing ones. Furthermore, our analysis reveals\nthat an LM's data generation ability doesn't necessarily correlate with its\nproblem-solving ability. Instead, multiple intrinsic features of data\nquality-including response quality, perplexity, and instruction\ndifficulty-collectively serve as better indicators. Finally, we demonstrate\nthat strategic choices in output format and cost-conscious model selection\nsignificantly impact data generation effectiveness.", "AI": {"tldr": "This paper introduces AgoraBench, a benchmark for evaluating language models' data generation capabilities in a systematic manner.", "motivation": "There is a growing need to understand and compare the ability of language models to generate high-quality synthetic data, especially as this ability becomes increasingly important for their application in various tasks.", "method": "The authors propose AgoraBench, which standardizes settings and metrics to evaluate language models as synthetic data generators. They synthesize 1.26 million training instances using 6 different LMs and subsequently train 99 student models to analyze performance.", "result": "The study reveals distinct strengths among different language models: GPT-4o excels at creating new problems, while Claude-3.5-Sonnet is superior at enhancing existing ones. Additionally, the correlation between data generation ability and problem-solving capability is weak, suggesting that intrinsic features like response quality and instruction difficulty are better indicators of data quality.", "conclusion": "Strategic choices in output format and model selection are crucial for optimizing the effectiveness of data generation, highlighting the multifaceted nature of language models' capabilities.", "key_contributions": ["Introduction of AgoraBench for systematic evaluation of LMs' data generation abilities", "Comparison of different language models revealing their unique strengths", "Insight into the relationship between data generation capability and problem-solving ability of LMs."], "limitations": "", "keywords": ["synthetic data", "language models", "data generation", "benchmark", "machine learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2412.04497", "pdf": "https://arxiv.org/pdf/2412.04497.pdf", "abs": "https://arxiv.org/abs/2412.04497", "title": "Opportunities and Challenges of Large Language Models for Low-Resource Languages in Humanities Research", "authors": ["Tianyang Zhong", "Zhenyuan Yang", "Zhengliang Liu", "Ruidong Zhang", "Yiheng Liu", "Haiyang Sun", "Yi Pan", "Yiwei Li", "Yifan Zhou", "Hanqi Jiang", "Junhao Chen", "Tianming Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Low-resource languages serve as invaluable repositories of human history,\nembodying cultural evolution and intellectual diversity. Despite their\nsignificance, these languages face critical challenges, including data scarcity\nand technological limitations, which hinder their comprehensive study and\npreservation. Recent advancements in large language models (LLMs) offer\ntransformative opportunities for addressing these challenges, enabling\ninnovative methodologies in linguistic, historical, and cultural research. This\nstudy systematically evaluates the applications of LLMs in low-resource\nlanguage research, encompassing linguistic variation, historical documentation,\ncultural expressions, and literary analysis. By analyzing technical frameworks,\ncurrent methodologies, and ethical considerations, this paper identifies key\nchallenges such as data accessibility, model adaptability, and cultural\nsensitivity. Given the cultural, historical, and linguistic richness inherent\nin low-resource languages, this work emphasizes interdisciplinary collaboration\nand the development of customized models as promising avenues for advancing\nresearch in this domain. By underscoring the potential of integrating\nartificial intelligence with the humanities to preserve and study humanity's\nlinguistic and cultural heritage, this study fosters global efforts towards\nsafeguarding intellectual diversity.", "AI": {"tldr": "This paper evaluates the application of LLMs in researching low-resource languages, focusing on linguistic, historical, and cultural studies while addressing challenges like data accessibility and cultural sensitivity.", "motivation": "To highlight the significance of low-resource languages as repositories of human history and the impact of LLMs in their preservation and study.", "method": "Systematic evaluation of LLM applications in low-resource language research, including analysis of technical frameworks, methodologies, and ethical considerations.", "result": "Identification of key challenges such as data accessibility, model adaptability, and the need for cultural sensitivity in the application of LLMs.", "conclusion": "Interdisciplinary collaboration and customized models are essential for advancing low-resource language research and preserving global linguistic diversity.", "key_contributions": ["Systematic analysis of LLM applications in low-resource languages", "Identification of key challenges in utilizing LLMs for linguistic and cultural research", "Emphasis on interdisciplinary approaches for research advancement"], "limitations": "", "keywords": ["low-resource languages", "large language models", "cultural preservation", "linguistic diversity", "ethical considerations"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2412.09318", "pdf": "https://arxiv.org/pdf/2412.09318.pdf", "abs": "https://arxiv.org/abs/2412.09318", "title": "Benchmarking LLMs for Mimicking Child-Caregiver Language in Interaction", "authors": ["Jing Liu", "Abdellah Fourtassi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "LLMs can generate human-like dialogues, yet their ability to simulate early\nchild-adult interactions remains largely unexplored. In this paper, we examined\nhow effectively LLMs can capture the distinctive features of child-caregiver\nlanguage in interaction, using both static and interactive benchmarking\nmethods. We found that state-of-the-art LLMs like Llama 3 and GPT-4o can\napproximate child-caregiver dialogues at the word and utterance level, but they\nstruggle to reproduce the child and caregiver's discursive patterns, exaggerate\nalignment, and fail to reach the level of diversity shown by humans. The\nbroader goal of this work is to initiate the development of a comprehensive\nbenchmark for LLMs in child-oriented applications.", "AI": {"tldr": "This paper investigates the ability of LLMs to simulate child-caregiver interactions, revealing strengths in approximating dialogue but limitations in diversity and discursive patterns.", "motivation": "To explore how effectively LLMs can replicate the distinctive language features found in child-caregiver interactions and to inform the development of benchmarks for child-oriented applications.", "method": "The researchers employed both static and interactive benchmarking methods to evaluate the performance of state-of-the-art LLMs like Llama 3 and GPT-4o in simulating child-caregiver dialogues.", "result": "LLMs can match dialogues at the word and utterance level, but they struggle with the discursive patterns and diversity present in human interactions, indicating significant areas for improvement.", "conclusion": "While LLMs show promise in generating child-caregiver dialogues, their current limitations suggest a need for further developments to create robust benchmarks for assessing LLM performance in child-oriented applications.", "key_contributions": ["Introduction of static and interactive benchmarking for LLMs in child-caregiver dialogue simulation.", "Identification of specific limitations in existing LLM performance regarding discourse patterns and linguistic diversity.", "Aiming to inform the development of comprehensive benchmarks for evaluating LLMs in child-oriented interactions."], "limitations": "The study indicates that LLMs exaggerate alignment in dialogues and do not achieve the diversity of human interactions.", "keywords": ["LLMs", "child-caregiver interactions", "benchmarking", "language simulation", "discourse patterns"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2412.12928", "pdf": "https://arxiv.org/pdf/2412.12928.pdf", "abs": "https://arxiv.org/abs/2412.12928", "title": "Truthful Text Sanitization Guided by Inference Attacks", "authors": ["Ildikó Pilán", "Benet Manzanares-Salor", "David Sánchez", "Pierre Lison"], "categories": ["cs.CL"], "comment": null, "summary": "Text sanitization aims to rewrite parts of a document to prevent disclosure\nof personal information. The central challenge of text sanitization is to\nstrike a balance between privacy protection (avoiding the leakage of personal\ninformation) and utility preservation (retaining as much as possible of the\ndocument's original content). To this end, we introduce a novel text\nsanitization method based on generalizations, that is, broader but still\ninformative terms that subsume the semantic content of the original text spans.\nThe approach relies on the use of instruction-tuned large language models\n(LLMs) and is divided into two stages. Given a document including text spans\nexpressing personally identifiable information (PII), the LLM is first applied\nto obtain truth-preserving replacement candidates for each text span and rank\nthose according to their abstraction level. Those candidates are then evaluated\nfor their ability to protect privacy by conducting inference attacks with the\nLLM. Finally, the system selects the most informative replacement candidate\nshown to be resistant to those attacks. This two-stage process produces\nreplacements that effectively balance privacy and utility.\n  We also present novel metrics to evaluate these two aspects without needing\nto manually annotate documents. Results on the Text Anonymization Benchmark\nshow that the proposed approach, implemented with Mistral 7B Instruct, leads to\nenhanced utility, with only a marginal (< 1 p.p.) increase in re-identification\nrisk compared to fully suppressing the original spans. Furthermore, our\napproach is shown to be more truth-preserving than existing methods such as\nMicrosoft Presidio's synthetic replacements.", "AI": {"tldr": "This paper presents a novel text sanitization method using LLMs to replace personally identifiable information while balancing privacy and utility.", "motivation": "To address the challenge of text sanitization, which requires both preventing the disclosure of personal information and preserving document utility.", "method": "The method involves a two-stage process that uses instruction-tuned LLMs to find and rank truth-preserving replacement candidates for PII, followed by evaluating their effectiveness against privacy attacks.", "result": "The proposed method achieves improved utility with only a marginal increase in re-identification risk compared to conventional suppression methods, and it is more truth-preserving than existing approaches like Microsoft Presidio.", "conclusion": "This novel approach is effective in balancing privacy protection and utility, offering a practical solution for text sanitization.", "key_contributions": ["Introduction of a two-stage text sanitization process using LLMs", "Development of novel metrics for evaluating privacy and utility", "Demonstration of improved performance over existing methods in preserving truthfulness of text."], "limitations": "", "keywords": ["text sanitization", "personal information", "large language models", "privacy", "utility"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2412.16556", "pdf": "https://arxiv.org/pdf/2412.16556.pdf", "abs": "https://arxiv.org/abs/2412.16556", "title": "Acquisition of Recursive Possessives and Recursive Locatives in Mandarin", "authors": ["Chenxi Fu", "Xiaoyi Wang", "Zaijiang Man", "Caimei Yang"], "categories": ["cs.CL"], "comment": null, "summary": "As recursion has been underlying any linguistic work for the last 60 years,\nthe acquisition of recursive structures by children during language learning\nhas become a focal point of inquiry. This study delves into the developmental\ntrajectory of Mandarin-speaking children's acquisition of recursive possessives\nand locatives, assessing the impact of structural diversity on language\nacquisition. The research contrasts the comprehension of two-level recursive\nstructures among children aged 3 to 7 years, employing answering question while\nseeing a picture task to elicit responses. The findings indicate that children\ndo not attain adult-like proficiency in two-level recursion until the age of 6,\nand there exists a notable asymmetry in the acquisition of recursive\npossessives versus locatives. These results underscore the primacy of\nstructural complexity and cognitive factors in the acquisition process,\nenhancing our comprehension of the cognitive foundations of language\ndevelopment and the pivotal role of recursion in child language acquisition.", "AI": {"tldr": "This study investigates how Mandarin-speaking children acquire recursive possessives and locatives, revealing that adult-like understanding is reached by age 6.", "motivation": "The study aims to understand the developmental process of acquiring recursive language structures in children, particularly focusing on Mandarin.", "method": "The research compares comprehension of two-level recursive structures among children aged 3 to 7 using a picture-based question-answering task.", "result": "Children achieve adult-like proficiency in two-level recursion by age 6, with notable differences in the acquisition of recursive possessives and locatives.", "conclusion": "The findings highlight the importance of structural complexity and cognitive factors in the language acquisition process, emphasizing recursion’s role in child development.", "key_contributions": ["Insights into the age at which children reach proficiency in recursive structures", "Comparison of recursive possessives versus locatives", "Implications for understanding cognitive foundations of language development"], "limitations": "", "keywords": ["language acquisition", "recursion", "Mandarin-speaking children", "cognitive development", "structural complexity"], "importance_score": 2, "read_time_minutes": 10}}
{"id": "2501.00045", "pdf": "https://arxiv.org/pdf/2501.00045.pdf", "abs": "https://arxiv.org/abs/2501.00045", "title": "Improving Low-Resource Machine Translation via Cross-Linguistic Transfer from Typologically Similar High-Resource Languages", "authors": ["Saughmon Boujkian"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This study examines the cross-linguistic effectiveness of transfer learning\nfor low-resource machine translation by fine-tuning models initially trained on\ntypologically similar high-resource languages, using limited data from the\ntarget low-resource language. We hypothesize that linguistic similarity enables\nefficient adaptation, reducing the need for extensive training data. To test\nthis, we conduct experiments on five typologically diverse language pairs\nspanning distinct families: Semitic (Modern Standard Arabic to Levantine\nArabic), Bantu (Hausa to Zulu), Romance (Spanish to Catalan), Slavic (Slovak to\nMacedonian), and a language isolate (Eastern Armenian to Western Armenian).\nResults show that transfer learning consistently improves translation quality\nacross all pairs, confirming its applicability beyond closely related\nlanguages. As a secondary analysis, we vary key hyperparameters learning rate,\nbatch size, number of epochs, and weight decay to ensure results are not\ndependent on a single configuration. We find that moderate batch sizes (e.g.,\n32) are often optimal for similar pairs, smaller sizes benefit less similar\npairs, and excessively high learning rates can destabilize training. These\nfindings provide empirical evidence for the generalizability of transfer\nlearning across language families and offer practical guidance for building\nmachine translation systems in low-resource settings with minimal tuning\neffort.", "AI": {"tldr": "The study investigates the effectiveness of transfer learning for low-resource machine translation by fine-tuning models from high-resource languages using limited data from target languages. It confirms the applicability of transfer learning across diverse language families and offers insights into optimal hyperparameters.", "motivation": "To explore how linguistic similarity impacts the transfer learning process in low-resource machine translation and to reduce the need for extensive training data.", "method": "Experiments conducted on five language pairs across different families to analyze the impact of transfer learning and hyperparameter variations on translation quality.", "result": "Transfer learning improved translation quality across all language pairs, demonstrating its effectiveness even among typologically diverse languages. Key hyperparameters influenced the stability and performance of training.", "conclusion": "Transfer learning can be employed effectively in low-resource settings, providing a practical framework for building machine translation systems with reduced tuning efforts.", "key_contributions": ["Demonstrated the effectiveness of transfer learning across diverse language families for low-resource translation.", "Provided empirical evidence supporting linguistic similarity in transfer learning adaptations.", "Identified optimal hyperparameter configurations for enhanced translation performance."], "limitations": "", "keywords": ["Transfer learning", "Machine translation", "Low-resource languages", "Hyperparameter tuning", "Linguistic similarity"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2501.00273", "pdf": "https://arxiv.org/pdf/2501.00273.pdf", "abs": "https://arxiv.org/abs/2501.00273", "title": "Echoes in AI: Quantifying lack of plot diversity in LLM outputs", "authors": ["Weijia Xu", "Nebojsa Jojic", "Sudha Rao", "Chris Brockett", "Bill Dolan"], "categories": ["cs.CL"], "comment": "PNAS Vol. 122 No. 35. Copyright \\c{opyright} 2025 the Author(s).\n  Published by PNAS. This open access article is distributed under Creative\n  Commons Attribution-NonCommercial-NoDerivatives License 4.0 (CC BY-NC-ND)", "summary": "With rapid advances in large language models (LLMs), there has been an\nincreasing application of LLMs in creative content ideation and generation. A\ncritical question emerges: can current LLMs provide ideas that are diverse\nenough to truly bolster collective creativity? We examine two state-of-the-art\nLLMs, GPT-4 and LLaMA-3, on story generation and discover that LLM-generated\nstories often consist of plot elements that are echoed across a number of\ngenerations. To quantify this phenomenon, we introduce the Sui Generis score,\nan automatic metric that measures the uniqueness of a plot element among\nalternative storylines generated using the same prompt under an LLM. Evaluating\non 100 short stories, we find that LLM-generated stories often contain\ncombinations of idiosyncratic plot elements echoed frequently across\ngenerations and across different LLMs, while plots from the original\nhuman-written stories are rarely recreated or even echoed in pieces. Moreover,\nour human evaluation shows that the ranking of Sui Generis scores among story\nsegments correlates moderately with human judgment of surprise level, even\nthough score computation is completely automatic without relying on human\njudgment.", "AI": {"tldr": "The paper investigates the diversity of story ideas generated by state-of-the-art LLMs, using a new metric called the Sui Generis score to analyze uniqueness in plots across different generations.", "motivation": "With the rise of LLMs, it is important to understand their ability to foster creativity by generating diverse story ideas.", "method": "Two LLMs, GPT-4 and LLaMA-3, were evaluated for story generation, using a new metric, the Sui Generis score, to quantify the uniqueness of plot elements across 100 stories.", "result": "LLM-generated stories often have repeated plot elements indicating a lack of diversity, while human-written stories show unique plots that are not echoed in LLM outputs.", "conclusion": "While LLMs can generate stories, they often lack the diversity seen in human creativity, as measured by the Sui Generis score, which also correlates with human assessments of surprise.", "key_contributions": ["Introduction of the Sui Generis score for measuring plot uniqueness.", "Evaluation of GPT-4 and LLaMA-3 in story generation and their repetitive use of plot elements.", "Correlation between Sui Generis scores and human judgment of surprise in stories."], "limitations": "", "keywords": ["large language models", "story generation", "creativity", "Sui Generis score", "plot uniqueness"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2502.03275", "pdf": "https://arxiv.org/pdf/2502.03275.pdf", "abs": "https://arxiv.org/abs/2502.03275", "title": "Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning", "authors": ["DiJia Su", "Hanlin Zhu", "Yingchen Xu", "Jiantao Jiao", "Yuandong Tian", "Qinqing Zheng"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.LO"], "comment": null, "summary": "Large Language Models (LLMs) excel at reasoning and planning when trained on\nchainof-thought (CoT) data, where the step-by-step thought process is\nexplicitly outlined by text tokens. However, this results in lengthy inputs\nwhere many words support textual coherence rather than core reasoning\ninformation, and processing these inputs consumes substantial computation\nresources. In this work, we propose a hybrid representation of the reasoning\nprocess, where we partially abstract away the initial reasoning steps using\nlatent discrete tokens generated by VQ-VAE, significantly reducing the length\nof reasoning traces. We explore the use of latent trace abstractions in two\nscenarios: 1) training the model from scratch for the Keys-Finding Maze\nproblem, 2) fine-tuning LLMs on this hybrid data with an extended vocabulary\nincluding unseen latent tokens, for both logical and mathematical reasoning\nproblems. To facilitate effective learning, we introduce a simple training\nprocedure that randomly mixes latent and text tokens, which enables fast\nadaptation to new latent tokens. Our approach consistently outperforms the\nbaselines methods in various benchmarks.", "AI": {"tldr": "This paper introduces a hybrid representation for reasoning processes in LLMs, using latent discrete tokens to reduce input length and computational resource consumption.", "motivation": "Large Language Models struggle with lengthy inputs for reasoning tasks, which consume significant resources; thus, there is a need for more efficient representations.", "method": "We propose a hybrid representation that abstracts initial reasoning steps with latent discrete tokens generated by VQ-VAE and train the model using a mixture of latent and text tokens.", "result": "The method consistently outperforms baseline approaches in benchmarks related to logical and mathematical reasoning problems.", "conclusion": "The hybrid approach enables LLMs to efficiently handle reasoning tasks while reducing input length and resource usage, and shows promise in various reasoning scenarios.", "key_contributions": ["Introduction of a hybrid reasoning representation using latent tokens", "Demonstrated efficiency in training and adaptability to new latent tokens", "Benchmark performance improvements over existing methods"], "limitations": "", "keywords": ["Large Language Models", "latent tokens", "reasoning", "VQ-VAE", "machine learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.10708", "pdf": "https://arxiv.org/pdf/2502.10708.pdf", "abs": "https://arxiv.org/abs/2502.10708", "title": "Injecting Domain-Specific Knowledge into Large Language Models: A Comprehensive Survey", "authors": ["Zirui Song", "Bin Yan", "Yuhan Liu", "Miao Fang", "Mingzhe Li", "Rui Yan", "Xiuying Chen"], "categories": ["cs.CL"], "comment": "EMNLP 2025 findings", "summary": "Large Language Models (LLMs) have demonstrated remarkable success in various\ntasks such as natural language understanding, text summarization, and machine\ntranslation. However, their general-purpose nature often limits their\neffectiveness in domain-specific applications that require specialized\nknowledge, such as healthcare, chemistry, or legal analysis. To address this,\nresearchers have explored diverse methods to enhance LLMs by integrating\ndomain-specific knowledge. In this survey, we provide a comprehensive overview\nof these methods, which we categorize into four key approaches: dynamic\nknowledge injection, static knowledge embedding, modular adapters, and prompt\noptimization. Each approach offers unique mechanisms to equip LLMs with domain\nexpertise, balancing trade-offs between flexibility, scalability, and\nefficiency. We discuss how these methods enable LLMs to tackle specialized\ntasks, compare their advantages and disadvantages, evaluate domain-specific\nLLMs against general LLMs, and highlight the challenges and opportunities in\nthis emerging field. For those interested in delving deeper into this area, we\nalso summarize the commonly used datasets and benchmarks. To keep researchers\nupdated on the latest studies, we maintain an open-source at:\nhttps://github.com/abilliyb/Knowledge_Injection_Survey_Papers, dedicated to\ndocumenting research in the field of specialized LLM.", "AI": {"tldr": "This survey categorizes methods to enhance Large Language Models (LLMs) with domain-specific knowledge into four key approaches: dynamic knowledge injection, static knowledge embedding, modular adapters, and prompt optimization.", "motivation": "To address the limitations of general-purpose LLMs in domain-specific applications such as healthcare and chemistry by integrating specialized knowledge.", "method": "The survey categorizes enhancement methods for LLMs into four approaches and discusses their mechanisms, trade-offs, and effectiveness in specialized tasks.", "result": "The methods enable LLMs to better tackle specialized tasks by incorporating domain expertise, and the paper compares their advantages, disadvantages, and evaluates performance against general LLMs.", "conclusion": "The paper highlights the challenges and opportunities in enhancing LLMs for domain-specific applications and maintains a resource repository for ongoing research.", "key_contributions": ["Comprehensive overview of methods to enhance LLMs with domain knowledge.", "Categorization of methods into four key approaches with discussion on their trade-offs.", "Evaluation of domain-specific LLMs against general-purpose LLMs and summarization of datasets."], "limitations": "", "keywords": ["Large Language Models", "domain-specific knowledge", "knowledge injection", "natural language processing", "health informatics"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2502.11176", "pdf": "https://arxiv.org/pdf/2502.11176.pdf", "abs": "https://arxiv.org/abs/2502.11176", "title": "LogiDynamics: Unraveling the Dynamics of Logical Inference in Large Language Model Reasoning", "authors": ["Tianshi Zheng", "Jiayang Cheng", "Chunyang Li", "Haochen Shi", "Zihao Wang", "Jiaxin Bai", "Yangqiu Song", "Ginny Y. Wong", "Simon See"], "categories": ["cs.CL"], "comment": "EMNLP 2025 Main", "summary": "Modern large language models (LLMs) employ various forms of logical\ninference, both implicitly and explicitly, when addressing reasoning tasks.\nUnderstanding how to optimally leverage these inference paradigms is critical\nfor advancing LLMs' reasoning capabilities. This paper adopts an exploratory\napproach by introducing a controlled evaluation environment for analogical\nreasoning -- a fundamental cognitive task -- that is systematically\nparameterized across three dimensions: modality (textual, visual, symbolic),\ndifficulty (easy, medium, hard), and task format (multiple-choice or free-text\ngeneration). We analyze the comparative dynamics of inductive, abductive, and\ndeductive inference pipelines across these dimensions, and demonstrate that our\nfindings generalize to broader in-context learning tasks. Additionally, we\ninvestigate advanced paradigms such as hypothesis selection, verification, and\nrefinement, revealing their potential to scale up logical inference in LLM\nreasoning. This exploratory study provides a foundation for future research in\nenhancing LLM reasoning through systematic logical inference strategies.\nResources are available at https://github.com/HKUST-KnowComp/LogiDynamics.", "AI": {"tldr": "The paper explores logical inference in large language models (LLMs) through a controlled evaluation environment for analogical reasoning, analyzing different inference pipelines and their impact on reasoning tasks.", "motivation": "Understanding how to optimally leverage logical inference paradigms is critical for advancing the reasoning capabilities of LLMs.", "method": "The paper introduces a controlled evaluation environment parameterized by modality, difficulty, and task format, analyzing inductive, abductive, and deductive inference pipelines.", "result": "The findings demonstrate that the comparative dynamics of different inference types generalize to broader in-context learning tasks, with advanced paradigms showing potential to enhance reasoning.", "conclusion": "This exploratory study lays a foundation for future research aimed at improving LLM reasoning through systematic logical inference strategies.", "key_contributions": ["Development of a controlled evaluation environment for analogical reasoning", "Comparative analysis of inference dynamics across modalities and task formats", "Investigation of advanced paradigms for scaling logical inference in LLM reasoning"], "limitations": "", "keywords": ["large language models", "logical inference", "analogical reasoning", "in-context learning", "hypothesis verification"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.11419", "pdf": "https://arxiv.org/pdf/2502.11419.pdf", "abs": "https://arxiv.org/abs/2502.11419", "title": "InsBank: Evolving Instruction Subset for Ongoing Alignment", "authors": ["Jiayi Shi", "Yiwei Li", "Shaoxiong Feng", "Peiwen Yuan", "Xinglin Wang", "Yueqi Zhang", "Chuyi Tan", "Boyuan Pan", "Huan Ren", "Yao Hu", "Kan Li"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) typically undergo instruction tuning to enhance\nalignment. Recent studies emphasize that quality and diversity of instruction\ndata are more crucial than quantity, highlighting the need to select diverse,\nhigh-quality subsets to reduce training costs. However, how to evolve these\nselected subsets alongside the development of new instruction data remains\ninsufficiently explored. To achieve LLMs' ongoing alignment, we introduce\nInstruction Bank (\\textbf{InsBank}), a continuously updated repository that\nintegrates the latest valuable instruction data. We further propose Progressive\nInstruction Bank Evolution (\\textbf{PIBE}), a novel framework designed to\nevolve InsBank effectively and efficiently over time. PIBE employs a gradual\ndata selection strategy to maintain long-term efficiency, leveraging a\nrepresentation-based diversity score to capture relationships between data\npoints and retain historical information for comprehensive diversity\nevaluation. This also allows for flexible combination of diversity and quality\nscores during data selection and ranking. Extensive experiments demonstrate\nthat PIBE significantly outperforms baselines in InsBank evolution and is able\nto extract budget-specific subsets, demonstrating its effectiveness and\nadaptability.", "AI": {"tldr": "The paper presents Instruction Bank (InsBank) and the Progressive Instruction Bank Evolution (PIBE) framework for effectively updating instruction data for large language models (LLMs) to enhance alignment.", "motivation": "To improve the ongoing alignment of LLMs through the quality and diversity of instruction data rather than sheer quantity, and to explore how to evolve selected subsets of instruction data over time.", "method": "Introduces InsBank as a continuously updated repository for instruction data and proposes the PIBE framework, which employs a gradual data selection strategy using a representation-based diversity score for efficient evolution of InsBank.", "result": "Experimental results show that PIBE significantly outperforms baselines in evolving InsBank and can extract subsets tailored to specific budgets.", "conclusion": "The proposed framework demonstrates improved effectiveness and adaptability in maintaining a valuable repository of instruction data for LLMs.", "key_contributions": ["Introduction of Instruction Bank (InsBank) for evolving instruction data", "Development of Progressive Instruction Bank Evolution (PIBE) for efficient data selection", "Utilization of a representation-based diversity score for comprehensive evaluation of instruction data"], "limitations": "", "keywords": ["Large Language Models", "Instruction Tuning", "Machine Learning", "Diversity Score", "Data Selection"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.13647", "pdf": "https://arxiv.org/pdf/2502.13647.pdf", "abs": "https://arxiv.org/abs/2502.13647", "title": "Instruction Tuning on Public Government and Cultural Data for Low-Resource Language: a Case Study in Kazakh", "authors": ["Nurkhan Laiyk", "Daniil Orel", "Rituraj Joshi", "Maiya Goloburda", "Yuxia Wang", "Preslav Nakov", "Fajri Koto"], "categories": ["cs.CL"], "comment": null, "summary": "Instruction tuning in low-resource languages remains underexplored due to\nlimited text data, particularly in government and cultural domains. To address\nthis, we introduce and open-source a large-scale (10,600 samples)\ninstruction-following (IFT) dataset, covering key institutional and cultural\nknowledge relevant to Kazakhstan. Our dataset enhances LLMs' understanding of\nprocedural, legal, and structural governance topics. We employ LLM-assisted\ndata generation, comparing open-weight and closed-weight models for dataset\nconstruction, and select GPT-4o as the backbone. Each entity of our dataset\nundergoes full manual verification to ensure high quality. We also show that\nfine-tuning Qwen, Falcon, and Gemma on our dataset leads to consistent\nperformance improvements in both multiple-choice and generative tasks,\ndemonstrating the potential of LLM-assisted instruction tuning for low-resource\nlanguages.", "AI": {"tldr": "This paper presents a large-scale instruction-following dataset for low-resource languages, specifically for Kazakhstan, and explores fine-tuning LLMs for improved performance in governance-related topics.", "motivation": "To improve instruction tuning for low-resource languages by providing a substantial dataset in the context of Kazakhstan's institutional and cultural knowledge.", "method": "The authors developed a dataset of 10,600 samples using LLM-assisted data generation and manually verified each entry. They compared the performance of various models (Qwen, Falcon, Gemma) after fine-tuning on the dataset.", "result": "Fine-tuning LLMs on this dataset resulted in consistent performance improvements in both multiple-choice and generative tasks, indicating successful application of instruction tuning in low-resource settings.", "conclusion": "The dataset opens up new possibilities for instruction tuning in low-resource languages and enhances the understanding of governance and cultural topics by LLMs.", "key_contributions": ["Introduction of a large instruction-following dataset for Kazakhstan", "Demonstration of LLM-assisted data generation", "Fine-tuning effects on multiple models showing performance improvements"], "limitations": "", "keywords": ["instruction tuning", "low-resource languages", "LLM-assisted data generation", "Kazakhstan", "governance"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2502.16534", "pdf": "https://arxiv.org/pdf/2502.16534.pdf", "abs": "https://arxiv.org/abs/2502.16534", "title": "Multilingual != Multicultural: Evaluating Gaps Between Multilingual Capabilities and Cultural Alignment in LLMs", "authors": ["Jonathan Rystrøm", "Hannah Rose Kirk", "Scott Hale"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "Accepted at OMMM@RANLP2025", "summary": "Large Language Models (LLMs) are becoming increasingly capable across global\nlanguages. However, the ability to communicate across languages does not\nnecessarily translate to appropriate cultural representations. A key concern is\nUS-centric bias, where LLMs reflect US rather than local cultural values. We\npropose a novel methodology that compares LLM-generated response distributions\nagainst population-level opinion data from the World Value Survey across four\nlanguages (Danish, Dutch, English, and Portuguese). Using a rigorous linear\nmixed-effects regression framework, we compare two families of models: Google's\nGemma models (2B--27B parameters) and successive iterations of OpenAI's\nturbo-series. Across the families of models, we find no consistent\nrelationships between language capabilities and cultural alignment. While the\nGemma models have a positive correlation between language capability and\ncultural alignment across languages, the OpenAI models do not. Importantly, we\nfind that self-consistency is a stronger predictor of multicultural alignment\nthan multilingual capabilities. Our results demonstrate that achieving\nmeaningful cultural alignment requires dedicated effort beyond improving\ngeneral language capabilities.", "AI": {"tldr": "This paper examines cultural alignment in LLMs across languages, revealing that self-consistency is a better predictor of multicultural alignment than language capabilities alone.", "motivation": "The paper addresses the issue of US-centric bias in Large Language Models (LLMs) and the need for appropriate cultural representations across different languages.", "method": "A novel methodology is proposed that compares LLM-generated responses with population-level opinion data from the World Value Survey using a linear mixed-effects regression framework.", "result": "The study finds that while Gemma models show a positive correlation between language capability and cultural alignment, OpenAI's models do not. Self-consistency emerges as a stronger predictor of multicultural alignment than multilingual capabilities.", "conclusion": "Achieving meaningful cultural alignment in LLMs necessitates focused efforts beyond just enhancing language capabilities.", "key_contributions": ["Introduction of a methodology for assessing cultural alignment in LLMs", "Comparison of Google's Gemma models and OpenAI's turbo-series regarding cultural bias", "Highlighting self-consistency as a key factor for multicultural alignment"], "limitations": "", "keywords": ["Large Language Models", "cultural alignment", "language capability", "multilingualism", "bias"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2502.16682", "pdf": "https://arxiv.org/pdf/2502.16682.pdf", "abs": "https://arxiv.org/abs/2502.16682", "title": "Automatic Input Rewriting Improves Translation with Large Language Models", "authors": ["Dayeon Ki", "Marine Carpuat"], "categories": ["cs.CL", "cs.AI"], "comment": "NAACL 2025", "summary": "Can we improve machine translation (MT) with LLMs by rewriting their inputs\nautomatically? Users commonly rely on the intuition that well-written text is\neasier to translate when using off-the-shelf MT systems. LLMs can rewrite text\nin many ways but in the context of MT, these capabilities have been primarily\nexploited to rewrite outputs via post-editing. We present an empirical study of\n21 input rewriting methods with 3 open-weight LLMs for translating from English\ninto 6 target languages. We show that text simplification is the most effective\nMT-agnostic rewrite strategy and that it can be improved further when using\nquality estimation to assess translatability. Human evaluation further confirms\nthat simplified rewrites and their MT outputs both largely preserve the\noriginal meaning of the source and MT. These results suggest LLM-assisted input\nrewriting as a promising direction for improving translations.", "AI": {"tldr": "This paper explores using LLMs for input rewriting to enhance machine translation efficacy, revealing that text simplification is the most effective strategy.", "motivation": "The study investigates whether rewriting inputs through LLMs can improve machine translation, as improved input quality is believed to enhance translation output.", "method": "An empirical study was conducted evaluating 21 input rewriting methods across 3 open-weight LLMs for English-to-MT translations in 6 target languages.", "result": "Text simplification emerged as the most effective rewrite strategy, with enhancements possible through quality estimation to assess translatability. Human evaluations confirmed that simplified rewrites retained original meaning effectively.", "conclusion": "LLM-assisted input rewriting shows promise as a method to enhance machine translation outcomes.", "key_contributions": ["Demonstration of 21 effective input rewriting methods for MT.", "Identification of text simplification as the top-performing strategy.", "Showcasing the utility of LLMs for pre-editing inputs to improve translations."], "limitations": "", "keywords": ["machine translation", "input rewriting", "LLMs", "text simplification", "quality estimation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.18848", "pdf": "https://arxiv.org/pdf/2502.18848.pdf", "abs": "https://arxiv.org/abs/2502.18848", "title": "A Causal Lens for Evaluating Faithfulness Metrics", "authors": ["Kerem Zaman", "Shashank Srivastava"], "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ME"], "comment": "25 pages, 22 figures, 9 tables", "summary": "Large Language Models (LLMs) offer natural language explanations as an\nalternative to feature attribution methods for model interpretability. However,\ndespite their plausibility, they may not reflect the model's true reasoning\nfaithfully, which is crucial for understanding the model's true decision-making\nprocesses. Although several faithfulness metrics have been proposed, they are\noften evaluated in isolation, making direct, principled comparisons between\nthem difficult. Here, we present Causal Diagnosticity, a framework that serves\nas a common testbed to evaluate faithfulness metrics for natural language\nexplanations. Our framework employs the concept of diagnosticity, and uses\nmodel-editing methods to generate faithful-unfaithful explanation pairs. Our\nbenchmark includes four tasks: fact-checking, analogy, object counting, and\nmulti-hop reasoning. We evaluate prominent faithfulness metrics, including\npost-hoc explanation and chain-of-thought-based methods. We find that\ndiagnostic performance varies across tasks and models, with Filler Tokens\nperforming best overall. Additionally, continuous metrics are generally more\ndiagnostic than binary ones but can be sensitive to noise and model choice. Our\nresults highlight the need for more robust faithfulness metrics.", "AI": {"tldr": "The paper proposes Causal Diagnosticity, a framework for evaluating faithfulness metrics of natural language explanations from LLMs.", "motivation": "To assess the fidelity of natural language explanations from LLMs, addressing the limitations in existing faithfulness metrics.", "method": "Causal Diagnosticity framework evaluates faithfulness metrics using model-editing methods to create explanations of varying faithfulness.", "result": "The study reveals that diagnostic performance of faithfulness metrics varies by task and model, with continuous metrics generally being more effective; Filler Tokens achieve the best performance overall.", "conclusion": "The research underscores the necessity for improved robustness in faithfulness metrics used for LLM explanations.", "key_contributions": ["Introduction of Causal Diagnosticity framework for evaluating faithfulness metrics", "Empirical evaluation of various faithfulness metrics across different tasks", "Insights on the effectiveness of continuous vs. binary metrics and the performance of Filler Tokens."], "limitations": "Focused primarily on a specific set of tasks and metrics; results may not generalize across all LLMs or applications.", "keywords": ["Large Language Models", "Model Interpretability", "Faithfulness Metrics", "Natural Language Explanations", "Causal Diagnosticity"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2503.12225", "pdf": "https://arxiv.org/pdf/2503.12225.pdf", "abs": "https://arxiv.org/abs/2503.12225", "title": "Interpretation Gaps in LLM-Assisted Comprehension of Privacy Documents", "authors": ["Rinku Dewri"], "categories": ["cs.CL", "cs.CY", "cs.IR"], "comment": "Minor revision", "summary": "This article explores the gaps that can manifest when using a large language\nmodel (LLM) to obtain simplified interpretations of data practices from a\ncomplex privacy policy. We exemplify these gaps to showcase issues in accuracy,\ncompleteness, clarity and representation, while advocating for continued\nresearch to realize an LLM's true potential in revolutionizing privacy\nmanagement through personal assistants and automated compliance checking.", "AI": {"tldr": "Explores gaps in using LLMs for interpreting complex privacy policies, highlighting issues in accuracy and completeness.", "motivation": "To understand the limitations and inaccuracies that LLMs may produce when simplifying data practices from privacy policies.", "method": "Analyzing various interpretations generated by an LLM against established accuracy and completeness benchmarks.", "result": "Identified specific gaps in interpretation concerning accuracy, completeness, clarity, and representation.", "conclusion": "There is a need for ongoing research to harness the full capabilities of LLMs in improving privacy management and compliance.", "key_contributions": ["Identification of accuracy gaps in LLM interpretations of privacy policies", "Discussion on clarity and representation issues in LLM outputs", "Call for further research to improve LLM applications in privacy management"], "limitations": "", "keywords": ["large language models", "privacy policy", "data practices", "automated compliance", "accuracy issues"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2503.12608", "pdf": "https://arxiv.org/pdf/2503.12608.pdf", "abs": "https://arxiv.org/abs/2503.12608", "title": "UniBERT: Adversarial Training for Language-Universal Representations", "authors": ["Andrei-Marius Avram", "Marian Lupaşcu", "Dumitru-Clementin Cercel", "Ionuţ Mironică", "Ştefan Trăuşan-Matu"], "categories": ["cs.CL"], "comment": null, "summary": "This paper presents UniBERT, a compact multilingual language model that uses\nan innovative training framework that integrates three components: masked\nlanguage modeling, adversarial training, and knowledge distillation.\nPre-trained on a meticulously curated Wikipedia corpus spanning 107 languages,\nUniBERT is designed to reduce the computational demands of large-scale models\nwhile maintaining competitive performance across various natural language\nprocessing tasks. Comprehensive evaluations on four tasks - named entity\nrecognition, natural language inference, question answering, and semantic\ntextual similarity - demonstrate that our multilingual training strategy\nenhanced by an adversarial objective significantly improves cross-lingual\ngeneralization. Specifically, UniBERT models show an average relative\nimprovement of 7.72% over traditional baselines, which achieved an average\nrelative improvement of only 1.17%, and statistical analysis confirms the\nsignificance of these gains (p-value = 0.0181). This work highlights the\nbenefits of combining adversarial training and knowledge distillation to build\nscalable and robust language models, thus advancing the field of multilingual\nand cross-lingual natural language processing.", "AI": {"tldr": "UniBERT is a compact multilingual language model that utilizes masked language modeling, adversarial training, and knowledge distillation, achieving improved performance on various NLP tasks.", "motivation": "To create a more efficient multilingual language model that reduces computational demands while preserving performance in natural language processing tasks.", "method": "UniBERT employs a novel training framework integrating masked language modeling, adversarial training, and knowledge distillation, pre-trained on a Wikipedia corpus in 107 languages.", "result": "UniBERT demonstrates an average relative improvement of 7.72% over traditional baselines across four NLP tasks, with statistical significance confirmed (p-value = 0.0181).", "conclusion": "Combining adversarial training and knowledge distillation can build scalable and robust language models, enhancing multilingual and cross-lingual NLP capabilities.", "key_contributions": ["Development of UniBERT model", "Successful integration of adversarial training and knowledge distillation", "Significant improvements in cross-lingual generalization"], "limitations": "", "keywords": ["multilingual language model", "adversarial training", "knowledge distillation", "NLP", "cross-lingual processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.18172", "pdf": "https://arxiv.org/pdf/2503.18172.pdf", "abs": "https://arxiv.org/abs/2503.18172", "title": "Unmasking Deceptive Visuals: Benchmarking Multimodal Large Language Models on Misleading Chart Question Answering", "authors": ["Zixin Chen", "Sicheng Song", "Kashun Shum", "Yanna Lin", "Rui Sheng", "Huamin Qu"], "categories": ["cs.CL", "cs.AI"], "comment": "30 pages in total. EMNLP 2025 Main", "summary": "Misleading visualizations, which manipulate chart representations to support\nspecific claims, can distort perception and lead to incorrect conclusions.\nDespite decades of research, they remain a widespread issue-posing risks to\npublic understanding and raising safety concerns for AI systems involved in\ndata-driven communication. While recent multimodal large language models\n(MLLMs) show strong chart comprehension abilities, their capacity to detect and\ninterpret misleading charts remains unexplored. We introduce Misleading ChartQA\nbenchmark, a large-scale multimodal dataset designed to evaluate MLLMs on\nmisleading chart reasoning. It contains 3,026 curated examples spanning 21\nmisleader types and 10 chart types, each with standardized chart code, CSV\ndata, multiple-choice questions, and labeled explanations, validated through\niterative MLLM checks and exhausted expert human review. We benchmark 24\nstate-of-the-art MLLMs, analyze their performance across misleader types and\nchart formats, and propose a novel region-aware reasoning pipeline that\nenhances model accuracy. Our work lays the foundation for developing MLLMs that\nare robust, trustworthy, and aligned with the demands of responsible visual\ncommunication.", "AI": {"tldr": "The paper introduces the Misleading ChartQA benchmark dataset to evaluate multimodal large language models (MLLMs) in detecting and interpreting misleading chart visualizations, highlighting the need for responsible visual communication in AI systems.", "motivation": "Misleading visualizations can distort perception and lead to incorrect conclusions, posing risks to public understanding and safety in AI-driven data communication.", "method": "The authors created the Misleading ChartQA benchmark, a dataset with 3,026 examples across 21 misleader types and 10 chart types, including standardized chart code, CSV data, multiple-choice questions, and labeled explanations. They benchmarked 24 state-of-the-art MLLMs and developed a novel reasoning pipeline.", "result": "The study found significant variation in the performance of MLLMs across different misleader types and chart formats, illustrating the challenges in recognizing misleading charts.", "conclusion": "This work establishes a benchmark for future research and development of MLLMs that can better interpret misleading visualizations, contributing to more trustworthy and responsible AI systems.", "key_contributions": ["Introduction of the Misleading ChartQA benchmark for MLLMs", "Benchmarking of 24 state-of-the-art MLLMs", "Development of a novel region-aware reasoning pipeline to enhance model accuracy."], "limitations": "Limited to evaluating MLLMs; does not address the broader implications of visual perception in general AI systems.", "keywords": ["MLLM", "chart comprehension", "misleading visualizations", "benchmarking", "responsible communication"], "importance_score": 8, "read_time_minutes": 30}}
{"id": "2503.23427", "pdf": "https://arxiv.org/pdf/2503.23427.pdf", "abs": "https://arxiv.org/abs/2503.23427", "title": "CoRanking: Collaborative Ranking with Small and Large Ranking Agents", "authors": ["Wenhan Liu", "Xinyu Ma", "Yutao Zhu", "Lixin Su", "Shuaiqiang Wang", "Dawei Yin", "Zhicheng Dou"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated superior listwise ranking\nperformance. However, their superior performance often relies on large-scale\nparameters (\\eg, GPT-4) and a repetitive sliding window process, which\nintroduces significant efficiency challenges. In this paper, we propose\n\\textbf{CoRanking}, a novel collaborative ranking framework that combines small\nand large ranking models for efficient and effective ranking. CoRanking first\nemploys a small-size reranker to pre-rank all the candidate passages, bringing\nrelevant ones to the top part of the list (\\eg, top-20). Then, the LLM listwise\nreranker is applied to only rerank these top-ranked passages instead of the\nwhole list, substantially enhancing overall ranking efficiency. Although more\nefficient, previous studies have revealed that the LLM listwise reranker have\nsignificant positional biases on the order of input passages. Directly feed the\ntop-ranked passages from small reranker may result in the sub-optimal\nperformance of LLM listwise reranker. To alleviate this problem, we introduce a\npassage order adjuster trained via reinforcement learning, which reorders the\ntop passages from the small reranker to align with the LLM's preferences of\npassage order. Extensive experiments on three IR benchmarks demonstrate that\nCoRanking significantly improves efficiency (reducing ranking latency by about\n70\\%) while achieving even better effectiveness compared to using only the LLM\nlistwise reranker.", "AI": {"tldr": "CoRanking is a collaborative ranking framework that combines small and large ranking models for efficient passage ranking using large language models (LLMs).", "motivation": "To address the efficiency challenges posed by LLMs' reliance on large-scale parameters and the sliding window process during ranking.", "method": "The method utilizes a small reranker to pre-rank candidate passages, bringing relevant ones to the top (e.g., top-20), followed by a listwise reranker powered by an LLM only on these top passages. A passage order adjuster trained via reinforcement learning is also introduced to optimize order alignment with LLM preferences.", "result": "CoRanking significantly improves ranking efficiency by reducing ranking latency by about 70% while achieving better effectiveness than using only the LLM listwise reranker.", "conclusion": "The proposed framework not only enhances efficiency in ranking but also addresses the positional biases inherent in LLM rerankers.", "key_contributions": ["Introduction of CoRanking framework that combines small and large models for ranking.", "Leveraging a passage order adjuster trained with reinforcement learning.", "Demonstrated significant improvements in ranking efficiency and effectiveness on multiple IR benchmarks."], "limitations": "The paper does not address potential drawbacks of using reinforcement learning for passage reordering and may require further validation across diverse datasets.", "keywords": ["Large Language Models", "Collaborative Ranking", "Passage Ranking", "Efficiency", "Reinforcement Learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.03612", "pdf": "https://arxiv.org/pdf/2504.03612.pdf", "abs": "https://arxiv.org/abs/2504.03612", "title": "AIR: A Systematic Analysis of Annotations, Instructions, and Response Pairs in Preference Dataset", "authors": ["Bingxiang He", "Wenbin Zhang", "Jiaxi Song", "Cheng Qian", "Zixuan Fu", "Bowen Sun", "Ning Ding", "Haiwen Hong", "Longtao Huang", "Hui Xue", "Ganqu Cui", "Wanxiang Che", "Zhiyuan Liu", "Maosong Sun"], "categories": ["cs.CL"], "comment": "Accept at the Conference On Language Modeling (COLM) 2025", "summary": "Preference learning is critical for aligning large language models (LLMs)\nwith human values, yet its success hinges on high-quality datasets comprising\nthree core components: Preference \\textbf{A}nnotations, \\textbf{I}nstructions,\nand \\textbf{R}esponse Pairs. Current approaches conflate these components,\nobscuring their individual impacts and hindering systematic optimization. In\nthis work, we propose \\textbf{AIR}, a component-wise analysis framework that\nsystematically isolates and optimizes each component while evaluating their\nsynergistic effects. Through rigorous experimentation, AIR reveals actionable\nprinciples: annotation simplicity (point-wise generative scoring), instruction\ninference stability (variance-based filtering across LLMs), and response pair\nquality (moderate margins + high absolute scores). When combined, these\nprinciples yield +5.3 average gains over baseline method, even with only 14k\nhigh-quality pairs. Our work shifts preference dataset design from ad hoc\nscaling to component-aware optimization, offering a blueprint for efficient,\nreproducible alignment.", "AI": {"tldr": "This paper introduces AIR, a component-wise analysis framework for optimizing preference learning datasets for large language models.", "motivation": "To enhance the alignment of LLMs with human values by improving the quality of datasets used in preference learning.", "method": "The authors developed the AIR framework to isolate and systematically optimize the three key components of preference datasets: Annotations, Instructions, and Response Pairs, while evaluating their combined effects.", "result": "The AIR framework demonstrates that applying principles like annotation simplicity and response pair quality can lead to an average performance improvement of +5.3 over baseline methods using only 14k high-quality pairs.", "conclusion": "The study advocates for a shift in preference dataset design towards a more structured, component-aware optimization approach rather than just scaling existing datasets.", "key_contributions": ["Introduction of the AIR framework for preference dataset optimization.", "Identification of key principles for improving dataset quality in LLM training.", "Evidence of significant performance gains with optimized components under controlled datasets."], "limitations": "", "keywords": ["Preference Learning", "Large Language Models", "Dataset Optimization"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2504.11381", "pdf": "https://arxiv.org/pdf/2504.11381.pdf", "abs": "https://arxiv.org/abs/2504.11381", "title": "RankAlign: A Ranking View of the Generator-Validator Gap in Large Language Models", "authors": ["Juan Diego Rodriguez", "Wenxuan Ding", "Katrin Erk", "Greg Durrett"], "categories": ["cs.CL"], "comment": "Published at COLM 2025", "summary": "Although large language models (LLMs) have become more capable and accurate\nacross many tasks, some fundamental sources of unreliability remain in their\nbehavior. One key limitation is their inconsistency at reporting the same\ninformation when prompts are changed. In this paper, we consider the\ndiscrepancy between a model's generated answer and their own verification of\nthat answer, the generator-validator gap. We define this gap in a more\nstringent way than prior work: we expect correlation of scores from a generator\nand a validator over the entire set of candidate answers, i.e., candidate\ncompletions that could possibly arise during ordinary language use without\nbreaking Gricean norms. We show that according to this measure, a large gap\nexists in various settings, including question answering, lexical semantics\ntasks, and next-word prediction. We then propose RankAlign, a ranking-based\ntraining method, and show that it significantly closes the gap, surpassing all\nbaseline methods. Moreover, this approach generalizes well to out-of-domain\ntasks and lexical items.", "AI": {"tldr": "This paper addresses the inconsistency of large language models (LLMs) in reporting information by introducing a generator-validator gap. It proposes a novel training method, RankAlign, to close this gap and improve model reliability.", "motivation": "To address the unreliability and inconsistency in LLMs' responses, particularly when prompts are altered, and to enhance the correlation between generated answers and their verification.", "method": "The paper defines a generator-validator gap based on score correlation of generated answers across various tasks. It proposes RankAlign, a ranking-based training method to reduce this gap.", "result": "The proposed RankAlign method significantly closes the generator-validator gap across several tasks, including question answering and lexical semantics, and shows generalization to out-of-domain tasks.", "conclusion": "RankAlign effectively improves the reliability of language models by aligning the outputs of generators better with their validations, thus minimizing inconsistencies in responses.", "key_contributions": ["Definition of a stringent generator-validator gap", "Introduction of the RankAlign ranking-based training method", "Demonstrated efficacy of RankAlign across multiple tasks"], "limitations": "", "keywords": ["large language models", "generator-validator gap", "RankAlign", "training method", "natural language processing"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2504.11582", "pdf": "https://arxiv.org/pdf/2504.11582.pdf", "abs": "https://arxiv.org/abs/2504.11582", "title": "AskQE: Question Answering as Automatic Evaluation for Machine Translation", "authors": ["Dayeon Ki", "Kevin Duh", "Marine Carpuat"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "How can a monolingual English speaker determine whether an automatic\ntranslation in French is good enough to be shared? Existing MT error detection\nand quality estimation (QE) techniques do not address this practical scenario.\nWe introduce AskQE, a question generation and answering framework designed to\ndetect critical MT errors and provide actionable feedback, helping users decide\nwhether to accept or reject MT outputs even without the knowledge of the target\nlanguage. Using ContraTICO, a dataset of contrastive synthetic MT errors in the\nCOVID-19 domain, we explore design choices for AskQE and develop an optimized\nversion relying on LLaMA-3 70B and entailed facts to guide question generation.\nWe evaluate the resulting system on the BioMQM dataset of naturally occurring\nMT errors, where AskQE has higher Kendall's Tau correlation and decision\naccuracy with human ratings compared to other QE metrics.", "AI": {"tldr": "AskQE is a framework that generates questions to assess the quality of machine translations for non-French speakers, enabling them to decide on sharing the translations.", "motivation": "Monolingual English speakers need a reliable way to evaluate the quality of automatic translations in languages they do not understand.", "method": "The framework generates questions and provides answers about machine translation errors using a dataset of contrastive synthetic MT errors and incorporates models like LLaMA-3 70B for optimization.", "result": "AskQE shows improved accuracy and correlation with human evaluations compared to existing quality estimation techniques on the BioMQM dataset.", "conclusion": "The AskQE framework successfully enhances the ability of users to assess MT outputs, making it a valuable tool for non-expert users.", "key_contributions": ["Introduction of the AskQE framework for MT error detection.", "Application of LLaMA-3 70B for question generation and evaluation.", "Demonstrated higher accuracy in quality assessment on the BioMQM dataset."], "limitations": "The system is primarily optimized for the COVID-19 domain and may not generalize to other contexts without further adaptation.", "keywords": ["Machine Translation", "Quality Estimation", "AskQE", "Question Generation", "MT Error Detection"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2504.14212", "pdf": "https://arxiv.org/pdf/2504.14212.pdf", "abs": "https://arxiv.org/abs/2504.14212", "title": "Bias Analysis and Mitigation through Protected Attribute Detection and Regard Classification", "authors": ["Takuma Udagawa", "Yang Zhao", "Hiroshi Kanayama", "Bishwaranjan Bhattacharjee"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025 (Findings)", "summary": "Large language models (LLMs) acquire general linguistic knowledge from\nmassive-scale pretraining. However, pretraining data mainly comprised of\nweb-crawled texts contain undesirable social biases which can be perpetuated or\neven amplified by LLMs. In this study, we propose an efficient yet effective\nannotation pipeline to investigate social biases in the pretraining corpora.\nOur pipeline consists of protected attribute detection to identify diverse\ndemographics, followed by regard classification to analyze the language\npolarity towards each attribute. Through our experiments, we demonstrate the\neffect of our bias analysis and mitigation measures, focusing on Common Crawl\nas the most representative pretraining corpus.", "AI": {"tldr": "The paper presents an annotation pipeline for investigating social biases in pretraining corpora of large language models (LLMs), specifically targeting biases in Web-crawled texts.", "motivation": "To address the issue of undesirable social biases in pretraining data for large language models (LLMs) that can be perpetuated or amplified by these models.", "method": "The proposed annotation pipeline includes steps for protected attribute detection to identify diverse demographics and a classification phase to analyze the language polarity towards these attributes.", "result": "The experiments demonstrate the effectiveness of the bias analysis and mitigation measures applied to the Common Crawl dataset, which is utilized as a representative pretraining corpus.", "conclusion": "The study highlights the importance of understanding and mitigating social biases in LLM training data to improve fairness in language model outputs.", "key_contributions": ["Introduction of a novel annotation pipeline for bias detection", "Application of protected attribute detection and regard classification", "Empirical analysis of biases in the Common Crawl dataset"], "limitations": "The study may not cover all forms of bias present in other datasets and models.", "keywords": ["social bias", "large language models", "annotation pipeline", "language polarity", "diverse demographics"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.22771", "pdf": "https://arxiv.org/pdf/2505.22771.pdf", "abs": "https://arxiv.org/abs/2505.22771", "title": "Automated Essay Scoring Incorporating Annotations from Automated Feedback Systems", "authors": ["Christopher Ormerod"], "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, AIME-Con Conference Submission", "summary": "This study illustrates how incorporating feedback-oriented annotations into\nthe scoring pipeline can enhance the accuracy of automated essay scoring (AES).\nThis approach is demonstrated with the Persuasive Essays for Rating, Selecting,\nand Understanding Argumentative and Discourse Elements (PERSUADE) corpus. We\nintegrate two types of feedback-driven annotations: those that identify\nspelling and grammatical errors, and those that highlight argumentative\ncomponents. To illustrate how this method could be applied in real-world\nscenarios, we employ two LLMs to generate annotations -- a generative language\nmodel used for spell correction and an encoder-based token-classifier trained\nto identify and mark argumentative elements. By incorporating annotations into\nthe scoring process, we demonstrate improvements in performance using\nencoder-based large language models fine-tuned as classifiers.", "AI": {"tldr": "This study explores enhancing automated essay scoring (AES) by integrating feedback-oriented annotations identifying errors and argumentative components, improving performance using LLMs.", "motivation": "To improve the accuracy of automated essay scoring systems, especially for educational assessments.", "method": "Feedback-driven annotations for spelling/grammatical errors and argumentative components were integrated into the AES pipeline, validated using the PERSUADE corpus.", "result": "The study showed improved scoring accuracy when feedback-oriented annotations were utilized in conjunction with LLMs for essay scoring.", "conclusion": "Incorporating feedback annotations into scoring processes can significantly enhance the effectiveness of automated essay scoring systems.", "key_contributions": ["Integration of feedback-oriented annotations in automated essay scoring", "Utilization of generative language models for corrections", "Demonstration of performance improvement with encoder-based LLMs"], "limitations": "", "keywords": ["automated essay scoring", "large language models", "feedback-oriented annotations", "spelling correction", "argumentative components"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.16217", "pdf": "https://arxiv.org/pdf/2507.16217.pdf", "abs": "https://arxiv.org/abs/2507.16217", "title": "Towards Compute-Optimal Many-Shot In-Context Learning", "authors": ["Shahriar Golchin", "Yanfei Chen", "Rujun Han", "Manan Gandhi", "Tianli Yu", "Swaroop Mishra", "Mihai Surdeanu", "Rishabh Agarwal", "Chen-Yu Lee", "Tomas Pfister"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Final version; accepted at COLM 2025", "summary": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL.", "AI": {"tldr": "This paper proposes two strategies for demonstration selection in many-shot in-context learning (ICL) for large language models (LLMs), enhancing performance while minimizing computational costs.", "motivation": "To improve the performance of many-shot ICL in large language models without incurring high inference costs and leveraging cached computations.", "method": "The paper introduces two methods: one combines a few similar demonstrations with a larger set of random demonstrations; the other replaces random demonstrations with those selected via k-means clustering based on test sample representations.", "result": "Experimental results show that these demonstration selection strategies consistently outperform random selection and match or surpass the best existing methods, while significantly reducing inference costs.", "conclusion": "The findings suggest that optimizing demonstration selection can lead to substantial improvements in efficiency and performance in many-shot ICL scenarios.", "key_contributions": ["Introduction of two novel strategies for demonstration selection in many-shot ICL", "Demonstration selection based on similarity and k-means clustering", "Reduction of inference costs by up to an order of magnitude"], "limitations": "", "keywords": ["long-context LLMs", "in-context learning", "demonstration selection"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.14146", "pdf": "https://arxiv.org/pdf/2508.14146.pdf", "abs": "https://arxiv.org/abs/2508.14146", "title": "MMReview: A Multidisciplinary and Multimodal Benchmark for LLM-Based Peer Review Automation", "authors": ["Xian Gao", "Jiacheng Ruan", "Zongyun Zhang", "Jingsheng Gao", "Ting Liu", "Yuzhuo Fu"], "categories": ["cs.CL"], "comment": "Work in progress", "summary": "With the rapid growth of academic publications, peer review has become an\nessential yet time-consuming responsibility within the research community.\nLarge Language Models (LLMs) have increasingly been adopted to assist in the\ngeneration of review comments; however, current LLM-based review tasks lack a\nunified evaluation benchmark to rigorously assess the models' ability to\nproduce comprehensive, accurate, and human-aligned assessments, particularly in\nscenarios involving multimodal content such as figures and tables. To address\nthis gap, we propose \\textbf{MMReview}, a comprehensive benchmark that spans\nmultiple disciplines and modalities. MMReview includes multimodal content and\nexpert-written review comments for 240 papers across 17 research domains within\nfour major academic disciplines: Artificial Intelligence, Natural Sciences,\nEngineering Sciences, and Social Sciences. We design a total of 13 tasks\ngrouped into four core categories, aimed at evaluating the performance of LLMs\nand Multimodal LLMs (MLLMs) in step-wise review generation, outcome\nformulation, alignment with human preferences, and robustness to adversarial\ninput manipulation. Extensive experiments conducted on 16 open-source models\nand 5 advanced closed-source models demonstrate the thoroughness of the\nbenchmark. We envision MMReview as a critical step toward establishing a\nstandardized foundation for the development of automated peer review systems.", "AI": {"tldr": "This paper introduces MMReview, a comprehensive benchmark designed to evaluate the ability of LLMs and MLLMs in generating peer review comments across multiple disciplines and modalities.", "motivation": "The lack of a unified evaluation benchmark for LLMs in the context of academic peer review, especially with multimodal content, creates difficulties in assessing their effectiveness.", "method": "The authors propose the MMReview benchmark, which includes 240 papers from 17 disciplines with expert-written review comments, and design 13 tasks to evaluate LLM and MLLM performance in review generation and alignment with human preferences.", "result": "Extensive experiments on 16 open-source and 5 closed-source models showcase the thorough evaluation capabilities of the MMReview benchmark.", "conclusion": "MMReview serves as a foundational step towards the development of standardized automated peer review systems in academia.", "key_contributions": ["Introduction of a comprehensive benchmark (MMReview) for evaluating LLMs in peer review tasks.", "Inclusion of multimodal content and expert comments across diverse research domains.", "Designing evaluation tasks that encompass various aspects of review generation performance."], "limitations": "", "keywords": ["Large Language Models", "peer review", "multimodal", "benchmark", "automated systems"], "importance_score": 8, "read_time_minutes": 10}}
