{"id": "2507.16922", "pdf": "https://arxiv.org/pdf/2507.16922.pdf", "abs": "https://arxiv.org/abs/2507.16922", "title": "A Unifying Scheme for Extractive Content Selection Tasks", "authors": ["Shmuel Amar", "Ori Shapira", "Aviv Slobodkin", "Ido Dagan"], "categories": ["cs.CL"], "comment": null, "summary": "A broad range of NLP tasks involve selecting relevant text spans from given\nsource texts. Despite this shared objective, such \\textit{content selection}\ntasks have traditionally been studied in isolation, each with its own modeling\napproaches, datasets, and evaluation metrics. In this work, we propose\n\\textit{instruction-guided content selection (IGCS)} as a beneficial unified\nframework for such settings, where the task definition and any\ninstance-specific request are encapsulated as instructions to a language model.\nTo promote this framework, we introduce \\igcsbench{}, the first unified\nbenchmark covering diverse content selection tasks. Further, we create a large\ngeneric synthetic dataset that can be leveraged for diverse content selection\ntasks, and show that transfer learning with these datasets often boosts\nperformance, whether dedicated training for the targeted task is available or\nnot. Finally, we address generic inference time issues that arise in LLM-based\nmodeling of content selection, assess a generic evaluation metric, and overall\npropose the utility of our resources and methods for future content selection\nmodels. Models and datasets available at https://github.com/shmuelamar/igcs.", "AI": {"tldr": "This paper introduces a unified framework for content selection tasks in NLP, proposing instruction-guided content selection (IGCS) and establishing a benchmark, igcsbench, alongside a large synthetic dataset to enhance performance across tasks.", "motivation": "To create a unified framework for content selection tasks, which have been studied in isolation, and to address the limitations in model performance and evaluation metrics.", "method": "The work proposes the IGCS framework which encapsulates task definitions and requests as instructions for language models. It establishes igcsbench as a benchmark and develops synthetic datasets for transfer learning.", "result": "The introduction of IGCS and igcsbench has shown improved performance in content selection tasks through transfer learning, regardless of the availability of dedicated training data.", "conclusion": "The proposed resources and methods are valuable for advancing content selection models and addressing common inference time issues in LLM-based approaches.", "key_contributions": ["Introduction of instruction-guided content selection (IGCS) as a unified framework.", "Development of igcsbench, the first benchmark for diverse content selection tasks.", "Creation of a large synthetic dataset to improve transfer learning in content selection."], "limitations": "", "keywords": ["NLP", "Content Selection", "Instruction-guided Learning", "Benchmarking", "Transfer Learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.16947", "pdf": "https://arxiv.org/pdf/2507.16947.pdf", "abs": "https://arxiv.org/abs/2507.16947", "title": "AI-based Clinical Decision Support for Primary Care: A Real-World Study", "authors": ["Robert Korom", "Sarah Kiptinness", "Najib Adan", "Kassim Said", "Catherine Ithuli", "Oliver Rotich", "Boniface Kimani", "Irene King'ori", "Stellah Kamau", "Elizabeth Atemba", "Muna Aden", "Preston Bowman", "Michael Sharman", "Rebecca Soskin Hicks", "Rebecca Distler", "Johannes Heidecke", "Rahul K. Arora", "Karan Singhal"], "categories": ["cs.CL"], "comment": "Blog: https://openai.com/index/ai-clinical-copilot-penda-health/", "summary": "We evaluate the impact of large language model-based clinical decision\nsupport in live care. In partnership with Penda Health, a network of primary\ncare clinics in Nairobi, Kenya, we studied AI Consult, a tool that serves as a\nsafety net for clinicians by identifying potential documentation and clinical\ndecision-making errors. AI Consult integrates into clinician workflows,\nactivating only when needed and preserving clinician autonomy. We conducted a\nquality improvement study, comparing outcomes for 39,849 patient visits\nperformed by clinicians with or without access to AI Consult across 15 clinics.\nVisits were rated by independent physicians to identify clinical errors.\nClinicians with access to AI Consult made relatively fewer errors: 16% fewer\ndiagnostic errors and 13% fewer treatment errors. In absolute terms, the\nintroduction of AI Consult would avert diagnostic errors in 22,000 visits and\ntreatment errors in 29,000 visits annually at Penda alone. In a survey of\nclinicians with AI Consult, all clinicians said that AI Consult improved the\nquality of care they delivered, with 75% saying the effect was \"substantial\".\nThese results required a clinical workflow-aligned AI Consult implementation\nand active deployment to encourage clinician uptake. We hope this study\ndemonstrates the potential for LLM-based clinical decision support tools to\nreduce errors in real-world settings and provides a practical framework for\nadvancing responsible adoption.", "AI": {"tldr": "The study evaluates the effectiveness of AI Consult, an LLM-based clinical decision support tool, in reducing diagnostic and treatment errors in live care settings.", "motivation": "To assess the impact of AI Consult in improving clinical decision-making and documentation accuracy in primary care.", "method": "A quality improvement study analyzing patient visit outcomes with and without access to AI Consult across 15 clinics, involving 39,849 patient visits.", "result": "Clinicians using AI Consult made 16% fewer diagnostic errors and 13% fewer treatment errors, potentially preventing 22,000 diagnostic and 29,000 treatment errors annually at Penda Health.", "conclusion": "The study demonstrates the efficacy of LLM-based clinical decision support in reducing errors and emphasizes the importance of integrating such tools into clinical workflows.", "key_contributions": ["Demonstrated the effectiveness of LLM-based decision support in real-world healthcare settings.", "Provided empirical evidence of reduced clinical errors with AI Consult use.", "Outlined a practical framework for implementing AI tools in clinical workflows."], "limitations": "", "keywords": ["clinical decision support", "large language models", "health informatics", "workflow integration", "error reduction"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.16951", "pdf": "https://arxiv.org/pdf/2507.16951.pdf", "abs": "https://arxiv.org/abs/2507.16951", "title": "Harnessing RLHF for Robust Unanswerability Recognition and Trustworthy Response Generation in LLMs", "authors": ["Shuyuan Lin", "Lei Duan", "Philip Hughes", "Yuxuan Sheng"], "categories": ["cs.CL"], "comment": null, "summary": "Conversational Information Retrieval (CIR) systems, while offering intuitive\naccess to information, face a significant challenge: reliably handling\nunanswerable questions to prevent the generation of misleading or hallucinated\ncontent. Traditional approaches often rely on external classifiers, which can\nintroduce inconsistencies with the core generative Large Language Models\n(LLMs). This paper introduces Self-Aware LLM for Unanswerability (SALU), a\nnovel approach that deeply integrates unanswerability detection directly within\nthe LLM's generative process. SALU is trained using a multi-task learning\nframework for both standard Question Answering (QA) and explicit abstention\ngeneration for unanswerable queries. Crucially, it incorporates a\nconfidence-score-guided reinforcement learning with human feedback (RLHF)\nphase, which explicitly penalizes hallucinated responses and rewards\nappropriate abstentions, fostering intrinsic self-awareness of knowledge\nboundaries. Through extensive experiments on our custom-built\nC-IR_Answerability dataset, SALU consistently outperforms strong baselines,\nincluding hybrid LLM-classifier systems, in overall accuracy for correctly\nanswering or abstaining from questions. Human evaluation further confirms\nSALU's superior reliability, achieving high scores in factuality, appropriate\nabstention, and, most importantly, a dramatic reduction in hallucination,\ndemonstrating its ability to robustly \"know when to say 'I don't know'.\"", "AI": {"tldr": "This paper presents SALU, a Self-Aware LLM for handling unanswerable questions in conversational information retrieval by integrating unanswerability detection within the generative process.", "motivation": "CIR systems struggle with unanswerable questions, risking misleading outputs and hallucinations.", "method": "SALU employs a multi-task learning framework combined with a reinforcement learning phase that incorporates human feedback to reduce hallucinations and improve accuracy.", "result": "SALU outperforms traditional LLM-classifier systems in accuracy for answering and abstaining from questions and demonstrates higher reliability as per human evaluations.", "conclusion": "SALU shows significant improvements in managing unanswerable queries, reducing hallucination while enhancing overall factuality and reliability.", "key_contributions": ["Introduction of Self-Aware LLM for unanswerability in CIR", "Multi-task learning integration for QA and abstention", "Reinforcement learning with human feedback to boost accuracy and reduce hallucination"], "limitations": "", "keywords": ["Conversational Information Retrieval", "Large Language Models", "Unanswerable Questions", "Reinforcement Learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.16971", "pdf": "https://arxiv.org/pdf/2507.16971.pdf", "abs": "https://arxiv.org/abs/2507.16971", "title": "Text-to-SPARQL Goes Beyond English: Multilingual Question Answering Over Knowledge Graphs through Human-Inspired Reasoning", "authors": ["Aleksandr Perevalov", "Andreas Both"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "During the final evaluation on the DBpedia- and Corporate-based KGQA\n  benchmarks within the Text2SPARQL challenge 2025, our approach took first\n  place among the other participants", "summary": "Accessing knowledge via multilingual natural-language interfaces is one of\nthe emerging challenges in the field of information retrieval and related ones.\nStructured knowledge stored in knowledge graphs can be queried via a specific\nquery language (e.g., SPARQL). Therefore, one needs to transform\nnatural-language input into a query to fulfill an information need. Prior\napproaches mostly focused on combining components (e.g., rule-based or\nneural-based) that solve downstream tasks and come up with an answer at the\nend. We introduce mKGQAgent, a human-inspired framework that breaks down the\ntask of converting natural language questions into SPARQL queries into modular,\ninterpretable subtasks. By leveraging a coordinated LLM agent workflow for\nplanning, entity linking, and query refinement - guided by an experience pool\nfor in-context learning - mKGQAgent efficiently handles multilingual KGQA.\nEvaluated on the DBpedia- and Corporate-based KGQA benchmarks within the\nText2SPARQL challenge 2025, our approach took first place among the other\nparticipants. This work opens new avenues for developing human-like reasoning\nsystems in multilingual semantic parsing.", "AI": {"tldr": "mKGQAgent is a framework that converts natural language questions into SPARQL queries through modular subtasks, utilizing a coordinated LLM agent workflow.", "motivation": "To address the challenge of accessing knowledge via multilingual natural-language interfaces and transforming natural-language input into queries for information retrieval.", "method": "mKGQAgent uses a human-inspired framework that breaks down the conversion task into modular, interpretable subtasks involving planning, entity linking, and query refinement, supported by in-context learning from an experience pool.", "result": "mKGQAgent achieved first place in the Text2SPARQL challenge 2025, demonstrating its effectiveness on DBpedia- and Corporate-based KGQA benchmarks.", "conclusion": "The work contributes to the development of human-like reasoning systems in multilingual semantic parsing.", "key_contributions": ["Introduction of mKGQAgent framework for KGQA", "Modular and interpretable subtasks for query conversion", "First-place performance in Text2SPARQL challenge 2025"], "limitations": "", "keywords": ["multilingual natural-language processing", "knowledge graphs", "query refinement"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.16819", "pdf": "https://arxiv.org/pdf/2507.16819.pdf", "abs": "https://arxiv.org/abs/2507.16819", "title": "Assessing Medical Training Skills via Eye and Head Movements", "authors": ["Kayhan Latifzadeh", "Luis A. Leiva", "Klen Čopič Pucihar", "Matjaž Kljun", "Iztok Devetak", "Lili Steblovnik"], "categories": ["cs.HC", "cs.CV"], "comment": null, "summary": "We examined eye and head movements to gain insights into skill development in\nclinical settings. A total of 24 practitioners participated in simulated baby\ndelivery training sessions. We calculated key metrics, including pupillary\nresponse rate, fixation duration, or angular velocity. Our findings indicate\nthat eye and head tracking can effectively differentiate between trained and\nuntrained practitioners, particularly during labor tasks. For example,\nhead-related features achieved an F1 score of 0.85 and AUC of 0.86, whereas\npupil-related features achieved F1 score of 0.77 and AUC of 0.85. The results\nlay the groundwork for computational models that support implicit skill\nassessment and training in clinical settings by using commodity eye-tracking\nglasses as a complementary device to more traditional evaluation methods such\nas subjective scores.", "AI": {"tldr": "This paper explores the use of eye and head tracking to assess and improve clinical skills in healthcare professionals during simulated training sessions.", "motivation": "To gain insights into skill development in clinical settings and enhance training methodologies.", "method": "Analysis of eye and head movements during simulated baby delivery training sessions using metrics like pupillary response rate and fixation duration.", "result": "Eye and head tracking metrics effectively distinguish between trained and untrained practitioners during clinical tasks, achieving high F1 scores and AUC values.", "conclusion": "The study supports the potential of eye-tracking technology as a valuable tool for skill assessment and enhancement in clinical training.", "key_contributions": ["Efficacy of eye and head tracking in skill differentiation", "High F1 and AUC scores demonstrating predictive power", "Foundation for computational models in clinical skill assessment"], "limitations": "", "keywords": ["Eye tracking", "Clinical training", "Skill assessment", "Human-Computer Interaction", "Machine Learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.16974", "pdf": "https://arxiv.org/pdf/2507.16974.pdf", "abs": "https://arxiv.org/abs/2507.16974", "title": "Leveraging Synthetic Data for Question Answering with Multilingual LLMs in the Agricultural Domain", "authors": ["Rishemjit Kaur", "Arshdeep Singh Bhankhar", "Surangika Ranathunga", "Jashanpreet Singh Salh", "Sudhir Rajput", "Vidhi", "Kashish Mahendra", "Bhavika Berwal", "Ritesh Kumar"], "categories": ["cs.CL", "cs.AI", "I.2.7; J.m"], "comment": "15 pages, 9 tables, Appendix A-K", "summary": "Enabling farmers to access accurate agriculture-related information in their\nnative languages in a timely manner is crucial for the success of the\nagriculture field. Although large language models (LLMs) can be used to\nimplement Question Answering (QA) systems, simply using publicly available\ngeneral-purpose LLMs in agriculture typically offer generic advisories, lacking\nprecision in local and multilingual contexts due to insufficient\ndomain-specific training and scarcity of high-quality, region-specific\ndatasets. Our study addresses these limitations by generating multilingual\nsynthetic agricultural datasets (English, Hindi, Punjabi) from\nagriculture-specific documents and fine-tuning language-specific LLMs. Our\nevaluation on curated multilingual datasets demonstrates significant\nimprovements in factual accuracy, relevance, and agricultural consensus for the\nfine-tuned models compared to their baseline counterparts. These results\nhighlight the efficacy of synthetic data-driven, language-specific fine-tuning\nas an effective strategy to improve the performance of LLMs in agriculture,\nespecially in multilingual and low-resource settings. By enabling more accurate\nand localized agricultural advisory services, this study provides a meaningful\nstep toward bridging the knowledge gap in AI-driven agricultural solutions for\ndiverse linguistic communities.", "AI": {"tldr": "This paper discusses the generation of multilingual synthetic agricultural datasets and the fine-tuning of language-specific LLMs to improve agricultural advisory services in various languages.", "motivation": "To provide accurate agricultural information in native languages for farmers, as existing LLMs offer generic and imprecise advice due to a lack of domain-specific training.", "method": "The study generates multilingual synthetic datasets from agriculture-specific documents in English, Hindi, and Punjabi, and fine-tunes language-specific LLMs using these datasets.", "result": "Fine-tuned models show significant improvements in factual accuracy, relevance, and consensus in agricultural knowledge compared to baseline models.", "conclusion": "Synthetic data-driven, language-specific fine-tuning enhances LLM performance in agriculture, facilitating better advisory services for multilingual communities.", "key_contributions": ["Generation of multilingual synthetic agricultural datasets", "Fine-tuning of language-specific LLMs", "Demonstration of improvements in accuracy and relevance of agricultural advisories"], "limitations": "Limited to the specific languages and contexts covered in the datasets; may not address all agricultural domains or languages.", "keywords": ["Multilingual LLMs", "Synthetic datasets", "Agricultural advisory systems"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2507.17024", "pdf": "https://arxiv.org/pdf/2507.17024.pdf", "abs": "https://arxiv.org/abs/2507.17024", "title": "Write, Rank, or Rate: Comparing Methods for Studying Visualization Affordances", "authors": ["Chase Stokes", "Kylie Lin", "Cindy Xiong Bearfield"], "categories": ["cs.HC", "H.5.0"], "comment": "11 pages, 8 figures, accepted to IEEE VIS", "summary": "A growing body of work on visualization affordances highlights how specific\ndesign choices shape reader takeaways from information visualizations. However,\nmapping the relationship between design choices and reader conclusions often\nrequires labor-intensive crowdsourced studies, generating large corpora of\nfree-response text for analysis. To address this challenge, we explored\nalternative scalable research methodologies to assess chart affordances. We\ntest four elicitation methods from human-subject studies: free response,\nvisualization ranking, conclusion ranking, and salience rating, and compare\ntheir effectiveness in eliciting reader interpretations of line charts, dot\nplots, and heatmaps. Overall, we find that while no method fully replicates\naffordances observed in free-response conclusions, combinations of ranking and\nrating methods can serve as an effective proxy at a broad scale. The two\nranking methodologies were influenced by participant bias towards certain chart\ntypes and the comparison of suggested conclusions. Rating conclusion salience\ncould not capture the specific variations between chart types observed in the\nother methods. To supplement this work, we present a case study with GPT-4o,\nexploring the use of large language models (LLMs) to elicit human-like chart\ninterpretations. This aligns with recent academic interest in leveraging LLMs\nas proxies for human participants to improve data collection and analysis\nefficiency. GPT-4o performed best as a human proxy for the salience rating\nmethodology but suffered from severe constraints in other areas. Overall, the\ndiscrepancies in affordances we found between various elicitation\nmethodologies, including GPT-4o, highlight the importance of intentionally\nselecting and combining methods and evaluating trade-offs.", "AI": {"tldr": "The paper explores scalable methodologies for assessing chart affordances through various elicitation methods, including a case study using GPT-4o as a proxy for human interpretation.", "motivation": "To address the labor-intensive nature of analyzing reader interactions with information visualizations by identifying effective research methodologies for studying chart affordances.", "method": "The study tests four elicitation methods: free response, visualization ranking, conclusion ranking, and salience rating, comparing their effectiveness in gathering reader interpretations of different types of charts.", "result": "None of the tested elicitation methods fully replicated affordances from free-response conclusions; however, combinations of ranking and rating methods can act as effective proxies.", "conclusion": "The findings indicate significant discrepancies between methods and the importance of carefully selecting and combining methodologies while considering their trade-offs.", "key_contributions": ["Testing and comparing four elicitation methods for chart interpretation", "Introduction of large language models as human proxies in visualization studies", "Highlighting the influence of participant bias in ranking methodologies"], "limitations": "GPT-4o faced severe constraints outside of the salience rating methodology, suggesting limitations in its use as a proxy.", "keywords": ["chart affordances", "human-computer interaction", "large language models"], "importance_score": 8, "read_time_minutes": 11}}
{"id": "2507.16989", "pdf": "https://arxiv.org/pdf/2507.16989.pdf", "abs": "https://arxiv.org/abs/2507.16989", "title": "Obscured but Not Erased: Evaluating Nationality Bias in LLMs via Name-Based Bias Benchmarks", "authors": ["Giulio Pelosio", "Devesh Batra", "Noémie Bovey", "Robert Hankache", "Cristovao Iglesias", "Greig Cowan", "Raad Khraishi"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) can exhibit latent biases towards specific\nnationalities even when explicit demographic markers are not present. In this\nwork, we introduce a novel name-based benchmarking approach derived from the\nBias Benchmark for QA (BBQ) dataset to investigate the impact of substituting\nexplicit nationality labels with culturally indicative names, a scenario more\nreflective of real-world LLM applications. Our novel approach examines how this\nsubstitution affects both bias magnitude and accuracy across a spectrum of LLMs\nfrom industry leaders such as OpenAI, Google, and Anthropic. Our experiments\nshow that small models are less accurate and exhibit more bias compared to\ntheir larger counterparts. For instance, on our name-based dataset and in the\nambiguous context (where the correct choice is not revealed), Claude Haiku\nexhibited the worst stereotypical bias scores of 9%, compared to only 3.5% for\nits larger counterpart, Claude Sonnet, where the latter also outperformed it by\n117.7% in accuracy. Additionally, we find that small models retain a larger\nportion of existing errors in these ambiguous contexts. For example, after\nsubstituting names for explicit nationality references, GPT-4o retains 68% of\nthe error rate versus 76% for GPT-4o-mini, with similar findings for other\nmodel providers, in the ambiguous context. Our research highlights the stubborn\nresilience of biases in LLMs, underscoring their profound implications for the\ndevelopment and deployment of AI systems in diverse, global contexts.", "AI": {"tldr": "This paper examines how substituting explicit nationality labels with culturally indicative names affects bias and accuracy in LLMs.", "motivation": "The research investigates latent biases in LLMs without explicit demographic markers, focusing on real-world implications of these biases.", "method": "A novel name-based benchmarking approach derived from the Bias Benchmark for QA (BBQ) dataset was used to compare bias magnitude and accuracy across different LLMs.", "result": "Smaller models show higher bias and lower accuracy than larger models; for example, Claude Haiku had a bias score of 9% compared to 3.5% for Claude Sonnet, with significant differences in accuracy.", "conclusion": "The study emphasizes the resilience of biases in LLMs, which has critical implications for AI system development in diverse contexts.", "key_contributions": ["Introduction of a name-based benchmarking approach for LLMs", "Demonstration of bias differences between small and large models", "Insights into error retention rates in ambiguous contexts"], "limitations": "", "keywords": ["Large Language Models", "bias", "natural language processing", "cultural names", "AI systems"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.17139", "pdf": "https://arxiv.org/pdf/2507.17139.pdf", "abs": "https://arxiv.org/abs/2507.17139", "title": "Evaluation of the effects of frame time variation on VR task performance", "authors": ["Benjamin Watson", "Victoria Spaulding", "Neff Walker", "William Ribarsky"], "categories": ["cs.HC", "cs.ET"], "comment": null, "summary": "We present a first study of the effects of frame time variations, in both\ndeviation around mean frame times and period of fluctuation, on task\nperformance in a virtual environment (VE). Chosen are open and closed loop\ntasks that are typical for current applications or likely to be prominent in\nfuture ones. The results show that at frame times in the range deemed\nacceptable for many applications, fairly large deviations in amplitude over a\nfairly wide range of periods do not significantly affect task performance.\nHowever, at a frame time often considered a minimum for immersive VR, frame\ntime variations do produce significant effects on closed loop task performance.\nThe results will be of use to designers of VEs and immersive applications, who\noften must control frame time variations due to large fluctuations of\ncomplexity (graphical and otherwise) in the VE.", "AI": {"tldr": "Study on how frame time variations impact task performance in virtual environments (VEs).", "motivation": "Understanding the effects of frame time variations is essential for designers of virtual environments and immersive applications.", "method": "The study involved testing both open and closed loop tasks in a virtual environment, measuring the impact of varying frame times on task performance.", "result": "Large deviations in frame times do not significantly affect performance for many applications; however, at lower frame thresholds, variations impact closed loop task performance significantly.", "conclusion": "Designers must consider frame time variations, especially at lower limits to ensure immersive experiences in VEs.", "key_contributions": ["First study to analyze the effects of frame time variations on task performance in VEs.", "Identifies significant impacts of frame time variations on closed loop tasks at immersive frame rates.", "Provides insights for the design of immersive applications"], "limitations": "The study focuses on specific types of tasks and environments; results may not generalize to all virtual environment designs.", "keywords": ["frame time variations", "task performance", "virtual environments", "immersive applications", "closed loop tasks"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2507.17009", "pdf": "https://arxiv.org/pdf/2507.17009.pdf", "abs": "https://arxiv.org/abs/2507.17009", "title": "Multi-Label Classification with Generative AI Models in Healthcare: A Case Study of Suicidality and Risk Factors", "authors": ["Ming Huang", "Zehan Li", "Yan Hu", "Wanjing Wang", "Andrew Wen", "Scott Lane", "Salih Selek", "Lokesh Shahani", "Rodrigo Machado-Vieira", "Jair Soares", "Hua Xu", "Hongfang Liu"], "categories": ["cs.CL", "cs.IR", "q-bio.QM"], "comment": null, "summary": "Suicide remains a pressing global health crisis, with over 720,000 deaths\nannually and millions more affected by suicide ideation (SI) and suicide\nattempts (SA). Early identification of suicidality-related factors (SrFs),\nincluding SI, SA, exposure to suicide (ES), and non-suicidal self-injury\n(NSSI), is critical for timely intervention. While prior studies have applied\nAI to detect SrFs in clinical notes, most treat suicidality as a binary\nclassification task, overlooking the complexity of cooccurring risk factors.\nThis study explores the use of generative large language models (LLMs),\nspecifically GPT-3.5 and GPT-4.5, for multi-label classification (MLC) of SrFs\nfrom psychiatric electronic health records (EHRs). We present a novel end to\nend generative MLC pipeline and introduce advanced evaluation methods,\nincluding label set level metrics and a multilabel confusion matrix for error\nanalysis. Finetuned GPT-3.5 achieved top performance with 0.94 partial match\naccuracy and 0.91 F1 score, while GPT-4.5 with guided prompting showed superior\nperformance across label sets, including rare or minority label sets,\nindicating a more balanced and robust performance. Our findings reveal\nsystematic error patterns, such as the conflation of SI and SA, and highlight\nthe models tendency toward cautious over labeling. This work not only\ndemonstrates the feasibility of using generative AI for complex clinical\nclassification tasks but also provides a blueprint for structuring unstructured\nEHR data to support large scale clinical research and evidence based medicine.", "AI": {"tldr": "This study leverages generative large language models (LLMs) for multi-label classification of suicidality-related factors from psychiatric electronic health records, achieving high performance metrics.", "motivation": "Address the urgent need for early identification of suicidality-related factors (SrFs) in the context of a global mental health crisis.", "method": "Employs generative LLMs (GPT-3.5 and GPT-4.5) for multi-label classification of SrFs, incorporating advanced evaluation techniques for better error analysis.", "result": "Finetuned GPT-3.5 reached 0.94 partial match accuracy and 0.91 F1 score; GPT-4.5 showed improved performance across diverse label sets, highlighting systematic error patterns.", "conclusion": "The research demonstrates the potential of generative AI in clinical classification tasks and suggests methods for structuring EHR data for enhanced clinical research.", "key_contributions": ["Novel end-to-end generative multi-label classification pipeline", "Introduction of advanced evaluation methods for clinical tasks", "Analysis of systematic error patterns in AI-generated classifications"], "limitations": "", "keywords": ["suicidality", "large language models", "classification", "electronic health records", "AI in healthcare"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.17209", "pdf": "https://arxiv.org/pdf/2507.17209.pdf", "abs": "https://arxiv.org/abs/2507.17209", "title": "HypoChainer: A Collaborative System Combining LLMs and Knowledge Graphs for Hypothesis-Driven Scientific Discovery", "authors": ["Haoran Jiang", "Shaohan Shi", "Yunjie Yao", "Chang Jiang", "Quan Li"], "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "Modern scientific discovery faces growing challenges in integrating vast and\nheterogeneous knowledge critical to breakthroughs in biomedicine and drug\ndevelopment. Traditional hypothesis-driven research, though effective, is\nconstrained by human cognitive limits, the complexity of biological systems,\nand the high cost of trial-and-error experimentation. Deep learning models,\nespecially graph neural networks (GNNs), have accelerated prediction\ngeneration, but the sheer volume of outputs makes manual selection for\nvalidation unscalable. Large language models (LLMs) offer promise in filtering\nand hypothesis generation, yet suffer from hallucinations and lack grounding in\nstructured knowledge, limiting their reliability. To address these issues, we\npropose HypoChainer, a collaborative visualization framework that integrates\nhuman expertise, LLM-driven reasoning, and knowledge graphs (KGs) to enhance\nhypothesis generation and validation. HypoChainer operates in three stages:\nFirst, exploration and contextualization -- experts use retrieval-augmented\nLLMs (RAGs) and dimensionality reduction to navigate large-scale GNN\npredictions, assisted by interactive explanations. Second, hypothesis chain\nformation -- experts iteratively examine KG relationships around predictions\nand semantically linked entities, refining hypotheses with LLM and KG\nsuggestions. Third, validation prioritization -- refined hypotheses are\nfiltered based on KG-supported evidence to identify high-priority candidates\nfor experimentation, with visual analytics further strengthening weak links in\nreasoning. We demonstrate HypoChainer's effectiveness through case studies in\ntwo domains and expert interviews, highlighting its potential to support\ninterpretable, scalable, and knowledge-grounded scientific discovery.", "AI": {"tldr": "HypoChainer is a visualization framework that integrates human expertise with LLMs and knowledge graphs to enhance the process of hypothesis generation and validation in scientific discovery.", "motivation": "To address challenges in integrating vast and heterogeneous knowledge in biomedical research and drug development, which are overwhelmed by complex biological systems and limitations of traditional research methods.", "method": "HypoChainer operates in three stages: exploration and contextualization using retrieval-augmented LLMs and dimensionality reduction, hypothesis chain formation with expert examination of KG relationships, and validation prioritization based on KG-supported evidence.", "result": "Case studies demonstrate HypoChainer's effectiveness in supporting interpretable and scalable scientific discovery, allowing for better hypothesis generation and validation driven by LLMs and expert insights.", "conclusion": "HypoChainer shows promise in enhancing the reliability and efficiency of hypothesis generation and validation through a collaborative framework that combines human and machine intelligence.", "key_contributions": ["Integration of human expertise with LLMs and KGs for hypothesis generation.", "Three-stage process enhances scalability and interpretability of scientific discovery.", "Demonstrated effectiveness through case studies and expert interviews."], "limitations": "", "keywords": ["hypothesis generation", "graph neural networks", "large language models", "knowledge graphs", "biomedical research"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.17015", "pdf": "https://arxiv.org/pdf/2507.17015.pdf", "abs": "https://arxiv.org/abs/2507.17015", "title": "Can External Validation Tools Improve Annotation Quality for LLM-as-a-Judge?", "authors": ["Arduin Findeis", "Floris Weers", "Guoli Yin", "Ke Ye", "Ruoming Pang", "Tom Gunter"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ACL 2025", "summary": "Pairwise preferences over model responses are widely collected to evaluate\nand provide feedback to large language models (LLMs). Given two alternative\nmodel responses to the same input, a human or AI annotator selects the \"better\"\nresponse. This approach can provide feedback for domains where other hard-coded\nmetrics are difficult to obtain (e.g., chat response quality), thereby helping\nmodel evaluation or training. However, for some domains high-quality pairwise\ncomparisons can be tricky to obtain - from AI and humans. For example, for\nresponses with many factual statements, annotators may disproportionately weigh\nwriting quality rather than underlying facts. In this work, we explore\naugmenting standard AI annotator systems with additional tools to improve\nperformance on three challenging response domains: long-form factual, math and\ncode tasks. We propose a tool-using agentic system to provide higher quality\nfeedback on these domains. Our system uses web-search and code execution to\nground itself based on external validation, independent of the LLM's internal\nknowledge and biases. We provide extensive experimental results evaluating our\nmethod across the three targeted response domains as well as general annotation\ntasks, using RewardBench (incl. AlpacaEval and LLMBar), RewardMath, as well as\nthree new datasets for domains with saturated pre-existing datasets. Our\nresults indicate that external tools can indeed improve performance in many,\nbut not all, cases. More generally, our experiments highlight the sensitivity\nof performance to simple parameters (e.g., prompt) and the need for improved\n(non-saturated) annotator benchmarks. We share our code at\nhttps://github.com/apple/ml-agent-evaluator.", "AI": {"tldr": "The paper proposes a tool-using agentic system to enhance the evaluation of large language models (LLMs) by using external validation via web-search and code execution, particularly for challenging domains such as long-form factual, math, and code tasks.", "motivation": "To improve the quality of feedback on LLM responses, especially in domains where traditional evaluation metrics are inadequate or biased, such as factual accuracy versus writing quality.", "method": "The authors developed a system that integrates tool usage (web-search and code execution) to provide ground-truth validation for evaluating responses, thereby reducing reliance on internal LLM knowledge.", "result": "Experimental results show that the proposed system can enhance performance in several domains but reveals that performance is highly sensitive to parameters like prompts.", "conclusion": "External tools can improve LLM evaluation performance but are not universally effective, indicating the need for better annotator benchmarks and more controlled evaluation methods.", "key_contributions": ["Introduction of a tool-using agentic system for LLM evaluation.", "Demonstration of improved feedback quality in challenging response domains.", "Highlighting sensitivity of performance to evaluation parameters."], "limitations": "Not all domains benefited from the external tools; results showed variability depending on specific contexts.", "keywords": ["Large Language Models", "Human-Computer Interaction", "Tool-using Systems", "Response Evaluation", "Machine Learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.17218", "pdf": "https://arxiv.org/pdf/2507.17218.pdf", "abs": "https://arxiv.org/abs/2507.17218", "title": "OceanVive: An Immersive Visualization System for Communicating Complex Oceanic Phenomena", "authors": ["Yang Ouyang", "Yuchen Wu", "Xiyuan Wang", "Laixin Xie", "Weicong Cheng", "Jianping Gan", "Quan Li", "Xiaojuan Ma"], "categories": ["cs.HC"], "comment": "To appear at the IEEE VIS Conference 2025", "summary": "Communicating the complexity of oceanic phenomena-such as hypoxia and\nacidification-poses a persistent challenge for marine science. Despite advances\nin sensing technologies and computational models, conventional formats like\nstatic visualizations and text-based reports often fall short in conveying the\ndynamics of ocean changes. To address this gap, we present OceanVive, an\nimmersive and interactive visualization system that transforms complex ocean\ndatasets into navigable spatial narratives. OceanVive incorporates an\nexploratory panel on a table-sized tablet for managing immersive content on a\nlarge screen and integrates adaptive visual encodings, contextual storytelling,\nand intuitive navigation pathways to support effective communication. We\nvalidate the system through expert interviews, demonstrating its potential to\nenhance science communication and promote deeper public understanding.", "AI": {"tldr": "OceanVive is an immersive visualization system designed to improve the communication of complex oceanic phenomena such as hypoxia and acidification.", "motivation": "Marine science struggles to effectively communicate complex oceanic phenomena due to limitations of traditional visualization and reporting methods.", "method": "The system uses a table-sized tablet combined with a large screen to create navigable spatial narratives, incorporating adaptive visual encodings and storytelling techniques.", "result": "Experts validate OceanVive as a tool that enhances science communication and fosters public understanding of ocean changes.", "conclusion": "OceanVive shows promise in transforming how complex ocean datasets are presented and interpreted by the public.", "key_contributions": ["Development of an immersive visualization system for ocean data", "Integration of adaptive visual encodings and storytelling in scientific communication", "Validation through expert interviews to demonstrate efficacy"], "limitations": "", "keywords": ["ocean visualization", "immersive system", "science communication"], "importance_score": 2, "read_time_minutes": 5}}
{"id": "2507.17025", "pdf": "https://arxiv.org/pdf/2507.17025.pdf", "abs": "https://arxiv.org/abs/2507.17025", "title": "Evolutionary Feature-wise Thresholding for Binary Representation of NLP Embeddings", "authors": ["Soumen Sinha", "Shahryar Rahnamayan", "Azam Asilian Bidgoli"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Efficient text embedding is crucial for large-scale natural language\nprocessing (NLP) applications, where storage and computational efficiency are\nkey concerns. In this paper, we explore how using binary representations\n(barcodes) instead of real-valued features can be used for NLP embeddings\nderived from machine learning models such as BERT. Thresholding is a common\nmethod for converting continuous embeddings into binary representations, often\nusing a fixed threshold across all features. We propose a Coordinate\nSearch-based optimization framework that instead identifies the optimal\nthreshold for each feature, demonstrating that feature-specific thresholds lead\nto improved performance in binary encoding. This ensures that the binary\nrepresentations are both accurate and efficient, enhancing performance across\nvarious features. Our optimal barcode representations have shown promising\nresults in various NLP applications, demonstrating their potential to transform\ntext representation. We conducted extensive experiments and statistical tests\non different NLP tasks and datasets to evaluate our approach and compare it to\nother thresholding methods. Binary embeddings generated using using optimal\nthresholds found by our method outperform traditional binarization methods in\naccuracy. This technique for generating binary representations is versatile and\ncan be applied to any features, not just limited to NLP embeddings, making it\nuseful for a wide range of domains in machine learning applications.", "AI": {"tldr": "The paper proposes a Coordinate Search-based optimization framework for creating binary embeddings in NLP, which identifies optimal thresholds for features, enhancing accuracy and efficiency in text representation.", "motivation": "The need for efficient text embedding techniques in large-scale NLP applications where storage and computational efficiency are critical.", "method": "A Coordinate Search-based optimization framework that identifies feature-specific optimal thresholds for converting continuous embeddings into binary representations.", "result": "Binary embeddings generated using the proposed method outperform traditional binarization methods in terms of accuracy across various NLP tasks and datasets.", "conclusion": "The proposed technique for generating binary representations is versatile, applicable beyond NLP, and shows promise for improving performance in machine learning applications.", "key_contributions": ["Introduction of a new Coordinate Search-based optimization framework for feature-specific threshold identification", "Demonstrated improved performance of binary embeddings in NLP applications", "Versatility of the technique for various machine learning domains"], "limitations": "", "keywords": ["text embedding", "binary representations", "NLP", "machine learning", "binarization"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.17226", "pdf": "https://arxiv.org/pdf/2507.17226.pdf", "abs": "https://arxiv.org/abs/2507.17226", "title": "A \"watch your replay videos\" reflection assignment on comparing programming without versus with generative AI: learning about programming, critical AI use and limitations, and reflection", "authors": ["Sarah \"Magz\" Fernandez", "Greg L Nelson"], "categories": ["cs.HC", "K.3"], "comment": null, "summary": "Generative AI is disrupting computing education. Most interventions focus on\nteaching GenAI use rather than helping students understand how AI changes their\nprogramming process. We designed and deployed a novel comparative video\nreflection assignment adapting the Describe, Examine, then Articulate Learning\n(DEAL) framework. In an introductory software engineering course, students\nrecorded themselves programming during their team project two times: first\nwithout, then with using generative AI. Students then analyzed their own videos\nusing a scaffolded set of reflection questions, including on their programming\nprocess and human, internet, and AI help-seeking. We conducted a qualitative\nthematic analysis of the reflections, finding students developed insights about\nplanning, debugging, and help-seeking behaviors that transcended AI use.\nStudents reported learning to slow down and understand before writing or\ngenerating code, recognized patterns in their problem-solving approaches, and\narticulated specific process improvements. Students also learned and reflected\non AI limits and downsides, and strategies to use AI more critically, including\nbetter prompting but also to benefit their learning instead of just completing\ntasks. Unexpectedly, the comparative reflection also scaffolded reflection on\nprogramming not involving AI use, and even led to students spontaneously\nsetting future goals to adopt video and other regular reflection. This work\ndemonstrates structured reflection on programming session videos can develop\nmetacognitive skills essential for programming with and without generative AI\nand also lifelong learning in our evolving field.", "AI": {"tldr": "This paper explores a novel assignment using video reflections in software engineering education to help students understand the impact of generative AI on their programming processes.", "motivation": "To address the gap in computing education where most interventions focus on using generative AI rather than understanding its implications on programming.", "method": "A comparative video reflection assignment was designed, where students recorded their programming sessions first without and then with generative AI, followed by thematic analysis of their reflections.", "result": "Students gained insights into their programming behavior, learning to critically evaluate their use of AI, recognize patterns, and improve their programming processes.", "conclusion": "Structured reflection on programming videos fosters metacognitive skills and lifelong learning, essential for adapting to generative AI in software development.", "key_contributions": ["Developed a comparative video reflection assignment to enhance understanding of AI in programming education.", "Found that students learned to critically engage with AI rather than simply using it as a tool.", "Demonstrated the importance of metacognitive skills in programming with and without the aid of AI."], "limitations": "", "keywords": ["Generative AI", "programming education", "video reflections", "metacognition", "software engineering"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.17147", "pdf": "https://arxiv.org/pdf/2507.17147.pdf", "abs": "https://arxiv.org/abs/2507.17147", "title": "CogDual: Enhancing Dual Cognition of LLMs via Reinforcement Learning with Implicit Rule-Based Rewards", "authors": ["Cheng Liu", "Yifei Lu", "Fanghua Ye", "Jian Li", "Xingyu Chen", "Feiliang Ren", "Zhaopeng Tu", "Xiaolong Li"], "categories": ["cs.CL"], "comment": null, "summary": "Role-Playing Language Agents (RPLAs) have emerged as a significant\napplication direction for Large Language Models (LLMs). Existing approaches\ntypically rely on prompt engineering or supervised fine-tuning to enable models\nto imitate character behaviors in specific scenarios, but often neglect the\nunderlying \\emph{cognitive} mechanisms driving these behaviors. Inspired by\ncognitive psychology, we introduce \\textbf{CogDual}, a novel RPLA adopting a\n\\textit{cognize-then-respond } reasoning paradigm. By jointly modeling external\nsituational awareness and internal self-awareness, CogDual generates responses\nwith improved character consistency and contextual alignment. To further\noptimize the performance, we employ reinforcement learning with two\ngeneral-purpose reward schemes designed for open-domain text generation.\nExtensive experiments on the CoSER benchmark, as well as Cross-MR and\nLifeChoice, demonstrate that CogDual consistently outperforms existing\nbaselines and generalizes effectively across diverse role-playing tasks.", "AI": {"tldr": "CogDual is a Role-Playing Language Agent that enhances character consistency and contextual alignment using a cognize-then-respond reasoning paradigm, outperforming existing models in various benchmarks.", "motivation": "Existing models for Role-Playing Language Agents often rely on prompt engineering and supervised fine-tuning, neglecting the cognitive mechanisms behind character behaviors.", "method": "CogDual employs a cognize-then-respond reasoning paradigm, modeling both situational awareness and self-awareness, along with reinforcement learning and general-purpose reward schemes for text generation.", "result": "CogDual outperforms existing baselines on the CoSER benchmark and generalizes well across diverse role-playing tasks.", "conclusion": "CogDual represents a significant advancement in Role-Playing Language Agents, effectively integrating cognitive psychology principles.", "key_contributions": ["Introduction of the cognize-then-respond reasoning paradigm.", "Joint modeling of situational and self-awareness in response generation.", "Use of reinforcement learning with novel reward schemes for improved performance."], "limitations": "", "keywords": ["Role-Playing Language Agents", "Cognitive Psychology", "Reinforcement Learning"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2507.17230", "pdf": "https://arxiv.org/pdf/2507.17230.pdf", "abs": "https://arxiv.org/abs/2507.17230", "title": "Designing for Learning with Generative AI is a Wicked Problem: An Illustrative Longitudinal Qualitative Case Series", "authors": ["Clara Scalzer", "Saurav Pokhrel", "Sara Hunt", "Greg L Nelson"], "categories": ["cs.HC", "K.3"], "comment": null, "summary": "Students continue their education when they feel their learning is meaningful\nand relevant for their future careers. Computing educators now face the\nchallenge of preparing students for careers increasingly shaped by generative\nAI (GenAI) with the goals of supporting their learning, motivation, ethics, and\ncareer development. Our longitudinal qualitative study of students in a\nGenAI-integrated creative media course shows how this is a \"wicked\" problem:\nprogress on one goal can then impede progress on other goals. Students\ndeveloped concerning patterns despite extensive instruction in critical and\nethical GenAI use including prompt engineering, ethics and bias, and industry\npanels on GenAI's career impact. We present an analysis of two students'\nexperiences to showcase this complexity. Increasing GenAI use skills can lower\nethics; for example, Pat started from purposefully avoiding GenAI use, to\ndependency. He described himself as a \"notorious cheater\" who now uses GenAi to\n\"get all the right answers\" while acknowledging he's learning less. Increasing\nethical awareness can lower the learning of GenAI use skills; for example,\nJay's newfound environmental concerns led to self-imposed usage limits that\nimpeded skill development, and new serious fears that GenAI would eliminate\ncreative careers they had been passionate about. Increased GenAI proficiency, a\npotential career skill, did not improve their career confidence. These findings\nsuggest that supporting student development in the GenAI era is a \"wicked\"\nproblem requiring multi-dimensional evaluation and design, rather than\noptimizing learning, GenAI skills, ethics, or career motivation individually.", "AI": {"tldr": "This study explores the complexities of integrating generative AI into education, highlighting how promoting GenAI skills can conflict with students' ethical concerns and learning outcomes.", "motivation": "To address the challenges computing educators face in preparing students for careers increasingly influenced by generative AI, focusing on their learning, motivation, ethics, and career development.", "method": "A longitudinal qualitative study analyzing students' experiences in a GenAI-integrated creative media course, involving extensive instruction on critical GenAI use, ethics, bias, and industry impacts.", "result": "The study reveals that while increasing GenAI skills can lead to ethical compromises, enhancing ethical awareness can hinder skill development, resulting in a complex relationship among learning, ethics, and career confidence.", "conclusion": "Supporting student development in the era of GenAI requires a multi-dimensional approach to evaluation and design, rather than optimizing specific separate elements like learning, skills, ethics, or motivation.", "key_contributions": ["Identifies the conflict between GenAI proficiency and ethical concerns in students.", "Highlights the complexities of student motivation and learning in the context of GenAI.", "Proposes a multi-dimensional approach to addressing educational challenges related to GenAI."], "limitations": "The study is based on a qualitative analysis of just two students, which may limit the generalizability of the findings.", "keywords": ["Generative AI", "Education", "Ethics", "Learning", "Career Development"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.17178", "pdf": "https://arxiv.org/pdf/2507.17178.pdf", "abs": "https://arxiv.org/abs/2507.17178", "title": "SKA-Bench: A Fine-Grained Benchmark for Evaluating Structured Knowledge Understanding of LLMs", "authors": ["Zhiqiang Liu", "Enpei Niu", "Yin Hua", "Mengshu Sun", "Lei Liang", "Huajun Chen", "Wen Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Although large language models (LLMs) have made significant progress in\nunderstanding Structured Knowledge (SK) like KG and Table, existing evaluations\nfor SK understanding are non-rigorous (i.e., lacking evaluations of specific\ncapabilities) and focus on a single type of SK. Therefore, we aim to propose a\nmore comprehensive and rigorous structured knowledge understanding benchmark to\ndiagnose the shortcomings of LLMs. In this paper, we introduce SKA-Bench, a\nStructured Knowledge Augmented QA Benchmark that encompasses four widely used\nstructured knowledge forms: KG, Table, KG+Text, and Table+Text. We utilize a\nthree-stage pipeline to construct SKA-Bench instances, which includes a\nquestion, an answer, positive knowledge units, and noisy knowledge units. To\nevaluate the SK understanding capabilities of LLMs in a fine-grained manner, we\nexpand the instances into four fundamental ability testbeds: Noise Robustness,\nOrder Insensitivity, Information Integration, and Negative Rejection. Empirical\nevaluations on 8 representative LLMs, including the advanced DeepSeek-R1,\nindicate that existing LLMs still face significant challenges in understanding\nstructured knowledge, and their performance is influenced by factors such as\nthe amount of noise, the order of knowledge units, and hallucination\nphenomenon. Our dataset and code are available at\nhttps://github.com/Lza12a/SKA-Bench.", "AI": {"tldr": "The paper introduces SKA-Bench, a benchmark for evaluating large language models' understanding of structured knowledge, highlighting their shortcomings in various capabilities.", "motivation": "The need for a rigorous evaluation of large language models' understanding of structured knowledge forms, as existing evaluations are limited and not comprehensive.", "method": "The paper presents SKA-Bench, a three-stage pipeline for creating benchmark instances that test LLM capabilities in areas like noise robustness and information integration.", "result": "Empirical evaluations reveal that current LLMs struggle significantly with structured knowledge understanding and their performance varies based on noise and order of the knowledge units.", "conclusion": "The findings underscore the challenges LLMs face in structured knowledge understanding, indicating the necessity for improved models and approaches in this domain.", "key_contributions": ["Introduction of SKA-Bench as a comprehensive benchmark for structured knowledge understanding", "Detailed evaluation of LLM capabilities across multiple structured knowledge formats", "Identification of specific challenges faced by existing models in SK understanding"], "limitations": "The evaluation is limited to 8 LLMs and may not represent all possible variations and contexts of structured knowledge understanding.", "keywords": ["Structured Knowledge", "Large Language Models", "Benchmarking", "QA Benchmark", "Noise Robustness"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.17242", "pdf": "https://arxiv.org/pdf/2507.17242.pdf", "abs": "https://arxiv.org/abs/2507.17242", "title": "High-Density EEG Enables the Fastest Visual Brain-Computer Interfaces", "authors": ["Gege Ming", "Weihua Pei", "Sen Tian", "Xiaogang Chen", "Xiaorong Gao", "Yijun Wang"], "categories": ["cs.HC", "eess.SP", "q-bio.NC"], "comment": null, "summary": "Brain-computer interface (BCI) technology establishes a direct communication\npathway between the brain and external devices. Current visual BCI systems\nsuffer from insufficient information transfer rates (ITRs) for practical use.\nSpatial information, a critical component of visual perception, remains\nunderexploited in existing systems because the limited spatial resolution of\nrecording methods hinders the capture of the rich spatiotemporal dynamics of\nbrain signals. This study proposed a frequency-phase-space fusion encoding\nmethod, integrated with 256-channel high-density electroencephalogram (EEG)\nrecordings, to develop high-speed BCI systems. In the classical frequency-phase\nencoding 40-target BCI paradigm, the 256-66, 128-32, and 64-21 electrode\nconfigurations brought theoretical ITR increases of 83.66%, 79.99%, and 55.50%\nover the traditional 64-9 setup. In the proposed frequency-phase-space encoding\n200-target BCI paradigm, these increases climbed to 195.56%, 153.08%, and\n103.07%. The online BCI system achieved an average actual ITR of 472.7 bpm.\nThis study demonstrates the essential role and immense potential of\nhigh-density EEG in decoding the spatiotemporal information of visual stimuli.", "AI": {"tldr": "This study presents a novel frequency-phase-space fusion encoding method for improving information transfer rates in brain-computer interface (BCI) systems using high-density EEG recordings.", "motivation": "Current visual BCI systems have low information transfer rates due to limitations in spatial resolution and underutilization of spatial information.", "method": "The study employed a frequency-phase-space fusion encoding method with 256-channel high-density EEG recordings to enhance BCI performance.", "result": "The proposed system exhibited significantly increased theoretical information transfer rates: 83.66%, 79.99%, and 55.50% enhancements for different electrode configurations compared to the traditional setup, and an actual ITR of 472.7 bpm.", "conclusion": "The findings highlight the critical importance of high-density EEG in effectively decoding spatiotemporal information from visual stimuli, leading to faster and more efficient BCIs.", "key_contributions": ["Development of a new frequency-phase-space fusion encoding method", "Demonstration of significant improvements in information transfer rates", "Empirical validation of high-speed BCI systems using 256-channel EEG recordings"], "limitations": "", "keywords": ["Brain-computer interface", "high-density EEG", "information transfer rate", "spatiotemporal information", "visual stimuli"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.17186", "pdf": "https://arxiv.org/pdf/2507.17186.pdf", "abs": "https://arxiv.org/abs/2507.17186", "title": "FinGAIA: An End-to-End Benchmark for Evaluating AI Agents in Finance", "authors": ["Lingfeng Zeng", "Fangqi Lou", "Zixuan Wang", "Jiajie Xu", "Jinyi Niu", "Mengping Li", "Yifan Dong", "Qi Qi", "Wei Zhang", "Ziwei Yang", "Jun Han", "Ruilun Feng", "Ruiqi Hu", "Lejie Zhang", "Zhengbo Feng", "Yicheng Ren", "Xin Guo", "Zhaowei Liu", "Dongpo Cheng", "Weige Cai", "Liwen Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "The booming development of AI agents presents unprecedented opportunities for\nautomating complex tasks across various domains. However, their multi-step,\nmulti-tool collaboration capabilities in the financial sector remain\nunderexplored. This paper introduces FinGAIA, an end-to-end benchmark designed\nto evaluate the practical abilities of AI agents in the financial domain.\nFinGAIA comprises 407 meticulously crafted tasks, spanning seven major\nfinancial sub-domains: securities, funds, banking, insurance, futures, trusts,\nand asset management. These tasks are organized into three hierarchical levels\nof scenario depth: basic business analysis, asset decision support, and\nstrategic risk management. We evaluated 10 mainstream AI agents in a zero-shot\nsetting. The best-performing agent, ChatGPT, achieved an overall accuracy of\n48.9\\%, which, while superior to non-professionals, still lags financial\nexperts by over 35 percentage points. Error analysis has revealed five\nrecurring failure patterns: Cross-modal Alignment Deficiency, Financial\nTerminological Bias, Operational Process Awareness Barrier, among others. These\npatterns point to crucial directions for future research. Our work provides the\nfirst agent benchmark closely related to the financial domain, aiming to\nobjectively assess and promote the development of agents in this crucial field.\nPartial data is available at https://github.com/SUFE-AIFLM-Lab/FinGAIA.", "AI": {"tldr": "This paper presents FinGAIA, a benchmark for evaluating AI agents in finance, consisting of 407 tasks across various financial sub-domains.", "motivation": "To assess and promote AI agents' capabilities in automating tasks in the financial sector, where current capabilities remain underexplored.", "method": "Evaluated 10 mainstream AI agents in a zero-shot setting using a set of 407 financial tasks organized into three levels of scenario depth.", "result": "The best-performing agent, ChatGPT, achieved an accuracy of 48.9%, which is significantly lower than financial experts by over 35 percentage points.", "conclusion": "FinGAIA establishes a foundational benchmark for AI agents in finance, highlighting key areas of improvement and directing future research.", "key_contributions": ["Introduction of the FinGAIA benchmark for AI in finance", "Identification of recurring failure patterns in AI agents", "Comprehensive task design spanning multiple financial sub-domains"], "limitations": "The benchmark is novel but inherently limited to the specific tasks designed and may not cover all financial scenarios.", "keywords": ["AI agents", "financial tasks", "evaluation benchmark", "ChatGPT", "error analysis"], "importance_score": 2, "read_time_minutes": 10}}
{"id": "2507.17248", "pdf": "https://arxiv.org/pdf/2507.17248.pdf", "abs": "https://arxiv.org/abs/2507.17248", "title": "Reality Proxy: Fluid Interactions with Real-World Objects in MR via Abstract Representations", "authors": ["Xiaoan Liu", "Difan Jia", "Xianhao Carton Liu", "Mar Gonzalez-Franco", "Chen Zhu-Tian"], "categories": ["cs.HC", "cs.AI", "cs.GR", "H.5.2; I.3.6"], "comment": "16 pages, 9 figures. Accepted for publication in UIST'25 (The 38th\n  Annual ACM Symposium on User Interface Software and Technology), Busan,\n  Republic of Korea, 28 Sep - 1 Oct 2025", "summary": "Interacting with real-world objects in Mixed Reality (MR) often proves\ndifficult when they are crowded, distant, or partially occluded, hindering\nstraightforward selection and manipulation. We observe that these difficulties\nstem from performing interaction directly on physical objects, where input is\ntightly coupled to their physical constraints. Our key insight is to decouple\ninteraction from these constraints by introducing proxies-abstract\nrepresentations of real-world objects. We embody this concept in Reality Proxy,\na system that seamlessly shifts interaction targets from physical objects to\ntheir proxies during selection. Beyond facilitating basic selection, Reality\nProxy uses AI to enrich proxies with semantic attributes and hierarchical\nspatial relationships of their corresponding physical objects, enabling novel\nand previously cumbersome interactions in MR - such as skimming,\nattribute-based filtering, navigating nested groups, and complex multi object\nselections - all without requiring new gestures or menu systems. We demonstrate\nReality Proxy's versatility across diverse scenarios, including office\ninformation retrieval, large-scale spatial navigation, and multi-drone control.\nAn expert evaluation suggests the system's utility and usability, suggesting\nthat proxy-based abstractions offer a powerful and generalizable interaction\nparadigm for future MR systems.", "AI": {"tldr": "The paper introduces Reality Proxy, a Mixed Reality system that decouples interaction from physical constraints by using proxies—abstract representations of real-world objects—to enhance user interaction.", "motivation": "The difficulties in interacting with real-world objects in Mixed Reality often arise when they are crowded, distant, or occluded, necessitating a need for improved interaction methods.", "method": "The paper presents Reality Proxy, a system that shifts interaction from physical objects to abstract proxies during selection, using AI to enrich these proxies with semantic attributes and spatial relationships.", "result": "Reality Proxy enables innovative interactions such as skimming, attribute-based filtering, and complex multi-object selections across various scenarios including office information retrieval and multi-drone control.", "conclusion": "The expert evaluation indicates that proxy-based abstractions provide a powerful interaction paradigm for future Mixed Reality systems, enhancing usability and utility.", "key_contributions": ["Introduction of semi-abstract proxies for interaction in MR", "AI-enhanced proxy attributes and relationships", "Versatile application scenarios including multi-drone control"], "limitations": "", "keywords": ["Mixed Reality", "Human-Computer Interaction", "Proxies", "AI", "User Interaction"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.17216", "pdf": "https://arxiv.org/pdf/2507.17216.pdf", "abs": "https://arxiv.org/abs/2507.17216", "title": "The Pluralistic Moral Gap: Understanding Judgment and Value Differences between Humans and Large Language Models", "authors": ["Giuseppe Russo", "Debora Nozza", "Paul Röttger", "Dirk Hovy"], "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 4 figures", "summary": "People increasingly rely on Large Language Models (LLMs) for moral advice,\nwhich may influence humans' decisions. Yet, little is known about how closely\nLLMs align with human moral judgments. To address this, we introduce the Moral\nDilemma Dataset, a benchmark of 1,618 real-world moral dilemmas paired with a\ndistribution of human moral judgments consisting of a binary evaluation and a\nfree-text rationale. We treat this problem as a pluralistic distributional\nalignment task, comparing the distributions of LLM and human judgments across\ndilemmas. We find that models reproduce human judgments only under high\nconsensus; alignment deteriorates sharply when human disagreement increases. In\nparallel, using a 60-value taxonomy built from 3,783 value expressions\nextracted from rationales, we show that LLMs rely on a narrower set of moral\nvalues than humans. These findings reveal a pluralistic moral gap: a mismatch\nin both the distribution and diversity of values expressed. To close this gap,\nwe introduce Dynamic Moral Profiling (DMP), a Dirichlet-based sampling method\nthat conditions model outputs on human-derived value profiles. DMP improves\nalignment by 64.3% and enhances value diversity, offering a step toward more\npluralistic and human-aligned moral guidance from LLMs.", "AI": {"tldr": "This paper introduces the Moral Dilemma Dataset, analyzing how well Large Language Models (LLMs) align with human moral judgments, finding significant gaps in alignment and value diversity, and proposes a method to improve this alignment.", "motivation": "To understand how closely LLMs match human moral judgments as people use them for moral advice, revealing potential influence on decision-making.", "method": "A benchmark dataset of 1,618 moral dilemmas paired with human judgments was created and analyzed, comparing distributions of LLM and human responses, and a novel method called Dynamic Moral Profiling was introduced to enhance alignment.", "result": "LLMs align with human moral judgments only when there's high consensus and show a sharper decline in alignment with increased human disagreement; LLMs utilize a narrower set of moral values compared to humans.", "conclusion": "Dynamic Moral Profiling improves model alignment and value diversity, addressing the identified moral gap.", "key_contributions": ["Introduction of the Moral Dilemma Dataset", "Discovery of a pluralistic moral gap between LLMs and human judgments", "Development of Dynamic Moral Profiling to enhance LLM alignment with human values"], "limitations": "The focus is primarily on alignment under specific conditions, and the results may not generalize across all types of moral dilemmas.", "keywords": ["Large Language Models", "moral judgments", "Dynamic Moral Profiling"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.17320", "pdf": "https://arxiv.org/pdf/2507.17320.pdf", "abs": "https://arxiv.org/abs/2507.17320", "title": "EventLines: Time Compression for Discrete Event Timelines", "authors": ["Yuet Ling Wong", "Niklas Elmqvist"], "categories": ["cs.HC"], "comment": "10 pages, 6 figures", "summary": "Discrete event sequences serve as models for numerous real-world datasets,\nincluding publications over time, project milestones, and medication dosing\nduring patient treatments. These event sequences typically exhibit bursty\nbehavior, where events cluster together in rapid succession, interspersed with\nperiods of inactivity. Standard timeline charts with linear time axes fail to\nadequately represent such data, resulting in cluttered regions during event\nbursts while leaving other areas unutilized. We introduce EventLines, a novel\ntechnique that dynamically adjusts the time scale to match the underlying event\ndistribution, enabling more efficient use of screen space. To address the\nchallenges of non-linear time scaling, EventLines employs the time axis's\nvisual representation itself to communicate the varying scale. We present\nfindings from a crowdsourced graphical perception study that examines how\ndifferent time scale representations influence temporal perception.", "AI": {"tldr": "Introduction of EventLines, a technique for visualizing discrete event sequences with dynamic time scaling.", "motivation": "To improve the representation of discrete event sequences that exhibit bursty behavior, which standard timeline charts fail to show effectively.", "method": "EventLines employs dynamic adjustments to the time scale based on the distribution of events, enhancing visual clarity by using the time axis's visual representation.", "result": "The study shows that EventLines allows for a more efficient use of screen space and enhances the perception of temporal data.", "conclusion": "EventLines provides a promising solution for visualizing event sequences by adapting the time scale to the data, which could improve user comprehension in various applications.", "key_contributions": ["Introduction of a novel dynamic time scaling technique for event sequences", "Findings from a study on graphical perception related to time visualization", "Demonstration of improved clarity and efficiency in the representation of bursty event data"], "limitations": "", "keywords": ["event sequences", "dynamic time scaling", "visualization", "temporal perception", "User study"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2507.17234", "pdf": "https://arxiv.org/pdf/2507.17234.pdf", "abs": "https://arxiv.org/abs/2507.17234", "title": "CLARIFID: Improving Radiology Report Generation by Reinforcing Clinically Accurate Impressions and Enforcing Detailed Findings", "authors": ["Kyeongkyu Lee", "Seonghwan Yoon", "Hongki Lim"], "categories": ["cs.CL"], "comment": null, "summary": "Automatic generation of radiology reports has the potential to alleviate\nradiologists' significant workload, yet current methods struggle to deliver\nclinically reliable conclusions. In particular, most prior approaches focus on\nproducing fluent text without effectively ensuring the factual correctness of\nthe reports and often rely on single-view images, limiting diagnostic\ncomprehensiveness. We propose CLARIFID, a novel framework that directly\noptimizes diagnostic correctness by mirroring the two-step workflow of experts.\nSpecifically, CLARIFID (1) learns the logical flow from Findings to Impression\nthrough section-aware pretraining, (2) is fine-tuned with Proximal Policy\nOptimization in which the CheXbert F1 score of the Impression section serves as\nthe reward, (3) enforces reasoning-aware decoding that completes \"Findings\"\nbefore synthesizing the \"Impression\", and (4) fuses multiple chest X-ray views\nvia a vision-transformer-based multi-view encoder. During inference, we apply a\nreasoning-aware next-token forcing strategy followed by report-level\nre-ranking, ensuring that the model first produces a comprehensive Findings\nsection before synthesizing the Impression and thereby preserving coherent\nclinical reasoning. Experimental results on the MIMIC-CXR dataset demonstrate\nthat our method achieves superior clinical efficacy and outperforms existing\nbaselines on both standard NLG metrics and clinically aware scores.", "AI": {"tldr": "CLARIFID is a framework that optimizes diagnostic correctness in radiology report generation by employing a structured approach that mimics expert workflows and utilizes multi-view data.", "motivation": "There is a need to improve the reliability and factual correctness of automatically generated radiology reports to alleviate radiologists' workload and enhance diagnostic comprehensiveness.", "method": "CLARIFID learns the logical flow of radiology reports, fine-tunes its model using Proximal Policy Optimization, ensures a reasoning-aware decoding process for coherent clinical reasoning, and integrates multiple chest X-ray views with a vision-transformer-based encoder.", "result": "CLARIFID achieves superior clinical efficacy compared to existing methods on standard NLG metrics and clinically relevant scores, demonstrating improved reliability in report generation.", "conclusion": "The proposed framework not only improves the factual correctness of radiology reports but also enhances their coherence and comprehensiveness, making it a significant advancement in automated medical report generation.", "key_contributions": ["Direct optimization of diagnostic correctness", "Reasoning-aware decoding process", "Integration of multiple views through a vision transformer"], "limitations": "", "keywords": ["radiology report generation", "diagnostic correctness", "multi-view learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.17430", "pdf": "https://arxiv.org/pdf/2507.17430.pdf", "abs": "https://arxiv.org/abs/2507.17430", "title": "Layered Interactions: Exploring Non-Intrusive Digital Craftsmanship Design Through Lacquer Art Interfaces", "authors": ["Yan Dong", "Hanjie Yu", "Yanran Chen", "Zipeng Zhang", "Qiong Wu"], "categories": ["cs.HC"], "comment": "21 pages, 16 figures, published in ACM CHI 2025", "summary": "Integrating technology with the distinctive characteristics of craftsmanship\nhas become a key issue in the field of digital craftsmanship. This paper\nintroduces Layered Interactions, a design approach that seamlessly merges\nHuman-Computer Interaction (HCI) technologies with traditional lacquerware\ncraftsmanship. By leveraging the multi-layer structure and material properties\nof lacquerware, we embed interactive circuits and integrate programmable\nhardware within the layers, creating tangible interfaces that support diverse\ninteractions. This method enhances the adaptability and practicality of\ntraditional crafts in modern digital contexts. Through the development of a\nlacquerware toolkit, along with user experiments and semi-structured\ninterviews, we demonstrate that this approach not only makes technology more\naccessible to traditional artisans but also enhances the materiality and\nemotional qualities of interactive interfaces. Additionally, it fosters mutual\nlearning and collaboration between artisans and technologists. Our research\nintroduces a cross-disciplinary perspective to the HCI community, broadening\nthe material and design possibilities for interactive interfaces.", "AI": {"tldr": "This paper presents Layered Interactions, a design approach that combines HCI technology with traditional lacquerware craftsmanship to create interactive interfaces.", "motivation": "To address the integration of technology with traditional craftsmanship, enhancing accessibility for artisans and modernizing the craft.", "method": "The study involves the development of a lacquerware toolkit, user experiments, and semi-structured interviews to explore the design approach and its implications.", "result": "The approach effectively integrates interactive circuits within lacquerware, demonstrating enhanced emotional qualities and material adaptability of interactive interfaces.", "conclusion": "Layered Interactions fosters collaboration between artisans and technologists, providing a new perspective for the HCI community regarding materiality and design.", "key_contributions": ["Introduction of Layered Interactions approach merging HCI with craftsmanship.", "Development of a toolkit for lacquerware artisans to integrate technology.", "Demonstration of enhanced emotional and material qualities in interactive designs."], "limitations": "", "keywords": ["Human-Computer Interaction", "Lacquerware", "Craftsmanship", "Interactive Interfaces", "Design Approach"], "importance_score": 7, "read_time_minutes": 30}}
{"id": "2507.17288", "pdf": "https://arxiv.org/pdf/2507.17288.pdf", "abs": "https://arxiv.org/abs/2507.17288", "title": "Triple X: A LLM-Based Multilingual Speech Recognition System for the INTERSPEECH2025 MLC-SLM Challenge", "authors": ["Miaomiao Gao", "Xiaoxiao Xiang", "Yiwen Guo"], "categories": ["cs.CL", "eess.AS"], "comment": null, "summary": "This paper describes our Triple X speech recognition system submitted to Task\n1 of the Multi-Lingual Conversational Speech Language Modeling (MLC-SLM)\nChallenge. Our work focuses on optimizing speech recognition accuracy in\nmultilingual conversational scenarios through an innovative encoder-adapter-LLM\narchitecture. This framework harnesses the powerful reasoning capabilities of\ntext-based large language models while incorporating domain-specific\nadaptations. To further enhance multilingual recognition performance, we\nadopted a meticulously designed multi-stage training strategy leveraging\nextensive multilingual audio datasets. Experimental results demonstrate that\nour approach achieves competitive Word Error Rate (WER) performance on both dev\nand test sets, obtaining second place in the challenge ranking.", "AI": {"tldr": "The Triple X speech recognition system optimizes accuracy in multilingual conversational scenarios using an encoder-adapter-LLM architecture, achieving competitive WER performance.", "motivation": "To enhance speech recognition accuracy in multilingual settings, particularly in conversational scenarios.", "method": "The approach utilizes an innovative encoder-adapter-LLM architecture along with a multi-stage training strategy leveraging extensive multilingual audio datasets.", "result": "Achieved competitive Word Error Rate (WER) results, ranking second in the MLC-SLM Challenge.", "conclusion": "The proposed system effectively combines LLM reasoning capabilities with domain-specific adaptations to improve multilingual speech recognition.", "key_contributions": ["Introduction of a novel encoder-adapter-LLM architecture", "Application of a multi-stage training strategy", "Demonstrated competitive WER performance in a multi-lingual conversational context"], "limitations": "", "keywords": ["speech recognition", "multilingual", "conversational AI", "large language models", "machine learning"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2507.17524", "pdf": "https://arxiv.org/pdf/2507.17524.pdf", "abs": "https://arxiv.org/abs/2507.17524", "title": "SDC-Net: A Domain Adaptation Framework with Semantic-Dynamic Consistency for Cross-Subject EEG Emotion Recognition", "authors": ["Jiahao Tang", "Youjun Li", "Xiangting Fan", "Yangxuan Zheng", "Siyuan Lu", "Xueping Li", "Peng Fang", "Chenxi Li", "Zi-Gang Huang"], "categories": ["cs.HC"], "comment": null, "summary": "Electroencephalography(EEG) based emotion recognition holds great promise for\naffective brain-computer interfaces (aBCIs), yet practical deployment remains\nchallenging due to substantial inter-subject variability and the lack of\nlabeled data in target domains. To overcome these limitations, we present a\nnovel unsupervised Semantic-Dynamic Consistency domain adaptation network for\nfully label-free cross-subject EEG emotion recognition. First, we introduce a\nSame-Subject Same-Trial Mixup strategy that generates augmented samples via\nintra-trial interpolation, enhancing data diversity while explicitly preserving\nindividual identity to mitigate label ambiguity. Second, we construct a dynamic\ndistribution alignment module in reproducing kernel Hilbert space (RKHS),\njointly aligning marginal and conditional distributions through multi-objective\nkernel mean embedding, and leveraging a confidence-aware pseudo-labeling\nstrategy to ensure stable adaptation. Third, we propose a dual-domain\nsimilarity consistency learning mechanism that enforces cross-domain structural\nconstraints based on latent pairwise similarities, enabling semantic boundary\nlearning without relying on temporal synchronization or label priors. To\nvalidate the effectiveness and robustness of the proposed SDC-Net, extensive\nexperiments are conducted on three widely used EEG benchmark datasets: SEED,\nSEED-IV, and Faced. Comparative results against existing unsupervised domain\nadaptation methods demonstrate that SDC-Net achieves state-of-the-art\nperformance in emotion recognition under both cross-subject and cross-session\nconditions. This advancement significantly improves the accuracy and\ngeneralization capability of emotion decoding, and lays a solid foundation for\nreal-world applications of personalized affective brain-computer interfaces\n(aBCIs). The source code will be released at\nhttps://github.com/XuanSuTrum/SDC-Net.", "AI": {"tldr": "This paper presents an unsupervised Semantic-Dynamic Consistency domain adaptation network (SDC-Net) for EEG-based emotion recognition, addressing challenges in inter-subject variability and lack of labeled data.", "motivation": "The practical deployment of EEG-based emotion recognition systems is hindered by inter-subject variability and limited labeled datasets, necessitating a robust method for cross-subject emotion recognition.", "method": "The proposed SDC-Net employs a Same-Subject Same-Trial Mixup strategy for data augmentation, a dynamic distribution alignment module using reproducing kernel Hilbert space (RKHS), and a dual-domain similarity consistency learning mechanism to enhance emotion recognition accuracy.", "result": "Extensive experiments on EEG benchmark datasets show that SDC-Net outperforms existing unsupervised domain adaptation methods in emotion recognition for both cross-subject and cross-session scenarios.", "conclusion": "SDC-Net significantly enhances the performance and generalization of EEG emotion recognition, paving the way for improved personalized affective brain-computer interfaces.", "key_contributions": ["Introduced Same-Subject Same-Trial Mixup for enhanced data diversity.", "Developed a dynamic distribution alignment module in RKHS for robust adaptation.", "Implemented a dual-domain similarity consistency learning mechanism for effective emotion decoding."], "limitations": "", "keywords": ["EEG", "emotion recognition", "domain adaptation", "brain-computer interface", "unsupervised learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.17399", "pdf": "https://arxiv.org/pdf/2507.17399.pdf", "abs": "https://arxiv.org/abs/2507.17399", "title": "Millions of $\\text{GeAR}$-s: Extending GraphRAG to Millions of Documents", "authors": ["Zhili Shen", "Chenxin Diao", "Pascual Merita", "Pavlos Vougiouklis", "Jeff Z. Pan"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Accepted by SIGIR 2025 LiveRAG Challenge Program", "summary": "Recent studies have explored graph-based approaches to retrieval-augmented\ngeneration, leveraging structured or semi-structured information -- such as\nentities and their relations extracted from documents -- to enhance retrieval.\nHowever, these methods are typically designed to address specific tasks, such\nas multi-hop question answering and query-focused summarisation, and therefore,\nthere is limited evidence of their general applicability across broader\ndatasets. In this paper, we aim to adapt a state-of-the-art graph-based RAG\nsolution: $\\text{GeAR}$ and explore its performance and limitations on the\nSIGIR 2025 LiveRAG Challenge.", "AI": {"tldr": "The paper adapts the GeAR graph-based retrieval-augmented generation method and evaluates its performance on the SIGIR 2025 LiveRAG Challenge.", "motivation": "To explore broader applicability of graph-based approaches in retrieval-augmented generation (RAG) beyond specific tasks.", "method": "Adaptation of the GeAR method for graph-based retrieval-augmented generation and performance evaluation on a live challenge dataset.", "result": "The paper presents findings on the performance and limitations of GeAR in the context of the LiveRAG Challenge.", "conclusion": "The study highlights the performance of GeAR and discusses its limitations, indicating avenues for future research.", "key_contributions": ["Adaptation of the GeAR retrieval-augmented generation model for broader datasets.", "Evaluation of performance in a live challenge setting.", "Identification of limitations in the current application of graph-based RAG methods."], "limitations": "Limited evidence of general applicability across broader datasets outside specific tasks.", "keywords": ["retrieval-augmented generation", "graph-based methods", "SIGIR 2025 LiveRAG Challenge"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.17543", "pdf": "https://arxiv.org/pdf/2507.17543.pdf", "abs": "https://arxiv.org/abs/2507.17543", "title": "Anticipate, Simulate, Reason (ASR): A Comprehensive Generative AI Framework for Combating Messaging Scams", "authors": ["Xue Wen Tan", "Kenneth See", "Stanley Kok"], "categories": ["cs.HC"], "comment": null, "summary": "The rapid growth of messaging scams creates an escalating challenge for user\nsecurity and financial safety. In this paper, we present the Anticipate,\nSimulate, Reason (ASR) framework, a generative AI method that enables users to\nproactively identify and comprehend scams within instant messaging platforms.\nUsing large language models, ASR predicts scammer responses, creates realistic\nscam conversations, and delivers real-time, interpretable support to end-users.\nWe develop ScamGPT-J, a domain-specific language model fine-tuned on a new,\nhigh-quality dataset of scam conversations covering multiple scam types.\nThorough experimental evaluation shows that the ASR framework substantially\nenhances scam detection, particularly in challenging contexts such as job\nscams, and uncovers important demographic patterns in user vulnerability and\nperceptions of AI-generated assistance. Our findings reveal a contradiction\nwhere those most at risk are often least receptive to AI support, emphasizing\nthe importance of user-centered design in AI-driven fraud prevention. This work\nadvances both the practical and theoretical foundations for interpretable,\nhuman-centered AI systems in combating evolving digital threats.", "AI": {"tldr": "The paper introduces the Anticipate, Simulate, Reason (ASR) framework for improving scam detection in messaging through generative AI, using a specific model (ScamGPT-J) trained on scam conversation data.", "motivation": "To enhance user security and financial safety against increasing messaging scams by providing a proactive identification and understanding of such scams.", "method": "The ASR framework uses large language models to simulate scam conversations and predict scammer responses, offering real-time, interpretable support for users.", "result": "Experimental evaluation shows that the ASR framework significantly improves scam detection, especially for job scams, and identifies demographic patterns in user vulnerability.", "conclusion": "The findings highlight the need for user-centered design in AI systems, as those most at risk are often the least open to AI support, underscoring the importance of addressing user perceptions in fraud prevention efforts.", "key_contributions": ["Introduction of the ASR framework for scam detection", "Development of ScamGPT-J, a language model fine-tuned for scam conversations", "Empirical evidence of demographic patterns affecting user vulnerability to scams"], "limitations": "", "keywords": ["scam detection", "generative AI", "user-centered design", "large language models", "fraud prevention"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.17409", "pdf": "https://arxiv.org/pdf/2507.17409.pdf", "abs": "https://arxiv.org/abs/2507.17409", "title": "Investigating Subjective Factors of Argument Strength: Storytelling, Emotions, and Hedging", "authors": ["Carlotta Quensel", "Neele Falk", "Gabriella Lapesa"], "categories": ["cs.CL"], "comment": "Accepted to the 12th Workshop on Argument Mining (ArgMining) 2025", "summary": "In assessing argument strength, the notions of what makes a good argument are\nmanifold. With the broader trend towards treating subjectivity as an asset and\nnot a problem in NLP, new dimensions of argument quality are studied. Although\nstudies on individual subjective features like personal stories exist, there is\na lack of large-scale analyses of the relation between these features and\nargument strength. To address this gap, we conduct regression analysis to\nquantify the impact of subjective factors $-$ emotions, storytelling, and\nhedging $-$ on two standard datasets annotated for objective argument quality\nand subjective persuasion. As such, our contribution is twofold: at the level\nof contributed resources, as there are no datasets annotated with all studied\ndimensions, this work compares and evaluates automated annotation methods for\neach subjective feature. At the level of novel insights, our regression\nanalysis uncovers different patterns of impact of subjective features on the\ntwo facets of argument strength encoded in the datasets. Our results show that\nstorytelling and hedging have contrasting effects on objective and subjective\nargument quality, while the influence of emotions depends on their rhetoric\nutilization rather than the domain.", "AI": {"tldr": "The paper conducts a regression analysis to explore how subjective factors such as emotions, storytelling, and hedging affect argument strength in NLP, highlighting contrasting impacts on objective and subjective quality.", "motivation": "To address the gap in large-scale analyses of subjective features and their relation to argument strength in NLP, emphasizing subjectivity's growing relevance.", "method": "Regression analysis on two datasets annotated for objective argument quality and subjective persuasion, evaluating automated annotation methods for subjective features.", "result": "The analysis reveals that storytelling and hedging influence argument quality differently, with emotions' effects varying by rhetorical context rather than domain.", "conclusion": "Understanding the impact of subjective features on arguments can enhance techniques in NLP, particularly in argument mining.", "key_contributions": ["Development of annotated datasets with subjective features", "Comparative evaluation of automated annotation methods", "Novel insights into the effects of subjective features on argument strength"], "limitations": "", "keywords": ["argument strength", "subjectivity", "NLP", "emotion", "storytelling"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2507.17597", "pdf": "https://arxiv.org/pdf/2507.17597.pdf", "abs": "https://arxiv.org/abs/2507.17597", "title": "Explainable AI for Collaborative Assessment of 2D/3D Registration Quality", "authors": ["Sue Min Cho", "Alexander Do", "Russell H. Taylor", "Mathias Unberath"], "categories": ["cs.HC", "cs.CV"], "comment": null, "summary": "As surgery embraces digital transformation--integrating sophisticated\nimaging, advanced algorithms, and robotics to support and automate complex\nsub-tasks--human judgment of system correctness remains a vital safeguard for\npatient safety. This shift introduces new \"operator-type\" roles tasked with\nverifying complex algorithmic outputs, particularly at critical junctures of\nthe procedure, such as the intermediary check before drilling or implant\nplacement. A prime example is 2D/3D registration, a key enabler of image-based\nsurgical navigation that aligns intraoperative 2D images with preoperative 3D\ndata. Although registration algorithms have advanced significantly, they\noccasionally yield inaccurate results. Because even small misalignments can\nlead to revision surgery or irreversible surgical errors, there is a critical\nneed for robust quality assurance. Current visualization-based strategies alone\nhave been found insufficient to enable humans to reliably detect 2D/3D\nregistration misalignments. In response, we propose the first artificial\nintelligence (AI) framework trained specifically for 2D/3D registration quality\nverification, augmented by explainability features that clarify the model's\ndecision-making. Our explainable AI (XAI) approach aims to enhance informed\ndecision-making for human operators by providing a second opinion together with\na rationale behind it. Through algorithm-centric and human-centered\nevaluations, we systematically compare four conditions: AI-only, human-only,\nhuman-AI, and human-XAI. Our findings reveal that while explainability features\nmodestly improve user trust and willingness to override AI errors, they do not\nexceed the standalone AI in aggregate performance. Nevertheless, future work\nextending both the algorithmic design and the human-XAI collaboration elements\nholds promise for more robust quality assurance of 2D/3D registration.", "AI": {"tldr": "This paper proposes an AI framework for verifying 2D/3D registration quality in surgeries, integrating explainability features to enhance human decision-making and operator trust.", "motivation": "The digital transformation in surgery raises the need for safeguarding patient safety through human verification of algorithmic outputs, especially in critical surgical decisions.", "method": "The study introduces a novel AI framework emphasizing explainability for 2D/3D registration quality verification, comparing AI-only, human-only, human-AI, and human-XAI conditions.", "result": "The study finds that explainability features modestly increase user trust but do not surpass the performance of AI alone, suggesting the potential for improving human-AI collaboration for future systems.", "conclusion": "While the proposed XAI approach shows promise for enhancing decision-making and quality assurance in surgical contexts, further work is needed to improve algorithmic design and collaboration dynamics.", "key_contributions": ["First AI framework for 2D/3D registration quality verification", "Introduction of explainable AI features", "Comparison of AI and human decision-making in surgical settings"], "limitations": "Explainability features improve user trust but not statistical performance; further algorithmic design needed for better outcomes.", "keywords": ["Human-Computer Interaction", "Explainable AI", "Surgical Navigation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.17442", "pdf": "https://arxiv.org/pdf/2507.17442.pdf", "abs": "https://arxiv.org/abs/2507.17442", "title": "Each to Their Own: Exploring the Optimal Embedding in RAG", "authors": ["Shiting Chen", "Zijian Zhao", "Jinsong Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recently, as Large Language Models (LLMs) have fundamentally impacted various\nfields, the methods for incorporating up-to-date information into LLMs or\nadding external knowledge to construct domain-specific models have garnered\nwide attention. Retrieval-Augmented Generation (RAG), serving as an\ninference-time scaling method, is notable for its low cost and minimal effort\nfor parameter tuning. However, due to heterogeneous training data and model\narchitecture, the variant embedding models used in RAG exhibit different\nbenefits across various areas, often leading to different similarity\ncalculation results and, consequently, varying response quality from LLMs. To\naddress this problem, we propose and examine two approaches to enhance RAG by\ncombining the benefits of multiple embedding models, named Mixture-Embedding\nRAG and Confident RAG. Mixture-Embedding RAG simply sorts and selects\nretrievals from multiple embedding models based on standardized similarity;\nhowever, it does not outperform vanilla RAG. In contrast, Confident RAG\ngenerates responses multiple times using different embedding models and then\nselects the responses with the highest confidence level, demonstrating average\nimprovements of approximately 10% and 5% over vanilla LLMs and RAG,\nrespectively. The consistent results across different LLMs and embedding models\nindicate that Confident RAG is an efficient plug-and-play approach for various\ndomains. We will release our code upon publication.", "AI": {"tldr": "This paper presents two approaches, Mixture-Embedding RAG and Confident RAG, to enhance Retrieval-Augmented Generation (RAG) by leveraging multiple embedding models to improve response quality from Large Language Models (LLMs).", "motivation": "To enhance the quality of responses generated by LLMs when using Retrieval-Augmented Generation (RAG) due to varying performance from different embedding models.", "method": "The paper proposes two methods: Mixture-Embedding RAG, which selects retrievals from multiple embedding models based on standardized similarity, and Confident RAG, which generates multiple responses using different models and selects the highest confidence response.", "result": "Confident RAG shows average improvements of approximately 10% over vanilla LLMs and 5% over RAG, indicating a significant enhancement in response quality.", "conclusion": "Confident RAG serves as an efficient plug-and-play solution for improving responses in various domains using LLMs.", "key_contributions": ["Introduction of Mixture-Embedding RAG and Confident RAG methods", "Demonstration of average improvements in response quality over existing methods", "Potential for release of practical code to facilitate further research in the area."], "limitations": "", "keywords": ["Large Language Models", "Retrieval-Augmented Generation", "Confident RAG", "embedding models", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2507.17688", "pdf": "https://arxiv.org/pdf/2507.17688.pdf", "abs": "https://arxiv.org/abs/2507.17688", "title": "Mindfulness Meditation and Respiration: Accelerometer-Based Respiration Rate and Mindfulness Progress Estimation to Enhance App Engagement and Mindfulness Skills", "authors": ["Mohammad Nur Hossain Khan", "David creswell", "Jordan Albert", "Patrick O'Connell", "Shawn Fallon", "Mathew Polowitz", "Xuhai \"orson\" Xu", "Bashima islam"], "categories": ["cs.HC", "cs.LG"], "comment": "Accepted in Proc. ACM Interact. Mob. Wearable Ubiquitous Technology\n  (IMWUT)", "summary": "Mindfulness training is widely recognized for its benefits in reducing\ndepression, anxiety, and loneliness. With the rise of smartphone-based\nmindfulness apps, digital meditation has become more accessible, but sustaining\nlong-term user engagement remains a challenge. This paper explores whether\nrespiration biosignal feedback and mindfulness skill estimation enhance system\nusability and skill development. We develop a smartphone's accelerometer-based\nrespiration tracking algorithm, eliminating the need for additional wearables.\nUnlike existing methods, our approach accurately captures slow breathing\npatterns typical of mindfulness meditation. Additionally, we introduce the\nfirst quantitative framework to estimate mindfulness skills-concentration,\nsensory clarity, and equanimity-based on accelerometer-derived respiration\ndata. We develop and test our algorithms on 261 mindfulness sessions in both\ncontrolled and real-world settings. A user study comparing an experimental\ngroup receiving biosignal feedback with a control group using a standard app\nshows that respiration feedback enhances system usability. Our respiration\ntracking model achieves a mean absolute error (MAE) of 1.6 breaths per minute,\nclosely aligning with ground truth data, while our mindfulness skill estimation\nattains F1 scores of 80-84% in tracking skill progression. By integrating\nrespiration tracking and mindfulness estimation into a commercial app, we\ndemonstrate the potential of smartphone sensors to enhance digital mindfulness\ntraining.", "AI": {"tldr": "This paper investigates the use of respiration biosignal feedback and mindfulness skill estimation to enhance user engagement in smartphone-based mindfulness apps.", "motivation": "To address the challenge of sustaining long-term user engagement in digital meditation by leveraging smartphone sensors for feedback and skill estimation.", "method": "Development of an accelerometer-based respiration tracking algorithm and a quantitative framework for estimating mindfulness skills, tested through a user study comparing biosignal feedback with standard app usage.", "result": "Respiration feedback improves system usability, with the tracking model achieving a mean absolute error of 1.6 breaths per minute and mindfulness skill estimation having F1 scores of 80-84% in skill progression.", "conclusion": "Integrating respiration tracking and mindfulness estimation into commercial apps can significantly enhance digital mindfulness training experience.", "key_contributions": ["Introduction of a smartphone-based respiration tracking algorithm", "First quantitative framework for estimating mindfulness skills", "Demonstrated improvement in usability through user studies"], "limitations": "", "keywords": ["Mindfulness", "Biosignal feedback", "User engagement", "Respiration tracking", "Skill estimation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.17476", "pdf": "https://arxiv.org/pdf/2507.17476.pdf", "abs": "https://arxiv.org/abs/2507.17476", "title": "MultiNRC: A Challenging and Native Multilingual Reasoning Evaluation Benchmark for LLMs", "authors": ["Alexander R. Fabbri", "Diego Mares", "Jorge Flores", "Meher Mankikar", "Ernesto Hernandez", "Dean Lee", "Bing Liu", "Chen Xing"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Although recent Large Language Models (LLMs) have shown rapid improvement on\nreasoning benchmarks in English, the evaluation of such LLMs' multilingual\nreasoning capability across diverse languages and cultural contexts remains\nlimited. Existing multilingual reasoning benchmarks are typically constructed\nby translating existing English reasoning benchmarks, biasing these benchmarks\ntowards reasoning problems with context in English language/cultures. In this\nwork, we introduce the Multilingual Native Reasoning Challenge (MultiNRC), a\nbenchmark designed to assess LLMs on more than 1,000 native, linguistic and\nculturally grounded reasoning questions written by native speakers in French,\nSpanish, and Chinese. MultiNRC covers four core reasoning categories:\nlanguage-specific linguistic reasoning, wordplay & riddles, cultural/tradition\nreasoning, and math reasoning with cultural relevance. For cultural/tradition\nreasoning and math reasoning with cultural relevance, we also provide English\nequivalent translations of the multilingual questions by manual translation\nfrom native speakers fluent in English. This set of English equivalents can\nprovide a direct comparison of LLM reasoning capacity in other languages vs.\nEnglish on the same reasoning questions. We systematically evaluate current 14\nleading LLMs covering most LLM families on MultiNRC and its English equivalent\nset. The results show that (1) current LLMs are still not good at native\nmultilingual reasoning, with none scoring above 50% on MultiNRC; (2) LLMs\nexhibit distinct strengths and weaknesses in handling linguistic, cultural, and\nlogical reasoning tasks; (3) Most models perform substantially better in math\nreasoning in English compared to in original languages (+10%), indicating\npersistent challenges with culturally grounded knowledge.", "AI": {"tldr": "The paper introduces the Multilingual Native Reasoning Challenge (MultiNRC), a benchmark assessing LLMs on culturally grounded reasoning questions in multiple languages and evaluates the performance of leading LLMs.", "motivation": "To address the limitations of existing multilingual reasoning benchmarks that are biased towards English language and culture by creating a benchmark for assessing reasoning capabilities in native languages.", "method": "MultiNRC was developed to include over 1,000 reasoning questions across four categories: linguistic reasoning, wordplay & riddles, cultural/tradition reasoning, and culturally relevant math reasoning. It evaluates 14 leading LLMs and compares their performances on the original multilingual questions and their English equivalents.", "result": "Current LLMs showed poor performance on MultiNRC, with none scoring above 50%. There were notable differences in strengths and weaknesses across linguistic, cultural, and logical reasoning tasks, with better performance in math reasoning in English compared to original languages (an increase of 10%).", "conclusion": "The study highlights persistent challenges for LLMs in native multilingual reasoning and illustrates the necessity for culturally grounded evaluation methods.", "key_contributions": ["Introduction of a new multilingual benchmark (MultiNRC) focused on native reasoning questions", "Systematic evaluation of multiple LLMs across various reasoning tasks", "Insights into LLMs' strengths and weaknesses in handling cultural and linguistic reasoning"], "limitations": "The benchmark currently includes only three languages (French, Spanish, and Chinese), which may limit its generalizability.", "keywords": ["multilingual reasoning", "large language models", "cultural context", "benchmark evaluation", "native reasoning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.17734", "pdf": "https://arxiv.org/pdf/2507.17734.pdf", "abs": "https://arxiv.org/abs/2507.17734", "title": "DataWink: Reusing and Adapting SVG-based Visualization Examples with Large Multimodal Models", "authors": ["Liwenhan Xie", "Yanna Lin", "Can Liu", "Huamin Qu", "Xinhuan Shu"], "categories": ["cs.HC"], "comment": "Accepted to the IEEE Visualization Conference (VIS'25). 11 pages, 6\n  figures", "summary": "Creating aesthetically pleasing data visualizations remains challenging for\nusers without design expertise or familiarity with visualization tools. To\naddress this gap, we present DataWink, a system that enables users to create\ncustom visualizations by adapting high-quality examples. Our approach combines\nlarge multimodal models (LMMs) to extract data encoding from existing SVG-based\nvisualization examples, featuring an intermediate representation of\nvisualizations that bridges primitive SVG and visualization programs. Users may\nexpress adaptation goals to a conversational agent and control the visual\nappearance through widgets generated on demand. With an interactive interface,\nusers can modify both data mappings and visual design elements while\nmaintaining the original visualization's aesthetic quality. To evaluate\nDataWink, we conduct a user study (N=12) with replication and free-form\nexploration tasks. As a result, DataWink is recognized for its learnability and\neffectiveness in personalized authoring tasks. Our results demonstrate the\npotential of example-driven approaches for democratizing visualization\ncreation.", "AI": {"tldr": "DataWink is a system that enables users to create custom visualizations by adapting existing high-quality examples, facilitating the visualization process for users without design expertise.", "motivation": "To assist users without design expertise in creating aesthetically pleasing data visualizations, thus democratizing the visualization creation process.", "method": "DataWink utilizes large multimodal models to extract data encoding from SVG-based visualization examples, enabling users to adapt visualizations through a conversational agent and interactive widgets.", "result": "User studies demonstrate that DataWink is effective and learnable for personalized authoring tasks, highlighting the viability of example-driven approaches in visualization.", "conclusion": "DataWink shows promise in making data visualization accessible to users lacking design skills by allowing customizable adaptations of existing models, thereby improving user engagement with visualization tools.", "key_contributions": ["Introduction of DataWink for custom visualization creation", "Use of large multimodal models for data encoding extraction", "User study validating effectiveness and learnability of the system"], "limitations": "", "keywords": ["data visualization", "user interface", "multimodal models", "example-driven design", "HCI"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.17527", "pdf": "https://arxiv.org/pdf/2507.17527.pdf", "abs": "https://arxiv.org/abs/2507.17527", "title": "Seed LiveInterpret 2.0: End-to-end Simultaneous Speech-to-speech Translation with Your Voice", "authors": ["Shanbo Cheng", "Yu Bao", "Zhichao Huang", "Yu Lu", "Ningxin Peng", "Lu Xu", "Runsheng Yu", "Rong Cao", "Ting Han", "Zeyang Li", "Sitong Liu", "Shengtao Ma", "Shiguang Pan", "Jiongchen Xiao", "Nuo Xu", "Meng Yang", "Rong Ye", "Yiming Yu", "Ruofei Zhang", "Wanyi Zhang", "Wenhao Zhu", "Liehao Zou", "Lu Lu", "Yuxuan Wang", "Yonghui Wu"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Seed-LiveInterpret 2.0 Technical Report", "summary": "Simultaneous Interpretation (SI) represents one of the most daunting\nfrontiers in the translation industry, with product-level automatic systems\nlong plagued by intractable challenges: subpar transcription and translation\nquality, lack of real-time speech generation, multi-speaker confusion, and\ntranslated speech inflation, especially in long-form discourses. In this study,\nwe introduce Seed-LiveInterpret 2.0, an end-to-end SI model that delivers\nhigh-fidelity, ultra-low-latency speech-to-speech generation with voice cloning\ncapabilities. As a fully operational product-level solution, Seed-LiveInterpret\n2.0 tackles these challenges head-on through our novel duplex speech-to-speech\nunderstanding-generating framework. Experimental results demonstrate that\nthrough large-scale pretraining and reinforcement learning, the model achieves\na significantly better balance between translation accuracy and latency,\nvalidated by human interpreters to exceed 70% correctness in complex scenarios.\nNotably, Seed-LiveInterpret 2.0 outperforms commercial SI solutions by\nsignificant margins in translation quality, while slashing the average latency\nof cloned speech from nearly 10 seconds to a near-real-time 3 seconds, which is\naround a near 70% reduction that drastically enhances practical usability.", "AI": {"tldr": "Introduction of Seed-LiveInterpret 2.0, an advanced simultaneous interpretation model that improves speech-to-speech generation quality and reduces latency.", "motivation": "To address longstanding challenges in the translation industry related to simultaneous interpretation, including transcription quality, real-time speech generation, and translation latency.", "method": "The model employs a duplex speech-to-speech understanding-generating framework, utilizing large-scale pretraining and reinforcement learning techniques.", "result": "Seed-LiveInterpret 2.0 achieves over 70% correctness in complex scenarios, significantly improving translation quality and reducing latency from nearly 10 seconds to 3 seconds.", "conclusion": "The model is demonstrated to outperform existing commercial solutions, enhancing usability in practical applications.", "key_contributions": ["Introduction of a novel duplex framework for speech-to-speech interpretation", "Significant improvement in translation accuracy and latency", "Demonstrated effectiveness validated by human interpreters"], "limitations": "", "keywords": ["Simultaneous Interpretation", "Speech Generation", "Voice Cloning", "Translation Quality", "Latency Reduction"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.17578", "pdf": "https://arxiv.org/pdf/2507.17578.pdf", "abs": "https://arxiv.org/abs/2507.17578", "title": "Synthetic Voice Data for Automatic Speech Recognition in African Languages", "authors": ["Brian DeRenzi", "Anna Dixon", "Mohamed Aymane Farhi", "Christian Resch"], "categories": ["cs.CL", "I.2.7"], "comment": "29 pages incl. appendix, 8 tables, 5 figures. Authors are listed in\n  alphabetical order", "summary": "Speech technology remains out of reach for most of the over 2300 languages in\nAfrica. We present the first systematic assessment of large-scale synthetic\nvoice corpora for African ASR. We apply a three-step process: LLM-driven text\ncreation, TTS voice synthesis, and ASR fine-tuning. Eight out of ten languages\nfor which we create synthetic text achieved readability scores above 5 out of\n7. We evaluated ASR improvement for three (Hausa, Dholuo, Chichewa) and created\nmore than 2,500 hours of synthetic voice data at below 1% of the cost of real\ndata. Fine-tuned Wav2Vec-BERT-2.0 models trained on 250h real and 250h\nsynthetic Hausa matched a 500h real-data-only baseline, while 579h real and\n450h to 993h synthetic data created the best performance. We also present\ngender-disaggregated ASR performance evaluation. For very low-resource\nlanguages, gains varied: Chichewa WER improved about 6.5% relative with a 1:2\nreal-to-synthetic ratio; a 1:1 ratio for Dholuo showed similar improvements on\nsome evaluation data, but not on others. Investigating intercoder reliability,\nASR errors and evaluation datasets revealed the need for more robust reviewer\nprotocols and more accurate evaluation data. All data and models are publicly\nreleased to invite further work to improve synthetic data for African\nlanguages.", "AI": {"tldr": "This paper presents a systematic assessment of large-scale synthetic voice corpora for African Automatic Speech Recognition (ASR), focusing on language processing using LLMs and TTS. It provides methods, findings, and implications for low-resource languages.", "motivation": "To address the lack of speech technology for over 2300 languages in Africa by creating synthetic voice data to improve ASR capabilities.", "method": "A three-step process involving LLM-driven text creation, TTS voice synthesis, and ASR fine-tuning was used. ASR improvements were evaluated on languages such as Hausa, Dholuo, and Chichewa.", "result": "Synthetic text for eight out of ten languages showed high readability scores. ASR performance improved significantly with the use of synthetic and real data combinations, achieving competitive results between model configurations.", "conclusion": "The study highlights the potential of synthetic data for enhancing ASR in low-resource African languages and emphasizes the need for improved evaluation protocols and datasets.", "key_contributions": ["Creation of synthetic voice corpora for African languages", "Demonstration of ASR performance improvement using a combination of synthetic and real data", "Public release of data and models for further research"], "limitations": "The need for more robust reviewer protocols and accurate evaluation datasets to ensure reliable ASR performance assessments.", "keywords": ["synthetic voice corpora", "Automatic Speech Recognition", "African languages", "text-to-speech", "language models"], "importance_score": 7, "read_time_minutes": 20}}
{"id": "2507.17618", "pdf": "https://arxiv.org/pdf/2507.17618.pdf", "abs": "https://arxiv.org/abs/2507.17618", "title": "A Hybrid Early-Exit Algorithm for Large Language Models Based on Space Alignment Decoding (SPADE)", "authors": ["Bowen Zheng", "Ming Ma", "Zhongqiao Lin", "Tianming Yang"], "categories": ["cs.CL", "cs.PF"], "comment": null, "summary": "Large language models are computationally expensive due to their deep\nstructures. Prior research has shown that intermediate layers contain\nsufficient information to generate accurate answers, leading to the development\nof early-exit algorithms that reduce inference costs by terminating computation\nat earlier layers. However, these methods often suffer from poor performance\ndue to misalignment between intermediate and output layer representations that\nlead to decoding inaccuracy. To address these challenges, we propose SPADE\n(SPace Alignment DEcoding), a novel decoding method that aligns intermediate\nlayer representations with the output layer by propagating a minimally reduced\nsequence consisting of only the start token and the answer token. We further\noptimize the early-exit decision-making process by training a linear\napproximation of SPADE that computes entropy-based confidence metrics. Putting\nthem together, we create a hybrid early-exit algorithm that monitors confidence\nlevels and stops inference at intermediate layers while using SPADE to generate\nhigh-quality outputs. This approach significantly reduces inference costs\nwithout compromising accuracy, offering a scalable and efficient solution for\ndeploying large language models in real-world applications.", "AI": {"tldr": "This paper introduces SPADE, a novel decoding method that aligns intermediate layer outputs with the output layer in large language models to improve inference efficiency.", "motivation": "Large language models are expensive to run, and early-exit algorithms can lower costs but often degrade performance.", "method": "The proposed SPADE method aligns intermediate layer representations with the output layer by using a minimally reduced sequence. Additionally, a hybrid early-exit algorithm is designed to evaluate confidence levels for more efficient real-time inference.", "result": "SPADE significantly reduces inference costs while maintaining high output accuracy compared to traditional methods.", "conclusion": "This approach provides a scalable solution for deploying large language models in practical applications without sacrificing performance.", "key_contributions": ["Introduction of SPADE for alignment of intermediate and output layer representations", "Development of a hybrid early-exit algorithm to optimize inference costs", "Utilization of confidence metrics based on entropy for decision making"], "limitations": "", "keywords": ["large language models", "early-exit algorithms", "decoding methods"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.17634", "pdf": "https://arxiv.org/pdf/2507.17634.pdf", "abs": "https://arxiv.org/abs/2507.17634", "title": "WSM: Decay-Free Learning Rate Schedule via Checkpoint Merging for LLM Pre-training", "authors": ["Changxin Tian", "Jiapeng Wang", "Qian Zhao", "Kunlong Chen", "Jia Liu", "Ziqi Liu", "Jiaxin Mao", "Wayne Xin Zhao", "Zhiqiang Zhang", "Jun Zhou"], "categories": ["cs.CL", "cs.LG", "I.2.7"], "comment": null, "summary": "Recent advances in learning rate (LR) scheduling have demonstrated the\neffectiveness of decay-free approaches that eliminate the traditional decay\nphase while maintaining competitive performance. Model merging techniques have\nemerged as particularly promising solutions in this domain. We present\nWarmup-Stable and Merge (WSM), a general framework that establishes a formal\nconnection between learning rate decay and model merging. WSM provides a\nunified theoretical foundation for emulating various decay strategies-including\ncosine decay, linear decay and inverse square root decay-as principled model\naveraging schemes, while remaining fully compatible with diverse optimization\nmethods. Through extensive experiments, we identify merge duration-the training\nwindow for checkpoint aggregation-as the most critical factor influencing model\nperformance, surpassing the importance of both checkpoint interval and merge\nquantity. Our framework consistently outperforms the widely-adopted\nWarmup-Stable-Decay (WSD) approach across multiple benchmarks, achieving\nsignificant improvements of +3.5% on MATH, +2.9% on HumanEval, and +5.5% on\nMMLU-Pro. The performance advantages extend to supervised fine-tuning\nscenarios, highlighting WSM's potential for long-term model refinement.", "AI": {"tldr": "The paper introduces WSM, a framework connecting learning rate scheduling and model merging for improved model performance without traditional decay.", "motivation": "The research aims to enhance learning rate scheduling by eliminating decay phases while maintaining competitive model performance.", "method": "A unifying theoretical framework called Warmup-Stable and Merge (WSM) that connects decay strategies to model averaging schemes is developed.", "result": "WSM shows significant improvements in multiple benchmarks, outperforming the traditional WSD approach by +3.5% on MATH, +2.9% on HumanEval, and +5.5% on MMLU-Pro.", "conclusion": "The study indicates that merge duration is crucial for model performance and suggests WSM as a promising method for long-term model refinement.", "key_contributions": ["Introduction of the WSM framework that links learning rate scheduling with model merging.", "Identification of merge duration as a critical factor influencing model performance.", "Demonstration of superior performance of WSM compared to existing methods across various benchmarks."], "limitations": "", "keywords": ["learning rate scheduling", "model merging", "machine learning", "model performance"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2507.17636", "pdf": "https://arxiv.org/pdf/2507.17636.pdf", "abs": "https://arxiv.org/abs/2507.17636", "title": "Who Attacks, and Why? Using LLMs to Identify Negative Campaigning in 18M Tweets across 19 Countries", "authors": ["Victor Hartman", "Petter Törnberg"], "categories": ["cs.CL"], "comment": null, "summary": "Negative campaigning is a central feature of political competition, yet\nempirical research has been limited by the high cost and limited scalability of\nexisting classification methods. This study makes two key contributions. First,\nit introduces zero-shot Large Language Models (LLMs) as a novel approach for\ncross-lingual classification of negative campaigning. Using benchmark datasets\nin ten languages, we demonstrate that LLMs achieve performance on par with\nnative-speaking human coders and outperform conventional supervised machine\nlearning approaches. Second, we leverage this novel method to conduct the\nlargest cross-national study of negative campaigning to date, analyzing 18\nmillion tweets posted by parliamentarians in 19 European countries between 2017\nand 2022. The results reveal consistent cross-national patterns: governing\nparties are less likely to use negative messaging, while ideologically extreme\nand populist parties -- particularly those on the radical right -- engage in\nsignificantly higher levels of negativity. These findings advance our\nunderstanding of how party-level characteristics shape strategic communication\nin multiparty systems. More broadly, the study demonstrates the potential of\nLLMs to enable scalable, transparent, and replicable research in political\ncommunication across linguistic and cultural contexts.", "AI": {"tldr": "This study uses zero-shot Large Language Models (LLMs) for cross-lingual classification of negative campaigning in political communication, analyzing 18 million tweets across 19 European countries.", "motivation": "To address the high cost and limited scalability of existing classification methods in political competition research, particularly in the context of negative campaigning.", "method": "The study employs zero-shot Large Language Models (LLMs) for classifying negative campaigning across multiple languages using benchmark datasets.", "result": "LLMs achieved performance comparable to that of native-speaking human coders and outperformed traditional supervised machine learning methods; it also identified that governing parties engage less in negative messaging compared to radical right parties.", "conclusion": "The research highlights the ability of LLMs to facilitate scalable and replicable studies in political communication across different languages and cultures.", "key_contributions": ["Introduction of zero-shot LLMs for cross-lingual classification of negative campaigning.", "Conducting the largest cross-national study of negative campaigning with 18 million tweets.", "Identification of consistent patterns in negative messaging across different political parties in Europe."], "limitations": "The study primarily focuses on tweets from parliamentarians and may not encompass broader public sentiment.", "keywords": ["negative campaigning", "zero-shot learning", "cross-lingual", "Large Language Models", "political communication"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2507.17702", "pdf": "https://arxiv.org/pdf/2507.17702.pdf", "abs": "https://arxiv.org/abs/2507.17702", "title": "Towards Greater Leverage: Scaling Laws for Efficient Mixture-of-Experts Language Models", "authors": ["Changxin Tian", "Kunlong Chen", "Jia Liu", "Ziqi Liu", "Zhiqiang Zhang", "Jun Zhou"], "categories": ["cs.CL", "I.2.7"], "comment": null, "summary": "Mixture-of-Experts (MoE) has become a dominant architecture for scaling Large\nLanguage Models (LLMs) efficiently by decoupling total parameters from\ncomputational cost. However, this decoupling creates a critical challenge:\npredicting the model capacity of a given MoE configurations (e.g., expert\nactivation ratio and granularity) remains an unresolved problem. To address\nthis gap, we introduce Efficiency Leverage (EL), a metric quantifying the\ncomputational advantage of an MoE model over a dense equivalent. We conduct a\nlarge-scale empirical study, training over 300 models up to 28B parameters, to\nsystematically investigate the relationship between MoE architectural\nconfigurations and EL. Our findings reveal that EL is primarily driven by the\nexpert activation ratio and the total compute budget, both following\npredictable power laws, while expert granularity acts as a non-linear modulator\nwith a clear optimal range. We integrate these discoveries into a unified\nscaling law that accurately predicts the EL of an MoE architecture based on its\nconfiguration. To validate our derived scaling laws, we designed and trained\nLing-mini-beta, a pilot model for Ling-2.0 series with only 0.85B active\nparameters, alongside a 6.1B dense model for comparison. When trained on an\nidentical 1T high-quality token dataset, Ling-mini-beta matched the performance\nof the 6.1B dense model while consuming over 7x fewer computational resources,\nthereby confirming the accuracy of our scaling laws. This work provides a\nprincipled and empirically-grounded foundation for the scaling of efficient MoE\nmodels.", "AI": {"tldr": "The paper introduces Efficiency Leverage (EL) as a metric for evaluating the computational advantage of Mixture-of-Experts (MoE) architectures, revealing relationships between MoE configurations and EL through empirical research on models with up to 28B parameters.", "motivation": "To find a reliable metric for predicting the model capacity of various MoE configurations, addressing a critical gap in understanding how architectural choices impact efficiency.", "method": "An empirical study was conducted involving the training of over 300 models with varying parameters, focusing on the expert activation ratio, total compute budget, and expert granularity to derive a unified scaling law for EL.", "result": "The study finds that EL correlates with expert activation ratio and total compute budget, typically following predictable power laws, while expert granularity influences EL non-linearly. A pilot model, Ling-mini-beta, demonstrated significant resource savings while matching performance with a larger dense model.", "conclusion": "The findings provide a solid, empirically-backed foundation for efficiently scaling MoE models, offering insights into architectural optimization.", "key_contributions": ["Introduction of Efficiency Leverage as a new metric for MoE efficiency", "Empirical analysis involving 300+ models leading to predictable scaling laws", "Development of a pilot model demonstrating significant resource efficiency."], "limitations": "", "keywords": ["Mixture-of-Experts", "Efficiency Leverage", "Large Language Models", "scaling laws", "computational efficiency"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.17709", "pdf": "https://arxiv.org/pdf/2507.17709.pdf", "abs": "https://arxiv.org/abs/2507.17709", "title": "TyDi QA-WANA: A Benchmark for Information-Seeking Question Answering in Languages of West Asia and North Africa", "authors": ["Parker Riley", "Siamak Shakeri", "Waleed Ammar", "Jonathan H. Clark"], "categories": ["cs.CL"], "comment": null, "summary": "We present TyDi QA-WANA, a question-answering dataset consisting of 28K\nexamples divided among 10 language varieties of western Asia and northern\nAfrica. The data collection process was designed to elicit information-seeking\nquestions, where the asker is genuinely curious to know the answer. Each\nquestion in paired with an entire article that may or may not contain the\nanswer; the relatively large size of the articles results in a task suitable\nfor evaluating models' abilities to utilize large text contexts in answering\nquestions. Furthermore, the data was collected directly in each language\nvariety, without the use of translation, in order to avoid issues of cultural\nrelevance. We present performance of two baseline models, and release our code\nand data to facilitate further improvement by the research community.", "AI": {"tldr": "TyDi QA-WANA is a question-answering dataset with 28K examples across 10 language varieties from western Asia and northern Africa, designed for evaluating models' abilities to use large text contexts in answering questions.", "motivation": "The study aims to create a dataset that allows for culturally relevant question-answering tasks in multiple language varieties without translation biases.", "method": "Data was collected by eliciting information-seeking questions directly in each language variety and pairing them with relevant articles.", "result": "Two baseline models were tested on the dataset, establishing benchmarks for future research.", "conclusion": "The dataset is released to enable the research community to improve question-answering systems using large contexts tailored to specific language and cultural features.", "key_contributions": ["Creation of a large-scale, culturally relevant question-answering dataset.", "Evaluation of baseline models that utilize large text contexts for question answering.", "Availability of code and data for community use."], "limitations": "", "keywords": ["question-answering", "dataset", "language varieties", "cultural relevance", "natural language processing"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2507.17718", "pdf": "https://arxiv.org/pdf/2507.17718.pdf", "abs": "https://arxiv.org/abs/2507.17718", "title": "AI Telephone Surveying: Automating Quantitative Data Collection with an AI Interviewer", "authors": ["Danny D. Leybzon", "Shreyas Tirumala", "Nishant Jain", "Summer Gillen", "Michael Jackson", "Cameron McPhee", "Jennifer Schmidt"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "With the rise of voice-enabled artificial intelligence (AI) systems,\nquantitative survey researchers have access to a new data-collection mode: AI\ntelephone surveying. By using AI to conduct phone interviews, researchers can\nscale quantitative studies while balancing the dual goals of human-like\ninteractivity and methodological rigor. Unlike earlier efforts that used\ninteractive voice response (IVR) technology to automate these surveys, voice AI\nenables a more natural and adaptive respondent experience as it is more robust\nto interruptions, corrections, and other idiosyncrasies of human speech.\n  We built and tested an AI system to conduct quantitative surveys based on\nlarge language models (LLM), automatic speech recognition (ASR), and speech\nsynthesis technologies. The system was specifically designed for quantitative\nresearch, and strictly adhered to research best practices like question order\nrandomization, answer order randomization, and exact wording.\n  To validate the system's effectiveness, we deployed it to conduct two pilot\nsurveys with the SSRS Opinion Panel and followed-up with a separate\nhuman-administered survey to assess respondent experiences. We measured three\nkey metrics: the survey completion rates, break-off rates, and respondent\nsatisfaction scores. Our results suggest that shorter instruments and more\nresponsive AI interviewers may contribute to improvements across all three\nmetrics studied.", "AI": {"tldr": "The paper presents an AI telephone surveying system that uses voice-enabled AI technologies to conduct quantitative surveys, enhancing respondent experience and methodological rigor.", "motivation": "The rise of voice-enabled AI systems provides a new mode for quantitative survey research, allowing for increased scale and interactivity in data collection.", "method": "An AI system built using large language models and speech technologies was tested in two pilot surveys among the SSRS Opinion Panel, following strict research methodologies.", "result": "The system demonstrated improved survey completion rates, lower break-off rates, and higher respondent satisfaction, particularly with shorter instruments and adaptive AI interviewers.", "conclusion": "AI-driven telephone surveying can effectively enhance the quality and efficiency of quantitative research by providing a more natural interaction for respondents.", "key_contributions": ["Development of a novel AI telephone surveying system utilizing LLM and ASR technologies", "Validation of effectiveness through pilot surveys and comparison with human-administered surveys", "Identification of factors influencing survey completion and satisfaction rates."], "limitations": "The study is limited to the specific context of the SSRS Opinion Panel and may require further testing across diverse populations.", "keywords": ["AI telephone surveying", "voice AI", "quantitative research", "survey methodology", "respondent experience"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.17717", "pdf": "https://arxiv.org/pdf/2507.17717.pdf", "abs": "https://arxiv.org/abs/2507.17717", "title": "From Feedback to Checklists: Grounded Evaluation of AI-Generated Clinical Notes", "authors": ["Karen Zhou", "John Giorgi", "Pranav Mani", "Peng Xu", "Davis Liang", "Chenhao Tan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "AI-generated clinical notes are increasingly used in healthcare, but\nevaluating their quality remains a challenge due to high subjectivity and\nlimited scalability of expert review. Existing automated metrics often fail to\nalign with real-world physician preferences. To address this, we propose a\npipeline that systematically distills real user feedback into structured\nchecklists for note evaluation. These checklists are designed to be\ninterpretable, grounded in human feedback, and enforceable by LLM-based\nevaluators. Using deidentified data from over 21,000 clinical encounters,\nprepared in accordance with the HIPAA safe harbor standard, from a deployed AI\nmedical scribe system, we show that our feedback-derived checklist outperforms\nbaseline approaches in our offline evaluations in coverage, diversity, and\npredictive power for human ratings. Extensive experiments confirm the\nchecklist's robustness to quality-degrading perturbations, significant\nalignment with clinician preferences, and practical value as an evaluation\nmethodology. In offline research settings, the checklist can help identify\nnotes likely to fall below our chosen quality thresholds.", "AI": {"tldr": "This paper proposes a systematic pipeline to create checklists for evaluating AI-generated clinical notes based on real user feedback, outperforming baseline methods in various measures.", "motivation": "Evaluating the quality of AI-generated clinical notes is challenging due to subjectivity and limitations of existing automated metrics.", "method": "The authors developed a pipeline that distills user feedback into structured evaluation checklists that are interpretable and grounded in human preferences, then evaluated its performance against existing methods.", "result": "The checklist derived from feedback showed superior coverage, diversity, and predictive power for human ratings compared to baseline approaches.", "conclusion": "The feedback-derived checklist is robust and aligns closely with clinician preferences, providing practical value in identifying clinical notes that do not meet quality standards.", "key_contributions": ["Development of a systematic pipeline for checklist creation from real user feedback", "Demonstration of enhanced evaluation effectiveness over baseline methods", "Practical application in identifying low-quality clinical notes"], "limitations": "", "keywords": ["AI-generated notes", "clinical evaluation", "human feedback", "checklist", "healthcare"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.17718", "pdf": "https://arxiv.org/pdf/2507.17718.pdf", "abs": "https://arxiv.org/abs/2507.17718", "title": "AI Telephone Surveying: Automating Quantitative Data Collection with an AI Interviewer", "authors": ["Danny D. Leybzon", "Shreyas Tirumala", "Nishant Jain", "Summer Gillen", "Michael Jackson", "Cameron McPhee", "Jennifer Schmidt"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "With the rise of voice-enabled artificial intelligence (AI) systems,\nquantitative survey researchers have access to a new data-collection mode: AI\ntelephone surveying. By using AI to conduct phone interviews, researchers can\nscale quantitative studies while balancing the dual goals of human-like\ninteractivity and methodological rigor. Unlike earlier efforts that used\ninteractive voice response (IVR) technology to automate these surveys, voice AI\nenables a more natural and adaptive respondent experience as it is more robust\nto interruptions, corrections, and other idiosyncrasies of human speech.\n  We built and tested an AI system to conduct quantitative surveys based on\nlarge language models (LLM), automatic speech recognition (ASR), and speech\nsynthesis technologies. The system was specifically designed for quantitative\nresearch, and strictly adhered to research best practices like question order\nrandomization, answer order randomization, and exact wording.\n  To validate the system's effectiveness, we deployed it to conduct two pilot\nsurveys with the SSRS Opinion Panel and followed-up with a separate\nhuman-administered survey to assess respondent experiences. We measured three\nkey metrics: the survey completion rates, break-off rates, and respondent\nsatisfaction scores. Our results suggest that shorter instruments and more\nresponsive AI interviewers may contribute to improvements across all three\nmetrics studied.", "AI": {"tldr": "This paper presents an AI-powered system for conducting quantitative surveys via telephone, focusing on enhancing respondent experience through voice AI.", "motivation": "The rise of voice-enabled AI systems enables a new methodology for conducting quantitative surveys, aiming for a blend of interactivity and methodological rigor.", "method": "An AI system utilizing large language models (LLM), automatic speech recognition (ASR), and speech synthesis was built to conduct phone interviews, adhering to research best practices in quantitative studies.", "result": "Pilot surveys showed that using responsive AI interviewers and shorter survey instruments can significantly improve completion rates, reduce break-off rates, and increase respondent satisfaction.", "conclusion": "The findings indicate the potential of voice AI in enhancing quantitative survey research, suggesting a shift from traditional methods to AI-based approaches.", "key_contributions": ["Development of an AI system for telephone surveying using LLM, ASR, and speech synthesis.", "Validation of the system through pilot surveys that compare AI-administered and human-administered surveys.", "Identification of key metrics for survey effectiveness such as completion rates and respondent satisfaction."], "limitations": "", "keywords": ["AI telephone surveying", "voice AI", "quantitative research"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2408.08068", "pdf": "https://arxiv.org/pdf/2408.08068.pdf", "abs": "https://arxiv.org/abs/2408.08068", "title": "The Paradox of Spreadsheet Self-Efficacy: Social Incentives for Informal Knowledge Sharing in End-User Programming", "authors": ["Qing", "Xia", "Advait Sarkar", "Duncan P. Brumby", "Anna Cox"], "categories": ["cs.HC"], "comment": "8 pages", "summary": "Informal Knowledge Sharing (KS) is vital for end-user programmers to gain\nexpertise. To better understand how personal (self-efficacy), social\n(reputational gains, trust between colleagues), and software-related\n(codification effort) variables influence spreadsheet KS intention, we\nconducted a multiple regressions analysis based on survey data from spreadsheet\nusers (n=100) in administrative and finance roles. We found that high levels of\nspreadsheet self-efficacy and a perception that sharing would result in\nreputational gains predicted higher KS intention, but individuals who found\nknowledge codification effortful showed lower KS intention. We also observed\nthat regardless of occupation, users tended to report a lower sense of\nself-efficacy in their general spreadsheet proficiency, despite also reporting\nhigh self-efficacy in spreadsheet use for job-related contexts. Our findings\nsuggest that acknowledging and designing for these social and personal\nvariables can help avoid situations where experienced individuals refrain\nunnecessarily from sharing, with implications for spreadsheet design.", "AI": {"tldr": "This paper examines factors influencing informal knowledge sharing among spreadsheet users, emphasizing personal, social, and software-related variables.", "motivation": "To understand how personal (self-efficacy), social (reputational gains, trust), and software-related (codification effort) factors influence knowledge sharing intentions among end-user programmers.", "method": "Multiple regression analysis based on survey data from 100 spreadsheet users in administrative and finance roles.", "result": "High self-efficacy in spreadsheet use and perceived reputational gains led to higher knowledge sharing intentions, while perceived effort in knowledge codification decreased these intentions.", "conclusion": "Designing for social and personal variables can encourage knowledge sharing among experienced users, which has implications for spreadsheet design.", "key_contributions": ["Identification of personal and social factors affecting knowledge sharing", "Quantitative analysis of spreadsheet users' sharing intentions", "Insights into spreadsheet design improvements for fostering knowledge sharing"], "limitations": "", "keywords": ["Knowledge Sharing", "Spreadsheet", "Self-efficacy", "Reputational Gains", "User Design"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2507.17728", "pdf": "https://arxiv.org/pdf/2507.17728.pdf", "abs": "https://arxiv.org/abs/2507.17728", "title": "Megrez2 Technical Report", "authors": ["Boxun Li", "Yadong Li", "Zhiyuan Li", "Congyi Liu", "Weilin Liu", "Guowei Niu", "Zheyue Tan", "Haiyang Xu", "Zhuyu Yao", "Tao Yuan", "Dong Zhou", "Yueqing Zhuang", "Bo Zhao", "Guohao Dai", "Yu Wang"], "categories": ["cs.CL"], "comment": null, "summary": "We present Megrez2, a novel lightweight and high-performance language model\narchitecture optimized for device native deployment. Megrez2 introduces a novel\ncross-layer expert sharing mechanism, which significantly reduces total\nparameter count by reusing expert modules across adjacent transformer layers\nwhile maintaining most of the model's capacity. It also incorporates pre-gated\nrouting, enabling memory-efficient expert loading and faster inference. As the\nfirst instantiation of the Megrez2 architecture, we introduce the\nMegrez2-Preview model, which is pre-trained on a 5-trillion-token corpus and\nfurther enhanced through supervised fine-tuning and reinforcement learning with\nverifiable rewards. With only 3B activated and 7.5B stored parameters,\nMegrez2-Preview demonstrates competitive or superior performance compared to\nlarger models on a wide range of tasks, including language understanding,\ninstruction following, mathematical reasoning, and code generation. These\nresults highlight the effectiveness of the Megrez2 architecture to achieve a\nbalance between accuracy, efficiency, and deployability, making it a strong\ncandidate for real-world, resource-constrained applications.", "AI": {"tldr": "Megrez2 is a lightweight, high-performance language model optimized for device-native deployment, featuring cross-layer expert sharing and pre-gated routing for enhanced efficiency.", "motivation": "To create an efficient language model architecture that can be deployed on resource-constrained devices without sacrificing quality.", "method": "Introduces a cross-layer expert sharing mechanism to reduce parameter count and employs pre-gated routing for memory-efficient expert loading.", "result": "Megrez2-Preview, with 3B activated and 7.5B stored parameters, outperforms or matches larger models across various language tasks, demonstrating strong accuracy and efficiency.", "conclusion": "The Megrez2 architecture strikes a balance between performance and resource efficiency, making it suitable for real-world applications.", "key_contributions": ["Introduction of cross-layer expert sharing mechanism", "Pre-gated routing for memory-efficient loading", "Demonstration of competitive performance with fewer parameters"], "limitations": "", "keywords": ["Megrez2", "language model", "expert sharing", "device deployment", "efficient inference"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2409.14659", "pdf": "https://arxiv.org/pdf/2409.14659.pdf", "abs": "https://arxiv.org/abs/2409.14659", "title": "Image memorability predicts social media virality and externally-associated commenting", "authors": ["Shikang Peng", "Wilma A. Bainbridge"], "categories": ["cs.HC", "cs.CE", "cs.SI", "J.4"], "comment": "47 pages, 5 figures", "summary": "Visual content on social media plays a key role in entertainment and\ninformation sharing, yet some images gain more engagement than others. We\npropose that image memorability - the ability to be remembered - may predict\nviral potential. Using 1,247 Reddit image posts across three timepoints, we\nassessed memorability with neural network ResMem and correlated the predicted\nmemorability scores with virality metrics. Memorable images are consistently\nassociated with more comments, even after controlling for image categories with\nResNet-152. Semantic analysis revealed that memorable images relate to more\nneutral-affect comments, suggesting a distinct pathway to virality from\nemotional contents. Additionally, visual consistency analysis showed that\nmemorable posts inspired diverse, externally-associated comments. By analyzing\nResMem's layers, we found that semantic distinctiveness was key to both\nmemorability and virality even after accounting for image category effects.\nThis study highlights memorability as a unique correlate of social media\nvirality, offering insights into how visual features and human cognitive\nbehavioral interactions are associated with online engagement.", "AI": {"tldr": "This study investigates the relationship between image memorability and virality on social media, particularly through the analysis of Reddit image posts.", "motivation": "To understand why certain images achieve more engagement on social media, specifically regarding their memorability and how it may predict viral potential.", "method": "Analyzed 1,247 Reddit image posts using the neural network ResMem to assess memorability and correlated that with virality metrics such as the number of comments, while controlling for image categories using ResNet-152.", "result": "Memorable images are linked to increased engagement, specifically more comments, and this effect persists when controlling for categories. Additionally, memorable images prompted more neutral-affect comments and were associated with semantic distinctiveness.", "conclusion": "Memorability is a significant predictor of social media virality, suggesting that visual features and cognitive interactions influence online engagement.", "key_contributions": ["Establishes a link between image memorability and social media virality.", "Identifies semantic distinctiveness as a key factor in memorability and virality.", "Provides insights into the types of comments associated with memorable versus non-memorable images."], "limitations": "Limited to images from Reddit and specific virality metrics; may not generalize to all social media platforms or image types.", "keywords": ["image memorability", "social media engagement", "virality", "neural networks", "semantic distinctiveness"], "importance_score": 4, "read_time_minutes": 47}}
{"id": "2507.17747", "pdf": "https://arxiv.org/pdf/2507.17747.pdf", "abs": "https://arxiv.org/abs/2507.17747", "title": "Pretraining on the Test Set Is No Longer All You Need: A Debate-Driven Approach to QA Benchmarks", "authors": ["Linbo Cao", "Jinman Zhao"], "categories": ["cs.CL", "cs.AI"], "comment": "22 pages, 7 figures. Accepted to COLM 2025. Code available at:\n  github.com/l6cao/Debate-Driven-Evaluation", "summary": "As frontier language models increasingly saturate standard QA benchmarks,\nconcerns about data contamination, memorization, and escalating dataset\ncreation costs persist. We propose a debate-driven evaluation paradigm that\ntransforms any existing QA dataset into structured adversarial debates--where\none model is given the official answer to defend, and another constructs and\ndefends an alternative answer--adjudicated by a judge model blind to the\ncorrect solution. By forcing multi-round argumentation, this approach\nsubstantially increases difficulty while penalizing shallow memorization, yet\nreuses QA items to reduce curation overhead. We make two main contributions:\n(1) an evaluation pipeline to systematically convert QA tasks into debate-based\nassessments, and (2) a public benchmark that demonstrates our paradigm's\neffectiveness on a subset of MMLU-Pro questions, complete with standardized\nprotocols and reference models. Empirical results validate the robustness of\nthe method and its effectiveness against data contamination--a Llama 3.1 model\nfine-tuned on test questions showed dramatic accuracy improvements (50% -> 82%)\nbut performed worse in debates. Results also show that even weaker judges can\nreliably differentiate stronger debaters, highlighting how debate-based\nevaluation can scale to future, more capable systems while maintaining a\nfraction of the cost of creating new benchmarks. Overall, our framework\nunderscores that \"pretraining on the test set is no longer all you need,\"\noffering a sustainable path for measuring the genuine reasoning ability of\nadvanced language models.", "AI": {"tldr": "Proposes a debate-driven evaluation paradigm for QA datasets to reduce data contamination and improve reasoning assessment of language models.", "motivation": "Standard QA benchmarks are saturated, raising issues of data contamination and the costs associated with dataset creation.", "method": "Transforms QA datasets into structured adversarial debates involving multi-round argumentation between models, adjudicated by a judge model.", "result": "The approach leads to dramatic improvements in a fine-tuned model's accuracy on traditional tasks, but highlights poorer performance during debates, indicating a stronger emphasis on reasoning.", "conclusion": "This framework offers a sustainable method for assessing reasoning abilities of language models, suggesting that relying solely on pretraining from test data is not sufficient anymore.", "key_contributions": ["An evaluation pipeline to convert QA tasks into debate-based assessments.", "A public benchmark demonstrating effectiveness on MMLU-Pro questions."], "limitations": "Focus on a specific subset of questions may limit generalizability and practical application.", "keywords": ["Human-Computer Interaction", "Machine Learning", "Language Models", "Evaluation", "Benchmarking"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2501.08518", "pdf": "https://arxiv.org/pdf/2501.08518.pdf", "abs": "https://arxiv.org/abs/2501.08518", "title": "Alleviating Seasickness through Brain-Computer Interface-based Attention Shift", "authors": ["Xiaoyu Bao", "Kailin Xu", "Jiawei Zhu", "Haiyun Huang", "Kangning Li", "Qiyun Huang", "Yuanqing Li"], "categories": ["cs.HC", "cs.AI", "eess.SP", "q-bio.QM"], "comment": null, "summary": "Seasickness poses a widespread problem that adversely impacts both passenger\ncomfort and the operational efficiency of maritime crews. Although attention\nshift has been proposed as a potential method to alleviate symptoms of motion\nsickness, its efficacy remains to be rigorously validated, especially in\nmaritime environments. In this study, we develop an AI-driven brain-computer\ninterface (BCI) to realize sustained and practical attention shift by\nincorporating tasks such as breath counting. Forty-three participants completed\na real-world nautical experiment consisting of a real-feedback session, a\nresting session, and a pseudo-feedback session. Notably, 81.39\\% of the\nparticipants reported that the BCI intervention was effective. EEG analysis\nrevealed that the proposed system can effectively regulate motion sickness EEG\nsignatures, such as an decrease in total band power, along with an increase in\ntheta relative power and a decrease in beta relative power. Furthermore, an\nindicator of attentional focus, the theta/beta ratio, exhibited a significant\nreduction during the real-feedback session, providing further evidence to\nsupport the effectiveness of the BCI in shifting attention. Collectively, this\nstudy presents a novel nonpharmacological, portable, and effective approach for\nseasickness intervention, which has the potential to open up a brand-new\napplication domain for BCIs.", "AI": {"tldr": "This study develops an AI-driven brain-computer interface (BCI) to alleviate seasickness by promoting attention shift, with significant participant effectiveness reported.", "motivation": "To address the widespread problem of seasickness affecting passenger comfort and maritime operations.", "method": "An AI-driven BCI was developed, incorporating tasks like breath counting, tested in a real-world nautical experiment with 43 participants across feedback sessions.", "result": "81.39% of participants found the BCI intervention effective; EEG analysis showed regulation of motion sickness EEG signatures and improved attentional focus.", "conclusion": "This BCI approach offers a novel, portable, nonpharmacological solution for seasickness, expanding BCI applications.", "key_contributions": ["Development of an AI-driven BCI for seasickness intervention", "Real-world testing with significant participant feedback", "EEG analysis supporting BCI effectiveness in attention shift"], "limitations": "", "keywords": ["brain-computer interface", "seasickness", "attention shift", "EEG analysis", "nonpharmaceutical intervention"], "importance_score": 3, "read_time_minutes": 5}}
{"id": "2507.16258", "pdf": "https://arxiv.org/pdf/2507.16258.pdf", "abs": "https://arxiv.org/abs/2507.16258", "title": "Animal Interaction with Autonomous Mobility Systems: Designing for Multi-Species Coexistence", "authors": ["Tram Thi Minh Tran", "Xinyan Yu", "Marius Hoggenmueller", "Callum Parker", "Paul Schmitt", "Julie Stephany Berrio Perez", "Stewart Worrall", "Martin Tomitsch"], "categories": ["cs.HC"], "comment": null, "summary": "Autonomous mobility systems increasingly operate in environments shared with\nanimals, from urban pets to wildlife. However, their design has largely focused\non human interaction, with limited understanding of how non-human species\nperceive, respond to, or are affected by these systems. Motivated by research\nin Animal-Computer Interaction (ACI) and more-than-human design, this study\ninvestigates animal interactions with autonomous mobility through a\nmulti-method approach combining a scoping review (45 articles), online\nethnography (39 YouTube videos and 11 Reddit discussions), and expert\ninterviews (8 participants). Our analysis surfaces five key areas of concern:\nPhysical Impact (e.g., collisions, failures to detect), Behavioural Effects\n(e.g., avoidance, stress), Accessibility Concerns (particularly for service\nanimals), Ethics and Regulations, and Urban Disturbance. We conclude with\ndesign and policy directions aimed at supporting multispecies coexistence in\nthe age of autonomous systems. This work underscores the importance of\nincorporating non-human perspectives to ensure safer, more inclusive futures\nfor all species.", "AI": {"tldr": "The paper investigates how autonomous mobility systems affect animal interactions, emphasizing the importance of including non-human perspectives in design.", "motivation": "To address the limited understanding of how non-human species perceive and are affected by autonomous mobility systems designed primarily for human interaction.", "method": "A multi-method approach combining a scoping review of 45 articles, online ethnography of 39 YouTube videos and 11 Reddit discussions, and interviews with 8 experts.", "result": "Identified five key areas of concern: Physical Impact, Behavioural Effects, Accessibility Concerns, Ethics and Regulations, and Urban Disturbance.", "conclusion": "The study provides design and policy directions for fostering multispecies coexistence with autonomous systems by considering the needs and perspectives of non-human species.", "key_contributions": ["Highlights the intersection of Animal-Computer Interaction with autonomous mobility design.", "Presents a comprehensive analysis of animal interactions with autonomous systems.", "Proposes design and policy recommendations for multispecies coexistence."], "limitations": "", "keywords": ["Autonomous Mobility", "Animal-Computer Interaction", "Multispecies Design", "HCI", "Ethics"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2403.14459", "pdf": "https://arxiv.org/pdf/2403.14459.pdf", "abs": "https://arxiv.org/abs/2403.14459", "title": "Multi-Level Explanations for Generative Language Models", "authors": ["Lucas Monteiro Paes", "Dennis Wei", "Hyo Jin Do", "Hendrik Strobelt", "Ronny Luss", "Amit Dhurandhar", "Manish Nagireddy", "Karthikeyan Natesan Ramamurthy", "Prasanna Sattigeri", "Werner Geyer", "Soumya Ghosh"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted as an oral presentation at ACL 2025. Code available at\n  https://github.com/IBM/ICX360", "summary": "Despite the increasing use of large language models (LLMs) for\ncontext-grounded tasks like summarization and question-answering, understanding\nwhat makes an LLM produce a certain response is challenging. We propose\nMulti-Level Explanations for Generative Language Models (MExGen), a technique\nto provide explanations for context-grounded text generation. MExGen assigns\nscores to parts of the context to quantify their influence on the model's\noutput. It extends attribution methods like LIME and SHAP to LLMs used in\ncontext-grounded tasks where (1) inference cost is high, (2) input text is\nlong, and (3) the output is text. We conduct a systematic evaluation, both\nautomated and human, of perturbation-based attribution methods for\nsummarization and question answering. The results show that our framework can\nprovide more faithful explanations of generated output than available\nalternatives, including LLM self-explanations. We open-source code for MExGen\nas part of the ICX360 toolkit: https://github$.$com/IBM/ICX360.", "AI": {"tldr": "The paper presents Multi-Level Explanations for Generative Language Models (MExGen), a technique designed to provide explanations for context-grounded text generation in LLMs, improving understanding of their responses.", "motivation": "There is a need for better understanding of how LLMs generate responses, particularly in context-grounded tasks like summarization and question-answering, where current methods fall short.", "method": "MExGen assigns scores to parts of the context to quantify their influence on the model's output, extending existing attribution methods (LIME, SHAP) to address challenges related to high inference costs and long inputs.", "result": "MExGen provides more faithful explanations of generated outputs compared to alternatives, including self-explanations from LLMs, as demonstrated through systematic evaluations.", "conclusion": "The framework improves the interpretability of LLMs in context-grounded tasks and the open-sourced code enables further exploration and application.", "key_contributions": ["Introduction of MExGen for explaining LLM outputs in context-aware tasks.", "Enhanced attribution methodology for LLMs beyond existing methods like LIME and SHAP.", "Comprehensive evaluation demonstrating improved explanation fidelity."], "limitations": "", "keywords": ["Large Language Models", "Interpretability", "Attribution Methods"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2405.08427", "pdf": "https://arxiv.org/pdf/2405.08427.pdf", "abs": "https://arxiv.org/abs/2405.08427", "title": "Impact of Stickers on Multimodal Sentiment and Intent in Social Media: A New Task, Dataset and Baseline", "authors": ["Yuanchen Shi", "Biao Ma", "Longyin Zhang", "Fang Kong"], "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, 7 figures", "summary": "Stickers are increasingly used in social media to express sentiment and\nintent. Despite their significant impact on sentiment analysis and intent\nrecognition, little research has been conducted in this area. To address this\ngap, we propose a new task: \\textbf{M}ultimodal chat \\textbf{S}entiment\n\\textbf{A}nalysis and \\textbf{I}ntent \\textbf{R}ecognition involving\n\\textbf{S}tickers (MSAIRS). Additionally, we introduce a novel multimodal\ndataset containing Chinese chat records and stickers excerpted from several\nmainstream social media platforms. Our dataset includes paired data with the\nsame text but different stickers, the same sticker but different contexts, and\nvarious stickers consisting of the same images with different texts, allowing\nus to better understand the impact of stickers on chat sentiment and intent. We\nalso propose an effective multimodal joint model, MMSAIR, featuring\ndifferential vector construction and cascaded attention mechanisms for enhanced\nmultimodal fusion. Our experiments demonstrate the necessity and effectiveness\nof jointly modeling sentiment and intent, as they mutually reinforce each\nother's recognition accuracy. MMSAIR significantly outperforms traditional\nmodels and advanced MLLMs, demonstrating the challenge and uniqueness of\nsticker interpretation in social media. Our dataset and code are available on\nhttps://github.com/FakerBoom/MSAIRS-Dataset.", "AI": {"tldr": "This paper introduces MSAIRS, a new framework for multimodal sentiment analysis and intent recognition using stickers in chat messages, along with a novel dataset and an effective multimodal model.", "motivation": "The use of stickers in social media for expressing sentiment and intent is significant, yet underexplored in research.", "method": "We propose the MSAIRS task and introduce a multimodal dataset of Chinese chat records and stickers. We develop a joint model, MMSAIR, with differential vector construction and cascaded attention for better sentiment and intent recognition.", "result": "MMSAIR outperforms traditional methods and advanced MLLMs in sentiment and intent recognition accuracy, highlighting the unique challenges of sticker interpretation.", "conclusion": "Jointly modeling sentiment and intent improves recognition accuracy, and our dataset and model contribute to better understanding the impact of stickers.", "key_contributions": ["Proposal of the MSAIRS task providing a new research direction.", "Introduction of a novel multimodal dataset for sentiment analysis and intent recognition.", "Development of an effective multimodal model, MMSAIR, enhancing analysis accuracy."], "limitations": "Focuses only on Chinese chat records and stickers, limiting generalizability to other languages or contexts.", "keywords": ["multimodal", "sentiment analysis", "intent recognition", "stickers", "social media"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2408.16446", "pdf": "https://arxiv.org/pdf/2408.16446.pdf", "abs": "https://arxiv.org/abs/2408.16446", "title": "Is text normalization relevant for classifying medieval charters?", "authors": ["Florian Atzenhofer-Baumgartner", "Tamás Kovács"], "categories": ["cs.CL", "cs.IR"], "comment": "This preprint has not undergone peer review or any post-submission\n  improvements or corrections. The Version of Record of this contribution is\n  published in LNCS volume 15178 and is available online at\n  https://doi.org/10.1007/978-3-031-72440-4_12", "summary": "This study examines the impact of historical text normalization on the\nclassification of medieval charters, specifically focusing on document dating\nand locating. Using a data set of Middle High German charters from a digital\narchive, we evaluate various classifiers, including traditional and\ntransformer-based models, with and without normalization. Our results indicate\nthat the given normalization minimally improves locating tasks but reduces\naccuracy for dating, implying that original texts contain crucial features that\nnormalization may obscure. We find that support vector machines and gradient\nboosting outperform other models, questioning the efficiency of transformers\nfor this use case. Results suggest a selective approach to historical text\nnormalization, emphasizing the significance of preserving some textual\ncharacteristics that are critical for classification tasks in document\nanalysis.", "AI": {"tldr": "This study assesses the impact of historical text normalization on classifying medieval charters, revealing minimal benefits for location tasks and diminished dating accuracy.", "motivation": "To explore how historical text normalization affects the classification process of medieval charters, focusing on dating and locating tasks.", "method": "Evaluation of various classifiers, including traditional and transformer-based models, on a dataset of Middle High German charters, with and without text normalization.", "result": "Normalization slightly enhances locating tasks but reduces accuracy for dating; support vector machines and gradient boosting outperform other models, challenging the effectiveness of transformers.", "conclusion": "A selective approach to historical text normalization is recommended, as preserving certain textual features is vital for effective classification in document analysis.", "key_contributions": ["Assessment of normalization effects on medieval charter classification", "Comparison of various classification models including transformer-based", "Recommendations for selective normalization in textual analysis"], "limitations": "Results are specific to Middle High German charters and may not generalize to other texts.", "keywords": ["text normalization", "classification", "medieval charters", "machine learning", "document analysis"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2502.12988", "pdf": "https://arxiv.org/pdf/2502.12988.pdf", "abs": "https://arxiv.org/abs/2502.12988", "title": "Beyond Profile: From Surface-Level Facts to Deep Persona Simulation in LLMs", "authors": ["Zixiao Wang", "Duzhen Zhang", "Ishita Agrawal", "Shen Gao", "Le Song", "Xiuying Chen"], "categories": ["cs.CL"], "comment": "19 pages, 3 figures, ACL 2025 Findings", "summary": "Previous approaches to persona simulation large language models (LLMs) have\ntypically relied on learning basic biographical information, or using limited\nrole-play dialogue datasets to capture a character's responses. However, a\nholistic representation of an individual goes beyond surface-level facts or\nconversations to deeper thoughts and thinking. In this work, we introduce\nCharacterBot, a model designed to replicate both the linguistic patterns and\ndistinctive thought processes of a character. Using Lu Xun, a renowned Chinese\nwriter, as a case study, we propose four training tasks derived from his 17\nessay collections. These include a pre-training task focused on mastering\nexternal linguistic structures and knowledge, as well as three fine-tuning\ntasks: multiple-choice question answering, generative question answering, and\nstyle transfer, each aligning the LLM with Lu Xun's internal ideation and\nwriting style. To optimize learning across these tasks, we introduce a CharLoRA\nparameter updating mechanism, where a general linguistic style expert\ncollaborates with other task-specific experts to better study both the language\nstyle and the understanding of deeper thoughts. We evaluate CharacterBot on\nthree tasks for linguistic accuracy and opinion comprehension, demonstrating\nthat it significantly outperforms the baselines on our adapted metrics. We hope\nthat this work inspires future research on deep character persona simulation\nLLM.", "AI": {"tldr": "CharacterBot is a model designed for persona simulation in language models, effectively replicating distinct thought processes and linguistic styles, demonstrated through the case study of writer Lu Xun.", "motivation": "Current methods for persona simulation in LLMs fall short by focusing solely on biographical data and limited dialogue, necessitating a more comprehensive understanding of characters.", "method": "CharacterBot employs a holistic approach that includes a pre-training task for linguistic structure mastery and three fine-tuning tasks focused on question answering and style transfer, complemented by a CharLoRA parameter updating mechanism for optimized learning.", "result": "CharacterBot demonstrates significant improvements in linguistic accuracy and opinion comprehension compared to baseline models, evaluated through tailored metrics.", "conclusion": "The study showcases the potential of CharacterBot in enhancing character persona simulation in language models, encouraging further exploration in this domain.", "key_contributions": ["Introduction of CharacterBot model for deeper persona simulation", "Proposal of CharLoRA parameter updating mechanism", "Demonstrated effectiveness through case study of Lu Xun's writing style"], "limitations": "", "keywords": ["persona simulation", "large language models", "character representation", "machine learning", "natural language processing"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2503.02382", "pdf": "https://arxiv.org/pdf/2503.02382.pdf", "abs": "https://arxiv.org/abs/2503.02382", "title": "An Efficient and Precise Training Data Construction Framework for Process-supervised Reward Model in Mathematical Reasoning", "authors": ["Wei Sun", "Qianlong Du", "Fuwei Cui", "Jiajun Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Enhancing the mathematical reasoning capabilities of Large Language Models\n(LLMs) is of great scientific and practical significance. Researchers typically\nemploy process-supervised reward models (PRMs) to guide the reasoning process,\neffectively improving the models' reasoning abilities. However, existing\nmethods for constructing process supervision training data, such as manual\nannotation and per-step Monte Carlo estimation, are often costly or suffer from\npoor quality. To address these challenges, this paper introduces a framework\ncalled EpicPRM, which annotates each intermediate reasoning step based on its\nquantified contribution and uses an adaptive binary search algorithm to enhance\nboth annotation precision and efficiency. Using this approach, we efficiently\nconstruct a high-quality process supervision training dataset named Epic50k,\nconsisting of 50k annotated intermediate steps. Compared to other publicly\navailable datasets, the PRM trained on Epic50k demonstrates significantly\nsuperior performance. Getting Epic50k at https://github.com/xiaolizh1/EpicPRM.", "AI": {"tldr": "This paper presents EpicPRM, a framework that improves the reasoning capabilities of Large Language Models by efficiently annotating intermediate reasoning steps and constructing a high-quality dataset called Epic50k.", "motivation": "To enhance mathematical reasoning in Large Language Models (LLMs) and address inefficiencies and costs associated with existing training data construction methods.", "method": "EpicPRM introduces an adaptive binary search algorithm for annotating each reasoning step based on its contribution, leading to the creation of a useful training dataset, Epic50k.", "result": "The Epic50k dataset consists of 50k high-quality annotated intermediate reasoning steps, which significantly improves the performance of the PRM compared to other available datasets.", "conclusion": "The EpicPRM framework not only optimizes the annotation process but also allows for the development of a substantial dataset that boosts the reasoning abilities of LLMs.", "key_contributions": ["Introduction of the EpicPRM framework for process supervision", "Creation of the Epic50k dataset with 50k annotated reasoning steps", "Improvement in reasoning performance of LLMs using the new dataset"], "limitations": "", "keywords": ["Large Language Models", "mathematical reasoning", "process supervision", "Epic50k", "adaptive binary search"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2503.03460", "pdf": "https://arxiv.org/pdf/2503.03460.pdf", "abs": "https://arxiv.org/abs/2503.03460", "title": "Visualising Policy-Reward Interplay to Inform Zeroth-Order Preference Optimisation of Large Language Models", "authors": ["Alessio Galatolo", "Zhenbang Dai", "Katie Winkle", "Meriem Beloucif"], "categories": ["cs.CL"], "comment": "ACL25 Findings", "summary": "Fine-tuning Large Language Models (LLMs) with first-order methods like\nback-propagation is computationally intensive. Zeroth-Order (ZO) optimisation\nuses function evaluations instead of gradients, reducing memory usage, but\nsuffers from slow convergence in high-dimensional models. As a result, ZO\nresearch in LLMs has mostly focused on classification, overlooking more complex\ngenerative tasks. In this paper, we introduce ZOPrO, a novel ZO algorithm\ndesigned for Preference Optimisation in LLMs. We begin by analysing the\ninterplay between policy and reward models during traditional (first-order)\nPreference Optimisation, uncovering patterns in their relative updates. Guided\nby these insights, we adapt Simultaneous Perturbation Stochastic Approximation\n(SPSA) with a targeted sampling strategy to accelerate convergence. Through\nexperiments on summarisation, machine translation, and conversational\nassistants, we demonstrate that our method consistently enhances reward signals\nwhile achieving convergence times comparable to first-order methods. While it\nfalls short of some state-of-the-art methods, our work is the first to apply\nZeroth-Order methods to Preference Optimisation in LLMs, going beyond\nclassification tasks and paving the way for a largely unexplored research\ndirection. Code and visualisations are available at\nhttps://github.com/alessioGalatolo/VisZOPrO", "AI": {"tldr": "This paper introduces ZOPrO, a novel zeroth-order optimization algorithm for Preference Optimization in Large Language Models (LLMs), which enhances convergence times and reward signals in generative tasks such as summarization and machine translation.", "motivation": "To address the computational intensity of fine-tuning LLMs with first-order methods and to explore the application of Zeroth-Order optimization in complex generative tasks beyond classification.", "method": "The authors adapt Simultaneous Perturbation Stochastic Approximation (SPSA) with a targeted sampling strategy to improve convergence in Preference Optimization for LLMs.", "result": "Experiments show that ZOPrO enhances reward signals and achieves comparable convergence times to first-order methods in tasks like summarization, machine translation, and conversational assistants.", "conclusion": "ZOPrO is the first application of Zeroth-Order methods to Preference Optimization in LLMs, offering insights and techniques that pave the way for future research in this largely unexplored area.", "key_contributions": ["Introduction of ZOPrO for Preference Optimization in LLMs", "Application of SPSA with targeted sampling to accelerate convergence", "Demonstrated applicability to generative tasks beyond classification"], "limitations": "While ZOPrO achieves significant improvements, it does not outperform all state-of-the-art methods in certain scenarios.", "keywords": ["Zeroth-Order optimization", "Preference Optimization", "Large Language Models", "SPSA", "generative tasks"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2503.05200", "pdf": "https://arxiv.org/pdf/2503.05200.pdf", "abs": "https://arxiv.org/abs/2503.05200", "title": "ORANSight-2.0: Foundational LLMs for O-RAN", "authors": ["Pranshav Gajjar", "Vijay K. Shah"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NI"], "comment": null, "summary": "Despite the transformative impact of Large Language Models (LLMs) across\ncritical domains such as healthcare, customer service, and business marketing,\ntheir integration into Open Radio Access Networks (O-RAN) remains limited. This\ngap is primarily due to the absence of domain-specific foundational models,\nwith existing solutions often relying on general-purpose LLMs that fail to\naddress the unique challenges and technical intricacies of O-RAN. To bridge\nthis gap, we introduce ORANSight-2.0 (O-RAN Insights), a pioneering initiative\nto develop specialized foundational LLMs tailored for O-RAN. Built on 18 models\nspanning five open-source LLM frameworks -- Mistral, Qwen, Llama, Phi, and\nGemma -- ORANSight-2.0 fine-tunes models ranging from 1B to 70B parameters,\nsignificantly reducing reliance on proprietary, closed-source models while\nenhancing performance in O-RAN-specific tasks. At the core of ORANSight-2.0 is\nRANSTRUCT, a novel Retrieval-Augmented Generation (RAG)-based\ninstruction-tuning framework that employs two LLM agents -- a Mistral-based\nQuestion Generator and a Qwen-based Answer Generator -- to create high-quality\ninstruction-tuning datasets. The generated dataset is then used to fine-tune\nthe 18 pre-trained open-source LLMs via QLoRA. To evaluate ORANSight-2.0, we\nintroduce srsRANBench, a novel benchmark designed for code generation and\ncodebase understanding in the context of srsRAN, a widely used 5G O-RAN stack.", "AI": {"tldr": "ORANSight-2.0 introduces specialized foundational LLMs for Open Radio Access Networks (O-RAN), improving performance for domain-specific tasks.", "motivation": "The integration of Large Language Models (LLMs) in Open Radio Access Networks (O-RAN) is limited due to the lack of domain-specific models, which are necessary to address unique technical challenges.", "method": "ORANSight-2.0 develops specialized foundational LLMs by fine-tuning 18 models across five open-source frameworks, using a newly introduced RAG-based framework for instruction-tuning.", "result": "The initiative enhances performance in O-RAN-specific tasks and reduces dependence on proprietary models, supported by the creation of high-quality instruction-tuning datasets.", "conclusion": "The developed srsRANBench benchmark serves as a tool for evaluating code generation and understanding in the context of a widely used 5G O-RAN stack, validating the effectiveness of ORANSight-2.0.", "key_contributions": ["Introduction of domain-specific foundational LLMs for O-RAN", "Development of RANSTRUCT, a novel RAG-based tuning framework", "Creation of the srsRANBench benchmark for evaluating model performance"], "limitations": "", "keywords": ["Open Radio Access Networks", "Large Language Models", "O-RAN", "AI in telecommunications", "Instruction tuning"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2503.13844", "pdf": "https://arxiv.org/pdf/2503.13844.pdf", "abs": "https://arxiv.org/abs/2503.13844", "title": "Towards Detecting Persuasion on Social Media: From Model Development to Insights on Persuasion Strategies", "authors": ["Elyas Meguellati", "Stefano Civelli", "Pietro Bernardelle", "Shazia Sadiq", "Irwin King", "Gianluca Demartini"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": null, "summary": "Political advertising plays a pivotal role in shaping public opinion and\ninfluencing electoral outcomes, often through subtle persuasive techniques\nembedded in broader propaganda strategies. Detecting these persuasive elements\nis crucial for enhancing voter awareness and ensuring transparency in\ndemocratic processes. This paper presents an integrated approach that bridges\nmodel development and real-world application through two interconnected\nstudies. First, we introduce a lightweight model for persuasive text detection\nthat achieves state-of-the-art performance in Subtask 3 of SemEval 2023 Task 3\nwhile requiring significantly fewer computational resources and training data\nthan existing methods. Second, we demonstrate the model's practical utility by\ncollecting the Australian Federal Election 2022 Facebook Ads (APA22) dataset,\npartially annotating a subset for persuasion, and fine-tuning the model to\nadapt from mainstream news to social media content. We then apply the\nfine-tuned model to label the remainder of the APA22 dataset, revealing\ndistinct patterns in how political campaigns leverage persuasion through\ndifferent funding strategies, word choices, demographic targeting, and temporal\nshifts in persuasion intensity as election day approaches. Our findings not\nonly underscore the necessity of domain-specific modeling for analyzing\npersuasion on social media but also show how uncovering these strategies can\nenhance transparency, inform voters, and promote accountability in digital\ncampaigns.", "AI": {"tldr": "This paper presents a lightweight model for detecting persuasive text in political advertising, showcasing its application on a dataset from the Australian Federal Election 2022.", "motivation": "The paper aims to enhance voter awareness and ensure transparency in democratic processes by detecting persuasive elements in political advertising.", "method": "The study introduces a lightweight model for persuasive text detection, validated through state-of-the-art performance in the SemEval 2023 Task 3, and applies it on the annotated Australian Federal Election 2022 Facebook Ads dataset.", "result": "The fine-tuned model reveals distinct patterns in political campaign persuasion strategies, including funding strategies, word choices, demographic targeting, and temporal shifts leading to election day.", "conclusion": "The findings highlight the need for domain-specific modeling in social media persuasion analysis and advocate for increased transparency and accountability in digital political campaigns.", "key_contributions": ["Development of a lightweight model achieving state-of-the-art results in persuasive text detection", "Application of the model on a newly annotated dataset from the Australian Federal Election 2022", "Identification of persuasive strategies used in political advertising on social media."], "limitations": "", "keywords": ["persuasive text detection", "political advertising", "social media", "elections", "transparency"], "importance_score": 2, "read_time_minutes": 10}}
{"id": "2507.16199", "pdf": "https://arxiv.org/pdf/2507.16199.pdf", "abs": "https://arxiv.org/abs/2507.16199", "title": "WAKENLLM: Evaluating Reasoning Potential and Stability in LLMs via Fine-Grained Benchmarking", "authors": ["Zipeng Ling", "Yuehao Tang", "Shuliang Liu", "Junqi Yang", "Shenghong Fu", "Yao Wan", "Kejia Huang", "Chen Huang", "Zhichao Hou", "Xuming Hu"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) frequently output the label Unknown, yet current\nevaluations focus almost exclusively on whether such answers are honest rather\nthan why they arise. This blurs two distinct cases: (i) an input that is\ngenuinely indeterminate and (ii) a solvable problem that the model fails to\nresolve. We call this phenomenon Vague Perception. And thus we introduce a\nframework that quantifies the proportion of Unknown responses attributable to\nmodel incapacity and tests whether guided stimulation can convert them into\neither correct Known or correct Unknown with valid reasoning. By separating\nthese sources of uncertainty, our method provides a clearer picture of LLM\nreasoning limits and their potential for improvement. As we get a theoretical\naccuracy of reasoning task on different LLMs, we apply different methods to\ntest whether the model can reach the accuracy given a baseline framework. Our\nwork is meaningful in exploring the potential reasoning ability of LLMs and\nproviding a new perspective on solving the Vague Perception phenomenon.", "AI": {"tldr": "The paper introduces a framework to analyze and improve Large Language Models' (LLMs) responses designated as Unknown, distinguishing between genuine indeterminacy and solvable problems due to model incapacity, referred to as Vague Perception.", "motivation": "To clarify and quantify why LLMs output Unknown responses, separating cases of true indeterminacy from model incapacity to improve LLM reasoning capabilities.", "method": "A framework that quantifies Unknown responses, tests guided stimulation methods to convert them into correct Known or Unknown responses, and assesses theoretical accuracy across various LLMs.", "result": "The proposed method successfully distinguishes sources of uncertainty in LLM outputs and shows potential for improving reasoning accuracy through guided stimulation.", "conclusion": "By addressing the Vague Perception phenomenon, this paper provides insights into LLM reasoning limits and methods for enhancing their decision-making capabilities.", "key_contributions": ["Introduction of the Vague Perception concept in LLMs", "A framework to analyze Unknown responses systematically", "Demonstration of methods to improve reasoning in LLMs using guided stimulation"], "limitations": "", "keywords": ["Large Language Models", "Vague Perception", "Unknown responses", "Guided stimulation", "LLM reasoning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.16284", "pdf": "https://arxiv.org/pdf/2507.16284.pdf", "abs": "https://arxiv.org/abs/2507.16284", "title": "Language Detection by Means of the Minkowski Norm: Identification Through Character Bigrams and Frequency Analysis", "authors": ["Paul-Andrei Pogăcean", "Sanda-Maria Avram"], "categories": ["cs.CL"], "comment": null, "summary": "The debate surrounding language identification has gained renewed attention\nin recent years, especially with the rapid evolution of AI-powered language\nmodels. However, the non-AI-based approaches to language identification have\nbeen overshadowed. This research explores a mathematical implementation of an\nalgorithm for language determinism by leveraging monograms and bigrams\nfrequency rankings derived from established linguistic research. The datasets\nused comprise texts varying in length, historical period, and genre, including\nshort stories, fairy tales, and poems. Despite these variations, the method\nachieves over 80\\% accuracy on texts shorter than 150 characters and reaches\n100\\% accuracy for longer texts. These results demonstrate that classical\nfrequency-based approaches remain effective and scalable alternatives to\nAI-driven models for language detection.", "AI": {"tldr": "This research presents a classical frequency-based algorithm for language identification that outperforms AI-driven models in accuracy for certain text lengths.", "motivation": "To explore non-AI-based approaches to language identification that have been overshadowed by the focus on AI-powered models.", "method": "A mathematical implementation leveraging monograms and bigrams frequency rankings derived from linguistic research, applied to a variety of text datasets.", "result": "Achieves over 80% accuracy on texts shorter than 150 characters and 100% accuracy for longer texts.", "conclusion": "Classical frequency-based methods are effective and scalable alternatives for language detection compared to AI-driven models.", "key_contributions": ["Introduction of a frequency-based algorithm for language identification", "Demonstrated effectiveness across diverse text genres and lengths", "Provided a comparative analysis with AI-based models"], "limitations": "", "keywords": ["language identification", "monograms", "bigrams", "frequency analysis", "non-AI methods"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2507.16799", "pdf": "https://arxiv.org/pdf/2507.16799.pdf", "abs": "https://arxiv.org/abs/2507.16799", "title": "Test-Time-Matching: Decouple Personality, Memory, and Linguistic Style in LLM-based Role-Playing Language Agent", "authors": ["Xiaoyu Zhan", "Xinyu Fu", "Hao Sun", "Yuanqi Li", "Jie Guo", "Yanwen Guo"], "categories": ["cs.CL"], "comment": null, "summary": "The rapid advancement of large language models (LLMs) has enabled\nrole-playing language agents to demonstrate significant potential in various\napplications. However, relying solely on prompts and contextual inputs often\nproves insufficient for achieving deep immersion in specific roles,\nparticularly well-known fictional or public figures. On the other hand,\nfine-tuning-based approaches face limitations due to the challenges associated\nwith data collection and the computational resources required for training,\nthereby restricting their broader applicability. To address these issues, we\npropose Test-Time-Matching (TTM), a training-free role-playing framework\nthrough test-time scaling and context engineering. TTM uses LLM agents to\nautomatically decouple a character's features into personality, memory, and\nlinguistic style. Our framework involves a structured, three-stage generation\npipeline that utilizes these features for controlled role-playing. It achieves\nhigh-fidelity role-playing performance, also enables seamless combinations\nacross diverse linguistic styles and even variations in personality and memory.\nWe evaluate our framework through human assessment, and the results demonstrate\nthat our method achieves the outstanding performance in generating expressive\nand stylistically consistent character dialogues.", "AI": {"tldr": "The paper introduces Test-Time-Matching (TTM), a training-free framework for enhancing role-playing in large language models (LLMs) by effectively decoupling character traits.", "motivation": "The need for deeper immersion in role-play applications involving LLMs, especially for iconic figures, amid challenges in existing methods.", "method": "TTM employs a three-stage generation pipeline to decouple character features into personality, memory, and linguistic style, optimized for role-playing without fine-tuning.", "result": "TTM achieves high-fidelity role-playing performance and can seamlessly combine diverse linguistic styles and variations in character traits.", "conclusion": "The method demonstrates outstanding results in generating expressive and stylistically consistent dialogues, fulfilling the immersive role-play requirement effectively.", "key_contributions": ["Introduction of a training-free role-playing framework (TTM) for LLMs.", "Decoupling of character features for improved dialogue control.", "Evaluation shows high performance in expressive and stylistically consistent dialogue generation."], "limitations": "", "keywords": ["large language models", "role-playing", "context engineering", "Test-Time-Matching", "dialogue generation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.16802", "pdf": "https://arxiv.org/pdf/2507.16802.pdf", "abs": "https://arxiv.org/abs/2507.16802", "title": "Agentar-Fin-R1: Enhancing Financial Intelligence through Domain Expertise, Training Efficiency, and Advanced Reasoning", "authors": ["Yanjun Zheng", "Xiyang Du", "Longfei Liao", "Xiaoke Zhao", "Zhaowen Zhou", "Bo Zhang", "Jiawei Liu", "Xiang Qi", "Zhe Li", "Zhiqiang Zhang", "Wei Wang", "Peng Zhang"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) exhibit considerable promise in financial\napplications; however, prevailing models frequently demonstrate limitations\nwhen confronted with scenarios that necessitate sophisticated reasoning\ncapabilities, stringent trustworthiness criteria, and efficient adaptation to\ndomain-specific requirements. We introduce the Agentar-Fin-R1 series of\nfinancial large language models (8B and 32B parameters), specifically\nengineered based on the Qwen3 foundation model to enhance reasoning\ncapabilities, reliability, and domain specialization for financial\napplications. Our optimization approach integrates a high-quality, systematic\nfinancial task label system with a comprehensive multi-layered trustworthiness\nassurance framework. This framework encompasses high-quality trustworthy\nknowledge engineering, multi-agent trustworthy data synthesis, and rigorous\ndata validation governance. Through label-guided automated difficulty-aware\noptimization, tow-stage training pipeline, and dynamic attribution systems, we\nachieve substantial improvements in training efficiency. Our models undergo\ncomprehensive evaluation on mainstream financial benchmarks including Fineva,\nFinEval, and FinanceIQ, as well as general reasoning datasets such as MATH-500\nand GPQA-diamond. To thoroughly assess real-world deployment capabilities, we\ninnovatively propose the Finova evaluation benchmark, which focuses on\nagent-level financial reasoning and compliance verification. Experimental\nresults demonstrate that Agentar-Fin-R1 not only achieves state-of-the-art\nperformance on financial tasks but also exhibits exceptional general reasoning\ncapabilities, validating its effectiveness as a trustworthy solution for\nhigh-stakes financial applications. The Finova bench is available at\nhttps://github.com/antgroup/Finova.", "AI": {"tldr": "Introduction of Agentar-Fin-R1, a series of financial large language models designed to enhance reasoning and adaptation in financial applications.", "motivation": "There is a need for improved reasoning capabilities and trustworthiness in financial large language models, as current models struggle with complex financial scenarios.", "method": "Development of the Agentar-Fin-R1 models based on Qwen3 with a structured optimization approach including a financial task label system and a trustworthiness assurance framework.", "result": "The models show improved performance on financial benchmarks (Fineva, FinEval, FinanceIQ) and general reasoning datasets (MATH-500, GPQA-diamond), demonstrating state-of-the-art capabilities in financial domains and general reasoning.", "conclusion": "Agentar-Fin-R1 provides an effective and trustworthy solution for high-stakes financial applications, with new evaluation benchmarks established for real-world deployment.", "key_contributions": ["Introduction of a novel finova evaluation benchmark for financial reasoning", "Enhancements in reasoning capabilities and reliability for financial applications", "High-quality data governance and synthesis for training efficiency"], "limitations": "", "keywords": ["Large Language Models", "Financial Applications", "Reasoning Capabilities"], "importance_score": 4, "read_time_minutes": 5}}
