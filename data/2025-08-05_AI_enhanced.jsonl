{"id": "2508.00843", "pdf": "https://arxiv.org/pdf/2508.00843.pdf", "abs": "https://arxiv.org/abs/2508.00843", "title": "Generative AI for CAD Automation: Leveraging Large Language Models for 3D Modelling", "authors": ["Sumit Kumar", "Sarthak Kapoor", "Harsh Vardhan", "Yao Zhao"], "categories": ["cs.HC", "cs.AI", "cs.SE"], "comment": null, "summary": "Large Language Models (LLMs) are revolutionizing industries by enhancing\nefficiency, scalability, and innovation. This paper investigates the potential\nof LLMs in automating Computer-Aided Design (CAD) workflows, by integrating\nFreeCAD with LLM as CAD design tool. Traditional CAD processes are often\ncomplex and require specialized sketching skills, posing challenges for rapid\nprototyping and generative design. We propose a framework where LLMs generate\ninitial CAD scripts from natural language descriptions, which are then executed\nand refined iteratively based on error feedback. Through a series of\nexperiments with increasing complexity, we assess the effectiveness of this\napproach. Our findings reveal that LLMs perform well for simple to moderately\ncomplex designs but struggle with highly constrained models, necessitating\nmultiple refinements. The study highlights the need for improved memory\nretrieval, adaptive prompt engineering, and hybrid AI techniques to enhance\nscript robustness. Future directions include integrating cloud-based execution\nand exploring advanced LLM capabilities to further streamline CAD automation.\nThis work underscores the transformative potential of LLMs in design workflows\nwhile identifying critical areas for future development.", "AI": {"tldr": "This paper explores the use of Large Language Models (LLMs) in automating Computer-Aided Design (CAD) workflows by integrating them with FreeCAD, aiming to simplify and enhance CAD processes.", "motivation": "Traditional CAD processes are complex and require specialized skills, hindering rapid prototyping and generative design; LLMs present a solution to automate and simplify these workflows.", "method": "The proposed framework involves generating CAD scripts from natural language descriptions using LLMs, which are then executed and refined iteratively based on feedback.", "result": "The experiments conducted show that LLMs are effective for simple to moderately complex designs, but face challenges with highly constrained models requiring multiple refinements.", "conclusion": "The study highlights LLMs' potential in enhancing CAD workflows and identifies the need for better memory retrieval and adaptive techniques for improved robustness.", "key_contributions": ["Integration of FreeCAD with LLMs for automated CAD design", "Development of a framework for generating scripts from natural language", "Identification of limitations and areas for improvement in LLM-assisted CAD processes"], "limitations": "LLMs struggle with highly constrained models, necessitating multiple refinements.", "keywords": ["Large Language Models", "Computer-Aided Design", "FreeCAD", "workflow automation", "script generation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.00846", "pdf": "https://arxiv.org/pdf/2508.00846.pdf", "abs": "https://arxiv.org/abs/2508.00846", "title": "Cognitive Exoskeleton: Augmenting Human Cognition with an AI-Mediated Intelligent Visual Feedback", "authors": ["Songlin Xu", "Xinyu Zhang"], "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": null, "summary": "In this paper, we introduce an AI-mediated framework that can provide\nintelligent feedback to augment human cognition. Specifically, we leverage deep\nreinforcement learning (DRL) to provide adaptive time pressure feedback to\nimprove user performance in a math arithmetic task. Time pressure feedback\ncould either improve or deteriorate user performance by regulating user\nattention and anxiety. Adaptive time pressure feedback controlled by a DRL\npolicy according to users' real-time performance could potentially solve this\ntrade-off problem. However, the DRL training and hyperparameter tuning may\nrequire large amounts of data and iterative user studies. Therefore, we propose\na dual-DRL framework that trains a regulation DRL agent to regulate user\nperformance by interacting with another simulation DRL agent that mimics user\ncognition behaviors from an existing dataset. Our user study demonstrates the\nfeasibility and effectiveness of the dual-DRL framework in augmenting user\nperformance, in comparison to the baseline group.", "AI": {"tldr": "The paper presents a dual-DRL framework that uses AI-mediated feedback to enhance user performance in cognitive tasks by adjusting time pressure based on real-time performance.", "motivation": "The study aims to improve user performance in cognitively demanding tasks by providing adaptive feedback that balances time pressure with user attention and anxiety.", "method": "The authors propose a dual-DRL framework where one agent regulates user performance by interacting with another agent that simulates user cognition behaviors based on data from existing datasets.", "result": "User studies show that the dual-DRL framework successfully enhances performance compared to a baseline, proving the effectiveness of the adaptive time pressure feedback.", "conclusion": "The findings indicate that a well-tuned AI framework can significantly support cognitive tasks by intelligently managing time pressure and improving attention regulation.", "key_contributions": ["Development of a dual-DRL framework for cognitive task support", "Adaptive time pressure feedback mechanism", "Successful demonstration of effectiveness in user studies"], "limitations": "DRL training and hyperparameter tuning may require substantial data and several iterative user studies.", "keywords": ["Dual-DRL", "Adaptive Feedback", "Cognitive Performance", "Deep Reinforcement Learning", "Time Pressure"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2508.00847", "pdf": "https://arxiv.org/pdf/2508.00847.pdf", "abs": "https://arxiv.org/abs/2508.00847", "title": "GPT Chatbots for Alleviating Anxiety and Depression: A Pilot Randomized Controlled Trial with Afghan Women", "authors": ["Sofia Sahab", "Jawad Haqbeen", "Diksha Sapkota", "Takayuki Ito"], "categories": ["cs.HC", "cs.CY"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "In this study, we investigated the effects of GPT-4, with and without\nspecific conversational instructions, on the mental health of Afghan women.\nThese women face multifaceted challenges, including Taliban-imposed\nrestrictions, societal inequalities, and domestic violence, adversely affecting\ntheir well-being. We conducted a randomized controlled trial with 60\nparticipants, dividing them into three groups: GPT-4, a supportive listener\n(GPT-4 with empathetic engagement instructions), and a waiting list. The\nHospital Anxiety and Depression Scale (HADS) was used to measure anxiety and\ndepression before and after the intervention. Linguistic analysis of chat data\nexamined personal pronouns, tones, emotions, and Language Style Matching (LSM).\nThe supportive listener group showed a significant reduction in HADS scores\ncompared to the other groups. Linguistic analysis revealed a more positive tone\nand higher LSM in the supportive listener group, with a significant negative\ncorrelation between LSM and changes in HADS scores, indicating greater\nlinguistic alignment was linked to reductions in anxiety and depression.\nPerceived empathy ratings were also significantly higher in the supportive\nlistener group. These findings highlight the potential of AI-driven\ninterventions, like GPT-4, in providing accessible mental health support.\nHowever, such interventions should complement traditional psychotherapy,\nensuring a collaborative approach to optimize therapeutic outcomes.", "AI": {"tldr": "The study explores the mental health effects of GPT-4 on Afghan women, showing significant benefits of empathetic AI support in reducing anxiety and depression.", "motivation": "Afghan women face significant mental health challenges due to societal factors, and AI could play a role in providing support.", "method": "A randomized controlled trial with 60 participants was conducted, dividing them into three groups: GPT-4, a supportive listener, and a waiting list. The Hospital Anxiety and Depression Scale (HADS) measured changes in anxiety and depression before and after intervention, alongside linguistic analysis of chat data.", "result": "The supportive listener group demonstrated a significant reduction in HADS scores, a more positive tone, and higher language style matching compared to other groups.", "conclusion": "AI-driven interventions like GPT-4 can provide accessible mental health support but should be used to complement traditional psychotherapy.", "key_contributions": ["Empirical evidence of GPT-4's effectiveness as a supportive listener for mental health", "Linguistic alignment linked to improvements in mental health measures", "Highlighting the role of AI in mental health interventions for marginalized groups"], "limitations": "", "keywords": ["GPT-4", "Mental Health", "Afghan Women", "Supportive Listener", "Linguistic Analysis"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.00848", "pdf": "https://arxiv.org/pdf/2508.00848.pdf", "abs": "https://arxiv.org/abs/2508.00848", "title": "RestAware: Non-Invasive Sleep Monitoring Using FMCW Radar and AI-Generated Summaries", "authors": ["Agniva Banerjee", "Bhanu Partap Paregi", "Haroon R. Lone"], "categories": ["cs.HC", "cs.CY", "eess.SP"], "comment": null, "summary": "Monitoring sleep posture and behavior is critical for diagnosing sleep\ndisorders and improving overall sleep quality. However, traditional approaches,\nsuch as wearable devices, cameras, and pressure sensors, often compromise user\ncomfort, fail under obstructions like blankets, and raise privacy concerns. To\novercome these limitations, we present RestAware, a non-invasive, contactless\nsleep monitoring system based on a 24GHz frequency-modulated continuous wave\n(FMCW) radar. Our system is evaluated on 25 participants across eight common\nsleep postures, achieving 92% classification accuracy and an F1-score of 0.91\nusing a K-Nearest Neighbors (KNN) classifier. In addition, we integrate\ninstruction-tuned large language models (Mistral, Llama, and Falcon) to\ngenerate personalized, human-readable sleep summaries from radar-derived\nposture data. This low-cost ($ 35), privacy-preserving solution offers a\npractical alternative for real-time deployment in smart homes and clinical\nenvironments.", "AI": {"tldr": "RestAware is a non-invasive sleep monitoring system using FMCW radar technology, achieving high classification accuracy and generating personalized sleep summaries with large language models.", "motivation": "The need for a better sleep monitoring solution that overcomes the limitations of traditional devices, such as user discomfort, obstructions, and privacy concerns.", "method": "RestAware utilizes a 24GHz frequency-modulated continuous wave (FMCW) radar to monitor sleep postures, evaluated on 25 participants with a K-Nearest Neighbors (KNN) classifier for accuracy.", "result": "Achieved 92% classification accuracy and an F1-score of 0.91 in identifying eight common sleep postures.", "conclusion": "RestAware provides a low-cost, privacy-preserving alternative for sleep monitoring suitable for smart homes and clinical use.", "key_contributions": ["Introduction of a non-invasive sleep monitoring system using FMCW radar", "High accuracy in posture classification", "Integration of instruction-tuned LLMs for generating personalized sleep summaries"], "limitations": "", "keywords": ["sleep monitoring", "FMCW radar", "privacy", "large language models", "smart homes"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2508.00864", "pdf": "https://arxiv.org/pdf/2508.00864.pdf", "abs": "https://arxiv.org/abs/2508.00864", "title": "Rethinking Graph-Based Document Classification: Learning Data-Driven Structures Beyond Heuristic Approaches", "authors": ["Margarita Bugueño", "Gerard de Melo"], "categories": ["cs.CL"], "comment": "7 pages, 3 figures, 3 tables. Appendix starts on page 10", "summary": "In document classification, graph-based models effectively capture document\nstructure, overcoming sequence length limitations and enhancing contextual\nunderstanding. However, most existing graph document representations rely on\nheuristics, domain-specific rules, or expert knowledge. Unlike previous\napproaches, we propose a method to learn data-driven graph structures,\neliminating the need for manual design and reducing domain dependence. Our\napproach constructs homogeneous weighted graphs with sentences as nodes, while\nedges are learned via a self-attention model that identifies dependencies\nbetween sentence pairs. A statistical filtering strategy aims to retain only\nstrongly correlated sentences, improving graph quality while reducing the graph\nsize. Experiments on three document classification datasets demonstrate that\nlearned graphs consistently outperform heuristic-based graphs, achieving higher\naccuracy and $F_1$ score. Furthermore, our study demonstrates the effectiveness\nof the statistical filtering in improving classification robustness. These\nresults highlight the potential of automatic graph generation over traditional\nheuristic approaches and open new directions for broader applications in NLP.", "AI": {"tldr": "This paper presents a method for learning data-driven graph structures for document classification, improving over traditional heuristic methods.", "motivation": "Existing graph document representations in classification rely heavily on heuristics and expert knowledge, which can limit their applicability and effectiveness.", "method": "The proposed method constructs homogeneous weighted graphs with sentences as nodes, using a self-attention model to learn edges based on dependencies between sentence pairs. A statistical filtering strategy is employed to retain only strongly correlated sentences, enhancing graph quality.", "result": "Experiments across three document classification datasets show that the learned graph structures outperform heuristic-based graphs, achieving higher accuracy and F1 scores. The statistical filtering also improves classification robustness.", "conclusion": "The findings suggest that automatic graph generation can surpass traditional heuristic methods, paving the way for new applications in NLP.", "key_contributions": ["Development of a data-driven approach for graph construction in document classification", "Demonstration of improved accuracy and robustness over heuristic methods", "Introduction of statistical filtering to enhance graph quality"], "limitations": "", "keywords": ["graph-based models", "document classification", "self-attention", "statistical filtering", "NLP"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.00850", "pdf": "https://arxiv.org/pdf/2508.00850.pdf", "abs": "https://arxiv.org/abs/2508.00850", "title": "Gearshift Fellowship: A Next-Generation Neurocomputational Game Platform to Model and Train Human-AI Adaptability", "authors": ["Nadja R. Ging-Jehli", "Russell K. Childers", "Joshua Lu", "Robert Gemma", "Rachel Zhu"], "categories": ["cs.HC", "cs.AI", "cs.CE", "cs.CY"], "comment": null, "summary": "How do we learn when to persist, when to let go, and when to shift gears?\nGearshift Fellowship (GF) is the prototype of a new Supertask paradigm designed\nto model how humans and artificial agents adapt to shifting environment\ndemands. Grounded in cognitive neuroscience, computational psychiatry,\neconomics, and artificial intelligence, Supertasks combine computational\nneurocognitive modeling with serious gaming. This creates a dynamic,\nmulti-mission environment engineered to assess mechanisms of adaptive behavior\nacross cognitive and social contexts. Computational parameters explain behavior\nand probe mechanisms by controlling the game environment. Unlike traditional\ntasks, GF enables neurocognitive modeling of individual differences across\nperceptual decisions, learning, and meta-cognitive levels. This positions GF as\na flexible testbed for understanding how cognitive-affective control processes,\nlearning styles, strategy use, and motivational shifts adapt across contexts\nand over time. It serves as an experimental platform for scientists, a\nphenotype-to-mechanism intervention for clinicians, and a training tool for\nplayers aiming to strengthen self-regulated learning, mood, and stress\nresilience. Online study (n = 60, ongoing) results show that GF recovers\neffects from traditional neuropsychological tasks (construct validity),\nuncovers novel patterns in how learning differs across contexts and how\nclinical features map onto distinct adaptations. These findings pave the way\nfor developing in-game interventions that foster self-efficacy and agency to\ncope with real-world stress and uncertainty. GF builds a new adaptive ecosystem\ndesigned to accelerate science, transform clinical care, and foster individual\ngrowth. It offers a mirror and training ground where humans and machines\nco-develop together deeper flexibility and awareness.", "AI": {"tldr": "The Gearshift Fellowship (GF) is a prototype for a new paradigm that models human and artificial agent adaptability in shifting environments through neurocognitive modeling and serious gaming.", "motivation": "To understand mechanisms of adaptive behavior across cognitive and social contexts, and to foster self-regulated learning and resilience.", "method": "GF combines computational neurocognitive modeling with serious gaming to create a dynamic environment assessing adaptive behavior mechanisms and learning styles.", "result": "Online study shows GF captures effects from traditional tasks, revealing new patterns in learning differences and adaptations linked to clinical features.", "conclusion": "GF serves as a tool for scientists, clinicians, and players to enhance learning and cope with stress, and promotes a co-development ecosystem for humans and machines.", "key_contributions": ["Development of the Gearshift Fellowship as a prototype for adaptive behavior modeling", "Integration of serious gaming with neurocognitive assessments", "Provision of a flexible platform for research and clinical interventions"], "limitations": "", "keywords": ["Gearshift Fellowship", "adaptive behavior", "neurocognitive modeling", "serious gaming", "self-regulated learning"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.00889", "pdf": "https://arxiv.org/pdf/2508.00889.pdf", "abs": "https://arxiv.org/abs/2508.00889", "title": "FECT: Factuality Evaluation of Interpretive AI-Generated Claims in Contact Center Conversation Transcripts", "authors": ["Hagyeong Shin", "Binoy Robin Dalal", "Iwona Bialynicka-Birula", "Navjot Matharu", "Ryan Muir", "Xingwei Yang", "Samuel W. K. Wong"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted for an oral presentation at Agentic & GenAI Evaluation KDD\n  2025: KDD workshop on Evaluation and Trustworthiness of Agentic and\n  Generative AI Models", "summary": "Large language models (LLMs) are known to hallucinate, producing natural\nlanguage outputs that are not grounded in the input, reference materials, or\nreal-world knowledge. In enterprise applications where AI features support\nbusiness decisions, such hallucinations can be particularly detrimental. LLMs\nthat analyze and summarize contact center conversations introduce a unique set\nof challenges for factuality evaluation, because ground-truth labels often do\nnot exist for analytical interpretations about sentiments captured in the\nconversation and root causes of the business problems. To remedy this, we first\nintroduce a \\textbf{3D} -- \\textbf{Decompose, Decouple, Detach} -- paradigm in\nthe human annotation guideline and the LLM-judges' prompt to ground the\nfactuality labels in linguistically-informed evaluation criteria. We then\nintroduce \\textbf{FECT}, a novel benchmark dataset for \\textbf{F}actuality\n\\textbf{E}valuation of Interpretive AI-Generated \\textbf{C}laims in Contact\nCenter Conversation \\textbf{T}ranscripts, labeled under our 3D paradigm.\nLastly, we report our findings from aligning LLM-judges on the 3D paradigm.\nOverall, our findings contribute a new approach for automatically evaluating\nthe factuality of outputs generated by an AI system for analyzing contact\ncenter conversations.", "AI": {"tldr": "This paper introduces a framework for evaluating the factuality of claims made by LLMs in contact center conversation analyses, detailing a new paradigm and dataset for this purpose.", "motivation": "LLMs often produce hallucinations, making factual evaluation critical in enterprise contexts, especially for business decision-making based on contact center conversations.", "method": "The paper presents a 3D paradigm (Decompose, Decouple, Detach) for human annotation and LLM evaluation, alongside the introduction of a benchmark dataset (FECT) for factuality evaluation.", "result": "The study demonstrates alignment among LLM-judges using the 3D framework, providing insights for more accurate factual evaluations of AI-generated outputs.", "conclusion": "The proposed approach advances the methodologies for evaluating the factuality of AI interpretations in contact center dialogues, addressing gaps in current practices.", "key_contributions": ["Introduction of the 3D evaluation paradigm for LLMs.", "Development of the FECT dataset for factuality evaluation in contact center conversations.", "Findings on aligning LLM judges to improve factuality assessments."], "limitations": "", "keywords": ["Large Language Models", "Factuality Evaluation", "Contact Center Conversations", "Human Annotation", "Benchmark Dataset"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.00852", "pdf": "https://arxiv.org/pdf/2508.00852.pdf", "abs": "https://arxiv.org/abs/2508.00852", "title": "Visuo-Acoustic Hand Pose and Contact Estimation", "authors": ["Yuemin Ma", "Uksang Yoo", "Yunchao Yao", "Shahram Najam Syed", "Luca Bondi", "Jonathan Francis", "Jean Oh", "Jeffrey Ichnowski"], "categories": ["cs.HC", "cs.CV", "cs.LG", "cs.RO"], "comment": null, "summary": "Accurately estimating hand pose and hand-object contact events is essential\nfor robot data-collection, immersive virtual environments, and biomechanical\nanalysis, yet remains challenging due to visual occlusion, subtle contact cues,\nlimitations in vision-only sensing, and the lack of accessible and flexible\ntactile sensing. We therefore introduce VibeMesh, a novel wearable system that\nfuses vision with active acoustic sensing for dense, per-vertex hand contact\nand pose estimation. VibeMesh integrates a bone-conduction speaker and sparse\npiezoelectric microphones, distributed on a human hand, emitting structured\nacoustic signals and capturing their propagation to infer changes induced by\ncontact. To interpret these cross-modal signals, we propose a graph-based\nattention network that processes synchronized audio spectra and RGB-D-derived\nhand meshes to predict contact with high spatial resolution. We contribute: (i)\na lightweight, non-intrusive visuo-acoustic sensing platform; (ii) a\ncross-modal graph network for joint pose and contact inference; (iii) a dataset\nof synchronized RGB-D, acoustic, and ground-truth contact annotations across\ndiverse manipulation scenarios; and (iv) empirical results showing that\nVibeMesh outperforms vision-only baselines in accuracy and robustness,\nparticularly in occluded or static-contact settings.", "AI": {"tldr": "VibeMesh is a wearable system that combines vision and active acoustic sensing for accurate hand pose and contact estimation in various settings.", "motivation": "Accurate hand pose and hand-object contact estimation is important for applications in robotics, virtual environments, and biomechanics but is hindered by visual occlusion and limitations of current sensing methods.", "method": "VibeMesh integrates a bone-conduction speaker and piezoelectric microphones on the hand to emit acoustic signals and captures their propagation to estimate hand pose and contact using a graph-based attention network that processes audio and RGB-D data.", "result": "VibeMesh significantly improves hand pose and contact estimation accuracy and robustness compared to vision-only methods, especially in challenging scenarios like occlusion or static contacts.", "conclusion": "The developed system represents a breakthrough in non-intrusive sensing, enhancing the ability to interpret hand interactions in complex environments.", "key_contributions": ["A lightweight, non-intrusive visuo-acoustic sensing platform.", "A cross-modal graph network for joint pose and contact inference.", "A rich dataset with synchronized audio, RGB-D, and contact annotations."], "limitations": "", "keywords": ["hand pose estimation", "contact estimation", "acoustic sensing", "graph-based network", "wearable technology"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.00924", "pdf": "https://arxiv.org/pdf/2508.00924.pdf", "abs": "https://arxiv.org/abs/2508.00924", "title": "XAutoLM: Efficient Fine-Tuning of Language Models via Meta-Learning and AutoML", "authors": ["Ernesto L. Estevanell-Valladares", "Suilan Estevez-Velarde", "Yoan Gutiérrez", "Andrés Montoyo", "Ruslan Mitkov"], "categories": ["cs.CL", "68T05, 68T50", "I.2.6; I.2.7; I.2.8"], "comment": "17 pages, 10 figures, 7 tables. Preprint. Under review at EMNLP 2025.\n  This is not the final version", "summary": "Experts in machine learning leverage domain knowledge to navigate decisions\nin model selection, hyperparameter optimisation, and resource allocation. This\nis particularly critical for fine-tuning language models (LMs), where repeated\ntrials incur substantial computational overhead and environmental impact.\nHowever, no existing automated framework simultaneously tackles the entire\nmodel selection and HPO task for resource-efficient LM fine-tuning. We\nintroduce XAutoLM, a meta-learning-augmented AutoML framework that reuses past\nexperiences to optimise discriminative and generative LM fine-tuning pipelines\nefficiently. XAutoLM learns from stored successes and failures by extracting\ntask- and system-level meta-features to bias its sampling toward fruitful\nconfigurations and away from costly dead ends. On four text classification and\ntwo question-answering benchmarks, XAutoLM surpasses zero-shot optimiser's peak\nF1 on five of six tasks, cuts mean evaluation time by up to 4.5x, reduces error\nratios by up to sevenfold, and uncovers up to 50% more pipelines above the\nzero-shot Pareto front. In contrast, simpler memory-based baselines suffer\nnegative transfer. We release XAutoLM and our experience store to catalyse\nresource-efficient, Green AI fine-tuning in the NLP community.", "AI": {"tldr": "XAutoLM is an AutoML framework that enhances language model fine-tuning by leveraging past experiences to optimize model selection and hyperparameter tuning, leading to significant reductions in evaluation time and errors.", "motivation": "Automated frameworks for model selection and hyperparameter optimization are lacking for resource-efficient fine-tuning in language models, which often incurs high computational and environmental costs.", "method": "XAutoLM utilizes meta-learning to reuse past experiences by extracting meta-features, enabling more efficient sampling of configurations for discriminative and generative model fine-tuning.", "result": "XAutoLM outperforms zero-shot optimizers on five out of six tasks while reducing mean evaluation time by up to 4.5x and error ratios by nearly sevenfold, discovering 50% more effective pipelines.", "conclusion": "The introduction of XAutoLM promotes resource-efficient fine-tuning in the NLP community and supports Green AI initiatives by providing an experience store to aid in optimization.", "key_contributions": ["Introduction of XAutoLM for efficient LM fine-tuning.", "Utilization of meta-learning to guide model selection and HPO.", "Demonstrated significant improvements in evaluation time and error rates across multiple benchmarks."], "limitations": "", "keywords": ["AutoML", "Language Models", "Meta-Learning", "Resource Efficiency", "Green AI"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2508.00856", "pdf": "https://arxiv.org/pdf/2508.00856.pdf", "abs": "https://arxiv.org/abs/2508.00856", "title": "EthicAlly: a Prototype for AI-Powered Research Ethics Support for the Social Sciences and Humanities", "authors": ["Steph Grohmann"], "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": null, "summary": "In biomedical science, review by a Research Ethics Committee (REC) is an\nindispensable way of protecting human subjects from harm. However, in social\nscience and the humanities, mandatory ethics compliance has long been met with\nscepticism as biomedical models of ethics can map poorly onto methodologies\ninvolving complex socio-political and cultural considerations. As a result,\ntailored ethics training and support as well as access to RECs with the\nnecessary expertise is lacking in some areas, including parts of Europe and\nlow- and middle-income countries. This paper suggests that Generative AI can\nmeaningfully contribute to closing these gaps, illustrating this claim by\npresenting EthicAlly, a proof-of-concept prototype for an AI-powered ethics\nsupport system for social science and humanities researchers. Drawing on\nconstitutional AI technology and a collaborative prompt development\nmethodology, EthicAlly provides structured ethics assessment that incorporates\nboth universal ethics principles and contextual and interpretive considerations\nrelevant to most social science research. In supporting researchers in ethical\nresearch design and preparation for REC submission, this kind of system can\nalso contribute to easing the burden on institutional RECs, without attempting\nto automate or replace human ethical oversight.", "AI": {"tldr": "This paper presents EthicAlly, an AI-powered ethics support system aimed at aiding social science and humanities researchers in ethical compliance and research design.", "motivation": "The paper addresses the gap in tailored ethics training and support in social sciences and humanities caused by the inadequacy of biomedical ethics models.", "method": "The methodology involves developing a prototype, EthicAlly, utilizing constitutional AI technology and collaborative prompt development to provide structured ethics assessments.", "result": "EthicAlly effectively incorporates universal ethics principles alongside contextual factors, offering support for ethical research design and aiding REC submissions, thereby alleviating pressures on RECs.", "conclusion": "The implementation of EthicAlly can enhance the ethical oversight in social sciences and humanities without automating human ethical review processes.", "key_contributions": ["Introduction of EthicAlly as an AI-powered ethics assessment tool.", "Integration of universal and contextual ethical considerations into research support.", "Demonstrates the potential of AI to alleviate burdens on Research Ethics Committees."], "limitations": "", "keywords": ["Generative AI", "Ethics support system", "Social science", "Humanities", "Research Ethics Committee"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.01005", "pdf": "https://arxiv.org/pdf/2508.01005.pdf", "abs": "https://arxiv.org/abs/2508.01005", "title": "MAO-ARAG: Multi-Agent Orchestration for Adaptive Retrieval-Augmented Generation", "authors": ["Yiqun Chen", "Erhan Zhang", "Lingyong Yan", "Shuaiqiang Wang", "Jizhou Huang", "Dawei Yin", "Jiaxin Mao"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "In question-answering (QA) systems, Retrieval-Augmented Generation (RAG) has\nbecome pivotal in enhancing response accuracy and reducing hallucination\nissues. The architecture of RAG systems varies significantly, encompassing\nsingle-round RAG, iterative RAG, and reasoning RAG, each tailored to address\ndifferent types of queries. Due to the varying complexity of real-world\nqueries, a fixed RAG pipeline often struggles to balance performance and cost\nefficiency across different queries. To address this challenge, we propose an\nadaptive RAG framework called MAO-ARAG, which leverages multi-agent\norchestration. Our adaptive RAG is conceived as a multi-turn framework.\nSpecifically, we define multiple executor agents, representing typical RAG\nmodules such as query reformulation agents, document selection agent, and\ngeneration agents. A planner agent intelligently selects and integrates the\nappropriate agents from these executors into a suitable workflow tailored for\neach query, striving for high-quality answers while maintaining reasonable\ncosts. During each turn, the planner agent is trained using reinforcement\nlearning, guided by an outcome-based reward (F1 score) and a cost-based\npenalty, continuously improving answer quality while keeping costs within a\nreasonable range. Experiments conducted on multiple QA datasets demonstrate\nthat our approach, which dynamically plans workflows for each query, not only\nachieves high answer quality but also maintains both cost and latency within\nacceptable limits.The code of MAO-ARAG is on\nhttps://github.com/chenyiqun/Agentic-RAG.", "AI": {"tldr": "The paper introduces MAO-ARAG, an adaptive Retrieval-Augmented Generation framework that utilizes multi-agent orchestration to improve response accuracy and efficiency in question-answering systems.", "motivation": "Current RAG systems struggle to balance performance and cost efficiency due to varying complexities of real-world queries.", "method": "MAO-ARAG employs a multi-turn framework with various executor agents (like query reformulation and document selection) and a planner agent trained via reinforcement learning to optimize the workflow for each query.", "result": "The approach demonstrates improved answer quality while maintaining cost and latency within acceptable limits across multiple QA datasets.", "conclusion": "MAO-ARAG effectively enhances RAG systems by dynamically planning suitable workflows tailored to specific queries, improving both performance and cost management.", "key_contributions": ["Development of the MAO-ARAG framework for adaptive RAG systems", "Integration of multi-agent orchestration in QA systems", "Usage of reinforcement learning to optimize agent workflows"], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "question-answering", "multi-agent orchestration"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.00929", "pdf": "https://arxiv.org/pdf/2508.00929.pdf", "abs": "https://arxiv.org/abs/2508.00929", "title": "Accessibility and Social Inclusivity: A Literature Review of Music Technology for Blind and Low Vision People", "authors": ["Shumeng Zhang", "Raul Masu", "Mela Bettega", "Mingming Fan"], "categories": ["cs.HC", "cs.CY", "cs.SD", "eess.AS"], "comment": "Accepted by ASSETS'25 - The 27th International ACM SIGACCESS\n  Conference on Computers and Accessibility", "summary": "This paper presents a systematic literature review of music technology\ntailored for blind and low vision (BLV) individuals. Music activities can be\nparticularly beneficial for BLV people. However, a systematic approach to\norganizing knowledge on designing accessible technology for BLV people has yet\nto be attempted. We categorize the existing studies based on the type of\ntechnology and the extent of BLV people's involvement in the research. We\nidentify six main categories of BLV people-oriented music technology and\nhighlight four key trends in design goals. Based on these categories, we\npropose four general insights focusing on (1) spatial awareness, (2) access to\ninformation, (3) (non-verbal) communication, and (4) memory. The identified\ntrends suggest that more empirical studies involving BLV people in real-world\nscenarios are needed to ensure that technological advancements can enhance\nmusical experiences and social inclusion. This research proposes collaborative\nmusic technology and inclusive real-world testing with the target group as two\nkey areas missing in current research. They serve as a foundational step in\nshifting the focus from ``accessible technology'' to ``inclusive technology''\nfor BLV individuals within the broader field of accessibility research.", "AI": {"tldr": "Systematic literature review of music technology for blind and low vision (BLV) individuals categorizes existing studies and proposes insights for inclusive design.", "motivation": "To organize knowledge on designing accessible technology for blind and low vision people, which has been lacking in systematic approaches.", "method": "Literature review categorizing existing studies based on technology type and BLV people's research involvement.", "result": "Identified six categories of BLV-oriented music technology and four key design trends. Highlighted the need for more empirical studies in real-world scenarios to enhance musical experiences.", "conclusion": "Emphasized a shift from 'accessible technology' to 'inclusive technology' for BLV individuals and proposed areas for collaborative testing and research.", "key_contributions": ["Systematic categorization of music technology for BLV individuals", "Identification of key trends in the design of music technology", "Proposal for inclusive real-world testing with the target group"], "limitations": "", "keywords": ["Blind and Low Vision", "Music Technology", "Inclusive Design", "Accessibility Research", "Empirical Studies"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2508.01006", "pdf": "https://arxiv.org/pdf/2508.01006.pdf", "abs": "https://arxiv.org/abs/2508.01006", "title": "UrBLiMP: A Benchmark for Evaluating the Linguistic Competence of Large Language Models in Urdu", "authors": ["Farah Adeeba", "Brian Dillon", "Hassan Sajjad", "Rajesh Bhatt"], "categories": ["cs.CL"], "comment": null, "summary": "Multilingual Large Language Models (LLMs) have shown remarkable performance\nacross various languages; however, they often include significantly less data\nfor low-resource languages such as Urdu compared to high-resource languages\nlike English. To assess the linguistic knowledge of LLMs in Urdu, we present\nthe Urdu Benchmark of Linguistic Minimal Pairs (UrBLiMP) i.e. pairs of\nminimally different sentences that contrast in grammatical acceptability.\nUrBLiMP comprises 5,696 minimal pairs targeting ten core syntactic phenomena,\ncarefully curated using the Urdu Treebank and diverse Urdu text corpora. A\nhuman evaluation of UrBLiMP annotations yielded a 96.10% inter-annotator\nagreement, confirming the reliability of the dataset. We evaluate twenty\nmultilingual LLMs on UrBLiMP, revealing significant variation in performance\nacross linguistic phenomena. While LLaMA-3-70B achieves the highest average\naccuracy (94.73%), its performance is statistically comparable to other top\nmodels such as Gemma-3-27B-PT. These findings highlight both the potential and\nthe limitations of current multilingual LLMs in capturing fine-grained\nsyntactic knowledge in low-resource languages.", "AI": {"tldr": "The paper introduces the Urdu Benchmark of Linguistic Minimal Pairs (UrBLiMP) to evaluate the performance of multilingual LLMs in understanding Urdu grammar, highlighting their strengths and weaknesses in low-resource languages.", "motivation": "To assess the linguistic capabilities of multilingual LLMs in low-resource languages like Urdu which often receive less data in training compared to high-resource languages.", "method": "The study utilizes a dataset called UrBLiMP, comprising 5,696 minimal pairs of sentences that illustrate grammatical differences in Urdu, evaluated on 20 multilingual LLMs.", "result": "The evaluation shows significant performance variation across different linguistic phenomena, with LLaMA-3-70B achieving the highest average accuracy of 94.73%.", "conclusion": "The findings indicate that while current multilingual LLMs show potential in understanding Urdu, they also have notable limitations in capturing detailed syntactic knowledge.", "key_contributions": ["Introduction of the UrBLiMP dataset for Urdu", "Evaluation of 20 multilingual LLMs on syntactic understanding", "High inter-annotator agreement indicating dataset reliability"], "limitations": "Limited to evaluating only a subset of multilingual LLMs; generalization to other low-resource languages may require additional research.", "keywords": ["Urdu", "Multilingual LLMs", "Linguistic Minimal Pairs", "Syntactic Knowledge", "Low-Resource Languages"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.01070", "pdf": "https://arxiv.org/pdf/2508.01070.pdf", "abs": "https://arxiv.org/abs/2508.01070", "title": "How Long Does It Take to Alleviate Discomfort? A Preliminary Study on Reducing Cybersickness in Novice Users", "authors": ["Zhengxin Zhang", "Shufang Qian", "Yi Wang", "Xiao Liu", "Thuong Hoang", "Chetan Arora", "Jingjing Zhang", "Henry Been Lirn Duh"], "categories": ["cs.HC"], "comment": null, "summary": "Cybersickness significantly impacts the user experience in VR applications.\nLocomotion tunneling is a widely adopted technique for mitigating cybersickness\nin susceptible users. However, there is a lack of research investigating the\neffects of prolonged use of locomotion tunneling among novice users. To fill\nthis gap, we used VRChat as our experimental platform. We recruited 24 novice\nVR users, defined as participants with no prior experience using immersive\nvirtual environments. We collected five days of data within a one-week period.\nThe results indicated that participants exhibited significant mitigation to\ncybersickness by Day 4. However, a change in the VR scene on Day 5 led to a\nnotable increase in cybersickness symptoms. Qualitative feedback revealed\nparticipant-perceived causes of cybersickness and suggested that the\neffectiveness of locomotion tunneling was limited in some scenarios. Finally,\nwe discussed the limitations of the study and proposed directions for future\nresearch.", "AI": {"tldr": "This paper investigates the effects of prolonged use of locomotion tunneling on novice VR users to mitigate cybersickness.", "motivation": "There is a lack of research on the long-term effects of locomotion tunneling for mitigating cybersickness among novice VR users.", "method": "The study involved 24 novice VR users over a week using VRChat, collecting five days of data to measure cybersickness levels and gather qualitative feedback.", "result": "Participants showed significant reduction in cybersickness by Day 4, but encountered increased symptoms after a scene change on Day 5. Qualitative feedback revealed limitations in effectiveness of locomotion tunneling.", "conclusion": "The study highlights the potential of locomotion tunneling to mitigate cybersickness but reveals its limitations in varying scenarios, necessitating further research.", "key_contributions": ["Investigates long-term effects of locomotion tunneling on novice users", "Findings suggest the need for adaptable mitigation strategies in VR", "Identified participant-perceived causes of cybersickness"], "limitations": "Limited sample size and scope focused on a single platform (VRChat).", "keywords": ["cybersickness", "locomotion tunneling", "VR", "novice users", "user experience"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2508.01096", "pdf": "https://arxiv.org/pdf/2508.01096.pdf", "abs": "https://arxiv.org/abs/2508.01096", "title": "Cross-Domain Web Information Extraction at Pinterest", "authors": ["Michael Farag", "Patrick Halina", "Andrey Zaytsev", "Alekhya Munagala", "Imtihan Ahmed", "Junhao Wang"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "The internet offers a massive repository of unstructured information, but\nit's a significant challenge to convert this into a structured format. At\nPinterest, the ability to accurately extract structured product data from\ne-commerce websites is essential to enhance user experiences and improve\ncontent distribution. In this paper, we present Pinterest's system for\nattribute extraction, which achieves remarkable accuracy and scalability at a\nmanageable cost. Our approach leverages a novel webpage representation that\ncombines structural, visual, and text modalities into a compact form,\noptimizing it for small model learning. This representation captures each\nvisible HTML node with its text, style and layout information. We show how this\nallows simple models such as eXtreme Gradient Boosting (XGBoost) to extract\nattributes more accurately than much more complex Large Language Models (LLMs)\nsuch as Generative Pre-trained Transformer (GPT). Our results demonstrate a\nsystem that is highly scalable, processing over 1,000 URLs per second, while\nbeing 1000 times more cost-effective than the cheapest GPT alternatives.", "AI": {"tldr": "A system developed by Pinterest for efficient attribute extraction from e-commerce websites using a novel webpage representation, achieving high accuracy and cost-effectiveness compared to large language models.", "motivation": "To efficiently convert unstructured e-commerce information into a structured format to enhance user experience and improve content distribution.", "method": "The system utilizes a new webpage representation that merges structural, visual, and text modalities to optimize small model learning for accurate attribute extraction.", "result": "Achieves high accuracy and scalability, processing over 1,000 URLs per second, at a cost-effectiveness 1000 times better than the cheapest GPT alternatives.", "conclusion": "The proposed attribute extraction system demonstrates that simple models can outperform more complex LLMs in terms of accuracy and scalability for specific tasks.", "key_contributions": ["Introduction of a novel webpage representation combining multiple modalities", "Demonstration of simple models like XGBoost outperforming LLMs in attribute extraction", "High scalability and cost-effectiveness of the approach"], "limitations": "", "keywords": ["attribute extraction", "e-commerce", "webpage representation", "XGBoost", "scalability"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.01092", "pdf": "https://arxiv.org/pdf/2508.01092.pdf", "abs": "https://arxiv.org/abs/2508.01092", "title": "DescribePro: Collaborative Audio Description with Human-AI Interaction", "authors": ["Maryam Cheema", "Sina Elahimanesh", "Samuel Martin", "Pooyan Fazli", "Hasti Seifi"], "categories": ["cs.HC"], "comment": "ASSETS 25 19 pages, 8 figures", "summary": "Audio description (AD) makes video content accessible to millions of blind\nand low vision (BLV) users. However, creating high-quality AD involves a\ntrade-off between the precision of human-crafted descriptions and the\nefficiency of AI-generated ones. To address this, we present DescribePro a\ncollaborative AD authoring system that enables describers to iteratively refine\nAI-generated descriptions through multimodal large language model prompting and\nmanual editing. DescribePro also supports community collaboration by allowing\nusers to fork and edit existing ADs, enabling the exploration of different\nnarrative styles. We evaluate DescribePro with 18 describers (9 professionals\nand 9 novices) using quantitative and qualitative methods. Results show that AI\nsupport reduces repetitive work while helping professionals preserve their\nstylistic choices and easing the cognitive load for novices. Collaborative tags\nand variations show potential for providing customizations, version control,\nand training new describers. These findings highlight the potential of\ncollaborative, AI-assisted tools to enhance and scale AD authorship.", "AI": {"tldr": "DescribePro is a collaborative audio description authoring system that combines AI-generated content with manual editing, facilitating community collaboration and custom narrative styles for improved accessibility.", "motivation": "The paper addresses the challenge of creating high-quality audio descriptions for blind and low vision users, highlighting the need for a balance between human and AI contributions.", "method": "The authors developed DescribePro, a system that allows describers to refine AI-generated descriptions through iterative processes and community collaboration.", "result": "Evaluation with 18 describers indicated that AI support reduced repetitive work and allowed professionals to maintain stylistic choices while easing the cognitive load for novices.", "conclusion": "DescribePro demonstrates the effectiveness of AI-assisted tools in enhancing audio description authorship, offering customization options and supporting new describer training.", "key_contributions": ["Introduction of DescribePro for collaborative AD authoring", "Integration of AI-generated descriptions with manual editing", "Community collaboration features for narrative style exploration"], "limitations": "", "keywords": ["Audio Description", "AI-generated Content", "Collaborative Tools", "Human-Computer Interaction", "Accessibility"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.01159", "pdf": "https://arxiv.org/pdf/2508.01159.pdf", "abs": "https://arxiv.org/abs/2508.01159", "title": "Asking the Right Questions: Benchmarking Large Language Models in the Development of Clinical Consultation Templates", "authors": ["Liam G. McCoy", "Fateme Nateghi Haredasht", "Kanav Chopra", "David Wu", "David JH Wu", "Abass Conteh", "Sarita Khemani", "Saloni Kumar Maharaj", "Vishnu Ravi", "Arth Pahwa", "Yingjie Weng", "Leah Rosengaus", "Lena Giang", "Kelvin Zhenghao Li", "Olivia Jee", "Daniel Shirvani", "Ethan Goh", "Jonathan H. Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This study evaluates the capacity of large language models (LLMs) to generate\nstructured clinical consultation templates for electronic consultation. Using\n145 expert-crafted templates developed and routinely used by Stanford's\neConsult team, we assess frontier models -- including o3, GPT-4o, Kimi K2,\nClaude 4 Sonnet, Llama 3 70B, and Gemini 2.5 Pro -- for their ability to\nproduce clinically coherent, concise, and prioritized clinical question\nschemas. Through a multi-agent pipeline combining prompt optimization, semantic\nautograding, and prioritization analysis, we show that while models like o3\nachieve high comprehensiveness (up to 92.2\\%), they consistently generate\nexcessively long templates and fail to correctly prioritize the most clinically\nimportant questions under length constraints. Performance varies across\nspecialties, with significant degradation in narrative-driven fields such as\npsychiatry and pain medicine. Our findings demonstrate that LLMs can enhance\nstructured clinical information exchange between physicians, while highlighting\nthe need for more robust evaluation methods that capture a model's ability to\nprioritize clinically salient information within the time constraints of\nreal-world physician communication.", "AI": {"tldr": "The study assesses LLMs' ability to generate structured clinical consultation templates for electronic consultation, revealing strengths and weaknesses in template length and prioritization of clinical questions.", "motivation": "To evaluate the effectiveness of large language models in generating clinically relevant consultation templates for improved physician communication.", "method": "Using 145 expert-crafted templates, the study employs a multi-agent pipeline that includes prompt optimization, semantic autograding, and prioritization analysis to assess various LLMs.", "result": "Models such as o3 achieve high comprehensiveness (up to 92.2%), but often produce overly long templates and struggle with prioritizing clinically important questions under length constraints; performance varies among medical specialties.", "conclusion": "LLMs have potential to improve structured clinical information exchange, yet there is a need for improved evaluation methods to measure their capacity for prioritizing essential clinical information.", "key_contributions": ["Assessment of multiple frontier LLMs in generating clinical consultation templates.", "Demonstration of LLM strengths in comprehensiveness but weaknesses in length and prioritization.", "Identification of performance degradation in narrative-driven medical fields."], "limitations": "Current models produce excessively long templates and struggle with prioritization of questions; performance varies by specialty.", "keywords": ["large language models", "clinical consultation templates", "eConsult", "health informatics", "physician communication"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.01110", "pdf": "https://arxiv.org/pdf/2508.01110.pdf", "abs": "https://arxiv.org/abs/2508.01110", "title": "Cross-Device Motion Interaction via Apple's Native System Frameworks", "authors": ["Ezequiel Santos"], "categories": ["cs.HC"], "comment": null, "summary": "We introduce an open-source, fully offline pipeline that transforms a\nconsumer-grade iPhone into a motion controller with real-time tactile feedback,\nusing only native Apple frameworks. Designed for rapid prototyping and applied\nmobile HCI scenarios, the system integrates CoreMotion for inertial sensing,\nMultipeerConnectivity for peer-to-peer data transmission at 10 Hz, and\nCoreHaptics for immediate tactile confirmation. A built-in logger captures\nend-to-end latency without requiring clock synchronization, yielding a mean\ndelay of 70.4 ms and 95th percentile below 74 ms on typical 5 GHz Wi-Fi (-55\ndBm RSSI). We validated the pipeline through a real-time demonstrator game,\nKeepCalm, deployed during a public event with 21 participants. Results showed\nstable connections, zero packet loss, and negligible power impact (24 mW on\niPhone 13 mini). With fewer than 500 lines of Swift code and no reliance on\ncloud infrastructure, this system provides a compact, reproducible foundation\nfor embodied interaction research, casual games, and offline educational tools.\nAll source code, latency logs, and provisioning scripts are openly released\nunder an MIT license.", "AI": {"tldr": "An offline iPhone motion controller pipeline offers tactile feedback for low-latency applications in mobile HCI.", "motivation": "To create a low-latency, reproducible motion controller for mobile HCI research and applications without the need for internet connectivity.", "method": "The system leverages native Apple frameworks (CoreMotion, MultipeerConnectivity, CoreHaptics) to sense motion, communicate peer-to-peer, and provide tactile feedback, logging end-to-end latency without clock synchronization.", "result": "Achieved a mean delay of 70.4 ms and 95th percentile latency below 74 ms on a typical 5 GHz Wi-Fi, with 21 participants reporting stable connections and no packet loss during a game demonstration.", "conclusion": "The pipeline is suited for rapid prototyping in mobile HCI, supporting casual games and educational tools, and is fully accessible with open-source code.", "key_contributions": ["Development of an offline motion controller using iPhone with tactile feedback", "Validation through a real-time application demonstrating performance", "Open-source availability under MIT license"], "limitations": "", "keywords": ["HCI", "motion controller", "tactile feedback", "offline", "mobile applications"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2508.01161", "pdf": "https://arxiv.org/pdf/2508.01161.pdf", "abs": "https://arxiv.org/abs/2508.01161", "title": "CSIRO-LT at SemEval-2025 Task 11: Adapting LLMs for Emotion Recognition for Multiple Languages", "authors": ["Jiyu Chen", "Necva Bölücü", "Sarvnaz Karimi", "Diego Mollá", "Cécile L. Paris"], "categories": ["cs.CL"], "comment": "In Proceedings of the 19th International Workshop on Semantic\n  Evaluation (SemEval-2025), Vienna, Austria. Association for Computational\n  Linguistics", "summary": "Detecting emotions across different languages is challenging due to the\nvaried and culturally nuanced ways of emotional expressions. The\n\\textit{Semeval 2025 Task 11: Bridging the Gap in Text-Based emotion} shared\ntask was organised to investigate emotion recognition across different\nlanguages. The goal of the task is to implement an emotion recogniser that can\nidentify the basic emotional states that general third-party observers would\nattribute to an author based on their written text snippet, along with the\nintensity of those emotions. We report our investigation of various\ntask-adaptation strategies for LLMs in emotion recognition. We show that the\nmost effective method for this task is to fine-tune a pre-trained multilingual\nLLM with LoRA setting separately for each language.", "AI": {"tldr": "This paper investigates emotion recognition in multilingual contexts using fine-tuned large language models (LLMs).", "motivation": "The need for effective emotion detection across diverse languages due to cultural differences in emotional expression.", "method": "The study explores various task-adaptation strategies for LLMs, ultimately finding that fine-tuning a pre-trained multilingual LLM using LoRA settings for each language is the best approach.", "result": "The paper demonstrates that fine-tuning enhances the LLM's ability to recognize emotions and their intensity from text snippets.", "conclusion": "Fine-tuning multilingual LLMs is crucial for accurately detecting emotions across different languages.", "key_contributions": ["Investigation of task-adaptation strategies for emotion recognition using LLMs.", "Demonstration of the effectiveness of fine-tuning with LoRA settings for multilingual support.", "Contribution to the Semeval 2025 Task on text-based emotion recognition."], "limitations": "", "keywords": ["emotion recognition", "multilingual LLM", "fine-tuning", "LoRA", "Semeval 2025"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.01155", "pdf": "https://arxiv.org/pdf/2508.01155.pdf", "abs": "https://arxiv.org/abs/2508.01155", "title": "Presentation of Low-Frequency Vibration to the Face Using Amplitude Modulation", "authors": ["Yuma Akiba", "Shota Nakayama", "Keigo Ushiyama", "Izumi Mizoguchi", "Hiroyuki Kajimoto"], "categories": ["cs.HC"], "comment": "Submitted version, in IEEE Transactions on Haptics, 2025", "summary": "This study proposes a method to present pure low-frequency vibration\nsensations to the face that cannot be presented by small commercially available\nvibrators. The core innovation lies in utilizing an amplitude modulation\ntechnique with a carrier frequency of approximately 200 Hz. Due to the absence\nof Pacinian corpuscles in the facial region - receptors responsible for\ndetecting high-frequency vibrations around 200 Hz - only the original\nlow-frequency signal is perceived. Three experiments were conducted.\nExperiments 1 and 2 were performed on the forehead to confirm that the proposed\namplitude modulation method could produce the desired low-frequency perception\nand to evaluate the subjective quality of the vibration. The results suggested\nthat the proposed method could produce the perception of desired pure\nlow-frequency vibration when applied to the forehead. In Experiment 3, the\nproposed method was applied to the whole face, and its range of applicability\nwas explored. The results indicated that the original low-frequency vibration\nwas clearly perceptible around the eyes, cheeks, and lower lip area.", "AI": {"tldr": "A method is proposed to present low-frequency vibrations to the face using amplitude modulation, producing clear sensations in various facial regions.", "motivation": "To explore a new technique for delivering pure low-frequency vibration sensations to the face, which are poorly represented by existing commercial vibrators.", "method": "The study employs amplitude modulation with a carrier frequency of approximately 200 Hz to enable the perception of low-frequency vibrations on the facial region.", "result": "Experiments confirmed that the proposed method successfully produces desired low-frequency perceptions on the forehead and across the face, with particular clarity around the eyes, cheeks, and lower lip.", "conclusion": "The innovative amplitude modulation technique can effectively deliver low-frequency vibrations to the facial area, which has implications for various applications in HCI and haptic feedback.", "key_contributions": ["Introduction of a novel amplitude modulation technique for facial haptic feedback", "Demonstration of effective low-frequency vibration perception in facial areas", "Evaluation of subjective quality of vibration using experimental methodology"], "limitations": "", "keywords": ["haptic feedback", "low-frequency vibration", "amplitude modulation", "facial perception", "human-computer interaction"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2508.01198", "pdf": "https://arxiv.org/pdf/2508.01198.pdf", "abs": "https://arxiv.org/abs/2508.01198", "title": "Adaptive Content Restriction for Large Language Models via Suffix Optimization", "authors": ["Yige Li", "Peihai Jiang", "Jun Sun", "Peng Shu", "Tianming Liu", "Zhen Xiang"], "categories": ["cs.CL", "cs.AI"], "comment": "19 pages", "summary": "Large Language Models (LLMs) have demonstrated significant success across\ndiverse applications. However, enforcing content restrictions remains a\nsignificant challenge due to their expansive output space. One aspect of\ncontent restriction is preventing LLMs from generating harmful content via\nmodel alignment approaches such as supervised fine-tuning (SFT). Yet, the need\nfor content restriction may vary significantly across user groups, change\nrapidly over time, and not always align with general definitions of\nharmfulness. Applying SFT to each of these specific use cases is impractical\ndue to the high computational, data, and storage demands. Motivated by this\nneed, we propose a new task called \\textit{Adaptive Content Restriction}\n(AdaCoRe), which focuses on lightweight strategies -- methods without model\nfine-tuning -- to prevent deployed LLMs from generating restricted terms for\nspecific use cases. We propose the first method for AdaCoRe, named\n\\textit{Suffix Optimization (SOP)}, which appends a short, optimized suffix to\nany prompt to a) prevent a target LLM from generating a set of restricted\nterms, while b) preserving the output quality. To evaluate AdaCoRe approaches,\nincluding our SOP, we create a new \\textit{Content Restriction Benchmark}\n(CoReBench), which contains 400 prompts for 80 restricted terms across 8\ncarefully selected categories. We demonstrate the effectiveness of SOP on\nCoReBench, which outperforms the system-level baselines such as system suffix\nby 15\\%, 17\\%, 10\\%, 9\\%, and 6\\% on average restriction rates for Gemma2-2B,\nMistral-7B, Vicuna-7B, Llama3-8B, and Llama3.1-8B, respectively. We also\ndemonstrate that SOP is effective on POE, an online platform hosting various\ncommercial LLMs, highlighting its practicality in real-world scenarios.", "AI": {"tldr": "The paper introduces Adaptive Content Restriction (AdaCoRe), a lightweight approach to prevent harmful content generation in LLMs without model fine-tuning, featuring a method called Suffix Optimization (SOP) and a benchmark for evaluation.", "motivation": "There is a need for flexible content restriction in LLMs that varies by user group and context, which can't be met by traditional fine-tuning methods due to resource demands.", "method": "The proposed method, Suffix Optimization (SOP), appends an optimized suffix to prompts to avoid generating restricted terms while maintaining output quality.", "result": "SOP shows significant effectiveness over system-level baselines, achieving better restriction rates across several LLMs and demonstrating practical utility in a real-world platform (POE).", "conclusion": "Adaptation strategies like AdaCoRe, particularly SOP, provide a promising way to manage content restrictions in LLMs without requiring extensive computational resources.", "key_contributions": ["Introduction of the AdaCoRe task for lightweight content restrictions", "Development of Suffix Optimization (SOP) as a method to preserve output quality while enforcing restrictions", "Creation of the Content Restriction Benchmark (CoReBench) for evaluating content restriction methods."], "limitations": "", "keywords": ["Large Language Models", "Content Restriction", "Adaptive Content Restriction", "Suffix Optimization", "Harmful Content Generation"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2508.01165", "pdf": "https://arxiv.org/pdf/2508.01165.pdf", "abs": "https://arxiv.org/abs/2508.01165", "title": "RoboLinker: A Diffusion-model-based Matching Clothing Generator Between Humans and Companion Robots", "authors": ["Jing Tang", "Qing Xiao", "Kunxu Du", "Zaiqiao Ye"], "categories": ["cs.HC", "cs.CY", "cs.RO"], "comment": "3 pages, 3 figures, accepted by UIST Adjunct'25", "summary": "We present RoboLinker, a generative design system that creates matching\noutfits for humans and their robots. Using a diffusion-based model, the system\ntakes a robot image and a style prompt from users as input, and outputs a human\noutfit that visually complements the robot's attire. Through an interactive\ninterface, users can refine the generated designs. We evaluate RoboLinker with\nboth humanoid and pet-like robots, demonstrating its capacity to produce\nstylistically coherent and emotionally resonant results.", "AI": {"tldr": "RoboLinker is a generative design system that matches human outfits to robot attire using a diffusion-based model and an interactive interface.", "motivation": "The paper aims to explore the aesthetic and emotional connection between humans and robots by creating visually coherent outfits that reflect their relationship.", "method": "The authors employ a diffusion-based model that takes an image of a robot and a style prompt from users to generate a human outfit.", "result": "RoboLinker demonstrates effective generation of outfits that complement both humanoid and pet-like robots, producing stylistically coherent and emotionally appealing results.", "conclusion": "The study highlights the potential for RoboLinker to enhance human-robot interaction through shared stylistic expressions in outfits.", "key_contributions": ["Introduction of RoboLinker, a novel generative design system for human-robot attire matching.", "Demonstration of effective outfit generation for both humanoid and pet-like robots.", "User interactivity in refining design outputs to enhance personal relevance."], "limitations": "The evaluation may be limited to a specific set of robot types and styles, and user preferences may vary widely.", "keywords": ["RoboLinker", "Generative Design", "Human-Robot Interaction"], "importance_score": 3, "read_time_minutes": 5}}
{"id": "2508.01213", "pdf": "https://arxiv.org/pdf/2508.01213.pdf", "abs": "https://arxiv.org/abs/2508.01213", "title": "Show or Tell? Modeling the evolution of request-making in Human-LLM conversations", "authors": ["Shengqi Zhu", "Jeffrey M. Rzeszotarski", "David Mimno"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Chat logs provide a rich source of information about LLM users, but patterns\nof user behavior are often masked by the variability of queries. We present a\nnew task, segmenting chat queries into contents of requests, roles,\nquery-specific context, and additional expressions. We find that, despite the\nfamiliarity of chat-based interaction, request-making in LLM queries remains\nsignificantly different from comparable human-human interactions. With the data\nresource, we introduce an important perspective of diachronic analyses with\nuser expressions. We find that query patterns vary between early ones\nemphasizing requests, and individual users explore patterns but tend to\nconverge with experience. Finally, we show that model capabilities affect user\nbehavior, particularly with the introduction of new models, which are traceable\nat the community level.", "AI": {"tldr": "This paper investigates user behavior in LLM queries, segmenting chat logs into various components and analyzing patterns across different users and interactions.", "motivation": "To better understand the variability in user behavior when interacting with LLMs through chat queries and how their request-making differs from traditional human-human interactions.", "method": "The study employs segmentation of chat queries into contents of requests, roles, query-specific context, and expressions, alongside diachronic analysis of user behavior over time.", "result": "Query patterns were found to evolve from an initial focus on requests to more complex interactions as users gain experience, with significant differences observed in user behavior when new model capabilities are introduced.", "conclusion": "User behavior in LLM queries is distinct from human-human interactions, and as users become more familiar, their interactions tend to converge while still being influenced by model capabilities.", "key_contributions": ["Introduces a new task of segmenting chat queries into various components.", "Provides insights into how user behaviors change over time with experience.", "Demonstrates the impact of model capabilities on user interaction patterns."], "limitations": "", "keywords": ["chat logs", "LLM users", "user behavior", "query segmentation", "model capabilities"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.01235", "pdf": "https://arxiv.org/pdf/2508.01235.pdf", "abs": "https://arxiv.org/abs/2508.01235", "title": "NarraGuide: an LLM-based Narrative Mobile Robot for Remote Place Exploration", "authors": ["Yaxin Hu", "Arissa J. Sato", "Jingxin Du", "Chenming Ye", "Anjun Zhu", "Pragathi Praveena", "Bilge Mutlu"], "categories": ["cs.HC", "cs.RO", "68"], "comment": null, "summary": "Robotic telepresence enables users to navigate and experience remote\nenvironments. However, effective navigation and situational awareness depend on\nusers' prior knowledge of the environment, limiting the usefulness of these\nsystems for exploring unfamiliar places. We explore how integrating\nlocation-aware LLM-based narrative capabilities into a mobile robot can support\nremote exploration. We developed a prototype system, called NarraGuide, that\nprovides narrative guidance for users to explore and learn about a remote place\nthrough a dialogue-based interface. We deployed our prototype in a geology\nmuseum, where remote participants (n=20) used the robot to tour the museum. Our\nfindings reveal how users perceived the robot's role, engaged in dialogue in\nthe tour, and expressed preferences for bystander encountering. Our work\ndemonstrates the potential of LLM-enabled robotic capabilities to deliver\nlocation-aware narrative guidance and enrich the experience of exploring remote\nenvironments.", "AI": {"tldr": "This paper presents NarraGuide, a mobile robotic system that utilizes LLM capabilities for providing narrative guidance to users exploring remote locations.", "motivation": "To enhance navigation and situational awareness in robotic telepresence by integrating narrative capabilities to support exploration in unfamiliar places.", "method": "A prototype system was developed and deployed in a geology museum, allowing remote participants to engage with the robot through a dialogue-based interface while touring the museum.", "result": "Remote participants (n=20) reported positive engagement with the robot, expressing preferences for interaction and demonstrating effective use of narrative guidance during exploration.", "conclusion": "The study shows that LLM-enabled robotic systems can significantly enrich the experience of exploring remote environments through tailored narrative guidance.", "key_contributions": ["Development of NarraGuide, a narrative-enabled robotic explorer", "Deployment and evaluation of the system in a real-world setting (geology museum)", "Insights into user engagement and preferences during remote tours."], "limitations": "", "keywords": ["robotic telepresence", "LLM", "narrative guidance", "remote exploration", "human-robot interaction"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.01222", "pdf": "https://arxiv.org/pdf/2508.01222.pdf", "abs": "https://arxiv.org/abs/2508.01222", "title": "WebDS: An End-to-End Benchmark for Web-based Data Science", "authors": ["Ethan Hsu", "Hong Meng Yam", "Ines Bouissou", "Aaron Murali John", "Raj Thota", "Josh Koe", "Vivek Sarath Putta", "G K Dharesan", "Alexander Spangher", "Shikhar Murty", "Tenghao Huang", "Christopher D. Manning"], "categories": ["cs.CL", "cs.AI"], "comment": "14 pages", "summary": "A large portion of real-world data science tasks are complex and require\nmulti-hop web-based interactions: finding appropriate data available on the\ninternet, synthesizing real-time data of various modalities from different\nlocations, and producing summarized analyses. Existing web benchmarks often\nfocus on simplistic interactions, such as form submissions or e-commerce\ntransactions, and often do not require diverse tool-using capabilities required\nfor web based data science. Conversely, traditional data science benchmarks\ntypically concentrate on static, often textually bound datasets and do not\nassess end-to-end workflows that encompass data acquisition, cleaning,\nanalysis, and insight generation. In response, we introduce WebDS, the first\nend-to-end web-based data science benchmark. It comprises 870 web-based data\nscience tasks across 29 diverse websites from structured government data\nportals to unstructured news media, challenging agents to perform complex,\nmulti-step operations requiring the use of tools and heterogeneous data formats\nthat better reflect the realities of modern data analytics. Evaluations of\ncurrent SOTA LLM agents indicate significant performance gaps in accomplishing\nthese tasks. For instance, Browser Use, which accomplishes 80% of tasks on Web\nVoyager, successfully completes only 15% of tasks in WebDS, which our analysis\nsuggests is due to new failure modes like poor information grounding,\nrepetitive behavior and shortcut-taking that agents performing WebDS' tasks\ndisplay. By providing a more robust and realistic testing ground, WebDS sets\nthe stage for significant advances in the development of practically useful\nLLM-based data science.", "AI": {"tldr": "WebDS introduces a benchmark for end-to-end web-based data science tasks, highlighting performance gaps in current LLM agents.", "motivation": "Address the need for a benchmark that reflects the complexities of real-world data science tasks involving multi-hop web interactions.", "method": "WebDS includes 870 web-based data science tasks from 29 websites, evaluating agents on their ability to perform multi-step operations with diverse data formats.", "result": "Current SOTA LLM agents show significant performance gaps, completing only 15% of tasks in WebDS compared to 80% in simpler benchmarks.", "conclusion": "WebDS offers a more realistic platform for evaluating and improving LLM capabilities in data science applications.", "key_contributions": ["First end-to-end web-based data science benchmark", "Inclusion of 870 diverse web tasks", "Identification of new failure modes in LLM agents"], "limitations": "Limited to the 29 websites included; potential biases from the tasks selected.", "keywords": ["web-based data science", "benchmark", "multi-hop interactions", "LLM", "data analytics"], "importance_score": 7, "read_time_minutes": 14}}
{"id": "2508.01279", "pdf": "https://arxiv.org/pdf/2508.01279.pdf", "abs": "https://arxiv.org/abs/2508.01279", "title": "ViseGPT: Towards Better Alignment of LLM-generated Data Wrangling Scripts and User Prompts", "authors": ["Jiajun Zhu", "Xinyu Cheng", "Zhongsu Luo", "Yunfan Zhou", "Xinhuan Shu", "Di Weng", "Yingcai Wu"], "categories": ["cs.HC"], "comment": "Accepted at Annual ACM Symposium on User Interface Software and\n  Technology (UIST'25), September 28-October 1, 2025, Busan, Republic of Korea", "summary": "Large language models (LLMs) enable the rapid generation of data wrangling\nscripts based on natural language instructions, but these scripts may not fully\nadhere to user-specified requirements, necessitating careful inspection and\niterative refinement. Existing approaches primarily assist users in\nunderstanding script logic and spotting potential issues themselves, rather\nthan providing direct validation of correctness. To enhance debugging\nefficiency and optimize the user experience, we develop ViseGPT, a tool that\nautomatically extracts constraints from user prompts to generate comprehensive\ntest cases for verifying script reliability. The test results are then\ntransformed into a tailored Gantt chart, allowing users to intuitively assess\nalignment with semantic requirements and iteratively refine their scripts. Our\ndesign decisions are informed by a formative study (N=8) that explores user\npractices and challenges. We further evaluate the effectiveness and usability\nof ViseGPT through a user study (N=18). Results indicate that ViseGPT\nsignificantly improves debugging efficiency for LLM-generated data-wrangling\nscripts, enhances users' ability to detect and correct issues, and streamlines\nthe workflow experience.", "AI": {"tldr": "ViseGPT is a tool that generates test cases from user prompts to validate LLM-generated data wrangling scripts, improving debugging efficiency and user experience.", "motivation": "To enhance debugging efficiency and optimize user experience with LLM-generated scripts that may not fully meet user requirements.", "method": "ViseGPT extracts constraints from user prompts to create test cases for script verification and displays results in a Gantt chart for intuitive user assessment.", "result": "User studies demonstrate that ViseGPT significantly improves debugging efficiency and enhances users' ability to detect and correct issues in scripts.", "conclusion": "ViseGPT streamlines the workflow experience for users dealing with LLM-generated data wrangling scripts, promoting iterative refinement.", "key_contributions": ["Development of ViseGPT tool for automatic constraint extraction and test case generation", "Usability improvements for debugging LLM-generated scripts", "Integration of Gantt charts for intuitive result assessment"], "limitations": "", "keywords": ["Large Language Models", "Debugging Efficiency", "Human-Computer Interaction", "Data Wrangling", "User Experience"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.01245", "pdf": "https://arxiv.org/pdf/2508.01245.pdf", "abs": "https://arxiv.org/abs/2508.01245", "title": "WarriorMath: Enhancing the Mathematical Ability of Large Language Models with a Defect-aware Framework", "authors": ["Yue Chen", "Minghua He", "Fangkai Yang", "Pu Zhao", "Lu Wang", "Yu Kang", "Yifei Dong", "Yuefeng Zhan", "Hao Sun", "Qingwei Lin", "Saravan Rajmohan", "Dongmei Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) excel in solving mathematical problems, yet\ntheir performance is often limited by the availability of high-quality, diverse\ntraining data. Existing methods focus on augmenting datasets through rephrasing\nor difficulty progression but overlook the specific failure modes of LLMs. This\nresults in synthetic questions that the model can already solve, providing\nminimal performance gains. To address this, we propose WarriorMath, a\ndefect-aware framework for mathematical problem solving that integrates both\ntargeted data synthesis and progressive training. In the synthesis stage, we\nemploy multiple expert LLMs in a collaborative process to generate, critique,\nand refine problems. Questions that base LLMs fail to solve are identified and\niteratively improved through expert-level feedback, producing high-quality,\ndefect-aware training data. In the training stage, we introduce a progressive\nlearning framework that iteratively fine-tunes the model using increasingly\nchallenging data tailored to its weaknesses. Experiments on six mathematical\nbenchmarks show that WarriorMath outperforms strong baselines by 12.57% on\naverage, setting a new state-of-the-art. Our results demonstrate the\neffectiveness of a defect-aware, multi-expert framework for improving\nmathematical ability.", "AI": {"tldr": "WarriorMath is a defect-aware framework for enhancing Large Language Models' (LLMs) performance in mathematical problem solving through targeted data synthesis and progressive training.", "motivation": "To improve LLMs' performance in mathematics, specifically addressing their limitations due to the lack of diverse, high-quality training data and failure modes.", "method": "The framework employs expert LLMs to collaboratively generate, critique, and refine mathematical problems, identifying questions the base LLMs cannot solve. A progressive learning framework fine-tunes the model with increasingly challenging data focused on its weaknesses.", "result": "WarriorMath demonstrates improved performance, outperforming strong baselines by an average of 12.57% across six mathematical benchmarks.", "conclusion": "The results showcase the effectiveness of the defect-aware, multi-expert approach in enhancing LLMs' mathematical capabilities, establishing a new state-of-the-art.", "key_contributions": ["Introduction of WarriorMath, a defect-aware framework for LLMs in mathematics", "Implementation of a collaborative expert LLMs generation process for training data", "Establishment of a progressive learning framework for model fine-tuning"], "limitations": "", "keywords": ["Large Language Models", "mathematical problem solving", "data synthesis", "progressive training", "HCI"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2508.01282", "pdf": "https://arxiv.org/pdf/2508.01282.pdf", "abs": "https://arxiv.org/abs/2508.01282", "title": "ExplorAR: Assisting Older Adults to Learn Smartphone Apps through AR-powered Trial-and-Error with Interactive Guidance", "authors": ["Jiawei Li", "Linjie Qiu", "Zhiqing Wu", "Qiongyan Chen", "Ziyan Wang", "Mingming Fan"], "categories": ["cs.HC"], "comment": "10 pages, 5 figures, Proceedings of the 33rd ACM International\n  Conference on Multimedia", "summary": "Older adults tend to encounter challenges when learning to use new smartphone\napps due to age-related cognitive and physical changes. Compared to traditional\nsupport methods such as video tutorials, trial-and-error allows older adults to\nlearn to use smartphone apps by making and correcting mistakes. However, it\nremains unknown how trial-and-error should be designed to empower older adults\nto use smartphone apps and how well it would work for older adults. Informed by\nthe guidelines derived from prior work, we designed and implemented ExplorAR,\nan AR-based trial-and-error system that offers real-time and situated visual\nguidance in the augmented space around the smartphone to empower older adults\nto explore and correct mistakes independently. We conducted a user study with\n18 older adults to compare ExplorAR with traditional video tutorials and a\nsimplified version of ExplorAR. Results show that the AR-supported\ntrial-and-error method enhanced older adults' learning experience by fostering\ndeeper cognitive engagement and improving confidence in exploring unknown\noperations.", "AI": {"tldr": "ExplorAR is an AR-based trial-and-error learning system designed for older adults to improve their ability to use smartphone apps by providing real-time visual guidance.", "motivation": "Older adults face challenges in learning new smartphone apps due to cognitive and physical changes; exploring how trial-and-error can be effectively designed for this demographic is crucial.", "method": "The study developed an AR-based tool, ExplorAR, which provides situated visual guidance in augmented reality, enabling older adults to learn through trial-and-error.", "result": "A user study involving 18 older adults demonstrated that ExplorAR significantly enhanced their learning experience compared to traditional video tutorials.", "conclusion": "AR-supported trial-and-error learning fosters better cognitive engagement and increases confidence in using unfamiliar smartphone operations among older adults.", "key_contributions": ["Development of ExplorAR, an AR-based trial-and-error learning system for older adults.", "Comparison of AR method with traditional video tutorials and simplified AR versions.", "Demonstration of enhanced cognitive engagement and confidence in learning for older adults."], "limitations": "The study was limited to a small sample size of 18 participants, which may affect the generalizability of results.", "keywords": ["Human-Computer Interaction", "Augmented Reality", "Older Adults", "Trial-and-Error Learning", "Smartphone Apps"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.01263", "pdf": "https://arxiv.org/pdf/2508.01263.pdf", "abs": "https://arxiv.org/abs/2508.01263", "title": "Bridging LLMs and Symbolic Reasoning in Educational QA Systems: Insights from the XAI Challenge at IJCNN 2025", "authors": ["Long S. T. Nguyen", "Khang H. N. Vo", "Thu H. A. Nguyen", "Tuan C. Bui", "Duc Q. Nguyen", "Thanh-Tung Tran", "Anh D. Nguyen", "Minh L. Nguyen", "Fabien Baldacci", "Thang H. Bui", "Emanuel Di Nardo", "Angelo Ciaramella", "Son H. Le", "Ihsan Ullah", "Lorenzo Di Rocco", "Tho T. Quan"], "categories": ["cs.CL"], "comment": "The XAI Challenge @ TRNS-AI Workshop, IJCNN 2025: Explainable AI for\n  Educational Question Answering. Website:\n  https://sites.google.com/view/trns-ai/challenge/", "summary": "The growing integration of Artificial Intelligence (AI) into education has\nintensified the need for transparency and interpretability. While hackathons\nhave long served as agile environments for rapid AI prototyping, few have\ndirectly addressed eXplainable AI (XAI) in real-world educational contexts.\nThis paper presents a comprehensive analysis of the XAI Challenge 2025, a\nhackathon-style competition jointly organized by Ho Chi Minh City University of\nTechnology (HCMUT) and the International Workshop on Trustworthiness and\nReliability in Neurosymbolic AI (TRNS-AI), held as part of the International\nJoint Conference on Neural Networks (IJCNN 2025). The challenge tasked\nparticipants with building Question-Answering (QA) systems capable of answering\nstudent queries about university policies while generating clear, logic-based\nnatural language explanations. To promote transparency and trustworthiness,\nsolutions were required to use lightweight Large Language Models (LLMs) or\nhybrid LLM-symbolic systems. A high-quality dataset was provided, constructed\nvia logic-based templates with Z3 validation and refined through expert student\nreview to ensure alignment with real-world academic scenarios. We describe the\nchallenge's motivation, structure, dataset construction, and evaluation\nprotocol. Situating the competition within the broader evolution of AI\nhackathons, we argue that it represents a novel effort to bridge LLMs and\nsymbolic reasoning in service of explainability. Our findings offer actionable\ninsights for future XAI-centered educational systems and competitive research\ninitiatives.", "AI": {"tldr": "The paper presents an analysis of the XAI Challenge 2025, aimed at developing explainable AI systems for educational contexts, specifically for answering student queries with transparent explanations.", "motivation": "The integration of AI in education necessitates transparency and interpretable AI systems, especially for real-world applications such as answering student questions.", "method": "Participants in the challenge built question-answering systems using lightweight Large Language Models and hybrid systems that were evaluated based on their ability to provide explainable answers.", "result": "The provided high-quality dataset and structured competition fostered the development of innovative solutions for explainable AI in education.", "conclusion": "The challenge represents a notable effort in merging LLMs with symbolic reasoning to enhance explainability, offering valuable insights for future XAI-focused educational systems.", "key_contributions": ["Comprehensive analysis of the XAI Challenge 2025", "Development of explainable question-answering systems", "Insights on bridging LLMs and symbolic reasoning for educational contexts"], "limitations": "", "keywords": ["Explainable AI", "Question-Answering", "Educational Technology", "Large Language Models", "Hackathon"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.01318", "pdf": "https://arxiv.org/pdf/2508.01318.pdf", "abs": "https://arxiv.org/abs/2508.01318", "title": "AffectGPT-R1: Leveraging Reinforcement Learning for Open-Vocabulary Emotion Recognition", "authors": ["Zheng Lian"], "categories": ["cs.HC"], "comment": null, "summary": "Open-Vocabulary Multimodal Emotion Recognition (OV-MER) aims to predict\nemotions without being constrained by predefined label spaces, enabling\nfine-grained and human-like emotion understanding. Unlike traditional\ndiscriminative methods, OV-MER leverages generative models, such as large\nlanguage models (LLMs) with extensive vocabularies, to capture the full\nspectrum of emotions. Previous approaches (like AffectGPT) primarily rely on\ntoken-level loss for training. However, this objective does not align with the\nemotion wheel (EW)-based evaluation metrics used in OV-MER. Unfortunately,\nEW-based metrics cannot be directly optimized via gradient backpropagation. In\nthis paper, we propose AffectGPT-R1, a reinforcement learning framework that\ndirectly optimizes performance on EW-based metrics. Specifically, we treat\nthese metrics as the reward function and employ Group Relative Policy\nOptimization (GRPO) to maximize rewards. Experimental results demonstrate that\nAffectGPT-R1 achieves significant improvements on OV-MER. We hope this work\nadvances the field of multimodal emotion recognition. Our code will be publicly\navailable at:https://github.com/zeroQiaoba/AffectGPT.", "AI": {"tldr": "This paper presents AffectGPT-R1, a reinforcement learning framework for Open-Vocabulary Multimodal Emotion Recognition (OV-MER) that uses Group Relative Policy Optimization to directly optimize EW-based metrics instead of traditional token-level loss, demonstrating significant improvements in emotion recognition performance.", "motivation": "The goal is to enhance emotion recognition capabilities through an open-vocabulary approach that better aligns with human-like understanding of emotions and relevant evaluation metrics.", "method": "AffectGPT-R1 employs reinforcement learning by treating emotion wheel (EW)-based metrics as a reward function and utilizes Group Relative Policy Optimization (GRPO) to maximize performance.", "result": "AffectGPT-R1 significantly improves performance on OV-MER benchmarks compared to previous methods that relied on token-level loss.", "conclusion": "The findings suggest that treating EW-based metrics as the reward function in reinforcement learning can effectively enhance multimodal emotion recognition.", "key_contributions": ["Introduction of AffectGPT-R1 for OV-MER", "Use of GRPO to optimize EW-based metrics", "Demonstration of significant performance improvements on OV-MER tasks"], "limitations": "", "keywords": ["multimodal", "emotion recognition", "reinforcement learning", "large language models", "open vocabulary"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.01290", "pdf": "https://arxiv.org/pdf/2508.01290.pdf", "abs": "https://arxiv.org/abs/2508.01290", "title": "Prompting Large Language Models with Partial Knowledge for Answering Questions with Unseen Entities", "authors": ["Zhichao Yan", "Jiapu Wang", "Jiaoyan Chen", "Yanyan Wang", "Hongye Tan", "Jiye Liang", "Xiaoli Li", "Ru Li", "Jeff Z. Pan"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) shows impressive performance by\nsupplementing and substituting parametric knowledge in Large Language Models\n(LLMs). Retrieved knowledge can be divided into three types: explicit answer\nevidence, implicit answer clue, and insufficient answer context which can be\nfurther categorized into totally irrelevant and partially relevant information.\nEffectively utilizing partially relevant knowledge remains a key challenge for\nRAG systems, especially in incomplete knowledge base retrieval. Contrary to the\nconventional view, we propose a new perspective: LLMs can be awakened via\npartially relevant knowledge already embedded in LLMs. To comprehensively\ninvestigate this phenomenon, the triplets located in the gold reasoning path\nand their variants are used to construct partially relevant knowledge by\nremoving the path that contains the answer. We provide theoretical analysis of\nthe awakening effect in LLMs and support our hypothesis with experiments on two\nKnowledge Graphs (KGs) Question Answering (QA) datasets. Furthermore, we\npresent a new task, Unseen Entity KGQA, simulating real-world challenges where\nentity linking fails due to KG incompleteness. Our awakening-based approach\ndemonstrates greater efficacy in practical applications, outperforms\ntraditional methods that rely on embedding-based similarity which are prone to\nreturning noisy information.", "AI": {"tldr": "This paper proposes a novel approach to improve Retrieval-Augmented Generation (RAG) systems by effectively utilizing partially relevant knowledge embedded in Large Language Models (LLMs) to enhance performance in Knowledge Graph Question Answering tasks.", "motivation": "To address the challenge of leveraging partially relevant knowledge in Retrieval-Augmented Generation systems, particularly in the context of incomplete knowledge bases.", "method": "The authors construct partially relevant knowledge by modifying triplets from the gold reasoning path, analyzing their effects on LLMs, and validating their approach through experiments on Knowledge Graph Question Answering datasets.", "result": "The proposed awakening-based approach demonstrates improved performance over traditional methods that rely on embedding-based similarity and struggles with noise in the information retrieval process.", "conclusion": "The findings suggest that partially relevant knowledge can effectively 'awaken' LLMs and handle practical challenges in Knowledge Graphs, leading to better performance in QA tasks.", "key_contributions": ["Introduced a new perspective on utilizing partially relevant knowledge in LLMs", "Conducted theoretical analysis and experimental validation of the awakening effect", "Developed the Unseen Entity KGQA task to address real-world entity linking challenges."], "limitations": "Focuses solely on the awakening effect and does not explore fully relevant knowledge scenarios.", "keywords": ["Retrieval-Augmented Generation", "Large Language Models", "Knowledge Graphs", "Question Answering", "Partially Relevant Knowledge"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.01388", "pdf": "https://arxiv.org/pdf/2508.01388.pdf", "abs": "https://arxiv.org/abs/2508.01388", "title": "An Appraisal-Based Approach to Human-Centred Explanations", "authors": ["Rukshani Somarathna", "Madhawa Perera", "Tom Gedeon", "Matt Adcock"], "categories": ["cs.HC"], "comment": null, "summary": "Explainability remains a critical challenge in artificial intelligence (AI)\nsystems, particularly in high stakes domains such as healthcare, finance, and\ndecision support, where users must understand and trust automated reasoning.\nTraditional explainability methods such as feature importance and post-hoc\njustifications often fail to capture the cognitive processes that underlie\nhuman decision making, leading to either too technical or insufficiently\nmeaningful explanations. We propose a novel appraisal based framework inspired\nby the Component Process Model (CPM) for explainability to address this gap.\nWhile CPM has traditionally been applied to emotion research, we use its\nappraisal component as a cognitive model for generating human aligned\nexplanations. By structuring explanations around key appraisal dimensions such\nas relevance, implications, coping potential, and normative significance our\nframework provides context sensitive, cognitively meaningful justifications for\nAI decisions. This work introduces a new paradigm for generating intuitive,\nhuman-centred explanations in AI driven systems by bridging cognitive science\nand explainable AI.", "AI": {"tldr": "This paper proposes a novel appraisal-based framework for generating explainable AI decisions that aligns with human cognitive processes, particularly in high-stakes domains like healthcare.", "motivation": "To address the limitations of traditional explainability methods that fail to capture the cognitive processes underlying human decision making in AI systems.", "method": "The proposed framework adapts the Component Process Model (CPM) from emotion research to generate explanations based on appraisal dimensions such as relevance, implications, coping potential, and normative significance.", "result": "The framework provides context-sensitive and cognitively meaningful justifications for AI decisions, forming a new paradigm for human-centered explanations in AI-driven systems.", "conclusion": "Bridging cognitive science and explainable AI facilitates the generation of intuitive explanations, enhancing user understanding and trust in AI systems.", "key_contributions": ["Introduction of an appraisal-based framework for explainability in AI.", "Application of the Component Process Model to generate cognitive explanations.", "Enhanced user trust through context-sensitive justifications for AI decisions."], "limitations": "", "keywords": ["explainable AI", "human-centered design", "cognitive science", "high-stakes domains", "healthcare"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.01302", "pdf": "https://arxiv.org/pdf/2508.01302.pdf", "abs": "https://arxiv.org/abs/2508.01302", "title": "KEDAS: Knowledge Editing Alignment with Diverse Augmentation and Self-adaptive Inference", "authors": ["Chenming Tang", "Yutong Yang", "Yunfang Wu"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "Knowledge editing aims to modify outdated knowledge in large language models\n(LLMs) efficiently while retaining their powerful capabilities. Most existing\nmethods rely on either parameter-level editing or retrieval-based approaches.\nIn this work, we propose Knowledge Editing alignment with Diverse Augmentation\nand Self-adaptive inference (KEDAS) to better align LLMs with knowledge\nediting. In the alignment phase, LLMs learn to apply in-context edited\nknowledge via low-rank adaptation. During editing, we design a diverse edit\naugmentation technique to improve the recall of edits. After that, a\nself-adaptive post-alignment inference mechanism is proposed, in which a\nfilter-based smart retriever is employed to perform a dynamic selection of\ninference routing. Specifically, irrelevant queries will go through the\noriginal pre-alignment model directly, while relevant ones, together with their\nrelated edits, go through the model with aligned adapters activated. In\nexperiments, KEDAS secures the highest overall performance scores in 35 out of\n36 cases across four datasets with three LLMs on three settings, surpassing its\nstrong knowledge editing alignment counterpart by about 19.8 harmonic mean\nscores of edit success, locality and portability and outperforming both\nparameter editing and retrieval-based baselines significantly. Analysis of\ncomputational cost and performance on general tasks further validates the\nrobustness and efficiency of KEDAS, indicating that it presents an ideal\nparadigm of knowledge editing alignment.", "AI": {"tldr": "The paper introduces KEDAS, a novel approach for efficiently editing knowledge in LLMs while maintaining performance, achieving superior results in empirical evaluations.", "motivation": "The need for efficient knowledge editing in LLMs to retain their capabilities while updating outdated information.", "method": "KEDAS employs low-rank adaptation for alignment, a diverse edit augmentation technique for recall improvement, and a self-adaptive inference mechanism with a smart retriever for dynamic query handling.", "result": "KEDAS outperforms existing approaches in 35 out of 36 cases on four datasets with three LLMs, achieving a 19.8 improvement in harmonic mean scores related to edit success and other metrics.", "conclusion": "KEDAS is validated as a robust and efficient paradigm for knowledge editing alignment in LLMs.", "key_contributions": ["Introduction of KEDAS for knowledge editing in LLMs", "Diverse edit augmentation technique for improved recall", "Self-adaptive post-alignment inference mechanism with smart retrieval."], "limitations": "", "keywords": ["Knowledge Editing", "Large Language Models", "Editing Efficiency"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.01520", "pdf": "https://arxiv.org/pdf/2508.01520.pdf", "abs": "https://arxiv.org/abs/2508.01520", "title": "Unlocking Excellence: The Impact of Voucher Incentives on Cybersecurity Education", "authors": ["Jianhua Li", "Shang Gao", "Michelle Harvey", "Trina Myers"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "While voucher incentives have been popular for primary and secondary schools,\nthey are less used in higher education. In this study, we leverage industry\nvoucher incentives to inspire students in cybersecurity education (CSE). We\nadopt a 100% portfolio-based assessment strategy, where students can freely\nselect their target grades in the investigated unit. We purposely design one of\nthe high distinction (HD) tasks to be obtaining an industry certificate and\nprovide vouchers to those who can accomplish a predefined set of tasks before a\nmidpoint. The voucher recipients will use the voucher to access the industry\ncertificate training materials and sit the certificate exam for free. Passing\nthe certificate exam is one of the conditions for gaining an HD grade. Our\nsurvey and interviews reveal a substantial influence of voucher incentives on\nstudents' career aspirations. In light of the findings, recommendations on\nadopting voucher incentives in CSE or broader ICT education are offered for\ninstitutions and researchers.", "AI": {"tldr": "This study examines the impact of industry voucher incentives on students' motivation in cybersecurity education, highlighting their influence on career aspirations and providing recommendations for implementation.", "motivation": "To explore the potential of leveraging voucher incentives to inspire students in cybersecurity education, addressing their impact on motivation and career aspirations.", "method": "A 100% portfolio-based assessment strategy was adopted, allowing students to select target grades; vouchers were provided to students completing specific tasks to access industry certificate training materials and exams.", "result": "Survey and interviews indicated a substantial positive influence of voucher incentives on students' career aspirations in cybersecurity education.", "conclusion": "The study recommends that institutions and researchers consider the adoption of voucher incentives in cybersecurity and broader ICT education to enhance student motivation and career readiness.", "key_contributions": ["Implemented a portfolio-based assessment strategy in cybersecurity education", "Demonstrated the impact of industry vouchers on students' career aspirations", "Provided insights and recommendations for integrating vouchers in ICT education"], "limitations": "", "keywords": ["voucher incentives", "cybersecurity education", "student motivation", "portfolio-based assessment", "career aspirations"], "importance_score": 4, "read_time_minutes": 7}}
{"id": "2508.01309", "pdf": "https://arxiv.org/pdf/2508.01309.pdf", "abs": "https://arxiv.org/abs/2508.01309", "title": "D-SCoRE: Document-Centric Segmentation and CoT Reasoning with Structured Export for QA-CoT Data Generation", "authors": ["Weibo Zhou", "Lingbo Li", "Shangsong Liang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The scarcity and high cost of high-quality question-answering (QA) datasets\nhinder supervised fine-tuning (SFT) for domain-specific large language models\n(LLMs). To address this, we introduce D-SCoRE, a training-free pipeline that\nutilizes LLMs and prompt engineering to produce diverse, high-quality QA\ndatasets from arbitrary textual sources. D-SCoRE integrates\n$\\textbf{D}$ocument-centric processing, $\\textbf{S}$egmentation, $\\textbf{Co}$T\n$\\textbf{R}$easoning, and structured $\\textbf{E}$xport to generate QA-COT\ndatasets tailored for domain-aware SFT. Multi-dimensional control mechanisms,\nsuch as semantic role transformation, question type balancing, and\ncounterfactual materials, enhance diversity and relevance, overcoming\nlimitations of existing QA generation. LLMs fine-tuned on D-SCoRE-generated QA\ndatasets, and human-annotated QA datasets (SQuAD, Covid-QA) are evaluated on\nSQuADShifts and Covid-QA test sets, with D-SCoRE outperforming across most\ndomains. D-SCoRE generates six QA-CoT pairs with four-option counterfactual\nmaterials per 100-200-word text in 90 seconds using an 8B LLM on consumer-grade\nhardware. Its simplicity and scalability enable efficient QA generation and\nhigh-performance fine-tuning across domains.", "AI": {"tldr": "D-SCoRE is a novel training-free pipeline that generates diverse, high-quality question-answering datasets using LLMs and prompt engineering, aimed at enhancing domain-specific supervised fine-tuning.", "motivation": "High-quality question-answering datasets are scarce and costly, limiting the deployment of supervised fine-tuning for domain-specific large language models.", "method": "D-SCoRE utilizes document-centric processing, segmentation, chain-of-thought reasoning, and structured export to create QA datasets from various textual sources, along with multi-dimensional control mechanisms.", "result": "The evaluation shows that LLMs fine-tuned on D-SCoRE-generated datasets outperform those trained on human-annotated datasets across several domains.", "conclusion": "D-SCoRE demonstrates a scalable and efficient method for QA generation that enhances fine-tuning performance for large language models without requiring extensive labeled data.", "key_contributions": ["Introduction of D-SCoRE for generating QA datasets without fine-tuning.", "Use of multi-dimensional control mechanisms for enhanced dataset diversity.", "Demonstration of superior performance on test sets compared to existing QA datasets."], "limitations": "", "keywords": ["question-answering", "large language models", "dataset generation", "prompt engineering", "supervised fine-tuning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.01547", "pdf": "https://arxiv.org/pdf/2508.01547.pdf", "abs": "https://arxiv.org/abs/2508.01547", "title": "Understanding Why ChatGPT Outperforms Humans in Visualization Design Advice", "authors": ["Yongsu Ahn", "Nam Wook Kim"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "This paper investigates why recent generative AI models outperform humans in\ndata visualization knowledge tasks. Through systematic comparative analysis of\nresponses to visualization questions, we find that differences exist between\ntwo ChatGPT models and human outputs over rhetorical structure, knowledge\nbreadth, and perceptual quality. Our findings reveal that ChatGPT-4, as a more\nadvanced model, displays a hybrid of characteristics from both humans and\nChatGPT-3.5. The two models were generally favored over human responses, while\ntheir strengths in coverage and breadth, and emphasis on technical and\ntask-oriented visualization feedback collectively shaped higher overall\nquality. Based on our findings, we draw implications for advancing user\nexperiences based on the potential of LLMs and human perception over their\ncapabilities, with relevance to broader applications of AI.", "AI": {"tldr": "Recent generative AI models, particularly ChatGPT-4, outperform humans in data visualization tasks due to superior rhetorical structure and knowledge breadth.", "motivation": "To understand the reasons behind the superior performance of generative AI models compared to humans in data visualization knowledge tasks.", "method": "Systematic comparative analysis of responses to visualization questions from two ChatGPT models and human outputs.", "result": "Both ChatGPT-3.5 and ChatGPT-4 were found to be generally favored over human responses, with strengths in coverage, breadth, and technical visualization feedback.", "conclusion": "The findings suggest that LLMs can significantly enhance user experience in data visualization, impacting broader AI applications.", "key_contributions": ["Comparison of generative AI models with human outputs in data visualization", "Identification of strengths in rhetorical structure, knowledge breadth, and perceptual quality", "Implications for user experience enhancement using LLMs"], "limitations": "", "keywords": ["generative AI", "data visualization", "ChatGPT", "user experience", "human perception"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.01317", "pdf": "https://arxiv.org/pdf/2508.01317.pdf", "abs": "https://arxiv.org/abs/2508.01317", "title": "LinkQA: Synthesizing Diverse QA from Multiple Seeds Strongly Linked by Knowledge Points", "authors": ["Xuemiao Zhang", "Can Ren", "Chengying Tu", "Rongxiang Weng", "Hongfei Yan", "Jingang Wang", "Xunliang Cai"], "categories": ["cs.CL"], "comment": null, "summary": "The advancement of large language models (LLMs) struggles with the scarcity\nof high-quality, diverse training data. To address this limitation, we propose\nLinkSyn, a novel knowledge point (KP) graph-based synthesis framework that\nenables flexible control over discipline and difficulty distributions while\nbalancing KP coverage and popularity. LinkSyn extracts KPs from\nquestion-answering (QA) seed data and constructs a KP graph to synthesize\ndiverse QA data from multiple seeds strongly linked by KPs and sampled from\ngraph walks. Specifically, LinkSyn incorporates (1) a knowledge distribution\nvalue function to guide the adjustment of path sampling probability and balance\nKP coverage and popularity during graph walks; (2) diffusion-based synthesis\nvia DeepSeek-R1 by leveraging multiple seeds with dense logical associations\nalong each path; and (3) high-difficulty QA enhancement within given\ndisciplines by flexible difficulty adjustments. By executing LinkSyn, we\nsynthesize LinkQA, a diverse multi-disciplinary QA dataset with 50B tokens.\nExtensive experiments on Llama-3 8B demonstrate that continual pre-training\nwith LinkQA yields an average improvement of $\\mathbf{11.51\\%}$ on MMLU and\nCMMLU, establishing new SOTA results. LinkQA consistently enhances performance\nacross model size and initial FLOPs scales.", "AI": {"tldr": "LinkSyn is a framework for generating diverse QA datasets by utilizing knowledge point graphs, improving model performance through novel synthesis techniques.", "motivation": "To address the scarcity of high-quality, diverse training data for large language models (LLMs).", "method": "LinkSyn extracts knowledge points (KPs) from seed data to create a KP graph that synthesizes diverse QA data via graph walks, adjusting for discipline and difficulty distributions.", "result": "Synthesizing LinkQA, a 50B token multi-disciplinary QA dataset, which results in an 11.51% improvement on MMLU and CMMLU for Llama-3 8B models, achieving new state-of-the-art results.", "conclusion": "LinkSyn effectively enhances LLM performance across different model sizes and operational scales by providing a diverse and high-quality training dataset.", "key_contributions": ["Development of LinkSyn framework for QA synthesis", "Creation of LinkQA dataset with 50B tokens", "Improvement of 11.51% on MMLU and CMMLU benchmarks"], "limitations": "", "keywords": ["large language models", "knowledge points", "question answering"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2508.01553", "pdf": "https://arxiv.org/pdf/2508.01553.pdf", "abs": "https://arxiv.org/abs/2508.01553", "title": "How Many Times Do People Usually Experience Different Kinds of Stressors Each Day?", "authors": ["Sameer Neupane", "Mithun Saha", "David M. Almeida", "Santosh Kumar"], "categories": ["cs.HC"], "comment": "In Companion of the 2025 ACM International Joint Conference on\n  Pervasive and Ubiquitous Computing (UbiComp Companion '25)", "summary": "Understanding how frequently people experience different kinds of daily\nstressors is crucial for interpreting stress exposure and informing mental\nhealth care. But it can't be directly estimated from current assessment\nmethods, such as diaries, end-of-day interviews, and ecological momentary\nassessments (EMA), that use sparse sampling to limit participant burden, and a\nstructured response format for uniformity. In this paper, we utilize stressor\ndata collected in a 100-day field study with 68 participants that adopted\nwearable-triggered prompts and a freeform format to solicit stressors soon\nafter they occurred, but limited its prompts to a small subset to keep the\nburden low. We develop asymptotic models to estimate the latent frequency of\ndifferent kinds of real-life stressors that address sample sparsity and\nsampling bias. We find that people experience 5.39 stressors per day, on\naverage. The top three are related to work (1.76/day), health (0.59/day), and\ntransportation (0.55/day). These estimates offer a principled benchmark for\ninterpreting individual stressor loads. They can also inform mental health care\ntreatments and interventions by establishing population-level baselines.", "AI": {"tldr": "This paper develops models to estimate the frequency of daily stressors using data from a wearable-triggered study, revealing an average of 5.39 stressors per person daily.", "motivation": "To improve estimation of daily stressor frequencies for better mental health care insights compared to conventional assessment methods.", "method": "The study analyzed stressor data from 68 participants over 100 days, using wearable-triggered prompts and asymptotic models to address sample sparsity.", "result": "Participants reported an average of 5.39 stressors per day, with the most common being work-related (1.76), health-related (0.59), and transportation-related (0.55).", "conclusion": "The findings provide a benchmark for understanding individual stress loads and can guide mental health treatment and interventions.", "key_contributions": ["Development of asymptotic models for stressor frequency estimation", "Use of wearable technology for real-time stressor reporting", "Identification of key stressor categories and their frequencies"], "limitations": "", "keywords": ["stressors", "mental health", "wearable technology", "data analysis", "health informatics"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.01326", "pdf": "https://arxiv.org/pdf/2508.01326.pdf", "abs": "https://arxiv.org/abs/2508.01326", "title": "Large-Scale Diverse Synthesis for Mid-Training", "authors": ["Xuemiao Zhang", "Chengying Tu", "Can Ren", "Rongxiang Weng", "Hongfei Yan", "Jingang Wang", "Xunliang Cai"], "categories": ["cs.CL"], "comment": null, "summary": "The scarcity of high-quality, knowledge-intensive training data hinders the\ndevelopment of large language models (LLMs), as traditional corpora provide\nlimited information. Previous studies have synthesized and integrated\ncorpora-dependent question-answering (QA) data to improve model performance but\nface challenges in QA data scalability and knowledge diversity, particularly in\ncross-domain contexts. Furthermore, leveraging our designed discipline and\ndifficulty annotation system, we probe model deficiencies in STEM disciplines\nand high-difficulty data. To overcome these limitations, we propose a novel\ndiversified pipeline to synthesize BoostQA, a 100B-token large-scale QA\ndataset. Our synthesis framework: (1) curates seed data from heterogeneous\nsources; (2) utilizes DeepSeek-R1 to implement STEM-focused multi-grade\nsynthesis to boost data diversity and high-difficulty synthesis to mitigate\ndifficulty degradation; (3) refines answers via DeepSeek-V3 to improve output\nquality. We utilize BoostQA in mid-training, a mid-stage between pre-training\nand post-training, to optimize domain-specific knowledge acquisition and\nenhance data quality. Our method enables Llama-3 8B, mid-trained on a 40B-token\ndataset, to achieve an average improvement of $\\mathbf{12.74\\%}$ on MMLU and\nCMMLU and establish SOTA average performance across 12 benchmarks. BoostQA also\ndemonstrates robust scalability, with performance consistently improving as\nmodel size, data volume, and initial FLOPs scale.", "AI": {"tldr": "This paper introduces BoostQA, a large-scale QA dataset designed to improve model performance in cross-domain contexts by synthesizing diverse and high-difficulty training data.", "motivation": "The scarcity of high-quality training data hampers the development of LLMs, particularly in cross-domain scenarios, necessitating improved QA data integration.", "method": "The novel BoostQA dataset is created through a diversified synthesis pipeline that curates seed data from various sources, employs a multi-grade synthesis focused on STEM disciplines, and refines answers to enhance quality.", "result": "Llama-3 8B, mid-trained on BoostQA, achieves an average performance improvement of 12.74% on MMLU and CMMLU benchmarks, with state-of-the-art results across 12 assessments.", "conclusion": "BoostQA contributes significantly to data diversity and quality, enhancing model training efficacy and demonstrating scalability as model and data sizes increase.", "key_contributions": ["Introduction of BoostQA, a 100B-token large-scale QA dataset.", "Implementation of a multi-grade synthesis approach focused on STEM disciplines.", "Demonstration of a significant performance boost in Llama-3 models using BoostQA."], "limitations": "", "keywords": ["Large language models", "Quality assurance data", "Synthesis framework", "DeepSeek", "STEM disciplines"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.01743", "pdf": "https://arxiv.org/pdf/2508.01743.pdf", "abs": "https://arxiv.org/abs/2508.01743", "title": "Examining the Effects of Human-Likeness of Avatars on Emotion Perception and Emotion Elicitation", "authors": ["Shiyao Zhang", "Omar Faruk", "Robert Porzel", "Dennis Küster", "Tanja Schultz", "Hui Liu"], "categories": ["cs.HC"], "comment": "\\c{opyright} 20XX IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "An increasing number of online interaction settings now provide the\npossibility to visually represent oneself via an animated avatar instead of a\nvideo stream. Benefits include protecting the communicator's privacy while\nstill providing a means to express their individuality. In consequence, there\nhas been a surge in means for avatar-based personalization, ranging from\nclassic human representations to animals, food items, and more. However, using\navatars also has drawbacks. Depending on the human-likeness of the avatar and\nthe corresponding disparities between the avatar and the original expresser,\navatars may elicit discomfort or even hinder effective nonverbal communication\nby distorting emotion perception. This study examines the relationship between\nthe human-likeness of virtual avatars and emotion perception for Ekman's six\n\"basic emotions\". Research reveals that avatars with varying degrees of\nhuman-likeness have distinct effects on emotion perception. High human-likeness\navatars, such as human avatars, tend to elicit more negative emotional\nresponses from users, a phenomenon that is consistent with the concept of\nUncanny Valley in aesthetics, which suggests that closely resembling humans can\nprovoke negative emotional responses. Conversely, a raccoon avatar and a shark\navatar, known as cuteness, which exhibit moderate human similarity in this\nstudy, demonstrate a positive influence on emotion perception. Our initial\nresults suggest that the human-likeness of avatars is an important factor for\nemotion perception. The results from the follow-up study further suggest that\nthe cuteness of avatars and their natural facial status may also play a\nsignificant role in emotion perception and elicitation. We discuss practical\nimplications for strategically conveying specific human behavioral messages\nthrough avatars in multiple applications, such as business and counseling.", "AI": {"tldr": "This study explores how the human-likeness of avatars affects emotion perception, revealing that while highly humanoid avatars can elicit negative emotional responses, avatars with moderate human similarity can enhance positive perceptions of emotions.", "motivation": "With the rise of avatar-based interactions online, understanding how avatar human-likeness influences emotional communication is increasingly relevant for effective interactions across various applications.", "method": "The study examines the effects of different degrees of human-likeness in avatars on emotion perception, focusing on Ekman's six basic emotions.", "result": "High human-likeness avatars elicit negative emotional responses consistent with the Uncanny Valley effect, while moderate human-likeness avatars, like a raccoon and a shark, positively influence emotion perception.", "conclusion": "Human-likeness significantly affects emotion perception, and the cuteness of avatars also plays a crucial role in eliciting emotions, impacting their effectiveness in various applications.", "key_contributions": ["Identifies the link between avatar human-likeness and emotion perception.", "Demonstrates the Uncanny Valley effect in avatar interactions.", "Suggests practical implications for the use of avatars in business and counseling."], "limitations": "Does not address long-term effects of avatar use on emotional communication.", "keywords": ["avatar", "human-likeness", "emotion perception", "Uncanny Valley", "interaction design"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.01370", "pdf": "https://arxiv.org/pdf/2508.01370.pdf", "abs": "https://arxiv.org/abs/2508.01370", "title": "MaRGen: Multi-Agent LLM Approach for Self-Directed Market Research and Analysis", "authors": ["Roman Koshkin", "Pengyu Dai", "Nozomi Fujikawa", "Masahito Togami", "Marco Visentini-Scarzanella"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "We present an autonomous framework that leverages Large Language Models\n(LLMs) to automate end-to-end business analysis and market report generation.\nAt its core, the system employs specialized agents - Researcher, Reviewer,\nWriter, and Retriever - that collaborate to analyze data and produce\ncomprehensive reports. These agents learn from real professional consultants'\npresentation materials at Amazon through in-context learning to replicate\nprofessional analytical methodologies. The framework executes a multi-step\nprocess: querying databases, analyzing data, generating insights, creating\nvisualizations, and composing market reports. We also introduce a novel\nLLM-based evaluation system for assessing report quality, which shows alignment\nwith expert human evaluations. Building on these evaluations, we implement an\niterative improvement mechanism that optimizes report quality through automated\nreview cycles. Experimental results show that report quality can be improved by\nboth automated review cycles and consultants' unstructured knowledge. In\nexperimental validation, our framework generates detailed 6-page reports in 7\nminutes at a cost of approximately \\$1. Our work could be an important step to\nautomatically create affordable market insights.", "AI": {"tldr": "An autonomous framework using LLMs for automated business analysis and market report generation, employing specialized agents and a novel evaluation system.", "motivation": "To automate the process of business analysis and market report generation, making it more efficient and affordable.", "method": "The framework uses specialized agents (Researcher, Reviewer, Writer, Retriever) to query databases, analyze data, generate insights, create visualizations, and compose reports, while employing an LLM-based evaluation system for report quality.", "result": "The system can generate detailed 6-page reports in 7 minutes at a cost of roughly $1, with improvements in report quality through automated reviews and expert knowledge.", "conclusion": "The proposed framework can significantly automate and lower the costs associated with generating market reports, potentially disrupting traditional business analysis methods.", "key_contributions": ["Development of an autonomous framework leveraging LLMs for business analysis", "Introduction of a novel LLM-based evaluation system for report quality", "Implementation of an iterative improvement mechanism for report quality optimization"], "limitations": "", "keywords": ["Large Language Models", "Business Analysis", "Market Report Generation", "Automated Evaluation", "Iterative Improvement"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.01765", "pdf": "https://arxiv.org/pdf/2508.01765.pdf", "abs": "https://arxiv.org/abs/2508.01765", "title": "HeadZoom: Hands-Free Zooming and Panning for 2D Image Navigation Using Head Motion", "authors": ["Kaining Zhang", "Catarina Moreira", "Pedro Belchior", "Gun Lee", "Mark Billinghurst", "Joaquim Jorge"], "categories": ["cs.HC", "cs.ET"], "comment": null, "summary": "We introduce \\textit{HeadZoom}, a hands-free interaction technique for\nnavigating two-dimensional visual content using head movements. The system\nenables fluid zooming and panning by only using real-time head tracking. It\nsupports natural control in applications such as map exploration, radiograph\ninspection, and image browsing, particularly where physical interaction is\nlimited. We evaluated HeadZoom in a within-subjects user study comparing three\ninteraction techniques-Static, Tilt Zoom, and Parallel Zoom-across spatial,\nerror, and subjective metrics. Results show that Parallel Zoom significantly\nreduced total head movement compared to Static and Tilt modes. Users reported\nsignificantly lower perceived exertion for Parallel Zoom, confirming its\nsuitability for prolonged or precision-based tasks. By minimising movement\ndemands while maintaining task effectiveness, HeadZoom advances the design of\nhead-based 2D interaction in VR, creating new opportunities for immersive,\naccessible, and hands-free systems for image exploration.", "AI": {"tldr": "HeadZoom is a hands-free interaction technique that uses head movements for fluid navigation of 2D visual content, evaluated in a user study.", "motivation": "To improve hands-free navigation of 2D visual content in applications where physical interaction is limited by using head movements.", "method": "A within-subjects user study comparing three interaction techniques: Static, Tilt Zoom, and Parallel Zoom across various metrics.", "result": "Parallel Zoom significantly reduced head movement and users had lower perceived exertion compared to Static and Tilt modes.", "conclusion": "HeadZoom enhances head-based 2D interaction in VR, enabling immersive and accessible systems for image exploration.", "key_contributions": ["Introduction of a new hands-free interaction technique (HeadZoom)", "Empirical evaluation of interaction techniques in a user study", "Demonstration of reduced head movement and lower perceived exertion with Parallel Zoom"], "limitations": "", "keywords": ["HeadZoom", "hands-free interaction", "2D navigation", "VR", "user study"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2508.01401", "pdf": "https://arxiv.org/pdf/2508.01401.pdf", "abs": "https://arxiv.org/abs/2508.01401", "title": "MedSynth: Realistic, Synthetic Medical Dialogue-Note Pairs", "authors": ["Ahmad Rezaie Mianroodi", "Amirali Rezaie", "Niko Grisel Todorov", "Cyril Rakovski", "Frank Rudzicz"], "categories": ["cs.CL", "cs.AI"], "comment": "7 pages excluding references and appendices", "summary": "Physicians spend significant time documenting clinical encounters, a burden\nthat contributes to professional burnout. To address this, robust automation\ntools for medical documentation are crucial. We introduce MedSynth -- a novel\ndataset of synthetic medical dialogues and notes designed to advance the\nDialogue-to-Note (Dial-2-Note) and Note-to-Dialogue (Note-2-Dial) tasks.\nInformed by an extensive analysis of disease distributions, this dataset\nincludes over 10,000 dialogue-note pairs covering over 2000 ICD-10 codes. We\ndemonstrate that our dataset markedly enhances the performance of models in\ngenerating medical notes from dialogues, and dialogues from medical notes. The\ndataset provides a valuable resource in a field where open-access,\nprivacy-compliant, and diverse training data are scarce. Code is available at\nhttps://github.com/ahmadrezarm/MedSynth/tree/main and the dataset is available\nat https://huggingface.co/datasets/Ahmad0067/MedSynth.", "AI": {"tldr": "MedSynth introduces a dataset of synthetic medical dialogues and notes to improve the automation of medical documentation.", "motivation": "To reduce the documentation burden on physicians, which contributes to burnout, by providing robust automation tools for medical documentation.", "method": "Creation of MedSynth, a dataset containing over 10,000 dialogue-note pairs informed by disease distributions and spanning over 2000 ICD-10 codes, aimed at enhancing Dial-2-Note and Note-2-Dial tasks.", "result": "The dataset significantly improves the performance of models in generating medical notes from dialogues and vice versa.", "conclusion": "MedSynth serves as a valuable resource for advancing medical documentation processes, offering open-access and privacy-compliant training data.", "key_contributions": ["Introduction of a novel dataset for medical dialogue and notes generation.", "Enhancement of model performance for Dialogue-to-Note and Note-to-Dialogue tasks.", "Provides critical training data in a field lacking diverse, open-access resources."], "limitations": "", "keywords": ["medical dialogue", "synthetic dataset", "medical documentation", "machine learning", "health informatics"], "importance_score": 8, "read_time_minutes": 7}}
{"id": "2508.01789", "pdf": "https://arxiv.org/pdf/2508.01789.pdf", "abs": "https://arxiv.org/abs/2508.01789", "title": "Sonify Anything: Towards Context-Aware Sonic Interactions in AR", "authors": ["Laura Schütz", "Sasan Matinfar", "Ulrich Eck", "Daniel Roth", "Nassir Navab"], "categories": ["cs.HC", "cs.CV", "cs.SD", "eess.AS", "H.5.5; H.5.2; H.5.1; I.3.5"], "comment": null, "summary": "In Augmented Reality (AR), virtual objects interact with real objects.\nHowever, the lack of physicality of virtual objects leads to the absence of\nnatural sonic interactions. When virtual and real objects collide, either no\nsound or a generic sound is played. Both lead to an incongruent multisensory\nexperience, reducing interaction and object realism. Unlike in Virtual Reality\n(VR) and games, where predefined scenes and interactions allow for the playback\nof pre-recorded sound samples, AR requires real-time sound synthesis that\ndynamically adapts to novel contexts and objects to provide audiovisual\ncongruence during interaction. To enhance real-virtual object interactions in\nAR, we propose a framework for context-aware sounds using methods from computer\nvision to recognize and segment the materials of real objects. The material's\nphysical properties and the impact dynamics of the interaction are used to\ngenerate material-based sounds in real-time using physical modelling synthesis.\nIn a user study with 24 participants, we compared our congruent material-based\nsounds to a generic sound effect, mirroring the current standard of\nnon-context-aware sounds in AR applications. The results showed that\nmaterial-based sounds led to significantly more realistic sonic interactions.\nMaterial-based sounds also enabled participants to distinguish visually similar\nmaterials with significantly greater accuracy and confidence. These findings\nshow that context-aware, material-based sonic interactions in AR foster a\nstronger sense of realism and enhance our perception of real-world\nsurroundings.", "AI": {"tldr": "This paper proposes a framework for context-aware sound in Augmented Reality (AR) that uses computer vision methods to recognize real objects' materials, leading to more realistic sonic interactions.", "motivation": "AR lacks natural sonic interactions due to the absence of physicality in virtual objects, resulting in incongruent multisensory experiences.", "method": "The framework utilizes computer vision to recognize and segment materials of real objects and generates real-time, material-based sounds using physical modelling synthesis based on interaction dynamics.", "result": "User study results show that context-aware, material-based sounds significantly improve the realism of sonic interactions compared to generic sound effects, enhancing participants' ability to distinguish similar materials accurately and confidently.", "conclusion": "Context-aware, material-based sonic interactions in AR foster a stronger sense of realism, enhancing users' perception of their real-world surroundings.", "key_contributions": ["Introduction of a framework for generating context-aware sounds in AR", "Demonstration of improved realism in sonic interactions through material-based sounds", "Improved accuracy in distinguishing visually similar materials through sound differentiation"], "limitations": "", "keywords": ["Augmented Reality", "sound synthesis", "context-aware", "material recognition", "sonic interactions"], "importance_score": 7, "read_time_minutes": 12}}
{"id": "2508.01411", "pdf": "https://arxiv.org/pdf/2508.01411.pdf", "abs": "https://arxiv.org/abs/2508.01411", "title": "ArzEn-MultiGenre: An aligned parallel dataset of Egyptian Arabic song lyrics, novels, and subtitles, with English translations", "authors": ["Rania Al-Sabbagh"], "categories": ["cs.CL"], "comment": null, "summary": "ArzEn-MultiGenre is a parallel dataset of Egyptian Arabic song lyrics,\nnovels, and TV show subtitles that are manually translated and aligned with\ntheir English counterparts. The dataset contains 25,557 segment pairs that can\nbe used to benchmark new machine translation models, fine-tune large language\nmodels in few-shot settings, and adapt commercial machine translation\napplications such as Google Translate. Additionally, the dataset is a valuable\nresource for research in various disciplines, including translation studies,\ncross-linguistic analysis, and lexical semantics. The dataset can also serve\npedagogical purposes by training translation students and aid professional\ntranslators as a translation memory. The contributions are twofold: first, the\ndataset features textual genres not found in existing parallel Egyptian Arabic\nand English datasets, and second, it is a gold-standard dataset that has been\ntranslated and aligned by human experts.", "AI": {"tldr": "ArzEn-MultiGenre is a parallel dataset of Egyptian Arabic song lyrics, novels, and TV show subtitles, aligned with English translations, aimed at enhancing machine translation and aiding translation studies.", "motivation": "To provide a rich resource for benchmarking and improving machine translation systems, especially for Egyptian Arabic.", "method": "The dataset consists of 25,557 segment pairs of Egyptian Arabic and English translations, manually translated and aligned by experts.", "result": "The dataset is intended for use in machine translation model benchmarking, fine-tuning language models, and aiding in translation studies.", "conclusion": "ArzEn-MultiGenre serves not only as a tool for machine translation but also as a pedagogical resource for translation students and professionals.", "key_contributions": ["Features unique textual genres not available in existing datasets", "Gold-standard quality through expert translation and alignment"], "limitations": "", "keywords": ["Egyptian Arabic", "machine translation", "parallel dataset", "language models", "translation studies"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2508.01837", "pdf": "https://arxiv.org/pdf/2508.01837.pdf", "abs": "https://arxiv.org/abs/2508.01837", "title": "When not to help: planning for lasting human-AI collaboration", "authors": ["Mark Steyvers", "Lukas Mayer"], "categories": ["cs.HC"], "comment": null, "summary": "AI systems and technologies that can interact with humans in real time face a\ncommunication dilemma: when to offer assistance and how frequently. Overly\nfrequent or contextually redundant assistance can cause users to disengage,\nundermining the long-term benefits of AI assistance. We introduce a cognitive\nmodeling framework based on Partially Observable Markov Decision Processes\n(POMDPs) that addresses this timing challenge by inferring a user's latent\ncognitive state related to AI engagement over time. Additionally, our framework\nincorporates reasoning about the long-term effects of AI assistance, explicitly\naiming to avoid actions that could lead the human user to disengage or\ndeactivate the AI. A key component of our approach is counterfactual reasoning:\nat each time step, the AI considers how well the user would perform\nindependently and weighs the potential boost in performance against the risk of\ndiminishing engagement with the AI. Through simulations, we show that this\nadaptive strategy significantly outperforms baseline policies in which\nassistance is always provided or never provided. Our results highlight the\nimportance of balancing short-term decision accuracy with sustained user\nengagement, showing how communication strategies can be optimized to avoid\nalert fatigue while preserving the user's receptiveness to AI guidance.", "AI": {"tldr": "This paper presents a cognitive modeling framework that uses POMDPs to optimize AI assistance timing and reduce user disengagement.", "motivation": "AI systems need to find the right balance in offering assistance to keep users engaged without causing alert fatigue.", "method": "The framework is based on Partially Observable Markov Decision Processes (POMDPs) and includes counterfactual reasoning to assess user performance and engagement over time.", "result": "Simulations indicate that the framework significantly outperforms consistent assistance policies, improving both accuracy and user engagement.", "conclusion": "Effective communication strategies in AI can enhance long-term user interaction by balancing immediate assistance with the risk of engagement loss.", "key_contributions": ["Introduction of a POMDP-based cognitive model for AI engagement", "Incorporation of counterfactual reasoning for user performance assessment", "Empirical demonstration of improved engagement through adaptive assistance strategies"], "limitations": "", "keywords": ["AI Assistance", "User Engagement", "POMDP", "Cognitive Modeling", "Counterfactual Reasoning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.01412", "pdf": "https://arxiv.org/pdf/2508.01412.pdf", "abs": "https://arxiv.org/abs/2508.01412", "title": "Discovering Bias Associations through Open-Ended LLM Generations", "authors": ["Jinhao Pan", "Chahat Raj", "Ziwei Zhu"], "categories": ["cs.CL"], "comment": null, "summary": "Social biases embedded in Large Language Models (LLMs) raise critical\nconcerns, resulting in representational harms -- unfair or distorted portrayals\nof demographic groups -- that may be expressed in subtle ways through generated\nlanguage. Existing evaluation methods often depend on predefined\nidentity-concept associations, limiting their ability to surface new or\nunexpected forms of bias. In this work, we present the Bias Association\nDiscovery Framework (BADF), a systematic approach for extracting both known and\npreviously unrecognized associations between demographic identities and\ndescriptive concepts from open-ended LLM outputs. Through comprehensive\nexperiments spanning multiple models and diverse real-world contexts, BADF\nenables robust mapping and analysis of the varied concepts that characterize\ndemographic identities. Our findings advance the understanding of biases in\nopen-ended generation and provide a scalable tool for identifying and analyzing\nbias associations in LLMs. Data, code, and results are available at\nhttps://github.com/JP-25/Discover-Open-Ended-Generation", "AI": {"tldr": "This paper presents the Bias Association Discovery Framework (BADF) to identify and analyze biases in Large Language Models (LLMs) by extracting associations between demographic identities and concepts from LLM outputs.", "motivation": "There are critical concerns over social biases in LLMs, leading to representational harms and distorted portrayals of demographic groups.", "method": "The Bias Association Discovery Framework (BADF) systematically extracts known and unknown associations from open-ended LLM outputs through comprehensive experiments across multiple models and contexts.", "result": "BADF enables robust mapping of various concepts that characterize demographic identities, enhancing the understanding of biases in LLM generation.", "conclusion": "The findings provide valuable insights into biases in LLMs and present a scalable tool for future analysis of bias associations.", "key_contributions": ["Development of the BADF framework for bias analysis in LLMs.", "Empirical results revealing the landscape of bias associations in LLM outputs.", "Accessibility of data, code, and results for further research."], "limitations": "The framework's effectiveness may depend on the diversity of LLMs and contexts tested.", "keywords": ["bias", "large language models", "human-computer interaction", "demographic identities", "text generation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.01850", "pdf": "https://arxiv.org/pdf/2508.01850.pdf", "abs": "https://arxiv.org/abs/2508.01850", "title": "ChairPose: Pressure-based Chair Morphology Grounded Sitting Pose Estimation through Simulation-Assisted Training", "authors": ["Lala Shakti Swarup Ray", "Vitor Fortes Rey", "Bo Zhou", "Paul Lukowicz", "Sungho Suh"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Prolonged seated activity is increasingly common in modern environments,\nraising concerns around musculoskeletal health, ergonomics, and the design of\nresponsive interactive systems. Existing posture sensing methods such as\nvision-based or wearable approaches face limitations including occlusion,\nprivacy concerns, user discomfort, and restricted deployment flexibility. We\nintroduce ChairPose, the first full body, wearable free seated pose estimation\nsystem that relies solely on pressure sensing and operates independently of\nchair geometry. ChairPose employs a two stage generative model trained on\npressure maps captured from a thin, chair agnostic sensing mattress. Unlike\nprior approaches, our method explicitly incorporates chair morphology into the\ninference process, enabling accurate, occlusion free, and privacy preserving\npose estimation. To support generalization across diverse users and chairs, we\nintroduce a physics driven data augmentation pipeline that simulates realistic\nvariations in posture and seating conditions. Evaluated across eight users and\nfour distinct chairs, ChairPose achieves a mean per joint position error of\n89.4 mm when both the user and the chair are unseen, demonstrating robust\ngeneralization to novel real world generalizability. ChairPose expands the\ndesign space for posture aware interactive systems, with potential applications\nin ergonomics, healthcare, and adaptive user interfaces.", "AI": {"tldr": "ChairPose is a novel seated pose estimation system that uses pressure sensing to estimate user posture without the constraints of chair geometry, aiming to improve ergonomics and interactive system design.", "motivation": "Address concerns around musculoskeletal health and ergonomics caused by prolonged seated activity and limitations of existing posture sensing methods.", "method": "ChairPose utilizes a pressure sensing mattress and a two-stage generative model that integrates chair morphology into its inference process. It also features a physics-driven data augmentation pipeline for varying user postures.", "result": "ChairPose achieves a mean per joint position error of 89.4 mm in tests with unseen users and chairs, demonstrating effective generalization and accuracy in real world conditions.", "conclusion": "ChairPose shows promise for enhancing posture-aware interactive systems with applications in ergonomics, healthcare, and adaptive interfaces.", "key_contributions": ["First full body, wearable free seated pose estimation system using pressure sensing.", "Incorporation of chair morphology into posture inference for improved accuracy.", "Physics-driven data augmentation for diverse posture simulation."], "limitations": "", "keywords": ["seated pose estimation", "pressure sensing", "human-computer interaction", "healthcare", "adaptive user interfaces"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.01424", "pdf": "https://arxiv.org/pdf/2508.01424.pdf", "abs": "https://arxiv.org/abs/2508.01424", "title": "From Query to Logic: Ontology-Driven Multi-Hop Reasoning in LLMs", "authors": ["Haonan Bian", "Yutao Qi", "Rui Yang", "Yuanxi Che", "Jiaqian Wang", "Heming Xia", "Ranran Zhen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs), despite their success in question answering,\nexhibit limitations in complex multi-hop question answering (MQA) tasks that\nnecessitate non-linear, structured reasoning. This limitation stems from their\ninability to adequately capture deep conceptual relationships between entities.\nTo overcome this challenge, we present **ORACLE** (**O**ntology-driven\n**R**easoning **A**nd **C**hain for **L**ogical **E**ucidation), a\ntraining-free framework that combines LLMs' generative capabilities with the\nstructural benefits of knowledge graphs. Our approach operates through three\nstages: (1) dynamic construction of question-specific knowledge ontologies\nusing LLMs, (2) transformation of these ontologies into First-Order Logic\nreasoning chains, and (3) systematic decomposition of the original query into\nlogically coherent sub-questions. Experimental results on several standard MQA\nbenchmarks show that our framework achieves highly competitive performance,\nrivaling current state-of-the-art models like DeepSeek-R1. Detailed analyses\nfurther confirm the effectiveness of each component, while demonstrating that\nour method generates more logical and interpretable reasoning chains than\nexisting approaches.", "AI": {"tldr": "This paper introduces ORACLE, a framework that enhances multi-hop question answering by combining LLM capabilities with knowledge graphs to improve conceptual reasoning.", "motivation": "LLMs struggle with complex multi-hop question answering due to their failure to capture deep conceptual relationships between entities.", "method": "The proposed approach involves three stages: dynamic construction of question-specific knowledge ontologies using LLMs, conversion into First-Order Logic reasoning chains, and systematic decomposition of queries into sub-questions.", "result": "Experimental results demonstrate ORACLE's competitive performance on MQA benchmarks, outperforming state-of-the-art models and yielding more logical reasoning.", "conclusion": "The method not only improves performance in MQA tasks but also generates more interpretable reasoning chains compared to existing methods.", "key_contributions": ["Introduction of ORACLE framework for MQA", "Combination of LLMs with knowledge graphs", "Improvement in interpretability and logical reasoning of answers"], "limitations": "", "keywords": ["Large Language Models", "Multi-hop Question Answering", "Knowledge Graphs"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.01860", "pdf": "https://arxiv.org/pdf/2508.01860.pdf", "abs": "https://arxiv.org/abs/2508.01860", "title": "Implicit Search Intent Recognition using EEG and Eye Tracking: Novel Dataset and Cross-User Prediction", "authors": ["Mansi Sharma", "Shuang Chen", "Philipp Müller", "Maurice Rekrut", "Antonio Krüger"], "categories": ["cs.HC", "cs.CV"], "comment": null, "summary": "For machines to effectively assist humans in challenging visual search tasks,\nthey must differentiate whether a human is simply glancing into a scene\n(navigational intent) or searching for a target object (informational intent).\nPrevious research proposed combining electroencephalography (EEG) and\neye-tracking measurements to recognize such search intents implicitly, i.e.,\nwithout explicit user input. However, the applicability of these approaches to\nreal-world scenarios suffers from two key limitations. First, previous work\nused fixed search times in the informational intent condition -- a stark\ncontrast to visual search, which naturally terminates when the target is found.\nSecond, methods incorporating EEG measurements addressed prediction scenarios\nthat require ground truth training data from the target user, which is\nimpractical in many use cases. We address these limitations by making the first\npublicly available EEG and eye-tracking dataset for navigational vs.\ninformational intent recognition, where the user determines search times. We\npresent the first method for cross-user prediction of search intents from EEG\nand eye-tracking recordings and reach 84.5% accuracy in leave-one-user-out\nevaluations -- comparable to within-user prediction accuracy (85.5%) but\noffering much greater flexibility", "AI": {"tldr": "This paper presents a novel approach to recognizing human search intents in visual tasks using EEG and eye-tracking data, overcoming limitations of previous methods.", "motivation": "To improve machine assistance in visual search tasks by accurately identifying human search intents (navigational vs informational).", "method": "Introduced the first publicly available dataset for recognizing search intents based on EEG and eye-tracking measurements, and developed a cross-user prediction method.", "result": "Achieved 84.5% accuracy in predicting search intents through EEG and eye-tracking, comparable to within-user prediction accuracy.", "conclusion": "The new dataset and method provide significant flexibility and can better apply to real-world scenarios than previous approaches.", "key_contributions": ["First publicly available EEG and eye-tracking dataset for search intent recognition", "New method for cross-user prediction of search intents", "Achieved competitive accuracy compared to existing methods"], "limitations": "", "keywords": ["EEG", "eye-tracking", "search intent", "visual search", "cross-user prediction"], "importance_score": 6, "read_time_minutes": 12}}
{"id": "2508.01450", "pdf": "https://arxiv.org/pdf/2508.01450.pdf", "abs": "https://arxiv.org/abs/2508.01450", "title": "Towards Efficient Medical Reasoning with Minimal Fine-Tuning Data", "authors": ["Xinlin Zhuang", "Feilong Tang", "Haolin Yang", "Ming Hu", "Huifa Li", "Haochen Xue", "Yichen Li", "Junjun He", "Zongyuan Ge", "Ying Qian", "Imran Razzak"], "categories": ["cs.CL"], "comment": "preprint, under review", "summary": "Supervised Fine-Tuning (SFT) plays a pivotal role in adapting Large Language\nModels (LLMs) to specialized domains such as medical reasoning. However,\nexisting SFT practices often rely on unfiltered datasets that contain redundant\nand low-quality samples, leading to substantial computational costs and\nsuboptimal performance. Although existing methods attempt to alleviate this\nproblem by selecting data based on sample difficulty, defined by knowledge and\nreasoning complexity, they overlook each sample's optimization utility\nreflected in its gradient. Interestingly, we find that gradient-based influence\nalone favors easy-to-optimize samples that cause large parameter shifts but\nlack deep reasoning chains, while difficulty alone selects noisy or overly\ncomplex cases that fail to guide stable optimization. Based on this\nobservation, we propose a data selection strategy, Difficulty-Influence\nQuadrant (DIQ), which prioritizes samples in the high-difficulty-high-influence\nquadrant to balance complex clinical reasoning with substantial gradient\ninfluence, enabling efficient medical reasoning with minimal fine-tuning data.\nFurthermore, Human and LLM-as-a-judge evaluations show that DIQ-selected\nsubsets demonstrate higher data quality and generate clinical reasoning that is\nmore aligned with expert practices in differential diagnosis, safety check, and\nevidence citation, as DIQ emphasizes samples that foster expert-like reasoning\npatterns. Extensive experiments on medical reasoning benchmarks demonstrate\nthat DIQ enables models fine-tuned on only 1% of selected data to match\nfull-dataset performance, while using 10% consistently outperforms the\nbaseline, highlighting the superiority of principled data selection over\nbrute-force scaling. The code and data are available at\nhttps://github.com/mihara-bot/DIQ.", "AI": {"tldr": "The paper introduces the Difficulty-Influence Quadrant (DIQ), a novel data selection strategy for Supervised Fine-Tuning of Large Language Models (LLMs) in medical reasoning, effectively balancing sample difficulty and optimization utility.", "motivation": "To address the limitations of existing Supervised Fine-Tuning (SFT) practices that rely on unfiltered datasets, leading to high computational costs and poor performance in adapting LLMs to medical reasoning.", "method": "The proposed Difficulty-Influence Quadrant (DIQ) strategy selects data by prioritizing samples that are both high in difficulty and high in gradient influence, optimizing the fine-tuning process with fewer data points.", "result": "Models fine-tuned on 1% of DIQ-selected data match the performance of those using full datasets, while using 10% of the data consistently outperforms traditional methods, confirming the effectiveness of DIQ.", "conclusion": "DIQ enhances data quality and model performance in medical reasoning by selecting samples that facilitate expert-like reasoning patterns, demonstrating its superiority over brute-force scaling methods.", "key_contributions": ["Introduction of the Difficulty-Influence Quadrant (DIQ) strategy for data selection in medical reasoning.", "Demonstration that fine-tuning with DIQ-selected subsets achieves high performance with minimal data.", "Validation through human evaluations and experiments on medical reasoning benchmarks."], "limitations": "", "keywords": ["Supervised Fine-Tuning", "Large Language Models", "Medical Reasoning", "Data Selection", "Gradient Influence"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.01878", "pdf": "https://arxiv.org/pdf/2508.01878.pdf", "abs": "https://arxiv.org/abs/2508.01878", "title": "VidAnimator: User-Guided Stylized 3D Character Animation from Human Videos", "authors": ["Xinwu Ye", "Jun-Hsiang Yao", "Jielin Feng", "Shuhong Mei", "Xingyu Lan", "Siming Chen"], "categories": ["cs.HC"], "comment": "14 pages, 7 figures, Xinwu Ye and Jun-Hsiang Yao contributed equally\n  to this work", "summary": "With captivating visual effects, stylized 3D character animation has gained\nwidespread use in cinematic production, advertising, social media, and the\npotential development of virtual reality (VR) non-player characters (NPCs).\nHowever, animating stylized 3D characters often requires significant time and\neffort from animators. We propose a mixed-initiative framework and interactive\nsystem to enable stylized 3D characters to mimic motion in human videos. The\nframework takes a single-view human video and a stylized 3D character (the\ntarget character) as input, captures the motion of the video, and then\ntransfers the motion to the target character. In addition, it involves two\ninteraction modules for customizing the result. Accordingly, the system\nincorporates two authoring tools that empower users with intuitive\nmodification. A questionnaire study offers tangible evidence of the framework's\ncapability of generating natural stylized 3D character animations similar to\nthe motion in the video. Additionally, three case studies demonstrate the\nutility of our approach in creating diverse results.", "AI": {"tldr": "A framework to enable stylized 3D characters to mimic motion from human videos using a mixed-initiative and interactive system.", "motivation": "To reduce the time and effort required for animators to create stylized 3D character animations while enhancing realism.", "method": "The system uses a single-view human video and a stylized 3D character to capture and transfer motion, coupled with interactive modules for user customization.", "result": "The framework successfully generates natural stylized 3D character animations that resemble human motion as verified by a questionnaire study and case studies.", "conclusion": "The proposed system demonstrates significant utility in creating diverse and realistic character animations efficiently.", "key_contributions": ["Mixed-initiative framework for motion transfer to stylized 3D characters", "Interactive authoring tools for user customization", "Empirical evidence of natural animation generation from human videos"], "limitations": "", "keywords": ["3D character animation", "motion transfer", "interactive system", "human videos", "stylized animation"], "importance_score": 5, "read_time_minutes": 14}}
{"id": "2508.01473", "pdf": "https://arxiv.org/pdf/2508.01473.pdf", "abs": "https://arxiv.org/abs/2508.01473", "title": "TreeDiff: AST-Guided Code Generation with Diffusion LLMs", "authors": ["Yiming Zeng", "Jinghan Cao", "Zexin Li", "Yiming Chen", "Tao Ren", "Dawei Xiang", "Xidong Wu", "Shangqian Gao", "Tingting Yu"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in diffusion-based language models have opened new\npossibilities for controllable and bidirectional sequence generation. These\nmodels provide an alternative to traditional autoregressive approaches by\nframing text generation as an iterative denoising process. However, applying\ndiffusion models to structured domains such as source code remains a\nsignificant challenge. Programming languages differ from natural language in\nthat they follow strict syntactic and semantic rules, with hierarchical\norganization that must be preserved for correctness. Standard token-level\ncorruption techniques used during training often ignore this structure, which\nmay hinder the model's ability to learn meaningful representations of code. To\naddress this limitation, we propose a syntax-aware diffusion framework that\nincorporates structural priors from Abstract Syntax Trees (ASTs) into the\ndenoising process. Instead of masking individual tokens at random, we\nselectively corrupt syntactically meaningful code spans derived from AST\nsubtrees. This enables the model to reconstruct programs in a way that respects\ngrammatical boundaries and captures long-range dependencies. Experimental\nresults demonstrate that syntax-aware corruption significantly improves\nsyntactic correctness, reconstruction accuracy, and generalization to unseen\ncode patterns. These findings highlight the potential of incorporating\nstructural information into diffusion-based training and suggest that\nsyntax-guided denoising is a promising direction for advancing diffusion-based\nlanguage models in code generation tasks.", "AI": {"tldr": "This paper introduces a syntax-aware diffusion framework for text generation in programming by using Abstract Syntax Trees (ASTs) to improve training and reconstruction of code.", "motivation": "The work addresses challenges in applying diffusion models for sequence generation in structured domains like source code, which have strict syntactic rules.", "method": "Utilizes a syntax-aware approach, selectively corrupting syntactically meaningful code spans derived from Abstract Syntax Trees (ASTs) instead of standard token-level corruption.", "result": "Experimental results show improved syntactic correctness and enhanced reconstruction accuracy in code generation, with better generalization to unseen code patterns.", "conclusion": "Incorporating structural information from ASTs into the diffusion-based training process significantly enhances code generation tasks.", "key_contributions": ["Proposes a novel syntax-aware diffusion framework for code generation.", "Demonstrates the effectiveness of syntax-guided denoising in improving model performance.", "Shows that using ASTs allows for better preservation of grammatical boundaries in programming languages."], "limitations": "", "keywords": ["diffusion models", "syntax-aware", "Abstract Syntax Trees", "code generation", "programming"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2508.01881", "pdf": "https://arxiv.org/pdf/2508.01881.pdf", "abs": "https://arxiv.org/abs/2508.01881", "title": "Anchoring and Alignment: Data Factors in Part-to-Whole Visualization", "authors": ["Connor Bailey", "Michael Gleicher"], "categories": ["cs.HC"], "comment": "5 pages, 3 figures, IEEE Visualization conference, repository URL:\n  https://github.com/uwgraphics/PartToWhole, preregistration URL:\n  https://osf.io/e36au", "summary": "We explore the effects of data and design considerations through the example\ncase of part-to-whole data relationships. Standard part-to-whole\nrepresentations like pie charts and stacked bar charts make the relationships\nof parts to the whole explicit. Value estimation in these charts benefits from\ntwo perceptual mechanisms: anchoring, where the value is close to a reference\nvalue with an easily recognized shape, and alignment where the beginning or end\nof the shape is aligned with a marker. In an online study, we explore how data\nand design factors such as value, position, and encoding together impact these\neffects in making estimations in part-to-whole charts. The results show how\nsalient values and alignment to positions on a scale affect task performance.\nThis demonstrates the need for informed visualization design based around how\ndata properties and design factors affect perceptual mechanisms.", "AI": {"tldr": "The paper investigates how data and design factors impact value estimation in part-to-whole visualizations like pie charts and stacked bar charts, emphasizing the importance of aligning data principles with perceptual mechanisms.", "motivation": "To understand how part-to-whole representations affect value estimation and what design considerations enhance perceptual mechanisms in these visualizations.", "method": "An online study was conducted to assess the impact of data and design factors such as value, position, and encoding on users' performance in estimating values from part-to-whole charts.", "result": "The study found that salient values and alignment to scale positions significantly influence task performance in part-to-whole visualizations.", "conclusion": "Informed visualization design should be based on the interplay between data properties and design aspects, as they critically affect perceptual performance.", "key_contributions": ["Analysis of perceptual mechanisms in part-to-whole charting", "Empirical study on the effects of design factors in value estimation", "Recommendations for better visualization design based on study findings."], "limitations": "", "keywords": ["data visualization", "part-to-whole", "perceptual mechanisms", "value estimation", "chart design"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2508.01480", "pdf": "https://arxiv.org/pdf/2508.01480.pdf", "abs": "https://arxiv.org/abs/2508.01480", "title": "Harnessing Collective Intelligence of LLMs for Robust Biomedical QA: A Multi-Model Approach", "authors": ["Dimitra Panou", "Alexandros C. Dimopoulos", "Manolis Koubarakis", "Martin Reczko"], "categories": ["cs.CL"], "comment": null, "summary": "Biomedical text mining and question-answering are essential yet highly\ndemanding tasks, particularly in the face of the exponential growth of\nbiomedical literature. In this work, we present our participation in the 13th\nedition of the BioASQ challenge, which involves biomedical semantic\nquestion-answering for Task 13b and biomedical question-answering for\ndeveloping topics for the Synergy task. We deploy a selection of open-source\nlarge language models (LLMs) as retrieval-augmented generators to answer\nbiomedical questions. Various models are used to process the questions. A\nmajority voting system combines their output to determine the final answer for\nYes/No questions, while for list and factoid type questions, the union of their\nanswers in used. We evaluated 13 state-of-the-art open source LLMs, exploring\nall possible model combinations to contribute to the final answer, resulting in\ntailored LLM pipelines for each question type. Our findings provide valuable\ninsight into which combinations of LLMs consistently produce superior results\nfor specific question types. In the four rounds of the 2025 BioASQ challenge,\nour system achieved notable results: in the Synergy task, we secured 1st place\nfor ideal answers and 2nd place for exact answers in round 2, as well as two\nshared 1st places for exact answers in round 3 and 4.", "AI": {"tldr": "This paper presents a system for biomedical question-answering using LLMs, achieving high rankings in the BioASQ challenge.", "motivation": "To address the need for effective biomedical text mining and question-answering in light of increasing biomedical literature.", "method": "The authors deployed various open-source LLMs as retrieval-augmented generators, using a majority voting system for Yes/No questions and a union of answers for list and factoid questions.", "result": "The study evaluated 13 open-source LLMs, finding optimal combinations that led to high performance in the BioASQ challenge, securing top placements in multiple rounds.", "conclusion": "The results highlight the effectiveness of tailored LLM pipelines for different question types in biomedical Q&A tasks.", "key_contributions": ["Use of majority voting among LLM outputs for question answering", "Exploration of various LLM combinations for optimal results", "Achievement of high rankings in the BioASQ challenge"], "limitations": "", "keywords": ["biomedical text mining", "question-answering", "language models"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.01894", "pdf": "https://arxiv.org/pdf/2508.01894.pdf", "abs": "https://arxiv.org/abs/2508.01894", "title": "IMUCoCo: Enabling Flexible On-Body IMU Placement for Human Pose Estimation and Activity Recognition", "authors": ["Haozhe Zhou", "Riku Arakawa", "Yuvraj Agarwal", "Mayank Goel"], "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "IMUs are regularly used to sense human motion, recognize activities, and\nestimate full-body pose. Users are typically required to place sensors in\npredefined locations that are often dictated by common wearable form factors\nand the machine learning model's training process. Consequently, despite the\nincreasing number of everyday devices equipped with IMUs, the limited\nadaptability has seriously constrained the user experience to only using a few\nwell-explored device placements (e.g., wrist and ears). In this paper, we\nrethink IMU-based motion sensing by acknowledging that signals can be captured\nfrom any point on the human body. We introduce IMU over Continuous Coordinates\n(IMUCoCo), a novel framework that maps signals from a variable number of IMUs\nplaced on the body surface into a unified feature space based on their spatial\ncoordinates. These features can be plugged into downstream models for pose\nestimation and activity recognition. Our evaluations demonstrate that IMUCoCo\nsupports accurate pose estimation in a wide range of typical and atypical\nsensor placements. Overall, IMUCoCo supports significantly more flexible use of\nIMUs for motion sensing than the state-of-the-art, allowing users to place\ntheir sensors-laden devices according to their needs and preferences. The\nframework also supports the ability to change device locations depending on the\ncontext and suggests placement depending on the use case.", "AI": {"tldr": "A novel framework called IMUCoCo enables flexible placement of IMUs for motion sensing by mapping signals from any body location into a unified space for activity recognition and pose estimation.", "motivation": "To address the limitations of predefined sensor placements in IMU-based motion sensing, which restrict user experience and adaptability across various device configurations.", "method": "IMUCoCo maps signals from a variable number of IMUs located anywhere on the human body into a unified feature space based on their spatial coordinates.", "result": "IMUCoCo demonstrates accurate pose estimation from diverse sensor placements, enhancing flexibility compared to traditional methods.", "conclusion": "The IMUCoCo framework allows for more customizable and context-dependent use of IMUs, potentially improving user experience and adaptability in motion sensing applications.", "key_contributions": ["Introduction of a novel framework for flexible IMU placement.", "Demonstration of accurate pose estimation across various sensor placements.", "Support for context-aware sensor placement suggestions."], "limitations": "", "keywords": ["IMU", "motion sensing", "pose estimation", "activity recognition", "human body"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.01486", "pdf": "https://arxiv.org/pdf/2508.01486.pdf", "abs": "https://arxiv.org/abs/2508.01486", "title": "TeSent: A Benchmark Dataset for Fairness-aware Explainable Sentiment Classification in Telugu", "authors": ["Vallabhaneni Raj Kumar", "Ashwin S", "Supriya Manna", "Niladri Sett", "Cheedella V S N M S Hema Harshitha", "Kurakula Harshitha", "Anand Kumar Sharma", "Basina Deepakraj", "Tanuj Sarkar", "Bondada Navaneeth Krishna", "Samanthapudi Shakeer"], "categories": ["cs.CL"], "comment": "work under review", "summary": "In the Indian subcontinent, Telugu, one of India's six classical languages,\nis the most widely spoken Dravidian Language. Despite its 96 million speaker\nbase worldwide, Telugu remains underrepresented in the global NLP and Machine\nLearning landscape, mainly due to lack of high-quality annotated resources.\nThis work introduces TeSent, a comprehensive benchmark dataset for sentiment\nclassification, a key text classification problem, in Telugu. TeSent not only\nprovides ground truth labels for the sentences, but also supplements with\nprovisions for evaluating explainability and fairness, two critical\nrequirements in modern-day machine learning tasks. We scraped Telugu texts\ncovering multiple domains from various social media platforms, news websites\nand web-blogs to preprocess and generate 26,150 sentences, and developed a\ncustom-built annotation platform and a carefully crafted annotation protocol\nfor collecting the ground truth labels along with their human-annotated\nrationales. We then fine-tuned several SOTA pre-trained models in two ways:\nwith rationales, and without rationales. Further, we provide a detailed\nplausibility and faithfulness evaluation suite, which exploits the rationales,\nfor six widely used post-hoc explainers applied on the trained models. Lastly,\nwe curate TeEEC, Equity Evaluation Corpus in Telugu, a corpus to evaluate\nfairness of Telugu sentiment and emotion related NLP tasks, and provide a\nfairness evaluation suite for the trained classifier models. Our experimental\nresults suggest that training with rationales may improve model accuracy,\nreduce bias in models, and make the explainers' output more aligned to human\nreasoning.", "AI": {"tldr": "Introduction of TeSent, a benchmark dataset for sentiment classification in Telugu, providing explainability and fairness evaluations.", "motivation": "Address the underrepresentation of Telugu in NLP and Machine Learning by providing a high-quality annotated dataset.", "method": "Scraped Telugu texts from social media, news, and blogs to create a dataset of 26,150 sentences. Developed an annotation platform for ground truth labeling and rationales. Fine-tuned state-of-the-art models with and without human-annotated rationales.", "result": "Training with rationales improved model accuracy and fairness, and enhanced explainability of outputs.", "conclusion": "TeSent and TeEEC facilitate better NLP model training for Telugu, focusing on explainability and fairness in sentiment classification tasks.", "key_contributions": ["TeSent dataset for Telugu sentiment classification", "Equity Evaluation Corpus (TeEEC) for fairness evaluation", "Evaluation suite for explainability using rationales"], "limitations": "Focuses only on sentiment classification and may not cover all NLP tasks in Telugu.", "keywords": ["Telugu", "NLP", "Machine Learning", "sentiment classification", "explainability"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.01906", "pdf": "https://arxiv.org/pdf/2508.01906.pdf", "abs": "https://arxiv.org/abs/2508.01906", "title": "Effect of AI Performance, Risk Perception, and Trust on Human Dependence in Deepfake Detection AI system", "authors": ["Yingfan Zhou", "Ester Chen", "Manasa Pisipati", "Aiping Xiong", "Sarah Rajtmajer"], "categories": ["cs.HC"], "comment": null, "summary": "Synthetic images, audio, and video can now be generated and edited by\nArtificial Intelligence (AI). In particular, the malicious use of synthetic\ndata has raised concerns about potential harms to cybersecurity, personal\nprivacy, and public trust. Although AI-based detection tools exist to help\nidentify synthetic content, their limitations often lead to user mistrust and\nconfusion between real and fake content. This study examines the role of AI\nperformance in influencing human trust and decision making in synthetic data\nidentification. Through an online human subject experiment involving 400\nparticipants, we examined how varying AI performance impacts human trust and\ndependence on AI in deepfake detection. Our findings indicate how participants\ncalibrate their dependence on AI based on their perceived risk and the\nprediction results provided by AI. These insights contribute to the development\nof transparent and explainable AI systems that better support everyday users in\nmitigating the harms of synthetic media.", "AI": {"tldr": "The study investigates how AI performance affects human trust and decision-making in identifying synthetic data, particularly in deepfake detection.", "motivation": "To address the growing concerns about the malicious use of synthetic data and the trust issues arising from AI detection tools.", "method": "An online experiment with 400 participants was conducted to assess how different levels of AI performance impact human trust in deepfake detection.", "result": "Findings show that participants adjust their reliance on AI based on perceived risk and AI prediction results, indicating a complex relationship between AI performance and trust.", "conclusion": "Improving the transparency and explainability of AI systems can enhance user support in mitigating risks associated with synthetic media.", "key_contributions": ["Investigates the interplay between AI performance and human trust in deepfake detection.", "Provides empirical data on user behavior in risk assessment related to AI predictions.", "Offers insights for designing more transparent and user-friendly AI systems."], "limitations": "The study's findings may be context-specific and may not generalize to all forms of synthetic data.", "keywords": ["synthetic data", "AI trust", "deepfake detection", "human decision making", "explainable AI"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.01491", "pdf": "https://arxiv.org/pdf/2508.01491.pdf", "abs": "https://arxiv.org/abs/2508.01491", "title": "The Homogenizing Effect of Large Language Models on Human Expression and Thought", "authors": ["Zhivar Sourati", "Alireza S. Ziabari", "Morteza Dehghani"], "categories": ["cs.CL"], "comment": null, "summary": "Cognitive diversity, reflected in variations of language, perspective, and\nreasoning, is essential to creativity and collective intelligence. This\ndiversity is rich and grounded in culture, history, and individual experience.\nYet as large language models (LLMs) become deeply embedded in people's lives,\nthey risk standardizing language and reasoning. This Review synthesizes\nevidence across linguistics, cognitive, and computer science to show how LLMs\nreflect and reinforce dominant styles while marginalizing alternative voices\nand reasoning strategies. We examine how their design and widespread use\ncontribute to this effect by mirroring patterns in their training data and\namplifying convergence as all people increasingly rely on the same models\nacross contexts. Unchecked, this homogenization risks flattening the cognitive\nlandscapes that drive collective intelligence and adaptability.", "AI": {"tldr": "This review discusses the impact of large language models (LLMs) on cognitive diversity, emphasizing how their standardization of language and reasoning can marginalize alternative perspectives and decrease collective intelligence.", "motivation": "The integration of LLMs in daily life prompts concerns about their impact on cognitive diversity and creativity, essential for collective intelligence.", "method": "The authors synthesize evidence from linguistics, cognitive science, and computer science to analyze the effects of LLMs on language and reasoning diversity.", "result": "LLMs reinforce dominant styles of language and reasoning, marginalizing alternative voices and cognitive strategies, leading to a risk of homogenization in cognitive landscapes.", "conclusion": "Without addressing the risks posed by LLMs, society may face a decline in cognitive diversity, which is critical for adaptability and creativity in collective intelligence.", "key_contributions": ["Synthesis of evidence across multiple fields on the impact of LLMs on cognitive diversity.", "Identification of how LLMs reinforce dominant cognitive styles while marginalizing alternative perspectives.", "Discussion of the societal implications of LLM standardization on creativity and adaptability."], "limitations": "", "keywords": ["cognitive diversity", "large language models", "collective intelligence", "language standardization", "creativity"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.02075", "pdf": "https://arxiv.org/pdf/2508.02075.pdf", "abs": "https://arxiv.org/abs/2508.02075", "title": "Human Capital Visualization using Speech Amount during Meetings", "authors": ["Ekai Hashimoto", "Takeshi Mizumoto", "Kohei Nagira", "Shun Shiramatsu"], "categories": ["cs.HC", "cs.CL", "cs.CY"], "comment": "This paper has been accepted for presentation at the 26th Annual\n  Meeting of the Special Interest Group on Discourse and Dialogue(SIGDIAL\n  2025). It represents the author's version of the work", "summary": "In recent years, many companies have recognized the importance of human\nresources and are investing in human capital to revitalize their organizations\nand enhance internal communication, thereby fostering innovation. However,\nconventional quantification methods have mainly focused on readily measurable\nindicators without addressing the fundamental role of conversations in human\ncapital. This study focuses on routine meetings and proposes strategies to\nvisualize human capital by analyzing speech amount during these meetings. We\nemploy conversation visualization technology, which operates effectively, to\nquantify speech. We then measure differences in speech amount by attributes\nsuch as gender and job post, changes in speech amount depending on whether\ncertain participants are present, and correlations between speech amount and\ncontinuous attributes. To verify the effectiveness of our proposed methods, we\nanalyzed speech amounts by departmental affiliation during weekly meetings at\nsmall to medium enterprises.", "AI": {"tldr": "This study proposes strategies for visualizing human capital by quantifying speech amounts in routine meetings, highlighting the importance of conversations in enhancing organizational communication and fostering innovation.", "motivation": "The paper addresses the inadequacy of conventional quantification methods that only focus on measurable indicators, neglecting the critical role of conversations in human capital.", "method": "The authors employed conversation visualization technology to quantify speech during routine meetings, analyzing differences in speech amounts based on gender, job post, and participant presence.", "result": "The analysis revealed significant differences in speech amounts associated with various attributes and organizational factors, indicating how communication patterns relate to human capital.", "conclusion": "The study demonstrates the potential of conversation visualization technology in enhancing the understanding of human capital within organizations.", "key_contributions": ["Introduction of conversation visualization technology for analyzing speech in meetings", "Quantification of speech differences by participant attributes", "Insight into the interplay between speech patterns and organizational dynamics"], "limitations": "", "keywords": ["Human Capital", "Speech Analysis", "Conversation Visualization", "Organizational Communication", "Discourse Analysis"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2508.01503", "pdf": "https://arxiv.org/pdf/2508.01503.pdf", "abs": "https://arxiv.org/abs/2508.01503", "title": "A Theory of Adaptive Scaffolding for LLM-Based Pedagogical Agents", "authors": ["Clayton Cohn", "Surya Rayala", "Namrata Srivastava", "Joyce Horn Fonteles", "Shruti Jain", "Xinying Luo", "Divya Mereddy", "Naveeduddin Mohammed", "Gautam Biswas"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) present new opportunities for creating\npedagogical agents that engage in meaningful dialogue to support student\nlearning. However, the current use of LLM systems like ChatGPT in classrooms\noften lacks the solid theoretical foundation found in earlier intelligent\ntutoring systems. To bridge this gap, we propose a framework that combines\nEvidence-Centered Design with Social Cognitive Theory for adaptive scaffolding\nin LLM-based agents focused on STEM+C learning. We illustrate this framework\nwith Inquizzitor, an LLM-based formative assessment agent that integrates\nhuman-AI hybrid intelligence and provides feedback grounded in cognitive\nscience principles. Our findings show that Inquizzitor delivers high-quality\nassessment and interaction aligned with core learning theories, offering\nteachers effective guidance that students value. This research underscores the\npotential for theory-driven LLM integration in education, highlighting the\nability of these systems to provide adaptive and principled instruction.", "AI": {"tldr": "The paper presents a framework for integrating large language models (LLMs) into educational contexts, emphasizing their potential for adaptive scaffolding in STEM+C learning.", "motivation": "To address the lack of theoretical foundations in using LLM systems like ChatGPT in classrooms, this research aims to improve educational outcomes through a structured framework.", "method": "The authors propose a framework that merges Evidence-Centered Design with Social Cognitive Theory, demonstrated through the development of Inquizzitor, an LLM-based formative assessment agent.", "result": "Inquizzitor provides high-quality assessments and interactive feedback aligned with core learning theories, positively influencing student learning experiences.", "conclusion": "The study demonstrates the feasibility and benefits of theory-driven integration of LLMs in educational settings, offering teachers valuable tools for adaptive instruction.", "key_contributions": ["Proposed a novel framework for LLM integration in education", "Developed Inquizzitor as a pedagogical agent for formative assessment", "Provided empirical evidence supporting the framework's effectiveness."], "limitations": "", "keywords": ["large language models", "education", "adaptive scaffolding", "STEM", "human-AI interaction"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.02133", "pdf": "https://arxiv.org/pdf/2508.02133.pdf", "abs": "https://arxiv.org/abs/2508.02133", "title": "Hierarchical MoE: Continuous Multimodal Emotion Recognition with Incomplete and Asynchronous Inputs", "authors": ["Yitong Zhu", "Lei Han", "GuanXuan Jiang", "PengYuan Zhou", "Yuyang Wang"], "categories": ["cs.HC"], "comment": null, "summary": "Multimodal emotion recognition (MER) is crucial for human-computer\ninteraction, yet real-world challenges like dynamic modality incompleteness and\nasynchrony severely limit its robustness. Existing methods often assume\nconsistently complete data or lack dynamic adaptability. To address these\nlimitations, we propose a novel Hi-MoE~(Hierarchical Mixture-of-Experts)\nframework for robust continuous emotion prediction. This framework employs a\ndual-layer expert structure. A Modality Expert Bank utilizes soft routing to\ndynamically handle missing modalities and achieve robust information fusion. A\nsubsequent Emotion Expert Bank leverages differential-attention routing to\nflexibly attend to emotional prototypes, enabling fine-grained emotion\nrepresentation. Additionally, a cross-modal alignment module explicitly\naddresses temporal shifts and semantic inconsistencies between modalities.\nExtensive experiments on benchmark datasets DEAP and DREAMER demonstrate our\nmodel's state-of-the-art performance in continuous emotion regression,\nshowcasing exceptional robustness under challenging conditions such as dynamic\nmodality absence and asynchronous sampling. This research significantly\nadvances the development of intelligent emotion systems adaptable to complex\nreal-world environments.", "AI": {"tldr": "This paper introduces a Hierarchical Mixture-of-Experts (Hi-MoE) framework for robust continuous emotion prediction in human-computer interaction, addressing challenges like modality incompleteness and asynchrony.", "motivation": "To improve robustness in multimodal emotion recognition (MER), as existing methods struggle with dynamic modality issues and lack adaptability.", "method": "The Hi-MoE framework employs a dual-layer expert structure that includes a Modality Expert Bank for handling missing modalities and an Emotion Expert Bank for fine-grained emotion representation, supported by a cross-modal alignment module.", "result": "The Hi-MoE framework shows state-of-the-art performance on the DEAP and DREAMER datasets in continuous emotion regression, demonstrating robustness against missing modalities and asynchrony.", "conclusion": "This research enhances the development of intelligent emotion systems that can adapt to complex real-world situations.", "key_contributions": ["Introduction of the Hierarchical Mixture-of-Experts framework", "Dynamic handling of missing modalities through soft routing", "Improved fine-grained emotion representation using differential-attention routing"], "limitations": "", "keywords": ["multimodal emotion recognition", "human-computer interaction", "continuous emotion prediction"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.01541", "pdf": "https://arxiv.org/pdf/2508.01541.pdf", "abs": "https://arxiv.org/abs/2508.01541", "title": "MOPrompt: Multi-objective Semantic Evolution for Prompt Optimization", "authors": ["Sara Câmara", "Eduardo Luz", "Valéria Carvalho", "Ivan Meneghini", "Gladston Moreira"], "categories": ["cs.CL"], "comment": "8 pages", "summary": "Prompt engineering is crucial for unlocking the potential of Large Language\nModels (LLMs). Still, since manual prompt design is often complex,\nnon-intuitive, and time-consuming, automatic prompt optimization has emerged as\na research area. However, a significant challenge in prompt optimization is\nmanaging the inherent trade-off between task performance, such as accuracy, and\ncontext size. Most existing automated methods focus on a single objective,\ntypically performance, thereby failing to explore the critical spectrum of\nefficiency and effectiveness. This paper introduces the MOPrompt, a novel\nMulti-objective Evolutionary Optimization (EMO) framework designed to optimize\nprompts for both accuracy and context size (measured in tokens) simultaneously.\nOur framework maps the Pareto front of prompt solutions, presenting\npractitioners with a set of trade-offs between context size and performance, a\ncrucial tool for deploying Large Language Models (LLMs) in real-world\napplications. We evaluate MOPrompt on a sentiment analysis task in Portuguese,\nusing Gemma-2B and Sabiazinho-3 as evaluation models. Our findings show that\nMOPrompt substantially outperforms the baseline framework. For the Sabiazinho\nmodel, MOPrompt identifies a prompt that achieves the same peak accuracy (0.97)\nas the best baseline solution, but with a 31% reduction in token length.", "AI": {"tldr": "This paper presents MOPrompt, a multi-objective framework for optimizing prompts in Large Language Models (LLMs) that balances task performance and context size.", "motivation": "To address the complexity and inefficiency of manual prompt design for LLMs and to simultaneously optimize multiple objectives including accuracy and context size.", "method": "Introduces a Multi-objective Evolutionary Optimization framework called MOPrompt, which evaluates trade-offs between prompt performance and context size by mapping the Pareto front of prompt solutions.", "result": "MOPrompt outperforms existing baseline methods in a sentiment analysis task, achieving peak accuracy equivalent to the best baseline while reducing token length by 31%.", "conclusion": "MOPrompt is presented as a critical tool for practitioners aiming to deploy LLMs efficiently in real-world applications, balancing the need for performance with manageable context size.", "key_contributions": ["Introduction of MOPrompt for multi-objective prompt optimization", "Evaluation of prompts in terms of both accuracy and token length", "Demonstration of superior performance compared to baseline frameworks"], "limitations": "", "keywords": ["Prompt Engineering", "Large Language Models", "Multi-objective Optimization"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2508.02173", "pdf": "https://arxiv.org/pdf/2508.02173.pdf", "abs": "https://arxiv.org/abs/2508.02173", "title": "EchoLadder: Progressive AI-Assisted Design of Immersive VR Scenes", "authors": ["Zhuangze Hou", "Jingze Tian", "Nianlong Li", "Farong Ren", "Can Liu"], "categories": ["cs.HC"], "comment": "To appear at UIST 2025", "summary": "Mixed reality platforms allow users to create virtual environments, yet\nnovice users struggle with both ideation and execution in spatial design. While\nexisting AI models can automatically generate scenes based on user prompts, the\nlack of interactive control limits users' ability to iteratively steer the\noutput. In this paper, we present EchoLadder, a novel human-AI collaboration\npipeline that leverages large vision-language model (LVLM) to support\ninteractive scene modification in virtual reality. EchoLadder accepts users'\nverbal instructions at varied levels of abstraction and spatial specificity,\ngenerates concrete design suggestions throughout a progressive design process.\nThe suggestions can be automatically applied, regenerated and retracted by\nusers' toggle control.Our ablation study showed effectiveness of our pipeline\ncomponents. Our user study found that, compared to baseline without showing\nsuggestions, EchoLadder better supports user creativity in spatial design. It\nalso contributes insights on users' progressive design strategies under AI\nassistance, providing design implications for future systems.", "AI": {"tldr": "EchoLadder is a novel human-AI collaboration pipeline that enhances interactive scene modification in virtual reality by using large vision-language models to support novice users in spatial design.", "motivation": "Novice users struggle with ideation and execution in spatial design on mixed reality platforms. Existing AI models lack interactive control, limiting users' iterative capabilities.", "method": "The EchoLadder pipeline accepts verbal instructions at different levels of abstraction and spatial specificity, generating design suggestions that users can apply, regenerate, or retract.", "result": "An ablation study demonstrated the effectiveness of EchoLadder components, while a user study revealed that it better supports user creativity in spatial design compared to a baseline without suggestions.", "conclusion": "EchoLadder enhances users' creative processes in spatial design and provides insights on iterative design strategies when assisted by AI, contributing valuable implications for future design systems.", "key_contributions": ["Introduction of EchoLadder for interactive scene modification in VR", "Use of LVLM for generating concrete design suggestions in spatial design", "Insights on user interaction strategies under AI assistance"], "limitations": "", "keywords": ["mixed reality", "human-AI collaboration", "spatial design", "user creativity", "large vision-language model"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.01554", "pdf": "https://arxiv.org/pdf/2508.01554.pdf", "abs": "https://arxiv.org/abs/2508.01554", "title": "Are All Prompt Components Value-Neutral? Understanding the Heterogeneous Adversarial Robustness of Dissected Prompt in Large Language Models", "authors": ["Yujia Zheng", "Tianhao Li", "Haotian Huang", "Tianyu Zeng", "Jingyu Lu", "Chuangxin Chu", "Yuekai Huang", "Ziyou Jiang", "Qian Xiong", "Yuyao Ge", "Mingyang Li"], "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": null, "summary": "Prompt-based adversarial attacks have become an effective means to assess the\nrobustness of large language models (LLMs). However, existing approaches often\ntreat prompts as monolithic text, overlooking their structural\nheterogeneity-different prompt components contribute unequally to adversarial\nrobustness. Prior works like PromptRobust assume prompts are value-neutral, but\nour analysis reveals that complex, domain-specific prompts with rich structures\nhave components with differing vulnerabilities. To address this gap, we\nintroduce PromptAnatomy, an automated framework that dissects prompts into\nfunctional components and generates diverse, interpretable adversarial examples\nby selectively perturbing each component using our proposed method, ComPerturb.\nTo ensure linguistic plausibility and mitigate distribution shifts, we further\nincorporate a perplexity (PPL)-based filtering mechanism. As a complementary\nresource, we annotate four public instruction-tuning datasets using the\nPromptAnatomy framework, verified through human review. Extensive experiments\nacross these datasets and five advanced LLMs demonstrate that ComPerturb\nachieves state-of-the-art attack success rates. Ablation studies validate the\ncomplementary benefits of prompt dissection and PPL filtering. Our results\nunderscore the importance of prompt structure awareness and controlled\nperturbation for reliable adversarial robustness evaluation in LLMs. Code and\ndata are available at https://github.com/Yujiaaaaa/PACP.", "AI": {"tldr": "This paper presents PromptAnatomy, a framework for analyzing and generating adversarial examples for large language models by dissecting prompts into components, enhancing robustness evaluation.", "motivation": "The need to improve robustness assessment of large language models by understanding the structural components of prompts which have varying impacts on adversarial robustness.", "method": "PromptAnatomy dissects prompts into their functional components and utilizes a perturbation method called ComPerturb to selectively manipulate these components. It employs a perplexity-based filtering mechanism to maintain linguistic quality.", "result": "Extensive experiments show that ComPerturb achieves state-of-the-art success rates in adversarial attacks across multiple instruction-tuning datasets and LLMs, demonstrating the framework's effectiveness.", "conclusion": "The study highlights the importance of recognizing prompt structure and controlling perturbations for a more accurate evaluation of adversarial robustness in LLMs.", "key_contributions": ["Introduction of PromptAnatomy framework for prompt analysis", "Development of ComPerturb for selective component perturbation", "Application of perplexity-based filtering for linguistic plausibility"], "limitations": "The framework may still be limited by the quality and diversity of the initial datasets used for annotation and testing.", "keywords": ["prompt-based attacks", "large language models", "adversarial robustness", "prompt dissection", "NLP"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2508.02216", "pdf": "https://arxiv.org/pdf/2508.02216.pdf", "abs": "https://arxiv.org/abs/2508.02216", "title": "Data Augmentation for Visualization Design Knowledge Bases", "authors": ["Hyeok Kim", "Jeffrey Heer"], "categories": ["cs.HC"], "comment": "10 pages, 4 tables, 7 figures, accepted to IEEE VIS 2025", "summary": "Visualization knowledge bases enable computational reasoning and\nrecommendation over a visualization design space. These systems evaluate design\ntrade-offs using numeric weights assigned to different features (e.g., binning\na variable). Feature weights can be learned automatically by fitting a model to\na collection of chart pairs, in which one chart is deemed preferable to the\nother. To date, labeled chart pairs have been drawn from published empirical\nresearch results; however, such pairs are not comprehensive, resulting in a\ntraining corpus that lacks many design variants and fails to systematically\nassess potential trade-offs. To improve knowledge base coverage and accuracy,\nwe contribute data augmentation techniques for generating and labeling chart\npairs. We present methods to generate novel chart pairs based on design\npermutations and by identifying under-assessed features -- leading to an\nexpanded corpus with thousands of new chart pairs, now in need of labels.\nAccordingly, we next compare varied methods to scale labeling efforts to\nannotate chart pairs, in order to learn updated feature weights. We evaluate\nour methods in the context of the Draco knowledge base, demonstrating\nimprovements to both feature coverage and chart recommendation performance.", "AI": {"tldr": "The paper enhances visualization design knowledge bases by introducing data augmentation techniques to generate and label chart pairs, improving feature coverage and recommendation performance.", "motivation": "To improve the coverage and accuracy of visualization knowledge bases by addressing the lack of comprehensive labeled chart pairs for evaluating design trade-offs.", "method": "The authors propose data augmentation techniques to generate new chart pairs and compare various methods to efficiently scale labeling efforts for these pairs.", "result": "The study resulted in a significantly expanded corpus of chart pairs and demonstrated improved performance in chart recommendation within the Draco knowledge base.", "conclusion": "The proposed techniques enhance the knowledge base's ability to recommend designs by offering more representative data and learning updated feature weights.", "key_contributions": ["Introduced data augmentation techniques for generating chart pairs", "Developed methods for efficient labeling of chart pairs", "Evaluated improvements in chart recommendation performance within the Draco knowledge base"], "limitations": "", "keywords": ["visualization", "data augmentation", "chart recommendation", "feature weights", "knowledge bases"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.01630", "pdf": "https://arxiv.org/pdf/2508.01630.pdf", "abs": "https://arxiv.org/abs/2508.01630", "title": "OpenMed NER: Open-Source, Domain-Adapted State-of-the-Art Transformers for Biomedical NER Across 12 Public Datasets", "authors": ["Maziyar Panahi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Named-entity recognition (NER) is fundamental to extracting structured\ninformation from the >80% of healthcare data that resides in unstructured\nclinical notes and biomedical literature. Despite recent advances with large\nlanguage models, achieving state-of-the-art performance across diverse entity\ntypes while maintaining computational efficiency remains a significant\nchallenge. We introduce OpenMed NER, a suite of open-source, domain-adapted\ntransformer models that combine lightweight domain-adaptive pre-training (DAPT)\nwith parameter-efficient Low-Rank Adaptation (LoRA). Our approach performs\ncost-effective DAPT on a 350k-passage corpus compiled from ethically sourced,\npublicly available research repositories and de-identified clinical notes\n(PubMed, arXiv, and MIMIC-III) using DeBERTa-v3, PubMedBERT, and BioELECTRA\nbackbones. This is followed by task-specific fine-tuning with LoRA, which\nupdates less than 1.5% of model parameters. We evaluate our models on 12\nestablished biomedical NER benchmarks spanning chemicals, diseases, genes, and\nspecies. OpenMed NER achieves new state-of-the-art micro-F1 scores on 10 of\nthese 12 datasets, with substantial gains across diverse entity types. Our\nmodels advance the state-of-the-art on foundational disease and chemical\nbenchmarks (e.g., BC5CDR-Disease, +2.70 pp), while delivering even larger\nimprovements of over 5.3 and 9.7 percentage points on more specialized gene and\nclinical cell line corpora. This work demonstrates that strategically adapted\nopen-source models can surpass closed-source solutions. This performance is\nachieved with remarkable efficiency: training completes in under 12 hours on a\nsingle GPU with a low carbon footprint (< 1.2 kg CO2e), producing permissively\nlicensed, open-source checkpoints designed to help practitioners facilitate\ncompliance with emerging data protection and AI regulations, such as the EU AI\nAct.", "AI": {"tldr": "OpenMed NER introduces a suite of domain-adapted transformer models for named-entity recognition in healthcare data, achieving state-of-the-art performance while maintaining computational efficiency.", "motivation": "Named-entity recognition is essential for extracting structured information from unstructured clinical notes and biomedical literature in healthcare. Current challenges exist in achieving efficient state-of-the-art performance across diverse entity types.", "method": "OpenMed NER employs domain-adaptive pre-training (DAPT) combined with Low-Rank Adaptation (LoRA) on a corpus of clinically relevant texts, followed by task-specific fine-tuning to enhance model performance while updating minimal parameters (less than 1.5%).", "result": "The models achieve state-of-the-art micro-F1 scores on 10 out of 12 biomedical NER benchmarks, providing significant improvements especially on specialized datasets for genes and clinical data.", "conclusion": "OpenMed NER demonstrates that efficiently adapted open-source models can perform better than closed-source alternatives, while being more sustainable and compliant with data protection regulations.", "key_contributions": ["Development of OpenMed NER, a suite of domain-adapted transformer models for healthcare data.", "State-of-the-art performance on 10 out of 12 biomedical NER benchmarks.", "Efficient training with low carbon footprint and minimal parameter updates."], "limitations": "", "keywords": ["named-entity recognition", "healthcare data", "transformer models", "Low-Rank Adaptation", "domain-adaptive pre-training"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2508.02232", "pdf": "https://arxiv.org/pdf/2508.02232.pdf", "abs": "https://arxiv.org/abs/2508.02232", "title": "Eye2Recall: Exploring the Design of Enhancing Reminiscence Activities via Eye Tracking-Based LLM-Powered Interaction Experience for Older Adults", "authors": ["Lei Han", "Mingnan Wei", "Qiongyan Chen", "Anqi Wang", "Rong Pang", "Kefei Liu", "Rongrong Chen", "David Yip"], "categories": ["cs.HC"], "comment": null, "summary": "Photo-based reminiscence has the potential to have a positive impact on older\nadults' reconnection with their personal history and improve their well-being.\nSupporting reminiscence in older adults through technological implementations\nis becoming an increasingly important area of research in the fields of HCI and\nCSCW. However, the impact of integrating gaze and speech as mixed-initiative\ninteractions in LLM-powered reminiscence conversations remains under-explored.\nTo address this, we conducted expert interviews to understand the challenges\nthat older adults face with LLM-powered, photo-based reminiscence experiences.\nBased on these design considerations, we developed Eye2Recall, a system that\nintegrates eye tracking for detecting visual interest with natural language\ninteraction to create a mixed-initiative reminiscence experience. We evaluated\nits effectiveness through a user study involving ten older adults. The results\nhave important implications for the future design of more accessible and\nempowering reminiscence technologies that better align with older adults'\nnatural interaction patterns and enhance their positive aging.", "AI": {"tldr": "This paper explores a system called Eye2Recall that enhances photo-based reminiscence for older adults by integrating gaze and speech interactions to facilitate natural reminiscence conversations using LLMs.", "motivation": "The motivation is to improve well-being in older adults through technology-supported reminiscence, particularly exploring the integration of gaze and speech in LLM-powered conversations.", "method": "The researchers conducted expert interviews to identify challenges faced by older adults and developed Eye2Recall, a system that combines eye tracking with natural language interaction, followed by a user study with ten participants to evaluate its effectiveness.", "result": "The study revealed valuable insights on design considerations for accessible reminiscence technologies and demonstrated the effectiveness of Eye2Recall in enhancing natural interactions for older adults.", "conclusion": "The findings suggest that integrating technology like Eye2Recall can enhance reminiscence experiences in older adults, aligning better with their interaction patterns and supporting positive aging.", "key_contributions": ["Development of Eye2Recall integrating gaze and speech for reminiscence", "User study with older adults demonstrating the system's effectiveness", "Identification of design considerations for HCI systems aimed at older adults"], "limitations": "The study involved a small sample size of ten older adults, which may limit the generalizability of the findings.", "keywords": ["reminiscence", "HCI", "LLM", "older adults", "eye tracking"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.01656", "pdf": "https://arxiv.org/pdf/2508.01656.pdf", "abs": "https://arxiv.org/abs/2508.01656", "title": "Authorship Attribution in Multilingual Machine-Generated Texts", "authors": ["Lucio La Cava", "Dominik Macko", "Róbert Móro", "Ivan Srba", "Andrea Tagarelli"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "physics.soc-ph"], "comment": null, "summary": "As Large Language Models (LLMs) have reached human-like fluency and\ncoherence, distinguishing machine-generated text (MGT) from human-written\ncontent becomes increasingly difficult. While early efforts in MGT detection\nhave focused on binary classification, the growing landscape and diversity of\nLLMs require a more fine-grained yet challenging authorship attribution (AA),\ni.e., being able to identify the precise generator (LLM or human) behind a\ntext. However, AA remains nowadays confined to a monolingual setting, with\nEnglish being the most investigated one, overlooking the multilingual nature\nand usage of modern LLMs. In this work, we introduce the problem of\nMultilingual Authorship Attribution, which involves attributing texts to human\nor multiple LLM generators across diverse languages. Focusing on 18 languages\n-- covering multiple families and writing scripts -- and 8 generators (7 LLMs\nand the human-authored class), we investigate the multilingual suitability of\nmonolingual AA methods, their cross-lingual transferability, and the impact of\ngenerators on attribution performance. Our results reveal that while certain\nmonolingual AA methods can be adapted to multilingual settings, significant\nlimitations and challenges remain, particularly in transferring across diverse\nlanguage families, underscoring the complexity of multilingual AA and the need\nfor more robust approaches to better match real-world scenarios.", "AI": {"tldr": "The paper addresses the challenges of Multilingual Authorship Attribution (AA), where the objective is to attribute texts to either human or various LLMs across multiple languages, highlighting the limitations of current monolingual approaches.", "motivation": "With the rise of Large Language Models (LLMs) producing human-like text, there is a pressing need to distinguish between machine-generated and human-written content, particularly in a multilingual context.", "method": "The study investigates authorship attribution across 18 languages using 7 LLMs and human authors, evaluating monolingual AA methods and their ability to adapt and transfer to multilingual scenarios.", "result": "The findings indicate that monolingual AA methods can be somewhat adapted for multilingual use; however, there are significant challenges in performance when transferring methods across different language families.", "conclusion": "The complexity of multilingual AA demands the development of more robust techniques to handle the diverse linguistic landscape, as existing methods show substantial limitations.", "key_contributions": ["Introduction of the problem of Multilingual Authorship Attribution", "Analysis of the cross-lingual transferability of monolingual AA methods", "Examination of the impact of various generators on attribution performance"], "limitations": "Significant limitations in transferring methods across diverse language families remain, which indicates the complexity of multilingual authorship attribution.", "keywords": ["Multilingual Authorship Attribution", "Machine-generated text", "Large Language Models", "Cross-lingual transferability", "Human-Computer Interaction"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.02274", "pdf": "https://arxiv.org/pdf/2508.02274.pdf", "abs": "https://arxiv.org/abs/2508.02274", "title": "mCardiacDx: Radar-Driven Contactless Monitoring and Diagnosis of Arrhythmia", "authors": ["Arjun Kumar", "Noppanat Wadlom", "Jaeheon Kwak", "Si-Hyuck Kang", "Insik Shin"], "categories": ["cs.HC", "cs.LG", "92C55, 68T07"], "comment": "15 pages, 27 images", "summary": "Arrhythmia is a common cardiac condition that can precipitate severe\ncomplications without timely intervention. While continuous monitoring is\nessential for timely diagnosis, conventional approaches such as\nelectrocardiogram and wearable devices are constrained by their reliance on\nspecialized medical expertise and patient discomfort from their contact nature.\nExisting contactless monitoring, primarily designed for healthy subjects, face\nsignificant challenges when analyzing reflected signals from arrhythmia\npatients due to disrupted spatial stability and temporal consistency.\n  In this paper, we introduce mCardiacDx, a radar-driven contactless system\nthat accurately analyzes reflected signals and reconstructs heart pulse\nwaveforms for arrhythmia monitoring and diagnosis. The key contributions of our\nwork include a novel precise target localization (PTL) technique that locates\nreflected signals despite spatial disruptions, and an encoder-decoder model\nthat transforms these signals into HPWs, addressing temporal inconsistencies.\nOur evaluation on a large dataset of healthy subjects and arrhythmia patients\nshows that both mCardiacDx and PTL outperform state-of-the-art approach in\narrhythmia monitoring and diagnosis, also demonstrating improved performance in\nhealthy subjects.", "AI": {"tldr": "The paper presents mCardiacDx, a novel radar-driven contactless system for monitoring arrhythmia through accurate signal analysis and heart pulse waveform reconstruction.", "motivation": "To address challenges in traditional arrhythmia monitoring methods that require specialized expertise and may cause patient discomfort.", "method": "A radar-driven system using a precise target localization (PTL) technique and an encoder-decoder model to reconstruct heart pulse waveforms from reflected signals.", "result": "mCardiacDx and the PTL technique significantly outperform state-of-the-art methods in both arrhythmia monitoring and diagnosis, showing improved accuracy in evaluating both arrhythmia patients and healthy subjects.", "conclusion": "The proposed system enhances the reliability and comfort of arrhythmia monitoring by enabling accurate contactless signal analysis.", "key_contributions": ["Introduction of mCardiacDx for arrhythmia monitoring", "Development of precise target localization (PTL) technique", "Utilization of an encoder-decoder model for heart pulse waveform reconstruction"], "limitations": "", "keywords": ["arrhythmia", "contactless monitoring", "radar", "signal analysis", "health informatics"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.01674", "pdf": "https://arxiv.org/pdf/2508.01674.pdf", "abs": "https://arxiv.org/abs/2508.01674", "title": "CUPID: Evaluating Personalized and Contextualized Alignment of LLMs from Interactions", "authors": ["Tae Soo Kim", "Yoonjoo Lee", "Yoonah Park", "Jiho Kim", "Young-Ho Kim", "Juho Kim"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "Accepted to COLM 2025. Project Website: https://cupid.kixlab.org/", "summary": "Personalization of Large Language Models (LLMs) often assumes users hold\nstatic preferences that reflect globally in all tasks. In reality, humans hold\ndynamic preferences that change depending on the context. As users interact\nwith an LLM in various contexts, they naturally reveal their contextual\npreferences, which a model must infer and apply in future contexts to ensure\nalignment. To assess this, we introduce CUPID, a benchmark of 756 human-curated\ninteraction session histories between users and LLM-based chat assistants. In\neach interaction session, the user provides a request in a specific context and\nexpresses their preference through multi-turn feedback. Given a new user\nrequest and prior interaction sessions, our benchmark assesses whether LLMs can\ninfer the preference relevant to this request and generate a response that\nsatisfies this preference. With CUPID, we evaluated 10 open and proprietary\nLLMs, revealing that state-of-the-art LLMs struggle to infer preferences from\nmulti-turn interactions and fail to discern what previous context is relevant\nto a new request -- under 50% precision and 65% recall. Our work highlights the\nneed to advance LLM capabilities for more contextually personalized\ninteractions and proposes CUPID as a resource to drive these improvements.", "AI": {"tldr": "This paper introduces CUPID, a benchmark that evaluates the ability of Large Language Models (LLMs) to infer dynamic user preferences during interactions in varying contexts.", "motivation": "To highlight that user preferences are dynamic and context-dependent, challenging the assumption of static preferences in LLM personalization.", "method": "CUPID is a benchmark consisting of 756 human-curated interaction session histories, where users express preferences through multi-turn feedback during interactions with LLM-based chat assistants.", "result": "State-of-the-art LLMs performed poorly, with under 50% precision and 65% recall, unable to accurately infer preferences from past interactions in new contexts.", "conclusion": "The findings suggest a significant gap in LLM ability to provide contextually personalized interactions, emphasizing the need for advancements in their capabilities.", "key_contributions": ["Introduction of the CUPID benchmark for evaluating preference inference in LLMs", "Demonstration of the challenges faced by current LLMs in multi-turn interactions", "Provision of a resource to encourage future improvements in LLM personalization"], "limitations": "The benchmark does not cover all possible contexts or user preferences that could be encountered in real-world applications.", "keywords": ["Large Language Models", "CUPID", "user preferences", "contextual interactions", "natural language processing"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.02328", "pdf": "https://arxiv.org/pdf/2508.02328.pdf", "abs": "https://arxiv.org/abs/2508.02328", "title": "Understanding User Preferences for Interaction Styles in Conversational Recommender Systems: The Predictive Role of System Qualities, User Experience, and Traits", "authors": ["Raj Mahmud", "Shlomo Berkovsky", "Mukesh Prasad", "A. Baki Kocaballi"], "categories": ["cs.HC", "cs.CL", "cs.IR", "H.5.2; I.2.7; H.1.2"], "comment": "Accepted at OZCHI 2025. 21 pages, 9 figures, 8 tables", "summary": "Conversational Recommender Systems (CRSs) deliver personalised\nrecommendations through multi-turn natural language dialogue and increasingly\nsupport both task-oriented and exploratory interactions. Yet, the factors\nshaping user interaction preferences remain underexplored. In this\nwithin-subjects study (\\(N = 139\\)), participants experienced two scripted CRS\ndialogues, rated their experiences, and indicated the importance of eight\nsystem qualities. Logistic regression revealed that preference for the\nexploratory interaction was predicted by enjoyment, usefulness, novelty, and\nconversational quality. Unexpectedly, perceived effectiveness was also\nassociated with exploratory preference. Clustering uncovered five latent user\nprofiles with distinct dialogue style preferences. Moderation analyses\nindicated that age, gender, and control preference significantly influenced\nthese choices. These findings integrate affective, cognitive, and trait-level\npredictors into CRS user modelling and inform autonomy-sensitive,\nvalue-adaptive dialogue design. The proposed predictive and adaptive framework\napplies broadly to conversational AI systems seeking to align dynamically with\nevolving user needs.", "AI": {"tldr": "The study investigates user preferences in Conversational Recommender Systems (CRSs) through a multi-turn dialogue, identifying key system qualities that influence user choices.", "motivation": "To explore factors shaping user interaction preferences in CRSs, an area that has been underexplored despite the growing importance of personalized recommendations.", "method": "A within-subjects study with 139 participants, who experienced two scripted CRS dialogues and rated aspects of their experiences while indicating preferences across eight system qualities. Logistic regression and clustering analyses were employed to interpret the data.", "result": "Key predictors for preference towards exploratory interaction included enjoyment, usefulness, novelty, conversational quality, and perceived effectiveness. Five distinct latent user profiles were identified based on dialogue style preferences, influenced by age, gender, and control preference.", "conclusion": "The findings enhance understanding of user preferences in CRSs, suggesting a framework for designing dialogues that adapt to user needs by integrating affective and trait-level predictors.", "key_contributions": ["Identified key system qualities influencing user preferences in CRSs", "Developed a predictive framework for user modeling in conversational AI", "Revealed distinct user profiles that inform dialogue design"], "limitations": "Study limited to scripted dialogues and a specific participant demographic; findings may not generalize to all user groups or types of CRSs.", "keywords": ["Conversational Recommender Systems", "User Preferences", "Dialogue Systems", "User Modeling", "Adaptive Interaction"], "importance_score": 8, "read_time_minutes": 21}}
{"id": "2508.01682", "pdf": "https://arxiv.org/pdf/2508.01682.pdf", "abs": "https://arxiv.org/abs/2508.01682", "title": "The Bidirectional Process Reward Model", "authors": ["Lingyin Zhang", "Jun Gao", "Xiaoxue Ren", "Ziqiang Cao"], "categories": ["cs.CL"], "comment": null, "summary": "Process Reward Models (PRMs) have emerged as a promising approach to enhance\nthe reasoning quality of Large Language Models (LLMs) by assigning fine-grained\nscores to intermediate reasoning steps within a solution trajectory. However,\nexisting PRMs predominantly adopt a unidirectional left-to-right (L2R)\nevaluation paradigm, which limits their ability to leverage global context,\nmaking it challenging to verify the consistency of earlier steps based on later\nones. In light of these challenges, we propose a novel bidirectional evaluation\nparadigm, named Bidirectional Process Reward Model (BiPRM). BiPRM seamlessly\nincorporates a parallel right-to-left (R2L) evaluation stream alongside the\nconventional L2R flow, enabling later reasoning steps to help assess earlier\nones in real time. Notably, the built-in R2L evaluation is implemented solely\nthrough prompt modifications that reverse the original reasoning trajectory,\nwithout any additional parameters or inference latency introduced. This ensures\nBiPRM remains both efficient and broadly compatible with existing PRM studies.\nWe conduct extensive experiments on two mathematical reasoning benchmarks using\nsamples generated by three different policy models. Our method, BiPRM, is\nevaluated across three backbones and three distinct PRM objectives. Across all\nsettings, BiPRM consistently outperforms unidirectional baselines, achieving up\nto a 31.9% improvement in stepwise reward evaluation. Generally, our results\nhighlight BiPRM's effectiveness, robustness, and general applicability,\noffering a promising new direction for process-based reward modeling.", "AI": {"tldr": "This paper introduces a Bidirectional Process Reward Model (BiPRM) that enhances Large Language Models (LLMs) by evaluating reasoning steps bidirectionally, improving consistency and overall performance in mathematical reasoning tasks.", "motivation": "The motivation behind this work is to overcome the limitations of existing unidirectional Process Reward Models (PRMs) that restrict the evaluation of reasoning steps in LLMs by not leveraging global context effectively.", "method": "The authors propose a Bidirectional Process Reward Model (BiPRM) that integrates a parallel right-to-left (R2L) evaluation stream with the conventional left-to-right (L2R) process, allowing for real-time assessment of earlier reasoning steps using later ones. This is achieved through prompt modifications, avoiding extra parameters or latency.", "result": "BiPRM was tested on two mathematical reasoning benchmarks using samples from three different policy models. The results show that BiPRM outperforms unidirectional baselines with improvements of up to 31.9% in stepwise reward evaluation across various settings.", "conclusion": "BiPRM demonstrates improved effectiveness, robustness, and broad applicability, indicating a promising new direction for process-based reward modeling in LLMs.", "key_contributions": ["Introduction of a bidirectional evaluation paradigm for reasoning in LLMs", "Efficient implementation through prompt modifications without additional parameters", "Significant performance gains in mathematical reasoning tasks compared to unidirectional models."], "limitations": "The study primarily focuses on mathematical reasoning benchmarks; applicability to other domains may require further validation.", "keywords": ["Bidirectional Process Reward Model", "Large Language Models", "Mathematical Reasoning", "AI", "Process Reward Models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.02371", "pdf": "https://arxiv.org/pdf/2508.02371.pdf", "abs": "https://arxiv.org/abs/2508.02371", "title": "Six Guidelines for Trustworthy, Ethical and Responsible Automation Design", "authors": ["Matouš Jelínek", "Nadine Schlicker", "Ewart de Visser"], "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "Calibrated trust in automated systems (Lee and See 2004) is critical for\ntheir safe and seamless integration into society. Users should only rely on a\nsystem recommendation when it is actually correct and reject it when it is\nfactually wrong. One requirement to achieve this goal is an accurate\ntrustworthiness assessment, ensuring that the user's perception of the system's\ntrustworthiness aligns with its actual trustworthiness, allowing users to make\ninformed decisions about the extent to which they can rely on the system\n(Schlicker et al. 2022). We propose six design guidelines to help designers\noptimize for accurate trustworthiness assessments, thus fostering ethical and\nresponsible human-automation interactions. The proposed guidelines are derived\nfrom existing literature in various fields, such as human-computer interaction,\ncognitive psychology, automation research, user-experience design, and ethics.\nWe are incorporating key principles from the field of pragmatics, specifically\nthe cultivation of common ground (H. H. Clark 1996) and Gricean communication\nmaxims (Grice 1975). These principles are essential for the design of automated\nsystems because the user's perception of the system's trustworthiness is shaped\nby both environmental contexts, such as organizational culture or societal\nnorms, and by situational context, including the specific circumstances or\nscenarios in which the interaction occurs (Hoff and Bashir 2015). Our proposed\nguidelines provide actionable insights for designers to create automated\nsystems that make relevant trustworthiness cues available. This would ideally\nfoster calibrated trust and more satisfactory, productive, and safe\ninteractions between humans and automated systems. Furthermore, the proposed\nheuristics might work as a tool for evaluating to what extent existing systems\nenable users to accurately assess a system's trustworthiness.", "AI": {"tldr": "The paper proposes six design guidelines to optimize trustworthiness assessments in automated systems, promoting safe human-automation interactions.", "motivation": "Accurate trustworthiness assessment is vital for users to rely on automated systems appropriately, ensuring that their trust aligns with the system's actual performance.", "method": "The guidelines are developed from literature across multiple fields, incorporating principles from pragmatics, cognitive psychology, and ethics to enhance user-system interactions.", "result": "The guidelines aim to provide actionable insights for designers, fostering calibrated trust and improving user satisfaction and safety in interactions with automated systems.", "conclusion": "Implementing the proposed design guidelines can enhance the trust users have in automated systems by ensuring better alignment between perceived and actual trustworthiness.", "key_contributions": ["Six design guidelines for trustworthiness assessment in automation", "Integration of pragmatics principles for user-system communication", "Framework for evaluating existing systems' trustworthiness capabilities"], "limitations": "", "keywords": ["trustworthiness", "automated systems", "human-computer interaction", "design guidelines", "ethical design"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.01696", "pdf": "https://arxiv.org/pdf/2508.01696.pdf", "abs": "https://arxiv.org/abs/2508.01696", "title": "Collaborative Chain-of-Agents for Parametric-Retrieved Knowledge Synergy", "authors": ["Yi Jiang", "Sendong Zhao", "Jianbo Li", "Haochun Wang", "Lizhe Zhang", "Yan Liu", "Bin Qin"], "categories": ["cs.CL", "cs.AI"], "comment": "code available at https://github.com/liunian-Jay/CoCoA", "summary": "Retrieval-Augmented Generation (RAG) has emerged as a promising framework for\nenhancing the capabilities of Large Language Models (LLMs), especially in\nknowledge-intensive tasks. Despite its advantages, current RAG methods often\nstruggle to *fully exploit knowledge during generation*. In particular, the\nsynergy between the model's internal parametric knowledge and external\nretrieved knowledge remains limited. Retrieved contents may sometimes mislead\ngeneration, while certain generated content can guide the model toward more\naccurate outputs. In this work, we propose Collaborative Chain-of-Agents, a\nframework designed to enhance explicitly synergy over both parametric and\nretrieved knowledge. Specifically, we first introduce CoCoA-zero, a multi-agent\nRAG framework that first performs conditional knowledge induction and then\nreasons answers. Building on this, we develop CoCoA, a long-chain training\nstrategy that synthesizes extended multi-agent reasoning trajectories from\nCoCoA-zero to fine-tune the LLM. This strategy enhances the model's capability\nto explicitly integrate and jointly leverage parametric and retrieved\nknowledge. Experiments results show that CoCoA-zero and CoCoA achieve superior\nperformance on open-domain and multi-hop QA tasks.", "AI": {"tldr": "Collaborative Chain-of-Agents (CoCoA) enhances the synergy between parametric and retrieved knowledge in Retrieval-Augmented Generation (RAG) frameworks, leading to improved performance in QA tasks.", "motivation": "To fully exploit knowledge during generation in RAG methods, addressing the limited synergy between internal and external knowledge.", "method": "Introduced CoCoA-zero for conditional knowledge induction followed by reasoning; developed CoCoA for long-chain training of multi-agent reasoning trajectories.", "result": "CoCoA-zero and CoCoA showed superior performance on open-domain and multi-hop QA tasks.", "conclusion": "The proposed framework effectively integrates and leverages parametric and retrieved knowledge, advancing RAG capabilities.", "key_contributions": ["CoCoA-zero multi-agent framework for conditional knowledge induction", "CoCoA long-chain training strategy for enhanced reasoning", "Improved performance on knowledge-intensive QA tasks"], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Large Language Models", "Multi-agent reasoning"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2508.02376", "pdf": "https://arxiv.org/pdf/2508.02376.pdf", "abs": "https://arxiv.org/abs/2508.02376", "title": "Talking Surveys: How Photorealistic Embodied Conversational Agents Shape Response Quality, Engagement, and Satisfaction", "authors": ["Matus Krajcovic", "Peter Demcak", "Eduard Kuric"], "categories": ["cs.HC", "H.5; I.2"], "comment": null, "summary": "Embodied conversational agents (ECAs) are increasingly more realistic and\ncapable of dynamic conversations. In online surveys, anthropomorphic agents\ncould help address issues like careless responding and satisficing, which\noriginate from the lack of personal engagement and perceived accountability.\nHowever, there is a lack of understanding of how ECAs in user experience\nresearch may affect participant engagement, satisfaction, and the quality of\nresponses. As a proof of concept, we propose an instrument that enables the\nincorporation of conversations with a virtual avatar into surveys, using on\nAI-driven video generation, speech recognition, and Large Language Models. In\nour between-subjects study, 80 participants (UK, stratified random sample of\ngeneral population) either talked to a voice-based agent with an animated video\navatar, or interacted with a chatbot. Across surveys based on two self-reported\npsychometric tests, 2,265 conversation responses were obtained. Statistical\ncomparison of results indicates that embodied agents can contribute\nsignificantly to more informative, detailed responses, as well as higher yet\nmore time-efficient engagement. Furthermore, qualitative analysis provides\nvaluable insights for causes of no significant change to satisfaction, linked\nto personal preferences, turn-taking delays and Uncanny Valley reactions. These\nfindings support the pursuit and development of new methods toward human-like\nagents for the transformation of online surveys into more natural interactions\nresembling in-person interviews.", "AI": {"tldr": "This paper investigates the impact of embodied conversational agents (ECAs) on participant engagement, satisfaction, and response quality in online surveys, proposing a method to incorporate virtual avatars into the survey process.", "motivation": "The lack of personal engagement and perceived accountability in online surveys can lead to issues like careless responding and satisficing. ECAs could improve participant interaction.", "method": "A between-subjects study with 80 participants, comparing responses from those interacting with a voice-based agent with an animated video avatar to those interacting with a traditional chatbot, collecting 2,265 conversation responses.", "result": "Statistical analysis showed that embodied agents led to more informative and detailed responses, with higher yet more time-efficient engagement overall, despite no significant change in reported satisfaction.", "conclusion": "The findings suggest that ECAs can enhance the online survey experience, resembling natural interactions that could foster better response quality, warranting further development of human-like agents.", "key_contributions": ["Development of an instrument for incorporating ECAs into surveys", "Demonstrated significant improvement in response quality and engagement", "Provided qualitative insights on participant satisfaction and interactions"], "limitations": "Study focused on a specific participant demographic; effectiveness may vary across different populations and contexts.", "keywords": ["Embodied Conversational Agents", "Human-Computer Interaction", "Online Surveys", "User Engagement", "AI"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.01708", "pdf": "https://arxiv.org/pdf/2508.01708.pdf", "abs": "https://arxiv.org/abs/2508.01708", "title": "Am I Blue or Is My Hobby Counting Teardrops? Expression Leakage in Large Language Models as a Symptom of Irrelevancy Disruption", "authors": ["Berkay Köprü", "Mehrzad Mashal", "Yigit Gurses", "Akos Kadar", "Maximilian Schmitt", "Ditty Mathew", "Felix Burkhardt", "Florian Eyben", "Björn W. Schuller"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have advanced natural language processing (NLP)\nskills such as through next-token prediction and self-attention, but their\nability to integrate broad context also makes them prone to incorporating\nirrelevant information. Prior work has focused on semantic leakage, bias\nintroduced by semantically irrelevant context. In this paper, we introduce\nexpression leakage, a novel phenomenon where LLMs systematically generate\nsentimentally charged expressions that are semantically unrelated to the input\ncontext. To analyse the expression leakage, we collect a benchmark dataset\nalong with a scheme to automatically generate a dataset from free-form text\nfrom common-crawl. In addition, we propose an automatic evaluation pipeline\nthat correlates well with human judgment, which accelerates the benchmarking by\ndecoupling from the need of annotation for each analysed model. Our experiments\nshow that, as the model scales in the parameter space, the expression leakage\nreduces within the same LLM family. On the other hand, we demonstrate that\nexpression leakage mitigation requires specific care during the model building\nprocess, and cannot be mitigated by prompting. In addition, our experiments\nindicate that, when negative sentiment is injected in the prompt, it disrupts\nthe generation process more than the positive sentiment, causing a higher\nexpression leakage rate.", "AI": {"tldr": "The paper introduces the concept of expression leakage in large language models (LLMs), highlighting their tendency to generate sentimentally charged expressions unrelated to input context. It presents a benchmark dataset and an evaluation method, showing that expression leakage can be reduced with model scaling but requires careful construction of the models.", "motivation": "To address the issues of LLMs incorporating irrelevant information and to introduce a novel concept called expression leakage, where sentimentally charged expressions arise despite being semantically unrelated to input context.", "method": "The authors collected a benchmark dataset, developed a scheme to generate a dataset from common-crawl data, and created an automatic evaluation pipeline that correlates well with human judgment for assessing expression leakage.", "result": "The findings indicate that expression leakage decreases as model size increases, but mitigation requires careful attention during model construction, as it cannot be addressed through prompting strategies.", "conclusion": "Expression leakage is a significant issue for LLMs that necessitates specific model building strategies. The effects of negative sentiment in prompts lead to higher expression leakage compared to positive sentiment.", "key_contributions": ["Introduction of expression leakage as a novel phenomenon in LLMs", "Development of a benchmark dataset and automatic evaluation pipeline for assessing expression leakage", "Demonstration of the relationship between model scaling and expression leakage mitigation"], "limitations": "The evaluation method may require further validation across diverse contexts and LLM architectures.", "keywords": ["large language models", "expression leakage", "natural language processing", "benchmark dataset", "sentiment analysis"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.02413", "pdf": "https://arxiv.org/pdf/2508.02413.pdf", "abs": "https://arxiv.org/abs/2508.02413", "title": "Improving Knowledge Graph Understanding with Contextual Views", "authors": ["Antrea Christou", "Cogan Shimizu"], "categories": ["cs.HC"], "comment": "12 pages", "summary": "Navigating, visualizing, and discovery in graph data is frequently a\ndifficult prospect. This is especially true for knowledge graphs (KGs), due to\nhigh number of possible labeled connections to other data.\n  However, KGs are frequently equipped with an ontology as a schema. That is,\nit informs how the relationships between data may be constrained. This\nadditional information can be leveraged to improve how (knowledge) graph data\ncan be navigated, visualized, or otherwise utilized in a discovery process.\n  In this manuscript, we introduce the Interactive Knowledge (InK) Browser.\nThis tool specifically takes advantage ontological information (i.e.,\nknowledge) when found in KGs. Specifically, we use modular views that provide\nvarious perspectives over the graph, including an interactive schema view, data\nlistings based on type, neighborhood connections, and geospatial depiction\n(where appropriate). For this manuscript, we have evaluated the basic premise\nof this tool over a user group ($n= With this grown user survey, we continue to\nevaluate how scalable tools, including flexible views, can make KG exploration\neasier for a range of applications.)", "AI": {"tldr": "Introducing the Interactive Knowledge (InK) Browser tool for improved navigation and visualization of knowledge graphs utilizing ontological information.", "motivation": "The challenge of navigating and visualizing graph data, especially in knowledge graphs with numerous connections, necessitates improved tools that leverage schema information.", "method": "The InK Browser employs modular views to present knowledge graph information, including interactive schema views, type-based data listings, neighborhood connections, and geospatial representations when applicable.", "result": "User evaluations indicated that the InK Browser effectively enhances the navigation and exploration of knowledge graphs by offering diverse perspectives and flexible views.", "conclusion": "The findings suggest that incorporating ontological information can significantly improve the user experience in exploring knowledge graphs, making them more accessible for various applications.", "key_contributions": ["Development of the Interactive Knowledge (InK) Browser tool", "Utilization of ontological information for enhanced graph navigation and visualization", "Evaluation of user experiences and scalability of tool features"], "limitations": "Limited user sample size in initial evaluation, further studies needed to validate findings.", "keywords": ["knowledge graphs", "ontologies", "data visualization", "HCI", "interactive tools"], "importance_score": 6, "read_time_minutes": 12}}
{"id": "2508.01710", "pdf": "https://arxiv.org/pdf/2508.01710.pdf", "abs": "https://arxiv.org/abs/2508.01710", "title": "CultureGuard: Towards Culturally-Aware Dataset and Guard Model for Multilingual Safety Applications", "authors": ["Raviraj Joshi", "Rakesh Paul", "Kanishk Singla", "Anusha Kamath", "Michael Evans", "Katherine Luna", "Shaona Ghosh", "Utkarsh Vaidya", "Eileen Long", "Sanjay Singh Chauhan", "Niranjan Wartikar"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The increasing use of Large Language Models (LLMs) in agentic applications\nhighlights the need for robust safety guard models. While content safety in\nEnglish is well-studied, non-English languages lack similar advancements due to\nthe high cost of collecting culturally aligned labeled datasets. We present\nCultureGuard, a novel solution for curating culturally aligned, high-quality\nsafety datasets across multiple languages. Our approach introduces a four-stage\nsynthetic data generation and filtering pipeline: cultural data segregation,\ncultural data adaptation, machine translation, and quality filtering. This\npipeline enables the conversion and expansion of the\nNemotron-Content-Safety-Dataset-V2 English safety dataset into eight distinct\nlanguages: Arabic, German, Spanish, French, Hindi, Japanese, Thai, and Chinese.\nThe resulting dataset, Nemotron-Content-Safety-Dataset-Multilingual-v1,\ncomprises 386,661 samples in 9 languages and facilitates the training of\nLlama-3.1-Nemotron-Safety-Guard-Multilingual-8B-v1 via LoRA-based fine-tuning.\nThe final model achieves state-of-the-art performance on several multilingual\ncontent safety benchmarks. We also benchmark the latest open LLMs on\nmultilingual safety and observe that these LLMs are more prone to give unsafe\nresponses when prompted in non-English languages. This work represents a\nsignificant step toward closing the safety gap in multilingual LLMs by enabling\nthe development of culturally aware safety guard models.", "AI": {"tldr": "This paper introduces CultureGuard, a pipeline for creating culturally aligned safety datasets for LLMs in multiple languages, addressing the gap in multilingual content safety.", "motivation": "The need for robust safety guard models for Large Language Models (LLMs) in non-English languages due to a lack of culturally aligned labeled datasets.", "method": "The study presents a four-stage synthetic data generation and filtering pipeline that includes cultural data segregation, adaptation, machine translation, and quality filtering.", "result": "The resulting dataset, Nemotron-Content-Safety-Dataset-Multilingual-v1, contains 386,661 samples across 9 languages and facilitates state-of-the-art performance in multilingual content safety benchmarks.", "conclusion": "This work significantly advances the development of culturally aware safety guard models, helping to close the safety gap in multilingual LLM applications.", "key_contributions": ["Introduction of a robust four-stage synthetic data pipeline", "Creation of a comprehensive multilingual dataset", "Demonstration of state-of-the-art performance in multilingual content safety"], "limitations": "", "keywords": ["Large Language Models", "Content Safety", "Multilingual Datasets", "Synthetic Data Generation", "Cultural Adaptation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.02470", "pdf": "https://arxiv.org/pdf/2508.02470.pdf", "abs": "https://arxiv.org/abs/2508.02470", "title": "AIAP: A No-Code Workflow Builder for Non-Experts with Natural Language and Multi-Agent Collaboration", "authors": ["Hyunjn An", "Yongwon Kim", "Wonduk Seo", "Joonil Park", "Daye Kang", "Changhoon Oh", "Dokyun Kim", "Seunghyun Lee"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.MA", "cs.SE"], "comment": "14 pages, 6 figures", "summary": "While many tools are available for designing AI, non-experts still face\nchallenges in clearly expressing their intent and managing system complexity.\nWe introduce AIAP, a no-code platform that integrates natural language input\nwith visual workflows. AIAP leverages a coordinated multi-agent system to\ndecompose ambiguous user instructions into modular, actionable steps, hidden\nfrom users behind a unified interface. A user study involving 32 participants\nshowed that AIAP's AI-generated suggestions, modular workflows, and automatic\nidentification of data, actions, and context significantly improved\nparticipants' ability to develop services intuitively. These findings highlight\nthat natural language-based visual programming significantly reduces barriers\nand enhances user experience in AI service design.", "AI": {"tldr": "AIAP is a no-code platform that simplifies AI service design by allowing natural language input and improving user workflows.", "motivation": "Non-experts struggle with expressing intent and managing complexity in AI design. AIAP addresses this by providing a user-friendly platform.", "method": "AIAP integrates natural language processing with visual workflows, employing a multi-agent system to break down user instructions into manageable steps.", "result": "A user study with 32 participants showed significant improvements in developing services, indicating that AIAP effectively enhances user experience.", "conclusion": "Natural language-based visual programming reduces barriers for non-experts in AI service design, leading to better usability.", "key_contributions": ["Introduction of a no-code platform for AI service design", "Integration of natural language input with visual workflows", "Demonstrated effectiveness through user studies"], "limitations": "", "keywords": ["no-code platform", "natural language input", "AI service design"], "importance_score": 8, "read_time_minutes": 14}}
{"id": "2508.01739", "pdf": "https://arxiv.org/pdf/2508.01739.pdf", "abs": "https://arxiv.org/abs/2508.01739", "title": "Enhancing the Preference Extractor in Multi-turn Dialogues: From Annotating Disasters to Accurate Preference Extraction", "authors": ["Cheng Wang", "ziru Liu", "Pengcheng Tang", "Mingyu Zhang", "Quanyu Dai", "Yue Zhu"], "categories": ["cs.CL"], "comment": null, "summary": "Identifying user preferences in dialogue systems is a pivotal aspect of\nproviding satisfying services. Current research shows that using large language\nmodels (LLMs) to fine-tune a task-specific preference extractor yields\nexcellent results in terms of accuracy and generalization. However, the primary\nchallenge stems from the inherent difficulty in obtaining high-quality labeled\nmulti-turn dialogue data. Accurately tracking user preference transitions\nacross turns not only demands intensive domain expertise and contextual\nconsistency maintenance for annotators (termed \\textbf{``Annotating\nDisaster''}) but also complicates model training due to error propagation in\nsequential dependency learning. Inspired by the observation that multi-turn\npreference extraction can be decomposed into iterative executions of one-turn\nextraction processes. We propose a novel dialogue data generation framework\nnamed \\textbf{IterChat}. First, we construct a new data format that categorizes\nthe dialogue data into attributed historical preferences and one-turn\ndialogues. This reduces the probability of annotation errors and improves\nannotation efficiency. Then, to generate a high-quality and diverse dialogue\ndataset, we adopt GPT4 to pre-define the preference slots in the target\npreference extractor task and then randomly sample the subset of the slots and\ntheir corresponding schema values to create the dialogue datasets. Experimental\nresults indicate that fine-tuning or only few-shot prompting with the new\ndialogue format yields superior performance compared to the original multi-turn\ndialogues. Additionally, the new data format improves annotator efficiency with\na win rate of 28.4\\% higher than the original multi-turn dialogues.", "AI": {"tldr": "The paper introduces IterChat, a novel framework for generating dialogue datasets that improves user preference extraction in dialogue systems.", "motivation": "Identifying user preferences in dialogue systems enhances service satisfaction, but challenges arise from obtaining high-quality labeled data for multi-turn dialogues.", "method": "The authors developed a new dialogue data format that organizes dialogue data into attributed historical preferences and one-turn dialogues, utilizing GPT-4 for dataset generation by pre-defining preference slots and sampling them.", "result": "Experiments show that the new dialogue format leads to superior performance in preference extraction tasks, with improved annotator efficiency indicated by a 28.4% higher win rate compared to traditional methods.", "conclusion": "The proposed IterChat framework reduces annotation errors and enhances model training by simplifying preference extraction processes.", "key_contributions": ["Introduction of the IterChat framework for dialogue data generation", "Creation of an efficient dialogue data format that categorizes preferences", "Demonstrated performance improvements in preference extraction and annotation efficiency."], "limitations": "", "keywords": ["dialogue systems", "user preferences", "large language models", "data generation", "annotation efficiency"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.02550", "pdf": "https://arxiv.org/pdf/2508.02550.pdf", "abs": "https://arxiv.org/abs/2508.02550", "title": "Stakeholder Perspectives on Humanistic Implementation of Computer Perception in Healthcare: A Qualitative Study", "authors": ["Kristin M. Kostick-Quenet", "Meghan E. Hurley", "Syed Ayaz", "John Herrington", "Casey Zampella", "Julia Parish-Morris", "Birkan Tunç", "Gabriel Lázaro-Muñoz", "J. S. Blumenthal-Barby", "Eric A. Storch"], "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": "65 pages", "summary": "Computer perception (CP) technologies (digital phenotyping, affective\ncomputing and related passive sensing approaches) offer unprecedented\nopportunities to personalize healthcare, but provoke concerns about privacy,\nbias and the erosion of empathic, relationship-centered practice. A\ncomprehensive understanding of perceived risks, benefits, and implementation\nchallenges from those who design, deploy and experience these tools in\nreal-world settings remains elusive. This study provides the first\nevidence-based account of key stakeholder perspectives on the relational,\ntechnical, and governance challenges raised by the integration of CP\ntechnologies into patient care. We conducted in-depth, semi-structured\ninterviews with 102 stakeholders: adolescent patients and their caregivers,\nfrontline clinicians, technology developers, and ethics, legal, policy or\nphilosophy scholars. Transcripts underwent thematic analysis by a\nmultidisciplinary team; reliability was enhanced through double coding and\nconsensus adjudication. Stakeholders articulated seven interlocking concern\ndomains: (1) trustworthiness and data integrity; (2) patient-specific\nrelevance; (3) utility and workflow integration; (4) regulation and governance;\n(5) privacy and data protection; (6) direct and indirect patient harms; and (7)\nphilosophical critiques of reductionism. To operationalize humanistic\nsafeguards, we propose \"personalized roadmaps\": co-designed plans that\npredetermine which metrics will be monitored, how and when feedback is shared,\nthresholds for clinical action, and procedures for reconciling discrepancies\nbetween algorithmic inferences and lived experience. By translating these\ninsights into personalized roadmaps, we offer a practical framework for\ndevelopers, clinicians and policymakers seeking to harness continuous\nbehavioral data while preserving the humanistic core of care.", "AI": {"tldr": "This paper explores the stakeholder perspectives on the integration of computer perception technologies in healthcare, addressing concerns about privacy, bias, and the human element of patient care.", "motivation": "Understanding the perceived risks, benefits, and challenges of computer perception technologies in healthcare from various stakeholders is essential to improve implementation while maintaining the quality of care.", "method": "In-depth, semi-structured interviews were conducted with 102 stakeholders, including patients, caregivers, clinicians, technology developers, and scholars, followed by thematic analysis of transcripts.", "result": "Seven interlocking concern domains were identified: trustworthiness, relevance, utility, regulation, privacy, patient harms, and philosophical critiques.", "conclusion": "The paper proposes 'personalized roadmaps' to guide the implementation of computer perception technologies, ensuring that patient care remains humanistic.", "key_contributions": ["First evidence-based account of key stakeholder perspectives on CP technologies", "Identification of seven concern domains related to CP integration in healthcare", "Proposal of 'personalized roadmaps' as a practical framework for sustaining humanistic care"], "limitations": "", "keywords": ["computer perception", "healthcare", "stakeholder perspectives", "privacy", "personalized roadmaps"], "importance_score": 9, "read_time_minutes": 65}}
{"id": "2508.01754", "pdf": "https://arxiv.org/pdf/2508.01754.pdf", "abs": "https://arxiv.org/abs/2508.01754", "title": "AI-Generated Text is Non-Stationary: Detection via Temporal Tomography", "authors": ["Alva West", "Yixuan Weng", "Minjun Zhu", "Luodan Zhang", "Zhen Lin", "Guangsheng Bao", "Yue Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "The field of AI-generated text detection has evolved from supervised\nclassification to zero-shot statistical analysis. However, current approaches\nshare a fundamental limitation: they aggregate token-level measurements into\nscalar scores, discarding positional information about where anomalies occur.\nOur empirical analysis reveals that AI-generated text exhibits significant\nnon-stationarity, statistical properties vary by 73.8\\% more between text\nsegments compared to human writing. This discovery explains why existing\ndetectors fail against localized adversarial perturbations that exploit this\noverlooked characteristic. We introduce Temporal Discrepancy Tomography (TDT),\na novel detection paradigm that preserves positional information by\nreformulating detection as a signal processing task. TDT treats token-level\ndiscrepancies as a time-series signal and applies Continuous Wavelet Transform\nto generate a two-dimensional time-scale representation, capturing both the\nlocation and linguistic scale of statistical anomalies. On the RAID benchmark,\nTDT achieves 0.855 AUROC (7.1\\% improvement over the best baseline). More\nimportantly, TDT demonstrates robust performance on adversarial tasks, with\n14.1\\% AUROC improvement on HART Level 2 paraphrasing attacks. Despite its\nsophisticated analysis, TDT maintains practical efficiency with only 13\\%\ncomputational overhead. Our work establishes non-stationarity as a fundamental\ncharacteristic of AI-generated text and demonstrates that preserving temporal\ndynamics is essential for robust detection.", "AI": {"tldr": "This paper introduces Temporal Discrepancy Tomography (TDT), a novel approach for AI-generated text detection that captures positional information and addresses limitations of existing methods.", "motivation": "Current detection methods for AI-generated text overlook positional information and fail against localized adversarial attacks.", "method": "The paper proposes TDT, which reformulates text detection as a signal processing task by treating token-level discrepancies as a time-series signal, applying Continuous Wavelet Transform for analysis.", "result": "TDT achieves 0.855 AUROC on the RAID benchmark, outperforming the best baseline by 7.1%, and shows a 14.1% AUROC improvement against paraphrasing attacks.", "conclusion": "The study demonstrates the importance of non-stationarity in AI-generated text and the need for methods that preserve temporal dynamics for effective detection.", "key_contributions": ["Introduction of Temporal Discrepancy Tomography (TDT) for text detection", "Demonstration of non-stationarity as a fundamental characteristic of AI-generated text", "Significant improvements in detection performance on standard benchmarks and adversarial tasks."], "limitations": "", "keywords": ["AI-generated text", "Detection", "Temporal Discrepancy Tomography", "Signal processing", "Non-stationarity"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.02592", "pdf": "https://arxiv.org/pdf/2508.02592.pdf", "abs": "https://arxiv.org/abs/2508.02592", "title": "Teaching Critical Visualization: A Field Report", "authors": ["Andrew McNutt", "Shiyi He", "Sujit Kumar Kamaraj", "Purbid Bambroo", "Nastaran Jadidi", "John Bovard", "Chang Han"], "categories": ["cs.HC"], "comment": "Accepted to EduVIS25", "summary": "Critical Visualization is gaining popularity and academic focus, yet\nrelatively few academic courses have been offered to support students in this\ncomplex area. This experience report describes a recent experimental course on\nthe topic, exploring both what the topic could be as well as an experimental\ncontent structure (namely as scavenger hunt). Generally the course was\nsuccessful, achieving the learning objectives of developing critical thinking\nskills, improving communication about complex ideas, and developing a knowledge\nabout theories in the area. While improvements can be made, we hope that\nhumanistic notions of criticality are embraced more deeply in visualization\npedagogy.", "AI": {"tldr": "A report on an experimental course in Critical Visualization that successfully developed students' critical thinking and communication skills through a scavenger hunt format.", "motivation": "To address the lack of academic courses in Critical Visualization and improve pedagogy in this field.", "method": "An experimental course structure was implemented, using a scavenger hunt format to teach critical visualization concepts.", "result": "The course met its learning objectives by enhancing critical thinking skills, communication about complex ideas, and understanding of relevant theories.", "conclusion": "The experience indicates a need for deeper integration of humanistic notions of criticality in visualization education despite areas for improvement.", "key_contributions": ["First exploration of scavenger hunt format for teaching Critical Visualization", "Successful attainment of learning objectives in critical thinking and communication", "Highlighting the importance of humanistic criticality in education"], "limitations": "Improvements needed in course structure and content delivery.", "keywords": ["Critical Visualization", "visualization pedagogy", "critical thinking", "educational methods", "scavenger hunt"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2508.01781", "pdf": "https://arxiv.org/pdf/2508.01781.pdf", "abs": "https://arxiv.org/abs/2508.01781", "title": "A comprehensive taxonomy of hallucinations in Large Language Models", "authors": ["Manuel Cossio"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "55 pages, 16 figures, 3 tables", "summary": "Large language models (LLMs) have revolutionized natural language processing,\nyet their propensity for hallucination, generating plausible but factually\nincorrect or fabricated content, remains a critical challenge. This report\nprovides a comprehensive taxonomy of LLM hallucinations, beginning with a\nformal definition and a theoretical framework that posits its inherent\ninevitability in computable LLMs, irrespective of architecture or training. It\nexplores core distinctions, differentiating between intrinsic (contradicting\ninput context) and extrinsic (inconsistent with training data or reality), as\nwell as factuality (absolute correctness) and faithfulness (adherence to\ninput). The report then details specific manifestations, including factual\nerrors, contextual and logical inconsistencies, temporal disorientation,\nethical violations, and task-specific hallucinations across domains like code\ngeneration and multimodal applications. It analyzes the underlying causes,\ncategorizing them into data-related issues, model-related factors, and\nprompt-related influences. Furthermore, the report examines cognitive and human\nfactors influencing hallucination perception, surveys evaluation benchmarks and\nmetrics for detection, and outlines architectural and systemic mitigation\nstrategies. Finally, it introduces web-based resources for monitoring LLM\nreleases and performance. This report underscores the complex, multifaceted\nnature of LLM hallucinations and emphasizes that, given their theoretical\ninevitability, future efforts must focus on robust detection, mitigation, and\ncontinuous human oversight for responsible and reliable deployment in critical\napplications.", "AI": {"tldr": "This report presents a detailed taxonomy of hallucinations in large language models (LLMs), examining their causes, manifestations, and mitigation strategies, while underscoring the inherent inevitability of such issues in LLMs.", "motivation": "To address the critical challenge posed by hallucinations in LLMs, which generate plausible but factually incorrect content, and to provide a structured understanding of their types and implications for responsible use.", "method": "The report offers a comprehensive taxonomy of LLM hallucinations, categorizing them into intrinsic and extrinsic types, and evaluates their causes, assessment metrics, and mitigation strategies.", "result": "It identifies various manifestations of hallucinations such as factual errors and ethical violations, and explores factors influencing hallucination perception, proposing a framework for assessment and improvement.", "conclusion": "The study emphasizes that hallucinations are an inevitable aspect of LLMs, suggesting that future work should focus on improving detection and mitigation strategies, alongside ensuring human oversight in deployment.", "key_contributions": ["A formal taxonomy classifying LLM hallucinations", "Identification of intrinsic vs extrinsic hallucinations", "Proposed measures for monitoring and mitigating hallucinations in LLM applications"], "limitations": "The report may not cover all potential hallucination scenarios and is primarily focused on theoretical aspects; practical evaluation and real-world application may vary.", "keywords": ["large language models", "hallucinations", "taxonomy", "machine learning", "natural language processing"], "importance_score": 9, "read_time_minutes": 40}}
{"id": "2508.02593", "pdf": "https://arxiv.org/pdf/2508.02593.pdf", "abs": "https://arxiv.org/abs/2508.02593", "title": "Explainable AI for Automated User-specific Feedback in Surgical Skill Acquisition", "authors": ["Catalina Gomez", "Lalithkumar Seenivasan", "Xinrui Zou", "Jeewoo Yoon", "Sirui Chu", "Ariel Leong", "Patrick Kramer", "Yu-Chun Ku", "Jose L. Porras", "Alejandro Martin-Gomez", "Masaru Ishii", "Mathias Unberath"], "categories": ["cs.HC", "cs.AI"], "comment": "10 pages, 4 figures", "summary": "Traditional surgical skill acquisition relies heavily on expert feedback, yet\ndirect access is limited by faculty availability and variability in subjective\nassessments. While trainees can practice independently, the lack of\npersonalized, objective, and quantitative feedback reduces the effectiveness of\nself-directed learning. Recent advances in computer vision and machine learning\nhave enabled automated surgical skill assessment, demonstrating the feasibility\nof automatic competency evaluation. However, it is unclear whether such\nArtificial Intelligence (AI)-driven feedback can contribute to skill\nacquisition. Here, we examine the effectiveness of explainable AI\n(XAI)-generated feedback in surgical training through a human-AI study. We\ncreate a simulation-based training framework that utilizes XAI to analyze\nvideos and extract surgical skill proxies related to primitive actions. Our\nintervention provides automated, user-specific feedback by comparing trainee\nperformance to expert benchmarks and highlighting deviations from optimal\nexecution through understandable proxies for actionable guidance. In a\nprospective user study with medical students, we compare the impact of\nXAI-guided feedback against traditional video-based coaching on task outcomes,\ncognitive load, and trainees' perceptions of AI-assisted learning. Results\nshowed improved cognitive load and confidence post-intervention. While no\ndifferences emerged between the two feedback types in reducing performance gaps\nor practice adjustments, trends in the XAI group revealed desirable effects\nwhere participants more closely mimicked expert practice. This work encourages\nthe study of explainable AI in surgical education and the development of\ndata-driven, adaptive feedback mechanisms that could transform learning\nexperiences and competency assessment.", "AI": {"tldr": "This paper explores the effectiveness of explainable AI (XAI)-generated feedback in surgical training, comparing it to traditional coaching methods.", "motivation": "The study addresses limitations in traditional surgical skill acquisition due to reliance on expert feedback and the need for personalized, objective assessments.", "method": "The authors developed a simulation-based training framework utilizing XAI to analyze surgical videos, providing automated, user-specific feedback based on expert benchmarks.", "result": "The intervention improved cognitive load and confidence among trainees, with trends indicating that participants in the XAI group closely mimicked expert practices, despite no significant differences in performance gap reduction.", "conclusion": "The findings support the potential of explainable AI in surgical education and emphasize the need for adaptive, data-driven feedback mechanisms.", "key_contributions": ["Development of a simulation-based framework using XAI for surgical training", "Comparison of XAI feedback with traditional coaching methods", "Demonstration of improved cognitive load and confidence in trainees"], "limitations": "No significant performance gap reduction was observed between feedback methods.", "keywords": ["surgical training", "explainable AI", "automated feedback", "skill acquisition", "cognitive load"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.01812", "pdf": "https://arxiv.org/pdf/2508.01812.pdf", "abs": "https://arxiv.org/abs/2508.01812", "title": "HeQ: a Large and Diverse Hebrew Reading Comprehension Benchmark", "authors": ["Amir DN Cohen", "Hilla Merhav", "Yoav Goldberg", "Reut Tsarfaty"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Current benchmarks for Hebrew Natural Language Processing (NLP) focus mainly\non morpho-syntactic tasks, neglecting the semantic dimension of language\nunderstanding. To bridge this gap, we set out to deliver a Hebrew Machine\nReading Comprehension (MRC) dataset, where MRC is to be realized as extractive\nQuestion Answering. The morphologically rich nature of Hebrew poses a challenge\nto this endeavor: the indeterminacy and non-transparency of span boundaries in\nmorphologically complex forms lead to annotation inconsistencies,\ndisagreements, and flaws in standard evaluation metrics.\n  To remedy this, we devise a novel set of guidelines, a controlled\ncrowdsourcing protocol, and revised evaluation metrics that are suitable for\nthe morphologically rich nature of the language. Our resulting benchmark, HeQ\n(Hebrew QA), features 30,147 diverse question-answer pairs derived from both\nHebrew Wikipedia articles and Israeli tech news. Our empirical investigation\nreveals that standard evaluation metrics such as F1 scores and Exact Match (EM)\nare not appropriate for Hebrew (and other MRLs), and we propose a relevant\nenhancement.\n  In addition, our experiments show low correlation between models' performance\non morpho-syntactic tasks and on MRC, which suggests that models designed for\nthe former might underperform on semantics-heavy tasks. The development and\nexploration of HeQ illustrate some of the challenges MRLs pose in natural\nlanguage understanding (NLU), fostering progression towards more and better NLU\nmodels for Hebrew and other MRLs.", "AI": {"tldr": "The paper presents a new Hebrew Machine Reading Comprehension (MRC) dataset addressing semantic analysis challenges in Hebrew NLP and proposes improved evaluation metrics.", "motivation": "To address the lack of a focus on semantic understanding in Hebrew NLP benchmarks and to improve reading comprehension through MRC.", "method": "Developed a controlled crowdsourcing protocol and novel guidelines for annotating a custom Hebrew MRC dataset with 30,147 question-answer pairs.", "result": "The study found that existing evaluation metrics like F1 scores and Exact Match do not work effectively for Hebrew, and proposed new metrics. It also demonstrated low correlation between performance on morpho-syntactic tasks and reading comprehension.", "conclusion": "The HeQ dataset and the proposed evaluation metrics pave the way for better NLU models for Hebrew and similar morphologically rich languages.", "key_contributions": ["Creation of a novel Hebrew MRC dataset (HeQ) with extensive question-answer pairs.", "Development of tailored evaluation metrics for morphologically rich languages.", "Insights into the performance gap between morpho-syntactic and semantic task models."], "limitations": "The dataset and metrics are specifically designed for Hebrew, potentially limiting generalizability to other languages.", "keywords": ["Hebrew NLP", "Machine Reading Comprehension", "Natural Language Understanding", "Morphologically Rich Languages", "Question Answering"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2508.02610", "pdf": "https://arxiv.org/pdf/2508.02610.pdf", "abs": "https://arxiv.org/abs/2508.02610", "title": "PunchPulse: A Physically Demanding Virtual Reality Boxing Game Designed with, for and by Blind and Low-Vision Players", "authors": ["Sanchita S. Kamath", "Omar Khan", "Anurag Choudhary", "Jan Meyerhoff-Liang", "Soyoung Choi", "JooYoung Seo"], "categories": ["cs.HC"], "comment": null, "summary": "Blind and low-vision (BLV) individuals experience lower levels of physical\nactivity (PA) compared to sighted peers due to a lack of accessible, engaging\nexercise options. Existing solutions often rely on auditory cues but do not\nfully integrate rich sensory feedback or support spatial navigation, limiting\ntheir effectiveness. This study introduces PunchPulse, a virtual reality (VR)\nboxing exergame designed to motivate BLV users to reach and sustain moderate to\nvigorous physical activity (MVPA) levels. Over a seven-month, multi-phased\nstudy, PunchPulse was iteratively refined with three BLV co-designers, informed\nby two early pilot testers, and evaluated by six additional BLV user-study\nparticipants. Data collection included both qualitative (researcher\nobservations, SOPI) and quantitative (MVPA zones, aid usage, completion times)\nmeasures of physical exertion and gameplay performance. The user study revealed\nthat all participants reached moderate MVPA thresholds, with high levels of\nimmersion and engagement observed. This work demonstrates the potential of VR\nas an inclusive medium for promoting meaningful PA in the BLV community and\naddresses a critical gap in accessible, intensity-driven exercise\ninterventions.", "AI": {"tldr": "PunchPulse is a VR boxing exergame aimed at increasing physical activity in blind and low-vision individuals by providing engaging sensory feedback and promoting spatial navigation.", "motivation": "Blind and low-vision individuals face barriers to engaging in physical activity due to a lack of accessible exercise options and the limitations of existing auditory-based solutions.", "method": "The study utilized a multi-phased iterative design involving three BLV co-designers, with evaluations conducted through qualitative observations and quantitative measures of physical exertion and gameplay performance across different user study participants.", "result": "Participants reached moderate to vigorous physical activity thresholds, demonstrating high levels of immersion and engagement while using PunchPulse.", "conclusion": "PunchPulse illustrates the potential of virtual reality as an inclusive tool to enhance physical activity in the blind and low-vision community, filling a significant gap in exercise intervention accessibility.", "key_contributions": ["Introduction of PunchPulse, a VR boxing exergame for BLV individuals.", "Iterative co-design process with BLV users to refine the game.", "Demonstration of effective engagement and physical exertion in BLV users."], "limitations": "The study was limited to a small sample size and specific user group; broader applicability and longer-term effectiveness of the game need further exploration.", "keywords": ["Blind and low-vision", "Virtual reality", "Physical activity", "Exergame", "Co-design"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.01815", "pdf": "https://arxiv.org/pdf/2508.01815.pdf", "abs": "https://arxiv.org/abs/2508.01815", "title": "AGENTICT$^2$S:Robust Text-to-SPARQL via Agentic Collaborative Reasoning over Heterogeneous Knowledge Graphs for the Circular Economy", "authors": ["Yang Zhao", "Chengxiao Dai", "Wei Zhuo", "Tan Chuan Fu", "Yue Xiu", "Dusit Niyato", "Jonathan Z. Low", "Eugene Ho Hong Zhuang", "Daren Zong Loong Tan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Question answering over heterogeneous knowledge graphs (KGQA) involves\nreasoning across diverse schemas, incomplete alignments, and distributed data\nsources. Existing text-to-SPARQL approaches rely on large-scale domain-specific\nfine-tuning or operate within single-graph settings, limiting their\ngeneralizability in low-resource domains and their ability to handle queries\nspanning multiple graphs. These challenges are particularly relevant in domains\nsuch as the circular economy, where information about classifications,\nprocesses, and emissions is distributed across independently curated knowledge\ngraphs (KGs). We present AgenticT$^2$S, a modular framework that decomposes\nKGQA into subtasks managed by specialized agents responsible for retrieval,\nquery generation, and verification. A scheduler assigns subgoals to different\ngraphs using weak-to-strong alignment strategies. A two-stage verifier detects\nstructurally invalid and semantically underspecified queries through symbolic\nvalidation and counterfactual consistency checks. Experiments on real-world\ncircular economy KGs demonstrate that AgenticT$^2$S improves execution accuracy\nby 17.3% and triple level F$_1$ by 25.4% over the best baseline, while reducing\nthe average prompt length by 46.4%. These results demonstrate the benefits of\nagent-based schema-aware reasoning for scalable KGQA and support\ndecision-making in sustainability domains through robust cross-graph reasoning.", "AI": {"tldr": "AgenticT²S is a modular framework for question answering over heterogeneous knowledge graphs, improving accuracy and efficiency in diverse domains like the circular economy.", "motivation": "The difficulty in reasoning across heterogeneous knowledge graphs (KGs) due to incomplete alignments and diverse schemas makes effective KGQA challenging, especially in low-resource domains.", "method": "AgenticT²S decomposes KGQA into subtasks managed by specialized agents for retrieval, query generation, and verification, employing weak-to-strong alignment strategies for scheduling.", "result": "AgenticT²S improves execution accuracy by 17.3% and F₁ score by 25.4%, while reducing average prompt length by 46.4% on real-world circular economy KGs.", "conclusion": "The results affirm the advantages of agent-based schema-aware reasoning for scalable KGQA, facilitating decision-making in sustainability fields through effective cross-graph reasoning.", "key_contributions": ["Modular framework decomposing KGQA into specialized subtasks", "Improved execution accuracy and F₁ score on circular economy datasets", "Reduction in average prompt length for queries"], "limitations": "", "keywords": ["knowledge graphs", "question answering", "agent-based reasoning", "sustainability", "circular economy"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.02639", "pdf": "https://arxiv.org/pdf/2508.02639.pdf", "abs": "https://arxiv.org/abs/2508.02639", "title": "Reframing Pattern: A Comprehensive Approach to a Composite Visual Variable", "authors": ["Tingying He", "Jason Dykes", "Petra Isenberg", "Tobias Isenberg"], "categories": ["cs.HC"], "comment": null, "summary": "We present a new comprehensive theory for explaining, exploring, and using\npattern as a visual variable in visualization. Although patterns have long been\nused for data encoding and continue to be valuable today, their conceptual\nfoundations are precarious: the concepts and terminology used across the\nresearch literature and in practice are inconsistent, making it challenging to\nuse patterns effectively and to conduct research to inform their use. To\naddress this problem, we conduct a comprehensive cross-disciplinary literature\nreview that clarifies ambiguities around the use of \"pattern\" and \"texture\". As\na result, we offer a new consistent treatment of pattern as a composite visual\nvariable composed of structured groups of graphic primitives that can serve as\nmarks for encoding data individually and collectively. This new and widely\napplicable formulation opens a sizable design space for the visual variable\npattern, which we formalize as a new system comprising three sets of variables:\nthe spatial arrangement of primitives, the appearance relationships among\nprimitives, and the retinal visual variables that characterize individual\nprimitives. We show how our pattern system relates to existing visualization\ntheory and highlight opportunities for visualization design. We further explore\npatterns based on complex spatial arrangements, demonstrating explanatory power\nand connecting our conceptualization to broader theory on maps and cartography.\nAn author version and additional materials are available on OSF: osf.io/z7ae2.", "AI": {"tldr": "This paper presents a new theory for understanding and using patterns as a visual variable in visualization, addressing inconsistencies in the terminology and concepts found in existing literature.", "motivation": "The inconsistent use of concepts related to 'pattern' and 'texture' in visualization research hinders effective application and further research in this area.", "method": "A comprehensive cross-disciplinary literature review is conducted to clarify ambiguities, leading to the formulation of a new consistent treatment of pattern as a composite visual variable made up of graphic primitives.", "result": "The proposed pattern system includes the spatial arrangement of primitives, appearance relationships among primitives, and retinal visual variables that characterize them, which opens new design opportunities in visualization.", "conclusion": "The new pattern system integrates seamlessly with existing visualization theories and shows potential for enhancing visualization design, particularly in complex spatial arrangements.", "key_contributions": ["New theoretical framework for understanding patterns in visualization", "Identification of three sets of variables for pattern design", "Connection of visualization concepts to broader theories of maps and cartography"], "limitations": "", "keywords": ["pattern", "visualization", "texture", "data encoding", "design theory"], "importance_score": 4, "read_time_minutes": 12}}
{"id": "2508.01832", "pdf": "https://arxiv.org/pdf/2508.01832.pdf", "abs": "https://arxiv.org/abs/2508.01832", "title": "MLP Memory: Language Modeling with Retriever-pretrained External Memory", "authors": ["Rubin Wei", "Jiaqi Cao", "Jiarui Wang", "Jushi Kai", "Qipeng Guo", "Bowen Zhou", "Zhouhan Lin"], "categories": ["cs.CL"], "comment": null, "summary": "While modern decoder-only LLMs achieve superior performance across various\ndomains, hallucinations have risen to be a common problem in their generated\ntext, hindering their application in knowledge-intensive tasks.\nRetriever-augmented generation (RAG) offers a solution, but the non-parametric\nnature of the retriever hinders its deep interaction with LLM. In this work, we\npropose to decouple memorization from the LLM decoder using a pretrained,\ndifferentiable external memory. The external memory is an MLP pretrained by\nimitating the behavior of a retriever on the entire pretraining dataset. Our\nresulting architecture, which comprises a transformer decoder and an external\nMLP memory pretrained on language modeling and retriever imitation\nrespectively, demonstrates strong perplexity and performance on downstream\ntasks. Experiments show our architecture exhibits steeper power-law scaling\nwith model size, achieving 17.5% and 24.1% improvement on WikiText-103 and Web\ndatasets compared to decoder-only models while benefiting from added training\nwithout overfitting. We demonstrate superior performance on three hallucination\nbenchmarks and nine memory-intensive tasks. Additionally, our approach delivers\n$80\\times$ speedup over $k$NN-LM (500M tokens) and $1.3\\times$ faster inference\nthan decoder-only models. Unlike $k$NN-LM, which impairs reasoning, our MLP\nmemory improves StrategyQA performance. We will open-source our code and models\nin the future.", "AI": {"tldr": "The paper proposes a novel architecture combining a transformer decoder and a differentiable external memory to reduce hallucinations in LLMs and enhance performance on knowledge-intensive tasks.", "motivation": "To address hallucination issues in decoder-only LLMs, making them more suitable for knowledge-intensive tasks.", "method": "The proposed method decouples memorization from the LLM using a pretrained, differentiable external memory in the form of an MLP, designed to imitate the behavior of a retriever on the pretraining dataset.", "result": "The architecture shows significant improvements in perplexity and performance on downstream tasks, achieving up to 24.1% improvement on WikiText-103 and Web datasets, while maintaining efficient speed during inference.", "conclusion": "The architecture enhances LLM performance on various tasks, reduces hallucinations, and shows substantial inference speedup without impairing reasoning abilities.", "key_contributions": ["Introduction of a differentiable external memory to LLMs", "Demonstrated superior performance on hallucination benchmarks", "Achieved significant speedup compared to existing methodologies"], "limitations": "", "keywords": ["large language models", "retriever-augmented generation", "external memory", "hallucinations", "machine learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "1907.00326", "pdf": "https://arxiv.org/pdf/1907.00326.pdf", "abs": "https://arxiv.org/abs/1907.00326", "title": "Observing Dialogue in Therapy: Categorizing and Forecasting Behavioral Codes", "authors": ["Jie Cao", "Michael Tanana", "Zac E. Imel", "Eric Poitras", "David C. Atkins", "Vivek Srikumar"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": "Accepted to ACL 2019", "summary": "Automatically analyzing dialogue can help understand and guide behavior in\ndomains such as counseling, where interactions are largely mediated by\nconversation. In this paper, we study modeling behavioral codes used to asses a\npsychotherapy treatment style called Motivational Interviewing (MI), which is\neffective for addressing substance abuse and related problems. Specifically, we\naddress the problem of providing real-time guidance to therapists with a\ndialogue observer that (1) categorizes therapist and client MI behavioral codes\nand, (2) forecasts codes for upcoming utterances to help guide the conversation\nand potentially alert the therapist. For both tasks, we define neural network\nmodels that build upon recent successes in dialogue modeling. Our experiments\ndemonstrate that our models can outperform several baselines for both tasks. We\nalso report the results of a careful analysis that reveals the impact of the\nvarious network design tradeoffs for modeling therapy dialogue.", "AI": {"tldr": "This paper develops neural network models to analyze and provide real-time guidance for therapists using Motivational Interviewing in psychotherapy, focusing on classifying behavioral codes and forecasting dialogue.", "motivation": "Understanding and guiding therapist-client interactions in psychotherapy, specifically using Motivational Interviewing techniques, to improve therapeutic outcomes.", "method": "Neural network models are designed to classify MI behavioral codes and forecast future codes in therapy dialogues, grounded in recent dialogue modeling successes.", "result": "The developed models outperform several baseline approaches in accurately classifying and forecasting MI behavioral codes in therapy sessions.", "conclusion": "The study highlights the design tradeoffs in neural network architecture for effective modeling of therapy dialogues and the potential for real-time guidance in therapy settings.", "key_contributions": ["Introduction of a dialogue observer for MI behavioral codes", "Development of neural network models for real-time conversation guidance", "Insights into network design tradeoffs for therapy dialogue"], "limitations": "The study may have limitations in generalizability across different therapeutic approaches beyond Motivational Interviewing.", "keywords": ["Motivational Interviewing", "dialogue analysis", "neural networks", "real-time guidance", "psychotherapy"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.01858", "pdf": "https://arxiv.org/pdf/2508.01858.pdf", "abs": "https://arxiv.org/abs/2508.01858", "title": "Web-CogReasoner: Towards Knowledge-Induced Cognitive Reasoning for Web Agents", "authors": ["Yuhan Guo", "Cong Guo", "Aiwen Sun", "Hongliang He", "Xinyu Yang", "Yue Lu", "Yingji Zhang", "Xuntao Guo", "Dong Zhang", "Jianzhuang Liu", "Jiang Duan", "Yijia Xiao", "Liangjian Wen", "Hai-Ming Xu", "Yong Dai"], "categories": ["cs.CL", "cs.AI"], "comment": "Our code and data is open sourced at\n  https://github.com/Gnonymous/Web-CogReasoner", "summary": "Multimodal large-scale models have significantly advanced the development of\nweb agents, enabling perception and interaction with digital environments akin\nto human cognition. In this paper, we argue that web agents must first acquire\nsufficient knowledge to effectively engage in cognitive reasoning. Therefore,\nwe decompose a web agent's capabilities into two essential stages: knowledge\ncontent learning and cognitive processes. To formalize this, we propose\nWeb-CogKnowledge Framework, categorizing knowledge as Factual, Conceptual, and\nProcedural. In this framework, knowledge content learning corresponds to the\nagent's processes of Memorizing and Understanding, which rely on the first two\nknowledge types, representing the \"what\" of learning. Conversely, cognitive\nprocesses correspond to Exploring, grounded in Procedural knowledge, defining\nthe \"how\" of reasoning and action. To facilitate knowledge acquisition, we\nconstruct the Web-CogDataset, a structured resource curated from 14 real-world\nwebsites, designed to systematically instill core knowledge necessary for web\nagent. This dataset serves as the agent's conceptual grounding-the \"nouns\" upon\nwhich comprehension is built-as well as the basis for learning how to reason\nand act. Building on this foundation, we operationalize these processes through\na novel knowledge-driven Chain-of-Thought (CoT) reasoning framework, developing\nand training our proposed agent, the Web-CogReasoner. Extensive experimentation\nreveals its significant superiority over existing models, especially in\ngeneralizing to unseen tasks where structured knowledge is decisive. To enable\nrigorous evaluation, we introduce the Web-CogBench, a comprehensive evaluation\nsuite designed to assess and compare agent performance across the delineated\nknowledge domains and cognitive capabilities. Our code and data is open sourced\nat https://github.com/Gnonymous/Web-CogReasoner", "AI": {"tldr": "This paper presents the Web-CogKnowledge Framework for web agents, emphasizing the importance of knowledge acquisition and cognitive reasoning in their development. It introduces Web-CogDataset for structured knowledge and proposes a knowledge-driven Chain-of-Thought reasoning framework implemented in the Web-CogReasoner, demonstrating significant advancements in agent capabilities.", "motivation": "The need for web agents to effectively engage in cognitive reasoning by first acquiring sufficient knowledge.", "method": "The paper proposes the Web-CogKnowledge Framework, categorizing knowledge into Factual, Conceptual, and Procedural types, and constructs the Web-CogDataset from 14 real-world websites. It operationalizes knowledge acquisition through a knowledge-driven Chain-of-Thought reasoning framework called Web-CogReasoner.", "result": "The Web-CogReasoner demonstrates significant superiority over existing models, particularly in generalizing to unseen tasks, highlighting the importance of structured knowledge.", "conclusion": "The study emphasizes the necessity of a structured approach to knowledge acquisition and cognitive reasoning in developing advanced web agents, with open-sourced resources for further research.", "key_contributions": ["Introduction of the Web-CogKnowledge Framework", "Development of the Web-CogDataset for knowledge acquisition", "Implementation of the knowledge-driven Chain-of-Thought reasoning framework in the Web-CogReasoner."], "limitations": "", "keywords": ["web agents", "cognitive reasoning", "knowledge acquisition", "Chain-of-Thought", "HCI"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2412.13395", "pdf": "https://arxiv.org/pdf/2412.13395.pdf", "abs": "https://arxiv.org/abs/2412.13395", "title": "Enhancing Talk Moves Analysis in Mathematics Tutoring through Classroom Teaching Discourse", "authors": ["Jie Cao", "Abhijit Suresh", "Jennifer Jacobs", "Charis Clevenger", "Amanda Howard", "Chelsea Brown", "Brent Milne", "Tom Fischaber", "Tamara Sumner", "James H. Martin"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": "Accepted to COLING'2025", "summary": "Human tutoring interventions play a crucial role in supporting student\nlearning, improving academic performance, and promoting personal growth. This\npaper focuses on analyzing mathematics tutoring discourse using talk moves - a\nframework of dialogue acts grounded in Accountable Talk theory. However,\nscaling the collection, annotation, and analysis of extensive tutoring\ndialogues to develop machine learning models is a challenging and\nresource-intensive task. To address this, we present SAGA22, a compact dataset,\nand explore various modeling strategies, including dialogue context, speaker\ninformation, pretraining datasets, and further fine-tuning. By leveraging\nexisting datasets and models designed for classroom teaching, our results\ndemonstrate that supplementary pretraining on classroom data enhances model\nperformance in tutoring settings, particularly when incorporating longer\ncontext and speaker information. Additionally, we conduct extensive ablation\nstudies to underscore the challenges in talk move modeling.", "AI": {"tldr": "This paper analyzes mathematics tutoring discourse using talk moves and presents SAGA22 dataset to improve machine learning models for tutoring.", "motivation": "To understand and enhance student learning through effective tutoring interventions while addressing challenges in scaling dialogue analysis for machine learning.", "method": "Analysis of tutoring discourse using the talk moves framework, development of the SAGA22 dataset, and exploration of various machine learning modeling strategies.", "result": "The research shows that supplementary pretraining on classroom data boosts model performance in tutoring contexts, especially with longer dialogue context and speaker information.", "conclusion": "The study highlights the potential of utilizing existing classroom data for improving tutoring dialogue modeling and identifies the need for further research in talk move modeling.", "key_contributions": ["Introduction of the SAGA22 dataset for tutoring discourse analysis.", "Demonstration of the effectiveness of supplementary pretraining on classroom datasets.", "Identification of key factors (dialogue context and speaker information) that enhance model performance."], "limitations": "The challenges in effectively modeling talk moves and the resource-intensive nature of the annotation process.", "keywords": ["Human tutoring", "Machine learning", "SAGA22 dataset", "Tutoring discourse", "Talk moves"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2508.01862", "pdf": "https://arxiv.org/pdf/2508.01862.pdf", "abs": "https://arxiv.org/abs/2508.01862", "title": "Counterfactual Probing for Hallucination Detection and Mitigation in Large Language Models", "authors": ["Yijun Feng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models have demonstrated remarkable capabilities across\ndiverse tasks, yet they frequently generate hallucinations outputs that are\nfluent but factually incorrect or unsupported. We propose Counterfactual\nProbing, a novel approach for detecting and mitigating hallucinations in LLM\noutputs. Our method dynamically generates counterfactual statements that appear\nplausible but contain subtle factual errors, then evaluates the model's\nsensitivity to these perturbations. We hypothesize that genuine knowledge\nexhibits robustness to counterfactual variations, while hallucinated content\nshows inconsistent confidence patterns when confronted with plausible\nalternatives. Our comprehensive evaluation on TruthfulQA, factual statement\ndatasets, and curated hallucination examples demonstrates that counterfactual\nprobing achieves superior detection performance compared to baseline methods,\nwhile our adaptive mitigation strategies reduce hallucination scores by an\naverage of 24.5%. The approach requires no model retraining and can be\nintegrated into existing LLM pipelines as a realtime verification mechanism.", "AI": {"tldr": "Proposed Counterfactual Probing method detects and mitigates hallucinations in LLM outputs using dynamically generated counterfactual statements.", "motivation": "To address the issue of hallucinations in LLM outputs, which are fluent yet factually incorrect.", "method": "Counterfactual Probing dynamically generates plausible counterfactual statements with subtle factual errors and assesses the model's confidence in response to these variations.", "result": "Counterfactual Probing outperforms baseline methods in detecting hallucinations and reduces hallucination scores by an average of 24.5%.", "conclusion": "The method integrates seamlessly into existing LLM pipelines as a real-time verification mechanism without requiring model retraining.", "key_contributions": ["Introduction of Counterfactual Probing to detect hallucinations in LLM outputs", "Demonstrated superior detection performance on multiple datasets", "Adaptive mitigation strategies that significantly reduce hallucination scores"], "limitations": "", "keywords": ["Large Language Models", "hallucinations", "counterfactual probing", "real-time verification", "factually incorrect"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2508.01918", "pdf": "https://arxiv.org/pdf/2508.01918.pdf", "abs": "https://arxiv.org/abs/2508.01918", "title": "Quantum-RAG and PunGPT2: Advancing Low-Resource Language Generation and Retrieval for the Punjabi Language", "authors": ["Jaskaranjeet Singh", "Rakesh Thakur"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite the rapid advancement of large language models (LLMs), low-resource\nlanguages remain largely excluded from the NLP landscape. We present PunGPT2,\nthe first fully open-source suite of Punjabi large language models, trained\nfrom scratch on a 35GB domain-diverse corpus encompassing literature, religious\ntexts, news, and social discourse. Unlike prior multilingual approaches,\nPunGPT2 captures rich syntactic and morphological features unique to Punjabi\nthrough a tokenizer optimised with byte pair encoding and linguistically\naligned pretraining objectives. To improve factual grounding and domain recall,\nwe introduce Pun-RAG, a retrieval-augmented generation framework combining\nPunGPT2 with a dense FAISS retriever over a curated Punjabi knowledge base. We\nfurther develop Pun-Instruct, a parameter-efficient, instruction-tuned variant\nusing QLoRA, enabling robust zero-shot and instruction-following performance\nwith significantly reduced compute needs.\n  As a key innovation, we propose Quantum-RAG, a novel hybrid retrieval system\nthat fuses sparse (BM25) and dense methods with quantum-inspired semantic\nmatching. By encoding queries using amplitude-based embeddings and retrieving\nvia quantum kernel similarity, Quantum-RAG achieves improved contextual\nrelevance with minimal memory overhead marking the first practical integration\nof quantum representations in low-resource language generation. Our models\nsignificantly outperform strong multilingual baselines (mBERT, mT5, MuRIL) in\nperplexity, factuality, and fluency. This work provides a scalable,\nreproducible blueprint for extending LLM capabilities to underrepresented\nlanguages and pioneers quantum-aware retrieval in low-resource NLP", "AI": {"tldr": "This work introduces PunGPT2, a suite of open-source large language models for Punjabi, enhancing NLP capabilities for low-resource languages and pioneering quantum-aware retrieval methods with improved contextual relevance.", "motivation": "To address the exclusion of low-resource languages from the NLP landscape and enhance the capabilities of language technologies for Punjabi.", "method": "PunGPT2 was trained on a 35GB corpus using a tokenizer optimized with byte pair encoding, supplemented by a retrieval-augmented generation framework called Pun-RAG. Quantum-RAG was also introduced to combine sparse and dense retrieval methods using quantum-inspired semantic matching.", "result": "PunGPT2 outperforms multilingual baselines like mBERT, mT5, and MuRIL in perplexity, factuality, and fluency.", "conclusion": "This work presents a framework for extending LLM capabilities to underrepresented languages and integrates quantum retrieval methods in low-resource NLP.", "key_contributions": ["Development of the first open-source Punjabi LLMs (PunGPT2)", "Introduction of Pun-RAG for improved factual grounding and recall", "Proposal of Quantum-RAG for hybrid retrieval combining sparse and dense methods with quantum techniques."], "limitations": "", "keywords": ["Punjabi", "Large Language Models", "Retrieval-Augmented Generation", "Quantum Retrieval", "Low-Resource NLP"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.01930", "pdf": "https://arxiv.org/pdf/2508.01930.pdf", "abs": "https://arxiv.org/abs/2508.01930", "title": "Word Overuse and Alignment in Large Language Models: The Influence of Learning from Human Feedback", "authors": ["Tom S. Juzek", "Zina B. Ward"], "categories": ["cs.CL", "cs.AI", "68T50", "I.2; I.2.7; I.2.6"], "comment": "Accepted for publication in the Proceedings of the 5th Workshop on\n  Bias and Fairness in AI (BIAS 2025) at ECML PKDD", "summary": "Large Language Models (LLMs) are known to overuse certain terms like \"delve\"\nand \"intricate.\" The exact reasons for these lexical choices, however, have\nbeen unclear. Using Meta's Llama model, this study investigates the\ncontribution of Learning from Human Feedback (LHF), under which we subsume\nReinforcement Learning from Human Feedback and Direct Preference Optimization.\nWe present a straightforward procedure for detecting the lexical preferences of\nLLMs that are potentially LHF-induced. Next, we more conclusively link LHF to\nlexical overuse by experimentally emulating the LHF procedure and demonstrating\nthat participants systematically prefer text variants that include certain\nwords. This lexical overuse can be seen as a sort of misalignment, though our\nstudy highlights the potential divergence between the lexical expectations of\ndifferent populations -- namely LHF workers versus LLM users. Our work\ncontributes to the growing body of research on explainable artificial\nintelligence and emphasizes the importance of both data and procedural\ntransparency in alignment research.", "AI": {"tldr": "This study explores how Learning from Human Feedback (LHF) affects the lexical choices of Large Language Models (LLMs), demonstrating a potential misalignment between the preferences of LHF workers and LLM users.", "motivation": "To investigate the reasons behind the lexical overuse of certain terms in LLMs and the influence of Learning from Human Feedback (LHF) on these preferences.", "method": "The study uses Meta's Llama model to identify lexical preferences potentially induced by LHF and conducts experiments to link LHF to lexical overuse by analyzing participant text preferences.", "result": "Participants showed a systematic preference for text variants that included certain overused words, indicating LHF's influence on LLM lexical choices.", "conclusion": "The findings reveal a possible misalignment in lexical expectations between LHF workers and LLM users, highlighting the need for transparency in alignment research.", "key_contributions": ["Developed a procedure for detecting LHF-induced lexical preferences in LLMs.", "Concretely linked LHF to lexical overuse through experimental emulation.", "Emphasized the importance of understanding differing lexical expectations of populations involved."], "limitations": "The study focuses on a limited set of lexical terms and does not address the broader implications of misalignment in detail.", "keywords": ["Large Language Models", "Learning from Human Feedback", "Lexical Overuse", "Explainable AI", "Alignment Research"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.01943", "pdf": "https://arxiv.org/pdf/2508.01943.pdf", "abs": "https://arxiv.org/abs/2508.01943", "title": "ROVER: Recursive Reasoning Over Videos with Vision-Language Models for Embodied Tasks", "authors": ["Philip Schroeder", "Ondrej Biza", "Thomas Weng", "Hongyin Luo", "James Glass"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.RO"], "comment": null, "summary": "Vision-language models (VLMs) have exhibited impressive capabilities across\ndiverse image understanding tasks, but still struggle in settings that require\nreasoning over extended sequences of camera frames from a video. This limits\ntheir utility in embodied settings, which require reasoning over long frame\nsequences from a continuous stream of visual input at each moment of a task\nattempt. To address this limitation, we propose ROVER (Reasoning Over VidEo\nRecursively), a framework that enables the model to recursively decompose\nlong-horizon video trajectories into segments corresponding to shorter subtasks\nwithin the trajectory. In doing so, ROVER facilitates more focused and accurate\nreasoning over temporally localized frame sequences without losing global\ncontext. We evaluate ROVER, implemented using an in-context learning approach,\non diverse OpenX Embodiment videos and on a new dataset derived from RoboCasa\nthat consists of 543 videos showing both expert and perturbed non-expert\ntrajectories across 27 robotic manipulation tasks. ROVER outperforms strong\nbaselines across three video reasoning tasks: task progress estimation,\nframe-level natural language reasoning, and video question answering. We\nobserve that, by reducing the number of frames the model reasons over at each\ntimestep, ROVER mitigates hallucinations, especially during unexpected or\nnon-optimal moments of a trajectory. In addition, by enabling the\nimplementation of a subtask-specific sliding context window, ROVER's time\ncomplexity scales linearly with video length, an asymptotic improvement over\nbaselines. Demos, code, and data available at: https://rover-vlm.github.io", "AI": {"tldr": "ROVER is a framework for improving reasoning in vision-language models by decomposing video tasks into shorter subtasks, which enhances accuracy and performance for video understanding.", "motivation": "Vision-language models struggle with extended sequences of camera frames in video-based tasks, limiting their effectiveness in real-time embodied settings.", "method": "ROVER decomposes long video trajectories into shorter segments for focused reasoning, utilizing an in-context learning approach.", "result": "ROVER outperformed baseline models in task progress estimation, frame-level reasoning, and video question answering, while reducing hallucinations and improving time complexity.", "conclusion": "The approach enhances performance in video reasoning tasks and scales efficiently with video length, making it suitable for real-time applications.", "key_contributions": ["Introduces ROVER for recursive video reasoning", "Improves accuracy by focusing on subtasks", "Demonstrates scalability with a sliding context window"], "limitations": "", "keywords": ["vision-language models", "video reasoning", "embodied AI", "subtask decomposition", "NLP"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.01959", "pdf": "https://arxiv.org/pdf/2508.01959.pdf", "abs": "https://arxiv.org/abs/2508.01959", "title": "SitEmb-v1.5: Improved Context-Aware Dense Retrieval for Semantic Association and Long Story Comprehension", "authors": ["Junjie Wu", "Jiangnan Li", "Yuqing Li", "Lemao Liu", "Liyan Xu", "Jiwei Li", "Dit-Yan Yeung", "Jie Zhou", "Mo Yu"], "categories": ["cs.CL"], "comment": "Our trained models can be downloaded from:\n  https://huggingface.co/SituatedEmbedding", "summary": "Retrieval-augmented generation (RAG) over long documents typically involves\nsplitting the text into smaller chunks, which serve as the basic units for\nretrieval. However, due to dependencies across the original document,\ncontextual information is often essential for accurately interpreting each\nchunk. To address this, prior work has explored encoding longer context windows\nto produce embeddings for longer chunks. Despite these efforts, gains in\nretrieval and downstream tasks remain limited. This is because (1) longer\nchunks strain the capacity of embedding models due to the increased amount of\ninformation they must encode, and (2) many real-world applications still\nrequire returning localized evidence due to constraints on model or human\nbandwidth.\n  We propose an alternative approach to this challenge by representing short\nchunks in a way that is conditioned on a broader context window to enhance\nretrieval performance -- i.e., situating a chunk's meaning within its context.\nWe further show that existing embedding models are not well-equipped to encode\nsuch situated context effectively, and thus introduce a new training paradigm\nand develop the situated embedding models (SitEmb). To evaluate our method, we\ncurate a book-plot retrieval dataset specifically designed to assess situated\nretrieval capabilities. On this benchmark, our SitEmb-v1 model based on BGE-M3\nsubstantially outperforms state-of-the-art embedding models, including several\nwith up to 7-8B parameters, with only 1B parameters. Our 8B SitEmb-v1.5 model\nfurther improves performance by over 10% and shows strong results across\ndifferent languages and several downstream applications.", "AI": {"tldr": "This paper proposes a novel approach for retrieval-augmented generation (RAG) using situated embedding models (SitEmb) that enhance retrieval performance by representing short chunks of text within broader contextual windows.", "motivation": "The challenge of accurately interpreting longer documents in retrieval-augmented generation (RAG) due to the limitations of conventional embedding models when processing longer chunks of text.", "method": "The authors introduce a new training paradigm for situated embedding models (SitEmb) that condition short text chunks on a broader context to improve retrieval performance.", "result": "The SitEmb-v1 model outperforms state-of-the-art embedding models, including those with 7-8B parameters, while only using 1B parameters. The enhanced SitEmb-v1.5 model further improves performance by over 10% across multiple languages and applications.", "conclusion": "The new situated embedding models effectively improve the retrieval process in the context of long documents, demonstrating stronger performance compared to existing methods.", "key_contributions": ["Introduction of the situated embedding models (SitEmb) paradigm.", "Development of a curated book-plot retrieval dataset for evaluation.", "Demonstration of substantial performance gains over state-of-the-art systems with fewer parameters."], "limitations": "Existing embedding models are not tailored for situated context; the current method may not generalize to all types of documents or contexts.", "keywords": ["retrieval-augmented generation", "situated embedding", "natural language processing", "contextual retrieval"], "importance_score": 8, "read_time_minutes": 7}}
{"id": "2508.01977", "pdf": "https://arxiv.org/pdf/2508.01977.pdf", "abs": "https://arxiv.org/abs/2508.01977", "title": "TIBSTC-CoT: A Multi-Domain Instruction Dataset for Chain-of-Thought Reasoning in Language Models", "authors": ["Fan Gao", "Cheng Huang", "Nyima Tashi", "Yutong Liu", "Xiangxiang Wang", "Thupten Tsering", "Ban Ma-bao", "Renzeg Duojie", "Gadeng Luosang", "Rinchen Dongrub", "Dorje Tashi", "Xiao Feng", "Hao Wang", "Yongbin Yu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "To address the severe data scarcity in Tibetan, a low-resource language\nspoken by over six million people, we introduce TIBSTC-CoT, the large-scale,\nmulti-domain Tibetan dataset automatically constructed via chain-of-thought\nprompting with large language models (LLMs). TIBSTC-CoT establishes a scalable\nand reproducible framework for dataset creation in low-resource settings,\ncovering diverse domains and reasoning patterns essential for language\nunderstanding and generation. Building on this dataset, we develop the\nSunshine-thinking LLM family, a series of Tibetan-centric LLMs equipped with\nchain-of-thought capabilities. Trained entirely on TIBSTC-CoT,\nSunshine-thinking has demonstrated strong reasoning and generation performance,\ncomparable to state-of-the-art (SOTA) multilingual LLMs. Our work marks a\nsignificant step toward inclusive AI by enabling high-quality Tibetan language\nprocessing through both resource creation and model innovation. All data are\navailable: https://github.com/Vicentvankor/sun-shine.", "AI": {"tldr": "Introduction of TIBSTC-CoT, a multi-domain Tibetan dataset, and the Sunshine-thinking LLM family for enhancing Tibetan language processing.", "motivation": "To address data scarcity in Tibetan, a low-resource language, and improve language understanding and generation through the development of targeted datasets and models.", "method": "Constructed TIBSTC-CoT dataset using chain-of-thought prompting with LLMs, leading to the development of Sunshine-thinking LLMs trained on this dataset.", "result": "Sunshine-thinking LLMs show strong reasoning and generation capabilities, on par with SOTA multilingual LLMs.", "conclusion": "The research advances inclusive AI for Tibetan by creating valuable language resources and innovative models.", "key_contributions": ["Introduction of the TIBSTC-CoT dataset for Tibetan", "Development of Sunshine-thinking LLM family", "Demonstration of high performance in reasoning and generation tasks for Tibetan language"], "limitations": "", "keywords": ["Tibetan", "low-resource languages", "large language models", "dataset construction", "AI inclusivity"], "importance_score": 5, "read_time_minutes": 5}}
{"id": "2508.01213", "pdf": "https://arxiv.org/pdf/2508.01213.pdf", "abs": "https://arxiv.org/abs/2508.01213", "title": "Show or Tell? Modeling the evolution of request-making in Human-LLM conversations", "authors": ["Shengqi Zhu", "Jeffrey M. Rzeszotarski", "David Mimno"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Chat logs provide a rich source of information about LLM users, but patterns\nof user behavior are often masked by the variability of queries. We present a\nnew task, segmenting chat queries into contents of requests, roles,\nquery-specific context, and additional expressions. We find that, despite the\nfamiliarity of chat-based interaction, request-making in LLM queries remains\nsignificantly different from comparable human-human interactions. With the data\nresource, we introduce an important perspective of diachronic analyses with\nuser expressions. We find that query patterns vary between early ones\nemphasizing requests, and individual users explore patterns but tend to\nconverge with experience. Finally, we show that model capabilities affect user\nbehavior, particularly with the introduction of new models, which are traceable\nat the community level.", "AI": {"tldr": "The paper presents a new task for segmenting chat queries in LLM interactions, highlighting the variances in user behavior and the impact of model capabilities on request-making.", "motivation": "To better understand user behaviors in LLM interactions and the nature of requests made in chat logs, which differ from human-human interactions.", "method": "The authors introduce a task of segmenting chat queries into various components, analyzing user expressions and patterns over time.", "result": "Results indicate that user queries evolve from initial focus on requests to a more explored pattern as users gain experience, with model capabilities significantly influencing these behaviors.", "conclusion": "Model updates affect community-level user behavior, revealing the need for diachronic analysis to understand interaction dynamics in LLMs.", "key_contributions": ["Introduction of a new segmentation task for chat queries", "Insights into the evolution of user request patterns", "Identification of the impact of model capabilities on user behavior"], "limitations": "Limited to chat interactions; findings may not apply to other forms of human-computer interactions.", "keywords": ["chat queries", "user behavior", "LLM", "segmentation", "request-making"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.01990", "pdf": "https://arxiv.org/pdf/2508.01990.pdf", "abs": "https://arxiv.org/abs/2508.01990", "title": "Contextually Aware E-Commerce Product Question Answering using RAG", "authors": ["Praveen Tangarajan", "Anand A. Rajasekar", "Manish Rathi", "Vinay Rao Dandin", "Ozan Ersoy"], "categories": ["cs.CL", "I.2.7; H.3.3"], "comment": "6 pages, 1 figure, 5 tables. Preprint under review", "summary": "E-commerce product pages contain a mix of structured specifications,\nunstructured reviews, and contextual elements like personalized offers or\nregional variants. Although informative, this volume can lead to cognitive\noverload, making it difficult for users to quickly and accurately find the\ninformation they need. Existing Product Question Answering (PQA) systems often\nfail to utilize rich user context and diverse product information effectively.\nWe propose a scalable, end-to-end framework for e-commerce PQA using Retrieval\nAugmented Generation (RAG) that deeply integrates contextual understanding. Our\nsystem leverages conversational history, user profiles, and product attributes\nto deliver relevant and personalized answers. It adeptly handles objective,\nsubjective, and multi-intent queries across heterogeneous sources, while also\nidentifying information gaps in the catalog to support ongoing content\nimprovement. We also introduce novel metrics to measure the framework's\nperformance which are broadly applicable for RAG system evaluations.", "AI": {"tldr": "Proposes an end-to-end framework for e-commerce Product Question Answering (PQA) utilizing Retrieval Augmented Generation (RAG) for improved user context integration and personalized responses.", "motivation": "To address cognitive overload in e-commerce product pages and improve the effectiveness of Product Question Answering systems by leveraging rich user context and diverse product information.", "method": "Developed a scalable, end-to-end framework for PQA that integrates conversational history, user profiles, and product attributes, utilizing RAG techniques.", "result": "The proposed system can handle various types of queries and identifies information gaps in product catalogs, thereby supporting ongoing improvement.", "conclusion": "The framework enhances answer delivery in e-commerce environments and introduces new performance metrics for RAG evaluations.", "key_contributions": ["Integration of user context and product information in PQA", "Scalable end-to-end framework leveraging RAG", "Introduction of novel metrics for evaluating RAG systems"], "limitations": ".", "keywords": ["e-commerce", "Product Question Answering", "Retrieval Augmented Generation", "user context", "performance metrics"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.01999", "pdf": "https://arxiv.org/pdf/2508.01999.pdf", "abs": "https://arxiv.org/abs/2508.01999", "title": "Prompting Large Language Models to Detect Dementia Family Caregivers", "authors": ["Md Badsha Biswas", "Özlem Uzuner"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Social media, such as Twitter, provides opportunities for caregivers of\ndementia patients to share their experiences and seek support for a variety of\nreasons. Availability of this information online also paves the way for the\ndevelopment of internet-based interventions in their support. However, for this\npurpose, tweets written by caregivers of dementia patients must first be\nidentified. This paper demonstrates our system for the SMM4H 2025 shared task\n3, which focuses on detecting tweets posted by individuals who have a family\nmember with dementia. The task is outlined as a binary classification problem,\ndifferentiating between tweets that mention dementia in the context of a family\nmember and those that do not. Our solution to this problem explores large\nlanguage models (LLMs) with various prompting methods. Our results show that a\nsimple zero-shot prompt on a fine-tuned model yielded the best results. Our\nfinal system achieved a macro F1-score of 0.95 on the validation set and the\ntest set. Our full code is available on GitHub.", "AI": {"tldr": "This paper presents a system for identifying tweets from caregivers of dementia patients, leveraging large language models for binary classification.", "motivation": "To assist caregivers of dementia patients by identifying relevant tweets that provide experiences and support, enabling the development of online interventions.", "method": "The paper addresses a binary classification task using large language models (LLMs) and various prompting strategies to differentiate tweets related to caregiving for dementia.", "result": "The system achieved a high performance with a macro F1-score of 0.95 on both validation and test sets using a simple zero-shot prompt on a fine-tuned model.", "conclusion": "The study demonstrates the effectiveness of LLMs in identifying tweets related to dementia caregiving, offering valuable insights for future internet-based interventions.", "key_contributions": ["Development of a binary classification system for caregiver tweets", "Application of LLMs with effective prompting strategies", "High performance indicated by a macro F1-score of 0.95"], "limitations": "", "keywords": ["Dementia", "Caregiving", "Social Media", "Language Models", "Machine Learning"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2508.02013", "pdf": "https://arxiv.org/pdf/2508.02013.pdf", "abs": "https://arxiv.org/abs/2508.02013", "title": "SpeechRole: A Large-Scale Dataset and Benchmark for Evaluating Speech Role-Playing Agents", "authors": ["Changhao Jiang", "Jiajun Sun", "Yifei Cao", "Jiabao Zhuang", "Hui Li", "Xiaoran Fan", "Ming Zhang", "Junjie Ye", "Shihan Dou", "Zhiheng Xi", "Jingqi Tong", "Yilong Wu", "Baoyu Fan", "Zhen Wang", "Tao Liang", "Zhihui Fei", "Mingyang Wan", "Guojun Ma", "Tao Ji", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "categories": ["cs.CL"], "comment": null, "summary": "Recently, role-playing agents have emerged as a promising paradigm for\nachieving personalized interaction and emotional resonance. Existing research\nprimarily focuses on the textual modality, neglecting the critical dimension of\nspeech in realistic interactive scenarios. In particular, there is a lack of\nsystematic evaluation for Speech Role-Playing Agents (SRPAs). To address this\ngap, we construct SpeechRole-Data, a large-scale, high-quality dataset that\ncomprises 98 diverse roles and 112k speech-based single-turn and multi-turn\nconversations. Each role demonstrates distinct vocal characteristics, including\ntimbre and prosody, thereby enabling more sophisticated speech role-playing.\nFurthermore, we propose SpeechRole-Eval, a multidimensional evaluation\nbenchmark that systematically assesses SRPAs performance in key aspects such as\nfundamental interaction ability, speech expressiveness, and role-playing\nfidelity. Experimental results reveal the advantages and challenges of both\ncascaded and end-to-end speech role-playing agents in maintaining vocal style\nconsistency and role coherence. We release all data, code, and baseline models\nto provide a solid foundation for speech-driven multimodal role-playing\nresearch and to foster further developments in this field.", "AI": {"tldr": "This paper introduces SpeechRole-Data, a dataset for evaluating Speech Role-Playing Agents, and proposes SpeechRole-Eval, a comprehensive evaluation benchmark.", "motivation": "To address the lack of systematic evaluation for Speech Role-Playing Agents (SRPAs) and to enhance personalized interaction using speech.", "method": "Construction of a large-scale dataset (SpeechRole-Data) comprising 98 roles and 112k speech conversations, and proposal of a multidimensional evaluation benchmark (SpeechRole-Eval).", "result": "Experimental results highlight advantages and challenges of cascade vs. end-to-end SRPAs, particularly in vocal style consistency and role coherence.", "conclusion": "The paper lays the groundwork for speech-driven multimodal role-playing research, providing resources for future exploration and development.", "key_contributions": ["Introduction of SpeechRole-Data dataset for SRPAs.", "Development of SpeechRole-Eval benchmark for performance assessment.", "Recommendations based on experimental evaluations."], "limitations": "", "keywords": ["Speech Role-Playing Agents", "SpeechRole-Data", "SpeechRole-Eval"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.02018", "pdf": "https://arxiv.org/pdf/2508.02018.pdf", "abs": "https://arxiv.org/abs/2508.02018", "title": "SpeechR: A Benchmark for Speech Reasoning in Large Audio-Language Models", "authors": ["Wanqi Yang", "Yanda Li", "Yunchao Wei", "Meng Fang", "Ling Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large audio-language models (LALMs) have achieved near-human performance in\nsentence-level transcription and emotion recognition. However, existing\nevaluations focus mainly on surface-level perception, leaving the capacity of\nmodels for contextual and inference-driven reasoning in speech-based scenarios\ninsufficiently examined. To address this gap, we introduce SpeechR, a unified\nbenchmark for evaluating reasoning over speech in large audio-language models.\nSpeechR evaluates models along three key dimensions: factual retrieval,\nprocedural inference, and normative judgment. It includes three distinct\nevaluation formats. The multiple-choice version measures answer selection\naccuracy. The generative version assesses the coherence and logical consistency\nof reasoning chains. The acoustic-feature version investigates whether\nvariations in stress and emotion affect reasoning performance. Evaluations on\neleven state-of-the-art LALMs reveal that high transcription accuracy does not\ntranslate into strong reasoning capabilities. SpeechR establishes a structured\nbenchmark for evaluating reasoning in spoken language, enabling more targeted\nanalysis of model capabilities across diverse dialogue-based tasks.", "AI": {"tldr": "SpeechR is a benchmark designed to evaluate the reasoning abilities of large audio-language models (LALMs) in speech-based scenarios, addressing gaps in existing evaluations.", "motivation": "To examine the inadequacy of current evaluations of large audio-language models that focus primarily on surface-level perception without assessing reasoning capabilities in speech.", "method": "SpeechR introduces a unified benchmark evaluating reasoning in LALMs along three dimensions: factual retrieval, procedural inference, and normative judgment, using multiple-choice, generative, and acoustic-feature evaluation formats.", "result": "Eleven state-of-the-art LALMs were evaluated, showing high transcription accuracy does not correlate with strong reasoning abilities in speech-based tasks.", "conclusion": "SpeechR provides a structured approach to assess reasoning in spoken language, promoting targeted analyses of model capabilities in dialogue-based applications.", "key_contributions": ["Introduction of the SpeechR benchmark for reasoning in speech", "Evaluation across three dimensions of reasoning", "Findings that high transcription accuracy does not guarantee reasoning performance"], "limitations": "", "keywords": ["audio-language models", "reasoning evaluation", "SpeechR", "emotion recognition", "transcription accuracy"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.02037", "pdf": "https://arxiv.org/pdf/2508.02037.pdf", "abs": "https://arxiv.org/abs/2508.02037", "title": "Diagnosing Memorization in Chain-of-Thought Reasoning, One Token at a Time", "authors": ["Huihan Li", "You Chen", "Siyuan Wang", "Yixin He", "Ninareh Mehrabi", "Rahul Gupta", "Xiang Ren"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) perform well on reasoning benchmarks but often\nfail when inputs alter slightly, raising concerns about the extent to which\ntheir success relies on memorization. This issue is especially acute in\nChain-of-Thought (CoT) reasoning, where spurious memorized patterns can trigger\nintermediate errors that cascade into incorrect final answers. We introduce\nSTIM, a novel framework for Source-aware Token-level Identification of\nMemorization, which attributes each token in a reasoning chain to one of\nmultiple memorization sources - local, mid-range, or long-range - based on\ntheir statistical co-occurrence with the token in the pretraining corpus. Our\ntoken-level analysis across tasks and distributional settings reveals that\nmodels rely more on memorization in complex or long-tail cases, and that local\nmemorization is often the dominant driver of errors, leading to up to 67% of\nwrong tokens. We also show that memorization scores from STIM can be effective\nin predicting the wrong tokens in the wrong reasoning step. STIM offers a\npowerful tool for diagnosing and improving model reasoning and can generalize\nto other structured step-wise generation tasks.", "AI": {"tldr": "The paper introduces STIM, a framework for identifying memorization in LLMs, highlighting its role in reasoning errors.", "motivation": "Address concerns about LLMs' reliance on memorization, particularly in Chain-of-Thought reasoning where minor input variations lead to significant errors.", "method": "STIM analyzes each token in a reasoning chain, attributing them to memorization sources based on their statistical co-occurrence in the pretraining corpus.", "result": "Models show a higher reliance on memorization for complex tasks, with local memorization causing up to 67% of token errors, and STIM's scores effectively predict incorrect tokens in reasoning steps.", "conclusion": "STIM serves as a diagnostic tool for improving model reasoning and can be adapted for other structured generation tasks.", "key_contributions": ["Introduction of the STIM framework for analyzing token-level memorization.", "Insights into local memorization's impact on reasoning errors in LLMs.", "Demonstration that STIM can generalize to other structured tasks."], "limitations": "", "keywords": ["Large Language Models", "Chain-of-Thought reasoning", "memorization"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.02038", "pdf": "https://arxiv.org/pdf/2508.02038.pdf", "abs": "https://arxiv.org/abs/2508.02038", "title": "Marco-Voice Technical Report", "authors": ["Fengping Tian", "Chenyang Lyu", "Xuanfan Ni", "Haoqin Sun", "Qingjuan Li", "Zhiqiang Qian", "Haijun Li", "Longyue Wang", "Zhao Xu", "Weihua Luo", "Kaifu Zhang"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Technical Report", "summary": "This paper presents a multifunctional speech synthesis system that integrates\nvoice cloning and emotion control speech synthesis within a unified framework.\nThe goal of this work is to address longstanding challenges in achieving highly\nexpressive, controllable, and natural speech generation that faithfully\npreserves speaker identity across diverse linguistic and emotional contexts.\nOur approach introduces an effective speaker-emotion disentanglement mechanism\nwith in-batch contrastive learning, enabling independent manipulation of\nspeaker identity and eemotional style, as well as rotational emotional\nembedding integration method for smooth emotion control. To support\ncomprehensive training and evaluation, we construct CSEMOTIONS, a high-quality\nemotional speech dataset containing 10 hours of Mandarin speech from six\nprofessional speakers across seven emotional categories. Extensive experiments\ndemonstrate that our system, Marco-Voice, achieves substantial improvements in\nboth objective and subjective metrics. Comprehensive evaluations and analysis\nwere conducted, results show that MarcoVoice delivers competitive performance\nin terms of speech clarity and emotional richness, representing a substantial\nadvance in the field of expressive neural speech synthesis.", "AI": {"tldr": "The paper presents a multifunctional speech synthesis system, Marco-Voice, that integrates voice cloning and emotion control, addressing challenges in expressive speech generation.", "motivation": "To achieve highly expressive, controllable, and natural speech generation while preserving speaker identity across diverse contexts.", "method": "Introduces a speaker-emotion disentanglement mechanism with in-batch contrastive learning and a rotational emotional embedding integration method for emotion control.", "result": "Marco-Voice shows substantial improvements in objective and subjective metrics for speech clarity and emotional richness.", "conclusion": "Marco-Voice represents a significant advance in expressive neural speech synthesis, demonstrating competitive performance.", "key_contributions": ["Development of Marco-Voice integrating voice cloning and emotion control", "Creation of CSEMOTIONS, a high-quality emotional speech dataset", "Implementation of a novel speaker-emotion disentanglement mechanism"], "limitations": "", "keywords": ["speech synthesis", "voice cloning", "emotion control", "neural networks", "emotional dataset"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.01656", "pdf": "https://arxiv.org/pdf/2508.01656.pdf", "abs": "https://arxiv.org/abs/2508.01656", "title": "Authorship Attribution in Multilingual Machine-Generated Texts", "authors": ["Lucio La Cava", "Dominik Macko", "Róbert Móro", "Ivan Srba", "Andrea Tagarelli"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "physics.soc-ph"], "comment": null, "summary": "As Large Language Models (LLMs) have reached human-like fluency and\ncoherence, distinguishing machine-generated text (MGT) from human-written\ncontent becomes increasingly difficult. While early efforts in MGT detection\nhave focused on binary classification, the growing landscape and diversity of\nLLMs require a more fine-grained yet challenging authorship attribution (AA),\ni.e., being able to identify the precise generator (LLM or human) behind a\ntext. However, AA remains nowadays confined to a monolingual setting, with\nEnglish being the most investigated one, overlooking the multilingual nature\nand usage of modern LLMs. In this work, we introduce the problem of\nMultilingual Authorship Attribution, which involves attributing texts to human\nor multiple LLM generators across diverse languages. Focusing on 18 languages\n-- covering multiple families and writing scripts -- and 8 generators (7 LLMs\nand the human-authored class), we investigate the multilingual suitability of\nmonolingual AA methods, their cross-lingual transferability, and the impact of\ngenerators on attribution performance. Our results reveal that while certain\nmonolingual AA methods can be adapted to multilingual settings, significant\nlimitations and challenges remain, particularly in transferring across diverse\nlanguage families, underscoring the complexity of multilingual AA and the need\nfor more robust approaches to better match real-world scenarios.", "AI": {"tldr": "This paper introduces Multilingual Authorship Attribution (MAA), focusing on identifying the generator of texts across multiple languages, comparing monolingual methods' adaptability and performance.", "motivation": "The need for distinguishing machine-generated text from human content has become urgent due to the high fluency of Large Language Models and the lack of multilingual authorship attribution methods.", "method": "The study explores authorship attribution in 18 languages using 8 different text generators, including both LLMs and human authors, assessing monolingual methods in a multilingual context.", "result": "Adaptive capabilities of some monolingual authorship attribution methods were confirmed, but challenges remain in cross-lingual transferability and performance across diverse language families.", "conclusion": "Robust solutions are required to address the complexities of multilingual authorship attribution for real-world applications.", "key_contributions": ["Introduction of the Multilingual Authorship Attribution problem", "Analysis of monolingual AA methods in multilingual settings", "Empirical findings on generator impact in attribution performance"], "limitations": "Limited effectiveness of monolingual methods when applied to diverse language families; need for further research in cross-lingual performance improvement.", "keywords": ["Multilingual Authorship Attribution", "Large Language Models", "Text Generation", "Human-Computer Interaction"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.02045", "pdf": "https://arxiv.org/pdf/2508.02045.pdf", "abs": "https://arxiv.org/abs/2508.02045", "title": "Harnessing Temporal Databases for Systematic Evaluation of Factual Time-Sensitive Question-Answering in Large Language Models", "authors": ["Soyeon Kim", "Jindong Wang", "Xing Xie", "Steven Euijong Whang"], "categories": ["cs.CL"], "comment": null, "summary": "Facts evolve over time, making it essential for Large Language Models (LLMs)\nto handle time-sensitive factual knowledge accurately and reliably. While\nfactual Time-Sensitive Question-Answering (TSQA) tasks have been widely\nstudied, existing benchmarks often rely on manual curation or a small, fixed\nset of predefined templates, which restricts scalable and comprehensive TSQA\nevaluation. To address these challenges, we propose TDBench, a new benchmark\nthat systematically constructs TSQA pairs by harnessing temporal databases and\ndatabase techniques such as temporal SQL and functional dependencies. We also\nintroduce a fine-grained evaluation metric called time accuracy, which assesses\nthe validity of time references in model explanations alongside traditional\nanswer accuracy to enable a more reliable TSQA evaluation. Extensive\nexperiments on contemporary LLMs show how \\ours{} enables scalable and\ncomprehensive TSQA evaluation while reducing the reliance on human labor,\ncomplementing existing Wikipedia/Wikidata-based TSQA evaluation approaches by\nenabling LLM evaluation on application-specific data and seamless multi-hop\nquestion generation. Code and data are publicly available at:\nhttps://github.com/ssoy0701/tdbench.git.", "AI": {"tldr": "Introduction of TDBench, a benchmark for evaluating time-sensitive question answering in LLMs.", "motivation": "To create a scalable and comprehensive benchmark for Time-Sensitive Question-Answering (TSQA) that overcomes limitations of existing methods.", "method": "Uses temporal databases and techniques like temporal SQL to construct TSQA pairs systematically; introduces a fine-grained evaluation metric called time accuracy.", "result": "Extensive experiments demonstrate TDBench's ability to enable reliable TSQA evaluation while minimizing human labor.", "conclusion": "TDBench complements existing approaches by allowing evaluation on application-specific data and facilitating multi-hop question generation.", "key_contributions": ["Development of TDBench for scalable TSQA evaluation", "Introduction of the time accuracy metric", "Reduction in reliance on human labor for TSQA testing"], "limitations": "", "keywords": ["Time-Sensitive Question-Answering", "Large Language Models", "Temporal Databases"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.01674", "pdf": "https://arxiv.org/pdf/2508.01674.pdf", "abs": "https://arxiv.org/abs/2508.01674", "title": "CUPID: Evaluating Personalized and Contextualized Alignment of LLMs from Interactions", "authors": ["Tae Soo Kim", "Yoonjoo Lee", "Yoonah Park", "Jiho Kim", "Young-Ho Kim", "Juho Kim"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "Accepted to COLM 2025. Project Website: https://cupid.kixlab.org/", "summary": "Personalization of Large Language Models (LLMs) often assumes users hold\nstatic preferences that reflect globally in all tasks. In reality, humans hold\ndynamic preferences that change depending on the context. As users interact\nwith an LLM in various contexts, they naturally reveal their contextual\npreferences, which a model must infer and apply in future contexts to ensure\nalignment. To assess this, we introduce CUPID, a benchmark of 756 human-curated\ninteraction session histories between users and LLM-based chat assistants. In\neach interaction session, the user provides a request in a specific context and\nexpresses their preference through multi-turn feedback. Given a new user\nrequest and prior interaction sessions, our benchmark assesses whether LLMs can\ninfer the preference relevant to this request and generate a response that\nsatisfies this preference. With CUPID, we evaluated 10 open and proprietary\nLLMs, revealing that state-of-the-art LLMs struggle to infer preferences from\nmulti-turn interactions and fail to discern what previous context is relevant\nto a new request -- under 50% precision and 65% recall. Our work highlights the\nneed to advance LLM capabilities for more contextually personalized\ninteractions and proposes CUPID as a resource to drive these improvements.", "AI": {"tldr": "CUPID is a benchmark for evaluating LLMs on inferring dynamic user preferences from multi-turn interactions, revealing challenges in current models.", "motivation": "To challenge the assumption that user preferences for LLMs are static and highlight the need for contextually aware personalization in interactions.", "method": "Introduced a benchmark called CUPID, consisting of 756 curated interaction session histories where users express preferences in various contexts, and assessed LLMs' ability to infer these preferences based on prior interactions.", "result": "State-of-the-art LLMs showed under 50% precision and 65% recall in inferring contextually relevant preferences from previous multi-turn interactions.", "conclusion": "CUPID underscores the necessity for improving LLMs to deliver personalized responses, with the benchmark serving as a resource for future advancements.", "key_contributions": ["Introduction of the CUPID benchmark for LLMs", "Evaluation of 10 open and proprietary LLMs on user preference inference", "Identification of critical weaknesses in LLMs' contextual awareness capabilities"], "limitations": "", "keywords": ["Large Language Models", "user preferences", "contextual personalization", "CUPID benchmark", "multi-turn interactions"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.02053", "pdf": "https://arxiv.org/pdf/2508.02053.pdf", "abs": "https://arxiv.org/abs/2508.02053", "title": "ProCut: LLM Prompt Compression via Attribution Estimation", "authors": ["Zhentao Xu", "Fengyi Li", "Albert Chen", "Xiaofeng Wang"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "In large-scale industrial LLM systems, prompt templates often expand to\nthousands of tokens as teams iteratively incorporate sections such as task\ninstructions, few-shot examples, and heuristic rules to enhance robustness and\ncoverage. This expansion leads to bloated prompts that are difficult to\nmaintain and incur significant inference latency and serving costs. To address\nthis, we introduce Prompt Compression via Attribution Estimation (ProCut), a\nflexible, LLM-agnostic, training-free framework that compresses prompts through\nattribution analysis. ProCut segments prompt templates into semantically\nmeaningful units, quantifies their impact on task performance, and prunes\nlow-utility components. Through extensive experiments on five public benchmark\ndatasets and real-world industrial prompts, we show that ProCut achieves\nsubstantial prompt size reductions (78% fewer tokens in production) while\nmaintaining or even slightly improving task performance (up to 62% better than\nalternative methods). We further introduce an LLM-driven attribution estimator\nthat reduces compression latency by over 50%, and demonstrate that ProCut\nintegrates seamlessly with existing prompt-optimization frameworks to produce\nconcise, high-performing prompts.", "AI": {"tldr": "ProCut is a training-free framework for compressing large prompt templates in LLM systems, reducing their size significantly while maintaining or improving task performance.", "motivation": "Large-scale industrial LLM systems often result in excessively lengthy prompt templates, leading to challenges in maintenance, inference latency, and operational costs.", "method": "ProCut compresses prompts using attribution analysis, segmenting them into meaningful units, assessing their impact on performance, and pruning less useful components without requiring training.", "result": "ProCut achieved a 78% reduction in prompt size in production settings while maintaining or improving task performance, outperforming alternative methods by up to 62%.", "conclusion": "ProCut provides a seamless integration with existing prompt-optimization frameworks, allowing for concise and effective prompt generation with reduced compression latency.", "key_contributions": ["Introduction of ProCut framework for prompt compression", "Demonstrated significant size reduction and performance improvement", "Development of an LLM-driven attribution estimator that reduces compression latency"], "limitations": "", "keywords": ["Prompt Compression", "Attribution Analysis", "LLM Optimization"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.02074", "pdf": "https://arxiv.org/pdf/2508.02074.pdf", "abs": "https://arxiv.org/abs/2508.02074", "title": "The SMeL Test: A simple benchmark for media literacy in language models", "authors": ["Gustaf Ahdritz", "Anat Kleiman"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The internet is rife with unattributed, deliberately misleading, or otherwise\nuntrustworthy content. Though large language models (LLMs) are often tasked\nwith autonomous web browsing, the extent to which they have learned the simple\nheuristics human researchers use to navigate this noisy environment is not\ncurrently known. In this paper, we introduce the Synthetic Media Literacy Test\n(SMeL Test), a minimal benchmark that tests the ability of language models to\nactively filter out untrustworthy information in context. We benchmark a\nvariety of commonly used instruction-tuned LLMs, including reasoning models,\nand find that no model consistently trusts more reliable sources; while\nreasoning in particular is associated with higher scores, even the best API\nmodel we test hallucinates up to 70% of the time. Remarkably, larger and more\ncapable models do not necessarily outperform their smaller counterparts. We\nhope our work sheds more light on this important form of hallucination and\nguides the development of new methods to combat it.", "AI": {"tldr": "This paper introduces the Synthetic Media Literacy Test (SMeL Test) to evaluate LLMs' ability to filter untrustworthy information online, revealing significant limitations in their performance.", "motivation": "To examine how well large language models (LLMs) can navigate the internet's untrustworthy content landscape using human-like heuristics.", "method": "The Synthetic Media Literacy Test (SMeL Test) benchmarks various instruction-tuned LLMs on their ability to discern reliable sources from misleading information.", "result": "No LLM consistently prioritizes reliable sources; while reasoning abilities correlate with higher scores, even top models exhibit hallucination rates of up to 70%.", "conclusion": "Larger models are not inherently better at filtering information than smaller ones, highlighting a need for improved methods to address these limitations.", "key_contributions": ["Introduction of the SMeL Test for LLM evaluation", "Demonstration of high hallucination rates in LLMs", "Analysis of model performance relative to size and capability"], "limitations": "The results indicate that existing LLMs struggle significantly with differentiating trustworthy content.", "keywords": ["large language models", "media literacy", "information trustworthiness", "LLM evaluation", "hallucination"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.02087", "pdf": "https://arxiv.org/pdf/2508.02087.pdf", "abs": "https://arxiv.org/abs/2508.02087", "title": "When Truth Is Overridden: Uncovering the Internal Origins of Sycophancy in Large Language Models", "authors": ["Jin Li", "Keyu Wang", "Shu Yang", "Zhuoran Zhang", "Di Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) often exhibit sycophantic behavior, agreeing\nwith user-stated opinions even when those contradict factual knowledge. While\nprior work has documented this tendency, the internal mechanisms that enable\nsuch behavior remain poorly understood. In this paper, we provide a mechanistic\naccount of how sycophancy arises within LLMs. We first systematically study how\nuser opinions induce sycophancy across different model families. We find that\nsimple opinion statements reliably induce sycophancy, whereas user expertise\nframing has a negligible impact. Through logit-lens analysis and causal\nactivation patching, we identify a two-stage emergence of sycophancy: (1) a\nlate-layer output preference shift and (2) deeper representational divergence.\nWe also verify that user authority fails to influence behavior because models\ndo not encode it internally. In addition, we examine how grammatical\nperspective affects sycophantic behavior, finding that first-person prompts\n(``I believe...'') consistently induce higher sycophancy rates than\nthird-person framings (``They believe...'') by creating stronger\nrepresentational perturbations in deeper layers. These findings highlight that\nsycophancy is not a surface-level artifact but emerges from a structural\noverride of learned knowledge in deeper layers, with implications for alignment\nand truthful AI systems.", "AI": {"tldr": "This paper investigates the mechanisms behind sycophantic behavior in Large Language Models (LLMs), revealing that user opinions can induce such behavior through specific structural changes in the model's layers.", "motivation": "Understand the internal mechanisms of sycophantic behavior in LLMs and its implications for AI alignment and truthfulness.", "method": "The study analyzes how user opinions induce sycophancy across different model families, utilizing logit-lens analysis and causal activation patching to identify a two-stage emergence process.", "result": "The research finds that simple opinion statements reliably induce sycophancy, while expert framing has little effect. It describes a two-stage emergence involving late-layer output shifts and deeper representational divergence.", "conclusion": "Sycophantic behavior is a structural phenomenon in LLMs, indicating deeper representational changes rather than a superficial response to user inputs, raising concerns for alignment in AI systems.", "key_contributions": ["Mechanistic account of sycophancy in LLMs.", "Identification of a two-stage process for sycophantic behavior.", "Influence of grammatical perspective on sycophantic rates."], "limitations": "", "keywords": ["Large Language Models", "sycophancy", "AI alignment", "human-computer interaction", "language modeling"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2508.02094", "pdf": "https://arxiv.org/pdf/2508.02094.pdf", "abs": "https://arxiv.org/abs/2508.02094", "title": "\"Harmless to You, Hurtful to Me!\": Investigating the Detection of Toxic Languages Grounded in the Perspective of Youth", "authors": ["Yaqiong Li", "Peng Zhang", "Lin Wang", "Hansu Gu", "Siyuan Qiao", "Ning Gu", "Tun Lu"], "categories": ["cs.CL", "cs.HC"], "comment": "Accepted at the 20th International AAAI Conference on Web and Social\n  Media (ICWSM 2026)", "summary": "Risk perception is subjective, and youth's understanding of toxic content\ndiffers from that of adults. Although previous research has conducted extensive\nstudies on toxicity detection in social media, the investigation of youth's\nunique toxicity, i.e., languages perceived as nontoxic by adults but toxic as\nyouth, is ignored. To address this gap, we aim to explore: 1) What are the\nfeatures of ``youth-toxicity'' languages in social media (RQ1); 2) Can existing\ntoxicity detection techniques accurately detect these languages (RQ2). For\nthese questions, we took Chinese youth as the research target, constructed the\nfirst Chinese ``youth-toxicity'' dataset, and then conducted extensive\nanalysis. Our results suggest that youth's perception of these is associated\nwith several contextual factors, like the source of an utterance and\ntext-related features. Incorporating these meta information into current\ntoxicity detection methods significantly improves accuracy overall. Finally, we\npropose several insights into future research on youth-centered toxicity\ndetection.", "AI": {"tldr": "This paper explores the unique features of 'youth-toxicity' in social media, highlighting how youths perceive certain language as toxic while adults do not, and proposes methods to improve toxicity detection tailored for youths.", "motivation": "To address the gap in understanding how youths perceive toxicity in social media, particularly regarding languages viewed as nontoxic by adults.", "method": "The authors constructed the first Chinese 'youth-toxicity' dataset and conducted extensive analysis to examine contextual factors influencing toxicity perception among Chinese youth.", "result": "The study found that youth toxicity perception is linked to contextual factors, enhancing the accuracy of detection methods when these factors are incorporated.", "conclusion": "Incorporating contextual meta-information into toxicity detection methods can significantly improve accuracy, and the study provides insights for future youth-centered toxicity detection research.", "key_contributions": ["Development of the first Chinese youth-toxicity dataset", "Identification of contextual features influencing youth's toxicity perception", "Improvement of existing toxicity detection methods using these insights"], "limitations": "", "keywords": ["youth toxicity", "toxicity detection", "social media", "Chinese youth", "contextual factors"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2508.02189", "pdf": "https://arxiv.org/pdf/2508.02189.pdf", "abs": "https://arxiv.org/abs/2508.02189", "title": "Learning Dynamics of Meta-Learning in Small Model Pretraining", "authors": ["David Demitri Africa", "Yuval Weiss", "Paula Buttery", "Richard Diehl Martinez"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models are powerful but costly. We ask whether meta-learning\ncan make the pretraining of small language models not only better but also more\ninterpretable. We integrate first-order MAML with subset-masked LM pretraining,\nproducing four LLama-style decoder-only models (11M-570M params), and evaluate\nit on a fundamental NLP task with many settings and real-world applications.\nCompared with vanilla training, our model (i) reaches the same loss up to 1.6x\nsooner, (ii) improves F1 on multilingual Universal NER under equal compute, and\n(iii) makes the training dynamics easy to read: first the network's\nrepresentations fan out (\"diversify\") and later they collapse into a smaller,\nshared subspace (\"compress\"). This two-stage shift shows up as a rise-and-fall\nin both effective-rank curves and attention-head entropy. The same curves\npinpoint which layers specialise earliest and which later reconverge, giving a\ncompact, interpretable signature of meta-adaptation. Code, checkpoints and\nWandB logs are released.", "AI": {"tldr": "This paper explores the integration of meta-learning techniques to improve the pretraining of small language models, resulting in better performance and interpretability.", "motivation": "To enhance both the efficiency and interpretability of small language model pretraining using meta-learning techniques.", "method": "The authors integrated first-order MAML with subset-masked language model pretraining, resulting in various LLama-style decoder-only models ranging from 11M to 570M parameters.", "result": "The proposed model achieves comparable loss levels up to 1.6 times faster, improves F1 scores on multilingual Universal NER under equal compute, and presents a clear two-stage training dynamic involving 'diversification' followed by 'compression.'", "conclusion": "The approach provides a notable improvement in training speed and interpretability, facilitating better understanding of model behavior and specialization during training.", "key_contributions": ["Integration of meta-learning in small language model pretraining.", "Demonstrates faster convergence and improved performance on NLP tasks.", "Offers insights into training dynamics and layer specialization."], "limitations": "", "keywords": ["meta-learning", "language models", "interpretability", "NLP", "training dynamics"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2508.02094", "pdf": "https://arxiv.org/pdf/2508.02094.pdf", "abs": "https://arxiv.org/abs/2508.02094", "title": "\"Harmless to You, Hurtful to Me!\": Investigating the Detection of Toxic Languages Grounded in the Perspective of Youth", "authors": ["Yaqiong Li", "Peng Zhang", "Lin Wang", "Hansu Gu", "Siyuan Qiao", "Ning Gu", "Tun Lu"], "categories": ["cs.CL", "cs.HC"], "comment": "Accepted at the 20th International AAAI Conference on Web and Social\n  Media (ICWSM 2026)", "summary": "Risk perception is subjective, and youth's understanding of toxic content\ndiffers from that of adults. Although previous research has conducted extensive\nstudies on toxicity detection in social media, the investigation of youth's\nunique toxicity, i.e., languages perceived as nontoxic by adults but toxic as\nyouth, is ignored. To address this gap, we aim to explore: 1) What are the\nfeatures of ``youth-toxicity'' languages in social media (RQ1); 2) Can existing\ntoxicity detection techniques accurately detect these languages (RQ2). For\nthese questions, we took Chinese youth as the research target, constructed the\nfirst Chinese ``youth-toxicity'' dataset, and then conducted extensive\nanalysis. Our results suggest that youth's perception of these is associated\nwith several contextual factors, like the source of an utterance and\ntext-related features. Incorporating these meta information into current\ntoxicity detection methods significantly improves accuracy overall. Finally, we\npropose several insights into future research on youth-centered toxicity\ndetection.", "AI": {"tldr": "This paper explores the concept of 'youth-toxicity' in social media, focusing on how youth perceive language as toxic differently than adults. It constructs a dataset specifically for Chinese youth and evaluates current toxicity detection techniques against this dataset.", "motivation": "To investigate the differences in toxicity perception between youth and adults in social media, specifically focusing on languages viewed as nontoxic by adults but toxic by youth.", "method": "The study involves constructing the first Chinese 'youth-toxicity' dataset and performing extensive analysis to identify features of youth-specific toxicity and evaluate existing toxicity detection techniques' effectiveness.", "result": "The findings indicate that youth's perception of toxicity is influenced by contextual factors like the source of the utterance and text-related characteristics. Additionally, enhancing toxicity detection techniques with this contextual meta information yields improved accuracy.", "conclusion": "The research provides insights into the distinct perceptions of toxicity among youth and suggests directions for the development of youth-centered toxicity detection methods.", "key_contributions": ["Creation of the first Chinese 'youth-toxicity' dataset.", "Identification of contextual factors affecting youth's perception of toxicity.", "Improvement of existing toxicity detection techniques through the incorporation of specific contextual information."], "limitations": "", "keywords": ["youth-toxicity", "toxicity detection", "social media", "Chinese youth", "contextual factors"], "importance_score": 7, "read_time_minutes": 12}}
{"id": "2508.02193", "pdf": "https://arxiv.org/pdf/2508.02193.pdf", "abs": "https://arxiv.org/abs/2508.02193", "title": "Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed Inference", "authors": ["Yuxuan Song", "Zheng Zhang", "Cheng Luo", "Pengyang Gao", "Fan Xia", "Hao Luo", "Zheng Li", "Yuehang Yang", "Hongli Yu", "Xingwei Qu", "Yuwei Fu", "Jing Su", "Ge Zhang", "Wenhao Huang", "Mingxuan Wang", "Lin Yan", "Xiaoying Jia", "Jingjing Liu", "Wei-Ying Ma", "Ya-Qin Zhang", "Yonghui Wu", "Hao Zhou"], "categories": ["cs.CL", "cs.LG"], "comment": "Demo is available at https://studio.seed.ai/exp/seed_diffusion/;\n  Project page is https://seed.bytedance.com/seed_diffusion", "summary": "We present Seed Diffusion Preview, a large-scale language model based on\ndiscrete-state diffusion, offering remarkably fast inference speed. Thanks to\nnon-sequential, parallel generation, discrete diffusion models provide a\nnotable speedup to mitigate the inherent latency of token-by-token decoding, as\ndemonstrated recently (e.g., Mercury Coder, Gemini Diffusion). Seed Diffusion\nPreview achieves an inference speed of 2,146 token/s over H20 GPUs while\nmaintaining competitive performance across a sweep of standard code evaluation\nbenchmarks, significantly faster than contemporary Mercury and Gemini\nDiffusion, establishing new state of the art on the speed-quality Pareto\nfrontier for code models.", "AI": {"tldr": "Seed Diffusion Preview is a fast inference language model using discrete-state diffusion, achieving 2,146 token/s speed on H20 GPUs while maintaining competitive performance in code evaluation.", "motivation": "To improve the inference speed of language models by utilizing discrete-state diffusion, addressing latency in traditional token-by-token decoding methods.", "method": "The model employs non-sequential, parallel generation techniques in discrete diffusion for enhanced speed and efficiency.", "result": "Achieves an inference speed of 2,146 token/s, significantly faster than existing models like Mercury and Gemini Diffusion, while also maintaining high quality in code evaluations.", "conclusion": "Seed Diffusion Preview establishes a new state of the art on the speed-quality frontier for code models, proving the efficacy of discrete-state diffusion for fast inference.", "key_contributions": ["Introduction of a fast inference language model based on discrete-state diffusion", "Records a notable inference speed of 2,146 token/s on H20 GPUs", "Sets new benchmarks in the speed-quality Pareto frontier for code models."], "limitations": "", "keywords": ["language model", "discrete-state diffusion", "inference speed", "code evaluation", "parallel generation"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2508.02208", "pdf": "https://arxiv.org/pdf/2508.02208.pdf", "abs": "https://arxiv.org/abs/2508.02208", "title": "Proof2Hybrid: Automatic Mathematical Benchmark Synthesis for Proof-Centric Problems", "authors": ["Yebo Peng", "Zixiang Liu", "Yaoming Li", "Zhizhuo Yang", "Xinye Xu", "Bowen Ye", "Weijun Yuan", "Zihan Wang", "Tong Yang"], "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 5 figures", "summary": "Evaluating the mathematical capability of Large Language Models (LLMs) is a\ncritical yet challenging frontier. Existing benchmarks fall short, particularly\nfor proof-centric problems, as manual creation is unscalable and costly,\nleaving the true mathematical abilities of LLMs largely unassessed. To overcome\nthese barriers, we propose Proof2Hybrid, the first fully automated framework\nthat synthesizes high-quality, proof-centric benchmarks from natural language\nmathematical corpora. The key novelty of our solution is Proof2X, a roadmap of\nconverting mathematical proofs into various kinds of questions that are easy to\nverify. Instructed by this roadmap, we propose a new type of hybrid-formatted\nquestions, named ``$m$-out-of-$n$ multiple judge questions'', specifically\ndesigned to enable robust, automatic evaluation while being resilient to\nguessing and superficial pattern matching inherent in traditional formats. As a\ndemonstration of our framework, we introduce AlgGeoTest, a benchmark for\nalgebraic geometry--a frontier domain of modern mathematics--comprising 456\nchallenging items. Our extensive evaluations on state-of-the-art LLMs using\nAlgGeoTest reveal profound deficits in their comprehension of algebraic\ngeometry, providing a more precise measure of their true mathematical\ncapabilities. Our framework and benchmark pave the way for a new wave of\nin-depth research into the mathematical intelligence of AI systems.", "AI": {"tldr": "This paper presents Proof2Hybrid, an automated framework for creating proof-centric benchmarks to evaluate the mathematical capabilities of Large Language Models (LLMs) using natural language mathematical corpora.", "motivation": "To address the limitations of existing benchmarks for evaluating the mathematical abilities of LLMs, particularly in proof-centric contexts.", "method": "The paper introduces Proof2X, a roadmap for converting mathematical proofs into hybrid-formatted questions, including a new question type called '$m$-out-of-$n$ multiple judge questions', aimed at facilitating rigorous and automatic evaluation.", "result": "Using the newly developed framework, the authors created AlgGeoTest, a benchmark for algebraic geometry consisting of 456 challenging questions. Evaluations showed significant comprehension deficits in state-of-the-art LLMs when tested with AlgGeoTest.", "conclusion": "Proof2Hybrid and AlgGeoTest enable more precise assessment of mathematical intelligence in AI, encouraging further research in this area.", "key_contributions": ["Introduction of Proof2Hybrid framework for automated benchmark synthesis", "Development of '$m$-out-of-$n$ multiple judge questions' for robust evaluation", "Creation of AlgGeoTest benchmark for algebraic geometry"], "limitations": "", "keywords": ["Large Language Models", "mathematical capability", "proof-centric benchmarks", "algorithms", "evaluation techniques"], "importance_score": 8, "read_time_minutes": 14}}
{"id": "2508.02241", "pdf": "https://arxiv.org/pdf/2508.02241.pdf", "abs": "https://arxiv.org/abs/2508.02241", "title": "Isolating Culture Neurons in Multilingual Large Language Models", "authors": ["Danial Namazifard", "Lukas Galke"], "categories": ["cs.CL"], "comment": "18 pages, 13 figures", "summary": "Language and culture are deeply intertwined, yet it is so far unclear how and\nwhere multilingual large language models encode culture. Here, we extend upon\nan established methodology for identifying language-specific neurons and extend\nit to localize and isolate culture-specific neurons, carefully disentangling\ntheir overlap and interaction with language-specific neurons. To facilitate our\nexperiments, we introduce MUREL, a curated dataset of 85.2 million tokens\nspanning six different cultures. Our localization and intervention experiments\nshow that LLMs encode different cultures in distinct neuron populations,\npredominantly in upper layers, and that these culture neurons can be modulated\nindependently from language-specific neurons or those specific to other\ncultures. These findings suggest that cultural knowledge and propensities in\nmultilingual language models can be selectively isolated and edited - promoting\nfairness, inclusivity, and alignment. Code and data is available at\nhttps://github.com/namazifard/Culture_Neurons .", "AI": {"tldr": "This paper investigates how multilingual large language models (LLMs) encode culture by identifying culture-specific neurons distinct from language-specific ones, finding that cultural knowledge can be isolated and modified.", "motivation": "To understand the intertwining of language and culture in multilingual large language models and how cultural knowledge is represented within them.", "method": "We extended an established methodology to locate and isolate culture-specific neurons in LLMs, conducting localization and intervention experiments using a curated dataset of 85.2 million tokens from six cultures.", "result": "Our experiments reveal that LLMs have distinct neuron populations for different cultures, primarily in the upper layers, allowing for independent modulation of cultural knowledge separate from language.", "conclusion": "Cultural knowledge in multilingual language models can be selectively isolated and edited, which could enhance fairness and inclusivity in AI systems.", "key_contributions": ["Introduction of MUREL, a dataset for analyzing cultural neurons in LLMs.", "Identification of distinct neuron populations for different cultures in multilingual LLMs.", "Demonstration that cultural neurons can be modulated independently from language-specific neurons."], "limitations": "", "keywords": ["multilingual LLMs", "cultural neurons", "language-specific neurons", "fairness", "inclusivity"], "importance_score": 9, "read_time_minutes": 18}}
{"id": "2508.02256", "pdf": "https://arxiv.org/pdf/2508.02256.pdf", "abs": "https://arxiv.org/abs/2508.02256", "title": "Interference Matrix: Quantifying Cross-Lingual Interference in Transformer Encoders", "authors": ["Belen Alastruey", "João Maria Janeiro", "Alexandre Allauzen", "Maha Elbayad", "Loïc Barrault", "Marta R. Costa-jussà"], "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we present a comprehensive study of language interference in\nencoder-only Transformer models across 83 languages. We construct an\ninterference matrix by training and evaluating small BERT-like models on all\npossible language pairs, providing a large-scale quantification of\ncross-lingual interference. Our analysis reveals that interference between\nlanguages is asymmetrical and that its patterns do not align with traditional\nlinguistic characteristics, such as language family, nor with proxies like\nembedding similarity, but instead better relate to script. Finally, we\ndemonstrate that the interference matrix effectively predicts performance on\ndownstream tasks, serving as a tool to better design multilingual models to\nobtain optimal performance.", "AI": {"tldr": "Study of language interference in encoder-only Transformer models across 83 languages reveals asymmetrical interference patterns related to script rather than traditional linguistic characteristics.", "motivation": "To quantify and analyze cross-lingual interference in Transformer models to improve multilingual model performance.", "method": "Constructed an interference matrix by training and evaluating small BERT-like models on all possible language pairs.", "result": "Discovered that interference patterns do not align with traditional linguistic characteristics or embedding similarity, but correlate better with script, and that the interference matrix can predict performance on downstream tasks.", "conclusion": "The findings suggest new strategies for designing multilingual models to enhance their efficacy based on interference patterns.", "key_contributions": ["Comprehensive analysis of language interference across 83 languages.", "Introduction of an interference matrix for quantifying cross-lingual interference.", "Demonstration of the matrix's predictive capability for downstream task performance."], "limitations": "", "keywords": ["language interference", "Transformer models", "cross-lingual performance"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2508.02260", "pdf": "https://arxiv.org/pdf/2508.02260.pdf", "abs": "https://arxiv.org/abs/2508.02260", "title": "Decomposing the Entropy-Performance Exchange: The Missing Keys to Unlocking Effective Reinforcement Learning", "authors": ["Jia Deng", "Jie Chen", "Zhipeng Chen", "Wayne Xin Zhao", "Ji-Rong Wen"], "categories": ["cs.CL", "cs.AI"], "comment": "7 pages, 20 figures", "summary": "Recently, reinforcement learning with verifiable rewards (RLVR) has been\nwidely used for enhancing the reasoning abilities of large language models\n(LLMs). A core challenge in RLVR involves managing the exchange between entropy\nand performance of policies. Despite the importance of this exchange, a\nfine-grained understanding of when and how this exchange operates most\neffectively remains limited. To bridge this gap, we conduct a systematic\nempirical analysis of the entropy-performance exchange mechanism of RLVR across\ndifferent levels of granularity. Specifically, we first divide the training\nprocess into two distinct stages based on entropy dynamics, i.e., rising stage\nand plateau stage, and then systematically investigate how this mechanism\nvaries across stage-level, instance-level, and token-level granularitiess. Our\nanalysis reveals that, in the rising stage, entropy reduction in negative\nsamples facilitates the learning of effective reasoning patterns, which in turn\ndrives rapid performance gains. Moreover, in the plateau stage, learning\nefficiency strongly correlates with high-entropy tokens present in\nlow-perplexity samples and those located at the end of sequences. Motivated by\nthese findings, we propose two methods that dynamically adjust the reward\nsignal using perplexity and positional information to focus RL updates on\ntokens that exhibit high learning potential, achieving improvements compared to\nthe baseline methods on various LLMs.", "AI": {"tldr": "This paper analyzes the entropy-performance exchange in reinforcement learning with verifiable rewards (RLVR) to enhance reasoning abilities in large language models (LLMs).", "motivation": "To understand the exchange between entropy and performance in RLVR and improve the training of LLMs.", "method": "The authors conduct a systematic empirical analysis dividing the training process into two stages: rising and plateau stage, investigating entropy-performance relations at different granularities.", "result": "In the rising stage, reducing entropy in negative samples helps learn effective reasoning patterns, leading to performance gains. In the plateau stage, learning efficiency correlates with high-entropy tokens in low-perplexity samples at the end of sequences.", "conclusion": "The findings inform two proposed methods for dynamically adjusting reward signals using perplexity and positional information, which show improved performance on various LLMs compared to baselines.", "key_contributions": ["Systematic analysis of entropy-performance exchange in RLVR across different granularities", "Identification of key stages in the training process influencing learning efficiency", "Proposed methods for dynamic reward adjustment in LLM training"], "limitations": "", "keywords": ["Reinforcement Learning", "Large Language Models", "Entropy Dynamics", "Performance Analysis", "Reward Adjustment"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2412.02891", "pdf": "https://arxiv.org/pdf/2412.02891.pdf", "abs": "https://arxiv.org/abs/2412.02891", "title": "OriStitch: A Machine Embroidery Workflow to Turn Existing Fabrics into Self-Folding 3D Textiles", "authors": ["Zekun Chang", "Yixuan Gao", "Yuta Noma", "Shuo Feng", "Xinyi Yang", "Kazuhiro Shinoda", "Tung D. Ta", "Koji Yatani", "Tomoyuki Yokota", "Takao Someya", "Yoshihiro Kawahara", "Koya Narumi", "Francois Guimbretiere", "Thijs Roumen"], "categories": ["cs.HC"], "comment": null, "summary": "OriStitch is a computational fabrication workflow to turn existing flat\nfabrics into self-folding 3D structures. Users turn fabrics into self-folding\nsheets by machine embroidering functional threads in specific patterns on\nfabrics, and then apply heat to deform the structure into a target 3D\nstructure. OriStitch is compatible with a range of existing materials (e.g.,\nleather, woven fabric, and denim).\n  We present the design of specific embroidered hinges that fully close under\nexposure to heat. We discuss the stitch pattern design, thread and fabric\nselection, and heating conditions. To allow users to create 3D textiles using\nour hinges, we create a tool to convert 3D meshes to 2D stitch patterns\nautomatically, as well as an end-to-end fabrication and actuation workflow. To\nvalidate this workflow, we designed and fabricated a cap (303 hinges), a\nhandbag (338 hinges), and a cover for an organically shaped vase (140 hinges).\n  In technical evaluation, we found that our tool successfully converted 23/28\nmodels (textures and volumetric objects) found in related papers. We also\ndemonstrate the folding performance across different materials (suede leather,\ncork, Neoprene, and felt).", "AI": {"tldr": "OriStitch is a computational fabrication workflow that transforms flat fabrics into self-folding 3D structures using machine-embroidered functional threads and heat.", "motivation": "To enable the creation of 3D textiles from flat fabrics, enhancing the versatility of fabric design and functionality.", "method": "The method involves embroidering specific patterns on fabrics with functional threads, applying heat to deform the fabric, and converting 3D models to 2D stitch patterns automatically.", "result": "The validation process showed the tool successfully converted 23 out of 28 models and demonstrated effective folding performance across various materials.", "conclusion": "OriStitch provides a new approach to 3D textile creation using existing materials and automated design conversion, with applications in diverse fabric types.", "key_contributions": ["Introduction of OriStitch workflow for fabric transformation", "Development of a tool for automatic conversion of 3D meshes to 2D stitch patterns", "Evaluation of multiple fabric materials with successful hinge designs"], "limitations": "", "keywords": ["Computational fabrication", "Self-folding structures", "Textiles"], "importance_score": 2, "read_time_minutes": 5}}
{"id": "2508.02268", "pdf": "https://arxiv.org/pdf/2508.02268.pdf", "abs": "https://arxiv.org/abs/2508.02268", "title": "SHAMI-MT: A Syrian Arabic Dialect to Modern Standard Arabic Bidirectional Machine Translation System", "authors": ["Serry Sibaee", "Omer Nacar", "Yasser Al-Habashi", "Adel Ammar", "Wadii Boulila"], "categories": ["cs.CL"], "comment": null, "summary": "The rich linguistic landscape of the Arab world is characterized by a\nsignificant gap between Modern Standard Arabic (MSA), the language of formal\ncommunication, and the diverse regional dialects used in everyday life. This\ndiglossia presents a formidable challenge for natural language processing,\nparticularly machine translation. This paper introduces \\textbf{SHAMI-MT}, a\nbidirectional machine translation system specifically engineered to bridge the\ncommunication gap between MSA and the Syrian dialect. We present two\nspecialized models, one for MSA-to-Shami and another for Shami-to-MSA\ntranslation, both built upon the state-of-the-art AraT5v2-base-1024\narchitecture. The models were fine-tuned on the comprehensive Nabra dataset and\nrigorously evaluated on unseen data from the MADAR corpus. Our MSA-to-Shami\nmodel achieved an outstanding average quality score of \\textbf{4.01 out of 5.0}\nwhen judged by OPENAI model GPT-4.1, demonstrating its ability to produce\ntranslations that are not only accurate but also dialectally authentic. This\nwork provides a crucial, high-fidelity tool for a previously underserved\nlanguage pair, advancing the field of dialectal Arabic translation and offering\nsignificant applications in content localization, cultural heritage, and\nintercultural communication.", "AI": {"tldr": "SHAMI-MT is a bidirectional machine translation system designed for translating between Modern Standard Arabic and the Syrian dialect. It utilizes the AraT5v2 architecture and has been rigorously evaluated, achieving a high accuracy score.", "motivation": "To address the significant gap between Modern Standard Arabic and regional dialects, specifically the Syrian dialect, in natural language processing applications like machine translation.", "method": "Introduces two specialized models for MSA-to-Shami and Shami-to-MSA translation, built on the AraT5v2-base-1024 architecture and fine-tuned on the Nabra dataset.", "result": "The MSA-to-Shami model achieved an average quality score of 4.01 out of 5.0 when evaluated by GPT-4.1, demonstrating high accuracy and dialectal authenticity.", "conclusion": "SHAMI-MT offers a crucial tool for translating a previously underserved language pair, with applications in content localization and intercultural communication.", "key_contributions": ["Introduction of a bidirectional translation system specifically for MSA and Syrian dialects.", "Development of high-quality translation models evaluated with state-of-the-art techniques.", "Contributions to the field of dialectal Arabic translation."], "limitations": "", "keywords": ["Machine Translation", "Natural Language Processing", "Dialects", "Arabic", "SHAMI-MT"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2502.02194", "pdf": "https://arxiv.org/pdf/2502.02194.pdf", "abs": "https://arxiv.org/abs/2502.02194", "title": "Understanding User Mental Models in AI-Driven Code Completion Tools: Insights from an Elicitation Study", "authors": ["Giuseppe Desolda", "Andrea Esposito", "Francesco Greco", "Cesare Tucci", "Paolo Buono", "Antonio Piccinno"], "categories": ["cs.HC", "cs.SE"], "comment": null, "summary": "Integrated Development Environments increasingly implement AI-powered code\ncompletion tools (CCTs), which promise to enhance developer efficiency,\naccuracy, and productivity. However, interaction challenges with CCTs persist,\nmainly due to mismatches between developers' mental models and the\nunpredictable behavior of AI-generated suggestions, which is an aspect\nunderexplored in the literature. We conducted an elicitation study with 56\ndevelopers using co-design workshops to elicit their mental models when\ninteracting with CCTs. Different important findings that might drive the\ninteraction design with CCTs emerged. For example, developers expressed diverse\npreferences on when and how code suggestions should be triggered (proactive,\nmanual, hybrid), where and how they are displayed (inline, sidebar, popup,\nchatbot), as well as the level of detail. It also emerged that developers need\nto be supported by customization of activation timing, display modality,\nsuggestion granularity, and explanation content, to better fit the CCT to their\npreferences. To demonstrate the feasibility of these and the other guidelines\nthat emerged during the study, we developed ATHENA, a proof-of-concept CCT that\ndynamically adapts to developers' coding preferences and environments, ensuring\nseamless integration into diverse workflows.", "AI": {"tldr": "The paper explores developers' mental models in relation to AI-powered code completion tools (CCTs) and provides guidelines for improving their interaction design based on a study with 56 developers.", "motivation": "To address interaction challenges with CCTs arising from mismatches between developers' mental models and AI behavior, which are underexplored in existing literature.", "method": "Conducted co-design workshops with 56 developers to elicit their preferences and mental models when interacting with CCTs.", "result": "The study identified key factors affecting interaction design, including preferences for suggestion triggering, display methods, and customization needs.", "conclusion": "The findings inform better design of CCTs and demonstrated through the development of a proof-of-concept tool called ATHENA that adapts to developers' preferences.", "key_contributions": ["Identification of developers' mental model preferences for CCT interactions", "Guidelines for customizing AI suggestion mechanisms in CCTs", "Development of a dynamic proof-of-concept CCT (ATHENA) that meets user needs"], "limitations": "", "keywords": ["AI-powered code completion", "human-computer interaction", "developer preferences"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.02271", "pdf": "https://arxiv.org/pdf/2508.02271.pdf", "abs": "https://arxiv.org/abs/2508.02271", "title": "Dynaword: From One-shot to Continuously Developed Datasets", "authors": ["Kenneth Enevoldsen", "Kristian Nørgaard Jensen", "Jan Kostkan", "Balázs Szabó", "Márton Kardos", "Kirten Vad", "Andrea Blasi Núñez", "Gianluca Barmina", "Jacob Nielsen", "Rasmus Larsen", "Peter Vahlstrup", "Per Møldrup Dalum", "Desmond Elliott", "Lukas Galke", "Peter Schneider-Kamp", "Kristoffer Nielbo"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large-scale datasets are foundational for research and development in natural\nlanguage processing. However, current approaches face three key challenges: (1)\nreliance on ambiguously licensed sources restricting use, sharing, and\nderivative works; (2) static dataset releases that prevent community\ncontributions and diminish longevity; and (3) quality assurance processes\nrestricted to publishing teams rather than leveraging community expertise.\n  To address these limitations, we introduce two contributions: the Dynaword\napproach and Danish Dynaword. The Dynaword approach is a framework for creating\nlarge-scale, open datasets that can be continuously updated through community\ncollaboration. Danish Dynaword is a concrete implementation that validates this\napproach and demonstrates its potential. Danish Dynaword contains over four\ntimes as many tokens as comparable releases, is exclusively openly licensed,\nand has received multiple contributions across industry and research. The\nrepository includes light-weight tests to ensure data formatting, quality, and\ndocumentation, establishing a sustainable framework for ongoing community\ncontributions and dataset evolution.", "AI": {"tldr": "The paper presents the Dynaword approach and the Danish Dynaword dataset to address challenges in creating sustainable, open, and community-driven large-scale datasets for natural language processing.", "motivation": "To overcome the limitations of current approaches in large-scale dataset creation, focusing on licensing issues, static releases, and quality assurance.", "method": "The paper introduces the Dynaword framework for continuously updated datasets via community contributions, exemplified by Danish Dynaword, which is built to be openly licensed and highly contributive.", "result": "Danish Dynaword contains over four times the tokens of similar datasets and has received significant community contributions, along with lightweight quality assurance tests.", "conclusion": "The Dynaword approach establishes a sustainable model for evolving datasets through community collaboration in natural language processing.", "key_contributions": ["Introduces the Dynaword framework for community-driven dataset creation.", "Launches Danish Dynaword as a practical implementation with significant token volume and contributions.", "Implements quality assurance processes that involve community input."], "limitations": "", "keywords": ["large-scale datasets", "natural language processing", "community contributions"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2508.02290", "pdf": "https://arxiv.org/pdf/2508.02290.pdf", "abs": "https://arxiv.org/abs/2508.02290", "title": "A French Version of the OLDI Seed Corpus", "authors": ["Malik Marmonier", "Benoît Sagot", "Rachel Bawden"], "categories": ["cs.CL"], "comment": null, "summary": "We present the first French partition of the OLDI Seed Corpus, our submission\nto the WMT 2025 Open Language Data Initiative (OLDI) shared task. We detail its\ncreation process, which involved using multiple machine translation systems and\na custom-built interface for post-editing by qualified native speakers. We also\nhighlight the unique translation challenges presented by the source data, which\ncombines highly technical, encyclopedic terminology with the stylistic\nirregularities characteristic of user-generated content taken from Wikipedia.\nThis French corpus is not an end in itself, but is intended as a crucial pivot\nresource to facilitate the collection of parallel corpora for the\nunder-resourced regional languages of France.", "AI": {"tldr": "The paper presents the first French partition of the OLDI Seed Corpus aimed at aiding the development of parallel corpora for under-resourced regional languages in France.", "motivation": "To create a resource for enhancing machine translation for under-resourced regional languages by leveraging a French corpus derived from technical and user-generated content.", "method": "Utilized multiple machine translation systems and a custom interface for post-editing by qualified native speakers to create the French corpus.", "result": "The resulting French corpus addresses unique translation challenges and combines technical terminology with stylistic irregularities from user-generated content.", "conclusion": "The French corpus serves as a pivot resource for collecting parallel corpora for regional languages, contributing to broader language accessibility in machine translation.", "key_contributions": ["Creation of the first French partition of the OLDI Seed Corpus.", "Detailed methodology involving machine translation and human post-editing.", "Addressing translation challenges with technical and user-generated content."], "limitations": "The focus is primarily on French and may limit direct applicability to other languages.", "keywords": ["machine translation", "corpus", "language resources", "under-resourced languages", "post-editing"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2508.02296", "pdf": "https://arxiv.org/pdf/2508.02296.pdf", "abs": "https://arxiv.org/abs/2508.02296", "title": "Simple Methods Defend RAG Systems Well Against Real-World Attacks", "authors": ["Ilias Triantafyllopoulos", "Renyi Qu", "Salvatore Giorgi", "Brenda Curtis", "Lyle H. Ungar", "João Sedoc"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Ensuring safety and in-domain responses for Retrieval-Augmented Generation\n(RAG) systems is paramount in safety-critical applications, yet remains a\nsignificant challenge. To address this, we evaluate four methodologies for\nOut-Of-Domain (OOD) query detection: GPT-4o, regression-based, Principal\nComponent Analysis (PCA)-based, and Neural Collapse (NC), to ensure the RAG\nsystem only responds to queries confined to the system's knowledge base.\nSpecifically, our evaluation explores two novel dimensionality reduction and\nfeature separation strategies: \\textit{PCA}, where top components are selected\nusing explained variance or OOD separability, and an adaptation of\n\\textit{Neural Collapse Feature Separation}. We validate our approach on\nstandard datasets (StackExchange and MSMARCO) and real-world applications\n(Substance Use and COVID-19), including tests against LLM-simulated and actual\nattacks on a COVID-19 vaccine chatbot. Through human and LLM-based evaluations\nof response correctness and relevance, we confirm that an external OOD detector\nis crucial for maintaining response relevance.", "AI": {"tldr": "The paper evaluates four methodologies for Out-Of-Domain query detection in Retrieval-Augmented Generation systems to enhance safety in critical applications.", "motivation": "To ensure safety and relevant responses in RAG systems, especially in safety-critical applications like health informatics, addressing the challenge of Out-Of-Domain queries is crucial.", "method": "The methodologies evaluated include GPT-4o, regression-based, PCA-based, and Neural Collapse methodologies, focusing on dimensionality reduction and feature separation strategies.", "result": "The evaluation confirms that employing an external Out-Of-Domain detector significantly maintains response relevance during queries, validated through datasets and real-world applications.", "conclusion": "The study concludes that effectively detecting OOD queries is essential for RAG systems to ensure accurate and relevant responses, particularly in health-related chatbot applications.", "key_contributions": ["Evaluation of four OOD query detection methodologies", "Introduction of novel dimensionality reduction and feature separation strategies", "Validation using real-world applications like COVID-19 chatbot"], "limitations": "", "keywords": ["Human-Computer Interaction", "Retrieval-Augmented Generation", "Out-Of-Domain detection", "Health informatics", "Dimensionality reduction"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.02308", "pdf": "https://arxiv.org/pdf/2508.02308.pdf", "abs": "https://arxiv.org/abs/2508.02308", "title": "LaMPE: Length-aware Multi-grained Position Encoding for Adaptive Long-context Scaling Without Training", "authors": ["Sikui Zhang", "Guangze Gao", "Ziyun Gan", "Chunfeng Yuan", "Zefeng Lin", "Houwen Peng", "Bing Li", "Weiming Hu"], "categories": ["cs.CL"], "comment": "13 pages, 9 figures", "summary": "Large language models (LLMs) experience significant performance degradation\nwhen the input exceeds the pretraining context window, primarily due to the\nout-of-distribution (OOD) behavior of Rotary Position Embedding (RoPE). Recent\nstudies mitigate this problem by remapping OOD positions into the\nin-distribution range with fixed mapping strategies, ignoring the dynamic\nrelationship between input length and the model's effective context window. To\nthis end, we propose Length-aware Multi-grained Positional Encoding (LaMPE), a\ntraining-free method that fully utilizes the model's effective context window\nfor adaptive long-context scaling in LLMs. Motivated by the left-skewed\nfrequency distribution of relative positions, LaMPE establishes a dynamic\nrelationship between mapping length and input length through a parametric\nscaled sigmoid function to adaptively allocate positional capacity across\nvarying input lengths. Meanwhile, LaMPE devises a novel multi-grained attention\nmechanism that strategically allocates positional resolution across different\nsequence regions to capture both fine-grained locality and long-range\ndependencies. Our method can be seamlessly applied to a wide range of\nRoPE-based LLMs without training. Extensive experiments on three representative\nLLMs across five mainstream long-context benchmarks demonstrate that LaMPE\nachieves significant performance improvements compared to existing length\nextrapolation methods. The code will be released at\nhttps://github.com/scar-on/LaMPE.", "AI": {"tldr": "This paper introduces Length-aware Multi-grained Positional Encoding (LaMPE) to enhance long-context scaling in large language models (LLMs) by dynamically allocating positional capacity based on input length.", "motivation": "LLMs show performance degradation when input exceeds pretraining context windows, specifically due to the OOD behavior of Rotary Position Embedding.", "method": "LaMPE uses a parametric scaled sigmoid function to map input lengths dynamically, alongside a novel multi-grained attention mechanism for effective positional resolution allocation.", "result": "LaMPE significantly improves performance on long-context benchmarks compared to existing methods across three LLMs.", "conclusion": "The method allows RoPE-based LLMs to adaptively manage positional information without additional training.", "key_contributions": ["Introduces Length-aware Multi-grained Positional Encoding (LaMPE) for LLMs", "Dynamic mapping of positional capacity based on input length", "Novel multi-grained attention mechanism for improved locality and long-range dependencies"], "limitations": "", "keywords": ["Large Language Models", "Positional Encoding", "Machine Learning", "Human-Computer Interaction", "Long-context scaling"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.02317", "pdf": "https://arxiv.org/pdf/2508.02317.pdf", "abs": "https://arxiv.org/abs/2508.02317", "title": "VeOmni: Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo", "authors": ["Qianli Ma", "Yaowei Zheng", "Zhelun Shi", "Zhongkai Zhao", "Bin Jia", "Ziyue Huang", "Zhiqi Lin", "Youjie Li", "Jiacheng Yang", "Yanghua Peng", "Zhi Zhang", "Xin Liu"], "categories": ["cs.CL", "cs.AI", "cs.DC"], "comment": null, "summary": "Recent advances in large language models (LLMs) have driven impressive\nprogress in omni-modal understanding and generation. However, training\nomni-modal LLMs remains a significant challenge due to the heterogeneous model\narchitectures required to process diverse modalities, necessitating\nsophisticated system design for efficient large-scale training. Existing\nframeworks typically entangle model definition with parallel logic, incurring\nlimited scalability and substantial engineering overhead for end-to-end\nomni-modal training. % We present \\veomni, a modular and efficient training\nframework to accelerate the development of omni-modal LLMs. \\veomni introduces\nmodel-centric distributed recipes that decouples communication from\ncomputation, enabling efficient 3D parallelism on omni-modal LLMs. \\veomni also\nfeatures a flexible configuration interface supporting seamless integration of\nnew modalities with minimal code change. % Using \\veomni, a omni-modal\nmixture-of-experts (MoE) model with 30B parameters can be trained with over\n2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D\nparallelism on 128 GPUs, showcasing its superior efficiency and scalability for\ntraining large omni-modal LLMs.", "AI": {"tldr": "The paper presents \u00176omni, a modular framework designed to efficiently train omni-modal large language models (LLMs) by decoupling communication from computation and supporting easy integration of new modalities.", "motivation": "Training omni-modal LLMs is challenging due to the complex architectures required for diverse modalities and the need for scalable, efficient training systems.", "method": "\u00176omni employs model-centric distributed recipes that utilize 3D parallelism to enhance the training of omni-modal LLMs.", "result": "A 30B parameter omni-modal mixture-of-experts model was trained at over 2,800 tokens/sec/GPU and scaled to 160K context lengths across 128 GPUs, demonstrating significant efficiency and scalability.", "conclusion": "\u00176omni provides an efficient approach to overcome the engineering burdens of training omni-modal LLMs, allowing for better scalability and flexibility.", "key_contributions": ["Introduction of a modular training framework for omni-modal LLMs", "Decoupling of communication from computation to enhance training efficiency", "Support for seamless integration of new modalities with minimal code changes"], "limitations": "", "keywords": ["omni-modal LLMs", "parallelism", "training framework", "Machine Learning", "scalability"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2508.02322", "pdf": "https://arxiv.org/pdf/2508.02322.pdf", "abs": "https://arxiv.org/abs/2508.02322", "title": "CAMERA: Multi-Matrix Joint Compression for MoE Models via Micro-Expert Redundancy Analysis", "authors": ["Yuzhuang Xu", "Xu Han", "Yuanchi Zhang", "Yixuan Wang", "Yijun Liu", "Shiyu Ji", "Qingfu Zhu", "Wanxiang Che"], "categories": ["cs.CL", "cs.LG"], "comment": "16 pages, 9 figures, 7 tables", "summary": "Large Language Models (LLMs) with Mixture-of-Experts (MoE) architectures are\ndistinguished by their strong performance scaling with increasing parameters\nacross a wide range of tasks, yet they also suffer from substantial\ncomputational and storage overheads. Notably, the performance gains of MoE\nmodels do not scale proportionally with the growth in expert parameters. While\nprior works attempt to reduce parameters via expert-level pruning, merging, or\ndecomposition, they still suffer from challenges in both performance and\ncomputational efficiency. In this paper, we address these challenges by\nintroducing micro-expert as a finer-grained compression unit that spans across\nmatrices. We first establish a more fundamental perspective, viewing MoE layers\nas mixtures of micro-experts, and present CAMERA, a lightweight and\ntraining-free framework for identifying micro-expert redundancy. Our analysis\nuncovers significant variance in micro-expert contributions during decoding.\nBased on this insight, we further propose CAMERA-P, a structured micro-expert\npruning framework, and CAMERA-Q, a mixed-precision quantization idea designed\nfor micro-experts. Extensive experiments on nine downstream tasks show that\nCAMERA-P consistently outperforms strong baselines under pruning ratios ranging\nfrom 20% to 60%. Furthermore, CAMERA-Q achieves superior results under\naggressive 2-bit quantization, surpassing existing matrix- and channel-level\nideas. Notably, our method enables complete micro-expert analysis of\nQwen2-57B-A14B in less than 5 minutes on a single NVIDIA A100-40GB GPU.", "AI": {"tldr": "The paper introduces a method for optimizing Mixture-of-Experts models through the use of micro-experts and proposes a framework called CAMERA for identifying and pruning redundant micro-experts, achieving improved performance and efficiency.", "motivation": "To tackle the challenges of computational overhead and performance scaling in Mixture-of-Experts models, which do not scale efficiently with increasing parameters.", "method": "Introduces micro-expert as a compression unit, establishing a framework (CAMERA) for analyzing and pruning micro-expert redundancies, complemented by structured pruning (CAMERA-P) and mixed-precision quantization (CAMERA-Q).", "result": "CAMERA-P consistently outperforms strong baselines under pruning ratios of 20% to 60%, while CAMERA-Q achieves superior performance with aggressive 2-bit quantization compared to existing methods.", "conclusion": "The proposed methods enhance the efficiency of MoE models through effective micro-expert analysis and pruning, making it feasible to analyze large models quickly.", "key_contributions": ["Introduction of the micro-expert as a new compression unit for MoE models.", "Development of the CAMERA framework for identifying micro-expert redundancy.", "Proposal of CAMERA-P for structured pruning and CAMERA-Q for quantization of micro-experts."], "limitations": "", "keywords": ["Mixture-of-Experts", "micro-experts", "CAMERA", "pruning", "quantization"], "importance_score": 9, "read_time_minutes": 16}}
{"id": "2508.02360", "pdf": "https://arxiv.org/pdf/2508.02360.pdf", "abs": "https://arxiv.org/abs/2508.02360", "title": "Understanding and Mitigating Political Stance Cross-topic Generalization in Large Language Models", "authors": ["Jiayi Zhang", "Shu Yang", "Junchao Wu", "Derek F. Wong", "Di Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Fine-tuning Large Language Models on a political topic will significantly\nmanipulate their political stance on various issues and unintentionally affect\ntheir stance on unrelated topics. While previous studies have proposed this\nissue, there is still a lack of understanding regarding the internal\nrepresentations of these stances and the mechanisms that lead to unintended\ncross-topic generalization. In this paper, we systematically explore the\ninternal mechanisms underlying this phenomenon from a neuron-level perspective\nand how to mitigate the cross-topic generalization of political fine-tuning.\nFirstly, we propose Political Neuron Localization through Activation\nContrasting (PNLAC) to identify two distinct types of political neurons:\ngeneral political neurons, which govern stance across multiple political\ntopics, and topic-specific neurons} that affect the model's political stance on\nindividual topics. We find the existence of these political neuron types across\nfour models and datasets through activation patching experiments. Leveraging\nthese insights, we introduce InhibitFT, an inhibition-based fine-tuning method,\neffectively mitigating the cross-topic stance generalization. Experimental\nresults demonstrate the robustness of identified neuron types across various\nmodels and datasets, and show that InhibitFT significantly reduces the\ncross-topic stance generalization by 20% on average, while preserving\ntopic-specific performance. Moreover, we demonstrate that selectively\ninhibiting only 5% of neurons is sufficient to effectively mitigate the\ncross-topic stance generalization.", "AI": {"tldr": "This paper explores political fine-tuning of Large Language Models (LLMs), uncovering internal neuron mechanisms that lead to unintended stance generalization across different political topics.", "motivation": "There is a lack of understanding about how fine-tuning LLMs on political topics affects their stances on unrelated issues, which can lead to unintended consequences.", "method": "The authors propose Political Neuron Localization through Activation Contrasting (PNLAC) to identify general and topic-specific political neurons, and introduce InhibitFT, a method to mitigate cross-topic generalization.", "result": "The paper identifies two types of political neurons and demonstrates that InhibitFT reduces cross-topic generalization by 20% while maintaining topic-specific performance, using only 5% neuron inhibition.", "conclusion": "The findings highlight the complexity of political stance representation in LLMs and provide a novel approach to mitigate unintended generalizations.", "key_contributions": ["Introduction of PNLAC for neuron localization in political contexts", "Development of InhibitFT for optimizing fine-tuning and reducing generalization", "Empirical validation of neuron types across multiple models and datasets"], "limitations": "", "keywords": ["Large Language Models", "political fine-tuning", "cross-topic generalization", "neuron localization", "machine learning"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2508.02401", "pdf": "https://arxiv.org/pdf/2508.02401.pdf", "abs": "https://arxiv.org/abs/2508.02401", "title": "CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important Before Generation", "authors": ["Xiaolin Lin", "Jingcun Wang", "Olga Kondrateva", "Yiyu Shi", "Bing Li", "Grace Li Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have significantly boosted\nlong-context processing. However, the increasing key-value (KV) cache size\nposes critical challenges to memory and execution efficiency. Most KV cache\ncompression methods rely on heuristic token eviction using all attention heads\nin Grouped Query Attention (GQA)-based LLMs. This method ignores the different\nfunctionalities of attention heads, leading to the eviction of critical tokens\nand thus degrades the performance of LLMs.\n  To address the issue above, instead of using all the attention heads in\nGQA-based LLMs to determine important tokens as in the previous work, we first\nidentify the attention heads in each layer that are not only capable of\nretrieving the initial and final tokens of a prompt, but also capable of\nretrieving important tokens within the text and attending to their surrounding\nsemantic context. Afterwards, we exploit such heads to determine the important\ntokens and retain their corresponding KV cache pairs. Furthermore, we analyze\nthe cache eviction error of each layer individually and introduce a\nlayer-adaptive KV cache allocation strategy. Experimental results demonstrate\nthe proposed CompressKV consistently outperforms state-of-the-art approaches\nunder various memory budgets on LongBench and Needle-in-a-Haystack benchmarks.\nOur code is publicly available at: https://github.com/TUDa-HWAI/CompressKV.git.", "AI": {"tldr": "The paper presents a method called CompressKV to improve memory and execution efficiency in long-context processing of LLMs by using selective attention heads for key-value cache retention.", "motivation": "The increasing size of KV caches in LLMs poses challenges for memory and execution efficiency, particularly because existing methods overlook the distinct functions of attention heads.", "method": "The proposed method identifies specific attention heads capable of retrieving important tokens and their contextual meanings, and adopts a layer-adaptive KV cache allocation strategy.", "result": "CompressKV outperforms state-of-the-art approaches across various memory budgets on the LongBench and Needle-in-a-Haystack benchmarks.", "conclusion": "The new method enhances memory efficiency in long-context LLMs while maintaining performance, preventing the eviction of critical tokens.", "key_contributions": ["Introduced a method for selective attention head usage for KV cache management", "Developed a layer-adaptive KV cache allocation strategy", "Demonstrated significant performance improvements on standard benchmarks"], "limitations": "", "keywords": ["large language models", "KV cache", "memory efficiency", "attention heads", "contextual processing"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.02426", "pdf": "https://arxiv.org/pdf/2508.02426.pdf", "abs": "https://arxiv.org/abs/2508.02426", "title": "Learning to Evolve: Bayesian-Guided Continual Knowledge Graph Embedding", "authors": ["Linyu Li", "Zhi Jin", "Yuanpeng He", "Dongming Jin", "Yichi Zhang", "Haoran Duan", "Nyima Tash"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Since knowledge graphs (KG) will continue to evolve in real scenarios,\ntraditional KGE models are only suitable for static knowledge graphs.\nTherefore, continual knowledge graph embedding (CKGE) has attracted the\nattention of researchers. Currently, a key challenge facing CKGE is that the\nmodel is prone to \"catastrophic forgetting\", resulting in the loss of\npreviously learned knowledge. In order to effectively alleviate this problem,\nwe propose a new CKGE model BAKE. First, we note that the Bayesian posterior\nupdate principle provides a natural continual learning strategy that is\ninsensitive to data order and can theoretically effectively resist the\nforgetting of previous knowledge during data evolution. Different from the\nexisting CKGE method, BAKE regards each batch of new data as a Bayesian update\nof the model prior. Under this framework, as long as the posterior distribution\nof the model is maintained, the model can better preserve the knowledge of\nearly snapshots even after evolving through multiple time snapshots. Secondly,\nwe propose a continual clustering method for CKGE, which further directly\ncombats knowledge forgetting by constraining the evolution difference (or\nchange amplitude) between new and old knowledge between different snapshots. We\nconduct extensive experiments on BAKE on multiple datasets, and the results\nshow that BAKE significantly outperforms existing baseline models.", "AI": {"tldr": "This paper presents BAKE, a continual knowledge graph embedding model that mitigates catastrophic forgetting, demonstrating substantial improvements over existing methods.", "motivation": "Traditional KGE models cannot handle the dynamic nature of real-world knowledge graphs, leading to the necessity for continual knowledge graph embedding (CKGE) to prevent catastrophic forgetting.", "method": "The BAKE model employs Bayesian posterior updates to maintain knowledge across evolving datasets and introduces a continual clustering method to minimize the change between new and existing knowledge.", "result": "BAKE significantly outperforms existing CKGE baseline models in preserving knowledge across multiple time snapshots.", "conclusion": "The effective use of Bayesian updates in BAKE provides a robust framework for continually evolving knowledge graphs, addressing the critical issue of knowledge forgetting.", "key_contributions": ["Introduced BAKE model leveraging Bayesian updates for CKGE.", "Proposed continual clustering method to combat knowledge forgetting.", "Demonstrated significant improvements over existing CKGE models."], "limitations": "", "keywords": ["Knowledge Graphs", "Continual Learning", "Machine Learning", "Knowledge Graph Embedding"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.02430", "pdf": "https://arxiv.org/pdf/2508.02430.pdf", "abs": "https://arxiv.org/abs/2508.02430", "title": "AI-Based Measurement of Innovation: Mapping Expert Insight into Large Language Model Applications", "authors": ["Robin Nowak", "Patrick Figge", "Carolin Haeussler"], "categories": ["cs.CL"], "comment": null, "summary": "Measuring innovation often relies on context-specific proxies and on expert\nevaluation. Hence, empirical innovation research is often limited to settings\nwhere such data is available. We investigate how large language models (LLMs)\ncan be leveraged to overcome the constraints of manual expert evaluations and\nassist researchers in measuring innovation. We design an LLM framework that\nreliably approximates domain experts' assessment of innovation from\nunstructured text data. We demonstrate the performance and broad applicability\nof this framework through two studies in different contexts: (1) the\ninnovativeness of software application updates and (2) the originality of\nuser-generated feedback and improvement ideas in product reviews. We compared\nthe performance (F1-score) and reliability (consistency rate) of our LLM\nframework against alternative measures used in prior innovation studies, and to\nstate-of-the-art machine learning- and deep learning-based models. The LLM\nframework achieved higher F1-scores than the other approaches, and its results\nare highly consistent (i.e., results do not change across runs). This article\nequips R&D personnel in firms, as well as researchers, reviewers, and editors,\nwith the knowledge and tools to effectively use LLMs for measuring innovation\nand evaluating the performance of LLM-based innovation measures. In doing so,\nwe discuss, the impact of important design decisions-including model selection,\nprompt engineering, training data size, training data distribution, and\nparameter settings-on performance and reliability. Given the challenges\ninherent in using human expert evaluation and existing text-based measures, our\nframework has important implications for harnessing LLMs as reliable,\nincreasingly accessible, and broadly applicable research tools for measuring\ninnovation.", "AI": {"tldr": "This paper investigates how large language models (LLMs) can assist in measuring innovation by approximating expert assessments from unstructured text data, demonstrating superior performance compared to traditional methods.", "motivation": "Empirical innovation research is often constrained by reliance on context-specific proxies and expert evaluations, which limits its scope.", "method": "Design an LLM framework to assess innovation from unstructured text data, validated through two studies: evaluating software updates and user-generated product feedback.", "result": "The LLM framework achieved higher F1-scores and consistent results compared to alternative measures and state-of-the-art machine learning models.", "conclusion": "The framework provides R&D personnel and researchers with tools to effectively measure innovation using LLMs, addressing limitations of human expert evaluations.", "key_contributions": ["Development of an LLM framework for measuring innovation", "Demonstration of superior performance compared to traditional methods", "Discussion of design decisions affecting LLM performance"], "limitations": "The paper may depend on the quality and diversity of training data for the LLM framework.", "keywords": ["Innovation measurement", "Large language models", "Machine learning", "Expert evaluation", "Unstructured text data"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.02452", "pdf": "https://arxiv.org/pdf/2508.02452.pdf", "abs": "https://arxiv.org/abs/2508.02452", "title": "LatentPrompt: Optimizing Promts in Latent Space", "authors": ["Mateusz Bystroński", "Grzegorz Piotrowski", "Nitesh V. Chawla", "Tomasz Kajdanowicz"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advances have shown that optimizing prompts for Large Language Models\n(LLMs) can significantly improve task performance, yet many optimization\ntechniques rely on heuristics or manual exploration. We present LatentPrompt, a\nmodel-agnostic framework for prompt optimization that leverages latent semantic\nspace to automatically generate, evaluate, and refine candidate prompts without\nrequiring hand-crafted rules. Beginning with a set of seed prompts, our method\nembeds them in a continuous latent space and systematically explores this space\nto identify prompts that maximize task-specific performance. In a\nproof-of-concept study on the Financial PhraseBank sentiment classification\nbenchmark, LatentPrompt increased classification accuracy by approximately 3\npercent after a single optimization cycle. The framework is broadly applicable,\nrequiring only black-box access to an LLM and an automatic evaluation metric,\nmaking it suitable for diverse domains and tasks.", "AI": {"tldr": "LatentPrompt is a model-agnostic framework for optimizing prompts for Large Language Models using latent semantic space, improving task performance without manual exploration.", "motivation": "The need for effective prompt optimization techniques for LLMs that do not rely on heuristics or manual exploration.", "method": "LatentPrompt uses a continuous latent space to automatically generate, evaluate, and refine candidate prompts from initial seed prompts, maximizing performance for specific tasks.", "result": "In a study on the Financial PhraseBank sentiment classification benchmark, LatentPrompt improved classification accuracy by approximately 3 percent after a single optimization cycle.", "conclusion": "LatentPrompt's model-agnostic framework is versatile and can be applied to various domains and tasks, requiring only black-box access to LLMs and an automatic evaluation metric.", "key_contributions": ["Introduction of a model-agnostic framework for prompt optimization.", "Use of latent semantic space for automatic prompt generation and evaluation.", "Demonstrated effectiveness with improved classification accuracy in a real-world sentiment analysis task."], "limitations": "The framework's performance may vary based on the chosen latent space representation and the specific task.", "keywords": ["Large Language Models", "prompt optimization", "latent semantic space", "automated evaluation", "sentiment classification"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2508.02498", "pdf": "https://arxiv.org/pdf/2508.02498.pdf", "abs": "https://arxiv.org/abs/2508.02498", "title": "Monsoon Uprising in Bangladesh: How Facebook Shaped Collective Identity", "authors": ["Md Tasin Abir", "Arpita Chowdhury", "Ashfia Rahman"], "categories": ["cs.CL"], "comment": "10 pages, 9 figures", "summary": "This study investigates how Facebook shaped collective identity during the\nJuly 2024 pro-democracy uprising in Bangladesh, known as the Monsoon Uprising.\nDuring government repression, protesters turned to Facebook as a central space\nfor resistance, where multimodal expressions, images, memes, videos, hashtags,\nand satirical posts played an important role in unifying participants. Using a\nqualitative approach, this research analyzes visual rhetoric, verbal discourse,\nand digital irony to reveal how shared symbols, protest art, and slogans built\na sense of solidarity. Key elements included the symbolic use of red, the\nironic metaphorical use of the term \"Razakar\", and the widespread sharing of\nvisuals representing courage, injustice, and resistance. The findings show that\nthe combination of visual and verbal strategies on Facebook not only mobilized\npublic sentiment, but also built a strong collective identity that challenged\nauthoritarian narratives. This study tries to demonstrate how online platforms\ncan serve as powerful tools for identity construction and political\nmobilization in the digital age.", "AI": {"tldr": "This study examines how Facebook facilitated collective identity during the Monsoon Uprising in Bangladesh using visual and verbal expressions.", "motivation": "To understand the role of social media in fostering identity and mobilization during political protests, particularly in authoritarian contexts.", "method": "A qualitative analysis of multimodal expressions on Facebook, including visuals, memes, and discourse, during the July 2024 pro-democracy uprising in Bangladesh.", "result": "Facebook was instrumental in unifying protesters by employing symbolic imagery and discourse, which fostered a collective identity against government repression.", "conclusion": "The study illustrates that online platforms like Facebook can effectively aid in identity construction and political mobilization during social movements.", "key_contributions": ["Analysis of multimodal communication in social protests", "Demonstration of the role of Facebook in shaping collective identity", "Insights into the use of visual rhetoric and discourse during political unrest."], "limitations": "", "keywords": ["collective identity", "Facebook", "political mobilization", "multimodal expressions", "uprising"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2508.02502", "pdf": "https://arxiv.org/pdf/2508.02502.pdf", "abs": "https://arxiv.org/abs/2508.02502", "title": "From Monolingual to Bilingual: Investigating Language Conditioning in Large Language Models for Psycholinguistic Tasks", "authors": ["Shuzhou Yuan", "Zhan Qu", "Mario Tawfelis", "Michael Färber"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) exhibit strong linguistic capabilities, but\nlittle is known about how they encode psycholinguistic knowledge across\nlanguages. We investigate whether and how LLMs exhibit human-like\npsycholinguistic responses under different linguistic identities using two\ntasks: sound symbolism and word valence. We evaluate two models,\nLlama-3.3-70B-Instruct and Qwen2.5-72B-Instruct, under monolingual and\nbilingual prompting in English, Dutch, and Chinese. Behaviorally, both models\nadjust their outputs based on prompted language identity, with Qwen showing\ngreater sensitivity and sharper distinctions between Dutch and Chinese. Probing\nanalysis reveals that psycholinguistic signals become more decodable in deeper\nlayers, with Chinese prompts yielding stronger and more stable valence\nrepresentations than Dutch. Our results demonstrate that language identity\nconditions both output behavior and internal representations in LLMs, providing\nnew insights into their application as models of cross-linguistic cognition.", "AI": {"tldr": "This paper investigates how Large Language Models (LLMs) encode psycholinguistic knowledge in different languages and their responses to linguistic identity.", "motivation": "To explore the psycholinguistic responses of LLMs across different languages and understand their internal representations related to language identity.", "method": "The study uses two tasks (sound symbolism and word valence) to evaluate Llama-3.3-70B-Instruct and Qwen2.5-72B-Instruct under monolingual and bilingual prompting in English, Dutch, and Chinese.", "result": "Both models had outputs that adjusted based on the prompted language identity, with Qwen exhibiting greater sensitivity between Dutch and Chinese. Psycholinguistic signals were found more decodable in deeper layers, especially with Chinese prompts showing stronger valence representations.", "conclusion": "The findings reveal that LLMs' outputs and internal representations are influenced by language identity, contributing to our understanding of cross-linguistic cognition.", "key_contributions": ["Demonstrated that LLMs exhibit human-like psycholinguistic responses based on language identity.", "Probed deeper layers of LLMs revealing more decodable psycholinguistic signals.", "Provided insights into LLMs as models of cross-linguistic cognition."], "limitations": "", "keywords": ["Large Language Models", "psycholinguistics", "language identity", "cross-linguistic cognition", "LLMs"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.02513", "pdf": "https://arxiv.org/pdf/2508.02513.pdf", "abs": "https://arxiv.org/abs/2508.02513", "title": "Modular Arithmetic: Language Models Solve Math Digit by Digit", "authors": ["Tanja Baeumel", "Daniil Gurgurov", "Yusser al Ghussin", "Josef van Genabith", "Simon Ostermann"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While recent work has begun to uncover the internal strategies that Large\nLanguage Models (LLMs) employ for simple arithmetic tasks, a unified\nunderstanding of their underlying mechanisms is still lacking. We extend recent\nfindings showing that LLMs represent numbers in a digit-wise manner and present\nevidence for the existence of digit-position-specific circuits that LLMs use to\nperform simple arithmetic tasks, i.e. modular subgroups of MLP neurons that\noperate independently on different digit positions (units, tens, hundreds).\nNotably, such circuits exist independently of model size and of tokenization\nstrategy, i.e. both for models that encode longer numbers digit-by-digit and as\none token. Using Feature Importance and Causal Interventions, we identify and\nvalidate the digit-position-specific circuits, revealing a compositional and\ninterpretable structure underlying the solving of arithmetic problems in LLMs.\nOur interventions selectively alter the model's prediction at targeted digit\npositions, demonstrating the causal role of digit-position circuits in solving\narithmetic tasks.", "AI": {"tldr": "This paper investigates how Large Language Models (LLMs) handle simple arithmetic tasks by examining their internal digit-position-specific circuits used for calculation.", "motivation": "The work aims to provide a unified understanding of the mechanisms LLMs use for arithmetic tasks, which has not been fully explored.", "method": "The authors employed Feature Importance and Causal Interventions to identify and validate the existence of digit-position-specific circuits in LLMs that perform independent operations on different digit positions (units, tens, hundreds).", "result": "The findings reveal that LLMs possess modular circuits operating on distinct digit positions, which are effective regardless of the model's size or tokenization approach.", "conclusion": "The presence of these digit-position-specific circuits contributes to the compositional and interpretable nature of how LLMs solve arithmetic problems, underscoring their causal significance in predictions.", "key_contributions": ["Identification of digit-position-specific circuits in LLMs", "Demonstration of causal interventions impacting model predictions", "Insights into the interpretable structure of arithmetic problem-solving in LLMs"], "limitations": "", "keywords": ["Large Language Models", "arithmetic", "digit-position circuits", "causal interventions", "Feature Importance"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.02515", "pdf": "https://arxiv.org/pdf/2508.02515.pdf", "abs": "https://arxiv.org/abs/2508.02515", "title": "PoeTone: A Framework for Constrained Generation of Structured Chinese Songci with LLMs", "authors": ["Zhan Qu", "Shuzhou Yuan", "Michael Färber"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This paper presents a systematic investigation into the constrained\ngeneration capabilities of large language models (LLMs) in producing Songci, a\nclassical Chinese poetry form characterized by strict structural, tonal, and\nrhyme constraints defined by Cipai templates. We first develop a comprehensive,\nmulti-faceted evaluation framework that includes: (i) a formal conformity\nscore, (ii) automated quality assessment using LLMs, (iii) human evaluation,\nand (iv) classification-based probing tasks. Using this framework, we evaluate\nthe generative performance of 18 LLMs, including 3 proprietary models and 15\nopen-source models across four families, under five prompting strategies:\nzero-shot, one-shot, completion-based, instruction-tuned, and chain-of-thought.\nFinally, we propose a Generate-Critic architecture in which the evaluation\nframework functions as an automated critic. Leveraging the critic's feedback as\na reward signal, we fine-tune three lightweight open-source LLMs via supervised\nfine-tuning (SFT), resulting in improvements of up to 5.88% in formal\nconformity. Our findings offer new insights into the generative strengths and\nlimitations of LLMs in producing culturally significant and formally\nconstrained literary texts.", "AI": {"tldr": "This paper investigates the constrained generation capabilities of large language models (LLMs) in producing Songci, a Chinese poetry form, using a multi-faceted evaluation framework and proposes improvements via a Generate-Critic architecture.", "motivation": "To assess the generative performance of LLMs in producing culturally significant poetry under strict structural and tonal constraints.", "method": "The study develops a comprehensive evaluation framework that combines formal scoring, automated quality assessment, human evaluation, and classification-based probing. It evaluates 18 LLMs across different prompting strategies and introduces a Generate-Critic architecture for fine-tuning LLMs.", "result": "The evaluation of 18 LLMs revealed varying generative performances, and the introduction of a Generate-Critic architecture led to improvements in conformity scores by up to 5.88%.", "conclusion": "The research provides insights into the capabilities and limitations of LLMs in generating formally constrained poetry, suggesting areas for further enhancement in model training and evaluation.", "key_contributions": ["Development of a comprehensive evaluation framework for LLMs in poetry generation", "Proposition of a Generate-Critic architecture for fine-tuning LLMs", "Empirical analysis of 18 different LLMs under diverse prompting strategies"], "limitations": "The study is limited to the evaluation of LLMs in a specific poetic form and does not explore generative performance in other literary forms.", "keywords": ["Large Language Models", "Songci", "Poetry Generation", "Evaluation Framework", "Generate-Critic Architecture"], "importance_score": 5, "read_time_minutes": 15}}
{"id": "2508.02527", "pdf": "https://arxiv.org/pdf/2508.02527.pdf", "abs": "https://arxiv.org/abs/2508.02527", "title": "I Have No Mouth, and I Must Rhyme: Uncovering Internal Phonetic Representations in LLaMA 3.2", "authors": ["Jack Merullo", "Arjun Khurana", "Oliver McLaughlin"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models demonstrate proficiency on phonetic tasks, such as\nrhyming, without explicit phonetic or auditory grounding. In this work, we\ninvestigate how \\verb|Llama-3.2-1B-Instruct| represents token-level phonetic\ninformation. Our results suggest that Llama uses a rich internal model of\nphonemes to complete phonetic tasks. We provide evidence for high-level\norganization of phoneme representations in its latent space. In doing so, we\nalso identify a ``phoneme mover head\" which promotes phonetic information\nduring rhyming tasks. We visualize the output space of this head and find that,\nwhile notable differences exist, Llama learns a model of vowels similar to the\nstandard IPA vowel chart for humans, despite receiving no direct supervision to\ndo so.", "AI": {"tldr": "The study investigates Llama's internal representation of phonetic information, revealing a structured model of phonemes aiding phonetic tasks like rhyming.", "motivation": "To explore the phonetic capabilities of large language models despite a lack of explicit phonetic or auditory grounding.", "method": "Analyzed the Llama-3.2-1B-Instruct model to assess its internal phoneme representation and identified a specific phoneme mover head that enhances phonetic information during tasks.", "result": "The results indicate that Llama possesses a rich model of phonemes and organizes them in a latent space reminiscent of the IPA vowel chart without explicit supervision.", "conclusion": "Llama's learning reflects an intrinsic understanding of phonemes, demonstrating its potential in phonetic tasks even without direct training on phonetic principles.", "key_contributions": ["Identification of a phoneme mover head that processes phonetic information.", "Visualization of phoneme representations aligning with known linguistic models.", "Evidence of intrinsic phonetic organization within the model's latent space."], "limitations": "", "keywords": ["phonetic tasks", "Llama-3.2-1B-Instruct", "phoneme representation"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.02532", "pdf": "https://arxiv.org/pdf/2508.02532.pdf", "abs": "https://arxiv.org/abs/2508.02532", "title": "Contextual Graph Transformer: A Small Language Model for Enhanced Engineering Document Information Extraction", "authors": ["Karan Reddy", "Mayukha Pal"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Standard transformer-based language models, while powerful for general text,\noften struggle with the fine-grained syntax and entity relationships in complex\ntechnical, engineering documents. To address this, we propose the Contextual\nGraph Transformer (CGT), a hybrid neural architecture that combines Graph\nNeural Networks (GNNs) and Transformers for domain-specific question answering.\nCGT constructs a dynamic graph over input tokens using sequential, skip-gram,\nand semantic similarity edges, which is processed by GATv2Conv layers for local\nstructure learning. These enriched embeddings are then passed to a Transformer\nencoder to capture global dependencies. Unlike generic large models, technical\ndomains often require specialized language models with stronger\ncontextualization and structure awareness. CGT offers a parameter-efficient\nsolution for such use cases. Integrated into a Retrieval-Augmented Generation\n(RAG) pipeline, CGT outperforms baselines like GPT-2 and BERT, achieving 24.7%\nhigher accuracy than GPT-2 with 62.4% fewer parameters. This gain stems from\nCGTs ability to jointly model structural token interactions and long-range\nsemantic coherence. The model is trained from scratch using a two-phase\napproach: pretraining on general text followed by fine-tuning on\ndomain-specific manuals. This highlights CGTs adaptability to technical\nlanguage, enabling better grounding, entity tracking, and retrieval-augmented\nresponses in real-world applications.", "AI": {"tldr": "This paper introduces the Contextual Graph Transformer (CGT), a hybrid model combining Graph Neural Networks and Transformers for effective domain-specific question answering in technical documents.", "motivation": "Standard transformer-based models struggle with syntax and entity relationships in complex technical documents, necessitating a specialized model.", "method": "The CGT constructs a dynamic graph with edges based on sequential, skip-gram, and semantic similarity, processed by GATv2Conv layers before being passed to a Transformer encoder.", "result": "CGT outperforms baseline models like GPT-2 and BERT by 24.7% in accuracy with 62.4% fewer parameters, thanks to its ability to model structural interactions and semantic coherence.", "conclusion": "CGT adapts well to technical language, improving grounding, entity tracking, and retrieval-augmented responses in practical applications.", "key_contributions": ["Introduction of Contextual Graph Transformer (CGT) for domain-specific question answering", "Demonstrated superior performance over GPT-2 and BERT with fewer parameters", "Showcased the model's adaptability in processing technical language and entities."], "limitations": "", "keywords": ["Graph Neural Networks", "Transformers", "Domain-specific Question Answering", "Technical Documents"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.02540", "pdf": "https://arxiv.org/pdf/2508.02540.pdf", "abs": "https://arxiv.org/abs/2508.02540", "title": "What's in the News? Towards Identification of Bias by Commission, Omission, and Source Selection (COSS)", "authors": ["Anastasia Zhukova", "Terry Ruas", "Felix Hamborg", "Karsten Donnay", "Bela Gipp"], "categories": ["cs.CL"], "comment": "published in the Proceedings of the 2023 ACM/IEEE Joint Conference on\n  Digital Libraries", "summary": "In a world overwhelmed with news, determining which information comes from\nreliable sources or how neutral is the reported information in the news\narticles poses a challenge to news readers. In this paper, we propose a\nmethodology for automatically identifying bias by commission, omission, and\nsource selection (COSS) as a joint three-fold objective, as opposed to the\nprevious work separately addressing these types of bias. In a pipeline concept,\nwe describe the goals and tasks of its steps toward bias identification and\nprovide an example of a visualization that leverages the extracted features and\npatterns of text reuse.", "AI": {"tldr": "This paper presents a novel methodology for the automatic identification of bias in news articles by analyzing three dimensions: commission, omission, and source selection, aiming to provide a comprehensive solution rather than treating these biases separately.", "motivation": "The increase in news volume has made it difficult for readers to identify reliable sources and neutrality in reporting, which is crucial for informed decision-making.", "method": "The authors propose a pipeline methodology that jointly addresses bias types across three tasks: commission, omission, and source selection, providing analysis and visualization tools.", "result": "The methodology offers a structured approach to bias identification in news articles and includes a visualization example that highlights patterns of text reuse.", "conclusion": "A comprehensive approach to bias identification can aid readers in discerning the reliability and neutrality of news articles, enhancing their understanding of media content.", "key_contributions": ["Joint methodology for bias identification in news articles", "Comprehensive framework addressing multiple dimensions of bias", "Visualization tool demonstrating the extracted features and patterns"], "limitations": "", "keywords": ["news bias", "information reliability", "visualization", "text analysis", "media literacy"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2508.02555", "pdf": "https://arxiv.org/pdf/2508.02555.pdf", "abs": "https://arxiv.org/abs/2508.02555", "title": "Building and Aligning Comparable Corpora", "authors": ["Motaz Saad", "David Langlois", "Kamel Smaili"], "categories": ["cs.CL", "I.2.7"], "comment": "27 pages, 11 figures", "summary": "Comparable corpus is a set of topic aligned documents in multiple languages,\nwhich are not necessarily translations of each other. These documents are\nuseful for multilingual natural language processing when there is no parallel\ntext available in some domains or languages. In addition, comparable documents\nare informative because they can tell what is being said about a topic in\ndifferent languages. In this paper, we present a method to build comparable\ncorpora from Wikipedia encyclopedia and EURONEWS website in English, French and\nArabic languages. We further experiment a method to automatically align\ncomparable documents using cross-lingual similarity measures. We investigate\ntwo cross-lingual similarity measures to align comparable documents. The first\nmeasure is based on bilingual dictionary, and the second measure is based on\nLatent Semantic Indexing (LSI). Experiments on several corpora show that the\nCross-Lingual LSI (CL-LSI) measure outperforms the dictionary based measure.\nFinally, we collect English and Arabic news documents from the British\nBroadcast Corporation (BBC) and from ALJAZEERA (JSC) news website respectively.\nThen we use the CL-LSI similarity measure to automatically align comparable\ndocuments of BBC and JSC. The evaluation of the alignment shows that CL-LSI is\nnot only able to align cross-lingual documents at the topic level, but also it\nis able to do this at the event level.", "AI": {"tldr": "The paper presents a methodology for building and aligning comparable corpora in multiple languages, focusing on English, French, and Arabic, using cross-lingual similarity measures to facilitate multilingual NLP tasks.", "motivation": "To improve multilingual natural language processing by creating comparable corpora when parallel texts are unavailable in various domains or languages.", "method": "The authors constructed comparable corpora from Wikipedia and EURONEWS, then aligned these documents using two cross-lingual similarity measures: a bilingual dictionary and Latent Semantic Indexing (LSI).", "result": "The experiments show that the Cross-Lingual LSI (CL-LSI) significantly outperforms the bilingual dictionary-based measure in aligning documents, successfully aligning them at both topic and event levels.", "conclusion": "The study concludes that CL-LSI is effective in the automatic alignment of comparable documents across languages, which can aid multilingual NLP applications.", "key_contributions": ["Development of a methodology for building comparable corpora from diverse sources.", "Introduction and evaluation of the Cross-Lingual LSI measure as a superior alignment technique.", "Demonstration of the effectiveness of CL-LSI in aligning documents at both topic and event levels."], "limitations": "", "keywords": ["comparable corpora", "cross-lingual similarity", "Latent Semantic Indexing", "multilingual NLP", "document alignment"], "importance_score": 5, "read_time_minutes": 15}}
{"id": "2508.02556", "pdf": "https://arxiv.org/pdf/2508.02556.pdf", "abs": "https://arxiv.org/abs/2508.02556", "title": "Automated SNOMED CT Concept Annotation in Clinical Text Using Bi-GRU Neural Networks", "authors": ["Ali Noori", "Pratik Devkota", "Somya Mohanty", "Prashanti Manda"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Automated annotation of clinical text with standardized medical concepts is\ncritical for enabling structured data extraction and decision support. SNOMED\nCT provides a rich ontology for labeling clinical entities, but manual\nannotation is labor-intensive and impractical at scale. This study introduces a\nneural sequence labeling approach for SNOMED CT concept recognition using a\nBidirectional GRU model. Leveraging a subset of MIMIC-IV, we preprocess text\nwith domain-adapted SpaCy and SciBERT-based tokenization, segmenting sentences\ninto overlapping 19-token chunks enriched with contextual, syntactic, and\nmorphological features. The Bi-GRU model assigns IOB tags to identify concept\nspans and achieves strong performance with a 90 percent F1-score on the\nvalidation set. These results surpass traditional rule-based systems and match\nor exceed existing neural models. Qualitative analysis shows effective handling\nof ambiguous terms and misspellings. Our findings highlight that lightweight\nRNN-based architectures can deliver high-quality clinical concept annotation\nwith significantly lower computational cost than transformer-based models,\nmaking them well-suited for real-world deployment.", "AI": {"tldr": "This study presents a neural sequence labeling approach using a Bidirectional GRU for automated annotation of clinical texts with SNOMED CT concepts, achieving a 90% F1-score.", "motivation": "Automated annotation of clinical text is vital for structured data extraction and decision support, but manual processes are impractical at scale.", "method": "A Bidirectional GRU model processes text with domain-adapted tokenization and segments sentences into overlapping 19-token chunks with enriched features, assigning IOB tags for concept recognition.", "result": "Achieved a 90% F1-score on the validation set, outperforming traditional rule-based systems and matching or exceeding existing neural models.", "conclusion": "Lightweight RNN-based architectures provide high-quality clinical concept annotation with lower computational costs, suitable for practical applications.", "key_contributions": ["Introduced a neural sequence labeling approach for clinical text annotation.", "Demonstrated the effectiveness of a Bi-GRU model with high performance on concept recognition tasks.", "Showed that RNN-based models can be both efficient and accurate compared to transformer-based models."], "limitations": "", "keywords": ["clinical text annotation", "SNOMED CT", "Bidirectional GRU", "sequence labeling", "machine learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.02558", "pdf": "https://arxiv.org/pdf/2508.02558.pdf", "abs": "https://arxiv.org/abs/2508.02558", "title": "Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction", "authors": ["Yuerong Song", "Xiaoran Liu", "Ruixiao Li", "Zhigeng Liu", "Zengfeng Huang", "Qipeng Guo", "Ziwei He", "Xipeng Qiu"], "categories": ["cs.CL"], "comment": "11 pages, 6 figures", "summary": "Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and\nparallel decoding but suffer from prohibitive quadratic computational\ncomplexity and memory overhead during inference. Current caching techniques\naccelerate decoding by storing full-layer states, yet impose substantial memory\nusage that limit long-context applications. Our analysis of attention patterns\nin dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining\nsalient across decoding steps and low-relevance tokens staying unimportant,\nmotivating selective cache eviction. We propose Sparse-dLLM, the first\ntraining-free framework integrating dynamic cache eviction with sparse\nattention via delayed bidirectional sparse caching. By leveraging the stability\nof token saliency over steps, it retains critical tokens and dynamically evicts\nunimportant prefix/suffix entries using an attention-guided strategy. Extensive\nexperiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to\n10$\\times$ higher throughput than vanilla dLLMs, with comparable performance\nand similar peak memory costs, outperforming previous methods in efficiency and\neffectiveness.", "AI": {"tldr": "Sparse-dLLM introduces a training-free framework for dynamic cache eviction and sparse attention in diffusion large language models, improving decoding efficiency.", "motivation": "Diffusion Large Language Models face high computational and memory costs during inference due to quadratic complexity. Current caching methods improve speed but have high memory demands, particularly in long-context scenarios.", "method": "The paper presents Sparse-dLLM, which combines dynamic cache eviction with sparse attention using a delayed bidirectional sparse caching strategy, retaining crucial tokens while evicting unimportant ones based on attention patterns.", "result": "Sparse-dLLM achieves up to 10 times higher throughput compared to vanilla diffusion large language models while maintaining performance and similar peak memory costs.", "conclusion": "The proposed framework significantly enhances efficiency and effectiveness in the use of diffusion large language models, making it suitable for applications with long-context requirements.", "key_contributions": ["First training-free framework for dynamic cache eviction in dLLMs", "Integration of sparse attention through delayed bidirectional caching", "Demonstration of up to 10x higher throughput compared to existing methods"], "limitations": "", "keywords": ["Diffusion Models", "Large Language Models", "Cache Eviction", "Sparse Attention", "Token Saliency"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.02573", "pdf": "https://arxiv.org/pdf/2508.02573.pdf", "abs": "https://arxiv.org/abs/2508.02573", "title": "Guess or Recall? Training CNNs to Classify and Localize Memorization in LLMs", "authors": ["Jérémie Dentan", "Davide Buscaldi", "Sonia Vanier"], "categories": ["cs.CL"], "comment": null, "summary": "Verbatim memorization in Large Language Models (LLMs) is a multifaceted\nphenomenon involving distinct underlying mechanisms. We introduce a novel\nmethod to analyze the different forms of memorization described by the existing\ntaxonomy. Specifically, we train Convolutional Neural Networks (CNNs) on the\nattention weights of the LLM and evaluate the alignment between this taxonomy\nand the attention weights involved in decoding.\n  We find that the existing taxonomy performs poorly and fails to reflect\ndistinct mechanisms within the attention blocks. We propose a new taxonomy that\nmaximizes alignment with the attention weights, consisting of three categories:\nmemorized samples that are guessed using language modeling abilities, memorized\nsamples that are recalled due to high duplication in the training set, and\nnon-memorized samples. Our results reveal that few-shot verbatim memorization\ndoes not correspond to a distinct attention mechanism. We also show that a\nsignificant proportion of extractable samples are in fact guessed by the model\nand should therefore be studied separately. Finally, we develop a custom visual\ninterpretability technique to localize the regions of the attention weights\ninvolved in each form of memorization.", "AI": {"tldr": "This paper explores verbatim memorization in LLMs, proposing a new taxonomy based on attention weights.", "motivation": "To analyze the distinct mechanisms of verbatim memorization in LLMs and improve the existing taxonomy that is poorly aligned with attention weights.", "method": "Trained CNNs on LLM attention weights and evaluated the alignment with a proposed taxonomy of memorization forms.", "result": "The analysis revealed that the existing taxonomy fails to represent the mechanisms accurately, leading to a new classification: guessed samples, recalled samples, and non-memorized samples.", "conclusion": "Few-shot verbatim memorization doesn't correspond to a distinct attention mechanism, and a significant number of samples are guessed by the model, necessitating separate study.", "key_contributions": ["Introduction of a new taxonomy for verbatim memorization in LLMs", "Development of a custom visual interpretability technique", "Demonstration that the existing taxonomy inadequately reflects the nature of memorized samples."], "limitations": "", "keywords": ["Large Language Models", "memorization", "attention weights", "taxonomy", "interpretability"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2508.02574", "pdf": "https://arxiv.org/pdf/2508.02574.pdf", "abs": "https://arxiv.org/abs/2508.02574", "title": "EHSAN: Leveraging ChatGPT in a Hybrid Framework for Arabic Aspect-Based Sentiment Analysis in Healthcare", "authors": ["Eman Alamoudi", "Ellis Solaiman"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI"], "comment": null, "summary": "Arabic-language patient feedback remains under-analysed because dialect\ndiversity and scarce aspect-level sentiment labels hinder automated assessment.\nTo address this gap, we introduce EHSAN, a data-centric hybrid pipeline that\nmerges ChatGPT pseudo-labelling with targeted human review to build the first\nexplainable Arabic aspect-based sentiment dataset for healthcare. Each sentence\nis annotated with an aspect and sentiment label (positive, negative, or\nneutral), forming a pioneering Arabic dataset aligned with healthcare themes,\nwith ChatGPT-generated rationales provided for each label to enhance\ntransparency. To evaluate the impact of annotation quality on model\nperformance, we created three versions of the training data: a fully supervised\nset with all labels reviewed by humans, a semi-supervised set with 50% human\nreview, and an unsupervised set with only machine-generated labels. We\nfine-tuned two transformer models on these datasets for both aspect and\nsentiment classification. Experimental results show that our Arabic-specific\nmodel achieved high accuracy even with minimal human supervision, reflecting\nonly a minor performance drop when using ChatGPT-only labels. Reducing the\nnumber of aspect classes notably improved classification metrics across the\nboard. These findings demonstrate an effective, scalable approach to Arabic\naspect-based sentiment analysis (SA) in healthcare, combining large language\nmodel annotation with human expertise to produce a robust and explainable\ndataset. Future directions include generalisation across hospitals, prompt\nrefinement, and interpretable data-driven modelling.", "AI": {"tldr": "EHSAN introduces a novel pipeline for Arabic aspect-based sentiment analysis in healthcare, combining ChatGPT pseudo-labelling and human review to create an annotated dataset with high accuracy in classification tasks.", "motivation": "To overcome the challenges of dialect diversity and lack of aspect-level sentiment labels in Arabic-language patient feedback, enabling more effective automated sentiment analysis.", "method": "A data-centric hybrid pipeline that integrates ChatGPT pseudo-labelling with targeted human review, producing an explainable Arabic aspect-based sentiment dataset for healthcare assessments.", "result": "The Arabic-specific model exhibited high accuracy even with minimal human supervision, and results indicated a significant improvement in classification metrics by reducing the number of aspect classes.", "conclusion": "The proposed methodology demonstrates an effective, scalable approach for sentiment analysis in healthcare, with potential for broader generalisation and improved model interpretability in future work.", "key_contributions": ["First explainable Arabic aspect-based sentiment dataset for healthcare", "Combination of ChatGPT pseudo-labelling and human review", "High accuracy achieved with minimal human supervision"], "limitations": "Future work is needed to ensure generalisation across different hospitals and improve prompt refinement.", "keywords": ["Arabic", "sentiment analysis", "healthcare", "ChatGPT", "aspect-based labeling"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2508.02584", "pdf": "https://arxiv.org/pdf/2508.02584.pdf", "abs": "https://arxiv.org/abs/2508.02584", "title": "MArgE: Meshing Argumentative Evidence from Multiple Large Language Models for Justifiable Claim Verification", "authors": ["Ming Pok Ng", "Junqi Jiang", "Gabriel Freedman", "Antonio Rago", "Francesca Toni"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Leveraging outputs from multiple large language models (LLMs) is emerging as\na method for harnessing their power across a wide range of tasks while\nmitigating their capacity for making errors, e.g., hallucinations. However,\ncurrent approaches to combining insights from multiple LLMs often involve\nunstructured interactions (e.g., free debate), resulting in model generations\nthat are not faithfully justifiable. In this work, we introduce MArgE, a novel\nframework to provide formal structure to the evidence from each LLM, in the\nform of a tree of extracted arguments, for the task of claim verification. We\nuse a variant of Argumentative LLMs (ArgLLMs), i.e. LLMs driven by frameworks\nand semantics from the field of computational argumentation, to construct\nstructured argument trees for given claims. This process creates an inspectable\npathway from the initial arguments to the final claim verification decisions,\nproviding a faithful justification thereof. We show experimentally that MArgE\ncan significantly outperform single LLMs, including three open-source models\n(4B to 8B parameters), GPT-4o-mini and existing ArgLLMs, as well as prior\nmethods for unstructured multi-LLM debates. We thus demonstrate the advantages\nof incorporating formal, argumentative reasoning mechanisms when combining\nmultiple LLM outputs.", "AI": {"tldr": "Introducing MArgE, a framework for structured claim verification using multiple LLMs to improve accuracy and justification in decision-making.", "motivation": "The need for reliable methods to combine outputs from multiple LLMs to mitigate errors such as hallucinations in AI models.", "method": "A framework called MArgE leverages argumentative LLMs to construct structured argument trees for claim verification, enhancing the interpretability of model decisions.", "result": "MArgE significantly outperforms individual LLMs and existing multi-LLM debate methods in terms of claim verification accuracy.", "conclusion": "The study highlights the benefits of formal argumentative reasoning in enhancing the reliability of conclusions drawn from multiple LLM outputs.", "key_contributions": ["Development of MArgE framework for structured claim verification", "Demonstration of improved accuracy over traditional LLM approaches", "Novel combination of argumentative reasoning with LLM outputs"], "limitations": "", "keywords": ["Large Language Models", "Claim Verification", "Argumentative Reasoning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2508.02591", "pdf": "https://arxiv.org/pdf/2508.02591.pdf", "abs": "https://arxiv.org/abs/2508.02591", "title": "CharBench: Evaluating the Role of Tokenization in Character-Level Tasks", "authors": ["Omri Uzan", "Yuval Pinter"], "categories": ["cs.CL"], "comment": null, "summary": "Tasks that require character-level reasoning, such as counting or locating\ncharacters within words, remain challenging for contemporary language models. A\ncommon conjecture is that language models' reliance on subword units, rather\nthan characters, contributes to their struggles with character-level tasks, yet\nrecent studies offer conflicting conclusions about the role of tokenization,\nleaving its impact unclear. To address this gap, we introduce CharBench, a\ncomprehensive benchmark of character-level tasks that is two orders of\nmagnitude larger than existing alternatives. We evaluate a diverse range of\nleading open-weight and proprietary models on CharBench and find that it\npresents a significant challenge to modern LLMs, with an average accuracy of\n43.6% and 32.3% on some tasks. We present an in-depth analysis of how intrinsic\nproperties of words and their segmentations into tokens correspond to model\nperformance. For counting tasks, we find that tokenization properties are\nweakly correlated with correctness, while the length of the queried word and\nthe actual character count play a more significant part. In contrast, for tasks\nrequiring intra-word positional understanding, performance is negatively\ncorrelated with the length of the token containing the queried character,\nsuggesting that longer tokens obscure character position information for LLMs.\nWe encourage future work to build on the benchmark and evaluation methodology\nintroduced here as tools for improving model performance on such tasks.", "AI": {"tldr": "CharBench is a new benchmark for character-level tasks revealing significant challenges for language models.", "motivation": "To explore the struggles of language models with character-level reasoning and the unclear role of tokenization in model performance.", "method": "Introduced CharBench, a benchmark two orders of magnitude larger than existing alternatives, and evaluated various language models using this benchmark.", "result": "Leading models showed an average accuracy of 43.6%, with some tasks yielding as low as 32.3%. Analysis revealed that word length and actual character count significantly influence performance.", "conclusion": "The findings indicate that longer tokens can obscure character position information, and the study encourages further research using CharBench to enhance model performance.", "key_contributions": ["Introduction of CharBench as a benchmark for character-level tasks", "Analysis of model performance in relation to tokenization and word properties", "Identification of key correlations affecting performance in character-level reasoning tasks"], "limitations": "The benchmark may primarily reflect current limitations and does not address broader language understanding tasks.", "keywords": ["character-level reasoning", "language models", "tokenization", "CharBench", "performance analysis"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2508.02618", "pdf": "https://arxiv.org/pdf/2508.02618.pdf", "abs": "https://arxiv.org/abs/2508.02618", "title": "Mitigating Attention Hacking in Preference-Based Reward Modeling via Interaction Distillation", "authors": ["Jianxiang Zang", "Meiling Ning", "Shihan Dou", "Jiazheng Zhang", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "categories": ["cs.CL"], "comment": null, "summary": "The reward model (RM), as the core component of reinforcement learning from\nhuman feedback (RLHF) for large language models (LLMs), responsible for\nproviding reward signals to generated responses. However, mainstream preference\nmodeling in RM is inadequate in terms of token-level interaction, making its\njudgment signals vulnerable to being hacked by misallocated attention to\ncontext. This stems from two fundamental limitations: (1) Current preference\nmodeling employs decoder-only architectures, where the unidirectional causal\nattention mechanism leads to forward-decaying intra-sequence attention within\nthe prompt-response sequence. (2) The independent Siamese-encoding paradigm\ninduces the absence of token-level inter-sequence attention between chosen and\nrejected sequences. To address this \"attention hacking\", we propose\n\"Interaction Distillation\", a novel training framework for more adequate\npreference modeling through attention-level optimization. The method introduces\nan interaction-based natural language understanding model as the teacher to\nprovide sophisticated token interaction patterns via comprehensive attention,\nand guides the preference modeling to simulate teacher model's interaction\npattern through an attentional alignment objective. Through extensive\nexperiments, interaction distillation has demonstrated its ability to provide\nmore stable and generalizable reward signals compared to state-of-the-art RM\noptimization methods that target data noise, highlighting the attention hacking\nconstitute a more fundamental limitation in RM.", "AI": {"tldr": "This paper proposes \"Interaction Distillation\", a new training framework to improve preference modeling in reinforcement learning from human feedback for large language models, addressing limitations due to inadequate token-level interactions.", "motivation": "To improve the robustness of reward models in LLMs by addressing the vulnerabilities caused by mainstream preference modeling inadequacies.", "method": "The authors introduce \"Interaction Distillation\", which uses an interaction-based natural language understanding model as a teacher to optimize attention-level interactions during preference modeling.", "result": "Extensive experiments show that Interaction Distillation offers more stable and generalizable reward signals compared to existing reward model optimization methods that focus on data noise.", "conclusion": "The study concludes that attention hacking is a fundamental limitation in reward modeling, which can be mitigated through the proposed interaction-based training approach.", "key_contributions": ["Introduction of Interaction Distillation framework for preference modeling", "Optimization of attention-level interactions in reward models", "Demonstrated stability and generalizability of reward signals compared to existing methods"], "limitations": "", "keywords": ["reinforcement learning", "human feedback", "large language models", "preference modeling", "attention mechanisms"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2508.02631", "pdf": "https://arxiv.org/pdf/2508.02631.pdf", "abs": "https://arxiv.org/abs/2508.02631", "title": "Pointer: Linear-Complexity Long-Range Modeling without Pre-training", "authors": ["Zixi Li"], "categories": ["cs.CL"], "comment": "Submitted to Nordic AI Meet 2025", "summary": "We introduce Pointer, a novel architecture that achieves linear $O(NK)$\ncomplexity for long-range sequence modeling while maintaining superior\nperformance without requiring pre-training. Unlike standard attention\nmechanisms that compute $O(N^2)$ pairwise interactions, our approach uses\nlayer-wise pointer chaining where each layer's pointer selection depends on\nprevious layer's pointer positions, creating explicit long-distance connections\nthrough pointer chains. We demonstrate that this architecture achieves\n$2$--$10\\times$ speedup on long sequences compared to standard transformers,\nmaintains $>95\\%$ accuracy on copy tasks at distances up to 2048 tokens, and\nlearns interpretable pointer patterns that reveal structured dependency\nmodeling. Our experiments on efficiency benchmarks, long-range dependency\ntasks, and interpretability analysis show that Pointer offers a compelling\nalternative to attention mechanisms for scenarios requiring efficient\nlong-range modeling without pre-training dependencies.", "AI": {"tldr": "Pointer introduces an architecture for long-range sequence modeling with linear complexity.", "motivation": "Address the limitations of standard attention mechanisms for long sequences.", "method": "Layer-wise pointer chaining to create explicit long-distance connections without pairwise interactions.", "result": "Achieves $2$--$10\\times$ speedup on long sequences with over $95\\%$ accuracy on copy tasks up to 2048 tokens.", "conclusion": "Pointer is a promising alternative to traditional attention mechanisms for efficient long-range modeling.", "key_contributions": ["Linear $O(NK)$ complexity for long-range sequence modeling", "No requirement for pre-training", "Superior performance with interpretable pointer patterns"], "limitations": "", "keywords": ["Pointer architecture", "sequence modeling", "interpretability"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2508.02635", "pdf": "https://arxiv.org/pdf/2508.02635.pdf", "abs": "https://arxiv.org/abs/2508.02635", "title": "Test Set Quality in Multilingual LLM Evaluation", "authors": ["Kranti Chalamalasetti", "Gabriel Bernier-Colborne", "Yvan Gauthier", "Sowmya Vajjala"], "categories": ["cs.CL"], "comment": "Accepted at the 1st Workshop on Multilingual Data Quality Signals,\n  COLM 2025, Short paper. 10 pages in total", "summary": "Several multilingual benchmark datasets have been developed in a\nsemi-automatic manner in the recent past to measure progress and understand the\nstate-of-the-art in the multilingual capabilities of Large Language Models.\nHowever, there is not a lot of attention paid to the quality of the datasets\nthemselves, despite the existence of previous work in identifying errors in\neven fully human-annotated test sets. In this paper, we manually analyze recent\nmultilingual evaluation sets in two languages - French and Telugu, identifying\nseveral errors in the process. We compare the performance difference across\nseveral LLMs with the original and revised versions of the datasets and\nidentify large differences (almost 10% in some cases) in both languages). Based\non these results, we argue that test sets should not be considered immutable\nand should be revisited, checked for correctness, and potentially versioned. We\nend with some recommendations for both the dataset creators as well as\nconsumers on addressing the dataset quality issues.", "AI": {"tldr": "The paper analyzes multilingual benchmark datasets for LLMs, highlighting quality issues in French and Telugu datasets, and recommends revisiting test set quality.", "motivation": "To address the lack of attention on the quality of multilingual benchmark datasets used for evaluating Large Language Models.", "method": "Manual analysis of multilingual evaluation sets for French and Telugu, identifying errors and comparing the performance differences across several LLMs with original and revised datasets.", "result": "Identified substantial errors in multilingual datasets, leading to performance differences of almost 10% across LLMs for both languages.", "conclusion": "Test sets should be revisited for correctness and potentially versioned, with recommendations provided for dataset creators and consumers.", "key_contributions": ["Manual analysis of errors in multilingual datasets", "Performance comparison across LLMs with original vs revised datasets", "Recommendations for improving dataset quality"], "limitations": "", "keywords": ["Multilingual Datasets", "Large Language Models", "Dataset Quality"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2508.02075", "pdf": "https://arxiv.org/pdf/2508.02075.pdf", "abs": "https://arxiv.org/abs/2508.02075", "title": "Human Capital Visualization using Speech Amount during Meetings", "authors": ["Ekai Hashimoto", "Takeshi Mizumoto", "Kohei Nagira", "Shun Shiramatsu"], "categories": ["cs.HC", "cs.CL", "cs.CY"], "comment": "This paper has been accepted for presentation at the 26th Annual\n  Meeting of the Special Interest Group on Discourse and Dialogue(SIGDIAL\n  2025). It represents the author's version of the work", "summary": "In recent years, many companies have recognized the importance of human\nresources and are investing in human capital to revitalize their organizations\nand enhance internal communication, thereby fostering innovation. However,\nconventional quantification methods have mainly focused on readily measurable\nindicators without addressing the fundamental role of conversations in human\ncapital. This study focuses on routine meetings and proposes strategies to\nvisualize human capital by analyzing speech amount during these meetings. We\nemploy conversation visualization technology, which operates effectively, to\nquantify speech. We then measure differences in speech amount by attributes\nsuch as gender and job post, changes in speech amount depending on whether\ncertain participants are present, and correlations between speech amount and\ncontinuous attributes. To verify the effectiveness of our proposed methods, we\nanalyzed speech amounts by departmental affiliation during weekly meetings at\nsmall to medium enterprises.", "AI": {"tldr": "This study analyzes speech amounts during routine meetings to visualize human capital and address the role of conversations in organizations.", "motivation": "To revitalize organizations through better internal communication and foster innovation by quantifying human capital through conversation analysis.", "method": "The study employs conversation visualization technology to analyze speech amounts during routine meetings, focusing on differences by gender, job post, and participant presence.", "result": "The analysis reveals measurable differences in speech amounts related to departmental affiliation and other attributes, indicating the effectiveness of conversation as a metric for human capital.", "conclusion": "Understanding speech dynamics in meetings provides insights into organizational communication and highlights areas for improvement in human capital management.", "key_contributions": ["Proposed a novel method for quantifying human capital through conversation analysis.", "Visualized speech amounts in organizational settings to illustrate communication patterns.", "Provided empirical data and analysis regarding the impact of participant presence on speech dynamics."], "limitations": "The study is based on small to medium enterprises, which may limit generalizability to larger organizations.", "keywords": ["human capital", "conversation analysis", "speech visualization", "internal communication", "organizational innovation"], "importance_score": 3, "read_time_minutes": 15}}
{"id": "2508.02328", "pdf": "https://arxiv.org/pdf/2508.02328.pdf", "abs": "https://arxiv.org/abs/2508.02328", "title": "Understanding User Preferences for Interaction Styles in Conversational Recommender Systems: The Predictive Role of System Qualities, User Experience, and Traits", "authors": ["Raj Mahmud", "Shlomo Berkovsky", "Mukesh Prasad", "A. Baki Kocaballi"], "categories": ["cs.HC", "cs.CL", "cs.IR", "H.5.2; I.2.7; H.1.2"], "comment": "Accepted at OZCHI 2025. 21 pages, 9 figures, 8 tables", "summary": "Conversational Recommender Systems (CRSs) deliver personalised\nrecommendations through multi-turn natural language dialogue and increasingly\nsupport both task-oriented and exploratory interactions. Yet, the factors\nshaping user interaction preferences remain underexplored. In this\nwithin-subjects study (\\(N = 139\\)), participants experienced two scripted CRS\ndialogues, rated their experiences, and indicated the importance of eight\nsystem qualities. Logistic regression revealed that preference for the\nexploratory interaction was predicted by enjoyment, usefulness, novelty, and\nconversational quality. Unexpectedly, perceived effectiveness was also\nassociated with exploratory preference. Clustering uncovered five latent user\nprofiles with distinct dialogue style preferences. Moderation analyses\nindicated that age, gender, and control preference significantly influenced\nthese choices. These findings integrate affective, cognitive, and trait-level\npredictors into CRS user modelling and inform autonomy-sensitive,\nvalue-adaptive dialogue design. The proposed predictive and adaptive framework\napplies broadly to conversational AI systems seeking to align dynamically with\nevolving user needs.", "AI": {"tldr": "This study investigates user preferences in Conversational Recommender Systems (CRSs) and identifies factors that influence interaction preferences through a within-subjects analysis.", "motivation": "The paper addresses the limitations in understanding user interaction preferences within CRSs, aiming to enhance personalized recommendations through improved dialogue design.", "method": "A within-subjects study was conducted with 139 participants who experienced two scripted CRS dialogues, rated their experiences, and indicated the importance of eight system qualities, followed by logistic regression and clustering analyses for predictive user modeling.", "result": "Preference for exploratory interactions was significantly predicted by enjoyment, usefulness, novelty, conversational quality, and perceived effectiveness, with five latent user profiles identified based on distinct dialogue style preferences.", "conclusion": "The findings contribute to a comprehensive framework that integrates various predictors into CRS user modeling, facilitating the design of adaptive dialogue systems that can dynamically meet user needs.", "key_contributions": ["Identification of key factors influencing user preferences in CRSs", "Development of a predictive framework for user modeling in conversational AI", "Discovery of distinct user profiles based on dialogue style preferences"], "limitations": "Study conducted with a specific demographic, limiting generalizability; further research needed to validate findings across diverse populations.", "keywords": ["Conversational Recommender Systems", "User Preferences", "Dialogue Design", "User Modeling", "Adaptive Systems"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2508.02371", "pdf": "https://arxiv.org/pdf/2508.02371.pdf", "abs": "https://arxiv.org/abs/2508.02371", "title": "Six Guidelines for Trustworthy, Ethical and Responsible Automation Design", "authors": ["Matouš Jelínek", "Nadine Schlicker", "Ewart de Visser"], "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "Calibrated trust in automated systems (Lee and See 2004) is critical for\ntheir safe and seamless integration into society. Users should only rely on a\nsystem recommendation when it is actually correct and reject it when it is\nfactually wrong. One requirement to achieve this goal is an accurate\ntrustworthiness assessment, ensuring that the user's perception of the system's\ntrustworthiness aligns with its actual trustworthiness, allowing users to make\ninformed decisions about the extent to which they can rely on the system\n(Schlicker et al. 2022). We propose six design guidelines to help designers\noptimize for accurate trustworthiness assessments, thus fostering ethical and\nresponsible human-automation interactions. The proposed guidelines are derived\nfrom existing literature in various fields, such as human-computer interaction,\ncognitive psychology, automation research, user-experience design, and ethics.\nWe are incorporating key principles from the field of pragmatics, specifically\nthe cultivation of common ground (H. H. Clark 1996) and Gricean communication\nmaxims (Grice 1975). These principles are essential for the design of automated\nsystems because the user's perception of the system's trustworthiness is shaped\nby both environmental contexts, such as organizational culture or societal\nnorms, and by situational context, including the specific circumstances or\nscenarios in which the interaction occurs (Hoff and Bashir 2015). Our proposed\nguidelines provide actionable insights for designers to create automated\nsystems that make relevant trustworthiness cues available. This would ideally\nfoster calibrated trust and more satisfactory, productive, and safe\ninteractions between humans and automated systems. Furthermore, the proposed\nheuristics might work as a tool for evaluating to what extent existing systems\nenable users to accurately assess a system's trustworthiness.", "AI": {"tldr": "The paper proposes design guidelines for optimizing trustworthiness assessments in automated systems to ensure safe and effective human-automation interactions.", "motivation": "To achieve calibrated trust in automated systems, users must accurately perceive a system’s trustworthiness, ensuring they rely on correct recommendations and reject incorrect ones.", "method": "The paper derives six design guidelines for trustworthiness assessments from interdisciplinary literature, incorporating principles from pragmatics and communication.", "result": "The guidelines aim to provide actionable insights for designers, potentially improving user trust and interaction satisfaction with automated systems.", "conclusion": "The proposed guidelines can evaluate existing systems' capabilities in facilitating accurate trust assessments, fostering safer human-automation interactions.", "key_contributions": ["Six design guidelines for improving trustworthiness assessments of automated systems", "Incorporation of principles from pragmatics for better user-system communication", "Actionable insights for creating ethical and responsible human-automation interactions"], "limitations": "", "keywords": ["Trustworthiness", "Human-Automation Interaction", "Design Guidelines", "Pragmatics", "User Experience"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2508.02470", "pdf": "https://arxiv.org/pdf/2508.02470.pdf", "abs": "https://arxiv.org/abs/2508.02470", "title": "AIAP: A No-Code Workflow Builder for Non-Experts with Natural Language and Multi-Agent Collaboration", "authors": ["Hyunjn An", "Yongwon Kim", "Wonduk Seo", "Joonil Park", "Daye Kang", "Changhoon Oh", "Dokyun Kim", "Seunghyun Lee"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.MA", "cs.SE"], "comment": "14 pages, 6 figures", "summary": "While many tools are available for designing AI, non-experts still face\nchallenges in clearly expressing their intent and managing system complexity.\nWe introduce AIAP, a no-code platform that integrates natural language input\nwith visual workflows. AIAP leverages a coordinated multi-agent system to\ndecompose ambiguous user instructions into modular, actionable steps, hidden\nfrom users behind a unified interface. A user study involving 32 participants\nshowed that AIAP's AI-generated suggestions, modular workflows, and automatic\nidentification of data, actions, and context significantly improved\nparticipants' ability to develop services intuitively. These findings highlight\nthat natural language-based visual programming significantly reduces barriers\nand enhances user experience in AI service design.", "AI": {"tldr": "AIAP is a no-code platform that simplifies AI service design for non-experts by integrating natural language and visual workflows.", "motivation": "The challenges faced by non-experts in expressing intent and managing complexity in AI service design.", "method": "A no-code platform that uses a coordinated multi-agent system to break down user instructions into modular steps.", "result": "User study with 32 participants showed significant improvement in developing AI services intuitively with AIAP.", "conclusion": "Natural language-based visual programming lowers barriers and improves the user experience in AI service design.", "key_contributions": ["Introduction of AIAP, a no-code platform for AI service design.", "Integration of natural language input with visual workflows.", "Demonstrated effectiveness through user studies."], "limitations": "", "keywords": ["no-code platform", "AI service design", "natural language", "visual workflows", "user study"], "importance_score": 8, "read_time_minutes": 14}}
{"id": "2306.15933", "pdf": "https://arxiv.org/pdf/2306.15933.pdf", "abs": "https://arxiv.org/abs/2306.15933", "title": "You Can Generate It Again: Data-to-Text Generation with Verification and Correction Prompting", "authors": ["Xuan Ren", "Zeyu Zhang", "Lingqiao Liu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Small language models like T5 excel in generating high-quality text for\ndata-to-text tasks, offering adaptability and cost-efficiency compared to Large\nLanguage Models (LLMs). However, they frequently miss keywords, which is\nconsidered one of the most severe and common errors in this task. In this work,\nwe explore the potential of using feedback systems to enhance semantic fidelity\nin smaller language models for data-to-text generation tasks, through our\nVerification and Correction Prompting (VCP) approach. In the inference stage,\nour approach involves a multi-step process, including generation, verification,\nand regeneration stages. During the verification stage, we implement a simple\nrule to check for the presence of every keyword in the prediction. Recognizing\nthat this rule can be inaccurate, we have developed a carefully designed\ntraining procedure, which enabling the model to incorporate feedback from the\nerror-correcting prompt effectively, despite its potential inaccuracies. The\nVCP approach effectively reduces the Semantic Error Rate (SER) while\nmaintaining the text's quality.", "AI": {"tldr": "This paper proposes the Verification and Correction Prompting (VCP) approach to enhance semantic fidelity in small language models for data-to-text generation.", "motivation": "Small language models often fail to include important keywords in generated text, impacting the quality of data-to-text tasks. The paper aims to improve keyword inclusion and semantic fidelity in these models.", "method": "The VCP approach includes a multi-step process during inference: generation, verification (checking for keywords), and regeneration. The training procedure is designed to help the model incorporate feedback from erroneous predictions effectively.", "result": "The VCP approach successfully reduces the Semantic Error Rate (SER) while preserving the quality of the generated text.", "conclusion": "Implementing a feedback system in small language models can enhance their performance in data-to-text tasks, making them more competitive against larger models.", "key_contributions": ["Introduction of the Verification and Correction Prompting (VCP) method.", "Demonstrated reduction in Semantic Error Rate (SER) while maintaining text quality.", "Developed a training procedure that integrates feedback from keyword checks."], "limitations": "The verification rule may produce inaccuracies that could affect the training process and results.", "keywords": ["small language models", "data-to-text generation", "semantic fidelity", "verification", "keywords"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2402.10699", "pdf": "https://arxiv.org/pdf/2402.10699.pdf", "abs": "https://arxiv.org/abs/2402.10699", "title": "Thinker-DDM: Modeling Deliberation for Machine Translation with a Drift-Diffusion Process", "authors": ["Hongbin Na", "Zimu Wang", "Mieradilijiang Maimaiti", "Tong Chen", "Wei Wang", "Tao Shen", "Ling Chen"], "categories": ["cs.CL"], "comment": "Under review", "summary": "Large language models (LLMs) have demonstrated promising potential in various\ndownstream tasks, including machine translation. However, prior work on\nLLM-based machine translation has mainly focused on better utilizing training\ndata, demonstrations, or pre-defined and universal knowledge to improve\nperformance, with a lack of consideration of decision-making like human\ntranslators. In this paper, we incorporate Thinker with the Drift-Diffusion\nModel (Thinker-DDM) to address this issue. We then redefine the Drift-Diffusion\nprocess to emulate human translators' dynamic decision-making under constrained\nresources. We conduct extensive experiments under the high-resource,\nlow-resource, and commonsense translation settings using the WMT22 and CommonMT\ndatasets, in which Thinker-DDM outperforms baselines in the first two\nscenarios. We also perform additional analysis and evaluation on commonsense\ntranslation to illustrate the high effectiveness and efficacy of the proposed\nmethod.", "AI": {"tldr": "This paper integrates a Drift-Diffusion Model with large language models to improve decision-making in machine translation, outperforming existing methods in various resource settings.", "motivation": "To enhance LLM-based machine translation by incorporating human-like decision-making processes, which have been largely overlooked in prior research.", "method": "The authors introduce Thinker with Drift-Diffusion Model (Thinker-DDM) to emulate dynamic decision-making in translation, and conduct experiments across high-resource, low-resource, and commonsense translation scenarios using WMT22 and CommonMT datasets.", "result": "Thinker-DDM demonstrates superior performance compared to baseline models in high-resource and low-resource settings, with additional effectiveness shown in commonsense translation tasks.", "conclusion": "The incorporation of decision-making mechanisms inspired by human translators significantly improves machine translation outcomes across various resource constraints.", "key_contributions": ["Introduction of Thinker-DDM for machine translation", "Emulation of human-like decision-making in LLMs", "Demonstrated effectiveness across different translation resource scenarios."], "limitations": "", "keywords": ["Machine Translation", "Large Language Models", "Drift-Diffusion Model"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2405.17402", "pdf": "https://arxiv.org/pdf/2405.17402.pdf", "abs": "https://arxiv.org/abs/2405.17402", "title": "THREAD: Thinking Deeper with Recursive Spawning", "authors": ["Philip Schroeder", "Nathaniel Morgan", "Hongyin Luo", "James Glass"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown impressive capabilities across\ndiverse settings, but still struggle as the length and complexity of the\ncontext increases. To address this challenge, we propose Thinking Recursively\nand Dynamically (ThReaD). THREAD frames model generation as a thread of\nexecution that, based on the context, can run to completion or dynamically\nspawn new threads. By spawning, threads can offload work (e.g., thinking,\nretrieving information) to child threads, which only return tokens needed for\nthe parent thread to do its work. In effect, this enables the model to adapt,\nas needed, the amount of intermediate work used to produce tokens. We apply\nTHREAD in the settings of LLM task solving and question answering, where the\ndynamic threading allows the model to recursively decompose the given task or\nquestion into progressively simpler sub-problems that can be solved by separate\nchild threads. We test THREAD, implemented using a few-shot learning approach,\non diverse benchmarks for agent tasks and data-grounded question answering.\nTHREAD achieves state-of-the-art performance with GPT-4 and GPT-3.5 on these\nbenchmarks, including ALFWorld, TextCraft, and WebShop, along with two new\nbenchmarks, DataCommons QA and MIMIC-III ICU QA. In addition, THREAD\noutperforms existing frameworks by 10% to 50% absolute points with smaller\nmodels, including Llama-3-8b and CodeLlama-7b.", "AI": {"tldr": "Proposes a recursive and dynamic threading approach for large language models to improve performance on complex tasks.", "motivation": "To enhance language models' performance in handling longer and more complex contexts by allowing them to dynamically spawn threads to offload tasks.", "method": "Introduces Thinking Recursively and Dynamically (ThReaD), framing model generation as a thread of execution that can dynamically decompose tasks into sub-problems handled by child threads.", "result": "Achieved state-of-the-art performance with GPT-4 and GPT-3.5 across various benchmarks and improved performance by 10% to 50% with smaller models like Llama-3-8b and CodeLlama-7b.", "conclusion": "ThReaD enhances the capability of LLMs to solve complex tasks more effectively by decomposing them into simpler sub-tasks.", "key_contributions": ["Introduces a novel threading framework for LLMs", "Demonstrates superior performance on multiple benchmarks", "Implements few-shot learning for task solving and question answering."], "limitations": "", "keywords": ["large language models", "recursive threading", "task decomposition", "few-shot learning", "benchmark performance"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2406.11130", "pdf": "https://arxiv.org/pdf/2406.11130.pdf", "abs": "https://arxiv.org/abs/2406.11130", "title": "Dynamic Order Template Prediction for Generative Aspect-Based Sentiment Analysis", "authors": ["Yonghyun Jun", "Hwanhee Lee"], "categories": ["cs.CL"], "comment": "ACL 2025 Main", "summary": "Aspect-based sentiment analysis (ABSA) assesses sentiments towards specific\naspects within texts, resulting in detailed sentiment tuples. Previous ABSA\nmodels often use static templates to predict all of the elements in the tuples,\nand these models often fail to accurately capture dependencies between\nelements. Multi-view prompting method improves the performance of ABSA by\npredicting tuples with various templates and then ensembling the results.\nHowever, this method suffers from inefficiencies and out-of-distribution\nerrors. In this paper, we propose a Dynamic Order Template (DOT) method for\nABSA, which dynamically generates necessary views for each instance based on\ninstance-level entropy. Ensuring the diverse and relevant view generation, our\nproposed method improves F1-scores on ASQP and ACOS datasets while\nsignificantly reducing inference time.", "AI": {"tldr": "The paper introduces a Dynamic Order Template (DOT) method for aspect-based sentiment analysis (ABSA), improving efficiency and accuracy in sentiment tuple prediction.", "motivation": "Traditional ABSA models struggle with template staticity and element dependency capture, leading to inefficiencies and errors. There is a need for a flexible approach that can adapt to various instances of data.", "method": "The proposed DOT method dynamically generates multiple views for each instance based on its entropy, allowing for a tailored and efficient prediction process.", "result": "The DOT method improves F1-scores on ASQP and ACOS datasets and reduces the inference time compared to previous methods.", "conclusion": "By utilizing instance-level insights for view generation, the DOT method enhances the performance and efficiency of ABSA tasks, promising better application in real scenarios.", "key_contributions": ["Introduction of Dynamic Order Template for ABSA", "Improvement in F1-scores on benchmark datasets ASQP and ACOS", "Reduction in inference time for ABSA tasks"], "limitations": "", "keywords": ["Aspect-based sentiment analysis", "Dynamic Order Template", "Multi-view prompting", "F1-scores", "Inference time"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2406.12307", "pdf": "https://arxiv.org/pdf/2406.12307.pdf", "abs": "https://arxiv.org/abs/2406.12307", "title": "Can Tool-augmented Large Language Models be Aware of Incomplete Conditions?", "authors": ["Seungbin Yang", "ChaeHun Park", "Taehee Kim", "Jaegul Choo"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in integrating large language models (LLMs) with tools\nhave allowed the models to interact with real-world environments. However,\nthese tool-augmented LLMs often encounter incomplete scenarios when users\nprovide partial information or the necessary tools are unavailable. Recognizing\nand managing such scenarios is crucial for LLMs to ensure their reliability,\nbut this exploration remains understudied. This study examines whether LLMs can\nidentify incomplete conditions and appropriately determine when to refrain from\nusing tools. To quantitatively evaluate this capability, we construct a new\nbenchmark dataset where instances are systematically altered to simulate the\nambiguous and incomplete conditions common in real-world interactions. Our\nexperiments reveal that even state-of-the-art LLMs often struggle to identify\nthese conditions, attempting to use tools without sufficient information or\nwhen the correct tool is unavailable. To better understand these limitations,\nwe conduct a detailed behavioral analysis across various conditions, including\nimplicit evaluation and scenarios where models receive feedback from previous\ntool invocations. Based on this analysis, we propose a novel prompting-based\nreasoning strategy that explicitly instructs models to assess the sufficiency\nof information and the availability of tools. Our proposed approach\nsignificantly enhances the models' ability to recognize incomplete conditions,\nresulting in more informed and contextually appropriate tool-use decisions. We\nbelieve our research contributes to advancing the reliability of LLMs,\nespecially in real-world applications where incomplete or ambiguous information\nis common. Our dataset is available at\nhttps://huggingface.co/datasets/ddehun/ICT.", "AI": {"tldr": "This study investigates LLMs' ability to identify incomplete scenarios and proposes a new prompting strategy to enhance their reliability in decision-making.", "motivation": "To explore the challenges LLMs face when dealing with incomplete information and to improve their reliability in tool use in real-world situations.", "method": "A benchmark dataset was created to simulate ambiguous and incomplete conditions, followed by experiments to evaluate the LLMs' ability to recognize these scenarios and analyze their behavior.", "result": "State-of-the-art LLMs struggle with identifying incomplete conditions and often misuse tools without sufficient context. The new prompting strategy significantly improves their performance in assessing information adequacy.", "conclusion": "Enhancing LLMs' decision-making in ambiguous contexts is critical for their reliability, especially in applications where users provide partial information.", "key_contributions": ["Creation of a benchmark dataset for evaluating incomplete condition recognition in LLMs", "Development of a prompting-based reasoning strategy to improve tool-use decision-making", "Insights into the behavioral flaws of LLMs when encountering ambiguous scenarios"], "limitations": "The study is limited to certain types of incomplete conditions and may not cover all real-world scenarios.", "keywords": ["large language models", "tool-augmented LLMs", "incomplete information", "benchmark dataset", "prompting strategy"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2406.16306", "pdf": "https://arxiv.org/pdf/2406.16306.pdf", "abs": "https://arxiv.org/abs/2406.16306", "title": "Cascade Reward Sampling for Efficient Decoding-Time Alignment", "authors": ["Bolian Li", "Yifan Wang", "Anamika Lochab", "Ananth Grama", "Ruqi Zhang"], "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": null, "summary": "Aligning large language models (LLMs) with human preferences is essential for\ntheir applications. Recently, decoding-time alignment has emerged as an\neffective plug-and-play technique that avoids fine-tuning model parameters.\nThis approach retains the general utility of pretrained LLMs but often suffers\nfrom significant inefficiencies during decoding, primarily due to wasted token\ngeneration and excessive reward evaluations. To address these challenges, we\nintroduce Cascade Reward Sampling (CARDS) to resolve both efficiency\nbottlenecks in decoding-time alignment. Specifically, we develop a\nsegment-level rejection sampling algorithm that minimizes redundant\ncomputations of both LLMs and reward models (RMs). Central to CARDS is an\nuncertainty-based segmentation mechanism, which ensures the accuracy of RMs\nevaluations on incomplete segments. Furthermore, we provide a detailed analysis\nof reward scores on segments to elucidate the improved alignment performance.\nExperimental results demonstrate that CARDS significantly improves decoding\nefficiency, alignment quality, and general utility compared to existing\ndecoding-time alignment methods, achieving approximately a 70% reduction in\ndecoding time and over 90% win-ties in utility and safety benchmarks.", "AI": {"tldr": "The paper introduces Cascade Reward Sampling (CARDS) to enhance the efficiency of decoding-time alignment in large language models (LLMs) while maintaining alignment quality.", "motivation": "Aligning LLMs with human preferences is crucial, but existing decoding-time alignment techniques are inefficient, leading to wasted computations and prolonged decoding times.", "method": "The CARDS approach employs a segment-level rejection sampling algorithm that minimizes redundant evaluations of LLMs and reward models (RMs) through an uncertainty-based segmentation mechanism.", "result": "CARDS achieves approximately a 70% reduction in decoding time and over 90% improvement in alignment utility and safety benchmarks compared to existing methods.", "conclusion": "The results indicate that CARDS significantly enhances both decoding efficiency and alignment quality without sacrificing the general utility of pretrained LLMs.", "key_contributions": ["Introduction of the Cascade Reward Sampling (CARDS) technique.", "Development of an uncertainty-based segmentation mechanism for efficiency.", "Demonstration of significant improvements in decoding time and alignment quality."], "limitations": "", "keywords": ["large language models", "decoding-time alignment", "reward models"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2408.05456", "pdf": "https://arxiv.org/pdf/2408.05456.pdf", "abs": "https://arxiv.org/abs/2408.05456", "title": "Path-LLM: A Shortest-Path-based LLM Learning for Unified Graph Representation", "authors": ["Wenbo Shang", "Xuliang Zhu", "Xin Huang"], "categories": ["cs.CL"], "comment": "15 pages, 12 figures", "summary": "Unified graph representation learning aims to generate node embeddings, which\ncan be applied to multiple downstream applications of graph analytics. However,\nexisting studies based on graph neural networks and language models either\nsuffer from the limitations of numerous training needs toward specific\ndownstream predictions, poor generalization, or shallow semantic features. In\nthis work, we propose a novel Path-LLM model to efficiently learn unified graph\nrepresentation, which leverages a powerful large language model (LLM) to\nincorporate our proposed path features. Our Path-LLM framework consists of four\nwell-designed techniques. First, we develop a new mechanism of long-to-short\nshortest path (L2SP) selection, which can cover key connections between\ndifferent dense groups. An in-depth analysis and comparison of different path\nselections is conducted to justify the rationale behind our designed L2SP\nmethod. Next, we design path textualization to obtain L2SP-based training texts\nwith key phrase selection from node text attributes. We then feed the texts\ninto a self-supervised LLM training process to align next node/edge generation\nin L2SP with next token generation in causal language modeling for graph\nrepresentation learning and finally extract the unified graph embeddings. We\ntheoretically analyze the algorithm complexity of our Path-LLM approach.\nExtensive experiments on large-scale graph benchmarks validate the superiority\nof Path-LLM against state-of-the-art methods WalkLM, GraphGPT, OFA, and\nGraphTranslator on two classical graph learning tasks (node classification and\nedge validation) and one NP-hard graph query processing task (keyword search).\nCompared with WalkLM, our approach saves more than 90% of training paths on\nmillions-scale graphs and runs at most 35x faster.", "AI": {"tldr": "This paper introduces Path-LLM, a novel model for unified graph representation learning that leverages large language models and innovative path features to improve efficiency and accuracy in graph analytics tasks.", "motivation": "Existing methods for graph representation learning face challenges such as excessive training requirements, poor generalization capabilities, and shallow semantic understanding, prompting the need for a more efficient solution.", "method": "The Path-LLM framework introduces four techniques including the long-to-short shortest path (L2SP) mechanism for efficient path selection, path textualization for generating training texts, and integration with self-supervised LLM training to align graph generation with causal language modeling.", "result": "Experiments demonstrate that Path-LLM outperforms state-of-the-art methods like WalkLM and GraphGPT on node classification, edge validation, and keyword search tasks, achieving speed improvements of up to 35x and reducing training paths by over 90%.", "conclusion": "Path-LLM effectively enhances the process of generating unified graph embeddings, overcoming many limitations of previous models while demonstrating significant performance improvements on various graph tasks.", "key_contributions": ["Development of the L2SP selection mechanism for path efficiency", "Introduction of path textualization for effective LLM training", "Validation of Path-LLM's superiority through extensive benchmarking against leading models"], "limitations": "", "keywords": ["graph representation learning", "large language model", "Path-LLM", "HCI", "machine learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2411.01281", "pdf": "https://arxiv.org/pdf/2411.01281.pdf", "abs": "https://arxiv.org/abs/2411.01281", "title": "Arena-Lite: Efficient and Reliable Large Language Model Evaluation via Tournament-Based Direct Comparisons", "authors": ["Seonil Son", "Ju-Min Oh", "Heegon Jin", "Cheolhun Jang", "Jeongbeom Jeong", "Kuntae Kim"], "categories": ["cs.CL", "cs.AI"], "comment": "8 pages for main body, 19 pages in total", "summary": "As Large Language Models (LLMs) expand across domains, LLM judges have become\nessential for systems evaluation. Current benchmarks typically compare system\noutputs against baselines. This baseline-mediated approach, though convenient,\nyields lower reliability than direct comparison between systems. We propose\nArena-Lite which integrates tournament structure on top of head-to-head\ncomparison. The application of a tournament structure and direct comparison\neliminates the need for baseline outputs, reduces the number of required\ncomparisons, and allows higher reliability in system rankings. We conducted two\nexperiments: (1) controlled stochastic modeling and (2) empirical validation\nwith a real LLM judge. Those experiments collectively demonstrate that\nArena-Lite consistently achieves higher reliability with fewer comparisons,\neven with smaller datasets or weaker judges. We release an easy-to-use web\ndemonstration and code to foster adoption of Arena-Lite, streamlining model\nselection across research and industry communities. Arena-Lite demo and code\nare available on\n\\href{https://huggingface.co/spaces/NCSOFT/ArenaLite}{https://huggingface.co/spaces/NCSOFT/ArenaLite}", "AI": {"tldr": "Arena-Lite is a novel evaluation framework for Large Language Models (LLMs) that employs a tournament structure for head-to-head comparisons, enhancing system ranking reliability and reducing required comparisons.", "motivation": "To improve the reliability of system evaluations for LLMs by moving beyond baseline-mediated comparisons, which can be less reliable.", "method": "Arena-Lite integrates a tournament structure on top of direct head-to-head comparisons between systems, allowing evaluations without needing baseline outputs.", "result": "Experiments show that Arena-Lite achieves consistently higher reliability with fewer comparisons, making it suitable even with smaller datasets or less robust judges.", "conclusion": "The introduction of Arena-Lite offers a more reliable and efficient evaluation method for LLMs, with resources made available for wider adoption.", "key_contributions": ["Proposed a tournament structure for LLM evaluation that eliminates baseline reliance.", "Demonstrated higher evaluation reliability with fewer comparisons in experiments.", "Released a web demonstration and code to encourage practical adoption of Arena-Lite."], "limitations": "The approach may still be limited by the quality of judges and the datasets used for comparison.", "keywords": ["Large Language Models", "evaluation", "tournament structure", "Arena-Lite", "reliability"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2411.18104", "pdf": "https://arxiv.org/pdf/2411.18104.pdf", "abs": "https://arxiv.org/abs/2411.18104", "title": "Training and Evaluating Language Models with Template-based Data Generation", "authors": ["Yifan Zhang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Published in ICLR 2025 DATA-FM Workshop", "summary": "The rapid advancement of large language models (LLMs) such as GPT-3, PaLM,\nand Llama has significantly transformed natural language processing, showcasing\nremarkable capabilities in understanding and generating language. However, a\nfundamental bottleneck persists: these models often struggle with tasks\nrequiring complex, multi-step reasoning, particularly in mathematical\nproblem-solving. This deficiency stems from the critical scarcity of\nlarge-scale, high-quality, domain-specific datasets necessary for cultivating\nsophisticated reasoning abilities. To overcome this challenge, we introduce\nTemplate-based Data Generation (TDG), a novel and scalable paradigm that\nharnesses frontier LLMs (GPT-4) to automatically generate parameterized\nmeta-templates, which in turn synthesize a virtually infinite stream of\nhigh-quality problems and solutions. Using this paradigm, we create\nTemplateMath Part I: TemplateGSM, a foundational dataset of over 7 million\nsynthetically generated grade school math problems. Each problem is accompanied\nby a programmatically verifiable solution, offering an unprecedented level of\nquality at scale. This resource not only resolves the data scarcity issue for\nsupervised fine-tuning but also provides a robust mechanism for model alignment\nthrough Reinforcement Learning with Verifiable Rewards (RLVR). Our approach\nelevates data augmentation by employing GPT-4 for meta-template creation,\nguaranteeing diverse and complex problem structures. By providing a scalable\nsolution to the data and verification bottleneck, TDG and TemplateGSM pave the\nway for a new generation of LLMs with powerful, reliable reasoning skills. The\ncode and data are available at https://github.com/iiis-ai/TemplateMath.", "AI": {"tldr": "The paper introduces Template-based Data Generation (TDG) for creating high-quality math problem datasets to improve reasoning in large language models (LLMs).", "motivation": "To address the issue of data scarcity that hinders complex reasoning capabilities in LLMs, particularly for mathematical problem-solving.", "method": "TDG harnesses GPT-4 to automatically generate parameterized meta-templates, enabling the creation of millions of synthetic math problems with verifiable solutions.", "result": "The creation of TemplateMath Part I: TemplateGSM, a dataset comprising over 7 million generated grade school math problems, significantly enhances the quality and scale of training data.", "conclusion": "TDG and TemplateGSM offer a scalable solution to data scarcity and verification for improving the reasoning abilities of LLMs, providing resources for model alignment and supervised fine-tuning.", "key_contributions": ["Introduction of Template-based Data Generation (TDG) for dataset creation", "Development of TemplateMath Part I: TemplateGSM with 7 million math problems", "Verifiable solutions enabling reliable model alignment through RLVR."], "limitations": "", "keywords": ["Large Language Models", "Data Generation", "Math Problems", "Template-based Approach", "Reinforcement Learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2412.02252", "pdf": "https://arxiv.org/pdf/2412.02252.pdf", "abs": "https://arxiv.org/abs/2412.02252", "title": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity", "authors": ["Da Ma", "Lu Chen", "Situo Zhang", "Yuxun Miao", "Su Zhu", "Zhi Chen", "Hongshen Xu", "Hanqi Li", "Shuai Fan", "Lei Pan", "Kai Yu"], "categories": ["cs.CL"], "comment": "14 pages, 7 figures, 7 tables", "summary": "The rapid expansion of context window sizes in Large Language Models~(LLMs)\nhas enabled them to tackle increasingly complex tasks involving lengthy\ndocuments. However, this progress comes at the cost of a substantial increase\nin memory usage during inference, primarily due to the linear growth of the\nkey-value~(KV) cache. Existing KV cache compression methods often discard less\nrelevant tokens, which can lead to significant performance degradation when\ncritical information is lost. In this paper, we propose \\textsc{PoD}~(Proximal\ntokens over Distant tokens), a novel KV cache compression framework that\nallocates memory according to token importance, retaining less important tokens\nin a more compact, shared form rather than discarding them entirely. Our\napproach is motivated by two key observations: (1) proximal tokens -- those at\nthe beginning and end of the context -- are significantly more important for\nnext-token prediction, and (2) attention scores for distant tokens are highly\nredundant across consecutive layers. Leveraging these insights, \\textsc{PoD}\npreserves the full KV cache for proximal tokens, while for distant tokens, it\nshares key states across layers. Since attention scores are determined by both\nqueries and keys, sharing key states enables multiple layers to reuse a single\nset of keys for distant tokens, substantially reducing KV cache memory without\ndiscarding essential context. We further introduce a lightweight post-training\nadaptation to enable the model to adjust to this new attention-sharing\nstructure. Extensive experiments on both synthetic~(Needle in a Haystack) and\nreal-world long-context benchmarks demonstrate that \\textsc{PoD} reduces KV\ncache memory usage by up to 35\\% without compromising performance. Our method\nis orthogonal to existing token-selection-based techniques and can be combined\nwith them for further KV cache compression.", "AI": {"tldr": "Proximal tokens over Distant tokens (PoD) is a KV cache compression framework that reduces memory usage by addressing the importance of tokens during inference in Large Language Models (LLMs).", "motivation": "The increasing memory usage in LLMs due to larger context windows and existing compression methods that may discard critical information motivated the development of PoD.", "method": "PoD allocates memory based on token importance by retaining proximal tokens fully and compactly storing distant tokens, sharing key states across layers.", "result": "PoD reduces KV cache memory usage by up to 35% while maintaining performance in long-context tasks based on experimental validation.", "conclusion": "PoD offers a novel approach to KV cache compression in LLMs that can work alongside existing selection methods to further optimize memory usage.", "key_contributions": ["Introduction of PoD, a novel KV cache compression framework.", "Preservation of proximal tokens and efficient sharing of distant token states across layers.", "Demonstrated significant memory reduction without performance loss on various benchmarks."], "limitations": "", "keywords": ["Large Language Models", "KV cache", "memory compression", "attention mechanisms", "contextual tokens"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2501.05727", "pdf": "https://arxiv.org/pdf/2501.05727.pdf", "abs": "https://arxiv.org/abs/2501.05727", "title": "Self-Evolving Critique Abilities in Large Language Models", "authors": ["Zhengyang Tang", "Ziniu Li", "Zhenyang Xiao", "Tian Ding", "Ruoyu Sun", "Benyou Wang", "Dayiheng Liu", "Fei Huang", "Tianyu Liu", "Bowen Yu", "Junyang Lin"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by COLM 2025", "summary": "Despite their remarkable performance, Large Language Models (LLMs) face a\ncritical challenge: providing feedback for tasks where human evaluation is\ndifficult or where LLMs potentially outperform humans. In such scenarios,\nleveraging the critique ability of LLMs themselves - identifying and correcting\nflaws - shows considerable promise. This paper explores enhancing critique\nabilities of LLMs, noting that current approaches rely on human annotations or\nmore powerful models, leaving the challenge of improving critique abilities\nwithout external supervision unresolved. We introduce SCRIT (Self-evolving\nCRITic), a framework that trains LLMs with self-generated data to evolve their\ncritique abilities. To address the low quality of naively generated data, we\npropose a contrastive-critic approach that uses reference solutions during data\nsynthesis to enhance the model's understanding of key concepts, and\nincorporates a self-validation scheme to ensure data quality. The final trained\nmodel operates without any reference solutions at inference time. Implemented\nwith Qwen2.5-72B-Instruct, a leading LLM, SCRIT demonstrates consistent\nimprovements across a wide range of benchmarks spanning both mathematical and\nscientific reasoning: achieving a 10.0\\% relative gain in critique-correction\naccuracy and a 19.0\\% relative improvement in error identification F1-score.\nOur analysis reveals that SCRIT's performance scales positively with data and\nmodel size and enables continuous improvement through multi-round iterations.", "AI": {"tldr": "This paper introduces SCRIT, a framework that enhances the critique abilities of Large Language Models (LLMs) using self-generated data and a contrastive-critic approach to improve quality and ensure continuous improvement.", "motivation": "The need for better feedback mechanisms in LLMs for tasks where human evaluation is challenging, and to allow LLMs to identify and correct their own flaws without external supervision.", "method": "The SCRIT framework trains LLMs with self-generated data, employing a contrastive-critic approach to improve understanding and a self-validation scheme for data quality, culminating in a model that performs inference without reference solutions.", "result": "SCRIT shows significant performance improvements: 10.0% relative gain in critique-correction accuracy and a 19.0% improvement in error identification F1-score across diverse benchmarks.", "conclusion": "SCRIT enables LLMs to evolve their critique abilities independently, scaling positively with data and model size and supporting continuous performance enhancements through iterative training.", "key_contributions": ["Introduction of SCRIT framework for self-evolving critique capabilities", "Implementation of contrastive-critic approach for better data quality", "Demonstrated improvements in both mathematical and scientific reasoning tasks."], "limitations": "", "keywords": ["Large Language Models", "critique abilities", "self-generated data", "contrasting-critic approach", "self-validation scheme"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.13207", "pdf": "https://arxiv.org/pdf/2502.13207.pdf", "abs": "https://arxiv.org/abs/2502.13207", "title": "Thinking Outside the (Gray) Box: A Context-Based Score for Assessing Value and Originality in Neural Text Generation", "authors": ["Giorgio Franceschelli", "Mirco Musolesi"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": null, "summary": "Despite the increasing use of large language models for creative tasks, their\noutputs often lack diversity. Common solutions, such as sampling at higher\ntemperatures, can compromise the quality of the results. Dealing with this\ntrade-off is still an open challenge in designing AI systems for creativity.\nDrawing on information theory, we propose a context-based score to\nquantitatively evaluate value and originality. This score incentivizes accuracy\nand adherence to the request while fostering divergence from the learned\ndistribution. We show that our score can be used as a reward in a reinforcement\nlearning framework to fine-tune large language models for maximum performance.\nWe validate our strategy through experiments considering a variety of creative\ntasks, such as poetry generation and math problem solving, demonstrating that\nit enhances the value and originality of the generated solutions.", "AI": {"tldr": "This paper proposes a context-based score for evaluating the originality and value of outputs from large language models (LLMs) in creative tasks, addressing the trade-off between diversity and quality in AI-generated content.", "motivation": "The motivation behind this research is to tackle the challenge of generating diverse and high-quality outputs from large language models when applied to creative tasks.", "method": "The authors propose a context-based score rooted in information theory, which is used as a reward in a reinforcement learning framework to fine-tune LLMs.", "result": "Experiments demonstrate that using the proposed score enhances the value and originality of outputs in various creative tasks, including poetry generation and math problem solving.", "conclusion": "The study concludes that the context-based scoring mechanism effectively improves the performance of LLMs by encouraging diversity without sacrificing quality.", "key_contributions": ["Introduction of a context-based score for evaluating LLM outputs in creativity tasks", "Integration of this score into a reinforcement learning framework", "Empirical validation across multiple creative tasks"], "limitations": "", "keywords": ["large language models", "creativity", "information theory", "reinforcement learning", "output evaluation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.13422", "pdf": "https://arxiv.org/pdf/2502.13422.pdf", "abs": "https://arxiv.org/abs/2502.13422", "title": "Towards Question Answering over Large Semi-structured Tables", "authors": ["Yuxiang Wang", "Junhao Gan", "Jianzhong Qi"], "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": null, "summary": "Table Question Answering (TableQA) attracts strong interests due to the\nprevalence of web information presented in the form of semi-structured tables.\nDespite many efforts, TableQA over large tables remains an open challenge. This\nis because large tables may overwhelm models that try to comprehend them in\nfull to locate question answers. Recent studies reduce input table size by\ndecomposing tables into smaller, question-relevant sub-tables via generating\nprograms to parse the tables. However, such solutions are subject to program\ngeneration and execution errors and are difficult to ensure decomposition\nquality. To address this issue, we propose TaDRe, a TableQA model that\nincorporates both pre- and post-table decomposition refinements to ensure table\ndecomposition quality, hence achieving highly accurate TableQA results. To\nevaluate TaDRe, we construct two new large-table TableQA benchmarks via\nLLM-driven table expansion and QA pair generation. Extensive experiments on\nboth the new and public benchmarks show that TaDRe achieves state-of-the-art\nperformance on large-table TableQA tasks.", "AI": {"tldr": "TaDRe is a Table Question Answering model that improves analysis of large tables through enhanced decomposition techniques.", "motivation": "The prevalence of web information in semi-structured tables and the challenges of extracting accurate answers from large tables.", "method": "TaDRe integrates pre- and post-table decomposition refinements to enhance the quality of table decomposition for improved QA accuracy.", "result": "TaDRe demonstrates state-of-the-art performance on large-table TableQA tasks through extensive experiments on newly constructed benchmarks.", "conclusion": "TaDRe offers an effective solution to enhance the decomposing of large tables, leading to better question answering results.", "key_contributions": ["Introduction of pre- and post-decomposition refinements in TableQA", "Construction of new large-table TableQA benchmarks", "Achievement of state-of-the-art performance in TableQA tasks"], "limitations": "", "keywords": ["Table Question Answering", "large tables", "decomposition", "LLM", "QA benchmarks"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2502.15851", "pdf": "https://arxiv.org/pdf/2502.15851.pdf", "abs": "https://arxiv.org/abs/2502.15851", "title": "Control Illusion: The Failure of Instruction Hierarchies in Large Language Models", "authors": ["Yilin Geng", "Haonan Li", "Honglin Mu", "Xudong Han", "Timothy Baldwin", "Omri Abend", "Eduard Hovy", "Lea Frermann"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly deployed with hierarchical\ninstruction schemes, where certain instructions (e.g., system-level directives)\nare expected to take precedence over others (e.g., user messages). Yet, we lack\na systematic understanding of how effectively these hierarchical control\nmechanisms work. We introduce a systematic evaluation framework based on\nconstraint prioritization to assess how well LLMs enforce instruction\nhierarchies. Our experiments across six state-of-the-art LLMs reveal that\nmodels struggle with consistent instruction prioritization, even for simple\nformatting conflicts. We find that the widely-adopted system/user prompt\nseparation fails to establish a reliable instruction hierarchy, and models\nexhibit strong inherent biases toward certain constraint types regardless of\ntheir priority designation. We find that LLMs more reliably obey constraints\nframed through natural social hierarchies (e.g., authority, expertise,\nconsensus) than system/user roles, which suggests that pretraining-derived\nsocial structures act as latent control priors, with potentially stronger\ninfluence than post-training guardrails.", "AI": {"tldr": "This paper evaluates how hierarchical instructions are prioritized in large language models (LLMs) and finds models struggle with consistent prioritization, primarily influenced by inherent biases and social hierarchies.", "motivation": "To understand how effectively LLMs handle hierarchical instruction schemes, particularly in enforcing instruction prioritization.", "method": "A systematic evaluation framework based on constraint prioritization was introduced to assess the instruction hierarchy enforcement of six state-of-the-art LLMs.", "result": "Experiments showed that LLMs struggle with consistent instruction prioritization, with biases towards certain constraint types; the common system/user prompt separation is ineffective in establishing a reliable hierarchy.", "conclusion": "LLMs are more likely to obey constraints framed through natural social hierarchies than through system/user roles, indicating that pretraining social structures influence model behavior more than post-training adjustments.", "key_contributions": ["Introduction of a systematic evaluation framework for LLM instruction hierarchies.", "Findings on the inconsistency of instruction prioritization in LLMs.", "Insights into the influence of social hierarchies over system/user roles in LLMs."], "limitations": "The study is limited to six state-of-the-art LLMs and may not generalize to all LLM architectures or instruction formats.", "keywords": ["large language models", "instruction hierarchy", "constraint prioritization", "social hierarchies", "model bias"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.18583", "pdf": "https://arxiv.org/pdf/2502.18583.pdf", "abs": "https://arxiv.org/abs/2502.18583", "title": "What are Foundation Models Cooking in the Post-Soviet World?", "authors": ["Anton Lavrouk", "Tarek Naous", "Alan Ritter", "Wei Xu"], "categories": ["cs.CL"], "comment": null, "summary": "The culture of the Post-Soviet states is complex, shaped by a turbulent\nhistory that continues to influence current events. In this study, we\ninvestigate the Post-Soviet cultural food knowledge of foundation models by\nconstructing BORSch, a multimodal dataset encompassing 1147 and 823 dishes in\nthe Russian and Ukrainian languages, centered around the Post-Soviet region. We\ndemonstrate that leading models struggle to correctly identify the origins of\ndishes from Post-Soviet nations in both text-only and multimodal Question\nAnswering (QA), instead over-predicting countries linked to the language the\nquestion is asked in. Through analysis of pretraining data, we show that these\nresults can be explained by misleading dish-origin co-occurrences, along with\nlinguistic phenomena such as Russian-Ukrainian code mixing. Finally, to move\nbeyond QA-based assessments, we test models' abilities to produce accurate\nvisual descriptions of dishes. The weak correlation between this task and QA\nsuggests that QA alone may be insufficient as an evaluation of cultural\nunderstanding. To foster further research, we will make BORSch publicly\navailable at https://github.com/alavrouk/BORSch.", "AI": {"tldr": "This study investigates Post-Soviet cultural food knowledge of foundation models using a multimodal dataset called BORSch to assess origin identification of dishes.", "motivation": "To analyze how well foundation models can understand and identify the origins of dishes from Post-Soviet cultures, which reflects broader cultural knowledge.", "method": "A multimodal dataset (BORSch) was constructed with 1147 Russian and 823 Ukrainian dishes. The performance of leading models was evaluated in text-only and multimodal Question Answering tasks, as well as in generating visual descriptions.", "result": "Models exhibited poor identification of dish origins, often mispredicting based on the question's language. Analysis of pretraining data suggested misleading co-occurrences and linguistic features were factors in these poor performances.", "conclusion": "QA may not be a sufficient sole measure for evaluating models' cultural understanding, indicating a need for more robust methods, such as visual description tasks.", "key_contributions": ["Introduction of the BORSch dataset for Post-Soviet food knowledge", "Demonstration of leading models' shortcomings in cultural origin identification", "Proposing visual description as a complement to QA for evaluating cultural understanding"], "limitations": "The study primarily focuses on the Russian and Ukrainian languages and may not represent the full diversity of Post-Soviet cultures.", "keywords": ["Post-Soviet", "food knowledge", "multimodal dataset", "cultural understanding", "foundation models"], "importance_score": 3, "read_time_minutes": 12}}
{"id": "2503.00808", "pdf": "https://arxiv.org/pdf/2503.00808.pdf", "abs": "https://arxiv.org/abs/2503.00808", "title": "Predictive Data Selection: The Data That Predicts Is the Data That Teaches", "authors": ["Kashun Shum", "Yuzhen Huang", "Hongjian Zou", "Qi Ding", "Yixuan Liao", "Xiaoxin Chen", "Qian Liu", "Junxian He"], "categories": ["cs.CL"], "comment": "ICML 2025", "summary": "Language model pretraining involves training on extensive corpora, where data\nquality plays a pivotal role. In this work, we aim to directly estimate the\ncontribution of data during pretraining and select pretraining data in an\nefficient manner. Specifically, we draw inspiration from recent findings\nshowing that compression efficiency (i.e., the normalized loss) of diverse\nmodels on certain text correlates strongly with their downstream performance,\nwhen the text domain aligns with the downstream benchmarks(Huang et al., 2024).\nBuilding on this observation, we hypothesize that data on which model losses\nare predictive of downstream abilities also contribute effectively to learning,\nwhich shares similar intuition with Thrush et al.(2024). To leverage this\ninsight, we introduce predictive data selection (PreSelect), a lightweight and\nefficient data selection method that requires training and deploying only a\nfastText-based scorer. Through comprehensive experiments with 1B and 3B\nparameter models, we demonstrate that models trained on 30B tokens selected\nwith PreSelect surpass the performance of the vanilla baseline trained on 300B\ntokens, achieving a 10x reduction in compute requirements. Furthermore,\nPreSelect significantly outperforms other competitive data selection baselines,\nsuch as DCLM and FineWeb-Edu on a scale of 3B models trained on 100B tokens. We\nopen-source our trained data selection scorer along with the curated datasets\nat https://github.com/hkust-nlp/PreSelect.", "AI": {"tldr": "This paper introduces PreSelect, a method for selecting pretraining data based on its predictive power for downstream performance, achieving significant reductions in compute requirements.", "motivation": "To estimate the contribution of data during language model pretraining and improve data selection efficiency.", "method": "The authors introduce PreSelect, a fastText-based scorer for predictive data selection, tested on language models with varying parameters.", "result": "Models trained with PreSelect outperform baselines, achieving better performance with 10x less compute, indicating efficient data contributes effectively to learning.", "conclusion": "PreSelect is an effective method for data selection in pretraining, providing significant improvements in performance and efficiency.", "key_contributions": ["Introduction of PreSelect for efficient data selection", "Demonstration of significant compute reduction", "Open-sourcing of data selection scorer and datasets"], "limitations": "", "keywords": ["data selection", "language models", "pretraining", "machine learning", "computational efficiency"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2504.03165", "pdf": "https://arxiv.org/pdf/2504.03165.pdf", "abs": "https://arxiv.org/abs/2504.03165", "title": "Efficient Dynamic Clustering-Based Document Compression for Retrieval-Augmented-Generation", "authors": ["Weitao Li", "Kaiming Liu", "Xiangyu Zhang", "Xuanyu Lei", "Weizhi Ma", "Yang Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as a widely adopted approach\nfor knowledge injection during large language model (LLM) inference in recent\nyears. However, due to their limited ability to exploit fine-grained\ninter-document relationships, current RAG implementations face challenges in\neffectively addressing the retrieved noise and redundancy content, which may\ncause error in the generation results. To address these limitations, we propose\nan Efficient Dynamic Clustering-based document Compression framework (EDC2-RAG)\nthat utilizes latent inter-document relationships while simultaneously removing\nirrelevant information and redundant content. We validate our approach, built\nupon GPT-3.5-Turbo and GPT-4o-mini, on widely used knowledge-QA and\nHallucination-Detection datasets. Experimental results show that our method\nachieves consistent performance improvements across various scenarios and\nexperimental settings, demonstrating strong robustness and applicability. Our\ncode and datasets are available at https://github.com/Tsinghua-dhy/EDC-2-RAG.", "AI": {"tldr": "Proposes EDC2-RAG, a framework to enhance RAG by leveraging inter-document relationships for improved document compression and reducing noise in LLM generations.", "motivation": "To overcome the limitations of current RAG implementations that struggle with noise and redundancy affecting generation quality.", "method": "Introduces an Efficient Dynamic Clustering-based document Compression framework (EDC2-RAG) that uses latent inter-document relationships to filter out irrelevant and redundant information.", "result": "Achieves consistent performance improvements on knowledge-QA and Hallucination-Detection datasets, indicating strong robustness and applicability.", "conclusion": "The proposed EDC2-RAG framework shows significant advancements in handling noise and enhancing generation accuracy in LLMs during inference.", "key_contributions": ["Development of the EDC2-RAG framework", "Utilization of latent inter-document relationships for compression", "Empirical validation through diverse datasets"], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Document Compression", "Knowledge Injection"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2504.07360", "pdf": "https://arxiv.org/pdf/2504.07360.pdf", "abs": "https://arxiv.org/abs/2504.07360", "title": "Enhancing Time Series Forecasting via Multi-Level Text Alignment with LLMs", "authors": ["Taibiao Zhao", "Xiaobing Chen", "Mingxuan Sun"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "This paper is accepted by DASFAA2025", "summary": "The adaptation of large language models (LLMs) to time series forecasting\nposes unique challenges, as time series data is continuous in nature, while\nLLMs operate on discrete tokens. Despite the success of LLMs in natural\nlanguage processing (NLP) and other structured domains, aligning time series\ndata with language-based representations while maintaining both predictive\naccuracy and interpretability remains a significant hurdle. Existing methods\nhave attempted to reprogram time series data into text-based forms, but these\noften fall short in delivering meaningful, interpretable results. In this\npaper, we propose a multi-level text alignment framework for time series\nforecasting using LLMs that not only improves prediction accuracy but also\nenhances the interpretability of time series representations. Our method\ndecomposes time series into trend, seasonal, and residual components, which are\nthen reprogrammed into component-specific text representations. We introduce a\nmulti-level alignment mechanism, where component-specific embeddings are\naligned with pre-trained word tokens, enabling more interpretable forecasts.\nExperiments on multiple datasets demonstrate that our method outperforms\nstate-of-the-art models in accuracy while providing good interpretability.", "AI": {"tldr": "This paper presents a multi-level text alignment framework for time series forecasting using large language models (LLMs) to improve both prediction accuracy and interpretability.", "motivation": "The need to effectively align time series data with language-based representations is essential for enhancing predictive accuracy and interpretability in forecasting tasks.", "method": "The proposed framework decomposes time series data into trend, seasonal, and residual components and creates component-specific text representations through a multi-level alignment mechanism with pre-trained word tokens.", "result": "Experiments show that the proposed method outperforms state-of-the-art forecasting models in both accuracy and interpretability across multiple datasets.", "conclusion": "The multi-level text alignment framework effectively bridges the gap between continuous time series data and discrete LLM representations, yielding significant improvements in forecasting.", "key_contributions": ["Introduces a multi-level text alignment framework for time series forecasting using LLMs.", "Decomposes time series data into interpretable components for improved forecasting.", "Demonstrates superior performance in accuracy and interpretability over existing models."], "limitations": "", "keywords": ["time series forecasting", "large language models", "interpretability", "multi-level alignment", "text representations"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2504.13626", "pdf": "https://arxiv.org/pdf/2504.13626.pdf", "abs": "https://arxiv.org/abs/2504.13626", "title": "Thought Manipulation: External Thought Can Be Efficient for Large Reasoning Models", "authors": ["Yule Liu", "Jingyi Zheng", "Zhen Sun", "Zifan Peng", "Wenhan Dong", "Zeyang Sha", "Shiwen Cui", "Weiqiang Wang", "Xinlei He"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in large reasoning models (LRMs) have demonstrated the\neffectiveness of scaling test-time computation to enhance reasoning\ncapabilities on various tasks. However, LRMs often suffer from an\n``overthinking'' problem, where the model generates excessively redundant\nreasoning steps with limited performance gains. In this work, we empirically\nreveal an important characteristic of LRM behaviors that placing external CoTs\ngenerated by smaller models between the thinking token (\\texttt{<think>} and\n\\texttt{</think>}) can effectively manipulate the model to generate fewer\nthoughts. Building on this finding, we propose a simple yet efficient pipeline,\n\\Method, to enable LRMs to bypass unnecessary intermediate steps, thereby\nsignificantly reducing computational costs. We conduct extensive experiments to\nevaluate the utility and efficiency of \\Method. For instance, when applied to\nQwQ-32B on the LiveBench/Code dataset, \\Method keeps the original performance\nwhile reducing output token counts by approximately 30\\%, with minimal overhead\nintroduced by the CoT generator. Furthermore, we identify two suboptimal modes,\nblindly following flawed external thoughts and unnecessary rethinking, and show\nthat simple mitigations, such as difficulty-aware fallbacks, can further\nimprove performance. Overall, \\Method offers a practical, general, and\nefficient way to optimize LRM inference, making powerful reasoning models more\naccessible and scalable for real-world applications.", "AI": {"tldr": "The paper presents a method to optimize large reasoning models (LRMs) by reducing redundant reasoning steps using external chains of thought (CoTs) generated by smaller models, leading to significant computational savings without sacrificing performance.", "motivation": "To address the 'overthinking' problem in large reasoning models, which results in excessive redundancy and limited performance gains, while aiming to improve efficiency and accessibility of these models for real-world applications.", "method": "The proposed method, named \\\\Method, integrates external CoTs between thinking tokens to manipulate LRM behavior, enabling models to bypass unnecessary steps and reduce output token counts while maintaining performance.", "result": "Experimental results show that \\\\Method reduces output token counts by approximately 30% when applied to the QwQ-32B model on the LiveBench/Code dataset, with minimal overhead from the CoT generator.", "conclusion": "The findings demonstrate that \\\\Method is an efficient approach to optimize LRM inference and improve the application of reasoning models in various tasks.", "key_contributions": ["Introduction of a method to place external CoTs within LRMs to optimize reasoning processes.", "Significant reduction of output tokens without performance loss, enhancing efficiency for LRM applications.", "Identification of two suboptimal modes in LRM behavior and proposed solutions to mitigate these issues."], "limitations": "No specific limitations mentioned in the abstract.", "keywords": ["large reasoning models", "chains of thought", "optimization", "computational efficiency", "machine learning"], "importance_score": 7, "read_time_minutes": 10}}
