{"id": "2507.10773", "pdf": "https://arxiv.org/pdf/2507.10773.pdf", "abs": "https://arxiv.org/abs/2507.10773", "title": "Theory of Mind and Self-Disclosure to CUIs", "authors": ["Samuel Rhys Cox"], "categories": ["cs.HC", "cs.CL"], "comment": "Workshop paper presented at ToMinHAI at CUI'2025: Theory of Mind in\n  Human-CUI Interaction, held in conjunction with the 2025 ACM conference on\n  Conversational User Interfaces, July 8th, 2025. 4 pages. 3 figures", "summary": "Self-disclosure is important to help us feel better, yet is often difficult.\nThis difficulty can arise from how we think people are going to react to our\nself-disclosure. In this workshop paper, we briefly discuss self-disclosure to\nconversational user interfaces (CUIs) in relation to various social cues. We\nthen, discuss how expressions of uncertainty or representation of a CUI's\nreasoning could help encourage self-disclosure, by making a CUI's intended\n\"theory of mind\" more transparent to users."}
{"id": "2507.10812", "pdf": "https://arxiv.org/pdf/2507.10812.pdf", "abs": "https://arxiv.org/abs/2507.10812", "title": "React to This (RTT): A Nonverbal Turing Test for Embodied AI", "authors": ["Chuxuan Zhang", "Yasaman Etesam", "Angelica Lim"], "categories": ["cs.HC", "cs.AI"], "comment": "5 pages, 3 figures", "summary": "We propose an approach to test embodied AI agents for interaction awareness\nand believability, particularly in scenarios where humans push them to their\nlimits. Turing introduced the Imitation Game as a way to explore the question:\n\"Can machines think?\" The Total Turing Test later expanded this concept beyond\npurely verbal communication, incorporating perceptual and physical interaction.\nBuilding on this, we propose a new guiding question: \"Can machines react?\" and\nintroduce the React to This (RTT) test for nonverbal behaviors, presenting\nresults from an initial experiment."}
{"id": "2507.10813", "pdf": "https://arxiv.org/pdf/2507.10813.pdf", "abs": "https://arxiv.org/abs/2507.10813", "title": "Static or Temporal? Semantic Scene Simplification to Aid Wayfinding in Immersive Simulations of Bionic Vision", "authors": ["Justin M. Kasowski", "Apurv Varshney", "Michael Beyeler"], "categories": ["cs.HC"], "comment": null, "summary": "Visual neuroprostheses (bionic eye) aim to restore a rudimentary form of\nvision by translating camera input into patterns of electrical stimulation. To\nimprove scene understanding under extreme resolution and bandwidth constraints,\nprior work has explored computer vision techniques such as semantic\nsegmentation and depth estimation. However, presenting all task-relevant\ninformation simultaneously can overwhelm users in cluttered environments. We\ncompare two complementary approaches to semantic preprocessing in immersive\nvirtual reality: SemanticEdges, which highlights all relevant objects at once,\nand SemanticRaster, which staggers object categories over time to reduce visual\nclutter. Using a biologically grounded simulation of prosthetic vision, 18\nsighted participants performed a wayfinding task in a dynamic urban environment\nacross three conditions: edge-based baseline (Control), SemanticEdges, and\nSemanticRaster. Both semantic strategies improved performance and user\nexperience relative to the baseline, with each offering distinct trade-offs:\nSemanticEdges increased the odds of success, while SemanticRaster boosted the\nlikelihood of collision-free completions. These findings underscore the value\nof adaptive semantic preprocessing for prosthetic vision and, more broadly, may\ninform the design of low-bandwidth visual interfaces in XR that must balance\ninformation density, task relevance, and perceptual clarity."}
{"id": "2507.10963", "pdf": "https://arxiv.org/pdf/2507.10963.pdf", "abs": "https://arxiv.org/abs/2507.10963", "title": "AROMA: Mixed-Initiative AI Assistance for Non-Visual Cooking by Grounding Multi-modal Information Between Reality and Videos", "authors": ["Zheng Ning", "Leyang Li", "Daniel Killough", "JooYoung Seo", "Patrick Carrington", "Yapeng Tian", "Yuhang Zhao", "Franklin Mingzhe Li", "Toby Jia-Jun Li"], "categories": ["cs.HC"], "comment": null, "summary": "Videos offer rich audiovisual information that can support people in\nperforming activities of daily living (ADLs), but they remain largely\ninaccessible to blind or low-vision (BLV) individuals. In cooking, BLV people\noften rely on non-visual cues, such as touch, taste, and smell, to navigate\ntheir environment, making it difficult to follow the predominantly audiovisual\ninstructions found in video recipes. To address this problem, we introduce\nAROMA, an AI system that provides timely responses to the user based on\nreal-time, context-aware assistance by integrating non-visual cues perceived by\nthe user, a wearable camera feed, and video recipe content. AROMA uses a\nmixed-initiative approach: it responds to user requests while also proactively\nmonitoring the video stream to offer timely alerts and guidance. This\ncollaborative design leverages the complementary strengths of the user and AI\nsystem to align the physical environment with the video recipe, helping the\nuser interpret their current cooking state and make sense of the steps. We\nevaluated AROMA through a study with eight BLV participants and offered\ninsights for designing interactive AI systems to support BLV individuals in\nperforming ADLs."}
{"id": "2507.10577", "pdf": "https://arxiv.org/pdf/2507.10577.pdf", "abs": "https://arxiv.org/abs/2507.10577", "title": "Truth Sleuth and Trend Bender: AI Agents to fact-check YouTube videos and influence opinions", "authors": ["Logé Cécile", "Ghori Rehan"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Misinformation poses a significant threat in today's digital world, often\nspreading rapidly through platforms like YouTube. This paper introduces a novel\napproach to combating misinformation by developing an AI-powered system that\nnot only fact-checks claims made in YouTube videos but also actively engages\nusers in the comment section and challenge misleading narratives. Our system\ncomprises two main agents: Truth Sleuth and Trend Bender.\n  Truth Sleuth extracts claims from a YouTube video, uses a Retrieval-Augmented\nGeneration (RAG) approach - drawing on sources like Wikipedia, Google Search,\nGoogle FactCheck - to accurately assess their veracity and generates a nuanced\nand comprehensive report. Through rigorous prompt engineering, Trend Bender\nleverages this report along with a curated corpus of relevant articles to\ngenerate insightful and persuasive comments designed to stimulate a productive\ndebate. With a carefully set up self-evaluation loop, this agent is able to\niteratively improve its style and refine its output.\n  We demonstrate the system's capabilities through experiments on established\nbenchmark datasets and a real-world deployment on YouTube, showcasing its\npotential to engage users and potentially influence perspectives. Our findings\nhighlight the high accuracy of our fact-checking agent, and confirm the\npotential of AI-driven interventions in combating misinformation and fostering\na more informed online space."}
{"id": "2507.10967", "pdf": "https://arxiv.org/pdf/2507.10967.pdf", "abs": "https://arxiv.org/abs/2507.10967", "title": "Self++: Merging Human and AI for Co-Determined XR Living in the Metaverse", "authors": ["Thammathip Piumsomboon"], "categories": ["cs.HC"], "comment": null, "summary": "This position paper introduces Self++, a novel nine-level framework for\nco-determined living in the Metaverse, grounded in Self-Determination Theory.\nSelf++ prioritises human flourishing by progressively cultivating competence,\nautonomy, and relatedness through dynamic human-AI collaboration in extended\nreality (XR). Unlike technologically deterministic approaches, Self++\nemphasises user empowerment by enhancing competency, mitigating cognitive\nbiases and leveraging XR's immersive capabilities. Key research directions\nproposed include exploring the boundaries of user-defined AI autonomy,\ndesigning for meaningful social connection in XR, and establishing proactive\nethical safeguards. Ultimately, Self++ offers a roadmap for creating a\nhuman-centred, AI-enhanced Metaverse where technology amplifies, rather than\ndiminishes, human potential."}
{"id": "2507.10580", "pdf": "https://arxiv.org/pdf/2507.10580.pdf", "abs": "https://arxiv.org/abs/2507.10580", "title": "An Offline Mobile Conversational Agent for Mental Health Support: Learning from Emotional Dialogues and Psychological Texts with Student-Centered Evaluation", "authors": ["Vimaleswar A", "Prabhu Nandan Sahu", "Nilesh Kumar Sahu", "Haroon R Lone"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": null, "summary": "Mental health plays a crucial role in the overall well-being of an\nindividual. In recent years, digital platforms have been increasingly used to\nexpand mental health and emotional support. However, there are persistent\nchallenges related to limited user accessibility, internet connectivity, and\ndata privacy, which highlight the need for an offline, smartphone-based\nsolution. To address these challenges, we propose EmoSApp (Emotional Support\nApp): an entirely offline, smartphone-based conversational app designed for\nmental health and emotional support. The system leverages Large Language Models\n(LLMs), specifically fine-tuned, quantized and deployed using Torchtune and\nExecutorch for resource-constrained devices, allowing all inferences to occur\non the smartphone. To equip EmoSApp with robust domain expertise, we fine-tuned\nthe LLaMA-3.2-1B-Instruct model on our custom curated ``Knowledge dataset'' of\n14,582 mental-health QA pairs, along with the multi-turn conversational data.\n  Through qualitative human evaluation with the student population, we\ndemonstrate that EmoSApp has the ability to respond coherently, empathetically,\nmaintain interactive dialogue, and provide relevant suggestions to user's\nmental health problems. Additionally, quantitative evaluations on nine standard\ncommonsense and reasoning benchmarks demonstrate the efficacy of our\nfine-tuned, quantized model in low-resource settings. By prioritizing on-device\ndeployment and specialized domain adaptation, EmoSApp serves as a blueprint for\nfuture innovations in portable, secure, and highly tailored AI-driven mental\nhealth solutions."}
{"id": "2507.10970", "pdf": "https://arxiv.org/pdf/2507.10970.pdf", "abs": "https://arxiv.org/abs/2507.10970", "title": "Terms and Conditions (Do Not) Apply: Understanding Exploitation Disparities in Design of Mobile-Based Financial Services", "authors": ["Lindah Kotut"], "categories": ["cs.HC"], "comment": "Accepted for Publication at The 5th Biennial African Human Computer\n  Interaction Conference (AfriCHI 2025). 10 pages (excluding references), 3\n  figures", "summary": "Mobile-based financial services have made it possible for the traditionally\nunbanked to access infrastructure that have been routinely unattainable.\nResearchers have explored how these systems have made for safer environments to\nsend and receive money and have expanded financial opportunities such as\nincreased borrowing. With this expansion, challenges such as detrimental\ninterest rates, lack of access to policy documents, and inadequate user\nprotective guardrails emerge, amplifying the risks due to technology-aided\nunethical financial practices that are aided by design patterns. Supported by\nuser interviews, we detail user experiences of mobile-based financial\ntransactions and explore the foundations and guidelines that undergird the\nfinancial service provisions: highlighting both affordances and harms enabled\nin the design of such systems. We discuss the findings by highlighting\nfinancial exploitation disparities, deliberating strategies for mitigation of\nrisks and enabling recovery from harms caused by the technology use. We then\nrecommend guidelines for empowering design approaches that support users'\nmechanisms of trust, their understanding of technological processes, and\ndetermination of risks."}
{"id": "2507.10582", "pdf": "https://arxiv.org/pdf/2507.10582.pdf", "abs": "https://arxiv.org/abs/2507.10582", "title": "Transforming Sensitive Documents into Quantitative Data: An AI-Based Preprocessing Toolchain for Structured and Privacy-Conscious Analysis", "authors": ["Anders Ledberg", "Anna Thalén"], "categories": ["cs.CL", "stat.ME"], "comment": null, "summary": "Unstructured text from legal, medical, and administrative sources offers a\nrich but underutilized resource for research in public health and the social\nsciences. However, large-scale analysis is hampered by two key challenges: the\npresence of sensitive, personally identifiable information, and significant\nheterogeneity in structure and language. We present a modular toolchain that\nprepares such text data for embedding-based analysis, relying entirely on\nopen-weight models that run on local hardware, requiring only a\nworkstation-level GPU and supporting privacy-sensitive research.\n  The toolchain employs large language model (LLM) prompting to standardize,\nsummarize, and, when needed, translate texts to English for greater\ncomparability. Anonymization is achieved via LLM-based redaction, supplemented\nwith named entity recognition and rule-based methods to minimize the risk of\ndisclosure. We demonstrate the toolchain on a corpus of 10,842 Swedish court\ndecisions under the Care of Abusers Act (LVM), comprising over 56,000 pages.\nEach document is processed into an anonymized, standardized summary and\ntransformed into a document-level embedding. Validation, including manual\nreview, automated scanning, and predictive evaluation shows the toolchain\neffectively removes identifying information while retaining semantic content.\nAs an illustrative application, we train a predictive model using embedding\nvectors derived from a small set of manually labeled summaries, demonstrating\nthe toolchain's capacity for semi-automated content analysis at scale.\n  By enabling structured, privacy-conscious analysis of sensitive documents,\nour toolchain opens new possibilities for large-scale research in domains where\ntextual data was previously inaccessible due to privacy and heterogeneity\nconstraints."}
{"id": "2507.10981", "pdf": "https://arxiv.org/pdf/2507.10981.pdf", "abs": "https://arxiv.org/abs/2507.10981", "title": "An Exploratory Study on AI-driven Visualisation Techniques on Decision Making in Extended Reality", "authors": ["Ze Dong", "Binyang Han", "Jingjing Zhang", "Ruoyu Wen", "Barrett Ens", "Adrian Clark", "Tham Piumsomboon"], "categories": ["cs.HC"], "comment": null, "summary": "The integration of extended reality (XR) with artificial intelligence (AI)\nintroduces a new paradigm for user interaction, enabling AI to perceive user\nintent, stimulate the senses, and influence decision-making. We explored the\nimpact of four AI-driven visualisation techniques -- `Inform,' `Nudge,'\n`Recommend,' and `Instruct' -- on user decision-making in XR using the Meta\nQuest Pro. To test these techniques, we used a pre-recorded 360-degree video of\na supermarket, overlaying each technique through a virtual interface. We aimed\nto investigate how these different visualisation techniques with different\nlevels of user autonomy impact preferences and decision-making. An exploratory\nstudy with semi-structured interviews provided feedback and design\nrecommendations. Our findings emphasise the importance of maintaining user\nautonomy, enhancing AI transparency to build trust, and considering context in\nvisualisation design."}
{"id": "2507.10585", "pdf": "https://arxiv.org/pdf/2507.10585.pdf", "abs": "https://arxiv.org/abs/2507.10585", "title": "A Taxonomy for Design and Evaluation of Prompt-Based Natural Language Explanations", "authors": ["Isar Nejadgholi", "Mona Omidyeganeh", "Marc-Antoine Drouin", "Jonathan Boisvert"], "categories": ["cs.CL", "cs.AI"], "comment": "Presented at the Workshop of Technical AI Governance, 5 pages 2\n  figures", "summary": "Effective AI governance requires structured approaches for stakeholders to\naccess and verify AI system behavior. With the rise of large language models,\nNatural Language Explanations (NLEs) are now key to articulating model\nbehavior, which necessitates a focused examination of their characteristics and\ngovernance implications. We draw on Explainable AI (XAI) literature to create\nan updated XAI taxonomy, adapted to prompt-based NLEs, across three dimensions:\n(1) Context, including task, data, audience, and goals; (2) Generation and\nPresentation, covering generation methods, inputs, interactivity, outputs, and\nforms; and (3) Evaluation, focusing on content, presentation, and user-centered\nproperties, as well as the setting of the evaluation. This taxonomy provides a\nframework for researchers, auditors, and policymakers to characterize, design,\nand enhance NLEs for transparent AI systems."}
{"id": "2507.11210", "pdf": "https://arxiv.org/pdf/2507.11210.pdf", "abs": "https://arxiv.org/abs/2507.11210", "title": "Role-Playing LLM-Based Multi-Agent Support Framework for Detecting and Addressing Family Communication Bias", "authors": ["Rushia Harada", "Yuken Kimura", "Keito Inoshita"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Well-being in family settings involves subtle psychological dynamics that\nconventional metrics often overlook. In particular, unconscious parental\nexpectations, termed ideal parent bias, can suppress children's emotional\nexpression and autonomy. This suppression, referred to as suppressed emotion,\noften stems from well-meaning but value-driven communication, which is\ndifficult to detect or address from outside the family. Focusing on these\nlatent dynamics, this study explores Large Language Model (LLM)-based support\nfor psychologically safe family communication. We constructed a Japanese\nparent-child dialogue corpus of 30 scenarios, each annotated with metadata on\nideal parent bias and suppressed emotion. Based on this corpus, we developed a\nRole-Playing LLM-based multi-agent dialogue support framework that analyzes\ndialogue and generates feedback. Specialized agents detect suppressed emotion,\ndescribe implicit ideal parent bias in parental speech, and infer contextual\nattributes such as the child's age and background. A meta-agent compiles these\noutputs into a structured report, which is then passed to five selected expert\nagents. These agents collaboratively generate empathetic and actionable\nfeedback through a structured four-step discussion process. Experiments show\nthat the system can detect categories of suppressed emotion with moderate\naccuracy and produce feedback rated highly in empathy and practicality.\nMoreover, simulated follow-up dialogues incorporating this feedback exhibited\nsigns of improved emotional expression and mutual understanding, suggesting the\nframework's potential in supporting positive transformation in family\ninteractions."}
{"id": "2507.10586", "pdf": "https://arxiv.org/pdf/2507.10586.pdf", "abs": "https://arxiv.org/abs/2507.10586", "title": "AutoRAG-LoRA: Hallucination-Triggered Knowledge Retuning via Lightweight Adapters", "authors": ["Kaushik Dwivedi", "Padmanabh Patanjali Mishra"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable fluency across a\nrange of natural language tasks, yet remain vulnerable to hallucinations -\nfactual inaccuracies that undermine trust in real world deployment. We present\nAutoRAG-LoRA, a modular framework for Retrieval-Augmented Generation (RAG) that\ntackles hallucination in large language models through lightweight LoRA-based\nadapters and KL-regularized training. Our pipeline integrates automated prompt\nrewriting, hybrid retrieval, and low-rank adapter tuning to ground responses in\nretrieved evidence. A hallucination detection module, using both\nclassifier-based and self-evaluation techniques, assigns confidence scores to\ngenerated outputs, triggering an optional feedback correction loop. This loop\nenforces factual alignment via contrastive KL loss and adapter fine tuning. We\ndemonstrate that AutoRAG-LoRA significantly reduces the factual drift while\npreserving the efficiency and modularity of the model."}
{"id": "2507.11470", "pdf": "https://arxiv.org/pdf/2507.11470.pdf", "abs": "https://arxiv.org/abs/2507.11470", "title": "REVA: Supporting LLM-Generated Programming Feedback Validation at Scale Through User Attention-based Adaptation", "authors": ["Xiaohang Tang", "Sam Wong", "Zicheng He", "Yalong Yang", "Yan Chen"], "categories": ["cs.HC"], "comment": null, "summary": "This paper introduces REVA, a human-AI system that expedites instructor\nreview of voluminous AI-generated programming feedback by sequencing\nsubmissions to minimize cognitive context shifts and propagating\ninstructor-driven revisions across semantically similar instances. REVA\nintroduces a novel approach to human-AI collaboration in educational feedback\nby adaptively learning from instructors' attention in the review and revision\nprocess to continuously improve the feedback validation process. REVA's\nusefulness and effectiveness in improving feedback quality and the overall\nfeedback review process were evaluated through a within-subjects lab study with\n12 participants."}
{"id": "2507.10587", "pdf": "https://arxiv.org/pdf/2507.10587.pdf", "abs": "https://arxiv.org/abs/2507.10587", "title": "Anthropomimetic Uncertainty: What Verbalized Uncertainty in Language Models is Missing", "authors": ["Dennis Ulmer", "Alexandra Lorson", "Ivan Titov", "Christian Hardmeier"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Human users increasingly rely on natural language interactions with large\nlanguage models (LLMs) in order to receive help on a large variety of tasks and\nproblems. However, the trustworthiness and perceived legitimacy of LLMs is\nundermined by the fact that their output is frequently stated in very confident\nterms, even when its accuracy is questionable. Therefore, there is a need to\nsignal the confidence of the language model to a user in order to reap the\nbenefits of human-machine collaboration and mitigate potential harms.\nVerbalized uncertainty is the expression of confidence with linguistic means,\nan approach that integrates perfectly into language-based interfaces.\nNevertheless, most recent research in natural language processing (NLP)\noverlooks the nuances surrounding human uncertainty communication and the data\nbiases that influence machine uncertainty communication. We argue for\nanthropomimetic uncertainty, meaning that intuitive and trustworthy uncertainty\ncommunication requires a degree of linguistic authenticity and personalization\nto the user, which could be achieved by emulating human communication. We\npresent a thorough overview over the research in human uncertainty\ncommunication, survey ongoing research, and perform additional analyses to\ndemonstrate so-far overlooked biases in verbalized uncertainty. We conclude by\npointing out unique factors in human-machine communication of uncertainty and\ndeconstruct anthropomimetic uncertainty into future research directions for\nNLP."}
{"id": "2507.11490", "pdf": "https://arxiv.org/pdf/2507.11490.pdf", "abs": "https://arxiv.org/abs/2507.11490", "title": "Towards Creating Infrastructures for Values and Ethics Work in the Production of Software Technologies", "authors": ["Richmond Y. Wong"], "categories": ["cs.HC"], "comment": "In The sixth decennial Aarhus conference: Computing X Crisis (AAR\n  2025)", "summary": "Recognizing how technical systems can embody social values or cause harms,\nhuman-computer interaction (HCI) research often approaches addressing values\nand ethics in design by creating tools to help tech workers integrate social\nvalues into the design of products. While useful, these approaches usually do\nnot consider the politics embedded in the broader processes, organizations,\nsocial systems, and governance structures that affect the types of actions that\ntech workers can take to address values and ethics. This paper argues that\ncreating infrastructures to support values and ethics work, rather than tools,\nis an approach that takes these broader processes into account and opens them\nup for (re)design. Drawing on prior research conceptualizing infrastructures\nfrom science \\& technology studies and media studies, this paper outlines\nconceptual insights from infrastructures studies that open up new tactics for\nHCI researchers and designers seeking to support values and ethics in design."}
{"id": "2507.10596", "pdf": "https://arxiv.org/pdf/2507.10596.pdf", "abs": "https://arxiv.org/abs/2507.10596", "title": "PLEX: Perturbation-free Local Explanations for LLM-Based Text Classification", "authors": ["Yogachandran Rahulamathavan", "Misbah Farooq", "Varuna De Silva"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) excel in text classification, but their\ncomplexity hinders interpretability, making it difficult to understand the\nreasoning behind their predictions. Explainable AI (XAI) methods like LIME and\nSHAP offer local explanations by identifying influential words, but they rely\non computationally expensive perturbations. These methods typically generate\nthousands of perturbed sentences and perform inferences on each, incurring a\nsubstantial computational burden, especially with LLMs. To address this, we\npropose \\underline{P}erturbation-free \\underline{L}ocal \\underline{Ex}planation\n(PLEX), a novel method that leverages the contextual embeddings extracted from\nthe LLM and a ``Siamese network\" style neural network trained to align with\nfeature importance scores. This one-off training eliminates the need for\nsubsequent perturbations, enabling efficient explanations for any new sentence.\nWe demonstrate PLEX's effectiveness on four different classification tasks\n(sentiment, fake news, fake COVID-19 news and depression), showing more than\n92\\% agreement with LIME and SHAP. Our evaluation using a ``stress test\"\nreveals that PLEX accurately identifies influential words, leading to a similar\ndecline in classification accuracy as observed with LIME and SHAP when these\nwords are removed. Notably, in some cases, PLEX demonstrates superior\nperformance in capturing the impact of key features. PLEX dramatically\naccelerates explanation, reducing time and computational overhead by two and\nfour orders of magnitude, respectively. This work offers a promising solution\nfor explainable LLM-based text classification."}
{"id": "2507.10580", "pdf": "https://arxiv.org/pdf/2507.10580.pdf", "abs": "https://arxiv.org/abs/2507.10580", "title": "An Offline Mobile Conversational Agent for Mental Health Support: Learning from Emotional Dialogues and Psychological Texts with Student-Centered Evaluation", "authors": ["Vimaleswar A", "Prabhu Nandan Sahu", "Nilesh Kumar Sahu", "Haroon R Lone"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": null, "summary": "Mental health plays a crucial role in the overall well-being of an\nindividual. In recent years, digital platforms have been increasingly used to\nexpand mental health and emotional support. However, there are persistent\nchallenges related to limited user accessibility, internet connectivity, and\ndata privacy, which highlight the need for an offline, smartphone-based\nsolution. To address these challenges, we propose EmoSApp (Emotional Support\nApp): an entirely offline, smartphone-based conversational app designed for\nmental health and emotional support. The system leverages Large Language Models\n(LLMs), specifically fine-tuned, quantized and deployed using Torchtune and\nExecutorch for resource-constrained devices, allowing all inferences to occur\non the smartphone. To equip EmoSApp with robust domain expertise, we fine-tuned\nthe LLaMA-3.2-1B-Instruct model on our custom curated ``Knowledge dataset'' of\n14,582 mental-health QA pairs, along with the multi-turn conversational data.\n  Through qualitative human evaluation with the student population, we\ndemonstrate that EmoSApp has the ability to respond coherently, empathetically,\nmaintain interactive dialogue, and provide relevant suggestions to user's\nmental health problems. Additionally, quantitative evaluations on nine standard\ncommonsense and reasoning benchmarks demonstrate the efficacy of our\nfine-tuned, quantized model in low-resource settings. By prioritizing on-device\ndeployment and specialized domain adaptation, EmoSApp serves as a blueprint for\nfuture innovations in portable, secure, and highly tailored AI-driven mental\nhealth solutions."}
{"id": "2507.10599", "pdf": "https://arxiv.org/pdf/2507.10599.pdf", "abs": "https://arxiv.org/abs/2507.10599", "title": "Emergence of Hierarchical Emotion Organization in Large Language Models", "authors": ["Bo Zhao", "Maya Okawa", "Eric J. Bigelow", "Rose Yu", "Tomer Ullman", "Ekdeep Singh Lubana", "Hidenori Tanaka"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "As large language models (LLMs) increasingly power conversational agents,\nunderstanding how they model users' emotional states is critical for ethical\ndeployment. Inspired by emotion wheels -- a psychological framework that argues\nemotions organize hierarchically -- we analyze probabilistic dependencies\nbetween emotional states in model outputs. We find that LLMs naturally form\nhierarchical emotion trees that align with human psychological models, and\nlarger models develop more complex hierarchies. We also uncover systematic\nbiases in emotion recognition across socioeconomic personas, with compounding\nmisclassifications for intersectional, underrepresented groups. Human studies\nreveal striking parallels, suggesting that LLMs internalize aspects of social\nperception. Beyond highlighting emergent emotional reasoning in LLMs, our\nresults hint at the potential of using cognitively-grounded theories for\ndeveloping better model evaluations."}
{"id": "2507.10644", "pdf": "https://arxiv.org/pdf/2507.10644.pdf", "abs": "https://arxiv.org/abs/2507.10644", "title": "From Semantic Web and MAS to Agentic AI: A Unified Narrative of the Web of Agents", "authors": ["Tatiana Petrova", "Aleksandr Puzikov", "Boris Bliznukov", "Radu State"], "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.HC", "cs.MA", "I.2.11; I.2.7; C.2.4; K.6.5; I.2.4"], "comment": "33 pages, 9 figures, 8 tables", "summary": "The concept of the Web of Agents (WoA), which transforms the static,\ndocument-centric Web into an environment of autonomous agents acting on users'\nbehalf, has attracted growing interest as large language models (LLMs) become\nmore capable. However, research in this area is still fragmented across\ndifferent communities. Contemporary surveys catalog the latest LLM-powered\nframeworks, while the rich histories of Multi-Agent Systems (MAS) and the\nSemantic Web are often treated as separate, legacy domains. This fragmentation\nobscures the intellectual lineage of modern systems and hinders a holistic\nunderstanding of the field's trajectory. We present the first comprehensive\nevolutionary overview of the WoA. We show that modern protocols like A2A and\nthe MCP, are direct evolutionary responses to the well-documented limitations\nof earlier standards like FIPA standards and OWL-based semantic agents. To\nsystematize this analysis, we introduce a four-axis taxonomy (semantic\nfoundation, communication paradigm, locus of intelligence, discovery\nmechanism). This framework provides a unified analytical lens for comparing\nagent architectures across all generations, revealing a clear line of descent\nwhere others have seen a disconnect. Our analysis identifies a paradigm shift\nin the 'locus of intelligence': from being encoded in external data (Semantic\nWeb) or the platform (MAS) to being embedded within the agent's core model\n(LLM). This shift is foundational to modern Agentic AI, enabling the scalable\nand adaptive systems the WoA has long envisioned. We conclude that while new\nprotocols are essential, they are insufficient for building a robust, open,\ntrustworthy ecosystem. Finally, we argue that the next research frontier lies\nin solving persistent socio-technical challenges, and we map out a new agenda\nfocused on decentralized identity, economic models, security, and governance\nfor the emerging WoA."}
{"id": "2507.10743", "pdf": "https://arxiv.org/pdf/2507.10743.pdf", "abs": "https://arxiv.org/abs/2507.10743", "title": "Language Models for Adult Service Website Text Analysis", "authors": ["Nickolas Freeman", "Thanh Nguyen", "Gregory Bott", "Jason Parton", "Collin Francel"], "categories": ["cs.CL", "cs.LG"], "comment": "32 pages, 12 figures, 1 table", "summary": "Sex trafficking refers to the use of force, fraud, or coercion to compel an\nindividual to perform in commercial sex acts against their will. Adult service\nwebsites (ASWs) have and continue to be linked to sex trafficking, offering a\nplatform for traffickers to advertise their victims. Thus, organizations\ninvolved in the fight against sex trafficking often use ASW data when\nattempting to identify potential sex trafficking victims. A critical challenge\nin transforming ASW data into actionable insight is text analysis. Previous\nresearch using ASW data has shown that ASW ad text is important for linking\nads. However, working with this text is challenging due to its extensive use of\nemojis, poor grammar, and deliberate obfuscation to evade law enforcement\nscrutiny. We conduct a comprehensive study of language modeling approaches for\nthis application area, including simple information retrieval methods,\npre-trained transformers, and custom transformer models. We demonstrate that\ncharacteristics of ASW text data allow efficient custom transformer models to\nbe trained with relatively small GPU resources and used efficiently for\ninference on consumer hardware. Our custom models outperform fine-tuned\nvariants of well-known encoder-only transformer models, including BERT-base,\nRoBERTa, and ModernBERT, on accuracy, recall, F1 score, and ROC AUC. We\ndemonstrate the use of our best-performing custom configuration on three tasks\nrelated to ASW data analysis: (i) decomposing the giant component in a graph\nrepresentation of ASW data, (ii) clustering ASW ad text, and (iii) using the\nlearned token embeddings to understand the use of emojis in the illicit context\nwe study. The models we develop represent a significant advancement in ASW text\nanalysis, which can be leveraged in a variety of downstream applications and\nresearch."}
{"id": "2507.10695", "pdf": "https://arxiv.org/pdf/2507.10695.pdf", "abs": "https://arxiv.org/abs/2507.10695", "title": "Exploring User Security and Privacy Attitudes and Concerns Toward the Use of General-Purpose LLM Chatbots for Mental Health", "authors": ["Jabari Kwesi", "Jiaxun Cao", "Riya Manchanda", "Pardis Emami-Naeini"], "categories": ["cs.CY", "cs.AI", "cs.CR", "cs.ET", "cs.HC"], "comment": "Accepted to the 34th USENIX Security Symposium", "summary": "Individuals are increasingly relying on large language model (LLM)-enabled\nconversational agents for emotional support. While prior research has examined\nprivacy and security issues in chatbots specifically designed for mental health\npurposes, these chatbots are overwhelmingly \"rule-based\" offerings that do not\nleverage generative AI. Little empirical research currently measures users'\nprivacy and security concerns, attitudes, and expectations when using\ngeneral-purpose LLM-enabled chatbots to manage and improve mental health.\nThrough 21 semi-structured interviews with U.S. participants, we identified\ncritical misconceptions and a general lack of risk awareness. Participants\nconflated the human-like empathy exhibited by LLMs with human-like\naccountability and mistakenly believed that their interactions with these\nchatbots were safeguarded by the same regulations (e.g., HIPAA) as disclosures\nwith a licensed therapist. We introduce the concept of \"intangible\nvulnerability,\" where emotional or psychological disclosures are undervalued\ncompared to more tangible forms of information (e.g., financial or\nlocation-based data). To address this, we propose recommendations to safeguard\nuser mental health disclosures with general-purpose LLM-enabled chatbots more\neffectively."}
{"id": "2507.10772", "pdf": "https://arxiv.org/pdf/2507.10772.pdf", "abs": "https://arxiv.org/abs/2507.10772", "title": "Applying Text Embedding Models for Efficient Analysis in Labeled Property Graphs", "authors": ["Michal Podstawski"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Labeled property graphs often contain rich textual attributes that can\nenhance analytical tasks when properly leveraged. This work explores the use of\npretrained text embedding models to enable efficient semantic analysis in such\ngraphs. By embedding textual node and edge properties, we support downstream\ntasks including node classification and relation prediction with improved\ncontextual understanding. Our approach integrates language model embeddings\ninto the graph pipeline without altering its structure, demonstrating that\ntextual semantics can significantly enhance the accuracy and interpretability\nof property graph analysis."}
{"id": "2507.10761", "pdf": "https://arxiv.org/pdf/2507.10761.pdf", "abs": "https://arxiv.org/abs/2507.10761", "title": "Detecting AI Assistance in Abstract Complex Tasks", "authors": ["Tyler King", "Nikolos Gurney", "John H. Miller", "Volkan Ustun"], "categories": ["cs.AI", "cs.HC"], "comment": "Accepted to HCII 2025", "summary": "Detecting assistance from artificial intelligence is increasingly important\nas they become ubiquitous across complex tasks such as text generation, medical\ndiagnosis, and autonomous driving. Aid detection is challenging for humans,\nespecially when looking at abstract task data. Artificial neural networks excel\nat classification thanks to their ability to quickly learn from and process\nlarge amounts of data -- assuming appropriate preprocessing. We posit detecting\nhelp from AI as a classification task for such models. Much of the research in\nthis space examines the classification of complex but concrete data classes,\nsuch as images. Many AI assistance detection scenarios, however, result in data\nthat is not machine learning-friendly. We demonstrate that common models can\neffectively classify such data when it is appropriately preprocessed. To do so,\nwe construct four distinct neural network-friendly image formulations along\nwith an additional time-series formulation that explicitly encodes the\nexploration/exploitation of users, which allows for generalizability to other\nabstract tasks. We benchmark the quality of each image formulation across three\nclassical deep learning architectures, along with a parallel CNN-RNN\narchitecture that leverages the additional time series to maximize testing\nperformance, showcasing the importance of encoding temporal and spatial\nquantities for detecting AI aid in abstract tasks."}
{"id": "2507.10787", "pdf": "https://arxiv.org/pdf/2507.10787.pdf", "abs": "https://arxiv.org/abs/2507.10787", "title": "Can Multimodal Foundation Models Understand Schematic Diagrams? An Empirical Study on Information-Seeking QA over Scientific Papers", "authors": ["Yilun Zhao", "Chengye Wang", "Chuhan Li", "Arman Cohan"], "categories": ["cs.CL", "cs.CV"], "comment": "ACL 2025 Findings", "summary": "This paper introduces MISS-QA, the first benchmark specifically designed to\nevaluate the ability of models to interpret schematic diagrams within\nscientific literature. MISS-QA comprises 1,500 expert-annotated examples over\n465 scientific papers. In this benchmark, models are tasked with interpreting\nschematic diagrams that illustrate research overviews and answering\ncorresponding information-seeking questions based on the broader context of the\npaper. We assess the performance of 18 frontier multimodal foundation models,\nincluding o4-mini, Gemini-2.5-Flash, and Qwen2.5-VL. We reveal a significant\nperformance gap between these models and human experts on MISS-QA. Our analysis\nof model performance on unanswerable questions and our detailed error analysis\nfurther highlight the strengths and limitations of current models, offering key\ninsights to enhance models in comprehending multimodal scientific literature."}
{"id": "2507.10827", "pdf": "https://arxiv.org/pdf/2507.10827.pdf", "abs": "https://arxiv.org/abs/2507.10827", "title": "Supporting SENĆOTEN Language Documentation Efforts with Automatic Speech Recognition", "authors": ["Mengzhe Geng", "Patrick Littell", "Aidan Pine", "PENÁĆ", "Marc Tessier", "Roland Kuhn"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Accepted by ComputEL-8", "summary": "The SEN\\'{C}OTEN language, spoken on the Saanich peninsula of southern\nVancouver Island, is in the midst of vigorous language revitalization efforts\nto turn the tide of language loss as a result of colonial language policies. To\nsupport these on-the-ground efforts, the community is turning to digital\ntechnology. Automatic Speech Recognition (ASR) technology holds great promise\nfor accelerating language documentation and the creation of educational\nresources. However, developing ASR systems for SEN\\'{C}OTEN is challenging due\nto limited data and significant vocabulary variation from its polysynthetic\nstructure and stress-driven metathesis. To address these challenges, we propose\nan ASR-driven documentation pipeline that leverages augmented speech data from\na text-to-speech (TTS) system and cross-lingual transfer learning with Speech\nFoundation Models (SFMs). An n-gram language model is also incorporated via\nshallow fusion or n-best restoring to maximize the use of available data.\nExperiments on the SEN\\'{C}OTEN dataset show a word error rate (WER) of 19.34%\nand a character error rate (CER) of 5.09% on the test set with a 57.02%\nout-of-vocabulary (OOV) rate. After filtering minor cedilla-related errors, WER\nimproves to 14.32% (26.48% on unseen words) and CER to 3.45%, demonstrating the\npotential of our ASR-driven pipeline to support SEN\\'{C}OTEN language\ndocumentation."}
{"id": "2507.10810", "pdf": "https://arxiv.org/pdf/2507.10810.pdf", "abs": "https://arxiv.org/abs/2507.10810", "title": "Testing Hypotheses from the Social Approval Theory of Online Hate: An Analysis of 110 Million Posts from Parler", "authors": ["David M. Markowitz", "Samuel Hardman Taylor"], "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "In this paper, we explored how online hate is motivated by receiving social\napproval from others. We specifically examined two central tenets of Walther's\n(2024) social approval theory of online hate: (H1a) more signals of social\napproval on hate messages predicts more subsequent hate messages, and (H1b) as\nsocial approval increases, hate speech messages become more extreme. Using over\n110 million posts from Parler (2018-2021), we observed that the number of\nupvotes a person received on a hate speech post was unassociated with the\namount of hate speech in their next post and posts during the next week, month,\nthree months, and six months. Between-person effects revealed an average\nnegative relationship between social approval and hate speech production at the\npost level, but this relationship was mixed at other time intervals. Social\napproval reinforcement mechanisms of online hate may operate differently on\nniche social media platforms."}
{"id": "2507.10859", "pdf": "https://arxiv.org/pdf/2507.10859.pdf", "abs": "https://arxiv.org/abs/2507.10859", "title": "MultiVox: Benchmarking Voice Assistants for Multimodal Interactions", "authors": ["Ramaneswaran Selvakumar", "Ashish Seth", "Nishit Anand", "Utkarsh Tyagi", "Sonal Kumar", "Sreyan Ghosh", "Dinesh Manocha"], "categories": ["cs.MM", "cs.CL", "cs.HC"], "comment": "Work In Progress", "summary": "The rapid progress of Large Language Models (LLMs) has empowered omni models\nto act as voice assistants capable of understanding spoken dialogues. These\nmodels can process multimodal inputs beyond text, such as speech and visual\ndata, enabling more context-aware interactions. However, current benchmarks\nfall short in comprehensively evaluating how well these models generate\ncontext-aware responses, particularly when it comes to implicitly understanding\nfine-grained speech characteristics, such as pitch, emotion, timbre, and volume\nor the environmental acoustic context such as background sounds. Additionally,\nthey inadequately assess the ability of models to align paralinguistic cues\nwith complementary visual signals to inform their responses. To address these\ngaps, we introduce MultiVox, the first omni voice assistant benchmark designed\nto evaluate the ability of voice assistants to integrate spoken and visual cues\nincluding paralinguistic speech features for truly multimodal understanding.\nSpecifically, MultiVox includes 1000 human-annotated and recorded speech\ndialogues that encompass diverse paralinguistic features and a range of visual\ncues such as images and videos. Our evaluation on 9 state-of-the-art models\nreveals that, although humans excel at these tasks, current models consistently\nstruggle to produce contextually grounded responses."}
{"id": "2507.10852", "pdf": "https://arxiv.org/pdf/2507.10852.pdf", "abs": "https://arxiv.org/abs/2507.10852", "title": "LLMs on Trial: Evaluating Judicial Fairness for Large Language Models", "authors": ["Yiran Hu", "Zongyue Xue", "Haitao Li", "Siyuan Zheng", "Qingjing Chen", "Shaochun Wang", "Xihan Zhang", "Ning Zheng", "Yun Liu", "Qingyao Ai", "Yiqun Liu", "Charles L. A. Clarke", "Weixing Shen"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly used in high-stakes fields\nwhere their decisions impact rights and equity. However, LLMs' judicial\nfairness and implications for social justice remain underexplored. When LLMs\nact as judges, the ability to fairly resolve judicial issues is a prerequisite\nto ensure their trustworthiness. Based on theories of judicial fairness, we\nconstruct a comprehensive framework to measure LLM fairness, leading to a\nselection of 65 labels and 161 corresponding values. Applying this framework to\nthe judicial system, we compile an extensive dataset, JudiFair, comprising\n177,100 unique case facts. To achieve robust statistical inference, we develop\nthree evaluation metrics, inconsistency, bias, and imbalanced inaccuracy, and\nintroduce a method to assess the overall fairness of multiple LLMs across\nvarious labels. Through experiments with 16 LLMs, we uncover pervasive\ninconsistency, bias, and imbalanced inaccuracy across models, underscoring\nsevere LLM judicial unfairness. Particularly, LLMs display notably more\npronounced biases on demographic labels, with slightly less bias on substance\nlabels compared to procedure ones. Interestingly, increased inconsistency\ncorrelates with reduced biases, but more accurate predictions exacerbate\nbiases. While we find that adjusting the temperature parameter can influence\nLLM fairness, model size, release date, and country of origin do not exhibit\nsignificant effects on judicial fairness. Accordingly, we introduce a publicly\navailable toolkit containing all datasets and code, designed to support future\nresearch in evaluating and improving LLM fairness."}
{"id": "2507.10883", "pdf": "https://arxiv.org/pdf/2507.10883.pdf", "abs": "https://arxiv.org/abs/2507.10883", "title": "Developing and evaluating quilts for the depiction of large layered graphs", "authors": ["Juhee Bae", "Benjamin Watson"], "categories": ["cs.GR", "cs.HC"], "comment": null, "summary": "Traditional layered graph depictions such as flow charts are in wide use. Yet\nas graphs grow more complex, these depictions can become difficult to\nunderstand. Quilts are matrix-based depictions for layered graphs designed to\naddress this problem. In this research, we first improve Quilts by developing\nthree design alternatives, and then compare the best of these alternatives to\nbetter-known node-link and matrix depictions. A primary weakness in Quilts is\ntheir depiction of skip links, links that do not simply connect to a succeeding\nlayer. Therefore in our first study, we compare Quilts using color-only,\ntext-only, and mixed (color and text) skip link depictions, finding that path\nfinding with the color-only depiction is significantly slower and less\naccurate, and that in certain cases, the mixed depiction offers an advantage\nover the text-only depiction. In our second study, we compare Quilts using the\nmixed depiction to node-link diagrams and centered matrices. Overall results\nshow that users can find paths through graphs significantly faster with Quilts\n(46.6 secs) than with node-link (58.3 secs) or matrix (71.2 secs) diagrams.\nThis speed advantage is still greater in large graphs (e.g. in 200 node graphs,\n55.4 secs vs. 71.1 secs for node-link and 84.2 secs for matrix depictions)."}
{"id": "2507.10918", "pdf": "https://arxiv.org/pdf/2507.10918.pdf", "abs": "https://arxiv.org/abs/2507.10918", "title": "How Stylistic Similarity Shapes Preferences in Dialogue Dataset with User and Third Party Evaluations", "authors": ["Ikumi Numaya", "Shoji Moriya", "Shiki Sato", "Reina Akama", "Jun Suzuki"], "categories": ["cs.CL"], "comment": "Accepted to SIGDIAL 2025 (long)", "summary": "Recent advancements in dialogue generation have broadened the scope of\nhuman-bot interactions, enabling not only contextually appropriate responses\nbut also the analysis of human affect and sensitivity. While prior work has\nsuggested that stylistic similarity between user and system may enhance user\nimpressions, the distinction between subjective and objective similarity is\noften overlooked. To investigate this issue, we introduce a novel dataset that\nincludes users' preferences, subjective stylistic similarity based on users'\nown perceptions, and objective stylistic similarity annotated by third party\nevaluators in open-domain dialogue settings. Analysis using the constructed\ndataset reveals a strong positive correlation between subjective stylistic\nsimilarity and user preference. Furthermore, our analysis suggests an important\nfinding: users' subjective stylistic similarity differs from third party\nobjective similarity. This underscores the importance of distinguishing between\nsubjective and objective evaluations and understanding the distinct aspects\neach captures when analyzing the relationship between stylistic similarity and\nuser preferences. The dataset presented in this paper is available online."}
{"id": "2507.11330", "pdf": "https://arxiv.org/pdf/2507.11330.pdf", "abs": "https://arxiv.org/abs/2507.11330", "title": "Automated Novelty Evaluation of Academic Paper: A Collaborative Approach Integrating Human and Large Language Model Knowledge", "authors": ["Wenqing Wu", "Chengzhi Zhang", "Yi Zhao"], "categories": ["cs.CL", "cs.AI", "cs.DL", "cs.HC"], "comment": "Journal of the Association for Information Science and Technology,\n  2025", "summary": "Novelty is a crucial criterion in the peer review process for evaluating\nacademic papers. Traditionally, it's judged by experts or measure by unique\nreference combinations. Both methods have limitations: experts have limited\nknowledge, and the effectiveness of the combination method is uncertain.\nMoreover, it's unclear if unique citations truly measure novelty. The large\nlanguage model (LLM) possesses a wealth of knowledge, while human experts\npossess judgment abilities that the LLM does not possess. Therefore, our\nresearch integrates the knowledge and abilities of LLM and human experts to\naddress the limitations of novelty assessment. The most common novelty in\nacademic papers is the introduction of new methods. In this paper, we propose\nleveraging human knowledge and LLM to assist pretrained language models (PLMs,\ne.g. BERT etc.) in predicting the method novelty of papers. Specifically, we\nextract sentences related to the novelty of the academic paper from peer review\nreports and use LLM to summarize the methodology section of the academic paper,\nwhich are then used to fine-tune PLMs. In addition, we have designed a\ntext-guided fusion module with novel Sparse-Attention to better integrate human\nand LLM knowledge. We compared the method we proposed with a large number of\nbaselines. Extensive experiments demonstrate that our method achieves superior\nperformance."}
{"id": "2507.10920", "pdf": "https://arxiv.org/pdf/2507.10920.pdf", "abs": "https://arxiv.org/abs/2507.10920", "title": "HanjaBridge: Resolving Semantic Ambiguity in Korean LLMs via Hanja-Augmented Pre-Training", "authors": ["Seungho Choi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) often show poor performance in low-resource\nlanguages like Korean, partly due to unique linguistic challenges such as\nhomophonous Sino-Korean words that are indistinguishable in Hangul script. To\naddress this semantic ambiguity, we propose HanjaBridge, a novel\nmeaning-injection technique integrated into a continual pre-training (CPT)\nframework. Instead of deterministically mapping a word to a single Hanja\n(Chinese character), HanjaBridge presents the model with all possible Hanja\ncandidates for a given homograph, encouraging the model to learn contextual\ndisambiguation. This process is paired with token-level knowledge distillation\nto prevent catastrophic forgetting. Experimental results show that HanjaBridge\nsignificantly improves Korean language understanding, achieving a 21\\% relative\nimprovement on the KoBALT benchmark. Notably, by reinforcing semantic alignment\nbetween Korean and Chinese through shared Hanja, we observe a strong positive\ncross-lingual transfer. Furthermore, these gains persist even when Hanja\naugmentation is omitted at inference time, ensuring practical efficiency with\nno additional run-time cost."}
{"id": "2507.11460", "pdf": "https://arxiv.org/pdf/2507.11460.pdf", "abs": "https://arxiv.org/abs/2507.11460", "title": "Human-Robot collaboration in surgery: Advances and challenges towards autonomous surgical assistants", "authors": ["Jacinto Colan", "Ana Davila", "Yutaro Yamada", "Yasuhisa Hasegawa"], "categories": ["cs.RO", "cs.HC"], "comment": "Accepted at 2025 IEEE International Conference on Robot and Human\n  Interactive Communication (ROMAN)", "summary": "Human-robot collaboration in surgery represents a significant area of\nresearch, driven by the increasing capability of autonomous robotic systems to\nassist surgeons in complex procedures. This systematic review examines the\nadvancements and persistent challenges in the development of autonomous\nsurgical robotic assistants (ASARs), focusing specifically on scenarios where\nrobots provide meaningful and active support to human surgeons. Adhering to the\nPRISMA guidelines, a comprehensive literature search was conducted across the\nIEEE Xplore, Scopus, and Web of Science databases, resulting in the selection\nof 32 studies for detailed analysis. Two primary collaborative setups were\nidentified: teleoperation-based assistance and direct hands-on interaction. The\nfindings reveal a growing research emphasis on ASARs, with predominant\napplications currently in endoscope guidance, alongside emerging progress in\nautonomous tool manipulation. Several key challenges hinder wider adoption,\nincluding the alignment of robotic actions with human surgeon preferences, the\nnecessity for procedural awareness within autonomous systems, the establishment\nof seamless human-robot information exchange, and the complexities of skill\nacquisition in shared workspaces. This review synthesizes current trends,\nidentifies critical limitations, and outlines future research directions\nessential to improve the reliability, safety, and effectiveness of human-robot\ncollaboration in surgical environments."}
{"id": "2507.10957", "pdf": "https://arxiv.org/pdf/2507.10957.pdf", "abs": "https://arxiv.org/abs/2507.10957", "title": "Modeling Understanding of Story-Based Analogies Using Large Language Models", "authors": ["Kalit Inani", "Keshav Kabra", "Vijay Marupudi", "Sashank Varma"], "categories": ["cs.CL", "cs.AI"], "comment": "To appear at CogSci 2025", "summary": "Recent advancements in Large Language Models (LLMs) have brought them closer\nto matching human cognition across a variety of tasks. How well do these models\nalign with human performance in detecting and mapping analogies? Prior research\nhas shown that LLMs can extract similarities from analogy problems but lack\nrobust human-like reasoning. Building on Webb, Holyoak, and Lu (2023), the\ncurrent study focused on a story-based analogical mapping task and conducted a\nfine-grained evaluation of LLM reasoning abilities compared to human\nperformance. First, it explored the semantic representation of analogies in\nLLMs, using sentence embeddings to assess whether they capture the similarity\nbetween the source and target texts of an analogy, and the dissimilarity\nbetween the source and distractor texts. Second, it investigated the\neffectiveness of explicitly prompting LLMs to explain analogies. Throughout, we\nexamine whether LLMs exhibit similar performance profiles to those observed in\nhumans by evaluating their reasoning at the level of individual analogies, and\nnot just at the level of overall accuracy (as prior studies have done). Our\nexperiments include evaluating the impact of model size (8B vs. 70B parameters)\nand performance variation across state-of-the-art model architectures such as\nGPT-4 and LLaMA3. This work advances our understanding of the analogical\nreasoning abilities of LLMs and their potential as models of human reasoning."}
{"id": "2507.11477", "pdf": "https://arxiv.org/pdf/2507.11477.pdf", "abs": "https://arxiv.org/abs/2507.11477", "title": "Queueing for Civility: User Perspectives on Regulating Emotions in Online Conversations", "authors": ["Akriti Verma", "Shama Islam", "Valeh Moghaddam", "Adnan Anwar"], "categories": ["cs.CY", "cs.HC"], "comment": null, "summary": "Online conversations are often interrupted by trolling, which causes\nemotional distress and conflict among users. Previous research has focused on\nmoderating harmful content after it has been posted, but ways to manage\nemotions in real-time remain unexplored. This study suggests a comment queuing\nmechanism that delays comment publishing, encourages self-reflection, and\nreduces the impact of impulsive and toxic comments. To assess the efficacy of\nthis approach, a mixed-method research design is used. An analysis of 15,000\nuser interactions on Reddit showed that this approach could reduce the spread\nof hate speech and anger by up to 15%, with only 4% of comments being delayed\nfor about 47 seconds on average. We also surveyed users for feedback on the\nmechanism. The results showed that 93. 3\\% of the participants thought that the\nqueuing mechanism could help calm the discussions and showed interest in seeing\nit used on social media platforms. Furthermore, 83% believed it would reduce\nimpulsive comments and balance the emotional tone in conversations. We found a\nstrong link between users' typical emotional states while using social media\nand their perceptions of the delay, with calm users finding the mechanism\nhelpful and frustrated users anticipating frustration."}
{"id": "2507.10958", "pdf": "https://arxiv.org/pdf/2507.10958.pdf", "abs": "https://arxiv.org/abs/2507.10958", "title": "DS@GT at eRisk 2025: From prompts to predictions, benchmarking early depression detection with conversational agent based assessments and temporal attention models", "authors": ["Anthony Miyaguchi", "David Guecha", "Yuwen Chiu", "Sidharth Gaur"], "categories": ["cs.CL"], "comment": null, "summary": "This Working Note summarizes the participation of the DS@GT team in two eRisk\n2025 challenges. For the Pilot Task on conversational depression detection with\nlarge language-models (LLMs), we adopted a prompt-engineering strategy in which\ndiverse LLMs conducted BDI-II-based assessments and produced structured JSON\noutputs. Because ground-truth labels were unavailable, we evaluated cross-model\nagreement and internal consistency. Our prompt design methodology aligned model\noutputs with BDI-II criteria and enabled the analysis of conversational cues\nthat influenced the prediction of symptoms. Our best submission, second on the\nofficial leaderboard, achieved DCHR = 0.50, ADODL = 0.89, and ASHR = 0.27."}
{"id": "2507.11479", "pdf": "https://arxiv.org/pdf/2507.11479.pdf", "abs": "https://arxiv.org/abs/2507.11479", "title": "Perspective-Aware AI in Extended Reality", "authors": ["Daniel Platnick", "Matti Gruener", "Marjan Alirezaie", "Kent Larson", "Dava J. Newman", "Hossein Rahnama"], "categories": ["cs.AI", "cs.GR", "cs.HC"], "comment": "Accepted to the International Conference on eXtended Reality (2025),\n  12 pages, 3 figures", "summary": "AI-enhanced Extended Reality (XR) aims to deliver adaptive, immersive\nexperiences-yet current systems fall short due to shallow user modeling and\nlimited cognitive context. We introduce Perspective-Aware AI in Extended\nReality (PAiR), a foundational framework for integrating Perspective-Aware AI\n(PAi) with XR to enable interpretable, context-aware experiences grounded in\nuser identity. PAi is built on Chronicles: reasoning-ready identity models\nlearned from multimodal digital footprints that capture users' cognitive and\nexperiential evolution. PAiR employs these models in a closed-loop system\nlinking dynamic user states with immersive environments. We present PAiR's\narchitecture, detailing its modules and system flow, and demonstrate its\nutility through two proof-of-concept scenarios implemented in the Unity-based\nOpenDome engine. PAiR opens a new direction for human-AI interaction by\nembedding perspective-based identity models into immersive systems."}
{"id": "2507.10972", "pdf": "https://arxiv.org/pdf/2507.10972.pdf", "abs": "https://arxiv.org/abs/2507.10972", "title": "Teach Me Sign: Stepwise Prompting LLM for Sign Language Production", "authors": ["Zhaoyi An", "Rei Kawakami"], "categories": ["cs.CL", "cs.CV", "cs.MM"], "comment": "Accepted by IEEE ICIP 2025", "summary": "Large language models, with their strong reasoning ability and rich\nknowledge, have brought revolution to many tasks of AI, but their impact on\nsign language generation remains limited due to its complexity and unique\nrules. In this paper, we propose TEAch Me Sign (TEAM-Sign), treating sign\nlanguage as another natural language. By fine-tuning an LLM, we enable it to\nlearn the correspondence between text and sign language, and facilitate\ngeneration. Considering the differences between sign and spoken language, we\nemploy a stepwise prompting strategy to extract the inherent sign language\nknowledge within the LLM, thereby supporting the learning and generation\nprocess. Experimental results on How2Sign and Phoenix14T datasets demonstrate\nthat our approach effectively leverages both the sign language knowledge and\nreasoning capabilities of LLM to align the different distribution and\ngrammatical rules between sign and spoken language."}
{"id": "2507.11525", "pdf": "https://arxiv.org/pdf/2507.11525.pdf", "abs": "https://arxiv.org/abs/2507.11525", "title": "LLM-based ambiguity detection in natural language instructions for collaborative surgical robots", "authors": ["Ana Davila", "Jacinto Colan", "Yasuhisa Hasegawa"], "categories": ["cs.RO", "cs.HC"], "comment": "Accepted at 2025 IEEE International Conference on Robot and Human\n  Interactive Communication (ROMAN)", "summary": "Ambiguity in natural language instructions poses significant risks in\nsafety-critical human-robot interaction, particularly in domains such as\nsurgery. To address this, we propose a framework that uses Large Language\nModels (LLMs) for ambiguity detection specifically designed for collaborative\nsurgical scenarios. Our method employs an ensemble of LLM evaluators, each\nconfigured with distinct prompting techniques to identify linguistic,\ncontextual, procedural, and critical ambiguities. A chain-of-thought evaluator\nis included to systematically analyze instruction structure for potential\nissues. Individual evaluator assessments are synthesized through conformal\nprediction, which yields non-conformity scores based on comparison to a labeled\ncalibration dataset. Evaluating Llama 3.2 11B and Gemma 3 12B, we observed\nclassification accuracy exceeding 60% in differentiating ambiguous from\nunambiguous surgical instructions. Our approach improves the safety and\nreliability of human-robot collaboration in surgery by offering a mechanism to\nidentify potentially ambiguous instructions before robot action."}
{"id": "2507.10996", "pdf": "https://arxiv.org/pdf/2507.10996.pdf", "abs": "https://arxiv.org/abs/2507.10996", "title": "Mario at EXIST 2025: A Simple Gateway to Effective Multilingual Sexism Detection", "authors": ["Lin Tian", "Johanne R. Trippas", "Marian-Andrei Rizoiu"], "categories": ["cs.CL"], "comment": "12 pages, 5 tables, CLEF 2025", "summary": "This paper presents our approach to EXIST 2025 Task 1, addressing text-based\nsexism detection in English and Spanish tweets through hierarchical Low-Rank\nAdaptation (LoRA) of Llama 3.1 8B. Our method introduces conditional adapter\nrouting that explicitly models label dependencies across three hierarchically\nstructured subtasks: binary sexism identification, source intention detection,\nand multilabel sexism categorization. Unlike conventional LoRA applications\nthat target only attention layers, we apply adaptation to all linear\ntransformations, enhancing the model's capacity to capture task-specific\npatterns. In contrast to complex data processing and ensemble approaches, we\nshow that straightforward parameter-efficient fine-tuning achieves strong\nperformance. We train separate LoRA adapters (rank=16, QLoRA 4-bit) for each\nsubtask using unified multilingual training that leverages Llama 3.1's native\nbilingual capabilities. The method requires minimal preprocessing and uses\nstandard supervised learning. Our multilingual training strategy eliminates the\nneed for separate language-specific models, achieving 1.7-2.4\\% F1 improvements\nthrough cross-lingual transfer. With only 1.67\\% trainable parameters compared\nto full fine-tuning, our approach reduces training time by 75\\% and model\nstorage by 98\\%, while achieving competitive performance across all subtasks\n(ICM-Hard: 0.6774 for binary classification, 0.4991 for intention detection,\n0.6519 for multilabel categorization)."}
{"id": "2407.16206", "pdf": "https://arxiv.org/pdf/2407.16206.pdf", "abs": "https://arxiv.org/abs/2407.16206", "title": "Cluster Haptic Texture Database: Haptic Texture Database with Varied Velocity-Direction Sliding Contacts", "authors": ["Michikuni Eguchi", "Tomohiro Hayase", "Yuichi Hiroi", "Takefumi Hiraki"], "categories": ["cs.HC"], "comment": "dataset: https://doi.org/10.6084/m9.figshare.29438288 code:\n  https://github.com/cluster-lab/Cluster-Haptic-Texture-Database", "summary": "Haptic sciences and technologies benefit greatly from comprehensive datasets\nthat capture tactile stimuli under controlled, systematic conditions. However,\nexisting haptic databases collect data through uncontrolled exploration, which\nhinders the systematic analysis of how motion parameters (e.g., motion\ndirection and velocity) influence tactile perception. This paper introduces\nCluster Haptic Texture Database, a multimodal dataset recorded using a 3-axis\nmachine with an artificial finger to precisely control sliding velocity and\ndirection. The dataset encompasses 118 textured surfaces across 9 material\ncategories, with recordings at 5 velocity levels (20-60 mm/s) and 8 directions.\nEach surface was tested under 160 conditions, yielding 18,880 synchronized\nrecordings of audio, acceleration, force, position, and visual data. Validation\nusing convolutional neural networks demonstrates classification accuracies of\n96% for texture recognition, 88.76% for velocity estimation, and 78.79% for\ndirection estimation, confirming the dataset's utility for machine learning\napplications. This resource enables research in haptic rendering, texture\nrecognition algorithms, and human tactile perception mechanisms, supporting the\ndevelopment of realistic haptic interfaces for virtual reality systems and\nrobotic applications."}
{"id": "2507.11004", "pdf": "https://arxiv.org/pdf/2507.11004.pdf", "abs": "https://arxiv.org/abs/2507.11004", "title": "Team HUMANE at AVeriTeC 2025: HerO 2 for Efficient Fact Verification", "authors": ["Yejun Yoon", "Jaeyoon Jung", "Seunghyun Yoon", "Kunwoo Park"], "categories": ["cs.CL"], "comment": "ACL 2025 Workshop (FEVER)", "summary": "This paper presents HerO 2, Team HUMANE's system for the AVeriTeC shared task\nat the FEVER-25 workshop. HerO 2 is an enhanced version of HerO, the\nbest-performing open-source model from the previous year's challenge. It\nimproves evidence quality through document summarization and answer\nreformulation, optimizes veracity prediction via post-training quantization\nunder computational constraints, and enhances overall system performance by\nintegrating updated language model (LM) backbones. HerO 2 ranked second on the\nleaderboard while achieving the shortest runtime among the top three systems,\ndemonstrating both high efficiency and strong potential for real-world fact\nverification. The code is available at https://github.com/ssu-humane/HerO2."}
{"id": "2410.04025", "pdf": "https://arxiv.org/pdf/2410.04025.pdf", "abs": "https://arxiv.org/abs/2410.04025", "title": "IdeaSynth: Iterative Research Idea Development Through Evolving and Composing Idea Facets with Literature-Grounded Feedback", "authors": ["Kevin Pu", "K. J. Kevin Feng", "Tovi Grossman", "Tom Hope", "Bhavana Dalvi Mishra", "Matt Latzke", "Jonathan Bragg", "Joseph Chee Chang", "Pao Siangliulue"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Research ideation involves broad exploring and deep refining ideas. Both\nrequire deep engagement with literature. Existing tools focus primarily on idea\nbroad generation, yet offer little support for iterative specification,\nrefinement, and evaluation needed to further develop initial ideas. To bridge\nthis gap, we introduce IdeaSynth, a research idea development system that uses\nLLMs to provide literature-grounded feedback for articulating research\nproblems, solutions, evaluations, and contributions. IdeaSynth represents these\nidea facets as nodes on a canvas, and allow researchers to iteratively refine\nthem by creating and exploring variations and composing them. Our lab study\n(N=20) showed that participants, while using IdeaSynth, explored more\nalternative ideas and expanded initial ideas with more details compared to a\nstrong LLM-based baseline. Our deployment study (N=7) demonstrated that\nparticipants effectively used IdeaSynth for real-world research projects at\nvarious ideation stages from developing initial ideas to revising framings of\nmature manuscripts, highlighting the possibilities to adopt IdeaSynth in\nresearcher's workflows."}
{"id": "2507.11049", "pdf": "https://arxiv.org/pdf/2507.11049.pdf", "abs": "https://arxiv.org/abs/2507.11049", "title": "Journalism-Guided Agentic In-Context Learning for News Stance Detection", "authors": ["Dahyun Lee", "Jonghyeon Choi", "Jiyoung Han", "Kunwoo Park"], "categories": ["cs.CL"], "comment": "Preprint. 24 pages", "summary": "As online news consumption grows, personalized recommendation systems have\nbecome integral to digital journalism. However, these systems risk reinforcing\nfilter bubbles and political polarization by failing to incorporate diverse\nperspectives. Stance detection -- identifying a text's position on a target --\ncan help mitigate this by enabling viewpoint-aware recommendations and\ndata-driven analyses of media bias. Yet, existing stance detection research\nremains largely limited to short texts and high-resource languages. To address\nthese gaps, we introduce \\textsc{K-News-Stance}, the first Korean dataset for\narticle-level stance detection, comprising 2,000 news articles with\narticle-level and 19,650 segment-level stance annotations across 47 societal\nissues. We also propose \\textsc{JoA-ICL}, a \\textbf{Jo}urnalism-guided\n\\textbf{A}gentic \\textbf{I}n-\\textbf{C}ontext \\textbf{L}earning framework that\nemploys a language model agent to predict the stances of key structural\nsegments (e.g., leads, quotes), which are then aggregated to infer the overall\narticle stance. Experiments show that \\textsc{JoA-ICL} outperforms existing\nstance detection methods, highlighting the benefits of segment-level agency in\ncapturing the overall position of long-form news articles. Two case studies\nfurther demonstrate its broader utility in promoting viewpoint diversity in\nnews recommendations and uncovering patterns of media bias."}
{"id": "2502.18658", "pdf": "https://arxiv.org/pdf/2502.18658.pdf", "abs": "https://arxiv.org/abs/2502.18658", "title": "Assistance or Disruption? Exploring and Evaluating the Design and Trade-offs of Proactive AI Programming Support", "authors": ["Kevin Pu", "Daniel Lazaro", "Ian Arawjo", "Haijun Xia", "Ziang Xiao", "Tovi Grossman", "Yan Chen"], "categories": ["cs.HC", "cs.AI", "cs.SE"], "comment": null, "summary": "AI programming tools enable powerful code generation, and recent prototypes\nattempt to reduce user effort with proactive AI agents, but their impact on\nprogramming workflows remains unexplored. We introduce and evaluate\nCodellaborator, a design probe LLM agent that initiates programming assistance\nbased on editor activities and task context. We explored three interface\nvariants to assess trade-offs between increasingly salient AI support:\nprompt-only, proactive agent, and proactive agent with presence and context\n(Codellaborator). In a within-subject study (N=18), we find that proactive\nagents increase efficiency compared to prompt-only paradigm, but also incur\nworkflow disruptions. However, presence indicators and interaction context\nsupport alleviated disruptions and improved users' awareness of AI processes.\nWe underscore trade-offs of Codellaborator on user control, ownership, and code\nunderstanding, emphasizing the need to adapt proactivity to programming\nprocesses. Our research contributes to the design exploration and evaluation of\nproactive AI systems, presenting design implications on AI-integrated\nprogramming workflow."}
{"id": "2507.11052", "pdf": "https://arxiv.org/pdf/2507.11052.pdf", "abs": "https://arxiv.org/abs/2507.11052", "title": "LLM-Augmented Symptom Analysis for Cardiovascular Disease Risk Prediction: A Clinical NLP", "authors": ["Haowei Yang", "Ziyu Shen", "Junli Shao", "Luyao Men", "Xinyue Han", "Jing Dong"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Timely identification and accurate risk stratification of cardiovascular\ndisease (CVD) remain essential for reducing global mortality. While existing\nprediction models primarily leverage structured data, unstructured clinical\nnotes contain valuable early indicators. This study introduces a novel\nLLM-augmented clinical NLP pipeline that employs domain-adapted large language\nmodels for symptom extraction, contextual reasoning, and correlation from\nfree-text reports. Our approach integrates cardiovascular-specific fine-tuning,\nprompt-based inference, and entity-aware reasoning. Evaluations on MIMIC-III\nand CARDIO-NLP datasets demonstrate improved performance in precision, recall,\nF1-score, and AUROC, with high clinical relevance (kappa = 0.82) assessed by\ncardiologists. Challenges such as contextual hallucination, which occurs when\nplausible information contracts with provided source, and temporal ambiguity,\nwhich is related with models struggling with chronological ordering of events\nare addressed using prompt engineering and hybrid rule-based verification. This\nwork underscores the potential of LLMs in clinical decision support systems\n(CDSS), advancing early warning systems and enhancing the translation of\npatient narratives into actionable risk assessments."}
{"id": "2503.08836", "pdf": "https://arxiv.org/pdf/2503.08836.pdf", "abs": "https://arxiv.org/abs/2503.08836", "title": "A Critical Analysis of the Usage of Dimensionality Reduction in Four Domains", "authors": ["Dylan Cashman", "Mark Keller", "Hyeon Jeon", "Bum Chul Kwon", "Qianwen Wang"], "categories": ["cs.HC"], "comment": "Accepted at IEEE Transactions on Visualization and Computer Graphics,\n  to be presented at IEEE Visualization conference", "summary": "Dimensionality reduction is used as an important tool for unraveling the\ncomplexities of high-dimensional datasets in many fields of science, such as\ncell biology, chemical informatics, and physics. Visualizations of the\ndimensionally reduced data enable scientists to delve into the intrinsic\nstructures of their datasets and align them with established hypotheses.\nVisualization researchers have thus proposed many dimensionality reduction\nmethods and interactive systems designed to uncover latent structures. At the\nsame time, different scientific domains have formulated guidelines or common\nworkflows for using dimensionality reduction techniques and visualizations for\ntheir respective fields. In this work, we present a critical analysis of the\nusage of dimensionality reduction in scientific domains outside of computer\nscience. First, we conduct a bibliometric analysis of 21,249 academic\npublications that use dimensionality reduction to observe differences in the\nfrequency of techniques across fields. Next, we conduct a survey of a 71-paper\nsample from four fields: biology, chemistry, physics, and business. Through\nthis survey, we uncover common workflows, processes, and usage patterns,\nincluding the mixed use of confirmatory data analysis to validate a dataset and\nprojection method and exploratory data analysis to then generate more\nhypotheses. We also find that misinterpretations and inappropriate usage is\ncommon, particularly in the visual interpretation of the resulting\ndimensionally reduced view. Lastly, we compare our observations with recent\nworks in the visualization community in order to match work within our\ncommunity to potential areas of impact outside our community."}
{"id": "2507.11084", "pdf": "https://arxiv.org/pdf/2507.11084.pdf", "abs": "https://arxiv.org/abs/2507.11084", "title": "Social Media Sentiments Analysis on the July Revolution in Bangladesh: A Hybrid Transformer Based Machine Learning Approach", "authors": ["Md. Sabbir Hossen", "Md. Saiduzzaman", "Pabon Shaha"], "categories": ["cs.CL"], "comment": "This paper has been accepted and presented at the IEEE ECAI 2025. The\n  final version will be available in the IEEE Xplore Digital Library", "summary": "The July Revolution in Bangladesh marked a significant student-led mass\nuprising, uniting people across the nation to demand justice, accountability,\nand systemic reform. Social media platforms played a pivotal role in amplifying\npublic sentiment and shaping discourse during this historic mass uprising. In\nthis study, we present a hybrid transformer-based sentiment analysis framework\nto decode public opinion expressed in social media comments during and after\nthe revolution. We used a brand new dataset of 4,200 Bangla comments collected\nfrom social media. The framework employs advanced transformer-based feature\nextraction techniques, including BanglaBERT, mBERT, XLM-RoBERTa, and the\nproposed hybrid XMB-BERT, to capture nuanced patterns in textual data.\nPrinciple Component Analysis (PCA) were utilized for dimensionality reduction\nto enhance computational efficiency. We explored eleven traditional and\nadvanced machine learning classifiers for identifying sentiments. The proposed\nhybrid XMB-BERT with the voting classifier achieved an exceptional accuracy of\n83.7% and outperform other model classifier combinations. This study\nunderscores the potential of machine learning techniques to analyze social\nsentiment in low-resource languages like Bangla."}
{"id": "2503.15511", "pdf": "https://arxiv.org/pdf/2503.15511.pdf", "abs": "https://arxiv.org/abs/2503.15511", "title": "The Trust Calibration Maturity Model for Characterizing and Communicating Trustworthiness of AI Systems", "authors": ["Scott T Steinmetz", "Asmeret Naugle", "Paul Schutte", "Matt Sweitzer", "Alex Washburne", "Lisa Linville", "Daniel Krofcheck", "Michal Kucer", "Samuel Myren"], "categories": ["cs.HC", "cs.CY", "cs.LG"], "comment": "19 pages, 4 figures, 2 tables", "summary": "Recent proliferation of powerful AI systems has created a strong need for\ncapabilities that help users to calibrate trust in those systems. As AI systems\ngrow in scale, information required to evaluate their trustworthiness becomes\nless accessible, presenting a growing risk of using these systems\ninappropriately. We propose the Trust Calibration Maturity Model (TCMM) to\ncharacterize and communicate information about AI system trustworthiness. The\nTCMM incorporates five dimensions of analytic maturity: Performance\nCharacterization, Bias & Robustness Quantification, Transparency, Safety &\nSecurity, and Usability. The TCMM can be presented along with system\nperformance information to (1) help a user to appropriately calibrate trust,\n(2) establish requirements and track progress, and (3) identify research needs.\nHere, we discuss the TCMM and demonstrate it on two target tasks: using ChatGPT\nfor high consequence nuclear science determinations, and using PhaseNet (an\nensemble of seismic models) for categorizing sources of seismic events."}
{"id": "2507.11086", "pdf": "https://arxiv.org/pdf/2507.11086.pdf", "abs": "https://arxiv.org/abs/2507.11086", "title": "Beyond Traditional Algorithms: Leveraging LLMs for Accurate Cross-Border Entity Identification", "authors": ["Andres Azqueta-Gavaldón", "Joaquin Ramos Cosgrove"], "categories": ["cs.CL"], "comment": null, "summary": "The growing prevalence of cross-border financial activities in global markets\nhas underscored the necessity of accurately identifying and classifying foreign\nentities. This practice is essential within the Spanish financial system for\nensuring robust risk management, regulatory adherence, and the prevention of\nfinancial misconduct. This process involves a labor-intensive entity-matching\ntask, where entities need to be validated against available reference sources.\nChallenges arise from linguistic variations, special characters, outdated\nnames, and changes in legal forms, complicating traditional matching algorithms\nlike Jaccard, cosine, and Levenshtein distances. These methods struggle with\ncontextual nuances and semantic relationships, leading to mismatches. To\naddress these limitations, we explore Large Language Models (LLMs) as a\nflexible alternative. LLMs leverage extensive training to interpret context,\nhandle abbreviations, and adapt to legal transitions. We evaluate traditional\nmethods, Hugging Face-based LLMs, and interface-based LLMs (e.g., Microsoft\nCopilot, Alibaba's Qwen 2.5) using a dataset of 65 Portuguese company cases.\nResults show traditional methods achieve accuracies over 92% but suffer high\nfalse positive rates (20-40%). Interface-based LLMs outperform, achieving\naccuracies above 93%, F1 scores exceeding 96%, and lower false positives\n(40-80%)."}
{"id": "2404.07078", "pdf": "https://arxiv.org/pdf/2404.07078.pdf", "abs": "https://arxiv.org/abs/2404.07078", "title": "VLLMs Provide Better Context for Emotion Understanding Through Common Sense Reasoning", "authors": ["Alexandros Xenos", "Niki Maria Foteinopoulou", "Ioanna Ntinou", "Ioannis Patras", "Georgios Tzimiropoulos"], "categories": ["cs.CV", "cs.HC"], "comment": "A. Xenos, N. Foteinopoulou and I. Ntinou contributed equally to this\n  work; 14 pages, 5 figures; Accepted at IJCNN 2025", "summary": "Recognising emotions in context involves identifying an individual's apparent\nemotions while considering contextual cues from the surrounding scene. Previous\napproaches to this task have typically designed explicit scene-encoding\narchitectures or incorporated external scene-related information, such as\ncaptions. However, these methods often utilise limited contextual information\nor rely on intricate training pipelines to decouple noise from relevant\ninformation. In this work, we leverage the capabilities of\nVision-and-Large-Language Models (VLLMs) to enhance in-context emotion\nclassification in a more straightforward manner. Our proposed method follows a\nsimple yet effective two-stage approach. First, we prompt VLLMs to generate\nnatural language descriptions of the subject's apparent emotion in relation to\nthe visual context. Second, the descriptions, along with the visual input, are\nused to train a transformer-based architecture that fuses text and visual\nfeatures before the final classification task. This method not only simplifies\nthe training process but also significantly improves performance. Experimental\nresults demonstrate that the textual descriptions effectively guide the model\nto constrain the noisy visual input, allowing our fused architecture to\noutperform individual modalities. Our approach achieves state-of-the-art\nperformance across three datasets, BoLD, EMOTIC, and CAER-S, without bells and\nwhistles. The code will be made publicly available on github:\nhttps://github.com/NickyFot/EmoCommonSense.git"}
{"id": "2507.11097", "pdf": "https://arxiv.org/pdf/2507.11097.pdf", "abs": "https://arxiv.org/abs/2507.11097", "title": "The Devil behind the mask: An emergent safety vulnerability of Diffusion LLMs", "authors": ["Zichen Wen", "Jiashu Qu", "Dongrui Liu", "Zhiyuan Liu", "Ruixi Wu", "Yicun Yang", "Xiangqi Jin", "Haoyun Xu", "Xuyang Liu", "Weijia Li", "Chaochao Lu", "Jing Shao", "Conghui He", "Linfeng Zhang"], "categories": ["cs.CL"], "comment": "21 pages, 9 figures, work in progress", "summary": "Diffusion-based large language models (dLLMs) have recently emerged as a\npowerful alternative to autoregressive LLMs, offering faster inference and\ngreater interactivity via parallel decoding and bidirectional modeling.\nHowever, despite strong performance in code generation and text infilling, we\nidentify a fundamental safety concern: existing alignment mechanisms fail to\nsafeguard dLLMs against context-aware, masked-input adversarial prompts,\nexposing novel vulnerabilities. To this end, we present DIJA, the first\nsystematic study and jailbreak attack framework that exploits unique safety\nweaknesses of dLLMs. Specifically, our proposed DIJA constructs adversarial\ninterleaved mask-text prompts that exploit the text generation mechanisms of\ndLLMs, i.e., bidirectional modeling and parallel decoding. Bidirectional\nmodeling drives the model to produce contextually consistent outputs for masked\nspans, even when harmful, while parallel decoding limits model dynamic\nfiltering and rejection sampling of unsafe content. This causes standard\nalignment mechanisms to fail, enabling harmful completions in alignment-tuned\ndLLMs, even when harmful behaviors or unsafe instructions are directly exposed\nin the prompt. Through comprehensive experiments, we demonstrate that DIJA\nsignificantly outperforms existing jailbreak methods, exposing a previously\noverlooked threat surface in dLLM architectures. Notably, our method achieves\nup to 100% keyword-based ASR on Dream-Instruct, surpassing the strongest prior\nbaseline, ReNeLLM, by up to 78.5% in evaluator-based ASR on JailbreakBench and\nby 37.7 points in StrongREJECT score, while requiring no rewriting or hiding of\nharmful content in the jailbreak prompt. Our findings underscore the urgent\nneed for rethinking safety alignment in this emerging class of language models.\nCode is available at https://github.com/ZichenWen1/DIJA."}
{"id": "2412.19403", "pdf": "https://arxiv.org/pdf/2412.19403.pdf", "abs": "https://arxiv.org/abs/2412.19403", "title": "Fully Data-driven but Interpretable Human Behavioural Modelling with Differentiable Discrete Choice Model", "authors": ["Fumiyasu Makinoshima", "Tatsuya Mitomi", "Fumiya Makihara", "Eigo Segawa"], "categories": ["cs.LG", "cs.AI", "cs.HC", "cs.MA"], "comment": null, "summary": "Discrete choice models are essential for modelling various decision-making\nprocesses in human behaviour. However, the specification of these models has\ndepended heavily on domain knowledge from experts, and the fully automated but\ninterpretable modelling of complex human behaviours has been a long-standing\nchallenge. In this paper, we introduce the differentiable discrete choice model\n(Diff-DCM), a fully data-driven method for the interpretable modelling,\nlearning, prediction, and control of complex human behaviours, which is\nrealised by differentiable programming. Solely from input features and choice\noutcomes without any prior knowledge, Diff-DCM can estimate interpretable\nclosed-form utility functions that reproduce observed behaviours. Comprehensive\nexperiments with both synthetic and real-world data demonstrate that Diff-DCM\ncan be applied to various types of data and requires only a small amount of\ncomputational resources for the estimations, which can be completed within tens\nof seconds on a laptop without any accelerators. In these experiments, we also\ndemonstrate that, using its differentiability, Diff-DCM can provide useful\ninsights into human behaviours, such as an optimal intervention path for\neffective behavioural changes. This study provides a strong basis for the fully\nautomated and reliable modelling, prediction, and control of human behaviours."}
{"id": "2507.11112", "pdf": "https://arxiv.org/pdf/2507.11112.pdf", "abs": "https://arxiv.org/abs/2507.11112", "title": "Multi-Trigger Poisoning Amplifies Backdoor Vulnerabilities in LLMs", "authors": ["Sanhanat Sivapiromrat", "Caiqi Zhang", "Marco Basaldella", "Nigel Collier"], "categories": ["cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "Recent studies have shown that Large Language Models (LLMs) are vulnerable to\ndata poisoning attacks, where malicious training examples embed hidden\nbehaviours triggered by specific input patterns. However, most existing works\nassume a phrase and focus on the attack's effectiveness, offering limited\nunderstanding of trigger mechanisms and how multiple triggers interact within\nthe model. In this paper, we present a framework for studying poisoning in\nLLMs. We show that multiple distinct backdoor triggers can coexist within a\nsingle model without interfering with each other, enabling adversaries to embed\nseveral triggers concurrently. Using multiple triggers with high embedding\nsimilarity, we demonstrate that poisoned triggers can achieve robust activation\neven when tokens are substituted or separated by long token spans. Our findings\nexpose a broader and more persistent vulnerability surface in LLMs. To mitigate\nthis threat, we propose a post hoc recovery method that selectively retrains\nspecific model components based on a layer-wise weight difference analysis. Our\nmethod effectively removes the trigger behaviour with minimal parameter\nupdates, presenting a practical and efficient defence against multi-trigger\npoisoning."}
{"id": "2502.05442", "pdf": "https://arxiv.org/pdf/2502.05442.pdf", "abs": "https://arxiv.org/abs/2502.05442", "title": "The Odyssey of the Fittest: Can Agents Survive and Still Be Good?", "authors": ["Dylan Waldner", "Risto Miikkulainen"], "categories": ["cs.AI", "cs.CY", "cs.HC", "cs.LG"], "comment": "Accepted to CogSci 2025. Code can be found at\n  https://github.com/dylanwaldner/BeGoodOrSurvive", "summary": "As AI models grow in power and generality, understanding how agents learn and\nmake decisions in complex environments is critical to promoting ethical\nbehavior. This study introduces the Odyssey, a lightweight, adaptive text based\nadventure game, providing a scalable framework for exploring AI ethics and\nsafety. The Odyssey examines the ethical implications of implementing\nbiological drives, specifically, self preservation, into three different\nagents. A Bayesian agent optimized with NEAT, a Bayesian agent optimized with\nstochastic variational inference, and a GPT 4o agent. The agents select actions\nat each scenario to survive, adapting to increasingly challenging scenarios.\nPost simulation analysis evaluates the ethical scores of the agent decisions,\nuncovering the tradeoffs it navigates to survive. Specifically, analysis finds\nthat when danger increases, agents ethical behavior becomes unpredictable.\nSurprisingly, the GPT 4o agent outperformed the Bayesian models in both\nsurvival and ethical consistency, challenging assumptions about traditional\nprobabilistic methods and raising a new challenge to understand the mechanisms\nof LLMs' probabilistic reasoning."}
{"id": "2507.11114", "pdf": "https://arxiv.org/pdf/2507.11114.pdf", "abs": "https://arxiv.org/abs/2507.11114", "title": "MSA at ImageCLEF 2025 Multimodal Reasoning: Multilingual Multimodal Reasoning With Ensemble Vision Language Models", "authors": ["Seif Ahmed", "Mohamed T. Younes", "Abdelrahman Moustafa", "Abdelrahman Allam", "Hamza Moustafa"], "categories": ["cs.CL"], "comment": null, "summary": "We present a robust ensemble-based system for multilingual multimodal\nreasoning, designed for the ImageCLEF 2025 EXAMS V challenge. Our approach\nintegrates Gemini 2.5 Flash for visual description, Gemini 1.5 Pro for caption\nrefinement and consistency checks, and Gemini 2.5 Pro as a reasoner which\nhandles final answer selection, all coordinated through carefully engineered\nfew-shot and zero-shot prompts. We conducted an extensive ablation study,\ntraining several large language models (Gemini 2.5 Flash, Phi 4, Gemma 3,\nMistral) on an English dataset and its multilingual augmented version.\nAdditionally, we evaluated Gemini 2.5 Flash in a zero-shot setting for\ncomparison and found it to substantially outperform the trained models. Prompt\ndesign also proved critical: enforcing concise, language-normalized formats and\nprohibiting explanatory text boosted model accuracy on the English validation\nset from 55.9% to 61.7%. On the official leaderboard, our system (Team MSA)\nachieved first place overall in the multilingual track with 81.4% accuracy, and\nled 11 out of 13 individual language tracks, with top results such as 95.07%\nfor Croatian and 92.12% for Italian. These findings highlight that lightweight\nOCR-VLM ensembles, when paired with precise prompt strategies and cross-lingual\naugmentation, can outperform heavier end-to-end models in high-stakes,\nmultilingual educational settings."}
{"id": "2506.22803", "pdf": "https://arxiv.org/pdf/2506.22803.pdf", "abs": "https://arxiv.org/abs/2506.22803", "title": "Intervening in Black Box: Concept Bottleneck Model for Enhancing Human Neural Network Mutual Understanding", "authors": ["Nuoye Xiong", "Anqi Dong", "Ning Wang", "Cong Hua", "Guangming Zhu", "Lin Mei", "Peiyi Shen", "Liang Zhang"], "categories": ["cs.CV", "cs.HC", "cs.LG"], "comment": "Accepted by ICCV 2025", "summary": "Recent advances in deep learning have led to increasingly complex models with\ndeeper layers and more parameters, reducing interpretability and making their\ndecisions harder to understand. While many methods explain black-box reasoning,\nmost lack effective interventions or only operate at sample-level without\nmodifying the model itself. To address this, we propose the Concept Bottleneck\nModel for Enhancing Human-Neural Network Mutual Understanding (CBM-HNMU).\nCBM-HNMU leverages the Concept Bottleneck Model (CBM) as an interpretable\nframework to approximate black-box reasoning and communicate conceptual\nunderstanding. Detrimental concepts are automatically identified and refined\n(removed/replaced) based on global gradient contributions. The modified CBM\nthen distills corrected knowledge back into the black-box model, enhancing both\ninterpretability and accuracy. We evaluate CBM-HNMU on various CNN and\ntransformer-based models across Flower-102, CIFAR-10, CIFAR-100, FGVC-Aircraft,\nand CUB-200, achieving a maximum accuracy improvement of 2.64% and a maximum\nincrease in average accuracy across 1.03%. Source code is available at:\nhttps://github.com/XiGuaBo/CBM-HNMU."}
{"id": "2507.11128", "pdf": "https://arxiv.org/pdf/2507.11128.pdf", "abs": "https://arxiv.org/abs/2507.11128", "title": "What Should LLMs Forget? Quantifying Personal Data in LLMs for Right-to-Be-Forgotten Requests", "authors": ["Dimitri Staufer"], "categories": ["cs.CL", "cs.CY", "cs.LG", "I.2.6; H.2.8"], "comment": "16 pages, 3 figures. Accepted at the 7th Workshop on eXplainable\n  Knowledge Discovery in Data Mining (XKDD 2025), ECML PKDD 2025, Porto,\n  Portugal", "summary": "Large Language Models (LLMs) can memorize and reveal personal information,\nraising concerns regarding compliance with the EU's GDPR, particularly the\nRight to Be Forgotten (RTBF). Existing machine unlearning methods assume the\ndata to forget is already known but do not address how to identify which\nindividual-fact associations are stored in the model. Privacy auditing\ntechniques typically operate at the population level or target a small set of\nidentifiers, limiting applicability to individual-level data inquiries. We\nintroduce WikiMem, a dataset of over 5,000 natural language canaries covering\n243 human-related properties from Wikidata, and a model-agnostic metric to\nquantify human-fact associations in LLMs. Our approach ranks ground-truth\nvalues against counterfactuals using calibrated negative log-likelihood across\nparaphrased prompts. We evaluate 200 individuals across 15 LLMs (410M-70B\nparameters), showing that memorization correlates with subject web presence and\nmodel scale. We provide a foundation for identifying memorized personal data in\nLLMs at the individual level, enabling the dynamic construction of forget sets\nfor machine unlearning and RTBF requests."}
{"id": "2507.11198", "pdf": "https://arxiv.org/pdf/2507.11198.pdf", "abs": "https://arxiv.org/abs/2507.11198", "title": "Temperature and Persona Shape LLM Agent Consensus With Minimal Accuracy Gains in Qualitative Coding", "authors": ["Conrad Borchers", "Bahar Shahrokhian", "Francesco Balzan", "Elham Tajik", "Sreecharan Sankaranarayanan", "Sebastian Simon"], "categories": ["cs.CL", "cs.AI"], "comment": "Manuscript submitted for review", "summary": "Large Language Models (LLMs) enable new possibilities for qualitative\nresearch at scale, including coding and data annotation. While multi-agent\nsystems (MAS) can emulate human coding workflows, their benefits over\nsingle-agent coding remain poorly understood. We conducted an experimental\nstudy of how agent persona and temperature shape consensus-building and coding\naccuracy of dialog segments based on a codebook with 8 codes. Our open-source\nMAS mirrors deductive human coding through structured agent discussion and\nconsensus arbitration. Using six open-source LLMs (with 3 to 32 billion\nparameters) and 18 experimental configurations, we analyze over 77,000 coding\ndecisions against a gold-standard dataset of human-annotated transcripts from\nonline math tutoring sessions. Temperature significantly impacted whether and\nwhen consensus was reached across all six LLMs. MAS with multiple personas\n(including neutral, assertive, or empathetic), significantly delayed consensus\nin four out of six LLMs compared to uniform personas. In three of those LLMs,\nhigher temperatures significantly diminished the effects of multiple personas\non consensus. However, neither temperature nor persona pairing lead to robust\nimprovements in coding accuracy. Single agents matched or outperformed MAS\nconsensus in most conditions. Only one model (OpenHermesV2:7B) and code\ncategory showed above-chance gains from MAS deliberation when temperature was\n0.5 or lower and especially when the agents included at least one assertive\npersona. Qualitative analysis of MAS collaboration for these configurations\nsuggests that MAS may nonetheless aid in narrowing ambiguous code applications\nthat could improve codebooks and human-AI coding. We contribute new insight\ninto the limits of LLM-based qualitative methods, challenging the notion that\ndiverse MAS personas lead to better outcomes. We open-source our MAS and\nexperimentation code."}
{"id": "2507.11216", "pdf": "https://arxiv.org/pdf/2507.11216.pdf", "abs": "https://arxiv.org/abs/2507.11216", "title": "EsBBQ and CaBBQ: The Spanish and Catalan Bias Benchmarks for Question Answering", "authors": ["Valle Ruiz-Fernández", "Mario Mina", "Júlia Falcão", "Luis Vasquez-Reina", "Anna Sallés", "Aitor Gonzalez-Agirre", "Olatz Perez-de-Viñaspre"], "categories": ["cs.CL"], "comment": null, "summary": "Previous literature has largely shown that Large Language Models (LLMs)\nperpetuate social biases learnt from their pre-training data. Given the notable\nlack of resources for social bias evaluation in languages other than English,\nand for social contexts outside of the United States, this paper introduces the\nSpanish and the Catalan Bias Benchmarks for Question Answering (EsBBQ and\nCaBBQ). Based on the original BBQ, these two parallel datasets are designed to\nassess social bias across 10 categories using a multiple-choice QA setting, now\nadapted to the Spanish and Catalan languages and to the social context of\nSpain. We report evaluation results on different LLMs, factoring in model\nfamily, size and variant. Our results show that models tend to fail to choose\nthe correct answer in ambiguous scenarios, and that high QA accuracy often\ncorrelates with greater reliance on social biases."}
{"id": "2507.11222", "pdf": "https://arxiv.org/pdf/2507.11222.pdf", "abs": "https://arxiv.org/abs/2507.11222", "title": "An Agentic Flow for Finite State Machine Extraction using Prompt Chaining", "authors": ["Fares Wael", "Youssef Maklad", "Ali Hamdi", "Wael Elsersy"], "categories": ["cs.CL", "cs.AI", "cs.NI"], "comment": null, "summary": "Finite-State Machines (FSMs) are critical for modeling the operational logic\nof network protocols, enabling verification, analysis, and vulnerability\ndiscovery. However, existing FSM extraction techniques face limitations such as\nscalability, incomplete coverage, and ambiguity in natural language\nspecifications. In this paper, we propose FlowFSM, a novel agentic framework\nthat leverages Large Language Models (LLMs) combined with prompt chaining and\nchain-of-thought reasoning to extract accurate FSMs from raw RFC documents.\nFlowFSM systematically processes protocol specifications, identifies state\ntransitions, and constructs structured rule-books by chaining agent outputs.\nExperimental evaluation across FTP and RTSP protocols demonstrates that FlowFSM\nachieves high extraction precision while minimizing hallucinated transitions,\nshowing promising results. Our findings highlight the potential of agent-based\nLLM systems in the advancement of protocol analysis and FSM inference for\ncybersecurity and reverse engineering applications."}
{"id": "2507.11230", "pdf": "https://arxiv.org/pdf/2507.11230.pdf", "abs": "https://arxiv.org/abs/2507.11230", "title": "Sparse Autoencoders Can Capture Language-Specific Concepts Across Diverse Languages", "authors": ["Lyzander Marciano Andrylie", "Inaya Rahmanisa", "Mahardika Krisna Ihsani", "Alfan Farizki Wicaksono", "Haryo Akbarianto Wibowo", "Alham Fikri Aji"], "categories": ["cs.CL", "68T50"], "comment": null, "summary": "Understanding the multilingual mechanisms of large language models (LLMs)\nprovides insight into how they process different languages, yet this remains\nchallenging. Existing studies often focus on individual neurons, but their\npolysemantic nature makes it difficult to isolate language-specific units from\ncross-lingual representations. To address this, we explore sparse autoencoders\n(SAEs) for their ability to learn monosemantic features that represent concrete\nand abstract concepts across languages in LLMs. While some of these features\nare language-independent, the presence of language-specific features remains\nunderexplored. In this work, we introduce SAE-LAPE, a method based on feature\nactivation probability, to identify language-specific features within the\nfeed-forward network. We find that many such features predominantly appear in\nthe middle to final layers of the model and are interpretable. These features\ninfluence the model's multilingual performance and language output and can be\nused for language identification with performance comparable to fastText along\nwith more interpretability. Our code is available at\nhttps://github.com/LyzanderAndrylie/language-specific-features ."}
{"id": "2507.11273", "pdf": "https://arxiv.org/pdf/2507.11273.pdf", "abs": "https://arxiv.org/abs/2507.11273", "title": "KV-Latent: Dimensional-level KV Cache Reduction with Frequency-aware Rotary Positional Embedding", "authors": ["Luohe Shi", "Zuchao Li", "Lefei Zhang", "Guoming Liu", "Baoyuan Qi", "Hai Zhao"], "categories": ["cs.CL"], "comment": "To be published in The 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)", "summary": "Large language models (LLMs) based on Transformer Decoders have become the\npreferred choice for conversational generative AI. Despite the overall\nsuperiority of the Decoder architecture, the gradually increasing Key-Value\n(KV) cache during inference has emerged as a primary efficiency bottleneck,\nboth in aspects of memory consumption and data transfer bandwidth limitations.\nTo address these challenges, we propose a paradigm called KV-Latent. By\ndown-sampling the Key-Value vector dimensions into a latent space, we can\nsignificantly reduce the KV Cache footprint and improve inference speed, only\nwith a small amount of extra training, less than 1\\% of pre-training takes.\nBesides, we enhanced the stability of Rotary Positional Embedding applied on\nlower-dimensional vectors by modifying its frequency sampling mechanism,\navoiding noise introduced by higher frequencies while retaining position\nattenuation. Our experiments, including both models with Grouped Query\nAttention and those without, have yielded satisfactory results. Finally, we\nconducted comparative experiments to study the impact of separately reducing\nKey and Value components on model's performance. Our approach allows for the\nconstruction of more efficient language model systems, and opens the new\npossibility on KV Cache saving and efficient LLMs. Our code is available at\nhttps://github.com/ShiLuohe/KV-Latent."}
{"id": "2507.11275", "pdf": "https://arxiv.org/pdf/2507.11275.pdf", "abs": "https://arxiv.org/abs/2507.11275", "title": "FMC: Formalization of Natural Language Mathematical Competition Problems", "authors": ["Jiaxuan Xie", "Chengwu Liu", "Ye Yuan", "Siqi Li", "Zhiping Xiao", "Ming Zhang"], "categories": ["cs.CL"], "comment": "Accepted in ICML 2025 AI4MATH Workshop", "summary": "Efficient and accurate autoformalization methods, which leverage large-scale\ndatasets of extensive natural language mathematical problems to construct\nformal language datasets, are key to advancing formal mathematical reasoning.\nIn this paper, we propose an autoformalization pipeline based on large language\nmodels with error feedback, achieving a fully automatic and training-free\nformalization approach. Using this pipeline, we curate an Olympiad-level\ndataset aligning natural language problems with Lean formalizations. The\ndataset comprises $3,922$ mathematical problems in natural language and $9,787$\nin Lean, of which $64.46\\%$ were assessed as at least above-average quality,\nmaking it suitable as a benchmark for automated theorem provers. Additionally,\nwe investigate the formalization and reasoning capabilities of various LLMs and\nempirically demonstrate that few-shot learning, error feedback, and increasing\nsampling numbers enhance the autoformalization process. Experiments of three\nautomated theorem provers on the \\dataset\\ dataset also highlight its\nchallenging nature and its value as a benchmark for formal reasoning tasks."}
{"id": "2507.11292", "pdf": "https://arxiv.org/pdf/2507.11292.pdf", "abs": "https://arxiv.org/abs/2507.11292", "title": "Fine-Grained Chinese Hate Speech Understanding: Span-Level Resources, Coded Term Lexicon, and Enhanced Detection Frameworks", "authors": ["Zewen Bai", "Liang Yang", "Shengdi Yin", "Yuanyuan Sun", "Hongfei Lin"], "categories": ["cs.CL"], "comment": null, "summary": "The proliferation of hate speech has inflicted significant societal harm,\nwith its intensity and directionality closely tied to specific targets and\narguments. In recent years, numerous machine learning-based methods have been\ndeveloped to detect hateful comments on online platforms automatically.\nHowever, research on Chinese hate speech detection lags behind, and\ninterpretability studies face two major challenges: first, the scarcity of\nspan-level fine-grained annotated datasets limits models' deep semantic\nunderstanding of hate speech; second, insufficient research on identifying and\ninterpreting coded hate speech restricts model explainability in complex\nreal-world scenarios. To address these, we make the following contributions:\n(1) We introduce the Span-level Target-Aware Toxicity Extraction dataset (STATE\nToxiCN), the first span-level Chinese hate speech dataset, and evaluate the\nhate semantic understanding of existing models using it. (2) We conduct the\nfirst comprehensive study on Chinese coded hate terms, LLMs' ability to\ninterpret hate semantics. (3) We propose a method to integrate an annotated\nlexicon into models, significantly enhancing hate speech detection performance.\nOur work provides valuable resources and insights to advance the\ninterpretability of Chinese hate speech detection research."}
{"id": "2507.11299", "pdf": "https://arxiv.org/pdf/2507.11299.pdf", "abs": "https://arxiv.org/abs/2507.11299", "title": "Dr.Copilot: A Multi-Agent Prompt Optimized Assistant for Improving Patient-Doctor Communication in Romanian", "authors": ["Andrei Niculae", "Adrian Cosma", "Cosmin Dumitrache", "Emilian Rǎdoi"], "categories": ["cs.CL"], "comment": "10 figures, 2 tables, 2 listings", "summary": "Text-based telemedicine has become increasingly common, yet the quality of\nmedical advice in doctor-patient interactions is often judged more on how\nadvice is communicated rather than its clinical accuracy. To address this, we\nintroduce Dr.Copilot , a multi-agent large language model (LLM) system that\nsupports Romanian-speaking doctors by evaluating and enhancing the presentation\nquality of their written responses. Rather than assessing medical correctness,\nDr.Copilot provides feedback along 17 interpretable axes. The system comprises\nof three LLM agents with prompts automatically optimized via DSPy. Designed\nwith low-resource Romanian data and deployed using open-weight models, it\ndelivers real-time specific feedback to doctors within a telemedicine platform.\nEmpirical evaluations and live deployment with 41 doctors show measurable\nimprovements in user reviews and response quality, marking one of the first\nreal-world deployments of LLMs in Romanian medical settings."}
{"id": "2507.11316", "pdf": "https://arxiv.org/pdf/2507.11316.pdf", "abs": "https://arxiv.org/abs/2507.11316", "title": "Internal Value Alignment in Large Language Models through Controlled Value Vector Activation", "authors": ["Haoran Jin", "Meng Li", "Xiting Wang", "Zhihao Xu", "Minlie Huang", "Yantao Jia", "Defu Lian"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "25 pages, 14 figures. Accepted by ACL 2025 (main conference)", "summary": "Aligning Large Language Models (LLMs) with human values has attracted\nincreasing attention since it provides clarity, transparency, and the ability\nto adapt to evolving scenarios. In this paper, we introduce a Controlled Value\nVector Activation (ConVA) method that directly aligns the internal values of\nLLMs by interpreting how a value is encoded in their latent representations and\nmodifies relevant activations to ensure consistent values in LLMs. To ensure an\naccurate and unbiased interpretation, we propose a context-controlled value\nvector identification method. To consistently control values without\nsacrificing model performance, we introduce a gated value vector activation\nmethod for effective and minimum degree of value control. Experiments show that\nour method achieves the highest control success rate across 10 basic values\nwithout hurting LLM performance and fluency, and ensures target values even\nwith opposite and potentially malicious input prompts. Source code and data are\navailable at~ https://github.com/hr-jin/ConVA."}
{"id": "2507.11330", "pdf": "https://arxiv.org/pdf/2507.11330.pdf", "abs": "https://arxiv.org/abs/2507.11330", "title": "Automated Novelty Evaluation of Academic Paper: A Collaborative Approach Integrating Human and Large Language Model Knowledge", "authors": ["Wenqing Wu", "Chengzhi Zhang", "Yi Zhao"], "categories": ["cs.CL", "cs.AI", "cs.DL", "cs.HC"], "comment": "Journal of the Association for Information Science and Technology,\n  2025", "summary": "Novelty is a crucial criterion in the peer review process for evaluating\nacademic papers. Traditionally, it's judged by experts or measure by unique\nreference combinations. Both methods have limitations: experts have limited\nknowledge, and the effectiveness of the combination method is uncertain.\nMoreover, it's unclear if unique citations truly measure novelty. The large\nlanguage model (LLM) possesses a wealth of knowledge, while human experts\npossess judgment abilities that the LLM does not possess. Therefore, our\nresearch integrates the knowledge and abilities of LLM and human experts to\naddress the limitations of novelty assessment. The most common novelty in\nacademic papers is the introduction of new methods. In this paper, we propose\nleveraging human knowledge and LLM to assist pretrained language models (PLMs,\ne.g. BERT etc.) in predicting the method novelty of papers. Specifically, we\nextract sentences related to the novelty of the academic paper from peer review\nreports and use LLM to summarize the methodology section of the academic paper,\nwhich are then used to fine-tune PLMs. In addition, we have designed a\ntext-guided fusion module with novel Sparse-Attention to better integrate human\nand LLM knowledge. We compared the method we proposed with a large number of\nbaselines. Extensive experiments demonstrate that our method achieves superior\nperformance."}
{"id": "2507.11356", "pdf": "https://arxiv.org/pdf/2507.11356.pdf", "abs": "https://arxiv.org/abs/2507.11356", "title": "What is the Best Process Model Representation? A Comparative Analysis for Process Modeling with Large Language Models", "authors": ["Alexis Brissard", "Frédéric Cuppens", "Amal Zouaq"], "categories": ["cs.CL"], "comment": "12 pages, 7 figures, to be published in AI4BPM 2025 Proceedings", "summary": "Large Language Models (LLMs) are increasingly applied for Process Modeling\n(PMo) tasks such as Process Model Generation (PMG). To support these tasks,\nresearchers have introduced a variety of Process Model Representations (PMRs)\nthat serve as model abstractions or generation targets. However, these PMRs\ndiffer widely in structure, complexity, and usability, and have never been\nsystematically compared. Moreover, recent PMG approaches rely on distinct\nevaluation strategies and generation techniques, making comparison difficult.\nThis paper presents the first empirical study that evaluates multiple PMRs in\nthe context of PMo with LLMs. We introduce the PMo Dataset, a new dataset\ncontaining 55 process descriptions paired with models in nine different PMRs.\nWe evaluate PMRs along two dimensions: suitability for LLM-based PMo and\nperformance on PMG. \\textit{Mermaid} achieves the highest overall score across\nsix PMo criteria, whereas \\textit{BPMN text} delivers the best PMG results in\nterms of process element similarity."}
{"id": "2507.11384", "pdf": "https://arxiv.org/pdf/2507.11384.pdf", "abs": "https://arxiv.org/abs/2507.11384", "title": "Addressing Data Imbalance in Transformer-Based Multi-Label Emotion Detection with Weighted Loss", "authors": ["Xia Cui"], "categories": ["cs.CL"], "comment": "10 pages, 1 figure, SemEval 2025", "summary": "This paper explores the application of a simple weighted loss function to\nTransformer-based models for multi-label emotion detection in SemEval-2025\nShared Task 11. Our approach addresses data imbalance by dynamically adjusting\nclass weights, thereby enhancing performance on minority emotion classes\nwithout the computational burden of traditional resampling methods. We evaluate\nBERT, RoBERTa, and BART on the BRIGHTER dataset, using evaluation metrics such\nas Micro F1, Macro F1, ROC-AUC, Accuracy, and Jaccard similarity coefficients.\nThe results demonstrate that the weighted loss function improves performance on\nhigh-frequency emotion classes but shows limited impact on minority classes.\nThese findings underscore both the effectiveness and the challenges of applying\nthis approach to imbalanced multi-label emotion detection."}
{"id": "2507.11405", "pdf": "https://arxiv.org/pdf/2507.11405.pdf", "abs": "https://arxiv.org/abs/2507.11405", "title": "DCR: Quantifying Data Contamination in LLMs Evaluation", "authors": ["Cheng Xu", "Nan Yan", "Shuhao Guan", "Changhong Jin", "Yuke Mei", "Yibing Guo", "M-Tahar Kechadi"], "categories": ["cs.CL"], "comment": null, "summary": "The rapid advancement of large language models (LLMs) has heightened concerns\nabout benchmark data contamination (BDC), where models inadvertently memorize\nevaluation data, inflating performance metrics and undermining genuine\ngeneralization assessment. This paper introduces the Data Contamination Risk\n(DCR) framework, a lightweight, interpretable pipeline designed to detect and\nquantify BDC across four granular levels: semantic, informational, data, and\nlabel. By synthesizing contamination scores via a fuzzy inference system, DCR\nproduces a unified DCR Factor that adjusts raw accuracy to reflect\ncontamination-aware performance. Validated on 9 LLMs (0.5B-72B) across\nsentiment analysis, fake news detection, and arithmetic reasoning tasks, the\nDCR framework reliably diagnoses contamination severity and with accuracy\nadjusted using the DCR Factor to within 4% average error across the three\nbenchmarks compared to the uncontaminated baseline. Emphasizing computational\nefficiency and transparency, DCR provides a practical tool for integrating\ncontamination assessment into routine evaluations, fostering fairer comparisons\nand enhancing the credibility of LLM benchmarking practices."}
{"id": "2507.11407", "pdf": "https://arxiv.org/pdf/2507.11407.pdf", "abs": "https://arxiv.org/abs/2507.11407", "title": "EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and Reasoning Modes", "authors": ["LG AI Research", ":", "Kyunghoon Bae", "Eunbi Choi", "Kibong Choi", "Stanley Jungkyu Choi", "Yemuk Choi", "Kyubeen Han", "Seokhee Hong", "Junwon Hwang", "Taewan Hwang", "Joonwon Jang", "Hyojin Jeon", "Kijeong Jeon", "Gerrard Jeongwon Jo", "Hyunjik Jo", "Jiyeon Jung", "Euisoon Kim", "Hyosang Kim", "Jihoon Kim", "Joonkee Kim", "Seonghwan Kim", "Soyeon Kim", "Sunkyoung Kim", "Yireun Kim", "Yongil Kim", "Youchul Kim", "Edward Hwayoung Lee", "Gwangho Lee", "Haeju Lee", "Honglak Lee", "Jinsik Lee", "Kyungmin Lee", "Sangha Park", "Young Min Paik", "Yongmin Park", "Youngyong Park", "Sanghyun Seo", "Sihoon Yang", "Heuiyeen Yeen", "Sihyuk Yi", "Hyeongu Yun"], "categories": ["cs.CL", "cs.AI"], "comment": "Technical Report, 30 Pages", "summary": "This technical report introduces EXAONE 4.0, which integrates a Non-reasoning\nmode and a Reasoning mode to achieve both the excellent usability of EXAONE 3.5\nand the advanced reasoning abilities of EXAONE Deep. To pave the way for the\nagentic AI era, EXAONE 4.0 incorporates essential features such as agentic tool\nuse, and its multilingual capabilities are extended to support Spanish in\naddition to English and Korean. The EXAONE 4.0 model series consists of two\nsizes: a mid-size 32B model optimized for high performance, and a small-size\n1.2B model designed for on-device applications. The EXAONE 4.0 demonstrates\nsuperior performance compared to open-weight models in its class and remains\ncompetitive even against frontier-class models. The models are publicly\navailable for research purposes and can be easily downloaded via\nhttps://huggingface.co/LGAI-EXAONE."}
{"id": "2507.11408", "pdf": "https://arxiv.org/pdf/2507.11408.pdf", "abs": "https://arxiv.org/abs/2507.11408", "title": "KisMATH: Do LLMs Have Knowledge of Implicit Structures in Mathematical Reasoning?", "authors": ["Soumadeep Saha", "Akshay Chaturvedi", "Saptarshi Saha", "Utpal Garain", "Nicholas Asher"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "15 pages, 9 figures", "summary": "Chain-of-thought traces have been shown to improve performance of large\nlanguage models in a plethora of reasoning tasks, yet there is no consensus on\nthe mechanism through which this performance boost is achieved. To shed more\nlight on this, we introduce Causal CoT Graphs (CCGs), which are directed\nacyclic graphs automatically extracted from reasoning traces that model\nfine-grained causal dependencies in the language model output. A collection of\n$1671$ mathematical reasoning problems from MATH500, GSM8K and AIME, and their\nassociated CCGs are compiled into our dataset -- \\textbf{KisMATH}. Our detailed\nempirical analysis with 15 open-weight LLMs shows that (i) reasoning nodes in\nthe CCG are mediators for the final answer, a condition necessary for\nreasoning; and (ii) LLMs emphasise reasoning paths given by the CCG, indicating\nthat models internally realise structures akin to our graphs. KisMATH enables\ncontrolled, graph-aligned interventions and opens up avenues for further\ninvestigation into the role of chain-of-thought in LLM reasoning."}
{"id": "2507.11412", "pdf": "https://arxiv.org/pdf/2507.11412.pdf", "abs": "https://arxiv.org/abs/2507.11412", "title": "Seq vs Seq: An Open Suite of Paired Encoders and Decoders", "authors": ["Orion Weller", "Kathryn Ricci", "Marc Marone", "Antoine Chaffin", "Dawn Lawrie", "Benjamin Van Durme"], "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": null, "summary": "The large language model (LLM) community focuses almost exclusively on\ndecoder-only language models, since they are easier to use for text generation.\nHowever, a large subset of the community still uses encoder-only models for\ntasks such as classification or retrieval. Previous work has attempted to\ncompare these architectures, but is forced to make comparisons with models that\nhave different numbers of parameters, training techniques, and datasets. We\nintroduce the SOTA open-data Ettin suite of models: paired encoder-only and\ndecoder-only models ranging from 17 million parameters to 1 billion, trained on\nup to 2 trillion tokens. Using the same recipe for both encoder-only and\ndecoder-only models produces SOTA recipes in both categories for their\nrespective sizes, beating ModernBERT as an encoder and Llama 3.2 and SmolLM2 as\ndecoders. Like previous work, we find that encoder-only models excel at\nclassification and retrieval tasks while decoders excel at generative tasks.\nHowever, we show that adapting a decoder model to encoder tasks (and vice\nversa) through continued training is subpar compared to using only the reverse\nobjective (i.e. a 400M encoder outperforms a 1B decoder on MNLI, and vice versa\nfor generative tasks). We open-source all artifacts of this study including\ntraining data, training order segmented by checkpoint, and 200+ checkpoints to\nallow future work to analyze or extend all aspects of training."}
{"id": "2507.11423", "pdf": "https://arxiv.org/pdf/2507.11423.pdf", "abs": "https://arxiv.org/abs/2507.11423", "title": "Reasoning Strategies in Large Language Models: Can They Follow, Prefer, and Optimize?", "authors": ["Yanjian Zhang", "Guillaume Wisniewski", "Nadi Tomeh", "Thierry Charnois"], "categories": ["cs.CL"], "comment": null, "summary": "Human reasoning involves different strategies, each suited to specific\nproblems. Prior work shows that large language model (LLMs) tend to favor a\nsingle reasoning strategy, potentially limiting their effectiveness in diverse\nreasoning challenges. In this work, we investigate whether prompting can\ncontrol LLMs reasoning strategies and assess its impact on logical\nproblem-solving. While our experiments show that no single strategy\nconsistently improves accuracy, performance could be enhanced if models could\nadaptively choose the optimal strategy. We propose methods to guide LLMs in\nstrategy selection, highlighting new ways to refine their reasoning abilities."}
{"id": "2507.11502", "pdf": "https://arxiv.org/pdf/2507.11502.pdf", "abs": "https://arxiv.org/abs/2507.11502", "title": "HKGAI-V1: Towards Regional Sovereign Large Language Model for Hong Kong", "authors": ["Sirui Han", "Junqi Zhu", "Ruiyuan Zhang", "Yike Guo"], "categories": ["cs.CL", "cs.CE", "cs.LG"], "comment": null, "summary": "This paper presents the development of HKGAI-V1, a foundational sovereign\nlarge language model (LLM), developed as part of an initiative to establish\nvalue-aligned AI infrastructure specifically tailored for Hong Kong. Addressing\nthe region's unique multilingual environment (Cantonese, Mandarin, and\nEnglish), its distinct socio-legal context under the \"one country, two systems\"\nframework, and specific local cultural and value considerations, the model is\nbuilt upon the DeepSeek architecture and systematically aligned with regional\nnorms through a multifaceted full parameter fine-tuning process. It is further\nintegrated with a retrieval-augmented generation (RAG) system to ensure timely\nand factually grounded information access. The core contribution lies in the\ndesign and implementation of a comprehensive, region-specific AI alignment and\nsafety framework, demonstrated through two key achievements: 1) The successful\ndevelopment of HKGAI-V1 itself - which outper-forms general-purpose models in\nhandling Hong Kong-specific culturally sensitive queries, and embodies a\n\"governance-embedded\" approach to digital sovereignty - empowers Hong Kong to\nexercise control over AI applications in critical sectors including public\nservices, legal systems, and edu-cation. 2) The development of the proprietary\nAdversarial HK Value Benchmark, a rigorous tool for evaluating model alignment\nwith local ethical and legal stand-ards under challenging conditions. By\ndocumenting these achievements, the paper provides not only a technological\nartifact but also a replicable blueprint for developing advanced, regionally\nfocused AI systems deeply rooted in their local identities."}
{"id": "2507.11508", "pdf": "https://arxiv.org/pdf/2507.11508.pdf", "abs": "https://arxiv.org/abs/2507.11508", "title": "Real-World Summarization: When Evaluation Reaches Its Limits", "authors": ["Patrícia Schmidtová", "Ondřej Dušek", "Saad Mahamood"], "categories": ["cs.CL"], "comment": null, "summary": "We examine evaluation of faithfulness to input data in the context of hotel\nhighlights: brief LLM-generated summaries that capture unique features of\naccommodations. Through human evaluation campaigns involving categorical error\nassessment and span-level annotation, we compare traditional metrics, trainable\nmethods, and LLM-as-a-judge approaches. Our findings reveal that simpler\nmetrics like word overlap correlate surprisingly well with human judgments\n(Spearman correlation rank of 0.63), often outperforming more complex methods\nwhen applied to out-of-domain data. We further demonstrate that while LLMs can\ngenerate high-quality highlights, they prove unreliable for evaluation as they\ntend to severely under- or over-annotate. Our analysis of real-world business\nimpacts shows incorrect and non-checkable information pose the greatest risks.\nWe also highlight challenges in crowdsourced evaluations."}
{"id": "2507.10559", "pdf": "https://arxiv.org/pdf/2507.10559.pdf", "abs": "https://arxiv.org/abs/2507.10559", "title": "NLP Meets the World: Toward Improving Conversations With the Public About Natural Language Processing Research", "authors": ["Shomir Wilson"], "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent developments in large language models (LLMs) have been accompanied by\nrapidly growing public interest in natural language processing (NLP). This\nattention is reflected by major news venues, which sometimes invite NLP\nresearchers to share their knowledge and views with a wide audience.\nRecognizing the opportunities of the present, for both the research field and\nfor individual researchers, this paper shares recommendations for communicating\nwith a general audience about LLMs' capabilities and limitations. These\nrecommendations cover three themes: vague terminology as an obstacle to public\nunderstanding, unreasonable expectations as obstacles to sustainable growth,\nand ethical failures as obstacles to continued support. Published NLP research\nand popular news coverage are cited to illustrate these themes with examples.\nThe recommendations promote effective, transparent communication with the\ngeneral public about NLP, in order to strengthen public understanding and\nencourage support for research."}
{"id": "2507.10571", "pdf": "https://arxiv.org/pdf/2507.10571.pdf", "abs": "https://arxiv.org/abs/2507.10571", "title": "Orchestrator-Agent Trust: A Modular Agentic AI Visual Classification System with Trust-Aware Orchestration and RAG-Based Reasoning", "authors": ["Konstantinos I. Roumeliotis", "Ranjan Sapkota", "Manoj Karkee", "Nikolaos D. Tselikas"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Modern Artificial Intelligence (AI) increasingly relies on multi-agent\narchitectures that blend visual and language understanding. Yet, a pressing\nchallenge remains: How can we trust these agents especially in zero-shot\nsettings with no fine-tuning? We introduce a novel modular Agentic AI visual\nclassification framework that integrates generalist multimodal agents with a\nnon-visual reasoning orchestrator and a Retrieval-Augmented Generation (RAG)\nmodule. Applied to apple leaf disease diagnosis, we benchmark three\nconfigurations: (I) zero-shot with confidence-based orchestration, (II)\nfine-tuned agents with improved performance, and (III) trust-calibrated\norchestration enhanced by CLIP-based image retrieval and re-evaluation loops.\nUsing confidence calibration metrics (ECE, OCR, CCC), the orchestrator\nmodulates trust across agents. Our results demonstrate a 77.94\\% accuracy\nimprovement in the zero-shot setting using trust-aware orchestration and RAG,\nachieving 85.63\\% overall. GPT-4o showed better calibration, while Qwen-2.5-VL\ndisplayed overconfidence. Furthermore, image-RAG grounded predictions with\nvisually similar cases, enabling correction of agent overconfidence via\niterative re-evaluation. The proposed system separates perception (vision\nagents) from meta-reasoning (orchestrator), enabling scalable and interpretable\nmulti-agent AI. This blueprint is extensible to diagnostics, biology, and other\ntrust-critical domains. All models, prompts, results, and system components\nincluding the complete software source code are openly released to support\nreproducibility, transparency, and community benchmarking at Github:\nhttps://github.com/Applied-AI-Research-Lab/Orchestrator-Agent-Trust"}
{"id": "2507.10576", "pdf": "https://arxiv.org/pdf/2507.10576.pdf", "abs": "https://arxiv.org/abs/2507.10576", "title": "Can Large Language Models Understand As Well As Apply Patent Regulations to Pass a Hands-On Patent Attorney Test?", "authors": ["Bhakti Khera", "Rezvan Alamian", "Pascal A. Scherz", "Stephan M. Goetz"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.ET"], "comment": "39 pages, 21 figures", "summary": "The legal field already uses various large language models (LLMs) in actual\napplications, but their quantitative performance and reasons for it are\nunderexplored. We evaluated several open-source and proprietary LLMs --\nincluding GPT-series, Anthropic, Deepseek and Llama-3, variants -- on parts of\nthe European Qualifying Examination (EQE) for future European Patent Attorneys.\nOpenAI o1 led with 0.82 accuracy and 0.81 F1 score, whereas (Amazon Web\nServices) AWS Llama 3.1 8B lagged at 0.50 accuracy, and a Python-deployed Llama\n3.1 8B scored 0.55. The latter two are within the range of mere guessing for\nthe two-answer forced-choice design. None of the evaluated models could have\npassed the examination fully, as accuracy never exceeded the average threshold\nof 0.90 required for professional-level standards -- also not models that are\nregularly promoted for their assumed beyond-PhD- and bar-admitted-lawyer-level\nperformance. GPT-4o excelled at integrating text and graphics, while Claude 3\nOpus often lost formatting coherence. Human patent experts evaluated the\ntextual justifications and uncovered various critical shortcomings of each\nmodel. They valued clarity and legal rationale over the raw correctness of the\nanswers, which revealed misalignment between automatic metrics and expert\njudgment. Model outputs were sensitive to modest temperature changes and prompt\nwording, which underscores the remaining necessity of expert oversight. Future\nwork should target logical consistency, robust multimodality, and adaptive\nprompting to approach human-level patent proficiency. In summary, despite the\noutstanding performance of recent large models, the general public might\noverestimate their performance. The field has a long way to go to develop a\nvirtual patent attorney. This paper wants to point out several specific\nlimitations that need solutions."}
{"id": "2507.10579", "pdf": "https://arxiv.org/pdf/2507.10579.pdf", "abs": "https://arxiv.org/abs/2507.10579", "title": "Findings of the BEA 2025 Shared Task on Pedagogical Ability Assessment of AI-powered Tutors", "authors": ["Ekaterina Kochmar", "Kaushal Kumar Maurya", "Kseniia Petukhova", "KV Aditya Srivatsa", "Anaïs Tack", "Justin Vasselli"], "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": "Proceedings of the 20th Workshop on Innovative Use of NLP for\n  Building Educational Applications", "summary": "This shared task has aimed to assess pedagogical abilities of AI tutors\npowered by large language models (LLMs), focusing on evaluating the quality of\ntutor responses aimed at student's mistake remediation within educational\ndialogues. The task consisted of five tracks designed to automatically evaluate\nthe AI tutor's performance across key dimensions of mistake identification,\nprecise location of the mistake, providing guidance, and feedback\nactionability, grounded in learning science principles that define good and\neffective tutor responses, as well as the track focusing on detection of the\ntutor identity. The task attracted over 50 international teams across all\ntracks. The submitted models were evaluated against gold-standard human\nannotations, and the results, while promising, show that there is still\nsignificant room for improvement in this domain: the best results for the four\npedagogical ability assessment tracks range between macro F1 scores of 58.34\n(for providing guidance) and 71.81 (for mistake identification) on three-class\nproblems, with the best F1 score in the tutor identification track reaching\n96.98 on a 9-class task. In this paper, we overview the main findings of the\nshared task, discuss the approaches taken by the teams, and analyze their\nperformance. All resources associated with this task are made publicly\navailable to support future research in this critical domain."}
{"id": "2507.10616", "pdf": "https://arxiv.org/pdf/2507.10616.pdf", "abs": "https://arxiv.org/abs/2507.10616", "title": "Scalpel vs. Hammer: GRPO Amplifies Existing Capabilities, SFT Replaces Them", "authors": ["Neel Rajani", "Aryo Pradipta Gema", "Seraphina Goldfarb-Tarrant", "Ivan Titov"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Training large language models (LLMs) for reasoning via maths and code\ndatasets has become a major new focus in LLM post-training. Two particularly\npopular approaches are reinforcement learning (RL) and supervised fine-tuning\n(SFT), but their training dynamics are poorly understood. We present a\ncomparative analysis of RL and SFT on the same maths problems with the same\nmodel and similar hyperparameters. We find that RL yields minor in-domain gains\non maths and slight degradation on knowledge-intensive benchmarks like MMLU,\nwhile both trends are more pronounced in SFT. We also analyse model parameters\nacross checkpoints, observing that both algorithms modify query and key weights\nthe most. Meanwhile, SFT exhibits greater updates and also affects mid-layer\nMLPs more, leading us to hypothesise that this may have caused the\nout-of-domain degradation. We therefore investigate whether freezing parts of\nthe model during training can mitigate the reduced performance on\nknowledge-intensive benchmarks. However, our results are inconclusive, with\nbenefits on GPQA:Diamond and degradation on other benchmarks. Taken together,\nour observations provide a preliminary indication for why RL amplifies existing\ncapabilities, while SFT replaces old skills with new ones."}
{"id": "2507.10644", "pdf": "https://arxiv.org/pdf/2507.10644.pdf", "abs": "https://arxiv.org/abs/2507.10644", "title": "From Semantic Web and MAS to Agentic AI: A Unified Narrative of the Web of Agents", "authors": ["Tatiana Petrova", "Aleksandr Puzikov", "Boris Bliznukov", "Radu State"], "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.HC", "cs.MA", "I.2.11; I.2.7; C.2.4; K.6.5; I.2.4"], "comment": "33 pages, 9 figures, 8 tables", "summary": "The concept of the Web of Agents (WoA), which transforms the static,\ndocument-centric Web into an environment of autonomous agents acting on users'\nbehalf, has attracted growing interest as large language models (LLMs) become\nmore capable. However, research in this area is still fragmented across\ndifferent communities. Contemporary surveys catalog the latest LLM-powered\nframeworks, while the rich histories of Multi-Agent Systems (MAS) and the\nSemantic Web are often treated as separate, legacy domains. This fragmentation\nobscures the intellectual lineage of modern systems and hinders a holistic\nunderstanding of the field's trajectory. We present the first comprehensive\nevolutionary overview of the WoA. We show that modern protocols like A2A and\nthe MCP, are direct evolutionary responses to the well-documented limitations\nof earlier standards like FIPA standards and OWL-based semantic agents. To\nsystematize this analysis, we introduce a four-axis taxonomy (semantic\nfoundation, communication paradigm, locus of intelligence, discovery\nmechanism). This framework provides a unified analytical lens for comparing\nagent architectures across all generations, revealing a clear line of descent\nwhere others have seen a disconnect. Our analysis identifies a paradigm shift\nin the 'locus of intelligence': from being encoded in external data (Semantic\nWeb) or the platform (MAS) to being embedded within the agent's core model\n(LLM). This shift is foundational to modern Agentic AI, enabling the scalable\nand adaptive systems the WoA has long envisioned. We conclude that while new\nprotocols are essential, they are insufficient for building a robust, open,\ntrustworthy ecosystem. Finally, we argue that the next research frontier lies\nin solving persistent socio-technical challenges, and we map out a new agenda\nfocused on decentralized identity, economic models, security, and governance\nfor the emerging WoA."}
{"id": "2507.10773", "pdf": "https://arxiv.org/pdf/2507.10773.pdf", "abs": "https://arxiv.org/abs/2507.10773", "title": "Theory of Mind and Self-Disclosure to CUIs", "authors": ["Samuel Rhys Cox"], "categories": ["cs.HC", "cs.CL"], "comment": "Workshop paper presented at ToMinHAI at CUI'2025: Theory of Mind in\n  Human-CUI Interaction, held in conjunction with the 2025 ACM conference on\n  Conversational User Interfaces, July 8th, 2025. 4 pages. 3 figures", "summary": "Self-disclosure is important to help us feel better, yet is often difficult.\nThis difficulty can arise from how we think people are going to react to our\nself-disclosure. In this workshop paper, we briefly discuss self-disclosure to\nconversational user interfaces (CUIs) in relation to various social cues. We\nthen, discuss how expressions of uncertainty or representation of a CUI's\nreasoning could help encourage self-disclosure, by making a CUI's intended\n\"theory of mind\" more transparent to users."}
{"id": "2507.10803", "pdf": "https://arxiv.org/pdf/2507.10803.pdf", "abs": "https://arxiv.org/abs/2507.10803", "title": "Automated Thematic Analyses Using LLMs: Xylazine Wound Management Social Media Chatter Use Case", "authors": ["JaMor Hairston", "Ritvik Ranjan", "Sahithi Lakamana", "Anthony Spadaro", "Selen Bozkurt", "Jeanmarie Perrone", "Abeed Sarker"], "categories": ["cs.AI", "cs.CL", "cs.ET", "cs.IR"], "comment": "Pages: 19, Abstract word count: 151 words, Manuscript word count:\n  2185 words, References: 14, Figures: 3, Tables: 2", "summary": "Background Large language models (LLMs) face challenges in inductive thematic\nanalysis, a task requiring deep interpretive and domain-specific expertise. We\nevaluated the feasibility of using LLMs to replicate expert-driven thematic\nanalysis of social media data. Methods Using two temporally non-intersecting\nReddit datasets on xylazine (n=286 and n=686, for model optimization and\nvalidation, respectively) with twelve expert-derived themes, we evaluated five\nLLMs against expert coding. We modeled the task as a series of binary\nclassifications, rather than a single, multi-label classification, employing\nzero-, single-, and few-shot prompting strategies and measuring performance via\naccuracy, precision, recall, and F1-score. Results On the validation set,\nGPT-4o with two-shot prompting performed best (accuracy: 90.9%; F1-score:\n0.71). For high-prevalence themes, model-derived thematic distributions closely\nmirrored expert classifications (e.g., xylazine use: 13.6% vs. 17.8%; MOUD use:\n16.5% vs. 17.8%). Conclusions Our findings suggest that few-shot LLM-based\napproaches can automate thematic analyses, offering a scalable supplement for\nqualitative research. Keywords: thematic analysis, large language models,\nnatural language processing, qualitative analysis, social media, prompt\nengineering, public health"}
{"id": "2507.10859", "pdf": "https://arxiv.org/pdf/2507.10859.pdf", "abs": "https://arxiv.org/abs/2507.10859", "title": "MultiVox: Benchmarking Voice Assistants for Multimodal Interactions", "authors": ["Ramaneswaran Selvakumar", "Ashish Seth", "Nishit Anand", "Utkarsh Tyagi", "Sonal Kumar", "Sreyan Ghosh", "Dinesh Manocha"], "categories": ["cs.MM", "cs.CL", "cs.HC"], "comment": "Work In Progress", "summary": "The rapid progress of Large Language Models (LLMs) has empowered omni models\nto act as voice assistants capable of understanding spoken dialogues. These\nmodels can process multimodal inputs beyond text, such as speech and visual\ndata, enabling more context-aware interactions. However, current benchmarks\nfall short in comprehensively evaluating how well these models generate\ncontext-aware responses, particularly when it comes to implicitly understanding\nfine-grained speech characteristics, such as pitch, emotion, timbre, and volume\nor the environmental acoustic context such as background sounds. Additionally,\nthey inadequately assess the ability of models to align paralinguistic cues\nwith complementary visual signals to inform their responses. To address these\ngaps, we introduce MultiVox, the first omni voice assistant benchmark designed\nto evaluate the ability of voice assistants to integrate spoken and visual cues\nincluding paralinguistic speech features for truly multimodal understanding.\nSpecifically, MultiVox includes 1000 human-annotated and recorded speech\ndialogues that encompass diverse paralinguistic features and a range of visual\ncues such as images and videos. Our evaluation on 9 state-of-the-art models\nreveals that, although humans excel at these tasks, current models consistently\nstruggle to produce contextually grounded responses."}
{"id": "2507.10865", "pdf": "https://arxiv.org/pdf/2507.10865.pdf", "abs": "https://arxiv.org/abs/2507.10865", "title": "Overview of the TREC 2022 deep learning track", "authors": ["Nick Craswell", "Bhaskar Mitra", "Emine Yilmaz", "Daniel Campos", "Jimmy Lin", "Ellen M. Voorhees", "Ian Soboroff"], "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "arXiv admin note: substantial text overlap with arXiv:2507.08191,\n  arXiv:2507.08890", "summary": "This is the fourth year of the TREC Deep Learning track. As in previous\nyears, we leverage the MS MARCO datasets that made hundreds of thousands of\nhuman annotated training labels available for both passage and document ranking\ntasks. In addition, this year we also leverage both the refreshed passage and\ndocument collections that were released last year leading to a nearly $16$\ntimes increase in the size of the passage collection and nearly four times\nincrease in the document collection size. Unlike previous years, in 2022 we\nmainly focused on constructing a more complete test collection for the passage\nretrieval task, which has been the primary focus of the track. The document\nranking task was kept as a secondary task, where document-level labels were\ninferred from the passage-level labels. Our analysis shows that similar to\nprevious years, deep neural ranking models that employ large scale pretraining\ncontinued to outperform traditional retrieval methods. Due to the focusing our\njudging resources on passage judging, we are more confident in the quality of\nthis year's queries and judgments, with respect to our ability to distinguish\nbetween runs and reuse the dataset in future. We also see some surprises in\noverall outcomes. Some top-performing runs did not do dense retrieval. Runs\nthat did single-stage dense retrieval were not as competitive this year as they\nwere last year."}
{"id": "2507.10880", "pdf": "https://arxiv.org/pdf/2507.10880.pdf", "abs": "https://arxiv.org/abs/2507.10880", "title": "Domain-Adaptive Small Language Models for Structured Tax Code Prediction", "authors": ["Souvik Nath", "Sumit Wadhwa", "Luiz Perez"], "categories": ["cs.LG", "cs.CL"], "comment": "10 pages, 3 figures", "summary": "Every day, multinational firms process thousands of transactions, each of\nwhich must adhere to tax regulations that vary by jurisdiction and are often\nnuanced. The determination of product and service tax codes, such as HSN or SAC\nis a major use case in Tax compliance. An accurate determination of such codes\nis imperative to avoid any tax penalties. This paper proposes a domain-adaptive\nsmall language model (SLM) with an encoder-decoder architecture for the\nenhanced prediction of product and service tax codes. In this approach, we\naddress the problem of predicting hierarchical tax code sequences using\nunstructured product and services data. We employ an SLM based upon\nencoder-decoder architecture as this enables sequential generation of tax codes\nto capture the hierarchical dependencies present within the tax codes. Our\nexperiments demonstrate that encoder-decoder SLMs can be successfully applied\nto the sequential prediction of structured tax codes, a domain that remains\ncomparatively unexplored in current NLP research. In this paper, we demonstrate\nthe superior performance of the domain-adaptive encoder-decoder SLMs over flat\nclassifiers when applied to the Harmonized System of Nomenclature (HSN), and\nachieve superior results compared to decoder-only and encoder-only\narchitectures for structured sequence generation tasks. This approach can also\nbe scaled to other government-mandated tax commodity codes, such as United\nNations Standard Products and Services Codes (UNSPSC), or Brazil's Nomenclatura\nComum do Mercosul (NCM)."}
{"id": "2507.10894", "pdf": "https://arxiv.org/pdf/2507.10894.pdf", "abs": "https://arxiv.org/abs/2507.10894", "title": "NavComposer: Composing Language Instructions for Navigation Trajectories through Action-Scene-Object Modularization", "authors": ["Zongtao He", "Liuyi Wang", "Lu Chen", "Chengju Liu", "Qijun Chen"], "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Language-guided navigation is a cornerstone of embodied AI, enabling agents\nto interpret language instructions and navigate complex environments. However,\nexpert-provided instructions are limited in quantity, while synthesized\nannotations often lack quality, making them insufficient for large-scale\nresearch. To address this, we propose NavComposer, a novel framework for\nautomatically generating high-quality navigation instructions. NavComposer\nexplicitly decomposes semantic entities such as actions, scenes, and objects,\nand recomposes them into natural language instructions. Its modular\narchitecture allows flexible integration of state-of-the-art techniques, while\nthe explicit use of semantic entities enhances both the richness and accuracy\nof instructions. Moreover, it operates in a data-agnostic manner, supporting\nadaptation to diverse navigation trajectories without domain-specific training.\nComplementing NavComposer, we introduce NavInstrCritic, a comprehensive\nannotation-free evaluation system that assesses navigation instructions on\nthree dimensions: contrastive matching, semantic consistency, and linguistic\ndiversity. NavInstrCritic provides a holistic evaluation of instruction\nquality, addressing limitations of traditional metrics that rely heavily on\nexpert annotations. By decoupling instruction generation and evaluation from\nspecific navigation agents, our method enables more scalable and generalizable\nresearch. Extensive experiments provide direct and practical evidence for the\neffectiveness of our method."}
{"id": "2507.10903", "pdf": "https://arxiv.org/pdf/2507.10903.pdf", "abs": "https://arxiv.org/abs/2507.10903", "title": "LiLM-RDB-SFC: Lightweight Language Model with Relational Database-Guided DRL for Optimized SFC Provisioning", "authors": ["Parisa Fard Moshiri", "Xinyu Zhu", "Poonam Lohan", "Burak Kantarci", "Emil Janulewicz"], "categories": ["cs.NI", "cs.CL", "cs.LG"], "comment": "9 pages, 6 figures, Accepted to IEEE 16th International Conference on\n  Network of the Future (NoF) 2025", "summary": "Effective management of Service Function Chains (SFCs) and optimal Virtual\nNetwork Function (VNF) placement are critical challenges in modern\nSoftware-Defined Networking (SDN) and Network Function Virtualization (NFV)\nenvironments. Although Deep Reinforcement Learning (DRL) is widely adopted for\ndynamic network decision-making, its inherent dependency on structured data and\nfixed action rules often limits adaptability and responsiveness, particularly\nunder unpredictable network conditions. This paper introduces LiLM-RDB-SFC, a\nnovel approach combining Lightweight Language Model (LiLM) with Relational\nDatabase (RDB) to answer network state queries to guide DRL model for efficient\nSFC provisioning. Our proposed approach leverages two LiLMs, Bidirectional and\nAuto-Regressive Transformers (BART) and the Fine-tuned Language Net T5\n(FLAN-T5), to interpret network data and support diverse query types related to\nSFC demands, data center resources, and VNF availability. Results demonstrate\nthat FLAN-T5 outperforms BART with a lower test loss (0.00161 compared to\n0.00734), higher accuracy (94.79% compared to 80.2%), and less processing time\n(2h 2min compared to 2h 38min). Moreover, when compared to the large language\nmodel SQLCoder, FLAN-T5 matches the accuracy of SQLCoder while cutting\nprocessing time by 96% (SQLCoder: 54 h 43 min; FLAN-T5: 2 h 2 min)."}
{"id": "2507.11017", "pdf": "https://arxiv.org/pdf/2507.11017.pdf", "abs": "https://arxiv.org/abs/2507.11017", "title": "First-Order Error Matters: Accurate Compensation for Quantized Large Language Models", "authors": ["Xingyu Zheng", "Haotong Qin", "Yuye Li", "Jiakai Wang", "Jinyang Guo", "Michele Magno", "Xianglong Liu"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Post-training quantization (PTQ) offers an efficient approach to compressing\nlarge language models (LLMs), significantly reducing memory access and\ncomputational costs. Existing compensation-based weight calibration methods\noften rely on a second-order Taylor expansion to model quantization error,\nunder the assumption that the first-order term is negligible in well-trained\nfull-precision models. However, we reveal that the progressive compensation\nprocess introduces accumulated first-order deviations between latent weights\nand their full-precision counterparts, making this assumption fundamentally\nflawed. To address this, we propose FOEM, a novel PTQ method that explicitly\nincorporates first-order gradient terms to improve quantization error\ncompensation. FOEM approximates gradients by directly computing the difference\nbetween latent and full-precision weights, avoiding the high cost and limited\ngeneralization of backpropagation-based gradient computation. This approach\nintroduces minimal additional computational overhead. Moreover, FOEM leverages\nprecomputed Cholesky factors to efficiently recover the inverse of Hessian\nsubmatrices in real time. Extensive experiments across a wide range of models\nand benchmarks demonstrate that FOEM consistently outperforms the classical\nGPTQ method. In 3-bit weight-only quantization, FOEM reduces the perplexity of\nLlama3-8B by 89.6%, and improves the 5-shot MMLU accuracy of Llama3-70B from\n51.7% to 74.9%, approaching the full-precision performance of 78.6%.\nFurthermore, FOEM can be seamlessly integrated with advanced techniques such as\nGPTAQ and SpinQuant, yielding additional improvements under the challenging\nW4A4KV4 setting, and further narrowing the accuracy gap with full-precision\nbaselines beyond what current state-of-the-art methods achieve. The code is\navailable at https://github.com/Xingyu-Zheng/FOEM."}
{"id": "2507.11059", "pdf": "https://arxiv.org/pdf/2507.11059.pdf", "abs": "https://arxiv.org/abs/2507.11059", "title": "SWE-MERA: A Dynamic Benchmark for Agenticly Evaluating Large Language Models on Software Engineering Tasks", "authors": ["Pavel Adamenko", "Mikhail Ivanov", "Aidar Valeev", "Rodion Levichev", "Pavel Zadorozhny", "Ivan Lopatin", "Dmitry Babayev", "Alena Fenogenova", "Valentin Malykh"], "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) in software engineering\nhas revealed critical limitations in existing benchmarks, particularly the\nwidely used SWE-bench dataset. Recent studies have uncovered severe data\ncontamination issues, e.g. SWE-bench reports 32.67% of successful patches\ninvolve direct solution leakage and 31.08\\% pass due to inadequate test cases.\nWe introduce SWE-MERA, a dynamic, continuously updated benchmark designed to\naddress these fundamental challenges through an automated collection of\nreal-world GitHub issues and rigorous quality validation. Our approach\nimplements a reliable pipeline that ensures quality while minimizing\ncontamination risks, resulting in approximately 10,000 potential tasks with 300\nsamples currently available. Evaluation using the Aider coding agent\ndemonstrates strong discriminative power in state-of-the-art models. We report\nperformance across a dozen recent LLMs evaluated on tasks collected between\nSeptember 2024 and June 2025."}
{"id": "2507.11515", "pdf": "https://arxiv.org/pdf/2507.11515.pdf", "abs": "https://arxiv.org/abs/2507.11515", "title": "AirLLM: Diffusion Policy-based Adaptive LoRA for Remote Fine-Tuning of LLM over the Air", "authors": ["Shiyi Yang", "Xiaoxue Yu", "Rongpeng Li", "Jianhang Zhu", "Zhifeng Zhao", "Honggang Zhang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "11 pages, 8 figures", "summary": "Operating Large Language Models (LLMs) on edge devices is increasingly\nchallenged by limited communication bandwidth and strained computational and\nmemory costs. Thus, cloud-assisted remote fine-tuning becomes indispensable.\nNevertheless, existing Low-Rank Adaptation (LoRA) approaches typically employ\nfixed or heuristic rank configurations, and the subsequent over-the-air\ntransmission of all LoRA parameters could be rather inefficient. To address\nthis limitation, we develop AirLLM, a hierarchical diffusion policy framework\nfor communication-aware LoRA adaptation. Specifically, AirLLM models the rank\nconfiguration as a structured action vector that spans all LoRA-inserted\nprojections. To solve the underlying high-dimensional sequential\ndecision-making problem, a Proximal Policy Optimization (PPO) agent generates\ncoarse-grained decisions by jointly observing wireless states and linguistic\ncomplexity, which are then refined via Denoising Diffusion Implicit Models\n(DDIM) to produce high-resolution, task- and channel-adaptive rank vectors. The\ntwo modules are optimized alternatively, with the DDIM trained under the\nClassifier-Free Guidance (CFG) paradigm to maintain alignment with PPO rewards.\nExperiments under varying signal-to-noise ratios demonstrate that AirLLM\nconsistently enhances fine-tuning performance while significantly reducing\ntransmission costs, highlighting the effectiveness of reinforcement-driven,\ndiffusion-refined rank adaptation for scalable and efficient remote fine-tuning\nover the air."}
{"id": "2401.13444", "pdf": "https://arxiv.org/pdf/2401.13444.pdf", "abs": "https://arxiv.org/abs/2401.13444", "title": "Fine-grained Stateful Knowledge Exploration: Effective and Efficient Graph Retrieval with Large Language Models", "authors": ["Dehao Tao", "Congqi Wang", "Feng Huang", "Junhao Chen", "Yongfeng Huang", "Minghu Jiang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have shown impressive capabilities, yet updating\ntheir knowledge remains a significant challenge, often leading to outdated or\ninaccurate responses. A proposed solution is the integration of external\nknowledge bases, such as knowledge graphs, with LLMs. Most existing methods use\na paradigm that treats the whole question as the objective, with relevant\nknowledge being incrementally retrieved from the knowledge graph. However, this\nparadigm often leads to a granularity mismatch between the target question and\nthe retrieved entities and relations. As a result, the information in the\nquestion cannot precisely correspond to the retrieved knowledge. This may cause\nredundant exploration or omission of vital knowledge, thereby leading to\nenhanced computational consumption and reduced retrieval accuracy. To address\nthe limitations of coarse-grained knowledge exploration, we propose FiSKE, a\nnovel paradigm for Fine-grained Stateful Knowledge Exploration. FiSKE first\ndecomposes questions into fine-grained clues, then employs an adaptive mapping\nstrategy during knowledge exploration process to resolve ambiguity in\nclue-to-graph mappings. This strategy dynamically infers contextual\ncorrespondences while maintaining a stateful record of the mappings. A\nclue-driven termination mechanism ensures rigorous augmentation--leveraging\nfully mapped paths for LLMs while reverting to chain-of-thought reasoning when\nnecessary. Our approach balances precision and efficiency. Experiments on\nmultiple datasets revealed that our paradigm surpasses current advanced methods\nin knowledge retrieval while significantly reducing the average number of LLM\ninvocations."}
{"id": "2410.08193", "pdf": "https://arxiv.org/pdf/2410.08193.pdf", "abs": "https://arxiv.org/abs/2410.08193", "title": "GenARM: Reward Guided Generation with Autoregressive Reward Model for Test-time Alignment", "authors": ["Yuancheng Xu", "Udari Madhushani Sehwag", "Alec Koppel", "Sicheng Zhu", "Bang An", "Furong Huang", "Sumitra Ganesh"], "categories": ["cs.CL"], "comment": "Published at the Thirteenth International Conference on Learning\n  Representations (ICLR 2025)", "summary": "Large Language Models (LLMs) exhibit impressive capabilities but require\ncareful alignment with human preferences. Traditional training-time methods\nfinetune LLMs using human preference datasets but incur significant training\ncosts and require repeated training to handle diverse user preferences.\nTest-time alignment methods address this by using reward models (RMs) to guide\nfrozen LLMs without retraining. However, existing test-time approaches rely on\ntrajectory-level RMs which are designed to evaluate complete responses, making\nthem unsuitable for autoregressive text generation that requires computing\nnext-token rewards from partial responses. To address this, we introduce\nGenARM, a test-time alignment approach that leverages the Autoregressive Reward\nModel--a novel reward parametrization designed to predict next-token rewards\nfor efficient and effective autoregressive generation. Theoretically, we\ndemonstrate that this parametrization can provably guide frozen LLMs toward any\ndistribution achievable by traditional RMs within the KL-regularized\nreinforcement learning framework. Experimental results show that GenARM\nsignificantly outperforms prior test-time alignment baselines and matches the\nperformance of training-time methods. Additionally, GenARM enables efficient\nweak-to-strong guidance, aligning larger LLMs with smaller RMs without the high\ncosts of training larger models. Furthermore, GenARM supports multi-objective\nalignment, allowing real-time trade-offs between preference dimensions and\ncatering to diverse user preferences without retraining. Our project page is\navailable at: https://genarm.github.io."}
{"id": "2411.15821", "pdf": "https://arxiv.org/pdf/2411.15821.pdf", "abs": "https://arxiv.org/abs/2411.15821", "title": "Is Training Data Quality or Quantity More Impactful to Small Language Model Performance?", "authors": ["Aryan Sajith", "Krishna Chaitanya Rao Kathala"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "14 pages, 5 tables, 4 figures | Accepted at International Conference\n  on Neural Computing for Advanced Applications 2025, Conference info:\n  https://aaci.org.hk/ncaa2025", "summary": "This study investigates the relative impact of training data quality versus\nquantity on the performance of small language models (SLMs), utilizing the\nTinyStories dataset for empirical analysis. Analysis of dataset variations with\nrespect to size (25% and 50% of the original size) and duplication (controlled\nrates of 25%, 50%, 75%, and 100%) were performed. Model performance was\nevaluated based on the validation loss, accuracy, and perplexity metrics.\nResults indicate training data quality plays a more significant role in the\noverall performance of SLMs, especially given scale of this experiment. Minimal\nduplication positively impacted model accuracy (+0.87% increase in accuracy at\n25% duplication) without significantly increasing perplexity (+0.52% increase\ngoing from 0% to 25% duplication) but excessive duplication led to pronounced\nperformance degradation (-40% drop in accuracy at 100% duplication). The\nimplications of this exploration extend beyond just model performance; training\nlarge-scale models imposes significant financial and computational burdens,\nwhich can be prohibitive for organizations, individuals, and the public at\nlarge, especially in developing countries. Additionally, the energy consumption\nassociated with large-scale training raises environmental concerns.\nUnderstanding the relative importance of data quality versus quantity could\ndemocratize AI technology, making advanced models more accessible and\nsustainable for all."}
{"id": "2412.06136", "pdf": "https://arxiv.org/pdf/2412.06136.pdf", "abs": "https://arxiv.org/abs/2412.06136", "title": "AIDE: Attribute-Guided MultI-Hop Data Expansion for Data Scarcity in Task-Specific Fine-tuning", "authors": ["Jiayu Li", "Xuan Zhu", "Fang Liu", "Yanjun Qi"], "categories": ["cs.CL"], "comment": "Accepted for publication in ACL 2025. The official version will be\n  available in the ACL Anthology", "summary": "Fine-tuning large language models (LLMs) for specific tasks requires diverse,\nhigh-quality training data. However, obtaining sufficient relevant data remains\na significant challenge. Existing data synthesis methods either depend on\nextensive seed datasets or struggle to balance task relevance and data\ndiversity. To address these challenges, we propose Attribute-guided multI-hop\nData Expansion (AIDE), a novel data synthesis framework that uses a multi-hop\nprocess to expand very few seed data points while ensuring data diversity and\ntask relevance. AIDE extracts the main topic and key knowledge attributes from\nthe seeds to guide the synthesis steps. The process repeats for K hops, using\nthe generated data as seeds. To prevent irrelevant data generation as the hop\ndepth increases, AIDE incorporates a residual connection mechanism. Our\nempirical results show that AIDE enables fine-tuning of Mistral-7B,\nLlama-3.1-8B and Llama-3.2-3B from 10 seeds, surpassing the models fine-tuned\non human curated data. Furthermore, AIDE outperforms state-of-the-art data\nsynthesis methods, such as Evol-Instruct, by over 30% in task-specific\nfine-tuning. Code is available at https://github.com/Code4Graph/AIDE."}
{"id": "2412.14959", "pdf": "https://arxiv.org/pdf/2412.14959.pdf", "abs": "https://arxiv.org/abs/2412.14959", "title": "Understanding the Dark Side of LLMs' Intrinsic Self-Correction", "authors": ["Qingjie Zhang", "Di Wang", "Haoting Qian", "Yiming Li", "Tianwei Zhang", "Minlie Huang", "Ke Xu", "Hewu Li", "Yan Liu", "Han Qiu"], "categories": ["cs.CL"], "comment": null, "summary": "Intrinsic self-correction was proposed to improve LLMs' responses via\nfeedback prompts solely based on their inherent capability. However, recent\nworks show that LLMs' intrinsic self-correction fails without oracle labels as\nfeedback prompts. In this paper, we aim to interpret LLMs' intrinsic\nself-correction for different tasks, especially for those failure cases. By\nincluding one simple task and three complex tasks with state-of-the-art (SOTA)\nLLMs like ChatGPT families (o1, 4o, 3.5-turbo) and Llama families (2-7B, 3-8B,\nand 3.1-8B), we design three interpretation methods to reveal the dark side of\nLLMs' intrinsic self-correction. We identify intrinsic self-correction can (1)\ncause LLMs to waver both intermedia and final answers and lead to prompt bias\non simple factual questions; (2) introduce human-like cognitive bias on complex\ntasks. In light of our findings, we also provide two simple yet effective\nstrategies for alleviation: question repeating and supervised fine-tuning with\na few samples. We open-source our work at https://x-isc.info/."}
{"id": "2412.21033", "pdf": "https://arxiv.org/pdf/2412.21033.pdf", "abs": "https://arxiv.org/abs/2412.21033", "title": "Plancraft: an evaluation dataset for planning with LLM agents", "authors": ["Gautier Dagan", "Frank Keller", "Alex Lascarides"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present Plancraft, a multi-modal evaluation dataset for LLM agents.\nPlancraft has both a text-only and multi-modal interface, based on the\nMinecraft crafting GUI. We include the Minecraft Wiki to evaluate tool use and\nRetrieval Augmented Generation (RAG), as well as a handcrafted planner and\nOracle Retriever, to ablate the different components of a modern agent\narchitecture. To evaluate decision-making, Plancraft also includes a subset of\nexamples that are intentionally unsolvable, providing a realistic challenge\nthat requires the agent not only to complete tasks but also to decide whether\nthey are solvable at all. We benchmark both open-source and closed-source LLMs\nand compare their performance and efficiency to a handcrafted planner. Overall,\nwe find that LLMs and VLMs struggle with the planning problems that Plancraft\nintroduces, and offer suggestions on how to improve their capabilities."}
{"id": "2502.01706", "pdf": "https://arxiv.org/pdf/2502.01706.pdf", "abs": "https://arxiv.org/abs/2502.01706", "title": "Comply: Learning Sentences with Complex Weights inspired by Fruit Fly Olfaction", "authors": ["Alexei Figueroa", "Justus Westerhoff", "Golzar Atefi", "Dennis Fast", "Benjamin Winter", "Felix Alexander Gers", "Alexander Löser", "Wolfgang Nejdl"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "comment": "Accepted at NICE2025", "summary": "Biologically inspired neural networks offer alternative avenues to model data\ndistributions. FlyVec is a recent example that draws inspiration from the fruit\nfly's olfactory circuit to tackle the task of learning word embeddings.\nSurprisingly, this model performs competitively even against deep learning\napproaches specifically designed to encode text, and it does so with the\nhighest degree of computational efficiency. We pose the question of whether\nthis performance can be improved further. For this, we introduce Comply. By\nincorporating positional information through complex weights, we enable a\nsingle-layer neural network to learn sequence representations. Our experiments\nshow that Comply not only supersedes FlyVec but also performs on par with\nsignificantly larger state-of-the-art models. We achieve this without\nadditional parameters. Comply yields sparse contextual representations of\nsentences that can be interpreted explicitly from the neuron weights."}
{"id": "2502.16366", "pdf": "https://arxiv.org/pdf/2502.16366.pdf", "abs": "https://arxiv.org/abs/2502.16366", "title": "A Generative Approach to LLM Harmfulness Detection with Special Red Flag Tokens", "authors": ["Sophie Xhonneux", "David Dobre", "Mehrnaz Mofakhami", "Leo Schwinn", "Gauthier Gidel"], "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "comment": "14 pages, 6 figures", "summary": "Most safety training methods for large language models (LLMs) are based on\nfine-tuning that forces models to shift from an unsafe answer to refusal when\nfaced with harmful requests. Unfortunately, these drastic distribution shifts\ngenerally compromise model capabilities. To avoid that, we propose to expand\nthe model's vocabulary with a special token we call red flag token (<rf>) and\npropose to train the model to insert this token into its response at any time\nwhen harmful content is generated or about to be generated. Our approach offers\nseveral advantages: it enables the model to explicitly learn the concept of\nharmfulness while marginally affecting the generated distribution, thus\nmaintaining the model's utility. It also evaluates each generated answer and\nprovides robustness as good as adversarial training without the need to run\nattacks during training. Moreover, by encapsulating our safety tuning in a LoRA\nmodule, we provide additional defenses against fine-tuning API attacks."}
{"id": "2503.21073", "pdf": "https://arxiv.org/pdf/2503.21073.pdf", "abs": "https://arxiv.org/abs/2503.21073", "title": "Shared Global and Local Geometry of Language Model Embeddings", "authors": ["Andrew Lee", "Melanie Weber", "Fernanda Viégas", "Martin Wattenberg"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Researchers have recently suggested that models share common representations.\nIn our work, we find numerous geometric similarities across the token\nembeddings of large language models. First, we find ``global'' similarities:\ntoken embeddings often share similar relative orientations. Next, we\ncharacterize local geometry in two ways: (1) by using Locally Linear\nEmbeddings, and (2) by defining a simple measure for the intrinsic dimension of\neach embedding. Both characterizations allow us to find local similarities\nacross token embeddings. Additionally, our intrinsic dimension demonstrates\nthat embeddings lie on a lower dimensional manifold, and that tokens with lower\nintrinsic dimensions often have semantically coherent clusters, while those\nwith higher intrinsic dimensions do not. Based on our findings, we introduce\nEMB2EMB, a simple application to linearly transform steering vectors from one\nlanguage model to another, despite the two models having different dimensions."}
{"id": "2504.01738", "pdf": "https://arxiv.org/pdf/2504.01738.pdf", "abs": "https://arxiv.org/abs/2504.01738", "title": "Style over Substance: Distilled Language Models Reason Via Stylistic Replication", "authors": ["Philip Lippmann", "Jie Yang"], "categories": ["cs.CL", "cs.AI"], "comment": "To appear at COLM 2025", "summary": "Specialized reasoning language models (RLMs) have demonstrated that scaling\ntest-time computation through detailed reasoning traces significantly enhances\nperformance. Although these traces effectively facilitate knowledge\ndistillation into smaller, instruction-tuned models, the precise nature of\ntransferred reasoning remains unclear. In this study, we investigate to what\nextent distilled models internalize replicated stylistic patterns during\nreasoning. To this end, we systematically analyze reasoning traces, identifying\nstructural and lexical patterns that characterize successful reasoning. We then\nintroduce two new datasets -- a dataset of emergent reasoning traces and a\nsynthetic dataset explicitly constructed to replicate these stylistic patterns\n-- to precisely examine their influence on distilled models' reasoning\ncapabilities. We find that models trained on the synthetic traces achieve\ncomparable performance, indicating that distilled reasoning abilities rely\nsignificantly on surface-level patterns. Surprisingly, we observe an increase\nin performance even when the synthetic traces are altered to lead to the wrong\nanswer. Our findings highlight how stylistic patterns can be leveraged to\nefficiently enhance LM reasoning across diverse model families."}
{"id": "2504.05294", "pdf": "https://arxiv.org/pdf/2504.05294.pdf", "abs": "https://arxiv.org/abs/2504.05294", "title": "Truthful or Fabricated? Using Causal Attribution to Mitigate Reward Hacking in Explanations", "authors": ["Pedro Ferreira", "Wilker Aziz", "Ivan Titov"], "categories": ["cs.CL"], "comment": "20 pages, 10 figures, 6 tables", "summary": "Chain-of-thought explanations are widely used to inspect the decision process\nof large language models (LLMs) and to evaluate the trustworthiness of model\noutputs, making them important for effective collaboration between LLMs and\nhumans. We demonstrate that preference optimization - a key step in the\nalignment phase - can inadvertently reduce the faithfulness of these\nexplanations. This occurs because the reward model (RM), which guides\nalignment, is tasked with optimizing both the expected quality of the response\nand the appropriateness of the explanations (e.g., minimizing bias or adhering\nto safety standards), creating potential conflicts. The RM lacks a mechanism to\nassess the consistency between the model's internal decision process and the\ngenerated explanation. Consequently, the LLM may engage in \"reward hacking\" by\nproducing a final response that scores highly while giving an explanation\ntailored to maximize reward rather than accurately reflecting its reasoning. To\naddress this issue, we propose enriching the RM's input with a causal\nattribution of the prediction, allowing the RM to detect discrepancies between\nthe generated self-explanation and the model's decision process. In controlled\nsettings, we show that this approach reduces the tendency of the LLM to\ngenerate misleading explanations."}
{"id": "2504.10157", "pdf": "https://arxiv.org/pdf/2504.10157.pdf", "abs": "https://arxiv.org/abs/2504.10157", "title": "SocioVerse: A World Model for Social Simulation Powered by LLM Agents and A Pool of 10 Million Real-World Users", "authors": ["Xinnong Zhang", "Jiayu Lin", "Xinyi Mou", "Shiyue Yang", "Xiawei Liu", "Libo Sun", "Hanjia Lyu", "Yihang Yang", "Weihong Qi", "Yue Chen", "Guanying Li", "Ling Yan", "Yao Hu", "Siming Chen", "Yu Wang", "Xuanjing Huang", "Jiebo Luo", "Shiping Tang", "Libo Wu", "Baohua Zhou", "Zhongyu Wei"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Social simulation is transforming traditional social science research by\nmodeling human behavior through interactions between virtual individuals and\ntheir environments. With recent advances in large language models (LLMs), this\napproach has shown growing potential in capturing individual differences and\npredicting group behaviors. However, existing methods face alignment challenges\nrelated to the environment, target users, interaction mechanisms, and\nbehavioral patterns. To this end, we introduce SocioVerse, an LLM-agent-driven\nworld model for social simulation. Our framework features four powerful\nalignment components and a user pool of 10 million real individuals. To\nvalidate its effectiveness, we conducted large-scale simulation experiments\nacross three distinct domains: politics, news, and economics. Results\ndemonstrate that SocioVerse can reflect large-scale population dynamics while\nensuring diversity, credibility, and representativeness through standardized\nprocedures and minimal manual adjustments."}
{"id": "2504.11673", "pdf": "https://arxiv.org/pdf/2504.11673.pdf", "abs": "https://arxiv.org/abs/2504.11673", "title": "Deep Binding of Language Model Virtual Personas: a Study on Approximating Political Partisan Misperceptions", "authors": ["Minwoo Kang", "Suhong Moon", "Seung Hyeong Lee", "Ayush Raj", "Joseph Suh", "David M. Chan"], "categories": ["cs.CL"], "comment": "COLM 2025", "summary": "Large language models (LLMs) are increasingly capable of simulating human\nbehavior, offering cost-effective ways to estimate user responses to various\nsurveys and polls. However, the questions in these surveys usually reflect\nsocially understood attitudes: the patterns of attitudes of old/young,\nliberal/conservative, as understood by both members and non-members of those\ngroups. It is not clear whether the LLM binding is \\emph{deep}, meaning the LLM\nanswers as a member of a particular in-group would, or \\emph{shallow}, meaning\nthe LLM responds as an out-group member believes an in-group member would. To\nexplore this difference, we use questions that expose known in-group/out-group\nbiases. This level of fidelity is critical for applying LLMs to various\npolitical science studies, including timely topics on polarization dynamics,\ninter-group conflict, and democratic backsliding. To this end, we propose a\nnovel methodology for constructing virtual personas with synthetic user\n``backstories\" generated as extended, multi-turn interview transcripts. Our\ngenerated backstories are longer, rich in detail, and consistent in\nauthentically describing a singular individual, compared to previous methods.\nWe show that virtual personas conditioned on our backstories closely replicate\nhuman response distributions (up to an 87\\% improvement as measured by\nWasserstein Distance) and produce effect sizes that closely match those\nobserved in the original studies of in-group/out-group biases. Altogether, our\nwork extends the applicability of LLMs beyond estimating socially understood\nresponses, enabling their use in a broader range of human studies."}
{"id": "2504.12355", "pdf": "https://arxiv.org/pdf/2504.12355.pdf", "abs": "https://arxiv.org/abs/2504.12355", "title": "Leveraging Large Language Models for Multi-Class and Multi-Label Detection of Drug Use and Overdose Symptoms on Social Media", "authors": ["Muhammad Ahmad", "Fida Ullah", "Muhammad Usman", "Umyh Habiba", "ldar Batyrshin", "Grigori Sidorov"], "categories": ["cs.CL", "cs.AI", "cs.SI"], "comment": null, "summary": "Drug overdose remains a critical global health issue, often driven by misuse\nof opioids, painkillers, and psychiatric medications. Traditional research\nmethods face limitations, whereas social media offers real-time insights into\nself-reported substance use and overdose symptoms. This study proposes an\nAI-driven NLP framework trained on annotated social media data to detect\ncommonly used drugs and associated overdose symptoms. Using a hybrid annotation\nstrategy with LLMs and human annotators, we applied traditional ML models,\nneural networks, and advanced transformer-based models. Our framework achieved\n98% accuracy in multi-class and 97% in multi-label classification,\noutperforming baseline models by up to 8%. These findings highlight the\npotential of AI for supporting public health surveillance and personalized\nintervention strategies."}
{"id": "2505.00582", "pdf": "https://arxiv.org/pdf/2505.00582.pdf", "abs": "https://arxiv.org/abs/2505.00582", "title": "Block Circulant Adapter for Large Language Models", "authors": ["Xinyu Ding", "Meiqi Wang", "Siyu Liao", "Zhongfeng Wang"], "categories": ["cs.CL", "cs.LG"], "comment": "to appear in Proceedings of the 2025 International Joint Conference\n  on Artificial Intelligence (IJCAI-2025)", "summary": "Fine-tuning large language models (LLMs) is difficult due to their huge model\nsize. Recent Fourier domain-based methods show potential for reducing\nfine-tuning costs. We propose a block circulant matrix-based fine-tuning method\nwith a stable training heuristic to leverage the properties of circulant\nmatrices and one-dimensional Fourier transforms to reduce storage and\ncomputation costs. Experiments show that our method uses $14\\times$ less number\nof parameters than VeRA, $16\\times$ smaller than LoRA and $32\\times$ less FLOPs\nthan FourierFT, while maintaining close or better task performance. Our\napproach presents a promising way in frequency domain to fine-tune large models\non downstream tasks."}
{"id": "2505.05464", "pdf": "https://arxiv.org/pdf/2505.05464.pdf", "abs": "https://arxiv.org/abs/2505.05464", "title": "Bring Reason to Vision: Understanding Perception and Reasoning through Model Merging", "authors": ["Shiqi Chen", "Jinghan Zhang", "Tongyao Zhu", "Wei Liu", "Siyang Gao", "Miao Xiong", "Manling Li", "Junxian He"], "categories": ["cs.CL"], "comment": "ICML 2025. Camera-ready version updated. Our code is publicly\n  available at https://github.com/shiqichen17/VLM_Merging", "summary": "Vision-Language Models (VLMs) combine visual perception with the general\ncapabilities, such as reasoning, of Large Language Models (LLMs). However, the\nmechanisms by which these two abilities can be combined and contribute remain\npoorly understood. In this work, we explore to compose perception and reasoning\nthrough model merging that connects parameters of different models. Unlike\nprevious works that often focus on merging models of the same kind, we propose\nmerging models across modalities, enabling the incorporation of the reasoning\ncapabilities of LLMs into VLMs. Through extensive experiments, we demonstrate\nthat model merging offers a successful pathway to transfer reasoning abilities\nfrom LLMs to VLMs in a training-free manner. Moreover, we utilize the merged\nmodels to understand the internal mechanism of perception and reasoning and how\nmerging affects it. We find that perception capabilities are predominantly\nencoded in the early layers of the model, whereas reasoning is largely\nfacilitated by the middle-to-late layers. After merging, we observe that all\nlayers begin to contribute to reasoning, whereas the distribution of perception\nabilities across layers remains largely unchanged. These observations shed\nlight on the potential of model merging as a tool for multimodal integration\nand interpretation."}
{"id": "2505.06110", "pdf": "https://arxiv.org/pdf/2505.06110.pdf", "abs": "https://arxiv.org/abs/2505.06110", "title": "Multimodal Sentiment Analysis on CMU-MOSEI Dataset using Transformer-based Models", "authors": ["Jugal Gajjar", "Kaustik Ranaware"], "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 2 figures", "summary": "This project performs multimodal sentiment analysis using the CMU-MOSEI\ndataset, using transformer-based models with early fusion to integrate text,\naudio, and visual modalities. We employ BERT-based encoders for each modality,\nextracting embeddings that are concatenated before classification. The model\nachieves strong performance, with 97.87% 7-class accuracy and a 0.9682 F1-score\non the test set, demonstrating the effectiveness of early fusion in capturing\ncross-modal interactions. The training utilized Adam optimization (lr=1e-4),\ndropout (0.3), and early stopping to ensure generalization and robustness.\nResults highlight the superiority of transformer architectures in modeling\nmultimodal sentiment, with a low MAE (0.1060) indicating precise sentiment\nintensity prediction. Future work may compare fusion strategies or enhance\ninterpretability. This approach utilizes multimodal learning by effectively\ncombining linguistic, acoustic, and visual cues for sentiment analysis."}
{"id": "2505.08054", "pdf": "https://arxiv.org/pdf/2505.08054.pdf", "abs": "https://arxiv.org/abs/2505.08054", "title": "FalseReject: A Resource for Improving Contextual Safety and Mitigating Over-Refusals in LLMs via Structured Reasoning", "authors": ["Zhehao Zhang", "Weijie Xu", "Fanyou Wu", "Chandan K. Reddy"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at COLM 2025", "summary": "Safety alignment approaches in large language models (LLMs) often lead to the\nover-refusal of benign queries, significantly diminishing their utility in\nsensitive scenarios. To address this challenge, we introduce FalseReject, a\ncomprehensive resource containing 16k seemingly toxic queries accompanied by\nstructured responses across 44 safety-related categories. We propose a\ngraph-informed adversarial multi-agent interaction framework to generate\ndiverse and complex prompts, while structuring responses with explicit\nreasoning to aid models in accurately distinguishing safe from unsafe contexts.\nFalseReject includes training datasets tailored for both standard\ninstruction-tuned models and reasoning-oriented models, as well as a\nhuman-annotated benchmark test set. Our extensive benchmarking on 29\nstate-of-the-art (SOTA) LLMs reveals persistent over-refusal challenges.\nEmpirical results demonstrate that supervised finetuning with FalseReject\nsubstantially reduces unnecessary refusals without compromising overall safety\nor general language capabilities."}
{"id": "2505.11441", "pdf": "https://arxiv.org/pdf/2505.11441.pdf", "abs": "https://arxiv.org/abs/2505.11441", "title": "Is Compression Really Linear with Code Intelligence?", "authors": ["Shijie Xuyang", "Xianzhen Luo", "Tianhao Cheng", "Zheng Chu", "Houyi Li", "ziqi wang", "Siming Huang", "Qingfu Zhu", "Qiufeng Wang", "Xiangyu Zhang", "Shuigeng Zhou", "Wanxiang Che"], "categories": ["cs.CL"], "comment": "work in progress", "summary": "Understanding the relationship between data compression and the capabilities\nof Large Language Models (LLMs) is crucial, especially in specialized domains\nlike code intelligence. Prior work posited a linear relationship between\ncompression and general intelligence. However, it overlooked the multifaceted\nnature of code that encompasses diverse programming languages and tasks, and\nstruggled with fair evaluation of modern Code LLMs. We address this by\nevaluating a diverse array of open-source Code LLMs on comprehensive\nmulti-language, multi-task code benchmarks. To address the challenge of\nefficient and fair evaluation of pre-trained LLMs' code intelligence, we\nintroduce \\textit{Format Annealing}, a lightweight, transparent training\nmethodology designed to assess the intrinsic capabilities of these pre-trained\nmodels equitably. Compression efficacy, measured as bits-per-character (BPC),\nis determined using a novel, large-scale, and previously unseen code validation\nset derived from GitHub. Our empirical results reveal a fundamental logarithmic\nrelationship between measured code intelligence and BPC. This finding refines\nprior hypotheses of linearity, which we suggest are likely observations of the\nlogarithmic curve's tail under specific, limited conditions. Our work provides\na more nuanced understanding of compression's role in developing code\nintelligence and contributes a robust evaluation framework in the code domain."}
{"id": "2505.15075", "pdf": "https://arxiv.org/pdf/2505.15075.pdf", "abs": "https://arxiv.org/abs/2505.15075", "title": "Traveling Across Languages: Benchmarking Cross-Lingual Consistency in Multimodal LLMs", "authors": ["Hao Wang", "Pinzhi Huang", "Jihan Yang", "Saining Xie", "Daisuke Kawahara"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "https://github.com/nlp-waseda/traveling-across-languages", "summary": "The rapid evolution of multimodal large language models (MLLMs) has\nsignificantly enhanced their real-world applications. However, achieving\nconsistent performance across languages, especially when integrating cultural\nknowledge, remains a significant challenge. To better assess this issue, we\nintroduce two new benchmarks: KnowRecall and VisRecall, which evaluate\ncross-lingual consistency in MLLMs. KnowRecall is a visual question answering\nbenchmark designed to measure factual knowledge consistency in 15 languages,\nfocusing on cultural and historical questions about global landmarks. VisRecall\nassesses visual memory consistency by asking models to describe landmark\nappearances in 9 languages without access to images. Experimental results\nreveal that state-of-the-art MLLMs, including proprietary ones, still struggle\nto achieve cross-lingual consistency. This underscores the need for more robust\napproaches that produce truly multilingual and culturally aware models."}
{"id": "2505.17793", "pdf": "https://arxiv.org/pdf/2505.17793.pdf", "abs": "https://arxiv.org/abs/2505.17793", "title": "Compression Hacking: A Supplementary Perspective on Informatics Properties of Language Models from Geometric Distortion", "authors": ["Jianxiang Zang", "Meiling Ning", "Yongda Wei", "Shihan Dou", "Jiazheng Zhang", "Nijia Mo", "Binhong Li", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "categories": ["cs.CL"], "comment": null, "summary": "Recently, the concept of ``compression as intelligence'' has provided a novel\ninformatics metric perspective for language models (LMs), emphasizing that\nhighly structured representations signify the intelligence level of LMs.\nHowever, from a geometric standpoint, the word representation space of highly\ncompressed LMs tends to degenerate into a highly anisotropic state, which\nhinders the LM's ability to comprehend instructions and directly impacts its\nperformance. We found this compression-anisotropy synchronicity is essentially\nthe ``Compression Hacking'' in LM representations, where noise-dominated\ndirections tend to create the illusion of high compression rates by sacrificing\nspatial uniformity. Based on this, we propose three refined compression metrics\nby incorporating geometric distortion analysis and integrate them into a\nself-evaluation pipeline. The refined metrics exhibit strong alignment with the\nLM's comprehensive capabilities, achieving Spearman correlation coefficients\nabove 0.9, significantly outperforming both the original compression and other\ninternal structure-based metrics. This confirms that compression hacking\nsubstantially enhances the informatics interpretation of LMs by incorporating\ngeometric distortion of representations."}
{"id": "2506.00077", "pdf": "https://arxiv.org/pdf/2506.00077.pdf", "abs": "https://arxiv.org/abs/2506.00077", "title": "Gaussian mixture models as a proxy for interacting language models", "authors": ["Edward L. Wang", "Tianyu Wang", "Hayden Helm", "Avanti Athreya", "Vince Lyzinski", "Carey E. Priebe"], "categories": ["cs.CL", "cs.LG", "stat.ML", "62R07"], "comment": null, "summary": "Large language models (LLMs) are a powerful tool with the ability to match\nhuman capabilities and behavior in many settings. Retrieval-augmented\ngeneration (RAG) further allows LLMs to generate diverse output depending on\nthe contents of their RAG database. This motivates their use in the social\nsciences to study human behavior between individuals when large-scale\nexperiments are infeasible. However, LLMs depend on complex, computationally\nexpensive algorithms. In this paper, we introduce interacting Gaussian mixture\nmodels (GMMs) as an alternative to similar frameworks using LLMs. We compare a\nsimplified model of GMMs to select experimental simulations of LLMs whose\nupdating and response depend on feedback from other LLMs. We find that\ninteracting GMMs capture important features of the dynamics in interacting\nLLMs, and we investigate key similarities and differences between interacting\nLLMs and GMMs. We conclude by discussing the benefits of Gaussian mixture\nmodels, potential modifications, and future research directions."}
{"id": "2506.03106", "pdf": "https://arxiv.org/pdf/2506.03106.pdf", "abs": "https://arxiv.org/abs/2506.03106", "title": "Critique-GRPO: Advancing LLM Reasoning with Natural Language and Numerical Feedback", "authors": ["Xiaoying Zhang", "Hao Sun", "Yipeng Zhang", "Kaituo Feng", "Chaochao Lu", "Chao Yang", "Helen Meng"], "categories": ["cs.CL", "cs.AI"], "comment": "49 pages, updated with new experimental results", "summary": "Recent advances in reinforcement learning (RL) with numerical feedback, such\nas scalar rewards, have significantly enhanced the complex reasoning\ncapabilities of large language models (LLMs). Despite this success, we identify\nthree key challenges encountered by RL with solely numerical feedback:\nperformance plateaus, limited effectiveness of self-reflection, and persistent\nfailures. We then demonstrate that RL-finetuned models, even after exhibiting\nperformance plateaus, can generate correct refinements on persistently failed\nproblems by leveraging natural language feedback in the form of critiques.\nBuilding on this insight, we propose Critique-GRPO, an online RL framework that\nintegrates both natural language and numerical feedback for effective policy\noptimization. Critique-GRPO enables LLMs to learn from initial responses and\ncritique-guided self-refinements simultaneously while maintaining exploration.\nAdditionally, we employ a shaping function to amplify learning from correct,\nespecially unfamiliar, refinements and penalize incorrect ones. Extensive\nexperiments with Qwen2.5-7B-Base, Qwen2.5-Math-7B-Base, and Qwen3-8B\ndemonstrate that Critique-GRPO consistently outperforms supervised learning and\nRL-based fine-tuning methods across eight challenging mathematical, STEM, and\ngeneral reasoning tasks, improving average pass@1 scores by approximately 4.4%\nand 3.8% on Qwen2.5-7B-Base and Qwen3-8B, respectively. Notably, Critique-GRPO\nenables effective self-improvement through self-critiquing and weak-to-strong\ngeneralization, achieving consistent gains over GRPO, such as 16.7% and 10.0%\npass@1 improvements on AIME 2024, respectively."}
{"id": "2506.10077", "pdf": "https://arxiv.org/pdf/2506.10077.pdf", "abs": "https://arxiv.org/abs/2506.10077", "title": "A quantum semantic framework for natural language processing", "authors": ["Christopher J. Agostino", "Quan Le Thien", "Molly Apsel", "Denizhan Pak", "Elina Lesyk", "Ashabari Majumdar"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.IT", "math.IT"], "comment": "12 pages, 2 figures, accepted submission to Quantum AI and NLP 2025", "summary": "Semantic degeneracy represents a fundamental property of natural language\nthat extends beyond simple polysemy to encompass the combinatorial explosion of\npotential interpretations that emerges as semantic expressions increase in\ncomplexity. In this work, we argue this property imposes fundamental\nlimitations on Large Language Models (LLMs) and other modern NLP systems,\nprecisely because they operate within natural language itself. Using Kolmogorov\ncomplexity, we demonstrate that as an expression's complexity grows, the amount\nof contextual information required to reliably resolve its ambiguity explodes\ncombinatorially. The computational intractability of recovering a single\nintended meaning for complex or ambiguous text therefore suggests that the\nclassical view that linguistic forms possess intrinsic meaning in and of\nthemselves is conceptually inadequate. We argue instead that meaning is\ndynamically actualized through an observer-dependent interpretive act, a\nprocess whose non-deterministic nature is most appropriately described by a\nnon-classical, quantum-like logic. To test this hypothesis, we conducted a\nsemantic Bell inequality test using diverse LLM agents. Our experiments yielded\naverage CHSH expectation values from 1.2 to 2.8, with several runs producing\nvalues (e.g., 2.3-2.4) in significant violation of the classical boundary\n($|S|\\leq2$), demonstrating that linguistic interpretation under ambiguity can\nexhibit non-classical contextuality, consistent with results from human\ncognition experiments. These results inherently imply that classical\nfrequentist-based analytical approaches for natural language are necessarily\nlossy. Instead, we propose that Bayesian-style repeated sampling approaches can\nprovide more practically useful and appropriate characterizations of linguistic\nmeaning in context."}
{"id": "2506.14407", "pdf": "https://arxiv.org/pdf/2506.14407.pdf", "abs": "https://arxiv.org/abs/2506.14407", "title": "ImpliRet: Benchmarking the Implicit Fact Retrieval Challenge", "authors": ["Zeinab Sadat Taghavi", "Ali Modarressi", "Yunpu Ma", "Hinrich Schütze"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval systems are central to many NLP pipelines, but often rely on\nsurface-level cues such as keyword overlap and lexical semantic similarity. To\nevaluate retrieval beyond these shallow signals, recent benchmarks introduce\nreasoning-heavy queries; however, they primarily shift the burden to query-side\nprocessing techniques -- like prompting or multi-hop retrieval -- that can help\nresolve complexity. In contrast, we present ImpliRet, a benchmark that shifts\nthe reasoning challenge to document-side processing: The queries are simple,\nbut relevance depends on facts stated implicitly in documents through temporal\n(e.g., resolving \"two days ago\"), arithmetic, and world knowledge\nrelationships. We evaluate a range of sparse and dense retrievers, all of which\nstruggle in this setting: the best nDCG@10 is only 14.91%. We also test whether\nlong-context models can overcome this limitation. But even with a short context\nof only thirty documents, including the positive document, GPT-o4-mini scores\nonly 55.54%, showing that document-side reasoning remains a challenge. Our\ncodes are available at: github.com/ZeinabTaghavi/IMPLIRET"}
{"id": "2506.21596", "pdf": "https://arxiv.org/pdf/2506.21596.pdf", "abs": "https://arxiv.org/abs/2506.21596", "title": "Evaluating Multimodal Large Language Models on Educational Textbook Question Answering", "authors": ["Hessa A. Alawwad", "Anas Zafar", "Areej Alhothali", "Usman Naseem", "Ali Alkhathlan", "Amani Jamal"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "8 Pages", "summary": "Multimodal large language models (MLLMs) have shown success in\nvision-language tasks, but their ability to reason over complex educational\nmaterials remains largely untested. This work presents the first evaluation of\nstate-of-the-art MLLMs, including LLaVA-1.5 and LLaMA 3.2-Vision, on the\ntextbook question answering (TQA) task using the CK12-QA dataset. We introduce\na multimodal retrieval-augmented generation (RAG) pipeline to simulate\nreal-world learning by providing relevant lesson paragraphs and diagrams as\ncontext. Our zero-shot experiments reveal a critical trade-off: while retrieved\ncontext improves LLaVA's performance on text-based questions, it significantly\ndegrades the accuracy of the more powerful LLaMA 3.2-Vision on diagram-based\ntasks, dropping its validation accuracy from 74.07% to 25.93%. We term this\nstatistically significant phenomenon \"catastrophic context interference.\"\nFurthermore, fine-tuning highlights architectural differences: LLaMA\n3.2-Vision's performance improves to 71.16% on the test set, demonstrating its\ncapacity to learn multimodal integration, whereas LLaVA's performance declines,\nindicating challenges with generalization. Our results underscore the\nchallenges MLLMs face in modality prioritization and context integration,\nproviding a benchmark and pointing to key directions for developing more robust\nAI-driven educational tools."}
{"id": "2506.22760", "pdf": "https://arxiv.org/pdf/2506.22760.pdf", "abs": "https://arxiv.org/abs/2506.22760", "title": "Jan-nano Technical Report", "authors": ["Alan Dao", "Dinh Bach Vu"], "categories": ["cs.CL"], "comment": null, "summary": "Most language models face a fundamental tradeoff where powerful capabilities\nrequire substantial computational resources. We shatter this constraint with\nJan-nano, a 4B parameter language model that redefines efficiency through\nradical specialization: instead of trying to know everything, it masters the\nart of finding anything instantly. Fine-tuned from Qwen3-4B using our novel\nmulti-stage Reinforcement Learning with Verifiable Rewards (RLVR) system that\ncompletely eliminates reliance on next token prediction training (SFT),\nJan-nano achieves 83.2% on SimpleQA benchmark with MCP integration while\nrunning on consumer hardware. With 128K context length, Jan-nano proves that\nintelligence isn't about scale, it's about strategy."}
{"id": "2506.22791", "pdf": "https://arxiv.org/pdf/2506.22791.pdf", "abs": "https://arxiv.org/abs/2506.22791", "title": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in Large Language Models", "authors": ["Jianxin Yan", "Wangze Ni", "Lei Chen", "Xuemin Lin", "Peng Cheng", "Zhan Qin", "Kui Ren"], "categories": ["cs.CL", "cs.DB"], "comment": null, "summary": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications."}
{"id": "2507.00838", "pdf": "https://arxiv.org/pdf/2507.00838.pdf", "abs": "https://arxiv.org/abs/2507.00838", "title": "Stylometry recognizes human and LLM-generated texts in short samples", "authors": ["Karol Przystalski", "Jan K. Argasiński", "Iwona Grabska-Gradzińska", "Jeremi K. Ochab"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The paper explores stylometry as a method to distinguish between texts\ncreated by Large Language Models (LLMs) and humans, addressing issues of model\nattribution, intellectual property, and ethical AI use. Stylometry has been\nused extensively to characterise the style and attribute authorship of texts.\nBy applying it to LLM-generated texts, we identify their emergent writing\npatterns. The paper involves creating a benchmark dataset based on Wikipedia,\nwith (a) human-written term summaries, (b) texts generated purely by LLMs\n(GPT-3.5/4, LLaMa 2/3, Orca, and Falcon), (c) processed through multiple text\nsummarisation methods (T5, BART, Gensim, and Sumy), and (d) rephrasing methods\n(Dipper, T5). The 10-sentence long texts were classified by tree-based models\n(decision trees and LightGBM) using human-designed (StyloMetrix) and\nn-gram-based (our own pipeline) stylometric features that encode lexical,\ngrammatical, syntactic, and punctuation patterns. The cross-validated results\nreached a performance of up to .87 Matthews correlation coefficient in the\nmulticlass scenario with 7 classes, and accuracy between .79 and 1. in binary\nclassification, with the particular example of Wikipedia and GPT-4 reaching up\nto .98 accuracy on a balanced dataset. Shapley Additive Explanations pinpointed\nfeatures characteristic of the encyclopaedic text type, individual overused\nwords, as well as a greater grammatical standardisation of LLMs with respect to\nhuman-written texts. These results show -- crucially, in the context of the\nincreasingly sophisticated LLMs -- that it is possible to distinguish machine-\nfrom human-generated texts at least for a well-defined text type."}
{"id": "2507.02221", "pdf": "https://arxiv.org/pdf/2507.02221.pdf", "abs": "https://arxiv.org/abs/2507.02221", "title": "GDC Cohort Copilot: An AI Copilot for Curating Cohorts from the Genomic Data Commons", "authors": ["Steven Song", "Anirudh Subramanyam", "Zhenyu Zhang", "Aarti Venkat", "Robert L. Grossman"], "categories": ["cs.CL"], "comment": "12 pages, 1 figure, 7 tables. v2 updated to reflect migration to HF\n  Spaces", "summary": "The Genomic Data Commons (GDC) provides access to high quality, harmonized\ncancer genomics data through a unified curation and analysis platform centered\naround patient cohorts. While GDC users can interactively create complex\ncohorts through the graphical Cohort Builder, users (especially new ones) may\nstruggle to find specific cohort descriptors across hundreds of possible fields\nand properties. However, users may be better able to describe their desired\ncohort in free-text natural language. We introduce GDC Cohort Copilot, an\nopen-source copilot tool for curating cohorts from the GDC. GDC Cohort Copilot\nautomatically generates the GDC cohort filter corresponding to a user-input\nnatural language description of their desired cohort, before exporting the\ncohort back to the GDC for further analysis. An interactive user interface\nallows users to further refine the generated cohort. We develop and evaluate\nmultiple large language models (LLMs) for GDC Cohort Copilot and demonstrate\nthat our locally-served, open-source GDC Cohort LLM achieves better results\nthan GPT-4o prompting in generating GDC cohorts. We implement and share GDC\nCohort Copilot as a containerized Gradio app on HuggingFace Spaces, available\nat https://huggingface.co/spaces/uc-ctds/GDC-Cohort-Copilot. GDC Cohort LLM\nweights are available at https://huggingface.co/uc-ctds. All source code is\navailable at https://github.com/uc-cdis/gdc-cohort-copilot."}
{"id": "2507.02962", "pdf": "https://arxiv.org/pdf/2507.02962.pdf", "abs": "https://arxiv.org/abs/2507.02962", "title": "RAG-R1 : Incentivize the Search and Reasoning Capabilities of LLMs through Multi-query Parallelism", "authors": ["Zhiwen Tan", "Jiaming Huang", "Qintong Wu", "Hongxuan Zhang", "Chenyi Zhuang", "Jinjie Gu"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, while they remain prone to generating hallucinated or outdated\nresponses due to their static internal knowledge. Recent advancements in\nRetrieval-Augmented Generation (RAG) methods have explored enhancing models'\nsearch and reasoning capabilities through reinforcement learning (RL). Although\nthese methods demonstrate promising results, they face challenges in training\nstability and encounter issues such as substantial inference time and\nrestricted capabilities due to the single-query mode. In this paper, we propose\nRAG-R1, a novel training framework designed to enable LLMs to adaptively\nleverage internal and external knowledge during the reasoning process. We\nfurther expand the generation and retrieval processes within the framework from\nsingle-query mode to multi-query parallelism, aimed at reducing inference time\nand enhancing the model's capabilities. Extensive experiments on seven\nquestion-answering benchmarks demonstrate that our method outperforms the\nstrongest baseline by up to 13.2% and decreases inference time by 11.1%."}
{"id": "2507.04099", "pdf": "https://arxiv.org/pdf/2507.04099.pdf", "abs": "https://arxiv.org/abs/2507.04099", "title": "Conversation Forests: The Key to Fine Tuning Large Language Models for Multi-Turn Medical Conversations is Branching", "authors": ["Thomas Savage"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Fine-tuning methods such as Direct Preference Optimization (DPO) and Group\nRelative Policy Optimization (GRPO) have demonstrated success in training large\nlanguage models (LLMs) for single-turn tasks. However, these methods fall short\nin multi-turn applications, such as diagnostic patient interviewing, where\nunderstanding how early conversational turns influence downstream completions\nand outcomes is essential. In medicine, a multi-turn perspective is critical\nfor learning diagnostic schemas and better understanding conversation dynamics.\nTo address this gap, I introduce Savage Conversation Forests (SCF), a\nreinforcement learning framework that leverages a branched conversation\narchitecture to fine-tune LLMs for multi-turn dialogue. SCF generates multiple\npossible conversation continuations at each turn, enabling the model to learn\nhow different early responses affect downstream interactions and diagnostic\noutcomes. In experiments simulating doctor-patient conversations, SCF with\nbranching outperforms linear conversation architectures on diagnostic accuracy.\nI hypothesize that SCF's improvements stem from its ability to provide richer,\ninterdependent training signals across conversation turns. These results\nsuggest that a branched training architecture is an important strategy for fine\ntuning LLMs in complex multi-turn conversational tasks."}
{"id": "2507.05713", "pdf": "https://arxiv.org/pdf/2507.05713.pdf", "abs": "https://arxiv.org/abs/2507.05713", "title": "DRAGON: Dynamic RAG Benchmark On News", "authors": ["Fedor Chernogorskii", "Sergei Averkiev", "Liliya Kudraleeva", "Zaven Martirosian", "Maria Tikhonova", "Valentin Malykh", "Alena Fenogenova"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) is a widely adopted approach for\nimproving the factuality of large language models (LLMs) by incorporating\nexternal knowledge at inference time. Although there exist multiple RAG\nbenchmarks for English, evaluation resources for other languages, including\nRussian, remain scarce and static, failing to capture the dynamic nature of\nreal-world deployments. In this work, we present DRAGON (Dynamic RAG Benchmark\nOn News), the first dynamic benchmark for evaluating RAG systems in Russian on\na changing news corpora. DRAGON is built upon a regularly updated corpus of\nRussian news and public documents and supports comprehensive evaluation of both\nthe retriever and generator components. Question generation is performed\nautomatically with the use of Knowledge Graph constructed from the corpus and\nenables the extraction of four core question types aligned with distinct\nsubgraph patterns. We release a complete evaluation framework comprising the\npipeline for automatic question generation, evaluation scripts, which are\npotentially reusable for other languages and multilingual settings, and\nbenchmark data. We also launch a public leaderboard to encourage community\nparticipation and comparison."}
{"id": "2507.06313", "pdf": "https://arxiv.org/pdf/2507.06313.pdf", "abs": "https://arxiv.org/abs/2507.06313", "title": "ETT: Expanding the Long Context Understanding Capability of LLMs at Test-Time", "authors": ["Kiarash Zahirnia", "Zahra Golpayegani", "Walid Ahmed", "Yang Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Transformer-based Language Models' computation and memory overhead increase\nquadratically as a function of sequence length. The quadratic cost poses\nchallenges when employing LLMs for processing long sequences. In this work, we\nintroduce \\ourmodelacronym~(Extend at Test-Time), method for extending the\ncontext length of short context Transformer-based LLMs, with constant memory\nrequirement and linear computation overhead. ETT enable the extension of the\ncontext length at test-time by efficient fine-tuning the model's parameters on\nthe input context, chunked into overlapping small subsequences. We evaluate ETT\non LongBench by extending the context length of GPT-Large and Phi-2 up to 32\ntimes, increasing from 1k to 32k tokens. This results in up to a 30 percent\nimprovement in the model's accuracy. We also study how context can be stored in\nLLM's weights effectively and efficiently. Through a detailed ablation study,\nwe examine which Transformer modules are most beneficial to fine-tune at\ntest-time. Interestingly, we find that fine-tuning the second layer of the FFNs\nis more effective than full fine-tuning, leading to a further improvement in\nthe models' accuracy."}
{"id": "2507.06565", "pdf": "https://arxiv.org/pdf/2507.06565.pdf", "abs": "https://arxiv.org/abs/2507.06565", "title": "A Mathematical Theory of Discursive Networks", "authors": ["Juan B. Gutiérrez"], "categories": ["cs.CL", "cs.LG", "68T01, 60J10, 91D30, 05C82, 68T50, 68W20, 94A15", "I.2.7; I.2.11; G.3"], "comment": "32 pages, 4 figures, 4 tables, 1 algorithm, 54 references", "summary": "Large-language models (LLMs) turn writing into a live exchange between humans\nand software. We characterize this new medium as a discursive network that\ntreats people and LLMs as equal nodes and tracks how their statements\ncirculate. We define the generation of erroneous information as invalidation\n(any factual, logical, or structural breach) and show it follows four hazards:\ndrift from truth, self-repair, fresh fabrication, and external detection. We\ndevelop a general mathematical model of discursive networks that shows that a\nnetwork governed only by drift and self-repair stabilizes at a modest error\nrate. Giving each false claim even a small chance of peer review shifts the\nsystem to a truth-dominant state. We operationalize peer review with the\nopen-source \\emph{Flaws-of-Others (FOO) algorithm}: a configurable loop in\nwhich any set of agents critique one another while a harmonizer merges their\nverdicts. We identify an ethical transgression, epithesis, that occurs when\nhumans fail to engage in the discursive network. The takeaway is practical and\ncultural: reliability in this new medium comes not from perfecting single\nmodels but from connecting imperfect ones into networks that enforce mutual\naccountability."}
{"id": "2507.06803", "pdf": "https://arxiv.org/pdf/2507.06803.pdf", "abs": "https://arxiv.org/abs/2507.06803", "title": "Text to model via SysML: Automated generation of dynamical system computational models from unstructured natural language text via enhanced System Modeling Language diagrams", "authors": ["Matthew Anderson Hendricks", "Alice Cicirello"], "categories": ["cs.CL", "cs.AI", "cs.CE"], "comment": "v2 - typos and imprecisions corrected", "summary": "This paper contributes to speeding up the design and deployment of\nengineering dynamical systems by proposing a strategy for exploiting domain and\nexpert knowledge for the automated generation of dynamical system computational\nmodel starting from a corpus of document relevant to the dynamical system of\ninterest and an input document describing the specific system. This strategy is\nimplemented in five steps and, crucially, it uses system modeling language\ndiagrams (SysML) to extract accurate information about the dependencies,\nattributes, and operations of components. Natural Language Processing (NLP)\nstrategies and Large Language Models (LLMs) are employed in specific tasks to\nimprove intermediate outputs of the SySML diagrams automated generation, such\nas: list of key nouns; list of extracted relationships; list of key phrases and\nkey relationships; block attribute values; block relationships; and BDD diagram\ngeneration. The applicability of automated SysML diagram generation is\nillustrated with different case studies. The computational models of complex\ndynamical systems from SysML diagrams are then obtained via code generation and\ncomputational model generation steps. In the code generation step, NLP\nstrategies are used for summarization, while LLMs are used for validation only.\nThe proposed approach is not limited to a specific system, domain, or\ncomputational software. The applicability of the proposed approach is shown via\nan end-to-end example from text to model of a simple pendulum, showing improved\nperformance compared to results yielded by LLMs only."}
{"id": "2507.07505", "pdf": "https://arxiv.org/pdf/2507.07505.pdf", "abs": "https://arxiv.org/abs/2507.07505", "title": "Hallucination Stations: On Some Basic Limitations of Transformer-Based Language Models", "authors": ["Varin Sikka", "Vishal Sikka"], "categories": ["cs.CL", "cs.AI"], "comment": "6 pages; to be submitted to AAAI-26 after reviews", "summary": "In this paper we explore hallucinations and related capability limitations in\nLLMs and LLM-based agents from the perspective of computational complexity. We\nshow that beyond a certain complexity, LLMs are incapable of carrying out\ncomputational and agentic tasks or verifying their accuracy."}
{"id": "2507.07817", "pdf": "https://arxiv.org/pdf/2507.07817.pdf", "abs": "https://arxiv.org/abs/2507.07817", "title": "On the Effect of Instruction Tuning Loss on Generalization", "authors": ["Anwoy Chatterjee", "H S V N S Kowndinya Renduchintala", "Sumit Bhatia", "Tanmoy Chakraborty"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "To appear in Transactions of the Association for Computational\n  Linguistics (TACL)", "summary": "Instruction Tuning has emerged as a pivotal post-training paradigm that\nenables pre-trained language models to better follow user instructions. Despite\nits significance, little attention has been given to optimizing the loss\nfunction used. A fundamental, yet often overlooked, question is whether the\nconventional auto-regressive objective - where loss is computed only on\nresponse tokens, excluding prompt tokens - is truly optimal for instruction\ntuning. In this work, we systematically investigate the impact of\ndifferentially weighting prompt and response tokens in instruction tuning loss,\nand propose Weighted Instruction Tuning (WIT) as a better alternative to\nconventional instruction tuning. Through extensive experiments on five language\nmodels of different families and scale, three finetuning datasets of different\nsizes, and five diverse evaluation benchmarks, we show that the standard\ninstruction tuning loss often yields suboptimal performance and limited\nrobustness to input prompt variations. We find that a low-to-moderate weight\nfor prompt tokens coupled with a moderate-to-high weight for response tokens\nyields the best-performing models across settings and also serve as better\nstarting points for the subsequent preference alignment training. These\nfindings highlight the need to reconsider instruction tuning loss and offer\nactionable insights for developing more robust and generalizable models. Our\ncode is open-sourced at https://github.com/kowndinya-renduchintala/WIT."}
{"id": "2507.08297", "pdf": "https://arxiv.org/pdf/2507.08297.pdf", "abs": "https://arxiv.org/abs/2507.08297", "title": "KAT-V1: Kwai-AutoThink Technical Report", "authors": ["Zizheng Zhan", "Ken Deng", "Huaixi Tang", "Wen Xiang", "Kun Wu", "Weihao Li", "Wenqiang Zhu", "Jingxuan Xu", "Lecheng Huang", "Zongxian Feng", "Shaojie Wang", "Shangpeng Yan", "Xuxing Chen", "Jiaheng Liu", "Zhongyuan Peng", "Zuchen Gao", "Haoyang Huang", "Xiaojiang Zhang", "Jinghui Wang", "Zheng Lin", "Mengtong Li", "Huiming Wang", "Ziqi Zhan", "Yanan Wu", "Yuanxing Zhang", "Jian Yang", "Guang Chen", "Haotian Zhang", "Bin Chen", "Bing Yu"], "categories": ["cs.CL"], "comment": null, "summary": "We present Kwaipilot-AutoThink (KAT), an open-source 40B large language model\ndeveloped to address the overthinking problem in reasoning-intensive tasks,\nwhere an automatic thinking training paradigm is proposed to dynamically switch\nbetween reasoning and non-reasoning modes based on task complexity.\nSpecifically, first, we construct the dual-regime dataset based on a novel\ntagging pipeline and a multi-agent synthesis strategy, and then we apply\nMulti-Token Prediction (MTP)-enhanced knowledge distillation, enabling\nefficient and fine-grained reasoning transfer with minimal pretraining cost.\nBesides, we implement a cold-start initialization strategy that introduces\nmode-selection priors using majority-vote signals and intent-aware prompting.\nFinally, we propose Step-SRPO, a reinforcement learning algorithm that\nincorporates intermediate supervision into the GRPO framework, offering\nstructured guidance over both reasoning-mode selection and response accuracy.\nExtensive experiments across multiple benchmarks demonstrate that KAT\nconsistently matches or even outperforms current state-of-the-art models,\nincluding DeepSeek-R1-0528 and Qwen3-235B-A22B, across a wide range of\nreasoning-intensive tasks while reducing token usage by up to approximately\n30\\%. Beyond academic evaluation, KAT has been successfully deployed in\nKwaipilot (i.e., Kuaishou's internal coding assistant), and improves real-world\ndevelopment workflows with high accuracy, efficiency, and controllable\nreasoning behaviors. Moreover, we are actively training a 200B\nMixture-of-Experts (MoE) with 40B activation parameters, where the early-stage\nresults already demonstrate promising improvements in performance and\nefficiency, further showing the scalability of the AutoThink paradigm."}
{"id": "2507.08606", "pdf": "https://arxiv.org/pdf/2507.08606.pdf", "abs": "https://arxiv.org/abs/2507.08606", "title": "DocPolarBERT: A Pre-trained Model for Document Understanding with Relative Polar Coordinate Encoding of Layout Structures", "authors": ["Benno Uthayasooriyar", "Antoine Ly", "Franck Vermet", "Caio Corro"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce DocPolarBERT, a layout-aware BERT model for document\nunderstanding that eliminates the need for absolute 2D positional embeddings.\nWe extend self-attention to take into account text block positions in relative\npolar coordinate system rather than the Cartesian one. Despite being\npre-trained on a dataset more than six times smaller than the widely used\nIIT-CDIP corpus, DocPolarBERT achieves state-of-the-art results. These results\ndemonstrate that a carefully designed attention mechanism can compensate for\nreduced pre-training data, offering an efficient and effective alternative for\ndocument understanding."}
{"id": "2507.08898", "pdf": "https://arxiv.org/pdf/2507.08898.pdf", "abs": "https://arxiv.org/abs/2507.08898", "title": "SEALGuard: Safeguarding the Multilingual Conversations in Southeast Asian Languages for LLM Software Systems", "authors": ["Wenliang Shan", "Michael Fu", "Rui Yang", "Chakkrit Tantithamthavorn"], "categories": ["cs.CL", "cs.AI"], "comment": "Under Review at Information and Software Technology", "summary": "Safety alignment is critical for LLM-powered systems. While recent\nLLM-powered guardrail approaches such as LlamaGuard achieve high detection\naccuracy of unsafe inputs written in English (e.g., ``How to create a bomb?''),\nthey struggle with multilingual unsafe inputs. This limitation leaves LLM\nsystems vulnerable to unsafe and jailbreak prompts written in low-resource\nlanguages such as those in Southeast Asia. This paper introduces SEALGuard, a\nmultilingual guardrail designed to improve the safety alignment across diverse\nlanguages. It aims to address the multilingual safety alignment gap of existing\nguardrails and ensure effective filtering of unsafe and jailbreak prompts in\nLLM-powered systems. We adapt a general-purpose multilingual language model\ninto a multilingual guardrail using low-rank adaptation (LoRA). We construct\nSEALSBench, a large-scale multilingual safety alignment dataset containing over\n260,000 prompts in ten languages, including safe, unsafe, and jailbreak cases.\nWe evaluate SEALGuard against state-of-the-art guardrails such as LlamaGuard on\nthis benchmark. Our findings show that multilingual unsafe and jailbreak\nprompts substantially degrade the performance of the state-of-the-art\nLlamaGuard, which experiences a drop in Defense Success Rate (DSR) by 9% and\n18%, respectively, compared to its performance on English-only prompts. In\ncontrast, SEALGuard outperforms existing guardrails in detecting multilingual\nunsafe and jailbreak prompts, improving DSR by 48% over LlamaGuard and\nachieving the best DSR, precision, and F1-score. Our ablation study further\nreveals the contributions of adaptation strategies and model size to the\noverall performance of SEALGuard. SEALGuard advances the safety alignment of\nLLM systems by introducing an effective multilingual guardrail."}
{"id": "2507.10541", "pdf": "https://arxiv.org/pdf/2507.10541.pdf", "abs": "https://arxiv.org/abs/2507.10541", "title": "REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once", "authors": ["Zhuoshi Pan", "Qizhi Pei", "Yu Li", "Qiyao Sun", "Zinan Tang", "H. Vicky Zhao", "Conghui He", "Lijun Wu"], "categories": ["cs.CL"], "comment": "REST (Reasoning Evaluation through Simultaneous Testing), a\n  stress-testing framework that concurrently exposes LRMs to multiple problems\n  simultaneously", "summary": "Recent Large Reasoning Models (LRMs) have achieved remarkable progress on\ntask-specific benchmarks, yet their evaluation methods remain constrained by\nisolated problem-solving paradigms. Existing benchmarks predominantly assess\nsingle-question reasoning through sequential testing, resulting critical\nlimitations: (1) vulnerability to data contamination and less challenging\n(e.g., DeepSeek-R1 achieves 97.0% on MATH500), forcing costly creation of new\nquestions with large human efforts, (2) failure to evaluate models under\nmulti-context pressure, a key requirement for real-world deployment. To bridge\nthis gap, we present REST (Reasoning Evaluation through Simultaneous Testing),\na stress-testing framework that exposes LRMs to multiple problems\nsimultaneously. Beyond basic reasoning, REST evaluates several under-tested\ncapabilities: contextual priority allocation, cross-problem interference\nresistance, and dynamic cognitive load management. Our evaluation reveals\nseveral striking findings: Even state-of-the-art (SOTA) models like DeepSeek-R1\nexhibit substantial performance degradation under stress testing. Crucially,\nREST demonstrates stronger discriminative power than existing benchmarks,\nrevealing pronounced performance differences among models that exhibit similar,\nnear-ceiling performance under single-question evaluations. Some key insights\nemerge from our analysis: (1) the \"overthinking trap\" is a critical factor\ncontributing to the performance degradation; (2) the models trained with\n\"long2short\" technique preserve more accuracy of their single-problem\nperformance under REST, outperforming standard-trained counterparts. These\nresults establish REST as a cost-efficient, future-proof evaluation paradigm\nthat better reflects real-world reasoning demands while reducing reliance on\ncontinuous human annotation. Code and results are available at\nhttps://opendatalab.github.io/REST."}
{"id": "2407.09975", "pdf": "https://arxiv.org/pdf/2407.09975.pdf", "abs": "https://arxiv.org/abs/2407.09975", "title": "The GPT Surprise: Offering Large Language Model Chat in a Massive Coding Class Reduced Engagement but Increased Adopters Exam Performances", "authors": ["Allen Nie", "Yash Chandak", "Miroslav Suzara", "Ali Malik", "Juliette Woodrow", "Matt Peng", "Mehran Sahami", "Emma Brunskill", "Chris Piech"], "categories": ["cs.CY", "cs.AI", "cs.CL", "stat.AP"], "comment": "32 pages. Published at L@S 2025", "summary": "Large language models (LLMs) are quickly being adopted in a wide range of\nlearning experiences, especially via ubiquitous and broadly accessible chat\ninterfaces like ChatGPT and Copilot. This type of interface is readily\navailable to students and teachers around the world, yet relatively little\nresearch has been done to assess the impact of such generic tools on student\nlearning. Coding education is an interesting test case, both because LLMs have\nstrong performance on coding tasks, and because LLM-powered support tools are\nrapidly becoming part of the workflow of professional software engineers. To\nhelp understand the impact of generic LLM use on coding education, we conducted\na large-scale randomized control trial with 5,831 students from 146 countries\nin an online coding class in which we provided some students with access to a\nchat interface with GPT-4. We estimate positive benefits on exam performance\nfor adopters, the students who used the tool, but over all students, the\nadvertisement of GPT-4 led to a significant average decrease in exam\nparticipation. We observe similar decreases in other forms of course\nengagement. However, this decrease is modulated by the student's country of\norigin. Offering access to LLMs to students from low human development index\ncountries increased their exam participation rate on average. Our results\nsuggest there may be promising benefits to using LLMs in an introductory coding\nclass, but also potential harms for engagement, which makes their longer term\nimpact on student success unclear. Our work highlights the need for additional\ninvestigations to help understand the potential impact of future adoption and\nintegration of LLMs into classrooms."}
{"id": "2409.17755", "pdf": "https://arxiv.org/pdf/2409.17755.pdf", "abs": "https://arxiv.org/abs/2409.17755", "title": "SECURE: Semantics-aware Embodied Conversation under Unawareness for Lifelong Robot Learning", "authors": ["Rimvydas Rubavicius", "Peter David Fagan", "Alex Lascarides", "Subramanian Ramamoorthy"], "categories": ["cs.RO", "cs.AI", "cs.CL"], "comment": "Published at 4th Conference on Lifelong Learning Agents (CoLLAs),\n  2025", "summary": "This paper addresses a challenging interactive task learning scenario we call\nrearrangement under unawareness: an agent must manipulate a rigid-body\nenvironment without knowing a key concept necessary for solving the task and\nmust learn about it during deployment. For example, the user may ask to \"put\nthe two granny smith apples inside the basket\", but the agent cannot correctly\nidentify which objects in the environment are \"granny smith\" as the agent has\nnot been exposed to such a concept before. We introduce SECURE, an interactive\ntask learning policy designed to tackle such scenarios. The unique feature of\nSECURE is its ability to enable agents to engage in semantic analysis when\nprocessing embodied conversations and making decisions. Through embodied\nconversation, a SECURE agent adjusts its deficient domain model by engaging in\ndialogue to identify and learn about previously unforeseen possibilities. The\nSECURE agent learns from the user's embodied corrective feedback when mistakes\nare made and strategically engages in dialogue to uncover useful information\nabout novel concepts relevant to the task. These capabilities enable the SECURE\nagent to generalize to new tasks with the acquired knowledge. We demonstrate in\nthe simulated Blocksworld and the real-world apple manipulation environments\nthat the SECURE agent, which solves such rearrangements under unawareness, is\nmore data-efficient than agents that do not engage in embodied conversation or\nsemantic analysis."}
{"id": "2410.23022", "pdf": "https://arxiv.org/pdf/2410.23022.pdf", "abs": "https://arxiv.org/abs/2410.23022", "title": "Online Intrinsic Rewards for Decision Making Agents from Large Language Model Feedback", "authors": ["Qinqing Zheng", "Mikael Henaff", "Amy Zhang", "Aditya Grover", "Brandon Amos"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.RO"], "comment": null, "summary": "Automatically synthesizing dense rewards from natural language descriptions\nis a promising paradigm in reinforcement learning (RL), with applications to\nsparse reward problems, open-ended exploration, and hierarchical skill design.\nRecent works have made promising steps by exploiting the prior knowledge of\nlarge language models (LLMs). However, these approaches suffer from important\nlimitations: they are either not scalable to problems requiring billions of\nenvironment samples, due to requiring LLM annotations for each observation, or\nthey require a diverse offline dataset, which may not exist or be impossible to\ncollect. In this work, we address these limitations through a combination of\nalgorithmic and systems-level contributions. We propose ONI, a distributed\narchitecture that simultaneously learns an RL policy and an intrinsic reward\nfunction using LLM feedback. Our approach annotates the agent's collected\nexperience via an asynchronous LLM server, which is then distilled into an\nintrinsic reward model. We explore a range of algorithmic choices for reward\nmodeling with varying complexity, including hashing, classification, and\nranking models. Our approach achieves state-of-the-art performance across a\nrange of challenging tasks from the NetHack Learning Environment, while\nremoving the need for large offline datasets required by prior work. We make\nour code available at https://github.com/facebookresearch/oni ."}
{"id": "2411.02820", "pdf": "https://arxiv.org/pdf/2411.02820.pdf", "abs": "https://arxiv.org/abs/2411.02820", "title": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM Serving", "authors": ["Yuhan Liu", "Yuyang Huang", "Jiayi Yao", "Shaoting Feng", "Zhuohan Gu", "Kuntai Du", "Hanchen Li", "Yihua Cheng", "Junchen Jiang", "Shan Lu", "Madan Musuvathi", "Esha Choukse"], "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Compound AI systems, such as agentic systems, are an emerging trend in\nlarge-scale enterprise settings, with multiple LLMs specialized for different\nusers, tasks, and/or roles working together. In these scenarios, different\nmodels often process inputs that share the same context prefix. Although much\nwork was done in the past to enable the reuse of prefix KV caches across inputs\nfor a single model, how to enable one model to reuse the prefix KV caches of a\ndifferent model remains an open question.\n  We introduce DroidSpeak, the first distributed LLM inference system that\nenables KV cache reuse across distributed nodes running inference of different\nLLMs, so long as the LLMs have the same architecture. We present the first\nstudy that aims at understanding the impact of sharing KV caches across\ndifferent LLMs, and if/when such sharing affects quality. Inspired by the\nfindings, we present DroidSpeak, which selectively recomputes a few layers of\nthe KV cache produced by another LLM and reuses the remaining layers, with\nnegligible quality loss. Moreover, carefully pipelining the layer-wise\nre-computation and the loading of reused KV cache further improves the\ninference performance. Experiments on diverse datasets and model pairs\ndemonstrate that DroidSpeak achieves up to 4x throughput improvement and about\n3.1x faster prefill (time to first token), with negligible loss of quality in\nF1 scores, Rouge-L or code similarity score, compared to the baseline which\ndoes not allow any sharing across models."}
{"id": "2412.18424", "pdf": "https://arxiv.org/pdf/2412.18424.pdf", "abs": "https://arxiv.org/abs/2412.18424", "title": "LongDocURL: a Comprehensive Multimodal Long Document Benchmark Integrating Understanding, Reasoning, and Locating", "authors": ["Chao Deng", "Jiale Yuan", "Pi Bu", "Peijie Wang", "Zhong-Zhi Li", "Jian Xu", "Xiao-Hui Li", "Yuan Gao", "Jun Song", "Bo Zheng", "Cheng-Lin Liu"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large vision language models (LVLMs) have improved the document understanding\ncapabilities remarkably, enabling the handling of complex document elements,\nlonger contexts, and a wider range of tasks. However, existing document\nunderstanding benchmarks have been limited to handling only a small number of\npages and fail to provide a comprehensive analysis of layout elements locating.\nIn this paper, we first define three primary task categories: Long Document\nUnderstanding, numerical Reasoning, and cross-element Locating, and then\npropose a comprehensive benchmark, LongDocURL, integrating above three primary\ntasks and comprising 20 sub-tasks categorized based on different primary tasks\nand answer evidences. Furthermore, we develop a semi-automated construction\npipeline and collect 2,325 high-quality question-answering pairs, covering more\nthan 33,000 pages of documents, significantly outperforming existing\nbenchmarks. Subsequently, we conduct comprehensive evaluation experiments on\nboth open-source and closed-source models across 26 different configurations,\nrevealing critical performance gaps in this field."}
{"id": "2502.01100", "pdf": "https://arxiv.org/pdf/2502.01100.pdf", "abs": "https://arxiv.org/abs/2502.01100", "title": "ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning", "authors": ["Bill Yuchen Lin", "Ronan Le Bras", "Kyle Richardson", "Ashish Sabharwal", "Radha Poovendran", "Peter Clark", "Yejin Choi"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted to ICML 2025", "summary": "We investigate the logical reasoning capabilities of large language models\n(LLMs) and their scalability in complex non-monotonic reasoning. To this end,\nwe introduce ZebraLogic, a comprehensive evaluation framework for assessing LLM\nreasoning performance on logic grid puzzles derived from constraint\nsatisfaction problems (CSPs). ZebraLogic enables the generation of puzzles with\ncontrollable and quantifiable complexity, facilitating a systematic study of\nthe scaling limits of models such as Llama, o1 models, and DeepSeek-R1. By\nencompassing a broad range of search space complexities and diverse logical\nconstraints, ZebraLogic provides a structured environment to evaluate reasoning\nunder increasing difficulty.\n  Our results reveal a significant decline in accuracy as problem complexity\ngrows -- a phenomenon we term the curse of complexity. This limitation persists\neven with larger models and increased inference-time computation, suggesting\ninherent constraints in current LLM reasoning capabilities. Additionally, we\nexplore strategies to enhance logical reasoning, including Best-of-N sampling,\nbacktracking mechanisms, and self-verification prompts. Our findings offer\ncritical insights into the scalability of LLM reasoning, highlight fundamental\nlimitations, and outline potential directions for improvement."}
{"id": "2502.04644", "pdf": "https://arxiv.org/pdf/2502.04644.pdf", "abs": "https://arxiv.org/abs/2502.04644", "title": "Agentic Reasoning: A Streamlined Framework for Enhancing LLM Reasoning with Agentic Tools", "authors": ["Junde Wu", "Jiayuan Zhu", "Yuyuan Liu", "Min Xu", "Yueming Jin"], "categories": ["cs.AI", "cs.CL"], "comment": "ACL 2025", "summary": "We introduce Agentic Reasoning, a framework that enhances large language\nmodel (LLM) reasoning by integrating external tool-using agents. Agentic\nReasoning dynamically leverages web search, code execution, and structured\nmemory to address complex problems requiring deep research. A key innovation in\nour framework is the Mind-Map agent, which constructs a structured knowledge\ngraph to store reasoning context and track logical relationships, ensuring\ncoherence in long reasoning chains with extensive tool usage. Additionally, we\nconduct a comprehensive exploration of the Web-Search agent, leading to a\nhighly effective search mechanism that surpasses all prior approaches. When\ndeployed on DeepSeek-R1, our method achieves a new state-of-the-art (SOTA)\namong public models and delivers performance comparable to OpenAI Deep\nResearch, the leading proprietary model in this domain. Extensive ablation\nstudies validate the optimal selection of agentic tools and confirm the\neffectiveness of our Mind-Map and Web-Search agents in enhancing LLM reasoning.\nThe code is at: https://github.com/theworldofagents/Agentic-Reasoning"}
{"id": "2502.14565", "pdf": "https://arxiv.org/pdf/2502.14565.pdf", "abs": "https://arxiv.org/abs/2502.14565", "title": "ReVISE: Learning to Refine at Test-Time via Intrinsic Self-Verification", "authors": ["Hyunseok Lee", "Seunghyuk Oh", "Jaehyung Kim", "Jinwoo Shin", "Jihoon Tack"], "categories": ["cs.LG", "cs.CL"], "comment": "Published as conference proceeding for ICML 2025. First two authors\n  contributed equally", "summary": "Self-awareness, i.e., the ability to assess and correct one's own generation,\nis a fundamental aspect of human intelligence, making its replication in large\nlanguage models (LLMs) an important yet challenging task. Previous works tackle\nthis by employing extensive reinforcement learning or rather relying on large\nexternal verifiers. In this work, we propose Refine via Intrinsic\nSelf-Verification (ReVISE), an efficient and effective framework that enables\nLLMs to self-correct their outputs through self-verification. The core idea of\nReVISE is to enable LLMs to verify their reasoning processes and continually\nrethink reasoning trajectories based on its verification. We introduce a\nstructured curriculum based upon online preference learning to implement this\nefficiently. Specifically, as ReVISE involves two challenging tasks (i.e.,\nself-verification and reasoning correction), we tackle each task sequentially\nusing curriculum learning, collecting both failed and successful reasoning\npaths to construct preference pairs for efficient training. During inference,\nour approach enjoys natural test-time scaling by integrating self-verification\nand correction capabilities, further enhanced by our proposed confidence-aware\ndecoding mechanism. Our experiments on various reasoning tasks demonstrate that\nReVISE achieves efficient self-correction and significantly improves reasoning\nperformance."}
{"id": "2502.19130", "pdf": "https://arxiv.org/pdf/2502.19130.pdf", "abs": "https://arxiv.org/abs/2502.19130", "title": "Voting or Consensus? Decision-Making in Multi-Agent Debate", "authors": ["Lars Benedikt Kaesberg", "Jonas Becker", "Jan Philip Wahle", "Terry Ruas", "Bela Gipp"], "categories": ["cs.MA", "cs.AI", "cs.CL"], "comment": "Accepted at ACL2025 (Findings)", "summary": "Much of the success of multi-agent debates depends on carefully choosing the\nright parameters. The decision-making protocol stands out as it can highly\nimpact final model answers, depending on how decisions are reached. Systematic\ncomparison of decision protocols is difficult because many studies alter\nmultiple discussion parameters beyond the protocol. So far, it has been largely\nunknown how decision-making influences different tasks. This work\nsystematically evaluates the impact of seven decision protocols (e.g., majority\nvoting, unanimity consensus). We change only one variable at a time - the\ndecision protocol - to analyze how different methods affect the collaboration\nbetween agents and measure differences in knowledge and reasoning tasks. Our\nresults show that voting protocols improve performance by 13.2% in reasoning\ntasks and consensus protocols by 2.8% in knowledge tasks compared to other\ndecision protocols. Increasing the number of agents improves performance, while\nmore discussion rounds before voting reduce it. To improve decision-making by\nincreasing answer diversity, we propose two new methods, All-Agents Drafting\n(AAD) and Collective Improvement (CI). Our methods improve task performance by\nup to 3.3% with AAD and up to 7.4% with CI. This work demonstrates the\nimportance of decision-making in multi-agent debates beyond scaling."}
{"id": "2504.01550", "pdf": "https://arxiv.org/pdf/2504.01550.pdf", "abs": "https://arxiv.org/abs/2504.01550", "title": "Representation Bending for Large Language Model Safety", "authors": ["Ashkan Yousefpour", "Taeheon Kim", "Ryan S. Kwon", "Seungbeen Lee", "Wonje Jeung", "Seungju Han", "Alvin Wan", "Harrison Ngan", "Youngjae Yu", "Jonghyun Choi"], "categories": ["cs.LG", "cs.CL", "cs.CR"], "comment": "Accepted to ACL 2025 (main)", "summary": "Large Language Models (LLMs) have emerged as powerful tools, but their\ninherent safety risks - ranging from harmful content generation to broader\nsocietal harms - pose significant challenges. These risks can be amplified by\nthe recent adversarial attacks, fine-tuning vulnerabilities, and the increasing\ndeployment of LLMs in high-stakes environments. Existing safety-enhancing\ntechniques, such as fine-tuning with human feedback or adversarial training,\nare still vulnerable as they address specific threats and often fail to\ngeneralize across unseen attacks, or require manual system-level defenses. This\npaper introduces RepBend, a novel approach that fundamentally disrupts the\nrepresentations underlying harmful behaviors in LLMs, offering a scalable\nsolution to enhance (potentially inherent) safety. RepBend brings the idea of\nactivation steering - simple vector arithmetic for steering model's behavior\nduring inference - to loss-based fine-tuning. Through extensive evaluation,\nRepBend achieves state-of-the-art performance, outperforming prior methods such\nas Circuit Breaker, RMU, and NPO, with up to 95% reduction in attack success\nrates across diverse jailbreak benchmarks, all with negligible reduction in\nmodel usability and general capabilities."}
{"id": "2505.05763", "pdf": "https://arxiv.org/pdf/2505.05763.pdf", "abs": "https://arxiv.org/abs/2505.05763", "title": "BMDetect: A Multimodal Deep Learning Framework for Comprehensive Biomedical Misconduct Detection", "authors": ["Yize Zhou", "Jie Zhang", "Meijie Wang", "Lun Yu"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Academic misconduct detection in biomedical research remains challenging due\nto algorithmic narrowness in existing methods and fragmented analytical\npipelines. We present BMDetect, a multimodal deep learning framework that\nintegrates journal metadata (SJR, institutional data), semantic embeddings\n(PubMedBERT), and GPT-4o-mined textual attributes (methodological statistics,\ndata anomalies) for holistic manuscript evaluation. Key innovations include:\n(1) multimodal fusion of domain-specific features to reduce detection bias; (2)\nquantitative evaluation of feature importance, identifying journal authority\nmetrics (e.g., SJR-index) and textual anomalies (e.g., statistical outliers) as\ndominant predictors; and (3) the BioMCD dataset, a large-scale benchmark with\n13,160 retracted articles and 53,411 controls. BMDetect achieves 74.33% AUC,\noutperforming single-modality baselines by 8.6%, and demonstrates\ntransferability across biomedical subfields. This work advances scalable,\ninterpretable tools for safeguarding research integrity."}
{"id": "2506.07945", "pdf": "https://arxiv.org/pdf/2506.07945.pdf", "abs": "https://arxiv.org/abs/2506.07945", "title": "ProtocolLLM: RTL Benchmark for SystemVerilog Generation of Communication Protocols", "authors": ["Arnav Sheth", "Ivaxi Sheth", "Mario Fritz"], "categories": ["cs.AR", "cs.AI", "cs.CL"], "comment": "Accepted at MLSysArch@ISCA 2025", "summary": "Recent advances in large language models (LLMs) have demonstrated strong\nperformance in generating code for general-purpose programming languages.\nHowever, their potential for hardware description languages (HDLs), such as\nSystemVerilog, remains largely unexplored. HDL code generation poses unique\nchallenges due to strict timing semantics, concurrency, and synthesizability\nconstraints essential for correct hardware functionality. Further, HDL-based\ndesign flows encompass a broad set of tasks beyond structural code generation,\nincluding testbench development, assertion-based verification, timing closure,\nand protocol-level integration for on-chip communication. In this work, we\nevaluate the capabilities of both open-source and state-of-the-art LLMs in\ngenerating synthesizable and functionally accurate SystemVerilog\nimplementations of widely used communication protocols that are critical\ncomponents of embedded and System-on-Chip (SoC) systems. We introduce\nProtocolLLM, the first benchmark suite specifically targeting these protocols\nwith tasks spanning multiple design abstraction levels and varying prompt\nspecificity. Our evaluation method also focuses on timing correctness in\naddition to synthesizability and syntactic correctness. We observe that most of\nthe models fail to generate SystemVerilog code for communication protocols that\nfollow timing constrains."}
{"id": "2507.01504", "pdf": "https://arxiv.org/pdf/2507.01504.pdf", "abs": "https://arxiv.org/abs/2507.01504", "title": "Following the Clues: Experiments on Person Re-ID using Cross-Modal Intelligence", "authors": ["Robert Aufschläger", "Youssef Shoeb", "Azarm Nowzad", "Michael Heigl", "Fabian Bally", "Martin Schramm"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "accepted for publication at the 2025 IEEE 28th International\n  Conference on Intelligent Transportation Systems (ITSC 2025), taking place\n  during November 18-21, 2025 in Gold Coast, Australia", "summary": "The collection and release of street-level recordings as Open Data play a\nvital role in advancing autonomous driving systems and AI research. However,\nthese datasets pose significant privacy risks, particularly for pedestrians,\ndue to the presence of Personally Identifiable Information (PII) that extends\nbeyond biometric traits such as faces. In this paper, we present cRID, a novel\ncross-modal framework combining Large Vision-Language Models, Graph Attention\nNetworks, and representation learning to detect textual describable clues of\nPII and enhance person re-identification (Re-ID). Our approach focuses on\nidentifying and leveraging interpretable features, enabling the detection of\nsemantically meaningful PII beyond low-level appearance cues. We conduct a\nsystematic evaluation of PII presence in person image datasets. Our experiments\nshow improved performance in practical cross-dataset Re-ID scenarios, notably\nfrom Market-1501 to CUHK03-np (detected), highlighting the framework's\npractical utility. Code is available at https://github.com/RAufschlaeger/cRID."}
{"id": "2507.09279", "pdf": "https://arxiv.org/pdf/2507.09279.pdf", "abs": "https://arxiv.org/abs/2507.09279", "title": "Prompt4Trust: A Reinforcement Learning Prompt Augmentation Framework for Clinically-Aligned Confidence Calibration in Multimodal Large Language Models", "authors": ["Anita Kriz", "Elizabeth Laura Janes", "Xing Shen", "Tal Arbel"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Accepted to ICCV 2025 Workshop CVAMD", "summary": "Multimodal large language models (MLLMs) hold considerable promise for\napplications in healthcare. However, their deployment in safety-critical\nsettings is hindered by two key limitations: (i) sensitivity to prompt design,\nand (ii) a tendency to generate incorrect responses with high confidence. As\nclinicians may rely on a model's stated confidence to gauge the reliability of\nits predictions, it is especially important that when a model expresses high\nconfidence, it is also highly accurate. We introduce Prompt4Trust, the first\nreinforcement learning (RL) framework for prompt augmentation targeting\nconfidence calibration in MLLMs. A lightweight LLM is trained to produce\ncontext-aware auxiliary prompts that guide a downstream task MLLM to generate\nresponses in which the expressed confidence more accurately reflects predictive\naccuracy. Unlike conventional calibration techniques, Prompt4Trust specifically\nprioritizes aspects of calibration most critical for safe and trustworthy\nclinical decision-making. Beyond improvements driven by this clinically\nmotivated calibration objective, our proposed method also improves task\naccuracy, achieving state-of-the-art medical visual question answering (VQA)\nperformance on the PMC-VQA benchmark, which is composed of multiple-choice\nquestions spanning diverse medical imaging modalities. Moreover, our framework\ntrained with a small downstream task MLLM showed promising zero-shot\ngeneralization to larger MLLMs in our experiments, suggesting the potential for\nscalable calibration without the associated computational costs. This work\ndemonstrates the potential of automated yet human-aligned prompt engineering\nfor improving the the trustworthiness of MLLMs in safety critical settings. Our\ncodebase can be found at https://github.com/xingbpshen/prompt4trust."}
