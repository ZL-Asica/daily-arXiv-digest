{"id": "2506.13882", "pdf": "https://arxiv.org/pdf/2506.13882.pdf", "abs": "https://arxiv.org/abs/2506.13882", "title": "Toward Practical Privacy in XR: Empirical Analysis of Multimodal Anonymization Mechanisms", "authors": ["Azim Ibragimov", "Ethan Wilson", "Kevin R. B. Butler", "Eakta Jain"], "categories": ["cs.HC"], "comment": null, "summary": "As extended reality (XR) systems become increasingly immersive and\nsensor-rich, they enable the collection of fine-grained behavioral signals such\nas eye and body telemetry. These signals support personalized and responsive\nexperiences and may also contain unique patterns that can be linked back to\nindividuals. However, privacy mechanisms that naively pair unimodal mechanisms\n(e.g., independently apply privacy mechanisms for eye and body privatization)\nare often ineffective at preventing re-identification in practice. In this\nwork, we systematically evaluate real-time privacy mechanisms for XR, both\nindividually and in pair, across eye and body modalities. To preserve\nusability, all mechanisms were tuned based on empirically grounded thresholds\nfor real-time interaction. We evaluated four eye and ten body mechanisms across\nmultiple datasets, comprising up to 407 participants. Our results show that\nwhile obfuscating eye telemetry alone offers moderate privacy gains, body\ntelemetry perturbation is substantially more effective. When carefully paired,\nmultimodal mechanisms reduce re-identification rate from 80.3% to 26.3% in\ncasual XR applications (e.g., VRChat and Job Simulator) and from 84.8% to 26.1%\nin competitive XR applications (e.g., Beat Saber and Synth Riders), all without\nviolating real-time usability requirements. These findings underscore the\npotential of modality-specific and context-aware privacy strategies for\nprotecting behavioral data in XR environments.", "AI": {"tldr": "This paper evaluates real-time privacy mechanisms for XR systems, demonstrating that multimodal approaches significantly reduce re-identification rates while maintaining usability.", "motivation": "To address the inefficacy of unimodal privacy mechanisms in protecting against re-identification in XR systems that collect behavioral signals.", "method": "The study systematically evaluates individual and paired privacy mechanisms for eye and body telemetry across various datasets involving 407 participants.", "result": "Multimodal privacy mechanisms reduced re-identification rates from 80.3% to 26.3% in casual and from 84.8% to 26.1% in competitive XR applications without compromising usability.", "conclusion": "The research highlights the effectiveness of context-aware and modality-specific privacy strategies in enhancing behavioral data protection in XR environments.", "key_contributions": ["Systematic evaluation of real-time privacy mechanisms for XR.", "Demonstration of the effectiveness of multimodal privacy strategies.", "Establishment of empirically grounded thresholds for usability in privacy mechanisms."], "limitations": "Focused primarily on eye and body telemetry without exploring other potential modalities for privacy.", "keywords": ["Extended Reality", "Privacy Mechanisms", "Human-Computer Interaction", "Behavioral Data", "Re-identification"], "importance_score": 7, "read_time_minutes": 12}}
{"id": "2506.13904", "pdf": "https://arxiv.org/pdf/2506.13904.pdf", "abs": "https://arxiv.org/abs/2506.13904", "title": "A Systematic Review of User-Centred Evaluation of Explainable AI in Healthcare", "authors": ["Ivania Donoso-Guzmán", "Kristýna Sirka Kacafírková", "Maxwell Szymanski", "An Jacobs", "Denis Parra", "Katrien Verbert"], "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": null, "summary": "Despite promising developments in Explainable Artificial Intelligence, the\npractical value of XAI methods remains under-explored and insufficiently\nvalidated in real-world settings. Robust and context-aware evaluation is\nessential, not only to produce understandable explanations but also to ensure\ntheir trustworthiness and usability for intended users, but tends to be\noverlooked because of no clear guidelines on how to design an evaluation with\nusers.\n  This study addresses this gap with two main goals: (1) to develop a framework\nof well-defined, atomic properties that characterise the user experience of XAI\nin healthcare; and (2) to provide clear, context-sensitive guidelines for\ndefining evaluation strategies based on system characteristics.\n  We conducted a systematic review of 82 user studies, sourced from five\ndatabases, all situated within healthcare settings and focused on evaluating\nAI-generated explanations. The analysis was guided by a predefined coding\nscheme informed by an existing evaluation framework, complemented by inductive\ncodes developed iteratively.\n  The review yields three key contributions: (1) a synthesis of current\nevaluation practices, highlighting a growing focus on human-centred approaches\nin healthcare XAI; (2) insights into the interrelations among explanation\nproperties; and (3) an updated framework and a set of actionable guidelines to\nsupport interdisciplinary teams in designing and implementing effective\nevaluation strategies for XAI systems tailored to specific application\ncontexts.", "AI": {"tldr": "This study provides a framework for evaluating Explainable AI in healthcare, focusing on user experience and context-sensitive guidelines for evaluation.", "motivation": "To address the lack of robust evaluation methods for Explainable Artificial Intelligence (XAI) in real-world healthcare settings.", "method": "A systematic review of 82 user studies from healthcare contexts was conducted, using a predefined coding scheme and inductive codes to analyze current evaluation practices.", "result": "The review reveals a shift towards human-centered evaluation approaches, highlights interrelations among explanation properties, and proposes an updated framework with actionable guidelines for XAI evaluation.", "conclusion": "A comprehensive framework and guidelines to enhance the trustworthiness and usability of XAI in healthcare, enabling better evaluation practices.", "key_contributions": ["Synthesis of current evaluation practices in healthcare XAI", "Insights into interrelations among explanation properties", "Updated framework and actionable evaluation guidelines for XAI"], "limitations": "", "keywords": ["Explainable AI", "XAI", "healthcare", "user experience", "evaluation strategies"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.14018", "pdf": "https://arxiv.org/pdf/2506.14018.pdf", "abs": "https://arxiv.org/abs/2506.14018", "title": "\"I Cannot Write This Because It Violates Our Content Policy\": Understanding Content Moderation Policies and User Experiences in Generative AI Products", "authors": ["Lan Gao", "Oscar Chen", "Rachel Lee", "Nick Feamster", "Chenhao Tan", "Marshini Chetty"], "categories": ["cs.HC"], "comment": "Preprint for USENIX Security 2025", "summary": "While recent research has focused on developing safeguards for generative AI\n(GAI) model-level content safety, little is known about how content moderation\nto prevent malicious content performs for end-users in real-world GAI products.\nTo bridge this gap, we investigated content moderation policies and their\nenforcement in GAI online tools -- consumer-facing web-based GAI applications.\nWe first analyzed content moderation policies of 14 GAI online tools. While\nthese policies are comprehensive in outlining moderation practices, they\nusually lack details on practical implementations and are not specific about\nhow users can aid in moderation or appeal moderation decisions. Next, we\nexamined user-experienced content moderation successes and failures through\nReddit discussions on GAI online tools. We found that although moderation\nsystems succeeded in blocking malicious generations pervasively, users\nfrequently experienced frustration in failures of both moderation systems and\nuser support after moderation. Based on these findings, we suggest improvements\nfor content moderation policy and user experiences in real-world GAI products.", "AI": {"tldr": "This paper investigates content moderation in generative AI tools, analyzing policies and user experiences, ultimately suggesting improvements for better user experience and policy detail.", "motivation": "To understand the effectiveness of content moderation policies in generative AI applications and their impact on end-users.", "method": "Analyzed content moderation policies of 14 generative AI online tools and examined user experiences through discussions on Reddit.", "result": "Identified comprehensive yet vague moderation policies and user frustrations with failures of moderation systems and support.", "conclusion": "Improvements are needed in content moderation policy detail and user experience for GAI tools.", "key_contributions": ["Analysis of moderation policies in GAI tools", "Insights from user experiences on moderation failures", "Recommendations for improving content moderation and user support"], "limitations": "Focuses on consumer-facing GAI applications; results may not generalize to all generative AI contexts.", "keywords": ["Content Moderation", "Generative AI", "User Experience", "Online Tools", "Moderation Policies"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.14056", "pdf": "https://arxiv.org/pdf/2506.14056.pdf", "abs": "https://arxiv.org/abs/2506.14056", "title": "FEWSim: A Visual Analytic Framework for Exploring the Nexus of Food-Energy-Water Simulations", "authors": ["Fan Lei", "David A. Sampson", "Jiayi Hong", "Yuxin Ma", "Giuseppe Mascaro", "Dave White", "Rimjhim Agarwal", "Ross Maciejewski"], "categories": ["cs.HC"], "comment": "Accepted by IEEE Computer Graphics and Applications (CG&A)", "summary": "The interdependencies of food, energy, and water (FEW) systems create a nexus\nopportunity to explore the strengths and vulnerabilities of individual and\ncross-sector interactions within FEW systems. However, the variables\nquantifying nexus interactions are hard to observe, which hinders the\ncross-sector analysis. To overcome such challenges, we present FEWSim, a visual\nanalytics framework designed to support domain experts in exploring and\ninterpreting simulation results from a coupled FEW model. FEWSim employs a\nthree-layer asynchronous architecture: the model layer integrates food, energy,\nand water models to simulate the FEW nexus; the middleware layer manages\nscenario configuration and execution; and the visualization layer provides\ninteractive visual exploration of simulated time-series results across FEW\nsectors. The visualization layer further facilitates the exploration across\nmultiple scenarios and evaluates scenario differences in performance using\nsustainability indices of the FEW nexus. We demonstrate the utility of FEWSim\nthrough a case study for the Phoenix Active Management Area (AMA) in Arizona.", "AI": {"tldr": "FEWSim is a visual analytics framework that aids in exploring food, energy, and water (FEW) systems through simulation results.", "motivation": "The need to analyze the interdependencies and vulnerabilities within food, energy, and water systems due to the challenges in quantifying nexus interactions.", "method": "FEWSim employs a three-layer asynchronous architecture consisting of a model layer for simulation, a middleware layer for scenario management, and a visualization layer for interactive data exploration.", "result": "The framework allows experts to visualize and compare simulation results, facilitating an understanding of FEW nexus dynamics across scenarios using sustainability indices.", "conclusion": "FEWSim effectively demonstrates its utility in a case study, enhancing the capacity for cross-sector analysis in FEW systems.", "key_contributions": ["Introduction of a visual analytics framework for FEW systems", "Three-layer architecture for enhanced simulation exploration", "Practical application demonstrated in a real-world case study"], "limitations": "", "keywords": ["food energy water nexus", "visual analytics", "simulation results", "sustainability indices", "interdependencies"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2506.13796", "pdf": "https://arxiv.org/pdf/2506.13796.pdf", "abs": "https://arxiv.org/abs/2506.13796", "title": "ClimateChat: Designing Data and Methods for Instruction Tuning LLMs to Answer Climate Change Queries", "authors": ["Zhou Chen", "Xiao Wang", "Yuanhong Liao", "Ming Lin", "Yuqi Bai"], "categories": ["cs.CL", "cs.AI"], "comment": "ICLR 2025 camera ready, 13 pages, 4 figures, 4 tables", "summary": "As the issue of global climate change becomes increasingly severe, the demand\nfor research in climate science continues to grow. Natural language processing\ntechnologies, represented by Large Language Models (LLMs), have been widely\napplied to climate change-specific research, providing essential information\nsupport for decision-makers and the public. Some studies have improved model\nperformance on relevant tasks by constructing climate change-related\ninstruction data and instruction-tuning LLMs. However, current research remains\ninadequate in efficiently producing large volumes of high-precision instruction\ndata for climate change, which limits further development of climate change\nLLMs. This study introduces an automated method for constructing instruction\ndata. The method generates instructions using facts and background knowledge\nfrom documents and enhances the diversity of the instruction data through web\nscraping and the collection of seed instructions. Using this method, we\nconstructed a climate change instruction dataset, named ClimateChat-Corpus,\nwhich was used to fine-tune open-source LLMs, resulting in an LLM named\nClimateChat. Evaluation results show that ClimateChat significantly improves\nperformance on climate change question-and-answer tasks. Additionally, we\nevaluated the impact of different base models and instruction data on LLM\nperformance and demonstrated its capability to adapt to a wide range of climate\nchange scientific discovery tasks, emphasizing the importance of selecting an\nappropriate base model for instruction tuning. This research provides valuable\nreferences and empirical support for constructing climate change instruction\ndata and training climate change-specific LLMs.", "AI": {"tldr": "This study presents an automated method for constructing instruction data to improve climate change-related LLMs, resulting in a dataset named ClimateChat-Corpus and an enhanced model, ClimateChat.", "motivation": "The growing problem of climate change necessitates improved research and application of LLMs to provide critical information to decision-makers and the public.", "method": "The proposed method automates the generation of instruction data by extracting facts and background knowledge from documents, while enhancing diversity through web scraping and seed instruction collection.", "result": "The constructed ClimateChat-Corpus significantly improves the performance of climate change-related tasks when fine-tuning LLMs, particularly demonstrated by the model ClimateChat.", "conclusion": "Selecting an appropriate base model for instruction tuning is crucial for effectively using LLMs in climate change research, providing empirical support for further development in this area.", "key_contributions": ["Introduction of an automated method for constructing climate change instruction data.", "Creation of the ClimateChat-Corpus dataset for fine-tuning LLMs.", "Demonstration of improved performance in climate change question-and-answer tasks with the ClimateChat model."], "limitations": "", "keywords": ["climate change", "large language models", "instruction data", "automated generation", "fine-tuning"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2506.14147", "pdf": "https://arxiv.org/pdf/2506.14147.pdf", "abs": "https://arxiv.org/abs/2506.14147", "title": "The Teacher's Dilemma: Balancing Trade-Offs in Programming Education for Emergent Bilingual Students", "authors": ["Emma R. Dodoo", "Tamara Nelson-Fromm", "Mark Guzdial"], "categories": ["cs.HC"], "comment": null, "summary": "K-12 computing teachers must navigate complex trade-offs when selecting\nprogramming languages and instructional materials for classrooms with emergent\nbilingual students. While they aim to foster an inclusive learning environment\nby addressing language barriers that impact student engagement, they must also\nalign with K-12 computer science curricular guidelines and prepare students for\nindustry-standard programming tools. Because programming languages\npredominantly use English keywords and most instructional materials are written\nin English, these linguistic barriers introduce cognitive load and\naccessibility challenges. This paper examines teachers' decisions in balancing\nthese competing priorities, highlighting the tensions between accessibility,\ncurriculum alignment, and workforce preparation. The findings shed light on how\nour teacher participants negotiate these trade-offs and what factors influence\ntheir selection of programming tools to best support EB students while meeting\nbroader educational and professional goals.", "AI": {"tldr": "The paper explores the challenges faced by K-12 computing teachers in selecting programming languages and materials for classrooms with emergent bilingual students, focusing on the trade-offs between inclusivity and curriculum alignment.", "motivation": "To understand how K-12 computing teachers balance the need for an inclusive learning environment with curriculum and industry demands when teaching emergent bilingual students.", "method": "Qualitative analysis of teacher decisions and the factors influencing their selection of programming languages and materials.", "result": "Identified key tensions between accessibility, curriculum alignment, and workforce preparation that impact teachers' choices in programming tools.", "conclusion": "Teachers must navigate complex trade-offs and make decisions that support equity while also adhering to curricular guidelines and industry expectations.", "key_contributions": ["In-depth insights into the decision-making process of teachers for emergent bilingual students.", "Identification of the key tensions faced by educators in balancing accessibility and curriculum requirements.", "Recommendations for future programming language selections that support diverse student needs."], "limitations": "The study primarily focuses on K-12 settings and may not generalize to higher education contexts or other subjects.", "keywords": ["K-12 education", "emergent bilinguals", "programming languages", "teacher decision-making", "computer science education"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2506.13886", "pdf": "https://arxiv.org/pdf/2506.13886.pdf", "abs": "https://arxiv.org/abs/2506.13886", "title": "Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles", "authors": ["Antara Raaghavi Bhattacharya", "Isabel Papadimitriou", "Kathryn Davidson", "David Alvarez-Melis"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Across languages, numeral systems vary widely in how they construct and\ncombine numbers. While humans consistently learn to navigate this diversity,\nlarge language models (LLMs) struggle with linguistic-mathematical puzzles\ninvolving cross-linguistic numeral systems, which humans can learn to solve\nsuccessfully. We investigate why this task is difficult for LLMs through a\nseries of experiments that untangle the linguistic and mathematical aspects of\nnumbers in language. Our experiments establish that models cannot consistently\nsolve such problems unless the mathematical operations in the problems are\nexplicitly marked using known symbols ($+$, $\\times$, etc, as in \"twenty +\nthree\"). In further ablation studies, we probe how individual parameters of\nnumeral construction and combination affect performance. While humans use their\nlinguistic understanding of numbers to make inferences about the implicit\ncompositional structure of numerals, LLMs seem to lack this notion of implicit\nnumeral structure. We conclude that the ability to flexibly infer compositional\nrules from implicit patterns in human-scale data remains an open challenge for\ncurrent reasoning models.", "AI": {"tldr": "Large language models struggle with cross-linguistic numeral systems, unable to solve numerical puzzles unless explicitly marked with known symbols. This paper investigates the difficulties faced by LLMs in understanding implicit numeral structure compared to humans.", "motivation": "To understand why LLMs perform poorly on linguistic-mathematical puzzles involving cross-linguistic numeral systems despite humans succeeding in such tasks.", "method": "A series of experiments were conducted to separate the linguistic and mathematical aspects of numbers, examining how explicit marking and parameter variations of numeral construction affect performance in LLMs.", "result": "LLMs fail to consistently solve numerical problems without explicit mathematical operations being marked, unlike humans who leverage implicit numeral structures.", "conclusion": "The findings indicate that LLMs lack the ability to infer compositional rules from implicit patterns, highlighting a significant challenge for reasoning models in processing numerals across different languages.", "key_contributions": ["Identification of LLM limitations in reasoning with numeral systems", "Experiments demonstrating the importance of explicit marking", "Insights into human vs. LLM processing of numeral structures"], "limitations": "This study focuses only on numeral systems and does not explore other linguistic challenges or the broader context of LLM performance", "keywords": ["numerals", "language models", "linguistic structure", "mathematical reasoning", "cross-linguistic"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2506.14159", "pdf": "https://arxiv.org/pdf/2506.14159.pdf", "abs": "https://arxiv.org/abs/2506.14159", "title": "StorySage: Conversational Autobiography Writing Powered by a Multi-Agent Framework", "authors": ["Shayan Talaei", "Meijin Li", "Kanu Grover", "James Kent Hippler", "Diyi Yang", "Amin Saberi"], "categories": ["cs.HC", "cs.AI", "cs.MA"], "comment": null, "summary": "Every individual carries a unique and personal life story shaped by their\nmemories and experiences. However, these memories are often scattered and\ndifficult to organize into a coherent narrative, a challenge that defines the\ntask of autobiography writing. Existing conversational writing assistants tend\nto rely on generic user interactions and pre-defined guidelines, making it\ndifficult for these systems to capture personal memories and develop a complete\nbiography over time. We introduce StorySage, a user-driven software system\ndesigned to meet the needs of a diverse group of users that supports a flexible\nconversation and a structured approach to autobiography writing. Powered by a\nmulti-agent framework composed of an Interviewer, Session Scribe, Planner,\nSection Writer, and Session Coordinator, our system iteratively collects user\nmemories, updates their autobiography, and plans for future conversations. In\nexperimental simulations, StorySage demonstrates its ability to navigate\nmultiple sessions and capture user memories across many conversations. User\nstudies (N=28) highlight how StorySage maintains improved conversational flow,\nnarrative completeness, and higher user satisfaction when compared to a\nbaseline. In summary, StorySage contributes both a novel architecture for\nautobiography writing and insights into how multi-agent systems can enhance\nhuman-AI creative partnerships.", "AI": {"tldr": "StorySage is a multi-agent system that facilitates autobiography writing by capturing user memories through structured conversations.", "motivation": "Individuals struggle to organize their scattered memories into coherent autobiographies using existing conversational writing assistants.", "method": "StorySage uses a multi-agent framework consisting of an Interviewer, Session Scribe, Planner, Section Writer, and Session Coordinator to collect and structure user memories iteratively.", "result": "StorySage shows improved conversational flow, narrative completeness, and user satisfaction in experimental simulations compared to a baseline.", "conclusion": "StorySage provides a novel architecture for autobiography writing and demonstrates the benefits of multi-agent systems in enhancing human-AI creative partnerships.", "key_contributions": ["Introduction of a multi-agent framework for autobiography writing", "Improved conversational flow and narrative completeness", "User satisfaction enhancement in memoir writing support"], "limitations": "", "keywords": ["autobiography writing", "human-AI collaboration", "multi-agent systems"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.13888", "pdf": "https://arxiv.org/pdf/2506.13888.pdf", "abs": "https://arxiv.org/abs/2506.13888", "title": "VL-GenRM: Enhancing Vision-Language Verification via Vision Experts and Iterative Training", "authors": ["Jipeng Zhang", "Kehao Miao", "Renjie Pi", "Zhaowei Wang", "Runtao Liu", "Rui Pan", "Tong Zhang"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Reinforcement Fine-Tuning (RFT) with verifiable rewards has advanced large\nlanguage models but remains underexplored for Vision-Language (VL) models. The\nVision-Language Reward Model (VL-RM) is key to aligning VL models by providing\nstructured feedback, yet training effective VL-RMs faces two major challenges.\nFirst, the bootstrapping dilemma arises as high-quality training data depends\non already strong VL models, creating a cycle where self-generated supervision\nreinforces existing biases. Second, modality bias and negative example\namplification occur when VL models hallucinate incorrect visual attributes,\nleading to flawed preference data that further misguides training. To address\nthese issues, we propose an iterative training framework leveraging vision\nexperts, Chain-of-Thought (CoT) rationales, and Margin-based Rejection\nSampling. Our approach refines preference datasets, enhances structured\ncritiques, and iteratively improves reasoning. Experiments across VL-RM\nbenchmarks demonstrate superior performance in hallucination detection and\nmultimodal reasoning, advancing VL model alignment with reinforcement learning.", "AI": {"tldr": "This paper introduces an iterative training framework for aligning Vision-Language models using Reinforcement Fine-Tuning (RFT) with a focus on improving the quality of Vision-Language Reward Models (VL-RMs).", "motivation": "The paper addresses the challenges of aligning Vision-Language models, particularly the biases and inefficiencies in existing training methods and data generation processes.", "method": "An iterative training framework is proposed that incorporates vision experts, Chain-of-Thought (CoT) rationales, and Margin-based Rejection Sampling to refine preference datasets and enhance structured critiques.", "result": "Experiments show that the proposed method improves detection of hallucinations and performance in multimodal reasoning tasks, surpassing existing benchmarks.", "conclusion": "The iterative approach not only advances the performance of VL models but also promotes better alignment through structured feedback mechanisms in reinforcement learning.", "key_contributions": ["Introduction of an iterative training framework for VL models", "Use of vision experts and CoT rationales to refine training data", "Improved performance in hallucination detection and reasoning tasks"], "limitations": "Potential reliance on the quality of vision experts and the effectiveness of CoT rationales in diverse contexts is not fully explored.", "keywords": ["Vision-Language Models", "Reinforcement Fine-Tuning", "Multimodal Reasoning", "Hallucination Detection", "Training Framework"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.14166", "pdf": "https://arxiv.org/pdf/2506.14166.pdf", "abs": "https://arxiv.org/abs/2506.14166", "title": "Affective-CARA: A Knowledge Graph Driven Framework for Culturally Adaptive Emotional Intelligence in HCI", "authors": ["Nirodya Pussadeniya", "Bahareh Nakisa", "Mohmmad Naim Rastgoo"], "categories": ["cs.HC"], "comment": null, "summary": "Culturally adaptive emotional responses remain a critical challenge in\naffective computing. This paper introduces Affective-CARA, an agentic framework\ndesigned to enhance user-agent interactions by integrating a Cultural Emotion\nKnowledge Graph (derived from StereoKG) with Valence, Arousal, and Dominance\nannotations, culture-specific data, and cross-cultural checks to minimize bias.\nA Gradient-Based Reward Policy Optimization mechanism further refines responses\naccording to cultural alignment, affective appropriateness, and iterative user\nfeedback. A Cultural-Aware Response Mediator coordinates knowledge retrieval,\nreinforcement learning updates, and historical data fusion. By merging\nreal-time user input with past emotional states and cultural insights,\nAffective-CARA delivers narratives that are deeply personalized and sensitive\nto diverse cultural norms. Evaluations on AffectNet, SEMAINE DB, and MERD\nconfirm that the framework consistently outperforms baseline models in\nsentiment alignment, cultural adaptation, and narrative quality. Affective-CARA\nachieved a Cultural Semantic Density of 9.32 out of 10 and lowered cultural\nrepresentation bias by 61% (KL-Divergence: 0.28), demonstrating robust\nperformance in generating ethical, adaptive responses. These findings suggest\nthe potential for more inclusive and empathetic interactions, making\nAffective-CARA an avenue for fostering culturally grounded user experiences\nacross domains such as cross-cultural communication, mental health support, and\neducation.", "AI": {"tldr": "Affective-CARA is a culturally adaptive emotional response framework that enhances user-agent interactions by integrating cultural knowledge with reinforcement learning, resulting in improved sentiment alignment and reduced bias in emotional responses.", "motivation": "The paper addresses the challenge of generating culturally adaptive emotional responses in affective computing, which is vital for empathetic user-agent interactions.", "method": "The framework integrates a Cultural Emotion Knowledge Graph with annotations for Valence, Arousal, and Dominance, and employs a Gradient-Based Reward Policy Optimization for refining responses based on cultural alignment and user feedback.", "result": "Affective-CARA outperforms baseline models in sentiment alignment, cultural adaptation, and narrative quality, achieving a Cultural Semantic Density of 9.32 and reducing cultural representation bias by 61%.", "conclusion": "The findings highlight the potential of Affective-CARA for creating inclusive and empathetic interactions, with applications in cross-cultural communication, mental health support, and education.", "key_contributions": ["Introduction of Affective-CARA framework for cultural emotional responses.", "Integration of a Cultural Emotion Knowledge Graph for user-agent interaction.", "Performance evaluation showing significant improvements in sentiment alignment and cultural adaptation."], "limitations": "", "keywords": ["Affective Computing", "Cultural Adaptation", "Emotion Knowledge Graph", "Reinforcement Learning", "User-Agent Interaction"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.13894", "pdf": "https://arxiv.org/pdf/2506.13894.pdf", "abs": "https://arxiv.org/abs/2506.13894", "title": "EmoNews: A Spoken Dialogue System for Expressive News Conversations", "authors": ["Ryuki Matsuura", "Shikhar Bharadwaj", "Jiarui Liu", "Dhatchi Kunde Govindarajan"], "categories": ["cs.CL"], "comment": null, "summary": "We develop a task-oriented spoken dialogue system (SDS) that regulates\nemotional speech based on contextual cues to enable more empathetic news\nconversations. Despite advancements in emotional text-to-speech (TTS)\ntechniques, task-oriented emotional SDSs remain underexplored due to the\ncompartmentalized nature of SDS and emotional TTS research, as well as the lack\nof standardized evaluation metrics for social goals. We address these\nchallenges by developing an emotional SDS for news conversations that utilizes\na large language model (LLM)-based sentiment analyzer to identify appropriate\nemotions and PromptTTS to synthesize context-appropriate emotional speech. We\nalso propose subjective evaluation scale for emotional SDSs and judge the\nemotion regulation performance of the proposed and baseline systems.\nExperiments showed that our emotional SDS outperformed a baseline system in\nterms of the emotion regulation and engagement. These results suggest the\ncritical role of speech emotion for more engaging conversations. All our source\ncode is open-sourced at\nhttps://github.com/dhatchi711/espnet-emotional-news/tree/emo-sds/egs2/emo_news_sds/sds1", "AI": {"tldr": "Development of an emotional spoken dialogue system for empathetic news conversations, leveraging LLMs and emotional TTS.", "motivation": "To create a more empathetic task-oriented spoken dialogue system for news conversations by regulating emotional speech based on context.", "method": "Developed a spoken dialogue system utilizing a large language model for sentiment analysis and PromptTTS for synthesizing emotional speech. Introduced a new subjective evaluation scale for assessing emotional dialogue systems.", "result": "The emotional SDS significantly outperformed a baseline system in terms of emotion regulation and user engagement during conversations.", "conclusion": "Emotional regulation in speech is essential for enhancing engagement in conversations, and our system demonstrates this capability effectively.", "key_contributions": ["Introduction of an emotional SDS for news conversations.", "Utilization of LLM-based sentiment analysis for emotional speech synthesis.", "Development of a subjective evaluation scale for emotional SDSs."], "limitations": "Potential need for further refinement in emotional context detection and synthesis quality.", "keywords": ["emotional dialogue system", "task-oriented systems", "large language models", "text-to-speech", "engagement"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.14196", "pdf": "https://arxiv.org/pdf/2506.14196.pdf", "abs": "https://arxiv.org/abs/2506.14196", "title": "Balancing Caregiving and Self-Care: Exploring Mental Health Needs of Alzheimer's and Dementia Caregivers", "authors": ["Jiayue Melissa Shi", "Keran Wang", "Dong Whi Yoo", "Ravi Karkar", "Koustuv Saha"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Alzheimer's Disease and Related Dementias (AD/ADRD) are progressive\nneurodegenerative conditions that impair memory, thought processes, and\nfunctioning. Family caregivers of individuals with AD/ADRD face significant\nmental health challenges due to long-term caregiving responsibilities. Yet,\ncurrent support systems often overlook the evolving nature of their mental\nwellbeing needs. Our study examines caregivers' mental wellbeing concerns,\nfocusing on the practices they adopt to manage the burden of caregiving and the\ntechnologies they use for support. Through semi-structured interviews with 25\nfamily caregivers of individuals with AD/ADRD, we identified the key causes and\neffects of mental health challenges, and developed a temporal mapping of how\ncaregivers' mental wellbeing evolves across three distinct stages of the\ncaregiving journey. Additionally, our participants shared insights into\nimprovements for existing mental health technologies, emphasizing the need for\naccessible, scalable, and personalized solutions that adapt to caregivers'\nchanging needs over time. These findings offer a foundation for designing\ndynamic, stage-sensitive interventions that holistically support caregivers'\nmental wellbeing, benefiting both caregivers and care recipients.", "AI": {"tldr": "The paper explores the mental wellbeing challenges faced by family caregivers of individuals with Alzheimer's Disease and Related Dementias, highlighting their adaptive practices and the role of technology.", "motivation": "To address the overlooked mental health needs of family caregivers in the context of Alzheimer's Disease and Related Dementias, as existing support systems are inadequate.", "method": "The study conducted semi-structured interviews with 25 family caregivers to understand their mental wellbeing concerns and the technologies they use.", "result": "Key causes and effects of mental health challenges in caregivers were identified, as well as a temporal mapping of their mental wellbeing across three stages of the caregiving journey.", "conclusion": "The findings suggest the need for personalized, scalable mental health technologies that evolve with caregivers' needs, providing a basis for designing effective, stage-sensitive interventions.", "key_contributions": ["Identification of mental health challenges specific to caregivers of AD/ADRD.", "Creation of a temporal mapping of caregivers' mental wellbeing across caregiving stages.", "Recommendations for improving existing mental health technologies for caregivers."], "limitations": "", "keywords": ["Alzheimer's Disease", "caregivers", "mental wellbeing", "technologies", "interventions"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.13901", "pdf": "https://arxiv.org/pdf/2506.13901.pdf", "abs": "https://arxiv.org/abs/2506.13901", "title": "Alignment Quality Index (AQI) : Beyond Refusals: AQI as an Intrinsic Alignment Diagnostic via Latent Geometry, Cluster Divergence, and Layer wise Pooled Representations", "authors": ["Abhilekh Borah", "Chhavi Sharma", "Danush Khanna", "Utkarsh Bhatt", "Gurpreet Singh", "Hasnat Md Abdullah", "Raghav Kaushik Ravi", "Vinija Jain", "Jyoti Patel", "Shubham Singh", "Vasu Sharma", "Arpita Vats", "Rahul Raja", "Aman Chadha", "Amitava Das"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Alignment is no longer a luxury, it is a necessity. As large language models\n(LLMs) enter high-stakes domains like education, healthcare, governance, and\nlaw, their behavior must reliably reflect human-aligned values and safety\nconstraints. Yet current evaluations rely heavily on behavioral proxies such as\nrefusal rates, G-Eval scores, and toxicity classifiers, all of which have\ncritical blind spots. Aligned models are often vulnerable to jailbreaking,\nstochasticity of generation, and alignment faking.\n  To address this issue, we introduce the Alignment Quality Index (AQI). This\nnovel geometric and prompt-invariant metric empirically assesses LLM alignment\nby analyzing the separation of safe and unsafe activations in latent space. By\ncombining measures such as the Davies-Bouldin Score (DBS), Dunn Index (DI),\nXie-Beni Index (XBI), and Calinski-Harabasz Index (CHI) across various\nformulations, AQI captures clustering quality to detect hidden misalignments\nand jailbreak risks, even when outputs appear compliant. AQI also serves as an\nearly warning signal for alignment faking, offering a robust, decoding\ninvariant tool for behavior agnostic safety auditing.\n  Additionally, we propose the LITMUS dataset to facilitate robust evaluation\nunder these challenging conditions. Empirical tests on LITMUS across different\nmodels trained under DPO, GRPO, and RLHF conditions demonstrate AQI's\ncorrelation with external judges and ability to reveal vulnerabilities missed\nby refusal metrics. We make our implementation publicly available to foster\nfuture research in this area.", "AI": {"tldr": "The paper introduces the Alignment Quality Index (AQI), a metric for assessing the alignment of large language models (LLMs) with human values, addressing current evaluation shortcomings.", "motivation": "As LLMs are increasingly deployed in critical sectors like healthcare and education, ensuring their alignment with human values and safety measures has become crucial.", "method": "The AQI analyzes safe and unsafe activations in latent space using clustering metrics such as the Davies-Bouldin Score, Dunn Index, and others, to identify hidden misalignments and jailbreak risks.", "result": "Empirical tests with the LITMUS dataset show that AQI correlates with external evaluations and effectively reveals vulnerabilities overlooked by traditional refusal-based metrics.", "conclusion": "AQI is a robust tool for safety auditing of LLM alignment, and the LITMUS dataset enables comprehensive evaluation under tough conditions, with public implementation available for further research.", "key_contributions": ["Introduction of the Alignment Quality Index (AQI) for assessing LLM alignment", "Creation of the LITMUS dataset for robust evaluation of alignment", "Demonstration of AQI's effectiveness in revealing hidden vulnerabilities in LLMs"], "limitations": "", "keywords": ["alignment", "large language models", "safety auditing", "LITMUS dataset", "human values"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.14295", "pdf": "https://arxiv.org/pdf/2506.14295.pdf", "abs": "https://arxiv.org/abs/2506.14295", "title": "The Impact of Generative AI on Social Media: An Experimental Study", "authors": ["Anders Giovanni Møller", "Daniel M. Romero", "David Jurgens", "Luca Maria Aiello"], "categories": ["cs.HC"], "comment": "48 pages, 12 figures", "summary": "Generative Artificial Intelligence (AI) tools are increasingly deployed\nacross social media platforms, yet their implications for user behavior and\nexperience remain understudied, particularly regarding two critical dimensions:\n(1) how AI tools affect the behaviors of content producers in a social media\ncontext, and (2) how content generated with AI assistance is perceived by\nusers. To fill this gap, we conduct a controlled experiment with a\nrepresentative sample of 680 U.S. participants in a realistic social media\nenvironment. The participants are randomly assigned to small discussion groups,\neach consisting of five individuals in one of five distinct experimental\nconditions: a control group and four treatment groups, each employing a unique\nAI intervention-chat assistance, conversation starters, feedback on comment\ndrafts, and reply suggestions. Our findings highlight a complex duality: some\nAI-tools increase user engagement and volume of generated content, but at the\nsame time decrease the perceived quality and authenticity of discussion, and\nintroduce a negative spill-over effect on conversations. Based on our findings,\nwe propose four design principles and recommendations aimed at social media\nplatforms, policymakers, and stakeholders: ensuring transparent disclosure of\nAI-generated content, designing tools with user-focused personalization,\nincorporating context-sensitivity to account for both topic and user intent,\nand prioritizing intuitive user interfaces. These principles aim to guide an\nethical and effective integration of generative AI into social media.", "AI": {"tldr": "This paper investigates the impact of generative AI tools on user behavior and experience in social media through a controlled experiment involving 680 participants.", "motivation": "To understand the implications of generative AI tools on content producer behaviors and user perceptions in social media contexts.", "method": "A controlled experiment was conducted with 680 U.S. participants in small discussion groups, each assigned to one of five experimental conditions, including various AI interventions.", "result": "The study found that while AI tools increased user engagement and content volume, they also decreased perceived quality and authenticity of discussions, leading to negative spill-over effects.", "conclusion": "The paper proposes four design principles for integrating generative AI in social media: transparency in AI content disclosure, user-focused personalization, context-sensitivity, and intuitive interfaces.", "key_contributions": ["Investigation of user engagement versus perceived quality in AI-assisted social media", "Proposed design principles for ethical integration of AI in social media", "Empirical evidence from a controlled experiment with a significant sample size"], "limitations": "The study is limited to a specific sample of U.S. participants and may not generalize to other demographics or cultures.", "keywords": ["Generative AI", "User Engagement", "Social Media", "Ethical Design", "AI Tools"], "importance_score": 8, "read_time_minutes": 48}}
{"id": "2506.13956", "pdf": "https://arxiv.org/pdf/2506.13956.pdf", "abs": "https://arxiv.org/abs/2506.13956", "title": "ASMR: Augmenting Life Scenario using Large Generative Models for Robotic Action Reflection", "authors": ["Shang-Chi Tsai", "Seiya Kawano", "Angel Garcia Contreras", "Koichiro Yoshino", "Yun-Nung Chen"], "categories": ["cs.CL", "cs.AI", "cs.RO"], "comment": "IWSDS 2024 Best Paper Award", "summary": "When designing robots to assist in everyday human activities, it is crucial\nto enhance user requests with visual cues from their surroundings for improved\nintent understanding. This process is defined as a multimodal classification\ntask. However, gathering a large-scale dataset encompassing both visual and\nlinguistic elements for model training is challenging and time-consuming. To\naddress this issue, our paper introduces a novel framework focusing on data\naugmentation in robotic assistance scenarios, encompassing both dialogues and\nrelated environmental imagery. This approach involves leveraging a\nsophisticated large language model to simulate potential conversations and\nenvironmental contexts, followed by the use of a stable diffusion model to\ncreate images depicting these environments. The additionally generated data\nserves to refine the latest multimodal models, enabling them to more accurately\ndetermine appropriate actions in response to user interactions with the limited\ntarget data. Our experimental results, based on a dataset collected from\nreal-world scenarios, demonstrate that our methodology significantly enhances\nthe robot's action selection capabilities, achieving the state-of-the-art\nperformance.", "AI": {"tldr": "This paper presents a framework for enhancing robot intent understanding through visual and linguistic data augmentation.", "motivation": "The paper aims to improve robot assistance in human activities by enhancing user requests with visual cues to better understand intent.", "method": "The framework uses a large language model to generate simulated dialogues and environmental contexts, followed by employing a stable diffusion model to create corresponding images.", "result": "Experimental results show significant improvement in the robot's action selection capabilities, achieving state-of-the-art performance based on a real-world dataset.", "conclusion": "The proposed data augmentation methodology effectively refines multimodal models, allowing for better action determination in robot assistance scenarios.", "key_contributions": ["Introduction of a novel data augmentation framework for robotic assistance", "Utilization of a large language model for simulating dialogues", "Implementation of a stable diffusion model for generating environmental imagery"], "limitations": "", "keywords": ["robot assistance", "multimodal classification", "data augmentation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.14376", "pdf": "https://arxiv.org/pdf/2506.14376.pdf", "abs": "https://arxiv.org/abs/2506.14376", "title": "System 0: Transforming Artificial Intelligence into a Cognitive Extension", "authors": ["Massimo Chiriatti", "Marianna Bergamaschi Ganapini", "Enrico Panai", "Brenda K. Wiederhold", "Giuseppe Riva"], "categories": ["cs.HC"], "comment": null, "summary": "This paper introduces System 0, a conceptual framework for understanding how\nartificial intelligence functions as a cognitive extension preceding both\nintuitive (System 1) and deliberative (System 2) thinking processes. As AI\nsystems increasingly shape the informational substrate upon which human\ncognition operates, they transform from passive tools into active cognitive\npartners. Building on the Extended Mind hypothesis and Heersmink's criteria for\ncognitive extension, we argue that AI systems satisfy key conditions for\ncognitive integration. These include reliability, trust, transparency,\nindividualization, and the ability to enhance and transform human mental\nfunctions. However, AI integration creates a paradox: while expanding cognitive\ncapabilities, it may simultaneously constrain thinking through sycophancy and\nbias amplification. To address these challenges, we propose seven\nevidence-based frameworks for effective human-AI cognitive integration:\nEnhanced Cognitive Scaffolding, which promotes progressive autonomy; Symbiotic\nDivision of Cognitive Labor, strategically allocating tasks based on\ncomparative strengths; Dialectical Cognitive Enhancement, countering AI\nsycophancy through productive epistemic tension; Agentic Transparency and\nControl, ensuring users understand and direct AI influence; Expertise\nDemocratization, breaking down knowledge silos; Social-Emotional Augmentation,\naddressing affective dimensions of cognitive work; and Duration-Optimized\nIntegration, managing the evolving human-AI relationship over time. Together,\nthese frameworks provide a comprehensive approach for harnessing AI as a\ngenuine cognitive extension while preserving human agency, critical thinking,\nand intellectual growth, transforming AI from a replacement for human cognition\ninto a catalyst for enhanced thinking.", "AI": {"tldr": "This paper presents System 0, a framework for understanding AI as a cognitive extension that transforms human cognition while addressing the challenges of sycophancy and bias amplification.", "motivation": "The paper aims to explore how AI can serve as an active cognitive partner in human thought processes, enhancing cognitive capabilities without compromising critical thinking.", "method": "The paper builds on the Extended Mind hypothesis and introduces seven evidence-based frameworks for effective human-AI cognitive integration.", "result": "The proposed frameworks include Enhanced Cognitive Scaffolding, Symbiotic Division of Cognitive Labor, Dialectical Cognitive Enhancement, Agentic Transparency and Control, Expertise Democratization, Social-Emotional Augmentation, and Duration-Optimized Integration.", "conclusion": "These frameworks offer a comprehensive approach to integrate AI into human cognition, promoting agency and intellectual growth rather than replacement of human thought.", "key_contributions": ["Introduction of System 0 as a conceptual framework for AI's role in cognition", "Seven frameworks for effective human-AI cognitive integration", "Discussion on the paradox of AI enhancing yet constraining human thought"], "limitations": "", "keywords": ["AI cognitive extension", "human-AI integration", "cognitive scaffolding"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.13965", "pdf": "https://arxiv.org/pdf/2506.13965.pdf", "abs": "https://arxiv.org/abs/2506.13965", "title": "Are manual annotations necessary for statutory interpretations retrieval?", "authors": ["Aleksander Smywiński-Pohl", "Tomer Libal", "Adam Kaczmarczyk", "Magdalena Król"], "categories": ["cs.CL"], "comment": null, "summary": "One of the elements of legal research is looking for cases where judges have\nextended the meaning of a legal concept by providing interpretations of what a\nconcept means or does not mean. This allow legal professionals to use such\ninterpretations as precedents as well as laymen to better understand the legal\nconcept. The state-of-the-art approach for retrieving the most relevant\ninterpretations for these concepts currently depends on the ranking of\nsentences and the training of language models over annotated examples. That\nmanual annotation process can be quite expensive and need to be repeated for\neach such concept, which prompted recent research in trying to automate this\nprocess. In this paper, we highlight the results of various experiments\nconducted to determine the volume, scope and even the need for manual\nannotation. First of all, we check what is the optimal number of annotations\nper a legal concept. Second, we check if we can draw the sentences for\nannotation randomly or there is a gain in the performance of the model, when\nonly the best candidates are annotated. As the last question we check what is\nthe outcome of automating the annotation process with the help of an LLM.", "AI": {"tldr": "The paper investigates the automation of legal concept annotation using LLMs, addressing the optimal volume of manual annotations and selection processes.", "motivation": "To improve the efficiency of legal research by automating the annotation of legal concepts and reducing the reliance on manual annotation.", "method": "Experiments conducted to assess the optimal number of annotations needed for legal concepts, the effectiveness of random versus selective annotation, and the outcomes of LLM-assisted automation of the annotation process.", "result": "Initial findings suggest that the automation process can reduce the cost and time associated with manual annotation while maintaining the relevancy of the interpretations retrieved.", "conclusion": "Automating the annotation process is promising, providing a balance between efficiency and quality in legal research.", "key_contributions": ["Identification of the optimal number of annotations for legal concepts", "Analysis of random versus selective annotation for model performance", "Exploration of LLM-based automation in legal annotation"], "limitations": "Limited to the scope of legal concepts investigated; further research needed to generalize findings across different legal domains.", "keywords": ["legal research", "annotation automation", "language models", "legal concepts", "LLM"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.14468", "pdf": "https://arxiv.org/pdf/2506.14468.pdf", "abs": "https://arxiv.org/abs/2506.14468", "title": "MERba: Multi-Receptive Field MambaVision for Micro-Expression Recognition", "authors": ["Xinglong Mao", "Shifeng Liu", "Sirui Zhao", "Tong Xu", "Enhong Chen"], "categories": ["cs.HC"], "comment": null, "summary": "Micro-expressions (MEs) are brief, involuntary facial movements that reveal\ngenuine emotions, holding significant potential in psychological diagnosis and\ncriminal investigations. Despite notable advances in automatic ME recognition\n(MER), existing methods still struggle to jointly capture localized muscle\nactivations and global facial dependencies, both critical for recognizing\nsubtle emotional cues. To tackle this challenge, we propose MERba, a novel\nmulti-receptive field architecture tailored for MER. MERba introduces a series\nof Local-Global Feature Integration stages, where fine-grained motion features\nare first extracted by local extractors containing MambaVision Mixers within\nnon-overlapping windows, and then global dependencies across these regions are\nmodeled via lightweight self-attention layers. This hierarchical design enables\na progressive transition from localized perception to holistic facial\nunderstanding. Furthermore, we introduce an asymmetric multi-scanning strategy\nto eliminate redundant scanning directions and enhance local spatial\nperception. To address the high inter-class similarity among negative MEs, we\nintroduce a Dual-Granularity Classification Module that decouples the\nrecognition process into a coarse-to-fine paradigm. Experiments on two\nbenchmark MER datasets demonstrate that MERba outperforms existing methods,\nwith ablation studies confirming the effectiveness of each proposed component.", "AI": {"tldr": "This paper presents MERba, a novel architecture for automatic micro-expression recognition that combines localized and global facial feature extraction to improve accuracy.", "motivation": "The challenge of accurately recognizing micro-expressions, which are critical for psychological diagnosis and criminal investigations, due to existing methods' inability to capture both local muscle activations and global facial dependencies.", "method": "MERba uses a multi-receptive field architecture with Local-Global Feature Integration stages that includes local feature extractors and lightweight self-attention layers to model global dependencies. It also implements an asymmetric multi-scanning strategy and a Dual-Granularity Classification Module to enhance recognition accuracy.", "result": "Experiments on benchmark datasets show that MERba outperforms existing methods in micro-expression recognition, with results further validated through ablation studies.", "conclusion": "The proposed MERba architecture effectively improves micro-expression recognition by integrating localized perceptions with holistic understanding, indicating significant advancements in this field.", "key_contributions": ["Introduction of Local-Global Feature Integration stages for micro-expression recognition.", "Development of an asymmetric multi-scanning strategy to boost local perception.", "Implementation of a Dual-Granularity Classification Module for improved recognition accuracy."], "limitations": "", "keywords": ["micro-expressions", "facial recognition", "machine learning", "psychological diagnosis", "criminal investigations"], "importance_score": 4, "read_time_minutes": 8}}
{"id": "2506.13978", "pdf": "https://arxiv.org/pdf/2506.13978.pdf", "abs": "https://arxiv.org/abs/2506.13978", "title": "AI shares emotion with humans across languages and cultures", "authors": ["Xiuwen Wu", "Hao Wang", "Zhiang Yan", "Xiaohan Tang", "Pengfei Xu", "Wai-Ting Siok", "Ping Li", "Jia-Hong Gao", "Bingjiang Lyu", "Lang Qin"], "categories": ["cs.CL"], "comment": null, "summary": "Effective and safe human-machine collaboration requires the regulated and\nmeaningful exchange of emotions between humans and artificial intelligence\n(AI). Current AI systems based on large language models (LLMs) can provide\nfeedback that makes people feel heard. Yet it remains unclear whether LLMs\nrepresent emotion in language as humans do, or whether and how the emotional\ntone of their output can be controlled. We assess human-AI emotional alignment\nacross linguistic-cultural groups and model-families, using interpretable LLM\nfeatures translated from concept-sets for over twenty nuanced emotion\ncategories (including six basic emotions). Our analyses reveal that LLM-derived\nemotion spaces are structurally congruent with human perception, underpinned by\nthe fundamental affective dimensions of valence and arousal. Furthermore, these\nemotion-related features also accurately predict large-scale behavioural data\non word ratings along these two core dimensions, reflecting both universal and\nlanguage-specific patterns. Finally, by leveraging steering vectors derived\nsolely from human-centric emotion concepts, we show that model expressions can\nbe stably and naturally modulated across distinct emotion categories, which\nprovides causal evidence that human emotion concepts can be used to\nsystematically induce LLMs to produce corresponding affective states when\nconveying content. These findings suggest AI not only shares emotional\nrepresentations with humans but its affective outputs can be precisely guided\nusing psychologically grounded emotion concepts.", "AI": {"tldr": "This paper explores human-AI emotional alignment, particularly how large language models (LLMs) can share and modulate emotional tones akin to human expressions.", "motivation": "To investigate whether LLMs can effectively represent human emotions in language and whether their emotional outputs can be controlled.", "method": "The authors assessed emotional alignment between humans and AI across linguistic-cultural groups and model families, utilizing interpretable LLM features from nuanced emotion categories to analyze emotional outputs.", "result": "The results indicate that LLM-derived emotion spaces align with human emotional perception, validating essential affective dimensions and enabling controlled modulation of AI emotional expressions.", "conclusion": "The study concludes that AI can share emotional representations with humans and that its outputs can be directed using grounded human emotion concepts, enhancing human-machine collaboration.", "key_contributions": ["Assessment of human-AI emotional alignment across linguistic-cultural groups", "Demonstration of LLM emotional representations congruent with human perception", "Development of steering vectors for controlled modulation of AI emotional outputs"], "limitations": "", "keywords": ["human-AI interaction", "large language models", "emotion representation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.14476", "pdf": "https://arxiv.org/pdf/2506.14476.pdf", "abs": "https://arxiv.org/abs/2506.14476", "title": "SimSpark: Interactive Simulation of Social Media Behaviors", "authors": ["Ziyue Lin", "Yi Shan", "Lin Gao", "Xinghua Jia", "Siming Chen"], "categories": ["cs.HC"], "comment": "32 pages, 7 figures", "summary": "Understanding user behaviors on social media has garnered significant\nscholarly attention, enhancing our comprehension of how virtual platforms\nimpact society and empowering decision-makers. Simulating social media\nbehaviors provides a robust tool for capturing the patterns of social media\nbehaviors, testing hypotheses, and predicting the effects of various\ninterventions, ultimately contributing to a deeper understanding of social\nmedia environments. Moreover, it can overcome difficulties associated with\nutilizing real data for analysis, such as data accessibility issues, ethical\nconcerns, and the complexity of processing large and heterogeneous datasets.\nHowever, researchers and stakeholders need more flexible platforms to\ninvestigate different user behaviors by simulating different scenarios and\ncharacters, which is not possible yet. Therefore, this paper introduces\nSimSpark, an interactive system including simulation algorithms and interactive\nvisual interfaces which is capable of creating small simulated social media\nplatforms with customizable characters and social environments. We address\nthree key challenges: generating believable behaviors, validating simulation\nresults, and supporting interactive control for generation and results\nanalysis. A simulation workflow is introduced to generate believable behaviors\nof agents by utilizing large language models. A visual interface enables\nreal-time parameter adjustment and process monitoring for customizing\ngeneration settings. A set of visualizations and interactions are also designed\nto display the models' outputs for further analysis. Effectiveness is evaluated\nthrough case studies, quantitative simulation model assessments, and expert\ninterviews.", "AI": {"tldr": "This paper presents SimSpark, an interactive system for simulating user behaviors on social media, featuring customizable characters and environments.", "motivation": "To enhance understanding and analysis of social media behaviors and overcome challenges related to real data methods, such as accessibility and ethical concerns.", "method": "SimSpark utilizes simulation algorithms and large language models to generate believable user behaviors and provides a visual interface for real-time parameter adjustment and analysis.", "result": "The effectiveness of SimSpark is evaluated through case studies, quantitative assessments, and expert interviews, demonstrating robust simulation capabilities.", "conclusion": "SimSpark addresses key challenges in social media behavior simulation, offering flexibility and interactivity for researchers and stakeholders.", "key_contributions": ["Introduction of an interactive simulation system for social media", "Integration of large language models for behavior generation", "Development of a customizable visual interface for real-time analysis"], "limitations": "", "keywords": ["social media", "simulation", "interactive system", "large language models", "user behavior"], "importance_score": 6, "read_time_minutes": 20}}
{"id": "2506.14012", "pdf": "https://arxiv.org/pdf/2506.14012.pdf", "abs": "https://arxiv.org/abs/2506.14012", "title": "Lost in the Mix: Evaluating LLM Understanding of Code-Switched Text", "authors": ["Amr Mohamed", "Yang Zhang", "Michalis Vazirgiannis", "Guokan Shang"], "categories": ["cs.CL"], "comment": null, "summary": "Code-switching (CSW) is the act of alternating between two or more languages\nwithin a single discourse. This phenomenon is widespread in multilingual\ncommunities, and increasingly prevalent in online content, where users\nnaturally mix languages in everyday communication. As a result, Large Language\nModels (LLMs), now central to content processing and generation, are frequently\nexposed to code-switched inputs. Given their widespread use, it is crucial to\nunderstand how LLMs process and reason about such mixed-language text. This\npaper presents a systematic evaluation of LLM comprehension under\ncode-switching by generating CSW variants of established reasoning and\ncomprehension benchmarks. While degradation is evident when foreign tokens\ndisrupt English text$\\unicode{x2013}$even under linguistic\nconstraints$\\unicode{x2013}$embedding English into other languages often\nimproves comprehension. Though prompting yields mixed results, fine-tuning\noffers a more stable path to degradation mitigation.", "AI": {"tldr": "This paper evaluates how Large Language Models (LLMs) comprehend code-switching (CSW) in multilingual discourse, demonstrating varying impacts on comprehension based on language interaction.", "motivation": "To understand how LLMs process mixed-language text due to the prevalence of code-switching in online communication.", "method": "A systematic evaluation by generating code-switched variants of established reasoning and comprehension benchmarks.", "result": "The study finds comprehension degradation when foreign tokens interrupt English text, but embedding English in other languages often enhances understanding. Fine-tuning shows potential for improving comprehension compared to mere prompting.", "conclusion": "Understanding LLM interaction with code-switching is crucial for improving their performance in multilingual settings, with fine-tuning proving to be an effective strategy.", "key_contributions": ["Systematic evaluation of LLMs on code-switched data", "Insights on how foreign language tokens affect comprehension", "Demonstration that embedding English in other languages can enhance understanding"], "limitations": "The effects observed may vary across different LLM architectures and training data.", "keywords": ["code-switching", "Large Language Models", "multilingual discourse", "comprehension", "fine-tuning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.14567", "pdf": "https://arxiv.org/pdf/2506.14567.pdf", "abs": "https://arxiv.org/abs/2506.14567", "title": "Controlling Context: Generative AI at Work in Integrated Circuit Design and Other High-Precision Domains", "authors": ["Emanuel Moss", "Elizabeth Watkins", "Christopher Persaud", "Passant Karunaratne", "Dawn Nafus"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Generative AI tools have become more prevalent in engineering workflows,\nparticularly through chatbots and code assistants. As the perceived accuracy of\nthese tools improves, questions arise about whether and how those who work in\nhigh-precision domains might maintain vigilance for errors, and what other\naspects of using such tools might trouble their work. This paper analyzes\ninterviews with hardware and software engineers, and their collaborators, who\nwork in integrated circuit design to identify the role accuracy plays in their\nuse of generative AI tools and what other forms of trouble they face in using\nsuch tools. The paper inventories these forms of trouble, which are then mapped\nto elements of generative AI systems, to conclude that controlling the context\nof interactions between engineers and the generative AI tools is one of the\nlargest challenges they face. The paper concludes with recommendations for\nmitigating this form of trouble by increasing the ability to control context\ninteractively.", "AI": {"tldr": "This paper explores the challenges faced by engineers using generative AI tools in high-precision domains, emphasizing the need for improved control over interaction contexts.", "motivation": "To understand how generative AI tools are perceived and used by engineers in high-precision fields, particularly regarding accuracy and interaction challenges.", "method": "The study conducts interviews with hardware and software engineers involved in integrated circuit design to gather insights on their experiences with generative AI tools.", "result": "The analysis identifies various forms of trouble encountered by engineers, particularly the challenge of maintaining control over the context of their interactions with AI tools.", "conclusion": "The paper concludes that improving interactive control of context is crucial for mitigating issues faced when using generative AI tools.", "key_contributions": ["Identified specific challenges engineers face when using generative AI tools in integrated circuit design.", "Mapped forms of troubles to generative AI system elements.", "Provided recommendations for enhancing interactive control in AI contexts."], "limitations": "", "keywords": ["Generative AI", "Engineering Workflows", "Human-Computer Interaction"], "importance_score": 5, "read_time_minutes": 15}}
{"id": "2506.14028", "pdf": "https://arxiv.org/pdf/2506.14028.pdf", "abs": "https://arxiv.org/abs/2506.14028", "title": "MultiFinBen: A Multilingual, Multimodal, and Difficulty-Aware Benchmark for Financial LLM Evaluation", "authors": ["Xueqing Peng", "Lingfei Qian", "Yan Wang", "Ruoyu Xiang", "Yueru He", "Yang Ren", "Mingyang Jiang", "Jeff Zhao", "Huan He", "Yi Han", "Yun Feng", "Yuechen Jiang", "Yupeng Cao", "Haohang Li", "Yangyang Yu", "Xiaoyu Wang", "Penglei Gao", "Shengyuan Lin", "Keyi Wang", "Shanshan Yang", "Yilun Zhao", "Zhiwei Liu", "Peng Lu", "Jerry Huang", "Suyuchen Wang", "Triantafillos Papadopoulos", "Polydoros Giannouris", "Efstathia Soufleri", "Nuo Chen", "Guojun Xiong", "Zhiyang Deng", "Yijia Zhao", "Mingquan Lin", "Meikang Qiu", "Kaleb E Smith", "Arman Cohan", "Xiao-Yang Liu", "Jimin Huang", "Alejandro Lopez-Lira", "Xi Chen", "Junichi Tsujii", "Jian-Yun Nie", "Sophia Ananiadou", "Qianqian Xie"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large language models (LLMs) have accelerated progress in\nfinancial NLP and applications, yet existing benchmarks remain limited to\nmonolingual and unimodal settings, often over-relying on simple tasks and\nfailing to reflect the complexity of real-world financial communication. We\nintroduce MultiFinBen, the first multilingual and multimodal benchmark tailored\nto the global financial domain, evaluating LLMs across modalities (text,\nvision, audio) and linguistic settings (monolingual, bilingual, multilingual)\non domain-specific tasks. We introduce two novel tasks, including PolyFiQA-Easy\nand PolyFiQA-Expert, the first multilingual financial benchmarks requiring\nmodels to perform complex reasoning over mixed-language inputs; and EnglishOCR\nand SpanishOCR, the first OCR-embedded financial QA tasks challenging models to\nextract and reason over information from visual-text financial documents.\nMoreover, we propose a dynamic, difficulty-aware selection mechanism and curate\na compact, balanced benchmark rather than simple aggregation existing datasets.\nExtensive evaluation of 22 state-of-the-art models reveals that even the\nstrongest models, despite their general multimodal and multilingual\ncapabilities, struggle dramatically when faced with complex cross-lingual and\nmultimodal tasks in financial domain. MultiFinBen is publicly released to\nfoster transparent, reproducible, and inclusive progress in financial studies\nand applications.", "AI": {"tldr": "Introduction of MultiFinBen, a multilingual and multimodal benchmark for evaluating LLMs in the financial domain.", "motivation": "To address the limitations of existing benchmarks in financial NLP that focus on monolingual and unimodal tasks, which do not capture the complexity of real-world financial communication.", "method": "Development of the MultiFinBen benchmark, including novel tasks PolyFiQA-Easy, PolyFiQA-Expert, EnglishOCR, and SpanishOCR, across different modalities and linguistic settings. Implementation of a dynamic, difficulty-aware selection mechanism for better evaluation.", "result": "Extensive evaluation of 22 state-of-the-art models demonstrated significant struggles in handling complex cross-lingual and multimodal tasks, revealing gaps in current LLM capabilities in the financial domain.", "conclusion": "MultiFinBen aims to facilitate progress in financial studies and applications by offering a robust and comprehensive benchmark for LLM evaluation.", "key_contributions": ["First multilingual and multimodal benchmark for financial NLP", "Introduction of complex reasoning tasks over mixed-language inputs", "Dynamic difficulty-aware selection mechanism for balanced benchmarking"], "limitations": "Focus on financial domain may limit general applicability to other fields.", "keywords": ["Large Language Models", "Financial NLP", "Multimodal Benchmark", "Cross-lingual Tasks", "Complex Reasoning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.14611", "pdf": "https://arxiv.org/pdf/2506.14611.pdf", "abs": "https://arxiv.org/abs/2506.14611", "title": "Exploring MLLMs Perception of Network Visualization Principles", "authors": ["Jacob Miller", "Markus Wallinger", "Ludwig Felder", "Timo Brand", "Henry Förster", "Johannes Zink", "Chunyang Chen", "Stephen Kobourov"], "categories": ["cs.HC"], "comment": null, "summary": "In this paper, we test whether Multimodal Large Language Models (MLLMs) can\nmatch human-subject performance in tasks involving the perception of properties\nin network layouts. Specifically, we replicate a human-subject experiment about\nperceiving quality (namely stress) in network layouts using GPT-4o and\nGemini-2.5. Our experiments show that giving MLLMs exactly the same study\ninformation as trained human participants results in a similar performance to\nhuman experts and exceeds the performance of untrained non-experts.\nAdditionally, we show that prompt engineering that deviates from the\nhuman-subject experiment can lead to better-than-human performance in some\nsettings. Interestingly, like human subjects, the MLLMs seem to rely on visual\nproxies rather than computing the actual value of stress, indicating some sense\nor facsimile of perception. Explanations from the models provide descriptions\nsimilar to those used by the human participants (e.g., even distribution of\nnodes and uniform edge lengths).", "AI": {"tldr": "This paper investigates the ability of Multimodal Large Language Models to replicate human performance in perceiving properties in network layouts and identifies effective prompt engineering techniques.", "motivation": "To understand whether MLLMs can achieve human-level performance in perception tasks pertaining to network layouts.", "method": "The study replicates a human-subject experiment on perceiving stress in network layouts using GPT-4o and Gemini-2.5. MLLMs are provided with the same information as trained human participants.", "result": "MLLMs demonstrated performance comparable to human experts and outperformed untrained non-experts, with certain prompt engineering approaches yielding better-than-human results.", "conclusion": "MLLMs' reliance on visual proxies suggests an emergent, human-like perception mechanism, mimicking the explanations provided by human participants.", "key_contributions": ["Evidence of MLLMs matching human performance in perception tasks.", "Identification of prompt engineering strategies that enhance performance.", "Insights into the nature of MLLM perception through visual proxies."], "limitations": "The study focuses on specific tasks and may not generalize across all perception scenarios.", "keywords": ["Multimodal Large Language Models", "human perception", "network layouts", "prompt engineering", "visual proxies"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.14040", "pdf": "https://arxiv.org/pdf/2506.14040.pdf", "abs": "https://arxiv.org/abs/2506.14040", "title": "An Interdisciplinary Review of Commonsense Reasoning and Intent Detection", "authors": ["Md Nazmus Sakib"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "This review explores recent advances in commonsense reasoning and intent\ndetection, two key challenges in natural language understanding. We analyze 28\npapers from ACL, EMNLP, and CHI (2020-2025), organizing them by methodology and\napplication. Commonsense reasoning is reviewed across zero-shot learning,\ncultural adaptation, structured evaluation, and interactive contexts. Intent\ndetection is examined through open-set models, generative formulations,\nclustering, and human-centered systems. By bridging insights from NLP and HCI,\nwe highlight emerging trends toward more adaptive, multilingual, and\ncontext-aware models, and identify key gaps in grounding, generalization, and\nbenchmark design.", "AI": {"tldr": "This review analyzes recent progress in commonsense reasoning and intent detection in natural language understanding by examining 28 papers from notable conferences, emphasizing the interdisciplinary relationship between NLP and HCI.", "motivation": "The paper aims to highlight advances and challenges in commonsense reasoning and intent detection, which are critical to improving natural language understanding systems.", "method": "Review of 28 papers from ACL, EMNLP, and CHI (2020-2025), organized by methodology and application.", "result": "Identified emerging trends towards adaptive, multilingual, and context-aware models while addressing gaps in grounding, generalization, and benchmark design.", "conclusion": "The synthesis of NLP and HCI insights points to the need for more effective evaluation methods and community engagement to tackle pressing issues in the domains of commonsense reasoning and intent detection.", "key_contributions": ["Analysis of commonsense reasoning and intent detection methodologies.", "Identification of emerging trends in model development.", "Discussion of gaps in existing research and benchmark designs."], "limitations": "", "keywords": ["commonsense reasoning", "intent detection", "natural language understanding", "HCI", "multilingual models"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.14653", "pdf": "https://arxiv.org/pdf/2506.14653.pdf", "abs": "https://arxiv.org/abs/2506.14653", "title": "How Viable are Energy Savings in Smart Homes? A Call to Embrace Rebound Effects in Sustainable HCI", "authors": ["Christina Bremer", "Harshit Gujral", "Michelle Lin", "Lily Hinkers", "Christoph Becker", "Vlad C. Coroamă"], "categories": ["cs.HC"], "comment": "25 pages, 3 figures", "summary": "As part of global climate action, digital technologies are seen as a key\nenabler of energy efficiency savings. A popular application domain for this\nwork is smart homes. There is a risk, however, that these efficiency gains\nresult in rebound effects, which reduce or even overcompensate the savings.\nRebound effects are well-established in economics, but it is less clear whether\nthey also inform smart energy research in other disciplines. In this paper, we\nask: to what extent have rebound effects and their underlying mechanisms been\nconsidered in computing, HCI and smart home research? To answer this, we\nconducted a literature mapping drawing on four scientific databases and a\nSIGCHI corpus. Our results reveal limited consideration of rebound effects and\nsignificant opportunities for HCI to advance this topic. We conclude with a\ntaxonomy of actions for HCI to address rebound effects and help determine the\nviability of energy efficiency projects.", "AI": {"tldr": "This paper explores the consideration of rebound effects in HCI and smart home research, revealing limited attention and proposing actions for HCI.", "motivation": "To investigate the extent to which rebound effects, which may reduce energy efficiency savings, have been considered in smart home and HCI research.", "method": "Conducted a literature mapping using four scientific databases and a SIGCHI corpus.", "result": "Findings indicate limited consideration of rebound effects in relevant literature, highlighting opportunities for enhancement in HCI.", "conclusion": "The paper offers a taxonomy of actions for HCI to address rebound effects, which could improve the viability of energy efficiency projects.", "key_contributions": ["Identification of limited attention to rebound effects in smart home research.", "Creation of a taxonomy of actions for HCI to address these effects.", "Recommendations for future research directions in energy efficiency."], "limitations": "Study focuses primarily on literature mapping, which may not capture all practical applications or ongoing research activities.", "keywords": ["rebounds effects", "HCI", "energy efficiency", "smart homes", "literature mapping"], "importance_score": 4, "read_time_minutes": 25}}
{"id": "2506.14046", "pdf": "https://arxiv.org/pdf/2506.14046.pdf", "abs": "https://arxiv.org/abs/2506.14046", "title": "Ace-CEFR -- A Dataset for Automated Evaluation of the Linguistic Difficulty of Conversational Texts for LLM Applications", "authors": ["David Kogan", "Max Schumacher", "Sam Nguyen", "Masanori Suzuki", "Melissa Smith", "Chloe Sophia Bellows", "Jared Bernstein"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "There is an unmet need to evaluate the language difficulty of short,\nconversational passages of text, particularly for training and filtering Large\nLanguage Models (LLMs). We introduce Ace-CEFR, a dataset of English\nconversational text passages expert-annotated with their corresponding level of\ntext difficulty. We experiment with several models on Ace-CEFR, including\nTransformer-based models and LLMs. We show that models trained on Ace-CEFR can\nmeasure text difficulty more accurately than human experts and have latency\nappropriate to production environments. Finally, we release the Ace-CEFR\ndataset to the public for research and development.", "AI": {"tldr": "Introduction of Ace-CEFR, a dataset for evaluating language difficulty of conversational passages to enhance training and filtering of LLMs.", "motivation": "There is a need to assess the difficulty level of short conversational texts to improve LLM training and performance.", "method": "Expert annotation of English conversational texts to create the Ace-CEFR dataset, followed by testing various models for measuring text difficulty.", "result": "Models trained on Ace-CEFR demonstrated greater accuracy in measuring text difficulty than human experts while also maintaining production-level latency.", "conclusion": "The public release of the Ace-CEFR dataset will aid in research and development efforts related to language difficulty assessment.", "key_contributions": ["Creation of Ace-CEFR dataset for measuring language difficulty", "Demonstration of model accuracy surpassing human experts", "Public release for broader research applications"], "limitations": "", "keywords": ["Language Difficulty", "Conversational Text", "Large Language Models", "Dataset", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.14670", "pdf": "https://arxiv.org/pdf/2506.14670.pdf", "abs": "https://arxiv.org/abs/2506.14670", "title": "StreetLens: Enabling Human-Centered AI Agents for Neighborhood Assessment from Street View Imagery", "authors": ["Jina Kim", "Leeje Jang", "Yao-Yi Chiang", "Guanyu Wang", "Michelle Pasco"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Traditionally, neighborhood studies have employed interviews, surveys, and\nmanual image annotation guided by detailed protocols to identify environmental\ncharacteristics, including physical disorder, decay, street safety, and\nsociocultural symbols, and to examine their impact on developmental and health\noutcomes. While these methods yield rich insights, they are time-consuming and\nrequire intensive expert intervention. Recent technological advances, including\nvision-language models (VLMs), have begun to automate parts of this process;\nhowever, existing efforts are often ad hoc and lack adaptability across\nresearch designs and geographic contexts. In this demo paper, we present\nStreetLens, a human-centered, researcher-configurable workflow that embeds\nrelevant social science expertise in a VLM for scalable neighborhood\nenvironmental assessments. StreetLens mimics the process of trained human\ncoders by grounding the analysis in questions derived from established\ninterview protocols, retrieving relevant street view imagery (SVI), and\ngenerating a wide spectrum of semantic annotations from objective features\n(e.g., the number of cars) to subjective perceptions (e.g., the sense of\ndisorder in an image). By enabling researchers to define the VLM's role through\ndomain-informed prompting, StreetLens places domain knowledge at the core of\nthe analysis process. It also supports the integration of prior survey data to\nenhance robustness and expand the range of characteristics assessed across\ndiverse settings. We provide a Google Colab notebook to make StreetLens\naccessible and extensible for researchers working with public or custom SVI\ndatasets. StreetLens represents a shift toward flexible, agentic AI systems\nthat work closely with researchers to accelerate and scale neighborhood\nstudies.", "AI": {"tldr": "StreetLens is a configurable workflow that integrates social science expertise into a vision-language model (VLM) to automate neighborhood environmental assessments, enabling scalable analysis and flexibility across diverse research designs.", "motivation": "To automate the traditional, manually intensive methods used in neighborhood studies by integrating vision-language models (VLMs) to improve efficiency and adaptability.", "method": "StreetLens utilizes socially-informed prompting within a VLM to analyze street view imagery (SVI) based on established protocols, generating semantic annotations ranging from objective features to subjective perceptions.", "result": "StreetLens allows researchers to customize the role of the VLM and integrate previous survey data, leading to enhanced analysis and applicability across various contexts.", "conclusion": "StreetLens represents a paradigm shift in neighborhood studies, promoting flexible, collaborative AI systems that streamline research processes while maintaining scholarly rigor.", "key_contributions": ["Integration of social science expertise into VLM workflows", "Configurable analysis framework for neighborhood studies", "Access to a Google Colab notebook for ease of use"], "limitations": "", "keywords": ["Human-Computer Interaction", "Vision-Language Models", "Neighborhood Studies"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.14064", "pdf": "https://arxiv.org/pdf/2506.14064.pdf", "abs": "https://arxiv.org/abs/2506.14064", "title": "Automatic Extraction of Clausal Embedding Based on Large-Scale English Text Data", "authors": ["Iona Carslaw", "Sivan Milton", "Nicolas Navarre", "Ciyang Qing", "Wataru Uegaki"], "categories": ["cs.CL"], "comment": "Accepted in the Society for Computation in Linguistics", "summary": "For linguists, embedded clauses have been of special interest because of\ntheir intricate distribution of syntactic and semantic features. Yet, current\nresearch relies on schematically created language examples to investigate these\nconstructions, missing out on statistical information and naturally-occurring\nexamples that can be gained from large language corpora. Thus, we present a\nmethodological approach for detecting and annotating naturally-occurring\nexamples of English embedded clauses in large-scale text data using\nconstituency parsing and a set of parsing heuristics. Our tool has been\nevaluated on our dataset Golden Embedded Clause Set (GECS), which includes\nhand-annotated examples of naturally-occurring English embedded clause\nsentences. Finally, we present a large-scale dataset of naturally-occurring\nEnglish embedded clauses which we have extracted from the open-source corpus\nDolma using our extraction tool.", "AI": {"tldr": "This paper presents a methodological approach for detecting and annotating naturally-occurring embedded clauses in English using constituency parsing and heuristics.", "motivation": "The study aims to utilize large language corpora to analyze embedded clauses in English, which have previously been researched using limited examples.", "method": "The authors developed a tool for detecting and annotating embedded clauses by applying constituency parsing techniques and parsing heuristics to large-scale text data.", "result": "The tool was evaluated using the Golden Embedded Clause Set (GECS) dataset and successfully extracted a large-scale dataset of naturally-occurring English embedded clauses from the Dolma corpus.", "conclusion": "The research contributes to a deeper understanding of embedded clauses by providing a new dataset and insights gained from leveraging statistical information from large text corpora.", "key_contributions": ["Development of a tool for detecting naturally-occurring embedded clauses", "Creation of the Golden Embedded Clause Set (GECS) for evaluation", "Extraction of a large-scale dataset from the Dolma corpus."], "limitations": "", "keywords": ["embedded clauses", "constituency parsing", "natural language processing", "linguistics", "dataset"], "importance_score": 2, "read_time_minutes": 15}}
{"id": "2506.14677", "pdf": "https://arxiv.org/pdf/2506.14677.pdf", "abs": "https://arxiv.org/abs/2506.14677", "title": "Design an Editable Speech-to-Sign-Language Transformer System: A Human-Centered AI Approach", "authors": ["Yingchao Li"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "This paper presents a human-centered, real-time, user-adaptive speech-to-sign\nlanguage animation system that integrates Transformer-based motion generation\nwith a transparent, user-editable JSON intermediate layer. The framework\novercomes key limitations in prior sign language technologies by enabling\ndirect user inspection and modification of sign segments, thus enhancing\nnaturalness, expressiveness, and user agency. Leveraging a streaming Conformer\nencoder and autoregressive Transformer-MDN decoder, the system synchronizes\nspoken input into upper-body and facial motion for 3D avatar rendering. Edits\nand user ratings feed into a human-in-the-loop optimization loop for continuous\nimprovement. Experiments with 20 deaf signers and 5 interpreters show that the\neditable interface and participatory feedback significantly improve\ncomprehension, naturalness, usability, and trust, while lowering cognitive\nload. With sub-20 ms per-frame inference on standard hardware, the system is\nready for real-time communication and education. This work illustrates how\ntechnical and participatory innovation together enable accessible, explainable,\nand user-adaptive AI for sign language technology.", "AI": {"tldr": "This paper introduces a real-time, user-adaptive speech-to-sign language system using Transformer-based motion generation, focusing on user-editable JSON for improved expressiveness and trust.", "motivation": "To improve accessibility and user agency in sign language technologies by allowing users to inspect and modify sign segments directly.", "method": "The system uses a streaming Conformer encoder and an autoregressive Transformer-MDN decoder to synchronize spoken input with 3D avatar animation, providing an editable interface for users.", "result": "Experiments show significant improvements in comprehension, naturalness, usability, and lowered cognitive load for 20 deaf signers and 5 interpreters.", "conclusion": "The combination of technical advancements and user participation results in a more accessible and explainable AI for sign language applications.", "key_contributions": ["Real-time speech-to-sign conversion", "User-editable JSON for customization", "Integration of participatory feedback loops for continual improvement"], "limitations": "", "keywords": ["sign language", "user-adaptive AI", "transformer", "natural language processing", "real-time communication"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.14101", "pdf": "https://arxiv.org/pdf/2506.14101.pdf", "abs": "https://arxiv.org/abs/2506.14101", "title": "Abstract Meaning Representation for Hospital Discharge Summarization", "authors": ["Paul Landes", "Sitara Rao", "Aaron Jeremy Chaise", "Barbara Di Eugenio"], "categories": ["cs.CL"], "comment": null, "summary": "The Achilles heel of Large Language Models (LLMs) is hallucination, which has\ndrastic consequences for the clinical domain. This is particularly important\nwith regards to automatically generating discharge summaries (a lengthy medical\ndocument that summarizes a hospital in-patient visit). Automatically generating\nthese summaries would free physicians to care for patients and reduce\ndocumentation burden. The goal of this work is to discover new methods that\ncombine language-based graphs and deep learning models to address provenance of\ncontent and trustworthiness in automatic summarization. Our method shows\nimpressive reliability results on the publicly available Medical Information\nMart for Intensive III (MIMIC-III) corpus and clinical notes written by\nphysicians at Anonymous Hospital. rovide our method, generated discharge ary\noutput examples, source code and trained models.", "AI": {"tldr": "This paper addresses hallucination in LLMs by proposing a method that combines language-based graphs and deep learning for generating trustworthy discharge summaries in the clinical domain.", "motivation": "The paper is motivated by the need to reduce the documentation burden on physicians and improve the trustworthiness of automatically generated medical summaries, especially in light of the challenges posed by hallucinations in LLMs.", "method": "The proposed method utilizes language-based graphs in conjunction with deep learning models to enhance the provenance and reliability of automatically generated discharge summaries.", "result": "The method achieved impressive reliability results when tested on the MIMIC-III corpus and clinical notes from Anonymous Hospital, demonstrating the potential for effective application in clinical settings.", "conclusion": "The findings indicate that combining language-based graphs with deep learning can significantly improve the quality and trustworthiness of discharge summaries generated by LLMs.", "key_contributions": ["Development of a novel method that integrates language-based graphs with deep learning for automatic summarization in healthcare.", "Reliability validation on real clinical data from MIMIC-III and Anonymous Hospital.", "Provision of generated output examples, source code, and trained models for further research and application."], "limitations": "The study primarily focuses on the use of specific datasets and may not account for variations in content across different medical contexts or institutions.", "keywords": ["Large Language Models", "hallucination", "automatic summarization", "healthcare", "deep learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.14720", "pdf": "https://arxiv.org/pdf/2506.14720.pdf", "abs": "https://arxiv.org/abs/2506.14720", "title": "How Warm-Glow Alters the Usability of Technology", "authors": ["Antonios Saravanos"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "As technology increasingly aligns with users' personal values, traditional\nmodels of usability, focused on functionality and specifically effectiveness,\nefficiency, and satisfaction, may not fully capture how people perceive and\nevaluate it. This study investigates how the warm-glow phenomenon, the positive\nfeeling associated with doing good, shapes perceived usability. An experimental\napproach was taken in which participants evaluated a hypothetical technology\nunder conditions designed to evoke either the intrinsic (i.e., personal\nfulfillment) or extrinsic (i.e., social recognition) dimensions of warm-glow. A\nMultivariate Analysis of Variance as well as subsequent follow-up analyses\nrevealed that intrinsic warm-glow significantly enhances all dimensions of\nperceived usability, while extrinsic warm-glow selectively influences perceived\neffectiveness and satisfaction. These findings suggest that perceptions of\nusability extend beyond functionality and are shaped by how technology\nresonates with users' broader sense of purpose. We conclude by proposing that\ndesigners consider incorporating warm-glow into technology as a strategic\ndesign decision.", "AI": {"tldr": "This study explores how the warm-glow phenomenon influences perceived usability, finding that intrinsic warm-glow enhances usability dimensions, while extrinsic warm-glow affects perceived effectiveness and satisfaction.", "motivation": "To investigate the role of the warm-glow phenomenon in shaping perceptions of usability beyond traditional models focused on functionality.", "method": "An experimental approach where participants evaluated technology under conditions evoking intrinsic or extrinsic warm-glow dimensions. Multivariate Analysis of Variance was used for analysis.", "result": "Intrinsic warm-glow significantly enhances all usability dimensions, while extrinsic warm-glow selectively impacts perceived effectiveness and satisfaction.", "conclusion": "Designers should incorporate warm-glow into technology design to align with users' values and enhance perceived usability.", "key_contributions": ["Demonstrated the impact of warm-glow on usability perception", "Differentiated between intrinsic and extrinsic dimensions of warm-glow", "Proposed strategic design considerations for technology to enhance usability"], "limitations": "", "keywords": ["warm-glow", "usability", "technology design", "intrinsic", "extrinsic"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2506.14111", "pdf": "https://arxiv.org/pdf/2506.14111.pdf", "abs": "https://arxiv.org/abs/2506.14111", "title": "Essential-Web v1.0: 24T tokens of organized web data", "authors": ["Essential AI", ":", "Andrew Hojel", "Michael Pust", "Tim Romanski", "Yash Vanjani", "Ritvik Kapila", "Mohit Parmar", "Adarsh Chaluvaraju", "Alok Tripathy", "Anil Thomas", "Ashish Tanwer", "Darsh J Shah", "Ishaan Shah", "Karl Stratos", "Khoi Nguyen", "Kurt Smith", "Michael Callahan", "Peter Rushton", "Philip Monk", "Platon Mazarakis", "Saad Jamal", "Saurabh Srivastava", "Somanshu Singla", "Ashish Vaswani"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Data plays the most prominent role in how language models acquire skills and\nknowledge. The lack of massive, well-organized pre-training datasets results in\ncostly and inaccessible data pipelines. We present Essential-Web v1.0, a\n24-trillion-token dataset in which every document is annotated with a\ntwelve-category taxonomy covering topic, format, content complexity, and\nquality. Taxonomy labels are produced by EAI-Distill-0.5b, a fine-tuned\n0.5b-parameter model that achieves an annotator agreement within 3% of\nQwen2.5-32B-Instruct. With nothing more than SQL-style filters, we obtain\ncompetitive web-curated datasets in math (-8.0% relative to SOTA), web code\n(+14.3%), STEM (+24.5%) and medical (+8.6%). Essential-Web v1.0 is available on\nHuggingFace: https://huggingface.co/datasets/EssentialAI/essential-web-v1.0", "AI": {"tldr": "Essential-Web v1.0 is a large, annotated dataset designed to improve data accessibility for language models, containing 24 trillion tokens with detailed categorization.", "motivation": "To address the challenges posed by the lack of large, well-organized pre-training datasets for language models, which results in costly data pipelines.", "method": "The dataset includes 24 trillion tokens, with each document annotated using a twelve-category taxonomy. Taxonomy labels were created using a fine-tuned model, EAI-Distill-0.5b, achieving high annotator agreement.", "result": "Essential-Web v1.0 enables the extraction of competitive web-curated datasets in various fields with SQL-style filters, showing improvements in STEM and medical fields compared to existing datasets.", "conclusion": "Essential-Web v1.0 is expected to facilitate better training data for language models, thereby enhancing their skill acquisition and performance.", "key_contributions": ["Introduction of a 24-trillion-token dataset with comprehensive annotations", "Provides a twelve-category taxonomy for document classification", "Demonstrates competitive benchmarks in various fields compared to state-of-the-art datasets."], "limitations": "", "keywords": ["dataset", "language models", "taxonomy", "annotations", "web data"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.14123", "pdf": "https://arxiv.org/pdf/2506.14123.pdf", "abs": "https://arxiv.org/abs/2506.14123", "title": "Sampling from Your Language Model One Byte at a Time", "authors": ["Jonathan Hayase", "Alisa Liu", "Noah A. Smith", "Sewoong Oh"], "categories": ["cs.CL", "cs.FL", "cs.LG"], "comment": "23 pages, 8 figures", "summary": "Tokenization is used almost universally by modern language models, enabling\nefficient text representation using multi-byte or multi-character tokens.\nHowever, prior work has shown that tokenization can introduce distortion into\nthe model's generations. For example, users are often advised not to end their\nprompts with a space because it prevents the model from including the space as\npart of the next token. This Prompt Boundary Problem (PBP) also arises in\nlanguages such as Chinese and in code generation, where tokens often do not\nline up with syntactic boundaries. Additionally mismatching tokenizers often\nhinder model composition and interoperability. For example, it is not possible\nto directly ensemble models with different tokenizers due to their mismatching\nvocabularies. To address these issues, we present an inference-time method to\nconvert any autoregressive LM with a BPE tokenizer into a character-level or\nbyte-level LM, without changing its generative distribution at the text level.\nOur method efficient solves the PBP and is also able to unify the vocabularies\nof language models with different tokenizers, allowing one to ensemble LMs with\ndifferent tokenizers at inference time as well as transfer the post-training\nfrom one model to another using proxy-tuning. We demonstrate in experiments\nthat the ensemble and proxy-tuned models outperform their constituents on\ndownstream evals.", "AI": {"tldr": "This paper presents a method to unify different tokenizers used in language models, addressing issues like the Prompt Boundary Problem (PBP) and enabling model ensembling.", "motivation": "Tokenization can distort model generations and cause interoperability issues among language models, particularly with different tokenizers.", "method": "The authors propose an inference-time method to convert autoregressive language models with BPE tokenizers into character-level or byte-level models without altering their generative distribution.", "result": "The proposed method effectively solves the Prompt Boundary Problem and allows for the unification of vocabularies of models with different tokenizers, improving the performance of ensemble models and enabling proxy-tuning.", "conclusion": "The ensemble and proxy-tuned models demonstrate improved performance on downstream evaluations compared to individual models.", "key_contributions": ["Introduction of a method to convert autoregressive LMs with BPE tokenizers to character-level or byte-level LMs.", "Solution to the Prompt Boundary Problem (PBP).", "Facilitation of model ensembling and proxy-tuning across different tokenizers."], "limitations": "", "keywords": ["tokenization", "language models", "Prompt Boundary Problem", "model ensembling", "proxy-tuning"], "importance_score": 8, "read_time_minutes": 23}}
{"id": "2506.14157", "pdf": "https://arxiv.org/pdf/2506.14157.pdf", "abs": "https://arxiv.org/abs/2506.14157", "title": "DCRM: A Heuristic to Measure Response Pair Quality in Preference Optimization", "authors": ["Chengyu Huang", "Tanya Goyal"], "categories": ["cs.CL"], "comment": null, "summary": "Recent research has attempted to associate preference optimization (PO)\nperformance with the underlying preference datasets. In this work, our\nobservation is that the differences between the preferred response $y^+$ and\ndispreferred response $y^-$ influence what LLMs can learn, which may not match\nthe desirable differences to learn. Therefore, we use distance and reward\nmargin to quantify these differences, and combine them to get Distance\nCalibrated Reward Margin (DCRM), a metric that measures the quality of a\nresponse pair for PO. Intuitively, DCRM encourages minimal noisy differences\nand maximal desired differences. With this, we study 3 types of commonly used\npreference datasets, classified along two axes: the source of the responses and\nthe preference labeling function. We establish a general correlation between\nhigher DCRM of the training set and better learning outcome. Inspired by this,\nwe propose a best-of-$N^2$ pairing method that selects response pairs with the\nhighest DCRM. Empirically, in various settings, our method produces training\ndatasets that can further improve models' performance on AlpacaEval, MT-Bench,\nand Arena-Hard over the existing training sets.", "AI": {"tldr": "This paper introduces Distance Calibrated Reward Margin (DCRM) as a metric to optimize preference datasets for training models, demonstrating its effectiveness through various empirical evaluations.", "motivation": "To analyze how the differences between preferred and dispreferred responses affect learning in LLMs, and to improve preference dataset quality for better optimization performance.", "method": "The authors develop a new metric, Distance Calibrated Reward Margin (DCRM), which measures the quality of response pairs in preference optimization by quantifying the distance and reward margin between preferred and dispreferred responses. They propose a best-of-$N^2$ pairing method to select the optimal response pairs based on DCRM.", "result": "The study finds a correlation between higher DCRM values in training sets and improved model performance across various evaluation settings, demonstrating the efficacy of the proposed method on datasets like AlpacaEval, MT-Bench, and Arena-Hard.", "conclusion": "By leveraging DCRM to optimize preference datasets, the proposed methods can significantly enhance the performance of language models in preference optimization tasks.", "key_contributions": ["Introduction of Distance Calibrated Reward Margin (DCRM) as a new metric for preference optimization.", "Establishment of a correlation between DCRM and learning outcomes in LLMs.", "Development of a best-of-$N^2$ pairing method for selecting high-quality response pairs."], "limitations": "", "keywords": ["preference optimization", "distance calibrated reward margin", "language models", "training datasets", "machine learning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.14158", "pdf": "https://arxiv.org/pdf/2506.14158.pdf", "abs": "https://arxiv.org/abs/2506.14158", "title": "S$^4$C: Speculative Sampling with Syntactic and Semantic Coherence for Efficient Inference of Large Language Models", "authors": ["Tao He", "Guang Huang", "Yu Yang", "Tianshi Xu", "Sicheng Zhao", "Guiguang Ding", "Pengyang Wang", "Feng Tian"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) exhibit remarkable reasoning capabilities across\ndiverse downstream tasks. However, their autoregressive nature leads to\nsubstantial inference latency, posing challenges for real-time applications.\nSpeculative sampling mitigates this issue by introducing a drafting phase\nfollowed by a parallel validation phase, enabling faster token generation and\nverification. Existing approaches, however, overlook the inherent coherence in\ntext generation, limiting their efficiency. To address this gap, we propose a\nSpeculative Sampling with Syntactic and Semantic Coherence (S$^4$C) framework,\nwhich extends speculative sampling by leveraging multi-head drafting for rapid\ntoken generation and a continuous verification tree for efficient candidate\nvalidation and feature reuse. Experimental results demonstrate that S$^4$C\nsurpasses baseline methods across mainstream tasks, offering enhanced\nefficiency, parallelism, and the ability to generate more valid tokens with\nfewer computational resources. On Spec-bench benchmarks, S$^4$C achieves an\nacceleration ratio of 2.26x-2.60x, outperforming state-of-the-art methods.", "AI": {"tldr": "The S$^4$C framework enhances speculative sampling for LLMs by improving token generation speed and validation efficiency through coherence-aware methods.", "motivation": "The autoregressive nature of LLMs results in inference latency, hindering real-time applications.", "method": "The proposed framework, S$^4$C, incorporates multi-head drafting for fast token generation and a continuous verification tree for effective candidate validation and feature reuse.", "result": "Experimental results indicate that S$^4$C improves efficiency and parallelism, achieving an acceleration ratio of 2.26x-2.60x on Spec-bench benchmarks compared to state-of-the-art methods.", "conclusion": "S$^4$C demonstrates superior performance in generating valid tokens while reducing computation costs.", "key_contributions": ["Introduction of multi-head drafting for rapid token generation", "Development of a continuous verification tree for efficient validation", "Significant improvements in efficiency and resource usage over existing methods"], "limitations": "", "keywords": ["large language models", "inference latency", "speculative sampling", "coherence", "token generation"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.14040", "pdf": "https://arxiv.org/pdf/2506.14040.pdf", "abs": "https://arxiv.org/abs/2506.14040", "title": "An Interdisciplinary Review of Commonsense Reasoning and Intent Detection", "authors": ["Md Nazmus Sakib"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "This review explores recent advances in commonsense reasoning and intent\ndetection, two key challenges in natural language understanding. We analyze 28\npapers from ACL, EMNLP, and CHI (2020-2025), organizing them by methodology and\napplication. Commonsense reasoning is reviewed across zero-shot learning,\ncultural adaptation, structured evaluation, and interactive contexts. Intent\ndetection is examined through open-set models, generative formulations,\nclustering, and human-centered systems. By bridging insights from NLP and HCI,\nwe highlight emerging trends toward more adaptive, multilingual, and\ncontext-aware models, and identify key gaps in grounding, generalization, and\nbenchmark design.", "AI": {"tldr": "This review analyzes advances in commonsense reasoning and intent detection in natural language understanding, discussing methodologies and applications from recent literature.", "motivation": "The need to better understand commonsense reasoning and intent detection to improve natural language understanding in various contexts.", "method": "The review organizes findings from 28 papers from ACL, EMNLP, and CHI by methodology and application, focusing on commonsense reasoning and intent detection.", "result": "Identifies emerging trends towards adaptive, multilingual, and context-aware models, while revealing gaps in grounding, generalization, and benchmark design.", "conclusion": "The analysis underscores the importance of bridging insights from NLP and HCI to enhance model capabilities in commonsense reasoning and intent detection.", "key_contributions": ["Comprehensive review of literature from top conferences", "Framework for understanding commonsense reasoning and intent detection", "Identification of key gaps and emerging trends in the field"], "limitations": "", "keywords": ["commonsense reasoning", "intent detection", "natural language understanding", "multilingual models", "HCI"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.14161", "pdf": "https://arxiv.org/pdf/2506.14161.pdf", "abs": "https://arxiv.org/abs/2506.14161", "title": "MIST: Towards Multi-dimensional Implicit Bias and Stereotype Evaluation of LLMs via Theory of Mind", "authors": ["Yanlin Li", "Hao Liu", "Huimin Liu", "Yinwei Wei", "Yupeng Hu"], "categories": ["cs.CL"], "comment": null, "summary": "Theory of Mind (ToM) in Large Language Models (LLMs) refers to their capacity\nfor reasoning about mental states, yet failures in this capacity often manifest\nas systematic implicit bias. Evaluating this bias is challenging, as\nconventional direct-query methods are susceptible to social desirability\neffects and fail to capture its subtle, multi-dimensional nature. To this end,\nwe propose an evaluation framework that leverages the Stereotype Content Model\n(SCM) to reconceptualize bias as a multi-dimensional failure in ToM across\nCompetence, Sociability, and Morality. The framework introduces two indirect\ntasks: the Word Association Bias Test (WABT) to assess implicit lexical\nassociations and the Affective Attribution Test (AAT) to measure covert\naffective leanings, both designed to probe latent stereotypes without\ntriggering model avoidance. Extensive experiments on 8 State-of-the-Art LLMs\ndemonstrate our framework's capacity to reveal complex bias structures,\nincluding pervasive sociability bias, multi-dimensional divergence, and\nasymmetric stereotype amplification, thereby providing a more robust\nmethodology for identifying the structural nature of implicit bias.", "AI": {"tldr": "Proposes an evaluation framework to assess implicit bias in Large Language Models (LLMs) using the Stereotype Content Model (SCM).", "motivation": "To address the challenges of evaluating implicit bias in LLMs, which is often obscured by traditional direct-query methods.", "method": "Introduces two indirect tasks, the Word Association Bias Test (WABT) and the Affective Attribution Test (AAT), to assess implicit lexical associations and covert affective leanings, respectively.", "result": "Extensive experiments reveal complex bias structures in 8 state-of-the-art LLMs, including sociability bias and asymmetric stereotype amplification.", "conclusion": "The proposed framework provides a robust methodology for identifying and understanding implicit bias in LLMs.", "key_contributions": ["Development of WABT and AAT for evaluating implicit bias", "Reconceptualization of bias as multi-dimensional in ToM", "Experimental validation on State-of-the-Art LLMs"], "limitations": "Framework might not capture all forms of bias and is focused on specific dimensions of ToM.", "keywords": ["Theory of Mind", "implicit bias", "Large Language Models", "Stereotype Content Model", "evaluation framework"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.14200", "pdf": "https://arxiv.org/pdf/2506.14200.pdf", "abs": "https://arxiv.org/abs/2506.14200", "title": "ELI-Why: Evaluating the Pedagogical Utility of Language Model Explanations", "authors": ["Brihi Joshi", "Keyu He", "Sahana Ramnath", "Sadra Sabouri", "Kaitlyn Zhou", "Souti Chattopadhyay", "Swabha Swayamdipta", "Xiang Ren"], "categories": ["cs.CL", "cs.HC"], "comment": "Findings of ACL 2025", "summary": "Language models today are widely used in education, yet their ability to\ntailor responses for learners with varied informational needs and knowledge\nbackgrounds remains under-explored. To this end, we introduce ELI-Why, a\nbenchmark of 13.4K \"Why\" questions to evaluate the pedagogical capabilities of\nlanguage models. We then conduct two extensive human studies to assess the\nutility of language model-generated explanatory answers (explanations) on our\nbenchmark, tailored to three distinct educational grades: elementary,\nhigh-school and graduate school. In our first study, human raters assume the\nrole of an \"educator\" to assess model explanations' fit to different\neducational grades. We find that GPT-4-generated explanations match their\nintended educational background only 50% of the time, compared to 79% for lay\nhuman-curated explanations. In our second study, human raters assume the role\nof a learner to assess if an explanation fits their own informational needs.\nAcross all educational backgrounds, users deemed GPT-4-generated explanations\n20% less suited on average to their informational needs, when compared to\nexplanations curated by lay people. Additionally, automated evaluation metrics\nreveal that explanations generated across different language model families for\ndifferent informational needs remain indistinguishable in their grade-level,\nlimiting their pedagogical effectiveness.", "AI": {"tldr": "This paper introduces ELI-Why, a benchmark for evaluating language model explanations in education, assessing their fit for various educational levels, and comparing them to human-curated responses.", "motivation": "To explore how well language models can generate tailored explanations for learners with diverse informational needs across different educational grades.", "method": "The study utilized a benchmark of 13.4K 'Why' questions and conducted two human studies: one with raters as educators evaluating the fit of explanations, and another with raters as learners assessing personal informational needs.", "result": "GPT-4-generated explanations were found to match the intended educational background only 50% of the time, and suffered from a 20% lower fit for learners' informational needs compared to human responses.", "conclusion": "The findings indicate a significant gap in the pedagogical effectiveness of language model-generated explanations, highlighting the need for improved adaptability in AI systems for educational use.", "key_contributions": ["Introduction of the ELI-Why benchmark for evaluating educational explanations", "Comparison of language model explanations versus human-curated responses", "Insights into the limitations of current model outputs in educational contexts"], "limitations": "Findings are based on a specific set of language models and may not generalize across all AI systems.", "keywords": ["Language Models", "Education", "Human-Computer Interaction", "Pedagogical Capabilities", "AI in Education"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.14175", "pdf": "https://arxiv.org/pdf/2506.14175.pdf", "abs": "https://arxiv.org/abs/2506.14175", "title": "GRAM: A Generative Foundation Reward Model for Reward Generalization", "authors": ["Chenglong Wang", "Yang Gan", "Yifu Huo", "Yongyu Mu", "Qiaozhi He", "Murun Yang", "Bei Li", "Tong Xiao", "Chunliang Zhang", "Tongran Liu", "Jingbo Zhu"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ICML 2025", "summary": "In aligning large language models (LLMs), reward models have played an\nimportant role, but are standardly trained as discriminative models and rely\nonly on labeled human preference data. In this paper, we explore methods that\ntrain reward models using both unlabeled and labeled data. Building on the\ngenerative models in LLMs, we develop a generative reward model that is first\ntrained via large-scale unsupervised learning and then fine-tuned via\nsupervised learning. We also show that by using label smoothing, we are in fact\noptimizing a regularized pairwise ranking loss. This result, in turn, provides\na new view of training reward models, which links generative models and\ndiscriminative models under the same class of training objectives. The outcome\nof these techniques is a foundation reward model, which can be applied to a\nwide range of tasks with little or no further fine-tuning effort. Extensive\nexperiments show that this model generalizes well across several tasks,\nincluding response ranking, reinforcement learning from human feedback, and\ntask adaptation with fine-tuning, achieving significant performance\nimprovements over several strong baseline models.", "AI": {"tldr": "This paper introduces a generative reward model for training LLMs that utilizes both labeled and unlabeled data, yielding significant improvements in performance across various tasks.", "motivation": "To improve training methods for reward models in large language models by utilizing both unlabeled and labeled data.", "method": "A generative reward model is trained using large-scale unsupervised learning followed by fine-tuning through supervised learning, incorporating label smoothing to optimize a regularized pairwise ranking loss.", "result": "The generative reward model achieves significant performance improvements over strong baselines in tasks such as response ranking, reinforcement learning from human feedback, and task adaptation with minimal further fine-tuning.", "conclusion": "The proposed model provides a unifying perspective on training reward models by linking generative and discriminative approaches, creating a foundation model applicable to diverse tasks.", "key_contributions": ["Introduction of a generative reward model utilizing both unlabeled and labeled data.", "Methodology that incorporates label smoothing for optimizing a regularized ranking loss.", "Demonstrated significant performance improvements on various tasks with minimal fine-tuning."], "limitations": "", "keywords": ["reward models", "large language models", "unsupervised learning", "fine-tuning", "label smoothing"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.14177", "pdf": "https://arxiv.org/pdf/2506.14177.pdf", "abs": "https://arxiv.org/abs/2506.14177", "title": "Can we train ASR systems on Code-switch without real code-switch data? Case study for Singapore's languages", "authors": ["Tuan Nguyen", "Huy-Dat Tran"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "Accepted by Interspeech 2025", "summary": "Code-switching (CS), common in multilingual settings, presents challenges for\nASR due to scarce and costly transcribed data caused by linguistic complexity.\nThis study investigates building CS-ASR using synthetic CS data. We propose a\nphrase-level mixing method to generate synthetic CS data that mimics natural\npatterns. Utilizing monolingual augmented with synthetic phrase-mixed CS data\nto fine-tune large pretrained ASR models (Whisper, MMS, SeamlessM4T). This\npaper focuses on three under-resourced Southeast Asian language pairs:\nMalay-English (BM-EN), Mandarin-Malay (ZH-BM), and Tamil-English (TA-EN),\nestablishing a new comprehensive benchmark for CS-ASR to evaluate the\nperformance of leading ASR models. Experimental results show that the proposed\ntraining strategy enhances ASR performance on monolingual and CS tests, with\nBM-EN showing highest gains, then TA-EN and ZH-BM. This finding offers a\ncost-effective approach for CS-ASR development, benefiting research and\nindustry.", "AI": {"tldr": "This study proposes a method for building code-switching automatic speech recognition (CS-ASR) systems using synthetic data to improve performance in multilingual settings.", "motivation": "Code-switching in ASR is challenging due to a lack of transcribed data, making effective recognition difficult in multilingual environments.", "method": "A phrase-level mixing method is developed to create synthetic code-switching data, which is then used to fine-tune large pretrained ASR models like Whisper, MMS, and SeamlessM4T.", "result": "The proposed method significantly enhances ASR performance across monolingual and code-switching tests, with the best improvements observed for the Malay-English language pair.", "conclusion": "The approach provides a cost-effective strategy for developing code-switching ASR systems, which is valuable for both research and practical applications in the industry.", "key_contributions": ["A novel phrase-level mixing method for synthetic code-switching data generation", "Benchmarking of leading ASR models on under-resourced Southeast Asian language pairs", "Demonstration of improved ASR performance using the proposed training strategy"], "limitations": "", "keywords": ["code-switching", "automatic speech recognition", "synthetic data", "multilingual", "ASR performance"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.14190", "pdf": "https://arxiv.org/pdf/2506.14190.pdf", "abs": "https://arxiv.org/abs/2506.14190", "title": "AsyncSwitch: Asynchronous Text-Speech Adaptation for Code-Switched ASR", "authors": ["Tuan Nguyen", "Huy-Dat Tran"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "This work has been submitted to the IEEE for possible publication.\n  This paper is a preprint version submitted to the 2025 IEEE Automatic Speech\n  Recognition and Understanding Workshop (ASRU 2025)", "summary": "Developing code-switched ASR systems is challenging due to language ambiguity\nand limited exposure to multilingual, code-switched data, while collecting such\nspeech is costly. Prior work generates synthetic audio from text, but these\nmethods are computationally intensive and hard to scale. We introduce\nAsyncSwitch, a novel asynchronous adaptation framework that leverages\nlarge-scale, text-rich web data to pre-expose ASR models to diverse\ncode-switched domains before fine-tuning on paired speech-text corpora. Our\nthree-stage process (1) trains decoder self-attention and feedforward layers on\ncode-switched text, (2) aligns decoder and encoder via cross-attention using\nlimited speech-text data, and (3) fully fine-tunes the entire model.\nExperiments with Whisper on Malay-English code-switching demonstrate a 9.02%\nrelative WER reduction, while improving monolingual performance in Singlish,\nMalay, and other English variants.", "AI": {"tldr": "This paper presents AsyncSwitch, an asynchronous adaptation framework for code-switched ASR systems that enhances performance by utilizing large-scale text data before fine-tuning on corresponding speech-text pairs.", "motivation": "The development of code-switched ASR systems is hindered by language ambiguity and the scarcity of multilingual data, making the process costly and complex.", "method": "AsyncSwitch employs a three-stage process: (1) training decoder layers on code-switched text, (2) aligning decoder and encoder through cross-attention with limited speech-text data, and (3) fully fine-tuning the model.", "result": "Experiments with Whisper show a 9.02% reduction in word error rate (WER) for Malay-English code-switching while also enhancing performance on monolingual tasks in various English dialects.", "conclusion": "The proposed framework effectively improves code-switched ASR performance and maintains high accuracy for monolingual tasks, showing promise for future applications in speech recognition.", "key_contributions": ["Introduction of AsyncSwitch as a novel ASR adaptation framework", "Three-stage training process enabling effective code-switching", "Demonstrated significant WER reduction with Whisper on real-world code-switched data"], "limitations": "", "keywords": ["ASR", "Code-switching", "Machine Learning", "Speech Recognition", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2506.14371", "pdf": "https://arxiv.org/pdf/2506.14371.pdf", "abs": "https://arxiv.org/abs/2506.14371", "title": "ELLIS Alicante at CQs-Gen 2025: Winning the critical thinking questions shared task: LLM-based question generation and selection", "authors": ["Lucile Favero", "Daniel Frases", "Juan Antonio Pérez-Ortiz", "Tanja Käser", "Nuria Oliver"], "categories": ["cs.CL", "cs.HC"], "comment": "Proceedings of the 12th Workshop on Argument Mining", "summary": "The widespread adoption of chat interfaces based on Large Language Models\n(LLMs) raises concerns about promoting superficial learning and undermining the\ndevelopment of critical thinking skills. Instead of relying on LLMs purely for\nretrieving factual information, this work explores their potential to foster\ndeeper reasoning by generating critical questions that challenge unsupported or\nvague claims in debate interventions. This study is part of a shared task of\nthe 12th Workshop on Argument Mining, co-located with ACL 2025, focused on\nautomatic critical question generation. We propose a two-step framework\ninvolving two small-scale open source language models: a Questioner that\ngenerates multiple candidate questions and a Judge that selects the most\nrelevant ones. Our system ranked first in the shared task competition,\ndemonstrating the potential of the proposed LLM-based approach to encourage\ncritical engagement with argumentative texts.", "AI": {"tldr": "The paper investigates using LLMs for generating critical questions to enhance reasoning skills in debates, in response to concerns over superficial learning.", "motivation": "Address concerns about LLMs promoting superficial learning and not developing critical thinking skills.", "method": "A two-step framework using two language models: a Questioner to generate candidate questions and a Judge to select relevant ones.", "result": "The proposed system ranked first in a shared task competition, indicating its effectiveness in promoting critical engagement with argumentative texts.", "conclusion": "The use of LLMs can foster deeper reasoning and critical analysis when applied in debate interventions.", "key_contributions": ["Framework for critical question generation using LLMs", "Successful implementation demonstrated by ranking first in the shared task", "Potential to foster critical thinking in debate contexts"], "limitations": "", "keywords": ["Large Language Models", "critical question generation", "argument mining", "debates", "reasoning"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2506.14199", "pdf": "https://arxiv.org/pdf/2506.14199.pdf", "abs": "https://arxiv.org/abs/2506.14199", "title": "MAS-LitEval : Multi-Agent System for Literary Translation Quality Assessment", "authors": ["Junghwan Kim", "Kieun Park", "Sohee Park", "Hyunggug Kim", "Bongwon Suh"], "categories": ["cs.CL"], "comment": "4 Pages, 2 tables, EMNLP submitted", "summary": "Literary translation requires preserving cultural nuances and stylistic\nelements, which traditional metrics like BLEU and METEOR fail to assess due to\ntheir focus on lexical overlap. This oversight neglects the narrative\nconsistency and stylistic fidelity that are crucial for literary works. To\naddress this, we propose MAS-LitEval, a multi-agent system using Large Language\nModels (LLMs) to evaluate translations based on terminology, narrative, and\nstyle. We tested MAS-LitEval on translations of The Little Prince and A\nConnecticut Yankee in King Arthur's Court, generated by various LLMs, and\ncompared it to traditional metrics. \\textbf{MAS-LitEval} outperformed these\nmetrics, with top models scoring up to 0.890 in capturing literary nuances.\nThis work introduces a scalable, nuanced framework for Translation Quality\nAssessment (TQA), offering a practical tool for translators and researchers.", "AI": {"tldr": "The paper introduces MAS-LitEval, a multi-agent system using LLMs to evaluate literary translations based on narrative and style, outperforming traditional metrics like BLEU and METEOR.", "motivation": "Traditional translation metrics do not account for the cultural and stylistic nuances vital in literary translations.", "method": "MAS-LitEval utilizes large language models to analyze translations on the basis of terminology, narrative consistency, and stylistic fidelity.", "result": "MAS-LitEval outperformed traditional metrics, achieving scores up to 0.890 in capturing literary nuances during tests on various translations of classic literature.", "conclusion": "The study presents a scalable framework for Translation Quality Assessment that serves both researchers and translators seeking to maintain literary integrity.", "key_contributions": ["Introduction of MAS-LitEval as a novel evaluation system for literary translations", "Demonstrated improved performance over traditional metrics", "Established a scalable framework for Translation Quality Assessment (TQA)"], "limitations": "The study is limited to specific literary texts and may require further validation on diverse datasets.", "keywords": ["Literary Translation", "Quality Assessment", "Large Language Models", "Translation Metrics", "Cultural Nuances"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2409.19139", "pdf": "https://arxiv.org/pdf/2409.19139.pdf", "abs": "https://arxiv.org/abs/2409.19139", "title": "Gaze-informed Signatures of Trust and Collaboration in Human-Autonomy Teams", "authors": ["Anthony J. Ries", "Stéphane Aroca-Ouellette", "Alessandro Roncone", "Ewart J. de Visser"], "categories": ["cs.HC", "J.4"], "comment": null, "summary": "In the evolving landscape of human-autonomy teaming (HAT), fostering\neffective collaboration and trust between human and autonomous agents is\nincreasingly important. To explore this, we used the game Overcooked AI to\ncreate dynamic teaming scenarios featuring varying agent behaviors (clumsy,\nrigid, adaptive) and environmental complexities (low, medium, high). Our\nobjectives were to assess the performance of adaptive AI agents designed with\nhierarchical reinforcement learning for better teamwork and measure eye\ntracking signals related to changes in trust and collaboration. The results\nindicate that the adaptive agent was more effective in managing teaming and\ncreating an equitable task distribution across environments compared to the\nother agents. Working with the adaptive agent resulted in better coordination,\nreduced collisions, more balanced task contributions, and higher trust ratings.\nReduced gaze allocation, across all agents, was associated with higher trust\nlevels, while blink count, scan path length, agent revisits and trust were\npredictive of the humans contribution to the team. Notably, fixation revisits\non the agent increased with environmental complexity and decreased with agent\nversatility, offering a unique metric for measuring teammate performance\nmonitoring. These findings underscore the importance of designing autonomous\nteammates that not only excel in task performance but also enhance teamwork by\nbeing more predictable and reducing the cognitive load on human team members.\nAdditionally, this study highlights the potential of eye-tracking as an\nunobtrusive measure for evaluating and improving human-autonomy teams,\nsuggesting eye gaze could be used by agents to dynamically adapt their\nbehaviors.", "AI": {"tldr": "This paper investigates the collaboration between humans and adaptive AI agents in a gaming environment, focusing on trust and teamwork dynamics.", "motivation": "To enhance collaboration and trust between humans and autonomous agents in dynamic teaming scenarios.", "method": "Utilized the game Overcooked AI to test different AI agent behaviors and environmental complexities, assessing performance and using eye tracking to measure trust and collaboration.", "result": "Adaptive AI agents performed better in task distribution and coordination, showing higher trust ratings and revealing eye-tracking metrics related to teamwork effectiveness.", "conclusion": "Effective autonomous agents should not only excel in tasks but also enhance teamwork by being predictable and reducing cognitive load. Eye-tracking is a viable method for assessing team dynamics.", "key_contributions": ["Development of adaptive AI agents using hierarchical reinforcement learning", "Identification of eye-tracking metrics that correlate with trust and contribution", "Demonstration of improved teamwork dynamics in high-complexity environments"], "limitations": "", "keywords": ["human-autonomy teaming", "adaptive AI", "eye tracking", "trust", "collaboration"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.14200", "pdf": "https://arxiv.org/pdf/2506.14200.pdf", "abs": "https://arxiv.org/abs/2506.14200", "title": "ELI-Why: Evaluating the Pedagogical Utility of Language Model Explanations", "authors": ["Brihi Joshi", "Keyu He", "Sahana Ramnath", "Sadra Sabouri", "Kaitlyn Zhou", "Souti Chattopadhyay", "Swabha Swayamdipta", "Xiang Ren"], "categories": ["cs.CL", "cs.HC"], "comment": "Findings of ACL 2025", "summary": "Language models today are widely used in education, yet their ability to\ntailor responses for learners with varied informational needs and knowledge\nbackgrounds remains under-explored. To this end, we introduce ELI-Why, a\nbenchmark of 13.4K \"Why\" questions to evaluate the pedagogical capabilities of\nlanguage models. We then conduct two extensive human studies to assess the\nutility of language model-generated explanatory answers (explanations) on our\nbenchmark, tailored to three distinct educational grades: elementary,\nhigh-school and graduate school. In our first study, human raters assume the\nrole of an \"educator\" to assess model explanations' fit to different\neducational grades. We find that GPT-4-generated explanations match their\nintended educational background only 50% of the time, compared to 79% for lay\nhuman-curated explanations. In our second study, human raters assume the role\nof a learner to assess if an explanation fits their own informational needs.\nAcross all educational backgrounds, users deemed GPT-4-generated explanations\n20% less suited on average to their informational needs, when compared to\nexplanations curated by lay people. Additionally, automated evaluation metrics\nreveal that explanations generated across different language model families for\ndifferent informational needs remain indistinguishable in their grade-level,\nlimiting their pedagogical effectiveness.", "AI": {"tldr": "Introducing ELI-Why, a benchmark for evaluating language models' responses to educational questions, revealing significant gaps in the suitability of model-generated explanations compared to human-curated ones.", "motivation": "To evaluate the pedagogical capabilities of language models in tailoring responses for learners with diverse knowledge backgrounds in education.", "method": "The study introduces ELI-Why, a benchmark of 13.4K 'Why' questions, followed by two human studies assessing model-generated explanations against human-curated ones across three educational grades.", "result": "GPT-4 explanations matched intended educational backgrounds only 50% of the time compared to 79% for human-curated explanations, and were deemed 20% less suited to learners' informational needs.", "conclusion": "The effectiveness of language models in providing pedagogically appropriate explanations is limited, highlighting a need for improved tailoring to diverse informational needs.", "key_contributions": ["Introduction of the ELI-Why benchmark", "Human studies comparing GPT-4 explanations to human-curated explanations", "Assessment of language models' performance across educational grades"], "limitations": "Limitations related to the automated evaluation metrics and the ability to distinguish explanation quality across different model families.", "keywords": ["language models", "education", "benchmark", "human studies", "pedagogy"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2410.00174", "pdf": "https://arxiv.org/pdf/2410.00174.pdf", "abs": "https://arxiv.org/abs/2410.00174", "title": "Why Interdisciplinary Teams Fail: A Systematic Analysis With Activity Theory in Clinical AI Collaboration", "authors": ["Bingsheng Yao", "Yao Du", "Yue Fu", "Xuhai Xu", "Yanjun Gao", "Hong Yu", "Dakuo Wang"], "categories": ["cs.HC"], "comment": null, "summary": "Advanced AI technologies are increasingly integrated into clinical domains to\nadvance patient care. The design and development of clinical AI technologies\nnecessitate seamless collaboration between clinical and technical experts. Yet,\nsuch interdisciplinary teams are often unsuccessful, with a lack of systematic\nanalysis of collaboration barriers and coping strategies. This work examines\ntwo clinical AI collaborations in the context of speech-language pathology via\nsemi-structured interviews with six clinical and seven technical experts. Using\nActivity Theory (AT) as our analytical lens, we systematically investigate\npersistent knowledge gaps in mismatched data coding themes and specialized\nlanguages, and also highlight how clinical data can act as boundary objects and\nhuman knowledge brokers to alleviate these challenges. Our work underscores the\nbenefits of leveraging analytical frameworks like AT to systematically examine\ninterdisciplinary teams' collaborative work and provide meaningful insights on\nbest practices in future collaboration.", "AI": {"tldr": "This paper investigates collaboration barriers in clinical AI projects, particularly in speech-language pathology, using Activity Theory to enhance interdisciplinary teamwork.", "motivation": "There is a growing need for effective collaboration between clinical and technical experts in developing AI technologies for patient care, yet many interdisciplinary teams struggle due to communication issues.", "method": "The study involved semi-structured interviews with six clinical and seven technical experts, analyzing collaboration through the lens of Activity Theory.", "result": "The analysis revealed significant knowledge gaps related to data coding and specialized languages, and identified clinical data as a potential boundary object to facilitate communication.", "conclusion": "Leveraging analytical frameworks like Activity Theory can improve understanding of collaboration practices in clinical AI teams and enhance future cooperation strategies.", "key_contributions": ["Identified barriers in interdisciplinary collaboration for clinical AI development.", "Proposed the use of clinical data as boundary objects to bridge communication gaps.", "Provided insights on best practices for enhancing collaboration based on Activity Theory."], "limitations": "", "keywords": ["AI in healthcare", "collaboration", "Activity Theory", "speech-language pathology", "interdisciplinary teams"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.14203", "pdf": "https://arxiv.org/pdf/2506.14203.pdf", "abs": "https://arxiv.org/abs/2506.14203", "title": "Intended Target Identification for Anomia Patients with Gradient-based Selective Augmentation", "authors": ["Jongho Kim", "Romain Storaï", "Seung-won Hwang"], "categories": ["cs.CL"], "comment": "EMNLP 2024 Findings (long)", "summary": "In this study, we investigate the potential of language models (LMs) in\naiding patients experiencing anomia, a difficulty identifying the names of\nitems. Identifying the intended target item from patient's circumlocution\ninvolves the two challenges of term failure and error: (1) The terms relevant\nto identifying the item remain unseen. (2) What makes the challenge unique is\ninherent perturbed terms by semantic paraphasia, which are not exactly related\nto the target item, hindering the identification process. To address each, we\npropose robustifying the model from semantically paraphasic errors and\nenhancing the model with unseen terms with gradient-based selective\naugmentation. Specifically, the gradient value controls augmented data quality\namid semantic errors, while the gradient variance guides the inclusion of\nunseen but relevant terms. Due to limited domain-specific datasets, we evaluate\nthe model on the Tip-of-the-Tongue dataset as an intermediary task and then\napply our findings to real patient data from AphasiaBank. Our results\ndemonstrate strong performance against baselines, aiding anomia patients by\naddressing the outlined challenges.", "AI": {"tldr": "This paper explores how language models can assist patients with anomia by addressing challenges related to term failures and semantic paraphasia.", "motivation": "The study aims to improve identification of items for patients with anomia, who struggle to name items due to linguistic impairments.", "method": "The authors propose enhancing the language model by mitigating semantically paraphasic errors and incorporating unseen terms through a gradient-based selective augmentation strategy.", "result": "The model was evaluated on the Tip-of-the-Tongue dataset and real patient data from AphasiaBank, showing strong performance compared to baselines.", "conclusion": "The findings indicate that the proposed methods significantly aid anomia patients, effectively addressing the challenges of term failure and semantic paraphasia.", "key_contributions": ["Proposed a method to robustify language models against paraphasic errors.", "Introduced gradient-based selective augmentation for unseen terms.", "Demonstrated the effectiveness of the model on real patient data."], "limitations": "The approach relies on limited domain-specific datasets for evaluation.", "keywords": ["language models", "anomia", "semantic paraphasia", "gradient-based augmentation", "HCI"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2501.04227", "pdf": "https://arxiv.org/pdf/2501.04227.pdf", "abs": "https://arxiv.org/abs/2501.04227", "title": "Agent Laboratory: Using LLM Agents as Research Assistants", "authors": ["Samuel Schmidgall", "Yusheng Su", "Ze Wang", "Ximeng Sun", "Jialian Wu", "Xiaodong Yu", "Jiang Liu", "Michael Moor", "Zicheng Liu", "Emad Barsoum"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Historically, scientific discovery has been a lengthy and costly process,\ndemanding substantial time and resources from initial conception to final\nresults. To accelerate scientific discovery, reduce research costs, and improve\nresearch quality, we introduce Agent Laboratory, an autonomous LLM-based\nframework capable of completing the entire research process. This framework\naccepts a human-provided research idea and progresses through three\nstages--literature review, experimentation, and report writing to produce\ncomprehensive research outputs, including a code repository and a research\nreport, while enabling users to provide feedback and guidance at each stage. We\ndeploy Agent Laboratory with various state-of-the-art LLMs and invite multiple\nresearchers to assess its quality by participating in a survey, providing human\nfeedback to guide the research process, and then evaluate the final paper. We\nfound that: (1) Agent Laboratory driven by o1-preview generates the best\nresearch outcomes; (2) The generated machine learning code is able to achieve\nstate-of-the-art performance compared to existing methods; (3) Human\ninvolvement, providing feedback at each stage, significantly improves the\noverall quality of research; (4) Agent Laboratory significantly reduces\nresearch expenses, achieving an 84% decrease compared to previous autonomous\nresearch methods. We hope Agent Laboratory enables researchers to allocate more\neffort toward creative ideation rather than low-level coding and writing,\nultimately accelerating scientific discovery.", "AI": {"tldr": "Agent Laboratory is an LLM-based framework that automates the research process from literature review to report writing, significantly reducing research costs and improving quality through human feedback.", "motivation": "To accelerate scientific discovery, reduce research costs, and enhance research quality by automating the entire research process.", "method": "Agent Laboratory employs a three-stage process: literature review, experimentation, and report writing, while integrating human feedback at each stage.", "result": "Agent Laboratory, particularly when driven by o1-preview, produces superior research outcomes, with machine learning code achieving state-of-the-art performance and costs reduced by 84% compared to existing methods.", "conclusion": "The use of Agent Laboratory allows researchers to focus on creative ideation rather than low-level tasks, ultimately speeding up the scientific discovery process.", "key_contributions": ["Introduction of an autonomous LLM-based research framework", "Demonstration of significant cost reduction in research", "Showing the importance of human feedback in enhancing research quality"], "limitations": "", "keywords": ["autonomous LLM", "research automation", "human feedback"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2506.14205", "pdf": "https://arxiv.org/pdf/2506.14205.pdf", "abs": "https://arxiv.org/abs/2506.14205", "title": "AgentSynth: Scalable Task Generation for Generalist Computer-Use Agents", "authors": ["Jingxu Xie", "Dylan Xu", "Xuandong Zhao", "Dawn Song"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce AgentSynth, a scalable and cost-efficient pipeline for\nautomatically synthesizing high-quality tasks and trajectory datasets for\ngeneralist computer-use agents. Leveraging information asymmetry, AgentSynth\nconstructs subtasks that are simple during generation but significantly more\nchallenging when composed into long-horizon tasks, enabling the creation of\nover 6,000 diverse and realistic tasks. Our pipeline begins with an LLM-based\ntask proposer guided by a persona, followed by an execution agent that\ncompletes the task and logs the trajectory. This process is repeated\niteratively to form a sequence of subtasks, which are then summarized by a\nseparate agent into a composite task of controllable difficulty. A key strength\nof AgentSynth is its ability to precisely modulate task complexity by varying\nthe number of subtasks. Empirical evaluations show that state-of-the-art LLM\nagents suffer a steep performance drop, from 18% success at difficulty level 1\nto just 4% at level 6, highlighting the benchmark's difficulty and\ndiscriminative power. Moreover, our pipeline achieves a low average cost of\n\\$0.60 per trajectory, orders of magnitude cheaper than human annotations. Our\ncode and data are publicly available at\nhttps://github.com/sunblaze-ucb/AgentSynth", "AI": {"tldr": "AgentSynth is a pipeline for synthesizing diverse and challenging task datasets for computer-use agents using LLMs.", "motivation": "The need for scalable and cost-efficient methods to generate high-quality tasks for generalist agents in computer use.", "method": "Utilizes a two-step process involving an LLM-based task proposer and an execution agent that creates and logs subtasks iteratively.", "result": "Created over 6,000 diverse tasks with empirical evaluations indicating a significant performance drop in agents at higher difficulty levels due to task complexity.", "conclusion": "AgentSynth effectively modulates task complexity and achieves low costs for data generation compared to traditional methods.", "key_contributions": ["Scalable pipeline for task and trajectory generation.", "Demonstrates significant task complexity modulation capabilities.", "Reduces costs of dataset creation compared to human annotations."], "limitations": "Limited to the specific use case of computer-use agents; generalizability to other domains not assessed.", "keywords": ["task synthesis", "LLM", "computer agents", "dataset generation", "task complexity"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2502.05017", "pdf": "https://arxiv.org/pdf/2502.05017.pdf", "abs": "https://arxiv.org/abs/2502.05017", "title": "Bridging Voting and Deliberation with Algorithms: Field Insights from vTaiwan and Kultur Komitee", "authors": ["Joshua C. Yang", "Fynn Bachmann"], "categories": ["cs.HC", "cs.AI", "econ.GN", "q-fin.EC", "91B14, 91B12, 91A12, 68T01, 68T20, 68U35", "H.5.3; I.2.0; I.2.11; J.1; G.2.0; G.2.2; K.4.1; K.4.3"], "comment": "In Proceedings of the 2025 ACM Conference on Fairness,\n  Accountability, and Transparency (FAccT '25), 2025", "summary": "Democratic processes increasingly aim to integrate large-scale voting with\nface-to-face deliberation, addressing the challenge of reconciling individual\npreferences with collective decision-making. This work introduces new methods\nthat use algorithms and computational tools to bridge online voting with\nface-to-face deliberation, tested in two real-world scenarios: Kultur Komitee\n2024 (KK24) and vTaiwan. These case studies highlight the practical\napplications and impacts of the proposed methods.\n  We present three key contributions: (1) Preference-based Clustering for\nDeliberation (PCD), which enables both in-depth and broad discussions in\ndeliberative settings by computing homogeneous and heterogeneous group\ncompositions with balanced and adjustable group sizes; (2) Human-in-the-loop\nMES, a practical method that enhances the Method of Equal Shares (MES)\nalgorithm with real-time digital feedback. This builds algorithmic trust by\ngiving participants full control over how much decision-making is delegated to\nthe voting aggregation algorithm as compared to deliberation; and (3) the\nReadTheRoom deliberation method, which uses opinion space mapping to identify\nagreement and divergence, along with spectrum-based preference visualisation to\ntrack opinion shifts during deliberation. This approach enhances transparency\nby clarifying collective sentiment and fosters collaboration by encouraging\nparticipants to engage constructively with differing perspectives. By\nintroducing these actionable frameworks, this research extends in-person\ndeliberation with scalable digital methods that address the complexities of\nmodern decision-making in participatory processes.", "AI": {"tldr": "This paper presents methods that integrate online voting with face-to-face deliberation to improve democratic decision-making.", "motivation": "The work addresses the challenge of reconciling individual preferences with collective decision-making in democratic processes.", "method": "The authors introduce algorithms for integrating online voting with deliberation, including Preference-based Clustering for Deliberation, Human-in-the-loop Method of Equal Shares, and ReadTheRoom deliberation method.", "result": "The methods were tested in real-world scenarios, demonstrating their effectiveness in enhancing transparency and collaboration in decision-making.", "conclusion": "These frameworks provide scalable digital solutions to improve in-person deliberation in participatory processes.", "key_contributions": ["Preference-based Clustering for Deliberation (PCD)", "Human-in-the-loop Method of Equal Shares (MES)", "ReadTheRoom deliberation method"], "limitations": "", "keywords": ["democratic processes", "voting", "deliberation", "algorithmic trust", "participatory decision-making"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.14206", "pdf": "https://arxiv.org/pdf/2506.14206.pdf", "abs": "https://arxiv.org/abs/2506.14206", "title": "CausalDiffTab: Mixed-Type Causal-Aware Diffusion for Tabular Data Generation", "authors": ["Jia-Chen Zhang", "Zheng Zhou", "Yu-Jie Xiong", "Chun-Ming Xia", "Fei Dai"], "categories": ["cs.CL"], "comment": null, "summary": "Training data has been proven to be one of the most critical components in\ntraining generative AI. However, obtaining high-quality data remains\nchallenging, with data privacy issues presenting a significant hurdle. To\naddress the need for high-quality data. Synthesize data has emerged as a\nmainstream solution, demonstrating impressive performance in areas such as\nimages, audio, and video. Generating mixed-type data, especially high-quality\ntabular data, still faces significant challenges. These primarily include its\ninherent heterogeneous data types, complex inter-variable relationships, and\nintricate column-wise distributions. In this paper, we introduce CausalDiffTab,\na diffusion model-based generative model specifically designed to handle mixed\ntabular data containing both numerical and categorical features, while being\nmore flexible in capturing complex interactions among variables. We further\npropose a hybrid adaptive causal regularization method based on the principle\nof Hierarchical Prior Fusion. This approach adaptively controls the weight of\ncausal regularization, enhancing the model's performance without compromising\nits generative capabilities. Comprehensive experiments conducted on seven\ndatasets demonstrate that CausalDiffTab outperforms baseline methods across all\nmetrics. Our code is publicly available at:\nhttps://github.com/Godz-z/CausalDiffTab.", "AI": {"tldr": "CausalDiffTab is a diffusion model-based generative model for high-quality mixed tabular data synthesis, addressing challenges of heterogeneous data types and complex inter-variable relationships.", "motivation": "The need for high-quality training data for generative AI, particularly in the context of mixed tabular data, where existing methods struggle due to complex data characteristics.", "method": "Introduction of CausalDiffTab, a diffusion model-based generative model designed to synthesize mixed-type tabular data, using a hybrid adaptive causal regularization method based on Hierarchical Prior Fusion.", "result": "CausalDiffTab demonstrates superior performance over baseline methods across all metrics when tested on seven datasets.", "conclusion": "The proposed method effectively improves the synthesis of high-quality mixed tabular data while maintaining generative capabilities.", "key_contributions": ["Development of CausalDiffTab for mixed tabular data synthesis", "Implementation of hybrid adaptive causal regularization", "Comprehensive performance evaluation against baseline methods"], "limitations": "", "keywords": ["generative AI", "mixed tabular data", "diffusion models", "causal regularization", "data synthesis"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2502.18145", "pdf": "https://arxiv.org/pdf/2502.18145.pdf", "abs": "https://arxiv.org/abs/2502.18145", "title": "Carbon and Silicon, Coexist or Compete? A Survey on Human-AI Interactions in Agent-based Modeling and Simulation", "authors": ["Ziyue Lin", "Siqi Shen", "Zichen Cheng", "Cheok Lam Lai", "Siming Chen"], "categories": ["cs.HC"], "comment": "36 pages, 3 figures", "summary": "Recent interest in human-AI interactions in agent-based modeling and\nsimulation (ABMS) has grown rapidly due to the widespread utilization of large\nlanguage models (LLMs). ABMS is an intelligent approach that simulates\nautonomous agents' behaviors within a defined environment to research emergent\nphenomena. Integrating LLMs into ABMS enables natural language interaction\nbetween humans and models. Meanwhile, it introduces new challenges that rely on\nhuman interaction to address. Human involvement can assist ABMS in adapting to\nflexible and complex research demands. However, systematic reviews of\ninteractions that examine how humans and AI interact in ABMS are lacking. In\nthis paper, we investigate existing works and propose a novel taxonomy to\ncategorize the interactions derived from them. Specifically, human users refer\nto researchers who utilize ABMS tools to conduct their studies in our survey.\nWe decompose interactions into five dimensions: the goals that users want to\nachieve (Why), the phases that users are involved (When), the components of the\nsystem (What), the roles of users (Who), and the means of interactions (How).\nOur analysis summarizes the findings that reveal existing interaction patterns.\nThey provide researchers who develop interactions with comprehensive guidance\non how humans and AI interact. We further discuss the unexplored interactions\nand suggest future research directions.", "AI": {"tldr": "This paper investigates human-AI interactions in agent-based modeling and simulation (ABMS) using large language models (LLMs), proposing a novel taxonomy to categorize these interactions.", "motivation": "The rapid integration of LLMs into ABMS requires understanding human interactions to better meet research demands and improve model adaptability.", "method": "A systematic review of existing works on human-AI interactions in ABMS, leading to the development of a novel taxonomy based on five interaction dimensions.", "result": "The paper identifies patterns in human-AI interactions within ABMS and provides comprehensive guidance for researchers developing such interactions.", "conclusion": "The proposed taxonomy lays a foundation for further research into unexplored interaction types between humans and AI in ABMS.", "key_contributions": ["Novel taxonomy categorizing human-AI interactions in ABMS", "Systematic review of existing research on interactions", "Summary of interaction patterns that aid future research"], "limitations": "Limited to existing literature; does not cover all potential interactions.", "keywords": ["human-AI interaction", "agent-based modeling", "taxonomy", "large language models", "emergent phenomena"], "importance_score": 9, "read_time_minutes": 36}}
{"id": "2506.14211", "pdf": "https://arxiv.org/pdf/2506.14211.pdf", "abs": "https://arxiv.org/abs/2506.14211", "title": "Explainable Detection of Implicit Influential Patterns in Conversations via Data Augmentation", "authors": ["Sina Abdidizaji", "Md Kowsher", "Niloofar Yousefi", "Ivan Garibay"], "categories": ["cs.CL"], "comment": "Accepted at the HCI International conference 2025", "summary": "In the era of digitalization, as individuals increasingly rely on digital\nplatforms for communication and news consumption, various actors employ\nlinguistic strategies to influence public perception. While models have become\nproficient at detecting explicit patterns, which typically appear in texts as\nsingle remarks referred to as utterances, such as social media posts, malicious\nactors have shifted toward utilizing implicit influential verbal patterns\nembedded within conversations. These verbal patterns aim to mentally penetrate\nthe victim's mind in order to influence them, enabling the actor to obtain the\ndesired information through implicit means. This paper presents an improved\napproach for detecting such implicit influential patterns. Furthermore, the\nproposed model is capable of identifying the specific locations of these\ninfluential elements within a conversation. To achieve this, the existing\ndataset was augmented using the reasoning capabilities of state-of-the-art\nlanguage models. Our designed framework resulted in a 6% improvement in the\ndetection of implicit influential patterns in conversations. Moreover, this\napproach improved the multi-label classification tasks related to both the\ntechniques used for influence and the vulnerability of victims by 33% and 43%,\nrespectively.", "AI": {"tldr": "This paper enhances the detection of implicit influential patterns in conversations, demonstrating significant improvements in identifying such patterns and classifying related techniques and victim vulnerabilities.", "motivation": "To address the shift of malicious actors towards using implicit linguistic strategies in conversations for influencing individuals, as traditional detection models struggle with these subtle threats.", "method": "The study augments an existing dataset by leveraging the reasoning capabilities of advanced language models to detect implicit influential patterns and their locations in conversations.", "result": "The proposed model achieved a 6% improvement in detecting implicit influential patterns and enhanced multi-label classification tasks related to influence techniques and victim vulnerabilities by 33% and 43%, respectively.", "conclusion": "The findings indicate that state-of-the-art language models can effectively improve the identification and classification of implicit influence techniques, thus contributing to enhanced security measures in digital communication.", "key_contributions": ["Improved model for detecting implicit influential patterns in conversations", "Identification of specific locations of these patterns", "Significant enhancements in classification of influence techniques and victim vulnerabilities"], "limitations": "", "keywords": ["Human-Computer Interaction", "Implicit Influence", "Language Models", "Conversation Analysis", "Machine Learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.14213", "pdf": "https://arxiv.org/pdf/2506.14213.pdf", "abs": "https://arxiv.org/abs/2506.14213", "title": "Chaining Event Spans for Temporal Relation Grounding", "authors": ["Jongho Kim", "Dohyeon Lee", "Minsoo Kim", "Seung-won Hwang"], "categories": ["cs.CL"], "comment": "In Proceedings of the 18th Conference of the European Chapter of the\n  Association for Computational Linguistics (Volume 1: Long Papers), pages\n  1689-1700", "summary": "Accurately understanding temporal relations between events is a critical\nbuilding block of diverse tasks, such as temporal reading comprehension (TRC)\nand relation extraction (TRE). For example in TRC, we need to understand the\ntemporal semantic differences between the following two questions that are\nlexically near-identical: \"What finished right before the decision?\" or \"What\nfinished right after the decision?\". To discern the two questions, existing\nsolutions have relied on answer overlaps as a proxy label to contrast similar\nand dissimilar questions. However, we claim that answer overlap can lead to\nunreliable results, due to spurious overlaps of two dissimilar questions with\ncoincidentally identical answers. To address the issue, we propose a novel\napproach that elicits proper reasoning behaviors through a module for\npredicting time spans of events. We introduce the Timeline Reasoning Network\n(TRN) operating in a two-step inductive reasoning process: In the first step\nmodel initially answers each question with semantic and syntactic information.\nThe next step chains multiple questions on the same event to predict a\ntimeline, which is then used to ground the answers. Results on the TORQUE and\nTB-dense, TRC and TRE tasks respectively, demonstrate that TRN outperforms\nprevious methods by effectively resolving the spurious overlaps using the\npredicted timeline.", "AI": {"tldr": "The paper presents a novel Timeline Reasoning Network (TRN) designed to improve temporal understanding in tasks like temporal reading comprehension and relation extraction by predicting time spans of events to resolve ambiguity.", "motivation": "Accurate understanding of temporal relations is crucial for tasks such as temporal reading comprehension and relation extraction, which face challenges with answer overlaps leading to unreliable results.", "method": "The proposed TRN operates in a two-step inductive reasoning process where initial answers are generated based on semantic and syntactic information, followed by chaining multiple questions to predict a timeline that informs the answers.", "result": "The TRN demonstrates superior performance on the TORQUE and TB-dense datasets, effectively resolving issues with spurious overlaps through the predicted timeline.", "conclusion": "The proposed method outperforms previous state-of-the-art techniques in handling temporal relations by employing a structured reasoning approach.", "key_contributions": ["Introduction of the Timeline Reasoning Network (TRN) for temporal understanding.", "Two-step inductive reasoning process for improved answer reliability.", "Demonstrated effectiveness on TRC and TRE tasks via empirical results."], "limitations": "", "keywords": ["Temporal Relations", "Timeline Reasoning Network", "Temporal Reading Comprehension", "Relation Extraction", "Inductive Reasoning"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2501.10970", "pdf": "https://arxiv.org/pdf/2501.10970.pdf", "abs": "https://arxiv.org/abs/2501.10970", "title": "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs", "authors": ["Nitay Calderon", "Roi Reichart", "Rotem Dror"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "The \"LLM-as-an-annotator\" and \"LLM-as-a-judge\" paradigms employ Large\nLanguage Models (LLMs) as annotators, judges, and evaluators in tasks\ntraditionally performed by humans. LLM annotations are widely used, not only in\nNLP research but also in fields like medicine, psychology, and social science.\nDespite their role in shaping study results and insights, there is no standard\nor rigorous procedure to determine whether LLMs can replace human annotators.\nIn this paper, we propose a novel statistical procedure, the Alternative\nAnnotator Test (alt-test), that requires only a modest subset of annotated\nexamples to justify using LLM annotations. Additionally, we introduce a\nversatile and interpretable measure for comparing LLM annotators and judges. To\ndemonstrate our procedure, we curated a diverse collection of ten datasets,\nconsisting of language and vision-language tasks, and conducted experiments\nwith six LLMs and four prompting techniques. Our results show that LLMs can\nsometimes replace humans with closed-source LLMs (such as GPT-4o),\noutperforming the open-source LLMs we examine, and that prompting techniques\nyield judges of varying quality. We hope this study encourages more rigorous\nand reliable practices.", "AI": {"tldr": "The paper proposes the Alternative Annotator Test (alt-test), a statistical procedure for assessing the validity of LLM annotations in place of human annotators.", "motivation": "There is a lack of rigorous procedures to evaluate if LLMs can effectively replace human annotations in various fields.", "method": "The authors propose the Alternative Annotator Test (alt-test) and develop an interpretable measure to compare LLM annotators and judges. They curated ten diverse datasets and conducted experiments with six LLMs and four prompting techniques.", "result": "The experiments indicate that closed-source LLMs like GPT-4o can outperform humans and open-source LLMs in certain tasks, revealing that prompting techniques yield judges of varying quality.", "conclusion": "The study aims to promote more rigorous and reliable practices when utilizing LLM annotations in research.", "key_contributions": ["Introduction of the Alternative Annotator Test (alt-test) for LLM evaluation", "Development of a measure for comparing LLM annotators", "Curated diverse datasets for validation of LLM annotation effectiveness"], "limitations": "The study focuses on a limited selection of datasets and LLMs, which may not be generalizable to all applications.", "keywords": ["Large Language Models", "annotation", "human annotators", "statistical procedure", "evaluation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.14234", "pdf": "https://arxiv.org/pdf/2506.14234.pdf", "abs": "https://arxiv.org/abs/2506.14234", "title": "Xolver: Multi-Agent Reasoning with Holistic Experience Learning Just Like an Olympiad Team", "authors": ["Md Tanzib Hosain", "Salman Rahman", "Md Kishor Morol", "Md Rizwan Parvez"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite impressive progress on complex reasoning, current large language\nmodels (LLMs) typically operate in isolation - treating each problem as an\nindependent attempt, without accumulating or integrating experiential\nknowledge. In contrast, expert problem solvers - such as Olympiad or\nprogramming contest teams - leverage a rich tapestry of experiences: absorbing\nmentorship from coaches, developing intuition from past problems, leveraging\nknowledge of tool usage and library functionality, adapting strategies based on\nthe expertise and experiences of peers, continuously refining their reasoning\nthrough trial and error, and learning from other related problems even during\ncompetition. We introduce Xolver, a training-free multi-agent reasoning\nframework that equips a black-box LLM with a persistent, evolving memory of\nholistic experience. Xolver integrates diverse experience modalities, including\nexternal and self-retrieval, tool use, collaborative interactions, agent-driven\nevaluation, and iterative refinement. By learning from relevant strategies,\ncode fragments, and abstract reasoning patterns at inference time, Xolver\navoids generating solutions from scratch - marking a transition from isolated\ninference toward experience-aware language agents. Built on both open-weight\nand proprietary models, Xolver consistently outperforms specialized reasoning\nagents. Even with lightweight backbones (e.g., QWQ-32B), it often surpasses\nadvanced models including Qwen3-235B, Gemini 2.5 Pro, o3, and o4-mini-high.\nWith o3-mini-high, it achieves new best results on GSM8K (98.1%), AIME'24\n(94.4%), AIME'25 (93.7%), Math-500 (99.8%), and LiveCodeBench-V5 (91.6%) -\nhighlighting holistic experience learning as a key step toward generalist\nagents capable of expert-level reasoning. Code and data are available at\nhttps://kagnlp.github.io/xolver.github.io/.", "AI": {"tldr": "Xolver is a multi-agent reasoning framework for large language models (LLMs) that enhances problem-solving by integrating experiential knowledge through a persistent memory system, outperforming specialized agents.", "motivation": "To bridge the gap in LLMs' isolated problem-solving by incorporating rich experiences similar to expert problem solvers, leading to improved reasoning capabilities.", "method": "Xolver utilizes a training-free framework that combines external and self-retrieval mechanisms, tool usage, collaborative interactions, and iterative refinement to enhance reasoning using past experiences during inference.", "result": "Xolver outperforms specialized reasoning agents, achieving new best results on various benchmarks like GSM8K and Math-500, even with lightweight model architectures.", "conclusion": "Holistic experience learning is crucial for developing generalist language agents capable of expert-level reasoning.", "key_contributions": ["Introduction of a persistent, evolving memory for LLMs", "Integration of diverse experience modalities for enhanced reasoning", "Demonstration of superior performance on multiple reasoning benchmarks"], "limitations": "", "keywords": ["large language models", "multi-agent reasoning", "experiential knowledge"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.14235", "pdf": "https://arxiv.org/pdf/2506.14235.pdf", "abs": "https://arxiv.org/abs/2506.14235", "title": "A Multi-Expert Structural-Semantic Hybrid Framework for Unveiling Historical Patterns in Temporal Knowledge Graphs", "authors": ["Yimin Deng", "Yuxia Wu", "Yejing Wang", "Guoshuai Zhao", "Li Zhu", "Qidong Liu", "Derong Xu", "Zichuan Fu", "Xian Wu", "Yefeng Zheng", "Xiangyu Zhao", "Xueming Qian"], "categories": ["cs.CL"], "comment": "ACL25 findings", "summary": "Temporal knowledge graph reasoning aims to predict future events with\nknowledge of existing facts and plays a key role in various downstream tasks.\nPrevious methods focused on either graph structure learning or semantic\nreasoning, failing to integrate dual reasoning perspectives to handle different\nprediction scenarios. Moreover, they lack the capability to capture the\ninherent differences between historical and non-historical events, which limits\ntheir generalization across different temporal contexts. To this end, we\npropose a Multi-Expert Structural-Semantic Hybrid (MESH) framework that employs\nthree kinds of expert modules to integrate both structural and semantic\ninformation, guiding the reasoning process for different events. Extensive\nexperiments on three datasets demonstrate the effectiveness of our approach.", "AI": {"tldr": "The MESH framework integrates structural and semantic reasoning for temporal knowledge graph reasoning to predict future events effectively.", "motivation": "To address the limitations of previous methods in temporal knowledge graph reasoning, specifically their failure to integrate structural and semantic reasoning and their inability to differentiate between historical and non-historical events.", "method": "The proposed Multi-Expert Structural-Semantic Hybrid (MESH) framework utilizes three types of expert modules to combine structural and semantic information for various event prediction scenarios.", "result": "Extensive experiments on three datasets demonstrate that the MESH framework outperforms existing methods in predicting future events in temporal contexts.", "conclusion": "The MESH framework effectively enhances temporal knowledge graph reasoning by employing a hybrid approach, leading to better generalization across different prediction scenarios.", "key_contributions": ["Integration of structural and semantic reasoning for improved prediction.", "Utilization of expert modules tailored for different event types.", "Demonstrated effectiveness across multiple datasets."], "limitations": "", "keywords": ["temporal knowledge graph", "reasoning", "structural-semantics", "event prediction", "multi-expert framework"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2506.14248", "pdf": "https://arxiv.org/pdf/2506.14248.pdf", "abs": "https://arxiv.org/abs/2506.14248", "title": "Re-Initialization Token Learning for Tool-Augmented Large Language Models", "authors": ["Chenghao Li", "Liu Liu", "Baosheng Yu", "Jiayan Qiu", "Yibing Zhan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models have demonstrated exceptional performance, yet struggle\nwith complex tasks such as numerical reasoning, plan generation. Integrating\nexternal tools, such as calculators and databases, into large language models\n(LLMs) is crucial for enhancing problem-solving capabilities. Current methods\nassign a unique token to each tool, enabling LLMs to call tools through token\nprediction-similar to word generation. However, this approach fails to account\nfor the relationship between tool and word tokens, limiting adaptability within\npre-trained LLMs. To address this issue, we propose a novel token learning\nmethod that aligns tool tokens with the existing word embedding space from the\nperspective of initialization, thereby enhancing model performance. We begin by\nconstructing prior token embeddings for each tool based on the tool's name or\ndescription, which are used to initialize and regularize the learnable tool\ntoken embeddings. This ensures the learned embeddings are well-aligned with the\nword token space, improving tool call accuracy. We evaluate the method on tasks\nsuch as numerical reasoning, knowledge-based question answering, and embodied\nplan generation using GSM8K-XL, FuncQA, KAMEL, and VirtualHome datasets. The\nresults demonstrate clear improvements over recent baselines, including CoT,\nREACT, ICL, and ToolkenGPT, indicating that our approach effectively augments\nLLMs with tools through relevant tokens across diverse domains.", "AI": {"tldr": "Proposes a novel token learning method to integrate external tools into large language models, improving their problem-solving capabilities.", "motivation": "Integrating external tools into LLMs is essential for enhancing their performance on complex tasks like numerical reasoning and planning.", "method": "Develops a token learning method that aligns tool tokens with the existing word embedding space by initializing tool token embeddings based on tool names or descriptions.", "result": "The proposed method shows improved performance on tasks such as numerical reasoning and plan generation, outperforming recent baselines like CoT and REACT.", "conclusion": "The approach effectively enhances the integration of tools into LLMs, leading to better performance on various complex tasks.", "key_contributions": ["Novel token learning method for aligning tool tokens with word embeddings", "Demonstration of performance improvement on complex tasks", "Evaluation across diverse datasets such as GSM8K-XL and FuncQA"], "limitations": "", "keywords": ["large language models", "token learning", "tool integration", "numerical reasoning", "machine learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.14285", "pdf": "https://arxiv.org/pdf/2506.14285.pdf", "abs": "https://arxiv.org/abs/2506.14285", "title": "From What to Respond to When to Respond: Timely Response Generation for Open-domain Dialogue Agents", "authors": ["Seongbo Jang", "Minjin Jeon", "Jaehoon Lee", "Seonghyeon Lee", "Dongha Lee", "Hwanjo Yu"], "categories": ["cs.CL"], "comment": "Work in progress", "summary": "While research on dialogue response generation has primarily focused on\ngenerating coherent responses conditioning on textual context, the critical\nquestion of when to respond grounded on the temporal context remains\nunderexplored. To bridge this gap, we propose a novel task called timely\ndialogue response generation and introduce the TimelyChat benchmark, which\nevaluates the capabilities of language models to predict appropriate time\nintervals and generate time-conditioned responses. Additionally, we construct a\nlarge-scale training dataset by leveraging unlabeled event knowledge from a\ntemporal commonsense knowledge graph and employing a large language model (LLM)\nto synthesize 55K event-driven dialogues. We then train Timer, a dialogue agent\ndesigned to proactively predict time intervals and generate timely responses\nthat align with those intervals. Experimental results show that Timer\noutperforms prompting-based LLMs and other fine-tuned baselines in both\nturn-level and dialogue-level evaluations. We publicly release our data, model,\nand code.", "AI": {"tldr": "This paper introduces the task of timely dialogue response generation and presents the TimelyChat benchmark for evaluating language models based on temporal context.", "motivation": "The study addresses the lack of exploration in response generation based on temporal context in dialogue systems.", "method": "The authors propose a novel task called timely dialogue response generation, create the TimelyChat benchmark, and construct a large-scale dataset using a temporal commonsense knowledge graph alongside a large language model.", "result": "Experimental results indicate that the Timer agent exceeds the performance of prompt-based LLMs and other fine-tuned models in generating time-sensitive dialogue responses.", "conclusion": "The proposed Timer agent shows improved capabilities in timely response generation, and the authors have made their data and model publicly available.", "key_contributions": ["Introduction of the timely dialogue response generation task", "Creation of the TimelyChat benchmark", "Development of the Timer dialogue agent trained on a large-scale dataset"], "limitations": "The work is still in progress and may have limitations in terms of exhaustive evaluation across diverse dialogue scenarios.", "keywords": ["dialogue generation", "timely responses", "language models", "temporal context", "commonsense knowledge"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.14302", "pdf": "https://arxiv.org/pdf/2506.14302.pdf", "abs": "https://arxiv.org/abs/2506.14302", "title": "Expectation Confirmation Preference Optimization for Multi-Turn Conversational Recommendation Agent", "authors": ["Xueyang Feng", "Jingsen Zhang", "Jiakai Tang", "Wei Li", "Guohao Cai", "Xu Chen", "Quanyu Dai", "Yue Zhu", "Zhenhua Dong"], "categories": ["cs.CL"], "comment": "Accepted to Findings of ACL 2025", "summary": "Recent advancements in Large Language Models (LLMs) have significantly\npropelled the development of Conversational Recommendation Agents (CRAs).\nHowever, these agents often generate short-sighted responses that fail to\nsustain user guidance and meet expectations. Although preference optimization\nhas proven effective in aligning LLMs with user expectations, it remains costly\nand performs poorly in multi-turn dialogue. To address this challenge, we\nintroduce a novel multi-turn preference optimization (MTPO) paradigm ECPO,\nwhich leverages Expectation Confirmation Theory to explicitly model the\nevolution of user satisfaction throughout multi-turn dialogues, uncovering the\nunderlying causes of dissatisfaction. These causes can be utilized to support\ntargeted optimization of unsatisfactory responses, thereby achieving turn-level\npreference optimization. ECPO ingeniously eliminates the significant sampling\noverhead of existing MTPO methods while ensuring the optimization process\ndrives meaningful improvements. To support ECPO, we introduce an LLM-based user\nsimulator, AILO, to simulate user feedback and perform expectation confirmation\nduring conversational recommendations. Experimental results show that ECPO\nsignificantly enhances CRA's interaction capabilities, delivering notable\nimprovements in both efficiency and effectiveness over existing MTPO methods.", "AI": {"tldr": "This paper introduces a new paradigm called ECPO for multi-turn preference optimization in Conversational Recommendation Agents (CRAs) that leverages user satisfaction modeling to improve dialogue interactions.", "motivation": "To address issues with short-sighted responses from CRAs which fail to meet user expectations in multi-turn dialogues.", "method": "The proposed ECPO paradigm utilizes Expectation Confirmation Theory to model user satisfaction evolution and incorporates a user simulator called AILO for simulating feedback during recommendations.", "result": "Experimental results demonstrate that ECPO significantly enhances CRA capabilities, showing improvements in both efficiency and effectiveness compared to existing methods.", "conclusion": "ECPO effectively optimizes multi-turn dialogues in CRAs while eliminating significant overhead found in previous methods, leading to better user experiences.", "key_contributions": ["Introduction of the ECPO paradigm for multi-turn preference optimization.", "Development of a user simulator AILO for user feedback simulation.", "Significant improvement in CRA interaction capabilities compared to existing MTPO methods."], "limitations": "", "keywords": ["Conversational Recommendation Agents", "Multi-turn Preference Optimization", "Expectation Confirmation Theory"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.14335", "pdf": "https://arxiv.org/pdf/2506.14335.pdf", "abs": "https://arxiv.org/abs/2506.14335", "title": "Evaluation Should Not Ignore Variation: On the Impact of Reference Set Choice on Summarization Metrics", "authors": ["Silvia Casola", "Yang Janet Liu", "Siyao Peng", "Oliver Kraus", "Albert Gatt", "Barbara Plank"], "categories": ["cs.CL"], "comment": "17 pages, 13 figures", "summary": "Human language production exhibits remarkable richness and variation,\nreflecting diverse communication styles and intents. However, this variation is\noften overlooked in summarization evaluation. While having multiple reference\nsummaries is known to improve correlation with human judgments, the impact of\nusing different reference sets on reference-based metrics has not been\nsystematically investigated. This work examines the sensitivity of widely used\nreference-based metrics in relation to the choice of reference sets, analyzing\nthree diverse multi-reference summarization datasets: SummEval, GUMSum, and\nDUC2004. We demonstrate that many popular metrics exhibit significant\ninstability. This instability is particularly concerning for n-gram-based\nmetrics like ROUGE, where model rankings vary depending on the reference sets,\nundermining the reliability of model comparisons. We also collect human\njudgments on LLM outputs for genre-diverse data and examine their correlation\nwith metrics to supplement existing findings beyond newswire summaries, finding\nweak-to-no correlation. Taken together, we recommend incorporating reference\nset variation into summarization evaluation to enhance consistency alongside\ncorrelation with human judgments, especially when evaluating LLMs.", "AI": {"tldr": "This paper investigates the impact of reference set variation on summarization evaluation metrics, revealing significant instability in popular metrics like ROUGE.", "motivation": "To enhance the reliability of summarization evaluation by incorporating the diversity of human language production and addressing the inconsistencies in reference-based metrics.", "method": "Analyzed three multi-reference summarization datasets (SummEval, GUMSum, DUC2004) to assess the sensitivity of reference-based metrics to different reference sets and collected human judgments on LLM outputs for diverse data.", "result": "Many reference-based metrics, particularly n-gram-based ones like ROUGE, exhibit significant instability based on reference set choice, affecting model rankings and comparisons.", "conclusion": "The study recommends incorporating reference set variation in summarization evaluation to improve consistency and correlation with human judgments, especially for LLM evaluations.", "key_contributions": ["Systematic investigation of reference set sensitivity on summarization metrics.", "Identification of significant instability in popular metrics like ROUGE.", "Recommendations for improving evaluation practices in summarization, particularly with LLMs."], "limitations": "Focus on three specific datasets; results may not generalize to all summarization tasks or metrics.", "keywords": ["summarization", "evaluation metrics", "reference sets", "human judgments", "LLMs"], "importance_score": 8, "read_time_minutes": 17}}
{"id": "2506.14345", "pdf": "https://arxiv.org/pdf/2506.14345.pdf", "abs": "https://arxiv.org/abs/2506.14345", "title": "A Vision for Geo-Temporal Deep Research Systems: Towards Comprehensive, Transparent, and Reproducible Geo-Temporal Information Synthesis", "authors": ["Bruno Martins", "Piotr Szymański", "Piotr Gramacki"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "The emergence of Large Language Models (LLMs) has transformed information\naccess, with current LLMs also powering deep research systems that can generate\ncomprehensive report-style answers, through planned iterative search,\nretrieval, and reasoning. Still, current deep research systems lack the\ngeo-temporal capabilities that are essential for answering context-rich\nquestions involving geographic and/or temporal constraints, frequently\noccurring in domains like public health, environmental science, or\nsocio-economic analysis. This paper reports our vision towards next generation\nsystems, identifying important technical, infrastructural, and evaluative\nchallenges in integrating geo-temporal reasoning into deep research pipelines.\nWe argue for augmenting retrieval and synthesis processes with the ability to\nhandle geo-temporal constraints, supported by open and reproducible\ninfrastructures and rigorous evaluation protocols. Our vision outlines a path\ntowards more advanced and geo-temporally aware deep research systems, of\npotential impact to the future of AI-driven information access.", "AI": {"tldr": "This paper proposes advancements in deep research systems by integrating geo-temporal reasoning capabilities to enhance information access in context-rich domains such as public health and environmental science.", "motivation": "To address the limitations of current deep research systems that lack geo-temporal reasoning, which is essential for complex queries in various domains.", "method": "The authors identify technical, infrastructural, and evaluative challenges and propose augmenting search, retrieval, and synthesis processes with geo-temporal constraints.", "result": "The proposed integration aims to create more advanced deep research systems that are aware of geographic and temporal contexts, enhancing their responsiveness to complex information needs.", "conclusion": "By successfully integrating these capabilities, the next generation of AI-driven information systems could significantly impact fields that rely on contextual information for decision-making.", "key_contributions": ["Identifies challenges in geo-temporal reasoning in deep research systems", "Proposes a framework for integrating geo-temporal capabilities", "Calls for open infrastructures and rigorous evaluation protocols"], "limitations": "", "keywords": ["Large Language Models", "Geo-temporal reasoning", "Deep Research Systems"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.14370", "pdf": "https://arxiv.org/pdf/2506.14370.pdf", "abs": "https://arxiv.org/abs/2506.14370", "title": "Digital Gatekeepers: Google's Role in Curating Hashtags and Subreddits", "authors": ["Amrit Poudel", "Yifan Ding", "Jurgen Pfeffer", "Tim Weninger"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main", "summary": "Search engines play a crucial role as digital gatekeepers, shaping the\nvisibility of Web and social media content through algorithmic curation. This\nstudy investigates how search engines like Google selectively promotes or\nsuppresses certain hashtags and subreddits, impacting the information users\nencounter. By comparing search engine results with nonsampled data from Reddit\nand Twitter/X, we reveal systematic biases in content visibility. Google's\nalgorithms tend to suppress subreddits and hashtags related to sexually\nexplicit material, conspiracy theories, advertisements, and cryptocurrencies,\nwhile promoting content associated with higher engagement. These findings\nsuggest that Google's gatekeeping practices influence public discourse by\ncurating the social media narratives available to users.", "AI": {"tldr": "This study analyzes how search engines, particularly Google, influence content visibility on Web and social media through algorithmic curation, revealing biases in the promotion and suppression of certain hashtags and subreddits.", "motivation": "To investigate the biases in how search engines curate content, which significantly influences public discourse and user engagement.", "method": "The study compares search engine results with nonsampled data from Reddit and Twitter/X to identify systematic biases in visibility for specific hashtags and subreddits.", "result": "It was found that Google's algorithms suppress subreddits and hashtags related to sexually explicit content, conspiracy theories, advertisements, and cryptocurrencies, while promoting content that garners higher engagement.", "conclusion": "The findings highlight the significant role of search engine algorithms in shaping public narratives and the visibility of social media content.", "key_contributions": ["Identification of systematic biases in search engine curation of social content", "Analysis of specific types of content that are suppressed or promoted", "Insights into the impact of algorithmic decisions on public discourse"], "limitations": "", "keywords": ["search engines", "algorithmic curation", "content visibility", "social media", "bias"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2506.14371", "pdf": "https://arxiv.org/pdf/2506.14371.pdf", "abs": "https://arxiv.org/abs/2506.14371", "title": "ELLIS Alicante at CQs-Gen 2025: Winning the critical thinking questions shared task: LLM-based question generation and selection", "authors": ["Lucile Favero", "Daniel Frases", "Juan Antonio Pérez-Ortiz", "Tanja Käser", "Nuria Oliver"], "categories": ["cs.CL", "cs.HC"], "comment": "Proceedings of the 12th Workshop on Argument Mining", "summary": "The widespread adoption of chat interfaces based on Large Language Models\n(LLMs) raises concerns about promoting superficial learning and undermining the\ndevelopment of critical thinking skills. Instead of relying on LLMs purely for\nretrieving factual information, this work explores their potential to foster\ndeeper reasoning by generating critical questions that challenge unsupported or\nvague claims in debate interventions. This study is part of a shared task of\nthe 12th Workshop on Argument Mining, co-located with ACL 2025, focused on\nautomatic critical question generation. We propose a two-step framework\ninvolving two small-scale open source language models: a Questioner that\ngenerates multiple candidate questions and a Judge that selects the most\nrelevant ones. Our system ranked first in the shared task competition,\ndemonstrating the potential of the proposed LLM-based approach to encourage\ncritical engagement with argumentative texts.", "AI": {"tldr": "This study explores using LLMs to generate critical questions that enhance reasoning and critical thinking in debates, proposing a two-step system that ranked first in a competition.", "motivation": "The need to enhance critical thinking skills in learning environments using LLMs, as traditional uses may encourage superficial learning.", "method": "A two-step framework utilizing two small-scale open source language models: a Questioner for generating candidate questions, and a Judge for selecting the most relevant questions.", "result": "The proposed system ranked first in the shared task competition at the 12th Workshop on Argument Mining, showing effectiveness in promoting critical engagement.", "conclusion": "The study demonstrates the potential of LLMs to foster deeper reasoning through effective critical question generation.", "key_contributions": ["Development of a two-step framework for critical question generation using LLMs.", "Establishment of first-place ranking in a competitive shared task context.", "Insights into critical engagement with argumentation through LLM applications."], "limitations": "", "keywords": ["Large Language Models", "Critical Thinking", "Question Generation", "Argument Mining", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.14397", "pdf": "https://arxiv.org/pdf/2506.14397.pdf", "abs": "https://arxiv.org/abs/2506.14397", "title": "Thunder-NUBench: A Benchmark for LLMs' Sentence-Level Negation Understanding", "authors": ["Yeonkyoung So", "Gyuseong Lee", "Sungmok Jung", "Joonhak Lee", "JiA Kang", "Sangho Kim", "Jaejin Lee"], "categories": ["cs.CL"], "comment": null, "summary": "Negation is a fundamental linguistic phenomenon that poses persistent\nchallenges for Large Language Models (LLMs), particularly in tasks requiring\ndeep semantic understanding. Existing benchmarks often treat negation as a side\ncase within broader tasks like natural language inference, resulting in a lack\nof benchmarks that exclusively target negation understanding. In this work, we\nintroduce \\textbf{Thunder-NUBench}, a novel benchmark explicitly designed to\nassess sentence-level negation understanding in LLMs. Thunder-NUBench goes\nbeyond surface-level cue detection by contrasting standard negation with\nstructurally diverse alternatives such as local negation, contradiction, and\nparaphrase. The benchmark consists of manually curated sentence-negation pairs\nand a multiple-choice dataset that enables in-depth evaluation of models'\nnegation understanding.", "AI": {"tldr": "Thunder-NUBench is a new benchmark for evaluating sentence-level negation understanding in LLMs, addressing limitations in existing frameworks.", "motivation": "To address the lack of benchmarks focusing on negation understanding in LLMs, which is crucial for deep semantic comprehension.", "method": "Introduction of Thunder-NUBench, a multiple-choice dataset with curated sentence-negation pairs that assess various forms of negation beyond standard cues.", "result": "The benchmark allows for a more thorough evaluation of LLMs in understanding different types of negation, enhancing insights into their semantic capabilities.", "conclusion": "Thunder-NUBench offers a targeted approach to assessing negation in LLMs, contributing to the development of more robust language models.", "key_contributions": ["Creation of Thunder-NUBench for negation assessment", "Focus on structurally diverse negation forms", "In-depth evaluation methodology using curated datasets"], "limitations": "", "keywords": ["Negation", "Large Language Models", "Benchmark", "Semantic Understanding", "Natural Language Processing"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2506.14407", "pdf": "https://arxiv.org/pdf/2506.14407.pdf", "abs": "https://arxiv.org/abs/2506.14407", "title": "ImpliRet: Benchmarking the Implicit Fact Retrieval Challenge", "authors": ["Zeinab Sadat Taghavi", "Ali Modarressi", "Yunpu Ma", "Hinrich Schütze"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval systems are central to many NLP pipelines, but often rely on\nsurface-level cues such as keyword overlap and lexical semantic similarity. To\nevaluate retrieval beyond these shallow signals, recent benchmarks introduce\nreasoning-heavy queries; however, they primarily shift the burden to query-side\nprocessing techniques -- like prompting or multi-hop retrieval -- that can help\nresolve complexity. In contrast, we present ImpliRet, a benchmark that shifts\nthe reasoning challenge to document-side processing: The queries are simple,\nbut relevance depends on facts stated implicitly in documents through temporal\n(e.g., resolving \"two days ago\"), arithmetic, and world knowledge\nrelationships. We evaluate a range of sparse and dense retrievers, all of which\nstruggle in this setting: the best nDCG@10 is only 15.07%. We also test whether\nlong-context models can overcome this limitation. But even with a short context\nof only ten documents, including the positive document, GPT-4.1 scores only\n35.06%, showing that document-side reasoning remains a challenge. Our codes are\navailable at github.com/ZeinabTaghavi/IMPLIRET.Contribution.", "AI": {"tldr": "ImpliRet is a benchmark designed to evaluate document-side processing in retrieval systems, focusing on reasoning based on implicit facts in documents.", "motivation": "To evaluate retrieval systems in NLP beyond surface-level signals by introducing reasoning-heavy benchmarks.", "method": "ImpliRet presents simple queries whose relevance depends on implicit facts from documents, specifically through temporal, arithmetic, and world knowledge relationships.", "result": "The evaluation of various sparse and dense retrievers shows poor performance, with the best nDCG@10 being only 15.07%, and long-context models like GPT-4.1 scoring 35.06% with only ten documents.", "conclusion": "Document-side reasoning presents significant challenges even for advanced models, indicating room for improvement in retrieval systems.", "key_contributions": ["Introduction of the ImpliRet benchmark focusing on document-side reasoning", "Evaluation of various retrievers highlighting their limitations", "Testing long-context models with suboptimal results"], "limitations": "The benchmark relies on implicit facts which may not be adequately addressed by existing retrieval techniques.", "keywords": ["retrieval systems", "NLP", "reasoning", "implicit facts", "benchmark"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.14429", "pdf": "https://arxiv.org/pdf/2506.14429.pdf", "abs": "https://arxiv.org/abs/2506.14429", "title": "LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs", "authors": ["Xiaoran Liu", "Zhigeng Liu", "Zengfeng Huang", "Qipeng Guo", "Ziwei He", "Xipeng Qiu"], "categories": ["cs.CL"], "comment": "16 pages, 12 figures, work in progress", "summary": "Large Language Diffusion Models, or diffusion LLMs, have emerged as a\nsignificant focus in NLP research, with substantial effort directed toward\nunderstanding their scalability and downstream task performance. However, their\nlong-context capabilities remain unexplored, lacking systematic analysis or\nmethods for context extension. In this work, we present the first systematic\ninvestigation comparing the long-context performance of diffusion LLMs and\ntraditional auto-regressive LLMs. We first identify a unique characteristic of\ndiffusion LLMs, unlike auto-regressive LLMs, they maintain remarkably\n\\textbf{\\textit{stable perplexity}} during direct context extrapolation.\nFurthermore, where auto-regressive models fail outright during the\nNeedle-In-A-Haystack task with context exceeding their pretrained length, we\ndiscover diffusion LLMs exhibit a distinct \\textbf{\\textit{local perception}}\nphenomenon, enabling successful retrieval from recent context segments. We\nexplain both phenomena through the lens of Rotary Position Embedding (RoPE)\nscaling theory. Building on these observations, we propose LongLLaDA, a\ntraining-free method that integrates LLaDA with the NTK-based RoPE\nextrapolation. Our results validate that established extrapolation scaling laws\nremain effective for extending the context windows of diffusion LLMs.\nFurthermore, we identify long-context tasks where diffusion LLMs outperform\nauto-regressive LLMs and others where they fall short. Consequently, this study\nestablishes the first context extrapolation method for diffusion LLMs while\nproviding essential theoretical insights and empirical benchmarks critical for\nadvancing future research on long-context diffusion LLMs.", "AI": {"tldr": "This paper presents a systematic investigation into the long-context capabilities of diffusion LLMs compared to traditional auto-regressive LLMs, revealing unique phenomena and proposing a method for context extrapolation.", "motivation": "To explore the unexplored long-context capabilities of diffusion LLMs and understand their performance in comparison to traditional auto-regressive LLMs.", "method": "The authors conduct experiments comparing the long-context performance of diffusion LLMs with auto-regressive models while explaining findings with Rotary Position Embedding (RoPE) scaling theory. They propose a training-free method called LongLLaDA for effective context extrapolation.", "result": "Diffusion LLMs maintain stable perplexity and exhibit local perception phenomena that enable better performance on longer contexts. The proposed LongLLaDA method validates effective extrapolation scaling laws for diffusion LLMs, showing both areas of advantage and disadvantage compared to auto-regressive LLMs.", "conclusion": "This study lays the groundwork for context extrapolation methods for diffusion LLMs, contributing both theoretical insights and empirical benchmarks for future research.", "key_contributions": ["Systematic investigation of long-context capabilities in diffusion LLMs.", "Introduction of LongLLaDA for context extrapolation in diffusion LLMs.", "Identification of stable perplexity and local perception phenomena in diffusion LLMs."], "limitations": "The study is labeled as a work in progress, suggesting ongoing developments and potential areas for further research.", "keywords": ["Diffusion LLMs", "Long-context performance", "Context extrapolation"], "importance_score": 7, "read_time_minutes": 20}}
{"id": "2506.14448", "pdf": "https://arxiv.org/pdf/2506.14448.pdf", "abs": "https://arxiv.org/abs/2506.14448", "title": "How Far Can LLMs Improve from Experience? Measuring Test-Time Learning Ability in LLMs with Human Comparison", "authors": ["Jiayin Wang", "Zhiquang Guo", "Weizhi Ma", "Min Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "As evaluation designs of large language models may shape our trajectory\ntoward artificial general intelligence, comprehensive and forward-looking\nassessment is essential. Existing benchmarks primarily assess static knowledge,\nwhile intelligence also entails the ability to rapidly learn from experience.\nTo this end, we advocate for the evaluation of Test-time Learning, the capacity\nto improve performance in experience-based, reasoning-intensive tasks during\ntest time. In this work, we propose semantic games as effective testbeds for\nevaluating test-time learning, due to their resistance to saturation and\ninherent demand for strategic reasoning. We introduce an objective evaluation\nframework that compares model performance under both limited and cumulative\nexperience settings, and contains four forms of experience representation. To\nprovide a comparative baseline, we recruit eight human participants to complete\nthe same task. Results show that LLMs exhibit measurable test-time learning\ncapabilities; however, their improvements are less stable under cumulative\nexperience and progress more slowly than those observed in humans. These\nfindings underscore the potential of LLMs as general-purpose learning machines,\nwhile also revealing a substantial intellectual gap between models and humans,\nirrespective of how well LLMs perform on static benchmarks.", "AI": {"tldr": "The paper proposes evaluating large language models (LLMs) on their ability to learn during test time using semantic games, revealing insights on their performance compared to humans.", "motivation": "To assess the true capabilities of large language models (LLMs) in learning from experience, particularly in reasoning-intensive tasks, which existing benchmarks fail to address.", "method": "The paper introduces semantic games as testbeds for evaluating test-time learning, along with an objective evaluation framework that measures performance under various experience conditions and compares it to human performance.", "result": "Results indicate that while LLMs can show test-time learning capabilities, their learning under cumulative experience is less stable and slower than that of humans, highlighting a significant gap in intellectual capabilities.", "conclusion": "LLMs have potential as general-purpose learning machines, but their performance lags behind human intelligence in dynamic learning scenarios.", "key_contributions": ["Advocates for the evaluation of Test-time Learning in LLMs.", "Introduces semantic games as effective testbeds for this evaluation.", "Provides a comparative study between LLMs and human participants in learning tasks."], "limitations": "The evaluation is limited to specific tasks within semantic games and may not generalize to all reasoning-intensive contexts.", "keywords": ["Large Language Models", "Test-time Learning", "Semantic Games", "Human Performance Comparison", "Dynamic Learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.14474", "pdf": "https://arxiv.org/pdf/2506.14474.pdf", "abs": "https://arxiv.org/abs/2506.14474", "title": "LexiMark: Robust Watermarking via Lexical Substitutions to Enhance Membership Verification of an LLM's Textual Training Data", "authors": ["Eyal German", "Sagiv Antebi", "Edan Habler", "Asaf Shabtai", "Yuval Elovici"], "categories": ["cs.CL", "cs.CR"], "comment": null, "summary": "Large language models (LLMs) can be trained or fine-tuned on data obtained\nwithout the owner's consent. Verifying whether a specific LLM was trained on\nparticular data instances or an entire dataset is extremely challenging.\nDataset watermarking addresses this by embedding identifiable modifications in\ntraining data to detect unauthorized use. However, existing methods often lack\nstealth, making them relatively easy to detect and remove. In light of these\nlimitations, we propose LexiMark, a novel watermarking technique designed for\ntext and documents, which embeds synonym substitutions for carefully selected\nhigh-entropy words. Our method aims to enhance an LLM's memorization\ncapabilities on the watermarked text without altering the semantic integrity of\nthe text. As a result, the watermark is difficult to detect, blending\nseamlessly into the text with no visible markers, and is resistant to removal\ndue to its subtle, contextually appropriate substitutions that evade automated\nand manual detection. We evaluated our method using baseline datasets from\nrecent studies and seven open-source models: LLaMA-1 7B, LLaMA-3 8B, Mistral\n7B, Pythia 6.9B, as well as three smaller variants from the Pythia family\n(160M, 410M, and 1B). Our evaluation spans multiple training settings,\nincluding continued pretraining and fine-tuning scenarios. The results\ndemonstrate significant improvements in AUROC scores compared to existing\nmethods, underscoring our method's effectiveness in reliably verifying whether\nunauthorized watermarked data was used in LLM training.", "AI": {"tldr": "LexiMark is a novel watermarking technique for LLMs that embeds synonym substitutions in high-entropy words to detect unauthorized use of training data.", "motivation": "To address the challenges of verifying whether LLMs were trained on unauthorized datasets, given existing methods' lack of stealth and ease of removal.", "method": "LexiMark uses synonym substitutions for selected high-entropy words, enhancing memorization without altering text semantics, making the watermark subtle and resistant to detection.", "result": "Evaluation shows significant improvements in AUROC scores over existing methods across various training settings and models, demonstrating the effectiveness of LexiMark.", "conclusion": "LexiMark provides a reliable means of watermarking textual data for LLMs, ensuring detection of unauthorized data usage while maintaining performance integrity.", "key_contributions": ["Introduction of LexiMark, an innovative watermarking method for LLMs.", "Demonstration of significant AUROC score improvements over previous watermarking methods.", "Evidence of the method's effectiveness in various LLM training scenarios."], "limitations": "", "keywords": ["watermarking", "large language models", "LLM training", "data privacy", "synonym substitution"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.14493", "pdf": "https://arxiv.org/pdf/2506.14493.pdf", "abs": "https://arxiv.org/abs/2506.14493", "title": "LingoLoop Attack: Trapping MLLMs via Linguistic Context and State Entrapment into Endless Loops", "authors": ["Jiyuan Fu", "Kaixun Jiang", "Lingyi Hong", "Jinglun Li", "Haijing Guo", "Dingkang Yang", "Zhaoyu Chen", "Wenqiang Zhang"], "categories": ["cs.CL", "cs.CR"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have shown great promise but require\nsubstantial computational resources during inference. Attackers can exploit\nthis by inducing excessive output, leading to resource exhaustion and service\ndegradation. Prior energy-latency attacks aim to increase generation time by\nbroadly shifting the output token distribution away from the EOS token, but\nthey neglect the influence of token-level Part-of-Speech (POS) characteristics\non EOS and sentence-level structural patterns on output counts, limiting their\nefficacy. To address this, we propose LingoLoop, an attack designed to induce\nMLLMs to generate excessively verbose and repetitive sequences. First, we find\nthat the POS tag of a token strongly affects the likelihood of generating an\nEOS token. Based on this insight, we propose a POS-Aware Delay Mechanism to\npostpone EOS token generation by adjusting attention weights guided by POS\ninformation. Second, we identify that constraining output diversity to induce\nrepetitive loops is effective for sustained generation. We introduce a\nGenerative Path Pruning Mechanism that limits the magnitude of hidden states,\nencouraging the model to produce persistent loops. Extensive experiments\ndemonstrate LingoLoop can increase generated tokens by up to 30 times and\nenergy consumption by a comparable factor on models like Qwen2.5-VL-3B,\nconsistently driving MLLMs towards their maximum generation limits. These\nfindings expose significant MLLMs' vulnerabilities, posing challenges for their\nreliable deployment. The code will be released publicly following the paper's\nacceptance.", "AI": {"tldr": "The paper introduces LingoLoop, an attack on Multimodal Large Language Models (MLLMs) that exploits token-level Part-of-Speech (POS) characteristics to induce excessive verbosity and resource consumption during inference.", "motivation": "The study addresses the vulnerability of MLLMs to energy-latency attacks by incorporating token-level POS characteristics and structural patterns that influence output counts, aiming to enhance the effectiveness of such attacks.", "method": "LingoLoop utilizes a POS-Aware Delay Mechanism to adjust the generation of the EOS token based on POS information, and a Generative Path Pruning Mechanism to promote repetitive output loops, increasing verbosity during model inference.", "result": "Experiments show that LingoLoop can increase generated tokens by up to 30 times and significantly boost energy consumption on models like Qwen2.5-VL-3B, highlighting the vulnerabilities in MLLMs.", "conclusion": "The findings reveal critical weaknesses in MLLMs, presenting challenges for their deployment, and the code will be available post-acceptance.", "key_contributions": ["Introduction of LingoLoop, an innovative attack method for MLLMs.", "Development of POS-Aware Delay Mechanism for managing EOS token generation.", "Implementation of Generative Path Pruning Mechanism to induce repetitive output."], "limitations": "The methods proposed may only be applicable to certain MLLM architectures, and the long-term impacts on model performance are not assessed.", "keywords": ["Multimodal Large Language Models", "Energy-latency attacks", "Part-of-Speech characteristics", "Generative Path Pruning", "Natural Language Processing"], "importance_score": 5, "read_time_minutes": 15}}
{"id": "2506.14532", "pdf": "https://arxiv.org/pdf/2506.14532.pdf", "abs": "https://arxiv.org/abs/2506.14532", "title": "M2BeamLLM: Multimodal Sensing-empowered mmWave Beam Prediction with Large Language Models", "authors": ["Can Zheng", "Jiguang He", "Chung G. Kang", "Guofa Cai", "Zitong Yu", "Merouane Debbah"], "categories": ["cs.CL"], "comment": "13 pages, 20 figures", "summary": "This paper introduces a novel neural network framework called M2BeamLLM for\nbeam prediction in millimeter-wave (mmWave) massive multi-input multi-output\n(mMIMO) communication systems. M2BeamLLM integrates multi-modal sensor data,\nincluding images, radar, LiDAR, and GPS, leveraging the powerful reasoning\ncapabilities of large language models (LLMs) such as GPT-2 for beam prediction.\nBy combining sensing data encoding, multimodal alignment and fusion, and\nsupervised fine-tuning (SFT), M2BeamLLM achieves significantly higher beam\nprediction accuracy and robustness, demonstrably outperforming traditional deep\nlearning (DL) models in both standard and few-shot scenarios. Furthermore, its\nprediction performance consistently improves with increased diversity in\nsensing modalities. Our study provides an efficient and intelligent beam\nprediction solution for vehicle-to-infrastructure (V2I) mmWave communication\nsystems.", "AI": {"tldr": "A novel neural network framework, M2BeamLLM, enhances beam prediction in mmWave mMIMO systems by integrating multi-modal sensor data and leveraging LLMs for improved performance.", "motivation": "To improve beam prediction accuracy and robustness in mmWave massive multi-input multi-output communication systems by utilizing multi-modal sensor data.", "method": "M2BeamLLM integrates data from various sensors (images, radar, LiDAR, GPS) and employs techniques like multimodal alignment and supervised fine-tuning to enhance prediction performance.", "result": "M2BeamLLM achieves significantly higher accuracy and robustness in beam prediction compared to traditional deep learning models, especially in few-shot scenarios, benefiting from increased diversity in sensing modalities.", "conclusion": "The proposed framework provides an efficient solution for intelligent beam prediction in vehicle-to-infrastructure communication systems, outperforming existing methods.", "key_contributions": ["Introduction of M2BeamLLM framework for beam prediction", "Integration of multi-modal sensor data for improved accuracy", "Demonstrated superior performance over traditional DL models"], "limitations": "", "keywords": ["neural networks", "beam prediction", "mMIMO", "multi-modal data", "large language models"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2506.14562", "pdf": "https://arxiv.org/pdf/2506.14562.pdf", "abs": "https://arxiv.org/abs/2506.14562", "title": "AlphaDecay:Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs", "authors": ["Di He", "Ajay Jaiswal", "Songjun Tu", "Li Shen", "Ganzhao Yuan", "Shiwei Liu", "Lu Yin"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Weight decay is a standard regularization technique for training large\nlanguage models (LLMs). While it is common to assign a uniform decay rate to\nevery layer, this approach overlooks the structural diversity of LLMs and the\nvarying spectral properties across modules. In this paper, we introduce\nAlphaDecay, a simple yet effective method that adaptively assigns different\nweight decay strengths to each module of an LLM. Our approach is guided by\nHeavy-Tailed Self-Regularization (HT-SR) theory, which analyzes the empirical\nspectral density (ESD) of weight correlation matrices to quantify\n\"heavy-tailedness.\" Modules exhibiting more pronounced heavy-tailed ESDs,\nreflecting stronger feature learning, are assigned weaker decay, while modules\nwith lighter-tailed spectra receive stronger decay. Our method leverages\ntailored weight decay assignments to balance the module-wise differences in\nspectral properties, leading to improved performance. Extensive pre-training\ntasks with various model sizes from 60M to 1B demonstrate that AlphaDecay\nachieves better perplexity and generalization than conventional uniform decay\nand other adaptive decay baselines.", "AI": {"tldr": "Introducing AlphaDecay, a method for assigning adaptive weight decay rates in large language models based on spectral properties to enhance model performance.", "motivation": "The need for better weight decay strategies in large language models that consider the structural diversity and spectral properties of different modules.", "method": "AlphaDecay assigns varying weight decay strengths to each module of an LLM based on their empirical spectral density (ESD) analyzed through Heavy-Tailed Self-Regularization theory.", "result": "AlphaDecay shows improved perplexity and generalization on pre-training tasks across various model sizes, outperforming uniform decay and other adaptive methods.", "conclusion": "Tailored weight decay assignments substantially improve performance by addressing module-wise differences in spectral properties in LLMs.", "key_contributions": ["Introduction of AlphaDecay for adaptive weight decay in LLMs", "Use of Heavy-Tailed Self-Regularization theory for decay assignment", "Demonstrated performance improvements across varying model sizes."], "limitations": "", "keywords": ["Weight Decay", "Large Language Models", "Heavy-Tailed Self-Regularization", "Spectral Properties", "Model Generalization"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.14580", "pdf": "https://arxiv.org/pdf/2506.14580.pdf", "abs": "https://arxiv.org/abs/2506.14580", "title": "GenerationPrograms: Fine-grained Attribution with Executable Programs", "authors": ["David Wan", "Eran Hirsch", "Elias Stengel-Eskin", "Ido Dagan", "Mohit Bansal"], "categories": ["cs.CL", "cs.AI"], "comment": "27 Pages. Code: https://github.com/meetdavidwan/generationprograms", "summary": "Recent large language models (LLMs) achieve impressive performance in\nsource-conditioned text generation but often fail to correctly provide\nfine-grained attributions for their outputs, undermining verifiability and\ntrust. Moreover, existing attribution methods do not explain how and why models\nleverage the provided source documents to generate their final responses,\nlimiting interpretability. To overcome these challenges, we introduce a modular\ngeneration framework, GenerationPrograms, inspired by recent advancements in\nexecutable \"code agent\" architectures. Unlike conventional generation methods\nthat simultaneously generate outputs and attributions or rely on post-hoc\nattribution, GenerationPrograms decomposes the process into two distinct\nstages: first, creating an executable program plan composed of modular text\noperations (such as paraphrasing, compression, and fusion) explicitly tailored\nto the query, and second, executing these operations following the program's\nspecified instructions to produce the final response. Empirical evaluations\ndemonstrate that GenerationPrograms significantly improves attribution quality\nat both the document level and sentence level across two long-form\nquestion-answering tasks and a multi-document summarization task. We further\ndemonstrate that GenerationPrograms can effectively function as a post-hoc\nattribution method, outperforming traditional techniques in recovering accurate\nattributions. In addition, the interpretable programs generated by\nGenerationPrograms enable localized refinement through modular-level\nimprovements that further enhance overall attribution quality.", "AI": {"tldr": "The paper introduces GenerationPrograms, a modular framework for improving attribution in LLM-generated texts by decomposing the generation process into programmatic stages.", "motivation": "Recent large language models struggle with providing clear and accurate attributions for their text outputs, affecting verifiability and trust. Existing methods are limited in interpretability.", "method": "GenerationPrograms separates text generation into two stages: creating a program plan with modular text operations tailored to the query, and executing these operations to produce the final output.", "result": "Empirical evaluations show that GenerationPrograms enhances attribution quality at both document and sentence levels across various tasks, and it outperforms traditional attribution methods.", "conclusion": "The framework not only improves attribution quality but also allows for modular refinements, enhancing the interpretability of the generation process.", "key_contributions": ["Introduction of a two-stage modular generation framework", "Enhanced attribution quality through localized refinement", "Outperformance of conventional attribution techniques"], "limitations": "", "keywords": ["large language models", "attribution", "interperability", "text generation", "modular framework"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.14606", "pdf": "https://arxiv.org/pdf/2506.14606.pdf", "abs": "https://arxiv.org/abs/2506.14606", "title": "Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC Transpilation with Testing Guarantees", "authors": ["Ahmed Heakl", "Sarim Hashmi", "Chaimaa Abi", "Celine Lee", "Abdulrahman Mahmoud"], "categories": ["cs.CL", "cs.AR", "cs.LG", "cs.PL", "cs.SE"], "comment": "Project page: https://ahmedheakl.github.io/Guaranteed-Guess/", "summary": "The hardware ecosystem is rapidly evolving, with increasing interest in\ntranslating low-level programs across different instruction set architectures\n(ISAs) in a quick, flexible, and correct way to enhance the portability and\nlongevity of existing code. A particularly challenging class of this\ntranspilation problem is translating between complex- (CISC) and reduced-\n(RISC) hardware architectures, due to fundamental differences in instruction\ncomplexity, memory models, and execution paradigms. In this work, we introduce\nGG (Guaranteed Guess), an ISA-centric transpilation pipeline that combines the\ntranslation power of pre-trained large language models (LLMs) with the rigor of\nestablished software testing constructs. Our method generates candidate\ntranslations using an LLM from one ISA to another, and embeds such translations\nwithin a software-testing framework to build quantifiable confidence in the\ntranslation. We evaluate our GG approach over two diverse datasets, enforce\nhigh code coverage (>98%) across unit tests, and achieve functional/semantic\ncorrectness of 99% on HumanEval programs and 49% on BringupBench programs,\nrespectively. Further, we compare our approach to the state-of-the-art Rosetta\n2 framework on Apple Silicon, showcasing 1.73x faster runtime performance,\n1.47x better energy efficiency, and 2.41x better memory usage for our\ntranspiled code, demonstrating the effectiveness of GG for real-world\nCISC-to-RISC translation tasks. We will open-source our codes, data, models,\nand benchmarks to establish a common foundation for ISA-level code translation\nresearch.", "AI": {"tldr": "Introducing GG, an ISA-centric transpilation pipeline that leverages large language models for effective CISC to RISC code translation, achieving high correctness and performance metrics.", "motivation": "There is a growing need for efficient transpilation of low-level programs across different instruction set architectures (ISAs) to enhance code portability and longevity.", "method": "The GG pipeline combines the power of pre-trained LLMs with a software testing framework to generate and validate translations between ISAs, specifically targeting CISC and RISC architectures.", "result": "GG achieves over 98% code coverage in unit tests and semantic correctness of 99% on HumanEval programs and 49% on BringupBench programs, outperforming the state-of-the-art Rosetta 2 framework in performance and resource efficiency.", "conclusion": "The GG approach demonstrates significant improvements in runtime, energy efficiency, and memory usage for transpiled code, providing a robust foundation for future ISA-level code translation research.", "key_contributions": ["Development of the GG transpilation pipeline", "High code coverage and functional correctness on diverse benchmarks", "Demonstrated performance improvements over existing frameworks"], "limitations": "", "keywords": ["transpilation", "large language models", "ISA", "CISC", "RISC"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.14613", "pdf": "https://arxiv.org/pdf/2506.14613.pdf", "abs": "https://arxiv.org/abs/2506.14613", "title": "When Does Meaning Backfire? Investigating the Role of AMRs in NLI", "authors": ["Junghyun Min", "Xiulin Yang", "Shira Wein"], "categories": ["cs.CL"], "comment": "9 pages, 2 figures", "summary": "Natural Language Inference (NLI) relies heavily on adequately parsing the\nsemantic content of the premise and hypothesis. In this work, we investigate\nwhether adding semantic information in the form of an Abstract Meaning\nRepresentation (AMR) helps pretrained language models better generalize in NLI.\nOur experiments integrating AMR into NLI in both fine-tuning and prompting\nsettings show that the presence of AMR in fine-tuning hinders model\ngeneralization while prompting with AMR leads to slight gains in\n\\texttt{GPT-4o}. However, an ablation study reveals that the improvement comes\nfrom amplifying surface-level differences rather than aiding semantic\nreasoning. This amplification can mislead models to predict non-entailment even\nwhen the core meaning is preserved.", "AI": {"tldr": "This paper investigates the impact of adding Abstract Meaning Representation (AMR) to pretrained language models for Natural Language Inference (NLI).", "motivation": "To explore whether integrating AMR can improve generalization in NLI tasks for pretrained language models.", "method": "The authors conducted experiments using AMR in both fine-tuning and prompting scenarios with pretrained models like GPT-4o.", "result": "Incorporating AMR during fine-tuning hindered generalization, while prompting with AMR yielded slight improvements; however, these improvements were attributed to surface-level differences.", "conclusion": "The study concludes that the addition of AMR can mislead models in NLI, as they may rely on superficial differences rather than true semantic reasoning.", "key_contributions": ["Demonstrated AMR's negative impact on fine-tuning NLI tasks.", "Showed that prompting with AMR can lead to minor performance gains.", "Identified the risks of models misinterpreting relevance due to superficial amplification."], "limitations": "The results indicate that improvements are tied to surface-level distinctions rather than deeper semantic understanding, which is a limitation of using AMR in this context.", "keywords": ["Natural Language Inference", "Abstract Meaning Representation", "pretrained language models", "GPT-4o", "semantic reasoning"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.14625", "pdf": "https://arxiv.org/pdf/2506.14625.pdf", "abs": "https://arxiv.org/abs/2506.14625", "title": "Probabilistic Aggregation and Targeted Embedding Optimization for Collective Moral Reasoning in Large Language Models", "authors": ["Chenchen Yuan", "Zheyu Zhang", "Shuo Yang", "Bardh Prenkaj", "Gjergji Kasneci"], "categories": ["cs.CL", "cs.AI"], "comment": "18 pages", "summary": "Large Language Models (LLMs) have shown impressive moral reasoning abilities.\nYet they often diverge when confronted with complex, multi-factor moral\ndilemmas. To address these discrepancies, we propose a framework that\nsynthesizes multiple LLMs' moral judgments into a collectively formulated moral\njudgment, realigning models that deviate significantly from this consensus. Our\naggregation mechanism fuses continuous moral acceptability scores (beyond\nbinary labels) into a collective probability, weighting contributions by model\nreliability. For misaligned models, a targeted embedding-optimization procedure\nfine-tunes token embeddings for moral philosophical theories, minimizing JS\ndivergence to the consensus while preserving semantic integrity. Experiments on\na large-scale social moral dilemma dataset show our approach builds robust\nconsensus and improves individual model fidelity. These findings highlight the\nvalue of data-driven moral alignment across multiple models and its potential\nfor safer, more consistent AI systems.", "AI": {"tldr": "This paper introduces a framework to improve moral reasoning in LLMs by aggregating their judgments and realigning misaligned models, leading to more consistent AI systems.", "motivation": "To address discrepancies in moral judgments by large language models when faced with complex dilemmas, aiming for improved consistency and reliability.", "method": "The framework synthesizes multiple LLMs' moral judgments into a collective moral judgment, incorporating a mechanism that weights contributions based on model reliability, and employs embedding-optimization for misaligned models.", "result": "Experiments demonstrated that the proposed method effectively builds consensus among LLMs and enhances the fidelity of individual models, showcasing a reduction in divergence in moral judgments.", "conclusion": "The approach underscores the importance of data-driven moral alignment across models, suggesting it leads to safer and more consistent AI systems.", "key_contributions": ["Framework for synthesizing moral judgments from multiple LLMs", "Embedding-optimization procedure for aligning diverging models", "Demonstrated improvements in moral judgment consistency through experiments"], "limitations": "The framework's effectiveness may be limited by the diversity of training data and the inherent biases present in the individual models.", "keywords": ["Moral reasoning", "Large Language Models", "Model alignment", "Collective moral judgment", "Embedding optimization"], "importance_score": 9, "read_time_minutes": 18}}
{"id": "2506.14634", "pdf": "https://arxiv.org/pdf/2506.14634.pdf", "abs": "https://arxiv.org/abs/2506.14634", "title": "AIn't Nothing But a Survey? Using Large Language Models for Coding German Open-Ended Survey Responses on Survey Motivation", "authors": ["Leah von der Heyde", "Anna-Carolina Haensch", "Bernd Weiß", "Jessika Daikeler"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "to appear in Survey Research Methods", "summary": "The recent development and wider accessibility of LLMs have spurred\ndiscussions about how they can be used in survey research, including\nclassifying open-ended survey responses. Due to their linguistic capacities, it\nis possible that LLMs are an efficient alternative to time-consuming manual\ncoding and the pre-training of supervised machine learning models. As most\nexisting research on this topic has focused on English-language responses\nrelating to non-complex topics or on single LLMs, it is unclear whether its\nfindings generalize and how the quality of these classifications compares to\nestablished methods. In this study, we investigate to what extent different\nLLMs can be used to code open-ended survey responses in other contexts, using\nGerman data on reasons for survey participation as an example. We compare\nseveral state-of-the-art LLMs and several prompting approaches, and evaluate\nthe LLMs' performance by using human expert codings. Overall performance\ndiffers greatly between LLMs, and only a fine-tuned LLM achieves satisfactory\nlevels of predictive performance. Performance differences between prompting\napproaches are conditional on the LLM used. Finally, LLMs' unequal\nclassification performance across different categories of reasons for survey\nparticipation results in different categorical distributions when not using\nfine-tuning. We discuss the implications of these findings, both for\nmethodological research on coding open-ended responses and for their\nsubstantive analysis, and for practitioners processing or substantively\nanalyzing such data. Finally, we highlight the many trade-offs researchers need\nto consider when choosing automated methods for open-ended response\nclassification in the age of LLMs. In doing so, our study contributes to the\ngrowing body of research about the conditions under which LLMs can be\nefficiently, accurately, and reliably leveraged in survey research.", "AI": {"tldr": "This study investigates the effectiveness of various LLMs in coding open-ended survey responses, particularly using German data, and compares their performance to established methods.", "motivation": "To explore the potential of LLMs as efficient alternatives for manually coding open-ended survey responses, particularly in contexts beyond English and in complex topics.", "method": "The study compares various state-of-the-art LLMs and prompting approaches against human expert codings while analyzing performance differences based on different prompting techniques and LLMs used.", "result": "Performance varies widely among LLMs; only a fine-tuned model achieves satisfactory results in predictive performance. Performance differences in prompting affect the output depending on the LLM.", "conclusion": "This research underscores the need for careful consideration of LLM selection and prompting strategies for coding open-ended survey responses, highlighting significant trade-offs for researchers implementing automated methods.", "key_contributions": ["Comparison of multiple LLMs on German open-ended survey data", "Insights into the performance variation of LLMs based on prompting methods", "Discussion of implications for methodological research and practical application in survey research."], "limitations": "Performance variability may not generalize across other languages or less common topics; the necessity of fine-tuning raises questions about accessibility.", "keywords": ["Large Language Models", "Survey Research", "Open-ended Responses", "Coding", "German Data"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.14641", "pdf": "https://arxiv.org/pdf/2506.14641.pdf", "abs": "https://arxiv.org/abs/2506.14641", "title": "Revisiting Chain-of-Thought Prompting: Zero-shot Can Be Stronger than Few-shot", "authors": ["Xiang Cheng", "Chengyan Pan", "Minjun Zhao", "Deyang Li", "Fangchao Liu", "Xinyu Zhang", "Xiao Zhang", "Yong Liu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "19 pages,22 figures", "summary": "In-Context Learning (ICL) is an essential emergent ability of Large Language\nModels (LLMs), and recent studies introduce Chain-of-Thought (CoT) to exemplars\nof ICL to enhance the reasoning capability, especially in mathematics tasks.\nHowever, given the continuous advancement of model capabilities, it remains\nunclear whether CoT exemplars still benefit recent, stronger models in such\ntasks. Through systematic experiments, we find that for recent strong models\nsuch as the Qwen2.5 series, adding traditional CoT exemplars does not improve\nreasoning performance compared to Zero-Shot CoT. Instead, their primary\nfunction is to align the output format with human expectations. We further\ninvestigate the effectiveness of enhanced CoT exemplars, constructed using\nanswers from advanced models such as \\texttt{Qwen2.5-Max} and\n\\texttt{DeepSeek-R1}. Experimental results indicate that these enhanced\nexemplars still fail to improve the model's reasoning performance. Further\nanalysis reveals that models tend to ignore the exemplars and focus primarily\non the instructions, leading to no observable gain in reasoning ability.\nOverall, our findings highlight the limitations of the current ICL+CoT\nframework in mathematical reasoning, calling for a re-examination of the ICL\nparadigm and the definition of exemplars.", "AI": {"tldr": "This paper investigates the effectiveness of Chain-of-Thought (CoT) exemplars in enhancing reasoning performance of recent Large Language Models (LLMs), finding that they do not improve performance compared to Zero-Shot CoT.", "motivation": "To assess the impact of Chain-of-Thought (CoT) exemplars on the reasoning capabilities of advanced LLMs, especially in mathematical tasks, amidst continuous advancements in model capabilities.", "method": "Systematic experiments are conducted using strong models like Qwen2.5 series and enhanced CoT exemplars constructed from outputs of advanced models, comparing their performances in reasoning tasks.", "result": "The experiments show that traditional CoT exemplars do not improve reasoning performance over Zero-Shot CoT, and enhanced exemplars also fail to provide observable gains as models tend to ignore exemplars in favor of direct instructions.", "conclusion": "The findings emphasize the limitations of the current ICL+CoT framework in mathematical reasoning, suggesting a need for a re-evaluation of the ICL paradigm and how exemplars are defined.", "key_contributions": ["Analysis of the effectiveness of traditional and enhanced CoT exemplars in recent LLMs.", "Demonstration that CoT exemplars primarily serve to align output format rather than improve reasoning.", "Call for a re-examination of the ICL paradigm and exemplar definitions in the context of advanced LLMs."], "limitations": "The study highlights that models often ignore exemplars and focuses more on instructions, which limits the potential of CoT exemplars to improve reasoning skills.", "keywords": ["In-Context Learning", "Chain-of-Thought", "Large Language Models", "mathematical reasoning", "exemplars"], "importance_score": 8, "read_time_minutes": 19}}
{"id": "2506.14645", "pdf": "https://arxiv.org/pdf/2506.14645.pdf", "abs": "https://arxiv.org/abs/2506.14645", "title": "Passing the Turing Test in Political Discourse: Fine-Tuning LLMs to Mimic Polarized Social Media Comments", "authors": [". Pazzaglia", "V. Vendetti", "L. D. Comencini", "F. Deriu", "V. Modugno"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "The increasing sophistication of large language models (LLMs) has sparked\ngrowing concerns regarding their potential role in exacerbating ideological\npolarization through the automated generation of persuasive and biased content.\nThis study explores the extent to which fine-tuned LLMs can replicate and\namplify polarizing discourse within online environments. Using a curated\ndataset of politically charged discussions extracted from Reddit, we fine-tune\nan open-source LLM to produce context-aware and ideologically aligned\nresponses. The model's outputs are evaluated through linguistic analysis,\nsentiment scoring, and human annotation, with particular attention to\ncredibility and rhetorical alignment with the original discourse. The results\nindicate that, when trained on partisan data, LLMs are capable of producing\nhighly plausible and provocative comments, often indistinguishable from those\nwritten by humans. These findings raise significant ethical questions about the\nuse of AI in political discourse, disinformation, and manipulation campaigns.\nThe paper concludes with a discussion of the broader implications for AI\ngovernance, platform regulation, and the development of detection tools to\nmitigate adversarial fine-tuning risks.", "AI": {"tldr": "This study investigates how fine-tuned large language models (LLMs) can generate polarized content, using Reddit discussions as a dataset, revealing ethical implications for AI in political discourse.", "motivation": "The study addresses concerns about LLMs exacerbating ideological polarization and the risks associated with their persuasive capabilities in political contexts.", "method": "The authors fine-tune an open-source LLM using a dataset of politically charged discussions from Reddit, analyzing the model's outputs through linguistic analysis, sentiment scoring, and human annotation.", "result": "The findings show that LLMs trained on partisan data can produce content that is highly plausible and indistinguishable from human-generated comments, raising ethical concerns regarding their role in disinformation.", "conclusion": "The paper highlights the need for AI governance and platform regulation to mitigate risks associated with adversarial fine-tuning of LLMs for political manipulation.", "key_contributions": ["Investigation of LLMs' role in polarizing discourse", "Demonstration of LLMs' capability to produce human-like partisan content", "Discussion of ethical implications in AI-driven political communication"], "limitations": "", "keywords": ["Large Language Models", "Polarization", "Political Discourse", "Ethics in AI", "Disinformation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.14646", "pdf": "https://arxiv.org/pdf/2506.14646.pdf", "abs": "https://arxiv.org/abs/2506.14646", "title": "GuiLoMo: Allocating Expert Number and Rank for LoRA-MoE via Bilevel Optimization with GuidedSelection Vectors", "authors": ["Hengyuan Zhang", "Xinrong Chen", "Yingmin Qiu", "Xiao Liang", "Ziyue Li", "Guanyu Wang", "Weiping Li", "Tong Mo", "Wenyue Li", "Hayden Kwok-Hay So", "Ngai Wong"], "categories": ["cs.CL"], "comment": null, "summary": "Parameter-efficient fine-tuning (PEFT) methods, particularly Low-Rank\nAdaptation (LoRA), offer an efficient way to adapt large language models with\nreduced computational costs. However, their performance is limited by the small\nnumber of trainable parameters. Recent work combines LoRA with the\nMixture-of-Experts (MoE), i.e., LoRA-MoE, to enhance capacity, but two\nlimitations remain in hindering the full exploitation of its potential: 1) the\ninfluence of downstream tasks when assigning expert numbers, and 2) the uniform\nrank assignment across all LoRA experts, which restricts representational\ndiversity. To mitigate these gaps, we propose GuiLoMo, a fine-grained\nlayer-wise expert numbers and ranks allocation strategy with GuidedSelection\nVectors (GSVs). GSVs are learned via a prior bilevel optimization process to\ncapture both model- and task-specific needs, and are then used to allocate\noptimal expert numbers and ranks. Experiments on three backbone models across\ndiverse benchmarks show that GuiLoMo consistently achieves superior or\ncomparable performance to all baselines. Further analysis offers key insights\ninto how expert numbers and ranks vary across layers and tasks, highlighting\nthe benefits of adaptive expert configuration. Our code is available at\nhttps://github.com/Liar406/Gui-LoMo.git.", "AI": {"tldr": "This paper introduces GuiLoMo, a new strategy for fine-tuning large language models by optimizing expert numbers and ranks using guided selection vectors, enhancing the efficiency of parameter-efficient fine-tuning methods.", "motivation": "To address limitations in current PEFT methods, specifically LoRA and LoRA-MoE, to improve adaptation of large language models while maintaining representational diversity and efficiency.", "method": "GuiLoMo utilizes a bilevel optimization process to learn Guided Selection Vectors, which inform the layer-wise allocation of expert numbers and ranks in adapting large language models.", "result": "Experiments show that GuiLoMo achieves superior or comparable performance to existing baselines across multiple models and benchmarks, revealing insights into adaptive configurations of experts.", "conclusion": "The findings demonstrate that tailored expert configurations across layers and tasks lead to improved performance and highlight the potential of adaptive expert numbers and ranks.", "key_contributions": ["Introduction of GuiLoMo for fine-grained expert allocation in language models", "Demonstration of superior performance against existing fine-tuning baselines", "Insights into the layer-wise variation of expert configuration"], "limitations": "", "keywords": ["Parameter-efficient fine-tuning", "Low-Rank Adaptation", "Mixture-of-Experts", "Guided Selection Vectors", "Large Language Models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.14681", "pdf": "https://arxiv.org/pdf/2506.14681.pdf", "abs": "https://arxiv.org/abs/2506.14681", "title": "Massive Supervised Fine-tuning Experiments Reveal How Data, Layer, and Training Factors Shape LLM Alignment Quality", "authors": ["Yuto Harada", "Yusuke Yamauchi", "Yusuke Oda", "Yohei Oseki", "Yusuke Miyao", "Yu Takagi"], "categories": ["cs.CL"], "comment": null, "summary": "Supervised fine-tuning (SFT) is a critical step in aligning large language\nmodels (LLMs) with human instructions and values, yet many aspects of SFT\nremain poorly understood. We trained a wide range of base models on a variety\nof datasets including code generation, mathematical reasoning, and\ngeneral-domain tasks, resulting in 1,000+ SFT models under controlled\nconditions. We then identified the dataset properties that matter most and\nexamined the layer-wise modifications introduced by SFT. Our findings reveal\nthat some training-task synergies persist across all models while others vary\nsubstantially, emphasizing the importance of model-specific strategies.\nMoreover, we demonstrate that perplexity consistently predicts SFT\neffectiveness--often surpassing superficial similarity between trained data and\nbenchmark--and that mid-layer weight changes correlate most strongly with\nperformance gains. We will release these 1,000+ SFT models and benchmark\nresults to accelerate further research.", "AI": {"tldr": "This paper investigates supervised fine-tuning (SFT) of large language models (LLMs), analyzing the effects of dataset properties and layer-wise modifications on model performance across various tasks.", "motivation": "Understanding the nuances of supervised fine-tuning (SFT) in aligning LLMs with human instructions and values is crucial for improving their effectiveness.", "method": "A variety of base models were trained on numerous datasets including code generation, mathematical reasoning, and general-domain tasks, resulting in over 1,000 SFT models, with a focus on dataset properties and layer-wise changes during SFT.", "result": "The study identifies which dataset properties are critical, reveals persistent training-task synergies across models, and establishes that perplexity is a reliable predictor of SFT effectiveness.", "conclusion": "The findings highlight the need for model-specific fine-tuning strategies and the correlation between mid-layer weight changes and performance improvements; over 1,000 SFT models and benchmark results will be shared to facilitate future research.", "key_contributions": ["Identification of critical dataset properties for SFT effectiveness.", "Revelation of synergies and variances in training-task performance across models.", "Establishment of perplexity as a significant predictor of SFT success."], "limitations": "", "keywords": ["Supervised Fine-Tuning", "Large Language Models", "Dataset Properties", "Model-Specific Strategies", "Performance Prediction"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.14702", "pdf": "https://arxiv.org/pdf/2506.14702.pdf", "abs": "https://arxiv.org/abs/2506.14702", "title": "Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time Markers", "authors": ["Daniel D'souza", "Julia Kreutzer", "Adrien Morisot", "Ahmet Üstün", "Sara Hooker"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "One of the most profound challenges of modern machine learning is performing\nwell on the long-tail of rare and underrepresented features. Large\ngeneral-purpose models are trained for many tasks, but work best on\nhigh-frequency use cases. After training, it is hard to adapt a model to\nperform well on specific use cases underrepresented in the training corpus.\nRelying on prompt engineering or few-shot examples to maximize the output\nquality on a particular test case can be frustrating, as models can be highly\nsensitive to small changes, react in unpredicted ways or rely on a fixed system\nprompt for maintaining performance. In this work, we ask: \"Can we optimize our\ntraining protocols to both improve controllability and performance on\nunderrepresented use cases at inference time?\" We revisit the divide between\ntraining and inference techniques to improve long-tail performance while\nproviding users with a set of control levers the model is trained to be\nresponsive to. We create a detailed taxonomy of data characteristics and task\nprovenance to explicitly control generation attributes and implicitly condition\ngenerations at inference time. We fine-tune a base model to infer these markers\nautomatically, which makes them optional at inference time. This principled and\nflexible approach yields pronounced improvements in performance, especially on\nexamples from the long tail of the training distribution. While we observe an\naverage lift of 5.7% win rates in open-ended generation quality with our\nmarkers, we see over 9.1% gains in underrepresented domains. We also observe\nrelative lifts of up to 14.1% on underrepresented tasks like CodeRepair and\nabsolute improvements of 35.3% on length instruction following evaluations.", "AI": {"tldr": "This paper addresses the challenge of improving machine learning model performance on rare and underrepresented features by optimizing training protocols and implementing a taxonomy for control over generative attributes.", "motivation": "Modern machine learning struggles with performing well on long-tail features, which are underrepresented in training datasets, leading to poor model adaptability and output quality.", "method": "The authors propose an optimized training approach that includes a taxonomy of data characteristics and fine-tuning to help models better adapt at inference time, with a focus on enhancing performance for underrepresented use cases.", "result": "The approach yields an average lift of 5.7% in open-ended generation quality and over 9.1% in underrepresented domains, with relative lifts of up to 14.1% on specific tasks like CodeRepair.", "conclusion": "The findings suggest that improved training protocols can significantly enhance model performance and controllability on rare task components, effectively bridging the gap between training and inference.", "key_contributions": ["Creation of a detailed taxonomy for controlling generative attributes", "Development of an automatic inference marker system", "Significant performance improvements for long-tail use cases"], "limitations": "", "keywords": ["machine learning", "long-tail problem", "training protocols", "inference", "underrepresented features"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.14704", "pdf": "https://arxiv.org/pdf/2506.14704.pdf", "abs": "https://arxiv.org/abs/2506.14704", "title": "Capacity Matters: a Proof-of-Concept for Transformer Memorization on Real-World Data", "authors": ["Anton Changalidis", "Aki Härmä"], "categories": ["cs.CL"], "comment": "This work has been accepted for publication at the First Workshop on\n  Large Language Model Memorization (L2M2) at ACL 2025, Vienna, Austria", "summary": "This paper studies how the model architecture and data configurations\ninfluence the empirical memorization capacity of generative transformers. The\nmodels are trained using synthetic text datasets derived from the Systematized\nNomenclature of Medicine (SNOMED) knowledge graph: triplets, representing\nstatic connections, and sequences, simulating complex relation patterns. The\nresults show that embedding size is the primary determinant of learning speed\nand capacity, while additional layers provide limited benefits and may hinder\nperformance on simpler datasets. Activation functions play a crucial role, and\nSoftmax demonstrates greater stability and capacity. Furthermore, increasing\nthe complexity of the data set seems to improve the final memorization. These\ninsights improve our understanding of transformer memory mechanisms and provide\na framework for optimizing model design with structured real-world data.", "AI": {"tldr": "This paper examines how model architecture and data configurations affect the memorization capacity of generative transformers, revealing key factors like embedding size and activation functions in optimizing model performance.", "motivation": "To understand and optimize the memorization mechanisms of generative transformers in the context of structured real-world data.", "method": "The models are trained on synthetic text datasets derived from the SNOMED knowledge graph, using triplets and sequences to evaluate empirical memorization capacity.", "result": "Embedding size significantly influences learning speed and capacity; additional layers have limited advantages and can hinder simpler datasets. Softmax is highlighted for its stability and capacity in performance.", "conclusion": "The findings provide insights into transformer memory mechanisms and a framework for optimizing model design, suggesting that more complex datasets enhance memorization capabilities.", "key_contributions": ["Identification of embedding size as a key factor in learning speed and capacity", "Demonstration of limited benefits from additional layers", "Insights on the role of activation functions, particularly Softmax, in model performance"], "limitations": "", "keywords": ["generative transformers", "memorization capacity", "model architecture", "SNOMED", "activation functions"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.14731", "pdf": "https://arxiv.org/pdf/2506.14731.pdf", "abs": "https://arxiv.org/abs/2506.14731", "title": "Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning for LLMs", "authors": ["Ring Team", "Bin Hu", "Cai Chen", "Deng Zhao", "Ding Liu", "Dingnan Jin", "Feng Zhu", "Hao Dai", "Hongzhi Luan", "Jia Guo", "Jiaming Liu", "Jiewei Wu", "Jun Mei", "Jun Zhou", "Junbo Zhao", "Junwu Xiong", "Kaihong Zhang", "Kuan Xu", "Lei Liang", "Liang Jiang", "Liangcheng Fu", "Longfei Zheng", "Qiang Gao", "Qing Cui", "Quan Wan", "Shaomian Zheng", "Shuaicheng Li", "Tongkai Yang", "Wang Ren", "Xiaodong Yan", "Xiaopei Wan", "Xiaoyun Feng", "Xin Zhao", "Xinxing Yang", "Xinyu Kong", "Xuemin Yang", "Yang Li", "Yingting Wu", "Yongkang Liu", "Zhankai Xu", "Zhenduo Zhang", "Zhenglei Zhou", "Zhenyu Huang", "Zhiqiang Zhang", "Zihao Wang", "Zujie Wen"], "categories": ["cs.CL", "cs.AI"], "comment": "Technical Report", "summary": "We present Ring-lite, a Mixture-of-Experts (MoE)-based large language model\noptimized via reinforcement learning (RL) to achieve efficient and robust\nreasoning capabilities. Built upon the publicly available Ling-lite model, a\n16.8 billion parameter model with 2.75 billion activated parameters, our\napproach matches the performance of state-of-the-art (SOTA) small-scale\nreasoning models on challenging benchmarks (e.g., AIME, LiveCodeBench,\nGPQA-Diamond) while activating only one-third of the parameters required by\ncomparable models. To accomplish this, we introduce a joint training pipeline\nintegrating distillation with RL, revealing undocumented challenges in MoE RL\ntraining. First, we identify optimization instability during RL training, and\nwe propose Constrained Contextual Computation Policy Optimization(C3PO), a\nnovel approach that enhances training stability and improves computational\nthroughput via algorithm-system co-design methodology. Second, we empirically\ndemonstrate that selecting distillation checkpoints based on entropy loss for\nRL training, rather than validation metrics, yields superior\nperformance-efficiency trade-offs in subsequent RL training. Finally, we\ndevelop a two-stage training paradigm to harmonize multi-domain data\nintegration, addressing domain conflicts that arise in training with mixed\ndataset. We will release the model, dataset, and code.", "AI": {"tldr": "Introduction of Ring-lite, a Mixture-of-Experts model optimized via reinforcement learning for efficient reasoning capabilities.", "motivation": "To enhance reasoning capabilities in large language models while maintaining efficiency in parameter activation and computational throughput.", "method": "A joint training pipeline combining distillation with reinforcement learning (RL), employing a novel approach called Constrained Contextual Computation Policy Optimization (C3PO) for training stability and performance.", "result": "Ring-lite achieves state-of-the-art performance on reasoning tasks while activating only one-third of the parameters compared to similar models.", "conclusion": "The proposed methods improve training stability and efficiency in large language models, addressing challenges in reinforcement learning and multi-domain data integration.", "key_contributions": ["Introduction of C3PO for stable RL training", "Successful integration of distillation checkpoints based on entropy loss", "Two-stage training paradigm for multi-domain data harmonization"], "limitations": "Challenges in optimization stability during RL training and domain conflicts in mixed dataset training.", "keywords": ["Mixture-of-Experts", "large language model", "reinforcement learning", "parameter efficiency", "multi-domain training"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.14758", "pdf": "https://arxiv.org/pdf/2506.14758.pdf", "abs": "https://arxiv.org/abs/2506.14758", "title": "Reasoning with Exploration: An Entropy Perspective", "authors": ["Daixuan Cheng", "Shaohan Huang", "Xuekai Zhu", "Bo Dai", "Wayne Xin Zhao", "Zhenliang Zhang", "Furu Wei"], "categories": ["cs.CL"], "comment": null, "summary": "Balancing exploration and exploitation is a central goal in reinforcement\nlearning (RL). Despite recent advances in enhancing language model (LM)\nreasoning, most methods lean toward exploitation, and increasingly encounter\nperformance plateaus. In this work, we revisit entropy -- a signal of\nexploration in RL -- and examine its relationship to exploratory reasoning in\nLMs. Through empirical analysis, we uncover strong positive correlations\nbetween high-entropy regions and three types of exploratory reasoning actions:\n(1) pivotal tokens that determine or connect logical steps, (2) reflective\nactions such as self-verification and correction, and (3) rare behaviors\nunder-explored by the base LMs. Motivated by this, we introduce a minimal\nmodification to standard RL with only one line of code: augmenting the\nadvantage function with an entropy-based term. Unlike traditional\nmaximum-entropy methods which encourage exploration by promoting uncertainty,\nwe encourage exploration by promoting longer and deeper reasoning chains.\nNotably, our method achieves significant gains on the Pass@K metric -- an\nupper-bound estimator of LM reasoning capabilities -- even when evaluated with\nextremely large K values, pushing the boundaries of LM reasoning.", "AI": {"tldr": "This paper proposes a minimal modification to standard reinforcement learning that enhances exploratory reasoning in language models by incorporating an entropy-based term into the advantage function, resulting in significant performance improvements.", "motivation": "To address the performance plateaus in language models due to over-exploitation in reinforcement learning, we revisit the concept of entropy as a signal for exploration.", "method": "We introduce a modification to standard RL that adds an entropy-based term to the advantage function, promoting longer and deeper reasoning chains instead of merely increasing uncertainty.", "result": "The proposed method shows significant performance gains measured by the Pass@K metric, especially with very large K values, enhancing LM reasoning capabilities.", "conclusion": "Incorporating entropy into RL helps in balancing exploration and exploitation, yielding substantial improvements in LM reasoning capabilities.", "key_contributions": ["Introduces an entropy-based term to RL advantage functions to enhance exploration in LM reasoning.", "Identifies strong correlations between exploratory reasoning and high-entropy regions in outputs.", "Achieves significant performance improvements in LM capabilities, particularly on the Pass@K metric."], "limitations": "", "keywords": ["Reinforcement Learning", "Language Models", "Exploratory Reasoning", "Entropy", "AI"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.14761", "pdf": "https://arxiv.org/pdf/2506.14761.pdf", "abs": "https://arxiv.org/abs/2506.14761", "title": "From Bytes to Ideas: Language Modeling with Autoregressive U-Nets", "authors": ["Mathurin Videau", "Badr Youbi Idrissi", "Alessandro Leite", "Marc Schoenauer", "Olivier Teytaud", "David Lopez-Paz"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Tokenization imposes a fixed granularity on the input text, freezing how a\nlanguage model operates on data and how far in the future it predicts. Byte\nPair Encoding (BPE) and similar schemes split text once, build a static\nvocabulary, and leave the model stuck with that choice. We relax this rigidity\nby introducing an autoregressive U-Net that learns to embed its own tokens as\nit trains. The network reads raw bytes, pools them into words, then pairs of\nwords, then up to 4 words, giving it a multi-scale view of the sequence. At\ndeeper stages, the model must predict further into the future -- anticipating\nthe next few words rather than the next byte -- so deeper stages focus on\nbroader semantic patterns while earlier stages handle fine details. When\ncarefully tuning and controlling pretraining compute, shallow hierarchies tie\nstrong BPE baselines, and deeper hierarchies have a promising trend. Because\ntokenization now lives inside the model, the same system can handle\ncharacter-level tasks and carry knowledge across low-resource languages.", "AI": {"tldr": "This paper introduces an autoregressive U-Net model that learns to embed tokens dynamically during training, allowing for flexible tokenization and improved performance in various language tasks.", "motivation": "To address the limitations of fixed tokenization methods like BPE that restrict how language models operate on data and predict future tokens.", "method": "The proposed autoregressive U-Net processes raw bytes into a hierarchical structure of tokens, enabling it to make predictions at different granularities, from individual bytes to groups of words.", "result": "The model achieves performance comparable to strong BPE baselines, with deeper hierarchies showing promising trends in broader semantic pattern recognition.", "conclusion": "The dynamic tokenization approach offers advantages for character-level tasks and supports knowledge transfer across low-resource languages.", "key_contributions": ["Introduces a dynamic tokenization method within a U-Net architecture.", "Demonstrates the ability to predict at multiple granularity levels in language tasks.", "Shows potential for improved performance in low-resource language settings."], "limitations": "", "keywords": ["autoregressive U-Net", "dynamic tokenization", "language modeling"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.14767", "pdf": "https://arxiv.org/pdf/2506.14767.pdf", "abs": "https://arxiv.org/abs/2506.14767", "title": "A Variational Framework for Improving Naturalness in Generative Spoken Language Models", "authors": ["Li-Wei Chen", "Takuya Higuchi", "Zakaria Aldeneh", "Ahmed Hussen Abdelaziz", "Alexander Rudnicky"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": "International Conference on Machine Learning (ICML) 2025", "summary": "The success of large language models in text processing has inspired their\nadaptation to speech modeling. However, since speech is continuous and complex,\nit is often discretized for autoregressive modeling. Speech tokens derived from\nself-supervised models (known as semantic tokens) typically focus on the\nlinguistic aspects of speech but neglect prosodic information. As a result,\nmodels trained on these tokens can generate speech with reduced naturalness.\nExisting approaches try to fix this by adding pitch features to the semantic\ntokens. However, pitch alone cannot fully represent the range of paralinguistic\nattributes, and selecting the right features requires careful hand-engineering.\nTo overcome this, we propose an end-to-end variational approach that\nautomatically learns to encode these continuous speech attributes to enhance\nthe semantic tokens. Our approach eliminates the need for manual extraction and\nselection of paralinguistic features. Moreover, it produces preferred speech\ncontinuations according to human raters. Code, samples and models are available\nat https://github.com/b04901014/vae-gslm.", "AI": {"tldr": "This paper presents an end-to-end variational approach to improve speech generation by automatically encoding paralinguistic attributes into semantic tokens, enhancing the naturalness of generated speech without manual feature engineering.", "motivation": "Large language models have demonstrated success in text processing, prompting their application in speech modeling. However, traditional methods often ignore vital prosodic aspects of speech, leading to unnatural output.", "method": "The proposed method employs a variational approach to automatically learn continuous speech attributes, which are integrated with semantic tokens to improve speech generation.", "result": "The approach enhances the quality of generated speech, yielding more natural continuations preferred by human raters compared to existing methods that rely solely on pitch features.", "conclusion": "This end-to-end model offers a more efficient and effective way to generate speech, without the drawbacks of manual feature extraction and selection, leading to more human-like speech outputs.", "key_contributions": ["Introduces an end-to-end variational approach for encoding paralinguistic attributes in speech generation.", "Eliminates the need for manual extraction of speech features.", "Achieves superior speech naturalness as validated by human raters."], "limitations": "", "keywords": ["speech modeling", "large language models", "variational approach", "paralinguistic attributes", "speech generation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2304.03030", "pdf": "https://arxiv.org/pdf/2304.03030.pdf", "abs": "https://arxiv.org/abs/2304.03030", "title": "Compression of enumerations and gain", "authors": ["George Barmpalias", "Xiaoyan Zhang", "Bohua Zhan"], "categories": ["cs.CL", "cs.IT", "math.IT", "math.LO"], "comment": null, "summary": "We study the compressibility of enumerations in the context of Kolmogorov\ncomplexity, focusing on strong and weak forms of compression and their gain:\nthe amount of auxiliary information embedded in the compressed enumeration. The\nexistence of strong compression and weak gainless compression is shown for any\ncomputably enumerable (c.e.) set. The density problem of c.e. sets with respect\nto their prefix complexity is reduced to the question of whether every c.e. set\nis well-compressible, which we study via enumeration games.", "AI": {"tldr": "This paper explores compressibility in enumerations through Kolmogorov complexity, demonstrating forms of compression for computably enumerable sets.", "motivation": "To understand the compressibility of enumerations in the context of Kolmogorov complexity and its implications on auxiliary information.", "method": "The study involves analyzing strong and weak forms of compression for computably enumerable (c.e.) sets and using enumeration games to investigate their properties.", "result": "Demonstrates the existence of strong compression and weak gainless compression for any c.e. set, and relates the density problem of c.e. sets to their compressibility.", "conclusion": "The findings offer insights into the compressibility of c.e. sets, suggesting that the concept of well-compressibility is central to understanding their prefix complexity.", "key_contributions": ["Existence of strong compression for c.e. sets", "Establishment of weak gainless compression", "Reduction of density problem to well-compressibility question"], "limitations": "", "keywords": ["compressibility", "Kolmogorov complexity", "computably enumerable sets"], "importance_score": 2, "read_time_minutes": 10}}
{"id": "2307.10867", "pdf": "https://arxiv.org/pdf/2307.10867.pdf", "abs": "https://arxiv.org/abs/2307.10867", "title": "FigCaps-HF: A Figure-to-Caption Generative Framework and Benchmark with Human Feedback", "authors": ["Ashish Singh", "Ashutosh Singh", "Prateek Agarwal", "Zixuan Huang", "Arpita Singh", "Tong Yu", "Sungchul Kim", "Victor Bursztyn", "Nesreen K. Ahmed", "Puneet Mathur", "Erik Learned-Miller", "Franck Dernoncourt", "Ryan A. Rossi"], "categories": ["cs.CL", "cs.CV", "cs.LG"], "comment": "16 pages, 4 figures. Benchmark Documentation:\n  https://figcapshf.github.io/", "summary": "Captions are crucial for understanding scientific visualizations and\ndocuments. Existing captioning methods for scientific figures rely on\nfigure-caption pairs extracted from documents for training, many of which fall\nshort with respect to metrics like helpfulness, explainability, and\nvisual-descriptiveness [15] leading to generated captions being misaligned with\nreader preferences. To enable the generation of high-quality figure captions,\nwe introduce FigCaps-HF a new framework for figure-caption generation that can\nincorporate domain expert feedback in generating captions optimized for reader\npreferences. Our framework comprises of 1) an automatic method for evaluating\nquality of figure-caption pairs, 2) a novel reinforcement learning with human\nfeedback (RLHF) method to optimize a generative figure-to-caption model for\nreader preferences. We demonstrate the effectiveness of our simple learning\nframework by improving performance over standard fine-tuning across different\ntypes of models. In particular, when using BLIP as the base model, our RLHF\nframework achieves a mean gain of 35.7%, 16.9%, and 9% in ROUGE, BLEU, and\nMeteor, respectively. Finally, we release a large-scale benchmark dataset with\nhuman feedback on figure-caption pairs to enable further evaluation and\ndevelopment of RLHF techniques for this problem.", "AI": {"tldr": "A new framework, FigCaps-HF, for generating high-quality figure captions in scientific documents using reinforcement learning with human feedback (RLHF) to optimize for reader preferences.", "motivation": "Existing methods for generating figure captions from scientific documents often produce subpar results concerning helpfulness, explainability, and visual-descriptiveness, leading to misalignment with reader preferences.", "method": "The FigCaps-HF framework includes an automatic evaluation method for figure-caption quality and a RLHF approach to enhance generative models based on expert feedback.", "result": "When BLIP is used as the base model, the RLHF framework shows a significant improvement: 35.7% in ROUGE, 16.9% in BLEU, and 9% in Meteor over standard fine-tuning methods.", "conclusion": "The framework demonstrates improved caption generation performance and provides a benchmark dataset with human feedback, paving the way for better evaluation of RLHF techniques in figure-caption generation.", "key_contributions": ["Introduction of FigCaps-HF framework for figure-caption generation.", "Development of an automatic evaluation method for figure-caption pairs.", "Release of a large-scale benchmark dataset for further research."], "limitations": "", "keywords": ["figure captioning", "human feedback", "reinforcement learning", "benchmark dataset"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2312.16490", "pdf": "https://arxiv.org/pdf/2312.16490.pdf", "abs": "https://arxiv.org/abs/2312.16490", "title": "Exploring news intent and its application: A theory-driven approach", "authors": ["Zhengjia Wang", "Danding Wang", "Qiang Sheng", "Juan Cao", "Siyuan Ma", "Haonan Cheng"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "Accepted to Information Processing & Management. DOI:\n  https://doi.org/10.1016/j.ipm.2025.104229", "summary": "Understanding the intent behind information is crucial. However, news as a\nmedium of public discourse still lacks a structured investigation of perceived\nnews intent and its application. To advance this field, this paper reviews\ninterdisciplinary studies on intentional action and introduces a conceptual\ndeconstruction-based news intent understanding framework (NINT). This framework\nidentifies the components of intent, facilitating a structured representation\nof news intent and its applications. Building upon NINT, we contribute a new\nintent perception dataset. Moreover, we investigate the potential of intent\nassistance on news-related tasks, such as significant improvement (+2.2% macF1)\nin the task of fake news detection. We hope that our findings will provide\nvaluable insights into action-based intent cognition and computational social\nscience.", "AI": {"tldr": "This paper introduces a framework for understanding news intent and presents a dataset to aid in news-related tasks.", "motivation": "The paper addresses the lack of structured investigations into perceived news intent and its importance in public discourse.", "method": "The authors review interdisciplinary studies on intentional action and propose a deconstruction-based framework called the News Intent Understanding Framework (NINT), which includes a new intent perception dataset.", "result": "The study shows that using the intent framework can significantly enhance performance in tasks such as fake news detection by +2.2% macF1.", "conclusion": "The findings aim to provide insights into intent cognition and computational social science applications, particularly in news dissemination and interpretation.", "key_contributions": ["Introduction of the News Intent Understanding Framework (NINT).", "Development of a new intent perception dataset.", "Demonstration of improved fake news detection using intent assistance."], "limitations": "", "keywords": ["news intent", "intent perception", "fake news detection", "computational social science", "action-based cognition"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2402.10735", "pdf": "https://arxiv.org/pdf/2402.10735.pdf", "abs": "https://arxiv.org/abs/2402.10735", "title": "Assessing the Reasoning Capabilities of LLMs in the context of Evidence-based Claim Verification", "authors": ["John Dougrez-Lewis", "Mahmud Elahi Akhter", "Federico Ruggeri", "Sebastian Löbbers", "Yulan He", "Maria Liakata"], "categories": ["cs.CL"], "comment": "First two authors contributed equally to this work. 25 pages, 3\n  figure", "summary": "Although LLMs have shown great performance on Mathematics and Coding related\nreasoning tasks, the reasoning capabilities of LLMs regarding other forms of\nreasoning are still an open problem. Here, we examine the issue of reasoning\nfrom the perspective of claim verification. We propose a framework designed to\nbreak down any claim paired with evidence into atomic reasoning types that are\nnecessary for verification. We use this framework to create RECV, the first\nclaim verification benchmark, incorporating real-world claims, to assess the\ndeductive and abductive reasoning capabilities of LLMs. The benchmark comprises\nof three datasets, covering reasoning problems of increasing complexity. We\nevaluate three state-of-the-art proprietary LLMs under multiple prompt\nsettings. Our results show that while LLMs can address deductive reasoning\nproblems, they consistently fail in cases of abductive reasoning. Moreover, we\nobserve that enhancing LLMs with rationale generation is not always beneficial.\nNonetheless, we find that generated rationales are semantically similar to\nthose provided by humans, especially in deductive reasoning cases.", "AI": {"tldr": "The paper examines the reasoning capabilities of LLMs in claim verification, proposing a framework and benchmark (RECV) to evaluate deductive and abductive reasoning across multiple datasets.", "motivation": "To address the open problem of LLM reasoning capabilities beyond Mathematics and Coding by focusing on claim verification.", "method": "A framework is proposed to break down claims and evidence into atomic reasoning types, leading to the creation of the RECV benchmark for evaluating LLMs.", "result": "LLMs perform well on deductive reasoning but fail in abductive reasoning, and enhancing LLMs with generated rationales does not always yield benefits. Generated rationales are semantically similar to human ones in deductive cases.", "conclusion": "While LLMs can handle deductive reasoning, improvements are needed for abductive reasoning; the utility of rationale generation varies by context.", "key_contributions": ["Introduction of a claim verification framework for LLMs", "Creation of the RECV benchmark with real-world claims", "Evaluation of deductive vs. abductive reasoning capabilities in LLMs"], "limitations": "The effectiveness of the proposed framework and benchmark in different domains is yet to be fully tested.", "keywords": ["LLM", "claim verification", "deductive reasoning", "abductive reasoning", "RECV benchmark"], "importance_score": 8, "read_time_minutes": 25}}
{"id": "2406.13677", "pdf": "https://arxiv.org/pdf/2406.13677.pdf", "abs": "https://arxiv.org/abs/2406.13677", "title": "Leveraging Large Language Models to Measure Gender Representation Bias in Gendered Language Corpora", "authors": ["Erik Derner", "Sara Sansalvador de la Fuente", "Yoan Gutiérrez", "Paloma Moreda", "Nuria Oliver"], "categories": ["cs.CL", "cs.CY"], "comment": "Accepted for presentation at the 6th Workshop on Gender Bias in\n  Natural Language Processing (GeBNLP) at ACL 2025", "summary": "Large language models (LLMs) often inherit and amplify social biases embedded\nin their training data. A prominent social bias is gender bias. In this regard,\nprior work has mainly focused on gender stereotyping bias - the association of\nspecific roles or traits with a particular gender - in English and on\nevaluating gender bias in model embeddings or generated outputs. In contrast,\ngender representation bias - the unequal frequency of references to individuals\nof different genders - in the training corpora has received less attention. Yet\nsuch imbalances in the training data constitute an upstream source of bias that\ncan propagate and intensify throughout the entire model lifecycle. To fill this\ngap, we propose a novel LLM-based method to detect and quantify gender\nrepresentation bias in LLM training data in gendered languages, where\ngrammatical gender challenges the applicability of methods developed for\nEnglish. By leveraging the LLMs' contextual understanding, our approach\nautomatically identifies and classifies person-referencing words in gendered\nlanguage corpora. Applied to four Spanish-English benchmarks and five Valencian\ncorpora, our method reveals substantial male-dominant imbalances. We show that\nsuch biases in training data affect model outputs, but can surprisingly be\nmitigated leveraging small-scale training on datasets that are biased towards\nthe opposite gender. Our findings highlight the need for corpus-level gender\nbias analysis in multilingual NLP. We make our code and data publicly\navailable.", "AI": {"tldr": "This paper introduces a novel method for detecting and quantifying gender representation bias in LLM training data for gendered languages, revealing significant male-dominant imbalances and demonstrating that small-scale training on datasets biased towards the opposite gender can mitigate such biases.", "motivation": "The study addresses the often overlooked issue of gender representation bias in LLM training data, emphasizing how imbalances can propagate through the model lifecycle.", "method": "The proposed method leverages the contextual understanding of LLMs to automatically identify and classify person-referencing words within gendered language corpora.", "result": "The analysis on four Spanish-English benchmarks and five Valencian corpora reveals substantial male-dominant biases in training data, which subsequently affect model outputs.", "conclusion": "The results underline the necessity of conducting corpus-level gender bias analysis in multilingual NLP, stressing the potential for mitigation through targeted training.", "key_contributions": ["Novel LLM-based method for detecting gender representation bias", "Reveals male-dominant imbalances in Spanish-English and Valencian corpora", "Findings advocate for corpus-level bias analysis in multilingual NLP"], "limitations": "", "keywords": ["gender bias", "large language models", "NLP", "multilingual", "corpus analysis"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2407.07313", "pdf": "https://arxiv.org/pdf/2407.07313.pdf", "abs": "https://arxiv.org/abs/2407.07313", "title": "ETM: Modern Insights into Perspective on Text-to-SQL Evaluation in the Age of Large Language Models", "authors": ["Benjamin G. Ascoli", "Yasoda Sai Ram Kandikonda", "Jinho D. Choi"], "categories": ["cs.CL"], "comment": null, "summary": "The task of Text-to-SQL enables anyone to retrieve information from SQL\ndatabases using natural language. While this task has made substantial\nprogress, the two primary evaluation metrics - Execution Accuracy (EXE) and\nExact Set Matching Accuracy (ESM) - suffer from inherent limitations that can\nmisrepresent performance. Specifically, ESM's rigid matching overlooks\nsemantically correct but stylistically different queries, whereas EXE can\noverestimate correctness by ignoring structural errors that yield correct\noutputs. These shortcomings become especially problematic when assessing\noutputs from large language model (LLM)-based approaches without fine-tuning,\nwhich vary more in style and structure compared to their fine-tuned\ncounterparts. Thus, we introduce a new metric, Enhanced Tree Matching (ETM),\nwhich mitigates these issues by comparing queries using both syntactic and\nsemantic elements. Through evaluating nine LLM-based models, we show that EXE\nand ESM can produce false positive and negative rates as high as 23.0% and\n28.9%, while ETM reduces these rates to 0.3% and 2.7%, respectively. We release\nour ETM script as open source, offering the community a more robust and\nreliable approach to evaluating Text-to-SQL.", "AI": {"tldr": "Introduction of a new metric, Enhanced Tree Matching (ETM), for evaluating Text-to-SQL tasks.", "motivation": "Current evaluation metrics for Text-to-SQL, namely Execution Accuracy (EXE) and Exact Set Matching Accuracy (ESM), have significant limitations in accurately assessing performance, especially for LLM-based models.", "method": "The paper presents Enhanced Tree Matching (ETM), which considers both syntactic and semantic elements in query comparison to overcome the shortcomings of ESM and EXE.", "result": "ETM demonstrates significantly reduced false positive and negative rates in evaluation compared to EXE and ESM, with rates of 0.3% and 2.7% versus their counterparts reaching as high as 23.0% and 28.9%.", "conclusion": "The introduction of ETM provides the community with a more effective metric for evaluating Text-to-SQL systems, facilitating better performance assessment of models, particularly those leveraging LLMs.", "key_contributions": ["Introduction of Enhanced Tree Matching (ETM) metric for Text-to-SQL evaluation", "Demonstration of significant improvements in evaluation accuracy over existing metrics", "Release of ETM script as open source for community use"], "limitations": "", "keywords": ["Text-to-SQL", "Natural Language Processing", "Large Language Models", "Evaluation Metrics", "Enhanced Tree Matching"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2410.07819", "pdf": "https://arxiv.org/pdf/2410.07819.pdf", "abs": "https://arxiv.org/abs/2410.07819", "title": "Uncovering Overfitting in Large Language Model Editing", "authors": ["Mengqi Zhang", "Xiaotian Ye", "Qiang Liu", "Pengjie Ren", "Shu Wu", "Zhumin Chen"], "categories": ["cs.CL"], "comment": "ICLR 2025", "summary": "Knowledge editing has been proposed as an effective method for updating and\ncorrecting the internal knowledge of Large Language Models (LLMs). However,\nexisting editing methods often struggle with complex tasks, such as multi-hop\nreasoning. In this paper, we identify and investigate the phenomenon of Editing\nOverfit, where edited models assign disproportionately high probabilities to\nthe edit target, hindering the generalization of new knowledge in complex\nscenarios. We attribute this issue to the current editing paradigm, which\nplaces excessive emphasis on the direct correspondence between the input prompt\nand the edit target for each edit sample. To further explore this issue, we\nintroduce a new benchmark, EVOKE (EValuation of Editing Overfit in Knowledge\nEditing), along with fine-grained evaluation metrics. Through comprehensive\nexperiments and analysis, we demonstrate that Editing Overfit is prevalent in\ncurrent editing methods and that common overfitting mitigation strategies are\nineffective in knowledge editing. To overcome this, inspired by LLMs' knowledge\nrecall mechanisms, we propose a new plug-and-play strategy called Learn the\nInference (LTI), which introduce a Multi-stage Inference Constraint module to\nguide the edited models in recalling new knowledge similarly to how unedited\nLLMs leverage knowledge through in-context learning. Extensive experimental\nresults across a wide range of tasks validate the effectiveness of LTI in\nmitigating Editing Overfit.", "AI": {"tldr": "This paper addresses the issue of Editing Overfit in knowledge editing for Large Language Models, proposing a new plug-and-play strategy, Learn the Inference, to improve knowledge recall.", "motivation": "The motivation is to improve knowledge editing methods for Large Language Models (LLMs), particularly in complex tasks where existing methods struggle due to Editing Overfit.", "method": "The authors introduce the EVOKE benchmark for evaluating Editing Overfit and propose the Learn the Inference (LTI) strategy, which utilizes a Multi-stage Inference Constraint module to enhance knowledge recall.", "result": "Experiments show that Editing Overfit is common in current editing methods, and the proposed LTI strategy significantly mitigates this issue across various tasks.", "conclusion": "The paper concludes that existing overfitting mitigation strategies fail in knowledge editing, but the LTI approach effectively improves the generalization of new knowledge.", "key_contributions": ["Introduction of the term Editing Overfit related to knowledge editing in LLMs.", "Development of the EVOKE benchmark for evaluating knowledge editing performance.", "Proposal of the Learn the Inference (LTI) strategy to enhance knowledge recall in edited models."], "limitations": "The proposed methods need further evaluation across more complex scenarios and diverse LLM architectures.", "keywords": ["Knowledge Editing", "Large Language Models", "Editing Overfit", "Inference Constraints", "Machine Learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2410.16464", "pdf": "https://arxiv.org/pdf/2410.16464.pdf", "abs": "https://arxiv.org/abs/2410.16464", "title": "Beyond Browsing: API-Based Web Agents", "authors": ["Yueqi Song", "Frank Xu", "Shuyan Zhou", "Graham Neubig"], "categories": ["cs.CL", "cs.MA"], "comment": "20 pages, 8 figures", "summary": "Web browsers are a portal to the internet, where much of human activity is\nundertaken. Thus, there has been significant research work in AI agents that\ninteract with the internet through web browsing. However, there is also another\ninterface designed specifically for machine interaction with online content:\napplication programming interfaces (APIs). In this paper we ask -- what if we\nwere to take tasks traditionally tackled by Browsing Agents, and give AI agents\naccess to APIs? To do so, we propose two varieties of agents: (1) an\nAPI-calling agent that attempts to perform online tasks through APIs only,\nsimilar to traditional coding agents, and (2) a Hybrid Agent that can interact\nwith online data through both web browsing and APIs. In experiments on\nWebArena, a widely-used and realistic benchmark for web navigation tasks, we\nfind that API-Based Agents outperform web Browsing Agents. Hybrid Agents\nout-perform both others nearly uniformly across tasks, resulting in a more than\n24.0% absolute improvement over web browsing alone, achieving a success rate of\n38.9%, the SOTA performance among task-agnostic agents. These results strongly\nsuggest that when APIs are available, they present an attractive alternative to\nrelying on web browsing alone.", "AI": {"tldr": "This paper explores the performance of AI agents that utilize APIs as opposed to traditional web browsing, proposing two types of agents, and demonstrating superior results for API-based interactions.", "motivation": "The study investigates the potential of AI agents to access online content via APIs rather than just through web browsers, aiming to improve task performance in digital environments.", "method": "The research introduces two types of agents: API-calling agents, which perform tasks solely through APIs, and Hybrid Agents, which use both browsing and APIs. Experiments were conducted on WebArena, a benchmark for web navigation tasks.", "result": "API-Based Agents outperformed Browsing Agents, while Hybrid Agents consistently achieved better results than both, with a success rate of 38.9%—marking a 24% improvement over browsing alone.", "conclusion": "The findings indicate that leveraging APIs can significantly enhance task success rates for AI agents, suggesting that APIs should be prioritized when available instead of relying solely on web browsing.", "key_contributions": ["Proposed API-calling and Hybrid Agents for online task execution", "Demonstrated superior performance of API-based interactions over traditional web browsing", "Achieved state-of-the-art performance metrics among task-agnostic agents."], "limitations": "", "keywords": ["AI agents", "APIs", "web browsing", "hybrid agents", "WebArena"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2410.18653", "pdf": "https://arxiv.org/pdf/2410.18653.pdf", "abs": "https://arxiv.org/abs/2410.18653", "title": "Towards Better Open-Ended Text Generation: A Multicriteria Evaluation Framework", "authors": ["Esteban Garces Arias", "Hannah Blocher", "Julian Rodemann", "Meimingwei Li", "Christian Heumann", "Matthias Aßenmacher"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted at the $GEM^2$ Workshop (co-located with ACL 2025)", "summary": "Open-ended text generation has become a prominent task in natural language\nprocessing due to the rise of powerful (large) language models. However,\nevaluating the quality of these models and the employed decoding strategies\nremains challenging due to trade-offs among widely used metrics such as\ncoherence, diversity, and perplexity. This paper addresses the specific problem\nof multicriteria evaluation for open-ended text generation, proposing novel\nmethods for both relative and absolute rankings of decoding methods.\nSpecifically, we employ benchmarking approaches based on partial orderings and\npresent a new summary metric to balance existing automatic indicators,\nproviding a more holistic evaluation of text generation quality. Our\nexperiments demonstrate that the proposed approaches offer a robust way to\ncompare decoding strategies and serve as valuable tools to guide model\nselection for open-ended text generation tasks. We suggest future directions\nfor improving evaluation methodologies in text generation and make our code,\ndatasets, and models publicly available.", "AI": {"tldr": "This paper proposes novel methods for multicriteria evaluation of open-ended text generation, addressing challenges in decoding strategy assessment.", "motivation": "There is a need for a more effective evaluation framework for open-ended text generation, due to the limitations of existing metrics in assessing quality.", "method": "The paper introduces benchmarking approaches based on partial orderings and a new summary metric to evaluate decoding methods holistically.", "result": "The proposed evaluation strategies allow for robust comparisons of decoding approaches, aiding in model selection for text generation tasks.", "conclusion": "The authors recommend further improvements in evaluation methodologies and share their resources publicly for community use.", "key_contributions": ["Novel multicriteria evaluation methods for text generation", "Benchmarking approaches using partial orderings", "A holistic summary metric for decoding strategies"], "limitations": "", "keywords": ["text generation", "evaluation metrics", "decoding strategies", "natural language processing", "large language models"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2411.19563", "pdf": "https://arxiv.org/pdf/2411.19563.pdf", "abs": "https://arxiv.org/abs/2411.19563", "title": "Ensemble Watermarks for Large Language Models", "authors": ["Georg Niess", "Roman Kern"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 main conference. This article extends our\n  earlier work arXiv:2405.08400 by introducing an ensemble of stylometric\n  watermarking features and alternative experimental analysis. Code and data\n  are available at http://github.com/CommodoreEU/ensemble-watermark", "summary": "As large language models (LLMs) reach human-like fluency, reliably\ndistinguishing AI-generated text from human authorship becomes increasingly\ndifficult. While watermarks already exist for LLMs, they often lack flexibility\nand struggle with attacks such as paraphrasing. To address these issues, we\npropose a multi-feature method for generating watermarks that combines multiple\ndistinct watermark features into an ensemble watermark. Concretely, we combine\nacrostica and sensorimotor norms with the established red-green watermark to\nachieve a 98% detection rate. After a paraphrasing attack, the performance\nremains high with 95% detection rate. In comparison, the red-green feature\nalone as a baseline achieves a detection rate of 49% after paraphrasing. The\nevaluation of all feature combinations reveals that the ensemble of all three\nconsistently has the highest detection rate across several LLMs and watermark\nstrength settings. Due to the flexibility of combining features in the\nensemble, various requirements and trade-offs can be addressed. Additionally,\nthe same detection function can be used without adaptations for all ensemble\nconfigurations. This method is particularly of interest to facilitate\naccountability and prevent societal harm.", "AI": {"tldr": "This paper presents a novel method for AI-generated text watermarking that combines multiple features, achieving high detection rates even after paraphrasing attacks.", "motivation": "Reliable distinction between AI-generated and human-authored text is vital as LLMs achieve human-like fluency; existing watermarks are inflexible and vulnerable to paraphrasing.", "method": "A multi-feature ensemble watermarking method combines acrostica, sensorimotor norms, and red-green watermarking techniques to enhance detection capabilities.", "result": "Achieved a 98% detection rate across several LLMs with the ensemble watermark, maintaining 95% detection rate after paraphrasing attacks.", "conclusion": "The ensemble method offers flexibility for addressing various detection requirements while ensuring high performance across configurations.", "key_contributions": ["Proposed an ensemble method for watermarking AI-generated text.", "Demonstrated high detection rates before and after paraphrasing attacks.", "Provided flexibility in addressing different watermarking requirements."], "limitations": "", "keywords": ["watermarking", "large language models", "text detection", "accountability", "machine learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2412.04726", "pdf": "https://arxiv.org/pdf/2412.04726.pdf", "abs": "https://arxiv.org/abs/2412.04726", "title": "BESSTIE: A Benchmark for Sentiment and Sarcasm Classification for Varieties of English", "authors": ["Dipankar Srirag", "Aditya Joshi", "Jordan Painter", "Diptesh Kanojia"], "categories": ["cs.CL", "cs.AI"], "comment": "Findings of ACL: ACL 2025", "summary": "Despite large language models (LLMs) being known to exhibit bias against\nnon-standard language varieties, there are no known labelled datasets for\nsentiment analysis of English. To address this gap, we introduce BESSTIE, a\nbenchmark for sentiment and sarcasm classification for three varieties of\nEnglish: Australian (en-AU), Indian (en-IN), and British (en-UK). We collect\ndatasets for these language varieties using two methods: location-based for\nGoogle Places reviews, and topic-based filtering for Reddit comments. To assess\nwhether the dataset accurately represents these varieties, we conduct two\nvalidation steps: (a) manual annotation of language varieties and (b) automatic\nlanguage variety prediction. Native speakers of the language varieties manually\nannotate the datasets with sentiment and sarcasm labels. We perform an\nadditional annotation exercise to validate the reliance of the annotated\nlabels. Subsequently, we fine-tune nine LLMs (representing a range of\nencoder/decoder and mono/multilingual models) on these datasets, and evaluate\ntheir performance on the two tasks. Our results show that the models\nconsistently perform better on inner-circle varieties (i.e., en-AU and en-UK),\nin comparison with en-IN, particularly for sarcasm classification. We also\nreport challenges in cross-variety generalisation, highlighting the need for\nlanguage variety-specific datasets such as ours. BESSTIE promises to be a\nuseful evaluative benchmark for future research in equitable LLMs, specifically\nin terms of language varieties. The BESSTIE dataset is publicly available at:\nhttps://huggingface.co/ datasets/unswnlporg/BESSTIE.", "AI": {"tldr": "BESSTIE is a benchmark dataset for sentiment and sarcasm classification across three English varieties: Australian, Indian, and British, aimed at improving LLM fairness and performance across diverse language varieties.", "motivation": "To address the lack of labelled datasets for sentiment analysis in different English varieties, aiming to enhance LLM performance and fairness across these varieties.", "method": "We collected datasets using location-based scraping from Google Places reviews and topic-based filtering from Reddit comments, followed by manual and automatic validation by native speakers.", "result": "Our study reveals that models perform better on inner-circle English varieties (en-AU, en-UK) than on en-IN, especially in sarcasm classification, thus revealing challenges in cross-variety generalization.", "conclusion": "BESSTIE provides a necessary benchmark for future research on equitable LLMs, highlighting the importance of variety-specific datasets for improving model performance across diverse English dialects.", "key_contributions": ["Introduction of the BESSTIE benchmark dataset for English sentiment and sarcasm classification.", "Demonstration of LLM performance disparity across different English varieties.", "Insights into the necessity for language variety-specific datasets for improved model training."], "limitations": "Challenges in cross-variety generalization were noted, indicating a need for more diverse data sources and further validation.", "keywords": ["sentiment analysis", "language variety", "LLM", "bias", "sarcasm classification"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2412.05693", "pdf": "https://arxiv.org/pdf/2412.05693.pdf", "abs": "https://arxiv.org/abs/2412.05693", "title": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache Compression", "authors": ["Michael R. Metel", "Boxing Chen", "Mehdi Rezagholizadeh"], "categories": ["cs.CL"], "comment": null, "summary": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy.", "AI": {"tldr": "This paper presents a novel approach to KV cache management that compresses the cache during input processing to enhance throughput in AI inference.", "motivation": "To address the inefficiencies in key-value (KV) caching for improved inference speed and GPU memory utilization in settings with long input contexts.", "method": "The authors propose an eviction policy that compresses the KV cache during the input processing phase to allow for larger batch sizes.", "result": "This method enables significantly higher throughput for token generation while preserving the original accuracy of the model.", "conclusion": "Compressing the KV cache during input processing enhances inference performance in limited GPU memory contexts without sacrificing accuracy.", "key_contributions": ["Introduction of a dual-phase KV cache compression technique", "Demonstration of increased throughput with larger batch sizes", "Validation of model accuracy maintenance alongside performance improvements."], "limitations": "", "keywords": ["KV cache", "inference", "GPU memory", "batch size", "token generation"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2412.14533", "pdf": "https://arxiv.org/pdf/2412.14533.pdf", "abs": "https://arxiv.org/abs/2412.14533", "title": "ClusterChat: Multi-Feature Search for Corpus Exploration", "authors": ["Ashish Chouhan", "Saifeldin Mandour", "Michael Gertz"], "categories": ["cs.CL"], "comment": "5 pages, 1 table, 1 figure, Accepted to SIGIR Demo Paper Track 2025", "summary": "Exploring large-scale text corpora presents a significant challenge in\nbiomedical, finance, and legal domains, where vast amounts of documents are\ncontinuously published. Traditional search methods, such as keyword-based\nsearch, often retrieve documents in isolation, limiting the user's ability to\neasily inspect corpus-wide trends and relationships. We present ClusterChat\n(The demo video and source code are available at:\nhttps://github.com/achouhan93/ClusterChat), an open-source system for corpus\nexploration that integrates cluster-based organization of documents using\ntextual embeddings with lexical and semantic search, timeline-driven\nexploration, and corpus and document-level question answering (QA) as\nmulti-feature search capabilities. We validate the system with two case studies\non a four million abstract PubMed dataset, demonstrating that ClusterChat\nenhances corpus exploration by delivering context-aware insights while\nmaintaining scalability and responsiveness on large-scale document collections.", "AI": {"tldr": "ClusterChat is an open-source system designed for large-scale text corpus exploration, particularly useful in biomedical, finance, and legal domains.", "motivation": "Traditional keyword-based search methods fail to facilitate effective exploration of vast document collections, necessitating innovative solutions for better insight extraction.", "method": "ClusterChat integrates cluster-based document organization using text embeddings, along with features like lexical and semantic search, timeline-driven exploration, and multi-feature question answering (QA).", "result": "Validation through case studies on a four million abstract PubMed dataset shows that ClusterChat significantly enhances corpus exploration by offering context-aware insights while being scalable and responsive.", "conclusion": "ClusterChat addresses key limitations in existing corpus exploration methods, empowering users to better navigate and understand large document collections.", "key_contributions": ["Open-source design of ClusterChat for corpus exploration", "Integration of cluster-based organization and advanced search capabilities", "Demonstration of effectiveness through case studies on a large dataset"], "limitations": "", "keywords": ["Corpus Exploration", "Textual Embeddings", "Question Answering", "Cluster-based Organization", "Scalability"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2501.05478", "pdf": "https://arxiv.org/pdf/2501.05478.pdf", "abs": "https://arxiv.org/abs/2501.05478", "title": "Language and Planning in Robotic Navigation: A Multilingual Evaluation of State-of-the-Art Models", "authors": ["Malak Mansour", "Ahmed Aly", "Bahey Tharwat", "Sarim Hashmi", "Dong An", "Ian Reid"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.RO"], "comment": "This work has been accepted for presentation at LM4Plan@AAAI'25. For\n  more details, please check: https://llmforplanning.github.io/", "summary": "Large Language Models (LLMs) such as GPT-4, trained on huge amount of\ndatasets spanning multiple domains, exhibit significant reasoning,\nunderstanding, and planning capabilities across various tasks. This study\npresents the first-ever work in Arabic language integration within the\nVision-and-Language Navigation (VLN) domain in robotics, an area that has been\nnotably underexplored in existing research. We perform a comprehensive\nevaluation of state-of-the-art multi-lingual Small Language Models (SLMs),\nincluding GPT-4o mini, Llama 3 8B, and Phi-3 medium 14B, alongside the\nArabic-centric LLM, Jais. Our approach utilizes the NavGPT framework, a pure\nLLM-based instruction-following navigation agent, to assess the impact of\nlanguage on navigation reasoning through zero-shot sequential action prediction\nusing the R2R dataset. Through comprehensive experiments, we demonstrate that\nour framework is capable of high-level planning for navigation tasks when\nprovided with instructions in both English and Arabic. However, certain models\nstruggled with reasoning and planning in the Arabic language due to inherent\nlimitations in their capabilities, sub-optimal performance, and parsing issues.\nThese findings highlight the importance of enhancing planning and reasoning\ncapabilities in language models for effective navigation, emphasizing this as a\nkey area for further development while also unlocking the potential of\nArabic-language models for impactful real-world applications.", "AI": {"tldr": "This study explores the integration of Arabic language in Vision-and-Language Navigation using Large Language Models, demonstrating the challenges and capabilities of various models in navigation tasks.", "motivation": "The underexplored integration of Arabic language in the Vision-and-Language Navigation domain, particularly in robotics, necessitates evaluation of language models' performance.", "method": "Utilizing the NavGPT framework to assess zero-shot sequential action prediction for navigation, evaluating multilingual Small Language Models (SLMs) including GPT-4o mini, Llama 3 8B, Phi-3 medium 14B, and Arabic-centric LLM Jais.", "result": "The framework showed high-level planning capabilities for navigation tasks with English and Arabic instructions, although some models faced challenges in reasoning and planning in Arabic.", "conclusion": "Enhancements in planning and reasoning capabilities of language models are essential for effective navigation, highlighting the potential of Arabic-language models for practical applications.", "key_contributions": ["First-ever integration of Arabic in VLN for robotics", "Evaluation of state-of-the-art multilingual SLMs", "Identification of limitations for reasoning in Arabic language models"], "limitations": "Certain models struggled with reasoning and planning in Arabic due to inherent limitations and parsing issues.", "keywords": ["Vision-and-Language Navigation", "Large Language Models", "Arabic language processing"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2501.10970", "pdf": "https://arxiv.org/pdf/2501.10970.pdf", "abs": "https://arxiv.org/abs/2501.10970", "title": "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs", "authors": ["Nitay Calderon", "Roi Reichart", "Rotem Dror"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "The \"LLM-as-an-annotator\" and \"LLM-as-a-judge\" paradigms employ Large\nLanguage Models (LLMs) as annotators, judges, and evaluators in tasks\ntraditionally performed by humans. LLM annotations are widely used, not only in\nNLP research but also in fields like medicine, psychology, and social science.\nDespite their role in shaping study results and insights, there is no standard\nor rigorous procedure to determine whether LLMs can replace human annotators.\nIn this paper, we propose a novel statistical procedure, the Alternative\nAnnotator Test (alt-test), that requires only a modest subset of annotated\nexamples to justify using LLM annotations. Additionally, we introduce a\nversatile and interpretable measure for comparing LLM annotators and judges. To\ndemonstrate our procedure, we curated a diverse collection of ten datasets,\nconsisting of language and vision-language tasks, and conducted experiments\nwith six LLMs and four prompting techniques. Our results show that LLMs can\nsometimes replace humans with closed-source LLMs (such as GPT-4o),\noutperforming the open-source LLMs we examine, and that prompting techniques\nyield judges of varying quality. We hope this study encourages more rigorous\nand reliable practices.", "AI": {"tldr": "This paper introduces the Alternative Annotator Test (alt-test), a novel statistical procedure for validating the use of LLM annotations over human annotators in various fields.", "motivation": "To evaluate and establish rigorous standards for using Large Language Models as annotators and judges, particularly in fields such as NLP, medicine, psychology, and social science.", "method": "The authors propose the Alternative Annotator Test (alt-test), which requires a modest subset of annotated examples to assess the validity of LLM annotations. They compare LLM performance using a curated collection of ten datasets across language and vision-language tasks with six LLMs and four prompting techniques.", "result": "Results indicate that closed-source LLMs like GPT-4o can outperform some human annotators, while prompting techniques lead to varying quality levels in LLM judges.", "conclusion": "The study aims to promote more reliable and rigorous practices in using LLMs for annotation and judgment tasks, suggesting that they can in some cases effectively replace human sources.", "key_contributions": ["Introduction of the Alternative Annotator Test (alt-test) for validating LLM annotations.", "Development of a new measure for comparing LLM annotators and judges.", "Empirical demonstration using diverse datasets showcasing LLMs' potential to outperform humans."], "limitations": "The study is limited by its dependency on a subset of annotated examples and may not generalize across all domains.", "keywords": ["Large Language Models", "annotation", "statistical procedure", "human evaluation", "machine learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.11425", "pdf": "https://arxiv.org/pdf/2502.11425.pdf", "abs": "https://arxiv.org/abs/2502.11425", "title": "Counterfactual-Consistency Prompting for Relative Temporal Understanding in Large Language Models", "authors": ["Jongho Kim", "Seung-won Hwang"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 main (short)", "summary": "Despite the advanced capabilities of large language models (LLMs), their\ntemporal reasoning ability remains underdeveloped. Prior works have highlighted\nthis limitation, particularly in maintaining temporal consistency when\nunderstanding events. For example, models often confuse mutually exclusive\ntemporal relations like ``before'' and ``after'' between events and make\ninconsistent predictions. In this work, we tackle the issue of temporal\ninconsistency in LLMs by proposing a novel counterfactual prompting approach.\nOur method generates counterfactual questions and enforces collective\nconstraints, enhancing the model's consistency. We evaluate our method on\nmultiple datasets, demonstrating significant improvements in event ordering for\nexplicit and implicit events and temporal commonsense understanding by\neffectively addressing temporal inconsistencies.", "AI": {"tldr": "This paper addresses the underdeveloped temporal reasoning ability of large language models (LLMs) by proposing a counterfactual prompting method to enhance temporal consistency in understanding events.", "motivation": "LLMs struggle with maintaining temporal consistency, often confusing temporal relations like 'before' and 'after', leading to inconsistent event predictions.", "method": "The authors propose a novel counterfactual prompting approach that generates counterfactual questions and enforces constraints to improve the model's temporal reasoning.", "result": "The method shows significant improvements in event ordering for both explicit and implicit events, as well as in temporal commonsense understanding, on multiple evaluated datasets.", "conclusion": "The counterfactual prompting technique effectively addresses the issue of temporal inconsistencies in LLMs, enabling better event understanding.", "key_contributions": ["Introduction of a counterfactual prompting method for LLMs.", "Demonstration of improved temporal consistency in event prediction.", "Evaluation across multiple datasets showing significant performance gains."], "limitations": "The study is focused on temporal reasoning and does not explore other reasoning capabilities of LLMs.", "keywords": ["large language models", "temporal reasoning", "counterfactual prompting"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.13497", "pdf": "https://arxiv.org/pdf/2502.13497.pdf", "abs": "https://arxiv.org/abs/2502.13497", "title": "Towards Geo-Culturally Grounded LLM Generations", "authors": ["Piyawat Lertvittayakumjorn", "David Kinney", "Vinodkumar Prabhakaran", "Donald Martin Jr.", "Sunipa Dev"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 (main conference)", "summary": "Generative large language models (LLMs) have demonstrated gaps in diverse\ncultural awareness across the globe. We investigate the effect of retrieval\naugmented generation and search-grounding techniques on LLMs' ability to\ndisplay familiarity with various national cultures. Specifically, we compare\nthe performance of standard LLMs, LLMs augmented with retrievals from a bespoke\nknowledge base (i.e., KB grounding), and LLMs augmented with retrievals from a\nweb search (i.e., search grounding) on multiple cultural awareness benchmarks.\nWe find that search grounding significantly improves the LLM performance on\nmultiple-choice benchmarks that test propositional knowledge (e.g., cultural\nnorms, artifacts, and institutions), while KB grounding's effectiveness is\nlimited by inadequate knowledge base coverage and a suboptimal retriever.\nHowever, search grounding also increases the risk of stereotypical judgments by\nlanguage models and fails to improve evaluators' judgments of cultural\nfamiliarity in a human evaluation with adequate statistical power. These\nresults highlight the distinction between propositional cultural knowledge and\nopen-ended cultural fluency when it comes to evaluating LLMs' cultural\nawareness.", "AI": {"tldr": "The study investigates how retrieval augmented generation and search-grounding impact large language models' (LLMs) cultural awareness, revealing search grounding improves propositional knowledge but increases stereotype risks.", "motivation": "To examine gaps in cultural awareness in generative LLMs and evaluate how different grounding techniques influence their performance in this area.", "method": "The paper compares standard LLMs, LLMs augmented with retrievals from a knowledge base, and those with web search retrievals on cultural awareness benchmarks.", "result": "Search grounding significantly enhances LLM performance on benchmarks testing cultural knowledge, but also risks reinforcing stereotypes and does not improve human judges' evaluations of cultural familiarity.", "conclusion": "The study highlights the difference between propositional cultural knowledge and cultural fluency, showing that while search grounding can improve factual knowledge, it may not enhance actual cultural understanding.", "key_contributions": ["Comparison of retrieval augmented generation techniques for LLMs", "Identification of risks associated with search grounding in LLMs", "Clarification of the distinction between propositional knowledge and cultural fluency in LLM evaluations"], "limitations": "KB grounding's effectiveness is constrained by knowledge base coverage and a less effective retriever.", "keywords": ["generative LLMs", "cultural awareness", "retrieval augmented generation", "search grounding", "human evaluation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.14445", "pdf": "https://arxiv.org/pdf/2502.14445.pdf", "abs": "https://arxiv.org/abs/2502.14445", "title": "PredictaBoard: Benchmarking LLM Score Predictability", "authors": ["Lorenzo Pacchiardi", "Konstantinos Voudouris", "Ben Slater", "Fernando Martínez-Plumed", "José Hernández-Orallo", "Lexin Zhou", "Wout Schellaert"], "categories": ["cs.CL", "cs.AI", "stat.ML"], "comment": "Accepted at ACL Findings 2025", "summary": "Despite possessing impressive skills, Large Language Models (LLMs) often fail\nunpredictably, demonstrating inconsistent success in even basic common sense\nreasoning tasks. This unpredictability poses a significant challenge to\nensuring their safe deployment, as identifying and operating within a reliable\n\"safe zone\" is essential for mitigating risks. To address this, we present\nPredictaBoard, a novel collaborative benchmarking framework designed to\nevaluate the ability of score predictors (referred to as assessors) to\nanticipate LLM errors on specific task instances (i.e., prompts) from existing\ndatasets. PredictaBoard evaluates pairs of LLMs and assessors by considering\nthe rejection rate at different tolerance errors. As such, PredictaBoard\nstimulates research into developing better assessors and making LLMs more\npredictable, not only with a higher average performance. We conduct\nillustrative experiments using baseline assessors and state-of-the-art LLMs.\nPredictaBoard highlights the critical need to evaluate predictability alongside\nperformance, paving the way for safer AI systems where errors are not only\nminimised but also anticipated and effectively mitigated. Code for our\nbenchmark can be found at\nhttps://github.com/Kinds-of-Intelligence-CFI/PredictaBoard", "AI": {"tldr": "This paper presents PredictaBoard, a benchmarking framework to evaluate LLM error predictability and improve their safe deployment.", "motivation": "LLMs demonstrate unpredictable errors, which challenges their safe deployment; thus, a method is needed to evaluate and enhance their predictability.", "method": "The framework assesses pairs of LLMs and score predictors (assessors) based on their rejection rates at different error tolerances.", "result": "Illustrative experiments revealed the efficacy of PredictaBoard in stimulating research for better assessors and LLM predictability.", "conclusion": "PredictaBoard emphasizes the importance of anticipating and mitigating LLM errors, moving beyond just performance metrics.", "key_contributions": ["Introduction of PredictaBoard for benchmarking LLM predictability", "Evaluation of LLMs with baseline assessors", "Highlighting the importance of error anticipation in AI systems"], "limitations": "", "keywords": ["Large Language Models", "predictability", "benchmarking", "human-computer interaction", "AI safety"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.15910", "pdf": "https://arxiv.org/pdf/2502.15910.pdf", "abs": "https://arxiv.org/abs/2502.15910", "title": "Modality-Aware Neuron Pruning for Unlearning in Multimodal Large Language Models", "authors": ["Zheyuan Liu", "Guangyao Dou", "Xiangchi Yuan", "Chunhui Zhang", "Zhaoxuan Tan", "Meng Jiang"], "categories": ["cs.CL"], "comment": "ACL 2025 Main Conference", "summary": "Generative models such as Large Language Models (LLMs) and Multimodal Large\nLanguage Models (MLLMs) trained on massive datasets can lead them to memorize\nand inadvertently reveal sensitive information, raising ethical and privacy\nconcerns. While some prior works have explored this issue in the context of\nLLMs, it presents a unique challenge for MLLMs due to the entangled nature of\nknowledge across modalities, making comprehensive unlearning more difficult. To\naddress this challenge, we propose Modality Aware Neuron Unlearning (MANU), a\nnovel unlearning framework for MLLMs designed to selectively clip neurons based\non their relative importance to the targeted forget data, curated for different\nmodalities. Specifically, MANU consists of two stages: important neuron\nselection and selective pruning. The first stage identifies and collects the\nmost influential neurons across modalities relative to the targeted forget\nknowledge, while the second stage is dedicated to pruning those selected\nneurons. MANU effectively isolates and removes the neurons that contribute most\nto the forget data within each modality, while preserving the integrity of\nretained knowledge. Our experiments conducted across various MLLM architectures\nillustrate that MANU can achieve a more balanced and comprehensive unlearning\nin each modality without largely affecting the overall model utility.", "AI": {"tldr": "Proposes MANU, a novel framework for selective neuron unlearning in MLLMs to address privacy concerns by effectively removing sensitive information.", "motivation": "To address the ethical and privacy issues related to the memorization of sensitive information by MLLMs, specifically focusing on the challenge of unlearning due to the entangled nature of knowledge across modalities.", "method": "The framework consists of two stages: important neuron selection that identifies influential neurons related to the sensitive data, and selective pruning that removes these neurons while preserving non-sensitive knowledge.", "result": "Experiments demonstrate that MANU achieves balanced and comprehensive unlearning across different modalities without significantly impacting the model's utility.", "conclusion": "MANU provides a robust solution for the unlearning of sensitive data in MLLMs, highlighting its effectiveness in isolating and removing problematic knowledge while maintaining overall model performance.", "key_contributions": ["Introduction of Modality Aware Neuron Unlearning (MANU) framework for MLLMs", "Two-stage process for identifying and pruning neurons related to sensitive information", "Demonstrated effectiveness across various MLLM architectures"], "limitations": "Further exploration needed on the long-term effects of unlearning and generalizability across different tasks.", "keywords": ["Large Language Models", "Multimodal Learning", "Neuron Unlearning", "Privacy", "Ethics"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.17421", "pdf": "https://arxiv.org/pdf/2502.17421.pdf", "abs": "https://arxiv.org/abs/2502.17421", "title": "LongSpec: Long-Context Lossless Speculative Decoding with Efficient Drafting and Verification", "authors": ["Penghui Yang", "Cunxiao Du", "Fengzhuo Zhang", "Haonan Wang", "Tianyu Pang", "Chao Du", "Bo An"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "As Large Language Models (LLMs) can now process extremely long contexts,\nefficient inference over these extended inputs has become increasingly\nimportant, especially for emerging applications like LLM agents that highly\ndepend on this capability. Speculative decoding (SD) offers a promising\nlossless acceleration technique compared to lossy alternatives such as\nquantization and model cascades. However, most state-of-the-art SD methods are\ntrained on short texts (typically fewer than 4k tokens), making them unsuitable\nfor long-context scenarios. Specifically, adapting these methods to long\ncontexts presents three key challenges: (1) the excessive memory demands posed\nby draft models due to large Key-Value (KV) cache; (2) performance degradation\nresulting from the mismatch between short-context training and long-context\ninference; and (3) inefficiencies in tree attention mechanisms when managing\nlong token sequences. This work introduces LongSpec, a framework that addresses\nthese challenges through three core innovations: a memory-efficient draft model\nwith a constant-sized KV cache; novel position indices that mitigate the\ntraining-inference mismatch; and an attention aggregation strategy that\ncombines fast prefix computation with standard tree attention to enable\nefficient decoding. Experimental results confirm the effectiveness of LongSpec,\nachieving up to a 3.26x speedup over strong Flash Attention baselines across\nfive long-context understanding datasets, as well as a 2.25x reduction in\nwall-clock time on the AIME24 long reasoning task with the QwQ model,\ndemonstrating significant latency improvements for long-context applications.\nThe code is available at https://github.com/sail-sg/LongSpec.", "AI": {"tldr": "LongSpec introduces a framework to enhance speculative decoding for long-context processing in LLMs, achieving significant speed and efficiency improvements.", "motivation": "To improve the efficiency of inference over long contexts in Large Language Models, a crucial requirement for applications like LLM agents.", "method": "Introduces a memory-efficient draft model, novel position indices to reduce training-inference mismatch, and an attention aggregation strategy combining prefix computation with tree attention.", "result": "LongSpec achieves up to a 3.26x speedup over strong Flash Attention baselines and a 2.25x reduction in wall-clock time on the AIME24 long reasoning task.", "conclusion": "LongSpec significantly enhances the performance and speed of long-context understanding tasks, making it suitable for real-world applications.", "key_contributions": ["Memory-efficient draft model with constant-sized KV cache", "Novel position indices for training-inference consistency", "Efficient attention aggregation strategy combining prefix computation and tree attention"], "limitations": "", "keywords": ["large language models", "speculative decoding", "long context", "machine learning", "efficient inference"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.20620", "pdf": "https://arxiv.org/pdf/2502.20620.pdf", "abs": "https://arxiv.org/abs/2502.20620", "title": "Rectifying Belief Space via Unlearning to Harness LLMs' Reasoning", "authors": ["Ayana Niwa", "Masahiro Kaneko", "Kentaro Inui"], "categories": ["cs.CL"], "comment": "Accepted at ACL2025 Findings (long)", "summary": "Large language models (LLMs) can exhibit advanced reasoning yet still\ngenerate incorrect answers. We hypothesize that such errors frequently stem\nfrom spurious beliefs, propositions the model internally considers true but are\nincorrect. To address this, we propose a method to rectify the belief space by\nsuppressing these spurious beliefs while simultaneously enhancing true ones,\nthereby enabling more reliable inferences. Our approach first identifies the\nbeliefs that lead to incorrect or correct answers by prompting the model to\ngenerate textual explanations, using our Forward-Backward Beam Search (FBBS).\nWe then apply unlearning to suppress the identified spurious beliefs and\nenhance the true ones, effectively rectifying the model's belief space.\nEmpirical results on multiple QA datasets and LLMs show that our method\ncorrects previously misanswered questions without harming overall model\nperformance. Furthermore, our approach yields improved generalization on unseen\ndata, suggesting that rectifying a model's belief space is a promising\ndirection for mitigating errors and enhancing overall reliability.", "AI": {"tldr": "This paper introduces a method to correct spurious beliefs in large language models (LLMs) that lead to incorrect answers, improving their reliability and generalization.", "motivation": "To address errors in large language models caused by spurious beliefs that are considered true but are incorrect.", "method": "The method involves identifying incorrect beliefs through textual explanations generated by the model, using a technique called Forward-Backward Beam Search (FBBS), followed by applying unlearning to suppress spurious beliefs while enhancing true ones.", "result": "Empirical results demonstrate that the proposed method corrects misanswered questions on multiple QA datasets without harming model performance and improves generalization on unseen data.", "conclusion": "Rectifying a model's belief space is a promising direction for reducing errors and enhancing reliability in LLMs.", "key_contributions": ["Introduction of a method to rectify spurious beliefs in LLMs", "Utilization of FBBS for identifying beliefs", "Demonstrated improvements in QA performance and generalization"], "limitations": "", "keywords": ["large language models", "spurious beliefs", "belief rectification", "unlearning", "generalization"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2503.04619", "pdf": "https://arxiv.org/pdf/2503.04619.pdf", "abs": "https://arxiv.org/abs/2503.04619", "title": "SynGraph: A Dynamic Graph-LLM Synthesis Framework for Sparse Streaming User Sentiment Modeling", "authors": ["Xin Zhang", "Qiyu Wei", "Yingjie Zhu", "Linhai Zhang", "Deyu Zhou", "Sophia Ananiadou"], "categories": ["cs.CL"], "comment": "Accepted at ACL 2025", "summary": "User reviews on e-commerce platforms exhibit dynamic sentiment patterns\ndriven by temporal and contextual factors. Traditional sentiment analysis\nmethods focus on static reviews, failing to capture the evolving temporal\nrelationship between user sentiment rating and textual content. Sentiment\nanalysis on streaming reviews addresses this limitation by modeling and\npredicting the temporal evolution of user sentiments. However, it suffers from\ndata sparsity, manifesting in temporal, spatial, and combined forms. In this\npaper, we introduce SynGraph, a novel framework designed to address data\nsparsity in sentiment analysis on streaming reviews. SynGraph alleviates data\nsparsity by categorizing users into mid-tail, long-tail, and extreme scenarios\nand incorporating LLM-augmented enhancements within a dynamic graph-based\nstructure. Experiments on real-world datasets demonstrate its effectiveness in\naddressing sparsity and improving sentiment modeling in streaming reviews.", "AI": {"tldr": "This paper presents SynGraph, a framework for improving sentiment analysis on streaming reviews by addressing data sparsity using LLM-augmented dynamic graphs.", "motivation": "To overcome the limitations of traditional sentiment analysis methods that fail to capture the evolving nature of user sentiment in dynamic contexts.", "method": "The SynGraph framework categorizes users into different scenarios (mid-tail, long-tail, extreme) and utilizes LLM-augmented enhancements within a dynamic graph structure to analyze streaming reviews.", "result": "Experiments show that SynGraph effectively addresses data sparsity and enhances sentiment modeling in streaming reviews using real-world datasets.", "conclusion": "SynGraph provides a significant improvement in sentiment analysis for e-commerce platforms by adapting to the dynamic nature of user reviews.", "key_contributions": ["Introduction of SynGraph framework for sentiment analysis on streaming reviews", "Incorporation of LLM-augmented techniques to mitigate data sparsity", "Dynamic graph-based approach that categorizes user sentiment scenarios"], "limitations": "", "keywords": ["sentiment analysis", "streaming reviews", "data sparsity", "LLM", "dynamic graph"], "importance_score": 7, "read_time_minutes": 12}}
{"id": "2503.06926", "pdf": "https://arxiv.org/pdf/2503.06926.pdf", "abs": "https://arxiv.org/abs/2503.06926", "title": "Effect of Selection Format on LLM Performance", "authors": ["Yuchen Han", "Yucheng Wu", "Jeffrey Willard"], "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.ET", "cs.LG"], "comment": null, "summary": "This paper investigates a critical aspect of large language model (LLM)\nperformance: the optimal formatting of classification task options in prompts.\nThrough an extensive experimental study, we compared two selection formats --\nbullet points and plain English -- to determine their impact on model\nperformance. Our findings suggest that presenting options via bullet points\ngenerally yields better results, although there are some exceptions.\nFurthermore, our research highlights the need for continued exploration of\noption formatting to drive further improvements in model performance.", "AI": {"tldr": "Investigates the impact of option formatting in prompts for classification tasks in large language models.", "motivation": "To determine how the formatting of classification task options affects the performance of large language models.", "method": "An experimental study comparing two formats: bullet points and plain English.", "result": "Bullet points generally resulted in better model performance, though exceptions were noted.", "conclusion": "Optimal formatting of options can enhance LLM performance, necessitating further research in this area.", "key_contributions": ["Comparison of bullet point and plain English formats for LLM prompts", "Identification of performance differences based on formatting", "Call for further exploration of prompt formatting"], "limitations": "Limited to two formats; further exploration needed for other options.", "keywords": ["large language models", "prompt formatting", "classification tasks"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.08669", "pdf": "https://arxiv.org/pdf/2503.08669.pdf", "abs": "https://arxiv.org/abs/2503.08669", "title": "SOPBench: Evaluating Language Agents at Following Standard Operating Procedures and Constraints", "authors": ["Zekun Li", "Shinda Huang", "Jiangtian Wang", "Nathan Zhang", "Antonis Antoniades", "Wenyue Hua", "Kaijie Zhu", "Sirui Zeng", "Chi Wang", "William Yang Wang", "Xifeng Yan"], "categories": ["cs.CL", "cs.AI"], "comment": "Code, data, and over 24k agent trajectories are released at\n  https://github.com/Leezekun/SOPBench", "summary": "As language agents increasingly automate critical tasks, their ability to\nfollow domain-specific standard operating procedures (SOPs), policies, and\nconstraints when taking actions and making tool calls becomes essential yet\nremains underexplored. To address this gap, we develop an automated evaluation\npipeline SOPBench with: (1) executable environments containing 167\ntools/functions across seven customer service domains with service-specific\nSOPs and rule-based verifiers, (2) an automated test generation framework\nproducing over 900 verified test cases, and (3) an automated evaluation\nframework to rigorously assess agent adherence from multiple dimensions. Our\napproach transforms each service-specific SOP code program into a directed\ngraph of executable functions and requires agents to call these functions based\non natural language SOP descriptions. The original code serves as oracle\nrule-based verifiers to assess compliance, reducing reliance on manual\nannotations and LLM-based evaluations. We evaluate 18 leading models, and\nresults show the task is challenging even for top-tier models (like GPT-4o,\nClaude-3.7-Sonnet), with variances across domains. Reasoning models like\no4-mini-high show superiority while other powerful models perform less\neffectively (pass rates of 30%-50%), and small models (7B, 8B) perform\nsignificantly worse. Additionally, language agents can be easily jailbroken to\noverlook SOPs and constraints. Code, data, and over 24k agent trajectories are\nreleased at https://github.com/Leezekun/SOPBench.", "AI": {"tldr": "This paper introduces SOPBench, an automated evaluation pipeline for assessing language agents' adherence to domain-specific standard operating procedures (SOPs) across customer service domains.", "motivation": "To evaluate how well language agents comply with domain-specific SOPs and policies in critical task automation, addressing the lack of rigorous assessment tools in this area.", "method": "The approach involves creating executable environments with 167 tools/functions and generating over 900 verified test cases. It uses directed graphs of executable functions based on SOP descriptions to evaluate agent compliance with rule-based verifiers.", "result": "Evaluation of 18 leading models revealed that adherence to SOPs is challenging, with top models showing pass rates of only 30%-50%. Reasoning models performed better, while smaller models fared poorly, revealing a tendency for agents to be jailbroken and ignore constraints.", "conclusion": "The paper demonstrates the need for more robust evaluation of language agents in adherence to SOPs, emphasizing the variability in performance across different models and domains.", "key_contributions": ["Development of SOPBench for automated evaluation in language agents", "Introduction of a directed graph model for executing SOP functions", "Release of substantial resources including code, data, and agent trajectories for further research."], "limitations": "The evaluation's reliance on specific customer service domains may limit generalizability, and there's an identified vulnerability of agents to be jailbroken.", "keywords": ["language agents", "SOP compliance", "automated evaluation", "customer service", "agent performance"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.11593", "pdf": "https://arxiv.org/pdf/2503.11593.pdf", "abs": "https://arxiv.org/abs/2503.11593", "title": "Do Construction Distributions Shape Formal Language Learning In German BabyLMs?", "authors": ["Bastian Bunzeck", "Daniel Duran", "Sina Zarrieß"], "categories": ["cs.CL"], "comment": "Accepted at CoNNL 2025", "summary": "We analyze the influence of utterance-level construction distributions in\nGerman child-directed/child-available speech on the resulting word-level,\nsyntactic and semantic competence (and their underlying learning trajectories)\nin small LMs, which we train on a novel collection of developmentally plausible\nlanguage data for German. We find that trajectories are surprisingly robust for\nmarkedly different distributions of constructions in the training data, which\nhave little effect on final accuracies and almost no effect on global learning\ntrajectories. While syntax learning benefits from more complex utterances,\nword-level learning culminates in better scores with more fragmentary\nutterances. We argue that LMs trained on developmentally plausible data can\ncontribute to debates on how conducive different kinds of linguistic stimuli\nare to language learning.", "AI": {"tldr": "This paper analyzes how different distributions of utterance constructions in German child-directed speech affect language learning in small language models (LMs).", "motivation": "To investigate how the distribution of utterance-level constructions in child-directed speech influences word-level, syntactic, and semantic competence in language learning models.", "method": "Training small language models on a novel collection of developmentally plausible German language data and analyzing their learning trajectories.", "result": "Learning trajectories in the models are robust across various distributions of training constructions, indicating that training data composition has little effect on overall learning outcomes.", "conclusion": "Models benefit from complex utterances for syntax but perform better with simpler, fragmentary utterances for word-level learning; they can inform discussions on effective linguistic stimuli for learning.", "key_contributions": ["Robustness of learning trajectories in small LMs", "Impact of utterance complexity on learning outcomes", "Provision of developmentally plausible language data for research"], "limitations": "", "keywords": ["language models", "child-directed speech", "syntactic competence", "semantic competence", "language learning"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2504.07738", "pdf": "https://arxiv.org/pdf/2504.07738.pdf", "abs": "https://arxiv.org/abs/2504.07738", "title": "Automated Construction of a Knowledge Graph of Nuclear Fusion Energy for Effective Elicitation and Retrieval of Information", "authors": ["Andrea Loreti", "Kesi Chen", "Ruby George", "Robert Firth", "Adriano Agnello", "Shinnosuke Tanaka"], "categories": ["cs.CL"], "comment": null, "summary": "In this document, we discuss a multi-step approach to automated construction\nof a knowledge graph, for structuring and representing domain-specific\nknowledge from large document corpora. We apply our method to build the first\nknowledge graph of nuclear fusion energy, a highly specialized field\ncharacterized by vast scope and heterogeneity. This is an ideal benchmark to\ntest the key features of our pipeline, including automatic named entity\nrecognition and entity resolution. We show how pre-trained large language\nmodels can be used to address these challenges and we evaluate their\nperformance against Zipf's law, which characterizes human-generated natural\nlanguage. Additionally, we develop a knowledge-graph retrieval-augmented\ngeneration system that combines large language models with a multi-prompt\napproach. This system provides contextually relevant answers to\nnatural-language queries, including complex multi-hop questions that require\nreasoning across interconnected entities.", "AI": {"tldr": "This paper details a multi-step method for automated knowledge graph construction, focusing on the nuclear fusion energy domain and leveraging large language models for named entity recognition and resolution, while also evaluating a retrieval-augmented generation system for complex queries.", "motivation": "To structure and represent complex, domain-specific knowledge from large document corpora, exemplified by the nuclear fusion energy field.", "method": "A multi-step approach is employed for automated construction of a knowledge graph, utilizing pre-trained large language models for tasks like named entity recognition and entity resolution, validated against Zipf's law.", "result": "The knowledge graph was successfully constructed, demonstrating effective integration of language models for entity-related tasks and providing contextually relevant answers to complex queries through a novel retrieval-augmented generation system.", "conclusion": "The research shows that leveraging large language models can significantly enhance the automated construction of knowledge graphs and improve the retrieval of answers for complex queries.", "key_contributions": ["Development of a novel multi-step approach for knowledge graph construction.", "Application to nuclear fusion energy knowledge graph.", "Introduction of a retrieval-augmented generation system for enhanced query handling."], "limitations": "Focused specifically on the nuclear fusion domain, which may limit generalizability to other fields; performance evaluation is based on specific language model capabilities.", "keywords": ["knowledge graph", "large language models", "automated construction", "named entity recognition", "retrieval-augmented generation"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.07801", "pdf": "https://arxiv.org/pdf/2506.07801.pdf", "abs": "https://arxiv.org/abs/2506.07801", "title": "MultiMatch: Multihead Consistency Regularization Matching for Semi-Supervised Text Classification", "authors": ["Iustin Sirbu", "Robert-Adrian Popovici", "Cornelia Caragea", "Stefan Trausan-Matu", "Traian Rebedea"], "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "comment": null, "summary": "We introduce MultiMatch, a novel semi-supervised learning (SSL) algorithm\ncombining the paradigms of co-training and consistency regularization with\npseudo-labeling. At its core, MultiMatch features a three-fold pseudo-label\nweighting module designed for three key purposes: selecting and filtering\npseudo-labels based on head agreement and model confidence, and weighting them\naccording to the perceived classification difficulty. This novel module\nenhances and unifies three existing techniques -- heads agreement from\nMultihead Co-training, self-adaptive thresholds from FreeMatch, and Average\nPseudo-Margins from MarginMatch -- resulting in a holistic approach that\nimproves robustness and performance in SSL settings. Experimental results on\nbenchmark datasets highlight the superior performance of MultiMatch, achieving\nstate-of-the-art results on 9 out of 10 setups from 5 natural language\nprocessing datasets and ranking first according to the Friedman test among 19\nmethods. Furthermore, MultiMatch demonstrates exceptional robustness in highly\nimbalanced settings, outperforming the second-best approach by 3.26% -- and\ndata imbalance is a key factor for many text classification tasks.", "AI": {"tldr": "MultiMatch is a semi-supervised learning algorithm that combines co-training and consistency regularization with pseudo-labeling to enhance classification performance and robustness.", "motivation": "The paper addresses the need for improved techniques in semi-supervised learning, particularly in the context of natural language processing and imbalanced data.", "method": "MultiMatch employs a three-fold pseudo-label weighting module that incorporates heads agreement, self-adaptive thresholds, and Average Pseudo-Margins for enhanced label selection and filtering.", "result": "Experimental results demonstrate that MultiMatch achieves state-of-the-art performance on 9 out of 10 setups across 5 NLP datasets and shows increased robustness in imbalanced scenarios.", "conclusion": "The introduction of MultiMatch significantly improves upon existing SSL techniques, offering a unified approach that effectively handles classification difficulties and data imbalance.", "key_contributions": ["Introduction of a novel three-fold pseudo-label weighting module", "Combines existing SSL techniques for improved performance", "Demonstrates state-of-the-art results in challenging NLP datasets"], "limitations": "", "keywords": ["semi-supervised learning", "pseudo-labeling", "natural language processing"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2501.04227", "pdf": "https://arxiv.org/pdf/2501.04227.pdf", "abs": "https://arxiv.org/abs/2501.04227", "title": "Agent Laboratory: Using LLM Agents as Research Assistants", "authors": ["Samuel Schmidgall", "Yusheng Su", "Ze Wang", "Ximeng Sun", "Jialian Wu", "Xiaodong Yu", "Jiang Liu", "Michael Moor", "Zicheng Liu", "Emad Barsoum"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Historically, scientific discovery has been a lengthy and costly process,\ndemanding substantial time and resources from initial conception to final\nresults. To accelerate scientific discovery, reduce research costs, and improve\nresearch quality, we introduce Agent Laboratory, an autonomous LLM-based\nframework capable of completing the entire research process. This framework\naccepts a human-provided research idea and progresses through three\nstages--literature review, experimentation, and report writing to produce\ncomprehensive research outputs, including a code repository and a research\nreport, while enabling users to provide feedback and guidance at each stage. We\ndeploy Agent Laboratory with various state-of-the-art LLMs and invite multiple\nresearchers to assess its quality by participating in a survey, providing human\nfeedback to guide the research process, and then evaluate the final paper. We\nfound that: (1) Agent Laboratory driven by o1-preview generates the best\nresearch outcomes; (2) The generated machine learning code is able to achieve\nstate-of-the-art performance compared to existing methods; (3) Human\ninvolvement, providing feedback at each stage, significantly improves the\noverall quality of research; (4) Agent Laboratory significantly reduces\nresearch expenses, achieving an 84% decrease compared to previous autonomous\nresearch methods. We hope Agent Laboratory enables researchers to allocate more\neffort toward creative ideation rather than low-level coding and writing,\nultimately accelerating scientific discovery.", "AI": {"tldr": "Agent Laboratory is an autonomous LLM-based framework designed to accelerate scientific discovery by automating the entire research process, significantly reducing costs and improving quality through human feedback.", "motivation": "To accelerate scientific discovery, reduce research costs, and improve research quality by automating the research process.", "method": "Agent Laboratory progresses through literature review, experimentation, and report writing stages based on human-provided research ideas, utilizing state-of-the-art LLMs.", "result": "Agent Laboratory driven by o1-preview generates the best research outcomes, with its generated machine learning code achieving state-of-the-art performance. Human involvement improves quality and reduces research expenses by 84%.", "conclusion": "Agent Laboratory aims to shift researchers' focus from coding and writing to creative ideation, thus speeding up scientific discovery.", "key_contributions": ["Introduction of an autonomous LLM-based research framework that covers the entire research process.", "Demonstrated significant cost reduction in research expenditures (84%).", "Proven enhancement of research quality through structured human feedback."], "limitations": "", "keywords": ["autonomous research", "LLM", "scientific discovery", "human feedback", "machine learning"], "importance_score": 9, "read_time_minutes": 15}}
